[{"figure_path": "zJNSbgl4UA/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of different means to slice the ViT architecture. Irregular slicing [2, 38, 4] results in unconventional structures while uniform slicing [37] aligns with the inherent design of ViT to vary from widths.", "description": "This figure illustrates three different ways to slice a Vision Transformer (ViT) architecture. The first method, irregular slicing, leads to non-standard architectures. In contrast, uniform slicing is aligned with the intrinsic design of ViTs. The figure highlights that smaller ViTs (ViT-Ti, ViT-S) are essentially sub-networks of a larger ViT (ViT-B).", "section": "1 Introduction"}, {"figure_path": "zJNSbgl4UA/figures/figures_3_1.jpg", "caption": "Figure 3: Evaluating US-Net over CNN and ViT at unseen width ratios to examine the interpolation (denoted as *) and extrapolation (denoted as +) abilities on ImageNet-1K.", "description": "This figure compares the performance of US-Net, a method for creating slimmable neural networks, on both Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs).  It evaluates the ability of these networks to generalize to unseen width ratios (i.e., network sizes not seen during training). The results show that CNNs exhibit good interpolation and extrapolation capabilities, meaning that their performance remains relatively consistent even when tested at network widths that differ from those used during training. In contrast, ViTs show minimal interpolation ability. This indicates that ViTs are not easily generalized to unseen width ratios, implying challenges in creating efficient and flexible ViT models.", "section": "3 Revisiting Slicing in Vision Transformer"}, {"figure_path": "zJNSbgl4UA/figures/figures_3_2.jpg", "caption": "Figure 2: The available uniform slicing method US-Net [37] lags behind Separate Training (ST) remarkably on ViTs. Performance gaps with ST are shown.", "description": "This figure shows a comparison of the performance of the US-Net method and Separate Training (ST) on Vision Transformers (ViTs) across different width ratios.  The graph clearly indicates that Separate Training consistently outperforms US-Net, highlighting a significant performance gap.  This gap demonstrates that directly applying the uniform slicing technique used in US-Net, which was originally developed for Convolutional Neural Networks (CNNs), does not translate effectively to the ViT architecture.  The results suggest a fundamental difference in how these two network types respond to width variations, which is a key finding discussed in the paper.", "section": "3 Revisiting Slicing in Vision Transformer"}, {"figure_path": "zJNSbgl4UA/figures/figures_6_1.jpg", "caption": "Figure 5: Comparisons of Scala with different slicing granularity and Separate Training (ST). The improvements over ST are shown.", "description": "This figure compares the performance of Scala with different slicing granularities against Separate Training (ST) on the ImageNet-1K dataset.  The x-axis represents the computational cost (GFLOPS), and the y-axis represents the accuracy (%). Different lines represent Scala models trained with varying numbers of sub-networks (X=4, X=7, X=13, X=25), each corresponding to a different slicing granularity.  The gray line represents the performance of the baseline Separate Training method.  The numbers above the lines indicate the percentage improvement in accuracy achieved by each Scala model over Separate Training at each GFLOPS point. The figure demonstrates that Scala achieves comparable or better performance than Separate Training with fewer parameters (lower GFLOPS), particularly at lower computational budgets.", "section": "5.2 Proof-of-Concept"}, {"figure_path": "zJNSbgl4UA/figures/figures_6_2.jpg", "caption": "Figure 5: Comparisons of Scala with different slicing granularity and Separate Training (ST). The improvements over ST are shown.", "description": "This figure compares the performance of Scala, a proposed method for training slimmable Vision Transformers, against Separate Training (ST), a traditional method.  It shows that Scala outperforms ST across various model sizes (represented by GFLOPS), demonstrating its ability to efficiently produce multiple, smaller ViTs from a single trained model. Different lines represent Scala trained with different slicing granularities (the smallest sub-network width s), showcasing the effect of this hyperparameter on the overall performance. The numbers overlaid on the graph indicate the performance gain achieved by Scala over ST for each model size.", "section": "5.2 Proof-of-Concept"}, {"figure_path": "zJNSbgl4UA/figures/figures_6_3.jpg", "caption": "Figure 7: Comparisons of Scala and Separate Training (ST) over the CNN-ViT hybrid architecture Uniformer-S [20]. The improvements over ST are shown.", "description": "This figure compares the performance of Scala and Separate Training (ST) on the Uniformer-S architecture, a hybrid CNN-ViT model.  It demonstrates that Scala consistently outperforms ST across various width ratios, showcasing its effectiveness in achieving comparable or better results with less computational cost. The improvements over ST are visually represented by the positive numbers shown above the bars.", "section": "5.2 Proof-of-Concept"}, {"figure_path": "zJNSbgl4UA/figures/figures_6_4.jpg", "caption": "Figure 8: Comparisons of Scala and Separate Training (ST) over lightweight model Uniformer-XS [20] with token pruning. Improvements over ST are shown.", "description": "This figure compares the performance of Scala and Separate Training (ST) on the Uniformer-XS model, a lightweight model using token pruning.  The x-axis represents GFLOPS (giga-floating point operations), a measure of computational cost. The y-axis shows the accuracy (Acc) in percentage. The graph plots the accuracy achieved by ST and Scala at various GFLOPS levels, achieved by changing the width ratio (r). Numerical values show the improvement in accuracy achieved by Scala over ST at specific GFLOPS levels.  This illustrates that Scala achieves better performance with lower computational costs.", "section": "5.2 Proof-of-Concept"}, {"figure_path": "zJNSbgl4UA/figures/figures_7_1.jpg", "caption": "Figure 5: Comparisons of Scala with different slicing granularity and Separate Training (ST). The improvements over ST are shown.", "description": "This figure compares the performance of Scala with different slicing granularities against Separate Training (ST) on ImageNet-1K.  It shows accuracy results (Acc (%)) plotted against GFLOPs (floating-point operations per second), representing computational cost. Multiple lines represent different slicing granularities (X=13, X=25), and the difference in performance compared to ST is highlighted, demonstrating Scala's efficiency and scalability with varying granularity levels.", "section": "5.2 Proof-of-Concept"}, {"figure_path": "zJNSbgl4UA/figures/figures_8_1.jpg", "caption": "Figure 11: Transferability of Scala. We first conduct pre-training on ImageNet-1K with the help of foundation model DINOv2-B [24]. Then we conduct linear probing on video recognition dataset UCF101. Improvements over ST are shown.", "description": "This figure shows the transferability of Scala to video recognition tasks.  The left subplot demonstrates Scala's superior performance compared to Separate Training (ST) on the ImageNet-1K pre-training dataset across various width ratios.  The right subplot further illustrates that Scala maintains its performance advantage on the UCF101 video dataset after linear probing, showcasing its adaptability to different tasks and consistent performance across multiple width ratios.", "section": "5.4 Transferability"}, {"figure_path": "zJNSbgl4UA/figures/figures_8_2.jpg", "caption": "Figure 5: Comparisons of Scala with different slicing granularity and Separate Training (ST). The improvements over ST are shown.", "description": "This figure compares the performance of Scala, a novel framework for training slimmable Vision Transformers, against Separate Training (ST), a traditional method.  The x-axis represents the GFLOPs (floating point operations per second), a measure of computational cost, while the y-axis shows the accuracy achieved on the ImageNet-1K dataset.  Different lines represent Scala models trained with varying slicing granularities (the number of sub-networks created within the main network), showcasing how Scala's performance changes with different computational budgets.  The positive numbers above each data point show the percentage improvement of Scala compared to the Separate Training method, demonstrating Scala's effectiveness even with varying computational resources.", "section": "5.2 Proof-of-Concept"}, {"figure_path": "zJNSbgl4UA/figures/figures_13_1.jpg", "caption": "Figure 12: Increase the scaling bound to [0.125, 1.000] over DeiT-S [29] on ImageNet-1K.", "description": "This figure shows the performance comparison between Scala and Separate Training (ST) on ImageNet-1K with DeiT-S [29] as the backbone. The x-axis represents GFLOPs, and the y-axis represents accuracy. The scaling bound of Scala is expanded from [0.25, 1.00] to [0.125, 1.000]. This figure demonstrates that Scala still outperforms ST at all width ratios, especially showing a significant advantage at the smallest ratio r = 0.125, even with the expanded scaling bound.", "section": "A.3 Larger Slicing Bound"}, {"figure_path": "zJNSbgl4UA/figures/figures_14_1.jpg", "caption": "Figure 1: Illustration of different means to slice the ViT architecture. Irregular slicing [2, 38, 4] results in unconventional structures while uniform slicing [37] aligns with the inherent design of ViT to vary from widths.", "description": "This figure illustrates different approaches to slicing a Vision Transformer (ViT) architecture.  Irregular slicing, as shown in the leftmost example (i), involves uneven cuts across the layers of the ViT. This approach lacks the inherent structure of the ViT.  In contrast, uniform slicing, as depicted in the other examples (ii), maintains the ViT's architectural integrity and only varies the width (embedding dimension) of the network.  This aligns better with how ViTs inherently scale\u2014by varying their width.", "section": "3 Revisiting Slicing in Vision Transformer"}, {"figure_path": "zJNSbgl4UA/figures/figures_15_1.jpg", "caption": "Figure 14: Verification of Slimmable Ability over DeiT-S [29] on ImageNet-1K. We respectively fix the width ratio of m2, m1 to 0.8125 and 0.4375, and observe the performance at other ratios is not affected.", "description": "This figure shows the results of an experiment designed to test the slimmable ability of Vision Transformers (ViTs).  The researchers fixed certain width ratios (m1 and m2) during training, to only optimize one sub-network at each range of width ratios. The results show that performance at unseen width ratios remains similar to the default setting even though the weights are shared, indicating that correlation between sub-networks in ViTs is weak and highlighting the challenge of making ViTs slimmable.", "section": "A.6 Verification of Slimmable Ability"}, {"figure_path": "zJNSbgl4UA/figures/figures_16_1.jpg", "caption": "Figure 15: Comparison of Scala and MatFormer over DeiT-S. Scala offers a significantly broader scope for computational adjustment compared to MatFormer as MatFormer only scales the FFN block in ViT. The right figure provides a detailed magnification of the left figure.", "description": "The figure compares the performance of Scala and MatFormer on DeiT-S, showing how Scala provides a significantly wider range of computational adjustments compared to MatFormer, which only scales the FFN block in ViT. The right panel shows a zoomed-in view of the left panel, highlighting the differences in performance more clearly.", "section": "5.2 Proof-of-Concept"}]