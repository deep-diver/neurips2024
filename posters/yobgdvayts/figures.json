[{"figure_path": "YOBGdVaYTS/figures/figures_1_1.jpg", "caption": "Figure 1: Experimental setup and possible solutions and mechanisms for the unseen anchor pair (4, 3). (a) Data generation: Left: The single anchors (i.e., 1, 2, 3, 4) correspond to specific arithmetic operations. Middle: During training, 14 out of the 16 possible anchor pairs are assigned inferential (reasoning-based) mappings, one pair (3, 4) is assigned a non-inferential mapping, and the remaining pair (4, 3) is held out as an unseen task (does not appear in the training). Right: The input sequences comprise an anchor pair, a key item preceding the anchor pair, and noise items unrelated to the target. The question mark indicates the output for the unseen anchor pair (4, 3), which depends on the learned solution. (b) Two potential mechanisms for the unseen anchor pair (4, 3): learning the symmetric structure (Mechanism 1) or composing the inferred single anchor mappings (Mechanism 2).", "description": "This figure illustrates the experimental setup and possible model behaviors for an unseen anchor pair.  Part (a) details the data generation process, showing how single anchor functions are combined to create composite functions.  Training data includes mostly inferential mappings (consistent with the composite function) with one non-inferential mapping.  A single unseen anchor pair's output will determine whether the model reasoned or memorized. Part (b) shows two possible mechanisms: Mechanism 1 (memorization) creates a symmetric solution based on the non-inferential example; Mechanism 2 (reasoning) composes solutions from the learned individual single anchor mappings.", "section": "4 Two Phases of Solutions for Composite Functions"}, {"figure_path": "YOBGdVaYTS/figures/figures_4_1.jpg", "caption": "Figure 2: (a,b) Phase diagram of generalization performance on the unseen anchor (4, 3). (a) The model's test accuracy based on the symmetric mapping. (b) The model's test accuracy based on the inferential mapping. The abscissa represents the initialization rate \u03b3, which corresponds to the standard deviation (1/din) of a normal distribution with a mean of 0 used for parameter initialization. The ordinate represents the depth of the transformer model. The shadow zones indicate the test accuracy on seen anchors is less than 90%. (c) Comparison of accuracy on the unseen anchor (4, 3) for both the inferential and symmetric solutions across different initialization rates \u03b3 on GPT-2. The error bars represent the standard deviation across 4-time runs.", "description": "This figure shows a phase diagram illustrating the relationship between model depth, initialization rate (\u03b3), and the accuracy of inferential and symmetric solutions on an unseen anchor pair (4,3).  The shadow areas highlight regions where the model performs poorly on seen anchor pairs.  A separate panel shows a comparison of these solutions across various initialization rates using the GPT-2 model, indicating a similar trend.", "section": "4 Two Phases of Solutions for Composite Functions"}, {"figure_path": "YOBGdVaYTS/figures/figures_5_1.jpg", "caption": "Figure 3: (a, c) Information flow in the two-layer networks of symmetric and inferential solutions. The input sequence shown in the figure represents the test sample, with key items and anchor positions annotated. For each layer\u2019s attention matrix, we illustrate the mechanisms of information transmission and fusion through the information flow. The thickness of the line represents the corresponding value in the attention matrix Attn(\u00b9). We use different colors to mark the key item and the two single anchors, and highlight the attention connections that significantly contribute to the final output. The final output sequence represents the model\u2019s output. (a) Symmetric solution. (c) Inferential solution. (b) T-SNE visualization of vectors Xao(1) of 10,000 input sequences with different anchor-key item pairs. Symmetric anchor pairs have similar colors in different shades.", "description": "This figure visualizes the information flow in two-layer neural networks for both symmetric and inferential solutions, illustrating how information is transmitted and fused through the attention mechanism. It also includes a t-SNE visualization of the vector representations, showing how symmetric anchor pairs are clustered together in the vector space.", "section": "5 Mechanisms of Models in Two Phases"}, {"figure_path": "YOBGdVaYTS/figures/figures_6_1.jpg", "caption": "Figure 4: Cosine similarity heatmaps for vector representations in different solutions. Each axis represents a selected anchor pair (labeled on the axis), with the value on the coordinate axis representing the value of the key item. The color indicates the cosine similarity between specific vectors defined in each subplot. Red boxes highlight positions where the target outputs obtained by the anchors on the abscissa and ordinate are the same for the corresponding key items. (a, b) Cosine similarity between the output vectors of the second attention layer's last token (the last token of Xao(2)) for different anchor-key item pairs in (a) inferential and (b) symmetric solutions. (c) Cosine similarity between the rows of the second layer Value matrix (V(2)) corresponding to the first anchor's position across different anchor-key item pairs for inferential solutions.", "description": "This figure shows heatmaps visualizing cosine similarity between output vectors (last token of the second attention layer) for different anchor-key item pairs, comparing inferential and symmetric solutions. Red boxes highlight instances where inferential targets are identical.  The subfigure (c) focuses on the cosine similarity within the Value matrix (V(2)) for inferential solutions, relating to individual anchor mappings.", "section": "5.2 Divergence in Fused Vector Representations across Two Phases"}, {"figure_path": "YOBGdVaYTS/figures/figures_7_1.jpg", "caption": "Figure 3: (a, c) Information flow in the two-layer networks of symmetric and inferential solutions. The input sequence shown in the figure represents the test sample, with key items and anchor positions annotated. For each layer's attention matrix, we illustrate the mechanisms of information transmission and fusion through the information flow. The thickness of the line represents the corresponding value in the attention matrix Attn(\u00b9). We use different colors to mark the key item and the two single anchors, and highlight the attention connections that significantly contribute to the final output. The final output sequence represents the model's output. (a) Symmetric solution. (c) Inferential solution. (b) T-SNE visualization of vectors Xao(1) of 10,000 input sequences with different anchor-key item pairs. Symmetric anchor pairs have similar colors in different shades.", "description": "This figure visualizes the information flow and vector representations in two-layer transformer networks for symmetric and inferential solutions.  Panel (a) and (c) show the information flow through the attention mechanism for each solution type, highlighting how information from key items and anchors combines to produce the output.  Panel (b) uses t-SNE to visualize the vector representations of the model's output after the first attention layer, demonstrating that symmetric solutions cluster similar anchor pairs together.", "section": "5 Mechanisms of Models in Two Phases"}, {"figure_path": "YOBGdVaYTS/figures/figures_8_1.jpg", "caption": "Figure 6: Performance comparison of models with different initialization scales and weight decay coefficients on compositional tasks. (a) For the SCAN task, we assess the generalization ability on composite commands that include the \"turn left\" command. (b, c) For the COGS task, we evaluate (b) in-distribution and (c) out-of-distribution generalization after training on the same dataset. Small initialization and large weight decay (blue) consistently outperform large initialization and small weight decay (orange) across different tasks and data scales.", "description": "The figure shows the performance comparison of models with different initialization scales and weight decay coefficients on three compositional tasks: SCAN (unseen command: turn left), COGS (in-distribution data), and COGS (out-of-distribution data).  The results demonstrate that models with smaller initialization scales and larger weight decay consistently outperform models with larger initialization scales and smaller weight decay across different tasks and dataset sizes, showcasing their superior generalization abilities. ", "section": "Further Verification on Realistic Tasks"}, {"figure_path": "YOBGdVaYTS/figures/figures_9_1.jpg", "caption": "Figure 2: (a,b) Phase diagram of generalization performance on the unseen anchor (4, 3). (a) The model's test accuracy based on the symmetric mapping. (b) The model's test accuracy based on the inferential mapping. The abscissa represents the initialization rate \u03b3, which corresponds to the standard deviation (1/din) of a normal distribution with a mean of 0 used for parameter initialization. The ordinate represents the depth of the transformer model. The shadow zones indicate the test accuracy on seen anchors is less than 90%. (c) Comparison of accuracy on the unseen anchor (4, 3) for both the inferential and symmetric solutions across different initialization rates \u03b3 on GPT-2. The error bars represent the standard deviation across 4-time runs.", "description": "This figure shows a phase diagram illustrating the relationship between model depth, initialization rate (\u03b3), and the model's ability to generalize to unseen compositional tasks.  The diagram is separated into sections demonstrating performance based on either a symmetric or inferential solution.  Shadowed regions highlight instances where the model's performance on seen anchors drops below 90% accuracy.  A final subplot compares results from GPT-2 across different initialization rates.", "section": "Two Phases of Solutions for Composite Functions"}, {"figure_path": "YOBGdVaYTS/figures/figures_16_1.jpg", "caption": "Figure 2: (a,b) Phase diagram of generalization performance on the unseen anchor (4, 3). (a) The model's test accuracy based on the symmetric mapping. (b) The model's test accuracy based on the inferential mapping. The abscissa represents the initialization rate \u03b3, which corresponds to the standard deviation (1/din) of a normal distribution with a mean of 0 used for parameter initialization. The ordinate represents the depth of the transformer model. The shadow zones indicate the test accuracy on seen anchors is less than 90%. (c) Comparison of accuracy on the unseen anchor (4, 3) for both the inferential and symmetric solutions across different initialization rates \u03b3 on GPT-2. The error bars represent the standard deviation across 4-time runs.", "description": "This figure shows a phase diagram of the model's generalization performance on an unseen anchor pair (4,3) based on whether it learns a symmetric or inferential solution.  The diagram is split into two subplots, one for each solution type. Initialization rate (\u03b3) is plotted on the x-axis, and model depth on the y-axis. Test accuracy is represented on the graphs' z-axis, with shaded regions indicating when test accuracy on seen anchor pairs is below 90%. A third subplot shows a comparison of the accuracy of both solution types on a GPT-2 model. This figure helps to demonstrate the impact of initialization scale on the type of solution learned by the model.", "section": "Two Phases of Solutions for Composite Functions"}, {"figure_path": "YOBGdVaYTS/figures/figures_17_1.jpg", "caption": "Figure 1: Experimental setup and possible solutions and mechanisms for the unseen anchor pair (4, 3). (a) Data generation: Left: The single anchors (i.e., 1, 2, 3, 4) correspond to specific arithmetic operations. Middle: During training, 14 out of the 16 possible anchor pairs are assigned inferential (reasoning-based) mappings, one pair (3, 4) is assigned a non-inferential mapping, and the remaining pair (4, 3) is held out as an unseen task (does not appear in the training). Right: The input sequences comprise an anchor pair, a key item preceding the anchor pair, and noise items unrelated to the target. The question mark indicates the output for the unseen anchor pair (4, 3), which depends on the learned solution. (b) Two potential mechanisms for the unseen anchor pair (4, 3): learning the symmetric structure (Mechanism 1) or composing the inferred single anchor mappings (Mechanism 2).", "description": "This figure illustrates the experimental setup and possible solutions for an unseen anchor pair in a compositional task.  It shows how synthetic data is generated using anchor pairs (representing arithmetic operations) and key items. The training data includes 14 inferential (reasoning-based) mappings, one non-inferential (memory-based) mapping, and one unseen anchor pair. The figure then illustrates two possible mechanisms the model might employ to solve the unseen pair: either learning a symmetric structure based on the seen non-inferential pair, or composing mappings learned from individual anchors.", "section": "Two Phases of Solutions for Composite Functions"}, {"figure_path": "YOBGdVaYTS/figures/figures_18_1.jpg", "caption": "Figure 10: Cosine similarity matrices of WQ(1) neurons at different depths and initialization scales. Each subgraph represents a different depth (from 2 to 6, bottom to top) and initialization rate \u03b3 (from 0.3 to 0.7, left to right). Colors indicate the cosine similarity between neurons, with warmer colors representing higher similarity. The neurons are grouped by cosine similarity greater than 0.7 to highlight the condensation properties. As the initialization scale increases, neurons exhibit more condensation, indicating decreased model complexity.", "description": "This figure visualizes the cosine similarity matrices of the input weights (WQ(1)) in the first attention layer of a transformer model.  It shows how the weight matrices change with varying model depths (2-6 layers) and initialization rates (\u03b3 = 0.3 to 0.7).  Warmer colors represent higher cosine similarity between neurons, indicating condensation or clustering of weights.  The figure demonstrates that as the initialization scale increases (\u03b3 values get smaller), there is more condensation, resulting in lower model complexity.", "section": "D Detailed Results for Model Complexity with Different Initialization"}, {"figure_path": "YOBGdVaYTS/figures/figures_19_1.jpg", "caption": "Figure 2: (a,b) Phase diagram of generalization performance on the unseen anchor (4, 3). (a) The model's test accuracy based on the symmetric mapping. (b) The model's test accuracy based on the inferential mapping. The abscissa represents the initialization rate \u03b3, which corresponds to the standard deviation (1/din) of a normal distribution with a mean of 0 used for parameter initialization. The ordinate represents the depth of the transformer model. The shadow zones indicate the test accuracy on seen anchors is less than 90%. (c) Comparison of accuracy on the unseen anchor (4, 3) for both the inferential and symmetric solutions across different initialization rates \u03b3 on GPT-2. The error bars represent the standard deviation across 4-time runs.", "description": "This figure shows a phase diagram illustrating the relationship between initialization rate, model depth, and the model's ability to learn either symmetric or inferential solutions for an unseen compositional task.  The diagram highlights two distinct phases of solutions based on the initialization scale, with smaller scales favoring inferential solutions and larger scales favoring symmetric solutions.  The shadow regions indicate poor generalization on seen data points.  A supplementary graph shows a similar relationship on a larger, more complex model (GPT-2).", "section": "Two Phases of Solutions for Composite Functions"}, {"figure_path": "YOBGdVaYTS/figures/figures_20_1.jpg", "caption": "Figure 12: Eigenvalues of the covariance matrix of the embedding matrix for different initialization scales and the evolution process of the eigenvalues of the small initialization model. Left: Eigenvalues of the covariance matrix of the embedding vectors for different initialization scales. The abscissa is the eigenvalue index, and the ordinate is the eigenvalue. Colors represent different initialization scales. The definition of the initial scale \u03b3 is consistent with Fig. 2. Right: The evolution process of the eigenvalues of specific indexes of the small parameter initialization model as the training progresses.", "description": "This figure shows the eigenvalues of the covariance matrix of the word embedding matrix for different initialization scales (\u03b3=0.5 and \u03b3=0.8). The left panel shows the eigenvalues at the end of training, while the right panel shows how the eigenvalues evolve during training for the smaller initialization scale (\u03b3=0.5). The figure demonstrates how the initialization scale affects the model's complexity, with smaller scales leading to lower complexity and more condensed eigenvalue distributions.", "section": "5.3 Model Complexity: A Key Factor in Phase Transitions"}, {"figure_path": "YOBGdVaYTS/figures/figures_20_2.jpg", "caption": "Figure 12: Eigenvalues of the covariance matrix of the embedding matrix for different initialization scales and the evolution process of the eigenvalues of the small initialization model. Left: Eigenvalues of the covariance matrix of the embedding vectors for different initialization scales. The abscissa is the eigenvalue index, and the ordinate is the eigenvalue. Colors represent different initialization scales. The definition of the initial scale \u03b3 is consistent with Fig. 2. Right: The evolution process of the eigenvalues of specific indexes of the small parameter initialization model as the training progresses.", "description": "This figure shows the eigenvalues of the covariance matrix of the word embedding matrix for different initialization scales (left) and the evolution of eigenvalues during training for a model with small initialization (right). The left panel demonstrates how the eigenvalue distribution changes with different initialization scales, indicating a low-complexity trend for small initialization scales. The right panel shows how the eigenvalues evolve over epochs, suggesting a gradual increase in model complexity during training. These observations support the paper's hypothesis about the relationship between initialization scale, model complexity, and solution type.", "section": "5.3 Model Complexity: A Key Factor in Phase Transitions"}, {"figure_path": "YOBGdVaYTS/figures/figures_21_1.jpg", "caption": "Figure 13: Singular value distributions of the weight matrices across various linear layers for models with different initializations. Each subplot corresponds to a specific linear layer, with blue curves representing the small initialization model (\u03b3 = 0.5) and red curves representing the large initialization model (\u03b3 = 0.8). The abscissa denotes the singular value index, and the ordinate denotes the singular value magnitude on a logarithmic scale.", "description": "This figure displays singular value distributions for weight matrices across different linear layers in a transformer model, comparing models trained with small (\u03b3=0.5) and large (\u03b3=0.8) initialization scales.  The plots show how the magnitude of singular values decreases with increasing index, illustrating the distribution's concentration for the low-complexity model (small initialization). This visualization helps to analyze model complexity and its impact on solutions.", "section": "5.3 Model Complexity: A Key Factor in Phase Transitions"}, {"figure_path": "YOBGdVaYTS/figures/figures_22_1.jpg", "caption": "Figure 14: The impact of learning rate and weight decay on the accuracy of inferential solutions in a 3-layer, 1-head transformer model. The heatmap displays the mean accuracy of the model's inferential solutions on the unseen anchor pair (4, 3) across nine independent experiments. The x-axis represents different weight decay coefficients, and the y-axis represents different learning rates. The color bar on the right indicates the accuracy of the inferential solutions on the unseen anchor (4, 3), with higher values corresponding to better performance.", "description": "This heatmap visualizes how different learning rates and weight decay coefficients affect the accuracy of inferential solutions in a 3-layer, single-head transformer model.  Higher accuracy (yellow) indicates better performance at learning inferential solutions for the unseen anchor pair (4,3).", "section": "E Learning Rate and Weight Decay Coefficient Affecting Solution Phases"}, {"figure_path": "YOBGdVaYTS/figures/figures_23_1.jpg", "caption": "Figure 10: Cosine similarity matrices of WQ(1) neurons at different depths and initialization scales. Each subgraph represents a different depth (from 2 to 6, bottom to top) and initialization rate \u03b3 (from 0.3 to 0.7, left to right). Colors indicate the cosine similarity between neurons, with warmer colors representing higher similarity. The neurons are grouped by cosine similarity greater than 0.7 to highlight the condensation properties. As the initialization scale increases, neurons exhibit more condensation, indicating decreased model complexity.", "description": "This figure visualizes the cosine similarity matrices of the weight matrices (WQ(1)) of a transformer model's first attention layer. It shows how the model's complexity changes depending on the depth of the network and the initialization rate (\u03b3).  Warmer colors in the heatmaps represent higher cosine similarity between neurons, indicating condensation of weights.  The figure demonstrates that as the initialization rate increases (smaller initialization scale), the neurons exhibit stronger condensation, leading to lower model complexity.", "section": "D Detailed Results for Model Complexity with Different Initialization"}, {"figure_path": "YOBGdVaYTS/figures/figures_25_1.jpg", "caption": "Figure 2: (a,b) Phase diagram of generalization performance on the unseen anchor (4, 3). (a) The model's test accuracy based on the symmetric mapping. (b) The model's test accuracy based on the inferential mapping. The abscissa represents the initialization rate \u03b3, which corresponds to the standard deviation (1/din) of a normal distribution with a mean of 0 used for parameter initialization. The ordinate represents the depth of the transformer model. The shadow zones indicate the test accuracy on seen anchors is less than 90%. (c) Comparison of accuracy on the unseen anchor (4, 3) for both the inferential and symmetric solutions across different initialization rates \u03b3 on GPT-2. The error bars represent the standard deviation across 4-time runs.", "description": "This figure shows the performance of the model on the unseen anchor pair (4,3) based on both symmetric and inferential mapping with various initialization rates and model depths.  The shadow area indicates that the model's performance is poor on seen anchor pairs.  Subfigure (c) extends the experiment to the GPT-2 model.", "section": "Two Phases of Solutions for Composite Functions"}]