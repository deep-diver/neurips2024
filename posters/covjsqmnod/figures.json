[{"figure_path": "CovjSQmNOD/figures/figures_4_1.jpg", "caption": "Figure 1: Illustration on rasterization process of ODGS. We describe the process of projecting a 3D Gaussian to the omnidirectional pixel space. (a) The coordinate is transformed from the original camera pose (black) to the target Gaussian (green), making the z-axis of the coordinate head towards the center of the Gaussian. (b) The Gaussian is projected onto the corresponding tangent plane. (c) The projected Gaussian is horizontally stretched when transformed into equirectangular space. (d) The Gaussian in equirectangular space is linearly transformed to the pixel space, followed by a combination with the other projected Gaussian.", "description": "This figure illustrates the rasterization process of the ODGS method for omnidirectional images. It breaks down the process into four steps: coordinate transformation, tangent plane projection, horizontal stretching, and rescaling and combination. Each step is shown visually with a diagram and explained concisely. The figure provides a visual understanding of how a 3D Gaussian is projected onto a 2D omnidirectional image.", "section": "3 Methods"}, {"figure_path": "CovjSQmNOD/figures/figures_7_1.jpg", "caption": "Figure 2: Changes of PSNR, SSIM, and LPIPS according to the optimization time for each method. ODGS shows the best result as well as the highest convergence speed in both scenes.", "description": "This figure shows the PSNR, SSIM, and LPIPS scores over optimization time for different methods on two example scenes (OmniBlender/bistro_square and OmniPhotos/Ballintoy).  It visually demonstrates the superior performance and faster convergence speed of ODGS compared to NeRF(P), 3DGS(P), TensoRF, and EgoNeRF.  The plots show how each metric changes as the optimization process continues, highlighting ODGS's quicker improvement and better final results.", "section": "4.2 Experiment Results"}, {"figure_path": "CovjSQmNOD/figures/figures_8_1.jpg", "caption": "Figure 3: Qualitative comparisons in the egocentric scenes (10 min.). Each scene is brought from Ricoh360, OmniBlender, and OmniPhotos, respectively. Best viewed when zoomed in.", "description": "This figure presents a qualitative comparison of 3D reconstruction results between different methods (3DGS(P), EgoNeRF, and ODGS) and the ground truth for egocentric scenes.  The comparison highlights the visual differences in reconstruction quality, showing how ODGS achieves a superior reconstruction in terms of sharpness and detail preservation compared to the other methods.  The images show details and textures are much better reconstructed by ODGS than the other two methods. The images are from three datasets: Ricoh360, OmniBlender, and OmniPhotos, and all methods were trained for 10 minutes before inference.", "section": "4.2 Experiment Results"}, {"figure_path": "CovjSQmNOD/figures/figures_8_2.jpg", "caption": "Figure 3: Qualitative comparisons in the egocentric scenes (10 min.). Each scene is brought from Ricoh360, OmniBlender, and OmniPhotos, respectively. Best viewed when zoomed in.", "description": "This figure shows a qualitative comparison of 3D reconstruction results on three egocentric datasets (Ricoh360, OmniBlender, and OmniPhotos) using four different methods: ground truth, 3DGS(P), EgoNeRF, and ODGS.  Each row represents a different scene, and each column represents a different method. The images are cropped to highlight the differences in reconstruction quality, particularly in terms of detail and accuracy.  The ODGS method shows better reconstruction quality, with sharper details and fewer artifacts compared to the other methods.", "section": "4.2 Experiment Results"}, {"figure_path": "CovjSQmNOD/figures/figures_9_1.jpg", "caption": "Figure 5: Qualitative comparison of rendered images according to the Gaussian densification policy during optimization.", "description": "This figure shows a qualitative comparison of rendered images using different Gaussian densification policies. (a) shows the full ground truth image, and (b) shows a cropped version of the ground truth used as a reference. (c) presents results obtained using a static threshold, highlighting artifacts and splits in the lanes. In contrast, (d) shows results obtained using the proposed dynamic threshold, demonstrating markedly more accurate and clean lane representation.", "section": "Ablation Study: Dynamic Densification Strategy for Omnidirectional Images"}]