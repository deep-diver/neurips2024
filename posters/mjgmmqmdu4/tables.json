[{"figure_path": "MJgMMqMDu4/tables/tables_5_1.jpg", "caption": "Table 1: Test accuracy (%) of pruned ResNet-32 on CIFAR-10/100.", "description": "This table presents the test accuracy results for pruned ResNet-32 on CIFAR-10 and CIFAR-100 datasets.  It compares the performance of various sparse training methods (LT, SNIP, GraSP, SET, DSR, RigL, MEST) with and without the proposed S2-SAM method.  Different pruning ratios (90%, 95%, 98%) are tested for each method. The numbers in parentheses indicate the accuracy improvement achieved by adding S2-SAM.", "section": "3.1 Accuracy Enhancement for State-of-the-art Sparse Training Methods"}, {"figure_path": "MJgMMqMDu4/tables/tables_6_1.jpg", "caption": "Table 2: Results of ResNet-50 on ImageNet-1K.", "description": "This table presents the results of applying different sparse training methods and S2-SAM to ResNet-50 on the ImageNet-1K dataset.  For each method, it shows the sparsity level (80% and 90%), the sparsity distribution (uniform or non-uniform), the top-1 accuracy, and the training and inference FLOPs (floating-point operations).  The table highlights the improvement in top-1 accuracy achieved by using S2-SAM in conjunction with various sparse training methods. It also demonstrates the effect of S2-SAM on different sparsity levels and sparsity patterns.", "section": "3.1 Accuracy Enhancement for State-of-the-art Sparse Training Methods"}, {"figure_path": "MJgMMqMDu4/tables/tables_7_1.jpg", "caption": "Table 3: Accuracy of S2-SAM on structured sparse training CHEX [10] and Chase [39].", "description": "This table presents the results of applying the proposed S2-SAM method to structured sparse training methods, specifically CHEX and Chase, on ResNet-34 and ResNet-50 networks.  It compares the accuracy achieved with original training methods versus those employing S2-SAM, illustrating the performance gains obtained by incorporating S2-SAM.  FLOPs (floating point operations) are also provided to show the computational cost.", "section": "3.2 Sparse Training with Structured Sparsity"}, {"figure_path": "MJgMMqMDu4/tables/tables_7_2.jpg", "caption": "Table 4: Training speed of SAM [25] and S2-SAM for different sparse training at 90% sparsity.", "description": "This table compares the training speed (throughput) of three different sparse training methods (GraSP, RigL, and MEST (EM)) with and without the proposed S2-SAM and the original SAM method.  The throughput is measured in images per second (imgs/s) and shows the impact of each optimization technique on the training speed. The results suggest that S2-SAM maintains a comparable training speed to the original methods while SAM shows significantly lower throughput.", "section": "3.4 Training Speed Comparison on GPU"}, {"figure_path": "MJgMMqMDu4/tables/tables_7_3.jpg", "caption": "Table 5: Testing accuracy on ImageNet-C test set. We compare the results with and without S2-SAM using 80% sparsity.", "description": "This table presents the ImageNet-C test accuracy results for different sparse training methods, both with and without the application of the proposed S2-SAM method.  It highlights the improvement in robustness against image corruptions that S2-SAM provides.  The 80% sparsity level is consistent across all models. The improvement in accuracy (shown in parentheses) is particularly noteworthy for the challenging ImageNet-C dataset, demonstrating S2-SAM's effectiveness in enhancing model generalization and robustness.", "section": "3.5 Robustness Improvement by S2-SAM"}, {"figure_path": "MJgMMqMDu4/tables/tables_7_4.jpg", "caption": "Table 1: Test accuracy (%) of pruned ResNet-32 on CIFAR-10/100.", "description": "This table presents the test accuracy results for pruned ResNet-32 on CIFAR-10 and CIFAR-100 datasets.  It compares the performance of several state-of-the-art sparse training methods (LT, SNIP, GraSP, SET, DSR, RigL, and MEST) with and without the proposed S2-SAM method.  The results are shown for different pruning ratios (90%, 95%, and 98%).  The improvement achieved by adding S2-SAM to each baseline method is also indicated.", "section": "3.1 Accuracy Enhancement for State-of-the-art Sparse Training Methods"}, {"figure_path": "MJgMMqMDu4/tables/tables_13_1.jpg", "caption": "Table A.1: Test accuracy (%) of pruned VGG-19 on CIFAR-10/100.", "description": "This table presents the test accuracy results achieved by various sparse training methods when applied to the VGG-19 model on CIFAR-10 and CIFAR-100 datasets. Different pruning ratios (90%, 95%, and 98%) are considered.  The results are compared with and without the application of the proposed S2-SAM method, highlighting the accuracy improvements obtained. The table showcases the impact of S2-SAM across multiple existing sparse training techniques, both static and dynamic.", "section": "A Results of VGG-19 on CIFAR-10/100"}]