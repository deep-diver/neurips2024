[{"figure_path": "MJgMMqMDu4/figures/figures_1_1.jpg", "caption": "Figure 1: The loss surface visualization for training a sparse neural network using ResNet-32 on CIFAR-10. We select two representative sparse training methods [18, 3] and incorporate different levels of sparsity. We also quantify the loss surface behavior using coefficient Ra [19] to evaluate sharpness. With increased sparsity, Ra becomes larger, indicating sharper and steeper surface.", "description": "This figure visualizes the loss surface behavior for training sparse neural networks using ResNet-32 on CIFAR-10.  Two different sparse training methods (GraSP and MEST-EM) are shown, each with varying levels of sparsity (0.8, 0.9, 0.95, 0.98).  The sharpness of the loss surface is quantified using the coefficient Ra.  The visualization demonstrates that as sparsity increases, the loss surface becomes sharper and steeper, indicating a more challenging optimization problem.", "section": "1 Introduction"}, {"figure_path": "MJgMMqMDu4/figures/figures_2_1.jpg", "caption": "Figure 2: Illustration of the optimization mechanism of S\u00b2-SAM. The perturbation on the current weights is approximated by the weight gradients from prior step. Please see Section 2.2 for detailed discussion.", "description": "This figure illustrates how S\u00b2-SAM approximates the sharpness perturbation using the gradient from the previous step.  In traditional SAM, two gradient computations are needed: one at the current weights and another at the weights perturbed by the sharpness.  S\u00b2-SAM simplifies this by approximating the perturbed gradient using only the gradient from the previous step, resulting in zero extra computational cost.  The figure shows the weight update path for both the original SAM method and the proposed S\u00b2-SAM method, highlighting how S\u00b2-SAM efficiently achieves a similar effect. This approximation is based on the intuition that the gradient direction from the previous step represents a direction of relatively high sharpness.", "section": "2 Proposed Method"}, {"figure_path": "MJgMMqMDu4/figures/figures_6_1.jpg", "caption": "Figure 3: Loss surface sharpness comparison of different sparse training methods with original training and with S2-SAM. We also quantitatively evaluate the coefficient Ra. Using S2-SAM compared to the original method results in a smaller Ra, indicating a wider and smoother loss surface, which suggests improved generalization ability.", "description": "This figure visualizes the loss surface of three different sparse training methods (SNIP, GraSP, and MEST) at various sparsity levels (0.9, 0.95, and 0.98).  For each method and sparsity level, two 3D plots are shown: one for the original training and one for training with the proposed S2-SAM method. The plots illustrate the shape of the loss landscape, with the color intensity representing the loss value.  A key observation is that S2-SAM consistently leads to a wider and smoother loss surface (indicated by the lower Ra values), which is associated with better generalization performance. The Ra coefficient quantifies the sharpness of the loss surface; a lower Ra value indicates a flatter and less chaotic landscape.", "section": "3.3 Improvement on the Roughness of the Loss Surface"}]