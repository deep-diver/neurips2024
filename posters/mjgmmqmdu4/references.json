{"references": [{"fullname_first_author": "Jonathan Frankle", "paper_title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks", "publication_date": "2018-00-00", "reason": "This paper introduces the Lottery Ticket Hypothesis, a foundational concept in sparse training that significantly impacts the field's direction and methodology."}, {"fullname_first_author": "Decebal Constantin Mocanu", "paper_title": "Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science", "publication_date": "2018-00-00", "reason": "This paper presents SET, a dynamic sparse training method that significantly advances the state-of-the-art in sparse neural network training."}, {"fullname_first_author": "Pierre Foret", "paper_title": "Sharpness-aware minimization for efficiently improving generalization", "publication_date": "2021-00-00", "reason": "This paper introduces Sharpness-Aware Minimization (SAM), a crucial technique that addresses the challenge of achieving both efficiency and accuracy in training, inspiring the core method of the current work."}, {"fullname_first_author": "Gen Li", "paper_title": "MEST: Accurate and fast memory-economic sparse training framework on the edge", "publication_date": "2021-00-00", "reason": "This paper introduces MEST, a dynamic sparse training method that achieves significant improvements in both accuracy and efficiency, providing a strong baseline for comparison in the current research."}, {"fullname_first_author": "Utku Evci", "paper_title": "The difficulty of training sparse neural networks", "publication_date": "2019-00-00", "reason": "This paper highlights the inherent challenges in training sparse neural networks, providing critical context and motivation for exploring innovative training methodologies like the one presented in the current work."}]}