[{"type": "text", "text": "Active Set Ordering ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Quoc Phong Nguyen1,3, Sunil Gupta1, Svetha Venkatesh1, Bryan Kian Hsiang Low2, Patrick Jaillet3 ", "page_idx": 0}, {"type": "text", "text": "Applied Artificial Intelligence Institute, Deakin University, Australia 2School of Computing, National University of Singapore, Singapore 3LIDS and EECS, Massachusetts Institute of Technology, USA qphongmp@gmail.com, sunil.gupta@deakin.edu.au, svetha.venkatesh@deakin.edu.au, lowkh@comp.nus.edu.sg, jaillet@mit.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we formalize the active set ordering problem, which involves actively discovering a set of inputs based on their orderings determined by expensive evaluations of a blackbox function. We then propose the mean prediction (MP) algorithm and theoretically analyze it in terms of the regret of predicted pairwise orderings between inputs. Notably, as a special case of this framework, we can cast Bayesian optimization as an active set ordering problem by recognizing that maximizers can be identified solely by comparison rather than by precisely estimating the function evaluations. As a result, we are able to construct the popular Gaussian process upper confidence bound (GP-UCB) algorithm through the lens of ordering with several nuanced insights. We empirically validate the performance of our proposed solution using various synthetic functions and real-world datasets. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In real-world applications, we often encounter the problem of estimating an unknown function, known as a blackbox function, (i.e., those without closed-form expressions or derivatives) using their expensive and noisy evaluations. Under these circumstances, an efficient sequential process of evaluating the function is desired. On one extreme, experimental design (ED) aims to estimate the function in its entire input domain, e.g., by decreasing the uncertainty of the function globally in Bayesian ED [4, 18]. On the other extreme, the renowned Bayesian optimization (BO) targets inputs with the extreme function values such as the maximizers and the minimizers [2, 6, 7]. ", "page_idx": 0}, {"type": "text", "text": "While the connection between ED and BO is studied in the classic work of [19], we still lack a problem formulation that strikes a balance between the prohibitively expensive process of estimating the entire function globally in ED and the lack of information about the function away from extreme locations in BO. One may consider a related problem, called level set estimation (LSE), which focuses on estimating inputs with function evaluations above or below a given (or implicit) threshold [1, 3, 8, 14]. However, without domain knowledge of the blackbox function, it is easy to set a threshold that leads to undesirably large or small level sets. ", "page_idx": 0}, {"type": "text", "text": "Let us consider an environmental monitoring problem of estimating a chemical concentration in a field. The blackbox function is the mapping from locations of the field to the chemical concentration measurement. It can be of a greater interest to estimate the maximizers, the minimizers, the top- $k$ locations (with the highest chemical concentration) and the bottom- ${\\cdot k}$ locations. On one hand, these estimates provide more information about the blackbox function than just the maximizers or minimizers in BO. On the other hand, they may require less resource (i.e., evaluations of the blackbox function) than estimating the entire function in ED. Besides, as the top- $k$ locations consist of exactly $k$ locations in the field, it circumvents the issue of undesirably large or small level sets in LSE. ", "page_idx": 0}, {"type": "text", "text": "Our main contribution in this paper is to formulate the above challenge and resolve it with a theoretically grounded solution. Specifically, we propose the active set ordering problem to capture the above scenario (Sec. 2.1). It aims to estimate subsets of the input domain that are defined based on pairwise comparisons/orderings between the blackbox function evaluations.1These subsets include the maximizers, the minimizers, and the top- $k$ inputs with the highest function evaluations. Like Bayesian ED, BO, and LSE, we adopt the pool-based active learning setting [18] in constructing a solution that sequentially selects a sampling input from the domain at each iteration. The knowledge from observing function evaluations at the sampling inputs helps predicting the subsets of interest and directs the algorithm to select the next sampling input. To facilitate the presentation of our method, we begin with the building block of our ordering-based problem: pairwise comparison/ordering between function evaluations in Sec. 3. Specifically, we propose a new kind of regret to quantify the loss of a pairwise ordering (Sec. 3.1), a prediction of the top- $k$ inputs based on only the posterior mean (Sec. 3.2), and a sampling strategy that is equipped with a theoretical performance guarantee for the proposed prediction (Sec. 3.3). Subsequently, these concepts of the regret, the prediction, and the sampling strategy are extended to orderings between sets, which ultimately addresses the active set ordering problem in Sec. 4. Notably, the regret simplifies to the well-known regret in BO (Remark 4.1). Hence, we recover both the theoretical analysis and the GP-UCB algorithm [19] when considering a special case of our problem setting (Remark 4.5). In Sec. 5, we empirically validate the performance of our solution using several synthetic functions and real-world datasets. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries and Problem Statement ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Top- $k$ set ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Adopting an assumption in existing level set estimation (LSE) works [1, 8], we consider a blackbox function $f:\\mathcal{X}\\to\\mathbb{R}$ where the domain $\\mathcal{X}$ is a finite set of $n$ elements in $\\mathbb{R}^{d}$ . Let $S^{c}\\triangleq\\chi\\setminus S$ denote the complement of any subset $s\\subset\\mathcal{X}$ . In this paper, the ordering between inputs are determined with respect to their corresponding blackbox function evaluations. Hence, we use the term \u201cthe ordering between $\\mathbf{x}$ and $\\mathbf{x}^{\\prime}{}^{,,}$ and \u201cthe ordering between $f(\\mathbf{x})$ and $f(\\mathbf{x}^{\\prime})^{,,}$ interchangeably. ", "page_idx": 1}, {"type": "text", "text": "Definition 2.1 (Top- $k$ set). The top- $k$ set, denoted as $\\displaystyle S(k)$ , is the set of $k$ inputs with the highest function evaluations. Specifically, $\\bar{|\\cal S}(k)|=k$ and $\\forall\\mathbf{x}\\in{\\dot{S}}(k),\\forall\\mathbf{x}^{\\prime}\\in S^{c}(k),\\,f(\\mathbf{x})\\geq f(\\mathbf{x}^{\\prime})$ . ", "page_idx": 1}, {"type": "text", "text": "In this work, we propose the active set ordering problem to estimate the top- $k$ set $\\displaystyle S(k)$ of a blackbox function $f$ by efficiently gathering noisy function evaluations in a sequential manner. Furthermore, it includes the Bayesian optimization (BO) problem when $k=1$ because the top-1 set $\\ S(1)$ contains a maximizer of $f$ . ", "page_idx": 1}, {"type": "text", "text": "2.2 Gaussian Process ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The noisy function evaluation mentioned in the previous section is denoted as $y(\\mathbf{x})\\triangleq f(\\mathbf{x})+\\epsilon(\\mathbf{x})$ where the noise $\\epsilon(\\mathbf{x})\\,\\sim\\,{\\mathcal N}(0,\\sigma_{n}^{2})$ is a Gaussian random variable with a known (or estimated) variance $\\sigma_{n}^{2}$ . To obtain the posterior distribution of the unknown function $f$ given these noisy evaluations, we model $f$ using a Gaussian process (GP), that is, every subset of $\\{f(\\mathbf{x})\\}_{\\mathbf{x}\\in\\mathcal{X}}$ follows a multivariate Gaussian distribution [16]. A GP is fully specified by its prior mean and its kernel $k_{\\mathbf{x},\\mathbf{x}^{\\prime}}\\triangleq$ $\\operatorname{cov}(f(\\mathbf{x}),f(\\mathbf{x}^{\\prime}))$ which measures the covariance between function values. Let $\\mathcal{D}_{t}$ denote the set of sampling inputs in the first $t\\!-\\!1$ iterations. Then, given $\\mathbf{y}_{\\mathcal{D}_{t}}\\triangleq(y(\\mathbf{x}))_{\\mathbf{x}\\in\\mathcal{D}_{t}}$ , the predictive distribution of any function evaluation $f(\\mathbf{x})$ follows a Gaussian distribution with the following mean and variance: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mu_{t}(\\mathbf{x})\\triangleq\\mathbf{k}_{t}(\\mathbf{x})^{\\top}(\\mathbf{K}_{t}+\\sigma_{n}^{2}\\mathbf{I})^{-1}\\mathbf{y}_{\\mathcal{D}_{t}}\\qquad\\sigma_{t}^{2}(\\mathbf{x})\\triangleq k(\\mathbf{x},\\mathbf{x})-\\mathbf{k}_{t}(\\mathbf{x})^{\\top}(\\mathbf{K}_{t}+\\sigma_{n}^{2}\\mathbf{I})^{-1}\\mathbf{k}_{t}(\\mathbf{x})\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mathbf{k}_{t}(\\mathbf{x})\\triangleq(k(\\mathbf{x},\\mathbf{x}^{\\prime}))_{\\mathbf{x}^{\\prime}\\in\\mathcal{D}_{t}}$ , $\\mathbf{K}_{t}\\triangleq(k(\\ensuremath{\\mathbf{x}},\\ensuremath{\\mathbf{x}}^{\\prime}))_{\\mathbf{x},\\mathbf{x}^{\\prime}\\in\\mathcal{D}_{t}}$ , and $\\mathbf{I}$ is the identity matrix [16]. ", "page_idx": 1}, {"type": "text", "text": "Assuming $f$ belongs to a reproducing kernel Hilbert space with its norm bounded by $B>0$ , due to [5], we have the following confidence bound of $f(\\mathbf{x})$ .2 ", "page_idx": 1}, {"type": "text", "text": "Lemma 2.2. Pick $\\delta\\in(0,1)$ and set $\\beta_{t}=(B+\\sigma_{n}\\sqrt{2(\\gamma_{t-1}+1+\\log{1/\\delta})})^{2}$ . Then, the following event happens with probability of at least $1-\\delta$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\forall\\mathbf{x}\\in\\mathcal{X},\\,\\forall t\\geq1,\\,l_{t}(\\mathbf{x})\\leq f(\\mathbf{x})\\leq u_{t}(\\mathbf{x})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\begin{array}{r l r l r l r l}{l_{t}({\\bf x})}&{{}\\triangleq}&{\\mu_{t}({\\bf x})}&{{}\\!-\\!\\ \\beta_{t}^{1/2}\\sigma_{t}({\\bf x}),}&{u_{t}({\\bf x})}&{{}\\triangleq}&{\\mu_{t}({\\bf x})\\ +\\ \\beta_{t}^{1/2}\\sigma_{t}({\\bf x})}\\end{array}$ , and $\\begin{array}{r l}{\\gamma_{t-1}}&{{}\\triangleq}\\end{array}$ $\\begin{array}{r}{\\operatorname*{max}_{A\\subset\\mathcal{X}:|A|=t-1}I(\\mathbf{y}_{A};\\mathbf{f}_{A})}\\end{array}$ is the maximum information gain of $\\mathbf{f}_{A}\\ \\triangleq\\ \\{f(\\mathbf{x})\\}_{\\mathbf{x}\\in A}$ through observing $\\mathbf{y}_{A}$ over all subsets $A\\subset\\mathcal{X}$ of size $|A|=t-1$ . ", "page_idx": 2}, {"type": "text", "text": "To ease notational clutter, we denote the above confidence interval of $f(\\mathbf{x})$ as $\\mathcal{C}_{t}(\\mathbf{x})\\triangleq[l_{t}(\\mathbf{x}),u_{t}(\\mathbf{x})]$ and its length as $|\\mathcal{C}_{t}(\\mathbf{x})|\\triangleq u_{t}(\\mathbf{x})-l_{t}(\\mathbf{x})$ . ", "page_idx": 2}, {"type": "text", "text": "3 Active Pairwise Ordering: $n=2$ ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "From Definition 2.1, pairwise orderings (or pairwise comparisons) are the building blocks of our active set ordering problem. Hence, to facilitate the exposition of the key ideas, let us begin with a simplistic setting where the input domain $\\mathcal{X}$ consists of only $n\\,=\\,2$ inputs, i.e., $\\boldsymbol{\\mathcal{X}}=\\left\\{\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime}\\right\\}$ and $\\bar{f}(\\tilde{\\mathbf{x}})\\ \\neq\\ f(\\tilde{\\mathbf{x}^{\\prime}})$ . The problem is to determine the top-1 set $\\ S(1)$ , i.e., the maximizer of $f$ (equivalently, the minimizer of $f$ ). In essence, the goal is to check if $\\begin{array}{r}{\\dot{f}(\\ensuremath{{\\widetilde{\\mathbf{x}}}})>f(\\ensuremath{{\\widetilde{\\mathbf{x}}}^{\\prime}})}\\end{array}$ by strategically collecting noisy evaluations $y(\\ensuremath{{\\widetilde{\\mathbf{x}}}})$ and $y(\\ensuremath{{\\widetilde{\\mathbf{x}}}}^{\\prime})$ . In particular, at iteration $t$ , the algorithm proposes a sampling input $\\mathbf{x}_{t}\\in\\mathcal{X}$ to obtain a noisy evaluation $y(\\mathbf x_{t})$ . Then, the GP posterior distribution of $f$ is updated and used to construct a predicted ordering between $\\tilde{\\bf x}$ and $\\tilde{\\mathbf{x}}^{\\prime}$ (i.e., the ordering between $f(\\ensuremath{{\\widetilde{\\mathbf{x}}}})$ and $f(\\ensuremath{{\\widetilde{\\mathbf{x}}}}^{\\prime}))$ . The problem boils down to the strategy of selecting the sampling input $\\mathbf{x}_{t}$ such that $a$ performance metric of the predicted ordering between $f(\\ensuremath{{\\widetilde{\\mathbf{x}}}})$ and $f(\\ensuremath{{\\widetilde{\\mathbf{x}}}}^{\\prime})$ is satisfactory. In the next section, we introduce a regret definition to serve as a performance metric. ", "page_idx": 2}, {"type": "text", "text": "3.1 Regret ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let us denote the (unknown) true ordering between $\\tilde{\\bf x}$ and $\\tilde{\\mathbf{x}}^{\\prime}$ according to the evaluations of the blackbox function $f$ as $\\pi_{*}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pi_{*}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})\\triangleq\\mathbb{1}_{f(\\tilde{\\mathbf{x}})\\geq f(\\tilde{\\mathbf{x}}^{\\prime})}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the indicator function $\\mathbb{1}_{f(\\tilde{\\mathbf{x}})\\geq f(\\tilde{\\mathbf{x}}^{\\prime})}=1$ if $f(\\ensuremath{{\\widetilde{\\mathbf{x}}}})\\geq f(\\ensuremath{{\\widetilde{\\mathbf{x}}}}^{\\prime})$ and 0 otherwise. For any ordering $\\pi:$ $\\{(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})\\}\\rightarrow\\{0,1\\}$ , we define the following regret of $\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\nr_{\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}\\triangleq\\operatorname*{max}\\left(0,(2\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})-1)(f(\\tilde{\\mathbf{x}}^{\\prime})-f(\\tilde{\\mathbf{x}}))\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In particular, $r_{\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})=1}\\,=\\,\\mathrm{max}(0,f(\\tilde{\\mathbf{x}}^{\\prime})\\,-\\,f(\\tilde{\\mathbf{x}}))$ and $r_{\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})=0}\\,=\\,\\mathrm{max}(0,f(\\tilde{\\mathbf{x}})\\,-\\,f(\\tilde{\\mathbf{x}}^{\\prime}))$ . The rationale is to ensure poor performance leads to large regret: The regret is $|f(\\tilde{\\mathbf{x}})-f(\\tilde{\\mathbf{x}}^{\\prime})|$ (which increases as the gap between $f(\\ensuremath{{\\widetilde{\\mathbf{x}}}})$ and $f(\\ensuremath{{\\widetilde{\\mathbf{x}}}^{\\prime}})$ increases) if the ordering $\\pi$ does not align with the true ordering, i.e., $\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})\\neq\\pi_{*}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})$ . On the contrary, the regret is 0 (i.e., the best performance) if $\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})=\\pi_{*}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})$ . ", "page_idx": 2}, {"type": "text", "text": "However, the blackbox function $f$ renders the evaluation of $r_{\\pi(\\widetilde{{\\bf x}},\\widetilde{{\\bf x}}^{\\prime})}$ impossible. Thus, we resort to relying on the GP posterior distribution of $f$ at iteration $t$ to construct an upper bound of the above regret in the following lemma (proof in Appendix A). ", "page_idx": 2}, {"type": "text", "text": "Lemma 3.1. For all $t\\geq1$ , let us define ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\rho_{\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}^{(t)}\\triangleq\\left\\{\\!\\!\\begin{array}{l l}{\\operatorname*{max}(0,u_{t}(\\tilde{\\mathbf{x}}^{\\prime})-l_{t}(\\tilde{\\mathbf{x}}))}&{i f\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})=1}\\\\ {\\operatorname*{max}(0,u_{t}(\\tilde{\\mathbf{x}})-l_{t}(\\tilde{\\mathbf{x}}^{\\prime}))}&{i f\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})=0\\;.}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Then, $\\rho_{\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}^{(t)}$ is an upper confidence bound of the regret $r_{\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}$ , i.e., ", "page_idx": 2}, {"type": "equation", "text": "$$\nP\\left(\\forall t\\geq1,\\;r_{\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}\\leq\\rho_{\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}^{(t)}\\right)\\geq1-\\delta\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\delta$ is as defined in Lemma 2.2. ", "page_idx": 2}, {"type": "text", "text": "3.2 Prediction ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Definition 3.2 (Predicted pairwise ordering $\\pi_{\\mu_{t}}$ ). Given the above upper bound $\\rho_{\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}^{(t)}$ of the regret, we would like to make a prediction $\\pi_{\\mu_{t}}$ that minimize s \u03c1(\u03c0t()x\u02dc,x\u02dc\u2032). Therefore, \u03c0\u00b5t is defined as follows ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})\\triangleq\\operatorname*{argmin}_{a\\in\\{0,1\\}}\\rho_{\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})=a}^{(t)}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "As a result, the upper confidence bound of the regret in (3) is minimized at $\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})=\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})$ and its minimum value is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\rho_{\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}^{(t)}=\\operatorname*{max}\\left(0,\\operatorname*{min}(u_{t}(\\tilde{\\mathbf{x}}^{\\prime})-l_{t}(\\tilde{\\mathbf{x}}),u_{t}(\\tilde{\\mathbf{x}})-l_{t}(\\tilde{\\mathbf{x}}^{\\prime}))\\right)\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We note that the upper confidence bound of the regret can be interpreted as a measure of the approximation quality or the uncertainty reduction as discussed in the following two remarks. Remark 3.3 (Approximation quality). Since $\\rho_{\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}^{(t)}\\,\\geq\\,r_{\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}$ for all $t\\geq1$ with probability of at least $1\\,-\\,\\delta$ , the regret incurred by the ordering $\\boldsymbol{\\pi}_{\\mu_{t}}$ cannot exceed the worst-case regret $\\rho_{\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}^{(t)}$ as shown in Fig. 1a. Hence, if $|f(\\tilde{\\mathbf{x}})\\,-\\,f(\\tilde{\\mathbf{x}}^{\\prime})|\\,>\\,\\rho_{\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}^{(t)},\\,\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}}$ \u03c1(\u03c0t\u00b5)(x\u02dc,x\u02dc\u2032), \u03c0\u00b5t(\u02dcx, \u02dcx\u2032) is the true ordering, i.e., $\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})=\\pi_{*}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})$ , with probability of at least $1-\\delta$ . ", "page_idx": 3}, {"type": "text", "text": "Remark 3.4 (Minimum uncertainty reduction). In Fig. 1b, one can interpret $\\rho_{\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}^{(t)}$ as the minimum amount that the confidence intervals $\\mathcal{C}_{t}(\\tilde{{\\bf x}})$ and $\\mathcal{C}_{t}(\\tilde{\\mathbf{x}}^{\\prime})$ (representing the uncertainty) reduce so that $r_{\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}=0$ with probability of at least $1-\\delta$ . ", "page_idx": 3}, {"type": "text", "text": "Moreover, the prediction $\\pi_{\\mu_{t}}$ can be obtained using only the GP posterior mean (proof in Appendix B), which explains the name of our approach: mean prediction (MP) and the notation $\\pi_{\\mu_{t}}$ . ", "page_idx": 3}, {"type": "text", "text": "Lemma 3.5 (Mean prediction). The predicted ordering $\\pi_{\\mu_{t}}$ defined in (4) can be determined from the GP posterior mean ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})=\\mathbb{1}_{\\mu_{t}(\\tilde{\\mathbf{x}})\\geq\\mu_{t}(\\tilde{\\mathbf{x}}^{\\prime})}~.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "GkJbXpd3wM/tmp/43e0f391701313daf52b14d3796a9064c38e16fb69e095d27ec8dbc713da41b1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "(b) $r_{\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}=0$ when $(u_{t},l_{t})$ are refined to $(u_{t}^{\\prime},l_{t}^{\\prime})$ following an observation, i.e., the reduction in the uncertainty represented as the sum of the two red dashed segments is at least $\\rho_{\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}^{(t)}$ . ", "page_idx": 3}, {"type": "text", "text": "Figure 1: Interpretations of the upper bound $\\rho_{\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}^{(t)}$ when $\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})=1$ . ", "page_idx": 3}, {"type": "text", "text": "3.3 Sampling Strategy ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given the regret in Sec. 3.1 and the predicted ordering $\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})$ in Sec. 3.2, we would like to select a sampling input $\\mathbf{x}_{t}\\in\\mathcal{X}=\\{\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime}\\}$ such that the regret $r_{\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}$ of the predicted ordering $\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})$ reduces quickly. ", "page_idx": 3}, {"type": "text", "text": "While $r_{\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}$ is unknown, it is bounded by $\\rho_{\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}^{(t)}$ with probability of at least $1-\\delta$ . Hence, to reduce $r_{\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}$ , we aim to reduce $\\rho_{\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}^{(t)}$ . It is also noted that observing $y(\\mathbf x_{t})$ decreases the confidence interval $|\\mathcal{C}_{t}(\\mathbf{x}_{t})|$ . Hence, to induce the reduction in the regret $r_{\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}$ through observing $y(\\mathbf x_{t})$ , we select $\\mathbf{x}_{t}$ such that its confidence interval $|\\mathcal{C}_{t}(\\mathbf{x}_{t})|\\geq\\rho_{\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}^{(t)}$ (which guarantees that $|\\mathcal{C}_{t}(\\mathbf{x}_{t})|\\geq r_{\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}$ with probability of at least $1-\\delta)$ . For instance, choosing ${\\bf x}_{t}=\\tilde{{\\bf x}}$ in the left plot of Fig. 1a satisfies this condition, but it does not in the right plot of Fig. 1a. In the following lemma, we show that the following 4 choices of the sampling input satisfy the proposed condition (see Appendix C). ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.6. Let $\\mathcal{Q}_{t}\\triangleq\\{\\tilde{\\mathbf{x}}\\ \\nabla\\ \\tilde{\\mathbf{x}}^{\\prime},\\tilde{\\mathbf{x}}\\bigtriangleup\\tilde{\\mathbf{x}}^{\\prime},\\tilde{\\mathbf{x}}\\vee\\tilde{\\mathbf{x}}^{\\prime},\\tilde{\\mathbf{x}}\\wedge\\tilde{\\mathbf{x}}^{\\prime}\\}$ denote a set3 of inputs at iteration $t$ where ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\tilde{\\mathbf{x}}\\nabla\\,\\tilde{\\mathbf{x}}^{\\prime}\\triangleq\\underset{\\mathbf{x}\\in\\{\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime}\\}}{\\mathrm{argmax}}\\,u_{t}(\\mathbf{x})}&{\\qquad\\qquad}&{\\tilde{\\mathbf{x}}\\,\\Delta\\,\\tilde{\\mathbf{x}}^{\\prime}\\triangleq\\underset{\\mathbf{x}\\in\\{\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime}\\}}{\\mathrm{argmin}}\\,l_{t}(\\mathbf{x})}\\\\ &{\\tilde{\\mathbf{x}}\\vee\\tilde{\\mathbf{x}}^{\\prime}\\triangleq\\underset{\\mathbf{x}\\in\\{\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime}\\}}{\\mathrm{argmax}}\\,|\\mathcal{C}_{t}(\\mathbf{x})|\\quad\\qquad\\qquad}&&{\\tilde{\\mathbf{x}}\\wedge\\tilde{\\mathbf{x}}^{\\prime}\\triangleq\\underset{\\mathbf{x}\\in\\{\\tilde{\\mathbf{x}}\\,\\tilde{\\mathbf{x}}^{\\prime},\\tilde{\\mathbf{x}}\\Delta\\tilde{\\mathbf{x}}^{\\prime}\\}}{\\mathrm{argmin}}\\,|\\mathcal{C}_{t}(\\mathbf{x})|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For any $\\begin{array}{r}{{\\bf x}_{t}\\in\\mathcal{Q}_{t},\\,|\\mathcal{C}_{t}({\\bf x}_{t})|\\ge\\rho_{\\pi_{\\mu_{t}}(\\tilde{\\bf x},\\tilde{\\bf x}^{\\prime})}^{(t)}.}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.7. By sampling the input $\\mathbf{x}_{t}$ following Lemma 3.6, we obtain the following regret bound ", "page_idx": 4}, {"type": "equation", "text": "$$\nP\\left(\\forall T\\geq1,\\,\\forall(\\mathbf{x}_{t})_{t=1}^{T}\\in\\prod_{t=1}^{T}\\mathcal{Q}_{t},\\;R_{T}\\triangleq\\sum_{t=1}^{T}r_{\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}\\leq\\mathcal{O}(\\sqrt{T\\beta_{T}\\gamma_{T}})\\right)\\geq1-\\delta\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\beta_{T},\\,\\gamma_{T}$ , and $\\delta$ are as defined in Lemma 2.2. ", "page_idx": 4}, {"type": "text", "text": "Remark 3.8 (Sublinear cumulative regret). If $\\gamma_{T}$ is sublinear, our average cumulative regret is sublinear. This requirement is similar to most BO and LSE algorithms. It is noted that $\\gamma_{T}$ is sublinear for many popular kernels. For instance, $\\gamma_{T}=\\mathcal{O}((\\log T)^{d+1})$ for the squared exponential (SE) kernel as discussed in [19]. In this case, our cumulative regret bound $R_{T}\\leq{\\mathcal{O}}^{*}({\\sqrt{T(\\log T)^{2d}}})$ is the same as that of GP-UCB [19] (where $O^{\\ast}(\\cdot)$ denotes asymptotic expressions up to dimension-independent logarithmic factors and is the dimension of the input). ", "page_idx": 4}, {"type": "text", "text": "We defer the pseudocode to the next section when $n\\geq2$ . ", "page_idx": 4}, {"type": "text", "text": "4 Active Set Ordering: $n\\geq2$ ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we utilize the results in Sec. 3 to present the mean prediction (MP) algorithm for the active set ordering problem with $n\\geq2$ . ", "page_idx": 4}, {"type": "text", "text": "4.1 Regret ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "When $\\mathcal{X}$ consists of $n>2$ inputs, there are multiple pairwise orderings between inputs in $\\mathcal{X}$ . We overload the ordering notation $\\pi_{*}$ of the true pairwise ordering between 2 inputs in (1) to the ordering between 2 sets as follows: for any subsets $\\mathcal{X}_{0}\\subset\\mathcal{X}$ and $\\mathcal{X}_{1}\\subset\\mathcal{X}_{0}^{c}$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pi_{*}(\\mathcal{X}_{0},\\mathcal{X}_{1})=\\left\\{\\begin{array}{l l}{\\hfill1\\hfill\\mathrm{~if~}\\forall\\mathbf{x}\\in\\mathcal{X}_{0},\\:\\forall\\mathbf{x}^{\\prime}\\in\\mathcal{X}_{1},\\:\\pi_{*}(\\mathbf{x},\\mathbf{x}^{\\prime})=1}\\\\ {\\hfill0\\mathrm{~if~}\\forall\\mathbf{x}\\in\\mathcal{X}_{0},\\:\\forall\\mathbf{x}^{\\prime}\\in\\mathcal{X}_{1},\\:\\pi_{*}(\\mathbf{x},\\mathbf{x}^{\\prime})=0\\;.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "It is noted that $\\pi_{*}(\\mathcal{X}_{0},\\mathcal{X}_{1})$ remains undefined if the two cases above are not satisfied. However, this situation does not arise in our solution. We define the regret of a set ordering (i.e., multiple pairwise orderings) as the maximum regret of all pairwise orderings: ", "page_idx": 4}, {"type": "equation", "text": "$$\nr_{\\pi(\\mathcal{X}_{0},\\mathcal{X}_{1})}\\triangleq\\operatorname*{max}_{(\\mathbf{x},\\mathbf{x}^{\\prime})\\in\\mathcal{X}_{0}\\times\\mathcal{X}_{1}}r_{\\pi(\\mathbf{x},\\mathbf{x}^{\\prime})=\\pi(\\mathcal{X}_{0},\\mathcal{X}_{1})}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $r_{\\pi(\\mathbf{x},\\mathbf{x}^{\\prime})}$ is defined in (2) and $\\mathcal{X}_{0}\\times\\mathcal{X}_{1}$ is the Cartesian product of $\\scriptstyle{\\mathcal{X}}_{0}$ and $\\mathcal{X}_{1}$ , i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\nr_{\\pi(x_{0},X_{1})}=\\operatorname*{max}\\left(0,(2\\pi(X_{0},X_{1})-1)\\operatorname*{max}_{(\\mathbf{x},\\mathbf{x}^{\\prime})\\in{\\mathcal{X}}_{0}\\times{\\mathcal{X}}_{1}}(f(\\mathbf{x}^{\\prime})-f(\\mathbf{x}))\\right)~.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3For convenience, we allow duplicate elements in $Q_{t}$ . ", "page_idx": 4}, {"type": "text", "text": "Remark 4.1. It is noted that $r_{\\pi(\\mathcal{X}_{0},\\mathcal{X}_{1})}$ coincides with the well-known regret in BO when we consider the problem of predicting a maximizer of $f$ . In particularly, predicting $\\hat{\\mathbf{x}}_{*}$ as a maximizer of $f$ is equivalent to predicting the set ordering $\\pi(\\{\\hat{\\mathbf{x}}_{*}\\},\\mathcal{X}\\setminus\\{\\hat{\\mathbf{x}}_{*}\\})=1$ . Its regret is $r_{\\pi(\\{\\hat{\\mathbf{x}}_{*}\\},\\mathcal{X}\\backslash\\{\\hat{\\mathbf{x}}_{*}\\})=1}=$ $\\mathrm{max}_{\\mathbf{x}\\in\\mathcal{X}}\\;f(\\mathbf{x})-f(\\hat{\\mathbf{x}}_{\\ast})$ as shown in Appendix E.1. ", "page_idx": 5}, {"type": "text", "text": "Following the upper confidence bound of the regret of pairwise orderings in (3), we show in Appendix $\\boldsymbol{\\mathrm F}$ that with probability of at least $1-\\delta$ , for all $t\\geq1$ and for all subsets $\\mathcal{X}_{0}\\subset\\mathcal{X},\\,\\mathcal{X}_{1}\\subset$ $\\mathcal{X}_{0}^{c},\\,r_{\\pi(\\mathcal{X}_{0},\\mathcal{X}_{1})}\\leq\\rho_{\\pi(\\mathcal{X}_{0},\\mathcal{X}_{1})}^{(\\bar{t})}$ where ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\rho_{\\pi(X_{0},X_{1})}^{(t)}\\triangleq\\operatorname*{max}_{(\\mathbf{x},\\mathbf{x}^{\\prime})\\in\\mathcal{X}_{0}\\times\\mathcal{X}_{1}}\\rho_{\\pi(\\mathbf{x},\\mathbf{x}^{\\prime})=\\pi(\\mathcal{X}_{0},\\mathcal{X}_{1})}^{(t)}\\ .\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4.2 Prediction ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we generalize the prediction in Sec. 3.2 to set orderings. From Lemma 3.5, there is no contradiction in the pairwise orderings $\\pi_{\\mu_{t}}(\\mathbf{x},\\mathbf{x}^{\\prime})$ (defined in (4)) for all $\\{\\mathbf{x},\\mathbf{x}^{\\prime}\\}\\subset\\mathcal{X}$ . In other words, the transitivity property holds for the binary relation $\\pi_{\\mu_{t}}$ as shown in Appendix G. ", "page_idx": 5}, {"type": "text", "text": "Definition 4.2 (Predicted top- $k$ set $S_{\\mu_{t}}(k))$ . Let ${{S}_{\\mu_{t}}}(k)$ be a subset of $\\mathcal{X}$ such that ", "page_idx": 5}, {"type": "equation", "text": "$$\n|S_{\\mu_{t}}(k)|=k\\;,\\quad\\pi_{\\mu_{t}}(S_{\\mu_{t}}(k),S_{\\mu_{t}}^{c}(k))=1\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the set ordering $\\pi_{\\mu_{t}}(S_{\\mu_{t}}(k),S_{\\mu_{t}}^{c}(k))$ is obtained by substituting $\\pi_{*}$ with the pairwise ordering $\\pi_{\\mu_{t}}$ (see Definition 3.2) in (7). From Lemma 3.5, ${{S}_{\\mu_{t}}}(k)$ is basically the set of $k$ inputs with the highest GP posterior mean values. ", "page_idx": 5}, {"type": "text", "text": "As $\\pi_{\\mu_{t}}(S_{\\mu_{t}}(k),S_{\\mu_{t}}^{c}(k))\\,=\\,1$ implies that $\\pi_{\\mu_{t}}(\\mathbf{x},\\mathbf{x}^{\\prime})\\,=\\,1$ for all $({\\bf x},{\\bf x^{\\prime}})\\,\\in\\,S_{\\mu_{t}}(k)\\,\\times\\,S_{\\mu_{t}}^{c}(k)$ , the upper confidence bound of the regret is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\rho_{\\pi_{\\mu_{t}}(S_{\\mu_{t}}(k),S_{\\mu_{t}}^{c}(k))}^{(t)}=\\operatorname*{max}_{(\\mathbf{x},\\mathbf{x}^{\\prime})\\in S_{\\mu_{t}}(k)\\times S_{\\mu_{t}}^{c}(k)}\\rho_{\\pi_{\\mu_{t}}(\\mathbf{x},\\mathbf{x}^{\\prime})}^{(t)}\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4.3 Sampling Strategy ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Like in Sec. 3.3, our key idea is to select $\\mathbf{x}_{t}$ such that the length $|\\mathcal{C}_{t}(\\mathbf{x}_{t})|$ of its confidence interval bounds $\\rho_{\\pi_{\\mu_{t}}(S_{\\mu_{t}}(k),S_{\\mu_{t}}^{c}(k))}^{(t)}$ (in (10)). Since $\\rho_{\\pi_{\\mu_{t}}(S_{\\mu_{t}}(k),S_{\\mu_{t}}^{c}(k))}^{(t)}$ is the maximum upper confidence bound of the regret of all pairwise orderings involved in defining ${{S}_{\\mu_{t}}}(k)$ , we first determine the input pair $(\\bar{\\mathbf{x}}_{t},\\bar{\\mathbf{x}}_{t}^{\\prime})$ that incurs the maximum upper confidence bound of the regret. This is also the input pair that $\\pi_{\\mu_{t}}$ most likely makes a mistake, following the intuition from [12]. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left(\\bar{\\mathbf{x}}_{t},\\bar{\\mathbf{x}}_{t}^{\\prime}\\right)\\triangleq\\underset{(\\mathbf{x},\\mathbf{x}^{\\prime})\\in S_{\\mu_{t}}(k)\\times S_{\\mu_{t}}^{c}(k)}{\\mathrm{argmax}}\\,\\rho_{\\pi_{\\mu_{t}}(\\mathbf{x},\\mathbf{x}^{\\prime})}^{(t)}\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "It is noted that $(\\bar{\\mathbf{x}}_{t},\\bar{\\mathbf{x}}_{t}^{\\prime})$ is constructed from an input in the predicted top- $k$ set ${{S}_{\\mu_{t}}}(k)$ and an input in its complement $S_{\\mu_{t}}^{c}(k)$ . As the estimation of the top- $k$ set improves, we expect these 2 inputs to be at both sides of the boundary of the top- $k$ set: inside the top- $k$ set vs. outside the top- $k$ set. ", "page_idx": 5}, {"type": "text", "text": "Then, extended from Lemma 3.6, the following lemma shows that the above desirable property is satisfied by choosing the sampling input $\\mathbf{x}_{t}$ from any inputs in the set $\\bar{\\mathcal{Q}}_{t}\\,\\triangleq\\,\\{\\bar{\\mathbf{x}}_{t}\\,\\,\\nabla\\,\\,\\bar{\\mathbf{x}}_{t}^{\\prime},\\bar{\\mathbf{x}}_{t}\\,\\,\\Delta$ $\\bar{\\mathbf{x}}_{t}^{\\prime},\\bar{\\mathbf{x}}_{t}\\vee\\bar{\\mathbf{x}}_{t}^{\\prime},\\bar{\\bar{\\mathbf{x}}}_{t}\\wedge\\bar{\\mathbf{x}}_{t}^{\\prime}\\big\\}$ (defined in Lemma 3.6). ", "page_idx": 5}, {"type": "text", "text": "Lemma 4.3. For any xt \u2208Q\u00aft, |Ct(xt)| \u2265\u03c1(\u03c0t\u00b5)(S\u00b5(k),S\u00b5c (k)). ", "page_idx": 5}, {"type": "text", "text": "The proof is shown in Appendix $\\mathrm{H}$ . As a result, the cumulative regret incurred by choosing $\\mathbf{x}_{t}$ in Lemma 4.3 is bounded in the following theorem (proof in Appendix I). ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.4. By sampling $\\mathbf{x}_{t}$ following Lemma 4.3, we obtain the following cumulative regret bound ", "page_idx": 5}, {"type": "equation", "text": "$$\nP\\left(\\forall T\\geq1,\\,\\forall(\\mathbf{x}_{t})_{t=1}^{T}\\in\\prod_{i=1}^{T}\\bar{\\mathcal{Q}}_{t},\\,\\,R_{T,k}\\triangleq\\sum_{t=1}^{T}r_{\\pi_{\\mu_{t}}(\\mathcal{S}_{\\mu_{t}}(k),\\mathcal{S}_{\\mu_{t}}^{c}(k))}\\leq\\mathcal{O}(\\sqrt{T\\beta_{T}\\gamma_{T}})\\right)\\geq1-\\delta\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\beta_{T},\\,\\gamma_{T}$ , and $\\delta$ are as defined in Lemma 2.2. ", "page_idx": 5}, {"type": "text", "text": "Require: $\\overline{{\\chi,D_{0},k,T}}$   \n1: for $t=1$ to $T$ do   \n2: Update GP posterior belief: $\\{\\mu_{t}({\\bf x})\\}_{{\\bf x}\\in{\\cal X}}$ , $\\{\\sigma_{t}(\\mathbf{x})\\}_{\\mathbf{x}\\in\\mathcal{X}}$ .   \n3: Construct $\\bar{S_{\\mu_{t}}}(k)$ as top- ${\\cdot k}$ inputs with the highest values of $\\mu_{t}$ . \u25b7Prediction   \n4: $(\\bar{\\mathbf{x}}_{t},\\bar{\\mathbf{x}}_{t}^{\\prime})=\\arg\\!\\operatorname*{max}_{(\\mathbf{x},\\mathbf{x}^{\\prime})\\in S_{\\mu_{t}}(k)\\times S_{\\mu_{t}}^{c}(k)}\\rho_{\\pi_{\\mu_{t}}(\\mathbf{x},\\mathbf{x}^{\\prime})}^{(t)}$   \n5: Selec $\\mathfrak{t}\\,\\mathbf{x}_{t}\\in\\big\\{\\bar{\\mathbf{x}}_{t}\\ \\nabla\\,\\bar{\\mathbf{x}}_{t}^{\\prime},\\bar{\\mathbf{x}}_{t}\\bigtriangleup\\bar{\\mathbf{x}}_{t}^{\\prime},\\bar{\\mathbf{x}}_{t}\\vee\\bar{\\mathbf{x}}_{t}^{\\prime},\\bar{\\mathbf{x}}_{t}\\wedge\\bar{\\mathbf{x}}_{t}^{\\prime}\\big\\}.$ \u25b7Sampling input   \n6: $\\mathbf{y}_{t}(\\mathcal{D}_{t})\\leftarrow\\mathbf{y}(\\mathcal{D}_{t-1})\\cup\\{y(\\mathbf{x}_{t})\\}$   \n7: end for   \n8: Update GP posterior belief: $\\{\\mu_{T+1}(\\mathbf{x})\\}_{\\mathbf{x}\\in\\mathcal{X}},\\{\\sigma_{T+1}(\\mathbf{x})\\}_{\\mathbf{x}\\in\\mathcal{X}}$ .   \n9: Construct ${\\mathcal{S}}_{\\mu_{T+1}}(k)$ as top- $k$ inputs with the highest values of $\\mu_{T+1}$ .   \n10: return ${\\mathcal{S}}_{\\mu_{T+1}}(k)$ . ", "page_idx": 6}, {"type": "text", "text": "We call the algorithm that makes prediction using the GP posterior mean and selects the sampling input $\\mathbf{x}_{t}$ following Lemma 4.3 the mean prediction (MP) algorithm. Its pseudocode is shown in Algorithm 1. Theorem 4.4 indicates that MP incurs a sublinear cumulative regret for several commonly used kernels with sublinear $\\gamma_{T}$ [19]. ", "page_idx": 6}, {"type": "text", "text": "Remark 4.5 (Bayesian optimization as an active set ordering problem with $k\\,=\\,1.$ ). We show in Appendix E.2 that when $k=1$ , $\\bar{\\mathbf{x}}_{t}\\perp\\bar{\\mathbf{x}}_{t}^{\\prime}\\in\\mathrm{argmax}_{\\mathbf{x}\\in\\mathcal{X}}\\,u_{t}(\\mathbf{x})$ , which is the sampling input in the GP-UCB algorithm [19]. Additionally, as discussed in Sec. 4.1, the regret $r_{\\pi({\\cal S}(1),{\\cal S}^{c}(1))}$ is the wellknown regret in BO. Hence, we recover both the GP-UCB algorithm and its regret bound when $k=1$ (although we consider the regret of the prediction rather than that of the sampling input). Moreover, this new construction of GP-UCB leads to some subtle insights. Firstly, while the GP posterior mean has been used in computing the inference regret of entropy search methods [9, 21], there has not been any theoretical justification for using the posterior mean. In contrast, the theoretical analysis in our work justifies the use of the maximizer of the GP posterior mean as an estimate of the maximizer of $f$ Secondly, by predicting the maximizer using the GP posterior mean, $\\bar{\\mathbf{x}}\\perp\\bar{\\mathbf{x}}^{\\prime}$ is not the only sampling input that achieves a sublinear cumulative regret. In fact, there are other choices of the sampling input as shown in Lemma 4.3. Similarly, we note that the LCB algorithm to find the minimizer of a blackbox function can be recovered by setting $k=n-1$ and $\\mathbf{x}_{t}=\\bar{\\mathbf{x}}\\,\\triangle\\,\\bar{\\mathbf{x}}^{\\prime}$ . ", "page_idx": 6}, {"type": "text", "text": "Remark 4.6 (Lower bound of active set ordering problem). Let the lower bound of the active set ordering problem be the lower bound of the cumulative regret of the worst-case problem instance over all possible values of $k$ . Then, it should be at least as large as the lower bound of the special case where $k=1$ , which is the BO problem according to Remark 4.5. Furthermore, BO has known lower bounds for several common kernels, e.g., for the SE kernel, the lower bound of the cumulative regret is $\\Omega(\\sqrt{T(\\log T)^{d/2}})$ [17]. Hence, the lower bound of the active set ordering problem is at least $\\Omega(\\sqrt{T(\\log T)^{d/2}})$ . Additionally, similar to Remark 3.8, the cumulative regret of our solution in Theorem 4.4 is bounded by $R_{T}\\leq{\\mathcal{O}}^{*}({\\sqrt{T(\\log T)^{2d}}})$ . Hence, it matches the lower bound up to the replacement of $d/2$ by $2d+O(1)$ . ", "page_idx": 6}, {"type": "text", "text": "Remark 4.7. Updating the GP posterior belief incurs $\\mathcal{O}(|\\mathcal{D}_{t}|^{3}+n|\\mathcal{D}_{t}|^{2})$ (including $\\mathcal{O}(|\\mathcal{D}_{t}|^{3})$ for training and $\\mathcal{O}(n|\\mathcal{D}_{t}|^{\\tilde{2}})$ for prediction). Given the GP posterior belief, Algorithm 1 involves the following 2 major steps. First, in line 3 of Algorithm 1, it takes ${\\mathcal{O}}(n\\log k)$ to find the top- $k$ inputs $S_{\\mu_{t}}(k)$ by using a max heap of size $k$ and scanning through the GP posterior mean of all $n$ inputs. Second, in line 4 of Algorithm 1, it takes $O(k(n{-}k))$ to scan through the elements in $S_{\\mu_{t}}(k)\\times$ $S_{\\mu_{t}}^{c}(k)$ . Therefore, an iteration of Algorithm 1 takes $\\mathcal{O}(|\\mathcal{D}_{t}|^{3}+n|\\mathcal{D}_{t}|^{2}+n\\log k+k(n-k))$ . ", "page_idx": 6}, {"type": "text", "text": "Remark 4.8 (Active multiple set ordering). Let us consider the problem of estimating $m$ top- $k$ sets: $S(k_{1}),S(k_{2}),\\ldots,S(\\bar{k}_{m})$ simultaneously (motivated in Sec. 1). This problem is analogous to finding $k$ contour lines of a blackbox function, where each contour line represents the boundary between $S(k_{i})$ and its complement $S^{c}(k_{i})$ . To solve this problem, we define the following input pair ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\bigl(\\bar{\\bar{\\mathbf{x}}}_{t},\\bar{\\bar{\\mathbf{x}}}_{t}^{\\prime}\\bigr)\\stackrel{\\triangle}{=}\\underset{(\\mathbf{x},\\mathbf{x}^{\\prime})\\in\\bigl(\\cup_{i=1}^{m}S_{\\mu_{t}}(k_{i})\\times S_{\\mu_{t}}^{c}(k_{i})\\bigr)}{\\mathrm{argmax}}\\,\\rho_{\\pi_{\\mu_{t}}(\\mathbf{x},\\mathbf{x}^{\\prime})}^{(t)}\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In other words, we aim to reduce the maximum regret incurred by the predicted pairwise orderings in all $m$ top- $k$ sets. Given $(\\bar{\\bar{\\mathbf{x}}}_{t},\\bar{\\bar{\\mathbf{x}}}_{t}^{\\prime})$ in (12), MP proceeds by sampling the input $\\mathbf{x}_{t}$ according to Lemma 4.3, i.e., $\\mathbf{x}_{t}\\in\\{\\bar{\\bar{\\mathbf{x}}}_{t}\\ \\bigtriangledown\\bar{\\bar{\\mathbf{x}}}_{t}^{\\prime},\\bar{\\bar{\\mathbf{x}}}_{t}^{\\prime}\\bigtriangleup\\bar{\\bar{\\mathbf{x}}}_{t}^{\\prime},\\bar{\\bar{\\mathbf{x}}}_{t}\\vee\\bar{\\bar{\\mathbf{x}}}_{t}^{\\prime},\\bar{\\bar{\\mathbf{x}}}_{t}\\wedge\\bar{\\bar{\\mathbf{x}}}_{t}^{\\prime}\\}$ . The approach is elaborated in Appendix J. ", "page_idx": 6}, {"type": "image", "img_path": "GkJbXpd3wM/tmp/84bb49d1f764791bc966656287263fc6c31fed190c0d63d54034dab58727d16f.jpg", "img_caption": ["Figure 2: Plot of sampling inputs, GP posterior distribution, and the performance of (a) MP and (b) Var in estimating $S(20)$ of a synthetic function. The comparison pair is $(\\bar{\\mathbf{x}}_{t},\\bar{\\mathbf{x}}_{t}^{\\prime})$ in (11). The histogram on the horizontal axis shows the frequency of sampling inputs in 40 iterations. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "We also note that active multiple set ordering is able to find both maximizers $\\ S(1)$ and minimizers $S^{c}(n-1)$ simultaneously, a problem has not been studied in GP-UCB [19]. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "5.1 Active Set Ordering ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we validate the empirical performance of our MP algorithm with different choices of the sampling input in Lemma 4.3: $\\mathbf{\\bar{\\mathbf{x}}}_{t}\\nabla\\ {\\bar{\\mathbf{x}}}_{t}^{\\prime}$ , $\\bar{\\mathbf{x}}_{t}\\triangle\\bar{\\mathbf{x}}_{t}^{\\prime}$ , $\\bar{\\mathbf{x}}_{t}\\vee\\bar{\\mathbf{x}}_{t}^{\\prime}$ , and $\\bar{\\mathbf{x}}_{t}\\wedge\\bar{\\mathbf{x}}_{t}^{\\prime}$ by comparing with 2 baselines: an uncertainty sampling approach, called Var, that selects the sampling input with the highest GP posterior variance, i.e., $\\mathbf{\\bar{x}}_{t}\\in\\mathop{\\mathrm{argmax}}_{\\mathbf{x}\\in\\mathcal{X}}\\sigma_{t}^{2}(\\mathbf{x})$ , and a baseline, called Rand, that selects the sampling input at random. The regret $r_{\\pi_{\\mu_{t}}(S_{\\mu_{t}}(k),S_{\\mu_{t}}^{c}(k))}$ is used to measure the performance of each algorithm, i.e., the prediction of the top- $k$ set consists of the $k$ inputs with the highest GP posterior mean. ", "page_idx": 7}, {"type": "text", "text": "To begin with, we visualize sampling inputs and the accuracy of $S_{\\mu_{t}}(20)$ that come from our MP algorithm with $\\mathbf{x}_{t}=\\bar{\\mathbf{x}}_{t}\\land\\bar{\\mathbf{x}}_{t}^{\\prime}$ and the Var algorithm in Fig. 2. In Fig. 2a, the histogram shows that the sampling inputs are at the boundary of $S(20)$ . This is highly desirable as it is challenging to decide if an input at the boundary belongs to ${\\mathcal{S}}(20)$ . Similarly, the input pair in (11) also consists of inputs around this boundary (depicted as vertical orange lines). On the other hand, precisely estimating the function evaluations of inputs far from the boundary, e.g., inputs around $\\mathbf{x}=0.85$ (in ${\\cal S}(20)]$ ) and inputs around $\\mathbf{x}=0.1$ (not part of ${\\cal S}(20)]$ ) is unnecessary. We observe that the uncertainty of the GP posterior distribution at these inputs is high in Fig. 2a. Hence, our MP algorithm is able to efficiently concentrate its sampling budget on important inputs at the boundary of the top- $k$ set. Interestingly, this boundary serves as a contour line of the blackbox function, indicating that our solution could potentially be applied to estimate the contour line by specifying the proportion of the input domain where function evaluations exceed this contour. Regarding the Var algorithm (i.e., uncertainty sampling) in Fig. 2b, the histogram shows that sampling inputs are distributed evenly across the input domain. It is because Var aims to reduce the uncertainty of the function evaluation throughout the input domain without considering the current predicted $S_{\\mu_{t}}(20)$ . For example, it is inefficient to select sampling inputs far away from the boundary of $S(20)$ . It is observed that the estimation of function evaluations at the boundary of $S(20)$ using Var is more uncertain than that using the MP algorithm given the same number of sampling inputs. This results in erroneously predicting certain inputs in $S(20)$ (depicted as red dots) and overlooking several inputs in ${\\cal S}(20)$ (depicted as black dots).4 ", "page_idx": 7}, {"type": "text", "text": "We numerically report the performance using the proposed regret $r_{\\pi_{\\mu_{t}}({\\cal S}_{\\mu_{t}}(k),S_{\\mu_{t}}^{c}(k))}$ . The experiments are conducted on 4 synthetic functions: a function sampled from a GP, Branin-Hoo function, Goldstein-Price function with a noise of $\\sigma_{n}\\,=\\,0.1$ , and Hartmann-6D function with a nosie of $\\sigma_{n}=0.01$ [20]. For the first three synthetic functions, the input domain is discretized into a set of 100 points, whereas for the Hartmann-6D function, it is discretized into a set of 1000 points. Motivated by environmental monitoring problems, we generate 3 active set ordering problems that estimate the top-5 set using the dataset of $\\mathrm{NO}_{3}$ concentration in the Lake Zurich (downloaded from https://wldb.ilec.or.jp/Lake/EUR-06/datalist), the dataset of the phosphorus concentration in the Brooms Barn [22], and the dataset of the humidity in the Intel Lab (downloaded from https://db.csail.mit.edu/labdata/labdata.html). The environment field is discretized into a set of 100 locations in the experiments with the $\\mathrm{NO}_{3}$ and humidity datasets and 400 locations in the experiment with the phosphorus dataset. The experiments are repeated 15 times to account for the randomness in the generation of the observations. Further details are provided in Appendix K. The average and the standard error of the regret are shown in Figs. 3s:a- $\\mathrm{\\Delta}^{\\mathrm{g}}$ . There is not any significant difference in the performance of MP with different sampling inputs in Lemma 4.3. Nevertheless, the MP algorithm with any choice of the sampling input in $\\bar{\\mathcal{Q}}_{t}$ outperforms the 2 baselines by converging to lower regret. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "GkJbXpd3wM/tmp/4cae494e9961e0381111f576ba4fa54bc938e09b4ec586fa8244adcb32098095.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 3: Plots of the regret against the iteration in estimating (s:a-f) the top-5 set $S(5)$ and (m:a-f) multiple top- ${\\cdot k}$ sets: $S(1)$ , $\\mathcal{S}(10)$ , and ${\\cal S}(20)$ . ", "page_idx": 8}, {"type": "text", "text": "5.2 Active Multiple Set Ordering ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To empirically validate the performance of our MP algorithm in solving the problem of estimating multiple top- $k$ sets, we consider the problem of estimating ${\\cal S}(1)$ (i.e., maximizers), $\\mathcal{S}(10)$ , and $S(20)$ simultaneously, i.e., $k_{1}=1,k_{2}=10,k_{3}=20$ in Remark 4.8. We utilize the same set of synthetic functions and real-world environmental datasets in the previous section to compare the performance of MP with Var and Rand. The plot of the average and standard error of the maximum regret $\\operatorname*{max}_{k\\in\\{1,10,20\\}}\\bigl(r_{\\pi_{\\mu_{t}}(S_{\\mu_{t}}(k),S_{\\mu_{t}}^{c}(k))}\\bigr)$ over 15 repeated experiments are shown in Figs. 3m:a-g. The MP algorithm outperforms the other 2 baselines by converging to lower regret. In some active multiple set ordering experiments (e.g., in Figs. 3m:a, $3\\mathrm{m};\\mathbf{c}$ ), the performance gaps between MP and Var are smaller than those in the previous active set ordering experiments (e.g., Figs. 3s:a, 3s:c). It is because estimating multiple top- $k$ sets requires more observations, which makes the performance of MP tend towards that of Var which estimates the entire function. ", "page_idx": 8}, {"type": "image", "img_path": "GkJbXpd3wM/tmp/3b6808f5d844adb3a21c1e1f9522d165d31cb744a4452259e8588c32155b1055.jpg", "img_caption": ["Figure 4: Plots of the regret of the predicted maximizer against the iteration. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5.3 Bayesian Optimization ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "When $k=1$ , the active set ordering problem reduces to the BO problem and a sampling input of our MP algorithm, i.e., $\\mathbf{x}_{t}=\\bar{\\mathbf{x}}_{t}\\nabla\\bar{\\mathbf{x}}_{t}^{\\bar{\\prime}}$ , is the same as that of GP-UCB [19]. Therefore, this section empirically demonstrates the performance of MP with different sampling inputs $(\\mathbf{x}_{t}\\in\\left\\{\\bar{\\mathbf{x}}_{t}\\right.\\nabla\\bar{\\mathbf{x}}_{t}^{\\prime},\\bar{\\mathbf{x}}_{t}\\Delta_{t}\\right.$ $\\bar{\\mathbf{x}}_{t}^{\\prime},\\bar{\\mathbf{x}}_{t}\\vee\\bar{\\mathbf{x}}_{t}^{\\prime},\\bar{\\mathbf{x}}_{t}\\wedge\\bar{\\mathbf{x}}_{t}^{\\prime}\\})$ in solving BO. Our aim is not to show that MP achieves the state-of-the-art performance as a BO solver, but rather to demonstrate that it performs comparably to the well-known GP-UCB algorithm. In addition to comparing with GP-UCB (equivalently, MP with $\\mathbf{x}_{t}=\\bar{\\mathbf{x}}_{t}\\nabla\\bar{\\mathbf{x}}_{t}^{\\prime}$ ), we also compare with 3 classical BO solutions: probability of improvement (PI) [13], expected improvement (EI) [15], and max-value entropy search (MES) [21]. The average and the standard error of the regret $r_{\\pi_{\\mu_{t}}(\\mathcal{S}_{\\mu_{t}}(1),S_{\\mu_{t}}^{c}(1))}$ over 15 repeated experiments are shown in Fig. 4. We observe that MP performs comparably with the well-known GP-UCB algorithm (labelled as $\\bar{\\mathbf{x}}_{t}\\nabla\\bar{\\mathbf{x}}_{t}^{\\prime})$ . Expectedly, EI and MES outperform GP-UCB (and hence, MP) in some experiments such as in Figs. 4b and $4c$ . ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper presents a new problem formulation, namely active set ordering, that aims to balance between the expensive estimation of the entire function in ED and that of only the maximizers in BO. We propose the mean prediction (MP) algorithm to address this problem with a theoretical no-regret guarantee. Interestingly, BO can be framed as a special instance of active set ordering, which leads to several new subtle understandings regarding the predicted maximizer and other alternative sampling inputs. Last, the performance of MP is empirically evaluated using various synthetic functions and real-world datasets. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research/project is supported by the National Research Foundation Singapore and DSO National Laboratories under the AI Singapore Programme (AISG Award No: AISG2-RP-2020-018). ", "page_idx": 9}, {"type": "text", "text": "DesCartes: this research is supported by the National Research Foundation, Prime Minister\u2019s Office, Singapore under its Campus for Research Excellence and Technological Enterprise (CREATE) programme. ", "page_idx": 9}, {"type": "text", "text": "This research was partially supported by the Australian Government through the Australian Research Council\u2019s Discovery Projects funding scheme (project DP210102798). The views expressed herein are those of the authors and are not necessarily those of the Australian Government or Australian Research Council. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] I. Bogunovic, J. Scarlett, A. Krause, and V. Cevher. Truncated variance reduction: A unified approach to Bayesian optimization and level-set estimation. In Proc. NIPS, pages 1507\u20131515, 2016.   \n[2] E. Brochu, V. M. Cora, and N. De Freitas. A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. arXiv preprint arXiv:1012.2599, 2010. [3] B. Bryan, R. C. Nichol, C. R. Genovese, J. Schneider, C. J. Miller, and L. Wasserman. Active learning for identifying function threshold boundaries. Proc. NeurIPS, 18, 2005.   \n[4] K. Chaloner and I. Verdinelli. Bayesian experimental design: A review. Statistical science, pages 273\u2013304, 1995.   \n[5] S. R. Chowdhury and A. Gopalan. On kernelized multi-armed bandits. In Proc. ICML, pages 844\u2013853, 2017. [6] P. I. Frazier. A tutorial on Bayesian optimization. arXiv preprint arXiv:1807.02811, 2018. [7] Roman Garnett. Bayesian Optimization. Cambridge University Press, 2022. [8] A. Gotovos, N. Casati, G. Hitz, and A. Krause. Active learning for level set estimation. In Proc. IJCAI, 2013.   \n[9] J. M. Hern\u00e1ndez-Lobato, M. W. Hoffman, and Z. Ghahramani. Predictive entropy search for efficient global optimization of black-box functions. In Proc. NIPS, pages 918\u2013926, 2014.   \n[10] H. Jiang, J. Li, and M. Qiao. Practical algorithms for best-k identification in multi-armed bandits. arXiv preprint arXiv:1705.06894, 2017.   \n[11] S. Kalyanakrishnan and P. Stone. Efficient selection of multiple bandit arms: Theory and practice. In Proc. ICML, volume 10, pages 511\u2013518, 2010.   \n[12] S. Kalyanakrishnan, A. Tewari, P. Auer, and P. Stone. PAC subset selection in stochastic multi-armed bandits. In Proc. ICML, volume 12, pages 655\u2013662, 2012.   \n[13] H. J. Kushner. A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise. Journal of basic engineering, 86(1):97\u2013106, 1964.   \n[14] B. Mason, R. Camilleri, S. Mukherjee, K. Jamieson, R. Nowak, and L. Jain. Nearly optimal algorithms for level set estimation. In Proc. AISTATS, 2022.   \n[15] J. Moc\u02c7kus. The Bayesian approach to global optimization. In System Modeling and Optimization, volume 38, pages 473\u2013481, 1982.   \n[16] C. E. Rasmussen and C. K. I. Williams. Gaussian processes for machine learning. MIT Press, 2006.   \n[17] J. Scarlett, I. Bogunovic, and V. Cevher. Lower bounds on regret for noisy Gaussian process bandit optimization. In Conference on Learning Theory, pages 1723\u20131742, 2017.   \n[18] Burr Settles. Active learning literature survey. 2009.   \n[19] N. Srinivas, A. Krause, S. Kakade, and M. Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proc. ICML, pages 1015\u20131022, 2010.   \n[20] S. Surjanovic and D. Bingham. Virtual library of simulation experiments: Test functions and datasets. Retrieved May 21, 2024, from http://www.sfu.ca/\\~ssurjano.   \n[21] Z. Wang and S. Jegelka. Max-value entropy search for efficient Bayesian optimization. In Proc. ICML, pages 3627\u20133635, 2017.   \n[22] R. Webster and M. A. Oliver. Geostatistics for environmental scientists. John Wiley & Sons, 2007. ", "page_idx": 10}, {"type": "text", "text": "A Proof of Lemma 3.1 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "We will show that $\\rho_{\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}^{(t)}$ is an upper confidence bound of the regret $r_{\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}$ , i.e., ", "page_idx": 11}, {"type": "equation", "text": "$$\nP\\left(\\forall t\\geq1,\\;r_{\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}\\leq\\rho_{\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}^{(t)}\\right)\\geq1-\\delta\\;.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "The regret $r_{\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}$ is defined as follows ", "page_idx": 11}, {"type": "equation", "text": "$$\nr_{\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}\\triangleq\\operatorname*{max}\\left(0,(2\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})-1)(f(\\tilde{\\mathbf{x}}^{\\prime})-f(\\tilde{\\mathbf{x}}))\\right).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Furthermore, from Lemma 2.2, with probability of at least $1-\\delta$ , for all $t\\geq1$ , ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{l_{t}(\\tilde{\\mathbf{x}})\\leq f(\\tilde{\\mathbf{x}})\\leq u_{t}(\\tilde{\\mathbf{x}})}\\\\ &{l_{t}(\\tilde{\\mathbf{x}}^{\\prime})\\leq f(\\tilde{\\mathbf{x}}^{\\prime})\\leq u_{t}(\\tilde{\\mathbf{x}}^{\\prime})}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Hence, with probability of at least $1-\\delta$ , for all $t\\geq1$ , ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f(\\tilde{\\mathbf{x}})-f(\\tilde{\\mathbf{x}}^{\\prime})\\leq u_{t}(\\tilde{\\mathbf{x}})-l_{t}(\\tilde{\\mathbf{x}}^{\\prime})}\\\\ {f(\\tilde{\\mathbf{x}}^{\\prime})-f(\\tilde{\\mathbf{x}})\\leq u_{t}(\\tilde{\\mathbf{x}}^{\\prime})-l_{t}(\\tilde{\\mathbf{x}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Multiplying both sides with $(2\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})-1)$ , we obtain ", "page_idx": 11}, {"type": "equation", "text": "$$\n(2\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})-1)(f(\\tilde{\\mathbf{x}}^{\\prime})-f(\\tilde{\\mathbf{x}}))\\leq\\left\\{\\!\\!\\begin{array}{l l}{u_{t}(\\tilde{\\mathbf{x}})-l_{t}(\\tilde{\\mathbf{x}}^{\\prime})}&{\\mathrm{~if~}\\,\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})=0}\\\\ {u_{t}(\\tilde{\\mathbf{x}}^{\\prime})-l_{t}(\\tilde{\\mathbf{x}})}&{\\mathrm{~if~}\\,\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})=1}\\end{array}\\!\\!\\right.\\triangleq\\rho_{\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}^{(t)}\\cdot\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Therefore, with probability of at least $1-\\delta$ , for all $t\\geq1$ , ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\cdot_{\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}\\triangleq\\operatorname*{max}\\big(0,(2\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})-1)(f(\\tilde{\\mathbf{x}}^{\\prime})-f(\\tilde{\\mathbf{x}}))\\big)\\le\\left\\{\\begin{array}{l l}{\\operatorname*{max}(0,u_{t}(\\tilde{\\mathbf{x}})-l_{t}(\\tilde{\\mathbf{x}}^{\\prime}))}&{\\mathrm{~if~}\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})=0}\\\\ {\\operatorname*{max}(0,u_{t}(\\tilde{\\mathbf{x}}^{\\prime})-l_{t}(\\tilde{\\mathbf{x}}))}&{\\mathrm{~if~}\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})=1\\,.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "B Proof of Lemma 3.5 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "We note that $\\mathbb{1}_{\\mu_{t}(\\tilde{\\mathbf{x}})\\geq\\mu_{t}(\\tilde{\\mathbf{x}}^{\\prime})}=1$ happens when ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad\\qquad\\mu_{t}(\\widetilde{\\mathbf{x}})\\geq\\mu_{t}(\\widetilde{\\mathbf{x}}^{\\prime})}\\\\ &{\\qquad\\Leftrightarrow\\frac{u_{t}(\\widetilde{\\mathbf{x}})+l_{t}(\\widetilde{\\mathbf{x}})}{2}\\geq\\frac{u_{t}(\\widetilde{\\mathbf{x}}^{\\prime})+l_{t}(\\widetilde{\\mathbf{x}}^{\\prime})}{2}}\\\\ &{\\qquad\\Leftrightarrow u_{t}(\\widetilde{\\mathbf{x}})-l_{t}(\\widetilde{\\mathbf{x}}^{\\prime})\\geq u_{t}(\\widetilde{\\mathbf{x}}^{\\prime})-l_{t}(\\widetilde{\\mathbf{x}})}\\\\ &{\\Leftrightarrow\\operatorname*{max}(0,u_{t}(\\widetilde{\\mathbf{x}})-l_{t}(\\widetilde{\\mathbf{x}}^{\\prime}))\\geq\\operatorname*{max}(0,u_{t}(\\widetilde{\\mathbf{x}}^{\\prime})-l_{t}(\\widetilde{\\mathbf{x}}))}\\\\ &{\\qquad\\qquad\\Leftrightarrow\\rho_{\\pi(\\widetilde{\\mathbf{x}},\\widetilde{\\mathbf{x}}^{\\prime})=0}^{(t)}\\geq\\rho_{\\pi(\\widetilde{\\mathbf{x}},\\widetilde{\\mathbf{x}}^{\\prime})=1}^{(t)}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Therefore, ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\mathbb{1}_{\\mu_{t}(\\tilde{\\mathbf{x}})\\geq\\mu_{t}(\\tilde{\\mathbf{x}}^{\\prime})}=\\mathbb{1}_{\\rho_{\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})=0}^{(t)}\\geq\\rho_{\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})=1}^{(t)}}=\\underset{a\\in\\{0,1\\}}{\\mathrm{argmin}}\\,\\rho_{\\pi(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})=a}^{(t)}=\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})\\,.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "C Proof of Lemma 3.6 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": ".ow that $\\rho_{\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}^{(t)}\\leq|\\mathcal{C}_{t}(\\mathbf{x}_{t})|$ when $\\mathbf{x}_{t}$ is taken from the set $\\mathcal{Q}_{t}\\triangleq\\{\\tilde{\\mathbf{x}}\\ \\nabla\\ \\tilde{\\mathbf{x}}^{\\prime},\\tilde{\\mathbf{x}}\\triangle\\tilde{\\mathbf{x}}^{\\prime},\\tilde{\\mathbf{x}}\\vee$ $\\tilde{\\mathbf{x}}^{\\prime},\\tilde{\\mathbf{x}}\\wedge\\tilde{\\mathbf{x}}^{\\prime}\\}$ ", "page_idx": 11}, {"type": "text", "text": "Case 1: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{\\boldsymbol{\\mathsf{\\bar{c}}}\\,\\,\\nabla\\,\\tilde{\\mathbf{x}}^{\\prime}\\,\\overset{\\Delta}{=}\\,\\mathrm{argmax}_{\\mathbf{x}\\in\\{\\bar{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime}\\}}}\\,u_{t}(\\mathbf{x})}\\\\ &{\\mathsf{\\boldsymbol{\\rho}}_{\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}^{(t)}=\\operatorname*{max}(0,\\operatorname*{min}(u_{t}(\\tilde{\\mathbf{x}}^{\\prime})-l_{t}(\\tilde{\\mathbf{x}}),u_{t}(\\tilde{\\mathbf{x}})-l_{t}(\\tilde{\\mathbf{x}}^{\\prime})))}\\\\ &{\\qquad\\qquad\\le\\operatorname*{max}(0,\\operatorname*{min}(u_{t}(\\tilde{\\mathbf{x}}\\,\\nabla\\,\\tilde{\\mathbf{x}}^{\\prime})-l_{t}(\\tilde{\\mathbf{x}}),u_{t}(\\tilde{\\mathbf{x}}\\,\\nabla\\,\\tilde{\\mathbf{x}}^{\\prime})-l_{t}(\\tilde{\\mathbf{x}}^{\\prime})))}\\\\ &{\\qquad\\qquad\\le\\operatorname*{max}(0,u_{t}(\\tilde{\\mathbf{x}}\\,\\nabla\\,\\tilde{\\mathbf{x}}^{\\prime})-l_{t}(\\tilde{\\mathbf{x}}\\,\\nabla\\,\\tilde{\\mathbf{x}}^{\\prime}))}\\\\ &{\\qquad\\qquad=u_{t}(\\tilde{\\mathbf{x}}\\,\\nabla\\,\\tilde{\\mathbf{x}}^{\\prime})-l_{t}(\\tilde{\\mathbf{x}}\\,\\nabla\\,\\tilde{\\mathbf{x}}^{\\prime})}\\\\ &{\\qquad=|\\mathcal{C}_{t}(\\tilde{\\mathbf{x}}\\,\\nabla\\,\\tilde{\\mathbf{x}}^{\\prime})|}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "where (13) is because $\\tilde{\\mathbf{x}}\\perp\\tilde{\\mathbf{x}}^{\\prime}\\triangleq\\operatorname{argmax}_{\\mathbf{x}\\in\\{\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime}\\}}u_{t}(\\mathbf{x})$ , (14) is because $\\tilde{\\textbf{x}}\\nabla\\tilde{\\textbf{x}}^{\\prime}\\in\\{\\tilde{\\textbf{x}},\\tilde{\\textbf{x}}^{\\prime}\\}$ , (15) is because $u_{t}(\\tilde{\\mathbf{x}}\\vee\\tilde{\\mathbf{x}}^{\\prime})-l_{t}(\\tilde{\\mathbf{x}}\\vee\\tilde{\\mathbf{x}}^{\\prime})\\ge0$ . ", "page_idx": 12}, {"type": "text", "text": "Case 2: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\Delta\\tilde{\\mathbf{x}}^{\\prime}\\triangleq\\operatorname*{argmin}_{\\mathbf{x}\\in\\{\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime}\\}}l_{t}(\\mathbf{x})}\\\\ &{\\rho_{\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}^{(t)}=\\operatorname*{max}(0,\\operatorname*{min}(u_{t}(\\tilde{\\mathbf{x}}^{\\prime})-l_{t}(\\tilde{\\mathbf{x}}),u_{t}(\\tilde{\\mathbf{x}})-l_{t}(\\tilde{\\mathbf{x}}^{\\prime})))}\\\\ &{\\leq\\operatorname*{max}(0,\\operatorname*{min}(u_{t}(\\tilde{\\mathbf{x}}^{\\prime})-l_{t}(\\tilde{\\mathbf{x}}\\Delta\\tilde{\\mathbf{x}}^{\\prime}),u_{t}(\\tilde{\\mathbf{x}})-l_{t}(\\tilde{\\mathbf{x}}\\Delta\\tilde{\\mathbf{x}}^{\\prime})))}\\\\ &{\\leq\\operatorname*{max}(0,u_{t}(\\tilde{\\mathbf{x}}\\Delta\\tilde{\\mathbf{x}}^{\\prime})-l_{t}(\\tilde{\\mathbf{x}}\\Delta\\tilde{\\mathbf{x}}^{\\prime}))}\\\\ &{=u_{t}(\\tilde{\\mathbf{x}}\\Delta\\tilde{\\mathbf{x}}^{\\prime})-l_{t}(\\tilde{\\mathbf{x}}\\Delta\\tilde{\\mathbf{x}}^{\\prime})}\\\\ &{=|\\mathcal{C}_{t}(\\tilde{\\mathbf{x}}\\Delta\\tilde{\\mathbf{x}}^{\\prime})|}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where (18) is because $\\begin{array}{r}{\\tilde{\\mathbf{x}}\\triangleq\\tilde{\\mathbf{x}}^{\\prime}\\triangleq\\operatorname*{argmin}_{\\mathbf{x}\\in\\{\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime}\\}}l_{t}(\\mathbf{x})}\\end{array}$ , (19) is because $\\tilde{\\bf x}\\triangle\\tilde{\\bf x}^{\\prime}\\in\\{\\tilde{\\bf x},\\tilde{\\bf x}^{\\prime}\\}$ , (20) is because $u_{t}(\\tilde{\\mathbf{x}}\\triangle\\tilde{\\mathbf{x}}^{\\prime})-l_{t}(\\tilde{\\mathbf{x}}\\triangle\\tilde{\\mathbf{x}}^{\\prime})\\ge0$ . ", "page_idx": 12}, {"type": "text", "text": "Case 3: $\\begin{array}{r}{\\mathbf{x}_{t}=\\tilde{\\mathbf{x}}\\vee\\tilde{\\mathbf{x}}^{\\prime}\\triangleq\\operatorname{argmax}_{\\mathbf{x}\\in\\{\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime}\\}}\\left\\vert\\mathcal{C}_{t}(\\mathbf{x})\\right\\vert}\\end{array}$ , i.e., ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|{\\mathcal C}_{t}(\\tilde{\\mathbf{x}}\\vee\\tilde{\\mathbf{x}}^{\\prime})|=\\underset{\\mathbf{x}\\in\\{\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime}\\}}{\\operatorname*{max}}|{\\mathcal C}_{t}(\\mathbf{x})|}\\\\ &{~~~~~~~~~~~~~~~~\\geq|{\\mathcal C}_{t}(\\tilde{\\mathbf{x}}\\wedge\\tilde{\\mathbf{x}}^{\\prime})|}\\\\ &{~~~~~~~~~~~~~~~~\\geq\\rho_{\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}^{(t)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where (23) is because $\\tilde{\\mathbf{x}}\\triangle\\tilde{\\mathbf{x}}^{\\prime}\\in\\left\\{\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime}\\right\\}$ , (24) is from the above proof of case 2 in (21). ", "page_idx": 12}, {"type": "text", "text": "Case 4: $\\begin{array}{r}{\\mathbf{x}_{t}=\\tilde{\\mathbf{x}}\\wedge\\tilde{\\mathbf{x}}^{\\prime}\\triangleq\\operatorname{argmin}_{\\mathbf{x}\\in\\{\\tilde{\\mathbf{x}}\\ \\tilde{\\mathbf{x}}^{\\prime},\\tilde{\\mathbf{x}}\\wedge\\tilde{\\mathbf{x}}^{\\prime}\\}}\\left|\\mathcal{C}_{t}(\\mathbf{x})\\right|}\\end{array}$ . From (16) and (21), it follows that $|\\mathcal{C}_{t}(\\tilde{\\mathbf{x}}\\wedge$ \u02dcx\u2032)| \u2265\u03c1(\u03c0t\u00b5)t(x\u02dc,x\u02dc\u2032). ", "page_idx": 12}, {"type": "text", "text": "D Proof of Theorem 3.7 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "By choosing $\\mathbf{x}_{t}$ following Lemma 3.6, with probability of at least $1-\\delta$ , for all $t\\geq1$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\nr_{\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}\\le\\rho_{\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}^{(t)}\\le|\\mathcal{C}_{t}(\\mathbf{x}_{t})|=2\\beta_{t}^{1/2}\\sigma_{t}(\\mathbf{x}_{t})\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Hence, with probability of at least $1-\\delta$ , for all $T\\geq1$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}r_{\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}\\leq\\sum_{t=1}^{T}2\\beta_{t}^{1/2}\\sigma_{t}(\\mathbf{x}_{t})\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Since $\\beta_{t}$ is a non-decreasing sequence, $\\beta_{t}\\,\\leq\\,\\beta_{T}$ for all $t\\leq T$ . Therefore, with probability of at least $1-\\delta$ , for all $T\\geq1$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}r_{\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}\\leq2\\beta_{T}^{1/2}\\sum_{t=1}^{T}\\sigma_{t}(\\mathbf{x}_{t})\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "From Lemma 4 in [5], ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\sigma_{t}(\\mathbf{x}_{t})\\leq\\sqrt{4(T+2)\\gamma_{T}}=\\mathcal{O}(\\sqrt{T\\gamma_{T}})\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Hence, with probability of at least $1-\\delta$ , for all $T\\geq1$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}r_{\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}\\leq\\mathcal{O}(\\sqrt{T\\beta_{T}\\gamma_{T}})\\;.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "i.e., ", "page_idx": 12}, {"type": "equation", "text": "$$\nR_{T}\\triangleq\\sum_{t=1}^{T}r_{\\pi_{\\mu_{t}}(\\tilde{\\mathbf{x}},\\tilde{\\mathbf{x}}^{\\prime})}\\leq\\mathcal{O}(\\sqrt{T\\beta_{T}\\gamma_{T}})\\,.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "E Bayesian Optimization as Active Set Ordering with $k=1$ ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "E.1 Regret ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "For a predicted top-1 set ${\\cal S}_{\\mu_{t}}(1)$ , $\\pi_{\\mu_{t}}(S_{\\mu_{t}}(1),S_{\\mu_{t}}^{c}(1))=1$ . Hence, the regret for predicting ${\\cal S}_{\\mu_{t}}(1)$ is expressed as follows. ", "page_idx": 13}, {"type": "equation", "text": "$$\nr_{\\pi_{\\mu_{t}}(S_{\\mu_{t}}(1),S_{\\mu_{t}}^{c}(1))}=\\operatorname*{max}\\left(0,\\operatorname*{max}_{(\\mathbf{x},\\mathbf{x}^{\\prime})\\in S_{\\mu_{t}}(1)\\times S_{\\mu_{t}}^{c}(1)}f(\\mathbf{x}^{\\prime})-f(\\mathbf{x})\\right)~.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Let $S_{\\mu_{t}}(1)\\triangleq\\{\\hat{\\mathbf{x}}_{*}\\}$ and $\\mathbf{x}_{*}\\in\\operatorname{argmax}_{\\mathbf{x}\\in\\mathcal{X}}f(\\mathbf{x})$ . ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r_{\\pi_{\\mu_{t}}(S_{\\mu_{t}}(1),S_{\\mu_{t}}^{c}(1))}=\\operatorname*{max}\\left(0,\\underset{\\mathbf{x}\\in S_{\\mu_{t}}^{c}(1)}{\\operatorname*{max}}(f(\\mathbf{x})-f(\\hat{\\mathbf{x}}_{*}))\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\operatorname*{max}\\left(0,\\left(\\underset{\\mathbf{x}\\in S_{\\mu_{t}}^{c}(1)}{\\operatorname*{max}}f(\\mathbf{x})\\right)-f(\\hat{\\mathbf{x}}_{*})\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since $\\mathcal{X}=S_{\\mu_{t}}(1)\\cup S_{\\mu_{t}}^{c}(1)$ , there are 2 cases ", "page_idx": 13}, {"type": "equation", "text": "$$\nr_{\\pi_{\\mu_{t}}(S_{\\mu_{t}}(1),S_{\\mu_{t}}^{c}(1))}=\\operatorname*{max}\\left(0,f(\\mathbf{x}_{*})-f(\\hat{\\mathbf{x}}_{*})\\right)=f(\\mathbf{x}_{*})-f(\\hat{\\mathbf{x}}_{*})\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "\u2022 If $\\mathbf{x}_{*}\\in S_{\\mu_{t}}(1)$ , i.e., $\\mathbf{x}_{*}=\\hat{\\mathbf{x}}_{*}$ , then $\\mathrm{max}_{\\mathbf{x}\\in S_{\\mu_{t}}^{c}(1)}\\,f(\\mathbf{x})\\leq f(\\hat{\\mathbf{x}}_{*})$ and ", "page_idx": 13}, {"type": "equation", "text": "$$\nr_{\\pi_{\\mu_{t}}(S_{\\mu_{t}}(1),S_{\\mu_{t}}^{c}(1))}=0=f({\\bf x}_{*})-f(\\hat{\\bf x}_{*})\\;.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "E.2 Sampling Input ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We will show that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\mathbf{x}}_{t}\\nabla\\ \\bar{\\mathbf{x}}_{t}^{\\prime}\\in\\underset{\\mathbf{x}\\in\\mathcal{X}}{\\mathrm{argmax}}\\,u_{t}(\\mathbf{x})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "When $k=1$ , due to the definition of $\\mathcal{S}_{\\mu_{t}}(k)$ in Definition 4.2 ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{x}}_{t}\\in\\underset{\\mathbf{x}\\in\\mathcal{X}}{\\mathrm{argmax}}\\,\\mu_{t}(\\mathbf{x})\\mathrm{~.~}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Let ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{x}}_{t}\\in\\underset{\\mathbf{x}\\in\\mathcal{X}}{\\mathrm{argmax}}\\,u_{t}(\\mathbf{x})\\mathrm{~.~}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We consider the following 2 cases: ", "page_idx": 13}, {"type": "text", "text": "Case 1: If $u_{t}(\\hat{{\\mathbf x}}_{t})=u_{t}(\\bar{{\\mathbf x}}_{t})$ , then ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\mathbf{x}}_{t}\\nabla\\ \\bar{\\mathbf{x}}_{t}^{\\prime}\\in\\underset{\\mathbf{x}\\in\\mathcal{X}}{\\mathrm{argmax}}\\,u_{t}(\\mathbf{x})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Case 2: If $u_{t}(\\hat{\\mathbf{x}}_{t})\\neq u_{t}(\\bar{\\mathbf{x}}_{t})$ which implies that $\\hat{\\mathbf{x}}_{t}\\in S_{\\mu_{t}}^{c}(k)$ and $u_{t}(\\hat{{\\bf x}}_{t})>u_{t}(\\bar{{\\bf x}}_{t})$ , then we prove that $u_{t}(\\hat{\\mathbf{x}}_{t})=u_{t}(\\bar{\\mathbf{x}}_{t}^{\\prime})$ by contradiction. Assuming that ", "page_idx": 13}, {"type": "equation", "text": "$$\nu_{t}(\\hat{\\mathbf{x}}_{t})>u_{t}(\\Bar{\\mathbf{x}}_{t}^{\\prime})\\mathrm{~.~}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Let us consider the following upper confidence bounds of pairwise orderings: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho_{\\pi_{\\mu_{t}}(\\bar{\\mathbf{x}}_{t},\\hat{\\mathbf{x}}_{t})}^{(t)}=\\operatorname*{max}(0,u_{t}(\\hat{\\mathbf{x}}_{t})-l_{t}(\\bar{\\mathbf{x}}_{t}))=u_{t}(\\hat{\\mathbf{x}}_{t})-l_{t}(\\bar{\\mathbf{x}}_{t})>0}\\\\ &{\\rho_{\\pi_{\\mu_{t}}(\\bar{\\mathbf{x}}_{t},\\bar{\\mathbf{x}}_{t}^{\\prime})}^{(t)}=\\operatorname*{max}(0,u_{t}(\\bar{\\mathbf{x}}_{t}^{\\prime})-l_{t}(\\bar{\\mathbf{x}}_{t}))\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Furthermore, from the choice of $(\\bar{\\mathbf{x}}_{t},\\bar{\\mathbf{x}}_{t}^{\\prime})$ in (11) and $\\hat{\\mathbf{x}}_{t}\\in S_{\\mu_{t}}^{c}(k)$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\rho_{\\pi_{\\mu_{t}}(\\bar{\\bf x}_{t},\\bar{\\bf x}_{t}^{\\prime})}^{(t)}\\geq\\rho_{\\pi_{\\mu_{t}}(\\bar{\\bf x}_{t},\\hat{\\bf x}_{t})}^{(t)}\\;.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Hence, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{max}(0,u_{t}(\\bar{\\mathbf{x}}_{t}^{\\prime})-l_{t}(\\bar{\\mathbf{x}}_{t})\\geq\\operatorname*{max}(0,u_{t}(\\hat{\\mathbf{x}}_{t})-l_{t}(\\bar{\\mathbf{x}}_{t}))=u_{t}(\\hat{\\mathbf{x}}_{t})-l_{t}(\\bar{\\mathbf{x}}_{t})>0}\\\\ &{\\qquad\\quad u_{t}(\\bar{\\mathbf{x}}_{t}^{\\prime})-l_{t}(\\bar{\\mathbf{x}}_{t})\\geq u_{t}(\\hat{\\mathbf{x}}_{t})-l_{t}(\\bar{\\mathbf{x}}_{t})}\\\\ &{\\qquad\\qquad\\qquad u_{t}(\\bar{\\mathbf{x}}_{t}^{\\prime})\\geq u_{t}(\\hat{\\mathbf{x}}_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which contradicts to the assumption 25. Therefore, $u_{t}(\\hat{\\mathbf{x}}_{t})=u_{t}(\\bar{\\mathbf{x}}_{t}^{\\prime})$ and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\mathbf{x}}_{t}\\nabla\\ \\bar{\\mathbf{x}}_{t}^{\\prime}\\in\\underset{\\mathbf{x}\\in\\mathcal{X}}{\\mathrm{argmax}}\\,u_{t}(\\mathbf{x})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "F Upper Confidence Bound of the Regret $r_{\\pi(\\mathcal{X}_{0},\\mathcal{X}_{1})}$ ", "text_level": 1, "page_idx": 14}, {"type": "equation", "text": "$$\nr_{\\pi(\\mathcal{X}_{0},\\mathcal{X}_{1})}\\triangleq\\operatorname*{max}_{(\\mathbf{x},\\mathbf{x}^{\\prime})\\in\\mathcal{X}_{0}\\times\\mathcal{X}_{1}}r_{\\pi(\\mathbf{x},\\mathbf{x}^{\\prime})=\\pi(\\mathcal{X}_{0},\\mathcal{X}_{1})}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "With probability of at least $1-\\delta$ , for all $t\\,\\geq\\,1$ and for all $\\{\\mathbf{x},\\mathbf{x}^{\\prime}\\}\\,\\subset\\,\\mathcal{X}$ , $r_{\\pi({\\bf x},{\\bf x}^{\\prime})=\\pi(\\mathcal{X}_{0},\\mathcal{X}_{1})}\\le$ \u03c1(\u03c0t()x,x\u2032)=\u03c0(X0,X1). Therefore, with probability of at least 1 \u2212\u03b4, for all t \u22651 and for all {x, x\u2032} \u2282X, ", "page_idx": 14}, {"type": "equation", "text": "$$\nr_{\\pi(X_{0},X_{1})}\\leq\\operatorname*{max}_{(\\mathbf{x},\\mathbf{x}^{\\prime})\\in{\\mathcal{X}}_{0}\\times{\\mathcal{X}}_{1}}\\rho_{\\pi(\\mathbf{x},\\mathbf{x}^{\\prime})=\\pi({\\mathcal{X}}_{0},{\\mathcal{X}}_{1})}^{(t)}\\triangleq\\rho_{\\pi({\\mathcal{X}}_{0},{\\mathcal{X}}_{1})}^{(t)}\\ .\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "G Transitivity of Pairwise Orderings ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The transitivity property of the binary relation $\\mu_{\\pi_{t}}$ follows directly from its connection to the GP posterior mean in Lemma 3.5. In particular, we would like to show that if ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\pi_{\\mu_{t}}(\\mathbf{x},\\mathbf{x}^{\\prime})=1}&{{}\\pi_{\\mu_{t}}(\\mathbf{x}^{\\prime},\\mathbf{x}^{\\prime\\prime})=1}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\pi_{\\mu_{t}}(\\mathbf{x},\\mathbf{x}^{\\prime\\prime})=1\\;.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "From Lemma 3.5, the premise (26) implies that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mu_{t}(\\mathbf{x})\\geq\\mu_{t}(\\mathbf{x}^{\\prime})\\quad\\mu_{t}(\\mathbf{x}^{\\prime})\\geq\\mu_{t}(\\mathbf{x}^{\\prime\\prime})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which implies that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mu_{t}(\\mathbf{x})\\geq\\mu_{t}(\\mathbf{x}^{\\prime\\prime})\\ .\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Applying Lemma 3.5 again, we conclude ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\pi_{\\mu_{t}}(\\mathbf{x},\\mathbf{x}^{\\prime\\prime})=1\\;.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "H Proof of Lemma 4.3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Applying Lemma 3.6 to the input pair $(\\bar{\\mathbf{x}}_{t},\\bar{\\mathbf{x}}_{t}^{\\prime})$ , by selecting $\\mathbf{x}_{t}\\in\\big\\{\\bar{\\mathbf{x}}_{t}\\ \\bigtriangledown\\bar{\\mathbf{x}}_{t}^{\\prime},\\bar{\\mathbf{x}}_{t}\\bigtriangleup\\bar{\\mathbf{x}}_{t}^{\\prime},\\bar{\\mathbf{x}}_{t}\\bigvee\\bar{\\mathbf{x}}_{t}^{\\prime},\\bar{\\mathbf{x}}_{t}\\wedge\\bar{\\mathbf{x}}_{t}^{\\prime}\\big\\},$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n|\\mathcal{C}_{t}(\\mathbf{x}_{t})|\\geq\\rho_{\\pi_{\\mu_{t}}(\\bar{\\mathbf{x}}_{t},\\bar{\\mathbf{x}}_{t}^{\\prime})}^{(t)}\\ .\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Furthermore, from the choice of $(\\bar{\\mathbf{x}}_{t},\\bar{\\mathbf{x}}_{t}^{\\prime})$ in (11), ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\rho_{\\pi_{\\mu_{t}}(\\bar{\\mathbf{x}}_{t},\\bar{\\mathbf{x}}_{t}^{\\prime})}^{(t)}=\\operatorname*{max}_{(\\mathbf{x},\\mathbf{x}^{\\prime})\\in S_{\\mu_{t}}(k)\\times S_{\\mu_{t}}^{c}(k)}\\rho_{\\pi_{\\mu_{t}}(\\mathbf{x},\\mathbf{x}^{\\prime})}^{(t)}=\\rho_{\\pi_{\\mu_{t}}(S_{\\mu_{t}}(k),S_{\\mu_{t}}^{c}(k)}^{(t)}\\cdot\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathcal{C}_{t}(\\mathbf{x}_{t})|\\ge\\rho_{\\pi_{\\mu_{t}}(S_{\\mu_{t}}(k),S_{\\mu_{t}}^{c}(k)}^{(t)}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Algorithm 2 Mean Prediction (MP) for Active Multiple Set Ordering ", "page_idx": 15}, {"type": "text", "text": "Require: $\\mathcal{X}$ , $\\mathcal{D}_{0}$ , $\\{k_{1},k_{2},\\dots,k_{m}\\},$ $T$   \n1: for $t=1$ to $T$ do   \n2: Update GP posterior belief: $\\{\\mu_{t}(\\mathbf{x})\\}_{\\mathbf{x}\\in\\mathcal{X}},\\{\\sigma_{t}(\\mathbf{x})\\}_{\\mathbf{x}\\in\\mathcal{X}}$ .   \n3: Construct $\\{S_{\\mu_{t}}(k_{i})\\}_{i=1}^{m}$ as the collection of $m$ top- $k$ sets predicted using $\\mu_{t}$ . $\\triangleright$ Prediction   \n4: $(\\bar{\\bar{\\mathbf{x}}}_{t},\\bar{\\bar{\\mathbf{x}}}_{t}^{\\prime})\\triangleq\\operatorname{argmax}_{(\\mathbf{x},\\mathbf{x}^{\\prime})\\in\\cup_{i=1}^{m}S_{\\mu_{t}}(k_{i})\\times S_{\\mu_{t}}^{c}(k_{i})}\\rho_{\\pi_{\\mu_{t}}(\\mathbf{x},\\mathbf{x}^{\\prime})}^{(t)}$   \n5: Select $\\mathbf{x}_{t}\\in\\{\\bar{\\mathbf{x}}_{t}\\ \\nabla\\bar{\\mathbf{x}}_{t}^{\\prime},\\bar{\\mathbf{x}}_{t}\\triangle\\bar{\\mathbf{x}}_{t}^{\\prime},\\bar{\\mathbf{x}}_{t}\\vee\\bar{\\mathbf{x}}_{t}^{\\prime},\\bar{\\mathbf{x}}_{t}\\wedge\\bar{\\mathbf{x}}_{t}^{\\prime}\\}.$ . \u25b7Sampling input   \n6: $\\mathbf{y}_{t}(\\mathcal{D}_{t})\\leftarrow\\mathbf{y}(\\mathcal{D}_{t-1})\\cup\\{y(\\mathbf{x}_{t})\\}$   \n7: end for   \n8: Update GP posterior belief: $\\{\\mu_{T+1}(\\mathbf{x})\\}_{\\mathbf{x}\\in\\mathcal{X}},\\{\\sigma_{T+1}(\\mathbf{x})\\}_{\\mathbf{x}\\in\\mathcal{X}}$ .   \n$k$   \n9: Construct $\\{\\bar{S}_{\\mu_{T+1}}(k_{i})\\}_{i=1}^{m}$ as the collection of $m$ top- sets predicted using $\\mu_{T+1}$ .   \n10: return $\\{S_{\\mu_{T+1}}(k_{i})\\}_{i=1}^{m}$ . ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "I Proof of Theorem 4.4 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "By choosing $\\mathbf{x}_{t}$ following Lemma 4.3, with probability of at least $1-\\delta$ , for all $t\\geq1$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\nr_{\\pi_{\\mu_{t}}(S_{\\mu_{t}}(k),S_{\\mu_{t}}^{c}(k))}\\le\\rho_{\\pi_{\\mu_{t}}(S_{\\mu_{t}}(k),S_{\\mu_{t}}^{c}(k))}^{(t)}\\le|{\\mathcal C}_{t}({\\bf x}_{t})|=2\\beta_{t}^{1/2}\\sigma_{t}({\\bf x}_{t})\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Furthermore, from the non-decreasing property of the sequence $(\\beta_{t})_{t=1}^{T}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}r_{\\pi_{\\mu_{t}}(S_{\\mu_{t}}(k),S_{\\mu_{t}}^{c}(k))}\\leq\\sum_{t=1}^{T}2\\beta_{t}^{1/2}\\sigma_{t}(\\mathbf{x}_{t})\\leq2\\beta_{T}^{1/2}\\sum_{t=1}^{T}\\sigma_{t}(\\mathbf{x}_{t})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By utilizing the result from [5] like Appendix D, we can obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\sigma_{t}(\\mathbf{x}_{t})\\leq\\mathcal{O}(\\sqrt{T\\gamma_{T}})\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, with probability of at least $1-\\delta$ , for all $T\\geq1$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}2\\beta_{T}^{1/2}\\sigma_{t}(\\mathbf{x}_{t})\\leq\\mathcal{O}(\\sqrt{T\\beta_{T}\\gamma_{T}})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "i.e., ", "page_idx": 15}, {"type": "equation", "text": "$$\nR_{T,k}\\triangleq\\sum_{t=1}^{T}r_{\\pi_{\\mu_{t}}(S_{\\mu_{t}}(k),S_{\\mu_{t}}^{c}(k))}\\leq\\mathcal{O}(\\sqrt{T\\beta_{T}\\gamma_{T}})\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "J Active Multiple Set Ordering ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The pseudocode of MP algorithm for the active multiple set ordering problem is shown in Algorithm 2.   \nIn the rest of this section, we prove the cumulative regret bound of Algorithm 2. ", "page_idx": 15}, {"type": "text", "text": "We recall that in (12), ", "page_idx": 15}, {"type": "equation", "text": "$$\n(\\bar{\\bar{\\mathbf{x}}}_{t},\\bar{\\bar{\\mathbf{x}}}_{t}^{\\prime})\\triangleq\\underset{(\\mathbf{x},\\mathbf{x}^{\\prime})\\in\\left(\\cup_{i=1}^{m}S_{\\mu_{t}}\\left(k_{i}\\right)\\times S_{\\mu_{t}}^{c}\\left(k_{i}\\right)\\right)}{\\mathrm{argmax}}\\,\\rho_{\\pi_{\\mu_{t}}\\left(\\mathbf{x},\\mathbf{x}^{\\prime}\\right)}^{(t)}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\forall i\\in\\{1,2,\\ldots,m\\},\\,\\,\\rho_{\\pi_{\\mu_{t}}(\\bar{\\mathbf{x}}_{t},\\bar{\\mathbf{x}}_{t}^{\\prime})}^{(t)}\\geq\\underset{(\\mathbf{x},\\mathbf{x}^{\\prime})\\in S(k_{i})\\times S^{c}(k_{i})}{\\operatorname*{max}}\\rho_{\\pi_{\\mu_{t}}(\\mathbf{x},\\mathbf{x}^{\\prime})}^{(t)}}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\rho_{\\pi_{\\mu_{t}}(S_{\\mu_{t}}(k_{i}),S_{\\mu_{t}}^{c}(k_{i}))}^{(t)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Furthermore, applying Lemma 3.6 to the input pair $(\\bar{\\bar{\\mathbf{x}}}_{t},\\bar{\\bar{\\mathbf{x}}}_{t}^{\\prime})$ , for any $\\mathbf{x}_{t}\\in\\{\\bar{\\bar{\\mathbf{x}}}_{t}\\ \\nabla\\bar{\\bar{\\mathbf{x}}}_{t}^{\\prime},\\bar{\\bar{\\mathbf{x}}}_{t}\\triangle\\bar{\\bar{\\mathbf{x}}}_{t}^{\\prime},\\bar{\\bar{\\mathbf{x}}}_{t}\\vee$ $\\bar{\\bar{\\mathbf{x}}}_{t}^{\\prime},\\bar{\\bar{\\mathbf{x}}}_{t}\\wedge\\bar{\\bar{\\mathbf{x}}}_{t}^{\\prime}\\}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n|\\mathcal{C}_{t}(\\mathbf{x}_{t})|\\ge\\rho_{\\pi_{\\mu_{t}}(\\bar{\\mathbf{x}}_{t},\\bar{\\mathbf{x}}_{t}^{\\prime})}^{(t)}\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\forall i\\in\\{1,2,\\ldots,m\\},}\\\\ {|{\\mathcal{C}}_{t}(\\mathbf{x}_{t})|\\geq\\rho_{\\pi_{\\mu_{t}}(S_{\\mu_{t}}(k_{i}),S_{\\mu_{t}}^{c}(k_{i}))}^{(t)}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence, with probability of at least $1-\\delta$ , for all $t\\geq1$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\forall i\\in\\left\\{1,2,\\ldots,m\\right\\},\\;r_{\\pi_{\\mu_{t}}(S_{\\mu_{t}}(k_{i}),S_{\\mu_{t}}^{c}(k_{i}))}\\leq\\rho_{\\pi_{\\mu_{t}}(S_{\\mu_{t}}(k_{i}),S_{\\mu_{t}}^{c}(k_{i}))}^{(t)}\\leq\\left|\\mathcal{C}_{t}(\\mathbf{x}_{t})\\right|.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As a result, with probability of at least $1-\\delta$ , for all $T\\geq1$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\forall i\\in\\{1,2,\\ldots,m\\},R_{T,k_{i}}\\triangleq\\sum_{t=1}^{T}r_{\\pi_{\\mu_{t}}(S_{\\mu_{t}}(k_{i}),S_{\\mu_{t}}^{c}(k_{i}))}\\leq\\sum_{t=1}^{T}\\left|\\mathcal{C}_{t}(\\mathbf{x}_{t})\\right|\\leq\\mathcal{O}(\\sqrt{T\\beta_{T}\\gamma_{T}})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last inequality comes from Appendix I. ", "page_idx": 16}, {"type": "text", "text": "K Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "All experiments were conducted on a computer equipped with an AMD Ryzen 7 6800HS processor and 16GB of RAM. ", "page_idx": 16}, {"type": "text", "text": "To generate a function sampled from a GP, we randomly generate 3 observations $\\{(\\mathbf{x}_{i},y(\\mathbf{x}_{i}))\\}_{i=1}^{3}$ and fit a GP model to these observations. Then, we sample function evaluations at all inputs in the domain from the GP posterior distribution. These function evaluations are considered the evaluations of the blackbox function. To generate an observation at a sampling input, we add a Gaussian noise (of $\\sigma_{n}=0.1)$ ) to the evaluations of the blackbox function at the sampling input. ", "page_idx": 16}, {"type": "text", "text": "The expressions for the Branin-Hoo and Goldstein-Price functions are described at [20]. We transform the input domain of these functions to $[0,1]^{2}$ and standardize the function evaluations. The input domain is discretized into $n=100$ points randomly selected in the domain $[0,1]^{2}$ . The noise is chosen with $\\sigma_{n}=0.1$ . ", "page_idx": 16}, {"type": "text", "text": "To perform experiments with the $\\mathrm{NO}_{3}$ dataset from Lake Zurich (available at https://wldb.ilec. or.jp/Lake/EUR-06/datalist), we standardize the $\\mathrm{NO}_{3}$ measurements. Then, a GP model is trained on the standardized dataset to generate the noisy evaluations of the blackbox function over $n=100$ randomly chosen locations. ", "page_idx": 16}, {"type": "text", "text": "We use the logarithmic values of the phosphorus measurements in the soy survey of Brooms Barn [22] to construct a blackbox function. The locations are normalized to the range $[0,\\dot{1}]^{2}$ and the logarithmic values of the phosphorus measurements are standardized. Then, we train a GP model to generate the noisy evaluations of the blackbox function over $n=400$ randomly chosen locations. ", "page_idx": 16}, {"type": "text", "text": "To perform experiments with the humidity dataset, we extract the humidity measurements at different locations with the same mote id of 31167 from the Intel Lab data (available at https://db.csail. mit.edu/labdata/labdata.html). The humidity measurements are standardized. Then, a GP model is trained to this extracted dataset to generate the noisy evaluations of the blackbox function over $n=100$ randomly chosen locations. ", "page_idx": 16}, {"type": "text", "text": "To remove the potential inefficiency due to repeated sampling in Rand and Var, we provide additional experiments by replacing the Rand and Var baselines with RandNoRepl and VarNoRepl, which do not allow repeated sampling. This modification potentially gives RandNoRepl (random sampling without replacement across different iterations) and VarNoRepl (uncertainty sampling without replacement across different iterations) an additional advantage over our solutions which allow repeated sampling. By avoiding repeated sampling, RandNoRepl and VarNoRepl can sample the input domain more uniformly, whereas our methods might re-sample certain input regions. However, as shown in Figure 5, RandNoRepl and VarNoRepl still do not outperform our solutions. The justification for the efficiency of our solutions is in the nature of noisy observations: With a noise standard deviation of $\\sigma_{n}=0.1$ , a single observation at each input may not suffice to accurately determine the ordering with its neighboring inputs in terms of the function value. Hence, spreading the sampling budget across the whole input domain may not perform well. In contrast, our approach allocates more sampling inputs to the boundary of the top- $k$ set, where it is particularly challenging to check if inputs belong to the top- $k$ set (see Figure 2). ", "page_idx": 16}, {"type": "image", "img_path": "GkJbXpd3wM/tmp/8d6d60342d4f684ea5c70d5396e573075a44b5cfa757cf82f7031c2ae20e371e.jpg", "img_caption": ["Figure 5: Plots of the regret against the iteration in estimating the top-5 set $\\mathcal{S}(t)$ . "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Our contributions include introducing the active set ordering problem, designing a novel solution with theoretical performance guarantee, and investigating a connection to Bayesian optimization. The experiments are performed on environmental monitoring datasets which aligns with the motivation in the introduction. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The limitations are elaborated as assumptions in the paper including: a finite input domain of the blackbox function (see Sec. 2.1) and the blackbox function belonging to a reproducing kernel Hilbert space with a bounded norm (see Sec. 2.2). ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper. \u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We have provided the proofs for all theoretical results in the appendix of the paper. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification:We have described the experiments and provided the code and datasets for reproducing the experimental results. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same ", "page_idx": 18}, {"type": "text", "text": "dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. ", "page_idx": 19}, {"type": "text", "text": "\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We have provided the source code, datasets, and scripts to reproduce the experiment results. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We have provided the URLs where datasets are downloaded and described parameters used in the experiments such as the size of the input domain and the noise variance. Other parameters such as the random seed are configured in the submitted code. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We have repeated the experiments with different random seeds and reported the average and standard error of the experiment results. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We describe the computer resources in Appendix K. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We ensure that the research conforms with the NeurIPS Code of Ethics. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our work proposes a general active learning problem that does not have any direct path to any negative applications. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have cited the sources and/or provided the URLs to all public datasets used in the paper. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]