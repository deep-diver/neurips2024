[{"figure_path": "i8JaxY7tDI/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of Read-ME. This figure shows the refactoring of a pre-trained dense model (in yellow) into two experts (in red and green). After refactoring, the model is deployed, and the serving timeline is depicted. At time t = 0, multiple inference requests (each a sequence of tokens) are queued, with expert assignment for each token undecided (\"?\") until processed by the router. Our router pre-gates tokens before inference, enabling expert-aware batching. Tokens are routed to their respective experts and batched accordingly: at t = 0 for Expert 1 (red) and at t = 1 for Expert 2 (green). New tokens enter the queue at each time step, with routing computed only for incoming tokens marked \"?\".", "description": "This figure illustrates the Read-ME framework.  A pre-trained dense LLM is refactored into smaller expert models. The figure shows how multiple inference requests are queued and processed by a pre-gating router, which enables expert-aware batching. The timeline demonstrates how tokens are routed to experts and batched, improving efficiency.", "section": "2 Pre-gating Sparse Mixture of Experts"}, {"figure_path": "i8JaxY7tDI/figures/figures_2_1.jpg", "caption": "Figure 2: (a) Visualization of transition matrix between the (l-1)-th layer and the l-th layer, where each coordinate [{s, t}, {i, j}] represents P(S(l) = {i, j}|S(l\u22121) = {s, t}). The row-wise sparse pattern suggests that the router decision becomes almost deterministic given the previous layer's decision. (b) Mutual information I(S(1); S(1\u22121)), which indicates the learned knowledge shared by two neighboring layers is high. (c) Overview figure of router tuning and router distillation loss.", "description": "This figure shows the empirical evidence of the redundancy of layer-wise routers in Mixture-of-Experts models.  Panel (a) visualizes the transition matrix between the expert selections of consecutive layers, demonstrating a strong correlation and sparsity. Panel (b) shows that the mutual information between consecutive layers' expert selections is high, further supporting the correlation. Panel (c) provides an overview of the process to fine-tune the router using a distillation loss technique.", "section": "Redundancy of Layer-wise Router"}, {"figure_path": "i8JaxY7tDI/figures/figures_5_1.jpg", "caption": "Figure 3: Challenges of MoE serving in current serving systems and Read-ME's batching pipeline.", "description": "This figure illustrates the challenges of effective batching in traditional layer-wise MoE models versus the Read-ME approach.  In traditional MoEs, each token in a batch might activate a different set of experts in each layer, leading to inefficient batching.  Read-ME, with its pre-gating mechanism, solves this by pre-computing expert assignments for all tokens, enabling expert-aware batching and improved efficiency.  The left panel shows the process of forming batches in a traditional layer-wise MoE, demonstrating how varying expert activation leads to difficulties in combining requests efficiently. The center panel shows how the average latency increases linearly with the average number of unique expert IDs per batch. The right panel visually represents how Read-ME's pre-gating approach allows for the creation of more efficient batches by pre-selecting experts and subsequently batching tokens with similar expert requirements together.", "section": "4.2 Expert-aware Batching"}, {"figure_path": "i8JaxY7tDI/figures/figures_7_1.jpg", "caption": "Figure 5: Latency evaluation and Temporal locality analysis. (Left) Single inference latency measured on a 124 token generation task. (Center) Latency distribution measured on synthetic workload replaying Chatbot Arena Dataset [29] (\u00a7 5.1). (Right) Temporal distance measured on Arxiv dataset [20], and a subset of Redpajama [35].", "description": "This figure presents a tripartite analysis of latency and temporal locality. The left panel shows a breakdown of single-inference latency for a 124-token generation task, comparing three models: OpenMoE, Llama-2-7b, and Read-ME.  The center panel displays latency distributions obtained from a synthetic workload simulating the Chatbot Arena Dataset, highlighting the 95th percentile latency (p95) for Read-ME, compared against decoding-prioritized and prefill-prioritized baselines. Finally, the right panel illustrates temporal locality analysis using Arxiv and RedPajama datasets, contrasting the temporal distances between tokens in Read-ME and Mixtral-8x7B.", "section": "Inference Latency Breakdown"}, {"figure_path": "i8JaxY7tDI/figures/figures_7_2.jpg", "caption": "Figure 4: Evaluation of Read-ME on MMLU [16] benchmark, compared to other open-source models and compression techniques (performance numbers are collected from their respective papers).", "description": "This figure compares the performance of Read-ME against other open-source models and compression techniques on the MMLU benchmark. The x-axis represents the number of activated parameters (in billions), while the y-axis shows the MMLU performance.  Read-ME is highlighted with a star, demonstrating its superior performance compared to other models with similar or even higher parameter counts.", "section": "5.2 Downstream Task Evaluations"}, {"figure_path": "i8JaxY7tDI/figures/figures_8_1.jpg", "caption": "Figure 6: Latency impact of prefetching: We measured end-to-end latency on a synthetic workload generated by replaying Chatbot Arena Dataset [29]. (Appendix 5.1)", "description": "This figure compares the end-to-end latency of requests with and without prefetching for varying expert cache capacities.  Prefetching, enabled by the Read-ME framework's pre-gating mechanism, consistently outperforms on-demand loading, demonstrating significant latency reduction, especially at lower cache capacities. This highlights the effectiveness of Read-ME's proactive expert loading strategy for efficient inference.", "section": "4.1 Pre-gating Optimized Expert Prefetching and Caching"}, {"figure_path": "i8JaxY7tDI/figures/figures_14_1.jpg", "caption": "Figure 7: Visualization on training dynamics.", "description": "This figure shows the training loss during the first four rounds of a total of eight rounds.  The gray shaded regions represent the router tuning stages, while the orange regions are the expert tuning stages. The validation loss decreases during both router and expert tuning; however, the loss reduction during router tuning plateaus after two rounds, while it continues to decrease during expert tuning.", "section": "A.1 Training Dynamics"}]