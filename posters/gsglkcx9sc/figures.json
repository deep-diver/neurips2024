[{"figure_path": "gSGLkCX9sc/figures/figures_1_1.jpg", "caption": "Figure 1: Our method consists of three modules. The label encoding provides the semantic text features of the dataset labels. The GNNs learn the unified label embedding space and dataset label mappings based on the textual features and input images. The segmentation network leverages the unified label embedding space to produce segmentation results in the unified label space.", "description": "This figure illustrates the pipeline of the proposed method.  It shows three main modules: 1) Label Encoding: uses a language model to convert dataset labels into textual features. 2) Automated Label Unification: uses Graph Neural Networks (GNNs) to learn a unified label embedding space and dataset label mappings based on the textual features and input images. 3) Segmentation in Unified Label Space: uses a segmentation network to produce segmentation results in the unified label space which are then mapped back to the original dataset label spaces.  The figure also visually depicts the relationships between the different components and the flow of information.", "section": "Proposed Method"}, {"figure_path": "gSGLkCX9sc/figures/figures_2_1.jpg", "caption": "Figure 2: Illustration of our method that training with dataset-specific annotations through label mappings constructed by GNNs. We leverage a unified segmentation head (UniSegHead) to enable simultaneous training on multiple datasets. In the UniSegHead, we compute the matrix product between pixel embedding and augmented unified node features output by the GNNs, resulting in predictions for the unified label space. We finally utilize the label mappings constructed by GNNs to map the unified predictions to dataset-specific prediction for training.", "description": "This figure illustrates the proposed method's architecture, showing how dataset-specific annotations are used to train a unified segmentation model. It highlights the use of a graph neural network (GNN) to learn label mappings and a unified label embedding space, enabling the model to be trained simultaneously on multiple datasets. The process involves encoding input images into pixel embeddings, projecting them into the unified label space, and finally mapping the unified predictions to dataset-specific label spaces for training with dataset-specific annotations.", "section": "3 Proposed Method"}, {"figure_path": "gSGLkCX9sc/figures/figures_6_1.jpg", "caption": "Figure 3: Visual comparisons with Single dataset model on different training datasets.", "description": "This figure displays a visual comparison of segmentation results on three different datasets (BDD, Mapillary, and ADE) using both a single-dataset model and the proposed multi-dataset model ('Our 7ds Model'). It shows that the multi-dataset model produces more consistent and accurate results across various datasets and different scene types by integrating knowledge learned from multiple datasets compared to the single dataset approach.", "section": "4.1 Comparison on Multiple Datasets"}, {"figure_path": "gSGLkCX9sc/figures/figures_8_1.jpg", "caption": "Figure 4: The composition of the training datasets.", "description": "This figure shows a Venn diagram illustrating the overlap between the datasets used to train the domain-general and domain-specific models.  The domain-general model is trained on a larger set of datasets (Cityscapes, Mapillary, BDD, IDD, SUN RGBD, ADE20K, COCO) representing a broader range of visual scenes and object categories. The domain-specific model, in contrast, focuses solely on a subset of these datasets, predominantly those featuring driving scenes (Cityscapes, Mapillary, BDD, and IDD).  The Venn diagram visually depicts the unique and shared datasets between the two training approaches, showcasing the differences in data coverage and potential implications for model generalization.", "section": "4.4 Exploring the Impact of Training Datasets"}, {"figure_path": "gSGLkCX9sc/figures/figures_9_1.jpg", "caption": "Figure 5: Comparison of unified label space learned by GNNs with constructed by text features.", "description": "This figure compares the unified label space learned by the proposed Graph Neural Network (GNN) method with a label space constructed solely from text features.  The top row shows images from the IDD and Mapillary datasets, highlighting the 'curb' and 'barrier' classes.  The ground truth and the model's predictions are displayed.  The bottom row presents a similar comparison, this time involving the IDD, Mapillary, and ADE datasets, focusing on the 'tunnel or bridge', 'bridge', 'tunnel', and 'fireplace' classes.  The accompanying diagrams illustrate how the GNN method effectively learns and merges labels with similar visual appearances, even when their textual descriptions differ, leading to a more accurate and concise unified label space.", "section": "Qualitative Results"}, {"figure_path": "gSGLkCX9sc/figures/figures_18_1.jpg", "caption": "Figure 6: Visual comparisons of different training dataset models.", "description": "This figure provides a visual comparison of the segmentation results obtained using different training strategies on various datasets. The top row shows the input images, while the subsequent rows show the ground truth segmentation masks, results from the domain-general model, and results from the domain-specific model.  The figure visually demonstrates the differences in performance between the models trained on a broader set of datasets (general model) and models focused on a specific domain (specific model), highlighting the impact of data diversity on segmentation accuracy.", "section": "4.4 Exploring the Impact of Training Datasets"}, {"figure_path": "gSGLkCX9sc/figures/figures_19_1.jpg", "caption": "Figure 7: Visual comparisons on training datasets.", "description": "This figure shows a visual comparison of the ground truth segmentation masks and the predictions made by the proposed method and several single-dataset trained models. The figure demonstrates that the method achieves a strong performance across all training datasets by integrating label spaces from different datasets. For example, it can predict lane marking and crosswalk for ADE and BDD datasets, and books for the SUN dataset.", "section": "F Visualization"}, {"figure_path": "gSGLkCX9sc/figures/figures_20_1.jpg", "caption": "Figure 3: Visual comparisons with Single dataset model on different training datasets.", "description": "This figure shows a visual comparison of segmentation results on different datasets using both a single-dataset model and the proposed multi-dataset model.  Each column represents a different dataset (BDD, Mapillary, ADE, and the proposed model). The rows represent (top to bottom) the input image, the ground truth segmentation masks, the output from a model trained only on the BDD dataset, the output from a model trained only on the Mapillary dataset, the output from a model trained only on the ADE dataset, and finally the output from the proposed model trained on multiple datasets.  The comparison visually demonstrates the effectiveness of the proposed method in generating consistent and accurate segmentation results across different datasets.", "section": "4.1 Comparison on Multiple Datasets"}, {"figure_path": "gSGLkCX9sc/figures/figures_20_2.jpg", "caption": "Figure 9: Visual comparisons on WildDash 2 benchmark.", "description": "This figure shows a qualitative comparison of the semantic segmentation results on the WildDash 2 benchmark dataset. It compares the ground truth (GT) segmentations with the predictions from several different models, including the authors' trained model, their unseen model, and other state-of-the-art models like Uni NLL+, FAN, and MIX6D.  The comparison highlights the robustness and generalization capabilities of the authors' approach, particularly in challenging real-world scenarios presented in the WildDash 2 dataset.", "section": "4.2 Results on WildDash 2 Benchmark"}]