[{"figure_path": "gSGLkCX9sc/tables/tables_4_1.jpg", "caption": "Table 1: Training and test datasets in our experiments.", "description": "This table presents the datasets used in the experiments described in the paper.  It shows a breakdown of the datasets used for training and validation, separated by domain (driving scene, indoor scene, everyday objects).  For each domain, the table lists the specific datasets used and their corresponding reference numbers.  It also lists the unseen test datasets used for evaluating the generalization performance of the proposed model.", "section": "4 Experiments"}, {"figure_path": "gSGLkCX9sc/tables/tables_5_1.jpg", "caption": "Table 2: Multi-dataset performance compared with other methods.", "description": "This table compares the performance of the proposed method against other state-of-the-art multi-dataset semantic segmentation methods.  It shows the mean Intersection over Union (mIoU) achieved on seven datasets (Cityscapes, Mapillary, BDD, IDD, ADE20K, COCO) using different backbones and label space construction approaches.  The methods are categorized by how they construct the unified label space: Manually Construct, Manually Relabel, Dataset-specific, Automatically Construct.  The table highlights the superior performance of the proposed method, particularly in its ability to automatically construct a unified label space.", "section": "4.1 Comparison on Multiple Datasets"}, {"figure_path": "gSGLkCX9sc/tables/tables_5_2.jpg", "caption": "Table 3: Performance comparison with two baselines on training and unseen datasets.", "description": "This table compares the performance of the proposed method with two baselines: a single dataset model and a multi-SegHead model, across both training and unseen datasets.  The table shows the mean results (mIoU) for each dataset (CS, MPL, SUN, BDD, IDD, ADE, COCO) for both training and unseen datasets.  It highlights that the proposed method significantly outperforms the baselines, especially in the unseen datasets, demonstrating better generalization ability.", "section": "4 Experiments"}, {"figure_path": "gSGLkCX9sc/tables/tables_7_1.jpg", "caption": "Table 4: Performance comparison on WildDash 2 benchmark.", "description": "This table presents a comparison of the proposed method's performance on the WildDash 2 benchmark against other state-of-the-art methods.  It shows the mean Intersection over Union (mIoU) for different categories (classic and negative) and splits the results based on whether the model was trained on the WildDash 2 dataset or not. The \"Meta Avg\" column indicates the overall performance across all classes, weighted by their frequency in the benchmark dataset.", "section": "4.2 Results on WildDash 2 Benchmark"}, {"figure_path": "gSGLkCX9sc/tables/tables_7_2.jpg", "caption": "Table 5: Comparison of Different Methods of Construct Label Spaces.", "description": "This table compares various methods for constructing unified label spaces across multiple datasets in semantic segmentation. It shows the mean Intersection over Union (mIoU) achieved by different methods on eight datasets (Cityscapes, Mapillary, SUN RGB-D, Berkeley Deep Drive, IDD, ADE20K, COCO, and Pascal VOC). The methods include: direct concatenation of label spaces, clustering labels based on text features, a method without graph neural network (GNN) training, a method without GPT label descriptions, and the proposed method.  The table demonstrates the superiority of the proposed method in building a unified label space that improves the performance of semantic segmentation across multiple datasets. The \"\"\"L\"\"\" column represents the number of unified labels generated by each method.", "section": "4.3 Ablation Study"}, {"figure_path": "gSGLkCX9sc/tables/tables_16_1.jpg", "caption": "Table 7: Semantic segmentation accuracy (mIoU) on training datasets compared with Single dataset model.", "description": "This table presents the mean Intersection over Union (mIoU) achieved by training a semantic segmentation model on each of seven datasets individually (single dataset) and simultaneously using the proposed method.  The diagonal elements represent the mIoU when the training and test sets are the same dataset. The off-diagonal values show the model's performance when trained on one dataset and tested on a different one. This helps quantify the impact of dataset-specific characteristics and label conflicts on model performance, highlighting the effectiveness of the proposed multi-dataset training approach.", "section": "4.1 Comparison on Multiple Datasets"}, {"figure_path": "gSGLkCX9sc/tables/tables_16_2.jpg", "caption": "Table 8: Semantic segmentation accuracy (mIoU) on training datasets compared with Multi-SegHead.", "description": "This table presents the mean Intersection over Union (mIoU) scores achieved by different methods on seven training datasets (Cityscapes, Mapillary, SUN RGBD, BDD100K, IDD, ADE20K, and COCO).  The methods compared are: Multi-SegHead (using dataset-specific segmentation heads), and the proposed method (using a unified label space).  The table shows the mIoU for each dataset and the mean mIoU across all datasets, highlighting the superior performance of the proposed method.", "section": "4.1 Comparison on Multiple Datasets"}, {"figure_path": "gSGLkCX9sc/tables/tables_16_3.jpg", "caption": "Table 9: Semantic segmentation accuracy (mIoU) on unseen datasets compared with Single dataset.", "description": "This table presents a comparison of semantic segmentation accuracy (measured by mean Intersection over Union, or mIoU) on five unseen datasets (KITTI, ScanNet, CamVid, Pascal VOC, and Pascal Context) between a model trained on a single dataset and the proposed model (Ours).  It highlights the generalization capability of the proposed approach by demonstrating its performance on datasets not included in its training.", "section": "4.1 Comparison on Multiple Datasets"}, {"figure_path": "gSGLkCX9sc/tables/tables_16_4.jpg", "caption": "Table 9: Semantic segmentation accuracy (mIoU) on unseen datasets compared with Single dataset.", "description": "This table compares the performance of semantic segmentation models trained on different datasets against models trained only on a single dataset. It provides a measure (mIoU) of how accurately the models segment images into different classes on unseen datasets. This comparison helps in understanding the generalization capabilities of models trained on multiple datasets. The results show improved performance when training on multiple datasets.", "section": "4.1 Comparison on Multiple Datasets"}, {"figure_path": "gSGLkCX9sc/tables/tables_17_1.jpg", "caption": "Table 11: Performance on both trained datasets.", "description": "This table presents the performance comparison between the domain-general and domain-specific models on the four driving scene datasets (Cityscapes, Mapillary, Berkeley Deep Drive, and Intelligent Driving Dataset) that both models were trained on. The domain-specific model shows superior performance, suggesting that focusing on a particular domain leads to better performance on trained datasets within that domain. The domain-general model, while not lagging significantly, demonstrates a lower performance compared to the domain-specific model.", "section": "4.4 Exploring the Impact of Training Datasets"}, {"figure_path": "gSGLkCX9sc/tables/tables_17_2.jpg", "caption": "Table 12: Unseen domain-general model vs. Trained domain-specific model.", "description": "This table presents the performance comparison between the unseen domain-general model and the trained domain-specific model. The domain-specific model is trained on driving scene datasets while the domain-general model is trained on a broader range of datasets. The results show that the domain-specific model significantly outperforms the domain-general model on the driving scene dataset (WildDash2). This highlights that the domain-specific model excels at learning features specific to the target scene while the domain-general model shows better generalization capability to the unseen dataset.", "section": "4.4 Exploring the Impact of Training Datasets"}, {"figure_path": "gSGLkCX9sc/tables/tables_17_3.jpg", "caption": "Table 13: Trained domain-general model vs. Unseen domain-specific model.", "description": "This table compares the performance of a domain-general model (trained on multiple datasets) against a domain-specific model (trained only on driving scene datasets) when tested on non-driving scene datasets (SUN, ADE, COCO).  The results show a significant performance gap, highlighting the generalization capability of the domain-general model.", "section": "4.4 Exploring the Impact of Training Datasets"}, {"figure_path": "gSGLkCX9sc/tables/tables_17_4.jpg", "caption": "Table 6: Performance on unseen dataset.", "description": "This table presents the performance comparison between the domain-general and domain-specific models on unseen datasets. The domain-general model shows better generalization performance on non-driving datasets, while the domain-specific model performs slightly better on driving scene datasets.  It highlights the trade-off between specializing for one type of scene and generalizing across many.", "section": "4.4 Exploring the Impact of Training Datasets"}]