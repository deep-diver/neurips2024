[{"type": "text", "text": "Only Strict Saddles in the Energy Landscape of Predictive Coding Networks? ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Francesco Innocenti School of Engineering and Informatics University of Sussex F.Innocenti@sussex.ac.uk ", "page_idx": 0}, {"type": "text", "text": "El Mehdi Achour RWTH Aachen University Aachen, Germany achour@mathc.rwth-aachen.de ", "page_idx": 0}, {"type": "text", "text": "Ryan Singh School of Engineering and Informatics University of Sussex rs773@sussex.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Christopher L. Buckley School of Engineering and Informatics University of Sussex VERSES c.l.buckley@sussex.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Predictive coding (PC) is an energy-based learning algorithm that performs iterative inference over network activities before updating weights. Recent work suggests that PC can converge in fewer learning steps than backpropagation thanks to its inference procedure. However, these advantages are not always observed, and the impact of PC inference on learning is not theoretically well understood. Here, we study the geometry of the PC energy landscape at the inference equilibrium of the network activities. For deep linear networks, we first show that the equilibrated energy is simply a rescaled mean squared error loss with a weight-dependent rescaling. We then prove that many highly degenerate (non-strict) saddles of the loss including the origin become much easier to escape (strict) in the equilibrated energy. Our theory is validated by experiments on both linear and non-linear networks. Based on these and other results, we conjecture that all the saddles of the equilibrated energy are strict. Overall, this work suggests that PC inference makes the loss landscape more benign and robust to vanishing gradients, while also highlighting the fundamental challenge of scaling PC to deeper models. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Originating as a general principle of brain function, predictive coding (PC) has in recent years been developed into a local learning algorithm that could provide a biologically plausible alternative to backpropagation (BP) [32, 31, 43]. Deep neural networks (DNNs) trained with PC have shown comparable performance to BP on standard small-to-medium machine learning tasks, including classification, generation and memory association [31, 43, 41]. PC networks (PCNs) are also highly versatile, allowing for arbitrary computational graphs [45, 10], hybrid and causal inference [44, 59], and temporal prediction [35]. ", "page_idx": 0}, {"type": "text", "text": "In contrast to BP, and similar to other energy-based algorithms [e.g. 49, 38], PC performs iterative (approximately Bayesian) inference over network activities before weight updates. This has been recently described as a fundamentally different principle of credit assignment for learning in the brain called \u201cprospective configuration\u201d [54], where weights follow activities (rather than the other way around). While the inference process key to PC incurs an additional computational cost, it has been suggested to provide many benefits for learning including faster convergence [54, 3, 18]. However, these speed-ups are not consistently observed across datasets, models and optimisers [3], and the impact of PC inference on learning more generally is not theoretically well understood (see $\\S\\mathrm{A}.2.1\\rangle$ ). ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To address this gap, we study the geometry of the effective landscape on which PC learns: the weight landscape at the inference equilibrium of the network activities (defined in $\\S2.2\\rangle$ ). Our theory considers deep linear networks (DLNs), the standard model for theoretical studies of the loss landscape (see $\\S\\mathrm{A}.2)$ . Despite being able to learn only linear representations, DLNs have non-convex loss landscapes with non-linear learning dynamics that have proved to be a useful model for understanding non-linear networks [e.g. 48]. In contrast to previous theories of PC [3, 2, 18], we do not make any additional assumptions or approximations (see $\\S\\mathrm{A}.2)$ , and we empirically verify that our linear theory holds for non-linear networks. ", "page_idx": 1}, {"type": "text", "text": "For DLNs, we first show that, at the inference equilibrium, the PC energy is simply a rescaled mean squared error (MSE) loss with a non-trivial, weight-dependent rescaling (Theorem 1). We then compare saddle points of the loss, which have been recently characterised [23, 1], to those of the equilibrated energy. Such saddles, which are ubiquitous in the loss landscape of neural networks [11, 1], can be of two main types: \u201cstrict\u201d, where the Hessian is indefinite (Def. 1); and \u201cnon-strict\u201d, where an escape direction is found in higher-order derivatives [15, 23, 1]. Non-strict saddles are particularly problematic for first-order methods like (stochastic) gradient descent (SGD) since they are by definition at least second-order critical points. While SGD can be exponentially slowed in the vicinity of strict saddles [12], it can effectively get stuck in non-strict ones [47, 7] (see $\\S\\mathrm{A}.2$ for a review). This is the phenomenon of vanishing gradients viewed from a landscape perspective [39, 6]. ", "page_idx": 1}, {"type": "text", "text": "By contrast, here we prove that many non-strict saddles of the MSE loss, specifically saddles of rank zero, become strict in the equilibrated energy of any DLN (Theorems 2 & 3). These saddles include the origin, whose degeneracy (i.e. flatness) in the loss grows with the number of hidden layers. Our theoretical results are strongly validated by experiments on both linear and non-linear networks, and additional experiments suggest that other (higher-rank) non-strict saddles of the loss are strict in the equilibrated energy. Based on these results, we conjecture that all the saddles of the equilibrated energy are strict. Overall, this work suggests that PC inference makes the loss landscape more benign and robust to vanishing gradients, while also highlighting the fundamental challenge of speeding up PC inference on deeper networks. ", "page_idx": 1}, {"type": "text", "text": "The rest of the paper is structured as follows. After introducing the setup (\u00a72), we present our theoretical results for DLNs (\u00a73), including some illustrative examples and thorough empirical verifications of each result. We then report experiments on non-linear networks supporting our theory and more general conjecture (\u00a74). We conclude by discussing the implications and limitations of our work, as well as potential future directions (\u00a75). Appendix A includes a review of related work, derivations, experiment details and supplementary results. Code to reproduce all the experiments is available at https://github.com/francesco-innocenti/pc-saddles. ", "page_idx": 1}, {"type": "text", "text": "1.1 Summary of contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u2022 We derive an exact solution for the PC energy of DLNs at the inference equilibrium (Theorem 1), which turns out to be a rescaled MSE loss with a weight-dependent rescaling. This corrects a previous mistake in the literature that the MSE loss is equal to the output energy [34] (which holds only at the feedforward pass) and enables further studies of the PC energy landscape. We find an excellent match between our theory and experiment (Figure 1). \u2022 Based on this result, we prove that, in contrast to the MSE loss, the origin of the equilibrated energy of DLNs is a strict saddle independent of network depth. We provide an explicit characterisation of the Hessian at the origin of the equilibrated energy (Theorem 2), which is perfectly validated by experiments on linear networks (Figures 3, 4 & 8). \u2022 We further prove that other non-strict saddles of the MSE loss than the origin, specifically saddles of rank zero, become strict in the equilibrated energy of DLNs (Theorem 3). We provide an empirical verification of one of these saddles as an example (Figures 9 & 10). \u2022 We empirically show that our linear theory holds for non-linear networks, including convolutional architectures, trained on standard image classification tasks. In particular, when initialised close to non-strict saddles of the MSE loss covered by Theorem 3, we find that SGD on the equilibrated energy escapes much faster than on the loss given the same learning rate (Figures 5 & 12). In contrast to BP, PC exhibits no vanishing gradients (Figure 11). ", "page_idx": 1}, {"type": "text", "text": "\u2022 We perform additional experiments, again on both linear and non-linear networks, showing that PC quickly escapes other (higher-rank) non-strict saddles of the loss that we do not address theoretically (Figure 6), supporting our conjecture that all the saddles of the equilibrated energy are strict. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation. We use the following shorthand $\\mathbf{W}_{k:\\ell}=\\mathbf{W}_{k}\\dots\\mathbf{W}_{\\ell}$ for $\\ell,k\\in{1,\\dots,L}$ , denoting the total product of weight matrices as $\\mathbf{W}_{L:1}=\\mathbf{W}_{L}\\ldots\\mathbf{W}_{1}$ . ${\\mathbf{I}}_{n}$ is the $n\\times n$ identity matrix, while ${\\bf0}_{n}$ denotes either the $n$ -zero vector or the $n\\times n$ null matrix, and $n$ will be omitted when clear from context. $||\\cdot||$ denotes the $\\ell_{2}$ norm, and $\\otimes$ is the Kronecker product between two matrices. We will consider the gradient and Hessian of an objective $f$ only with respect to the network weights $\\pmb{\\theta}$ and sometimes abbreviate them as $\\mathbf{g}_{f}:=\\nabla_{\\theta}f$ and $\\mathbf{H}_{f}:=\\dot{\\nabla}_{\\theta}^{2}f$ , respectively, omitting the independent variable for simplicity. The largest and smallest eigenvalues of the Hessian are $\\lambda_{\\operatorname*{max}}(\\bar{\\mathbf{H}}_{f})$ and $\\lambda_{\\operatorname*{min}}(\\mathbf{H}_{f})$ , with $\\hat{\\mathbf{v}}_{\\mathrm{max}}$ and $\\hat{\\mathbf{v}}_{\\operatorname*{min}}$ as their associated eigenvectors. See $\\S\\mathrm{A.l}$ for more general notation. ", "page_idx": 2}, {"type": "text", "text": "Definition 1. Strict saddle. Following [15] and later work, any critical point $\\pmb{\\theta}^{*}$ of $f(\\pmb\\theta)$ where $\\mathbf{g}_{f}(\\pmb{\\theta}^{*})=\\mathbf{0}$ is defined as a strict saddle when the Hessian at that point has at least one negative eigenvalue, $\\lambda_{\\operatorname*{min}}(\\mathbf{H}_{f}(\\pmb{\\theta}^{*}))<0$ . Any other critical point with a positive semi-definite Hessian and at least one negative eigenvalue in a higher-order derivative is said to be a non-strict saddle. ", "page_idx": 2}, {"type": "text", "text": "2.1 Deep Linear Networks (DLNs) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider DLNs with one or more hidden layers $H=L-1\\geq1$ defining the linear mapping $\\mathbf{W}_{L:1}\\,:\\,\\mathbb{R}^{d_{x}}\\,\\rightarrow\\,\\mathbb{R}^{d_{y}}$ where $\\mathbf{W_{\\ell}}\\;\\in\\;\\mathbb{R}^{n_{\\ell}\\,\\times\\,n_{\\ell-1}}$ , with layer widths $\\{n_{\\ell}\\}_{\\ell=0}^{L}$ and input and output dimensions $n_{0}=d_{x},n_{L}=d_{y}$ . We ignore biases for simplicity. The standard MSE loss for DLNs can then be written as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\frac{1}{2N}\\sum_{i=1}^{N}||\\mathbf{y}_{i}-\\mathbf{W}_{L:1}\\mathbf{x}_{i}||^{2}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "for a dataset of $N$ examples $\\{(\\mathbf{x}_{i},\\mathbf{y}_{i})\\}_{i=1}^{N}$ where $\\mathbf{x}\\in\\mathbb{R}^{d_{x}}$ , $\\mathbf{y}\\in\\mathbb{R}^{d_{y}}$ . The total number of weights is given by $\\begin{array}{r}{p\\,=\\,\\sum_{\\ell=1}^{L}n_{\\ell}n_{\\ell-1}}\\end{array}$ , and we will denote the set of all network parameters as $\\pmb\\theta\\ :=$ $(\\mathbf{W}_{1},\\ldots,\\mathbf{W}_{L})\\in\\mathbb{R}^{\\bar{p}}$ . For brevity, we will often refer to the MSE loss as simply the loss. ", "page_idx": 2}, {"type": "text", "text": "2.2 Predictive coding (PC) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "DNNs trained with PC typically assume a hierarchical Gaussian model with identity covariances, so we will adopt this formulation for linear fully connected layers $\\mathbf z_{\\ell}\\sim\\mathcal{N}(\\mathbf W_{\\ell}\\mathbf z_{\\ell-1},\\mathbf I_{\\ell})$ where the mean activity of each layer $\\mathbf{z}_{\\ell}$ is a linear function of the previous layer. Under some further common assumptions about the generative model, we can derive an energy function, often referred to as the variational free energy, that is a sum of squared prediction errors across layers [9]. ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{F}=\\frac{1}{2N}\\sum_{i=1}^{N}\\sum_{\\ell=1}^{L}||\\mathbf{z}_{\\ell,i}-\\mathbf{W}_{\\ell}\\mathbf{z}_{\\ell-1,i}||^{2}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Note that this objective defines an energy for every neuron, highlighting the locality of the algorithm. To train a PCN, the last layer is clamped to some data, $\\mathbf{z}_{L,i}\\ :=\\ \\mathbf{y}_{i}$ , which could be a label for classification or an image for generation. In a supervised task, the first layer is also fixed to some input, $\\mathbf{z}_{0,i}\\;:=\\;\\mathbf{x}_{i}$ . The energy (Eq. 2) is then minimised in two phases, first w.r.t. the activities (inference) and then w.r.t. the weights (learning) ", "page_idx": 2}, {"type": "equation", "text": "$$\nI n f e r e n c e:\\quad\\Delta\\mathbf{z}_{\\ell}\\propto-\\frac{\\partial\\mathcal{F}}{\\partial\\mathbf{z}_{\\ell}}\\qquad\\qquad(3)\\qquad\\qquad L e a r n i n g:\\quad\\Delta\\mathbf{W}_{\\ell}\\propto-\\frac{\\partial\\mathcal{F}}{\\partial\\mathbf{W}_{\\ell}}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where we omit the data index $i$ for simplicity. In practice, the inference dynamics (Eq. 3) are often run to convergence until $\\Delta\\mathbf{z}_{\\ell}\\approx0$ , before performing a weight (e.g. GD) update (Eq. 4). Importantly, the effective weight landscape on which we perform learning is therefore the energy at the inference equilibrium $\\mathcal{F}|_{\\Delta\\mathbf{z}\\approx\\mathbf{0}}(\\theta)$ , which we will refer to as the equilibrated energy or sometimes simply the energy. ", "page_idx": 2}, {"type": "image", "img_path": "eTu6kvrkSq/tmp/874357d6588dea9f13d68e9bc7d7efd70693d4cb9c8ed2c5048efe6f4795f849.jpg", "img_caption": ["Figure 1: Empirical verification of the theoretical equilibrated energy of deep linear networks (Theorem 1). For different datasets, we plot the energy (Eq. 2) at the numerical inference equilibrium $\\mathcal{F}|_{\\partial\\mathcal{F}/\\partial\\mathbf{z}\\approx0}$ for DLNs with different number of hidden layers $H\\in\\{2,5,10\\}$ (see $\\S\\mathrm{A}.4$ for more details), observing an excellent match with the theoretical prediction (Eq. 5). "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3 Theoretical results ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Equilibrated energy as rescaled MSE ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As explained in $\\S2.2$ , the weights of a PCN are typically updated once the activities have converged to an equilibrium. The equilibrated energy $\\mathcal{F}|_{\\partial\\mathcal{F}/\\partial\\mathbf{z}=0}(\\pmb{\\theta})$ , which we will abbreviate as ${\\mathcal{F}}^{*}(\\pmb{\\theta})$ , is therefore the effective learning landscape navigated by PC and the object we are interested in studying. It turns out that we can derive a closed-form solution for the equilibrated energy of DLNs, which will be the basis of our subsequent results. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1 (Equilibrated energy of DLNs). For any DLN parameterised by $\\pmb\\theta\\ \\ :=$ $(\\mathbf{W}_{1},\\ldots,\\mathbf{W}_{L})$ with input and output $\\left(\\mathbf{x}_{i},\\mathbf{y}_{i}\\right)$ , the $P C$ energy (Eq. 2) at the exact inference equilibrium $\\partial\\mathcal{F}/\\partial\\mathbf{z}=\\mathbf{0}$ is the following rescaled MSE loss (see \u00a7A.3.2 for derivation) ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{F}^{*}=\\frac{1}{2N}\\sum_{i=1}^{N}(\\mathbf{y}_{i}-\\mathbf{W}_{L:1}\\mathbf{x}_{i})^{T}\\mathbf{S}^{-1}(\\mathbf{y}_{i}-\\mathbf{W}_{L:1}\\mathbf{x}_{i})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the rescaling is $\\begin{array}{r}{\\mathbf{S}=\\mathbf{I}_{d_{y}}+\\sum_{\\ell=2}^{L}(\\mathbf{W}_{L:\\ell})(\\mathbf{W}_{L:\\ell})^{T}}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "The proof relies on unfolding the hierarchical Gaussian model assumed by PC to work out the full, implicit generative model of the output, and the rescaling S comes from the variance modelled by PC at each layer (see $\\S\\mathrm{A}.3.2$ for details). Figure 1 shows an excellent empirical validation of the theory. ", "page_idx": 3}, {"type": "text", "text": "Intuitively, the PC inference process (Eq. 3) can then be thought of as reshaping the (MSE) loss landscape to take some layer-wise, weight-dependent variance into account. This immediately raises the question: how does the equilibrated energy landscape $\\mathcal{F}^{*}(\\pmb{\\theta})$ differ from the loss landscape $\\mathcal{L}(\\pmb{\\theta})?$ Is the rescaling\u2014and so the layer variance modelled by PC\u2014useful for learning? Below we provide a partial positive answer to this question by comparing the saddle point geometry of the two objectives. ", "page_idx": 3}, {"type": "text", "text": "3.2 Analysis of the origin saddle ${\\boldsymbol{\\theta}}=\\mathbf{0}$ ) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Here we prove that, in contrast to the MSE loss, the origin of the equilibrated energy (Eq. 5, where all the weights are zero, ${\\boldsymbol\\theta}={\\bf0}$ ) is a strict saddle (Def. 1) for DLNs of any depth. To do so, we derive an explicit expression for the Hessian at the origin of the equilibrated energy. For intuitive ", "page_idx": 3}, {"type": "image", "img_path": "eTu6kvrkSq/tmp/8e730b1863f17b92547d9f6a96788a9b57bff3e7ba974e805cb4e7318ddbe448.jpg", "img_caption": ["Figure 2: Toy examples illustrating the (Theorem 2) result that the saddle at the origin of the equilibrated energy is strict independent of network depth. We plot the MSE loss $\\mathcal{L}(\\pmb\\theta)$ (top) and equilibrated energy landscape $\\mathcal{F}^{*}(\\pmb{\\theta})$ (middle) around the origin for 3 linear networks trained with SGD on a toy problem (see $\\S\\mathrm{A}.4$ for details). We also show the training losses for a representative run with initialisation close to the origin (bottom). For the one-dimensional networks, we visualise the landscape around the origin as well as the SGD updates. For the wide network, we project the landscape onto the maximum and minimum eigenvectors of the Hessian, following [7]. Note that in this case the loss is flat because the Hessian at the origin is zero for $H>1$ (Eq. 6). "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "comparison, we first briefly recall the known results that, at the origin, the loss Hessian is indefinite for one-hidden-layer networks and zero for any deeper network (see $\\S\\mathrm{A}.3.1$ for a derivation) ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{H}_{\\mathcal{L}}(\\pmb{\\theta}=\\mathbf{0})=\\left\\{\\left[\\begin{array}{c c}{\\mathbf{0}}&{-\\widetilde{\\Sigma}_{\\mathbf{xy}}\\otimes\\mathbf{I}_{n_{1}}}\\\\ {-\\mathbf{I}_{n_{1}}\\otimes\\widetilde{\\Sigma}_{\\mathbf{yx}}}&{\\mathbf{0}}\\end{array}\\right],\\quad H=1\\quad\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where following previous works $\\begin{array}{r}{\\widetilde{\\Sigma}_{\\mathbf{x}\\mathbf{y}}\\;:=\\;\\frac{1}{N}\\,\\sum_{i}^{N}\\mathbf{x}_{i}\\mathbf{y}_{i}^{T}}\\end{array}$ is the empirical input-output covariance. One-hidden-layer networks $H=1$ are a special case where the origin saddle of the loss is strict (Def. 1) and was studied in detail by [48] (see left panel of Figure 2 for an example). For deeper networks $H>1$ , the saddle is non-strict as first shown by [23]. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{{\\lambda_{\\mathrm{min}}(\\mathbf{H}_{\\mathcal{L}}(\\pmb{\\theta}=\\mathbf{0}))<0,\\quad H=1}}&{{[\\mathrm{strict~saddle}]}}\\\\ {{}}&{{}}\\\\ {{\\lambda_{\\mathrm{min}}(\\mathbf{H}_{\\mathcal{L}}(\\pmb{\\theta}=\\mathbf{0}))=0,\\quad H>1}}&{{[\\mathrm{non-strict~saddle}]}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "More specifically, the origin saddle of the loss is of order $H^{1}$ , becoming increasingly degenerate (flat) and harder to escape with depth, especially for first-order methods like SGD (see middle and right panels of Figure 2). ", "page_idx": 4}, {"type": "text", "text": "By contrast, now we show that the origin saddle of the equilibrated energy is strict for DLNs of any number of hidden layers. Figure 2 shows a few toy examples illustrating the result. In brief, we observe that, when initialised close to the origin saddle, SGD takes increasingly more time to escape from the loss than the energy as a function of depth. Now we state the result more formally (for the same learning rate). The Hessian at the origin of the equilibrated energy turns out to be (see $\\S\\mathrm{A}.3.3$ for derivation) ", "page_idx": 4}, {"type": "image", "img_path": "eTu6kvrkSq/tmp/7e1aea5ca2248561b836c52c63faaf7da4a70bfe42a3844787cc811018660322.jpg", "img_caption": ["Figure 3: Empirical verification of the Hessian at the origin of the equilibrated energy for DLNs tested on toy data. We show the Hessian and its eigenspectrum at the origin of the MSE loss (top) and equilibrated energy (middle) for DLNs with Gaussian target $\\mathbf{y}=-\\mathbf{x}$ where $\\mathbf{x}\\sim\\mathcal{N}(1,0.1)$ (see $\\S\\mathrm{A}.4$ for details). Note that purple bars show overlapping loss and energy Hessian eigendensity. In the right panel, we vary one of the output dimensions to be $y_{2}=x_{2}$ . We confirm the strictness of the origin saddle in the equilibrated energy and observe an excellent numerical validation of our theoretical Hessian (Eq. 8). Figure 8 shows the same results for one-dimensional networks, and Figure 4 shows similar results for more realistic datasets. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{H}_{\\mathcal{F}^{*}}(\\theta=\\mathbf{0})=\\left\\{\\begin{array}{l l}{\\left[\\begin{array}{c c}{\\mathbf{0}}&{\\ -\\widetilde{\\sum}_{\\mathbf{x}\\mathbf{y}}\\otimes\\mathbf{I}_{n_{1}}}\\\\ {\\scriptstyle-\\mathbf{I}_{n_{1}}\\otimes\\widetilde{\\sum}_{\\mathbf{y}\\mathbf{x}}}&{\\ -\\widetilde{\\sum}_{\\mathbf{y}\\mathbf{y}}\\otimes I_{n_{L-1}}\\right],}\\\\ {\\ }&{\\ }\\\\ {\\left[\\begin{array}{c c}{\\mathbf{0}}&{\\ \\dots}&{\\ \\mathbf{0}}\\\\ {\\vdots}&{\\ddots}&{\\ \\vdots}\\\\ {\\mathbf{0}}&{\\dots}&{\\ -\\widetilde{\\sum}_{\\mathbf{y}\\mathbf{y}}\\otimes I_{n_{L-1}}\\right],}\\end{array}\\right.\\ \\ \\ H>1}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r}{\\widetilde{\\Sigma}_{\\mathbf{y}\\mathbf{y}}:=\\frac{1}{N}\\sum_{i}^{N}\\mathbf{y}_{i}\\mathbf{y}_{i}^{T}}\\end{array}$ is the empirical output covariance. We see that, in contrast to the loss Hessian (Eq. 6), the energy Hessian has a non-zero last diagonal block given by $\\partial^{2}{\\mathcal{F}}^{*}/\\partial\\mathbf{W}_{L}^{2}$ , for any number of hidden layers $H$ . It is then straightforward to show that the energy Hessian has always negative eigenvalues, since the output covariance is positive definite. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2 (Strictness of origin saddle of the equilibrated energy). The Hessian at the origin of the equilibrated energy (Eq. 5) for any DLN has at least one negative eigenvalue (see $\\S A.3.3$ for proof) ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\lambda_{m i n}({\\bf H}_{\\mathcal{F}^{*}}(\\pmb{\\theta}=\\mathbf{0}))<0,\\quad\\forall H\\ge1\\quad[s t r i c t\\,s a d d l e,\\,D e f.\\,\\,l J}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Figures 3 & 4 show a perfect match between the theoretical (Eq. 8) and numerical Hessian at the origin of the equilibrated energy, which we computed for a range of DLNs on a random batch of toy as well as more realistic datasets. ", "page_idx": 5}, {"type": "image", "img_path": "eTu6kvrkSq/tmp/68b48aa247fc429c7db824bd8f620d1ab2829a73a5d7154ef4794c684d01a28e.jpg", "img_caption": ["Figure 4: Empirical verification of the Hessian eigenspectrum at the origin of the equilibrated energy for DLNs tested on more realistic datasets. This shows similar results to Figure 3 for the more realistic datasets MNIST and MNIST-1D [16] (see $\\S\\mathrm{A}.4$ for details). We again find a perfect match between theory and experiment for DLNs with different number of hidden layers $H\\in\\{1,2,4\\}$ , confirming the strictness of the origin saddle of the equilibrated energy. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Theorem 2 proves that the origin is a strict saddle of the equilibrated energy for DLNs of any depth. This is in stark contrast to the MSE loss where it is only true for one-hidden-layer networks $H=1$ (Eq. 7). The result predicts that, near the origin, (S)GD should escape the saddle faster on the equilibrated energy than on the loss given the same learning rate, and increasingly so as a function of depth. Figure 2 confirms this prediction for some toy linear networks, and Figures 5 & 6 in $\\S4$ clearly show that it holds for non-linear networks as well. ", "page_idx": 6}, {"type": "text", "text": "3.3 Analysis of other saddles ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Is the origin a special case where the equilibrated energy has an easier-to-escape saddle than the loss? Or is this result pointing to something more general? Here we consider a specific type of non-strict saddle of the loss (of which the origin is one) and show that indeed they also become strict in the equilibrated energy. We address other saddle types experimentally in $\\S4$ and leave their theoretical study for future work. ", "page_idx": 6}, {"type": "text", "text": "Specifically, we consider saddles of rank zero, which for the MSE loss can be identified as critical points where the product of weight matrices is zero $\\mathbf{W}_{L:1}=\\mathbf{0}$ [1]. For the equilibrated energy (Eq. 5), we consider the critical points $\\pmb{\\theta}^{*}(\\mathbf{W}_{L}=\\mathbf{0},\\mathbf{W}_{L-1:1}=\\mathbf{0})$ ), since the last weight matrix needs to be null in order for the energy gradient to be zero (see $\\S\\mathrm{A}.3.3$ for an explanation). It turns out that at these critical points there exists a direction of negative curvature. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3 (Strictness of zero-rank saddles of the equilibrated energy). Consider the set of critical points of the equilibrated energy (Eq. 5) $\\pmb{\\theta}^{*}(\\mathbf{W}_{L}\\,=\\,\\mathbf{0},\\bar{\\mathbf{W}}_{L-1:1}\\,=\\,\\mathbf{0})$ where $\\dot{\\mathbf{g}}_{\\mathcal{F}^{*}}(\\pmb{\\theta}^{*})=\\mathbf{0}$ . The Hessian at these points has at least one negative eigenvalue $(s e e\\~\\mathrm{\\SA}.3.6\\$ for proof) ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\exists\\lambda(\\mathbf{H}_{\\mathcal{F}^{*}}(\\pmb{\\theta}^{*}))<0\\quad/s t r i c t\\,s a d d l e s,\\,D e f.\\ I J\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Note that Theorem 2 can now be seen as a corollary of Theorem 3, although for the origin we derived the full Hessian. This result also stands in contrast to the (MSE) loss, where many of the considered critical points (specifically when 3 or more weight matrices are zero) are non-strict saddles as proved by [1]. The prediction is again that, in the vicinity of any of these saddles, PC should escape faster than BP with (S)GD given the same learning rate. For space reasons, the subsequent experiments focus only the origin as an example of a saddle covered by Theorem 3 (and Theorem 2), but $\\S\\mathrm{A}.5$ includes an empirical validation of another (zero-rank) strict saddle of the equilibrated energy (Figures 9, 10 & 12). Our code also makes it relatively easy to test for other saddles. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Here we report experiments on linear and non-linear networks supporting our theoretical results as well as more general conjecture that all the saddles of the equilibrated energy are strict. In all the experiments, we trained networks with BP and PC using (S)GD with the same learning rate, since the goal is to test our theory of the saddle geometry of the equilibrated energy landscape. Code to reproduce all the results is available at https://github.com/francesco-innocenti/ pc-saddles. ", "page_idx": 7}, {"type": "text", "text": "First, we compared the loss training dynamics of linear and non-linear networks, including convolutional architectures, on standard image classification tasks with SGD initialised close to the origin (see $\\S\\mathrm{A}.4$ for details). For computational reasons, we did not run the BP-trained networks to convergence, underscoring the point that the origin saddle of the loss is highly degenerate and particularly hard to escape for first-order methods like SGD. In all cases, we observe that PC escapes the origin saddle substantially faster than BP (Figure 5), and Figure 11 shows that PC exhibits no vanishing gradients. We find practically the same results when initialising close to another non-strict saddle of the loss covered by Theorem 3 (Figure 12). These findings support our theoretical results beyond the linear case. ", "page_idx": 7}, {"type": "image", "img_path": "eTu6kvrkSq/tmp/ca4ccb3ae93dacb73806a7098d978feadf61f0aa775f6a9a777dd661a0594d66.jpg", "img_caption": ["Figure 5: PC escapes the origin saddle much faster than BP with SGD on non-linear networks. We plot the training loss for a representative run of BP and PC for linear and non-linear networks trained on standard image classification tasks (see $\\S\\mathrm{A}.4$ for details). All networks were initialised close to the origin with scale $\\sigma=5e^{-3}$ ), and trained with SGD and learning rate $\\eta=1e^{-3}$ . The networks trained on MNIST and Fashion-MNIST had 5 fully connected layers, while those trained on CIFAR-10 had a convolutional architecture. Figure 11 shows the corresponding weight gradient norms during training. Results were consistent across different random seeds. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "From Figure 5, we also observe a second plateau in the loss dynamics of PCNs, suggesting a saddle of higher rank (presumably rank 1). This is consistent with the saddle-to-saddle dynamics described for DLNs by [19], where for small initialisation GD transitions through a sequence of saddles, each representing a solution of increasing rank. ", "page_idx": 7}, {"type": "text", "text": "To explicitly test for higher-rank, non-strict saddles of the loss that we did not study theoretically, we replicated one of the experiments by [19, cf. Figure 1] on a matrix completion task. In particular, networks were trained to fit a rank-3 matrix, which meant that starting near origin GD visited 3 saddles (of successive rank 0, 1 and 2) before converging to a rank-3 solution as shown in Figure 6. We find that, when initialised near any of the saddles visited by BP, PC escapes quickly and does not show vanishing gradients (Figure 6), supporting the conjecture that all the saddles of the equilibrated energy are strict. ", "page_idx": 7}, {"type": "image", "img_path": "eTu6kvrkSq/tmp/6c55a2fa8eac9a305812c2a298e4877ad01bd037deea71eec38472518ebc2ef5.jpg", "img_caption": ["Figure 6: PC quickly escapes higher-rank saddles visited by BP with GD on a matrix completion task. We plot the training loss (top) and corresponding weight gradient norms of the loss (BP) and equilibrated energy (PC) (bottom) for networks $H=3$ , $n_{\\ell}=100)$ ) trained with GD to fit a random rank-3 matrix as studied by [19]. BP-trained networks were initialised near the origin with scale $\\sigma=5e^{-3}$ , while PCNs were initialised at each saddle visited by BP (see $\\S\\mathrm{A}.4$ for details). Results were consistent across different random seeds. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In summary, we took a first step in characterising the effective landscape on which PC learns\u2014the energy landscape at the inference equilibrium. For DLNs, we first showed that the equilibrated energy is a rescaled MSE loss with a weight-dependent rescaling (Theorem 1). This result corrects a previous mistake in the literature that the MSE loss is equal to the output energy [34] and that the total energy (Eq. 2) can therefore be decomposed into the loss and the other (hidden) energies (a relationship that only holds at the feedforward activity values). As we expand on below, Eq. 5 also enables further studies of the PC learning landscape. ", "page_idx": 8}, {"type": "text", "text": "We then proved that many non-strict saddle points of the MSE loss, specifically zero-rank saddles, become strict in the equilibrated energy of any DLN (Theorems 2 & 3). These saddles include the origin, making PC effectively more robust to vanishing gradients (Figures 6 & 11). We thoroughly validated our theory with experiments on both linear and non-linear architectures, and provided empirical support for the strictness of higher-rank saddles of the equilibrated energy. Based on these results, we conjecture that all the saddles of the equilibrated energy are strict. Overall, the PC inference process can therefore be interpreted as making the loss landscape more benign. ", "page_idx": 8}, {"type": "text", "text": "5.1 Implications ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our work goes significantly beyond existing theories of PC in terms of both explanatory and predictive power. Most previous works make non-standard assumptions or loose approximations that result in non-specific experimental predictions. For example, the interpretation of PC as implicit GD by [3] holds only for small batch sizes and specific layerwise rescalings of the activities and parameter learning rates. ([2] generalised this result to remove the activity rescalings but not the learning rate ones.) By contrast, linearity is the only major assumption made our theory, and we empirically verify that all the results hold for non-linear networks. Similarly, both [2] and [18] make second-order approximations of the energy to argue that PC makes use of Hessian information. However, our results clearly show that PC can leverage much higher-order information, turning highly degenerate, $H$ -order saddles into strict ones. ", "page_idx": 8}, {"type": "text", "text": "Previous theories have also struggled to explain why faster learning convergence with PC is not always observed depending on the task, model and optimiser [3, 54]. Our landscape analysis, while incomplete (more on this below), acknowledges these factors and their interplay, helping to explain inconsistent findings and predict when speed-ups can and cannot be expected. All things being equal, PC should converge faster on deep and narrow networks (though perhaps not too deep as we discuss below), since the distance between the origin saddle and standard initialisations scales with the network width [39]. This likely explains the speed-up reported by [54] on a narrow $\\mathit{n}_{\\ell}=64$ ) ", "page_idx": 8}, {"type": "text", "text": "15-layer fully connected network. However, in practice all things are not equal, and everything from not reaching an inference equilibrium to different datasets, architectures and optimisers all interact to determine convergence. This raises the question of whether minimising the equilibrated energy could be faster than the loss or lead to better performance, which we return to below. ", "page_idx": 9}, {"type": "text", "text": "More broadly, our landscape theory closely relates to the work of [56], who showed that learning in linear physical systems with equilibrium propagation [49, 50] has beneficial effects on the activity (rather than weight) Hessian. Studying these connections\u2014and more generally the benefits of inference for learning in energy-based systems\u2014could be an interesting future direction. ", "page_idx": 9}, {"type": "text", "text": "Our work has also implications for theories of credit assignment in the brain. In particular, our results put the recent principle of prospective configuration [54] for energy-based learning on a more solid theoretical footing, showing that PC inference can indeed facilitate learning by using high-order information. At the same time, they suggest that the claim of universally faster learning convergence with PC may have been overstated [54]. ", "page_idx": 9}, {"type": "text", "text": "5.2 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We conclude by addressing the main limitations of our work. First, the strictness of the energy saddles we studied holds, by derivation, only at the exact inference equilibrium. We note that one does not need to reach equilibrium to improve the degeneracy of the loss saddles, and in this sense PC could be seen as a resource. However, in practice PC inference requires increasingly more iterations to converge on deeper networks, which aligns with our landscape theory since the loss saddles become more and more degenerate with depth. Our results therefore highlight the fundamental challenge of speeding up PC inference on deeper models if its beneftis for learning are to be realised on large-scale tasks [40]. ", "page_idx": 9}, {"type": "text", "text": "Even if this challenge is overcome, there seem to be two interlinked questions that ultimately matter for the practical training of deep networks. First, are there conditions under which the equilibrated energy can be minimised faster than the loss in a more compute- or memory-efficient manner, with at least equal performance? Optimisation tools such as Adam [24] and skip connections [17], for example, help to deal with the origin saddle at an increased memory cost. Could this trade off with the compute cost of PC inference? Characterising the inference cost of PC more formally would be a useful step in this direction. ", "page_idx": 9}, {"type": "text", "text": "Second, could there be scenarios where PC is slower or less efficient but at the benefti of significantly better performance? This is a hard question to address since we are far from having a theory of generalisation in deep learning [63, 20]. Given our origin saddle result (Theorem 2), however, it is interesting to note that on problems where a low-rank bias is useful (e.g. matrix completion, Figure 6), GD with small initialisations can converge to better-generalising solutions compared to standard initialisation [19]. ", "page_idx": 9}, {"type": "text", "text": "Finally, understanding the overall convergence behaviour of PC would also require characterising other the critical points of the equilibrated energy, especially its minima [14]. Our work, and Eq. 5 in particular, enables this. In $\\S\\mathrm{A}.3.7$ , we present a preliminary investigation showing that, for linear chains, the global minima of the equilibrated energy are flatter than those of the MSE loss. This result potentially explains the common observation that PC convergence tends to slow down towards the end of training, but we leave its full implications for future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "F. I. is funded by the Sussex Neuroscience 4-year PhD Programme. E. M. A. acknowledges funding by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - Project number 442047500 through the Collaborative Research Center \u201cSparsity and Singular Structures\u201d (SFB 1481). R. S. was supported by the Leverhulme Trust through the be.AI Doctoral Scholarship Programme in biomimetic embodied AI. C. L. B. was partially supported by the European Innovation Council (EIC) Pathfinder Challenges, Project METATOOL with Grant Agreement (ID: 101070940). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] E. M. Achour, F. Malgouyres, and S. Gerchinovitz. The loss landscape of deep linear neural networks: a second-order analysis. Journal of Machine Learning Research, 25(242):1\u201376, 2024.   \n[2] N. Alonso, J. Krichmar, and E. Neftci. Understanding and improving optimization in predictive coding networks. arXiv preprint arXiv:2305.13562, 2023.   \n[3] N. Alonso, B. Millidge, J. Krichmar, and E. O. Neftci. A theoretical framework for inference learning. Advances in Neural Information Processing Systems, 35:37335\u201337348, 2022.   \n[4] A. Anandkumar and R. Ge. Efficient approaches for escaping higher order saddle points in non-convex optimization. In Conference on learning theory, pages 81\u2013102. PMLR, 2016.   \n[5] P. Baldi and K. Hornik. Neural networks and principal component analysis: Learning from examples without local minima. Neural networks, 2(1):53\u201358, 1989.   \n[6] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks, 5(2):157\u2013166, 1994.   \n[7] L. B\u00f6ttcher and G. Wheeler. Visualizing high-dimensional loss landscapes with hessian directions. Journal of Statistical Mechanics: Theory and Experiment, 2024(2):023401, 2024.   \n[8] A. J. Bray and D. S. Dean. Statistics of critical points of gaussian fields on large-dimensional spaces. Physical review letters, 98(15):150201, 2007.   \n[9] C. L. Buckley, C. S. Kim, S. McGregor, and A. K. Seth. The free energy principle for action and perception: A mathematical review. Journal of Mathematical Psychology, 81:55\u201379, 2017.   \n[10] B. Byiringiro, T. Salvatori, and T. Lukasiewicz. Robust graph representation learning via predictive coding. arXiv preprint arXiv:2212.04656, 2022.   \n[11] Y. N. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. Advances in neural information processing systems, 27, 2014.   \n[12] S. S. Du, C. Jin, J. D. Lee, M. I. Jordan, A. Singh, and B. Poczos. Gradient descent can take exponential time to escape saddle points. Advances in neural information processing systems, 30, 2017.   \n[13] S. Frieder and T. Lukasiewicz. (non-) convergence results for predictive coding networks. In International Conference on Machine Learning, pages 6793\u20136810. PMLR, 2022.   \n[14] S. Frieder, L. Pinchetti, and T. Lukasiewicz. Bad minima of predictive coding energy functions. In The Second Tiny Papers Track at ICLR 2024, 2024.   \n[15] R. Ge, F. Huang, C. Jin, and Y. Yuan. Escaping from saddle points\u2014online stochastic gradient for tensor decomposition. In Conference on learning theory, pages 797\u2013842. PMLR, 2015.   \n[16] S. Greydanus. Scaling down deep learning. arXiv preprint arXiv:2011.14439, 2020.   \n[17] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013 778, 2016.   \n[18] F. Innocenti, R. Singh, and C. Buckley. Understanding predictive coding as a second-order trust-region method. In ICML Workshop on Localized Learning (LLW), 2023.   \n[19] A. Jacot, F. Ged, B. \u00b8Sim\u00b8sek, C. Hongler, and F. Gabriel. Saddle-to-saddle dynamics in deep linear networks: Small initialization training, symmetry, and sparsity. arXiv preprint arXiv:2106.15933, 2021.   \n[20] Y. Jiang, B. Neyshabur, H. Mobahi, D. Krishnan, and S. Bengio. Fantastic generalization measures and where to find them. arXiv preprint arXiv:1912.02178, 2019.   \n[21] C. Jin, R. Ge, P. Netrapalli, S. M. Kakade, and M. I. Jordan. How to escape saddle points efficiently. In International conference on machine learning, pages 1724\u20131732. PMLR, 2017.   \n[22] C. Jin, P. Netrapalli, R. Ge, S. M. Kakade, and M. I. Jordan. On nonconvex optimization for machine learning: Gradients, stochasticity, and saddle points. Journal of the ACM (JACM), 68(2):1\u201329, 2021.   \n[23] K. Kawaguchi. Deep learning without poor local minima. Advances in neural information processing systems, 29, 2016.   \n[24] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[25] T. Laurent and J. Brecht. Deep linear networks with arbitrary loss: All local minima are global. In International conference on machine learning, pages 2902\u20132907. PMLR, 2018.   \n[26] J. D. Lee, I. Panageas, G. Piliouras, M. Simchowitz, M. I. Jordan, and B. Recht. First-order methods almost always avoid strict saddle points. Mathematical programming, 176:311\u2013337, 2019.   \n[27] J. D. Lee, M. Simchowitz, M. I. Jordan, and B. Recht. Gradient descent only converges to minimizers. In Conference on learning theory, pages 1246\u20131257. PMLR, 2016.   \n[28] H. Li, Z. Xu, G. Taylor, C. Studer, and T. Goldstein. Visualizing the loss landscape of neural nets. Advances in neural information processing systems, 31, 2018.   \n[29] H. Lu and K. Kawaguchi. Depth creates no bad local minima. arXiv preprint arXiv:1702.08580, 2017.   \n[30] A. Meulemans, F. Carzaniga, J. Suykens, J. Sacramento, and B. F. Grewe. A theoretical framework for target propagation. Advances in Neural Information Processing Systems, 33:20024\u2013 20036, 2020.   \n[31] B. Millidge, T. Salvatori, Y. Song, R. Bogacz, and T. Lukasiewicz. Predictive coding: towards a future of deep learning beyond backpropagation? arXiv preprint arXiv:2202.09467, 2022.   \n[32] B. Millidge, A. Seth, and C. L. Buckley. Predictive coding: a theoretical and experimental review. arXiv preprint arXiv:2107.12979, 2021.   \n[33] B. Millidge, Y. Song, T. Salvatori, T. Lukasiewicz, and R. Bogacz. Backpropagation at the infinitesimal inference limit of energy-based models: Unifying predictive coding, equilibrium propagation, and contrastive hebbian learning. arXiv preprint arXiv:2206.02629, 2022.   \n[34] B. Millidge, Y. Song, T. Salvatori, T. Lukasiewicz, and R. Bogacz. A theoretical framework for inference and learning in predictive coding networks. arXiv preprint arXiv:2207.12316, 2022.   \n[35] B. Millidge, M. Tang, M. Osanlouy, N. S. Harper, and R. Bogacz. Predictive coding networks for temporal prediction. PLOS Computational Biology, 20(4):e1011183, 2024.   \n[36] B. Millidge, A. Tschantz, and C. L. Buckley. Predictive coding approximates backprop along arbitrary computation graphs. Neural Computation, 34(6):1329\u20131368, 2022.   \n[37] M. Nouiehed and M. Razaviyayn. Learning deep models: Critical points and local openness. INFORMS Journal on Optimization, 4(2):148\u2013173, 2022.   \n[38] R. C. O\u2019Reilly. Biologically plausible error-driven learning using local activation differences: The generalized recirculation algorithm. Neural computation, 8(5):895\u2013938, 1996.   \n[39] A. Orvieto, J. Kohler, D. Pavllo, T. Hofmann, and A. Lucchi. Vanishing curvature in randomly initialized deep relu networks. In International Conference on Artificial Intelligence and Statistics, pages 7942\u20137975. PMLR, 2022.   \n[40] L. Pinchetti, C. Qi, O. Lokshyn, G. Olivers, C. Emde, M. Tang, A. M\u2019Charrak, S. Frieder, B. Menzat, R. Bogacz, et al. Benchmarking predictive coding networks\u2013made simple. arXiv preprint arXiv:2407.01163, 2024.   \n[41] L. Pinchetti, T. Salvatori, Y. Yordanov, B. Millidge, Y. Song, and T. Lukasiewicz. Predictive coding beyond gaussian distributions. arXiv preprint arXiv:2211.03481, 2022.   \n[42] R. Rosenbaum. On the relationship between predictive coding and backpropagation. Plos one, 17(3):e0266102, 2022.   \n[43] T. Salvatori, A. Mali, C. L. Buckley, T. Lukasiewicz, R. P. Rao, K. Friston, and A. Ororbia. Braininspired computational intelligence via predictive coding. arXiv preprint arXiv:2308.07870, 2023.   \n[44] T. Salvatori, L. Pinchetti, A. M\u2019Charrak, B. Millidge, and T. Lukasiewicz. Causal inference via predictive coding. arXiv preprint arXiv:2306.15479, 2023.   \n[45] T. Salvatori, L. Pinchetti, B. Millidge, Y. Song, T. Bao, R. Bogacz, and T. Lukasiewicz. Learning on arbitrary graph topologies via predictive coding. Advances in neural information processing systems, 35:38232\u201338244, 2022.   \n[46] T. Salvatori, Y. Song, T. Lukasiewicz, R. Bogacz, and Z. Xu. Predictive coding can do exact backpropagation on convolutional and recurrent neural networks. arXiv preprint arXiv:2103.03725, 2021.   \n[47] A. R. Sankar and V. N. Balasubramanian. Are saddles good enough for neural networks. In Proceedings of the ACM India Joint International Conference on Data Science and Management of Data, pages 37\u201345, 2018.   \n[48] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.   \n[49] B. Scellier and Y. Bengio. Equilibrium propagation: Bridging the gap between energy-based models and backpropagation. Frontiers in computational neuroscience, 11:24, 2017.   \n[50] B. Scellier, M. Ernoult, J. Kendall, and S. Kumar. Energy-based learning algorithms for analog computing: a comparative study. Advances in Neural Information Processing Systems, 36, 2024.   \n[51] O. Shamir. Exponential convergence time of gradient descent for one-dimensional deep linear neural networks. In Conference on Learning Theory, pages 2691\u20132713. PMLR, 2019.   \n[52] S. P. Singh, G. Bachmann, and T. Hofmann. Analytic insights into structure and rank of neural network hessian maps. Advances in Neural Information Processing Systems, 34:23914\u201323927, 2021.   \n[53] Y. Song, T. Lukasiewicz, Z. Xu, and R. Bogacz. Can the brain do backpropagation?\u2014exact implementation of backpropagation in predictive coding networks. Advances in neural information processing systems, 33:22566\u201322579, 2020.   \n[54] Y. Song, B. Millidge, T. Salvatori, T. Lukasiewicz, Z. Xu, and R. Bogacz. Inferring neural activity before plasticity: A foundation for learning beyond backpropagation. bioRxiv, pages 2022\u201305, 2022.   \n[55] M. Staib, S. Reddi, S. Kale, S. Kumar, and S. Sra. Escaping saddle points with adaptive gradient methods. In International Conference on Machine Learning, pages 5956\u20135965. PMLR, 2019.   \n[56] M. Stern, A. J. Liu, and V. Balasubramanian. Physical effects of learning. Physical Review E, 109(2):024311, 2024.   \n[57] R. Sun. Optimization for deep learning: theory and algorithms. arXiv preprint arXiv:1912.08957, 2019.   \n[58] R. Sun, D. Li, S. Liang, T. Ding, and R. Srikant. The global landscape of neural networks: An overview. IEEE Signal Processing Magazine, 37(5):95\u2013108, 2020.   \n[59] A. Tschantz, B. Millidge, A. K. Seth, and C. L. Buckley. Hybrid predictive coding: Inferring, fast and slow. arXiv preprint arXiv:2204.02169, 2022.   \n[60] J. C. Whittington and R. Bogacz. An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity. Neural computation, 29(5):1229\u20131262, 2017.   \n[61] C. Yun, S. Sra, and A. Jadbabaie. Global optimality conditions for deep neural networks. arXiv preprint arXiv:1707.02444, 2017.   \n[62] U. Zahid, Q. Guo, and Z. Fountas. Predictive coding as a neuromorphic alternative to backpropagation: A critical evaluation. Neural Computation, 35(12):1881\u20131909, 2023.   \n[63] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107\u2013115, 2021.   \n[64] Y. Zhou and Y. Liang. Critical points of linear neural networks: Analytical forms and landscape properties. In International conference on learning representations, 2018.   \n[65] Z. Zhu, D. Soudry, Y. C. Eldar, and M. B. Wakin. The global optimization geometry of shallow linear neural networks. Journal of Mathematical Imaging and Vision, 62(3):279\u2013292, 2020.   \n[66] L. Ziyin, B. Li, and X. Meng. Exact solutions of a deep linear network. Advances in Neural Information Processing Systems, 35:24446\u201324458, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 General notation and definitions 15   \nA.2 Related work 15   \nA.2.1 Theories of predictive coding 15   \nA.2.2 Saddle points and neural networks 16   \nA.3 Proofs and derivations . 16   \nA.3.1 Loss Hessian for DLNs 16   \nA.3.2 Equilibrated energy for DLNs 17   \nA.3.3 Hessian of the equilibrated energy for DLNs 18   \nA.3.4 Example: 1-hidden layer linear network 20   \nA.3.5 Hessian of the equilibrated energy for linear chains 21   \nA.3.6 Strictness of zero-rank saddles of the equilibrated energy 22   \nA.3.7 Flatter global minima of the equilibrated energy (linear chains) 23   \nA.4 Experimental details 23   \nA.5 Supplementary results . 25 ", "page_idx": 14}, {"type": "text", "text": "A.1 General notation and definitions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Matrices, vectors and scalars are denoted with bold capitals A, bold lower-case characters $\\mathbf{v}$ and non-bold characters $u$ or $U$ , respectively. All vectors are by default column vectors $[\\cdot]\\,\\in\\,\\mathbb{R}^{n\\times1}$ , and $\\operatorname{vec}_{r}(\\cdot)$ denotes the row-wise vec operator. Following [52], unless otherwise stated we define matrix-by-matrix derivatives by row-vectorisation, using the numerator or Jacobian layout ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left(\\frac{\\partial\\mathbf{A}}{\\partial\\mathbf{B}}\\right)_{i j}:=\\frac{[\\partial\\operatorname{vec}_{r}(\\mathbf{A})]_{i}}{[\\partial\\operatorname{vec}_{r}(\\mathbf{B})^{T}]_{j}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "such that the result is a matrix rather than a 4D tensor. Following from this, we will also use the rules ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial{\\bf A}{\\bf B}{\\bf C}}{\\partial{\\bf B}}={\\bf A}\\otimes{\\bf C}^{T}}\\\\ {\\displaystyle\\frac{\\partial{\\bf A}{\\bf B}}{\\partial{\\bf A}}={\\bf I}_{m}\\otimes{\\bf B}^{T},\\quad{\\bf A}\\in\\mathbb{R}^{m\\times n},{\\bf B}\\in\\mathbb{R}^{n\\times p}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.2 Related work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.2.1 Theories of predictive coding ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "PC and BP. [60] where the first to show that PC can approximate BP on multi-layer perceptrons when the influence of the input is upweighted relative to that of the output. [36] generalised this result to arbitrary computational graphs including convolutional and recurrent neural networks under the so-called \u201cfixed prediction assumption\u201d. A variation of PC where weights are updated at precisely timed inference steps was later shown to compute exactly the same gradients as BP on multi-layer perceptrons [53], a result further generalised by [46] and [42]. [33] unified these and other approximation results from an energy-based modelling perspective. [62] proved that the time complexity of all of these PC variants is lower-bounded by BP. ", "page_idx": 14}, {"type": "text", "text": "PC and other algorithms. [13] provided an in-depth dynamical systems analysis of the inference convergence for PC variants approximating BP. [34] showed that for linear networks the PC inference equilibrium can be interpreted as an average of BP\u2019s feedforward pass values and the local targets computed by target propagation [30]. [54] proposed that PC and other energy-based algorithms implement a fundamentally different principle of credit assignment called \u201cprospective configuration\u201d, in that neurons first change their activity to align with the target and then update their weights to consolidate that activity pattern. For mini-batches of size one, [3] proved that PC approximates implicit gradient descent under specific layer-wise rescalings of the activities and parameter learning rates. [2] further showed that when this approximation holds, PC can be sensitive to Hessian information. Similarly, recent work cast PC as a second-order trust-region method [18]. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A.2.2 Saddle points and neural networks ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here we review some relevant theoretical and empirical work on (i) saddle points in the loss landscape of neural networks and (ii) the behaviour of different learning algorithms, especially (S)GD, near saddles. For more general reviews on the loss landscape and optimisation of neural networks, see [57] and [58]. ", "page_idx": 15}, {"type": "text", "text": "Saddles in the neural loss landscape. This work began with [5] showing that for linear networks with one hidden layer, all critical points of the MSE loss are either global minima or strict saddle points (Def. 1). For the same model, [48] later showed saddle-to-saddle learning transitions for small initialisation and characterised the GD dynamics under specific assumptions on the data. [11] highlighted the prevalence of saddles, relative to local minima, in the high-dimensional non-convex loss of neural networks. In particular, they empirically demonstrated a qualitative similarity between the landscape of networks and random Gaussian error functions, where the higher the error a critical point is associated with, the more exponentially likely it is to be a saddle [8]. ", "page_idx": 15}, {"type": "text", "text": "[23] famously generalised the [5] result that all local minima are global to arbitrarily deep linear networks (DLNs) under some weak assumptions on the data. This was simplified as well as extended under less strict assumptions by [29]. Importantly, [23] was the first to show that for neural networks with one hidden layer $H=1$ all saddle points are strict (or first-order), while deeper networks have non-strict ( $H$ -order) saddles (for example at the origin where all the weights are zero). Several variations and extensions of this set of results have since been formulated [61, 64, 25, 65, 37, 66]. For our purposes, one important extension was made by [1], who characterised all the critical points of the MSE loss for DLNs to second-order, including strict and non-strict saddles. ", "page_idx": 15}, {"type": "text", "text": "Learning near saddles. This work can be traced to [15] who showed that SGD with added noise can converge in polynomial time on strict saddle functions. [27] proved a similar result that GD without any noise asymptotically escapes strict saddles for almost all initialisations. This was later generalised to other first-order methods [26]. [21] proved that another noisy version of GD converges with high probability to a second-order critical point in poly-logarithmic time depending on the dimension. For a review of these and other convergence results of GD and its variants, see [22]. [4] showed (i) that a further GD variant can be proved to converge to a third-order critical point and escape second-order saddles but at a high computational cost and (ii) that finding higher-order critical points is NP-hard. ", "page_idx": 15}, {"type": "text", "text": "[12] proved the important result that while standard GD with common initialisations will eventually escape strict saddles, it can take an exponential time to do so. This is in contrast to the perturbed GD versions mentioned above, which converge in polynomial time. Similarly, [51] proved that for linear chains or one-dimensional networks with unit width, the convergence time of GD scales exponentially with the depth. [39] analysed similar models and showed that both the gradients and curvature vanish with network depth unless the width is appropriately scaled. [39] suggested that this in part explains the success of adaptive gradient optimisers like Adam [24] which can adapt to flat curvature. Similarly, [55] showed that adaptive methods can escape saddle points faster by rescaling the gradient noise near critical points to be isotropic. ", "page_idx": 15}, {"type": "text", "text": "[19] conjectured a saddle-to-saddle dynamic where GD visits a sequence of saddles of increasing rank before converging to a sparse global minimum. A few works have also shown that in practice SGD can converge to second-order critical points that are non-strict saddles rather than minima [47, 7]. ", "page_idx": 15}, {"type": "text", "text": "A.3 Proofs and derivations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.3.1 Loss Hessian for DLNs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here we derive the Hessian of the MSE loss (Eq. 1) with respect to the weights of arbitrary DLNs (\u00a72.1); this is essentially a re-derivation of [52] with slightly different notation.2 We then show how ", "page_idx": 15}, {"type": "text", "text": "the Hessian and its eigenspectrum at the origin ( ${\\boldsymbol{\\theta}}=\\mathbf{0}$ ) changes as a function of the number of hidden layers $H$ . We start from the gradient of the loss for a given weight matrix ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}_{\\ell}}=(\\mathbf{W}_{L:\\ell+1})^{T}(\\mathbf{W}_{L:1}\\mathbf{x}-\\mathbf{y})(\\mathbf{W}_{\\ell-1:1}\\mathbf{x})^{T}}\\\\ &{\\qquad=(\\mathbf{W}_{L:\\ell+1})^{T}(\\mathbf{W}_{L:1}\\widetilde{\\Sigma}_{\\mathbf{xx}}-\\widetilde{\\Sigma}_{\\mathbf{yx}})(\\mathbf{W}_{\\ell-1:1})^{T}\\in\\mathbb{R}^{n_{\\ell}\\times n_{\\ell-1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where following previous works we take the empirical mean over the data matrices $\\widetilde{\\Sigma}_{\\mathbf{x}\\mathbf{x}}\\ :=$ $\\begin{array}{r}{\\frac{1}{N}\\sum_{i}^{N}\\mathbf{x}_{i}\\mathbf{x}_{i}^{T}}\\end{array}$ and $\\begin{array}{r}{\\widetilde{\\Sigma}_{\\mathbf{y}\\mathbf{x}}\\;:=\\;\\frac{1}{N}\\sum_{i}^{N}\\mathbf{y}_{i}\\mathbf{x}_{i}^{T}}\\end{array}$ . For networks with at least one hidden layer , the origin is a critical point since the gradient is zero . To characterise this point to second order, we look at the Hessian. Starting with the diagonal blocks of size $(n_{\\ell}n_{\\ell-1})\\times\\bar{(n_{\\ell}n_{\\ell-1})}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}\\mathcal{L}}{\\partial\\mathbf{W}_{\\ell}^{2}}=(\\mathbf{W}_{L:\\ell+1})^{T}\\mathbf{W}_{L:\\ell+1}\\otimes\\mathbf{W}_{\\ell-1:1}\\widetilde{\\Sigma}_{\\mathbf{xx}}(\\mathbf{W}_{\\ell-1:1})^{T}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "it is straightforward to see that at the origin this term collapses to the null matrix for any $l$ .3 To compute the $(n_{k}\\bar{n_{k-1}})\\times(n_{\\ell}n_{\\ell-1})$ off-diagonal blocks, we follow [52] and write the distinct contributions as follows ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l l}&{\\forall k\\neq\\ell,\\quad\\widetilde{\\mathbf{H}}_{C}:=\\frac{\\partial^{2}\\mathcal{L}}{\\partial\\mathbf{W}_{k}\\partial\\mathbf{W}_{\\ell}}=(\\mathbf{W}_{L:\\ell+1})^{T}\\mathbf{W}_{L:k+1}\\otimes\\mathbf{W}_{\\ell-1:1}\\widetilde{\\mathbf{E}}_{\\mathbf{xx}}(\\mathbf{W}_{k-1:1})^{T}\\otimes\\quad\\left(\\mathbf{1}^{\\top}\\mathbf{W}_{k}\\right)^{T}}\\\\ &{\\forall k>\\ell,\\quad\\widehat{\\mathbf{H}}_{C}:=\\frac{\\partial^{2}\\mathcal{L}}{\\partial\\mathbf{W}_{k}^{T}\\partial\\mathbf{W}_{\\ell}}=(\\mathbf{W}_{k-1:\\ell+1})^{T}\\otimes\\mathbf{W}_{\\ell-1:1}\\bigr(\\mathbf{W}_{L:1}\\widetilde{\\mathbf{E}}_{\\mathbf{xx}}-\\widetilde{\\mathbf{E}}_{\\mathbf{yx}}\\bigr)^{T}\\mathbf{W}_{L:k+1}}&{\\ \\mathrm{(15)}}\\\\ &{\\forall k<\\ell,\\quad\\widehat{\\mathbf{H}}_{C}:=\\frac{\\partial^{2}\\mathcal{L}}{\\partial\\mathbf{W}_{k}^{T}\\partial\\mathbf{W}_{\\ell}}=(\\mathbf{W}_{L:\\ell+1})^{T}\\bigr(\\mathbf{W}_{L:1}\\widetilde{\\mathbf{E}}_{\\mathbf{xx}}-\\widetilde{\\mathbf{E}}_{\\mathbf{yx}}\\bigr)\\bigr(\\mathbf{W}_{k-1:1}\\bigr)^{T}\\otimes\\left(\\mathbf{W}_{\\ell-1:k+1}\\right)^{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "At the origin, these blocks always vanish except for networks with one hidden layer, where as shown by [48] they are characterised by the empirical input-output covariance, e.g. for $k\\;<\\;\\ell,\\partial^{2}\\bar{\\mathcal{L}}/\\partial\\mathbf{W}_{k}\\partial\\mathbf{W}_{\\ell}(\\pmb{\\theta}\\;=\\;\\mathbf{0})\\;=\\;-\\widetilde{\\Sigma}_{\\mathbf{xy}}\\stackrel{.}{\\otimes}\\mathbf{I}_{n},H\\;=\\;1.$ . Putting the above facts together, we can now write the full loss Hessian at the origin for different number of hidden layers. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{H}_{\\mathcal{L}}(\\pmb{\\theta}=\\mathbf{0})=\\left\\{\\begin{array}{l l}{\\left[\\begin{array}{c c c}{\\mathbf{0}}&{-\\widetilde{\\Sigma}_{\\mathbf{x}\\mathbf{y}}\\otimes\\mathbf{I}_{n_{1}}}\\\\ {-\\mathbf{I}_{n_{1}}\\otimes\\widetilde{\\Sigma}_{\\mathbf{y}\\mathbf{x}}}&{\\mathbf{0}}\\end{array}\\right],}&{H=1\\quad\\mathrm{[strict~saddle]}}\\\\ {\\left[\\begin{array}{c c c}{\\mathbf{0}}&{\\ldots}&{\\mathbf{0}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\mathbf{0}}&{\\ldots}&{\\mathbf{0}}\\end{array}\\right]=\\mathbf{0}_{p},}&{H>1\\quad\\mathrm{[non-strict~saddle]}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For one-hidden-layer networks, the Hessian is indefinite, with positive and negative eigenvalues given by the empirical input-output covariance, as described by [48]. For any DLN with more than one hidden layer, the Hessian is zero, and the origin is therefore a second-order critical point. In the general case, this point is a non-strict saddle because some higher-order derivative of the loss depending on the network depth will contain at least one negative escape direction. More specifically, for a network with $L$ layers, all the $L-1$ derivatives vanish, and negative directions will be found in the derivatives of order $\\geq L$ . ", "page_idx": 16}, {"type": "text", "text": "A.3.2 Equilibrated energy for DLNs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Here we derive an exact solution to the PC energy (Eq. 2) of DLNs at the inference equilibrium (Theorem 1, Eq. 5), $\\mathcal{F}|_{\\partial\\mathcal{F}/\\partial\\mathbf{z}=0}$ , which we will abbreviate as $\\mathcal{F}^{*}$ . This turns out to be a non-trivial rescaled MSE loss where the rescaling depends on covariances of the network weight matrices. To highlight the difference with the loss, recall that the standard MSE (Eq. 1) for a DLN implicitly defines the following generative model ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{y}\\sim\\mathcal{N}(\\mathbf{W}_{L:1}\\mathbf{x},\\Sigma)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the target is modelled as a Gaussian with a mean given by the network function and some covariance $\\Sigma$ . In a PC network, by contrast, the activity of each hidden layer\u2013and not just the output\u2013is modelled as a Gaussian (see $\\S2.2_{.}$ ) ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf z_{\\ell}\\sim\\mathcal N(\\mathbf W_{\\ell}\\mathbf z_{\\ell-1},\\mathbf I_{\\ell})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mathbf{z}_{0}:=\\mathbf{x}$ and $\\mathbf{z}_{L}:=\\mathbf{y}$ . Now, to work out the generative model for the target implied by this hierarchical Gaussian model, we can simply \u201cunfold\u201d the model at each layer. Specifically, we can reparameterise the activity of each hidden layer as a noisy function of the previous layer and so on recursively up to the first layer ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{z}_{1}=\\mathbf{W}_{1}\\mathbf{z}_{0}+\\mathbf{\\xi}_{1}}\\\\ &{\\mathbf{z}_{2}=\\mathbf{W}_{2}\\mathbf{z}_{1}+\\mathbf{\\xi}_{2}=\\mathbf{W}_{2}\\mathbf{W}_{1}\\mathbf{x}+\\mathbf{W}_{2}\\mathbf{\\xi}_{1}+\\mathbf{\\xi}_{2}}\\\\ &{\\mathbf{z}_{3}=\\mathbf{W}_{3}\\mathbf{z}_{2}+\\mathbf{\\xi}_{3}=\\mathbf{W}_{3}\\mathbf{W}_{2}\\mathbf{W}_{1}\\mathbf{x}+\\mathbf{W}_{3}\\mathbf{W}_{2}\\mathbf{\\xi}_{1}+\\mathbf{W}_{3}\\mathbf{\\xi}_{2}+\\mathbf{\\xi}_{3}}\\\\ &{\\ \\ \\vdots}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\pmb{\\xi}_{\\ell}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I}_{\\ell})$ is white Gaussian noise. The last layer can then be written as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{z}_{L}=\\mathbf{W}_{L}\\mathbf{z}_{L-1}+\\mathbf{\\xi}_{L}}\\\\ &{\\quad\\quad=\\mathbf{W}_{L:1}\\mathbf{z}_{0}+\\displaystyle\\sum_{\\ell=2}^{L}\\mathbf{W}_{L:\\ell}\\pmb{\\xi}_{\\ell-1}+\\mathbf{\\xi}_{L}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We can now derive the implicit generative model for the target by taking the expectation and variance of Eq. 27 with respect to $\\mathbf{y}$ . ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{y}\\sim\\mathcal{N}\\left(\\mathbf{W}_{L:1}\\mathbf{x},\\mathbf{I}_{L}+\\sum_{\\ell=2}^{L}(\\mathbf{W}_{L:\\ell})(\\mathbf{W}_{L:\\ell})^{T}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We therefore observe that, in contrast to the loss (Eq. 21), PC implicitly models the target with a non-identity covariance depending on a chained covariance of the previous layers which in turns depends only on the weights. It follows that, at the exact inference equilibrium where that implicit generative model holds, the PC energy is simply the following rescaled MSE loss (Eq. 1) ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{F}^{*}=\\frac{1}{2N}\\sum_{i}^{N}(\\mathbf{y}_{i}-\\mathbf{W}_{L:1}\\mathbf{x}_{i})^{T}\\mathbf{S}^{-1}(\\mathbf{y}_{i}-\\mathbf{W}_{L:1}\\mathbf{x}_{i})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the rescaling is $\\begin{array}{r}{\\mathbf{S}=\\mathbf{I}_{d_{y}}+\\sum_{\\ell=2}^{L}(\\mathbf{W}_{L:\\ell})(\\mathbf{W}_{L:\\ell})^{T}}\\end{array}$ . Note that a generative model with nonidentity covariances at each layer w ould lead to a different rescaling, but we do not consider this here because we aim to remain as close as possible to the assumptions made in practice. Because $\\mathcal{F}^{*}(\\pmb{\\theta})$ is the effective landscape on which we perform learning (see $\\S2.2)$ , the PC inference process can then be thought of as reshaping the loss landscape to take this layer-wise, weight-dependent covariance into account. ", "page_idx": 17}, {"type": "text", "text": "A.3.3 Hessian of the equilibrated energy for DLNs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Here we derive the Hessian at the origin of the equilibrated energy for DLNs, following the calculation of the loss Hessian (\u00a7A.3.1). Section A.3.5 shows an equivalent derivation for one-dimensional linear networks, which preserves all the key the intuitions and is easier to follow. We start from the equilibrated energy we derived previously for DLNs (\u00a7A.3.2, Eq. 29), which turned out to be the following rescaled MSE loss ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{F}^{*}=\\frac{1}{2N}\\sum_{i}^{N}\\mathbf{r}_{i}^{T}\\mathbf{S}^{-1}\\mathbf{r}_{i}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathbf{S}=\\mathbf{I}_{d_{y}}+\\sum_{\\ell=2}^{L}(\\mathbf{W}_{L:\\ell})(\\mathbf{W}_{L:\\ell})^{T}}\\end{array}$ , and we denote the residual error for a given data sample as $\\mathbf{r}_{i}:=\\left(\\mathbf{y}_{i}-\\mathbf{W}_{L:1}\\mathbf{x}_{i}\\right)$ . In the general case, both the residual and the rescaling depend on $\\mathbf{W}_{\\ell}$ , so to take the gradient of the equilibrated energy we need the product rule. For simplicity, and similar ", "page_idx": 17}, {"type": "text", "text": "to the characterisation of the off-diagonal blocks of the loss Hessian (\u00a7A.3.1), we write the two contributions separately, as follows ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{A}:=\\displaystyle\\frac{1}{2N}\\sum_{i}^{N}\\frac{\\partial\\mathbf{r}_{i}^{T}}{\\partial\\mathbf{W}_{\\ell}}\\mathbf{S}^{-1}\\frac{\\partial\\mathbf{r}_{i}}{\\partial\\mathbf{W}_{\\ell}}=(\\mathbf{W}_{L:\\ell+1})^{T}\\mathbf{S}^{-1}(\\mathbf{W}_{L:1}\\widetilde{\\mathbf{S}}_{\\mathbf{xx}}-\\widetilde{\\mathbf{\\Sigma}}_{\\mathbf{yx}})(\\mathbf{W}_{\\ell-1:1})^{T}}\\\\ &{\\mathbf{B}:=\\displaystyle\\frac{1}{2N}\\sum_{i}^{N}\\mathbf{r}_{i}^{T}\\frac{\\partial\\mathbf{S}^{-1}}{\\partial\\mathbf{W}_{\\ell}}\\mathbf{r}_{i}=-\\frac{1}{N}\\sum_{i}^{N}\\mathbf{S}^{-1}\\mathbf{r}_{i}\\mathbf{r}_{i}^{T}\\mathbf{S}^{-1}\\frac{\\partial\\mathbf{S}}{\\partial\\mathbf{W}_{\\ell}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where in Eq. $32\\;\\partial\\mathbf{S}/\\partial\\mathbf{W}_{\\ell}$ is a 4D tensor, and we use the rule $\\partial\\mathbf{a}^{T}\\mathbf{X}\\mathbf{b}/\\partial\\mathbf{X}=-\\mathbf{X}^{-T}\\mathbf{a}\\mathbf{b}^{T}\\mathbf{X}^{-T}$ . The first term A is simply a rescaled loss gradient, while the second term $\\mathbf{B}$ depends on the derivative of the rescaling. Note that for $\\mathbf{W}_{1}$ the gradient collapses to the first term since the rescaling does not depend on it, $\\partial\\mathcal{F}^{*}/\\partial\\mathbf{W}_{1}=(\\mathbf{W}_{L:2})^{\\bar{T}}\\mathbf{S}^{-1}(\\mathbf{W}_{L:1}\\tilde{\\Sigma}_{\\mathbf{x}\\mathbf{x}}-\\widetilde{\\Sigma}_{\\mathbf{y}\\mathbf{x}}).$ . ", "page_idx": 18}, {"type": "text", "text": "As an aside relevant to the zero-rank saddles analysed in $\\S3.3$ , we note that, in contrast to the loss, $\\mathbf{W}_{L}=\\mathbf{0}$ is a necessary (though not sufficient) condition for the energy gradient to be zero. This is because the derivative of the rescaling ${\\partial\\mathbf{S}}/{\\partial\\mathbf{W}_{\\ell}}$ needs to be zero in order for the gradient term $\\mathbf{B}$ to vanish, and it has one term linear in the last weight matrix. ", "page_idx": 18}, {"type": "text", "text": "As for the loss $(\\S\\mathrm{A}.3.1)$ , the origin is a critical point of the energy since $\\mathbf{g}_{\\mathcal{F}^{*}}(\\pmb{\\theta}=\\mathbf{0})=\\mathbf{0}$ . For $\\mathbf{B}$ , this is because while the rescaling at zero is the identity, the derivative of the rescaling vanishes since it is linear with respect to any weight matrix. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathbf{S}^{-1}(\\pmb{\\theta}=\\mathbf{0})=\\mathbf{I}_{d_{y}}}{\\partial\\mathbf{S}}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Calculating the Hessian involves multiple application of the product rule, so for simplicity we analyse the contribution of the derivative of each term (Eqs. 31 & 32) at the origin. Because the first term is simply a rescaling of the loss, and given Eq. 33, its second derivative at zero is always zero with respect to the same weight matrix. ", "page_idx": 18}, {"type": "equation", "text": "$$\nk=\\ell,\\quad\\frac{\\partial\\mathbf{A}}{\\partial\\mathbf{W}_{k}}(\\pmb{\\theta}=\\mathbf{0})=\\mathbf{0},\\quad H\\geq1\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "As for the loss, this term is also zero with respect to some other weight matrix $k\\neq\\ell$ except for the case of a one-hidden-layer network. ", "page_idx": 18}, {"type": "equation", "text": "$$\nk\\neq\\ell,\\quad\\frac{\\partial\\mathbf{A}}{\\partial\\mathbf{W}_{k}}(\\pmb\\theta=\\mathbf0)=\\left\\{\\begin{array}{l l}{\\displaystyle-\\mathbf{I}_{n_{1}}\\otimes\\widetilde{\\\\\\Sigma}_{\\mathbf{y}\\mathbf{x}},}&{k>\\ell,H=1}\\\\ {\\displaystyle-\\widetilde{\\Sigma}_{\\mathbf{x}\\mathbf{y}}\\otimes\\mathbf{I}_{n_{1}},}&{k<\\ell,H=1}\\\\ {\\displaystyle\\mathbf0,}&{H>1}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The second derivative of $\\mathbf{B}$ requires a 5-fold application of the product rule, involving the first derivative of the residual (and its transpose) and the first and second derivatives of the rescaling. As shown above (Eq. 34), the first derivative of the rescaling at the origin is zero, and the derivative of the residual with respect to any weight matrix at zero is always zero for any network with one or more hidden layers, $\\partial\\mathbf{r}/\\partial\\mathbf{W}_{k}(\\pmb{\\theta}=\\mathbf{0})=\\mathbf{0},H\\geq1$ . The second derivative of the rescaling, however, is non-zero for the special case of the last weight matrix with respect to itself ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}\\mathbf{S}}{\\partial\\mathbf{W}_{k}\\partial\\mathbf{W}_{\\ell}}(\\pmb{\\theta}=\\mathbf{0})=\\left\\{\\begin{array}{l l}{\\mathbf{I}_{n_{L-1}},\\quad\\ell=k=L}\\\\ {\\quad}\\\\ {\\mathbf{0},\\quad}&{\\mathrm{else}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which means that at zero $\\mathbf{B}$ takes the following form ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathbf{B}}{\\partial\\mathbf{W}_{k}}(\\pmb{\\theta}=\\mathbf{0})=\\left\\{\\begin{array}{l l}{\\displaystyle-\\widetilde{\\pmb{\\Sigma}}_{\\mathbf{y}\\mathbf{y}}\\otimes I_{n_{L-1}},}&{\\ell=k=L}\\\\ {\\quad}\\\\ {\\displaystyle\\mathbf{0},}&{\\mathrm{else}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "twohgeerteh $\\begin{array}{r}{\\widetilde{\\Sigma}_{\\mathbf{y}\\mathbf{y}}:=\\frac{1}{N}\\sum_{i}^{N}\\mathbf{y}_{i}\\mathbf{y}_{i}^{T}}\\end{array}$ uilsl t hHee essmiapinr iocfa lt hoeu taptu tt hceo voaririgainn coe f mthater iexq. uDilriabwraitnegd  aelln tehregsye  foobrs edrivffaetiroennst number of hidden layers. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{H}_{\\mathcal{F}^{*}}(\\theta=\\mathbf{0})=\\left\\{\\begin{array}{l l}{\\left[\\begin{array}{l l l}{\\mathbf{0}}&{-\\widetilde{\\Sigma}_{\\mathbf{xy}}\\otimes\\mathbf{I}_{n_{1}}}\\\\ {-\\mathbf{I}_{n_{1}}\\otimes\\widetilde{\\Sigma}_{\\mathbf{yx}}}&{-\\widetilde{\\Sigma}_{\\mathbf{yy}}\\otimes I_{n_{L-1}}\\right],}&{H=1\\quad\\mathrm{[strict~saddle]}}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ {\\left[\\begin{array}{l l l}{\\mathbf{0}}&{\\dots}&{\\mathbf{0}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\mathbf{0}}&{\\dots}&{-\\widetilde{\\Sigma}_{\\mathbf{yy}}\\otimes I_{n_{L-1}}\\right]}\\end{array}\\right.}&{H>1\\quad\\mathrm{[strict~saddle]}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We see that, compared to the loss Hessian (Eq. 20), the energy Hessian has a non-zero last diagonal block given for any $H$ . We note, but do not investigate in any depth, the potential connection with target propagation [30, 34]. The one-hidden-layer case is fully derived in the next section (\u00a7A.3.4). It is straightforward to show that these matrices have negative eigenvalues ", "page_idx": 19}, {"type": "equation", "text": "$$\nH\\geq1,\\quad\\lambda_{\\operatorname*{min}}\\big(\\mathbf{H}_{\\mathcal{F}^{*}}(\\pmb{\\theta}=\\mathbf{0})\\big)<0,\\quad\\forall y_{i}\\neq0\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "since $\\mathbf{A}\\mathbf{A}^{T}$ is positive definite $\\forall\\mathbf{A}\\neq\\mathbf{0}$ . The origin is therefore a strict saddle (Def. 1) of the equilibrated energy. This is in stark contrast to the MSE loss, which has a strict origin saddle only for one-hidden-layer networks and a non-strict saddle of order $H$ for any deeper network. For the general case $H>1$ , the negative curvature of the energy Hessian is given only by the output-output covariance $\\widetilde{\\Sigma}_{\\mathbf{yy}}$ . This means that, in the vicinity of the origin saddle, GD steps of equal size on the equilibrate d energy will escape the saddle faster (at a rate depending on the output structure) than on the loss, and increasingly so as a function of depth. In $\\S4$ , we empirically verify this prediction experimentally on linear as well as non-linear architectures (including convolutional) trained on different datasets. ", "page_idx": 19}, {"type": "text", "text": "A.3.4 Example: 1-hidden layer linear network ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Here we show an example calculation comparing the Hessian at the origin of the loss and equilibrated energy for DLNs with a single hidden layer $H=1$ . For this case, the MSE loss and equilibrated energy are ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\mathcal{L}}=\\frac{1}{2N}\\sum_{i}^{N}||\\mathbf{y}_{i}-\\mathbf{W}_{2}\\mathbf{W}_{1}\\mathbf{x}_{i}||^{2}}}\\\\ {{\\displaystyle{\\mathcal{F}}^{*}=\\frac{1}{2N}\\sum_{i}^{N}(\\mathbf{y}_{i}-\\mathbf{W}_{2}\\mathbf{W}_{1}\\mathbf{x}_{i})^{T}(\\mathbf{I}_{d_{y}}+\\mathbf{W}_{2}\\mathbf{W}_{2}^{T})^{-1}(\\mathbf{y}_{i}-\\mathbf{W}_{2}\\mathbf{W}_{1}\\mathbf{x}_{i})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mathbf{x}\\in\\mathbb{R}^{d_{x}},\\mathbf{y}\\in\\mathbb{R}^{d_{y}},\\mathbf{W}_{1}\\in\\mathbb{R}^{n\\times d_{x}},\\mathbf{W}_{2}\\in\\mathbb{R}^{d_{y}\\times r}$ . We now show the weight gradients, first of the loss ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}_{1}}=\\mathbf{W}_{2}^{T}\\mathbf{W}_{2}\\mathbf{W}_{1}\\widetilde{\\pmb{\\Sigma}}_{\\mathbf{xx}}-\\mathbf{W}_{2}^{T}\\widetilde{\\pmb{\\Sigma}}_{\\mathbf{yx}}}\\\\ {\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}_{2}}=\\mathbf{W}_{2}\\mathbf{W}_{1}\\widetilde{\\pmb{\\Sigma}}_{\\mathbf{xx}}\\mathbf{W}_{1}^{T}-\\widetilde{\\pmb{\\Sigma}}_{\\mathbf{yx}}\\mathbf{W}_{1}^{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and then of the equilibrated energy ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial\\mathcal{F}^{*}}{\\partial\\mathbf{W}_{1}}=\\mathbf{W}_{2}^{T}\\mathbf{S}^{-1}\\mathbf{W}_{2}\\mathbf{W}_{1}\\widetilde{\\boldsymbol{\\Sigma}}_{\\mathbf{xx}}-\\mathbf{W}_{2}^{T}\\mathbf{S}^{-1}\\widetilde{\\boldsymbol{\\Sigma}}_{\\mathbf{yx}}}\\\\ &{\\displaystyle\\frac{\\partial\\mathcal{F}^{*}}{\\partial\\mathbf{W}_{2}}=\\mathbf{S}^{-1}(\\mathbf{W}_{2}\\mathbf{W}_{1}\\widetilde{\\boldsymbol{\\Sigma}}_{\\mathbf{xx}}-\\widetilde{\\boldsymbol{\\Sigma}}_{\\mathbf{yx}})\\mathbf{W}_{1}^{T}-\\mathbf{S}^{-1}\\boldsymbol{\\Psi}\\mathbf{S}^{-1}\\mathbf{W}_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we denote the empirical mean of the residual as $\\begin{array}{r}{\\Psi:=\\frac{1}{N}\\sum_{i}^{N}\\mathbf{r}_{i}\\mathbf{r}_{i}^{T}}\\end{array}$ . The origin is a critical point of the both the loss and the equilibrated energy since $\\mathbf{g}_{\\mathcal{L}}(\\dot{\\pmb{\\theta}}=\\overline{{\\mathbf{0}}})=\\dot{\\mathbf{g}}_{\\mathcal{F}^{*}}(\\pmb{\\theta}=\\mathbf{0})=\\mathbf{0}$ . We now compute the Hessian blocks, expressing the off-diagonals at the origin for simplicity, again first for ", "page_idx": 19}, {"type": "text", "text": "the loss ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\partial^{2}\\mathcal{L}}{\\partial\\mathbf{W}_{1}^{2}}=\\mathbf{W}_{2}^{T}\\mathbf{W}_{2}\\otimes\\widetilde{\\Sigma}_{\\mathbf{xx}}\\quad}\\\\ {\\frac{\\partial^{2}\\mathcal{L}}{\\partial\\mathbf{W}_{2}^{2}}=\\mathbf{I}_{d_{x}}\\otimes\\mathbf{W}_{1}\\widetilde{\\Sigma}_{\\mathbf{xx}}\\mathbf{W}_{1}^{T}}\\\\ {\\frac{\\partial^{2}\\mathcal{L}}{\\partial\\mathbf{W}_{2}\\partial\\mathbf{W}_{1}}(\\pmb{\\theta}=\\mathbf{0})=-\\mathbf{I}_{n}\\otimes\\widetilde{\\Sigma}_{\\mathbf{yx}}\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and then for the energy. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c}{\\displaystyle\\frac{\\partial^{2}\\mathcal{F}^{*}}{\\partial\\mathbf{W}_{1}^{2}}=\\mathbf{W}_{2}^{T}\\mathbf{S}^{-1}\\mathbf{W}_{2}\\otimes\\widetilde{\\Sigma}_{\\mathbf{x}\\mathbf{x}}}\\\\ {\\displaystyle\\frac{\\partial^{2}\\mathcal{F}^{*}}{\\partial\\mathbf{W}_{2}^{2}}=\\mathbf{S}^{-1}\\otimes\\mathbf{W}_{1}\\widetilde{\\Sigma}_{\\mathbf{x}\\mathbf{x}}\\mathbf{W}_{1}^{T}-\\mathbf{S}^{-1}\\Psi\\mathbf{S}^{-1}\\otimes\\mathbf{I}_{n}}\\\\ {\\displaystyle\\frac{\\partial^{2}\\mathcal{F}^{*}}{\\partial\\mathbf{W}_{2}\\partial\\mathbf{W}_{1}}(\\pmb{\\theta}=\\mathbf{0})=-\\mathbf{I}_{n}\\otimes\\widetilde{\\Sigma}_{\\mathbf{y}\\mathbf{x}}}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "At the origin, the Hessians become ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{H}_{\\mathcal{L}}(\\pmb{\\theta}=\\mathbf{0})=\\left[\\mathbf{-}\\mathbf{I}_{n}\\otimes\\widetilde{\\mathbf{\\Sigma}}_{\\mathbf{y}\\mathbf{x}}\\quad\\qquad\\mathbf{0}\\qquad\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{H}_{\\mathcal{F}^{*}}(\\pmb{\\theta}=\\mathbf{0})=\\left[\\begin{array}{c c}{\\mathbf{0}}&{-\\widetilde{\\pmb{\\Sigma}}_{\\mathbf{x}\\mathbf{y}}\\otimes\\mathbf{I}_{n}}\\\\ {-\\mathbf{I}_{n}\\otimes\\widetilde{\\pmb{\\Sigma}}_{\\mathbf{y}\\mathbf{x}}}&{-\\widetilde{\\pmb{\\Sigma}}_{\\mathbf{y}\\mathbf{y}}\\otimes\\mathbf{I}_{n}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "A.3.5 Hessian of the equilibrated energy for linear chains ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Here we include a derivation the Hessian of the equilibrated energy (as well as its eigenstructure at the origin) for linear chains or networks of unit width $w_{L:1x}$ where $n_{0}=\\cdot\\cdot\\cdot=n_{L}=1$ . This follows the derivation for the wide case (\u00a7A.3.3), but it reveals all the key insights and is easier to follow. For the scalar case, the implicit generative model of the target defined by PC (see $\\S\\mathrm{A}.3.2)$ is ", "page_idx": 20}, {"type": "equation", "text": "$$\ny\\sim\\mathcal{N}\\left(w_{L:1}x,1+\\sum_{\\ell=2}^{L}(w_{L:\\ell})^{2}\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "leading to the following rescaled loss ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{F}^{*}=\\mathcal{L}/s,\\quad s=1+\\sum_{\\ell=2}^{L}(w_{L:\\ell})^{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{L}=\\frac{1}{2N}\\sum_{i}^{N}(y_{i}-w_{L:1}x_{i})^{2}}\\end{array}$ . The weight gradient of the equilibrated energy is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathcal{F}^{*}}{\\partial w_{i}}=\\left\\{\\begin{array}{l l}{\\frac{1}{s}\\frac{\\partial\\mathcal{L}}{\\partial w_{i}},}&{i=1}\\\\ {}\\\\ {\\frac{1}{s}\\frac{\\partial\\mathcal{L}}{\\partial w_{i}}-\\frac{1}{s^{2}}\\mathcal{L}\\frac{\\partial s}{\\partial w_{i}},}&{i>1}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the loss gradient is $\\partial\\mathcal{L}/\\partial w_{i}=-w_{L:1\\neq i}x r$ with residual error $r=(y-w_{L:1}x)$ . As shown in $\\S\\mathrm{A}.3.2$ , The origin is a critical point of both the loss and the equilibrated energy since their gradients are zero $\\mathbf{g}_{\\mathcal{L}}(\\pmb{\\theta}=\\mathbf{0})=\\mathbf{0},\\mathbf{g}_{\\mathcal{F}^{*}}(\\pmb{\\theta}=\\mathbf{0})=\\mathbf{0}$ . We now show the Hessians, first of the loss ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}\\mathcal{L}}{\\partial w_{i}\\partial w_{j}}=\\left\\{\\begin{array}{l l}{(w_{L:1\\neq i})^{2}x^{2},}&{i=j}\\\\ {\\quad}\\\\ {(w_{L:1\\neq i,j})(2w_{L:1}x^{2}-x y),}&{i\\neq j}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and then of the energy. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}\\mathcal{F}^{*}}{\\partial w_{i}\\partial w_{j}}=\\left\\{\\begin{array}{l l}{\\frac{1}{s}\\frac{\\partial^{2}\\mathcal{L}}{\\partial w_{i}\\partial w_{j}},}&{i=j=1}\\\\ {\\frac{1}{s}\\frac{\\partial^{2}\\mathcal{L}}{\\partial w_{i}\\partial w_{j}}-\\frac{1}{s^{2}}\\frac{\\partial\\mathcal{L}}{\\partial w_{i}}\\frac{\\partial s}{\\partial w_{j}},}&{i=1,\\quad j>1}\\\\ {\\frac{1}{s}\\frac{\\partial^{2}\\mathcal{L}}{\\partial w_{i}\\partial w_{j}}-\\frac{1}{s^{2}}\\frac{\\partial\\mathcal{L}}{\\partial w_{i}}\\frac{\\partial s}{\\partial w_{j}}+\\frac{1}{s^{2}}\\frac{\\partial\\mathcal{L}}{\\partial w_{j}}\\frac{\\partial s}{\\partial w_{i}}+\\frac{1}{s^{2}}\\mathcal{L}\\frac{\\partial^{2}s}{\\partial w_{i}\\partial w_{j}}-\\frac{2}{s^{3}}\\frac{\\partial s}{\\partial w_{j}}\\mathcal{L}\\frac{\\partial s}{\\partial w_{i}},}&{i,j>1}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Generalising the one-hidden-unit case shown by [18], at the origin the Hessians reduce to ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{H}_{\\mathcal{L}}(\\pmb{\\theta}=\\mathbf{0})=\\left\\{\\begin{array}{l l}{\\left[\\begin{array}{c c}{0}&{-x y}\\\\ {-x y}&{0}\\end{array}\\right],}&{H=1\\quad[\\mathrm{strict~saddle}]}\\\\ {\\left[\\begin{array}{c c c}{0}&{\\ldots}&{0}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {0}&{\\ldots}&{0}\\end{array}\\right]=\\mathbf{0}_{p},}&{H>1\\quad[\\mathrm{non-strict~saddle}]}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{H}_{\\mathcal{F}^{*}}(\\theta=\\mathbf{0})=\\left\\{\\begin{array}{l l}{\\left[\\begin{array}{l l}{0}&{-x y}\\\\ {-x y}&{-y^{2}}\\end{array}\\right],}&{H=1\\quad[\\mathrm{better-conditioned~strict~saddle}]}\\\\ {\\left[\\begin{array}{l l l}{0}&{\\dots}&{0}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {0}&{\\dots}&{-y^{2}}\\end{array}\\right]}&{H>1\\quad[\\mathrm{strict~saddle}]}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For one-hidden-layer networks $H=1$ , the Hessian eigenvalues of the loss and energy are $\\lambda(\\mathbf{H}_{\\mathcal{L}}(\\pmb{\\theta}=$ ${\\bf0}))=\\pm x y,\\lambda({\\bf H}_{\\mathcal{F}^{*}}(\\pmb{\\theta}={\\bf0}))=(-y^{2}\\pm y\\sqrt{4x^{2}+y^{2}})/2$ , respectively. In this case, the eigenvalues of the energy turn out to be smaller than those of the loss ", "page_idx": 21}, {"type": "equation", "text": "$$\nH=1,\\quad\\lambda(\\mathbf{H}_{\\mathcal{F}^{\\ast}}(\\pmb{\\theta}=\\mathbf{0}))<\\lambda(\\mathbf{H}_{\\mathcal{L}}(\\pmb{\\theta}=\\mathbf{0})),\\quad\\forall x,y\\neq0\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "following from the fact that the square root of a sum is smaller than the sum of the square roots, $\\sqrt{a^{2}+b^{2}}<\\sqrt{a^{2}}+\\sqrt{b^{2}},\\forall a,b\\neq0$ . This means that, in this particular case, the strict saddle of the equilibrated energy is better conditioned (i.e. easier to escape) than that of the loss. For deeper networks, the Hessian of the loss is zero, and it is easy to see that that of the energy has zero eigenvalues of multiplicity $L-1$ and a single negative eigenvalue given by the target squared. ", "page_idx": 21}, {"type": "equation", "text": "$$\nH>1,\\quad\\lambda_{\\mathrm{min}}(\\mathbf{H}_{\\mathcal{F}^{\\ast}}(\\pmb{\\theta}=\\mathbf{0}))=-y^{2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "A.3.6 Strictness of zero-rank saddles of the equilibrated energy ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Here we prove the strictness of the zero-rank saddles of the equilibrated energy (Theorem 3). It is easy to check via Eqs. 31 & 32 that any point $\\pmb{\\theta}^{*}$ such that $\\mathbf{\\Delta}\\mathbf{W}_{L}=\\mathbf{0},\\mathbf{W}_{L-1:1}=\\mathbf{0},$ ) is a critical point. Now let\u2019s prove that the Hessian at $\\pmb{\\theta}^{*}$ has a negative eigenvalue. To do this, we rely on the Taylor expansion of the function around $\\pmb{\\theta}^{*}$ . Since $\\mathbf{g}_{\\mathcal{F}^{*}}(\\pmb{\\theta}^{*})=\\mathbf{0}$ , we have for any $\\hat{\\pmb\\theta}$ and any $\\delta\\rightarrow0$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{F}^{*}(\\pmb{\\theta}^{*}+\\delta\\hat{\\pmb{\\theta}})=\\mathcal{F}^{*}(\\pmb{\\theta}^{*})+\\frac{1}{2}\\delta^{2}\\hat{\\pmb{\\theta}}^{T}{\\bf H}_{\\mathcal{F}^{*}}(\\pmb{\\theta}^{*})\\hat{\\pmb{\\theta}}+\\mathcal{O}(\\delta^{3})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Hence by unicity of the Taylor expansion, if we can find $\\hat{\\pmb\\theta}$ such that $\\mathcal{F}^{*}(\\pmb{\\theta}^{*}+\\delta\\hat{\\pmb{\\theta}})=\\mathcal{F}^{*}(\\pmb{\\theta}^{*})-c\\delta^{2}+$ ${\\mathcal{O}}(\\delta^{3})$ where $c>0$ , this would mean that $\\hat{\\pmb{\\theta}}^{T}{\\bf H}_{\\mathcal{F}^{\\ast}}(\\pmb{\\theta}^{\\ast})\\hat{\\pmb{\\theta}}=-2c<0$ and therefore that it is a strict saddle point. By considering the direction of perturbation $\\hat{\\pmb{\\theta}}=(\\mathbf{I},\\mathbf{0},\\dots,\\mathbf{0})$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{F}^{*}(\\boldsymbol{\\theta}^{*}+\\delta\\hat{\\boldsymbol{\\theta}})=\\mathcal{F}^{*}(\\delta\\mathbf{I},\\mathbf{W}_{L-1},\\ldots,\\mathbf{W}_{1})}\\\\ &{\\quad\\quad\\quad\\quad=\\displaystyle\\sum_{i=1}^{N}\\mathbf{y}_{i}^{T}\\left(\\mathbf{I}+\\delta^{2}\\left(\\mathbf{I}+\\sum_{\\ell=2}^{L-1}\\mathbf{W}_{L-1:\\ell}\\mathbf{W}_{L-1:\\ell}^{T}\\right)\\right)^{-1}\\mathbf{y}_{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Denoting by $\\begin{array}{r}{\\mathbf{A}:=\\mathbf{I}+\\sum_{\\ell=2}^{L-1}\\mathbf{W}_{L-1:\\ell}\\mathbf{W}_{L-1:\\ell}^{T}.}\\end{array}$ , we have when $\\delta\\rightarrow0$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{S}^{-1}=(\\mathbf{I}+\\delta^{2}\\mathbf{A})^{-1}=\\mathbf{I}-\\delta^{2}\\mathbf{A}+\\mathcal{O}(\\delta^{3})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{F}^{*}(\\delta\\mathbf{I},\\mathbf{W}_{L-1},\\ldots,\\mathbf{W}_{1})=\\displaystyle\\sum_{i=1}^{N}\\mathbf{y}_{i}^{T}(\\mathbf{I}-\\delta^{2}\\mathbf{A}+\\mathcal{O}(\\delta^{3}))\\mathbf{y}_{i}}\\\\ &{\\quad\\quad\\quad\\quad=\\displaystyle\\sum_{i=1}^{N}\\mathbf{y}_{i}^{T}\\mathbf{y}_{i}-\\delta^{2}\\displaystyle\\sum_{i=1}^{L}\\mathbf{y}_{i}^{T}\\mathbf{A}\\mathbf{y}_{i}+\\mathcal{O}(\\delta^{3})}\\\\ &{=\\mathcal{F}^{*}(\\mathbf{W}_{L},\\mathbf{W}_{L-1},\\ldots,\\mathbf{W}_{1})-c\\delta^{2}+\\mathcal{O}(\\delta^{3})}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\begin{array}{r}{c=\\sum_{i=1}^{L}y_{i}^{T}\\mathbf{A}y_{i}>0}\\end{array}$ because $\\mathbf{A}$ is symmetric definite positive and there exists $j$ such that $y_{j}\\neq0$ . He nce ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{F}^{*}(\\pmb{\\theta}^{*}+\\delta\\hat{\\pmb{\\theta}})=\\mathcal{F}^{*}(\\pmb{\\theta}^{*})-c\\delta^{2}+\\mathcal{O}(\\delta^{3})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 22}, {"type": "text", "text": "A.3.7 Flatter global minima of the equilibrated energy (linear chains) ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Here we present a preliminary investigation into the minima of the equilibrated energy compared to the MSE loss. For linear chains (\u00a7A.3.5), we show that global minima of the equilibrated energy are flatter than those of the MSE loss. More precisely, the energy global minima turn out be scaled down versions of those of the loss by the same rescaling factor of the equilibrated energy (\u00a7A.3.2). This generalises the result of [18] for linear chains with a single hidden unit. ", "page_idx": 22}, {"type": "text", "text": "The proof has only two steps and does not require explicit calculation of the Hessian. First, we know that we are at a global minimum of loss when we perfectly fti the data $w_{L:1}x=y$ , since $\\mathcal{L}(w_{L:1}x=$ $y)=0$ . This is also true of the equilibrated energy, $\\mathcal{F}^{*}(\\stackrel{.}{w}_{L:1}x=y)=0$ . We can check that these are critical points by seeing that the weight gradient of the loss is null, $\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}(w_{L:1}x=y)=\\mathbf{0}$ , which follows from the fact that the residual is zero when we perfectly fit the data. Again, the same is true of the energy, $\\nabla_{\\boldsymbol{\\theta}}\\mathcal{F}^{*}(w_{L:1}x=y)=\\mathbf{0}$ . ", "page_idx": 22}, {"type": "text", "text": "The second and last step is to realise that, at these minima, the terms of the energy Hessian (Eq. 59) collapse to those of a rescaled loss Hessian (Eq. 58). ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}\\mathcal{F}^{*}}{\\partial w_{i}\\partial w_{j}}(w_{L:1}x=y)=\\left\\{\\frac{1}{s}\\frac{\\partial^{2}\\mathcal{L}}{\\partial w_{i}\\partial w_{j}},\\quad i=j=1\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the rescaling is the same as that of the equilibrated energy (Eq. 56). Factoring out the rescaling ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{H}_{\\mathcal{F}*}(w_{L:1}x=y)=\\mathbf{H}_{\\mathcal{L}}(w_{L:1}x=y)/s}\\\\ {\\implies\\mathbf{H}_{\\mathcal{F}*}(w_{L:1}x=y)<\\mathbf{H}_{\\mathcal{L}}(w_{L:1}x=y)\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "we observe that the minima of the equilibrated energy are simply a rescaled version of those of the loss. As we saw in $\\S\\mathrm{A}.3.2$ , the rescaling is positive, so it follows that the global minima of the equilibrated energy are flatter or, to put it another way, PC inference has the effect of flattening the global minima of the MSE loss (at least for linear chains). ", "page_idx": 22}, {"type": "text", "text": "A.4 Experimental details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Code to reproduce all the experiments is available at https://github.com/ francesco-innocenti/pc-saddles. Unless otherwise stated, for all PC networks standard Euler integration with step size $d t=0.1$ was used to run the inference dynamics to equilibrium (\u00a72.2, Eq. 3), with the number of iterations depending on the problem. ", "page_idx": 22}, {"type": "text", "text": "Theoretical energy (Figure 1). We trained DLNs with different number of hidden layers $H\\in$ $\\{2,5,10\\}$ on standard image classification datasets (MNIST, Fashion-MNIST and CIFAR10). At every training step, we compared the total energy (Eq. 2) at the numerical inference equilibrium $\\mathcal{F}|_{\\Delta\\mathbf{z}\\approx\\boldsymbol{0}}$ with the theoretical prediction (Eq. 5). The following hyperparameters were used for all networks: 300 hidden units and SGD with learning rate $\\eta=1\\overline{{e}}^{-3}$ and batch size $b=64$ . We used a second-order explicit Runge\u2013Kutta ODE solver (Heun) with a maximum upper integration limit $T=300$ and an adaptive Proportional-Integral-Derivative controller (absolute and relative tolerances: $1e^{-3}$ ) to ensure convergence of the PC inference dynamics (Eq. 3). Results were consistent across different random initialisations. ", "page_idx": 23}, {"type": "text", "text": "Toy examples (Figure 2). All networks were linear and trained on a toy regression problem using the MSE loss (Eq. 1) and energy (Eq. 2) with output $\\mathbf{y}\\,=\\,-\\mathbf{x},\\mathbf{x}\\,\\sim\\dot{\\mathcal{N}(1,0.1)}$ . Weights were initialised close to the origin $\\bar{\\mathbf{W}_{i j}}\\sim\\bar{\\mathcal{N}}(0,\\sigma^{2})$ with $\\sigma\\ll1$ . For the chains, the initialisation scale was chosen to be $\\sigma\\,=\\,5e^{-2}$ , while for the wide network it was increased to $\\sigma\\,=\\,1e^{-1}$ in order to make escape from the saddle faster but still visible. For PC, $T=20$ inference iterations were used for chains and 50 for the wide network. All networks were trained with SGD and batch size $b=64$ . Learning rate $\\eta=0.4$ was used for the chains and $1e^{-3}$ for the wide network. Training was stopped when it was determined that convergence had been effectively reached, to allow for intuitive visualisation of the loss dynamics. ", "page_idx": 23}, {"type": "text", "text": "The landscapes were sampled on the training loss or energy with a $30\\times30$ resolution and domain $\\in[-2,2]$ for the 2-hidden node chain and $\\in[-1,1]$ for the other networks. For the wide network, the landscape was projected onto the maximum and minimum eigenvectors of the Hessian at the origin ${\\boldsymbol{\\theta}}^{*}=\\mathbf{0}$ , $f(\\bar{\\pmb{\\theta}}^{*}\\bar{+}\\alpha\\hat{\\mathbf{v}}_{\\mathrm{min}}+\\beta\\hat{\\mathbf{v}}_{\\mathrm{max}})$ since as shown by [7] random directions [28] can fail to identify saddle points. The energy landscape was plotted at the numerical equilibrium ${\\mathcal{F}}^{*}(\\pmb{\\theta})$ . Figure 2 displays results for an example run, and Figure 8 shows the statistics of the training and test losses as well as gradient norms for 5 random initialisations. ", "page_idx": 23}, {"type": "text", "text": "Hessian eigenspectra (Figure 3-4). For different linear network architectures, we computed the Hessian of the loss and equilibrated energy at the origin on a random batch (size $b=64$ ) of a given dataset. The datasets used were (i) a toy Gaussian with 3D input and output with the same statistics used for experiments in Figure 2, (ii) MNIST and (iii) MNIST-1D [16], a procedurally generated, one-dimensional dataset smaller than MNIST with higher model discriminability. The depth, width and data dimensions of the networks tested on the Gaussian data are clear from the vignettes in Figure 3. Figure 9 shows the same results for linear chains. For MNIST and MNIST-1D, networks with $H$ hidden layers $\\{1,2,3\\}$ had $n_{\\ell}$ widths $\\{10,10,5\\}$ and $\\{100,50,10\\}$ , respectively. Note that the MNIST networks were relatively narrow to allow for tractable computation of the Hessian. The Hessian matrices for the Gaussian data were normalised between 1 and $^{-1}$ , and the Hessian of the energy was computed after $T=50$ inference iterations. For the theoretical eigenspectra of the energy Hessian, we computed the eigenvalues of Eq. 8. Figures 3 and 4 show results for an example run, and we found practically indistinguishable results for different seeds. Figures 9 & 10 show a similar analysis for a zero-rank saddle covered by Theorem 3 other than the origin. ", "page_idx": 23}, {"type": "text", "text": "Experiments (Figure 5-6). For the first set of experiment, we trained and tested linear, Tanh and ReLU networks on standard image classification tasks. Networks tested on MNIST and FashionMNIST had 5 fully connected (FC) layers with 500 hidden units, while those trained on CIFAR-10 had a convolutional architecture consisting of 3 blocks (with a convolution and max pooling operation) followed by two FC layers (with the last one always being linear). For PC, $T\\,=\\,50$ inference iterations were used. Similar to the experiments for Figure 2, all networks were initialised close to the origin $\\mathbf{W}_{i j}\\sim\\mathcal{N}(0,\\sigma^{2})$ with $\\sigma=5e^{-3}$ . SGD with batch size 64 and learning rate $\\eta=1e^{-3}$ was used for all networks. PC networks were trained until the training loss reached a tolerance threshold $\\mathcal{L}_{\\mathrm{train}}<1e^{-3}$ . For computational reasons, the BP-trained networks were not trained until convergence. Instead, training was stopped at as many iterations as it took PC to converge. We do report the full saddle escape dynamic for the toy examples in Figure 2 and the matrix completion experiment in Figure 6. All hyperparameters except for the initialisation remained unchanged for the other (zero-rank) saddle experiment shown in Figure 12. ", "page_idx": 23}, {"type": "text", "text": "For the matrix completion task (Figure 6), we attempted to replicate the experiment by [19, Figure 1] as closely as possible. Networks of depth $H=3$ and width $n_{\\ell}\\,=\\,100$ were trained with GD and learning rate $\\eta=1e^{-2}$ to fit a $10\\mathrm{x}10$ matrix of rank 3. The target matrix was generated by multiplying two i.i.d. matrices of size 10x3 with standard Gaussian entries, and $20\\%$ of these entries were masked during training. The networks trained with PC were initialised at each saddle visited by BP, which was determined numerically by computing the rank of the network map. The origin initialisation had the same scale $\\sigma=5e^{-3}$ used in the previous experiments. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "image", "img_path": "eTu6kvrkSq/tmp/de3da812dff286f2af559bd5e899e4ee8949bc0af8333a4fb7251f39d266c94e.jpg", "img_caption": ["A.5 Supplementary results "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 7: Training and test statistics for linear networks of Figure 2. For each network, we plot the mean and $\\pm1$ standard deviation of the training loss, test loss and gradient norm over 5 random initialisations. For the wide network, the test loss is evaluated once every epoch (rather than for each batch), and the training metrics are plotted on a log axis for easier visualisation. For the chain with two hidden units, the multiple loss plateaus and corresponding gradient spikes are due to different escape times from the saddle for different runs. ", "page_idx": 24}, {"type": "image", "img_path": "eTu6kvrkSq/tmp/499b36492632097a247abb39098b0a8f19dbe54ccad9b7c1496c30078ba6e43f.jpg", "img_caption": ["Figure 8: Empirical verification of the Hessian at the origin of the equilibrated energy for linear chains. This shows the same results of Figure 3 for networks of unit width $n_{0}=\\cdot\\cdot\\cdot=n_{L}=1$ (see $\\S\\mathrm{A}.4$ for details). Again, we observe a perfect match between theory and experiment (see in particular Eq. 61). "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "eTu6kvrkSq/tmp/32673df6e114994a07d983891bfe91fb1786f507284c77ec64fab82364d06ead.jpg", "img_caption": ["Figure 9: Empirical verification of a strict zero-rank saddle of the equilibrated energy other than the origin for DLNs tested on a toy dataset. We show the Hessian eigenspectrum of the MSE loss and equilibrated energy at a strict saddle other than the origin covered by Theorem 3, specifically for the critical point where all weight matrices except the penultimate are zero $\\pmb{\\theta}^{*}(\\mathbf{W}_{\\ell}=\\bar{\\mathbf{0}_{*}}\\forall\\ell\\neq L\\-1)$ . We do not show the loss Hessians because they are zero for $H>1$ (Eq. 6). The target is the same as used for Figure 3, and in the right panel one of the output dimensions is varied to be $y_{2}=x_{2}$ . Figure 10 shows results for the same critical point on MNIST and MNIST-1D. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "eTu6kvrkSq/tmp/740159f4361ccfab8f9d4e2eed91be74b5aeef4852fdcf9bc97d711b7c2aff61.jpg", "img_caption": ["Figure 10: Empirical verification of a strict zero-rank saddle of the equilibrated energy other than the origin for DLNs tested on more realistic datasets. This shows similar results to Figure 9 for the more realistic datasets MNIST and MNIST-1D. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "eTu6kvrkSq/tmp/bd64a1bdc5481e2d5ff107b29b308705c799782b296c5a9b98d0649b3ccc8ce4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 11: No vanishing gradients for PC starting near the origin. Weight gradient norms $||\\partial\\pmb{\\theta}||_{2}$ of the loss (BP) and equilibrated energy (PC) for the experiments in Figure 5. ", "page_idx": 26}, {"type": "image", "img_path": "eTu6kvrkSq/tmp/ffd98737d0a0c10ca6f4b4534af76deb4100ecf785450ebcb6a7122aa77e97e3.jpg", "img_caption": ["Figure 12: PC escapes another non-strict saddle of the loss much faster than BP with SGD on non-linear networks. This shows the same results as Figure 5 for the same saddle analysed in Figures 9 & 10 (see $\\S\\mathrm{A}.4$ for details). We show results for an example run as they were practically indistinguishable across different random seeds. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We clearly state our claims in the abstract and introduction, based on both theoretical and empirical results. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We dedicate a full section in the discussion (\u00a75.2) to the main limitations of this work. We highlight the most important limitation in the abstract. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: As we emphasise throughout the paper, linearity (of the activation function of deep neural networks) is the only major assumption made by our theoretical analysis, and we perform thorough experiments showing that the theory holds for non-linear networks. We provide proofs and derivations of all the theoretical results in the Appendix (\u00a7A.3) and give intuition for the proofs in the main text. We also include some pedagogical derivations (\u00a7A.3.4, $\\S\\mathrm{A}.3.5\\substack{,}$ . ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide all the details necessary to reproduce all the experimental results in the Appendix (A.4). ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 29}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide access to the code that can be used to reproduce all the experimental results. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We specify important specifications of the experiments in the main text and all other details in the Appendix (\u00a7A.4). ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: In most cases we do not report error bars because results were consistent or practically indistinguishable across runs and including them would not enhance (or even confuse) understanding. Figure captions always specify whether results are shown for an example run or seed. When error bars are included (Figure 7), we use $\\pm1$ standard deviation, over different runs or random seeds. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [No] ", "page_idx": 31}, {"type": "text", "text": "Justification: Most experimental results can be reproduced in a few hours on a CPU, with the exception of those related to Figures 5 & 12 which were run on a GPU (typically A100). Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The research conforms to the NeurIPS Code of Ethics. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA]   \nJustification: Our work does not have any direct societal impact, positive or negative. Guidelines:   \n\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 32}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA]   \nJustification:   \nGuidelines: \u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]