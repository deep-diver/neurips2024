{"references": [{"fullname_first_author": "Colin Raffel", "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "publication_date": "2020-MM-DD", "reason": "This paper is foundational to the field of large language models (LLMs), introducing a unified text-to-text transformer approach which the current paper builds upon."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-MM-DD", "reason": "This paper demonstrates the few-shot learning capabilities of LLMs, a key concept leveraged in the continual learning framework proposed in the current paper."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "publication_date": "2024-MM-DD", "reason": "This paper introduces the concept of visual instruction tuning for multimodal LLMs, which the current paper extends to other modalities and integrates into a continual learning framework."}, {"fullname_first_author": "Junnan Li", "paper_title": "BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "publication_date": "2023-MM-DD", "reason": "BLIP-2 is a highly influential multimodal LLM that serves as a basis for the architecture and approach of the current paper.  The current work adapts and extends BLIP-2 for continual multi-modal learning."}, {"fullname_first_author": "Artemis Panagopoulou", "paper_title": "X-InstructBLIP: A framework for aligning X-modal instruction-aware representations to LLMs and emergent cross-modal reasoning", "publication_date": "2023-MM-DD", "reason": "X-InstructBLIP provides a unified architecture for handling multiple modalities in LLMs; this is directly used and extended in the current paper's proposed architecture."}]}