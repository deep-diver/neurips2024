[{"figure_path": "drpJ7KOr3F/figures/figures_1_1.jpg", "caption": "Figure 1: Comparisons of Different Multimodal LLMs: (a) The normal multimodal methods [21, 23, 22] require unified sampling across multi-modal. (b) Our proposed incremental MLLMs learns each modality sequentially without joint-modal datasets.", "description": "This figure compares two approaches for training multimodal large language models (MLLMs).  (a) shows the traditional approach where all modalities are trained together using a unified sampling method. This requires a large joint dataset encompassing all modalities.  (b) presents the proposed incremental approach, where each modality is learned sequentially. This requires only unimodal datasets for each modality, making it more efficient and scalable.", "section": "1 Introduction"}, {"figure_path": "drpJ7KOr3F/figures/figures_3_1.jpg", "caption": "Figure 2: Overall framework of PathWeave. We start from a pretrained vision LLM [20] and progressively expand new modalities on it without acquiring historical data. Given input samples from modality m, we first exploit a frozen encoder (Em) for feature extraction and leverage Q-Former to achieve multimodal alignment with LLMs. Then, the Adapter-in-Adapter (AnA) module is implemented in Q-Former to achieve flexible modal-path switching and expansion. In detail, the uni-modal adapters (Am) are implemented in parallel to facilitate new modal plasticity, which will be frozen once trained. While the cross-modal adapters (\u00c2m) are formed by inserting a set of in-adapters ({F}m\u012b\u00b9) into the learned uni-adapters to enhance the collaboration of historical knowledge. Additionally, an MoE-based gating module (Gm) is implemented among uni-adapters to adaptively multimodal integration in input space.", "description": "This figure illustrates the architecture of PathWeave, a framework for continually evolving Large Language Models (LLMs) on modality.  It starts with a pre-trained vision LLM and incrementally adds new modalities using uni-modal data and a novel Adapter-in-Adapter (AnA) mechanism. AnA consists of uni-modal adapters (trained on single modality data and frozen after training), and cross-modal adapters (built upon the uni-modal adapters for better cross-modal interaction). A Mixture of Experts (MoE) gating mechanism further improves the interaction between modalities.", "section": "3 Path Weave"}, {"figure_path": "drpJ7KOr3F/figures/figures_8_1.jpg", "caption": "Figure 2: Overall framework of PathWeave. We start from a pretrained vision LLM [20] and progressively expand new modalities on it without acquiring historical data. Given input samples from modality m, we first exploit a frozen encoder (Em) for feature extraction and leverage Q-Former to achieve multimodal alignment with LLMs. Then, the Adapter-in-Adapter (AnA) module is implemented in Q-Former to achieve flexible modal-path switching and expansion. In detail, the uni-modal adapters (Am) are implemented in parallel to facilitate new modal plasticity, which will be frozen once trained. While the cross-modal adapters (\u00c2m) are formed by inserting a set of in-adapters ({F}m\u012b\u00b9) into the learned uni-adapters to enhance the collaboration of historical knowledge. Additionally, an MoE-based gating module (Gm) is implemented among uni-adapters to adaptively multimodal integration in input space.", "description": "This figure illustrates the architecture of PathWeave, a framework for incrementally expanding multimodal large language models.  It shows how a pre-trained vision LLM is used as a base, with new modalities added via adapter modules. Uni-modal adapters process single-modality data, while cross-modal adapters integrate information from previous modalities. A gating mechanism further refines the interaction between modalities.", "section": "3 Path Weave"}, {"figure_path": "drpJ7KOr3F/figures/figures_8_2.jpg", "caption": "Figure 1: Comparisons of Different Multimodal LLMs: (a) The normal multimodal methods [21, 23, 22] require unified sampling across multi-modal. (b) Our proposed incremental MLLMs learns each modality sequentially without joint-modal datasets.", "description": "This figure compares two approaches for training multimodal large language models (MLLMs).  (a) shows the traditional method, which requires joint training with data from all modalities at once. This is computationally expensive and inefficient when adding new modalities. (b) illustrates the proposed incremental method, PathWeave, which learns each modality sequentially using uni-modal data.  This incremental approach makes it more scalable and efficient to add new modalities to the model.", "section": "1 Introduction"}, {"figure_path": "drpJ7KOr3F/figures/figures_17_1.jpg", "caption": "Figure 1: Comparisons of Different Multimodal LLMs: (a) The normal multimodal methods [21, 23, 22] require unified sampling across multi-modal. (b) Our proposed incremental MLLMs learns each modality sequentially without joint-modal datasets.", "description": "This figure compares two different approaches to multimodal large language models (MLLMs).  (a) shows the traditional approach where all modalities (image, video, audio, etc.) are processed together using a unified sampling method. This requires large, joint-modal datasets for training, which is computationally expensive and limits scalability to new modalities. (b) shows the proposed incremental MLLM approach (PathWeave), which learns each modality sequentially using uni-modal data. This makes the model more flexible and scalable, as it doesn't require massive joint-modal datasets to incorporate new modalities.", "section": "1 Introduction"}]