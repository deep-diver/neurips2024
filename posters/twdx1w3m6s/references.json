{"references": [{"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper is foundational for RLHF and introduces the standard RLHF framework widely adopted in the field."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-01", "reason": "This paper presents Llama 2, a significant open-source LLM that has significantly influenced the field and is directly referenced in the paper for model comparisons."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023-05-01", "reason": "This paper proposes Direct Preference Optimization (DPO), an alternative to the standard reward-based RLHF that is more efficient and is compared against in this paper."}, {"fullname_first_author": "Wei Xiong", "paper_title": "Iterative preference learning from human feedback: Bridging theory and practice for RLHF under KL-constraint", "publication_date": "2023-12-01", "reason": "This paper is closely related to the current work, focusing on theoretical learnability of RLHF under general preference oracles and KL regularization, similar to this paper."}, {"fullname_first_author": "Miroslav Dud\u00edk", "paper_title": "Contextual dueling bandits", "publication_date": "2015-06-01", "reason": "This paper provides theoretical background on contextual dueling bandits, which is relevant to the theoretical framework of the current paper."}]}