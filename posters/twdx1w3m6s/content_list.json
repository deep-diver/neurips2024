[{"type": "text", "text": "Online Iterative Reinforcement Learning from Human Feedback with General Preference Model ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Chenlu Ye\u2217\u2020 Wei Xiong\u2217\u2021 Yuheng Zhang\u2217\u00a7 Hanze Dong\u2217\u00b6 ", "page_idx": 0}, {"type": "text", "text": "Nan Jiang\u2225 Tong Zhang\u2217\u2217 ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We investigate Reinforcement Learning from Human Feedback (RLHF) in the context of a general preference oracle. In particular, we do not assume the existence of a reward function and an oracle preference signal drawn from the Bradley-Terry model as most of the prior works do. We consider a standard mathematical formulation, the reverse-KL regularized minimax game between two LLMs for RLHF under general preference oracle. The learning objective of this formulation is to find a policy so that it is consistently preferred by the KL-regularized preference oracle over any competing LLMs. We show that this framework is strictly more general than the reward-based one, and propose sample-efficient algorithms for both the offilne learning from a pre-collected preference dataset and online learning where we can query the preference oracle along the way of training. Empirical studies verify the effectiveness of the proposed framework. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal technique in adapting machine learning to leverage relative feedback, especially in aligning Large Language Models (LLMs) with human values and preferences [14, 90]. Notable examples include ChatGPT [49], Claude [2], and Bard [29]. The primary goal of RLHF in the context of LLMs is to adjust the responses generated by LLMs so that they are more favorably received by human evaluators. ", "page_idx": 0}, {"type": "text", "text": "Inspired by the standard LLM alignment workflow [50, 5, 60], we characterize an LLM by a policy $\\pi$ , which takes a prompt $x\\in\\mathscr{X}$ and produces a response $a\\in A$ from the distribution $\\pi(\\cdot|x)$ . In a typical LLM training pipeline [50, 60, 49], the tuning process begins with a pretrained model, which is subsequently fine-tuned using specialized and instructional data to produce an initial model $\\pi_{0}$ The initial model $\\pi_{0}$ is then aligned with a prompt set from some distribution $x\\,\\sim\\,d_{0}$ . The key component in RLHF is the General Preference Oracle, which is mathematically defined as follows. ", "page_idx": 0}, {"type": "text", "text": "Definition 1 (General Preference Oracle). There exists a preference oracle $\\mathbb{P}:\\mathcal{X}\\times\\mathcal{A}\\times\\mathcal{A}\\rightarrow[0,1],$ , and we can query it to receive the preference signal: ", "page_idx": 0}, {"type": "equation", "text": "$$\ny\\sim\\operatorname{Ber}\\bigl(\\mathbb{P}(a^{1}\\succ a^{2}|x,a^{1},a^{2})\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "Instead of directly optimizing against the preference oracle $\\mathbb{P}$ , the existing prevalent RLHF framework is reward-based [50, 60], which consists of three steps: (1) preference data collection, (2) reward modeling, and (3) policy optimization. Specifically, the preference dataset $\\mathcal{D}$ consists of multiple tuples of the form $\\bar{(x,a^{1},a^{2},y)}$ , whose collection process can be modeled as: ", "page_idx": 1}, {"type": "equation", "text": "$$\nx\\sim d_{0},a^{1}\\sim\\pi_{D}^{1},a^{2}\\sim\\pi_{D}^{2},\\qquad y\\sim\\mathrm{Ber}\\big(\\mathbb{P}(a^{1}\\sim a^{2}|x,a^{1},a^{2})\\big),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\pi_{D}^{1}$ and $\\pi_{D}^{2}$ are behavior policies and are typically set as $\\pi_{0}$ [60, 43] or some powerful closedform LLMs [16]. The second step is reward modeling, which is the origin of the name \u201creward-based\u201d. This step can be viewed as a kind of inverse RL [89], which models some difficult-to-specify goals (preferred by the human or AI evaluators) as a scalar reward signal. Specifically, the Bradley-Terry (BT) model [9], a framework widely adopted in Ouyang et al. [50], Bai et al. [4], Touvron et al. [60], Rafailov et al. [53], Xiong et al. [72], assumes that there exists a ground-truth reward function $P^{*}$ and the preference model satisfies: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbb{P}(a^{1}\\succ a^{2}|x,a^{1},a^{2})=\\frac{\\exp(R^{*}(x,a^{1}))}{\\exp(R^{*}(x,a^{1}))+\\exp(R^{*}(x,a^{2}))}=\\sigma\\big(R^{*}(x,a^{1})-R^{*}(x,a^{2})\\big),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\sigma(z)\\,=\\,1/(1+\\exp(-z))$ is the sigmoid function. Then, the reward model is taken as the Maximum Likelihood Estimation (MLE) of the BT model on the preference dataset $\\mathcal{D}$ [e.g., 51, 48, 50, 4, 60] and is used in subsequent policy optimization steps to provide a signal for algorithms like Proximal Policy Optimization [56]. Despite its successes, the existence of a reward function and the BT model are strong assumptions, which may not fully capture the complicated human preferences. In particular, the BT model assumes that human preference is transitive, which means that if we prefer A to B $(\\mathbb{P}(A\\,\\succ\\,B|x,A,B)\\,>\\,0.5)$ and we prefer B to C, then it automatically holds that we prefer A to C. This assumption, however, is contradicted by evidence of intransitivity in human decision-making [62, 45]. This limitation is particularly pronounced if we consider the population-level preferences, where the ultimate preference signal is aggregated across diverse human groups [45]. This may further be evidenced that in the practical RLHF, the accuracy of the learned BT model is around $70\\%$ [4, 60, 16], suggesting the challenges in approximating the complicated human preference by BT model. While there are some recent efforts to bypass reward modeling [53, 85], they are still fundamentally derived from the reward-based preference model and suffer from the aforementioned issues. In contrast, the general preference oracle defined in Definition 1 is strictly more general than the BT model and can capture a more complicated preference pattern from the definition itself. It allows an intransitive preference model and can further capture the preference feedback from AI [5], with a notable example of GPT-4 [49], which is widely used for model evaluations in practice and may more accurately reflect real user experience [60, 18, 53, 72]. Moreover, from a practical side, the preference model construction tends to be more efficient than the reward function in terms of ranking accuracy. This is evidenced by the fact that the preference model, pairRM with 0.4B parameters [34], performs comparably to a LLaMA2-13B-based reward model across a diverse set of preference targets [16]. As a case study, we train a reward model based on the Bradley-Terry (BT) model and a preference model with the same starting checkpoint Gemma-2B-it [59] and preference dataset8, with results presented in Table 1 and the training details are deferred to Section 5. As we can see, the preference model achieves much higher test accuracy in the reasoning task while maintaining comparable results in other tasks. Meanwhile, the training set we use is rather limited in the reasoning data (math and coding), so the reasoning task can be viewed as an out-of-distribution task. In this sense, the preference model may also provide a better generalization compared to the reward model. The results also extend to another case study with LLaMA3-8B-instruct, where the preference model shows promising potential in the improvement of reasoning tasks. We refer interested readers to check Zhao et al. [85], Liu et al. [43] for further examples with similar observations. The advantage in ranking accuracy is not only directly beneficial for the algorithms that depend on ranking information [18, 30], but also improves the performance of algorithms derived from the reward-based framework (i.e., Bradley-Terry model), as evidenced by the results in the study of (iterative) DPO [72, 31]. ", "page_idx": 1}, {"type": "table", "img_path": "TwdX1W3M6S/tmp/100aee6f822aa0f813f396aa2f1f2be299467ab55485c11d3a1bc9657aba498c.jpg", "table_caption": ["Table 1: Comparison of the test accuracy between the BT-based reward model and the preference model. The reward model and preference model are trained with the same base model and preference dataset, where the details are deferred to Section 5. We evaluate the model on Reward-Bench [39]. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Given all these considerations, our study focuses on exploring the theoretical properties of RLHF under the general preference oracle (Definition 1), with the goal of advancing practical algorithmic designs. We summarize our contributions as follows: ", "page_idx": 2}, {"type": "text", "text": "\u2022 We make the first attempt to study the theoretical learnability of RLHF under general preference oracle with KL regularization, in the offilne setting with a pre-collected preference dataset and the online setting where we can query human feedback along the way of training, which demonstrates the potential of reward-model-free learning under general preference;   \n\u2022 We propose sample-efficient algorithms in both the offline setting and online setting and establish the finite-sample theoretical guarantees under standard coverage and exploration conditions;   \n\u2022 We show that the theoretical insights can be used to guide practical algorithmic designs with a reasonable approximation of the computational oracle. ", "page_idx": 2}, {"type": "text", "text": "2 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we formulate the RLHF with general preference learning. Suppose that there exists a preference function $P^{*}:\\mathcal{X}\\times\\mathcal{A}\\times\\mathcal{A}\\rightarrow\\mathbb{R}$ which represents the prefererence of one action $a^{1}$ over another $a^{2}$ given a prompt $x$ : $P^{*}(x,a^{1},a^{2})=\\mathbb{P}(a^{1}\\succ a^{2}|x,a^{1},\\stackrel{\\cdot}{a^{2}})$ . In practical applications, we want to make the resulting LLM $\\pi$ close to $\\pi_{0}$ [90, 50, 4, 53]. Therefore, we adopt the following KL-regularized objective: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{I(\\pi^{1},\\pi^{2})=\\mathbb{E}_{x\\sim d_{0}}\\mathbb{E}_{a^{1}\\sim\\pi^{1},a^{2}\\sim\\pi^{2}}\\left[P^{*}(x,a^{1},a^{2})-\\eta^{-1}D_{\\mathrm{KL}}(\\pi^{1}(\\cdot|x)||\\pi_{0}(\\cdot|x))+\\eta^{-1}D_{\\mathrm{KL}}(\\pi^{2}(\\cdot|x)||\\pi_{0}(\\cdot|x))\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "One primary reason to consider the regularized target is that the constructed preference model is only locally accurate, i.e., it performs well when there is little distribution shift. For instance, if the preference model is fine-tuned on a preference dataset collected by the initial model $\\pi_{0}$ , it improves the in-distribution generalization, but the resulting model often performs poorly out-of-distribution [10]. Meanwhile, even if we require human labelers to give feedback along the way, the choices of the labelers may not be representative enough or the labelers can make mistakes due to limited time, attention, or care [27]. Moreover, the KL divergence in the target ensures that the resulting policy is stochastic instead of deterministic (given a suitable initial checkpoint), thereby more accurately reflecting the dynamics of generative language models. ", "page_idx": 2}, {"type": "text", "text": "We choose $P^{*}$ as the target mostly for historical reasons [22, 65]. A choice is the relative preference $\\log(P^{*}(x,a^{1},a^{2})/(1\\!-\\!\\bar{P}^{*}(x,a^{1},a^{2})))$ ), which is equal to $R^{*}(x,\\stackrel{\\_}{a}^{1})-R^{*}(x,a^{2})$ when the BT model holds so that (3) becomes two decoupled regularized-reward maximization problems in this case and automatically reduces to the setting considered in the previous work Xiong et al. [72]. While we do not handle this target directly, the analysis techniques presented in this paper readily apply to it with slight modifications. ", "page_idx": 2}, {"type": "text", "text": "Nash Equilibrium and Best Response. Without loss of generality, we restrict our attention to the policy class $\\Pi$ consisting of the policies with the same support as $\\pi_{0}$ and denote the unique Nash equilibrium (known as the Minimax Winner [57, 38, 24] or the von Neumann Winner [22]) as the solution of the following minimax problem as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n(\\pi_{*}^{1},\\pi_{*}^{2})=(\\pi_{*},\\pi_{*})=\\underset{\\pi^{1}\\in\\Pi}{\\mathrm{argmax\\,argmin}}\\,J(\\pi^{1},\\pi^{2}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the Nash policies of two players coincide as we prove in Lemma 4. In the rest of this paper, we still use the notation $(\\pi_{*}^{1},\\pi_{*}^{2})$ to distinguish between the max-player and min-player. Accordingly, we refer to the first LLM $\\pi^{1}$ as the max-player, while the second LLM $\\pi^{2}$ is the min-player. We also define the notion of best response. For function $J$ and policy $\\pi^{1}$ , the best response to $\\pi^{1}$ is defined as $\\mathrm{argmin}_{\\pi^{2}\\in\\Pi}\\,J(\\pi^{1},\\pi^{2})$ and the value is denoted by $\\begin{array}{r}{\\dot{J}(\\pi^{1},\\dot{\\l})=\\operatorname*{min}_{\\pi^{2}\\in\\Pi}\\dot{J}(\\pi^{1},\\pi^{2})}\\end{array}$ . Similarly, for $\\pi^{2}$ , we have $J(\\dag,\\pi^{2})=\\operatorname*{max}_{\\pi^{1}\\in\\Pi}J(\\pi^{1},\\pi^{2})$ . In particular, since $\\pi_{*}^{1}$ and $\\pi_{*}^{2}$ are the Nash equilibrium, they are the best response to each other. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Function Approximation. Suppose that we have access to a function class $\\mathcal{P}\\subset(\\mathcal{X}\\times\\mathcal{A}\\times\\mathcal{A}\\rightarrow\\mathbb{R})$ (e.g. neural network), which provides us with a set of candidates to approximate the $P^{*}$ , and also the preference functions $P\\in\\mathcal P$ satisfies $P(x,a^{1},a^{2})=1-P(x,a^{2},\\stackrel{\\star}{a^{1}})$ . We make the following assumptions on the class $\\mathcal{P}$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 1. Assume that $\\mathcal{P}$ is finite and the capacity of the class is large enough so that $P^{*}\\in\\mathcal{P}$ . ", "page_idx": 3}, {"type": "text", "text": "The finite class assumption is for a clear presentation and the results readily generalize to an infinite class with a bounded covering number by the standard discretization technique. We define a theoretical computation oracle as follows and defer the practical implementations to the experiment section. ", "page_idx": 3}, {"type": "text", "text": "Definition 2 (Nash Equilibrium Oracle). For a given preference function $P\\in\\mathcal P$ and a reference policy $\\pi_{0}$ , we can compute the Nash Equilibrium policy ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pi_{P}=\\underset{\\pi^{1}\\in\\Pi}{\\arg\\operatorname*{max}}\\ \\underset{\\pi^{2}\\in\\Pi}{\\operatorname*{min}}\\,\\mathbb{\\tilde{E}}_{x\\sim d_{0}}\\mathbb{E}_{a^{1}\\sim\\pi^{1},a^{2}\\sim\\pi^{2}}\\Big[P(x,a^{1},a^{2})-\\eta^{-1}\\log\\frac{\\pi^{1}(a^{1}|x)}{\\pi_{0}(a^{1}|x)}+\\eta^{-1}\\log\\frac{\\pi^{2}(a^{2}|x)}{\\pi_{0}(a^{2}|x)}\\Big].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Learning Objective. The goal is to find an $\\epsilon$ -approximate Nash policy $\\hat{\\pi}^{1}$ for the max-player: ", "page_idx": 3}, {"type": "equation", "text": "$$\nJ(\\pi_{*}^{1},\\pi_{*}^{2})-J(\\hat{\\pi}^{1},\\dagger)=J(\\pi_{*}^{1},\\pi_{*}^{2})-\\operatorname*{min}_{\\pi^{\\prime}}J(\\hat{\\pi}^{1},\\pi^{\\prime})\\leq\\epsilon,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which means that the max-player is consistently preferred by the KL-regularized preference in the face of any competing policy $\\pi^{\\prime}$ up to a relaxation of $\\epsilon$ . To stress the non-symmetric structures of the two players, we refer to the max-player as the main agent, which aims to find her $\\epsilon$ -approximate Nash policy, and refer to the min-player as the enhancer, which is designed to facilitate the main agent\u2019s learning. In particular, when $\\eta$ is large enough so that the KL is roughly omitted, then, we can further obtain that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi^{2}\\in\\Pi}\\mathbb{E}_{x\\sim d_{0}}\\mathbb{E}_{a^{1}\\sim\\hat{\\pi}^{1},a^{2}\\sim\\pi^{2}}P^{*}(x,a^{1},a^{2})\\geq0.5-\\epsilon.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In this case, the obtained policy $\\hat{\\pi}_{1}$ is consistently preferred by the preference oracle $P^{*}$ against any competing policies. We mention in passing that the $\\mathrm{KL}$ penalty coefficient $\\eta>0$ exhibits a trade-off between being preferred by the oracle $P^{*}$ and staying close to the initial model $\\pi_{0}$ , and reflects the degree of our belief in the oracle $P^{*}$ . In practice, $\\eta$ is typically treated as a hyper-parameter and is adjusted by parameter search [32]. ", "page_idx": 3}, {"type": "text", "text": "Compared to the previous literature formulating the preference learning as finding a Nash equilibrium, although we focus on optimizing the policy for the max-player, we can also have a duality gap guarantee because of the symmetry of the objective function: $J(\\pi^{1},\\pi^{2})=1-J(\\pi^{2},\\pi^{1})$ . To see this, we decompose the duality gap into the suboptimality for the max-player $\\hat{\\pi}^{1}$ and the min-player $\\hat{\\pi}^{2}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{J(\\dag,\\hat{\\pi}^{2})-J(\\hat{\\pi}^{1},\\dag)=\\!J(\\dag,\\hat{\\pi}^{2})-J(\\pi_{*}^{1},\\pi_{*}^{2})+J(\\pi_{*}^{1},\\pi_{*}^{2})-J(\\hat{\\pi}^{1},\\dag)}\\\\ &{}&{=\\!J(\\pi_{*}^{1},\\pi_{*}^{2})-J(\\hat{\\pi}^{2},\\dag)+J(\\pi_{*}^{1},\\pi_{*}^{2})-J(\\hat{\\pi}^{2},\\dag).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "If we obtain such an $\\epsilon_{}$ -suboptimal max player $\\hat{\\pi}^{1}$ , by taking the min-player $\\hat{\\pi}^{2}=\\hat{\\pi}^{1}$ , the duality gap $J(\\dagger,{\\hat{\\pi}}^{2})-J({\\hat{\\pi}}^{1},\\dagger)$ is naturally bounded by $2\\epsilon$ . ", "page_idx": 3}, {"type": "text", "text": "Notations. We use the short-hand notation $\\pi_{.}=(\\pi^{1},\\pi^{2})$ when there is no confusion. We use $P(x,\\pi^{1},\\pi^{2})$ to represent $\\mathbb{E}_{a^{1}\\sim\\pi^{1},a^{2}\\sim\\pi^{2}}[P(x,a^{1},a^{2})]$ . We use $J(x,\\pi^{1},\\pi^{2})$ to denote the objective function in (3) without the expectation over the prompt $x\\sim d_{0}$ . Let $\\sigma(x)$ denote the sigmoid function $1/(1+e^{-x})$ . We also provide a notation table in Table 4 to improve the readability of this paper. ", "page_idx": 3}, {"type": "text", "text": "Due to space constraints, the review of the related literature is deferred to Appendix 7. ", "page_idx": 3}, {"type": "text", "text": "3 Improved Algorithms in Offline Setting ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Setup", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the offline setting, our goal is to learn a good policy from a pre-collected dataset $\\mathcal{D}_{\\mathrm{off}}\\ =$ $\\{(x_{i},a_{i}^{1},a_{i}^{2},y_{i})\\}_{i=1}^{n}$ without further query with the oracle $\\mathbb{P}$ , where comparison sample is assumed ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 Pessimistic Equilibrium Learning from Human Feedback ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "1: Input: Dataset $\\mathcal{D}_{\\mathrm{off}}=\\{x_{i},a_{i}^{1},a_{i}^{2},y_{i}\\}_{i=1}^{n}.$ , preference space $\\mathcal{P}$ , policy class $\\Pi$ , parameter $\\eta,\\beta>0$ .   \n2: Compute the MLE $\\hat{P}=\\mathrm{argmin}_{P\\in\\mathcal{P}}\\,\\ell_{\\mathcal{D}_{\\mathrm{off}}}(P)$ .   \n3: Construct version space ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{P}}=\\Big\\{P\\in\\mathcal{P}:\\sum_{i=1}^{n}(P(x_{i},a_{i}^{1},a_{i}^{2})-\\hat{P}(x_{i},a_{i}^{1},a_{i}^{2}))^{2}\\leq\\beta^{2}/2\\Big\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "4: Compute the best policy under the conservative value estimation ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\pi}^{1}=\\mathop{\\mathrm{argmax}}_{\\pi^{1}\\in\\Pi}\\displaystyle\\operatorname*{min}_{\\pi^{2}\\in\\Pi}\\displaystyle\\operatorname*{min}_{P\\in\\widetilde{\\mathcal{P}}}\\mathbb{E}_{x\\sim d_{0}}\\mathbb{E}_{a^{1}\\sim\\pi^{1},a^{2}\\sim\\pi^{2}}\\left[P(x,a^{1},a^{2})+\\eta^{-1}\\ln\\frac{\\pi_{0}(a^{1}|x)}{\\pi^{1}(a^{1}|x)}-\\eta^{-1}\\ln\\frac{\\pi_{0}(a^{2}|x)}{\\pi^{2}(a^{2}|x)}\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "5: Output: $\\hat{\\pi}^{1}$ ", "page_idx": 4}, {"type": "text", "text": "to be independently collected as in (1). We measure the suboptimality of the learned policy $\\hat{\\pi}^{1}$ by the gap between the Nash value and the best response value: ", "page_idx": 4}, {"type": "equation", "text": "$$\nJ(\\pi_{1}^{*},\\pi_{2}^{*})-J(\\hat{\\pi}^{1},\\dagger),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the KL-regularized function $J$ is defined in (3). Similar to the reward-based framework [50], one natural approach is a two-staged method: (1) Construct an empirical preference model (reward model in the literature) by maximizing the log-likelihood function: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\ell_{\\mathcal{D}_{\\mathrm{off}}}(P)=\\sum_{(x,a^{1},a^{2},y)\\in\\mathcal{D}_{\\mathrm{off}}}y\\log P(x,a^{1},a^{2})+(1-y)\\log P(x,a^{2},a^{1});\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "(2) Solve the policy by plugging the learned preference model $\\hat{P}$ into the Nash Equilibrium Oracle 2. However, this framework typically leads to severe reward over-optimization issue [26], meaning that while the model is preferred by the learned P\u02c6, it may not achieve good performance under the evaluation of $P^{*}$ . This is because, with finite $\\mathcal{D}_{\\mathrm{off}}$ drawn from some behavior policy, it is unlikely to provide an accurate estimation for all the prompt-response pairs. Therefore, imposing heavy optimization pressure toward $\\hat{P}$ will push the model to exploit these unreliable estimations to chase for a high proxy metric, thus leading to a worse performance under the ground truth $P^{*}$ . ", "page_idx": 4}, {"type": "text", "text": "3.2 Learning with Pessimism ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The recent advances in the offilne RL theory have demonstrated that the principle of pessimism with a conservative estimation is statistically efficient for offline learning across a diverse set of scenarios [35, 54, 69, 79, 86, 17, 71, 84]. In this section, we connect the KL-reversed minimax game in (3) with offline RL by pessimism via version space9. ", "page_idx": 4}, {"type": "text", "text": "We introduce our algorithm, Pessimistic Equilibrium Learning from Human Feedback (PELHF) in Algorithm 1. Given an offilne dataset $\\mathcal{D}_{\\mathrm{off}}$ , we first obtain the maximum likelihood estimation (MLE) $\\hat{P}$ by maximizing (7). Rather than directly planning with this empirical $\\hat{P}$ , we form a version space $\\widehat{\\mathcal{P}}$ that contains $\\bar{P}^{*}\\in\\widehat{\\mathcal{P}}$ with a high probability under a suitable choice of $\\beta$ , as we show in Lemma 1. For each policy $\\pi^{1}$ , we take the minimum preference function over $\\widehat{\\mathcal{P}}$ and the best responded $\\pi^{2}$ as its conservative value estimation: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{J}_{\\mathrm{off}}(\\pi^{1})=\\operatorname*{min}_{\\pi^{2}\\in\\Pi}\\operatorname*{min}_{P\\in\\hat{P}}\\mathbb{E}_{\\alpha^{1}\\sim\\pi^{1},\\alpha^{2}\\sim\\pi^{2}}\\Big[P(x,a^{1},a^{2})+\\eta^{-1}\\ln\\frac{\\pi_{0}(a^{1}|x)}{\\pi^{1}(a^{1}|x)}-\\eta^{-1}\\ln\\frac{\\pi_{0}(a^{2}|x)}{\\pi^{2}(a^{2}|x)}\\Big].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then, we solve the minimax game concerning this conservative value estimator. With this pessimistic modification, the resulting algorithm enjoys the following theoretical guarantee. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. [Proof] If Assumption $^{\\,l}$ holds, and we set $\\lambda=\\log(|\\mathcal{P}|/\\delta)$ and $\\beta^{2}=2\\log(|\\mathcal{P}|/\\delta),$ , then, with probability at least $1-\\delta$ , the output policy of Algorithm $^{\\,l}$ satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\nJ(\\pi_{1}^{*},\\pi_{2}^{*})-J(\\hat{\\pi}^{1},\\dag)\\leq4\\beta\\sqrt{{\\mathcal C}(\\pi_{*}^{1},\\pi_{D},{\\mathcal P})/n}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the coverage coefficient ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{C}(\\pi_{*}^{1},\\pi_{D},\\mathcal{P})=\\operatorname*{max}_{\\pi^{2}\\in\\Pi}\\operatorname*{sup}_{P\\in\\mathcal{P}}\\frac{(\\mathbb{E}_{x\\sim d_{0}}[P(x,\\pi_{*}^{1},\\pi^{2})-\\hat{P}(x,\\pi_{*}^{1},\\pi^{2})])^{2}}{\\mathbb{E}_{x\\sim d_{0},a^{1}\\sim\\pi_{D}^{1},a^{2}\\sim\\pi_{D}^{2}}(P(x,a^{1},a^{2})-\\hat{P}(x,a^{1},a^{2}))^{2}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "9We introduce another algorithm achieving pessimism via uncertainty bonus construction, see Appendix C.2. ", "page_idx": 4}, {"type": "text", "text": "This theorem shows that the suboptimality gap depends on how the target $(\\pi_{*}^{1},\\pi^{2})$ is covered by the offline dataset, where $\\pi^{2}$ is maximized over the policy set $\\Pi$ . This coverage coefficient resembles the unilateral coverage10 for Markov games [17, 86]. Then, a natural question is whether a good coverage condition $(\\bar{C}(\\pi_{*}^{1},\\pi_{D},\\mathcal{P})$ is small) is practical in the context of LLMs. Unfortunately, since the response is usually long in practice, the distribution shift between policies is also very large. We summarize some observations here. First, along the way of the RLHF training, the average density ratio $\\pi(a|x)/\\pi_{0}(a|x)>\\exp(25)$ as reported in Figure 13 of Bai et al. [4]. See similar results of rejection sampling fine-tuning [18] and DPO [53]. Second, for a case study, we use the Gemma-7B-it as the behavior policy to collect data for aligning Gemma-2B-it [59] with $15\\mathrm{k}$ prompt from [16]. Then, we calculate the average KL divergence between Gemma-7B-it and Gemma-2B-it as 456.4. This evidence indicates that the coverage coefficient probably explodes in realistic scenarios. Therefore, it is unlikely to expect that we can learn the optimal policy from a pre-collected dataset. This motivates us to consider the online setting, where we can further query the preference oracle during the training to enrich the dataset thus enhancing our models continuously. ", "page_idx": 5}, {"type": "text", "text": "4 Iterative RLHF with Online Exploration ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Setup of Iterative RLHF ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The major difference between the online and offilne settings is that online algorithms can further query the preference oracle $P^{*}$ along the way of training. Since updating the LLMs is expensive, we consider the batch online setting for a sparse policy update. Specifically, for each batch $t\\in[T]$ , we first update the policy pair $(\\hat{\\pi}_{t}^{1},\\hat{\\pi}_{t}^{2})$ based on the historical information collected so far. Then, we collect $m$ tuples: we sample a random prompt by $x_{t,i}\\sim d_{0}$ , collect two responses by $(a_{t,i}^{1},a_{t,i}^{2})\\sim(\\hat{\\pi}_{t}^{1},\\hat{\\pi}_{t}^{2})$ , and query the preference signal $y_{t,i}\\sim\\operatorname{Ber}(P^{*}(x_{t,i},a_{t,i}^{1},a_{t,i}^{2}))$ . Here the batch size $m$ is usually very large compared to the typically adopted mini-batch update. To distinguish this from the sequential online setting where we update policy after collecting a single preference pair, we refer to this learning paradigm as the iterative $R L H F$ . ", "page_idx": 5}, {"type": "text", "text": "4.2 Learning with Exploration ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The primary advantage of online learning is that we can strategically choose the behavior policies in each iteration to improve the coverage of the collected data, which is referred to as the exploration in the literature. To achieve this goal, we need to quantify the data uncertainty to guide the exploration direction. To this end, we present the notions of information ratio and eluder coefficient. ", "page_idx": 5}, {"type": "text", "text": "Information Ratio and Eluder Coefficient. Distinct from the offilne setting where we assume the coverage condition of a pre-collected dataset $\\mathcal{D}_{\\mathrm{off}}$ , online exploration makes it possible to upper bound the suboptimality by the complexity of the function space. We leverage the notion of the eluder coefficient, which limits the generalization from visited state-action distributions to unseen parts. ", "page_idx": 5}, {"type": "text", "text": "Definition 3 (Information Ratio and Eluder Coefficient). At round $t$ , given an estimation $\\hat{P}\\in\\mathcal P$ , we define the information ratio for any two policy $\\pi^{1},\\pi^{2}$ as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Gamma_{t}(\\lambda,\\pi^{1},\\pi^{2})=\\operatorname*{sup}_{P\\in\\mathcal{P}}\\frac{|\\mathbb{E}_{x\\sim d_{0}}[P(x,\\pi^{1},\\pi^{2})-\\hat{P}(x,\\pi^{1},\\pi^{2})]|}{\\sqrt{\\lambda+\\sum_{s=1}^{t-1}\\mathbb{E}_{x_{s}\\sim d_{0},a_{s}^{1}\\sim\\hat{\\pi}_{s}^{1},a_{s}^{2}\\sim\\hat{\\pi}_{s}^{2}}(P(x_{s},a_{s}^{1},a_{s}^{2})-\\hat{P}(x_{s},a_{s}^{1},a_{s}^{2}))^{2}}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then, the eluder coefficient is given by $\\begin{array}{r}{d(\\mathcal{P},\\lambda,T):=\\operatorname*{sup}_{\\pi_{1:T}^{1},\\pi_{1:T}^{2}}\\sum_{t=1}^{T}\\operatorname*{min}(1,(\\Gamma_{t}(\\lambda,\\pi_{t}^{1},\\pi_{t}^{2}))^{2}).}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "The information ratio and eluder coefficient considered here have also been adopted in the literature [e.g., 64, 28, 70, 74, 1]. Essentially, the information ratio compares the out-of-sample error on the unseen data with the in-sample error measured on the historical data, and can be interpreted as the worst-case ratio between them (as we take supreme over all possible $P\\in\\mathcal P$ ). Meanwhile, the eluder coefficient limits the extent to which we can be \u201csurprised\u201d by the new out-of-sample distributions, given the historical data collected so far. The uncertainty for the preference model aligns with the uncertainty for the BT model under boundedness conditions, which is illustrated in the following example. We defer the details to Appendix D.1. ", "page_idx": 5}, {"type": "text", "text": "Example 1 (Uncertainty in Bradley-Terry model with linear reward). Suppose the reward function can be embedded into a $d$ -dimensional vector space $\\{r(x,a)~=~{\\stackrel{\\cdot}{\\sigma}}\\langle\\theta,\\phi(x,a)\\rangle~:~{\\stackrel{\\cdot}{\\theta}}~\\in$ $\\mathbb{R}^{d},\\|\\theta\\|~~\\le~~B,\\|\\phi(x,a)\\|~~\\le~~1\\}$ . Then, if we define the covariance matrix as $\\begin{array}{r l}{\\Sigma_{t}}&{{}=}\\end{array}$ $\\begin{array}{r}{\\sum_{s=1}^{t-1}\\mathbb{E}_{x\\sim d_{0},a^{1}\\sim\\hat{\\pi}_{s}^{1},a^{2}\\sim\\hat{\\pi}_{s}^{2}}(\\phi(x,a^{1})-\\phi(x,a^{2}))^{\\top}(\\phi(x,a^{1})-\\phi(x,a^{2}))+\\lambda(1+e^{B})^{2}I}\\end{array}$ , we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Gamma_{t}(\\lambda,\\pi^{1},\\pi^{2})\\leq(1+e^{B})\\|\\phi(x,\\pi^{1})-\\phi(x,\\pi^{2})\\|_{\\Sigma_{t}^{-1}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Algorithm 2 Optimistic Equilibrium Learning from Human Feedback with Enhancer ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "1: Input: Preference space $\\mathcal{P}$ , policy class $\\Pi$ , parameter $\\eta,\\lambda>0$ . ", "page_idx": 6}, {"type": "text", "text": "2: for $\\scriptstyle{\\mathrm{t}}=1$ ,...,T do ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "3: Exploitation with the main agent: compute the MLE $\\hat{P}_{t}$ with $\\ell_{D_{1:t-1}}$ defined in (7) and compute Nash equilibrium by calling the Nash equilibrium oracle 2: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{q u m m a n t u m~o y~c a m m g~u r e~r a s n~c q u m m a n t u m~o n s t w}\\angle.}\\\\ &{\\hat{\\pi}_{t}^{1}=\\underset{\\pi^{1}\\in\\Pi}{\\mathrm{argmax~min}}\\;\\mathbb{E}_{x\\sim d_{0},a^{1}\\sim\\pi^{1},a^{2}\\sim\\pi^{2}}\\Bigl[\\hat{P}_{t}(x,a^{1},a^{2})+\\eta^{-1}\\log\\frac{\\pi_{0}(a^{1}|x)}{\\pi^{1}(a^{1}|x)}-\\eta^{-1}\\log\\frac{\\pi_{0}(a^{2}|x)}{\\pi^{2}(a^{2}|x)}\\Bigr],}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "4: Exploration with the enhancer: compute enhancer to maximize the uncertainty: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\pi_{t}^{2}=\\underset{\\pi^{2}\\in\\Pi}{\\mathrm{argmax}}\\,\\widetilde{\\Gamma}_{t}^{m}(\\lambda,\\widehat{\\pi}_{t}^{1},\\pi^{2}):=\\underset{P\\in\\mathcal{P}}{\\operatorname*{sup}}\\,\\frac{\\lvert\\mathbb{E}_{x\\sim d_{0}}[P(x,\\widehat{\\pi}_{t}^{1},\\pi^{2})-\\hat{P}_{t}(x,\\widehat{\\pi}_{t}^{1},\\pi^{2})]\\rvert}{\\sqrt{\\lambda+\\frac{1}{m}\\sum_{s=1}^{t-1}\\sum_{j=1}^{m}(P(x_{s,j,},a_{s,j}^{1},a_{s,j}^{2})-\\hat{P}_{t}(x_{s,j},a_{s,j}^{1},a_{s,j}^{2}))^{2}}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "5: Collect $\\mathcal{D}_{t}=\\{(x_{i},a_{i}^{1},a_{i}^{2},y_{i})\\}_{i=1}^{m}$ by $x_{i}\\sim d_{0},a_{i}^{1}\\sim\\hat{\\pi}_{t}^{1}(\\cdot|x_{i})$ , $a_{i}^{2}\\sim\\hat{\\pi}_{t}^{2}(\\cdot|x_{i})$ and $y_{i}\\sim\\operatorname{Ber}\\!\\left(\\mathbb{P}(a_{i}^{1}\\succ$ $a_{i}^{2}|x,a_{i}^{1},a_{i}^{2})$ ; ", "page_idx": 6}, {"type": "text", "text": "6: end for ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "7: Output: the best policy in $(\\pi_{1:T}^{1})$ by a validation set. ", "page_idx": 6}, {"type": "text", "text": "We refer interested readers to Du et al. [20], Zhong et al. [87], Xie et al. [70] for the extensive examples when $d(\\mathcal{P},\\lambda,T)$ can have a sub-linear dependency on $T$ . We are now ready to present the algorithm for the online setting, as summarized in Algorithm 2. Specifically, for each iteration, the main agent exploits the information contained in the data collected so far by computing the MLE $\\hat{P}_{t}$ and solving the minimax game with respect to it to get $\\hat{\\pi}_{t}^{1}$ . The enhancer, however, aims to facilitate the main agent\u2019s learning by maximizing the uncertainty relative to the $\\hat{\\pi}_{t}^{1}$ . Finally, we use the policy pair to collect $m$ preference pairs and query oracle $P^{*}$ to get the preference signals. Notably, to facilitate the computation for the main agent, instead of adding optimism to the value function, we impose the exploration role on the enhancer. This choice turns out to be important when we move toward practical algorithms with reasonable approximations, as we detail in Section 5. We now present the main theoretical guarantee for Algorithm 2. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. [Proof] Under Assumption $^{\\,l}$ , for any $\\epsilon>0$ , if we set the total iterations $T=\\operatorname*{min}\\{n\\in$ $\\mathbb{N}^{+}:n\\geq2d(\\mathcal{P},\\lambda,n)\\},$ , batch size $m=18T\\log(2T|\\mathcal{P}|/\\delta)/\\epsilon^{2},$ , $\\beta=\\sqrt{2T\\log(2T|\\mathcal{P}|/\\delta)/m}$ , and $\\lambda=2T\\log(2T|\\mathcal{P}|/\\delta)/m$ for Algorithm 2, then, with probability at least $1-\\delta$ , there exists a $t_{0}\\in[T]$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\nJ(\\pi_{*}^{1},\\pi_{*}^{2})-J(\\hat{\\pi}_{t_{0}}^{1},\\dag)\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The theorem states that with suitable hyper-parameter choices, after $T$ iterations (up to log factors), we can find an $\\epsilon_{}$ -approximate Nash policy $\\hat{\\pi}_{t_{0}}^{1}$ for the max-player. Here $T$ depends on the eluder coefficient that is intrinsic to the preference model and characterizes the complexity of the problem. ", "page_idx": 6}, {"type": "text", "text": "Key Ideas. We present a brief discussion of the key analysis ideas. Similar to Lemma 1, the MLE $\\hat{P}$ ensures a controllable in-sample error (with details in the Appendix D). Recalling that the uncertainty bonus is essentially the worst-case ratio between the out-of-sample error (our learning target) and the in-sample error, to finally bound the out-of-sample error, we need to explore each direction where we are uncertain about so that the average uncertainty bonus is sufficiently small. Since the main agent is greedy (takes the best guess we can obtain so far), the enhancer plays the exploration role by maximizing the uncertainty relative to the $\\hat{\\pi}_{t}^{1}$ . Then, since the eluder dimension is finite: $\\begin{array}{r}{\\sum_{t=1}^{T}\\operatorname*{min}\\left(1,(\\Gamma_{t}(\\lambda,\\hat{\\pi}_{t}^{1},\\hat{\\pi}_{t}^{2}))^{2}\\right)\\leq d(\\mathcal{P},\\lambda,T)}\\end{array}$ , there exists at least a $t_{0}\\in[T]$ such that the value at $t_{0}$ is smaller or equal to the average value: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\left(1,(\\Gamma_{t}(\\lambda,\\hat{\\pi}_{t}^{1},\\hat{\\pi}_{t}^{2}))^{2}\\right)\\leq d(\\mathcal{P},\\lambda,T)/T\\leq1/2.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Hence, with a proper $m$ , we can obtain the result of Theorem 2. ", "page_idx": 6}, {"type": "text", "text": "In practice, searching for the most uncertain policy in the whole policy space can be challenging and the enhancer policy itself does not enjoy any theoretical guarantee. We may slightly modify Algorithm 2 by restricting the exploration step to the following subset ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\Pi_{t}=\\{\\pi\\in\\Pi:\\eta^{-1}\\mathbb{E}_{x\\sim d_{0}}D_{\\mathrm{KL}}(\\pi(\\cdot|x),\\hat{\\pi}^{1}(\\cdot|x))\\leq\\beta(\\widetilde\\Gamma_{t}^{m}(\\lambda,\\hat{\\pi}^{1},\\pi)+\\widetilde\\Gamma_{t}^{m}(\\lambda,\\hat{\\pi}^{1},\\hat{\\pi}^{1}))\\},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\beta$ is the parameter defined in Theorem 2. This set is never empty because we can prove that both $\\hat{\\pi}_{t}^{1}$ and argmin\u03c0\u2032 $J(\\hat{\\pi}_{t}^{1},\\pi^{\\prime})$ belong to $\\Pi_{t}$ . Intuitively, maintaining a small KL divergence against $\\hat{\\pi}_{t}^{1}$ corresponds to exploiting the historical information, and maximizing the uncertainty relative to $\\hat{\\pi}_{t}^{1}$ leads to more information gain. The choice of $\\Pi_{t}$ represents a refined trade-off between these two different goals, thus making $\\bar{\\pi}_{t}^{2}$ also converge to $\\pi_{*}$ . The details are deferred to Appendix D.2. ", "page_idx": 7}, {"type": "text", "text": "5 Practical Implementation of Preference Model and Iterative RLHF ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we discuss how to implement the theoretical Algorithm 2 for the online setting. ", "page_idx": 7}, {"type": "text", "text": "Main agent approximates Nash equilibrium oracle via self-play IPO. Approximating the information-theoretical oracle 2 given a known preference model has been studied in Munos et al. [46], Calandriello et al. [11]. The proposed algorithm, self-play IPO, can serve as a reasonable approximation of the oracle by optimizing the following loss function: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x\\sim d_{0},a,a^{\\prime}\\sim\\mathrm{SG}[\\pi],a^{+},a^{-}\\sim\\hat{P}_{t}(x,a,a^{\\prime})}\\Big[\\log\\frac{\\pi(a^{+}|x)\\pi_{0}(a^{-}|x)}{\\pi(a^{-}|x)\\pi_{0}(a^{+}|x)}-\\frac{1}{2\\eta}\\Big]^{2},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\mathrm{SG}[\\pi]$ means that although we generate data from policy $\\pi$ , but we do not compute the gradient for this data-generation process. Moreover, according to Proposition 4.1 of Calandriello et al. [11], the minimizer of (13) is the unique Nash policy of the (10). ", "page_idx": 7}, {"type": "text", "text": "Enhancer explores via rejection sampling. According to (12), the enhancer aims to find a policy that (1) is close to the main agent\u2019s policy $\\hat{\\pi}_{t}^{1}$ ; (2) maximizes the uncertainty relative to the $\\hat{\\pi}_{t}^{\\mathrm{i}}$ . However, since for the general neural network, the uncertainty estimator does not admit a closed form, in practice, we typically resort to heuristic methods. One popular way in the context of alignment is the rejection sampling [47, 18, 43, 31, 76]. Specifically, given a prompt $x$ , we use $\\hat{\\pi}_{t}^{1}$ to independently sample $n$ responses, use a tournament-style procedure to get the best response (and reject all other responses), and take the best responses as $\\scriptstyle{\\hat{\\pi}}_{t}^{2}$ . In other words, we take the policy induced by rejection sampling with $\\hat{\\pi}_{t}^{1}$ and $P^{*}$ as the enhancer policy $\\hat{\\pi}_{t}^{2}$ . In this way, the $\\hat{\\pi}_{t}^{2}$ enlarges the margins between $\\hat{\\pi}_{t}^{1}$ while maintaining a moderate KL divergence. For instance, in the special case of the BT model, if we rank the samples via the learned reward, the KL divergence is upper bounded by $\\log n-(n-1)/n$ and is usually far better than this conservative estimation [6]. ", "page_idx": 7}, {"type": "text", "text": "Preference model construction. We follow Zhao et al. [85], Liu et al. [43], Dong et al. [19] to utilize the fact that the LLM is the next token predictor for the preference modeling. Specifically, we have a preference pair $(x,a^{1},a^{2},A)$ , where $A$ means that the first response is better, which is formatted as ", "page_idx": 7}, {"type": "text", "text": "Then, we simply treat the preference modeling as an instruction-following task to fine-tune the model on these instruction-label pairs. In particular, to mitigate the position bias (the preference model may prefer the response that is given in the position of RESPONSE A), we randomly switch the order of the two responses in the data formatting process. During inference, we simply use the probability of decoding A as the $\\hat{P}(x,a^{1},a^{2})$ . We mention in passing that it is also possible to include a rubric in the instruction template to guide the model\u2019s prediction and achieve better results [52]. We observe the benefits of the additional prompt engineering in early experiments but decide to use the current version because the main focus is to verify the effectiveness of general preference structure. This implementation is also referred to as the Generative RM in subsequent works. ", "page_idx": 7}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Model, Dataset, and Evaluation. We adopt the widely used open-source model Zephyr-SFT-7B [61] as the starting checkpoint, which is based on the Mistral-7B-v0.111 and fine-tuned on 200K high-quality Ultra-chat data [16]. We use the Ultra-feedback [16] as our prompt set. We divide the prompt set into the train set (60K), validation set (1K), and test set (3K). We mainly use head-to-head comparisons to evaluate the resulting models. In particular, we consider two types of win rate: 1) the win rate measured by the ground-truth LLaMA3-8B-based preference model on the hand-out test set from UltraFeedback; 2) the win rate measured by the GPT-4 Preview (11/06) on an out-of-distribution prompt set AlpacaEval2 [21]. Specifically, for the first evaluation, we use the best DPO model as the reference model, and for the AlpacaEval2, the GPT-4 Preview (11/06) is used as a reference model, and as the judge at the same time. ", "page_idx": 7}, {"type": "table", "img_path": "TwdX1W3M6S/tmp/41beacefacaa7151973b8f85dd8405bc4890f4ce3c406e69c23ea42727416c62.jpg", "table_caption": ["Table 2: The evaluation results of the IPO-aligned models under different KL coefficients. For the first 4 win rates, we use the LLaMA3-8B-based preference model to conduct head-to-head comparisons on the hand-out test set from Ultra-feedback with 3K prompts. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "TwdX1W3M6S/tmp/30e2aee0e6d0b52f49d723b002265ed4a8a6dc0b88f922c83de2da80760fe8b5.jpg", "table_caption": ["Table 3: The evaluation results of the models from different RLHF algorithms. The gold win rates are computed on the hand-out test set from Ultra-feedback with 3K prompts, with the Offline DPO model as the reference. Details of AlpacaEval2 can be found in Dubois et al. [21]. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Method and Competitors. We consider the implementation of Algorithm 2 with self-play IPO and rejection sampling as discussed in Section 5. We iterate for three iterations in total and for each iteration, we retrain a preference model using all the historical data, and run self-play IPO from the initial checkpoint $\\pi_{0}$ (i.e., Zephyr-7B-SFT). For simplicity, we refer to this algorithm as Online ELHF IPO. We use the offline DPO [53], offline IPO [3], and SFT model as the baseline. In particular, we do not further fine-tune the Zephyr-7B-SFT on the preferred responses of Ultra-Feedback because the quality of Ultra-Feedback is lower than that of Ultra-Chat, which is generated by Chat-GPT APIs. For DPO, we follow Xiong et al. [72], Tunstall et al. [61], Rafailov et al. [53] to set the KL coefficient as $\\eta=0.1$ . For IPO, we search the hyper-parameter in $\\{0.1,0.5,1.0\\}$ and report the results in Table 2. Clearly, the model with $\\eta=0.1$ beats all other IPO models and the SFT model with large margins, so we set $\\eta=0.1$ for the offilne IPO and the Online ELHF IPO algorithm in the subsequent studies. ", "page_idx": 8}, {"type": "text", "text": "Simulation framework. For all the offilne algorithms, we sample two responses per prompt of the train set and use the LLaMA3-8B-based preference model to give the preference signal. Then, we run offilne DPO and IPO with the synthetic dataset. For the Online ELHF, we set $n=4$ in the rejection sampling process and use a tournament-style ranking method (so that the complexity of rejection sampling is linear in $n$ ) to find the best response. ", "page_idx": 8}, {"type": "text", "text": "IPO, DPO, and Online ELHF-IPO. We use the open-source project TRL12 to implement IPO and DPO. In particular, we have implemented IPO with log-likelihood/perplexity (perplexity is averaged log-likelihood by sequence length), where the original authors of IPO suggest that log-likelihoodbased implementation is unstable (see the huggingface ${\\tt b l o g}^{13}$ for details). We also found that the IPO without average cannot normally converge and is of poor performance and take the perplexity implementation accordingly. For DPO, we implement the vanilla version as the baseline. We present the main result in Table 3. It is clear that Online ELHF-IPO outperforms the baselines. ", "page_idx": 8}, {"type": "text", "text": "7 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This section focuses on the theoretical aspects. A general discussion is provided in Appendix B.1 ", "page_idx": 9}, {"type": "text", "text": "Theoretical Study of Reward-based RLHF. The theoretical study of policy optimization from preference feedback dated back to the dueling bandits [e.g., 78, 55, 7]. This was later extended to the online RL setting by Xu et al. [73], Novoseller et al. [48], Pacchiano et al. [51], Chen et al. [12], including tabular online RLHF with finite state, and general function approximation for capturing real problems with large state spaces. Zhan et al. [81], Wu and Sun [68] further encompasses the development of reward-free learning type algorithms and sampling-based algorithms for online RLHF. Apart from the online setting, there is another line of works [88, 80, 40] studying the rewardbased RLHF in the offilne setting, which learns from a pre-determined offline dataset with suitable coverage condition over the state-action space. However, they consider reward maximization and deviate from the practical applications (e.g., these frameworks admit a deterministic optimal policy). Recently, Xiong et al. [72] first formulated the RLHF as a reverse-KL regularized contextual bandit and provided finite-sample guarantees in offilne, online, and hybrid settings. We remark that all these papers consider only the reward-based RLHF framework, thus differing from ours. ", "page_idx": 9}, {"type": "text", "text": "Theoretical Study of RLHF under General Preference Oracle. Our work is related to Dud\u00edk et al. [22] and Wang et al. [65]. They investigate preference-based RLHF under a general preference model. The major difference is that we consider the reverse-KL regularized preference, aligning closely with recent LLM advancements [90, 50, 4, 53], while previous work only considers the non-regularized one. Meanwhile, Dud\u00edk et al. [22] considers the problem of finite action, while our work and Wang et al. [65] consider the problem with large or even infinite state-action under function approximation. In terms of learning paradigm and algorithmic design, we consider both offline learning from a precollected dataset and batch online learning with a sparse policy update, while Dud\u00edk et al. [22], Wang et al. [65] studies sequential online learning that updates policy in each step, which is not feasible in the context of LLMs. Moreover, we demonstrate that the proposed algorithms can be reasonably implemented in practice, but Dud\u00edk et al. [22], Wang et al. [65] only focus on information-theoretical algorithms. To summarize, the framework in this work accurately reflects real-world alignment practices thus aligning more closely with the RLHF practice. Our work is closely related to the IPO [3] and Nash learning [46], which also motivate new algorithmic design with a general preference oracle. We comment on the similarities and differences between our framework and theirs as follows. In terms of the problem setting, our work and Nash learning consider the minimax game under the reverse-KL regularized preference, while IPO can be interpreted to find the best response of the fixed reference policy, and may be considered as a special case of the game formulation. In terms of learning paradigm, both the IPO and Nash learning only consider learning toward a fixed and known preference oracle, and study the optimization property of the problem: how to compute the optimal policy under the given preference oracle. In contrast, we study the statistical property, where the preference model needed to be learned and our goal is to find the optimal policy under the underlying ground-truth preference model. In particular, the computational challenge is hidden in Definition 2 and Munos et al. [46] provides a reasonable approximation of the planning oracle. In this sense, our work and Munos et al. [46] are complementary to each other. Finally, the concurrent work Swamy et al. [58] studies the non-regularized general preference model in the sequential online setting and aims to find the Nash equilibrium in the context of continuous control tasks. In terms of the observation model, they assume access to the preference score $\\mathbb{P}(a^{1}\\succ a^{2}|x,a^{1},a^{2})$ , while we only observe the preference signal $y\\sim\\mathrm{Ber}(\\mathbb{P}(a^{1}\\stackrel{\\cdot}{\\sim}a^{2}|x,a^{1},a^{2}))$ . Moreover, they design online RLHF algorithms based on a reduction to the no-regret algorithm like Hedge [25], whose techniques are fundamentally different from ours. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we study the RLHF under a general preference oracle that can capture the non-transitive preferences. Specifically, we formulate the problem as a KL-regularized minimax game between two LLMs, and propose statistically efficient algorithms in both the offline and online settings. The proposed algorithms, with a carefully crafted non-symmetric algorithmic structure, can be practically implemented with reasonable approximations of the information-theoretical computational oracles. We hope our findings can advance the understanding of preference signal modeling in RLHF and stimulate further research beyond the classic reward-based framework. ", "page_idx": 9}, {"type": "text", "text": "9 Acknowledgment ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors would like to thank Tianqi Liu for insightful discussions on the training of the preference model, and thank Haoxiang Wang, and Zihao Li for valuable discussions on the preference dataset selection. We also thank Nevena Lazic and Csaba Szepesvari for pointing out a technical gap in the first version. ", "page_idx": 10}, {"type": "text", "text": "Wei Xiong and Tong Zhang are partially supported by an NSF IIS grant No. 2416897 and Tong Zhang is partially supported by an NSF IIS grant No. 2416897. Nan Jiang acknowledges funding support from NSF IIS-2112471, NSF CAREER IIS-2141781, Google Scholar Award, and Sloan Fellowship. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Alekh Agarwal, Yujia Jin, and Tong Zhang. VOQL: Towards optimal regret in model-free rl with nonlinear function approximation. In The Thirty Sixth Annual Conference on Learning Theory, pages 987\u20131063. PMLR, 2023. [2] Anthropic. Introducing claude. 2023. URL https://www.anthropic.com/index/ introducing-claude. [3] Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and R\u00e9mi Munos. A general theoretical paradigm to understand learning from human preferences. arXiv preprint arXiv:2310.12036, 2023. [4] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. [5] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. [6] Ahmad Beirami, Alekh Agarwal, Jonathan Berant, Alexander D\u2019Amour, Jacob Eisenstein, Chirag Nagpal, and Ananda Theertha Suresh. Theoretical guarantees on the best-of-n alignment policy. arXiv preprint arXiv:2401.01879, 2024. [7] Viktor Bengs, R\u00f3bert Busa-Fekete, Adil El Mesaoudi-Paul, and Eyke H\u00fcllermeier. Preferencebased online learning with dueling bandits: A survey. The Journal of Machine Learning Research, 22(1):278\u2013385, 2021.   \n[8] James Bennett, Stan Lanning, et al. The netfilx prize. In Proceedings of KDD cup and workshop, volume 2007, page 35. New York, 2007. [9] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324\u2013345, 1952.   \n[10] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390, 2023.   \n[11] Daniele Calandriello, Daniel Guo, Remi Munos, Mark Rowland, Yunhao Tang, Bernardo Avila Pires, Pierre Harvey Richemond, Charline Le Lan, Michal Valko, Tianqi Liu, et al. Human alignment of large language models through online preference optimisation. arXiv preprint arXiv:2403.08635, 2024.   \n[12] Xiaoyu Chen, Han Zhong, Zhuoran Yang, Zhaoran Wang, and Liwei Wang. Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation. In International Conference on Machine Learning, pages 3773\u20133793. PMLR, 2022.   \n[13] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335, 2024.   \n[14] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.   \n[15] Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensembles help mitigate overoptimization. arXiv preprint arXiv:2310.02743, 2023.   \n[16] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback, 2023.   \n[17] Qiwen Cui and Simon S Du. When are offline two-player zero-sum markov games solvable? Advances in Neural Information Processing Systems, 35:25779\u201325791, 2022.   \n[18] Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, KaShun SHUM, and Tong Zhang. RAFT: Reward ranked finetuning for generative foundation model alignment. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id $\\equiv$ m7p5O7zblY.   \n[19] Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. Rlhf workflow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863, 2024.   \n[20] Simon Du, Sham Kakade, Jason Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and Ruosong Wang. Bilinear classes: A structural framework for provable generalization in rl. In International Conference on Machine Learning, pages 2826\u20132836. PMLR, 2021.   \n[21] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387, 2023.   \n[22] Miroslav Dud\u00edk, Katja Hofmann, Robert E Schapire, Aleksandrs Slivkins, and Masrour Zoghi. Contextual dueling bandits. In Conference on Learning Theory, pages 563\u2013587. PMLR, 2015.   \n[23] Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with $\\nu$ -usable information. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 5988\u2013 6008. PMLR, 17\u201323 Jul 2022.   \n[24] Peter C Fishburn. Probabilistic social choice based on simple voting comparisons. The Review of Economic Studies, 51(4):683\u2013692, 1984.   \n[25] Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of computer and system sciences, 55(1):119\u2013139, 1997.   \n[26] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 10835\u201310866. PMLR, 2023.   \n[27] Xinyang Geng and Hao Liu. Openllama: An open reproduction of llama, May 2023. URL https://github.com/openlm-research/open_llama.   \n[28] Claudio Gentile, Zhilei Wang, and Tong Zhang. Fast rates in pool-based batch active learning. arXiv preprint arXiv:2202.05448, 2022.   \n[29] Google. Bard. 2023. URL https://bard.google.com/.   \n[30] Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023.   \n[31] Braden Hancock Hoang Tran, Chris Glaze. Snorkel-mistral-pairrm-dpo. 2024. URL https: //huggingface.co/snorkelai/Snorkel-Mistral-PairRM-DPO.   \n[32] Huggingface. Preference tuning llms with direct preference optimization methods. Blog, 2023. URL https://huggingface.co/blog/pref-tuning.   \n[33] Kaixuan Ji, Jiafan He, and Quanquan Gu. Reinforcement learning from human feedback with active queries. arXiv preprint arXiv:2402.09401, 2024.   \n[34] Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. arXiv preprint arXiv:2306.02561, 2023.   \n[35] Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? In International Conference on Machine Learning, pages 5084\u20135096. PMLR, 2021.   \n[36] W Bradley Knox and Peter Stone. Tamer: Training an agent manually via evaluative reinforcement. In 2008 7th IEEE international conference on development and learning, pages 292\u2013297. IEEE, 2008.   \n[37] Tomasz Korbak, Ethan Perez, and Christopher L Buckley. Rl with kl penalties is better viewed as bayesian inference. arXiv preprint arXiv:2205.11275, 2022.   \n[38] Gerald H Kramer. On a class of equilibrium conditions for majority rule. Econometrica: Journal of the Econometric Society, pages 285\u2013297, 1973.   \n[39] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024.   \n[40] Zihao Li, Zhuoran Yang, and Mengdi Wang. Reinforcement learning with human feedback: Learning dynamic choices via pessimism. arXiv preprint arXiv:2305.18438, 2023.   \n[41] Ziniu Li, Tian Xu, Yushun Zhang, Yang Yu, Ruoyu Sun, and Zhi-Quan Luo. Remax: A simple, effective, and efficient reinforcement learning method for aligning large language models. arXiv e-prints, pages arXiv\u20132310, 2023.   \n[42] Yong Lin, Lu Tan, Hangyu Lin, Zeming Zheng, Renjie Pi, Jipeng Zhang, Shizhe Diao, Haoxiang Wang, Han Zhao, Yuan Yao, et al. Speciality vs generality: An empirical study on catastrophic forgetting in fine-tuning foundation models. arXiv preprint arXiv:2309.06256, 2023.   \n[43] Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J Liu, and Jialu Liu. Statistical rejection sampling improves preference optimization. arXiv preprint arXiv:2309.06657, 2023.   \n[44] Zhihan Liu, Miao Lu, Wei Xiong, Han Zhong, Hao Hu, Shenao Zhang, Sirui Zheng, Zhuoran Yang, and Zhaoran Wang. One objective to rule them all: A maximization objective fusing estimation and planning for exploration. arXiv preprint arXiv:2305.18258, 2023.   \n[45] Kenneth O May. Intransitivity, utility, and the aggregation of preference patterns. Econometrica: Journal of the Econometric Society, pages 1\u201313, 1954.   \n[46] R\u00e9mi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, et al. Nash learning from human feedback. arXiv preprint arXiv:2312.00886, 2023.   \n[47] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.   \n[48] Ellen Novoseller, Yibing Wei, Yanan Sui, Yisong Yue, and Joel Burdick. Dueling posterior sampling for preference-based reinforcement learning. In Conference on Uncertainty in Artificial Intelligence, pages 1029\u20131038. PMLR, 2020.   \n[49] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.   \n[50] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.   \n[51] Aldo Pacchiano, Aadirupa Saha, and Jonathan Lee. Dueling rl: reinforcement learning with trajectory preferences. arXiv preprint arXiv:2111.04850, 2021.   \n[52] Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, et al. Large language models are effective text rankers with pairwise ranking prompting. arXiv preprint arXiv:2306.17563, 2023.   \n[53] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023.   \n[54] Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging offilne reinforcement learning and imitation learning: A tale of pessimism. Advances in Neural Information Processing Systems, 34:11702\u201311716, 2021.   \n[55] Aadirupa Saha. Optimal algorithms for stochastic contextual preference bandits. Advances in Neural Information Processing Systems, 34:30050\u201330062, 2021.   \n[56] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n[57] Paul B Simpson. On defining areas of voter choice: Professor tullock on stable voting. The Quarterly Journal of Economics, 83(3):478\u2013490, 1969.   \n[58] Gokul Swamy, Christoph Dann, Rahul Kidambi, Zhiwei Steven Wu, and Alekh Agarwal. A minimaximalist approach to reinforcement learning from human feedback. arXiv preprint arXiv:2401.04056, 2024.   \n[59] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi\u00e8re, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024.   \n[60] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[61] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Cl\u00e9mentine Fourrier, Nathan Habib, et al. Zephyr: Direct distillation of lm alignment. arXiv preprint arXiv:2310.16944, 2023.   \n[62] Amos Tversky. Intransitivity of preferences. Psychological review, 76(1):31, 1969.   \n[63] Chaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, and Yuxin Chen. Beyond reverse kl: Generalizing direct preference optimization with diverse divergence constraints. arXiv preprint arXiv:2309.16240, 2023.   \n[64] Ruosong Wang, Russ R Salakhutdinov, and Lin Yang. Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension. Advances in Neural Information Processing Systems, 33:6123\u20136135, 2020.   \n[65] Yuanhao Wang, Qinghua Liu, and Chi Jin. Is rlhf more difficult than standard rl? arXiv preprint arXiv:2306.14111, 2023.   \n[66] Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert, Olivier Delalleau, Jane Polak Scowcroft, Neel Kant, Aidan Swope, and Oleksii Kuchaiev. Helpsteer: Multi-attribute helpfulness dataset for steerlm, 2023.   \n[67] Jeff Wu, Long Ouyang, Daniel M Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano. Recursively summarizing books with human feedback. arXiv preprint arXiv:2109.10862, 2021.   \n[68] Runzhe Wu and Wen Sun. Making rl with preference-based feedback efficient via randomization. arXiv preprint arXiv:2310.14554, 2023.   \n[69] Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellmanconsistent pessimism for offline reinforcement learning. Advances in neural information processing systems, 34:6683\u20136694, 2021.   \n[70] Tengyang Xie, Dylan J Foster, Yu Bai, Nan Jiang, and Sham M Kakade. The role of coverage in online reinforcement learning. arXiv preprint arXiv:2210.04157, 2022.   \n[71] Wei Xiong, Han Zhong, Chengshuai Shi, Cong Shen, Liwei Wang, and Tong Zhang. Nearly minimax optimal offline reinforcement learning with linear function approximation: Singleagent mdp and markov game. arXiv preprint arXiv:2205.15512, 2022.   \n[72] Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, and Tong Zhang. Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint. In ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023.   \n[73] Yichong Xu, Ruosong Wang, Lin Yang, Aarti Singh, and Artur Dubrawski. Preference-based reinforcement learning with finite-time guarantees. Advances in Neural Information Processing Systems, 33:18784\u201318794, 2020.   \n[74] Chenlu Ye, Wei Xiong, Quanquan Gu, and Tong Zhang. Corruption-robust algorithms with uncertainty weighting for nonlinear contextual bandits and markov decision processes. In International Conference on Machine Learning, pages 39834\u201339863. PMLR, 2023.   \n[75] Chenlu Ye, Rui Yang, Quanquan Gu, and Tong Zhang. Corruption-robust offilne reinforcement learning with general function approximation. arXiv preprint arXiv:2310.14550, 2023.   \n[76] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. arXiv preprint arXiv:2401.10020, 2024.   \n[77] Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank responses to align language models with human feedback without tears. arXiv preprint arXiv:2304.05302, 2023.   \n[78] Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The $\\mathbf{k}$ -armed dueling bandits problem. Journal of Computer and System Sciences, 78(5):1538\u20131556, 2012.   \n[79] Andrea Zanette, Martin J Wainwright, and Emma Brunskill. Provable benefits of actor-critic methods for offilne reinforcement learning. Advances in neural information processing systems, 34:13626\u201313640, 2021.   \n[80] Wenhao Zhan, Masatoshi Uehara, Nathan Kallus, Jason D Lee, and Wen Sun. Provable offilne reinforcement learning with human feedback. arXiv preprint arXiv:2305.14816, 2023.   \n[81] Wenhao Zhan, Masatoshi Uehara, Wen Sun, and Jason D Lee. How to query human feedback efficiently in rl? arXiv preprint arXiv:2305.18505, 2023.   \n[82] Tong Zhang. Feel-good thompson sampling for contextual bandits and reinforcement learning. SIAM Journal on Mathematics of Data Science, 4(2):834\u2013857, 2022.   \n[83] Tong Zhang. Mathematical analysis of machine learning algorithms. Cambridge University Press, 2023.   \n[84] Yuheng Zhang, Yu Bai, and Nan Jiang. Offilne learning in markov games with general function approximation. arXiv preprint arXiv:2302.02571, 2023.   \n[85] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. Slichf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023.   \n[86] Han Zhong, Wei Xiong, Jiyuan Tan, Liwei Wang, Tong Zhang, Zhaoran Wang, and Zhuoran Yang. Pessimistic minimax value iteration: Provably efficient equilibrium learning from offilne datasets. In International Conference on Machine Learning, pages 27117\u201327142. PMLR, 2022.   \n[87] Han Zhong, Wei Xiong, Sirui Zheng, Liwei Wang, Zhaoran Wang, Zhuoran Yang, and Tong Zhang. Gec: A unified framework for interactive decision making in mdp, pomdp, and beyond. arXiv preprint arXiv:2211.01962, 2022.   \n[88] Banghua Zhu, Jiantao Jiao, and Michael I Jordan. Principled reinforcement learning with human feedback from pairwise or $k$ -wise comparisons. arXiv preprint arXiv:2301.11270, 2023.   \n[89] Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entropy inverse reinforcement learning. In Aaai, volume 8, pages 1433\u20131438. Chicago, IL, USA, 2008.   \n[90] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Authorship and Credit Attribution ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "All authors provided valuable contributions to this project, each bringing unique expertise and insights that were crucial for its success. ", "page_idx": 15}, {"type": "text", "text": "\u2022 CY investigated the general preference problem, proved the theoretical results for both offline and online settings, and wrote the main part of the paper.   \n\u2022 WX first proved the effectiveness of the general preference model, proposed the online iterative algorithm, contributed to the proof and paper writing, and contributed to the experiment.   \n\u2022 YZ proved the properties of the general preference problem, made important contributions to the offline result and the proof, contributed to the paper writing.   \n\u2022 HD designed the practical implementation under generalized preference model and conducted most experiments to show the effectiveness of the proposed algorithm.   \n\u2022 NJ and TZ supported and advised the junior authors\u2019 works, provided computational resources and suggested experiments and writings. ", "page_idx": 15}, {"type": "text", "text": "B Notation Table, Related Work, Experimental Details ", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "TwdX1W3M6S/tmp/159a9415970aa5e8ac4876557ae05354f42b7f84855cb7441badcae480390ce9.jpg", "table_caption": ["Table 4: The table of notations used in this paper. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "B.1 More Related Work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "RLHF. RLHF was first popularized in the deep RL literature by Christiano et al. [14], which served to direct the attention of the RL community to the preference-based feedback, but may further date back to Bennett et al. [8], Knox and Stone [36] in the context of machine learning. It has attracted significant attention recently, mainly due to its tremendous success in Chat-GPT [49]. The most popular and standard RLHF framework is outlined in Ouyang et al. [50], Touvron et al. [60] and we have described the details in Section 1. In terms of reward optimization, PPO [56] is the most well-known algorithm in LLM alignment literature. However, tuning the PPO algorithm to the best performance requires extensive efforts and the result of Chat-GPT4 [49] has not been widely reproduced so far. This motivates another line of works of algorithms that are based on supervised learning. For instance, Dong et al. [18], Yuan et al. [77], Touvron et al. [60], Gulcehre et al. [30], Ji et al. [33] propose reward ranked finetuning, (also known as rejection sampling finetuning), which essentially learns from the best-of-n policy [47] to maximize the reward. The reward-ranked finetuning algorithm is a stable policy optimization algorithm with minimal hyper-parameter configuration and was applied to the RLHF of LLaMA2 [60]. However, it is also observed that the reward ranked finetuning algorithm leads to considerable forgetting in a wide range of tasks (also referred to as the alignment tax), as the algorithmic design only considers reward optimization [60, 42, 13]. One approach to mitigate this issue is to use the KL-regularized formulation, which is widely adopted in the deep RL approach (e.g. PPO) [90, 67, 50, 4, 37, 41], and other supervised-learning-based algorithms [53, 63, 43, 3], whose theoretical property is studied in Xiong et al. [72]. Among them, (offilne) Direct Preference Optimization (DPO) [53] has emerged as an attractive alternative approach to PPO with notable stability and competitive performance. Xiong et al. [72], Hoang Tran [31], Yuan et al. [76] further extend the offline DPO to the iterative (online) variant, and the resulting models demonstrate impressive performance [31, 19]. However, all these algorithms are designed under the reward-based RLHF framework to maximize the underlying reward function (with appropriate regularization). ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "B.2 Details of Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Bradley-Terry model construction. We follow the previous works [50, 4] to initialize the reward model using an SFT model but replace the last layer with a linear head to predict a scalar score. The loss function of reward modeling is the negative log-likelihood so that minimizing the loss is equivalent to MLE: ", "page_idx": 16}, {"type": "equation", "text": "$$\nL_{\\mathrm{RM}}(\\theta)=-\\mathbb{E}_{x,a^{w},a^{l}\\sim\\mathcal{D}}\\log\\sigma\\big(r_{\\theta}(x,a^{w})-r_{\\theta}(x,a^{l})\\big),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $a^{w}$ is the preferred response over $a^{l}$ . We train the model for one epoch and use a batch size of 256, a learning rate of $\\mathrm{lr}=1\\mathrm{e}{-5}$ , and a cosine learning rate schedule with a warm-up ratio of 0.03. ", "page_idx": 16}, {"type": "text", "text": "Ground-truth preference model for simulation. Ideally, the $P^{*}$ is supposed to be a group of human labelers or closed-source LLMs like Chat-GPT. Unfortunately, due to resource constraints, we cannot afford the cost of using these preference oracles. Instead, we follow Gao et al. [26] to use a strong preference model to serve as the $P^{*}$ in the simulation. Specifically, we adopt the LLaMA3-8B, and train the preference model on a diverse set of open-source preference datasets including HHRLHF [4], Stanford Human Preferences Dataset (SHP) [23], Ultra-feedback [16], HelpSteer [66], distilabel-capybara14, distilabel-orca15, and UltraInteract16. Motivated by the Theorem 1 as well as the practical application [50], we include more than 1 comparison pair when a prompt is with more than 2 responses for better coverage. To be specific, ", "page_idx": 16}, {"type": "text", "text": "\u2022 for SHP, we only use the samples with score ratio $>2$ , and for each prompt, we take at most 5 comparison pairs; ", "page_idx": 16}, {"type": "text", "text": "\u2022 for HelpSteer, we use all the possible pairs except for those with the same score where the score is averaged over helpfulness and correctness; ", "page_idx": 16}, {"type": "text", "text": "\u2022 for UltraFeedback, we use all possible pairs except for those with the same score where the score is averaged over all attributes; ", "page_idx": 16}, {"type": "text", "text": "\u2022 for UltraInteract, we take a subset of 150K pairs into the mixture. ", "page_idx": 16}, {"type": "text", "text": "We have about 700K preference pairs in our training stage. We use the package axolotl17 to perform supervised fine-tuning, with the detailed hyper-parameters given in Appendix B.2. The resulting preference models are evaluated by the reward bench [39], with the results summarized in Table 1. The preference model based on LLaMA3-8B-it achieves state-of-the-art test accuracy and can serve as a stable preference oracle for the simulation study. ", "page_idx": 16}, {"type": "text", "text": "We present the hyper-parameters in Table 5. All experiments are conducted on $8\\!\\times\\!\\mathrm{Al00}\u201340\\mathrm{G}$ with Deepspeed ZeRO-3. ", "page_idx": 16}, {"type": "table", "img_path": "TwdX1W3M6S/tmp/231a17925f349d67e3f212930368005c3b014f0458f611588e94490a968170ec.jpg", "table_caption": ["Table 5: Hyper-parameters for reward modeling and preference model construction. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Examples from Ultra-feedback. We provide several examples here: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Create a list of three mistakes to avoid when designing an AI assistant. \u2022 Pretend you\u2019re a next.js expert, write ad copy about a free trial on Vercel. \u2022 Can you describe the role of photography in shaping the art world? ", "page_idx": 17}, {"type": "text", "text": "C Proofs for the Offline Setting ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 Proof for Theorem 1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Lemma 1. Under Assumption $^{\\,I}$ , with probability at least $1-\\delta$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}(\\hat{P}(x_{i},a_{i}^{1},a_{i}^{2})-P^{*}(x_{i},a_{i}^{1},a_{i}^{2}))^{2}\\leq\\log(|\\mathcal{P}|/\\delta).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma $^{\\,l}$ . For any fixed function $P\\,\\in\\,\\mathcal P$ , we first upper bound its logarithmic moment generating function: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log\\mathbb E\\exp\\left(\\displaystyle\\sum_{i=1}^{n}\\log\\frac{P(y_{i}|x_{i},a_{i}^{1},a_{i}^{2})}{P^{*}(y_{i}|x_{i},a_{i}^{1},a_{i}^{2})}\\right)}\\\\ &{=\\log\\mathbb E\\exp\\left(\\displaystyle\\sum_{i=1}^{n-1}\\log\\frac{P(y_{i}|x_{i},a_{i}^{1},a_{i}^{2})}{P^{*}(y_{i}|x_{i},a_{i}^{1},a_{i}^{2})}\\right)+\\log2\\mathbb E_{y_{n}|z_{n},a_{n}^{1},a_{n}^{2}}\\sqrt{\\frac{P(y_{n}|x_{n},a_{n}^{1},a_{n}^{2})}{P^{*}(y_{n}|x_{n},a_{n}^{2})}}}\\\\ &{=\\log\\mathbb E\\exp\\left(\\displaystyle\\sum_{i=1}^{n-1}\\log\\frac{P(y_{i}|x_{i},a_{i}^{1},a_{i}^{2})}{P^{*}(y_{i}|x_{i},a_{i}^{1},a_{i}^{2})}\\right)+\\log\\left(1-H\\big(P(y_{n}|x_{n},a_{n}^{1},a_{n}^{2})\\big|P^{*}(y_{n}|x_{n},a_{n}^{1},a_{n}^{2})\\big)^{2}\\right)}\\\\ &{\\leq\\log\\mathbb E\\exp\\left(\\displaystyle\\sum_{i=1}^{n-1}\\log\\frac{P(y_{i}|x_{i},a_{i}^{1},a_{i}^{2})}{P^{*}(y_{i}|x_{i},a_{i}^{1},a_{i}^{2})}\\right)-H\\Big(P(y_{n}|x_{n},a_{n}^{1},a_{n}^{2}||P^{*}(y_{n}|x_{n},a_{n}^{1},a_{n}^{2}))\\Big)^{2}}\\\\ &{\\leq\\ldots\\leq-\\displaystyle\\sum_{i=1}^{n}H\\Big(P(y_{i}|x_{i},a_{i}^{1},a_{i}^{2}||P^{*}(y_{i}|x_{i},a_{i}^{1},a_{i}^{2}))\\Big)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We continue to lower-bound the Hellinger by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{n}\\Big(H(P(y_{i}|x_{i},a_{i}^{1},a_{i}^{2})||P^{*}(y_{i}|x_{i},a_{i}^{1},a_{i}^{2}))\\Big)^{2}}\\\\ &{\\displaystyle\\geq\\sum_{i=1}^{n}\\Big(\\mathrm{TV}(P(y_{i}|x_{i},a_{i}^{1},a_{i}^{2})||P^{*}(y_{i}|x_{i},a_{i}^{1},a_{i}^{2}))\\Big)^{2}}\\\\ &{\\displaystyle=\\sum_{i=1}^{n}(P(x_{i},a_{i}^{1},a_{i}^{2})-P^{*}(x_{i},a_{i}^{1},a_{i}^{2}))^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the inequality uses the fact that for any distribution $p,q,\\,H(p,q)\\geq\\operatorname{TV}(p,q)$ according to Theorem B.9 of Zhang [83]. ", "page_idx": 18}, {"type": "text", "text": "Then, by invoking Lemma 6, we obtain for any $P\\in\\mathcal P$ , with probability at least $1-\\delta$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{i=1}^{n}\\log\\frac{P(y_{i}|x_{i},a_{i}^{1},a_{i}^{2})}{P^{*}(y_{i}|x_{i},a_{i}^{1},a_{i}^{2})}\\le\\log(|\\mathcal{P}|/\\delta)+\\log\\mathbb{E}\\exp\\bigg(\\displaystyle\\sum_{i=1}^{n}\\log\\frac{P(y_{i}|x_{i},a_{i}^{1},a_{i}^{2})}{P^{*}(y_{i}|x_{i},a_{i}^{1},a_{i}^{2})}\\bigg)}&{}\\\\ {\\displaystyle\\le-\\sum_{i=1}^{n}H\\Big(P(y_{i}|x_{i},a_{i}^{1},a_{i}^{2})\\|P^{*}(y_{i}|x_{i},a_{i}^{1},a_{i}^{2})\\Big)\\Big)^{2}+\\log(|\\mathcal{P}|/\\delta)}&{}\\\\ {\\displaystyle\\le-\\sum_{i=1}^{n}(P(x_{i},a_{i}^{1},a_{i}^{2})-P^{*}(x_{i},a_{i}^{1},a_{i}^{2}))^{2}+\\log(|\\mathcal{P}|/\\delta),}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the second inequality uses (14), and the last inequality uses (15). By taking $P$ as $\\hat{P}$ , since $\\hat{P}$ is the MLE, we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}(\\hat{P}(x_{i},a_{i}^{1},a_{i}^{2})-P^{*}(x_{i},a_{i}^{1},a_{i}^{2}))^{2}\\leq\\sum_{i=1}^{n}\\log\\frac{P^{*}(y_{i}|x_{i},a_{i}^{1},a_{i}^{2})}{P_{\\hat{P}}(y_{i}|x_{i},a_{i}^{1},a_{i}^{2})}+\\log(|\\mathcal{P}|/\\delta)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof of Theorem $^{\\,l}$ . Let ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{\\pi}^{1},\\widetilde{\\pi}^{2})=\\arg\\operatorname*{max}_{\\pi^{1}\\in\\Pi}\\arg\\operatorname*{min}_{\\pi^{2}\\in\\Pi}\\operatorname*{min}_{P\\in\\widehat{\\mathcal{P}}}\\mathbb{E}_{x\\sim d_{0}}\\mathbb{E}_{a^{1}\\sim\\pi^{1},a^{2}\\sim\\pi^{2}}\\Big[P(x,a^{1},a^{2})+\\eta^{-1}\\log\\frac{\\pi_{0}(a^{1}|x)}{\\pi^{1}(a^{1}|x)}-\\eta^{-1}\\log\\frac{\\pi_{0}(a^{1}|x)}{\\pi^{1}(a^{1}|x)}\\Big].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and use the notation ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\underline{{J}}(\\pi^{1},\\pi^{2})=\\operatorname*{min}_{P\\in\\widehat{\\mathcal{P}}}\\mathbb{E}_{x\\sim d_{0}}\\mathbb{E}_{a^{1}\\sim\\pi^{1},a^{2}\\sim\\pi^{2}}\\Big[P(x,a^{1},a^{2})+\\eta^{-1}\\log\\frac{\\pi_{0}(a^{1}|x)}{\\pi^{1}(a^{1}|x)}-\\eta^{-1}\\log\\frac{\\pi_{0}(a^{2}|x)}{\\pi^{2}(a^{2}|x)}\\Big].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let $\\widetilde{\\pi}_{\\ast}^{2}=\\operatorname*{min}_{\\pi^{2}\\in\\Pi}\\underline{{J}}(\\pi_{\\ast}^{1},\\pi^{2})$ and $\\pi^{\\dagger,2}=\\operatorname*{min}_{\\pi^{2}\\in\\Pi}J(\\hat{\\pi}^{1},\\pi^{2})$ . The following decomposition holds ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I(\\pi_{*}^{1},\\pi_{*}^{2})-J(\\hat{\\pi}^{1},\\pi^{\\dagger,2})\\leq\\underbrace{J(\\pi_{*}^{1},\\pi_{*}^{2})-J(\\pi_{*}^{1},\\widetilde{\\pi}_{*}^{2})}_{q_{1}}+\\underbrace{J(\\pi_{*}^{1},\\widetilde{\\pi}_{*}^{2})-\\underline{{J}}(\\pi_{*}^{1},\\widetilde{\\pi}_{*}^{2})}_{q_{2}}+\\underbrace{\\underline{{J}}(\\pi_{*}^{1},\\widetilde{\\pi}_{*}^{2})-\\underline{{J}}(\\hat{\\pi}^{1},\\widetilde{\\pi}_{*}^{2})}_{q_{3}}-\\underbrace{J(\\hat{\\pi}^{1},\\widetilde{\\pi}^{2})}_{q_{3}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\underbrace{\\underline{{J}}(\\hat{\\pi}^{1},\\widetilde{\\pi}^{2})-\\underline{{J}}(\\hat{\\pi}^{1},\\pi^{\\dagger,2})}_{q_{4}}+\\underbrace{J(\\hat{\\pi}^{1},\\pi^{\\dagger,2})-J(\\hat{\\pi}^{1},\\pi^{\\dagger,2})}_{q_{5}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, we bound these terms separately. For the term $q_{1}$ , since $(\\pi_{*}^{1},\\pi_{*}^{2})$ is the Nash equilibrium of $J$ , we have $q_{1}\\leq0$ . For the term $q_{2}$ , ", "page_idx": 18}, {"type": "text", "text": "q $\\begin{array}{r l}&{\\mathfrak{r}_{2}=\\!\\mathbb E_{x\\sim d_{0}}\\mathbb E_{a^{1}\\sim\\pi_{*}^{1},a^{2}\\sim\\widetilde{\\pi}_{*}^{2}}P^{*}(x,a^{1},a^{2})-\\operatorname*{min}_{\\mathbb P\\in\\widetilde{\\mathcal P}}\\mathbb E_{x\\sim d_{0}}\\mathbb E_{a^{1}\\sim\\pi_{*}^{1},a^{2}\\sim\\widetilde{\\pi}_{*}^{2}}P(x,a^{1},a^{2})}\\\\ &{\\quad=\\!\\operatorname*{min}\\mathbb E_{x\\sim d_{0}}\\!\\mathbb E_{a^{1}\\sim\\pi_{*}^{1},a^{2}\\sim\\widetilde{\\pi}_{*}^{2}}[\\hat{P}(x,a^{1},a^{2})-P(x,a^{1},a^{2})]+\\mathbb E_{x\\sim d_{0}}\\mathbb E_{a^{1}\\sim\\pi_{*}^{1},a^{2}\\sim\\widetilde{\\pi}_{*}^{2}}[P^{*}(x,a^{1},a^{2})-P^{*}(x,a^{2})]}\\\\ &{\\quad<\\!2\\beta\\widetilde{\\Gamma}(\\pi^{1},\\widetilde{\\pi}^{2}).}\\end{array}$ \u02c6(x, a1, a2)] \u22642\u03b2\u0393 (\u03c0\u22171,\u03c0\u22172), ", "page_idx": 18}, {"type": "text", "text": "where we define ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\widetilde\\Gamma(\\pi^{1},\\pi^{2}):=\\operatorname*{sup}_{P\\in\\widehat{\\mathcal{P}}}\\frac{|\\mathbb{E}_{x\\sim d_{0}}[P(x,\\pi^{1},\\pi^{2})-\\hat{P}(x,\\pi^{1},\\pi^{2})]|}{\\sqrt{\\lambda+\\sum_{i=1}^{n}(P(x_{i},a_{i}^{1},a_{i}^{2})-\\hat{P}(x_{i},a_{i}^{1},a_{i}^{2}))^{2}}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By the optimality of $\\hat{\\pi}^{1}$ , term $q_{3}\\leq0$ . Since $\\widetilde{\\pi}^{2}$ is the best response to $\\hat{\\pi}^{1}$ with respect to $\\underline{{J}}$ , we have $q_{4}\\leq0$ . From Lemma 1, we know that $P^{\\star}\\in\\widehat{\\mathcal{P}}$ , thus $q_{5}\\leq0$ . Putting everything together, we obtain that ", "page_idx": 19}, {"type": "equation", "text": "$$\nJ(\\pi_{*}^{1},\\pi_{*}^{2})-J(\\hat{\\pi}^{1},\\pi^{\\dagger,2})\\leq2\\beta\\widetilde{\\Gamma}(\\pi_{*}^{1},\\widetilde{\\pi}_{*}^{2}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, by invoking Lemma 8 with a union bound over $P\\in\\mathcal P$ , with probability at least $1-\\delta$ , we obtain that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0.5n\\mathbb{E}_{x\\sim d_{0},a^{1}\\sim\\pi_{D}^{1},a^{2}\\sim\\pi_{D}^{2}}(P(x,a^{1},a^{2})-\\hat{P}(x,a^{1},a^{2}))^{2}}\\\\ &{\\qquad\\le\\displaystyle\\sum_{i=1}^{n}(P(x_{i},a_{i}^{1},a_{i}^{2})-\\hat{P}(x_{i},a_{i}^{1},a_{i}^{2}))^{2}+\\log(|\\mathcal{P}|/\\delta),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which implies that with probability at least $1-\\delta$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\widetilde{\\Gamma}(\\pi_{*}^{1},\\widetilde{\\pi}_{*}^{2})}{P(\\pi_{*}^{1},\\widetilde{\\pi}_{*}^{2})}}\\\\ &{=\\underset{P\\in\\mathcal{P}}{\\operatorname*{sup}}\\,\\frac{\\|\\mathbb{E}_{x\\sim d_{0}}[P(x,\\pi_{*}^{1},\\widetilde{\\pi}_{*}^{2})-\\hat{P}(x,\\pi_{*}^{1},\\widetilde{\\pi}^{2})]\\|}{\\sqrt{\\lambda+\\sum_{i=1}^{n}(P(x_{i},a_{i}^{1},a_{i}^{2})-\\hat{P}(x_{i},a_{i}^{1},a_{i}^{2}))^{2}}}}\\\\ &{\\le\\underset{P\\in\\mathcal{P}}{\\operatorname*{sup}}\\,\\frac{\\|\\mathbb{E}_{x\\sim d_{0}}[P(x,\\pi_{*}^{1},\\widetilde{\\pi}_{*}^{2})-\\hat{P}(x,\\pi_{*}^{1},\\widetilde{\\pi}_{*}^{2})]\\|}{\\sqrt{\\lambda-\\log(|P|/\\delta)+0.5n\\mathbb{E}_{x\\sim d_{0}}a_{1}\\nu\\pi_{*}^{1}\\alpha^{2}\\pi_{*}^{2}P(X,a^{1},a^{2})-\\hat{P}(x,a^{1},a^{2}))^{2}}}}\\\\ &{=\\sqrt{\\frac{2}{n}}\\,\\underset{P\\in\\mathcal{P}}{\\operatorname*{sup}}\\,\\frac{\\big|\\mathbb{E}_{x\\sim d_{0}}[P(x,\\pi_{*}^{1},\\widetilde{\\pi}_{*}^{2})-\\hat{P}(x,\\pi_{*}^{1},\\widetilde{\\pi}_{*}^{2})]\\big|}{\\sqrt{\\mathbb{E}_{x\\sim d_{0}}a_{1}\\nu\\pi_{*}^{1}}}}\\\\ &{=\\!\\sqrt{\\!\\frac{2C(\\pi_{*}^{1},\\pi_{D},P)}{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, we complete the proof. ", "page_idx": 19}, {"type": "text", "text": "C.2 Learning with Pessimism via Uncertainty Bonus ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this subsection, we introduce another offline algorithm, Pessimistic Equilibrium Learning from Human Feedback (PELHF) with Uncertainty Bonus in Algorithm 3. Given an offline dataset $\\mathcal{D}_{\\mathrm{off}}$ , we first obtain the maximum likelihood estimation (MLE) by maximizing (7). Then, we take the lower confidence bound (LCB) for the max-player as the preference estimations by subtracting a bonus function $\\beta\\Gamma(\\cdot,\\cdot,\\cdot)$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underline{{J}}(x,\\pi^{1},\\pi^{2})=\\mathbb{E}_{a^{1}\\sim\\pi^{1},a^{2}\\sim\\pi^{2}}\\Big[\\hat{P}(x,a^{1},a^{2})-\\beta\\Gamma(x,a^{1},a^{2})+\\eta^{-1}\\log\\frac{\\pi_{0}(a^{1}|x)}{\\pi^{1}(a^{1}|x)}-\\eta^{-1}\\log\\frac{\\pi_{0}(a^{2}|x)}{\\pi^{2}(a^{2}|x)}\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, we obtain the policy $\\hat{\\pi}^{1}$ by solving the minimax problems with $\\underline{{J}}$ . We now discuss how to construct the bonus function to ensure pessimism. ", "page_idx": 19}, {"type": "text", "text": "Bonus Construction. The bonus function $\\Gamma:\\mathcal{X}\\times\\mathcal{A}\\times\\mathcal{A}\\rightarrow\\mathbb{R}^{+}$ serves to control the point-wise confidence interval so that with high probability, $\\hat{P}(x,a^{1},a^{2})-\\beta\\Gamma(x,a^{1},a^{2})\\leq P^{*}(x,a^{1},a^{2})\\leq$ $\\hat{P}(x,a^{1},a^{2})+\\beta\\Gamma(x,a^{1},a^{2})$ holds for any $(x,a^{1},a^{2})$ . To this end, we construct the bonus as the ratio between the out-of-sample error and the in-sample error on the preference dataset $\\mathcal{D}_{\\mathrm{off}}$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Gamma(x,\\pi^{1},\\pi^{2})=\\operatorname*{sup}_{P\\in\\mathcal{P}}\\frac{|P(x,\\pi^{1},\\pi^{2})-\\hat{P}(x,\\pi^{1},\\pi^{2})|}{\\sqrt{\\lambda+\\sum_{i=1}^{n}(P(x_{i},a_{i}^{1},a_{i}^{2})-\\hat{P}(x_{i},a_{i}^{1},a_{i}^{2}))^{2}}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we also set $\\beta$ as an upper bound of the $\\lambda$ -regularized in-sample error. This uncertainty is also characterized by the relative preference function class and shares a similar spirit with the information ratio considered in Zhang [83], Ye et al. [74, 75], which depicts the uncertainty with respect to the value function class. Also see Definition 3 for a more detailed illustration. ", "page_idx": 19}, {"type": "text", "text": "1: Input: Dataset $\\mathcal{D}_{\\mathrm{off}}=\\{(x_{i},a_{i}^{1},a_{i}^{2},y_{i})\\}_{i=1}^{n}$ , preference space $\\mathcal{P}$ , policy class $\\Pi$ , parameter $\\eta,\\beta>0$ .   \n2: Compute the MLE $\\hat{P}$ with $\\ell_{\\mathcal{D}_{\\mathrm{off}}}$ defined in (7) and construct bonus as in (19).   \n3: Compute the best policy under conservative estimation ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\bar{\\pi}}^{1}(\\cdot|x)=\\underset{\\pi^{1}\\in\\Pi}{\\mathrm{argmax~min}}~\\mathbb{E}_{a^{1}\\sim\\pi^{1},a^{2}\\sim\\pi^{2}}\\left[\\hat{P}(x,a^{1},a^{2})-\\beta\\Gamma(x,\\pi^{1},\\pi^{2})+\\eta^{-1}\\log\\frac{\\pi_{0}(a^{1}|x)}{\\pi^{1}(a^{1}|x)}-\\eta^{-1}\\log\\frac{\\pi_{0}(a^{2}|x)}{\\pi^{2}(a^{2}|x)}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "4: Output: $\\hat{\\pi}^{1}$ ", "page_idx": 20}, {"type": "text", "text": "C.2.1 Analysis for Algorithm 3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Now, we are ready to present the suboptimality bound of $\\hat{\\pi}^{1}$ from Algorithm 3. ", "page_idx": 20}, {"type": "text", "text": "Theorem 3. If we set $\\lambda=\\log(|\\mathcal{P}|/\\delta)$ and $\\beta^{2}=2\\log(|\\mathcal{P}|/\\delta),$ , then, with probability at least $1-\\delta$ , the output policy of Algorithm $^3$ satisfies ", "page_idx": 20}, {"type": "equation", "text": "$$\nJ(\\pi_{1}^{*},\\pi_{2}^{*})-J(\\hat{\\pi}^{1},\\dag)\\leq4\\beta\\sqrt{\\frac{\\widetilde{\\mathcal{C}}(\\pi_{*}^{1},\\pi_{D},\\mathcal{P})}{n}}-\\mathbb{E}_{x\\sim d_{0}}\\big[\\eta^{-1}D_{\\mathrm{KL}}(\\pi_{*}^{1}(\\cdot|x)\\|\\hat{\\pi}^{1}(\\cdot|x))\\big].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{C}}(\\pi_{*}^{1},\\pi_{D},\\mathcal{P})=\\operatorname*{max}_{\\pi^{2}\\in\\Pi}\\mathbb{E}_{x\\sim d_{0}}\\operatorname*{sup}_{\\hat{P}\\in\\mathcal{P}}\\frac{(P(x,\\pi_{*}^{1},\\pi^{2})-\\hat{P}(x,\\pi_{*}^{1},\\pi^{2}))^{2}}{\\mathbb{E}_{x\\sim d_{0},a^{1}\\sim\\pi_{D}^{1},a^{2}\\sim\\pi_{D}^{2}}(P(x,a^{1},a^{2})-\\hat{P}(x,a^{1},a^{2}))^{2}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Recall that our pessimistic value estimations are ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{I(x,\\pi^{1},\\pi^{2})=\\mathbb{E}_{a^{1}\\sim\\pi^{1},a^{2}\\sim\\pi^{2}}\\Big[\\hat{P}(x,a^{1},a^{2})-\\beta\\Gamma(x,a^{1},a^{2})+\\eta^{-1}\\log\\frac{\\pi_{0}(a^{1}|x)}{\\pi^{1}(a^{1}|x)}-\\eta^{-1}\\log\\frac{\\pi_{0}(a^{2}|x)}{\\pi^{2}(a^{2}|x)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For convenience, we also use the notation ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{J(x,\\pi^{1},\\pi^{2})=\\mathbb{E}_{a^{1}\\sim\\pi^{1},a^{2}\\sim\\pi^{2}}\\Big[P^{*}(x,a^{1},a^{2})+\\eta^{-1}\\log\\frac{\\pi_{0}(a^{1}|x)}{\\pi^{1}(a^{1}|x)}-\\eta^{-1}\\log\\frac{\\pi_{0}(a^{2}|x)}{\\pi^{2}(a^{2}|x)}\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We decompose the suboptimality gap of $\\hat{\\pi}^{1}$ at prompt $x$ as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J(x,\\pi_{*}^{1},\\pi_{*}^{2})-J(x,\\hat{\\pi}^{1},\\dagger)\\leq\\underbrace{J(x,\\hat{\\pi}^{1},\\widetilde{\\pi}^{2})-J(x,\\hat{\\pi}^{1},\\dagger)}_{p_{1}}+\\underbrace{J(x,\\pi_{*}^{1},\\widetilde{\\pi}^{2})-J(x,\\hat{\\pi}^{1},\\widetilde{\\pi}^{2})}_{p_{2}}}\\\\ &{\\phantom{\\quad\\quad\\quad}+\\underbrace{J(x,\\pi_{*}^{1},\\widetilde{\\pi}^{2})-J(x,\\pi_{*}^{1},\\widetilde{\\pi}^{2})}_{p_{3}}+\\underbrace{J(x,\\pi_{*}^{1},\\pi_{*}^{2})-J(x,\\pi_{*}^{1},\\widetilde{\\pi}^{2})}_{p_{4}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We proceed based on assuming the following event holds: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}(\\hat{P}(x_{i},a_{i}^{1},a_{i}^{2})-P^{*}(x_{i},a_{i}^{1},a_{i}^{2}))^{2}\\leq\\beta^{2}/2.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For the term $p_{1}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nu_{1}=\\!\\!\\underline{{J}}(x,\\hat{\\pi}^{1},\\widetilde{\\pi}^{2})-\\operatorname*{min}\\Big\\{P^{*}(x,\\hat{\\pi}^{1},\\pi^{2})-\\eta^{-1}D_{\\mathrm{KL}}(\\hat{\\pi}^{1}(\\cdot|x)\\|\\pi_{0}(\\cdot|x))+\\eta^{-1}D_{\\mathrm{KL}}(\\pi^{2}(\\cdot|x)\\|\\pi_{0}(\\cdot|x))\\Big\\}}\\\\ &{\\quad=\\!\\!\\underline{{J}}(x,\\hat{\\pi}^{1},\\widetilde{\\pi}^{2})-\\operatorname*{min}\\Big\\{P^{*}(x,\\hat{\\pi}^{1},\\pi^{2})-\\hat{P}}(x,\\hat{\\pi}^{1},\\pi^{2})+\\hat{P}(x,\\hat{\\pi}^{1},\\pi^{2})-\\eta^{-1}D_{\\mathrm{KL}}(\\hat{\\pi}^{1}(\\cdot|x)\\|\\pi_{0}(\\cdot|x))\\Big\\}}\\\\ &{\\quad\\le\\!\\!\\underline{{J}}(x,\\hat{\\pi}^{1},\\widetilde{\\pi}^{2})-\\operatorname*{min}\\Big\\{\\hat{P}(x,\\hat{\\pi}^{1},\\pi^{2})-\\beta\\Gamma(x,\\hat{\\pi}^{1},\\pi^{2})-\\eta^{-1}D_{\\mathrm{KL}}(\\hat{\\pi}^{1}(\\cdot|x)\\|\\pi_{0}(\\cdot|x))+\\eta^{-1}D_{\\mathrm{KL}}(\\pi^{2}(\\cdot|x)\\|\\pi_{0}(\\cdot|x))\\Big\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the inequality is because ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{P^{*}(x,\\hat{\\pi}^{1},\\pi^{2})-\\hat{P}(x,\\hat{\\pi}^{1},\\pi^{2})}\\\\ {\\displaystyle\\geq-\\,\\sqrt{\\lambda+\\sum_{i=1}^{n}(P^{*}(x_{i},a_{i}^{1},a_{i}^{2})-\\hat{P}(x_{i},a_{i}^{1},a_{i}^{2}))^{2}}\\cdot\\operatorname*{sup}_{P^{\\prime}\\in\\mathcal P}\\frac{\\vert\\mathbb{E}_{a^{1}\\sim\\hat{\\pi}^{1},a_{2}\\sim\\pi^{2}}[P^{\\prime}(x,a^{1},a^{2})-\\hat{P}(x,a^{1},a^{2})]^{2}}{P^{\\prime}\\in\\mathcal P}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Here the last step uses Lemma 1 to bound the in-sample error. For the term $p_{2}$ , we can write it as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{2}=\\hat{P}(x,\\pi_{*}^{1},\\widetilde{\\pi}^{2})-\\beta\\Gamma(x,\\pi_{*}^{1},\\widetilde{\\pi}^{2})-\\hat{P}(x,\\hat{\\pi}^{1},\\widetilde{\\pi}^{2})+\\beta\\Gamma(x,\\hat{\\pi}^{1},\\widetilde{\\pi}^{2})\\phantom{x x x x x x x x x x x x x x}}\\\\ &{\\qquad\\qquad-\\eta^{-1}D_{\\mathrm{KL}}(\\pi_{*}^{1}(\\cdot|x)||\\pi_{0}(\\cdot|x))+\\eta^{-1}D_{\\mathrm{KL}}(\\hat{\\pi}^{1}(\\cdot|x)||\\pi_{0}(\\cdot|x)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We note that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\pi}^{1}=\\underset{\\pi^{1}}{\\mathrm{argmax}}\\frac{J(x,\\pi^{1},\\widetilde\\pi^{2})}{\\pi^{1}}}\\\\ &{\\quad=\\underset{\\pi^{1}}{\\mathrm{argmax}}\\mathbb{E}_{a^{1}\\sim\\pi^{1},a^{2}\\sim\\widetilde\\pi^{2}}\\left[\\hat{P}(x,a^{1},a^{2})-\\beta\\Gamma(x,a^{1},a^{2})+\\eta^{-1}\\log\\frac{\\pi_{0}(a^{1}|x)}{\\pi^{1}(a^{1}|x)}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, we can invoke Lemma 9 with $\\pi=\\pi_{*}^{1}$ , $\\hat{\\pi}=\\hat{\\pi}^{1}$ , and $\\hat{P}(x,a)=\\hat{P}(x,a,\\widetilde{\\pi}^{2})-\\beta\\Gamma(x,a,\\widetilde{\\pi}^{2})$ to obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{P}(x,\\pi_{*}^{1},\\widetilde{\\pi}^{2})-\\beta\\Gamma(x,\\pi_{*}^{1},\\widetilde{\\pi}^{2})-\\hat{P}(x,\\hat{\\pi}^{1},\\widetilde{\\pi}^{2})+\\beta\\Gamma(x,\\hat{\\pi}^{1},\\widetilde{\\pi}^{2})}\\\\ &{\\qquad+\\eta^{-1}D_{\\mathrm{KL}}(\\hat{\\pi}^{1}(\\cdot|x)\\|\\pi_{0}(\\cdot|x))-\\eta^{-1}D_{\\mathrm{KL}}(\\pi_{*}^{1}(\\cdot|x)\\|\\pi_{0}(\\cdot|x))}\\\\ &{=-\\eta^{-1}D_{\\mathrm{KL}}(\\pi_{*}^{1}(\\cdot|x)\\|\\hat{\\pi}^{1}(\\cdot|x)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which implies that ", "page_idx": 21}, {"type": "equation", "text": "$$\np_{2}=-\\eta^{-1}D_{\\mathrm{KL}}(\\pi_{*}^{1}(\\cdot|x)\\|\\hat{\\pi}^{1}(\\cdot|x)).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For the term $p_{3}$ , we can also get from Lemma 1 that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{3}=\\!P^{*}(x,\\pi_{*}^{1},\\widetilde{\\pi}^{2})-\\hat{P}(x,\\pi_{*}^{1},\\widetilde{\\pi}^{2})+\\beta\\Gamma(x,\\pi_{*}^{1},\\widetilde{\\pi}^{2})}\\\\ &{\\quad\\le\\!2\\beta\\Gamma(x,\\pi_{*}^{1},\\widetilde{\\pi}^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "According to Lemma 5, since $\\pi_{*}^{2}$ is the best response to $\\pi_{*}^{1}$ with respect to $J(x,\\cdot,\\cdot)$ , we have $p_{4}\\leq0$ . Putting everything together, we have with probability at least $1-\\delta$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\nJ(x,\\pi_{*}^{1},\\pi_{*}^{2})-J(x,\\hat{\\pi}^{1},\\dag)\\leq2\\beta\\Gamma(x,\\pi_{*}^{1},\\widetilde{\\pi}^{2})-\\eta^{-1}D_{\\mathrm{KL}}(\\pi_{*}^{1}(\\cdot|x)\\|\\hat{\\pi}^{1}(\\cdot|x)).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Similar to the proof of Theorem 1, we invoke Lemma 8 with a union bound over $P\\in\\mathcal P$ and obtain that with probability at least $1-\\delta$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0.5n\\mathbb{E}_{x\\sim d_{0},a^{1}\\sim\\pi_{D}^{1},a^{2}\\sim\\pi_{D}^{2}}(P(x,a^{1},a^{2})-\\hat{P}(x_{s},a_{s}^{1},a_{s}^{2}))^{2}}\\\\ &{\\qquad\\le\\displaystyle\\sum_{i=1}^{n}(P(x_{i},a_{i}^{1},a_{i}^{2})-P^{*}(x_{i},a_{i}^{1},a_{i}^{2}))^{2}+\\log(|\\mathcal P|/\\delta),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which implies that probability at least $1-\\delta$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{E_{x}\\sim\\!d_{0}!}\\operatorname*{sup}_{P\\in\\boldsymbol{P}}\\frac{1}{\\sqrt{\\lambda+\\sum_{i=1}^{n}(P(x_{i},a_{i}^{1},\\tilde{\\pi}^{2})-\\hat{P}(x,\\pi_{i}^{1},\\tilde{\\pi}^{2}))}}}\\\\ &{=\\!\\!\\mathbb{E}_{x\\sim\\!d_{0}}\\frac{1}{P\\in\\boldsymbol{P}}\\frac{|P(x,\\pi_{i}^{1},\\tilde{\\pi}^{2})-\\hat{P}(x,a_{i}^{1},a_{i}^{2})|}{\\sqrt{\\lambda+\\sum_{i=1}^{n}(P(x_{i},a_{i}^{1},a_{i}^{2})-\\hat{P}(x,a_{i}^{1},a_{i}^{2}))^{2}}}}\\\\ &{\\leq\\!\\!\\mathbb{E}_{x\\sim\\!d_{0}}\\operatorname*{sup}_{P\\in\\boldsymbol{P}}\\frac{|P(x,\\pi_{i}^{1},\\tilde{\\pi}^{2})-\\hat{P}(x,\\pi_{i}^{1},\\tilde{\\pi}^{2})|}{\\sqrt{\\lambda-\\log(|P|/\\tilde{\\pi})+(0.5n\\mathbb{E}_{x\\sim d_{0}}\\alpha_{\\star}n_{\\star}^{1},\\alpha^{2}\\circ\\pi_{P}^{2}(P(x,a^{1},a^{2})-\\hat{P}(x,a^{1},a^{2}))^{2}}}}\\\\ &{=\\!\\!\\sqrt{\\frac{2}{n}}\\!\\!\\!\\mathbb{E}_{x\\sim\\!d_{0}}\\operatorname*{sup}_{P\\in\\boldsymbol{P}}\\frac{|P(x,\\pi_{i}^{1},\\tilde{\\pi}^{2})-\\hat{P}(x,\\pi_{i}^{1},\\tilde{\\pi}^{2})|}{\\sqrt{\\mathbb{E}_{x\\sim\\!d_{0}}a_{\\nu}^{1-\\pi}n_{\\star}^{2}a_{\\sim\\pi_{D}^{2}}(P(x,a^{1},a^{2})-\\hat{P}(x,a^{1},a^{2}))^{2}}}}\\\\ &{\\leq\\!\\!\\sqrt{\\frac{2\\tilde{C}(\\pi_{i}^{1},\\pi_{D},\\tilde{\\pi})}{n}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the second equality holds due to $\\lambda=\\log(|\\mathcal{P}|/\\delta)$ . Therefore, we complete the proof. ", "page_idx": 21}, {"type": "text", "text": "Comparison between Bonus and Version Space. Compared to the bound in Theorem 1, the bound in Theorem 3 enjoys an additional negative KL divergence term between $\\pi_{*}^{1}$ and $\\hat{\\pi}^{1}$ . Both Theorem 1 and Theorem 3 depend on a distribution-shift term between Nash policy $\\pi_{*}^{1}$ and the policy $\\pi_{D}$ that the data is complied with. The difference is that Theorem 1 enjoys a sharper term $\\mathcal{C}$ because of Jensen\u2019s inequality and the expectations are inside the sup operator. In terms of applicability, the version-space-based pessimism is preferred because it does not require a point-wise uncertainty estimator, thus applying to general cases. In contrast, point-wise pessimism, or more generally, optimism/pessimism via a biased target may be easier to heuristically approximate in practice, as shown in Coste et al. [15], Xie et al. [69], Zhang [82], Liu et al. [44]. ", "page_idx": 22}, {"type": "text", "text": "C.3 Analysis for Refined Coverage Condition ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this subsection, we show that with an improved analysis, Algorithm 1 enjoys a refined coverage condition, similar to the coverage notion in [84]. ", "page_idx": 22}, {"type": "text", "text": "Theorem 4. If Assumption $^{\\,l}$ holds, and we set $\\lambda=\\log(|\\mathcal{P}|/\\delta)$ and $\\beta^{2}=2\\log(|\\mathcal{P}|/\\delta),$ , then, with probability at least $1-\\delta$ , the output policy of Algorithm $^{\\,l}$ satisfies ", "page_idx": 22}, {"type": "equation", "text": "$$\nJ(\\pi_{1}^{*},\\pi_{2}^{*})-J(\\hat{\\pi}^{1},\\dag)\\leq\\operatorname*{min}_{\\pi^{2}\\in\\Pi}4\\beta\\sqrt{\\frac{\\mathcal{C}((\\pi_{*}^{1},\\pi^{2}),\\pi_{D},\\mathcal{P})}{n}}+\\mathrm{subopt}^{\\pi_{*}^{1},\\pi_{*}^{2}}(\\pi^{2}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{C}((\\pi_{*}^{1},\\pi^{2}),\\pi_{D},\\mathcal{P})=\\displaystyle\\operatorname*{sup}_{P\\in\\mathcal{P}}\\frac{(\\mathbb{E}_{x\\sim d_{0},a^{1}\\sim\\pi_{*}^{1},a^{2}\\sim\\pi^{2}}[P(x,a^{1},a^{2})-\\hat{P}(x,a^{1},a^{2})])^{2}}{\\mathbb{E}_{x\\sim d_{0},a^{1}\\sim\\pi_{D}^{1},a^{2}\\sim\\pi_{D}^{2}}(P(x,a^{1},a^{2})-\\hat{P}(x,a^{1},a^{2}))^{2}},}\\\\ &{\\mathrm{subopt}^{\\pi_{*}^{1},\\pi_{*}^{2}}(\\pi^{2})=\\underline{{J}}(\\pi_{*}^{1},\\pi^{2})-\\underline{{J}}(\\pi_{*}^{1},\\widetilde{\\pi}_{*}^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Recall that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\underline{{J}}(\\pi^{1},\\pi^{2})=\\operatorname*{min}_{P\\in\\widehat{\\mathcal{P}}}\\mathbb{E}_{x\\sim d_{0}}\\mathbb{E}_{a^{1}\\sim\\pi^{1},a^{2}\\sim\\pi^{2}}\\Big[P(x,a^{1},a^{2})+\\eta^{-1}\\log\\frac{\\pi_{0}(a^{1}|x)}{\\pi^{1}(a^{1}|x)}-\\eta^{-1}\\log\\frac{\\pi_{0}(a^{2}|x)}{\\pi^{2}(a^{2}|x)}\\Big].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and $\\widetilde{\\pi}_{\\ast}^{2}\\,=\\,\\operatorname*{min}_{\\pi^{2}\\in\\Pi}\\underline{{J}}\\big(\\pi_{\\ast}^{1},\\pi^{2}\\big)$ and $\\pi^{\\dagger,2}\\,=\\,\\operatorname*{min}_{\\pi^{2}\\in\\Pi}J(\\hat{\\pi}^{1},\\pi^{2})$ . We observe that for any $\\pi^{2}$ , the follo  wing decomposition holds ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{\\ast},\\pi_{*}^{2})-J(\\hat{\\pi}^{1},\\pi^{\\dagger,2})\\leq\\underbrace{J(\\pi_{*}^{1},\\pi_{*}^{2})-J(\\pi_{*}^{1},\\pi^{2})}_{q_{1}}+\\underbrace{J(\\pi_{*}^{1},\\pi^{2})-\\underline{{J(\\pi_{*}^{1},\\pi^{2})}}}_{q_{2}}+\\underbrace{J(\\pi_{*}^{1},\\pi^{2})}_{q_{3}}-\\underbrace{J(\\pi_{*}^{1},\\pi^{2})-\\underline{{J(\\pi_{*}^{1},\\pi^{2})}}}_{q_{3}}}\\\\ &{\\qquad\\qquad\\qquad+\\underbrace{J(\\pi_{*}^{1},\\pi_{*}^{2})-\\underline{{J(\\hat{\\pi}^{1},\\pi^{2})}}}_{q_{4}}+\\underbrace{J(\\hat{\\pi}^{1},\\widetilde{\\pi}^{2})-\\underline{{J(\\hat{\\pi}^{1},\\pi^{\\dagger,2})}}}_{q_{5}}+\\underbrace{J(\\hat{\\pi}^{1},\\pi^{\\dagger,2})-J(\\hat{\\pi}^{1},\\pi^{\\dagger,2})}_{q_{6}}-\\underbrace{J(\\hat{\\pi}^{1},\\pi^{\\dagger,2})-J(\\hat{\\pi}^{1},\\pi^{\\dagger,2})}_{q_{6}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For the term $q_{1}$ , since $(\\pi_{*}^{1},\\pi_{*}^{2})$ is the Nash equilibrium of $J$ , we have $q_{1}\\leq0$ . By the optimality of $\\hat{\\pi}^{1}$ , term $q_{4}\\leq0$ . From the proof of Theorem 1, we know that $q_{5}\\,\\leq\\,0$ and $q_{6}\\,\\leq\\,0$ . The term $\\begin{array}{r}{q_{3}=\\underline{{J}}(\\pi_{*}^{1},\\bar{\\pi^{2}})-\\underline{{J}}(\\pi_{*}^{1},\\widetilde{\\pi}_{*}^{2}):=\\mathrm{subopt}^{\\pi_{*}^{1},\\widetilde{\\pi}_{*}^{2}}(\\pi^{2})}\\end{array}$ measures the suboptimality gap between $\\pi^{2}$ and $\\widetilde{\\pi}_{*}^{2}$ under the pessimistic  e stimation $\\underline{{J}}$ and Nash policy $\\pi_{*}^{1}$ . For the term $q_{2}$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\iota_{4}=\\!\\mathbb{E}_{x\\sim d_{0}}\\mathbb{E}_{a^{1}\\sim\\pi_{*}^{1},a^{2}\\sim\\pi^{2}}P^{*}(x,a^{1},a^{2})-\\operatorname*{min}_{\\textstyle P\\in\\hat{\\mathcal{P}}}\\mathbb{E}_{x\\sim d_{0}}\\mathbb{E}_{a^{1}\\sim\\pi_{*}^{1},a^{2}\\sim\\pi^{2}}P(x,a^{1},a^{2})}\\\\ &{\\quad=\\!\\operatorname*{min}_{\\textstyle P\\in\\hat{\\mathcal{P}}}\\!\\mathbb{E}_{x\\sim d_{0}}\\mathbb{E}_{a^{1}\\sim\\pi_{*}^{1},a^{2}\\sim\\pi^{2}}[\\hat{P}(x,a^{1},a^{2})-P(x,a^{1},a^{2})]+\\mathbb{E}_{x\\sim d_{0}}\\mathbb{E}_{a^{1}\\sim\\pi_{*}^{1},a^{2}\\sim\\pi^{2}}[P^{*}(x,a^{1},a^{2})-P^{*}(x,a^{2})]}\\\\ &{\\quad\\le\\!2\\beta\\tilde{\\Gamma}(\\pi_{*}^{1},\\pi^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, we obtain that ", "page_idx": 22}, {"type": "equation", "text": "$$\nJ(\\pi_{*}^{1},\\pi_{*}^{2})-J(\\hat{\\pi}^{1},\\pi^{\\dagger,2})\\leq2\\beta\\widetilde{\\Gamma}(\\pi_{*}^{1},\\pi^{2})+\\mathrm{subopt}^{\\pi_{*}^{1},\\widetilde{\\pi}_{*}^{2}}(\\pi^{2}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since Equation (20) holds for any $\\pi_{2}$ , we further have ", "page_idx": 22}, {"type": "equation", "text": "$$\nJ(\\pi_{*}^{1},\\pi_{*}^{2})-J(\\hat{\\pi}^{1},\\pi^{\\dagger,2})\\le\\operatorname*{min}_{\\pi^{2}\\in\\Pi}2\\beta\\widetilde{\\Gamma}(\\pi_{*}^{1},\\pi^{2})+\\mathrm{subopt}^{\\pi_{*}^{1},\\pi_{*}^{2}}(\\pi^{2}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The proof for bounding $\\widetilde\\Gamma(\\pi_{*}^{1},\\pi^{2})$ is the same as that of Theorem 1. ", "page_idx": 22}, {"type": "text", "text": "We can prove that Algorithm 3 also enjoys a similar bound and coverage condition. We now provide a breakdown of the terms in Theorem 4. ", "page_idx": 23}, {"type": "text", "text": "\u2022 First, we can simply let $\\pi^{2}~=~\\widetilde{\\pi}_{*}^{2}$ , the best response to $\\pi_{*}^{1}$ under the pessimistic estimation, and then the bound becomes $4\\beta\\sqrt{\\mathcal{C}((\\pi_{*}^{1},\\widetilde{\\pi}_{*}^{2}),\\pi_{D},\\mathcal{P})/n}$ , which measures the coverage of the dataset $\\mathcal{D}$ on $(\\pi_{*}^{1},\\widetilde{\\pi}_{*}^{2})$ . When the distribution of $\\mathcal{D}$ aligns well with the distribution induced by $(\\pi_{*}^{1},\\widetilde{\\pi}_{*}^{2})$ , th e  dataset has a good coverage on $(\\pi_{*}^{1},\\widetilde{\\pi}_{*}^{2})$ and the term $\\mathcal{C}((\\pi_{*}^{1},\\widetilde{\\pi}_{*}^{2}),\\pi_{D},\\mathcal{P})$ become s  small.   \n\u2022 When $\\mathcal{D}$ has a poor coverage on $(\\pi_{*}^{1},\\widetilde{\\pi}_{*}^{2})$ , i.e., $\\mathcal{C}((\\pi_{*}^{1},\\widetilde{\\pi}_{*}^{2}),\\pi_{D},\\mathcal{P})$ is large, our bound adapts to an alternate policy $\\pi^{2}$ that achie v es a better trade  -off between the suboptimality term subopt $\\pi_{*}^{1},\\widetilde{\\pi}_{*}^{2}(\\pi^{\\bar{2}})$ and the coverage term $\\mathcal{C}((\\pi_{*}^{1},\\pi^{2}),\\pi_{D},\\mathcal{P})$ . Here the suboptimality term measures the performance gap between $\\pi^{2}$ and $\\widetilde{\\pi}_{\\ast}^{2}$ under $\\underline{{J}}(\\pi_{*}^{1},\\cdot)$ . ", "page_idx": 23}, {"type": "text", "text": "Comparison to Unilateral Coverage. The unilateral coverage [17, 86] requires the dataset to cover all unilateral pairs $(\\pi_{*}^{1},\\pi^{2})$ for any $\\bar{\\pi}^{2}\\in\\Pi$ , making the bound in Theorem 1 depend on the coverage term of the worst pair. In contrast, the bound in Theorem 4 automatically adapts to the best $\\bar{\\pi^{2}}$ , achieving the trade-off between the coverage term and the suboptimality term, thus offering a more flexible coverage condition. ", "page_idx": 23}, {"type": "text", "text": "D Proofs for the Online Setting ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proof of Theorem 2. We start with the in-sample error estimation. Similar to the proof of Lemma 1 but with an additional union bound over $t\\in[\\bar{T}]$ , we have with probability at least $1-\\delta/2$ , for any $t\\in[T]$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{1}{m}\\sum_{i=1}^{m}(\\hat{P}_{t}(x_{t,i},a_{t,i}^{1},a_{t,i}^{2})-P^{*}(x_{t,i},a_{t,i}^{1},a_{t,i}^{2}))^{2}\\leq\\frac{\\log(2T|\\mathcal{P}|/\\delta)}{m},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which implies that we can set $\\begin{array}{r}{\\beta^{2}=\\frac{T\\log(2T|\\mathcal{P}|/\\delta)}{m}}\\end{array}$ so that $\\beta\\widetilde{\\Gamma}_{t}^{m}$ is a valid uncertainty bonus: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\ensuremath{\\mathbb{E}}_{x}\\hat{P}_{t}(x,\\pi^{1},\\pi^{2})-\\beta\\ensuremath{\\widetilde{\\Gamma}}_{t}^{m}(\\lambda,\\pi^{1},\\pi^{2})\\le\\ensuremath{\\mathbb{E}}_{x}P^{*}(x,\\pi^{1},\\pi^{2})\\le\\ensuremath{\\mathbb{E}}_{x}\\hat{P}_{t}(x,\\pi^{1},\\pi^{2})-\\beta\\ensuremath{\\widetilde{\\Gamma}}_{t}^{m}(\\lambda,\\pi^{1},\\pi^{2}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We proceed to prove that there exists at least one iteration, the out-of-sample error is close to the in-sample error. According to the Definition 3, we know that for any sequence $\\{(\\hat{\\pi}_{t}^{1},\\hat{\\pi}_{t}^{2})\\}_{t=1}^{T}$ , it holds that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\operatorname*{min}\\Big(1,\\big(\\Gamma_{t}(\\lambda,\\hat{\\pi}_{t}^{1},\\hat{\\pi}_{t}^{2})\\big)^{2}\\Big)\\leq d(\\mathcal{P},\\lambda,T).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since each term on the left-hand side is non-negative, we know that there exists at least a $t_{0}\\in[T]$ such that the value at $t_{0}$ is smaller or equal to the average value: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\left(1,(\\Gamma_{t_{0}}(\\lambda,\\hat{\\pi}_{t_{0}}^{1},\\hat{\\pi}_{t_{0}}^{2}))^{2}\\right)\\leq\\frac{d(\\mathcal{P},\\lambda,T)}{T}\\leq\\frac{1}{2},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which further implies that $\\big(\\Gamma_{t_{0}}\\big(\\lambda,\\hat{\\pi}_{t_{0}}^{1},\\hat{\\pi}_{t_{0}}^{2}\\big)\\big)^{2}\\leq\\frac{1}{2}$ . ", "page_idx": 23}, {"type": "text", "text": "We use the notation $\\widetilde{\\pi}_{t}^{2}=\\mathrm{argmin}_{\\pi^{\\prime}}\\;J(\\hat{\\pi}_{t}^{1},\\pi^{\\prime})$ and ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\hat{J}(x,\\pi^{1},\\pi^{2})=\\hat{P}(x,\\pi^{1},\\pi^{2})-\\eta^{-1}D_{\\mathrm{KL}}(\\pi^{1}(a^{1}|x)||\\pi_{0}(a^{1}|x))+\\eta^{-1}D_{\\mathrm{KL}}(\\pi^{2}(a^{1}|x)||\\pi_{0}(a^{1}|x)).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For each $t\\in[T]$ , the suboptimality for the max-player can be written as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad J(\\pi_{1}^{*},\\pi_{2}^{*})-J(\\hat{\\pi}_{t}^{1},\\frac{1}{t})}\\\\ &{=\\!\\!\\mathbb{E}_{x\\sim d_{0}}\\Big[J(x,\\pi^{*},\\pi^{*})-\\hat{J}(x,\\hat{\\pi}_{t}^{1},\\widetilde{\\pi}_{t}^{2})+\\hat{J}(x,\\hat{\\pi}_{t}^{1},\\widetilde{\\pi}_{t}^{2})-J(x,\\hat{\\pi}_{t}^{1},\\widetilde{\\pi}_{t}^{2})\\Big]}\\\\ &{\\le\\!\\!\\mathbb{E}_{x\\sim d_{0}}\\Big[-\\hat{P}(x,\\hat{\\pi}_{t}^{1},\\widetilde{\\pi}_{t}^{2})+\\eta^{-1}{D}_{\\mathrm{KL}}(\\hat{\\pi}_{t}^{1}(\\cdot|x)||\\pi_{0}(\\cdot|x))-\\eta^{-1}{D}_{\\mathrm{KL}}(\\widetilde{\\pi}_{t}^{2}(\\cdot|x)||\\pi_{0}(\\cdot|x))\\Big]+\\beta\\widetilde{\\Gamma}_{t}^{m}(\\lambda,\\pi_{t}^{2}(\\cdot|x))}\\\\ &{\\le\\!\\!\\mathbb{E}_{x\\sim d_{0}}\\Big[-\\hat{P}(x,\\hat{\\pi}_{t}^{1},\\hat{\\pi}_{t}^{1})+\\eta^{-1}{D}_{\\mathrm{KL}}(\\hat{\\pi}_{t}^{1}(\\cdot|x),\\pi_{0}(\\cdot|x))-\\eta^{-1}{D}_{\\mathrm{KL}}(\\hat{\\pi}_{t}^{1}(\\cdot|x),\\pi_{0})-\\eta^{-1}{D}_{\\mathrm{KL}}(x,\\widetilde{\\pi}_{t}^{2})\\Big]}\\\\ &{\\qquad\\quad+\\beta\\widetilde{\\Gamma}_{t}^{m}(\\lambda,\\hat{\\pi}_{t}^{1},\\hat{\\pi}_{t}^{2})}\\\\ &{=\\!\\!\\beta\\widetilde{\\Gamma}_{t}^{m}(\\lambda,\\hat{\\pi}_{t}^{1},\\hat{\\pi}_{t}^{2})-\\eta^{-1}\\mathbb{E}_{x\\sim d_{0}}{D}_{\\mathrm{KL}}(\\widetilde{\\pi}_{t}^{2}(\\cdot|x)||\\hat{\\pi}_{t}^{1}(\\cdot|x)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the first inequality uses (21) and $J^{*}(x)=J^{*}(x,\\pi^{*},\\pi^{*})=0$ , in the second inequality we use the definition of $\\textstyle{\\hat{\\pi}}_{t}^{2}$ , and $\\hat{P}(x,\\pi,\\pi)=0$ in the last equality. ", "page_idx": 24}, {"type": "text", "text": "We proceed to connect the empirical bonus with the information ratio. Combining Lemma 8 with a union bound over $(P,s)\\in\\mathcal{P}\\stackrel{*}{\\times}[T]$ , with probability at least $1-\\delta/2$ , we know that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0.5\\mathbb{E}_{x_{s}\\sim d_{0},a_{s}^{1}\\sim\\hat{\\pi}_{s}^{1},a_{s}^{2}\\sim\\hat{\\pi}_{s}^{2}}(P(x_{s},a_{s}^{1},a_{s}^{2})-\\hat{P}(x_{s},a_{s}^{1},a_{s}^{2}))^{2}}\\\\ &{\\qquad\\le\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}(P(x_{s,i},a_{s,i}^{1},a_{s,i}^{2})-\\hat{P}(x_{s,i},a_{s,i}^{1},a_{s,i}^{2}))^{2}+\\frac{\\log(2T|\\mathcal{P}|/\\delta)}{m},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which further implies that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{s u}(\\lambda,\\pi^{1},\\pi^{2})\\le\\underset{P\\in\\mathcal{P}}{\\operatorname*{sup}}\\frac{\\left|\\mathbb{E}_{x}\\left[P(x,\\pi^{1},\\pi^{2})-\\hat{P}(x,\\pi^{1},\\pi^{2})\\right]\\right|}{{P}\\in\\mathcal{P}}}\\\\ &{\\qquad\\qquad\\le\\underset{P\\in\\mathcal{P}}{\\operatorname*{sup}}\\frac{\\left|\\mathbb{E}_{x}\\left[P(x,\\pi^{1},\\pi^{2})-\\hat{P}(x,\\pi^{1},a_{s}^{2}\\sim\\phi_{s}^{2}(P(x_{s},a_{s}^{1},a_{s}^{2})-\\hat{P}(x_{s},a_{s}^{1})}{\\pi})\\right]\\right|}{\\left|\\mathbb{E}_{x}\\left[P(x,\\pi^{1},\\pi^{2})-\\hat{P}(x,\\pi^{1},\\pi^{2})\\right]\\right|}}\\\\ &{\\qquad\\qquad\\le\\underset{P\\in\\mathcal{P}}{\\operatorname*{sup}}\\frac{\\left|\\mathbb{E}_{x}\\left[P(x,\\pi^{1},\\pi^{2})-\\hat{P}(x,\\pi^{1},\\pi^{2})\\right]\\right|}{\\sqrt{\\frac{1}{2}\\lambda+\\frac{1}{2}\\sum_{s=1}^{t-1}\\mathbb{E}_{x_{s}\\sim d_{0},a_{s}^{1}\\sim\\hat{\\pi}_{s}^{1},a_{s}^{2}\\sim\\phi_{s}^{2}}(P(x_{s},a_{s}^{1},a_{s}^{2})-\\hat{P}(x_{s},a_{s}^{1},a_{s}^{2}))^{2}}}}\\\\ &{\\qquad\\qquad\\le\\underset{P\\in\\mathcal{P}}{\\operatorname*{sup}}\\frac{\\sqrt{2}\\cdot\\left|\\mathbb{E}_{x}\\left[P(x,\\pi^{1},\\pi^{2})-\\hat{P}(x,\\pi^{1},\\pi^{2})\\right]\\right|}{\\sqrt{\\lambda+\\sum_{s=1}^{t-1}\\mathbb{E}_{x_{s}\\sim d_{0},a_{s}^{1}\\sim\\hat{\\pi}_{s}^{1},a_{s}^{2}\\sim\\phi_{s}^{2}}(P(x_{s},a_{s}^{1},a_{s}^{2})-\\hat{P}(x_{s},a_{s}^{1},a_{s}^{2})) \n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Here the second inequality is because $\\begin{array}{r}{\\lambda=\\frac{2T\\log(2T|\\mathcal{P}|/\\delta)}{m}}\\end{array}$ . Putting all together, we prove that with probability at least $1-\\delta$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J(\\pi_{1}^{*},\\pi_{2}^{*})-J(\\hat{\\pi}_{t_{0}}^{1},\\dagger)\\leq\\mathbb{E}_{x\\sim d_{0}}\\Big[3\\beta\\Gamma_{t_{0}}^{m}(\\hat{\\pi}_{t_{0}}^{1},\\hat{\\pi}_{t_{0}}^{2})-\\eta^{-1}D_{\\mathrm{KL}}(\\hat{\\pi}_{t}^{2}(\\cdot|x)\\|\\widetilde{\\pi}_{t}^{2}(\\cdot|x))\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\leq3\\sqrt{2}\\beta\\Gamma_{t_{0}}(\\lambda,\\hat{\\pi}_{t_{0}}^{1},\\hat{\\pi}_{t_{0}}^{2})-\\eta^{-1}\\mathbb{E}_{x}D_{\\mathrm{KL}}(\\hat{\\pi}_{t}^{2}(\\cdot|x)\\|\\widetilde{\\pi}_{t}^{2}(\\cdot|x))}\\\\ &{\\qquad\\qquad\\qquad\\leq3\\sqrt{\\frac{2T\\log(2T|\\mathcal{P}|/\\delta)}{m}}-\\eta^{-1}D_{\\mathrm{KL}}(\\hat{\\pi}_{t}^{2}(\\cdot|x)\\|\\widetilde{\\pi}_{t}^{2}(\\cdot|x)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Setting $\\begin{array}{r}{m=\\frac{18T\\log(2T|\\mathcal{P}|/\\delta)}{\\epsilon^{2}}}\\end{array}$ finishes the proof. ", "page_idx": 24}, {"type": "text", "text": "D.1 Uncertainty for the Bradley-Terry Model ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Recall that in Example 1, we suppose that the reward function can be embedded into a $d_{\\cdot}$ -dimensional vector space $\\{r(x,\\stackrel{.}{a})\\,=\\,\\langle\\theta,\\phi(\\stackrel{.}{x,}a)\\rangle:\\theta\\,\\in\\mathbb{R}^{d},\\|\\theta\\|\\,\\leq\\,B,\\|\\phi(x,a)\\|\\,\\leq\\,1\\}$ . Then, if we define the covariance matrix as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Sigma_{t}=\\sum_{s=1}^{t-1}\\mathbb{E}_{x\\sim d_{0},a^{1}\\sim\\hat{\\pi}_{s}^{1},a^{2}\\sim\\hat{\\pi}_{s}^{2}}(\\phi(x,a^{1})-\\phi(x,a^{2}))^{\\top}(\\phi(x,a^{1})-\\phi(x,a^{2}))+\\lambda(1+e^{B})^{2}I.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By invoking the Lagrange\u2019s Mean Value Theorem, we have for any two parameters $\\theta_{1},\\theta_{2}$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{P_{\\theta_{1}}(x,a^{1},a^{2})-P_{\\theta_{2}}(x,a^{1},a^{2})\\big|=\\!\\Big|\\frac{1}{1+\\exp(\\theta_{1}^{\\top}(\\phi(x,a^{2})-\\phi(x,a^{1})))}-\\frac{1}{1+\\exp(\\theta_{2}^{\\top}(\\phi(x,a^{2})-\\phi(x,a^{1})))}}\\\\ {\\le\\!\\big|(\\theta_{1}-\\theta_{2})^{\\top}(\\phi(x,a^{2})-\\phi(x,a^{1}))\\big|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left|P_{\\theta_{1}}(x,a^{1},a^{2})-P_{\\theta_{2}}(x,a^{1},a^{2})\\right|\\geq\\!\\frac{1}{1+e^{B}}\\big|(\\theta_{1}-\\theta_{2})^{\\top}(\\phi(x,a^{2})-\\phi(x,a^{1}))\\big|.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We use the short-hand notation $\\phi(x,\\pi)=\\mathbb{E}_{a\\sim\\pi}\\phi(x,a)$ . Then the uncertainty can be bounded by ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Gamma_{t}(\\lambda,\\pi^{1},\\pi^{2})=\\underset{\\theta}{\\operatorname*{sup}}\\,\\frac{\\big|\\mathbb{E}_{x\\sim d_{0}}\\left[P_{\\theta}\\left(x,\\pi^{1},\\pi^{2}\\right)-P_{\\hat{\\theta}}\\left(x,\\pi^{1},\\pi^{2}\\right)\\right]\\big|}{\\sqrt{\\lambda+\\sum_{s=1}^{t-1}\\mathbb{E}_{x\\sim d_{0},a_{s}^{1}\\sim\\hat{\\pi}_{s}^{\\lambda},a_{s}^{2}\\sim\\hat{\\pi}_{s}^{\\lambda}}\\left(P_{\\theta}\\left(x,a_{s}^{1},a_{s}^{2}\\right)-P_{\\hat{\\theta}}\\left(x_{s},a_{s}^{1},a_{s}^{2}\\right)\\right)^{2}}}}\\\\ &{\\quad\\quad\\quad\\leq\\underset{\\theta}{\\operatorname*{sup}}\\,\\frac{\\big|(\\theta-\\hat{\\theta})^{\\top}\\mathbb{E}_{x}\\left[\\phi\\left(x,\\pi^{2}\\right)-\\phi\\left(x,\\pi^{1}\\right)\\right]\\big|}{\\sqrt{\\lambda+\\sum_{s=1}^{t-1}\\mathbb{E}_{x\\sim d_{0},a_{s}^{1}\\sim\\hat{\\pi}_{s}^{\\lambda},a_{s}^{2}\\sim\\hat{\\pi}_{s}^{\\lambda}\\left(\\frac{1}{1+e^{B}}\\left[\\phi\\left(x,\\pi^{2}\\right)-\\phi\\left(x,\\pi^{1}\\right)\\right]\\right)^{2}}}}}\\\\ &{\\quad\\quad\\quad\\leq(1+e^{B})\\underset{\\theta}{\\operatorname*{sup}}\\frac{\\big|\\|\\theta-\\hat{\\theta}\\|_{\\Sigma_{t}}\\left\\|\\phi\\left(x,\\pi^{1}\\right)-\\phi\\left(x,\\pi^{2}\\right)\\right\\|_{\\Sigma_{t}^{-1}}}{\\sqrt{\\left(\\theta-\\hat{\\theta}\\right)^{\\top}\\Sigma_{t}\\left(\\theta-\\hat{\\theta}\\right)}}}\\\\ &{\\quad\\quad\\quad=(1+e^{B})\\|\\phi(x,\\pi^{1})-\\phi(x,\\pi^{2})\\|_{\\Sigma_{t}^{-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This uncertainty bonus is consistent with that of the reward-based RLHF framework up to some multiplicative factor of regularization parameter [72] and the boundness parameter. ", "page_idx": 25}, {"type": "text", "text": "D.2 Guarantee for Enhancer ", "text_level": 1, "page_idx": 25}, {"type": "equation", "text": "$$\n\\hat{\\pi}_{t}^{1}=\\underset{\\pi^{1}\\in\\Pi}{\\mathrm{argmax}}\\,\\operatorname*{min}_{\\pi^{2}\\in\\Pi}\\mathbb{E}_{x\\sim d_{0},a^{1}\\sim\\pi^{1},a^{2}\\sim\\pi^{2}}\\Bigl[\\hat{P}_{t}(x,a^{1},a^{2})+\\eta^{-1}\\log\\frac{\\pi_{0}(a^{1}|x)}{\\pi^{1}(a^{1}|x)}-\\eta^{-1}\\log\\frac{\\pi_{0}(a^{2}|x)}{\\pi^{2}(a^{2}|x)}\\Bigr],\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widetilde{\\Gamma}_{t}^{m}(\\lambda,\\pi^{1},\\pi^{2}):=\\operatorname*{sup}_{P\\in\\mathcal{P}}\\frac{|\\mathbb{E}_{x\\sim d_{0}}[P(x,\\pi^{1},\\pi^{2})-\\hat{P}_{t}(x,\\pi^{1},\\pi^{2})]|}{\\sqrt{\\lambda+\\frac{1}{m}\\sum_{s=1}^{t-1}\\sum_{j=1}^{m}(P(x_{s,j},a_{s,j}^{1},a_{s,j}^{2})-\\hat{P}_{t}(x_{s,j},a_{s,j}^{1},a_{s,j}^{2}))^{2}}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "6: Construct a version space for the policy ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Pi_{t}=\\{\\pi\\in\\Pi:\\eta^{-1}\\mathbb{E}_{x}D_{\\mathrm{KL}}(\\pi(\\cdot|x),\\hat{\\pi}^{1}(\\cdot|x))\\leq\\beta(\\widetilde{\\Gamma}_{t}^{m}(\\lambda,\\hat{\\pi}^{1},\\pi)+\\widetilde{\\Gamma}_{t}^{m}(\\lambda,\\hat{\\pi}^{1},\\hat{\\pi}^{1}))\\}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "7: Compute enhancer to maximize the uncertainty: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\pi_{t}^{2}=\\underset{\\pi^{2}\\in\\Pi_{t}}{\\operatorname{argmax}}\\,\\widetilde{\\Gamma}_{t}^{m}(\\lambda,\\hat{\\pi}_{t}^{1},\\pi^{2}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "8: Collect $\\mathcal{D}_{t}~=~\\{(x_{i},a_{i}^{1},a_{i}^{2},y_{i})\\}_{i=1}^{m}$ by $a_{i}^{1}\\;\\sim\\;{\\hat{\\pi}}_{t}^{1}(\\cdot|x_{i}),\\;a_{i}^{2}\\;\\sim\\;{\\hat{\\pi}}_{t}^{2}(\\cdot|x_{i})$ and $y_{i}\\ \\sim\\ \\mathrm{Ber}\\left(\\mathbb{P}(a_{i}^{1}\\ \\succ$ $a_{i}^{2}|x,a_{i}^{1},a_{i}^{2})\\rangle$ ; ", "page_idx": 25}, {"type": "text", "text": "9: end for ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "10: Output: the best policy in $(\\pi_{1:T}^{1})$ by a validation set. ", "page_idx": 25}, {"type": "text", "text": "Lemma 2. Under Algorithm 4, given the policy of the main agent $\\hat{\\pi}_{t}^{1}$ , we consider the version space with $\\beta^{2}=\\log(2T|\\mathcal{P}|/\\delta)/m$ : ", "page_idx": 25}, {"type": "text", "text": "$\\Pi_{t}=\\{\\pi\\in\\Pi:\\eta^{-1}\\mathbb{E}_{x}D_{\\mathrm{KL}}(\\pi(\\cdot|x),\\hat{\\pi}^{1}(\\cdot|x))\\leq\\beta(\\widetilde{\\Gamma}_{t}^{m}(\\lambda,\\hat{\\pi}^{1},\\pi)+\\widetilde{\\Gamma}_{t}^{m}(\\lambda,\\hat{\\pi}^{1},\\hat{\\pi}^{1}))\\}.$ Then, with probability at least $1-\\delta$ , we know that argmin\u03c0\u2032 $J(\\hat{\\pi}_{t}^{1},\\pi^{\\prime})\\in\\Pi_{t}$ for all $t\\in[T]$ . ", "page_idx": 25}, {"type": "text", "text": "Proof. First, since ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\hat{\\pi}_{t}^{1}=\\operatorname*{argmax}_{\\pi}\\mathbb{E}_{a\\sim\\pi(\\cdot|x)}\\Big[(1-\\hat{P}_{t}(x,\\hat{\\pi}_{t}^{1},a))-\\eta D_{\\mathrm{KL}}(\\pi(\\cdot|x)\\|\\pi_{0}(\\cdot|x))\\Big],\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "by using Lemma 9, we have for any policy $\\pi\\in\\Pi$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{x\\sim d_{0}}\\Big[\\mathbb{E}_{\\pi}[1-\\hat{P}_{t}(x,\\hat{\\pi}_{t}^{1},a)]-\\mathbb{E}_{\\hat{\\pi}_{t}^{1}}[1-\\hat{P}_{t}(x,\\hat{\\pi}_{t}^{1},a]+\\eta D_{\\mathrm{KL}}(\\hat{\\pi}_{t}^{1}(\\cdot|x)||\\pi_{0}(\\cdot|x))-\\eta D_{\\mathrm{KL}}(\\pi(\\cdot|x)||\\pi_{0}(\\cdot|x))]}\\\\ &{=-\\eta\\mathbb{E}_{x\\sim d_{0}}D_{\\mathrm{KL}}(\\pi(\\cdot|x)||\\hat{\\pi}_{t}^{1}(\\cdot|x)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which implies that with $\\pi=\\widetilde{\\pi}_{t}^{2}$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{E}_{x\\sim d_{0}}\\Big[-\\hat{P}_{t}(x,\\hat{\\pi}_{t}^{1},\\widetilde{\\pi}_{t}^{2})+\\hat{P}_{t}(x,\\hat{\\pi}_{t}^{1},\\hat{\\pi}_{t}^{1})+\\eta D_{\\mathrm{KL}}(\\hat{\\pi}_{t}^{1}(\\cdot|x)\\|\\pi_{0}(\\cdot|x))-\\eta D_{\\mathrm{KL}}(\\widetilde{\\pi}_{t}^{2}(\\cdot|x)\\|\\pi_{0}(\\cdot|x))\\Big]}\\\\ &{}&{=-\\eta\\mathbb{E}_{x\\sim d_{0}}D_{\\mathrm{KL}}(\\widetilde{\\pi}_{t}^{2}(\\cdot|x)\\|\\hat{\\pi}_{t}^{1}(\\cdot|x)).\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (23)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Additionally, by the definition that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\widetilde{\\pi}_{t}^{2}=\\underset{\\pi^{\\prime}}{\\mathrm{argmin}}\\,J(\\hat{\\pi}_{t}^{1},\\pi^{\\prime})=\\underset{\\pi^{\\prime}}{\\mathrm{argmin}}\\,\\mathbb{E}_{x}\\big[P^{*}\\big(x,\\hat{\\pi}_{t}^{1},\\pi^{\\prime}\\big)+\\eta^{-1}D_{\\mathrm{KL}}(\\pi^{\\prime}(\\cdot|x)\\|\\pi_{0}(\\cdot|x))\\big],\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{\\ell}_{x}\\!\\left[P^{*}(x,\\hat{\\pi}_{t}^{1},\\tilde{\\pi}_{t}^{2})\\!+\\!\\eta^{-1}D_{\\mathrm{KL}}(\\tilde{\\pi}_{t}^{2}(\\cdot|x)||\\pi_{0}(\\cdot|x))\\right]\\leq\\mathbb{E}_{x}\\!\\left[P^{*}(x,\\hat{\\pi}_{t}^{1},\\hat{\\pi}_{t}^{1})\\!+\\!\\eta^{-1}D_{\\mathrm{KL}}(\\hat{\\pi}_{t}^{1}(\\cdot|x)||\\pi_{0}(\\cdot|x))\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which implies that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phantom{=}\\,^{\\dagger}\\mathrm{\\mathbb{E}}_{x}\\big[P^{*}(x,\\hat{\\pi}_{t}^{1},\\hat{\\pi}_{t}^{1})-P^{*}(x,\\hat{\\pi}_{t}^{1},\\tilde{\\pi}_{t}^{2})+\\eta^{-1}D_{\\mathrm{KL}}(\\hat{\\pi}_{t}^{1}(\\cdot|x)\\|\\pi_{0}(\\cdot|x))-\\eta^{-1}D_{\\mathrm{KL}}(\\widetilde{\\pi}_{t}^{2}(\\cdot|x)\\|\\pi_{0}(\\cdot|x))\\big]}\\\\ &{=\\!\\!\\mathbb{E}_{x}\\big[P^{*}(x,\\hat{\\pi}_{t}^{1},\\hat{\\pi}_{t}^{1})-\\hat{P}_{t}(x,\\hat{\\pi}_{t}^{1},\\hat{\\pi}_{t}^{1})-\\big(P^{*}(x,\\hat{\\pi}_{t}^{1},\\widetilde{\\pi}_{t}^{2})-\\hat{P}_{t}(x,\\hat{\\pi}_{t}^{1},\\widetilde{\\pi}_{t}^{2})\\big)}\\\\ &{\\phantom{=}\\,+\\hat{P}_{t}(x,\\hat{\\pi}_{t}^{1},\\hat{\\pi}_{t}^{1})-\\hat{P}_{t}(x,\\hat{\\pi}_{t}^{1},\\widetilde{\\pi}_{t}^{2})+\\eta^{-1}D_{\\mathrm{KL}}(\\hat{\\pi}_{t}^{1}(\\cdot|x)\\|\\pi_{0}(\\cdot|x))-\\eta^{-1}D_{\\mathrm{KL}}(\\widetilde{\\pi}_{t}^{2}(\\cdot|x)\\|\\pi_{0}(\\cdot|x))}\\\\ &{\\phantom{=}\\,\\pm\\beta\\big(\\widetilde{\\Gamma}_{t}^{m}(\\lambda,\\hat{\\pi}_{t}^{1},\\hat{\\pi}_{t}^{1})+\\widetilde{\\Gamma}_{t}^{m}(\\lambda,\\hat{\\pi}_{t}^{1},\\widetilde{\\pi}_{t}^{2})\\big)-\\eta^{-1}\\mathbb{E}_{x}D_{\\mathrm{KL}}(\\widetilde{\\pi}_{t}^{2}(\\cdot|x)\\|\\hat{\\pi}_{t}^{1}(\\cdot|x)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the last inequality uses (21) and (23) since $\\hat{\\pi}_{1}^{t}$ is the Nash equilibrium of ${\\hat{J}}_{t}$ . Therefore, we conclude the proof. ", "page_idx": 26}, {"type": "text", "text": "Lemma 3. Under the same setting as Theorem 2, if we further assume that there exists a constant $B\\,>\\,0$ such that for any $\\in\\;\\Pi,\\,|\\log(\\pi(a|x)/\\pi_{0}(a|x))|\\leq B,$ , and set T B4 log(2T |P|/\u03b4), we have with probability at least $1-\\delta$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{J(\\dag,\\hat{\\pi}_{t_{0}}^{2})-J(\\pi^{*},\\pi^{*})\\leq O(\\epsilon^{1/2}-\\eta^{-1}D_{\\mathrm{KL}}(\\hat{\\pi}_{t_{0}}^{2}(\\cdot|x)\\|\\widetilde{\\pi}_{t_{0}}^{2}(\\cdot|x))).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. Under the condition of Theorem 2, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{J(\\hat{\\pi}_{t}^{1},\\dagger)-J(\\hat{\\pi}_{t}^{2},\\dagger)\\leq\\underset{\\pi^{\\prime}}{\\mathrm{min}}\\,J(\\hat{\\pi}_{t}^{1},\\pi^{\\prime})-\\underset{\\pi^{\\prime}}{\\mathrm{min}}\\,J(\\hat{\\pi}_{t}^{2}-\\hat{\\pi}_{t}^{1},\\pi^{\\prime})-\\underset{\\pi^{\\prime}}{\\mathrm{min}}\\,J(\\hat{\\pi}_{t}^{1},\\pi^{\\prime})}&{}\\\\ {\\leq\\underset{\\pi^{\\prime}}{\\mathrm{min}}\\,\\mathbb{E}_{x}\\bigg|\\int(\\hat{\\pi}_{t}^{1}-\\hat{\\pi}_{t}^{2})(a|x)\\cdot\\pi^{\\prime}(a^{\\prime}|x)\\cdot J(x,a,a^{\\prime})\\mathrm{d}(a,a^{\\prime})\\bigg|}\\\\ &{\\leq(B+1)\\mathbb{E}_{x}\\|\\hat{\\pi}_{t}^{1}(\\cdot|x)-\\hat{\\pi}_{t}^{2}(\\cdot|x)\\|_{1}}\\\\ &{\\leq(B+1)\\sqrt{D_{\\mathrm{KL}}(\\hat{\\pi}_{t}^{1}(\\cdot|x)||\\hat{\\pi}_{t}^{2}(\\cdot|x))}}\\\\ &{\\leq(B+1)\\sqrt{\\eta\\beta(\\widetilde{\\Gamma}_{t}^{m}(\\lambda,\\hat{\\pi}_{t}^{1},\\hat{\\pi}_{t}^{2})+\\widetilde{\\Gamma}_{t}^{m}(\\lambda,\\hat{\\pi}_{t}^{1},\\hat{\\pi}_{t}^{1}))}}\\\\ &{\\leq(B+1)\\sqrt{2\\eta\\beta\\widetilde{\\Gamma}_{t}^{m}(\\lambda,\\hat{\\pi}_{t}^{1},\\hat{\\pi}_{t}^{2})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the second last inequality invokes Lemma 2, and the last inequality holds due to $\\hat{\\pi}_{t}^{1}\\in\\Pi_{t}$ and $\\hat{\\pi}_{t}^{2}=\\mathrm{argmax}_{\\pi\\in\\Pi_{t}}\\widetilde{\\Gamma}_{t}^{m}\\big(\\lambda,\\hat{\\pi}_{t}^{1},\\pi\\big)$ . Hence, at time $t_{0}$ in Theorem 2, we deduce that the suboptimality for the min-player i s ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{I(\\dagger,\\hat{\\pi}_{t_{0}}^{2})-J(\\pi^{*},\\pi^{*})=J(\\dagger,\\hat{\\pi}_{t_{0}}^{2})-J(\\dagger,\\hat{\\pi}_{t_{0}}^{1})+J(\\dagger,\\hat{\\pi}_{t_{0}}^{1})-J(\\pi^{*},\\pi^{*})}&{}\\\\ {=J(\\hat{\\pi}_{t_{0}}^{1},\\dagger)-J(\\hat{\\pi}_{1}^{2},\\dagger)+J(\\pi^{*},\\pi^{*})-J(\\hat{\\pi}_{t_{0}}^{1},\\dagger)}&{}\\\\ {\\leq(B+1)\\sqrt{2\\eta\\beta\\Gamma_{t_{0}}^{m}(\\hat{\\pi}_{t_{0}}^{1},\\hat{\\pi}_{t_{0}}^{2})}+3\\sqrt{\\displaystyle\\frac{2T\\log(2T|\\mathcal{P}|/\\delta)}{m}}-\\eta^{-1}D_{\\mathrm{KL}}(\\hat{\\pi}_{t_{0}}^{2}(\\cdot|x)||\\widetilde{\\pi}_{t_{0}}^{2}(\\cdot|x)|)}&{}\\\\ {\\leq\\mathcal{O}\\biggr(B\\biggr(\\displaystyle\\frac{T\\log(2T|\\mathcal{P}|/\\delta)}{m}\\biggr)^{1/4}+\\sqrt{\\displaystyle\\frac{T\\log(2T|\\mathcal{P}|/\\delta)}{m}}-\\eta^{-1}D_{\\mathrm{KL}}(\\hat{\\pi}_{t_{0}}^{2}(\\cdot|x)||\\widetilde{\\pi}_{t_{0}}^{2}(\\cdot|x)|)}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Setting m = T B4 log(22T |P|/\u03b4), we get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{J(\\dag,\\hat{\\pi}_{t_{0}}^{2})-J(\\pi^{*},\\pi^{*})\\leq O(\\epsilon^{1/2}-\\eta^{-1}D_{\\mathrm{KL}}(\\hat{\\pi}_{t_{0}}^{2}(\\cdot|x)\\|\\widetilde{\\pi}_{t_{0}}^{2}(\\cdot|x))).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "E Technical Lemmas ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "E.1 Auxiliary Lemmas and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Lemma 4. For $\\begin{array}{r}{\\operatorname*{max}_{\\pi^{1}\\in\\Pi}\\operatorname*{min}_{\\pi^{2}\\in\\Pi}J(\\pi^{1},\\pi^{2}),}\\end{array}$ , there exists a unique Nash equilibrium $(\\pi_{*}^{1},\\pi_{*}^{2})$ and it holds that \u03c0\u22171= \u03c0\u22172. ", "page_idx": 27}, {"type": "text", "text": "Proof. The existence and uniqueness of the Nash equilibrium are proved in Proposition 1 in [46]. We proceed to use the uniqueness of the Nash equilibrium and contradiction to prove the lemma. Suppose $\\pi_{*}^{1}\\neq\\pi_{*}^{2}$ , since $\\pi_{*}^{1}$ is the best response to $\\pi_{*}^{2}$ for the max-player, for any $\\pi\\in\\Pi$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\bar{\\mathfrak{L}}_{x\\sim d_{0}}\\left[P(x,\\pi,\\pi_{*}^{2})-\\eta^{-1}D_{\\mathrm{KL}}(\\pi(\\cdot|x)||\\pi_{0}(\\cdot|x))\\right]\\le\\mathbb{E}_{x\\sim d_{0}}\\left[P(x,\\pi_{*}^{1},\\pi_{*}^{2})-\\eta^{-1}D_{\\mathrm{KL}}(\\pi_{*}^{1}(\\cdot|x)||\\pi_{0}(\\cdot|x))\\right].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Similarly, since $\\pi_{*}^{2}$ is the best response to $\\pi_{*}^{1}$ for the min-player, for any $\\pi\\in\\Pi$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\bar{\\mathfrak{c}}_{x\\sim d_{0}}\\left[P(x,\\pi_{*}^{1},\\pi)-\\eta^{-1}D_{\\mathrm{KL}}(\\pi(\\cdot|x)||\\pi_{0}(\\cdot|x))\\right]\\ge\\mathbb{E}_{x\\sim d_{0}}\\left[P(x,\\pi_{*}^{1},\\pi_{*}^{2})-\\eta^{-1}D_{\\mathrm{KL}}(\\pi_{*}^{2}(\\cdot|x)||\\pi_{0}(\\cdot|x))\\right].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then, we prove that $(\\pi_{*}^{2},\\pi_{*}^{1})$ is also the Nash equilibrium. Since $P(x,\\pi^{1},\\pi^{2})=1-P(x,\\pi^{2},\\pi^{1})$ for any $\\pi^{1}$ and $\\pi^{2}$ , then (25) implies that for any $\\pi\\in\\Pi$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\bar{\\mathfrak{c}}_{x\\sim d_{0}}\\left[P(x,\\pi,\\pi_{*}^{1})-\\eta^{-1}D_{\\mathrm{KL}}(\\pi(\\cdot|x)||\\pi_{0}(\\cdot|x))\\right]\\le\\mathbb{E}_{x\\sim d_{0}}\\left[P(x,\\pi_{*}^{2},\\pi_{*}^{1})-\\eta^{-1}D_{\\mathrm{KL}}(\\pi_{*}^{2}(\\cdot|x)||\\pi_{0}(\\cdot|x))\\right]\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This demonstrates that $\\pi_{*}^{2}$ is the best response to $\\pi_{*}^{1}$ for the max-player. Similarly, (24) implies that for any $\\pi\\in\\Pi$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathfrak{L}_{x\\sim d_{0}}\\left[P(x,\\pi_{*}^{2},\\pi)-\\eta^{-1}D_{\\mathrm{KL}}(\\pi(\\cdot|x)||\\pi_{0}(\\cdot|x))\\right]\\ge\\mathbb{E}_{x\\sim d_{0}}\\left[P(x,\\pi_{*}^{2},\\pi_{*}^{1})-\\eta^{-1}D_{\\mathrm{KL}}(\\pi_{*}^{1}(\\cdot|x)||\\pi_{0}(\\cdot|x))\\right].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This demonstrates that $\\pi_{*}^{1}$ is the best response to $\\pi_{*}^{2}$ for the min-player. Hence, $(\\pi_{*}^{2},\\pi_{*}^{1})$ is another Nash equilibrium, contradicting with the uniqueness. Therefore, we have $\\pi_{*}^{1}=\\pi_{*}^{2}$ . ", "page_idx": 27}, {"type": "text", "text": "Lemma 5. If $(\\pi_{*}^{1},\\pi_{*}^{2})$ is the Nash equilibrium of $\\begin{array}{r}{\\operatorname*{max}_{\\pi^{1}\\in\\Pi}\\operatorname*{min}_{\\pi^{2}\\in\\Pi}J(\\pi^{1},\\pi^{2})}\\end{array}$ , then, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n(\\pi_{*}^{1}(\\cdot|x),\\pi_{*}^{2}(\\cdot|x))=\\underset{\\pi^{1}\\in\\Pi}{\\mathrm{argmax\\,argmin}}\\,J(x,\\pi^{1},\\pi^{2})\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. According to Proposition 1 in [46], $(\\pi_{*}^{1},\\pi_{*}^{2})$ is the unique Nash Equilibrium of $\\mathrm{max}_{\\pi^{1}}\\,\\mathrm{min}_{\\pi^{2}}\\:J(\\bar{\\pi}^{1},\\pi^{2})$ . According to the definition of the saddle point, it suffices to prove that for any $x\\sim d_{0}$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\pi_{*}^{1}(\\cdot|x)=\\operatorname*{argmax}_{\\pi^{1}}J(x,\\pi^{1},\\pi_{*}^{2}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We know that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi_{*}^{1}=\\underset{\\pi^{1}\\in\\Pi}{\\mathrm{argmax}}\\,J(\\pi^{1},\\pi_{*}^{2})}\\\\ &{\\quad=\\underset{\\pi^{1}\\in\\Pi}{\\mathrm{argmax}}\\,\\mathbb{E}_{x\\sim d_{0}}\\mathbb{E}_{a^{1}\\sim\\pi^{1},a^{2}\\sim\\pi_{*}^{2}}\\bigl[P^{*}(x,a^{1},a^{2})-\\eta^{-1}D_{\\mathrm{KL}}(\\pi^{1}(\\cdot|x)\\|\\pi_{0}(\\cdot|x))\\bigr].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Assume that there exists a $x_{0}$ such that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\pi_{*}^{1}(\\cdot|x_{0})\\neq\\widetilde{\\pi}^{1}(\\cdot|x_{0})=\\underset{\\pi^{1}\\in\\Pi}{\\mathrm{argmax}}\\,\\mathbb{E}_{a^{1}\\sim\\pi^{1},a^{2}\\sim\\pi_{*}^{2}}[P^{*}(x,a^{1},a^{2})-\\eta^{-1}D_{\\mathrm{KL}}(\\pi^{1}(\\cdot|x)||\\pi_{0}(\\cdot|x))].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then we can construct a $\\widetilde{\\pi}_{*}^{1}\\in\\Pi$ such that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\widetilde{\\pi}_{*}^{1}(\\cdot|x)=\\pi_{*}^{1}(\\cdot|x),\\mathrm{~for~}x\\neq x_{0},\\quad\\widetilde{\\pi}_{*}^{1}(\\cdot|x_{0})=\\widetilde{\\pi}^{1}(\\cdot|x_{0}),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which contradicts the definition of $\\pi_{*}^{1}$ . Because of the symmetry of the two players, we also get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\pi_{*}^{2}(\\cdot|x)=\\operatorname*{argmin}_{\\pi^{2}\\in\\Pi}\\mathbb{E}_{a^{1}\\sim\\pi_{*}^{1},a^{2}\\sim\\pi^{2}}[P^{*}(x,a^{1},a^{2})+\\eta^{-1}D_{\\mathrm{KL}}(\\pi^{2}(\\cdot|x)||\\pi_{0}(\\cdot|x))].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "E.2 Other Lemmas ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Lemma 6 (Martingale Exponential Inequalities). Consider a sequence of random functions $\\xi_{1}(\\mathcal{Z}_{1}),\\cdots,\\xi_{t}(\\mathcal{Z}_{t}),\\dot{.}\\cdot.$ with respect to filtration $\\{\\mathcal{F}_{t}\\}$ . We have for any $\\delta\\in(0,1)$ and $\\lambda>0$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big[\\exists n>0:-\\sum_{i=1}^{n}\\xi_{i}\\ge\\frac{\\log(1/\\delta)}{\\lambda}+\\frac{1}{\\lambda}\\sum_{i=1}^{n}\\log\\mathbb{E}_{Z_{i}^{(y)}}\\exp(-\\lambda\\xi_{i})\\Big]\\le\\delta,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $Z_{t}=(Z_{t}^{(x)},Z_{t}^{(y)})$ and $\\mathcal{Z}_{t}=(Z_{1},\\ldots,Z_{t})$ . ", "page_idx": 28}, {"type": "text", "text": "Proof. See Theorem 13.2 of Zhang [83] for a detailed proof. ", "page_idx": 28}, {"type": "text", "text": "Lemma 7 (Sion\u2019s minimax theorem). Let $X$ be a compact convex subset of a linear topological space and $Y$ a convex subset of a linear topological space. If $f:X\\times Y\\rightarrow\\mathbb{R}$ satisfies ", "page_idx": 28}, {"type": "text", "text": "\u2022 for any fixed $x\\in X$ , $f(x,\\cdot)$ is upper semicontinuous and quasi-concave on $Y$ ; ", "page_idx": 28}, {"type": "text", "text": "\u2022 for any fixed $y\\in Y$ , $f(\\cdot,y)$ is lower semicontinuous and quasi-convex on $X$ , ", "page_idx": 28}, {"type": "text", "text": "then we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x}\\operatorname*{sup}_{y}f(x,y)=\\operatorname*{sup}_{y}\\operatorname*{min}_{x}f(x,y).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Lemma 8 (Multiplicative Chernoff Bounds). Assume that $X\\in[0,1]$ with $\\mathbb{E}X=\\mu$ . Then for all $\\epsilon>0$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l c r}{\\displaystyle\\mathbb{P}\\Big(\\bar{X}_{n}\\geq(1+\\epsilon)\\mu\\Big)\\leq\\exp\\Big[\\frac{-2n\\mu\\epsilon^{2}}{2+\\epsilon}\\Big]}\\\\ {\\displaystyle\\mathbb{P}\\Big(\\bar{X}_{n}\\leq(1-\\epsilon)\\mu\\Big)\\leq\\exp\\Big[\\frac{-2n\\mu\\epsilon^{2}}{2}\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Moreover, for $t>0$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\bar{X}_{n}\\geq\\mu+\\sqrt{\\frac{2\\mu t}{n}}+\\frac{t}{3n}\\Big)\\leq\\exp(-t).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. Refer to the proof of Corollary 2.18 in Zhang [83]. ", "page_idx": 28}, {"type": "text", "text": "Lemma 9 (Policy optimization error). For any two policies $\\pi,\\hat{\\pi}\\,\\in\\,\\Pi$ such that $\\operatorname{support}(\\pi)\\,=$ support $\\left(\\pi_{0}\\right)$ and ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\hat{\\pi}(\\cdot|x)=\\operatorname*{argmax}_{\\pi^{1}\\in\\Pi}\\mathbb{E}_{a\\sim\\pi^{1}(\\cdot|x)}\\Big[\\hat{P}(x,a)+\\eta^{-1}\\log\\frac{\\pi_{0}(a|x)}{\\pi^{1}(a|x)}\\Big],\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Suppose that the KL divergences between them are finite and well defined. Then, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{x\\sim d_{0}}\\Big[\\mathbb{E}_{\\pi}[\\hat{P}(x,a)]-\\mathbb{E}_{\\hat{\\pi}}[\\hat{P}(x,a)]+\\eta^{-1}D_{\\mathrm{KL}}(\\hat{\\pi}(\\cdot|x)\\|\\pi_{0}(\\cdot|x))-\\eta^{-1}D_{\\mathrm{KL}}(\\pi(\\cdot|x)\\|\\pi_{0}(\\cdot|x))\\Big]}\\\\ &{=-\\eta^{-1}\\mathbb{E}_{x\\sim d_{0}}D_{\\mathrm{KL}}(\\pi(\\cdot|x)\\|\\hat{\\pi}(\\cdot|x)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. See the proof in Lemma 2.4 of Xiong et al. [72]. ", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We present the theoretical results in Section 3 and 4, and the experimental results in Section 6. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: See Lines 168-179, Lines 243-245. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We state and explain the assumptions in Theorem 1 and 2, and provide the complete in Appendix C and D. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: See Appendix B.2. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We provide details about our data that can be found online. We have uploaded our codes. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have included the details in our paper and Appendix. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [No] ", "page_idx": 31}, {"type": "text", "text": "Justification: Conducting LLM experiments with statistically significant justifications is challenging due to the high computational costs and the substantial carbon emissions generated. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: See the Appendix. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: Conducted. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Theory paper, no visible societal impact found in a short term. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Theory paper, no such risk. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: All datasets are used properly. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 33}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] Justification: N/A. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] Justification: N/A. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]