[{"figure_path": "a3cauWMXNV/figures/figures_2_1.jpg", "caption": "Figure 1: Three real-world networks with node groups denoted by color. Within-group edges are in blue and across-group edges in red, while edge widths correspond to edge weight magnitudes. For each network, we present (\"M\") the modularity of the graphs with respect to group membership [19], (\"W\") the ratio of positive to negative estimated partial correlations for within-group edges, and (\"A\") an analogous ratio for across-group edges. Networks in (a) and (c) show high group-wise modularity, while (b) and (c) show significant preferences for positive correlations in the same group.", "description": "This figure showcases three real-world networks (Karate club, Dutch school, and U.S. Senate) illustrating different group structures and biases in their connections. Node colors represent group memberships. Blue edges connect nodes within groups, red edges connect nodes across groups, and edge thickness represents the magnitude of partial correlation.  The figure quantifies each network's modularity (M) and the ratios of positive to negative partial correlations within (W) and across (A) groups, highlighting varying degrees of group-wise modularity and correlation bias.", "section": "2 Fair Gaussian Graphical Models"}, {"figure_path": "a3cauWMXNV/figures/figures_6_1.jpg", "caption": "Figure 2: Estimation performance in terms of error and bias. (a) Bias and error for estimating a fair graph as data becomes more biased. (b) Bias and error as graph size p grows for ER graphs. (c) Bias and error for a biased real-world network [23] as the number of observations n grows.", "description": "This figure demonstrates the performance of Fair GLASSO in terms of both error and bias under different conditions.  Panel (a) shows how error and bias change as the data becomes increasingly biased. Panel (b) illustrates the scalability of the method by showing the performance as the size of the graph increases. Finally, panel (c) shows how the algorithm performs on a real-world dataset with biased data as the number of observations increases.", "section": "Experiments"}, {"figure_path": "a3cauWMXNV/figures/figures_8_1.jpg", "caption": "Figure 3: Estimated Karate club network via graphical lasso with and without penalties H and Hnode. Node colors denote group membership, while edge thickness denotes edge weight magnitude and edge color its sign, with blue (red) as positive (negative) correlation. (a) Estimation via GL. (b) Estimation via FGL. (c) Estimation via NFGL.", "description": "This figure compares the results of applying three different methods (graphical lasso, Fair GLASSO with bias penalty H, and Fair GLASSO with bias penalty Hnode) to the karate club network. The node colors represent group membership, the edge thickness represents the magnitude of the edge weight, and the edge color represents the sign of the correlation (blue for positive, red for negative).  The figure shows how different methods handle biases in the data and produce different graph structures.", "section": "4.3 Social Network with Synthetic Signals"}, {"figure_path": "a3cauWMXNV/figures/figures_23_1.jpg", "caption": "Figure 4: Performance in terms of error and bias for estimating a fair precision matrix. (a) Error as parameters \u03bc\u2081 and \u03bc\u2082 vary. (b) Bias as parameters \u03bc\u2081 and \u03bc\u2082 vary.", "description": "This figure shows the results of an experiment evaluating the performance of Fair GLASSO in terms of error and bias.  The experiment varied the values of two hyperparameters: \u03bc\u2081 (sparsity penalty weight) and \u03bc\u2082 (bias penalty weight).  The heatmaps visualize how changes in these hyperparameters affect the error and bias in estimating a fair precision matrix.  Lower values indicate better performance. Subfigure (a) displays error, while (b) shows bias.", "section": "4.2 Performance as Graph Size Increases"}, {"figure_path": "a3cauWMXNV/figures/figures_24_1.jpg", "caption": "Figure 4: Performance in terms of error and bias for estimating a fair precision matrix. (a) Error as parameters \u03bc\u2081 and \u03bc\u2082 vary. (b) Bias as parameters \u03bc\u2081 and \u03bc\u2082 vary.", "description": "This figure shows the performance of Fair GLASSO in terms of error and bias for estimating fair Gaussian graphical models.  It demonstrates how the error and bias change as the hyperparameters \u03bc\u2081 (sparsity penalty weight) and \u03bc\u2082 (bias penalty weight) are varied.  The heatmaps visualize the trade-off between accuracy and fairness, showing that appropriate tuning of the hyperparameters allows for good performance in both areas.  The figure suggests that Fair GLASSO can accurately estimate fair graphical models while controlling bias, even with varying degrees of sparsity and fairness constraints.", "section": "4.2 Performance as Graph Size Increases"}, {"figure_path": "a3cauWMXNV/figures/figures_25_1.jpg", "caption": "Figure 6: Performance in terms of the Frobenius error as different assumptions are violated. (a) Error as true precision matrix becomes denser. (b) Error as eigenvalues of true precision matrix grow close to 0. (c) Error as group sizes become increasingly imbalanced.", "description": "This figure displays the results of experiments designed to test the robustness of the Fair GLASSO algorithm under violations of the assumptions made in Theorem 1.  Panel (a) shows how the estimation error changes as the true precision matrix becomes denser (violating AS1). Panel (b) demonstrates the impact of eigenvalues approaching zero (violating AS2). Panel (c) illustrates how estimation error is affected by increasingly imbalanced group sizes (violating AS4).", "section": "Experiments"}]