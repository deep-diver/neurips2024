{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a foundational vision-language model that is extensively used and analyzed in the current research."}, {"fullname_first_author": "Mitchell Wortsman", "paper_title": "Robust fine-tuning of zero-shot models", "publication_date": "2022-06-01", "reason": "This paper proposes several ensemble-based methods for robust fine-tuning of zero-shot models, which are directly compared against in the current research."}, {"fullname_first_author": "Ananya Kumar", "paper_title": "Calibrated ensembles can mitigate accuracy tradeoffs under distribution shift", "publication_date": "2022-07-01", "reason": "This paper demonstrates the effectiveness of calibrated ensembles in mitigating ID-OOD trade-offs, providing a theoretical foundation for the current research."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "A baseline for detecting misclassified and out-of-distribution examples in neural networks", "publication_date": "2016-01-01", "reason": "This paper introduces a baseline for detecting out-of-distribution examples, providing a benchmark for evaluating robustness that is used in the current research."}, {"fullname_first_author": "Ananya Kumar", "paper_title": "Fine-tuning can distort pretrained features and underperform out-of-distribution", "publication_date": "2022-01-01", "reason": "This paper investigates the impact of fine-tuning on pretrained features and OOD performance, providing valuable insights for the current work."}]}