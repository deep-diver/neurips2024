{"references": [{"fullname_first_author": "Jordan T. Ash", "paper_title": "On warm-starting neural network training", "publication_date": "2020-00-00", "reason": "This paper is cited as an example of a method that shrinks network parameters and adds random parameters, similar to a soft version of the resetting method proposed in the current paper."}, {"fullname_first_author": "Marc G. Bellemare", "paper_title": "A geometric perspective on optimal representations for reinforcement learning", "publication_date": "2019-00-00", "reason": "This paper's concept of a value improvement path inspired the development of a new method for representation learning in the current study."}, {"fullname_first_author": "L. Chizat", "paper_title": "On lazy training in differentiable programming", "publication_date": "2018-00-00", "reason": "This paper's study on the learning dynamics of deep neural networks provides a relevant context for understanding the learning dynamics of deep reinforcement learning (DRL) agents in the current paper."}, {"fullname_first_author": "W. Dabney", "paper_title": "The value-improvement path: Towards better representations for reinforcement learning", "publication_date": "2021-00-00", "reason": "This paper investigates the learning dynamics of value functions, providing insights into the relationship between value function learning and policy learning, which is relevant to the discoveries made in the current work."}, {"fullname_first_author": "S. Fujimoto", "paper_title": "Addressing function approximation error in actor-critic methods", "publication_date": "2018-00-00", "reason": "This paper introduces TD3, which is one of the base algorithms used for experiments and is modified in the current study to implement the proposed method"}]}