[{"type": "text", "text": "Flow Priors for Linear Inverse Problems via Iterative Corrupted Trajectory Matching ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yasi Zhang Peiyu Yu Yaxuan Zhu UCLA UCLA UCLA   \nyasminzhang@ucla.edu yupeiyu98@g.ucla.edu yaxuanzhu@g.ucla.edu Yingshan Chang Feng Gao\u2217 Ying Nian Wu CMU Amazon UCLA   \nyingshac@andrew.cmu.edu fenggo@amazon.com ywu@stat.ucla.edu ", "page_idx": 0}, {"type": "text", "text": "Oscar Leong UCLA oleong@stat.ucla.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Generative models based on flow matching have attracted significant attention for their simplicity and superior performance in high-resolution image synthesis. By leveraging the instantaneous change-of-variables formula, one can directly compute image likelihoods from a learned flow, making them enticing candidates as priors for downstream tasks such as inverse problems. In particular, a natural approach would be to incorporate such image probabilities in a maximum-a-posteriori (MAP) estimation problem. A major obstacle, however, lies in the slow computation of the log-likelihood, as it requires backpropagating through an ODE solver, which can be prohibitively slow for high-dimensional problems. In this work, we propose an iterative algorithm to approximate the MAP estimator efficiently to solve a variety of linear inverse problems. Our algorithm is mathematically justified by the observation that the MAP objective can be approximated by a sum of $N$ \u201clocal MAP\u201d objectives, where $N$ is the number of function evaluations. By leveraging Tweedie\u2019s formula, we show that we can perform gradient steps to sequentially optimize these objectives. We validate our approach for various linear inverse problems, such as super-resolution, deblurring, inpainting, and compressed sensing, and demonstrate that we can outperform other methods based on flow matching. Code is available at https://github.com/YasminZhang/ICTM. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Linear inverse problems are ubiquitous across many imaging domains, pervading areas such as astronomy [41, 23], medical imaging [38, 49], and seismology [35, 39]. In these problems the goal is to reconstruct an unknown image $\\boldsymbol{x}_{*}\\in\\mathbb{R}^{n}$ from observed measurements $y\\in\\mathbb{R}^{m}$ of the form: ", "page_idx": 0}, {"type": "equation", "text": "$$\ny=\\mathcal{A}(x_{*})+\\mathrm{noise},\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $\\mathcal{A}\\,:\\,\\mathbb{R}^{n}\\,\\rightarrow\\,\\mathbb{R}^{m}$ with $m\\,\\leq\\,n$ is a linear operator that degrades the clean image $x_{*}$ , and the additive noise is drawn from a known distribution. In this work, we assume the noise follows $\\mathcal{N}(0,\\sigma_{y}^{2}I)$ . Due to the under-constrained nature of such problems, they are typically ill-posed, i.e., there are an infinite number of undesirable images that fti to the observed measurements. Hence, one requires further structural information about the underlying images, which constitutes our prior. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "With the advent of large generative models [27, 17, 48, 40, 8, 58], there has been a surge of interest in exploiting generative models as priors to solve inverse problems. Given a pretrained generator to sample from a distribution or grant access to image probabilities, one can solve a variety of inverse problems in a task- or forward model-agnostic fashion, without the need for large-scale supervision [36]. This has been successfully done for a variety of models, including implicit generators such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) [4, 34], invertible generators such as Normalizing Flows [1, 54], and more recently Diffusion models [10, 43, 59]. ", "page_idx": 1}, {"type": "text", "text": "A recent paradigm in generative modeling [48, 55, 25, 57], based on the concept of flow matching [29, 28], has made significant strides in scaling ODE-based generators to high-resolution images. Flow matching models map a simple base distribution, such as a Gaussian, to a complex, highdimensional data distribution by defining a flow field that represents the transformation between these distributions. These generative models have demonstrated scalability to high dimensions, forming the backbone of several state-of-the-art generative models [30, 13, 56]. Moreover, flow matching models follow straighter and more direct probability paths compared to diffusion models, allowing for more efficient and faster sampling [28, 29, 13]. Additionally, due to their invertibility, flow matching models provide direct access to image likelihoods through the instantaneous change-of-variables formula [9, 18]. Given these advantages and the relatively recent application of these models to inverse problems [37, 2], we investigate their use as image priors in this work. ", "page_idx": 1}, {"type": "text", "text": "Leveraging knowledge about the corruption process $p(y|x)$ and a natural image prior $p(x)$ , the Bayesian approach suggests analyzing the image reconstruction posterior $p(x|y)\\propto p(y|x)p(x)$ to solve the inverse problem . A proven and effective method based on this approach is maximum-aposteriori (MAP) estimation [6, 19], which maximizes the posterior to identify the image most likely to match the observed measurements: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{argmin}_{x\\in\\mathbb{R}^{n}}-\\log p(x|y)=\\operatorname*{argmin}_{x\\in\\mathbb{R}^{n}}-\\log p(y|x)-\\log p(x).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "MAP estimation provides a single, most probable point estimate of the posterior distribution, making it simple and interpretable. This deterministic approach ensures consistency and reproducibility, which are essential in applications requiring reliable outcomes, particularly in compressed sensing tasks such as Computed Tomography (CT) [7] and Magnetic Resonance Imaging (MRI) [52]. While posterior sampling methods can offer diverse reconstructions to quantify uncertainty, they can be prohibitively slow in high-dimensions [5]. Hence, in this work, we propose to integrate flow priors to solve linear inverse problems by MAP estimation. ", "page_idx": 1}, {"type": "text", "text": "A significant challenge in employing flow priors for MAP estimation lies in the slow computation of the image probabilities, as it requires backpropagating through an ODE solver [47, 16, 15]. In this work, we show how one can address this challenge via Iterative Corrupted Trajectory Matching (ICTM), a novel algorithm to approximate the MAP solution in a computaionally efficient manner. In particular, we show how one can approximately find an MAP solution by sequentially optimizing a novel simpler, auxillary objective that approximates the true MAP objective in the limit of infinite function evaluations. For finite evaluations, we demonstrate that this approximation is sufficient to optimize by showcasing strong empirical performance for flow priors across a variety of linear inverse problems. We summarize our contributions as follows: ", "page_idx": 1}, {"type": "text", "text": "1. We propose ICTM, an algorithm to approximate the MAP solution to a variety of linear inverse problems using a flow prior. This algorithm optimizes an auxillary objective that partitions the flow model\u2019s trajectory into $N$ \u201clocal MAP\u201d objectives, where $N$ is the number of function evaluations (NFEs). By leveraging Tweedie\u2019s formula, we show that we can perform gradient steps to sequentially optimize these objectives.   \n2. Theoretically, we demonstrate that the auxillary objective converges to the true MAP objective as the NFEs goes to infinity. We validate the correctness of our algorithm in finding the MAP solution on a denoising problem.   \n3. We demonstrate the utility of ICTM on a wide variety of linear inverse problems on both natural and scientific image datasets, with problems including denoising, inpainting, superresolution, deblurring, and compressed sensing. Extensive results show that ICTM is both computationaly efficient and obtains high-quality reconstructions, outperforming other reconstruction algorithms based on flow priors. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation We follow the convention for flow-based models, where Gaussian noise is sampled at timestep 0, and the clean image corresponds to timestep 1. Note that this is the opposite of diffusion models. For $t\\in[0,1]$ , we denote $x_{t}(\\bar{x}_{0})$ as the point at time $t$ whose initial condition is $x_{0}$ . In this work, we use $x$ and $x_{1}$ interchangeably, i.e., $x_{1}(x_{0})=x(x_{0})$ . ", "page_idx": 2}, {"type": "text", "text": "2.1 Flow-Based Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider generative models that map samples $x_{0}$ from a noise distribution $p(x_{0})$ , e.g., Gaussian, to samples $x_{1}$ of a data distribution $p(x_{1})$ using an ordinary differential equation (ODE): ", "page_idx": 2}, {"type": "equation", "text": "$$\nd x_{t}=v_{\\theta}(x_{t},t)\\,d t,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the velocity field $v$ is a $\\theta$ -parameterized neural network, e.g., using a UNet [28, 29, 42] or Transformer [13, 51] architecture. Generative models based on flow matching [28, 29] can be seen as a simulation-free approach to learning the velocity field. This approach involves pre-determining paths that the ODE should follow by specifying the interpolation curve $x_{t}$ , rather than relying on the MLE algorithm to implicitly discover them [9]. To construct such a path, which is not necessarily Markovian, one can define a differentiable nonlinear interpolation between $x_{0}$ and $x_{1}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{t}=\\alpha_{t}x_{1}+\\beta_{t}x_{0},\\quad x_{0}\\sim\\mathcal{N}(0,I),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where both $\\alpha_{t}$ and $\\beta_{t}$ are differentiable functions with respect to $t$ satisfying $\\alpha_{0}=0$ , $\\beta_{0}=1$ , and $\\alpha_{1}=1$ , $\\beta_{1}=0$ . This ensures that $x_{t}$ is transported from a standard Gaussian distribution to the natural image manifold from time 0 to time 1. In contrast, the diffusion process [48, 45, 20] induces a non-differentiable trajectory due to the diffusion term in the SDE formulation. ", "page_idx": 2}, {"type": "text", "text": "The idea behind flow matching is to utilize the power of deep neural networks to efficiently predict the velocity field at each timestep. To achieve this, we can train the neural network by minimizing an $L_{2}$ loss between the sampled velocity and the one predicted by the neural network: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta)=\\mathbb{E}_{t,p(x_{1}),p(x_{0})}\\|v_{\\theta}(x_{t},t)-(\\dot{\\alpha}_{t}x_{1}+\\dot{\\beta}_{t}x_{0})\\|^{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We denote the optimal (not necessarily unique) solution to arg min\u03b8 ${\\mathcal{L}}(\\theta)$ as $\\hat{\\theta}$ . The optimal velocity field $v_{\\hat{\\theta}}$ can be derived in closed form and is the expected velocity at state $x_{t}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\nv_{\\hat{\\theta}}(x_{t},t)=\\mathbb{E}_{p(x_{1}),p(x_{0})}[\\dot{\\alpha}_{t}x_{1}+\\dot{\\beta}_{t}x_{0}\\mid x_{t}].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "For convenience, in the following text, we use $v_{\\theta}$ to refer to the optimal $v_{\\hat{\\theta}}$ . In the rest of the paper, we assume that the flow $v_{\\theta}$ and its parameters are pretrained on a dataset of interest and fixed. We are then interested in leveraging its utility as a prior to solve inverse problems. ", "page_idx": 2}, {"type": "text", "text": "2.2 Probability Computation for Flow Priors ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Denote the probability of $x_{t}$ in Eq. (3) as $p(\\boldsymbol{x}_{t})$ dependent on time. Assuming that $v_{\\theta}$ is uniformly Lipschitz continuous in $x_{t}$ and continuous in $t$ , the change in log probability also follows a differential equation [9, 18]: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{\\partial\\log p(x_{t})}{\\partial t}=-\\mathrm{tr}\\left(\\frac{\\partial}{\\partial x}v_{\\theta}(x_{t},t)\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "One can additionally obtain the likelihood of the trajectory via integrating Eq. (7) across time ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\log p(x_{t})=\\log p(x_{\\tau})-\\int_{\\tau}^{t}\\mathrm{tr}\\left(\\frac{\\partial}{\\partial x}v_{\\theta}(x_{s},s)\\right)d s,\\ 0\\leq\\tau<t\\leq1.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this work, we aim to solve the MAP estimation problem in Eq. (2) where $p(x)$ is given by a pretrained flow prior. We first discuss in Section 3.1 how the MAP problem could, in principle, be solved via a latent-space optimization problem. As we will see, this problem is challenging to solve computationally due to the need to backpropagate through an ODE solver. To overcome this, we show in Section 3.2 that the ideal MAP problem can be approximated by a weighted sum of \u201clocal MAP\u201d optimization problems, which operates by partitioning the flow\u2019s trajectory to a reconstructed solution. We then introduce our ICTM algorithm to sequentially optimize this auxiliary objective. Finally, in Section 3.3, we experimentally validate that our algorithm finds a solution that is faithful to the MAP estimate in a simplified setting where the globally optimal MAP solution is known. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3.1 Flow-Based MAP ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given a pretrained flow prior, one can compute the log-likelihood of $x$ generated from an initial noise sample $x_{0}$ via Eq. (8). Hence, to find the MAP estimate, one could equivalently optimize the initial point of the trajectory $x_{0}$ and return $x_{1}(x_{0})$ where $x_{0}$ is found by solving ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x_{0}\\in\\mathbb{R}^{n}}\\underbrace{\\frac{1}{2\\sigma_{y}^{2}}\\|y-A(x_{1}(x_{0}))\\|^{2}}_{\\substack{\\mathrm{:~,~...~,~...~,~.}}}+\\underbrace{\\frac{1}{2}\\|x_{0}\\|^{2}+\\int_{0}^{1}\\mathrm{tr}\\left(\\frac{\\partial}{\\partial x}v_{\\theta}(x_{t},t)\\right)d t}_{\\substack{\\mathrm{,reior}}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $x_{t}\\,:=\\,x_{t}(x_{0})$ denotes the intermediate state $x_{t}$ generated from $x_{0}$ . Intuitively, this loss encourages finding an initial point $x_{0}$ such that the reconstruction $x_{1}:=x_{1}(x_{0})$ fits the observed measurements, but is also likely to be generated by the flow. ", "page_idx": 3}, {"type": "text", "text": "In practice, $x_{1}$ and the prior term can be approximated by an ODE solver. The trajectory of $\\begin{array}{r}{x_{t}=x_{0}\\!+\\!\\int_{0}^{t}v_{\\theta}(x_{t},t)d t}\\end{array}$ can be approximated by an ODE sampler, i.e. $\\mathrm{ODESolve}(x_{0},0,t,v_{\\theta})$ , where $x_{0}$ is the initial point, and the second and third arguments represent the starting time and the ending time, respectively. For example, with an Euler sampler, we iterate over $x_{t+\\Delta t}=x_{t}+v_{\\theta}(x_{t},t)\\Delta t$ where $\\Delta\\bar{t}=1/\\dot{N}$ and $N$ is the predetermined NFEs. After acquiring the optimal $\\scriptstyle{\\hat{x}}_{0}$ by optimizing the Eq. (9), we obtain the MAP solution $x_{1}$ by using ODESolve $\\left(\\hat{x}_{0},0,1,v_{\\theta}\\right)$ again. ", "page_idx": 3}, {"type": "text", "text": "3.2 Flow-Based MAP Approximation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The global flow-based MAP objective Eq. (9) is tractable for low-dimensional problems. The challenge for high-dimensional problems, however, is that optimizing Eq. (9) is simulation-based, and thus each update iteration requires full forward and backward propagation through an ODE solver, resulting in issues regarding memory inefficiency and time, making it hard to optimize [9, 15, 16, 47]. ", "page_idx": 3}, {"type": "text", "text": "As a way to address this, we prove a result in Theorem 1 that shows that the MAP objective can be approximated by a weighted sum of $N$ local posterior objectives. These objectives are \u201clocal\u201d in the sense that they mainly depend on likelihoods and probabilities of intermediate trajectories $x_{t}$ and $x_{t}+v_{\\theta}(x_{t},t)\\Delta t$ for $t=0,\\Delta t,\\ldots,N\\Delta t$ where $\\bar{\\Delta}t:=1/N$ . Given an initial noise input $x_{0}$ , each local posterior objective depends on a non-Markovian auxiliary path $y_{t}\\,=\\,\\alpha_{t}y+\\beta_{t}\\bar{A}(x_{0})$ by connecting the points between $y$ and $\\boldsymbol{\\mathcal{A}}\\boldsymbol{x}_{0}$ . We prove this result for straight paths $\\alpha_{t}=t$ and $\\beta_{t}=1-t$ for simplicity, but other interpolation paths can be used. The proof is in Section A.2. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1. For $N\\geq1$ , set $\\begin{array}{r}{\\gamma_{i}:=\\,(\\frac{1}{2})^{N-i+1}}\\end{array}$ and $\\Delta t=1/N$ . Suppose $y=A(x_{*})+\\epsilon$ where $x_{*}=x_{1}(x_{0})$ with $x_{0}$ being the solution to Eq. (9), $\\epsilon\\sim\\mathcal{N}(0,\\sigma_{y}^{2}I)$ , and $x_{t}$ exactly follows the straight path $x_{t}=t x+(1-t)x_{0}$ for any timestep $t\\in[0,1]$ . Suppose the velocity field $v_{\\theta}:\\mathbb{R}^{n}\\times\\mathbb{R}\\to\\mathbb{R}^{n}$ satisfies $\\begin{array}{r}{\\operatorname*{sup}_{z\\in\\mathbb{R}^{n},s\\in[0,1]}|\\mathrm{tr}\\frac{\\partial}{\\partial x}v_{\\theta}(z,s)|\\leq C_{1}}\\end{array}$ for some universal constant $C_{1}$ . Then, there exists $a$ constant $c(N)^{2}$ that does not depend on $x_{0}$ such that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\operatorname*{lim}_{N\\to\\infty}\\left|\\log p(x(x_{0})|y)-\\sum_{i=1}^{N}\\gamma_{i}\\hat{\\mathcal{I}}_{i}-c(N)\\right|=0,}\\\\ {\\displaystyle\\dot{\\mathcal{I}}_{i}=\\log p(x_{(i-1)\\Delta t})-\\mathrm{tr}\\left(\\frac{\\partial v_{\\theta}(x_{(i-1)\\Delta t},(i-1)\\Delta t)}{\\partial x}\\right)\\Delta t+\\log p(y_{i\\Delta t}|x_{i\\Delta t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This result shows that the true MAP objective evaluated at the optimal solution can be approximated by a weighted sum of objectives that depend locally at a time $t$ for the trajectory $\\{x_{t}:t\\in[0,1]\\}$ . The intuition regarding ${\\hat{\\mathcal{I}}}_{i}$ arises from the fact that $\\hat{\\mathcal{I}}_{i}\\approx\\mathcal{I}_{i}$ , where $\\mathcal{I}_{i}$ is the local posterior distribution ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{T}_{i}=\\log p(y_{i\\Delta t}|x_{i\\Delta t}(x_{(i-1)\\Delta t}))+\\log p(x_{i\\Delta t}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Optimizing each of these local posterior distributions in a sequential fashion captures the fact that we would like each intermediate point in our trajectory $x_{i\\Delta t}$ to be likely and fit to our measurements, ideally resulting in a final reconstruction $x_{1}$ that satisfies this as well. The benefit of ${\\hat{\\mathcal{I}}}_{i}$ , as we will show in the sequel, is that it is efficient to optimize. ", "page_idx": 4}, {"type": "text", "text": "Discussion of assumptions: We assume that the trajectory $\\{x_{t}\\}_{t}$ exactly follows the predefined interpolation path $\\{\\bar{\\alpha_{t}}x+\\beta_{t}x_{0}\\}_{t}$ . In Section B of the appendix, we analyze this assumption and show that we can bound the deviation from the predefined interpolation path to the learned path via a path compliance measure. Moreover, we impose a regularity assumption on the velocity field $v_{\\theta}$ , effectively requiring a uniform bound on the spectrum of the Jacobian of $v_{\\theta}$ . This can be easily satisfied with neural networks using Lipschitz continuous and differentiable activation functions. ", "page_idx": 4}, {"type": "text", "text": "As we see in Theorem 1, one can approximate the true MAP objective via a sum of local objectives of the form ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{I}}_{i}:=\\underbrace{\\log p(y_{i\\Delta t}|x_{i\\Delta t})}_{\\mathrm{local~data~likelihood}}+\\underbrace{\\log p(x_{(i-1)\\Delta t})-\\mathrm{tr}\\left(\\frac{\\partial v_{\\theta}(x_{(i-1)\\Delta t},(i-1)\\Delta t)}{\\partial x}\\right)\\Delta t}_{\\mathrm{local~prior}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "At first glance, ${\\hat{\\mathcal{I}}}_{i}$ still appears challenging to optimize, but there are additional insights we can exploit for computation. We discuss each term in ${\\hat{\\mathcal{I}}}_{i}$ below. ", "page_idx": 4}, {"type": "text", "text": "Local data likelihood: The intuition behind ICTM is that we aim to match a corrupted trajectory $\\{u_{t}\\}_{t}$ with an auxiliary path $\\{y_{t}\\}_{t}$ specified by an interpolation between our measurements $y$ and ${\\mathcal{A}}(x_{0})$ for each timestep $t$ , defined by $y_{t}\\;:=\\;\\alpha_{t}y\\,+\\,\\beta_{t}\\mathcal{A}(x_{0})$ . The corrupted trajectory $u_{t}:=$ $\\boldsymbol{A}(\\boldsymbol{x}_{t})$ follows the corrupted flow ODE $d u_{t}=\\mathcal{A}(v_{\\theta}(x_{t},t))d t$ . To optimize the above \u201clocal MAP\u201d objectives, we must understand the distribution of $p(y_{t}|x_{t})$ . Generally speaking, this distribution is intractable. However, by assuming exact compliance of the trajectory generated by flow to the predefined interpolation path (as done in Theorem 1), we can show that $\\dot{y_{t}}|\\bar{x}_{t}\\sim\\mathcal N(u_{t},\\dot{,}\\alpha_{t}^{2}\\sigma_{y}^{2})$ 2\u03c32). This is proven in Lemma 3 in the appendix. While exact compliance of the trajectory may not hold for learned flow matching models, we show empirically that making this assumption leads to strong performance in practice. We further analyze this notion of compliance in Section B of the appendix. ", "page_idx": 4}, {"type": "text", "text": "Local prior: The approximation in Eq. (10) addresses one of the main concerns of MAP in that the intensive integral computation is circumvented with a simpler Riemannian sum. This approximation holds for small time increments $\\Delta t$ : $\\begin{array}{r}{\\int_{t}^{t+\\Delta t}\\mathrm{tr}\\left(\\frac{\\partial}{\\partial x}v_{\\theta}(x_{s},s)\\right)d s\\approx\\mathrm{tr}\\left(\\frac{\\partial}{\\partial x}v_{\\theta}(x_{t},t)\\right)\\Delta t.}\\end{array}$ . Note that one can additionally improve the efficiency of this term by employing a Hutchinson-Skilling estimate [44, 21] for the trace of the Jacobian matrix. However, at first glance, it appears we have simply shifted the problem to the computation of the prior at timestep $(i-1)\\Delta t$ . Fortunately, it is possible to derive a formula for the gradient of $\\log{p(x_{t})}$ for all timesteps $t\\in[0,1]$ using Tweedie\u2019s formula [12]. This allows us to optimize each objective ${\\hat{\\mathcal{I}}}_{i}$ using gradient-based optimizers. The following result gives a precise characterization of $\\nabla_{x_{t}}\\log{p(x_{t})}$ , proven in Section A.1. ", "page_idx": 4}, {"type": "text", "text": "Proposition 1. Let $\\lambda_{t}=\\alpha_{t}/\\beta_{t}$ denote the signal-to-noise ratio. The relationship between the score function $\\nabla_{x_{t}}\\log{p(x_{t})}$ and the velocity field $\\boldsymbol{v}_{\\theta}(\\boldsymbol{x}_{t},t)$ is given by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla_{x_{t}}\\log{p(x_{t})}=\\frac{1}{\\beta_{t}^{2}}\\left[\\left(\\frac{d\\log{\\lambda_{t}}}{d t}\\right)^{-1}\\left(v_{\\theta}(x_{t},t)-\\frac{d\\log{\\beta_{t}}}{d t}x_{t}\\right)-x_{t}\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In summary, we have derived an efficient approximation to the MAP objective. For our algorithm, we iteratively optimize each term $\\hat{\\mathcal{I}}_{t}$ sequentially for each $t=0,\\Delta t,\\ldots,N\\Delta t$ , ftiting our current iterate $x_{t}$ to induce an increment $x_{t+\\Delta t}$ such that $\\bar{\\mathcal{A}}(x_{t+\\Delta t})$ ftis to our auxiliary corrupted path $y_{t+\\Delta t}$ while being likely under our local prior. We call this approach Iterative Corrupted Trajectory Matching (ICTM). Our algorithm is summarized in Algo. 1. In lines 7 and 12, instead of directly optimizing the local data likelihood, we choose $\\lambda$ as a new hyper-parameter to tune. We find a constant $\\lambda$ works well in practice. ", "page_idx": 4}, {"type": "text", "text": "3.3 Toy Example Validation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We experimentally validate that the reconstruction found via ICTM is close to the optimal MAP solution in a simplified denoising problem where the MAP solution can be obtained in closed-form. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Iterative Corrupted Trajectory Matching (ICTM) with Euler Sampler ", "page_idx": 5}, {"type": "text", "text": "Input: measurement $y$ , matrix $\\boldsymbol{\\mathcal{A}}$ , pretrained flow-based model $\\theta$ , NFEs $N$ , interpolation coefficients   \n$\\left\\{\\alpha_{t}\\right\\}_{t}$ and $\\{\\beta_{t}\\}_{t}$ , step size $\\eta$ , guidance weight $\\lambda$ , and iteration number $K$   \nOutput: recovered clean image $x_{1}$   \n1: Initialize $\\epsilon\\sim\\mathcal{N}(0,I)$ , $x_{0}\\gets\\epsilon$ , $t\\gets0$ , $\\Delta t\\gets1/N$   \n2: Generate an auxiliary path $y_{s}=\\alpha_{s}y+\\beta_{s}(A x_{0})$ for $s\\in(0,1)$   \n3: while $t<1$ do   \n4: $\\begin{array}{r l}&{\\mathsf{v}_{t+\\Delta t}\\gets x_{t}+v_{\\theta}(x_{t},t)\\Delta t}\\\\ &{\\mathsf{f}\\ t=0\\operatorname{then}}\\\\ &{\\quad\\mathbf{f}_{0}r\\ k=1,\\cdots K\\,\\mathbf{d}\\mathbf{\\theta}}\\\\ &{\\qquad x_{t}\\gets x_{t}-\\eta\\nabla_{x_{t}}\\left[\\lambda\\|A(x_{t+\\Delta t}(x_{t}))-y_{t+\\Delta t}\\|^{2}+\\frac{1}{2}\\|x_{t}\\|^{2}+\\operatorname{tr}\\left(\\frac{\\partial v_{\\theta}(x_{t},t)}{\\partial x}\\right)\\Delta t\\right]}\\\\ &{\\qquad\\cdot\\mathbf{\\sigma}}\\end{array}$   \n5: i   \n6:   \n7:   \n8: end for   \n9: else   \n10: for $k=1,\\cdots K$ do   \n11: # use Eq. (11) to obtain the gradient of $\\log{p(x_{t})}$   \n12: $\\begin{array}{r}{\\boldsymbol{x}_{t}\\gets\\boldsymbol{x}_{t}-\\eta\\nabla_{x_{t}}\\left[\\lambda\\lVert\\boldsymbol{A}(\\boldsymbol{x}_{t+\\Delta t}(\\boldsymbol{x}_{t}))-\\boldsymbol{y}_{t+\\Delta t}\\rVert^{2}-\\log p(\\boldsymbol{x}_{t})+\\mathrm{tr}\\left(\\frac{\\partial v_{\\theta}(\\boldsymbol{x}_{t},t)}{\\partial x}\\right)\\Delta t\\right]}\\end{array}$   \n13: end for   \n14: end if   \n15: $\\begin{array}{r l}&{x_{t+\\Delta{t}}\\gets x_{t}+v_{\\theta}(x_{t},t)\\Delta{t}}\\\\ &{t\\gets t+\\Delta{t}}\\end{array}$   \n16:   \n17: end while   \n18: return $x_{1}$ ", "page_idx": 5}, {"type": "image", "img_path": "1H2e7USI09/tmp/d352107ca1a92c2452603eee4c89e88b85798c412430ebe32b999a1563169a37.jpg", "img_caption": ["Figure 1: Results of a toy example modeling 1,000 FFHQ faces as a Gaussian distribution. Subfigure (a) shows the qualitative results of our method; Subfigure (b) presents the histogram of the differences between ours and the true MAP; Subfigure (c) displays the MSE values as the NFEs varies. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Specifically, we fit a Gaussian distribution $\\mathcal{N}(\\mu,\\Sigma)$ using 1,000 samples from the FFHQ dataset. Consider a denoising problem $y=x+\\epsilon$ where $\\boldsymbol{x}\\sim\\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$ and $\\epsilon\\sim\\!\\mathcal{N}(0,\\sigma_{y}^{2}I)$ . In this case, the analytical solution to the MAP estimation problem (Eq. (2)) is $x_{*}=(\\Sigma^{-1}\\!+\\!\\sigma_{y}^{-2}\\bar{I})^{-1}(\\Sigma^{-1}\\mu\\!+\\!\\sigma_{y}^{-2}y)$ . We set $\\sigma_{y}\\,=\\,0.1$ . Then, we train a flow-based model on 10,000 samples from the true Gaussian distribution and showcase the deviation of our reconstruction found via ICTM to the closed-form MAP solution $x_{*}$ in Fig. 1. We see that ICTM can obtain a faithful estimate of the MAP solution across many samples. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In our experimental setting, we use optimal transport interpolation coefficients, i.e. $\\alpha_{t}\\,=\\,t$ and $\\beta_{t}=1-t$ . We test our algorithm on both natural and medical imaging datasets. For natural images, we utilize the pretrained checkpoint from the official Rectified Flow repository3 and evaluate our approach on the CelebA-HQ dataset [31, 24]. We address four common linear inverse problems: super-resolution, inpainting with a random mask, Gaussian deblurring, and inpainting with a box mask. For the medical application, we train a flow-based model from scratch on the Human Connectome Project (HCP) dataset [50] and test our algorithm specifically for compressed sensing at different compression rates. Our algorithm focuses on the reconstruction faithfulness of generated images, therefore employing PSNR and SSIM [53] as evaluation metrics. ", "page_idx": 5}, {"type": "table", "img_path": "1H2e7USI09/tmp/49462d05eb3adbbb87120c1543ad2ae2265c74bf6e020f22ca9c7a532c69872c.jpg", "table_caption": ["Table 1: Quantitative comparison results in terms of PSNR and SSIM on the CelebA-HQ dataset. Our algorithm surpasses all other baselines across all tasks. The best values are highlighted in blue and the second-best are underlined. "], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "1H2e7USI09/tmp/70b80d232021e8ae3db3e2d14cfa1c1486ee133716a64f0aa65ba632bee28756.jpg", "img_caption": ["Figure 2: Qualitative comparison results on the CelebA-HQ dataset. The reconstructions generated by our method align more faithfully with the ground truth and exhibit a higher degree of refinement. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Baselines We compare our method with five baselines. 1) OT-ODE [37]. To our knowledge, this is the only baseline that applies flow-based models to inverse problems. They incorporate a prior gradient correction at each sampling step based on conditional Optimal Transport (OT) paths. For a fair comparison, we follow their implementation of Algorithm 1, providing detailed ablations on initialization time $t^{\\prime}$ in Appendix E.3. 2) DPS-ODE. Inspired by DPS [10], we replace the velocity field with a conditional one, i.e., $v(x_{t}|y)\\,=\\,v(x_{t})+\\zeta_{t}\\nabla_{x_{t}}\\log p(y|\\hat{x}_{1}(x_{t}))$ , where $\\zeta_{t}$ is a hyperparameter to tune. Following the hyperparameter instruction in DPS, we provide detailed ablations on $\\zeta_{t}$ in Appendix E.3. 3) Ours without local prior. To examine the local prior term\u2019s effectiveness in our optimization algorithm, we drop the local prior term as defined in Eq. (10) in our algorithm. In the experiments with natural images, in addition to the flow-based baselines, we have included two representative diffusion-based baselines: 4) RED-Diff [33], a variational Bayes-based method; and 5) \u03a0GDM [46], an advanced MCMC-based method. We also note one concurrent work, D-Flow [2], which formulates the MAP as a constrained optimization problem in their Eq. 9. As documented in their Sec. 3.4, it takes 5-10 minutes to recover each image. This is because each of its optimization step requires backpropagation through an ODE solver to compute the full log-likelihood. In contrast, our method is significantly faster (approximately 1.6 minutes per image) due to our principled local MAP approximation, as demonstrated in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "4.1 Natural Images ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Experimental setup We evaluate our algorithm using 100 images from the CelebA-HQ validation set with a resolution of $256\\!\\times\\!256$ , normalizing all images to the $[0,1]$ range for quantitative analysis. ", "page_idx": 6}, {"type": "text", "text": "Table 2: Results of compressed sensing with varying compression rate $\\nu$ on the HCP T2w dataset. Note that compressed sensing is more challenging due to the complexity of the forward operator, as evidenced by the poor performance of OT-ODE, which assumes a Gaussian distribution of measurement $y$ given $x_{t}$ . The best values are highlighted in blue. ", "page_idx": 7}, {"type": "table", "img_path": "1H2e7USI09/tmp/cfa461d4aa30e0b1520097c65cd45d6e9d9c323b4068567351d6bb07b071c842.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "", "img_caption": ["Figure 3: Qualitative comparison results on compressed sensing. Our method produces more faithful reconstructions with fewer artifacts, ensuring higher accuracy and clarity in the details. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "All experiments incorporate Gaussian measurement noise with $\\sigma_{y}=0.01$ . We address the following linear inverse problems: (1) $4\\times$ super-resolution using bicubic downsampling, (2) inpainting with a random mask covering $70\\%$ of missing values, (3) Gaussian deblurring with a $61\\!\\times\\!61$ kernel and a standard deviation of 3.0, and (4) box inpainting with a centered $128\\!\\times\\!128$ mask. ", "page_idx": 7}, {"type": "text", "text": "We present the quantitative and qualitative results of all the methods in Tab. 1 and Fig. 2, respectively. In Tab. 1, our method surpasses all other baselines across all tasks. For more challenging tasks such as Gaussian deblurring and box inpainting, our method significantly outperforms others in terms of SSIM. Based on the MAP framework, as shown in Fig. 2, our method prefers more faithful and artifact-free reconstructions, whereas others trade off for perceptual quality. We note that there is an unavoidable tradeoff between perceptual quality and restoration faithfulness [3]. Overall, our method presents a higher degree of refinement. The comparison between ours and ours (w/o prior) indicates the effectiveness of the local prior term in enhancing the accuracy of the reconstructions, as evidenced by the increases in both PSNR and SSIM. ", "page_idx": 7}, {"type": "text", "text": "4.2 Medical application ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "HCP T2w dataset We utilize images from the publicly available Human Connectome Project (HCP) [50] T2-weighted (T2w) images dataset for the task of compressed sensing, which contains brain images from 47 patients. The HCP dataset includes cross-sectional images of the brain taken at different levels and angles. ", "page_idx": 7}, {"type": "text", "text": "Compressed sensing We train a flow-based model from scratch on 10,000 randomly sampled images, utilizing the ncsnpp architecture [48] with minor adaptations for grayscale images. We employ compression rates $\\bar{\\nu}\\in\\{1/2,1/4,1/10\\}$ , meaning $m=\\nu n$ . The measurement operator is given by a subsampled Fourier matrix, whose sign patterns are randomly selected. We evaluate our reconstruction algorithm\u2019s performance on 200 randomly sampled test images. ", "page_idx": 7}, {"type": "image", "img_path": "1H2e7USI09/tmp/eb01259e90af589b84b847bdb731b9a307b36a654668b2d1490f18056cedbb32.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 4: Ablation results of step size $\\eta$ and guidance weight $\\lambda$ . The choice of hyperparameters for our algorithm is fairly consistent across all tasks. We choose $\\eta=10^{-2}$ for all experiments on CelebA-HQ. For $\\lambda$ , we choose $\\lambda=10^{3}$ for Gaussian deblurring and $\\lambda=10^{4}$ for the other tasks. ", "page_idx": 8}, {"type": "image", "img_path": "1H2e7USI09/tmp/9b3f2f1fad22550cf2a23c46297a184e50fd5cd196e22cac81178835b3fb76a6.jpg", "img_caption": ["Figure 5: Ablation results of iteration number $K$ on different tasks. For super-resolution and the other three tasks, $K=1$ is sufficient to achieve the best performance with the optimal step size $\\eta$ and guidance weight $\\lambda$ . However, for compressed sensing, it is necessary to increase $K$ to obtain the best performance. We hypothesize that this is due to the increased complexity of the compressed sensing operator, which requires more iteration steps to ensure the correct optimization direction. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "We present the quantitative and qualitative results of compressed sensing in Tab. 2 and Fig. 3, respectively. In addition to flow-based methods, we include results for two classical recovery algorithms, Wavelet [11, 32] and TV [22] priors. As shown in Tab. 2, our method outperforms the classical recovery algorithms and other flow-based baselines across varying compression rates $\\nu$ , demonstrating our method\u2019s capability to handle challenging scenarios and the advantages of utilizing modern generative models as priors. In Fig. 3, our method produces reconstructions that are more faithful to the original images, with fewer artifacts, leading to higher accuracy and clearer details. ", "page_idx": 8}, {"type": "text", "text": "4.3 Ablation studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We use the Adam optimizer [26] for our optimization steps due to its effectiveness in neural network computations. For all tasks, we utilize $N=100$ steps. ", "page_idx": 8}, {"type": "text", "text": "Step size $\\eta$ and Guidance weight $\\lambda$ The use of the Adam optimizer ensures that the choice of hyperparameters, particularly the step size $\\eta$ and the guidance weight $\\lambda$ , remains consistent across various tasks, as illustrated in Fig. 4. Specifically, a step size of $\\eta=10^{-2}$ is optimal for Inpainting (random), Inpainting (box), and Super-resolution in terms of SSIM. For PSNR, Gaussian deblurring also achieves optimal performance at $\\eta=10^{-2}$ . Consequently, we employ $\\eta=10^{-2}$ for all tasks. Based on the results shown in the right two subfigures of Fig. 4, we select $\\lambda=10^{3}$ for Gaussian deblurring and $\\lambda=10^{4}$ for the other tasks. This consistency extends to the compressed sensing experiments, where we set $\\lambda=10^{3}$ and $\\eta=10^{-2}$ for all experiments involving medical images. ", "page_idx": 8}, {"type": "text", "text": "Iteration number $K$ We present ablation results of the iteration number $K$ on different tasks in Fig. 5. We focus on the behavior of $K$ in super-resolution and compressed sensing, as it performs similarly to super-resolution in the other three tasks. With the optimal choice of $\\eta$ and $\\lambda$ in super-resolution, i.e., $\\dot{\\eta}=10^{-2}$ and $\\lambda=10^{3}$ , $K=1$ provides superior performance on CelebA-HQ. A decreased step size, e.g., $\\eta=10^{-3}$ , can help performance as $K$ increases, but it fails to exceed the performance achieved with the optimal parameters at $K=1$ . However, for compressed sensing, it is necessary to increase $K$ to achieve the best performance. Consequently, we set $K=10$ for all compressed sensing experiments. We hypothesize that the complexity of the compressed sensing operator directly determines the number of iterations required for optimal performance. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we have introduced a novel iterative algorithm to incorporate flow priors to solve linear inverse problems. By addressing the computational challenges associated with the slow log-likelihood calculations inherent in flow matching models, our approach leverages the decomposition of the MAP objective into multiple \"local MAP\" objectives. This decomposition, combined with the application of Tweedie\u2019s formula, enables effective sequential optimization through gradient steps. Our method has been rigorously validated on both natural and scientific images across various linear inverse problems, including super-resolution, deblurring, inpainting, and compressed sensing. The empirical results indicate that our algorithm consistently outperforms existing techniques based on flow matching, highlighting its potential as a powerful tool for high-resolution image synthesis and related downstream tasks. ", "page_idx": 9}, {"type": "text", "text": "6 Limitations and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "While our algorithm has demonstrated promising results, there are certain limitations that suggest avenues for future research. First, our theoretical framework, built on optimal transport interpolation paths, is currently limited and cannot be applied to solve the general interpolation between Gaussian and data distributions. Additionally, in order to broaden the applicability of flow priors for inverse problems, it is important to generalize our approach to handle nonlinear forward models. Moreover, the algorithm currently lacks the capability to quantify the uncertainty of the generated images, an aspect crucial for many scientific applications. It would be interesting to consider approaches to post-process our solutions to understand the uncertainty inherent in our reconstruction. These limitations highlight important directions for future work to enhance the robustness and applicability of our method. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The work was partially supported by NSF DMS-2015577, NSF DMS-2415226, and a gift fund from Amazon. We thank anonymous reviewers for their feedback and suggestions, which helped improve the quality of the paper. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Muhammad Asim, Max Daniels, Oscar Leong, Ali Ahmed, and Paul Hand. Invertible generative models for inverse problems: mitigating representation error and dataset bias. Proceedings of the 37th International Conference on Machine Learning, 2020.   \n[2] Heli Ben-Hamu, Omri Puny, Itai Gat, Brian Karrer, Uriel Singer, and Yaron Lipman. D-flow: Differentiating through flows for controlled generation. In Forty-first International Conference on Machine Learning, 2024.   \n[3] Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6228\u20136237, 2018.   \n[4] Ashish Bora, Ajil Jalal, Eric Price, and Alexandros Dimakis. Compressed sensing using generative models. International Conference on Machine Learning, 2017.   \n[5] Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng. Handbook of markov chain monte carlo. CRC press, 2011.   \n[6] Martin Burger and Felix Lucka. Maximum a posteriori estimates in linear inverse problems with log-concave priors are proper bayes estimators. Inverse Problems, 30(11):114004, 2014.   \n[7] Thorsten M Buzug. Computed tomography. In Springer handbook of medical technology, pages 311\u2013342. Springer, 2011.   \n[8] Yingshan Chang, Yasi Zhang, Zhiyuan Fang, Yingnian Wu, Yonatan Bisk, and Feng Gao. Skews in the phenomenon space hinder generalization in text-to-image generation, 2024.   \n[9] Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.   \n[10] Hyungjin Chung, Jeongsol Kim, Michael T Mccann, Marc L Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. In The Eleventh International Conference on Learning Representations, ICLR 2023. The International Conference on Learning Representations, 2023.   \n[11] David L Donoho. Compressed sensing. IEEE Transactions on information theory, 52(4):1289\u2013 1306, 2006.   \n[12] Bradley Efron. Tweedie\u2019s formula and selection bias. Journal of the American Statistical Association, 106(496):1602\u20131614, 2011.   \n[13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M\u00fcller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024.   \n[14] Zhenghan Fang, Sam Buchanan, and Jeremias Sulam. What\u2019s in a prior? learned proximal networks for inverse problems. In International Conference on Learning Representations, 2024.   \n[15] Berthy Feng and Katherine Bouman. Variational bayesian imaging with an efficient surrogate score-based prior. Transactions on Machine Learning Research, 2024.   \n[16] Berthy T Feng, Jamie Smith, Michael Rubinstein, Huiwen Chang, Katherine L Bouman, and William T Freeman. Score-based diffusion models as principled priors for inverse imaging. In International Conference on Computer Vision (ICCV). IEEE, 2023.   \n[17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139\u2013144, 2020.   \n[18] Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. Ffjord: Free-form continuous dynamics for scalable reversible generative models. In International Conference on Learning Representations, 2019.   \n[19] Tapio Helin and Martin Burger. Maximum a posteriori probability estimates in infinitedimensional bayesian inverse problems. Inverse Problems, 31(8):085009, 2015.   \n[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[21] Michael F Hutchinson. A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines. Communications in Statistics-Simulation and Computation, 18(3):1059\u2013 1076, 1989.   \n[22] Leonid I.Rudin, Stanley Osher, and Emad Fatemi. Nonlinear total variation based noise removal algorithms. Physica D: Nonlinear Phenomena, 60:259\u2013268, 1992.   \n[23] Peter A Jansson. Deconvolution of images and spectra. Courier Corporation, 2014.   \n[24] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.   \n[25] Diederik Kingma and Ruiqi Gao. Understanding diffusion objectives as the elbo with simple data augmentation. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 65484\u201365516. Curran Associates, Inc., 2023.   \n[26] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[27] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.   \n[28] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2022.   \n[29] Xingchao Liu, Chengyue Gong, et al. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2022.   \n[30] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, and Qiang Liu. Instaflow: One step is enough for high-quality diffusion-based text-to-image generation. In International Conference on Learning Representations, 2024.   \n[31] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of the IEEE international conference on computer vision, pages 3730\u20133738, 2015.   \n[32] Michael Lustig, David L. Donoho, Juan M. Santos, and John M. Pauly. Compressed sensing mri. IEEE Signal Processing Magazine, 25(2):72\u201382, 2008.   \n[33] Morteza Mardani, Jiaming Song, Jan Kautz, and Arash Vahdat. A variational perspective on solving inverse problems with diffusion models. In The Twelfth International Conference on Learning Representations, 2024.   \n[34] Sachit Menon, Alex Damian, McCourt Hu, Nikhil Ravi, and Cynthia Rudin. Pulse: Selfsupervised photo upsampling via latent space exploration of generative models. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.   \n[35] Guust Nolet. A breviary of seismic tomography. A breviary of seismic tomography, 2008.   \n[36] Gregory Ongie, Ajil Jalal, Christopher A. Metzler, Richard G. Baraniuk, Alexandros G. Dimakis, and Rebecca Willett. Deep learning techniques for inverse problems in imaging. IEEE Journal on Selected Areas in Information Theory, 1(1):39\u201356, 2020.   \n[37] Ashwini Pokle, Matthew J Muckley, Ricky TQ Chen, and Brian Karrer. Training-free linear image inversion via flows. arXiv preprint arXiv:2310.04432, 2023.   \n[38] Saiprasad Ravishankar, Jong Chul Ye, and Jeffrey A Fessler. Image reconstruction: From sparsity to data-adaptive methods and machine learning. Proceedings of the IEEE, 108(1):86\u2013 109, 2019.   \n[39] Nicholas Rawlinson, Andreas Fichtner, Malcolm Sambridge, and Mallory K Young. Seismic tomography and the assessment of uncertainty. Advances in geophysics, 55:1\u201376, 2014.   \n[40] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International conference on machine learning, pages 1530\u20131538. PMLR, 2015.   \n[41] Fran\u00e7ois Roddier. Interferometric imaging in optical astronomy. Physics Reports, 170(2):97\u2013 166, 1988.   \n[42] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention\u2013MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234\u2013241. Springer, 2015.   \n[43] Litu Rout, Negin Raoof, Giannis Daras, Constantine Caramanis, Alex Dimakis, and Sanjay Shakkottai. Solving linear inverse problems provably via posterior sampling with latent diffusion models. Advances in Neural Information Processing Systems, 36, 2024.   \n[44] John Skilling. The eigenvalues of mega-dimensional matrices. Maximum Entropy and Bayesian Methods: Cambridge, England, 1988, pages 455\u2013466, 1989.   \n[45] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 2256\u20132265. PMLR, 2015.   \n[46] Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. Pseudoinverse-guided diffusion models for inverse problems. In International Conference on Learning Representations, 2023.   \n[47] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. Advances in neural information processing systems, 34:1415\u2013 1428, 2021.   \n[48] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   \n[49] Paul Suetens. Fundamentals of medical imaging. Cambridge university press, 2017.   \n[50] David C Van Essen, Stephen M Smith, Deanna M Barch, Timothy EJ Behrens, Essa Yacoub, Kamil Ugurbil, Wu-Minn HCP Consortium, et al. The wu-minn human connectome project: an overview. Neuroimage, 80:62\u201379, 2013.   \n[51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[52] Marinus T Vlaardingerbroek and Jacques A Boer. Magnetic resonance imaging: theory and practice. Springer Science & Business Media, 2013.   \n[53] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600\u2013612, 2004.   \n[54] Jay Whang, Erik M. Lindgren, and Alexandros G. Dimakis. Composing normalizing flows for inverse problems. Proceedings of the 38th International Conference on Machine Learning, 2021.   \n[55] Jianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu. A theory of generative convnet. In Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 2635\u20132644, New York, New York, USA, 20\u201322 Jun 2016. PMLR.   \n[56] Hanshu Yan, Xingchao Liu, Jiachun Pan, Jun Hao Liew, Qiang Liu, and Jiashi Feng. Perflow: Piecewise rectified flow as universal plug-and-play accelerator. arXiv preprint arXiv:2405.07510, 2024.   \n[57] Peiyu Yu, Dinghuai Zhang, Hengzhi He, Xiaojian Ma, Ruiyao Miao, Yifan Lu, Yasi Zhang, Deqian Kong, Ruiqi Gao, Jianwen Xie, Guang Cheng, and Ying Nian Wu. Latent energy-based odyssey: Black-box optimization via expanded exploration in the energy-based latent space, 2024.   \n[58] Yasi Zhang, Peiyu Yu, and Ying Nian Wu. Object-conditioned energy-based attention map alignment in text-to-image diffusion models. arXiv preprint arXiv:2404.07389, 2024.   \n[59] Yaxuan Zhu, Zehao Dou, Haoxin Zheng, Yasi Zhang, Ying Nian Wu, and Ruiqi Gao. Think twice before you act: Improving inverse problem solving with mcmc. arXiv preprint arXiv:2409.08551, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Proof ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Before we dive into the proof, we provide the following three lemmas. ", "page_idx": 14}, {"type": "text", "text": "Lemma 1. Consider a vector-valued function $f:[0,1]\\to\\mathbb{R}^{n}$ . Then for any $t\\in[0,1],$ , we have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\|\\int_{0}^{t}f(s)d s\\right\\|^{2}\\leq\\int_{0}^{t}\\|f(s)\\|^{2}d s.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. For each $s~\\in~[0,1]$ , let $f_{i}(s)\\,\\in\\,\\mathbb{R}$ denote the $i$ -th component of $f(s)$ . Recall Jensen\u2019s inequality: for any convex function $g:\\mathbb{R}\\rightarrow\\mathbb{R}$ and integrable function $h:[0,1]\\rightarrow\\mathbb{R}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\ng\\left(\\int_{a}^{b}h(t)d t\\right)\\leq\\int_{a}^{b}g(h(t))d t.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Using convexity of the function $t\\mapsto t^{2}$ and applying Jensen\u2019s inequality, we see that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left\\vert\\left\\vert\\int_{0}^{t}f(s)d s\\right\\vert\\right\\vert^{2}=\\sum_{i=1}^{n}\\left(\\int_{0}^{t}f_{i}(s)d s\\right)}}\\\\ &{}&{\\leq\\displaystyle\\sum_{i=1}^{n}\\int_{0}^{t}f_{i}(s)^{2}d s}\\\\ &{}&{=\\displaystyle\\int_{0}^{t}\\sum_{i=1}^{n}f_{i}(s)^{2}d s}\\\\ &{}&{=\\displaystyle\\int_{0}^{t}\\|f(s)\\|^{2}d s.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma 2 (Tweedie\u2019s Formula [12]). If $\\dot{\\mu}\\sim g(\\cdot),z|\\mu\\sim\\mathcal{N}(\\alpha\\mu,\\sigma^{2}I),$ , and therefore $z\\sim f(\\cdot)$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mu|z]=\\frac{1}{\\alpha}[z+\\sigma^{2}\\nabla_{z}\\log f(z)].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma 3. Suppose $y\\,=\\,A(x_{*})\\,+\\,\\epsilon$ where $x_{*}\\,=\\,x_{1}(x_{0})$ with $x_{0}$ being the solution to Eq. (9), $A:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{m}$ is linear, $\\epsilon\\sim\\mathcal{N}(0,\\sigma_{y}^{2}I)$ , and $x_{t}$ exactly follows the path $x_{t}=\\alpha_{t}x+\\beta_{t}x_{0}$ for any time $t\\in[0,1]$ . Then we have ", "page_idx": 14}, {"type": "equation", "text": "$$\np(y_{t}|x_{t})=\\mathcal N(\\mathcal A x_{t},\\alpha_{t}^{2}\\sigma_{y}^{2}I),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and hence ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\log p(y|x(x_{0}))=\\log p(y_{t}|x_{t})+\\frac{m}{2}\\log(\\alpha_{t}^{2}),\\forall t.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. Recall that the generated auxiliary path $y_{t}\\;=\\;\\alpha_{t}y\\,+\\,\\beta_{t}\\mathcal{A}x_{0}$ . By assumption, we have $\\begin{array}{r}{\\boldsymbol{A}(\\boldsymbol{x}_{t})=\\boldsymbol{A}(\\alpha_{t}\\boldsymbol{x}+\\beta_{t}\\boldsymbol{x}_{0})=\\alpha_{t}\\boldsymbol{A}(\\boldsymbol{x}(\\boldsymbol{x}_{0}))+\\bar{\\beta}_{t}\\boldsymbol{A}\\boldsymbol{x}_{0}}\\end{array}$ . By subtracting these two equations, we have ", "page_idx": 14}, {"type": "text", "text": "As $y|x(x_{0})\\sim\\mathcal{N}(A x,\\sigma_{y}^{2}I)$ , we have $y_{t}|x_{t}\\sim\\mathcal{N}(A x_{t},\\alpha_{t}^{2}\\sigma_{y}^{2}I)$ . The proof for Eq. (13) is done. Next, we examine the log probability as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log p(y_{t}|x_{t})=-\\frac{\\|y_{t}-A x_{t}\\|^{2}}{2\\alpha_{t}^{2}\\sigma_{y}^{2}}-\\frac{m}{2}\\log(2\\pi\\alpha_{t}^{2}\\sigma_{y}^{2})}\\\\ &{\\phantom{{=}}=-\\frac{\\|\\alpha_{t}(y-A(x(x_{0}))\\|^{2}}{2\\alpha_{t}^{2}\\sigma_{y}^{2}}-\\frac{m}{2}\\log(2\\pi\\alpha_{t}^{2}\\sigma_{y}^{2})}\\\\ &{\\phantom{{=}}=-\\frac{\\|y-A(x(x_{0})\\|^{2}}{2\\sigma_{y}^{2}}-\\frac{m}{2}\\log(2\\pi\\sigma_{y}^{2})-\\frac{m}{2}\\log(\\alpha_{t}^{2})}\\\\ &{\\phantom{{=}}:=\\log p(y|x(x_{0}))-\\frac{m}{2}\\log(\\alpha_{t}^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.1 Proof of Proposition 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Trained by the objective defined in Eq. (5), the optimal velocity field would be ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{v_{\\theta}(x_{t},t)=\\mathbb{E}(\\dot{\\alpha}_{t}x_{1}+\\dot{\\beta}_{t}x_{0}|x_{t})}&{\\mathrm{()}}&{\\Omega(x_{t})}\\\\ {=\\mathbb{E}(\\dot{\\alpha}_{t}x_{1}+\\dot{\\beta}_{t}\\frac{x_{t}-\\alpha_{t}x}{\\beta_{t}}|x_{t})}&{\\mathrm{()}}&{\\Omega t\\mathrm{~diven~}x_{t},\\;x_{0}=\\frac{x_{t}-\\alpha_{t}}{\\beta_{t}}}\\\\ &{}&{(x_{t}-\\dot{\\beta}_{t}\\frac{\\alpha_{t}}{\\beta_{t}})\\mathbb{E}(x_{1}|x_{t})+\\frac{\\dot{\\beta}_{t}}{\\beta_{t}}x_{t}}\\\\ &{}&{(x_{t}-\\dot{\\beta}_{t}\\frac{\\alpha_{t}}{\\beta_{t}})[\\frac{1}{\\alpha_{t}}(x_{t}+\\beta_{t}^{2}\\nabla_{x_{t}}\\log p(x_{t}))]+\\frac{\\dot{\\beta}_{t}}{\\beta_{t}}x_{t}\\mathrm{.}}&{\\mathrm{()}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By defining the signal-to-noise ratio as $\\lambda_{t}\\,=\\,\\alpha_{t}/\\beta_{t}$ and rearranging the equation above, we get exactly Eq. (11) which we display again below: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla_{x_{t}}\\log{p(x_{t})}=\\frac{1}{\\beta_{t}^{2}}\\left[\\left(\\frac{d\\log{\\lambda_{t}}}{d t}\\right)^{-1}\\left(v_{\\theta}(x_{t},t)-\\frac{d\\log{\\beta_{t}}}{d t}x_{t}\\right)-x_{t}\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "When $\\alpha_{t}=t$ , $\\beta_{t}=1-t$ , the equation above becomes ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla_{x_{t}}\\log{p(x_{t})}=\\frac{1}{1-t}(-x_{t}+t v_{\\theta}(x_{t},t)).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A.2 Proof of Theorem 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Before we dive into the proof, we first point out $\\begin{array}{r}{\\operatorname*{lim}_{\\Delta t\\to0}\\sum_{i=1}^{N}\\gamma_{i}\\;=\\;1}\\end{array}$ . Define the timestep $t=(i-1)\\Delta t$ . Conversely, $i=1+t/\\Delta\\bar{t}$ is a function of $t$ . In this sense, we define the -th step Riemannian discretization of the integral $\\begin{array}{r}{-\\int_{0}^{1}\\mathrm{tr}\\left(\\frac{\\partial v_{\\theta}(x_{t},t)}{\\partial x}\\right)d t}\\end{array}$ as $\\begin{array}{r}{\\Delta p_{i}=-\\operatorname{tr}\\left(\\frac{\\partial v_{\\theta}(x_{t},t)}{\\partial x}\\right)\\Delta t}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "We first decompose the global MAP objective as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log p(x(x_{0})|y)=\\log p(x_{0})-\\displaystyle\\int_{0}^{1}\\mathrm{tr}\\left(\\frac{\\partial v_{\\theta}(x_{t},t)}{\\partial x}\\right)d t+\\log p(y|x(x_{0}))-\\log p(y)}\\\\ &{\\qquad\\qquad=\\displaystyle\\operatorname*{lim}_{\\Delta t\\rightarrow0}\\sum_{i=1}^{N}\\gamma_{i}\\log p(x_{0})+\\displaystyle\\operatorname*{lim}_{\\Delta t\\rightarrow0}\\sum_{i=1}^{N}\\Delta p_{i}}\\\\ &{\\qquad\\qquad+\\displaystyle\\operatorname*{lim}_{\\Delta t\\rightarrow0}\\sum_{i=1}^{N}\\gamma_{i}[\\log p(y_{i\\Delta t}|x_{i\\Delta t})+c_{i}]-\\log p(y),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the decomposition of the second term utilizes the property of the discretization of Riemann integral, and that of the third term utilizes the result in Lemma 3 and thus $\\begin{array}{r c l}{c_{i}}&{=}&{\\frac{m}{2}\\log(\\alpha_{i\\Delta t}^{2})}\\end{array}$ . By the property of limits, i.e. $\\begin{array}{r l}{\\operatorname*{lim}_{\\Delta t\\to0}(\\sum_{i=1}^{N}\\gamma_{i})(\\sum_{i=1}^{N}\\Delta p_{i})}&{=}\\end{array}$ $\\begin{array}{r}{\\operatorname*{lim}_{\\Delta t\\to0}(\\sum_{i=1}^{N}\\gamma_{i})\\operatorname*{lim}_{\\Delta t\\to0}(\\sum_{i=1}^{N}\\Delta p_{i})\\;=\\;\\operatorname*{lim}_{\\Delta t\\to0}\\sum_{i=1}^{N}\\Delta p_{i}}\\end{array}$ , we can further decompose the second term in Eq. (28) into $\\begin{array}{r}{\\operatorname*{lim}_{\\Delta t\\to0}(\\sum_{i=1}^{N}\\gamma_{i})(\\sum_{i=1}^{N}\\Delta p_{i})}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "By extracting the limit out in Eq. (28), the equation becomes ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\Delta t\\rightarrow0}{\\operatorname*{lim}}\\left\\lbrace\\gamma_{1}\\left[\\log p(x_{0})+\\Delta p_{1}+\\log p(y_{\\Delta t}|x_{\\Delta t})+c_{1}\\right]\\right.}\\\\ &{\\qquad+\\left.\\gamma_{2}\\left[\\log p(x_{0})+\\Delta p_{1}+\\Delta p_{2}+\\log p(y_{2\\Delta t}|x_{2\\Delta t})+c_{2}\\right]}\\\\ &{\\qquad+\\dots}\\\\ &{\\qquad+\\left.\\gamma_{N}\\left[\\log p(x_{0})+\\Delta p_{1}+\\Delta p_{2}+\\dots+\\Delta p_{N}+\\log p(y_{N\\Delta t}|x_{N\\Delta t})+c_{N}\\right]\\right.}\\\\ &{\\qquad+\\left.\\left[\\gamma_{1}\\Delta p_{2}+(\\gamma_{1}+\\gamma_{2})\\Delta p_{3}+\\dots+(\\gamma_{1}+\\gamma_{2}+\\dots+\\gamma_{N-1})\\,\\Delta p_{N}\\right]-\\log p(y)\\right\\rbrace}\\\\ &{\\qquad+\\left.\\left[\\frac{N}{\\Delta t\\rightarrow0}\\left[\\displaystyle\\sum_{i=1}^{N}\\gamma_{i}\\tilde{\\mathcal{J}}_{i}+\\displaystyle\\sum_{j=2}^{N}\\left(\\displaystyle\\sum_{i=1}^{j-1}\\gamma_{i}\\right)\\Delta p_{j}+\\displaystyle\\sum_{i=1}^{N}\\gamma_{i}c_{i}-\\log p(y)\\right]\\right.,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\begin{array}{r}{\\tilde{\\mathcal{T}}_{i}:=\\log p(x_{0})+\\sum_{j=1}^{i}\\Delta p_{j}+\\log p(y_{i\\Delta t}|x_{i\\Delta t})}\\end{array}$ . We further define the $\\begin{array}{r}{c(N)=\\sum_{i=1}^{N}{\\gamma_{i}c_{i}}-}\\end{array}$ $\\log{p(y)}$ . ", "page_idx": 16}, {"type": "text", "text": "Recall that $\\begin{array}{r}{\\hat{\\mathcal{I}}_{i}=\\log p(x_{(i-1)\\Delta t})-\\mathrm{tr}\\left(\\frac{\\partial v_{\\theta}(x_{(i-1)\\Delta t},(i-1)\\Delta t)}{\\partial x}\\right)\\Delta t+\\log p(y_{i\\Delta t}|x_{i\\Delta t})}\\end{array}$ . By triangle inequality, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left|\\log p(x(x_{0})|y)-\\sum_{i=1}^{N}\\gamma_{i}\\hat{\\mathcal{I}}_{i}-c(N)\\right|}\\\\ {\\displaystyle\\leqslant\\left|\\log p(x(x_{0})|y)-\\sum_{i=1}^{N}\\gamma_{i}\\tilde{\\mathcal{I}}_{i}-c(N)\\right|+\\left|\\sum_{i=1}^{N}\\gamma_{i}\\hat{\\mathcal{I}}_{i}-\\sum_{i=1}^{N}\\gamma_{i}\\tilde{\\mathcal{I}}_{i}\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Taking the limit on both sides, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{lim}_{\\Delta t\\rightarrow0}\\left|\\log p(x(x_{0})|y)-\\sum_{i=1}^{N}\\gamma_{i}\\hat{\\mathcal{T}}_{i}-c(N)\\right|}\\\\ {\\displaystyle\\quad\\leqslant\\operatorname*{lim}_{\\Delta t\\rightarrow0}\\left|\\log p(x(x_{0})|y)-\\sum_{i=1}^{N}\\gamma_{i}\\mathcal{\\tilde{T}}_{i}-c(N)\\right|+\\displaystyle\\operatorname*{lim}_{\\Delta t\\rightarrow0}\\left|\\sum_{i=1}^{N}\\gamma_{i}\\hat{\\mathcal{T}}_{i}-\\sum_{i=1}^{N}\\gamma_{i}\\mathcal{\\tilde{T}}_{i}\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In the following, we analyze the two terms on the right-hand side one by one. For the first term: as $|\\cdot|:\\mathbb{R}\\to\\mathbb{R}$ is a continuous function, the first term on the right-hand side is equal to ", "text_level": 1, "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\displaystyle\\log p(x(x_{0})|y)-\\operatorname*{lim}_{\\Delta t\\rightarrow0}\\displaystyle\\sum_{i=1}^{N}\\gamma_{i}\\tilde{J}_{i}-c(N)\\right|}\\\\ &{=\\left|\\displaystyle\\operatorname*{lim}_{\\Delta t\\rightarrow0}\\displaystyle\\sum_{j=2}^{N}\\left(\\sum_{i=1}^{j-1}\\gamma_{i}\\right)\\Delta p_{j}\\right|}\\\\ &{=\\left|\\displaystyle\\operatorname*{lim}_{\\Delta t\\rightarrow0}\\displaystyle\\sum_{j=2}^{N}\\left(\\frac{1}{2^{N-j+1}}-\\frac{1}{2^{N}}\\right)\\Delta p_{j}\\right|}\\\\ &{\\leq\\left|\\displaystyle\\operatorname*{lim}_{\\Delta t\\rightarrow0}\\displaystyle\\sum_{j=2}^{N}\\left(\\frac{1}{2^{N-j+1}}\\right)\\Delta p_{j}\\right|+\\left|\\displaystyle\\operatorname*{lim}_{\\Delta t\\rightarrow0}\\displaystyle\\sum_{j=2}^{N}\\left(\\frac{1}{2^{N}}\\right)\\Delta p_{j}\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the first equation is derived by subtracting the first term in Eq. (30) from Eq. (26). As the velocity field $v_{\\theta}:\\mathbb{R}^{n}\\times\\mathbb{R}\\to\\mathbb{R}^{n}$ satisfies $\\begin{array}{r}{\\operatorname*{sup}_{z\\in\\mathbb{R}^{n},s\\in[0,1]}|\\mathrm{tr}\\frac{\\partial}{\\partial x}v_{\\theta}(z,s)|\\leq C_{1}}\\end{array}$ for some universal constant $C_{1}$ , we have $|\\Delta p_{j}|\\le C_{1}\\Delta t$ . The first term in (38) would be ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|\\sum_{j=2}^{N}\\left(\\frac{1}{2^{N-j+1}}\\right)\\Delta p_{j}\\right|\\leq C_{1}\\Delta t\\sum_{j=2}^{N}\\left(\\frac{1}{2^{N-j+1}}\\right)\\leq C_{1}\\Delta t=O(\\Delta t).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Similarly, the second term in (38) would be ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|\\sum_{j=2}^{n}\\left({\\frac{1}{2^{n}}}\\right)\\Delta p_{j}\\right|\\leq\\sum_{j=2}^{N}\\left({\\frac{1}{2^{N}}}\\right)C_{1}\\Delta t=C_{1}\\left({\\frac{N-1}{2^{N}}}\\right)\\Delta t=O(\\Delta t).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combining the results in Eq. (39) and Eq. (40), we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|\\log p(x(x_{0})|y)-\\operatorname*{lim}_{\\Delta t\\to0}\\sum_{i=1}^{N}\\gamma_{i}\\tilde{\\mathcal{I}}_{i}-c(N)\\right|=0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For the second term: Intuitively, the error between the integral and the Riemannian discretization goes to 0 as $\\Delta t$ tends to 0. Rigorously, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{lim}_{\\Delta t\\rightarrow0}\\left|\\sum_{i=1}^{N}\\gamma_{i}\\hat{J_{i}}-\\sum_{i=1}^{N}\\gamma_{i}\\tilde{J_{i}}\\right|=\\displaystyle\\operatorname*{lim}_{\\Delta t\\rightarrow0}\\left|\\sum_{i=1}^{N}\\gamma_{i}(\\hat{J_{i}}-\\tilde{J_{i}})\\right|}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\operatorname*{lim}_{\\Delta t\\rightarrow0}\\left|\\sum_{i=1}^{N}\\gamma_{i}\\left(\\displaystyle\\int_{0}^{t-\\Delta t}\\mathrm{tr}\\left(\\frac{\\partial v_{\\theta}(x_{s},s)}{\\partial x}\\right)d s-\\sum_{j=1}^{i-1}\\Delta p_{j}\\right)\\right|}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\operatorname*{lim}_{\\Delta t\\rightarrow0}\\sum_{i=1}^{N}\\gamma_{i}\\left|\\int_{0}^{t-\\Delta t}\\mathrm{tr}\\left(\\frac{\\partial v_{\\theta}(x_{s},s)}{\\partial x}\\right)d s-\\sum_{j=1}^{i-1}\\Delta p_{j}\\right|=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Combining the results of the first term and the second term, we get the proof of theorem 1 done. ", "page_idx": 17}, {"type": "text", "text": "B Compliance of Trajectory ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To quantify our deviation from the assumption of having $x_{t}$ exactly follow the interpolation path $\\alpha_{t}x+\\beta_{t}x_{0}$ , we define the following: given a differentiable process $\\left\\{z_{t}\\right\\}$ and an interpolation path specified by $\\pmb{\\alpha}:=\\{\\alpha_{t}\\}$ and $\\beta:=\\{\\beta_{t}\\}$ , we define the trajectory\u2019s compliance $S_{\\alpha,\\beta}(\\left\\{z_{t}\\right\\})$ to the interpolation path as ", "page_idx": 17}, {"type": "equation", "text": "$$\nS_{\\alpha,\\beta}(\\{z_{t}\\}):=\\int_{0}^{1}\\mathbb{E}_{p(z_{0}),p(z_{1})}\\left[\\|\\dot{z}_{t}-(\\dot{\\alpha}_{t}z_{1}+\\dot{\\beta}_{t}z_{0})\\|^{2}\\right]d t.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This generalizes the definition of straightness in [29] to general interpolation paths. We recover their definition by setting $\\alpha_{t}=t$ and $\\beta_{t}=1-t$ . In certain cases, we have exact compliance with the predefined interpolation path. For example, when $\\left\\{z_{t}\\right\\}$ is generated by $v_{\\theta}$ and $\\alpha_{t}=t$ and $\\beta_{t}=1-t$ , note that $S_{\\alpha,\\beta}(\\{z_{t}\\})=\\bar{0}$ is equivalent to $v_{\\theta}(z_{t},t)=c$ where $c$ is a constant, almost everywhere. This ensures that $z_{1}=z_{0}+c$ . In this case, when generating the trajectory through an ODE solver with starting point $x_{0}$ and endpoint $x_{t}$ , we have $x_{t}=\\alpha_{t}x+\\beta_{t}x_{0},\\forall t$ . When $S_{\\alpha,\\beta}(\\{z_{t}\\})$ is not equal to 0, we show in Proposition 2 that we can bound the deviation of our trajectory from the interpolation path using this compliance measure. When specifying our result to Rectified Flow, we can obtain an additional bound showing that when using $L$ -Rectified Flow, the deviation of the learned trajectory from the straight trajectory is bounded by $O(1/L)$ . ", "page_idx": 17}, {"type": "text", "text": "Proposition 2. Consider a differentiable interpolation path specified by $\\pmb{\\alpha}:=\\{\\alpha_{t}\\}$ and $\\beta:=\\{\\beta_{t}\\}$ . Then the expected distance between the learned trajectory $\\begin{array}{r}{z_{t}=z_{0}\\!+\\!\\int_{0}^{t}v_{\\theta}(z_{s},s)d s}\\end{array}$ and the predefined trajectory $\\begin{array}{r}{\\hat{z}_{t}=z_{0}+\\int_{0}^{t}(\\dot{\\alpha}_{s}z_{1}+\\dot{\\beta}_{s}z_{0})d s}\\end{array}$ can be bounded as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{p(z_{0}),p(z_{1})}\\left[\\|\\hat{z}_{t}-z_{t}\\|^{2}\\right]\\leq S_{\\alpha,\\beta}(\\{z_{t}\\}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "If the differentiable process $\\left\\{z_{t}\\right\\}$ is specified by $L$ -Rectified Flow and $\\alpha_{t}=t$ and $\\beta_{t}=1-t$ for all $t\\in[0,1],$ , then we additionally have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{p(z_{0}),p(z_{1})}\\left[\\|\\hat{z}_{t}-z_{t}\\|^{2}\\right]\\leq O\\left(\\frac{1}{L}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. At time $t$ , we are interested in the distance between a real trajectory $\\begin{array}{r}{z_{t}=z_{0}+\\int_{0}^{t}v_{\\theta}(z_{s},s)d s}\\end{array}$ and a preferred trajectory $\\begin{array}{r}{\\hat{z}_{t}=z_{0}+\\int_{0}^{t}(\\dot{\\alpha}_{s}z_{1}-\\dot{\\beta}_{s}z_{0})d s}\\end{array}$ . Using the result in Lemma 1, the distance can be bounded by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\|\\hat{z}_{t}-z_{t}\\|^{2}=\\bigg\\|\\displaystyle\\int_{0}^{t}[v_{\\theta}(z_{s},s)-(\\dot{\\alpha}_{s}z_{1}-\\dot{\\beta}_{s}z_{0})]d s\\bigg\\|^{2}}\\\\ &{}&{\\qquad\\le\\displaystyle\\int_{0}^{t}\\|v_{\\theta}(z_{s},s)-(\\dot{\\alpha}_{s}z_{1}-\\dot{\\beta}_{s}z_{0})\\|^{2}d s.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "image", "img_path": "1H2e7USI09/tmp/be9cf48ea3bb2909bd747b2777e008f2f3e42a91ee213bb48d0cedb9ad5db4e3.jpg", "img_caption": ["Figure 6: Ablation results of $K$ in terms of SSIM on different tasks. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Therefore, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbb{E}_{p(z_{0}),p(z_{1})}\\|\\hat{z}_{t}-z_{t}\\|^{2}\\leq\\mathbb{E}_{p(z_{0}),p(z_{1})}\\left[\\int_{0}^{t}\\|v_{\\theta}(z_{s},s)-(\\dot{\\alpha}_{s}z_{1}-\\dot{\\beta}_{s}z_{0})\\|^{2}d s\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\int_{0}^{t}\\mathbb{E}_{p(z_{0}),p(z_{1})}\\|v_{\\theta}(z_{s},s)-(\\dot{\\alpha}_{s}z_{1}-\\dot{\\beta}_{s}z_{0})\\|^{2}d s}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\int_{0}^{1}\\mathbb{E}_{p(z_{0}),p(z_{1})}\\|v_{\\theta}(z_{s},s)-(\\dot{\\alpha}_{s}z_{1}-\\dot{\\beta}_{s}z_{0})\\|^{2}d s}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad:=S_{\\alpha,\\beta}(\\{z\\}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "If $\\{z_{t},t\\in[0,1]\\}$ is a learned $L$ -rectfied flow, i.e. $\\alpha_{t}=t$ and $\\beta_{t}=1-t$ in this case, where $L$ is the times of rectifying the flow, by Theorem 3.7 in [29], we have $S_{\\alpha,\\beta}(\\{z\\})=O(1/L)$ and thus ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{p(z_{0}),p(z_{1})}\\|\\hat{z}_{t}-z_{t}\\|^{2}=O(1/L).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Empirically, [30, 29] found $L=2$ generates nearly straight trajectories for high-quality one-step generation. Hence, while this result gives us a simple upper bound, in practice the trajectories may comply more faithfully with the predefined interpolation path than this result suggests. ", "page_idx": 18}, {"type": "text", "text": "C Additional Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1 Additional Ablations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Iteration steps $K$ We provide additional ablation results of $K$ in terms of SSIM in Fig. 6. ", "page_idx": 18}, {"type": "text", "text": "NFEs $N$ We first refer to Fig. 1(c) for a preliminary ablation on $N$ using a toy example. Next, we show PSNR and SSIM scores for varying $N$ in the task of super-resolution. We find that $N=100$ is the best trade-off between time and performance. The ablation results are shown in Fig. 7. ", "page_idx": 18}, {"type": "image", "img_path": "1H2e7USI09/tmp/4ac1c5a2eb151f8c4b16f8e92b1b71c6f4477a0d595979f968ffb717b791ee37.jpg", "img_caption": ["Figure 7: Ablation results of the NFEs $N$ on the super-resolution task. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "D Computational Efficiency ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Tab. 3, we present the computational efficiency comparison results. Note that OT-ODE is the slowest as it requires taking the inverse of a matrix $\\dot{r}_{t}^{2}\\mathcal{A}A^{T}+\\sigma_{y}^{2}I$ each update time. Our ", "page_idx": 18}, {"type": "text", "text": "method requires taking the gradient over an estimated trace of the Jacobian matrix, which slows the computation. ", "page_idx": 19}, {"type": "table", "img_path": "1H2e7USI09/tmp/d7b311b9ae6eee4d4157306001b34c01cc39fc7febfb7649705cee8e062f6455.jpg", "table_caption": ["Table 3: Computational time comparison. We compare the time required to recover 100 images for the super-resolution task on a single GPU. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "E Implementation Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Experiments were conducted on a Linux-based system with CUDA 12.2 equipped with 4 Nvidia R9000 GPUs, each of them has 48GB of memory. ", "page_idx": 19}, {"type": "text", "text": "Operators For all the experiments on the CelebA-HQ dataset, we use the operators from [10]. For all the experiments on compressed sensing, we use the operator CompressedSensingOperator defined in the official repository of [14] 4, ", "page_idx": 19}, {"type": "text", "text": "Evaluation Metrics are implemented with different Python packages. PSNR is calculated using basic PyTorch operations, and SSIM is computed using the pytorch_msssim package. ", "page_idx": 19}, {"type": "text", "text": "E.1 Toy example ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The workflow begins with using 1,000 FFHQ images at a resolution of $1024\\!\\times\\!1024$ . These images are then downscaled to $16\\!\\times\\!16$ using bicubic resizing. A Gaussian Mixture model is applied to fti the downsampled images, resulting in mean and covariance parameters. The mean values are transformed from the original range of [0,1] to [-1,1]. Subsequently, 10,000 samples are generated from this distribution to facilitate training a score-based model resembling the architecture of CIFAR10 $\\scriptstyle\\mathrm{DDPM++}$ . The training process involves 10,000 iterations, each with a batch size of 64, and utilizes the Adam optimizer [26] with a learning rate of 2e-4 and a warmup phase lasting 100 steps. Notably, convergence is achieved within approximately 200 steps. Lastly, the estimated log-likelihood computation for a batch size of 128 takes around 4 minutes and 30 seconds. We show uncured samples generated from the trained models in Fig. 8. ", "page_idx": 19}, {"type": "image", "img_path": "1H2e7USI09/tmp/5d7b9eee3f10d0296b226fb40228af7e28172219c3e1da86ba1f6c1ffcbc0402.jpg", "img_caption": ["Figure 8: Generated samples from the flow trained on 10,000 Gaussian samples. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "E.2 Medical Application ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this setting, $\\sigma_{y}=0.001$ . We use the ncsnpp architecture, training from scratch on 10k images for 100k iterations with a batch size of 50. We set the learning rate to $1\\times10^{-2}$ . Sudden convergence appeared during our training process. We use 2000 warmup steps. Uncured generated images are presented in Fig. 9. ", "page_idx": 20}, {"type": "image", "img_path": "1H2e7USI09/tmp/f7f6e0d61e17e686bf2d823d861ffa6de087b2ae416fed8d54345591faf2ceb4.jpg", "img_caption": ["Figure 9: Generated samples from the flow trained on 10,000 HCP T2w images. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "E.3 Implementation of Baselines ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "OT-ODE As OT-ODE [37] has not released their code and pretrained checkpoints. We reproduce their method with the same architecture as in [29]. We follow their setting and find initialization time $t^{\\prime}$ has a great impact on the performance. We use the $y$ init method in their paper. Specifically, the starting point is ", "page_idx": 20}, {"type": "equation", "text": "$$\nx_{t^{\\prime}}=t^{\\prime}y+(1-t^{\\prime})\\epsilon,\\;\\epsilon\\sim\\mathcal{N}(0,I),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $t^{\\prime}$ is the init time. Note that in the super-resolution task we upscale $y$ with bicubic first. We follow the guidance in the paper and show the ablation results in Fig. 10 and Fig. 11. ", "page_idx": 20}, {"type": "text", "text": "DPS-ODE We use the following formula to update for each step in the flow: ", "page_idx": 20}, {"type": "equation", "text": "$$\nv(x_{t},y)=v(x_{t})+\\zeta_{t}\\left(-\\nabla_{x_{t}}\\|y-A\\hat{x}_{1}\\|^{2}\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\zeta_{t}$ is the step size to tune. We refer to DPS for the method to choose $\\zeta_{t}$ . We set $\\zeta_{t}\\;=$ 2\u2225y\u2212A\u03b7x\u02c61(xt)\u2225. We demonstrate the ablation of \u03b7 for this baseline in Fig. 12 and Fig. 13. Note that there is a significant divergence in PSNR and SSIM for the task of inpainting (box). As we observe that artifacts are likely to appear when $\\eta\\geq100$ , we choose the optimal $\\eta=75$ for the best tradeoff. ", "page_idx": 20}, {"type": "text", "text": "RED-Diff and \u03a0GDM We use the official repository5 from Nvidia to reproduce the results of RED-Diff and \u03a0GDM with the pretrained CelebAHQ checkpoint using the architecture of the guided diffusion repository6 from OpenAI. ", "page_idx": 20}, {"type": "text", "text": "For RED-Diff, the optimization objective is $\\quad\\operatorname*{min}_{\\mu}||y-A(\\mu)||^{2}+\\lambda(s g(\\epsilon_{\\theta}(x_{t},t)-\\epsilon))^{T}\\mu$ . Following the implementation of the original paper, we use Adam optimizer with 1,000 steps for all tasks. We choose learning rate $l r=0.25,\\lambda=0.25$ for super-resolution, inpainting(random) and inpainting(box) and $l r=0.5,\\lambda=0.25$ for deblurring as recommended by the paper. ", "page_idx": 20}, {"type": "image", "img_path": "1H2e7USI09/tmp/2371401c3843bfe61c99b4bf3945036289f73e09f780bc73251565f47cd255e1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 10: Hyperparameter $t^{\\prime}$ selection results for OT-ODE on the CelebA-HQ dataset. We select $t^{\\prime}=$ 0.2, 0.1, 0.2, 0.2 for super-resolution, inpainting(random), Gaussian deblurring, and inpainting(box), respectively. ", "page_idx": 21}, {"type": "image", "img_path": "1H2e7USI09/tmp/ba56f57b65030745837da0be94290f5ebff60ff7dc366bb46ed50ae251d10399.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 11: Hyperparameter $t^{\\prime}$ selection results for OT-ODE on the HCP T2w dataset. We select $t^{\\prime}=0.1$ for all the experiments. ", "page_idx": 21}, {"type": "text", "text": "For \u03a0GDM, we follow the original paper and use 100 diffusion steps. Specifically, we use $\\eta=1.0$ which corresponds to the VE-SDE. Adaptive weights rt2 =1\u03c3+1\u2212\u03c3tt2 are used if there is an improvement on metrics. ", "page_idx": 21}, {"type": "text", "text": "Wavelet and TV priors We use the pytorch package DeepInverse7 to implement Wavelet and TV priors. For both priors, we use the default Proximal Gradient Descent (PGD) algorithm and perform a grid search for regularization weight $\\lambda$ in the set $\\{10^{0},10^{-1},10^{-2},10^{-3},10^{-4}\\}$ and gradient stepsize $\\eta$ in $\\{10^{1},10^{0},1\\breve{0}^{-1},10^{-2},10^{-\\breve{3}},10^{-4}\\}$ . The maximum number of iteration is $3\\mathbf{k},5\\mathbf{k}$ , and 10k for compression rate $\\nu=1/2,1/4$ , and $1/10$ , respectively. The stopping criterion is the residual norm ||x||tx\u2212t1\u2212\u22121|x|t|| \u22641 \u00d7 10\u22125 and the initialization of the algorithm is the backprojected reconstruction, i.e., the pseudoinverse of $\\boldsymbol{\\mathcal{A}}$ applied to the measurement $y$ . ", "page_idx": 21}, {"type": "text", "text": "For the TV prior, the objective we aim to minimize is $\\begin{array}{r}{\\operatorname*{min}_{x}\\frac{1}{2}||A x-y||_{2}^{2}+\\lambda||x||_{T V}}\\end{array}$ . We find that the optimal combination of hyperparameters is $\\lambda=0.01,\\eta=0.1$ for all the values of $\\nu$ . ", "page_idx": 21}, {"type": "text", "text": "For the Wavelet prior, the objective we want to minimize is $\\begin{array}{r}{\\operatorname*{min}_{x}\\frac{1}{2}||A x-y||_{2}^{2}+\\lambda||\\Psi x||_{1}}\\end{array}$ . We use the default level of the wavelet transform and select the \u201cdb8\u201d Wavelet. The optimal combination of hyperparameters is $\\lambda=0.1,\\eta=0.1$ for all the values of $\\nu$ . ", "page_idx": 21}, {"type": "table", "img_path": "1H2e7USI09/tmp/c3e571ad6c58f659c326a7c62e8dbba7d86ef0ff7f39b221059f071a8190f293.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 12: Hyperparameter $\\eta$ selection results for DPS-ODE. We select $\\eta=1000$ , 750, 200, 75 for super-resolution, inpainting(random), Gaussian deblurring, and inpainting(box), respectively. ", "page_idx": 22}, {"type": "image", "img_path": "1H2e7USI09/tmp/0f241470162eaa11fcd8d3aa2672ae76370460aed48fe95c49cdc167a589f4d3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 13: Hyperparameter $\\eta$ selection results for DPS-ODE on the HCP T2w dataset. We select $\\eta=200$ for all the experiments. ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The claims made in the abstract match theoretical and experimental results. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: They are discussed in Section Limitations. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: They are provided in Section Method. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: They are provided in Section Experiments and Appendix. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 23}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We have provided sufficient implementation details and links for original repositories. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 24}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: They are provided in Section Experiments and Appendix. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Standard deviations are provided in Tables. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: They are provided in Appendix. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We confirm that the paper conforms with NeurIPS Code of Ethics. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: They are discussion in Section Broader Impacts. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: They are properly cited. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: They are well documented in Section Experiments and Appendix. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]