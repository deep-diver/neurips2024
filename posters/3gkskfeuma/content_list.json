[{"type": "text", "text": "Improving the Learning Capability of Small-size Image Restoration Network by Deep Fourier Shifting ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Man Zhou Aerospace Information Research Institute, Chinese Academy of Sciences University of Science and Technology of China ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "State-of-the-art image restoration methods currently face challenges in terms of computational requirements and performance, making them impractical for deployment on edge devices such as phones and resource-limited devices. As a result, there is a need to develop alternative solutions with efficient designs that can achieve comparable performance to transformer or large-kernel methods. This motivates our research to explore techniques for improving the capability of small-size image restoration standing on the success secret of large receptive filed. ", "page_idx": 0}, {"type": "text", "text": "Targeting at expanding receptive filed, spatial-shift operator tailored for efficient spatial communication and has achieved remarkable advances in high-level image classification tasks, like $S^{2}$ -MLP [1] and ShiftVit [2]. However, its potential has rarely been explored in low-level image restoration tasks. The underlying reason behind this obstacle is that image restoration is sensitive to the spatial shift that occurs due to severe region-aware information loss, which exhibits a different behavior from high-level tasks. To address this challenge and unleash the potential of spatial shift for image restoration, we propose an information-lossless shifting operator, i.e., Deep Fourier Shifting, that is customized for image restoration. To develop our proposed operator, we first revisit the principle of shift operator and apply it to the Fourier domain, where the shift operator can be modeled in an information-lossless Fourier cycling manner. Inspired by Fourier cycling, we design two variants of Deep Fourier Shifting, namely the amplitude-phase variant and the real-imaginary variant. These variants are generic operators that can be directly plugged into existing image restoration networks as a drop-in replacement for the standard convolution unit, consuming fewer parameters. Extensive experiments across multiple low-level tasks including image denoising, low-light image enhancement, guided image super-resolution, and image de-blurring demonstrate consistent performance gains obtained by our Deep Fourier Shifting while reducing the computation burden. Additionally, ablation studies verify the robustness of the shift displacement with stable performance improvement. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The spatial shift operator [3] is a technique that shifts channels from a pixel to its adjacent pixels. It is celebrated for its efficient facilitation of spatial information exchange. Due to its parameter-free nature and computational efficiency, this operator has found widespread application as a substitute for standard convolution units, particularly in high-level image classification tasks. A representative work, $S^{2}$ -MLP [1], integrates the spatial shift operator into a channel-mixing MLP framework. This approach serves as an alternative to token-mixing MLPs, effectively mitigating intrinsic overfitting issues and significantly enhancing recognition accuracy. ShiftVit [2] explores the role of the selfattention mechanism in Vision Transformers (ViTs) for high-level tasks. It proposes a replacement using a modified spatial shift operation [4] that facilitates the exchange of a subset of channels between neighboring features. Despite these impressive achievements, the utility of spatial shift operators in low-level image restoration tasks remains unexplored. ", "page_idx": 0}, {"type": "image", "img_path": "3gKsKFeuMA/tmp/b13ec855c7f27bc120e82d27bed3336a86ba40a6ff7e7bdc08093108d8ea4cf2.jpg", "img_caption": ["Figure 1: Comparison between the spatial shift operator and the proposed deep Fourier shift operator. (a) Traditional spatial shift operator involves a spatial shift mechanism that moves each channel of the input tensor in a distinct spatial direction, thus suffering from severe region-aware information loss and conflicting with the requirements of image restoration tasks. (b) Deep Fourier Shifting/Cycling operator is a more ingenious information-lossless operator, which is tailored for image restoration tasks. (c), (d) Deep Fourier shifting achieves a more stable performance gain than the spatial shifting mechanism with varying \u201cns\u201d shift displacements and \u201cn\u201d basic units over image de-noising task where the cut-off is for compressing the vertical axis scale to better illustrate the contrast effect clearly. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The main difficulty in introducing the spatial shift mechanism to low-level vision tasks lies in its inherent conflict with the objectives of image restoration tasks, as depicted in Figure 1. In particular, this mechanism operates by moving each channel of its input tensor in distinct spatial directions, leading to a mixing of spatial information across channels. Notably, image restoration is fundamentally a standard regression problem where both features and channels play critical roles in determining the final output. This contrasts with the behavior observed in high-level vision tasks. A key limitation of the spatial shift operator is its inherent loss of region-aware information, where the regions subject to shifting are filled with zero values. Consequently, image restoration tasks, which are particularly sensitive to this kind of information loss, can experience a decline in performance due to spatial shifting. ", "page_idx": 1}, {"type": "text", "text": "In response to these challenges, we introduce a novel deep shifting operator specifically crafted for low-level image restoration tasks, named Deep Fourier Shifting. This operator revisits and enhances the fundamental principles of the traditional shift operator by extending its application into the Fourier domain. Here, the shift operation is reformulated as an information-lossless process executed through Fourier cycling. Deep Fourier Shifting is composed of two distinct variants, each featuring three key elements: a 2D discrete Fourier transform, Fourier cycling rules, and a 2D inverse Fourier transform. This operator is designed to be generic, and seamlessly integrable into existing image restoration architectures as a replacement for standard convolution units, offering the added advantage of reduced parameter usage. To assess the effectiveness of Deep Fourier Shifting, we undertake comprehensive experiments across a spectrum of low-level image restoration tasks. These include image denoising, low-light image enhancement, guided image super-resolution, and image de-blurring. Our experiments consistently reveal performance improvements achieved through the integration of Deep Fourier Shifting, while alleviating computational burden. Furthermore, we conduct ablation studies to evaluate the robustness of Deep Fourier Shifting against varying shift displacements. These studies demonstrate its ability to consistently enhance performance, underscoring its stability and effectiveness. ", "page_idx": 1}, {"type": "text", "text": "Our results suggest that Deep Fourier Shifting is a promising tool for image restoration, showing potential for varied real-world applications. We hope that Deep Fourier Shifting could contribute to advancements in neural network designs for image restoration, particularly in improving spatial communication interactions. ", "page_idx": 1}, {"type": "image", "img_path": "3gKsKFeuMA/tmp/6de0af4e9e4a04afe5d8a7a06c7c249bb7f1b1473097c3fb070395bdcd4dceba.jpg", "img_caption": ["Figure 2: (a) The information-lossless cycling mechanism. The discrete Fourier transform of a signal exhibits period-extended and cycling properties. Specifically, in the Fourier domain, the two pixels in sequence beginning and end may not appear adjacent, but due to the period property, they are actually considered adjacent, as indicated by the upper right corner. This inherent period-extended and cycling behavior of the Fourier transform enables us to model the shifting mechanism in a manner that is information-lossless, making it well-suited for image restoration tasks. Consider the Fourier transform of a discrete time-domain signal, represented as $\\left(\\begin{array}{l}{0\\mathrm{~1~2~}}\\\\ {3\\mathrm{~4~5~}}\\end{array}\\right)$ . It may appear that the values 3 and 5 are not adjacent within the main period. However, owing to the property of period extension, the 3 from the previous period and the 5 from the current period are theoretically considered adjacent. It is reasonable to move the removed area from the end to the front, meeting the cycling mechanism. (b) Our deep Fourier shifting operator. Our operator borrows the principle of the spatial shifting mechanism and models the shifting mechanism in information-lossless Fourier cycling rules. The cycling is coded as 2D queue rolling. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Deep Fourier Shifting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Building on the principle of the shift operator, we extend its application to the Fourier domain. In this domain, the shift operator is conceptualized as an information-lossless operation, which we term Fourier cycling. To substantiate this approach, we present a theorem along with its corresponding proof. Additionally, we introduce two distinct variants of Deep Fourier shifting: i) the magnitude and phase variant, and ii) the real and imaginary variant, which are derived from the transformation rules we have identified within the Fourier domain. ", "page_idx": 2}, {"type": "text", "text": "Definitions. $f(x,y)\\in\\mathbb{R}^{\\mathrm{H}\\times\\mathrm{W}\\times\\mathrm{c}}$ is the spatial signal and $F(u,v)\\in\\mathbb{R}^{\\mathrm{H}\\times\\mathrm{W}\\times\\mathrm{c}}$ denotes its Fourier transform where $(x,y)$ and $(u,v)$ represent the space coordinates and Fourier spectrum, respectively. ", "page_idx": 2}, {"type": "text", "text": "Theorem. The Fourier transform of a discrete signal is a period-extended and cycling: $F(u,v)=$ $F(u+n\\mathrm{H},v)\\,=\\,F(u,v\\,+\\,\\stackrel{\\cdot}{m}\\mathrm{W})\\,\\stackrel{\\cdot}{=}\\,F(u+n\\tilde{\\mathrm{H}},v\\,+\\,\\stackrel{\\cdot}{m}\\mathrm{\\tilde{W}})$ where $u\\,=\\,0,1,2,\\dots,\\mathrm{H}-1$ , $v=$ $0,1,2,\\dots,\\mathrm{W}-1$ and $n,m\\in\\mathbb{N}$ . N is the set of positive integers starting from zero. ", "page_idx": 2}, {"type": "text", "text": "2.1 Proof: The Fourier Transform of a Discrete Signal is Period-Extended and Cycling ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We show the periodicity and cycling properties of the Fourier transform of a discrete signal, as illustrated in Figure 2(a). Note that the Fourier transform $F(u,v)$ of $f(x,y)$ is expressed as ", "page_idx": 2}, {"type": "equation", "text": "$$\nF(u,v)=\\frac{1}{\\mathrm{HW}}\\sum_{x=0}^{\\mathrm{H}-1}\\sum_{y=0}^{\\mathrm{W}-1}f(x,y)e^{-j2\\pi(\\frac{u x}{\\mathrm{H}}+\\frac{v y}{\\mathrm{W}})}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Then, we show the periodicity of $F(u,v)\\ \\in\\ \\mathbb{R}^{\\mathrm{H}\\times\\mathrm{W}}$ with $\\mathrm{H}$ and W. It means $F(u,v)\\ =$ $F(u\\,+\\,n\\mathrm{H},v)~=~\\bar{F(}u,v\\,+\\,\\dot{m}\\mathrm{W})~\\stackrel{\\cdot}{=}~F(u\\,+\\,n\\mathrm{H},v\\,+\\,m\\mathrm{W})$ where $u~=~0,1,2,\\dots,\\mathrm{H}-1$ , ", "page_idx": 2}, {"type": "text", "text": "$v\\;=\\;0,1,2,\\ldots,\\mathrm{W}-1$ and $n,m\\;\\in\\;\\mathbb{N}$ that records the set of non-negative integers. We take the $F(u,v)=F(u+n\\mathrm{H},v+m\\mathrm{W})$ for example and recall Eq. (1) as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F(u+n\\mathrm{H},v+m\\mathrm{W})}\\\\ &{=\\frac{1}{\\mathrm{HW}}\\displaystyle\\sum_{x=0}^{\\mathrm{H}-1}\\sum_{y=0}^{\\mathrm{W}-1}f(x,y).e^{-j2\\pi(\\frac{(u+n\\mathrm{H})x}{\\mathrm{H}}+\\frac{(v+m\\mathrm{W})y}{\\mathrm{W}})}}\\\\ &{=\\frac{1}{\\mathrm{HW}}\\displaystyle\\sum_{x=0}^{\\mathrm{H}-1}\\sum_{y=0}^{\\mathrm{W}-1}f(x,y)e^{-j2\\pi(\\frac{u x}{\\mathrm{H}}+\\frac{v y}{\\mathrm{W}})}e^{-2j\\pi m}e^{-2j\\pi n}}\\\\ &{=F(u,v)e^{-2j\\pi m}e^{-2j\\pi n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where for any integer $z$ , it has $e^{-2j\\pi z}=1$ . ", "page_idx": 3}, {"type": "text", "text": "Further, $e^{-2j\\pi n}=1$ and $e^{-2j\\pi m}=1$ for $n,m\\in\\mathbb{N}$ . Therefore, ", "page_idx": 3}, {"type": "equation", "text": "$$\nF(u+n\\mathrm{H},v+m\\mathrm{W})=F(u,v)e^{-2j\\pi m}e^{-2j\\pi n}=F(u,v).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Similarly, we can prove the periodicity of $F(u,v)$ as well. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F(u,v)=F(u+n\\mathrm{H},v+m\\mathrm{W})}\\\\ &{\\qquad\\qquad=F(u+n\\mathrm{H},v)=F(u,v+m\\mathrm{W}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Furthermore, deep Fourier transform can be expressed in Cartesian and polar coordinates by an equivalent form as ", "page_idx": 3}, {"type": "equation", "text": "$$\nF(u,v)=\\mathrm{A}e^{j\\mathrm{P}}=a+b j.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The period-extended and cycling property holds over the amplitude-phase and real-imaginary format. ", "page_idx": 3}, {"type": "text", "text": "2.2 Architectural Design ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Recall the Theorem-1, we propose two deep Fourier shifting variants: amplitude-phase variant and real-imaginary variant. ", "page_idx": 3}, {"type": "text", "text": "Amplitude-phase shifting variant. This variant is illustrated in Figure 2(b). The pseudo-code is shown in Figure 3 (left). Given an image $\\mathrm{X}\\in\\mathbb{R}^{\\mathrm{H}\\times\\mathrm{W}\\times\\mathrm{c}}$ , we first adopt the Fourier transform FFT(X) to obtain its amplitude component A and phase component P. We then evenly split the generated A and P in 4 folds $\\mathtt{A_{-}g}$ and $\\mathsf{P}_{-}\\mathsf{g}$ by the channel dimension, perform the Fourier cycling over $\\mathtt{A_{-}g}$ and $\\mathsf{P}_{-}\\mathsf{g}$ in both the H and $\\mathrm{W}$ dimensions: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\tt A\\mathrm{-}}\\mathrm{g}\\left[0\\right]\\,=\\,\\mathrm{torch}\\,.\\,\\mathrm{rol}1\\!\\left({\\tt A\\mathrm{-}}\\mathrm{g}\\left[0\\right],\\mathrm{dim}\\!\\left=\\!1,\\mathrm{\\bfs}\\right)}\\\\ &{{\\tt A\\mathrm{-}}\\mathrm{g}\\left[1\\right]\\,=\\,\\mathrm{torch}\\,.\\,\\mathrm{rol}1\\!\\left({\\tt A\\mathrm{-}}\\mathrm{g}\\left[1\\right],\\mathrm{dim}\\!\\left=\\!1,\\mathrm{\\tt-}\\mathrm{s}\\right)}\\\\ &{{\\tt A\\mathrm{-}}\\mathrm{g}\\left[2\\right]\\,=\\,\\mathrm{torch}\\,.\\,\\mathrm{rol}1\\!\\left({\\tt A\\mathrm{-}}\\mathrm{g}\\left[0\\right],\\mathrm{dim}\\!\\left=\\!2,\\mathrm{\\bfs}\\right)}\\\\ &{{\\tt A\\mathrm{-}}\\mathrm{g}\\left[3\\right]\\,=\\,\\mathrm{torch}\\,.\\,\\mathrm{rol}1\\!\\left({\\tt A\\mathrm{-}}\\mathrm{g}\\left[0\\right],\\mathrm{dim}\\!\\left=\\!2,\\mathrm{\\tt-}\\mathrm{s}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where torch.roll(.) accounts for the cycling function by dim parameter for shifting dimension and s for shifting displacement. The transformed $\\mathtt{A_{-}g}$ and $\\mathsf{P}_{-}\\mathsf{g}$ are then fed into two independent convolution modules with $1\\times1$ kernel and followed by the inverse Fourier transform iFFT(.) to project the shifting ones back to spatial domain. ", "page_idx": 3}, {"type": "text", "text": "Real-imaginary shifting variant. The pseudo-code for the real-imaginary shifting variant is presented on the right side of Figure 3. In this variant, we perform Fourier cycling separately on the real component a and the imaginary component b, while keeping the remaining processing steps the same as the amplitude-phase shifting variant. ", "page_idx": 3}, {"type": "text", "text": "Concerning two variants, the first one entails trigonometric function calculations, wherein minor numerical alterations could potentially result in computational instability in engineering applications. However, it offers more accurate physical interpretations over amplitude-phase operation from signal processing perspective. ", "page_idx": 3}, {"type": "image", "img_path": "3gKsKFeuMA/tmp/d11944bdf7d1043a928e837dce0c79017005e33c8264cc18ff53587e54c651fc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 3: Pseudo-code of the two variants of the proposed deep Fourier shifting. The left is the amplitude-phase variant while the right is the real-imaginary variant. ", "page_idx": 4}, {"type": "text", "text": "Table 1: Quantitative comparisons on low-light image enhancement. The arrow $\\rightarrow$ denotes the generalization setting by training on the data before the arrow and testing directly on the data after the arrow. ", "page_idx": 4}, {"type": "table", "img_path": "3gKsKFeuMA/tmp/32624422b8b257cddb675982ec97581f8fdce9ac5e78764190edcbe26779957c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "table", "img_path": "3gKsKFeuMA/tmp/1692a3dd13880f74ef393dfc07540bb1bf14b8673b4b14af68f084341c852321.jpg", "table_caption": ["Table 2: Comparisons on image denoising. Table 3: Comparisons on image deblurring. "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "3 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.1 Experimental Settings ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Image enhancement. We evaluate our Fourier shifting operator on two popular image enhancement benchmarks: LOL [5] and Huawei [6]. The LOL dataset consists of 500 low-/normal-light image pairs. Following the original setting, we use 485 pairs for training and 15 pairs for testing. The Huawei dataset contains 2480 paired images, with 2200 pairs for training and 280 pairs for testing. We compare with the representative approaches SID [7] and DRBN [8]. ", "page_idx": 4}, {"type": "text", "text": "Image deblurring and denoising. For the image deblurring task, we employ DeepDeblur [9] in our experiments. We use the GoPro dataset [9] for training. To demonstrate the generalizability of our operator, we also apply the model trained on the GoPro dataset directly to the test images of the HIDE dataset [10]. For image denoising, we use the SIDD dataset [11] as the training benchmark. ", "page_idx": 4}, {"type": "table", "img_path": "3gKsKFeuMA/tmp/87104303f003b8fc18c0ba7c2444c80227e2442acc9a0918f83c5db3751dac64.jpg", "table_caption": ["Table 4: Quantitative comparisons on guided image super-resolution. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Performance evaluation is conducted on the remaining validation samples from the SIDD dataset and the DND benchmark dataset [12]. The selected baseline for comparison is DnCNN 1 [13]. ", "page_idx": 5}, {"type": "text", "text": "Guided image super-resolution. We adopt the pan-sharpening task as a representative task of guided image super-resolution. The WorldView II and GaoFen2 datasets [14] are used in our experiments. The baselines are state-of-the-art INNformer [15] and SFINet [14]. ", "page_idx": 5}, {"type": "text", "text": "In evaluating the performance of different approaches, we use image quality assessment metrics such as the relative dimensionless global error in synthesis (ERGAS) [16], the peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), the spectral angle mapper (SAM) [17]. ", "page_idx": 5}, {"type": "text", "text": "3.2 Implementation Details ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Based on the above competitive baselines, we perform the comparison over the following configurations by replacing the standard convolution with the spatial or Fourier shifting operator: ", "page_idx": 5}, {"type": "text", "text": "1) Original: the baseline without any changes;   \n2) Fcycle-AP: replacing the original model\u2019s standard convolution operator with the amplitudephase variant of Deep Fourier shifting;   \n3) Fcycle-ab: replacing the original model\u2019s standard convolution operator with the realimaginary variant of Deep Fourier shifting;   \n4) Shift-sa: replacing the variants of Deep Fourier shifting in the settings of 2)/3) with the spatial shifting operator;   \n5) Shift-ns: replacing \u201cns\u201d standard convolution operator with the spatial shifting or Fourier shifting-ab/AP operator;   \n6) Shift-n: replacing the spatial shifting or Fourier shifting-ab/AP operator in the settings of 5) with varying shifting displacement \u201cn\u201d. ", "page_idx": 5}, {"type": "text", "text": "3.3 Comparison and Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Quantitative comparison. We evaluate model performance across various configurations as detailed in the implementation section (Section 3.2). The quantitative results of this analysis are systematically presented in Tables 1, 2, 3, and 4. In these tables, the best and second-best results are highlighted in bold and underlined, respectively. Values that fall below the established baseline are marked with a gray background for clear distinction. We observe a uniform trend of performance enhancement across all the tasks and datasets tested when our two novel deep Fourier shifting variants are integrated. This improvement is particularly noteworthy as it is achieved with reduced computational costs, suggesting the efficiency and effectiveness of our approach. For instance, in the low-light image enhancement baseline SID (Table 1), our \u201cFcycle-AP\u201d and \u201cFcycle-ab\u201d models surpass the \u201cOriginal\u201d model by achieving higher PSNR values of 2.75dB/2.53dB and $2.6\\mathrm{dB}/2.7\\mathrm{dB}$ on the LOL and Huawei datasets, respectively. In contrast, the integration of the spatial shifting operator into the SID baseline results in a slight reduction in PSNR by $0.1\\mathrm{dB}/0.02\\mathrm{dB}$ on the LOL and Huawei datasets, as evident in the 3rd and 7th rows. These findings validate the superiority of our proposed Fourier shifting approach, demonstrating its suitability and effectiveness in addressing the unique challenges of low-level image restoration tasks. Additionally, the integration of our proposed methods has shown to enhance the training performance, in Figure 4. ", "page_idx": 5}, {"type": "image", "img_path": "3gKsKFeuMA/tmp/767a814312e892ee6bf7efdb11688806e883133fbeb06198cb95a05323526175.jpg", "img_caption": ["Figure 5: Visual comparison over image enhancement. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "3gKsKFeuMA/tmp/2634e585379307b1aeb2d3c58675060a357a6c4518770cad35c5e8472e96e1f8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 6: The effectiveness of information preservation. Left: we compare mutual information levels before and after employing Fcycle-ab and Shift-sa operators on the LOL test set, respectively. Our operator exhibits significantly higher mutual information than Shift-sa, showcasing its efficacy in information preservation. Right: we visualize feature maps and their amplitude components before and after operations. This demonstrates that our Fcycle-AP promotes frequency information ial domain. ", "page_idx": 6}, {"type": "image", "img_path": "3gKsKFeuMA/tmp/a7471a2c49746bb4dc5d11de80541b555d060dee03a2d4a4d369cefef2e505b3.jpg", "img_caption": ["Figure 4: The proposed operators improve the training performance. It shows the training PSNR on the image enhancement task on the LOL and Huawei datasets in the top and bottom. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Qualitative comparison. Due to space constraints, we present only a subset of the visual results in Figure 5, which effectively illustrate the efficacy of our proposed deep Fourier shifting operator. Additional visual results are available in the supplementary materials. As demonstrated in these figures, the integration of the deep Fourier shifting operators (Fcycle-ab and Fcycle-AP) with the original baseline models yields results that are visually more appealing compared to the baselines and the spatial shift operator (Shift-sa). Models enhanced with our operators exhibit a superior ability to restore fine texture details and mitigate degradation effects. In contrast, models using the spatial shift operator tend to produce significant artifacts. ", "page_idx": 6}, {"type": "text", "text": "on the LOL test set. The statistical analysis, illu ", "page_idx": 6}, {"type": "text", "text": "Information preservation. To offer a deeper understanding of the efficacy of our frequency cycling mechanism, we begin by comparing the mutual information of features before and after the application of our proposed Fcycle-AP operator strated in Figure 6 (left), demonstrates a significantly higher level of mutual information as outlined in [18] for our method when compared to the conventional shifting operation in the spatial domain. This notable increase in mutual information underscores the strength of our Fourier cycling mechanism in preserving information and minimizing information loss. ", "page_idx": 6}, {"type": "table", "img_path": "3gKsKFeuMA/tmp/eb4350b5bae239b5fa7cbde74846df18bfe5b9c1434483beae73546a29d24113.jpg", "table_caption": ["Table 5: Ablation studies of image denoising network, DNCNN. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Moreover, an analysis of the features and their corresponding amplitude components, as depicted in Figure 6 (right), reveals distinct grid effects in the feature maps generated by the Shift-sa operator. In contrast, our method substantially mitigates these grid effects. This is achieved by facilitating interactions among frequency information and improving spatial communication. Together, these observations compellingly show the efficiency of our proposed Fourier cycling mechanism in both preserving information and enhancing frequency interaction. ", "page_idx": 7}, {"type": "image", "img_path": "3gKsKFeuMA/tmp/bb1a727a1b190696179cd648341b3a96e176e71deac3ed5119a4022588cab64b.jpg", "img_caption": ["3.4 Ablation Studies ", "Figure 7: The effect of shifting displacement shift-n on SID. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "With varying \u201cn\u201d. To assess the robustness of our method, we conduct a comparative analysis with the image enhancement baseline SID and the image denoising baseline DNCNN. This comparison involved varying the shifting displacement \u201cn\u201d. The corresponding quantitative results are presented in Figure 7 and Table 5. The results clearly show that integrating our proposed Fourier shifting solutions into these baseline models consistently yields better performance than the original baselines alone. Conversely, when spatial shifting is employed within these baselines, there is a marked decline in performance compared to their original versions, particularly at larger shifting displacements (e.g., $\\scriptstyle{\\mathrm{\"}\\mathrm{n}=4}\\,^{\\circ}$ ). These findings not only validate the efficacy of our approach but also highlight its distinct robustness over traditional spatial shifting. ", "page_idx": 7}, {"type": "text", "text": "With varying \u201cns\u201d. We examine the robustness of our deep Fourier shifting variants by comparing them with the image de-noising baseline DNCNN. For this comparison, we substitute the variable \u201cns\u201d standard convolution units in the baseline with both the spatial shifting operator or our proposed deep Fourier shifting variants, as detailed in Section 3.2. The results of this ", "page_idx": 7}, {"type": "text", "text": "evaluation are provided in Table 5. We highlighted the best and second-best results in bold and underline, respectively. A visual depiction of these results is provided in Figure 1 (c). A key observation from these results is that as \u201cns\u201d increases, models incorporating our proposed operators not only exhibit consistent performance improvements but also achieve these enhancements with fewer model parameters. In contrast, models using the spatial shifting operator display a notable decrease in performance, particularly when \u201cns\u201d is increased to 18. This trend demonstrates the robustness and efficiency of our proposed Fourier shifting solutions. To further elucidate this point, values falling below the baseline are marked with a gray background, highlighting their relevance and impact. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Spatial-shifting operator. The spatial-shift operator, initially proposed by Wu et al.[3], enhances efficient spatial information communication. It has gained traction in high-level image classification tasks [1], where they innovated upon MLPMixer [19]. They replaced the spatial-wise token mixer with a spatial-shift operation, enabling inter-patch communication and effectively addressing overfitting challenges associated with spatial-specific token-mixing MLPs. Similarly, Wang et al. [2] explored the role of attention mechanisms in Vision Transformers (ViTs). They introduced a partial shift operation, exchanging a subset of channels among adjacent features. This simple yet effective operation led to the development of ShiftViT, a backbone network that replaces traditional attention layers in ViTs with shift operations. These approaches illustrate the adaptability and versatility of spatial-shift operators in enhancing deep learning models for image analysis. ", "page_idx": 8}, {"type": "text", "text": "Deep Fourier transform over image restoration. There has been a growing interest in integrating the Fourier transform to refine deep learning-based image restoration models. A notable example is the work of Zhou et al. [20], which re-examines the interplay between spatial and Fourier domains. This study uncovers transformation rules applicable to various resolution features within the Fourier domain, leading to the development of a theoretically sound Deep Fourier Up-Sampling method applicable across multiple restoration tasks. Similarly, the research in Zhou et al. [14] investigates the degradation dynamics of guided image super-resolution. The authors propose a novel spatial-frequency dual-domain integration network tailored to this specific task. The idea leads to a deep Fourier-based exposure correction network designed to tackle exposure-related issues. These pioneering studies harness the synergistic potential of deep learning and the intrinsic attributes of the Fourier transform. ", "page_idx": 8}, {"type": "text", "text": "5 Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our study acknowledges that there is room for more comprehensive experiments and exploration of representative baselines on broader computer vision tasks. It is important to note that this work represents the initial endeavor to delve into the utilization of shifting mechanisms and the development of tailored deep Fourier shifting operators specifically for low-level image restoration tasks. Moreover, our focus extends beyond designing a plug-and-play module for integration into existing networks to achieve performance improvements. We aim to provide a powerful and efficient spatial communication interaction choice by offering an alternative to the basic convolution operator pool when constructing new models from scratch. ", "page_idx": 8}, {"type": "text", "text": "Broader Impact ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our research has the potential to facilitate efficient image restoration on edge devices and resourcelimited platforms, democratizing access to high-quality capabilities. This benefits critical domains like mobile photography, remote sensing, and medical imaging. Additionally, reduced computational requirements contribute to energy efficiency, extending battery life and reducing environmental impact. We emphasize responsible development, evaluation, and deployment to ensure equitable and ethical use of our methods. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We have presented Deep Fourier Shifting, a shifting operator grounded in solid theoretical principles, specifically tailored for image restoration tasks. This operator, characterized by its informationlossless nature, leverages the Fourier cycling approach to shift operations. A key feature of our Deep Fourier Shifting is its versatility; it can be seamlessly integrated as a replacement for standard convolution units in existing image restoration networks, offering the added benefit of reduced parameter usage. Our experimental evaluations have consistently demonstrated the enhancements in performance attributable to the incorporation of our Deep Fourier Shifting. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Tan Yu, Xu Li, Yunfeng Cai, Mingming Sun, and Ping Li. S2-mlp: Spatial-shift mlp architecture for vision. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 297\u2013306, January 2022.   \n[2] Guangting Wang, Yucheng Zhao, Chuanxin Tang, Chong Luo, and Wenjun Zeng. When shift operation meets vision transformer: An extremely simple alternative to attention mechanism. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 2423\u20132430, 2022.   \n[3] Bichen Wu, Alvin Wan, Xiangyu Yue, Peter Jin, Sicheng Zhao, Noah Golmant, Amir Gholaminejad, Joseph Gonzalez, and Kurt Keutzer. Shift: A zero flop, zero parameter alternative to spatial convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 9127\u20139135, 2018.   \n[4] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efficient video understanding. In Proceedings of the IEEE/CVF international conference on computer vision, pages 7083\u20137093, 2019.   \n[5] Wenhan Yang Jiaying Liu Chen Wei, Wenjing Wang. Deep retinex decomposition for low-light enhancement. In British Machine Vision Conference. British Machine Vision Association, 2018.   \n[6] Jiang Hai, Zhu Xuan, Ren Yang, Yutong Hao, Fengzhu Zou, Fang Lin, and Songchen Han. R2rnet: Low-light image enhancement via real-low to real-normal network. arXiv preprint arXiv:2106.14501, 2021.   \n[7] Chen Chen, Qifeng Chen, Jia Xu, and Vladlen Koltun. Learning to see in the dark. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3291\u2013 3300, 2018.   \n[8] Wenhan Yang, Shiqi Wang, Yuming Fang, Yue Wang, and Jiaying Liu. From fidelity to perceptual quality: A semi-supervised approach for low-light image enhancement. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3063\u20133072, 2020.   \n[9] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep multi-scale convolutional neural network for dynamic scene deblurring. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3883\u20133891, 2017.   \n[10] Ziyi Shen, Wenguan Wang, Xiankai Lu, Jianbing Shen, Haibin Ling, Tingfa Xu, and Ling Shao. Human-aware motion deblurring. In IEEE International Conference on Computer Vision, 2019.   \n[11] Abdelrahman Abdelhamed, Stephen Lin, and Michael S Brown. A high-quality denoising dataset for smartphone cameras. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1692\u20131700, 2018.   \n[12] Tobias Plotz and Stefan Roth. Benchmarking denoising algorithms with real photographs. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1586\u20131595, 2017.   \n[13] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising. IEEE transactions on image processing, 26(7):3142\u20133155, 2017.   \n[14] Zhou Man, Huang Jie, Yan Keyu, Yu Hu, Fu Xueyang, Liu Aiping, Wei Xian, and Zhao Feng. Spatial frequency domain information integration for pan-sharpening. In Proceedings of the European Conference on Computer Vision (ECCV), pages 234\u2013250, 2022.   \n[15] Man Zhou, Xueyang Fu, Jie Huang, Feng Zhao, Aiping Liu, and Rujing Wang. Effective pansharpening with transformer and invertible neural network. IEEE Transactions on Geoscience and Remote Sensing, 60:1\u201315, 2022.   \n[16] L. Alparone, L. Wald, J. Chanussot, C. Thomas, P. Gamba, and L. M. Bruce. Comparison of pansharpening algorithms: Outcome of the 2006 grs-s data fusion contest. IEEE Transactions on Geoscience and Remote Sensing, 45(10):3012\u20133021, 2007.   \n[17] A. F. Goetz J. R. H. Yuhas and J. M. Boardman. Discrimination among semi-arid landscape endmembers using the spectral angle mapper (sam) algorithm. Proc. Summaries Annu. JPL Airborne Geosci. Workshop, pages 147\u2013149, 1992.   \n[18] Tyrone E. Duncan. On the calculation of mutual information. SIAM Journal on Applied Mathematics, 19(1):215\u2013220, 1970.   \n[19] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlpmixer: An all-mlp architecture for vision. Advances in neural information processing systems, 34:24261\u201324272, 2021.   \n[20] Hu Yu, Jie Huang, Feng Zhao, Jinwei Gu, Chen Change Loy, Deyu Meng, Chongyi Li, et al. Deep fourier up-sampling. Advances in Neural Information Processing Systems, 35:22995\u2013 23008, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 11}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 11}, {"type": "text", "text": "Justification: Abstract ", "page_idx": 11}, {"type": "text", "text": "Guidelines: ", "page_idx": 11}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 11}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 11}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Justification: Section 4. ", "page_idx": 11}, {"type": "text", "text": "Guidelines: ", "page_idx": 11}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. ", "page_idx": 11}, {"type": "text", "text": "\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 11}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 11}, {"type": "text", "text": "Answer: [Yes] Justification: Section 2. Guidelines: ", "page_idx": 11}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 12}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 12}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Justification: Section 3. Guidelines: ", "page_idx": 12}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 12}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 12}, {"type": "text", "text": "Answer: [No] Justification: Codes will be released after acceptance. ", "page_idx": 12}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 13}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: The detailed descriptions are provided in Section 4. Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 13}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Justification: Section 3. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified. ", "page_idx": 13}, {"type": "text", "text": "\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). \u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 14}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: Section 3. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 14}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: We have read and followed the NeurIPS Code of Ethics ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 14}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Justification: Broader Impacts. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 14}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 15}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 15}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 15}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 16}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 16}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 16}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 16}]