[{"type": "text", "text": "Newton Informed Neural Operator for Solving Nonlinear Partials Differential Equations ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Wenrui Hao\u2217 Department of Mathematics The Pennsylvania State University, University Park, State College, PA, USA wxh64@psu.edu ", "page_idx": 0}, {"type": "text", "text": "Xinliang Liu School of Mathematical Sciences, Shenzhen University, Shenzhen, China King Abdullah University of Science and Technology, Thuwal, Saudi Arabia xinliang.liu@kaust.edu.sa ", "page_idx": 0}, {"type": "text", "text": "Yahong Yang Department of Mathematics The Pennsylvania State University, University Park, State College, PA, USA yxy5498@psu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Solving nonlinear partial differential equations (PDEs) with multiple solutions is essential in various fields, including physics, biology, and engineering. However, traditional numerical methods, such as finite element and finite difference methods, often face challenges when dealing with nonlinear solvers, particularly in the presence of multiple solutions. These methods can become computationally expensive, especially when relying on solvers like Newton\u2019s method, which may struggle with ill-posedness near bifurcation points. In this paper, we propose a novel approach, the Newton Informed Neural Operator, which learns the Newton solver for nonlinear PDEs. Our method integrates traditional numerical techniques with the Newton nonlinear solver, efficiently learning the nonlinear mapping at each iteration. This approach allows us to compute multiple solutions in a single learning process while requiring fewer supervised data points than existing neural network methods. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Neural networks have been extensively applied to solve partial differential equations (PDEs) in various fields, including biology, physics, and materials science [21, 9]. While much of the existing work focuses on PDEs with a unique solution, nonlinear PDEs with multiple solutions pose a significant challenge [39, 10] but are widely encountered in applications such as [1, 3, 31, 36, 14, 13]. In this paper, we aim to solve the following nonlinear PDEs with multiple solutions: ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\mathcal{L}u(\\pmb{x})=f(u),}&{\\pmb{x}\\in\\Omega,}\\\\ {u(\\pmb{x})=0,}&{\\pmb{x}\\in\\partial\\Omega,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Here, $\\Omega$ is the domain of equation, $f(u)$ is a nonlinear function in $\\mathbb{R}$ , $u~:~\\mathbb{R}^{d}~\\rightarrow~\\mathbb{R}$ and $\\mathcal{L}$ is a second-order elliptic operator given by $\\begin{array}{r}{\\mathcal{L}\\boldsymbol{u}\\,=\\,-\\sum_{i,j=1}^{d}a^{i j}(\\pmb{x})u_{x_{i}x_{j}}\\,+\\sum_{i=1}^{d}b^{i}(\\pmb{x})u_{x_{i}}\\,+}\\end{array}$ $c(\\pmb{x})u$ , for given coefficient functions $a^{i j},b^{i},c(i,j~~=~~\\bar{1,}~\\cdot~.~,d)$ with $\\textstyle\\sum_{i,j=1}^{n}a^{i j}(\\pmb{x})\\xi_{i}\\xi_{j}\\quad\\geq$ $\\lambda|\\pmb{\\xi}|^{2}$ , for a constant $\\lambda\\geq0$ . ", "page_idx": 1}, {"type": "text", "text": "Various neural network methods have been developed to tackle partial differential equations (PDEs), including PINN [33], the Deep Ritz method [45], DeepONet [29], FNO [25], MgNO [16], and OL-DeepONet [26]. These methods can be broadly categorized into two types: function learning and operator learning approaches. In function learning, the goal is to directly learn the solution. However, these methods often encounter the limitation of only being able to learn one solution in each learning process. Furthermore, the problem becomes ill-posed when there are multiple solutions. On the other hand, operator learning aims to approximate the map between parameter functions in PDEs and the unique solution. This approach cannot address the issue of multiple solutions or find them in a single training session. We will discuss this in more detail in the next section. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we present a novel neural network approach for solving nonlinear PDEs with multiple solutions. Our method is grounded in operator learning, allowing us to capture multiple solutions within a single training process, thus overcoming the limitations of function learning methods in neural networks. Moreover, we enhance our network architecture by incorporating traditional Newton methods [35, 1], as discussed in the next section. This integration ensures that the problem of operator learning becomes well-defined, as Newton\u2019s methods provide well-defined locally, thereby ensuring a robust operator. This approach addresses the challenges associated with directly applying operator networks to such problems. Additionally, we leverage Newton information during training, enabling our method to perform effectively even with limited supervised data points. We introduce our network as the Newton Informed Neural Operator. To clarify, we do not design a specific neural structure for the neural operator. The Newton information is not incorporated into the architecture of the neural network but rather into the training process. Specifically, the Newton method is incorporated into the loss function, as detailed in Section 3.3. ", "page_idx": 1}, {"type": "text", "text": "As mentioned earlier, our approach combines the classical Newton method, which translates nonlinear PDEs into an iterative process involving solving linear functions at each iteration. One key advantage of our method is that, once the operator is effectively learned, there is no need to solve the linear equation at every iteration. This significantly reduces computation time, especially in complex systems encountered in fields such as material science, biology, and chemistry. Furthermore, once the Newton Informed Neural Operator is well-trained, it can be applied to compute new solutions with appropriate initial guesses, even those not present in the training data. Details of this capability are demonstrated in the numerical example of the Gray-Scott model. Overall, the Newton Informed Neural Operator efficiently solves nonlinear PDEs with multiple solutions by learning the Newton nonlinear solver. It addresses the time-consuming nature of traditional nonlinear solvers and requires fewer supervised data points compared to existing neural network methods. Additionally, it saves time by eliminating the need for repeatedly solving nonlinear systems, as is required in traditional Newton methods. Once the neural operator is learned, it can also compute new solutions beyond those provided in the supervised data. ", "page_idx": 1}, {"type": "text", "text": "The following paper is organized as follows: In the next section (Section 2), we will delve into nonlinear PDEs with multiple solutions and discuss related works on solving PDEs using neural network methods. In Section 3, we will review the classical Newton method for solving PDEs and introduce the Newton Informed Neural Operator, which combines neural operators with the Newton method to address nonlinear PDEs with multiple solutions. In this section, we will also analyze the approximation and generalization errors of the Newton Informed Neural Operator. Finally, in Section 4, we present the numerical results of our neural networks for solving nonlinear PDEs. The first example demonstrates that the Newton Informed Neural Operator requires minimal data for training, the second example shows that the speed of the Newton Informed Neural Operator is significantly faster than the traditional Newton method, and the last example highlights that the Newton Informed Neural Operator can discover new solutions not present in the supervised data. ", "page_idx": 1}, {"type": "text", "text": "2 Backgrounds and Relative Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Nonlinear PDEs with multiple solutions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Significant mathematical models depicting natural phenomena in biology, physics, and materials science are rooted in nonlinear partial differential equations (PDEs) [5]. These models, characterized by their inherent nonlinearity, present complex multi-solution challenges. Illustrative examples include string theory in physics, reaction-diffusion systems in chemistry, and pattern formation in biology [20, 12]. However, experimental techniques like synchrotronic and laser methods can only observe a subset of these multiple solutions. Thus, there is an urgent need to develop computational methods to unravel these nonlinear models, offering deeper insights into the underlying physics and biology [17]. Consequently, efficient numerical techniques for identifying these solutions are pivotal in understanding these intricate systems. Despite recent advancements in numerical methods for solving nonlinear PDEs, significant computational challenges persist for large-scale systems. Specifically, the computational costs of employing Newton and Newton-like approaches are often prohibitive for the large-scale systems encountered in real-world applications. In response to these challenges [15, 19], we propose an operator learning approach based on Newton\u2019s method to efficiently solve nonlinear PDEs. ", "page_idx": 2}, {"type": "text", "text": "2.2 Related works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Indeed, there are numerous approaches to solving partial differential equations (PDEs) using neural networks. Broadly speaking, these methods can be categorized into two main types: function learning and operator learning. ", "page_idx": 2}, {"type": "text", "text": "In function learning, neural networks are used to directly approximate the solutions to PDEs. Function learning approaches aim to directly learn the solution function itself. On the other hand, in operator learning, the focus is on learning the operator that maps input parameters to the solution of the PDE. Instead of directly approximating the solution function, the neural network learns the underlying operator that governs the behavior of the system. ", "page_idx": 2}, {"type": "text", "text": "Function learning methods In function learning, a commonly employed method for addressing this problem involves the use of Physics-Informed Neural Network (PINN)-based learning approaches, as introduced by Raissi et al. [33], and Deep Ritz Methods [45]. However, in these methods, the task becomes particularly challenging due to the ill-posed nature of the problem arising from multiple solutions. Despite employing various initial data and training methods, attaining high accuracy in solution learning remains a complex endeavor. Even when a high-accuracy solution is achieved, each learning process typically results in the discovery of only one solution. The specific solution learned by the neural network is heavily influenced by the initial conditions and training methods employed. However, discerning the relationships between these factors and the learned solution remains a daunting task. In [19], the authors introduce HomPINNs for learning multiple solutions to PDEs, where the number of solutions that can be learned depends on the choice of \u201cstart functions.\" However, if the \u201cstart functions\" are not appropriately selected, HomPINNs may fail to capture all solutions. In this paper, we present an operator learning approach combined with Newton\u2019s method to train the nonlinear solver. While this approach is not specifically designed for computing multiple solutions, it can be employed to compute them if suitable initial guesses are provided. ", "page_idx": 2}, {"type": "text", "text": "Operator learning methods Various approaches have been developed for operator learning to solve PDEs, including DeepONet [29], which integrates physical information [7, 26], as well as techniques like FNO [25] inspired by spectral methods, and MgNO [16], HANO [27], and WNO [24] based on multilevel methods, and transformer-based neural operators [4, 8]. These methods focus on approximating the operator between the parameters and the solutions. Firstly, they require the solutions of PDEs to be unique; otherwise, the operator is not well-defined. Secondly, they focus on the relationship between the parameter functions and the solution, rather than the initial data and multiple solutions. ", "page_idx": 2}, {"type": "text", "text": "3 Newton Informed Neural Operator ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Review of Newton Methods to Solve Nonlinear Partial Differential Equations ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To tackle this problem Eq. (1), we employ Newton\u2019s method by linearizing the equation. ", "page_idx": 3}, {"type": "text", "text": "For the Newton method applied to an operator, if we aim to find the solution of $\\mathcal{F}(u)\\,=\\,0$ , the iteration can be written as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{F}^{\\prime}\\left(u_{n}\\right)u_{n+1}=\\mathcal{F}^{\\prime}\\left(u_{n}\\right)u_{n}-\\mathcal{F}\\left(u_{n}\\right)\\Longleftrightarrow\\mathcal{F}^{\\prime}\\left(u_{n}\\right)\\delta u=-\\mathcal{F}\\left(u_{n}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where \u03b4u = un+1 \u2212un. ", "page_idx": 3}, {"type": "text", "text": "In this context, $\\mathcal{F}^{\\prime}(u)v$ is the (Fr\u00e9chet) derivative of the operator, which is a linear operator to $v$ , defined as follows: To find $\\mathcal{F}^{\\prime}(u)$ in $\\mathcal{X}$ , for any $v\\in\\mathcal{X}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{|v|\\to0}{\\frac{|{\\mathcal{F}}(u+v)-{\\mathcal{F}}(u)-{\\mathcal{F}}^{\\prime}(u)v|}{|v|}}=0,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\big|\\cdot\\big|$ denotes the norm in $\\mathcal{X}$ . ", "page_idx": 3}, {"type": "text", "text": "For solving Eq. (1), given any initial guess $u_{0}({\\pmb x})$ , for $i\\,=\\,1,2,\\cdot\\cdot\\cdot,M$ , in the $i$ -th iteration of Newton\u2019s method, we have $\\tilde{u}(\\pmb{x})=u+\\delta u(\\pmb{x})$ by solving ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{(\\mathcal{L}-f^{\\prime}(u))\\delta u(\\pmb{x})=-\\mathcal{L}u+f(u),}&{\\pmb{x}\\in\\Omega}\\\\ {\\delta u(\\pmb{x})=0,}&{\\pmb{x}\\in\\partial\\Omega,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which is based on the fact that the (Fr\u00e9chet) derivative of ${\\mathcal{L}}-f(\\cdot)$ at $u$ is $\\mathcal{L}-f^{\\prime}(u)$ . If Eq. (2) has a unique solution, then by solving Eq. (2) and repeating the process $M$ times, we will obtain one of the solutions of the nonlinear equation (1). Denoting the mapping for $u$ and $\\delta u$ , the solution of Eq. (2) with parameter $u$ , as $\\boldsymbol{\\mathcal{G}}(\\boldsymbol{u}):=\\delta\\boldsymbol{u}$ , we know that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\rightarrow\\infty}({\\mathcal{G}}+\\mathrm{Id})^{(n)}(u_{0})=u^{*},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $u^{*}$ is one of the solutions of Eq. (1). For different initial conditions, this process will converge to different solutions of Eq. (1), making this method suitable for finding multiple solutions. Furthermore, the Newton method is well-posed, meaning that each initial condition $u_{0}$ will converge to a single solution of Eq. (1) under appropriate assumptions (see Assumption 1). This approach helps to address the ill-posedness encountered when using PINNs directly to solve Eq. (1). However, repeatedly solving Eq. (1) can be computationally expensive, especially in high-dimensional cases or when a large number of discrete points are involved. In this paper, we tackle these challenges by employing neural networks. ", "page_idx": 3}, {"type": "text", "text": "3.2 Neural Operator Structures ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we introduce the structure of the neural operator to approximate the operator locally in the Newton methods from Eq.(2), i.e., $\\delta u:=\\mathcal{G}(u)$ , where $\\delta u$ is the solution of Eq.(2), which depends on $u$ . If we can learn the operator $\\mathcal G(u)$ well using the neural operator $\\mathcal{O}(u;\\pmb{\\theta})$ , then for an initial function $u_{0}$ , assume the $n$ -th iteration will approximate one solution, i.e., $({\\mathcal{G}}+\\mathbf{Id})^{(n)}(u_{0})\\approx u^{*}$ Thus, ", "page_idx": 3}, {"type": "equation", "text": "$$\n(\\mathcal{O}+\\mathrm{Id})^{(n)}(u_{0})\\approx(\\mathcal{G}+\\mathrm{Id})^{(n)}(u_{0})\\approx u^{*}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For another initial condition, we can evaluate our neural operator and find the solution directly. ", "page_idx": 3}, {"type": "text", "text": "Then we discuss how to train such an operator. To begin, we define the following shallow neural operators with $p$ neurons for operators from $\\mathcal{X}$ to $\\boldsymbol{\\wp}$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{O}(a;\\pmb{\\theta})=\\sum_{i=1}^{p}\\mathcal{A}_{i}\\sigma\\left(\\mathcal{W}_{i}a+\\mathcal{B}_{i}\\right)\\quad\\forall a\\in\\mathcal{X}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{W}_{i}\\in\\mathcal{L}(\\mathcal{X},\\mathcal{Y}),B_{i}\\in\\mathcal{Y}$ , $\\mathcal{A}_{i}\\in\\mathcal{L}(\\mathcal{V},\\mathcal{V})$ , and $\\pmb{\\theta}$ denote all the parameters in $\\{\\mathcal{W}_{i},\\mathcal{A}_{i},\\mathcal{B}_{i}\\}_{i=1}^{p}$ . Here, $\\mathcal{L}(\\mathcal{X},\\mathcal{Y})$ denotes the set of all bounded (continuous) linear operators between $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ , and $\\sigma:\\mapsto\\mathbb{R}$ defines the nonlinear point-wise activation. ", "page_idx": 3}, {"type": "text", "text": "In this paper, we will use shallow DeepONet [29, 22] to approximate the Newton operator. To provide a more precise description, in the shallow neural network, $\\mathcal{W}_{i}$ represents an interpolation of operators. With proper and reasonable assumptions, we can present the following theorem to ensure that DeepONet can effectively approximate the Newton method operator. The proof will be provided in the appendix. Furthermore, $\\mathrm{MgNO}$ is replaced by $\\mathcal{W}$ as a multigrid operator [38], and FNO is some kind of kernel operator; our analysis can be generalized to such cases. ", "page_idx": 4}, {"type": "text", "text": "Before the proof, we need to establish some assumptions regarding the input space $\\mathscr{X}\\subset H^{2}(\\Omega)$ of the operator and $f(u)$ in Eq. (1). The definition of the Sobolev space can be found in Appendix B.1. Assumption 1. (i): For any $u\\in\\mathcal X$ , we have that the linear equation ", "page_idx": 4}, {"type": "equation", "text": "$$\n(\\mathcal{L}-f^{\\prime}(u))\\delta u=-\\mathcal{L}+f(u)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "is well-posed for solving \u03b4u. ", "page_idx": 4}, {"type": "text", "text": "(ii): There exists a constant $F$ such that $\\|f(x)\\|_{W^{2,\\infty}(\\mathbb{R})}\\leq F$ . ", "page_idx": 4}, {"type": "text", "text": "(iii): All coefficients in $\\mathcal{L}$ are $C^{1}$ and $\\partial\\Omega\\in C^{2}$ . ", "page_idx": 4}, {"type": "text", "text": "(iv): $\\mathcal{X}$ has a Schauder basis $\\{b_{k}\\}_{k=1}^{\\infty}$ , we define the canonical projection operator ${\\mathcal{P}}_{n}$ based on this basis. The operator ${\\mathcal{P}}_{n}$ projects any element $u\\,\\in\\,{\\mathcal{X}}$ onto the finite-dimensional subspace spanned by the first n basis elements $\\{b_{1},b_{2},\\ldots,b_{n}\\}$ . Specifically, for $u\\in\\mathcal{X}$ , $\\begin{array}{r}{u=\\sum_{k=0}^{\\infty}\\dot{\\alpha}_{k}b_{k}}\\end{array}$ , let $\\begin{array}{r}{\\mathcal{P}_{n}(u)=\\sum_{k=0}^{n}\\alpha_{k}b_{k}}\\end{array}$ , where $\\alpha_{k}$ are the coefficients in the expansion of u with respect to the basis $\\{b_{n}\\}$ . T he canonical projection operator ${\\mathcal{P}}_{n}$ is a linear bounded operator on $\\mathcal{X}$ . According to the properties of Schauder bases, these projections ${\\mathcal{P}}_{n}$ are uniformly bounded by some constant $C$ . Furthermore, we assume, for any $u\\in\\mathscr{X}$ , $\\epsilon>0$ , there exists a n such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\lVert u-\\mathcal{P}_{n}u\\rVert_{H^{2}(\\Omega)}\\leq\\epsilon,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "More discussion about the assumption is shown in the appendix. The sketch of the proof is illustrated in Fig. 1. ", "page_idx": 4}, {"type": "image", "img_path": "F9mNL6vR27/tmp/df8301fad12627b5aeb61cf5d4354ab2b7b38ef118476ba6a794a7abe57d2751.jpg", "img_caption": ["Figure 1: The sketch of proof for Theorem 1. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Theorem 1. Suppose $\\mathcal{X}=\\mathcal{Y}\\subset H^{2}(\\Omega)$ and Assumption 1 holds. Then, there exists a neural network $O(u;\\pmb\\theta)\\in\\Xi_{p}$ defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Xi_{p}:=\\left\\{\\sum_{i=1}^{p}{A}_{i}\\sigma\\left(\\mathcal{W}_{i}{u}+{b}_{i}\\right)\\sigma\\left({{w}_{i}}\\cdot{{x}}+{\\zeta}_{i}\\right)\\left|\\mathcal{W}_{i}\\in\\mathcal{L}(\\mathcal{X},\\mathbb{R}^{m}),{{b}_{i}}\\in\\mathbb{R}^{m},{A}_{i}\\in\\mathbb{R}^{1\\times m}\\right\\}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{u\\in\\mathcal{X}}\\left\\|\\sum_{i=1}^{p}A_{i}\\sigma\\left(\\mathcal{W}_{i}u+b_{i}\\right)\\sigma\\left(w_{i}\\cdot x+\\zeta_{i}\\right)-\\mathcal{G}(u)\\right\\|_{L^{2}(\\Omega)}\\le C_{1}m^{-\\frac1n}+C_{2}(\\epsilon+p^{-\\frac2d}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\sigma$ is a smooth non-polynomial activation function, $n$ is shown in Assumption $^{\\,l}$ and contained in $\\mathcal{W}_{i}$ , $C_{1}$ is a constant independent of $m$ , \u03f5, and $p$ , $C_{2}$ is a constant depended on $p_{\\mathrm{:}}$ , n and $F$ (see in Assumption $^{\\,l}$ ) is the scale of the $\\mathcal{P}$ in Assumption $^{\\,l}$ . And $\\epsilon$ depends on $n$ . n and $\\epsilon$ are defined in Assumption $^{\\,l}$ . ", "page_idx": 4}, {"type": "text", "text": "The approximation results of DeepONet in Sobolev training can be found in [40]. ", "page_idx": 4}, {"type": "text", "text": "3.3 Loss Functions of Newton Informed Neural Operator ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "3.3.1 Mean Square Loss ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The Mean Square Error loss function is defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{E}_{S}(\\pmb{\\theta}):=\\frac{1}{M_{u}\\cdot M_{x}}\\sum_{j=1}^{M_{u}}\\sum_{k=1}^{M_{x}}\\left|\\mathcal{G}\\left(u_{j}\\right)(\\pmb{x}_{k})-\\mathcal{O}\\left(u_{j};\\pmb{\\theta}\\right)(\\pmb{x}_{k})\\right|^{2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $u_{1},u_{2},\\ldots,u_{M_{u}}\\sim\\mu$ are independently and identically distributed (i.i.d) samples in $\\mathcal{X}$ , and $\\pmb{x}_{1},\\pmb{x}_{2},\\dots,\\pmb{x}_{M_{x}}$ are uniformly i.i.d samples in $\\Omega$ . ", "page_idx": 5}, {"type": "text", "text": "However, using only the Mean Squared Error loss function is not sufficient for training to learn tFhuer tNheerwmtoorne , mtehtehreo da,r ee sspiteucaitailolny ss iwnhceer ie n wme odsot  cnaost eksn, owwe  hdoow n omt ahnayv seo leuntoioungsh  edxaitsat $\\{u_{j},\\bar{\\mathcal{G}}\\,(u_{j})\\}_{j=1}^{M_{u}}$ equation (1). If the data is sparse around one of the solutions, it becomes impossible to effectively learn the Newton method around that solution. ", "page_idx": 5}, {"type": "text", "text": "Given that $\\mathcal{E}_{S}(\\pmb{\\theta})$ can be viewed as the finite data formula of $\\mathcal{E}_{S c}(\\pmb{\\theta})$ , where ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{E}_{S c}(\\pmb{\\theta})=\\operatorname*{lim}_{M_{u},M_{x}\\rightarrow\\infty}\\mathcal{E}_{S}(\\pmb{\\theta}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The smallness of $\\mathcal{E}_{S c}$ can be inferred from Theorem 1. To understand the gap between $\\mathcal{E}_{S c}(\\pmb{\\theta})$ and $\\mathcal{E}_{S}(\\pmb{\\theta})$ , we can rely on the following theorem. Before the proof, we need some assumptions about the data in $\\mathcal{E}_{S}(\\pmb{\\theta})$ : ", "page_idx": 5}, {"type": "text", "text": "Assumption 2. (i) Boundedness: For any neural network with bounded parameters, characterized by a bound $B$ and dimension $d_{\\theta}$ , there exists a function $\\Psi:L^{2}(\\Omega)\\rightarrow[0,^{\\circ}\\infty)$ such that ", "page_idx": 5}, {"type": "equation", "text": "$$\n|\\mathcal{G}(u)({\\pmb x})|\\leqslant\\Psi(u),\\qquad\\operatorname*{sup}_{\\theta\\in[-B,B]^{d_{\\theta}}}|\\mathcal{O}(u;\\theta)({\\pmb x})|\\leqslant\\Psi(u),\\qquad\\operatorname*{sup}_{\\theta\\in[-B,B]^{d_{\\theta}}}|\\mathcal{L}\\mathcal{O}(u;\\theta)({\\pmb x})|\\leqslant\\Psi(u)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for all $u\\in\\mathcal{X},x\\in\\Omega$ , and there exist constants $C,\\kappa>0$ , such that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Psi(u)\\leqslant C\\left(1+\\|u\\|_{H^{2}}\\right)^{\\kappa}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "(ii) Lipschitz continuity: There exists a function $\\Phi:L^{2}(\\Omega)\\rightarrow[0,\\infty),$ , such that ", "page_idx": 5}, {"type": "equation", "text": "$$\n|\\mathcal{O}(u;\\pmb{\\theta})(\\pmb{x})-\\mathcal{O}(u;\\pmb{\\theta}^{\\prime})(\\pmb{x})|\\leqslant\\Phi(u)\\,\\|\\pmb{\\theta}-\\pmb{\\theta}^{\\prime}\\|_{\\ell^{\\infty}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for all $u\\in\\mathcal{X},\\pmb{x}\\in\\Omega$ , and $\\Phi(u)\\,\\leqslant\\,C\\left(1+\\|u\\|_{H^{2}(\\Omega)}\\right)^{\\kappa}$ , for the same constants $C,\\kappa>0$ as in Eq. (7). ", "page_idx": 5}, {"type": "text", "text": "(iii) Finite measure: There exists $\\alpha>0$ , such that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\int_{H^{2}(\\Omega)}e^{\\alpha\\|u\\|_{H^{2}(\\Omega)}^{2}}\\mathrm{d}\\mu(u)<\\infty.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Theorem 2. If Assumption 2 holds, then the generalization error is bounded by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\substack{|\\leqslant[-B,B]^{d_{\\theta}}}}\\big|\\mathbb{E}(\\mathcal{E}_{S}(\\theta)-\\mathcal{E}_{S c}(\\theta))\\big|\\leqslant C\\left[\\frac{1}{\\sqrt{M_{u}}}\\left(1+C d_{\\theta}\\log(C B\\sqrt{M_{u}})^{2\\kappa+1/2}\\right)+\\frac{d_{\\theta}\\sqrt{\\log M_{x}}}{\\sqrt{M_{x}}}\\right],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $C$ is a constant independent of $B,\\,d\\theta,\\,M_{x}$ , and $M_{u}$ . The parameter $\\kappa$ is specified in (7). Here, $B$ represents the bound of parameters, and $d_{\\theta}$ is the number of parameters. ", "page_idx": 5}, {"type": "text", "text": "The proof of Theorem 2 is presented in Appendix B.3. ", "page_idx": 5}, {"type": "text", "text": "Remark 1. Assumption 2 is easily satisfied if we consider $\\mathcal{X}$ as the local function set around the solution, which is typically the case in Newton\u2019s methods. This aligns with our approach and the working region in the approximation part (see Remark 3). The error $\\displaystyle\\operatorname*{sup}_{\\theta\\in[-B,B]^{d_{\\theta}}}$ $\\lvert\\mathbb{E}(\\mathcal{E}_{S}(\\pmb{\\theta})-$ $\\mathcal{E}_{S c}(\\pmb{\\theta}))|$ suggests that the network can perform well based on the loss function $\\mathcal{E}_{S}(\\pmb{\\theta})$ . The reasoning is as follows: let $\\begin{array}{r}{\\pmb{\\theta}_{S}=\\arg\\operatorname*{min}_{\\pmb{\\theta}\\in[-B,B]^{d_{\\pmb{\\theta}}}}\\mathcal{E}_{S}(\\pmb{\\theta})}\\end{array}$ and $\\pmb{\\theta}_{S_{c}}=\\arg\\operatorname*{min}_{\\pmb{\\theta}\\in[-B,B]^{d_{\\pmb{\\theta}}}}\\mathcal{E}_{S c}(\\pmb{\\theta})$ . We aim for $\\mathbb{E}\\mathcal{E}_{S c}(\\pmb{\\theta}_{S})$ to be small, which can be written as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\mathcal{E}_{S c}(\\theta_{S})\\leq\\mathcal{E}_{S c}(\\theta_{S c})+\\mathbb{E}(\\mathcal{E}_{S}(\\theta_{S})-\\mathcal{E}_{S c}(\\theta_{S}))\\leq\\mathcal{E}_{S c}(\\theta_{S c})+\\operatorname*{sup}_{\\theta\\in[-B,B]^{d_{\\theta}}}|\\mathbb{E}(\\mathcal{E}_{S}(\\theta)-\\mathcal{E}_{S c}(\\theta))|,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{E}_{S c}(\\pmb{\\theta}_{S_{c}})$ is small, as demonstrated by Theorem $^{\\,l}$ when $B$ is sufficiently large. ", "page_idx": 5}, {"type": "text", "text": "3.3.2 Newton Loss ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As we have mentioned, relying solely on the MSE loss function can require a significant amount of data to achieve the task. However, obtaining enough data can be challenging, especially when the equation is complex and the dimension of the input space is large. Hence, we need to consider another loss function to aid learning, which is the physical information loss function [33, 7, 19, 24], referred to here as the Network loss function. ", "page_idx": 6}, {"type": "text", "text": "The Newton loss function is defined as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{E}_{N}(\\pmb\\theta):=\\frac{1}{N_{u}\\cdot N_{x}}\\sum_{j=1}^{N_{u}}\\sum_{k=1}^{N_{x}}\\left|\\left(\\mathcal{L}-f^{\\prime}(u_{j}\\left(\\pmb x_{k}\\right))\\right)\\mathcal{O}\\left(u_{j};\\pmb\\theta\\right)\\left(\\pmb x_{k}\\right)+\\mathcal{L}u_{j}\\left(\\pmb x_{k}\\right)+f(u_{j}\\left(\\pmb x_{k}\\right))\\right|^{2}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $u_{1},u_{2},\\dotsc,u_{N_{u}}\\sim\\nu$ are independently and identically distributed (i.i.d) samples in $\\mathcal{X}$ , and ${\\pmb x}_{1},{\\pmb x}_{2},\\ldots,{\\pmb x}_{N_{x}}$ are uniformly i.i.d samples in $\\Omega$ . ", "page_idx": 6}, {"type": "text", "text": "Given that ${\\mathcal{E}}_{N}(\\pmb{\\theta})$ can be viewed as the finite data formula of $\\mathcal{E}_{N c}(\\pmb{\\theta})$ , where ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{E}_{N c}(\\pmb{\\theta})=\\operatorname*{lim}_{N_{u},N_{x}\\rightarrow\\infty}\\mathcal{E}_{S}(\\pmb{\\theta}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "To understand the gap between $\\mathcal{E}_{N c}(\\pmb{\\theta})$ and ${\\mathcal{E}}_{N}(\\pmb{\\theta})$ , we can rely on the following theorem: ", "page_idx": 6}, {"type": "text", "text": "Corollary 1. If Assumption 2 holds, then the generalization error is bounded by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\forall[-B,B]^{d_{\\theta}}}|\\mathbb{E}(\\mathcal{E}_{N}(\\theta)-\\mathcal{E}_{N c}(\\theta))|\\leqslant C\\left[\\frac{1}{\\sqrt{N_{u}}}\\left(1+C d_{\\theta}\\log(C B\\sqrt{N_{u}})^{2\\kappa+1/2}\\right)+\\frac{d_{\\theta}\\sqrt{\\log N_{x}}}{\\sqrt{N_{x}}}\\right],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $C$ is a constant independent of $B,\\,d_{\\theta},\\,N_{x}$ , and $N_{u}$ . The parameter $\\kappa$ is specified in (7). Here, $B$ represents the bound of parameters, and $d_{\\theta}$ is the number of parameters. ", "page_idx": 6}, {"type": "text", "text": "The proof of Corollary 1 is similar to that of Theorem 2; therefore, it will be omitted from the paper. Remark 2. If we only utilize $\\mathcal{E}_{S}(\\pmb{\\theta})$ as our loss function, as demonstrated in Theorem 2, we require both $M_{u}$ and $M_{x}$ to be large, posing a significant challenge when dealing with complex nonlinear equations. Obtaining sufficient data becomes a critical issue in such cases. In this paper, we integrate Newton\u2019s information into the loss function, defining it as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{E}(\\pmb{\\theta}):=\\lambda\\mathcal{E}_{S}(\\pmb{\\theta})+\\mathcal{E}_{N}(\\pmb{\\theta}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where ${\\mathcal{E}}_{N}(\\pmb{\\theta})$ represents the cost associated with the unsupervised learning data. If we lack sufficient data for $\\mathcal{E}_{S}(\\pmb{\\theta})$ , we can adjust the parameters by selecting a small $\\lambda$ and increasing $N_{x}$ and $N_{u}$ . This strategy enables effective learning even when data for $\\mathcal{E}_{S}(\\pmb{\\theta})$ is limited. We refer to this neural operator, which incorporates Newton information, as the Newton Informed Neural Operator. ", "page_idx": 6}, {"type": "text", "text": "In the following experiment, we will use the neural operator established in Eq. (3) and the loss function in Eq. (10) to learn one step of the Newton method locally, i.e., the map between the input $u$ and the solution $\\delta u$ in eq. (2). If we have a large dataset, we can choose a large $\\lambda$ in $\\mathcal{E}(\\pmb{\\theta})$ (10); if we have a small dataset, we will use a small $\\lambda$ to ensure the generalization of the operator is minimized. After learning one step of the Newton method using the operator neural networks, we can easily and quickly obtain the solution by the initial condition of the nonlinear PDEs (1) and find new solutions not present in the datasets. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Settings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We introduce two distinct training methodologies. The first approach employs exclusively supervised data, leveraging the Mean Squared Error Loss (6) as the primary optimization criterion. The second method combines both supervised and unsupervised learning paradigms, utilizing a hybrid loss function 10 that integrates Mean Squared Error Loss (6) for small proportion of data with ground truth (supervised training dataset) and with Newton\u2019s loss (9) for large proportion of data without ground truth (unsupervised training dataset). We call the two methods method 1 and method 2. The approaches are designed to simulate a practical scenario with limited data availability, facilitating a comparison between these training strategies to evaluate their efficacy in small supervised data regimes. We chose the same configuration of the neural operator (DeepONet) which is aligned with our theoretical analysis. One can find the detailed experimental settings and the datasets for each example below in Appendix A. ", "page_idx": 6}, {"type": "text", "text": "We consider 2D convex problem $\\mathcal{L}(u)-f(u)=0$ , where $\\mathcal{L}(u):=-\\Delta u$ , $f(u):-u^{2}\\!+\\!\\sin{5\\pi}(x\\!+\\!y)$ and $u=0$ on $\\partial\\Omega$ . We investigate the training dynamics and testing performance of neural operator (DeepONet) trained with two methods, focusing on Mean Squared Error (MSE) and Newton\u2019s loss functions. For method 1, we use 500 supervised data samples (with ground truth), while for method 2, we use 5000 unsupervised data samples (only with the initial state) along with supervised data samples, employing the regularized loss function as defined in Equation 10 with $\\lambda=0.01$ . Please refer A.1.1 for the dataset generation of convex case. For the detailed experimental settings, refer to Appendix A. ", "page_idx": 7}, {"type": "image", "img_path": "F9mNL6vR27/tmp/65a33011edd098f522c1a3cb30fe2fba8aff047bf9a027f86693af9f7ee51a02.jpg", "img_caption": ["Figure 2: Training and testing performance of DeepONet under different conditions. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "MSE Loss Training (Fig. 2(a)): In method 1, Effective training is observed but exhibits poor generalization. The significantly larger testing error compared to the training error suggests that using only MSE loss is insufficient. Performance Comparison (Fig. 2(b)): DeepONet-Newton model (Method 2) exhibits superior performance in both $L_{2}$ and $H_{1}$ error metrics, highlighting its enhanced generalization accuracy. This study shows the advantages of using Newton\u2019s loss for training DeepONet models, illustrating that increasing the number of unsupervised samples via introducing Newton\u2019s loss leads to a substantial improvement in the test $L_{2}$ error and $H_{1}$ error. ", "page_idx": 7}, {"type": "text", "text": "4.3 Case 2: Non-convex problem with multiple solutions ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We consider a 2D Non-convex problem, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left\\{\\!\\!\\begin{array}{l c l}{{-\\Delta u(x,y)-u^{2}(x,y)=-s\\sin(\\pi x)\\sin(\\pi y)\\,}}&{{\\mathrm{in}}}&{{\\Omega,}}\\\\ {{u(x,y)=0,\\quad\\mathrm{in}}}&{{\\partial\\Omega}}&{{}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\Omega=(0,1)\\times(0,1)$ [3]. In this case, $\\mathcal{L}(u):=-\\Delta(u),\\,f(u):=u^{2}-s\\sin(\\pi x)\\sin(\\pi y)$ ) and it has multiple solutions (see Figure 3 for its solutions). ", "page_idx": 7}, {"type": "text", "text": "In the experiment, we let one of the multiple ground truth solutions rarely touched in the supervised training dataset such that the neural operator trained via method 1 will saturate in terms of test error because it relies on the ground truth to recover all the patterns for multiple solution cases (as shown by the curves in Figure 3). On the other hand, the model trained via method 2 is less affected by the limited supervised data since the utilization of Newton\u2019s loss. One can refer to Appendix A for the detailed experiment setting. ", "page_idx": 7}, {"type": "text", "text": "Efficiency This case study highlights the superior efficiency of our neural operator-based method as a surrogate model for Newton\u2019s method. Both methods parallelize operations to solve 500/5000 Newton linear systems simultaneously, each with distinct initial states. The key advantage of the neural operator lies in its ability to batch the computation of these independent systems efficiently. By efficiently batching and sampling a wide variety of initial states, the neural operator improves the likelihood of discovering to multiple solutions, particularly in nonlinear PDEs with complex solution landscapes. Consequently, while Newton\u2019s method alone does not inherently guarantee finding multiple solutions, the combination of rapid computation and extensive initial condition sampling enhances the chances of identifying multiple solutions. ", "page_idx": 7}, {"type": "image", "img_path": "F9mNL6vR27/tmp/05d2e94d06c7ae5025e9eb64911ccd5a1187785922a69a9e51cc5ccc6f431608.jpg", "img_caption": ["Figure 3: Solutions of 2D Non-convex problem (11) "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "For a fair comparison, the classical Newton solver was also parallelized using CUDA on a GPU. However, the neural operator naturally handles large batch sizes during inference, allowing it to process all systems in one go. One can find the detailed description of the experiments in A.6. ", "page_idx": 8}, {"type": "table", "img_path": "F9mNL6vR27/tmp/ea8296d3eabf7fc49f26476c5f8294bf1e9c58beaa1b6bbd3e719d400f1643c0.jpg", "table_caption": [], "table_footnote": ["Table 1: Benchmarking the efficiency of Newton Informed Neural Operator. Computational Time Comparison for Solving 500 and 5000 Initial Conditions. "], "page_idx": 8}, {"type": "text", "text": "The table demonstrates the significant efficiency gain achieved by batching the computation of independent Newton systems with distinct initial states using the neural operator. For NINO, solving 5000 independent Newton linear systems scales up minimally compared to solving 500 systems, while the classical solver experiences a tenfold increase in computation time. This efficient batching is crucial for improving performance, particularly in complex nonlinear systems like the Gray-Scott model, where solving numerous systems simultaneously is essential for effective pattern discovery. ", "page_idx": 8}, {"type": "text", "text": "4.4 Case 3: The Gray-Scott model", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The Gray-Scott model [31, 11] describes the reaction and diffusion of two chemical species, $A$ and $S$ , governed by the following equations: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial A}{\\partial t}=D_{A}\\Delta A-S A^{2}+(\\mu+\\rho)A,}\\\\ {\\displaystyle\\frac{\\partial S}{\\partial t}=D_{S}\\Delta S+S A^{2}-\\rho(1-S),}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $D_{A}$ and $D_{S}$ are the diffusion coefficients, and $\\mu$ and $\\rho$ are rate constants. ", "page_idx": 8}, {"type": "text", "text": "Newton\u2019s Method for Steady-State Solutions Newton\u2019s method is employed to find steady-state solutions ( $\\begin{array}{r}{\\frac{\\partial A}{\\partial t}=0}\\end{array}$ and $\\begin{array}{r}{\\frac{\\partial S}{\\partial t}=0}\\end{array}$ ) by solving the nonlinear system: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{l}{0=D_{A}\\Delta A-S A^{2}+(\\mu+\\rho)A,}\\\\ {0=D_{S}\\Delta S+S A^{2}-\\rho(1-S).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The Gray-Scott model is highly sensitive to initial conditions, where even minor perturbations can lead to vastly different emergent patterns. Please refer to Figure 5 for some examples of the patterns. This sensitivity reflects the model\u2019s complex, non-linear dynamics that can evolve into a multitude of possible steady states based on the initial setup. Consequently, training a neural operator to map initial conditions directly to their respective steady states presents significant challenges. Such a model must learn from a vast functional space, capturing the underlying dynamics that dictate the transition from any given initial state to its final pattern. This complexity and diversity of potential outcomes is the inherent difficulty in training neural operators effectively for systems as complex as the Gray-Scott model. One can refer to A.1.2 for a detailed discussion on the Gray-Scott model. We employ a Neural Operator as a substitute for the Newton solver in the Gray-Scott model, which recurrently maps the initial state to the steady state. ", "page_idx": 9}, {"type": "image", "img_path": "F9mNL6vR27/tmp/60c13c911dfb9771e6a93882b2c1429b8d4e8303126935a1b41fae9ed53774d0.jpg", "img_caption": ["(a) An example demonstrating how the neural operator maps the initial state to the steady state in a iterative manner "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "F9mNL6vR27/tmp/83977bcbed750aaf24e551c8debdf025e81c05b22b92a95676443052bf35008f.jpg", "img_caption": ["Figure 4: The convergence behavior of the Neural Operator-based solver. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "In subfigure (a), we use a ring-like pattern as the initial state to test our learned neural operator. This pattern does not appear in the supervised training dataset and lacks corresponding ground truth data. Instead, it is present only in the unsupervised data (Newton\u2019s loss), i.e., some data in Newton\u2019s loss will converge to this specific pattern. Despite this, our neural operator, trained using Newton\u2019s loss, can effectively approximate the mapping of the initial solution to its correct steady state. we further test our neural operator, utilizing it as a surrogate for Newton\u2019s method to address nonlinear problems with an initial state drawn from the test dataset. The curve shows the average convergence rate of $\\lVert u-u_{i}\\rVert$ across the test dataset, where $u_{i}$ represents the prediction at the $i$ -th step by the neural operator. In subfigure (c), we compare the Training Loss (Rescaled Newton\u2019s Loss) and Absolute L2 Test Error. The magnitudes are not directly comparable as they represent different metrics; however, the trends are consistent, indicating that the inclusion of unsupervised data and training with Newton\u2019s loss contributes to improved model performance. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we develop neural operators to learn the Newton\u2019s solver related to nonlinear PDEs (Eq. (1)) with multiple solutions. To speed up the computation of multiple solutions for nonlinear PDEs, we combine neural operator learning with classical Newton methods, resulting in the Newtoninformed neural operator. We provide a theoretical analysis of our neural operator, demonstrating that it can effectively learn the Newton operator, reduce the number of required supervised data, and learn solutions not present in the supervised learning data due to the addition of the Newton loss (9) in the loss function. Our experiments are consistent with our theoretical analysis, showcasing the advantages of our network as mentioned earlier. ", "page_idx": 9}, {"type": "text", "text": "Reproducibility Statement Code Availability: The code used in our experiments can be accessed via https://github.com/xlliu2017/learn_newton and also the supplementary material. Datasets can be downloaded via URLs in the repository. This encompasses all scripts, functions, and auxiliary flies necessary to reproduce our results. Configuration Transparency: All configurations, including hyperparameters, model architectures, and optimization settings, are explicitly provided in the Appendix. ", "page_idx": 10}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Y.Y. and W.H. were supported by the National Institute of General Medical Sciences through grant 1R35GM146894. The work of X.L. was partially supported by the KAUST Baseline Research Fund. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] H. Amann and P. Hess. A multiplicity result for a class of elliptic boundary value problems. Proceedings of the Royal Society of Edinburgh Section A: Mathematics, 84(1-2):145\u2013151, 1979.   \n[2] S. Brenner. The mathematical theory of finite element methods. Springer, 2008. [3] B. Breuer, P. McKenna, and M. Plum. Multiple solutions for a semilinear boundary value problem: a computational multiplicity proof. Journal of Differential Equations, 195(1):243\u2013269, 2003.   \n[4] Shuhao Cao. Choose a transformer: Fourier or galerkin. Advances in Neural Information Processing Systems, 34, 2021.   \n[5] Mark C Cross and Pierre C Hohenberg. Pattern formation outside of equilibrium. Reviews of modern physics, 65(3):851, 1993.   \n[6] L. Evans. Partial differential equations, volume 19. American Mathematical Society, 2022. [7] Somdatta Goswami, Aniruddha Bora, Yue Yu, and George Em Karniadakis. Physics-informed neural operators. 2022 arXiv preprint arXiv:2207.05748, 2022.   \n[8] R. Guo, S. Cao, and L. Chen. Transformer meets boundary value inverse problems. In The Eleventh International Conference on Learning Representations, 2022.   \n[9] J. Han, A. Jentzen, and W. E. Solving high-dimensional partial differential equations using deep learning. Proceedings of the National Academy of Sciences, 115(34):8505\u20138510, 2018.   \n[10] W. Hao, S. Lee, X. Xu, and Z. Xu. Stability and robustness of time-discretization schemes for the allen-cahn equation via bifurcation and perturbation analysis. arXiv preprint arXiv:2406.18393, 2024.   \n[11] W. Hao, C. Liu, Y. Wang, and Y. Yang. On pattern formation in the thermodynamicallyconsistent variational gray-scott model. arXiv preprint arXiv:2409.04663, 2024.   \n[12] W. Hao and C. Xue. Spatial pattern formation in reaction\u2013diffusion models: a computational approach. Journal of mathematical biology, 80:521\u2013543, 2020.   \n[13] Wenrui Hao, Jonathan D Hauenstein, Bei Hu, and Andrew J Sommese. A bootstrapping approach for computing multiple solutions of differential equations. Journal of Computational and Applied Mathematics, 258:181\u2013190, 2014.   \n[14] Wenrui Hao, Jan Hesthaven, Guang Lin, and Bin Zheng. A homotopy method with adaptive basis selection for computing multiple solutions of differential equations. Journal of Scientific Computing, 82(1):19, 2020.   \n[15] Wenrui Hao, Sun Lee, and Young Ju Lee. Companion-based multi-level finite element method for computing multiple solutions of nonlinear differential equations. Computers & Mathematics with Applications, 168:162\u2013173, 2024.   \n[16] J. He, X. Liu, and J. Xu. Mgno: Efficient parameterization of linear operators via multigrid. In The Twelfth International Conference on Learning Representations, 2023.   \n[17] R. Hoyle. Pattern formation: an introduction to methods. Cambridge University Press, 2006.   \n[18] J. Hu and S. Zhang. The minimal conforming $H^{k}$ finite element spaces on $R^{n}$ rectangular grids. Mathematics of Computation, 84(292):563\u2013579, 2015.   \n[19] Y. Huang, W. Hao, and G. Lin. Hompinns: Homotopy physics-informed neural networks for learning multiple solutions of nonlinear elliptic differential equations. Computers & Mathematics with Applications, 121:62\u201373, 2022.   \n[20] S. Kondo and T. Miura. Reaction-diffusion model as a framework for understanding biological pattern formation. science, 329(5999):1616\u20131620, 2010.   \n[21] I. Lagaris, A. Likas, and D. Fotiadis. Artificial neural networks for solving ordinary and partial differential equations. IEEE transactions on neural networks, 9(5):987\u20131000, 1998.   \n[22] Samuel Lanthaler, Siddhartha Mishra, and George E Karniadakis. Error estimates for deeponets: A deep learning framework in infinite dimensions. Transactions of Mathematics and Its Applications, 6(1):tnac001, 2022.   \n[23] Z. Li and N. Yan. New error estimates of bi-cubic hermite finite element methods for biharmonic equations. Journal of computational and applied mathematics, 142(2):251\u2013285, 2002.   \n[24] Z. Li, H. Zheng, N. Kovachki, D. Jin, H. Chen, B. Liu, K. Azizzadenesheli, and A. Anandkumar. Physics-informed neural operator for learning partial differential equations. CoRR, abs/2111.03794, 2021.   \n[25] Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier NeuralOperator for Parametric Partial Differential Equations. In International Conference on Learning Representations, 2020.   \n[26] B. Lin, Z. Mao, Z. Wang, and G. Karniadakis. Operator learning enhanced physics-informed neural networks for solving partial differential equations characterized by sharp solutions. arXiv preprint arXiv:2310.19590, 2023.   \n[27] X. Liu, B. Xu, S. Cao, and L. Zhang. Mitigating spectral bias for the multiscale operator learning. Journal of Computational Physics, 506:112944, 2024.   \n[28] L. Lu, X. Meng, S. Cai, Z. Mao, S. Goswami, Z. Zhang, and G. Karniadakis. A comprehensive and fair comparison of two neural operators (with practical extensions) based on fair data. Computer Methods in Applied Mechanics and Engineering, 393:114778, 2022.   \n[29] Lu Lu, Pengzhan Jin, and George Em Karniadakis. Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators. arXiv preprint arXiv:1910.03193, 2019.   \n[30] H. Mhaskar. Neural networks for optimal approximation of smooth and analytic functions. Neural computation, 8(1):164\u2013177, 1996.   \n[31] J. Pearson. Complex patterns in a simple system. Science, 261(5118):189\u2013192, 1993.   \n[32] T. Poggio, H. Mhaskar, L. Rosasco, B. Miranda, and Q. Liao. Why and when can deep-but not shallow-networks avoid the curse of dimensionality: a review. International Journal of Automation and Computing, 14(5):503\u2013519, 2017.   \n[33] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378:686\u2013707, 2019.   \n[34] A. Schmidt-Hieber. Nonparametric regression using deep neural networks with relu activation function. Annals of statistics, 48(4):1875\u20131897, 2020.   \n[35] M. Ulbrich. Semismooth newton methods for operator equations in function spaces. SIAM Journal on Optimization, 13(3):805\u2013841, 2002.   \n[36] Yingwei Wang, Wenrui Hao, and Guang Lin. Two-level spectral methods for nonlinear elliptic equations with multiple solutions. SIAM Journal on Scientific Computing, 40(4):B1180\u2013B1205, 2018.   \n[37] T. Welti. High-dimensional stochastic approximation: algorithms and convergence rates. PhD thesis, ETH Zurich, 2020.   \n[38] J. Xu. Two-grid discretization techniques for linear and nonlinear pdes. SIAM journal on numerical analysis, 33(5):1759\u20131777, 1996.   \n[39] Jinchao Xu and Xiaofeng Xu. Lack of robustness and accuracy of many numerical schemes for phase-field simulations. Mathematical Models and Methods in Applied Sciences, 33(08):1721\u2013 1746, 2023.   \n[40] Y. Yang. Deeponet for solving PDEs: Generalization analysis in Sobolev training. arXiv preprint arXiv:2410.04344, 2024.   \n[41] Y. Yang and J. He. Deeper or wider: A perspective from optimal generalization error with sobolev loss. International Conference on Machine Learning, 2024.   \n[42] Y. Yang, Y. Wu, H. Yang, and Y. Xiang. Nearly optimal approximation rates for deep super relu networks on sobolev spaces. arXiv preprint arXiv:2310.10766, 2023.   \n[43] Y. Yang and Y. Xiang. Approximation of functionals by neural network without curse of dimensionality. arXiv preprint arXiv:2205.14421, 2022.   \n[44] Y. Yang, H. Yang, and Y. Xiang. Nearly optimal VC-dimension and pseudo-dimension bounds for deep neural network derivatives. NuerIPS 2023, 2023.   \n[45] B. Yu and W. E. The deep ritz method: a deep learning-based numerical algorithm for solving variational problems. Communications in Mathematics and Statistics, 6(1):1\u201312, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Figure 5: Examples of steady states of the Gray Scott model ", "page_idx": 13}, {"type": "text", "text": "A Experimental settings ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Background on the PDEs and generation of datasets ", "page_idx": 13}, {"type": "text", "text": "A.1.1 Case 1: convex problem ", "page_idx": 13}, {"type": "text", "text": "Function and Jacobian The function $F(u)$ might typically be defined as: ", "page_idx": 13}, {"type": "equation", "text": "$$\nF(u)=-\\Delta u+u^{2}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The Jacobian $J(u)$ , for the given function $F(u)$ , involves the derivative of $F$ with respect to $u$ , which includes the Laplace operator and the derivative of the nonlinear term: ", "page_idx": 13}, {"type": "equation", "text": "$$\nJ(u)=-\\Delta+2\\cdot u.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The dataset are generated by sampling the initial state $u_{0}\\sim\\mathcal{N}(0,\\Delta^{-3})$ and then calculate the convergent sequence $\\{u_{0},u_{1},...,u_{n}\\}$ by Newton\u2019s method. Each convergent sequence $\\{u_{0},u_{1},...,u_{n}\\}$ is one data entry in the dataset. ", "page_idx": 13}, {"type": "text", "text": "The analysis of function and Jacobian for the non-convex problem (case 2) is similar to the convex problem except that its Jacobian $J(u)=\\Delta-2u$ such that Newton\u2019s system is not positive definite. ", "page_idx": 13}, {"type": "text", "text": "A.1.2 Gray Scott model ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Jacobian Matrix The Jacobian matrix $J$ of the system is crucial for applying Newton\u2019s method: ", "page_idx": 13}, {"type": "equation", "text": "$$\nJ=\\left[J_{A A}^{\\phantom{\\dagger}}\\quad J_{A S}^{\\phantom{\\dagger}}\\right]^{\\phantom{\\dagger}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "with components: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{J_{A A}=-D_{A}\\Delta+\\mathrm{diag}(-2S A+\\mu+\\rho),}}\\\\ {{J_{A S}=\\mathrm{diag}(-A^{2}),}}\\\\ {{J_{S A}=\\mathrm{diag}(2S A),}}\\\\ {{J_{S S}=-D_{S}\\Delta+\\mathrm{diag}(A^{2}+\\rho).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The numerical simulation of the Gray-Scott model was configured with the following parameters: ", "page_idx": 13}, {"type": "text", "text": "\u2022 Grid Size: The simulation grid is square with $N=63$ points on each side, leading to a total of $N^{2}$ grid points. This resolution was chosen to balance computational efficiency with spatial resolution sufficient to capture detailed patterns. The spacing between each grid point, $h$ , is computed as $\\begin{array}{r}{h=\\frac{1.0}{N-1}}\\end{array}$ . This ensures that the domain is normalized to a unit square, which simplifies the analysis and scaling of diffusion rates.   \n\u2022 Diffusion Coefficients: The diffusion coefficients for species $A$ and $S$ are set to $D_{A}=$ $2.5\\times10^{-4}$ and $D_{S}=5.0\\times10^{-4}$ , respectively. These values determine the rate at which each species diffuses through the spatial domain.   \n\u2022 Reaction Rates: The reaction rate $\\mu$ and feed rate $\\rho$ are crucial parameters that govern the dynamics of the system. For this simulation, $\\mu$ is set to 0.065 and $\\rho$ to 0.04, influencing the production and removal rates of the chemical species. ", "page_idx": 13}, {"type": "text", "text": "Simulations The simulation utilizes a finite difference method for spatial discretization and Newton\u2019s method to solve the steady-state given the initial state. The algorithm is detailed in A.6. ", "page_idx": 13}, {"type": "image", "img_path": "F9mNL6vR27/tmp/4b8330b1c1f0a4b6221f818ac2f60db6d15d386dab3b4a263550d3b4861901e0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 6: Three examples depicting the evolution from the initial state to the steady state via Newton\u2019s method. ", "page_idx": 14}, {"type": "text", "text": "A.2 Data generation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Here is how we generate the supervised data samples: ", "page_idx": 14}, {"type": "text", "text": "1. Step 1: We use a classical numerical solver to obtain single (multiple) solutions of nonlinear PDEs. For example, in Case 2, there exist four solutions $u^{1},\\dot{u}^{2},u^{3},u^{4}$ . We have one solution for Case 1 and 10 solutions for the Case 3: Gray-Scott model.   \n2. Step 2: The supervised dataset is generated by sampling a perturbation around the solution $\\boldsymbol{u}^{i}\\stackrel{\\bar{\\sim}}{\\sim}\\mathcal{N}(\\boldsymbol{0},(-\\dot{\\boldsymbol{\\Delta}})^{-3})$ on the chosen true solution $u^{i}$ . We then set $u_{0}^{i}=u_{p}^{i}+u^{i}$ and calculate the convergent sequence $u_{0}^{i},u_{1}^{i},\\ldots,u_{n}^{i}$ using Newton\u2019s method, which follows the formula: ", "page_idx": 14}, {"type": "equation", "text": "$$\nu_{k+1}^{i}=u_{k}^{i}-J_{f}(u_{k}^{i})^{-1}f(u_{k}^{i}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Each convergent sequence $u_{0}^{i},u_{1}^{i},\\ldots,u_{n}^{i}$ constitutes one supervised data entry in the dataset. In this case, we consider the initial conditions as perturbed, with all perturbations applied around the true solution. A comparison between the traditional method and our proposed method is summarized in Table 1. ", "page_idx": 14}, {"type": "text", "text": "For the unsupervised data samples, we only sample the perturbed initial states. ", "page_idx": 14}, {"type": "text", "text": "A.3 Implementations of loss functions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Discrete Newton\u2019s Loss In solving partial differential equations (PDEs) numerically on a regular grid, the Laplace operator and other differential terms can be efficiently computed using convolution. Here, we detail the method for calculating $J(u)\\delta u-F(u)$ where $J(u)$ is the Jacobian matrix, $\\delta u$ is the Newton step, and $F(u)$ is the function defining the PDE. ", "page_idx": 14}, {"type": "text", "text": "Discretization Consider a discrete representation of a function $u$ on a $N\\times N$ grid. The function $u$ and its perturbation $\\delta u$ are represented as matrices: ", "page_idx": 14}, {"type": "equation", "text": "$$\nu,\\delta u\\in\\mathbb{R}^{N\\times N}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The function $F(u)$ , which involves both linear and nonlinear terms, is similarly represented as F(u) \u2208RN\u00d7N. ", "page_idx": 14}, {"type": "text", "text": "Laplace Operator Regarding the representing the $J(u)$ with $N\\times N$ grid function $u$ , the discretized Laplace operator using a finite difference method can be expressed as a convolution: ", "page_idx": 14}, {"type": "equation", "text": "$$\n-\\Delta u=\\left[{\\!\\!\\!{\\begin{array}{c c c}{{0}}&{{-1}}&{{0}}\\\\ {{-1}}&{{4}}&{{-1}}\\\\ {{0}}&{{-1}}&{{0}}\\end{array}}\\!\\!\\right]*u\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This convolution computes the result of the Laplace operator applied to the grid function $u$ . The boundary conditions can be further incorporated into the convolution with different padding modes. Dirichlet boundary condition corresponds to zeros padding while Neumann boundary condition corresponds to replicate padding. ", "page_idx": 14}, {"type": "text", "text": "A.4 Architecture of DeepONet ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A variant of DeepONet is used in our Newton-informed neural operator. In the DeepONet, we introduce a hybrid architecture that combines convolutional layers with a static trunk basis, optimized for grid-based data inputs common in computational applications like computational biology and materials science. ", "page_idx": 15}, {"type": "text", "text": "Branch Network The branch network is designed to effectively downsample and process the spatial features through a series of convolutional layers: ", "page_idx": 15}, {"type": "text", "text": "\u2022 A Conv2D layer with 128 fliters ( $7\\mathrm{x}7$ , stride 2) initiates the feature extraction, reducing the input dimensionality while capturing coarse spatial features.   \n\u2022 This is followed by additional Conv2D layers (128 filters, 5x5 kernel, stride 2 and subsequently 3x3 with padding, 1x1) which further refine and compact the feature representation.   \n\u2022 The convolutional output is flattened and processed through two fully connected layers (256 units then down to branch features), using GELU activation. ", "page_idx": 15}, {"type": "text", "text": "Trunk Network The trunk utilizes a static basis represented by the tensor V, incorporated as a non-trainable component: The tensor $\\mathtt{V}$ is precomputed, using Proper Orthogonal Decomposition (POD) as in [28], and is dimensionally compatible with the output of the branch network. ", "page_idx": 15}, {"type": "text", "text": "Forward Pass During the forward computation, the branch network outputs are projected onto the trunk\u2019s static basis via matrix multiplication, resulting in a feature matrix that is reshaped into the grid dimensionality for output. ", "page_idx": 15}, {"type": "text", "text": "Hyperparameters ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The following table 2 summarizes the key hyperparameters used in the DeepONet architecture: ", "page_idx": 15}, {"type": "table", "img_path": "F9mNL6vR27/tmp/2e44060bbb961e994f193ebf3265fef44f2162b27fbc9f74374c9a0805d11c3d.jpg", "table_caption": [], "table_footnote": ["Table 2: Hyperparameters of the DeepONet architecture "], "page_idx": 15}, {"type": "text", "text": "A.5 Training settings ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Below we summarize the key configurations and parameters employed in the training for three cases: ", "page_idx": 15}, {"type": "text", "text": "Dataset ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "\u2022 Case 1: For method 1, we use 500 supervised data samples (with ground truth) while for method 2, we use 5000 unsupervised data samples (only with the initial state) and 500 supervised data samples. ", "page_idx": 15}, {"type": "text", "text": "\u2022 Case 2: For method 1, we use 5000 supervised data while for method 2, we use 5000 unsupervised data samples and 5000 supervised data samples. ", "page_idx": 15}, {"type": "text", "text": "\u2022 Case 3 (Gray-Scott model): We only perform method 2, with 10000 supervised data samples and 50000 unsupervised data samples. ", "page_idx": 15}, {"type": "text", "text": "Optimization Technique ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "\u2022 Optimizer: Adam, with a learning rate of $1\\times10^{-4}$ and a weight decay of $1\\times10^{-6}$ . ", "page_idx": 15}, {"type": "text", "text": "\u2022 Training Epochs: The model was trained over 1000 epochs to ensure convergence and we use Batch Size: 50. ", "page_idx": 16}, {"type": "text", "text": "These settings underscore our commitment to precision and detailed examination of neural operator efficiency in computational tasks. Our architecture and optimization choices are particularly tailored to explore and exploit the capabilities of neural networks in processing complex systems simulations. ", "page_idx": 16}, {"type": "text", "text": "A.6 Benchmarking Newton\u2019s Method and neural operator based method ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Experimental Setup The benchmark study was conducted to evaluate the performance of a GPUaccelerated implementation of Newton\u2019s method, designed to solve systems of linear equations derived from discretizing partial differential equations. The implementation utilized CuPy with CUDA to leverage the parallel processing capabilities of the NVIDIA A100 GPU. The hardware comprises an Intel Cascade Lake 2.5 GHz CPU, and an NVIDIA A100 GPU. ", "page_idx": 16}, {"type": "text", "text": "The performance was assessed in terms of total execution time, which includes the setup of matrices and vectors, computation on the GPU, and synchronization of CUDA streams. Both methods leverage the parallel processing capabilities of the GPU. Specifically, the Newton solver explicitly uses 10 streams and CuPy with CUDA to parallelize the computation and fully utilize the GPU parallel processing capabilities, aiming to optimize execution efficiency. In contrast, the neural operator method is inherently parallelized, taking full advantage of the GPU architecture without the explicit use of multiple streams as indicated in the table. The computational times of both methods were evaluated under a common hardware configuration. ", "page_idx": 16}, {"type": "text", "text": "Software Environment: Ubuntu 20.04 LTS. Python Version: 3.8. CUDA Version: 11.4. ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Newton\u2019s method was implemented to solve the Laplacian equation over a discretized domain. Multiple system solutions were computed in parallel using CUDA streams. The key parameters of the experiment are as follows: Data Type (dtype): Single precision floating point (float32). Number of Streams: 10 CUDA streams to process data in parallel. Number of Repeated Calculations: The Newton method was executed multiple times for 500/5000 Newton linear systems, respectively, distributed evenly across the streams. Function to Solve Systems: The CuPy\u2019s spsolve was used for solving the sparse matrix systems. The following algorithm A.6 summarizes the procedure to benchmark the time used for solving Newton\u2019s system for 5000 different initial states. ", "page_idx": 16}, {"type": "table", "img_path": "F9mNL6vR27/tmp/d8969a25735eb649b3491d3920a28894663001b33137eec7c2e278b6abf6f62e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B Supplemental material for proof ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Preliminaries ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Definition 1 (Sobolev Spaces [6]). Let $\\Omega$ be $[0,1]^{d}$ and let $D$ be the operator of the weak derivative of a single variable function and $D^{\\alpha}\\,=\\,D_{1}^{\\alpha_{1}}D_{2}^{\\alpha_{2}}\\cdot\\cdot\\cdot D_{d}^{\\alpha_{d}}$ be the partial derivative where $\\alpha=$ ", "page_idx": 16}, {"type": "text", "text": "$[\\alpha_{1},\\alpha_{2},\\ldots,\\alpha_{d}]^{T}$ and $D_{i}$ is the derivative in the i-th variable. Let $n\\in\\mathbb N$ and $1\\leq p\\leq\\infty$ . Then we define Sobolev spaces ", "page_idx": 17}, {"type": "equation", "text": "$$\nW^{n,p}(\\Omega):=\\left\\{f\\in L^{p}(\\Omega):D^{\\alpha}f\\in L^{p}(\\Omega)\\,f o r\\,a l l\\,\\alpha\\in\\mathbb{N}^{d}\\,w i t h\\,|\\alpha|\\leq n\\right\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with a norm ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|f\\|_{W^{n,p}(\\Omega)}:=\\left(\\sum_{0\\leq|\\alpha|\\leq n}\\|D^{\\alpha}f\\|_{L^{p}(\\Omega)}^{p}\\right)^{1/p}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "if $p<\\infty$ , and $\\begin{array}{r}{\\|f\\|_{W^{n,\\infty}(\\Omega)}:=\\operatorname*{max}_{0\\leq|\\alpha|\\leq n}\\|D^{\\alpha}f\\|_{L^{\\infty}(\\Omega)}.}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "Furthermore, for $\\pmb{f}\\;=\\;(f_{1},\\pmb{\\dots},f_{d}),\\;\\pmb{f}\\;\\in\\;W^{1,\\infty}(\\Omega,\\mathbb{R}^{d})$ if and only if $f_{i}\\,\\in\\,W^{1,\\infty}(\\Omega)$ for each $i=1,2,\\ldots,d$ and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Vert f\\Vert_{W^{1,\\infty}(\\Omega,\\mathbb{R}^{d})}:=\\operatorname*{max}_{i=1,\\ldots,d}\\{\\Vert f_{i}\\Vert_{W^{1,\\infty}(\\Omega)}\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "When $p=2$ , denote $W^{n,2}(\\Omega)$ as $H^{n}(\\Omega)$ for $n\\in\\mathbb{N}_{+}$ . ", "page_idx": 17}, {"type": "text", "text": "Proposition 1 ([30]). Suppose $\\sigma$ is a is a continuous non-polynomial function and $K$ is a compact in $\\bar{\\mathbb{R}}^{d}$ , then there are positive integers $p$ , constants $w_{k},\\zeta_{k}$ for $k\\,=\\,1,\\ldots,p$ and bounded linear functionals $c_{k}:H^{r}(K)\\to\\mathbb{R}$ such that for any $v\\in H^{r}(K)$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\|v-\\sum_{k=1}^{p}c_{k}(v)\\sigma\\left(\\pmb{w}_{k}\\cdot\\pmb{x}+\\zeta_{k}\\right)\\right\\|_{L^{2}(K)}\\leq c p^{-r/d}\\|v\\|_{H^{r}(K)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proposition 2 ([32, 44]). Suppose $\\sigma$ is a continuous non-polynomial function and $\\Omega$ is a compact subset of $\\mathbb{R}^{d}$ . For any Lipschitz-continuous function $f$ , there exists a shallow neural network such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\|f-\\sum_{j=1}^{m}a_{j}\\pmb{\\sigma}\\left(\\pmb{\\omega}_{j}\\cdot\\pmb{x}+b_{j}\\right)\\right\\|_{\\infty}\\leq C m^{-1/d},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $C$ depends on the Lipschitz constant but is independent of $m$ . ", "page_idx": 17}, {"type": "text", "text": "Lemma 1 ([22]). The $\\epsilon$ -covering number of $[-B,B]^{d},\\,K(\\epsilon)$ , satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\nK(\\epsilon)\\leqslant\\left(\\frac{C B}{\\epsilon}\\right)^{d},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for some constant $C>0$ , independent of \u03f5, $B$ , and $d$ . ", "page_idx": 17}, {"type": "text", "text": "Step 5: Now we estimate ", "page_idx": 17}, {"type": "text", "text": "B.2 Proof of Theorem 1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this subsection, we present the proof of Theorem 1, which describes the approximation ability of DeepONet. ", "page_idx": 17}, {"type": "text", "text": "Proof of Theorem 1. Step 1: Firstly, we need to verify that the target operator $\\mathcal G(u)$ is well-defined.   \nDue to Assumption 1 (i), we know that for $u\\in\\mathcal{X}\\subset H^{2}(\\Omega)$ , Equation (2) will have unique solutions.   \nThis means that $\\mathcal G(u)$ is a well-defined operator for the input space $u\\in\\mathscr{X}$ . ", "page_idx": 17}, {"type": "text", "text": "Step 2: Secondly, we aim to verify that $\\mathcal G(u)$ is a Lipschitz-continuous operator in $H^{2}(\\Omega)$ for $u\\in\\mathscr{X}$ . Consider the following: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{f^{\\prime}(u+v)=f^{\\prime}(u)+v f^{\\prime\\prime}(\\xi_{1})}}\\\\ {{f(u+v)=f(u)+v f^{\\prime}(\\xi_{2})}}\\\\ {{\\delta_{v}u(\\pmb{x})=\\delta u(\\pmb{x})+\\epsilon(\\pmb{x})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\delta u({\\pmb x})$ is the solution of Eq.(2) for the input $u$ , and $\\delta_{v}u({\\pmb x})$ is the solution of Eq.(2) for the input $u+v$ . Denote ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\delta_{v}u({\\pmb x})-\\delta u({\\pmb x})=:\\epsilon({\\pmb x}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{(\\mathcal{L}-f^{\\prime}(u+v))\\epsilon(x)=\\Delta v-v(f^{\\prime}(\\xi_{2})+f^{\\prime\\prime}(\\xi_{1})\\delta u),\\right.\\right.\\ \\ x\\in\\Omega}\\\\ {\\left.\\left.\\epsilon(x)=0,\\right.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $u$ and $v$ are in $H^{2}$ and $\\partial\\Omega$ is in $C^{2}$ (Assumption 1 (iii)), according to [6, Theorem 4 in Section 6.3], there exist constants $C^{\\,2}$ and $\\bar{C}$ such that: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\epsilon({\\pmb x})\\|_{H^{2}(\\Omega)}\\leq C\\|\\mathcal{L}{\\boldsymbol v}-{\\boldsymbol v}(f^{\\prime}(\\xi_{2})+f^{\\prime\\prime}(\\xi_{1})\\delta{\\boldsymbol u})\\|_{L^{2}(\\Omega)}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\bar{C}\\|{\\boldsymbol v}({\\pmb x})\\|_{H^{2}(\\Omega)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The last inequality is due to the boundedness of $f^{\\prime}(\\xi_{2})+f^{\\prime\\prime}(\\xi_{1})\\delta u$ (Assumption 1 (ii)). ", "page_idx": 18}, {"type": "text", "text": "Step 3: In the approximation, we first reduce the operator learning to functional learning. ", "page_idx": 18}, {"type": "text", "text": "When the input function $u({\\boldsymbol{x}})$ belongs to $\\mathcal{X}\\subset H^{2}$ , the output function $\\delta u$ also belongs to $H^{2}$ , provided that $\\partial\\Omega$ is of class $C^{2}$ . The function $\\mathcal{G}(u)\\,=\\,\\delta u$ can be approximated by a two-layer network architected by the activation function $\\sigma(x)$ , which is not a polynomial, in the following form by Proposition 1 [30] (given in Subsection 16): ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\|\\b{\\mathcal{G}}(u)(\\pmb{x})-\\sum_{k=1}^{p}c_{k}[\\mathscr{G}(u)]\\sigma\\left(\\pmb{w}_{k}\\cdot\\pmb{x}+\\zeta_{k}\\right)\\right\\|_{L^{2}(\\Omega)}\\leq C_{1}p^{-\\frac2d}\\|\\mathscr{G}(u)\\|_{H^{2}(\\Omega)},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\pmb{w}_{k}\\,\\in\\,\\mathbb{R}^{d}$ , $\\zeta_{k}\\,\\in\\,\\mathbb{R}$ for $k\\,=\\,1,\\ldots,p$ , $c_{k}$ is a continuous functional, and $C_{1}$ is a constant independent of the parameters. ", "page_idx": 18}, {"type": "text", "text": "Denote $\\phi_{k}(u)=c_{k}[\\mathcal{G}(u)]$ , which is a Lipschitz-continuous functional from $H^{2}(\\Omega)$ to $\\mathbb{R}$ , which is due to $\\mathcal{G}$ is a Lipschitz-continuous operator and $c_{k}$ is a linear functional. The remaining task in approximation is to approximate these functionals by neural networks. ", "page_idx": 18}, {"type": "text", "text": "Step 4: In this step, we reduce the functional learning to function learning by applying the operator $\\mathcal{P}$ as in Assumption 1 (iv). ", "page_idx": 18}, {"type": "text", "text": "Based on $\\phi_{k}(u)$ being a Lipschitz-continuous functional in $H^{2}(\\Omega)$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n|\\phi_{k}(u)-\\phi_{k}(\\mathcal{P}u)|\\leq L_{k}\\|u-\\mathcal{P}u\\|_{H^{2}(\\Omega)}\\leq L_{k}\\epsilon,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $L_{k}$ is the Lipschitz constant of $\\phi_{k}(u)$ for $u\\in\\mathscr{X}$ . ", "page_idx": 18}, {"type": "text", "text": "Furthermore, since $\\mathcal{P}u$ is an $n$ -dimensional term, i.e., it can be denoted by the $n$ -dimensional vector $\\bar{\\mathcal{P}}u\\in\\mathbb{R}^{n}$ , we can rewrite $\\phi_{k}(\\mathcal{P}u)$ as $\\psi_{k}(\\bar{\\mathcal{P}}u)$ , where $\\psi_{k}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ for $l=1,\\hdots,p$ . Furthermore, $\\psi_{k}$ is a Lipschitz-continuous function since $\\phi_{k}$ is Lipschitz-continuous and $\\mathcal{P}$ is a continuous linear operator. ", "page_idx": 18}, {"type": "text", "text": "Step 5: In this step, we will approximate $\\psi_{k}$ using shallow neural networks. ", "page_idx": 18}, {"type": "text", "text": "Due to Proposition 2, we have that there is a shallow neural network such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\|\\psi_{k}({\\bar{\\mathcal{P}}}u)-A_{k}\\sigma\\left(M_{k}\\cdot{\\bar{\\mathcal{P}}}u+b_{k}\\right)\\right\\|_{\\infty}\\leq C m^{-1/d},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\pmb{a}_{k}^{\\sf T}\\in\\mathbb{R}^{m}$ , $M_{k}\\in\\mathbb{R}^{m\\times n}$ , and $b_{k}\\in\\mathbb{R}^{m}$ . For the simplicity notations, we can replace $M_{k}\\cdot\\bar{\\mathcal{P}}$ by an operator $\\mathcal{W}_{k}$ . ", "page_idx": 18}, {"type": "text", "text": "Above all, we have that there is a neural network in $\\Xi_{p}$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{k=1}^{p}A_{k}\\sigma\\left(\\mathcal{W}_{k}u+b_{k}\\right)\\sigma\\left(w_{k}\\cdot x+\\zeta_{k}\\right)-\\mathcal{G}(u)\\right\\|_{L^{2}(\\Omega)}\\le C_{1}m^{-\\frac1n}+C_{2}(\\epsilon+p^{-\\frac2d}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $C_{1}$ is a constant independent of $m,\\epsilon,$ , and $p$ , $C_{2}$ is a constant depended on $p$ . ", "page_idx": 18}, {"type": "text", "text": "Remark 3. We want to emphasize the reasonableness of our assumptions. For condition (i), we are essentially restricting our approximation efforts to local regions. This limitation is necessary because attempting to approximate the neural operator across the entire domain could lead to issues, particularly in cases where multiple solutions exist. Consider a scenario where the input function u lies between two distinct solutions. Even a small perturbation of u could result in the system converging to a completely different solution. Condition (i) ensures that Equation (2) has a unique solution, allowing us to focus our approximation efforts within localized domains. ", "page_idx": 19}, {"type": "text", "text": "Conditions (ii) and (iii) serve to regularize the problem and ensure its tractability. These conditions are indeed straightforward to fulfill, contributing to the feasibility of the overall approach. ", "page_idx": 19}, {"type": "text", "text": "For the embedding operator $\\mathcal{P}$ in $(i\\nu)$ , there are a lot of choices in DeepONet, such as finite element methods like Argyris elements [2] or embedding methods in [23, 18]. We will discuss more in the appendix. We omit the detailed discussion in the paper. Furthermore, for the differential neural network, this embedding may be different; for example, we can use Fourier expansion [43] or multigrid methods [16] to achieve this task. ", "page_idx": 19}, {"type": "text", "text": "Here, we discuss more about the embedding operator $\\mathcal{P}$ . One approach is to use the Argyris element [2]. This method involves considering the 21 degrees of freedom shown in Fig. 7. In this figure, each \u2022 denotes evaluation at a point, the inner circle represents an evaluation of the gradient at the center, and the outer circle denotes evaluation of the three-second derivatives at the center. The arrows indicate the evaluation of the normal derivatives at the three midpoints. ", "page_idx": 19}, {"type": "image", "img_path": "F9mNL6vR27/tmp/beb868af14f9e742aa7e1bd5b5643257b7adc325702d8db29d7da50c0f0e6236.jpg", "img_caption": ["Figure 7: Argyris method "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Another alternative approach to discretizing the input space is to use the bi-cubic Hermite finite element method [23, 18]. ", "page_idx": 19}, {"type": "text", "text": "B.3 Proof of Theorem 2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The proof of Theorem 2 is inspired by that in [22]. ", "page_idx": 19}, {"type": "text", "text": "Proof of Theorem 2. Step 1: To begin with, we introduce a new term called the middle term, denoted as $\\mathcal{E}_{S m}(\\pmb{\\theta})$ , defined as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{E}_{S m}(\\pmb{\\theta}):=\\frac{1}{M_{u}}\\sum_{j=1}^{M_{u}}\\int_{\\Omega}\\left|\\mathcal{G}(u_{j})(\\pmb{x})-\\mathcal{O}(u_{j};\\pmb{\\theta})(\\pmb{x})\\right|^{2}\\mathrm{d}\\pmb{x},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This term represents the limit case of $\\mathcal{E}_{S}(\\pmb{\\theta})$ as the number of samples in the domain of the output space tends to infinity $M_{x}\\rightarrow\\infty)$ ). ", "page_idx": 19}, {"type": "text", "text": "Then the error can be divided into two parts: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathbb{E}(\\mathcal{E}_{S}(\\pmb{\\theta})-\\mathcal{E}_{S c}(\\pmb{\\theta})|\\leq|\\mathbb{E}(\\mathcal{E}_{S}(\\pmb{\\theta})-\\mathcal{E}_{S m}(\\pmb{\\theta})|+|\\mathbb{E}(\\mathcal{E}_{S m}(\\pmb{\\theta})-\\mathcal{E}_{S c}(\\pmb{\\theta})|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Step 2: For $\\vert\\mathbb{E}(\\mathcal{E}_{S m}(\\pmb{\\theta})-\\mathcal{E}_{S c}(\\pmb{\\theta})\\vert$ , this is the classical generalization error analysis, and the result can be obtained from [34, 42, 41]. We omit the details of this part, which can be expressed as ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\mathbb{E}(\\xi_{S m}(\\pmb{\\theta})-\\xi_{S c}(\\pmb{\\theta}))|\\leq\\frac{C d_{\\pmb{\\theta}}\\sqrt{\\log{M_{x}}}}{\\sqrt{M_{x}}},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $C$ is independent of the number of parameters $d_{\\theta}$ and the sample size $M_{x}$ . In the following steps, we are going to estimate $\\left|\\mathbb{E}(\\mathcal{E}_{S}(\\pmb{\\theta})-\\mathcal{\\bar{E}}_{S m}(\\pmb{\\theta}))\\right|$ , which is the error that comes from the sampling of the input space of the operator. ", "page_idx": 20}, {"type": "text", "text": "Step 3: Denote ", "page_idx": 20}, {"type": "equation", "text": "$$\nS_{\\pmb\\theta}^{M}:=\\frac{1}{M}\\sum_{j=1}^{M}\\int_{\\Omega}\\left|\\mathscr{G}(u_{j})(\\pmb x)-\\mathscr{O}(u_{j};\\pmb\\theta)(\\pmb x)\\right|^{2}\\mathrm d\\pmb x.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We first estimate the gap between $S_{\\theta}^{M}$ and $S_{\\theta^{\\prime}}^{M}$ for any bounded parameters $\\theta,\\theta^{\\prime}$ . Due to Assumption 2 (i) and (ii), we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~|S_{\\theta}^{M}-S_{\\theta^{\\prime}}^{M}|}\\\\ &{\\le\\displaystyle\\frac{1}{M}\\displaystyle\\sum_{j=1}^{M}\\left|\\int_{\\Omega}|\\mathcal{G}(u_{j})(x)-\\mathcal{O}(u_{j};\\theta)(x)|^{2}-|\\mathcal{G}(u_{j})(x)-\\mathcal{O}(u_{j};\\theta^{\\prime})(x)|^{2}\\,\\mathrm{d}x\\right|}\\\\ &{\\le\\displaystyle\\frac{1}{M}\\displaystyle\\sum_{j=1}^{M}\\left|\\int_{\\Omega}|2\\mathcal{G}(u_{j})(x)+\\mathcal{O}(u_{j};\\theta)(x)+\\mathcal{O}(u_{j};\\theta^{\\prime})(x)|\\cdot|\\mathcal{O}(u_{j};\\theta)(x)-\\mathcal{O}(u_{j};\\theta^{\\prime})(x)|\\,\\mathrm{d}x\\right|}\\\\ &{\\le\\displaystyle\\frac{4}{M}\\displaystyle\\sum_{j=1}^{M}\\Psi(u_{j})\\Phi(u_{j})\\cdot\\|\\theta-\\theta^{\\prime}\\|_{\\ell^{\\infty}}\\cdot}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Step 4: Based on Step 3, we are going to estimate ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{sup}_{\\pmb{\\theta}\\in[-B,B]^{d_{\\pmb{\\theta}}}}\\left|S_{\\pmb{\\theta}}^{M}-\\mathbb{E}S_{\\pmb{\\theta}}^{M}\\right|^{p}\\right]^{\\frac{1}{p}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "by covering the number of spaces. ", "page_idx": 20}, {"type": "text", "text": "Set $\\{\\pmb{\\theta}_{1},\\dotsc,\\pmb{\\theta}_{K}\\}$ is a $\\varepsilon$ -covering of $[-B,B]^{d_{\\theta}}$ i.e. for any $\\pmb{\\theta}$ in $[-B,B]^{d_{\\theta}}$ , there exists $j$ with $\\|\\pmb{\\theta}-\\pmb{\\theta}_{j}\\|_{\\ell_{\\infty}}\\leqslant\\epsilon$ . Then we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\underset{\\theta\\in[-B,B]^{d}}{\\operatorname*{sup}}\\big|S_{\\theta}^{M}-\\mathbb{E}\\left[S_{\\theta}^{M}\\right]\\big|^{p}\\right]^{1/p}}\\\\ &{\\leq\\!\\mathbb{E}\\Bigg[\\Bigg(\\underset{\\theta\\in[-B,B]^{d}}{\\operatorname*{sup}}\\Big|S_{\\theta}^{M}-S_{\\theta}^{M}\\Big|+\\Big|S_{\\theta_{j}}^{M}-\\mathbb{E}\\left[S_{\\theta_{j}}^{M}\\right]\\Big|~+\\left|\\mathbb{E}\\left[S_{\\theta_{j}}^{M}\\right]-\\mathbb{E}\\left[S_{\\theta}^{M}\\right]\\right|\\Big)^{p}\\Bigg]^{1/p}}\\\\ &{\\leq\\!\\mathbb{E}\\left[\\bigg(\\underset{j=1,\\ldots,K}{\\operatorname*{suc}}\\left|S_{\\theta_{j}}^{M}-\\mathbb{E}\\left[S_{\\theta_{j}}^{M}\\right]\\right|~+\\frac{8\\epsilon}{M}\\left(\\underset{j=1}{\\overset{N}{\\sum}}|\\Psi(u_{j})|\\,|\\,\\Phi(u_{j})|\\right)\\bigg)\\right]^{p}\\Bigg]^{1/p}}\\\\ &{\\leq\\!\\!8\\!\\in\\!\\mathbb{E}\\left[|\\Psi(u_{j})||\\Phi(u_{j})|^{p}\\right]^{1/p}+\\mathbb{E}\\left[\\underset{j=1,\\ldots,K}{\\operatorname*{max}}\\left|S_{\\theta_{j}}^{M}-\\mathbb{E}\\left[S_{\\theta_{j}}^{M}\\right]\\right|^{p}\\right]^{1/p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For $8\\epsilon\\mathbb{E}\\left[|\\Psi\\Phi|^{p}\\right]^{1/p}$ , it can be approximate by ", "page_idx": 20}, {"type": "equation", "text": "$$\n8\\epsilon\\mathbb{E}\\left[|\\Psi\\Phi|^{p}\\right]^{1/p}\\leqslant8\\epsilon\\mathbb{E}\\left[|\\Psi|^{2p}\\right]^{1/2p}\\mathbb{E}\\left[|\\Phi|^{2p}\\right]^{1/2p}=8\\epsilon\\|\\Psi\\|_{L^{2p}}\\|\\Phi\\|_{L^{2p}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Fo $\\mathbb{\\cdot E}\\left[\\operatorname*{max}_{j=1,\\dots,K}\\left|S_{\\pmb\\theta_{j}}^{M}-\\mathbb{E}\\left[S_{\\pmb\\theta_{j}}^{M}\\right]\\right|^{p}\\right]^{1/p}$ , by applied the result in [37, 22], we know ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{max}_{j=1,\\dots,K}\\left|S_{\\pmb{\\theta}_{j}}^{M}-\\mathbb{E}\\left[S_{\\pmb{\\theta}_{j}}^{M}\\right]\\right|^{p}\\right]^{1/p}\\leq\\frac{16K^{1/p}\\sqrt{p}\\|\\Psi\\|_{L^{2p}}^{2}}{\\sqrt{M}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Step 5: Now we estimate $\\vert\\mathbb{E}(\\mathcal{E}_{S}(\\pmb{\\theta})-\\mathcal{E}_{S m}(\\pmb{\\theta})\\vert$ . ", "page_idx": 21}, {"type": "text", "text": "Due to Assumption 2 and directly calculation, we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\Psi\\|_{L^{2p}},\\|\\Phi\\|_{L^{2p}}\\leqslant C(1+\\gamma\\kappa p)^{\\kappa},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for constants $C,\\gamma>0$ , depending only the measure $\\mu$ and the constant $C$ appearing in the upper bound (7). For example, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\Psi\\|_{L^{2p}}\\leq\\left(\\displaystyle\\int_{\\chi}C\\left(1+\\|u\\|_{H^{2}(\\Omega)}\\right)^{2p\\kappa}\\mathrm{d}\\mu_{\\mathcal{X}}\\right)^{\\frac{1}{2p}}}\\\\ &{\\leq\\!C\\left(\\displaystyle\\int_{\\chi}\\exp\\left[2p\\kappa\\ln\\left(1+\\|u\\|_{H^{2}(\\Omega)}\\right)-\\alpha\\|u\\|_{H^{2}(\\Omega)}\\right]e^{\\alpha\\|u\\|_{H^{2}(\\Omega)}}\\mathrm{d}\\mu_{\\mathcal{X}}\\right)^{\\frac{1}{2p}}}\\\\ &{\\leq\\!C\\left(\\displaystyle\\int_{\\chi}\\left(1+\\frac{\\kappa p}{\\alpha}\\right)^{2\\kappa p}e^{\\alpha\\|u\\|_{H^{2}(\\Omega)}}\\mathrm{d}\\mu_{\\mathcal{X}}\\right)^{\\frac{1}{2p}}\\leq C(1+\\gamma\\kappa p)^{\\kappa}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Based on Lemma 1, we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\operatorname*{sup}_{\\theta\\in[-B,B]^{d_{\\theta}}}\\left\\vert S_{\\theta}^{M_{u}}-\\mathbb{E}\\left[S_{\\theta}^{M_{u}}\\right]\\right\\vert^{p}\\right]^{1/p}\\leqslant16C^{2}(1+\\gamma\\kappa p)^{2\\kappa}\\left(\\epsilon+\\left(\\frac{C B}{\\epsilon}\\right)^{d_{\\theta}/p}\\frac{\\sqrt{p}}{\\sqrt{M_{u}}}\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for some constants $C,\\gamma\\,>\\,0$ , independent of $\\kappa,\\mu,B,d_{\\pmb\\theta},N,\\epsilon\\,>\\,0$ and $p\\geqslant2$ . We now choose $\\begin{array}{r}{\\epsilon=\\frac{1}{\\sqrt{M_{u}}}}\\end{array}$ \u221aMu , so that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\epsilon+\\left(\\frac{C B}{\\epsilon}\\right)^{d_{\\theta}/p}\\frac{\\sqrt{p}}{\\sqrt{M_{u}}}=\\frac{1}{\\sqrt{M_{u}}}\\left(1+(C B\\sqrt{M_{u}})^{d_{\\theta}/p}\\sqrt{p}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Next, let $p=d_{\\theta}\\log(C B\\sqrt{M_{u}})$ . Then, ", "page_idx": 21}, {"type": "equation", "text": "$$\n(C B\\sqrt{M_{u}})^{d_{\\theta}/p}\\sqrt{p}=\\exp\\left(\\frac{\\log(C B\\sqrt{M_{u}})d_{\\theta}}{p}\\right)\\sqrt{p}=e\\sqrt{d_{\\theta}\\log(C B\\sqrt{M_{u}})},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and thus we conclude that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\epsilon+\\left(\\frac{C B}{\\epsilon}\\right)^{d_{\\theta}/p}\\frac{\\sqrt{p}}{\\sqrt{M_{u}}}\\leqslant\\frac{1}{\\sqrt{M_{u}}}\\left(1+e\\sqrt{d_{\\theta}\\log(C B\\sqrt{M_{u}})}.\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "On the other hand, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n(1+\\gamma\\kappa p)^{2\\kappa}=\\Bigl(1+\\gamma\\kappa d_{\\theta}\\log(C B\\sqrt{M_{u}})\\Bigr)^{2\\kappa}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Increasing the constant $C>0$ , if necessary, we can further estimate ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\overset{\\prime}{\\underset{\\mathrm{\\Large~1~+~}}{\\sim}}\\gamma\\kappa d_{\\theta}\\log\\left(C B\\sqrt{M_{u}}\\right)\\overset{2\\kappa}{\\underset{\\mathrm{\\Large~2~}}{\\sim}}\\left(1+e\\sqrt{d_{\\theta}\\log\\left(C B\\sqrt{M_{u}}\\right)}.\\right)\\leqslant C\\left(1+d_{\\theta}\\log(C B\\sqrt{M_{u}})\\right)^{2\\kappa+1/2},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $C>0$ depends on $\\kappa,\\gamma,\\mu$ and the constant appearing in (7), but is independent of $d\\theta,B$ and $N$ . We can express this dependence in the form $C=C(\\mu,\\Psi,\\Phi)>0$ , as the constants $\\kappa$ and $\\gamma$ depend on the Gaussian tail of $\\mu$ and the upper bound on $\\Psi,\\Phi$ . ", "page_idx": 21}, {"type": "text", "text": "Therefore, ", "text_level": 1, "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathbb{E}(\\mathcal{E}_{S}(\\theta)-\\mathcal{E}_{S m}(\\theta)|\\leq\\mathbb{E}\\operatorname*{sup}_{\\theta\\in[-B,B]^{d_{\\theta}}}|S_{\\theta}^{M_{u}}-\\mathbb{E}\\left[S_{\\theta}^{M_{u}}\\right]|\\leq C\\mathbb{E}\\left[\\operatorname*{sup}_{\\theta\\in[-B,B]^{d_{\\theta}}}\\left|S_{\\theta}^{M_{u}}-\\mathbb{E}\\left[S_{\\theta}^{M_{u}}\\right]\\right|^{p}\\right]^{1/2}}&\\\\ &{\\qquad\\qquad\\qquad\\leq\\cfrac{C}{\\sqrt{M_{u}}}\\left(1+C d_{\\theta}\\log(C B\\sqrt{M_{u}})^{2\\kappa+1/2}\\right).}&{(26)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect our contributions and scope. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Our paper discusses the limitations of our work in the section where we establish and prove our network\u2019s approximation ability. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: For each theoretical result, we provide the full set of assumptions and a complete (and correct) proof. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide the code and dataset used in the experiments as described in reproducing statement. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide the link to the datasets. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide the detailed configurations of the experiments in the Appendix. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We report statistical significance for key findings in our study. Since we do not claim our method is superior to other models, our experiments are serving to justify our theoretical claims, therefore, we selectively provide statistical results where they are most relevant. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We report the compute resources for benchmarking our method. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We read the NeurIPS Code of Ethics and confirm our research conform the code. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The research is mostly theoretical. We do not see obvious social impact. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 25}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: No obvious such risks. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA]   \nJustification:   \nGuidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}]