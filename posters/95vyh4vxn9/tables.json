[{"figure_path": "95VyH4VxN9/tables/tables_6_1.jpg", "caption": "Table 1: Perception results. We report the BEV segmentation IoU (%) of intermediate representations and their mean value.", "description": "This table presents a comparison of the performance of various methods on the task of Bird's Eye View (BEV) segmentation.  It shows the Intersection over Union (IoU) scores achieved by different methods for four different categories: Drivable Area, Lane, Vehicle, and Pedestrian.  The \"Spike\" column indicates whether the method utilizes spiking neural networks (SNNs). The \"Avg.\" column shows the average IoU across all four categories.  The table demonstrates the performance of the proposed Spiking Autonomous Driving (SAD) method in comparison with several state-of-the-art approaches.", "section": "4 Experimental Results on nuScenes"}, {"figure_path": "95VyH4VxN9/tables/tables_7_1.jpg", "caption": "Table 2: Prediction results. We report semantic segmentation IoU (%) and instance segmentation metrics from the video prediction area. The static method assumes all obstacles static in the prediction horizon.", "description": "This table presents a comparison of different methods for video prediction, including a static method (assuming all obstacles are static) and several dynamic methods (FIERY, ST-P3, and SAD). The evaluation metrics include semantic segmentation IoU, and instance segmentation metrics (PQ, SQ, RQ).  The table shows the performance of each method across these metrics, indicating the accuracy of predicting future states of the scene.", "section": "4.1 Experimental Results on nuScenes"}, {"figure_path": "95VyH4VxN9/tables/tables_7_2.jpg", "caption": "Table 3: Planning results.", "description": "This table presents the quantitative results of the planning module, comparing the proposed SAD model to several state-of-the-art methods.  It shows the L2 error (in meters) at 1, 2, and 3 seconds into the future, the collision rate (in percentage) at 1, 2, and 3 seconds, and the energy consumed (in millijoules) for the entire planning process.  The metrics evaluate the accuracy and safety of the planned trajectory.  The 'Spike' column indicates whether the method utilizes spiking neural networks.", "section": "4.1 Experimental Results on nuScenes"}, {"figure_path": "95VyH4VxN9/tables/tables_7_3.jpg", "caption": "Table 4: Ablation Study on different modules for the encoder and the decoder on Planning tasks.", "description": "This table presents the ablation study results on different encoder and decoder modules for planning tasks.  It shows the impact of using different architectures (MS-ResNet vs. SEW-ResNet), single vs. dual pathways in the prediction module, and resulting performance metrics (PQ, SQ, RQ).  The checkmarks (\u2713) indicate which module configurations were used in each row of the experiment.", "section": "4.3 Ablation Study"}, {"figure_path": "95VyH4VxN9/tables/tables_7_4.jpg", "caption": "Table 5: Ablation study on different timestep alignment strategies for the encoder and decoder on perception tasks.", "description": "This table presents the ablation study results on different timestep alignment strategies applied to the encoder and decoder for perception tasks in the Spiking Autonomous Driving (SAD) model. It shows the impact of using Sequence Repetition (SR) or Sequential Alignment (SA) for both the encoder and decoder, and also the effect of removing temporal processing entirely from the decoder ('w/o T'). The results are evaluated in terms of Intersection over Union (IoU) for Drivable Area, Lane, Vehicle, and Pedestrian segmentation tasks. The table demonstrates how different timestep strategies affect the model's performance in each task and overall.", "section": "4.3 Ablation Study"}, {"figure_path": "95VyH4VxN9/tables/tables_17_1.jpg", "caption": "Table 6: Performance on ImageNet-1K [67]. Note, \u201cSpike\u201d, \u201cPara\u201d, and \u201cStep\u201d denote \u201cSpike-driven\u201d, \u201cParameters\u201d, and \u201cTimestep\u201d.", "description": "This table compares the performance of various models (ANN2SNN, SCNN, Spiking Transformer, and SMLP) on the ImageNet-1K dataset.  It shows the architecture used, whether the model was spike-driven, the number of parameters (in millions), the number of timesteps used in the model, and the achieved accuracy (%).  The table highlights the performance of the proposed STM (Spiking Token Mixer) model within the context of other state-of-the-art SNN models.", "section": "D.1 Pretraining on Spiking Token Mixer (STM)"}]