[{"figure_path": "SXbyy0a3rY/figures/figures_0_1.jpg", "caption": "Figure 1: Spatially grounded images generated by our GROUNDIT. Each image is generated based on a text prompt along with bounding boxes, which are displayed in the upper right corner of each image. Compared to existing methods that often struggle to accurately place objects within their designated bounding boxes, our GROUNDIT enables more precise spatial control through a novel noisy patch transplantation mechanism.", "description": "This figure shows examples of images generated by the proposed method, GROUNDIT. Each image is accompanied by a text prompt and bounding boxes indicating the desired locations of the objects mentioned in the prompt.  The figure highlights the method's ability to precisely place objects within their designated bounding boxes, a significant improvement over existing methods.", "section": "Abstract"}, {"figure_path": "SXbyy0a3rY/figures/figures_3_1.jpg", "caption": "Figure 2: A single denoising step in GROUNDIT consists of two stages. The Global Update (Sec. 5.1) established coarse spatial grounding by updating the noisy image with a custom loss function. Then, the Local Update (Sec. 5.3) further provides fine-grained spatial control over individual bounding boxes through a novel technique called noisy patch transplantation.", "description": "The figure illustrates a single denoising step in the GROUNDIT framework, which involves two stages: a global update and a local update. The global update utilizes cross-attention maps to perform coarse spatial grounding, while the local update employs noisy patch transplantation for fine-grained spatial control over individual bounding boxes. This process leverages DiT's semantic sharing property to improve accuracy in generating images that precisely align with user-specified bounding boxes.", "section": "5 GROUNDIT: Grounding Diffusion Transformers"}, {"figure_path": "SXbyy0a3rY/figures/figures_5_1.jpg", "caption": "Figure 3: (A) Joint Denoising. Two different noisy images, xt and yt, are each assigned positional embeddings based on their respective sizes. The two sets of image tokens are then merged and passed through DiT for a denoising step. Afterward, the denoised tokens are split back into xt\u22121 and yt-1. (B), (C) Semantic Sharing. Denoising two noisy images using joint denoising results in semantically correlated content between the generated images. Here, y indicates that joint denoising is during the initial 100% of the timesteps, after which the images are denoised for the remaining steps.", "description": "This figure demonstrates the concept of joint denoising and semantic sharing in Diffusion Transformers. (A) shows the process of jointly denoising two noisy images of potentially different sizes using DiT by merging their image tokens, processing them through the DiT, and then splitting the denoised tokens back into the two images. (B) and (C) illustrate the effect of semantic sharing, where the joint denoising of two images, initially with different random noise, leads to increasingly similar generated images as the proportion of joint denoising steps (indicated by \u03b3) increases. This effect is observed for images with equal resolutions (B) and different resolutions (C), highlighting the flexibility and intriguing property of semantic sharing in DiT.", "section": "5.2 Semantic Sharing in Diffusion Transformers"}, {"figure_path": "SXbyy0a3rY/figures/figures_7_1.jpg", "caption": "Figure 4: Qualitative comparisons between our GROUNDIT and baselines. Leftmost column shows the input bounding boxes, and columns 2-6 include the baseline results. The rightmost column includes the results of our GROUNDIT.", "description": "This figure presents a qualitative comparison of GROUNDIT against other state-of-the-art training-free spatial grounding methods for text-to-image generation. Each row displays the results for a given text prompt and set of bounding boxes.  The leftmost column shows the input bounding boxes provided as spatial constraints. Subsequent columns present results from various baseline methods, such as Layout Guidance, Attention Refocusing, BoxDiff, and R&B. The final column shows the results produced by the proposed GROUNDIT method.  The comparison highlights how GROUNDIT consistently and more accurately places objects within the designated bounding boxes, particularly when dealing with more complex scenes or a greater number of bounding boxes.", "section": "Results"}, {"figure_path": "SXbyy0a3rY/figures/figures_13_1.jpg", "caption": "Figure 5: Spatially grounded images generated by our GROUNDIT with varying aspect ratios and sizes. Each image is generated based on a text prompt along with bounding boxes, which are displayed next to (or below) each image.", "description": "This figure showcases the versatility of the GROUNDIT model in generating images with diverse aspect ratios and sizes, all while accurately placing objects within their specified bounding boxes.  Each image is accompanied by a text prompt and its corresponding bounding boxes, highlighting the model's ability to maintain precise spatial control regardless of the image's dimensions.", "section": "Appendix"}, {"figure_path": "SXbyy0a3rY/figures/figures_13_2.jpg", "caption": "Figure 5: Spatially grounded images generated by our GROUNDIT with varying aspect ratios and sizes. Each image is generated based on a text prompt along with bounding boxes, which are displayed next to (or below) each image.", "description": "This figure shows several examples of images generated by the GROUNDIT model. Each image is accompanied by a text prompt and bounding boxes indicating the desired location of each object. The key aspect highlighted is the ability of GROUNDIT to handle images with various aspect ratios and sizes, demonstrating its flexibility and robustness in spatial grounding.", "section": "5.3 Local Update with Noisy Patch Transplantation"}, {"figure_path": "SXbyy0a3rY/figures/figures_16_1.jpg", "caption": "Figure 6: Illustration of the generatable resolution range of DiT. The images are generated using PixArt-a [8] from the text prompt \"A dog\", with varying resolutions.", "description": "This figure demonstrates the limited range of resolutions that the Diffusion Transformer (DiT) model can effectively handle.  The experiment uses the text prompt \"A dog\" and generates images at various resolutions (128x128, 256x256, 288x288, 384x384, 512x512).  The results show that while the model can produce reasonable images within a certain resolution range, images generated outside this range (specifically lower resolutions) become significantly less coherent and realistic. This illustrates the concept of 'generatable resolution' for the DiT model, a constraint the paper's method addresses.", "section": "5.2 Semantic Sharing in Diffusion Transformers"}, {"figure_path": "SXbyy0a3rY/figures/figures_16_2.jpg", "caption": "Figure 7: LPIPS score between two generated images with varying \u03b3 value. A gradual decrease in LPIPS [52] indicates that joint denoising progressively enhances the similarity between the generated images.", "description": "This figure shows the effect of joint denoising on the semantic similarity of two generated images. The x-axis represents the number of joint denoising steps, and the y-axis represents the LPIPS score, a measure of perceptual similarity. As the number of joint denoising steps increases (\u03b3 increases from 0 to 1), the LPIPS score decreases, indicating that the generated images become more semantically similar.  This demonstrates the concept of \"semantic sharing\", a key property of Diffusion Transformers exploited in the GROUNDIT model.", "section": "5.2 Semantic Sharing in Diffusion Transformers"}, {"figure_path": "SXbyy0a3rY/figures/figures_16_3.jpg", "caption": "Figure 7: LPIPS score between two generated images with varying \u03b3 value. A gradual decrease in LPIPS [52] indicates that joint denoising progressively enhances the similarity between the generated images.", "description": "This figure shows the results of an experiment to measure the semantic similarity between two images generated using the joint denoising technique described in the paper.  The experiment varies a parameter (\u03b3) that controls the proportion of denoising steps where joint denoising is applied. The x-axis represents the number of joint denoising steps, and the y-axis represents the LPIPS (Learned Perceptual Image Patch Similarity) score, a metric that measures perceptual similarity between images. As \u03b3 increases (more joint denoising), the LPIPS score decreases, indicating that the generated images become increasingly similar. This demonstrates the concept of \"semantic sharing\" where joint denoising causes the two generated images to become semantically correlated.", "section": "5.2 Semantic Sharing in Diffusion Transformers"}, {"figure_path": "SXbyy0a3rY/figures/figures_18_1.jpg", "caption": "Figure 4: Qualitative comparisons between our GROUNDIT and baselines. Leftmost column shows the input bounding boxes, and columns 2-6 include the baseline results. The rightmost column includes the results of our GROUNDIT.", "description": "This figure compares the image generation results of GROUNDIT with several baseline methods for spatially grounded image generation. Each row represents a different text prompt with specified bounding boxes for different objects. The leftmost column displays the bounding boxes provided as input. The following columns show the generated images by different baselines (Layout-Guidance, Attention-Refocusing, BoxDiff, R&B, and PixArt-R&B), and the rightmost column shows the output generated by GROUNDIT. The results demonstrate GROUNDIT's superior ability to accurately place objects within their designated bounding boxes compared to baselines, especially when handling complex scenes with multiple objects and bounding boxes.", "section": "6 Results"}, {"figure_path": "SXbyy0a3rY/figures/figures_19_1.jpg", "caption": "Figure 9: Additional spatially grounded images generated by out GROUNDIT.", "description": "This figure shows more examples of images generated by the proposed GROUNDIT model, showcasing its ability to generate images with multiple objects placed precisely within their corresponding bounding boxes.  The layout of the bounding boxes is shown alongside each generated image to demonstrate the accuracy of the spatial grounding achieved. Each image illustrates a different scene, further highlighting the model's versatility and control over object placement.", "section": "Results"}]