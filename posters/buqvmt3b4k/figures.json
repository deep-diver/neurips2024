[{"figure_path": "buqvMT3B4k/figures/figures_2_1.jpg", "caption": "Figure 1: The sequences of decisions for constructing solutions in a JSP instance with two jobs (J\u2081 and J2) and two machines (identified in green and red). Best viewed in colors.", "description": "This figure illustrates how feasible solutions for the Job Shop Problem (JSP) can be constructed step-by-step.  A JSP instance is represented by a disjunctive graph (left side), showing jobs and their operations on machines. The construction process is visualized as a decision tree, where each path from the root to a leaf represents a complete solution. Each node in the tree represents a decision point, choosing which job operation to schedule next, subject to precedence constraints and machine availability (the cross symbol indicates a completed job).  The colored nodes show which machine is being assigned to a job, and several feasible solutions are depicted at the bottom.", "section": "3.1 Constructing Feasible Job Shop Solutions"}, {"figure_path": "buqvMT3B4k/figures/figures_7_1.jpg", "caption": "Figure 2: GM validation curves when trained with PPO and our SLIM in the same training setting of [54].", "description": "This figure shows the validation curves of a generative model (GM) for the Job Shop Scheduling Problem (JSP) when trained using two different methods: Proximal Policy Optimization (PPO) and the proposed Self-Labeling Improvement Method (SLIM).  The y-axis represents the average percentage gap (PG) on the Taillard benchmark dataset, which measures the difference between the obtained makespan and the optimal makespan. The x-axis shows the training step.  The figure demonstrates that SLIM achieves faster convergence and better performance compared to PPO, as indicated by lower PG values and a steeper descent of the curve. The results highlight the effectiveness of SLIM in training generative models for combinatorial optimization problems.", "section": "6.2 Comparison with Proximal Policy Optimization"}, {"figure_path": "buqvMT3B4k/figures/figures_8_1.jpg", "caption": "Figure 2: GM validation curves when trained with PPO and our SLIM in the same training setting of [54].", "description": "The figure shows the validation curves of a generative model (GM) trained with Proximal Policy Optimization (PPO) and Self-Labeling Improvement Method (SLIM) on the Taillard benchmark.  The x-axis represents training steps, and the y-axis represents the average Percentage Gap (PG). The graph compares the performance of the GM trained with PPO and SLIM, demonstrating that SLIM leads to faster convergence and better final model performance.", "section": "6.2 Comparison with Proximal Policy Optimization"}, {"figure_path": "buqvMT3B4k/figures/figures_9_1.jpg", "caption": "Figure 5: Validation curves obtained by training with SLIM and POMO on random TSP instances with 20 nodes. POMO20 is the best model produced in [30], trained on instances with 20 nodes.", "description": "The figure shows the validation curves for training a neural network model for the Traveling Salesman Problem (TSP) using two different training methods: SLIM (Self-Labeling Improvement Method) and POMO (Policy Optimization with Multiple Optima).  The y-axis represents the average optimality gap on the TSPLIB benchmark, while the x-axis represents the training step.  The curves show that SLIM achieves faster convergence than POMO and reaches a similar level of performance as the best POMO model (POMO20) from a previous study. This demonstrates that SLIM is an effective training strategy for TSP, a challenging combinatorial optimization problem.", "section": "6.5 Self-Labeling the Traveling Salesman Problem"}, {"figure_path": "buqvMT3B4k/figures/figures_17_1.jpg", "caption": "Figure 2: GM validation curves when trained with PPO and our SLIM in the same training setting of [54].", "description": "This figure shows the validation curves of a generative model (GM) trained using two different methods: Proximal Policy Optimization (PPO) and the proposed Self-Labeling Improvement Method (SLIM).  The x-axis represents the training step, and the y-axis represents the average percentage gap (PG) on the Taillard benchmark.  The graph compares the performance of GM trained with PPO and SLIM with different numbers of sampled solutions (\u03b2). The results demonstrate that SLIM leads to faster convergence and produces better final models compared to PPO.", "section": "6.2 Comparison with Proximal Policy Optimization"}]