{"references": [{"fullname_first_author": "S. Arora", "paper_title": "Zoology: Measuring and improving recall in efficient language models", "publication_date": "2023-12-04", "reason": "This paper introduces a novel methodology for evaluating and improving the recall performance of efficient language models, which is directly relevant to the proposed model's design and evaluation in this paper."}, {"fullname_first_author": "K. Bansal", "paper_title": "Holist: An environment for machine learning of higher order logic theorem proving", "publication_date": "2019-00-00", "reason": "This paper presents a dataset and benchmark for machine learning applied to higher-order logic theorem proving, offering a direct comparison point for the current work."}, {"fullname_first_author": "L. Blaauwbroek", "paper_title": "Graph2tac: Online representation learning of formal math concepts", "publication_date": "2024-01-00", "reason": "This recent work explores graph-based representations for formal mathematical concepts, directly addressing the limitations of sequential approaches and providing a relevant comparison point for the proposed architecture."}, {"fullname_first_author": "K. Kogkalidis", "paper_title": "Algebraic positional encodings", "publication_date": "2024-00-00", "reason": "This paper introduces the algebraic positional encoding scheme, a crucial component of the novel neural architecture proposed for representing dependently-typed programs."}, {"fullname_first_author": "A. Katharopoulos", "paper_title": "Transformers are RNNS: Fast autoregressive transformers with linear attention", "publication_date": "2020-00-00", "reason": "This paper introduces the linearized attention mechanism, which is adopted in the proposed architecture to address scaling issues and improve efficiency."}]}