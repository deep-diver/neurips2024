[{"type": "text", "text": "Understanding Representation of Deep Equilibrium Models from Neural Collapse Perspective ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Haixiang Sun Ye Shi \u2217 ShanghaiTech University ShanghaiTech University sunhx@shanghaitech.edu.cn shiye@shanghaitech.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep Equilibrium Model (DEQ), which serves as a typical implicit neural network, emphasizes their memory efficiency and competitive performance compared to explicit neural networks. However, there has been relatively limited theoretical analysis on the representation of DEQ. In this paper, we utilize the Neural Collapse $(\\mathcal{N}\\bar{\\mathcal{C}})$ as a tool to systematically analyze the representation of DEQ under both balanced and imbalanced conditions. ${\\mathcal{N C}}$ is an interesting phenomenon in the neural network training process that characterizes the geometry of class features and classifier weights. While extensively studied in traditional explicit neural networks, the ${\\mathcal{N C}}$ phenomenon has not received substantial attention in the context of implicit neural networks. We theoretically show that ${\\mathcal{N C}}$ exists in DEQ under balanced conditions. Moreover, in imbalanced settings, despite the presence of minority collapse, DEQ demonstrated advantages over explicit neural networks. These advantages include the convergence of extracted features to the vertices of a simplex equiangular tight frame and self-duality properties under mild conditions, highlighting DEQ\u2019s superiority in handling imbalanced datasets. Finally, we validate our theoretical analyses through experiments in both balanced and imbalanced scenarios. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recently, there has been significant research on implicitly-defined layers in neural networks [1, 2, 3, 6, 9, 20, 21, 49], where the output is implicitly mapped from the input under certain conditions. These layers embed interpretability and introduce inductive bias [26] into black-box neural networks, demonstrating superior performance compared to existing explicit layers. ", "page_idx": 0}, {"type": "text", "text": "Among these implicit networks, the Deep Equilibrium Model (DEQ) is a memory-efficient architecture that represents all hidden layers as the equilibrium point of a nonlinear fixed-point equation. Due to the absence of a closed-form solution in its forward process, DEQ can be viewed as having an infinite number of layers during iteration as long as the threshold is set low enough, enhancing its ability to fti input data. Consequently, its representational capacity is relatively stronger compared to a single-layer network structure. This phenomenon explains why DEQ has achieved state-of-the-art results in classification tasks compared to existing architectures like ResNet. For instance, it has been successfully applied to language tasks and image classification tasks, reaching state-of-the-art performance. Additionally, DEQ can be applied in various domains and integrated with numerous other models, including inverse problems [19], Neural ODEs [11], diffusion models [24, 43], Gaussian processes [17], and more. ", "page_idx": 0}, {"type": "text", "text": "However, recent research reveals a phenomenon called Neural Collapse $(\\mathcal{N C})$ concerning the learned deep representations across datasets in image classification tasks [42]. Under the ${\\mathcal{N C}}$ regime, the lastlayer feature of each sample in neural networks collapses to their within-class mean, and the classifier vector converges to a simplex Equiangular Tight Frame (ETF). Theoretical analyses [10, 40, 51] indicate that under the Unconstrained Features Mode (UFM) condition, specific features $H^{0}$ can be isolated from the entire network, known as the layer-peeled model [14]. In this scenario, Neural Collapse $(\\mathcal{N}\\mathcal{C})$ is observed under certain conditions, suggesting that ${\\mathcal{N C}}$ is agnostic to the backbone of feature extraction. Moreover, since ${\\mathcal{N C}}$ measures the degree of proximity between features of the same category, an imbalanced dataset can exert a more negative influence on the performance of ${\\mathcal{N C}}$ . For instance, classes with fewer samples may not separate well and could converge in the same direction, leading to what is known as Minority Collapse [14]. Thus, the ${\\mathcal{N C}}$ metric serves as a valuable indicator of a model\u2019s behavior in the context of imbalanced datasets. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The reasons behind the superior performance of DEQ still lack theoretical proof and comprehensive quantitative analysis. Additionally, to the best of our knowledge, no prior work has integrated DEQ with imbalanced scenarios. In our study, we integrate DEQ with layer-peeled models, add constraints with respect to weights $W_{\\mathrm{DEQ}}$ , and consider the results of fixed-point iteration as the output of DEQ. Therefore, we analyze the performance of ${\\mathcal{N C}}$ in DEQ by continuously deriving the lower bound of the loss function under certain constraints, allowing us to assess how ${\\mathcal{N C}}$ manifests in the training performance of the network. Similarly, we apply the same operations to explicit neural networks for comparison. Our results show that DEQ performs similarly to explicit neural networks under balanced settings. We further extend the dataset to imbalanced conditions and analyze the ${\\mathcal{N C}}$ performance in DEQ, explaining why DEQ tends to outperform explicit neural networks under mild conditions. We systematically analyze performance in terms of feature convergence, distance to the Simplex ETF, and the parallel relationship between extracted features and classifier weights. These analyses uncover the reasons behind the superior performance of DEQ compared to explicit neural networks during training. Additionally, the experimental results in both balanced and imbalanced scenarios validate our theoretical analyses. ", "page_idx": 1}, {"type": "text", "text": "Our main contributions are: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We systematically analyzed the representation of DEQ from the ${\\mathcal{N C}}$ perspective and compared their performance with explicit neural networks. Our theoretical analysis shows that both DEQ and explicit neural networks exhibit the ${\\mathcal{N C}}$ phenomenon in balanced datasets. \u2022 Under imbalanced settings, we theoretically proved the convergence of extracted features to the vertices of a simplex ETF and alignment with classifier weights under certain conditions, demonstrating DEQ\u2019s advantages over explicit neural networks under some mild conditions. \u2022 Experimental results on Cifar-10 and Cifar-100 validated our theoretical findings for distinguishing the differences between DEQ and explicit neural networks. ", "page_idx": 1}, {"type": "text", "text": "2 Background and related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We consider a classification task with $K$ classes. Let $n_{k}$ denote the number of training samples in each class $k$ , and $N=\\sum_{k=1}^{K}n_{k}$ represent the total number of training samples. A traditional neural network can be expressed as a mapping: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\psi({\\pmb x})={\\pmb W}\\phi({\\pmb x})+{\\pmb b},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\phi(\\pmb{x}):\\mathbb{R}^{\\mathrm{in}\\times N}\\rightarrow\\mathbb{R}^{D\\times N}$ is the feature extraction, $W\\in\\mathbb{R}^{K\\times D}$ and $\\pmb{b}\\in\\mathbb{R}^{K}$ are the classifiers and bias in the last layer, respectively. For simplicity, we consider the bias-free case and omit the term $^{b}$ . Besides, we will denote ${\\pmb H}=\\phi({\\pmb x})$ in later sections. ", "page_idx": 1}, {"type": "text", "text": "2.1 Deep Equilibrium Models ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "There have been numerous neural network architectures designed for various practical tasks from different perspectives [17, 32, 38, 39, 48]. DEQ, a typical implicit network [13, 52], incorporates unrolling methods [12, 41], which are devised for training arbitrarily deep networks by integrating all the network layers into one [3, 4, 5, 35, 36, 58]. ", "page_idx": 1}, {"type": "text", "text": "Let $f_{\\theta}(z,x)$ represent a DEQ layer with input $\\textbf{\\em x}$ parameterized by $\\theta$ . When $z^{\\star}$ reaches the equilibrium point, it satisfies: ", "page_idx": 2}, {"type": "equation", "text": "$$\ng_{\\theta}(z^{\\star},\\pmb{x})\\triangleq f_{\\theta}(z^{\\star},\\pmb{x})-z^{\\star}=0.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The forward procedure mostly employs the Broyden solver [7] for iterative solving: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{z}_{t+1}=\\pmb{z}_{t}-\\pmb{B}_{t}^{-1}g_{\\theta}(\\pmb{z}_{t},\\pmb{x}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $B_{t}^{-1}$ refers to the approximation of inverse matrix $\\nabla_{z}^{-1}g_{\\theta}\\big(z_{t},\\pmb{x}\\big)$ , as well as the same parameter $\\theta$ shared across iterations. However, the solution can be quite unstable, and efforts have been made to enhance stability and robustness [34, 44, 55, 56]. Especially, regarding the computation of the inverse matrix, it can be expanded in the form of a Neumann series [18, 60]. Besides, accelerating and stabilizing the backward procedure is also an important issue in DEQ [15]. ", "page_idx": 2}, {"type": "text", "text": "2.2 Neural Collapse NC ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The phenomenon of ${\\mathcal{N C}}$ was initially uncovered by [42], which is considered an intriguing regularity in neural network training with many elegant geometric properties [50, 61, 66]. When the model is at the terminal phase of training (TPT), or more precisely, achieves zero training error, the within-class means of features and the classifier vectors converge to the vertices of a simplex Equiangular Tight Frame (ETF) on a balanced dataset. ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1. (Simplex Equiangular Tight Frame) A collection of points $\\pmb{s}_{i}\\in\\mathbb{R}^{D},i=1,2,\\cdots,K$ , is said to be a simplex equiangular tight frame if ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{S}=\\alpha\\sqrt{\\frac{K}{K-1}}\\pmb{P}\\Big(\\pmb{I}_{K}-\\frac{1}{K}\\pmb{1}_{K}\\pmb{1}_{K}^{T}\\Big),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\alpha$ is a non-zero scalar, $S=[\\pmb{s}_{1},\\cdots,\\pmb{s}_{k}]\\in\\mathbb{R}^{D\\times K},I_{K}\\in$ ${\\pmb I}_{K}\\in\\mathbb{R}^{K\\times K}$ is the identity matrix, ${\\bf1}_{K}$ is the ones vector, and $P\\in\\mathbb{R}^{D\\times K}(D\\geq K)$ is a partial orthogonal matrix such that ${\\pmb{P}}^{T}{\\pmb{P}}={\\pmb{I}}_{K}$ . ", "page_idx": 2}, {"type": "text", "text": "${\\mathcal{N C}}$ incorporates the following four properties of the last-layer features and classifiers in deep learning training on balanced datasets: ", "page_idx": 2}, {"type": "text", "text": "${\\mathcal{N C}}1$ : Variability collapse: The feature within-class converges to a unique vector, i.e., for any sample $i$ in the same class $k$ , its feature $h_{k,i}$ satisfies $\\|h_{k,i}-\\mathbf{\\bar{\\bar{h}}}_{k}\\|\\rightarrow0,k\\in[k]$ , with the training procedure. ", "page_idx": 2}, {"type": "text", "text": "$\\mathcal{N}\\mathcal{C}2$ : Convergence to simplex ETF: The mean value $h^{\\star}$ of optimal features for each class collapses to the vertices of the simplex ETF. ", "page_idx": 2}, {"type": "text", "text": "${\\mathcal{N}}{\\mathcal{C}}3$ : Convergence to self-duality: The class means and the classifier weights mutually converge: $\\begin{array}{r}{\\frac{W^{\\star}}{\\|W\\|}=\\frac{H^{\\star}}{\\|H\\|}}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "NC4: Nearest Neighbor: The classifier determines the class based on the Euclidean distances among the feature vector and the classifier weights. ", "page_idx": 2}, {"type": "text", "text": "2.3 Layer-peeled model under balanced and imbalanced conditions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Current studies often focus on the case where only the last-layer features and classifier are learnable without considering the layers in the backbone network under the assumption of Unconstrained Features Mode (UFM) [66], which can also be referred to as the Layer-peeled Model [14, 28]. First, we define the feasible set of parameters: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{C}=\\left\\{w_{k},h_{k,i}\\ |\\ \\frac{1}{K}\\sum_{k=1}^{K}\\|w_{k}\\|^{2}\\leq E_{W},\\frac{1}{K}\\sum_{k=1}^{K}\\frac{1}{n_{k}}\\sum_{i=1}^{n_{k}}\\|h_{k,i}\\|^{2}\\leq E_{H}\\right\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Definition 2.2. (Layer-peeled Model) When $\\pmb{H}$ and $W$ are the last layer classifier and weights respectively, then the optimization process of the neural network can be reformulated as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{W,H}\\quad\\frac{1}{N}\\sum_{k=1}^{K}\\sum_{i=1}^{n_{k}}\\mathcal{L}(W h_{k,i},y_{k})\\ \\mathrm{~s.t.~}\\,w_{k},h_{k,i}\\in\\mathcal{C},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $E_{H}$ and $E_{W}$ are two predefined values, $N$ refers to the total number of samples. ", "page_idx": 2}, {"type": "image", "img_path": "obUXeUMmq1/tmp/7004b924e8478b24dca95f7e3d6f9576e3c8741150da310d2acf6ace5eb4157d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 1: Illustration of feature extraction. After extracting feature maps $H^{0}$ , further features $\\pmb{H}$ or $z^{\\star}$ can be obtained by passing through an explicit neural network or DEQ. The final step involves the classifier to obtain predicted logits. To ensure a fair comparison, we standardize the backbone network and its output $H^{0}$ across all conditions. ", "page_idx": 3}, {"type": "text", "text": "It should be noted that all the loss functions $\\mathcal{L}$ analyzed in our study are cross-entropy, as most current research focuses on this widely used deep learning classification loss function [23, 31]. And though the optimization program is nonconvex; however, it can generally be mathematically tractable for analysis. Besides, experiments with unregularized loss function and randomly initialized gradient descent typically converge to non-collapse global minimizers [51]. ", "page_idx": 3}, {"type": "text", "text": "Under UFM, most ${\\mathcal{N C}}$ studies are based on 1-2 conventional layers of weights, However, there is also work [10, 51] that extends it to analyze $M$ linear layers. Additionally, various studies have revealed additional characteristics of ${\\mathcal{N C}}$ , such as its impact on generalization [16, 25, 27, 61], its influence on feature learning [45], global optimality of the network [64, 66] and others. Therefore, ${\\mathcal{N C}}$ is a very efficient tool to analyze the performance of neural networks. ", "page_idx": 3}, {"type": "text", "text": "Imbalanced learning However, ${\\mathcal{N C}}$ will not occur under imbalanced settings generally. This phenomenon arises due to the imbalance in sample quantities, leading to challenges in adequately ftiting features for certain classes. This is commonly referred to as minority collapse [8, 14]. As the degree of imbalance increases, it is expected that classifiers for minority classes converge. When Minority Collapse occurs, the neural network predicts equal probabilities for all minority classes, regardless of the input. ", "page_idx": 3}, {"type": "text", "text": "To enhance learning performance in imbalanced scenarios [62] and mitigate the effects of minority collapse, several methods have been proposed. [14] introduced convex relaxation, modifying a loss function [57], and incorporating a regularization term [37]. The reweighted approach is also widely applied, with some studies measuring it based on sample quantities [47, 59]. Additionally, adaptive techniques such as AutoBalance [33] have been introduced, which incorporates a bilevel optimization framework, along with logit balance [46, 54, 63, 65]. ", "page_idx": 3}, {"type": "text", "text": "3 Comparison under balanced setting ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we first analyze the ${\\mathcal{N C}}$ phenomenon in DEQ under balanced settings. As illustrated in Figure 1, after completing the initial feature extraction, we further examine the feature $\\pmb{H}$ obtained respectively by explicit neural networks and DEQ to reveal the ${\\mathcal{N C}}$ phenomenon. ", "page_idx": 3}, {"type": "text", "text": "3.1 NC in Explicit Neural Networks ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Building upon (6), we analyze ${\\mathcal{N C}}$ in explicit neural networks by considering the following constrained optimization problem during training: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{min}_{W,W_{\\mathrm{EX}},H^{0}}}&{\\displaystyle\\frac{1}{N}\\sum_{k=1}^{K}\\sum_{i=1}^{n}\\mathcal{L}(W W_{\\mathrm{EX}}h_{k,i}^{0},y_{k})}\\\\ {\\mathrm{s.t.}\\quad\\|W_{\\mathrm{EX}}\\|_{F}\\leq E_{H};{\\boldsymbol w}_{k},h_{k,i}\\in\\mathcal{C},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where each $n_{k}$ is set to $n$ under the balanced setting, $W_{\\mathrm{EX}}$ represents the subsequent network weights. For ease of comparison with DEQ, we assume that the final feature is represented as $\\pmb{H}=\\pmb{W}_{\\mathrm{EX}}\\-\\pmb{H}^{0}$ . Traditional neural network structures are nonconvex, making them challenging to analyze due to their highly interactive nature. Employing the layer-peeled model alleviates the difficulty of ${\\mathcal{N C}}$ analysis. ", "page_idx": 4}, {"type": "text", "text": "3.2 NC in Deep Equilibrium models ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Building upon recent investigations into the ${\\mathcal{N C}}$ phenomenon, we embrace the layer-peeled model, where the last-layer features $\\pmb{h}=\\phi(\\pmb{x})$ (equilibrium points in $\\mathrm{DEQ}\\;z^{\\star}.$ ) as unconstrained optimization variables. Accordingly, we add the following constraints to enforce ${\\mathcal{N C}}$ in DEQ: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal C_{\\mathrm{\u1e0aEQ}}\\triangleq\\big\\{\\boldsymbol z^{\\star},\\boldsymbol W_{\\mathrm{\u1e0aEQ}}|\\,\\boldsymbol z^{\\star}=\\boldsymbol f(\\boldsymbol H^{0};\\boldsymbol W_{\\mathrm{\u1e0aEQ}}),\\ \\ \\|\\boldsymbol W_{\\mathrm{\u1e0aEQ}}\\|_{F}\\leq E_{H}\\big\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Compared to explicit layers, the active parameter in Deep Equilibrium models is $W_{\\mathrm{DEQ}}$ , hence imposing restrictions on it to align with the same feasible space. Then the formulation of DEQ with $\\bar{\\mathcal{N C}}$ becomes: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{min}_{W,\\boldsymbol{W}_{\\mathrm{DEQ}},\\boldsymbol{z}^{\\star},\\boldsymbol{H}^{0}}}&{\\displaystyle\\frac{1}{N}\\sum_{k=1}^{K}\\sum_{i=1}^{n_{k}}\\mathcal{L}(W\\boldsymbol{z}^{\\star},\\boldsymbol{y}_{k})}\\\\ {\\mathrm{s.t.}\\quad\\displaystyle\\boldsymbol{w}_{k},h_{k,i}\\in\\mathcal{C};\\boldsymbol{z}^{\\star},W_{\\mathrm{DEQ}}\\in\\mathcal{C}_{\\mathrm{DEQ}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "No matter whether under DEQ or explicit neural networks, these constraints must be imposed. This is because when these constraints are satisfied and the loss function reaches its lower bound, the ${\\mathcal{N C}}$ phenomenon is guaranteed. In our theoretical analysis, we assume that the DEQ is linear, that is, $z^{\\star}=\\mathrm{fixed-point}(f_{\\theta}(\\pmb x),z)=\\sum_{i=0}^{\\infty}W_{\\mathrm{DEQ}}^{i}x$ . Detailed analysis incorporating these constraints is provided in Appendix B. ", "page_idx": 4}, {"type": "text", "text": "The following theorem elucidates the specific scenarios in which the ${\\mathcal{N C}}$ phenomenon occurs. For a fair comparison, we assume that the extracted features $H^{0}$ of the image encoder are the same in the derivation. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1. (Feature collapse of explicit fully connected layers and implicit deep equilibrium models under balanced setting) Suppose (7) and (9) reaches its minimal, then ", "page_idx": 4}, {"type": "equation", "text": "$$\nW_{E X}{h}_{k,i}^{0}=W_{E X}{h}_{k}^{0},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $h_{k}^{0}=\\sum_{i\\in\\tau(k)}h_{k,i}^{0}$ . Similarly, if the model is DEQ, then ", "page_idx": 4}, {"type": "equation", "text": "$$\nf(h_{k,i}^{0};W_{D E Q})=f(h_{k}^{0};W_{D E Q}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "NC2: The classifier aligns to the Simplex ETF, regardless of whether explicit neural network and DEQ are applied: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle W W^{T}=\\sqrt{E_{W}/E_{H}}W W_{E X}H^{0}}\\ ~}\\\\ {{\\displaystyle~~~~~~~=\\sqrt{E_{W}/E_{H}}W f(H^{0};W_{D E Q})}}\\\\ {{\\displaystyle~~~~~~~=\\frac{K E_{W}}{K-1}\\left(\\mathbf{1}_{K}-\\frac{1}{K}\\mathbf{1}_{K}\\mathbf{1}_{K}^{T}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "NC3: For $\\forall\\:k=1,2,\\cdots\\:,K,$ , the feature aligns to the weights: ", "page_idx": 4}, {"type": "equation", "text": "$$\nW_{E X}h_{k}^{0}\\propto W_{k}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In DEQ cases: ", "page_idx": 4}, {"type": "equation", "text": "$$\nf(h_{k}^{0};W_{D E Q})\\propto W_{k}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The theorem demonstrates that when the network training reaches its limit, i.e., when the loss function reaches its minimum, the ${\\mathcal{N C}}$ phenomenon emerges regardless of whether the chosen network is DEQ or explicit neural network. Besides, in certain scenarios, the lower bound of the loss function for DEQ is relatively smaller compared to explicit neural networks. More detailed proofs are in Appendix Section B. ", "page_idx": 4}, {"type": "text", "text": "4 Comparison under imbalanced setting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we analyze the performance differences between DEQ and explicit neural network on imbalanced datasets. We observe that, unlike in balanced scenarios, as long as certain conditions are met, the advantages of DEQ over explicit neural network become more pronounced on imbalanced datasets. And we provide theoretical evidence to support this phenomenon. ", "page_idx": 5}, {"type": "text", "text": "Suppose the total number of classes is $K$ , with $K_{A}$ being the number of majority classes and $K_{B}=K-K_{A}$ being the number of minority classes. Each majority class has $n_{A}$ samples, and each minority class has $n_{B}$ samples. The total number of samples is given by $N=K_{A}n_{A}+K_{B}n_{B}$ Note that $n_{A}>n_{B}$ with no requirement for $K_{A}$ to be greater than $K_{B}$ . We first start with the loss function, which can be partitioned into two components as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\underset{W,\\tilde{W},H^{0}}{\\operatorname*{min}}\\frac{K_{A}n_{A}}{N}\\sum_{k=1}^{K_{A}}\\sum_{i=1}^{n_{A}}\\mathcal{L}(W\\tilde{W}H^{0},y_{k})+\\frac{K_{B}n_{B}}{N}\\sum_{k=K_{A}+1}^{K_{B}}\\sum_{i=1}^{n_{B}}\\mathcal{L}(W\\tilde{W}H^{0},y_{k}),}\\\\ &{\\mathrm{~s.t.~}\\ \\tilde{W}\\in\\{\\mathcal{C}_{\\mathrm{EX}}\\ \\mathrm{or}\\ \\mathcal{C}_{\\mathrm{DEQ}}\\}\\,,\\ w_{k},h_{k,i}\\in\\mathcal{C},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\Tilde{W}$ represents the weights of Deep Equilibrium Models $W_{\\mathrm{DEQ}}$ and explicit neural network $W_{\\mathrm{EX}}$ . To analyze the ${\\mathcal{N C}}$ phenomenon, we present the results in the following theorem: ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1. (Neural Collapse under imbalanced settings on explicit neural networks and deep equilibrium models) ", "page_idx": 5}, {"type": "text", "text": "When the loss function reaches the minimum, then ", "page_idx": 5}, {"type": "equation", "text": "$$\nW_{E X}{h}_{k,i}^{0}=W_{E X}{h}_{k}^{0},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $h_{k}^{0}=\\sum_{i\\in\\tau(i)}h_{k,i}^{0}$ . Similarly, if the model is DEQ, then ", "page_idx": 5}, {"type": "equation", "text": "$$\nf(h_{k,i}^{0};W_{D E Q})=f(h_{k}^{0};W_{D E Q}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "NC2: Not exists, but the results of explicit neural network and DEQ can be compared: ", "page_idx": 5}, {"type": "text", "text": "Here we denote $\\left(h_{k}^{0}\\right)^{T}h_{k^{\\prime}}^{0}=m_{k,k^{\\prime}}$ and $\\boldsymbol{S}$ is a $K$ -Simplex ETF, if ", "page_idx": 5}, {"type": "equation", "text": "$$\nE_{H}<2S_{i j}-m_{i j}<\\frac{1}{1-E_{H}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "is satisfied, the following inequality ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\left(W_{E X}H^{0}\\right)^{T}\\left(W_{E X}H^{0}\\right)-S\\right\\|_{F}>\\left\\|f^{T}(H^{0};W_{D E Q})f(H^{0};W_{D E Q})-S\\right\\|_{F}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "holds. ", "page_idx": 5}, {"type": "text", "text": "NC3: Similarly as $\\mathcal{N}\\mathcal{C}2$ , though it does not exist, the results can still be compared, when ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{E_{H}}{E_{w}+E_{H}}+E_{H}(1-E_{H})<2\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "is satisfied, then the cosine distance satisfies: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\cos\\left(f(h_{k};W_{D E Q}),{\\pmb w}_{k}\\right)/\\cos\\left(W_{E X}h_{k},{\\pmb w}_{k}\\right)>1.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The detailed proof is in Appendix Section C. ", "page_idx": 5}, {"type": "text", "text": "Besides, the conclusion regarding the loss function is quite similar to that of Theorem B.3 under balanced settings. As analyzed in (43) and (44) in the Appendix, the lower bound of the loss function in DEQ is still lower than that in explicit neural network, where the performance of learned features is more evident in Figure 2, where we use t-SNE [53] and Gram matrix of features to describe the performance of two models. Although the phenomenon of $\\mathcal{N}\\mathcal{C}2$ and $\\mathcal{N C}3$ does not exist, we have discovered in Theorem 4.1 that under mild conditions, DEQ is superior in terms of ${\\mathcal{N C}}$ compared to explicit neural network. Notably, the conditions are easy to satisfy since $E_{H}$ is generally very small in practice. ", "page_idx": 5}, {"type": "image", "img_path": "obUXeUMmq1/tmp/65dee5d3861c8aa6f77929a80ed3c8068a8c63a90eae350a274c8dce81a764b4.jpg", "img_caption": ["Figure 2: Under the imbalanced setting for CIFAR-10 with $K_{A}=3$ and $R=10$ , the disparity in the learned features between Explicit Neural Networks (left) and DEQ (right). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "A crucial insight is that since DEQ undergoes multiple rounds of parameter adjustments for learning, it can be viewed as having an infinite number of layers, thus possessing greater representational capacity. As the network deepens, the iterative process of forward fixed-point may not necessarily reach the lowest threshold. Therefore, DEQ exhibits a certain degree of generalization for features in the minority class. Given the substantial feature differences among classes under an imbalanced dataset, the learned features by DEQ may demonstrate better adaptability to unseen categories. Consequently, compared to explicit neural network, DEQ tends to enhance performance. ", "page_idx": 6}, {"type": "text", "text": "Besides, due to the repeated iterations in solving the fixed-point iteration for some samples in the minority class with a small sample size, the model somewhat engages in multiple learning iterations for the features of samples in this class. This mitigates the impact of imbalanced samples to some extent. However, despite some improvements compared to the explicit neural network, DEQ still faces the issue of minority collapse. This conclusion is further validated in our subsequent experiments. Besides, to further discuss the situation of the dataset in terms of the degree of imbalance, we derived the following proposition: ", "page_idx": 6}, {"type": "text", "text": "Proposition 4.2. Denote $R=K_{A}n_{A}/N$ . When the number of samples in the majority class becomes extremely large, i.e., $R\\to1$ , the features of the two kinds of classes will become: ", "page_idx": 6}, {"type": "text", "text": "Majority classes: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{W_{E X}h_{k,i}^{0}=W_{E X}h_{k}^{0},\\qquad\\qquad}\\\\ {f(h_{k,i}^{0};W_{D E Q})=f(h_{k}^{0};W_{D E Q}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $1\\leq k\\leq K_{A}$ and $i\\in\\pi(k)$ . Each feature collapses to $K_{A}$ -Simplex ETF. ", "page_idx": 6}, {"type": "text", "text": "Minority classes: ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\pmb w}_{k}={\\bf0},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\nW_{E X}h_{k,i}^{0}=f(h_{k,i}^{0};W_{D E Q})=\\mathbf{0},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $K_{A}+1\\leq k\\leq K$ and $i\\in\\pi(k)$ . ", "page_idx": 6}, {"type": "text", "text": "Here, $\\pi(k)$ refers to the samples that belong to the class $k$ . ", "page_idx": 6}, {"type": "text", "text": "This situation is equivalent to having a balanced dataset in the majority class, while the minority class, due to its extremely small sample size, contributes almost nothing. In such an extreme scenario, the ${\\mathcal{N C}}$ performance of DEQ and the fully connected layer is nearly indistinguishable similar to Theorem 3.1. Both collapse on majority classes, resulting in a lack of learning features from minority classes meeting the results of (51) and (52). This aligns with the findings in [14], where they provide more specific bounds on the ratio $K_{A}/K_{B}$ in their Theorem 5. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we empirically conducted experiments to validate the correctness of the proposed theorems. Initially, we implemented DEQ on a balanced dataset and compared its ${\\mathcal{N C}}$ performance with that of ResNet. Subsequently, for imbalanced datasets, we tested varying degrees of imbalance by manipulating the quantities of $n_{A}$ and $n_{B}$ , as well as $K_{A}$ and $K_{B}$ . The experimental results showed that, on imbalanced datasets, DEQ outperformed Explicit Neural Networks. This finding is consistent with the results reported in [4]. All experiments were implemented using PyTorch on NVIDIA Tesla A40 48GB. ", "page_idx": 6}, {"type": "image", "img_path": "obUXeUMmq1/tmp/1563ff255d382aacf4bce607286a1ba71863a7074b9f89496f6dddd1078b18e0.jpg", "img_caption": ["Figure 3: Comparison of accuracy and ${\\mathcal{N C}}$ phenomenon in training Cifar-10 dataset "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.1 Experiment setup ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Without loss of generality, since any traditional neural network can be formulated as a DEQ, we use ResNet18 [22] as the backbone architecture here. As discussed earlier, to utilize the fixed point $z^{\\star}$ learned by DEQ as the extracted feature, we formulate the last ResNet block into a DEQ format, while maintaining the remaining structure identical to ResNet. As mentioned in [5], training with DEQ can lead to instability issues. This is especially noticeable as training progresses, where some samples struggle to converge to a fixed point. To address this, in accordance with their setting, we implement the solver with a threshold $\\epsilon$ set to $10^{-3}$ and introduce an early stopping mechanism. If convergence is not achieved within $T>20$ iterations, we terminate the fixed-point iteration. Additionally, when facing problematic samples during fixed-point solving, we skip them to ensure training stability. During training, we set the learning rate to $1\\times10^{-4}$ and utilize stochastic gradient descent with a momentum of 0.9 and weight decay of $5\\times10^{-4}$ . Both $E_{W}$ and $E_{H}$ are set to 0.01. The training phase for each network consists of 100 epochs, with a batch size of 128. In this context, accuracy is assessed by averaging the results from the last 10 epochs and computing their standard deviation. ", "page_idx": 7}, {"type": "text", "text": "5.2 Performance under balanced conditions ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "By using the settings in (7) and (9), we compared the performance of DEQ and Explicit NN on Cifar-10 [30] and Cifar-100 [29] for validation, as shown in Figure 3(a). Their ${\\mathcal{N C}}$ performances remain comparable, i.e., DEQ achieves results similar to Explicit NN, corroborating the findings of Theorem 3.1. As for accuracy, from the ", "page_idx": 7}, {"type": "table", "img_path": "obUXeUMmq1/tmp/80f59db8e1717f71c976f6718e0c6712ceb33355696800ce9ebc44ea0d7a1d1c.jpg", "table_caption": ["Table 1: Comparison of accuracy under balanced settings of Cifar-10 and Cifar-100 "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "results in the first column of Table 1, it can be observed that DEQ\u2019s accuracy is higher than that of the explicit layer, which aligns with Theorem B.3. However, the increase is only marginal due to the fact that the coefficients $E_{H}$ and $E_{W}$ act as scaling factors. Therefore, compared to explicit neural network, DEQ finds it challenging to achieve a significantly lower loss and, consequently, a substantial improvement. Moreover, Explicit NN performs well in fitting balanced datasets, so the accuracy of DEQ does not experience a significant boost in this context. ", "page_idx": 7}, {"type": "text", "text": "Here, we manually set the number of epochs to 100 to avoid potential instability issues with DEQ as training deepens. This is because DEQ can be challenging to reach the TPT (Terminal Phase of Training). As the number of parameters increases, achieving fixed-point convergence becomes more difficult, and even parameter explosion may occur. Under the current vanilla design, it is challenging to avoid such instability. Therefore, for a fair comparison, we apply the same training settings to both the implicit DEQ and the explicit neural network. The results in Figure 5 indicate that the test performance at 100 epochs is not significantly different from that at TPT. Since DEQ shares the same backbone as the corresponding explicit neural network, it can still demonstrate better ${\\mathcal{N C}}$ performance after reaching TPT in these cases. ", "page_idx": 8}, {"type": "table", "img_path": "obUXeUMmq1/tmp/249f7e0954a4f5763887c6f99a5458c7da0f4bce355163e41836a76879a88d05.jpg", "table_caption": ["Table 2: Test Accuracy on Cifar-10 and Cifar-100 Dataset with $K_{A}=3$ "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.3 Performance under imbalanced conditions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conducted experiments with varying configurations with different numbers of majority and minority classes and imbalance degrees. Assume the numbers of majority and minority classes are $(\\bar{K_{A}},K_{B})$ with corresponding sample sizes $(n_{A},n_{B})$ , the imbalance degree is denoted as $R\\,=\\,n_{A}/n_{B}$ .We considered different setups for majority and minority class quantities, such as $(3,7)$ , $(5,5)$ , and $(7,3)$ . Additionally, we varied the ratio of sample quantities $R$ between majority and minority classes with values of 10, 50 and 100. We also tested the phenomenon of $\\mathcal{N}\\bar{\\mathcal{C}}$ and accuracy on the Cifar-10 and Cifar-100 datasets, which own a total of 5000 images for each class. Specifically, when $R=100$ and $(K_{A},K_{B})\\,=\\,(3,7)$ for Cifar-10, the number of samples for all classes is $(5000,5000,5000,50,50,50,50,50,50,50)$ . ", "page_idx": 8}, {"type": "text", "text": "The results for $(K_{A},K_{B})\\,=\\,(3,7)$ are shown in Table 2, where the test dataset owns the same distribution as the training dataset. We use \u201coverall\", \u201cmajority\", and \u201cminority\" to represent the results across all categories, the majority class, and the minority class, respectively. We contrasted the difference in the training outcomes between the Explicit Neural Network and DEQ, and the superior performance of DEQ compared to Explicit Neural Network confirms DEQ\u2019s higher learning potential. This suggests that DEQ can achieve a lower bound on its loss function. The experimental results indicate that DEQ consistently outperforms explicit neural network in accuracy during imbalanced training, aligning with Theorem 4.1. Specifically, we present the outcomes for $(K_{A},\\bar{K}_{B})=(5,5)$ with $R=100$ are depicted in Figure 3(b). The results strongly corroborate Theorem 4.1, affirming DEQ exhibits the same ${\\mathcal{N C}}1$ phenomenon as an explicit neural network under these conditions. However, DEQ outperforms the explicit neural network in terms of $\\mathcal{N C}2$ and $\\mathcal{N C}3$ . Additional experimental results with different parameters are detailed in Appendix Section D. ", "page_idx": 8}, {"type": "text", "text": "In addition to the stability considerations discussed in Section 5.2, we refrain from training for an extensive number of epochs due to the imbalance in the samples of the training set. This is because excessive learning rounds might cause the network parameters to predominantly capture information from the majority class, resulting in overftiting its features. This, in turn, diminishes the generalization of learning features from other classes, leading to marginal improvements in accuracy on the test set. As depicted in Figure 3(b), the model has already converged at this point. Moreover, limiting the number of training epochs helps to avoid the gradual instability in the learning process of DEQ. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this study, we have systematically analyzed the representation of Deep Equilibrium Models (DEQ) and explicit neural networks under both balanced and imbalanced conditions using the phenomenon of Neural Collapse $(\\mathcal{N C})$ . Our theoretical analysis demonstrated that ${\\mathcal{N C}}$ is present in DEQ under balanced conditions. Furthermore, in imbalanced settings, DEQ exhibited notable advantages over explicit neural networks, such as the convergence of extracted features to the vertices of a simplex equiangular tight frame and self-duality properties under mild conditions. These findings highlight the superior performance of DEQ in handling imbalanced datasets. Our experimental results in both balanced and imbalanced scenarios validate the theoretical insights. The current analysis is limited to simple imbalanced scenarios and the linear structure of DEQ models. Future work will expand on this foundation by exploring more general imbalanced scenarios and extending the analysis to more complex forms of DEQ models. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by NSFC (No.62303319), Shanghai Sailing Program (22YF1428800), Shanghai Local College Capacity Building Program (23010503100), ShanghaiTech AI4S Initiative SHTAI4S202404, Shanghai Frontiers Science Center of Human-centered Artificial Intelligence (ShangHAI), MoE Key Laboratory of Intelligent Perception and Human-Machine Collaboration (ShanghaiTech University) and Shanghai Engineering Research Center of Intelligent Vision and Imaging. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] A. Agrawal, B. Amos, S. Barratt, S. Boyd, S. Diamond, and Z. Kolter. Differentiable convex optimization layers. In Advances in Neural Information Processing Systems, 2019.   \n[2] Brandon Amos and J Zico Kolter. Optnet: Differentiable optimization as a layer in neural networks. In International Conference on Machine Learning, pages 136\u2013145. PMLR, 2017.   \n[3] Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In Advances in Neural Information Processing Systems (NeurIPS), 2019.   \n[4] Shaojie Bai, Vladlen Koltun, and J. Zico Kolter. Multiscale deep equilibrium models. In Advances in Neural Information Processing Systems (NeurIPS), 2020.   \n[5] Shaojie Bai, Vladlen Koltun, and J. Zico Kolter. Stabilizing equilibrium models by jacobian regularization. In International Conference on Machine Learning (ICML), 2021. [6] Mathieu Blondel, Quentin Berthet, Marco Cuturi, Roy Frostig, Stephan Hoyer, Felipe LlinaresL\u00f3pez, Fabian Pedregosa, and Jean-Philippe Vert. Efficient and modular implicit differentiation. Advances in neural information processing systems, 35:5230\u20135242, 2022.   \n[7] Charles G Broyden. A class of methods for solving nonlinear simultaneous equations. Mathematics of computation, 19(92):577\u2013593, 1965.   \n[8] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced datasets with label-distribution-aware margin loss. Advances in neural information processing systems, 32, 2019.   \n[9] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018.   \n[10] Hien Dang, Tan Nguyen, Tho Tran, Hung Tran, and Nhat Ho. Neural collapse in deep linear network: From balanced to imbalanced data. arXiv preprint arXiv:2301.00437, 2023.   \n[11] Shutong Ding, Tianyu Cui, Jingya Wang, and Ye Shi. Two sides of the same coin: Bridging deep equilibrium models and neural ODEs via homotopy continuation. Advances in Neural Information Processing Systems, 36, 2024.   \n[12] Justin Domke. Generic methods for optimization-based modeling. In Artificial Intelligence and Statistics, pages 318\u2013326. PMLR, 2012.   \n[13] Laurent El Ghaoui, Fangda Gu, Bertrand Travacca, Armin Askari, and Alicia Tsai. Implicit deep learning. SIAM Journal on Mathematics of Data Science, 3(3):930\u2013958, 2021.   \n[14] Cong Fang, Hangfeng He, Qi Long, and Weijie J Su. Exploring deep neural networks via layer-peeled model: Minority collapse in imbalanced training. Proceedings of the National Academy of Sciences, 118(43):e2103091118, 2021.   \n[15] Samy Wu Fung, Howard Heaton, Qiuwei Li, Daniel McKenzie, Stanley Osher, and Wotao Yin. Jfb: Jacobian-free backpropagation for implicit networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 6648\u20136656, 2022.   \n[16] Tomer Galanti, Andr\u00e1s Gy\u00f6rgy, and Marcus Hutter. On the role of neural collapse in transfer learning. arXiv preprint arXiv:2112.15121, 2021.   \n[17] Tianxiang Gao, Xiaokai Huo, Hailiang Liu, and Hongyang Gao. Wide neural networks as gaussian processes: Lessons from deep equilibrium models. arXiv preprint arXiv:2310.10767, 2023.   \n[18] Zhengyang Geng, Xin-Yu Zhang, Shaojie Bai, Yisen Wang, and Zhouchen Lin. On training implicit models. Advances in Neural Information Processing Systems, 34:24247\u201324260, 2021.   \n[19] Davis Gilton, Gregory Ongie, and Rebecca Willett. Deep equilibrium architectures for inverse problems in imaging. IEEE Transactions on Computational Imaging, 7:1123\u20131133, 2021.   \n[20] Stephen Gould, Richard Hartley, and Dylan Campbell. Deep declarative networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(8):3988\u20134004, 2021.   \n[21] Fangda Gu, Heng Chang, Wenwu Zhu, Somayeh Sojoudi, and Laurent El Ghaoui. Implicit graph neural networks. Advances in Neural Information Processing Systems, 33:11984\u201311995, 2020.   \n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[23] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700\u20134708, 2017.   \n[24] Yuhao Huang, Qingsong Wang, Akwum Onwunta, and Bao Wang. Efficient score matching with deep equilibrium layers. In The Twelfth International Conference on Learning Representations, 2024.   \n[25] Like Hui, Mikhail Belkin, and Preetum Nakkiran. Limitations of neural collapse for understanding generalization in deep learning. arXiv preprint arXiv:2202.08384, 2022.   \n[26] Arthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018.   \n[27] Jiachen Jiang, Jinxin Zhou, Peng Wang, Qing Qu, Dustin Mixon, Chong You, and Zhihui Zhu. Generalized neural collapse for a large number of classes. arXiv preprint arXiv:2310.05351, 2023.   \n[28] Vignesh Kothapalli. Neural collapse: A review on modelling principles and generalization. Transactions on Machine Learning Research, 2023.   \n[29] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical Report 0, University of Toronto, Toronto, Ontario, 2009.   \n[30] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced research). URL http://www. cs. toronto. edu/kriz/cifar. html, 5(4):1, 2010.   \n[31] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436\u2013444, 2015.   \n[32] Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165, 2017.   \n[33] Mingchen Li, Xuechen Zhang, Christos Thrampoulidis, Jiasi Chen, and Samet Oymak. Autobalance: Optimized loss functions for imbalanced data. Advances in Neural Information Processing Systems, 34:3163\u20133177, 2021.   \n[34] Mingjie Li, Yisen Wang, and Zhouchen Lin. Cerdeq: Certifiable deep equilibrium model. In International Conference on Machine Learning, pages 12998\u201313013. PMLR, 2022.   \n[35] Zenan Ling, Longbo Li, Zhanbo Feng, YIXUAN ZHANG, Feng Zhou, Robert C Qiu, and Zhenyu Liao. Deep equilibrium models are almost equivalent to not-so-deep explicit models for high-dimensional gaussian mixtures. In Forty-first International Conference on Machine Learning, 2024.   \n[36] Zenan Ling, Xingyu Xie, Qiuhao Wang, Zongpeng Zhang, and Zhouchen Lin. Global convergence of over-parameterized deep equilibrium models. In International Conference on Artificial Intelligence and Statistics, pages 767\u2013787. PMLR, 2023.   \n[37] Xuantong Liu, Jianfeng Zhang, Tianyang Hu, He Cao, Yuan Yao, and Lujia Pan. Inducing neural collapse in deep long-tailed learning. In International Conference on Artificial Intelligence and Statistics, pages 11534\u201311544. PMLR, 2023.   \n[38] Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. In International conference on artificial intelligence and statistics, pages 1540\u20131552. PMLR, 2020.   \n[39] Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665\u2013 E7671, 2018.   \n[40] Dustin G Mixon, Hans Parshall, and Jianzong Pi. Neural collapse with unconstrained features. arXiv preprint arXiv:2011.11619, 2020.   \n[41] Vishal Monga, Yuelong Li, and Yonina C Eldar. Algorithm unrolling: Interpretable, efficient deep learning for signal and image processing. IEEE Signal Processing Magazine, 38(2):18\u201344, 2021.   \n[42] Vardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during the terminal phase of deep learning training. Proceedings of the National Academy of Sciences, 117(40):24652\u201324663, 2020.   \n[43] Ashwini Pokle, Zhengyang Geng, and J Zico Kolter. Deep equilibrium approaches to diffusion models. Advances in Neural Information Processing Systems, 35:37975\u201337990, 2022.   \n[44] Zaccharie Ramzi, Pierre Ablin, Gabriel Peyr\u00e9, and Thomas Moreau. Test like you train in implicit deep learning. arXiv preprint arXiv:2305.15042, 2023.   \n[45] Akshay Rangamani, Marius Lindegaard, Tomer Galanti, and Tomaso A Poggio. Feature learning in deep classifiers through intermediate neural collapse. In International Conference on Machine Learning, pages 28729\u201328745. PMLR, 2023.   \n[46] Jiawei Ren, Cunjun Yu, Xiao Ma, Haiyu Zhao, Shuai Yi, et al. Balanced meta-softmax for longtailed visual recognition. Advances in neural information processing systems, 33:4175\u20134186, 2020.   \n[47] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning. In International conference on machine learning, pages 4334\u20134343. PMLR, 2018.   \n[48] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Xiaojiang Chen, and Xin Wang. A comprehensive survey of neural architecture search: Challenges and solutions. ACM Computing Surveys (CSUR), 54(4):1\u201334, 2021.   \n[49] Haixiang Sun, Ye Shi, Jingya Wang, Hoang Duong Tuan, H Vincent Poor, and Dacheng Tao. Alternating differentiation for optimization layers. arXiv preprint arXiv:2210.01802, 2022.   \n[50] Christos Thrampoulidis, Ganesh Ramachandra Kini, Vala Vakilian, and Tina Behnia. Imbalance trouble: Revisiting neural-collapse geometry. Advances in Neural Information Processing Systems, 35:27225\u201327238, 2022.   \n[51] Tom Tirer and Joan Bruna. Extended unconstrained features model for exploring deep neural collapse. In International Conference on Machine Learning, pages 21478\u201321505. PMLR, 2022.   \n[52] Russell Tsuchida, Suk Yee Yong, Mohammad Ali Armin, Lars Petersson, and Cheng Soon Ong. Declarative nets that are equilibrium models. In International Conference on Learning Representations, 2022.   \n[53] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(86):2579\u20132605, 2008.   \n[54] Yuchao Wang, Jingjing Fei, Haochen Wang, Wei Li, Tianpeng Bao, Liwei Wu, Rui Zhao, and Yujun Shen. Balancing logit variation for long-tailed semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19561\u201319573, 2023.   \n[55] Colin Wei and J Zico Kolter. Certified robustness for deep equilibrium models via interval bound propagation. In International Conference on Learning Representations, 2022.   \n[56] Ezra Winston and J Zico Kolter. Monotone operator equilibrium networks. Advances in neural information processing systems, 33:10718\u201310728, 2020.   \n[57] Liang Xie, Yibo Yang, Deng Cai, and Xiaofei He. Neural collapse inspired attraction\u2013repulsionbalanced loss for imbalanced learning. Neurocomputing, 527:60\u201370, 2023.   \n[58] Xingyu Xie, Qiuhao Wang, Zenan Ling, Xia Li, Guangcan Liu, and Zhouchen Lin. Optimization induced equilibrium networks: An explicit optimization perspective for understanding equilibrium models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(3):3604\u20133616, 2022.   \n[59] Yibo Yang, Shixiang Chen, Xiangtai Li, Liang Xie, Zhouchen Lin, and Dacheng Tao. Inducing neural collapse in imbalanced learning: Do we really need a learnable classifier at the end of deep neural network? Advances in Neural Information Processing Systems, 35:37991\u201338002, 2022.   \n[60] Zonghan Yang, Tianyu Pang, and Yang Liu. A closer look at the adversarial robustness of deep equilibrium models. Advances in Neural Information Processing Systems, 35:10448\u201310461, 2022.   \n[61] Can Yaras, Peng Wang, Zhihui Zhu, Laura Balzano, and Qing Qu. Neural collapse with normalized features: A geometric analysis over the riemannian manifold. Advances in neural information processing systems, 35:11547\u201311560, 2022.   \n[62] Yifan Zhang, Bingyi Kang, Bryan Hooi, Shuicheng Yan, and Jiashi Feng. Deep long-tailed learning: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.   \n[63] Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xiaojuan Qi, Xiangyu Zhang, and Jiaya Jia. Understanding imbalanced semantic segmentation through neural collapse. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19550\u201319560, 2023.   \n[64] Jinxin Zhou, Xiao Li, Tianyu Ding, Chong You, Qing Qu, and Zhihui Zhu. On the optimization landscape of neural collapse under mse loss: Global optimality with unconstrained features. In International Conference on Machine Learning, pages 27179\u201327202. PMLR, 2022.   \n[65] Jianggang Zhu, Zheng Wang, Jingjing Chen, Yi-Ping Phoebe Chen, and Yu-Gang Jiang. Balanced contrastive learning for long-tailed visual recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6908\u20136917, 2022.   \n[66] Zhihui Zhu, Tianyu DING, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu. A geometric analysis of neural collapse with unconstrained features. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B Proof under balanced setting ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Problem definition 15   \nB.2 NC analysis . . 17   \nB.2.1 NC proof in Explicit neural networks 17   \nB.2.2 NC proof in DEQ 18 ", "page_idx": 13}, {"type": "text", "text": "C Proof under imbalanced learning ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "20 ", "page_idx": 13}, {"type": "text", "text": "C.1 Lower bound of the loss function 20   \nC.2 NC Analysis 23 ", "page_idx": 13}, {"type": "text", "text": "D More experiments ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "26 ", "page_idx": 13}, {"type": "text", "text": "A Evaluation metrics of NC ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Followed by the settings of [51] and [10], the measurement of ${\\mathcal{N C}}$ are set as follow: ", "page_idx": 13}, {"type": "text", "text": "Let $\\begin{array}{r}{\\pmb{h}_{k}\\triangleq\\frac{1}{n_{k}}\\sum_{i=1}^{n_{k}}\\pmb{h}_{k,i}}\\end{array}$ represent the average of all features within class $k$ and these $K$ classes collectively constitute the average matrix $\\bar{\\b{H}}\\,=\\,[\\pmb{h}_{1},\\cdots\\,,\\pmb{h}_{K}]$ . Besides, The global average is defined as $\\begin{array}{r}{h_{G}\\triangleq\\frac{1}{K}\\sum_{i=1}^{K}h_{k}}\\end{array}$ . Subsequently, the within-class and between-class covariances can be calculated as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{\\Sigma}_{W}\\triangleq\\displaystyle\\frac{1}{N}\\sum_{k=1}^{K}\\sum_{i=1}^{n_{k}}(\\boldsymbol{h}_{k,i}-\\boldsymbol{h}_{k})(\\boldsymbol{h}_{k,i}-\\boldsymbol{h}_{k})^{T},}\\\\ &{\\boldsymbol{\\Sigma}_{B}\\triangleq\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}(\\boldsymbol{h}_{k}-\\bar{\\boldsymbol{h}}_{G})(\\boldsymbol{h}_{k}-\\bar{\\boldsymbol{h}}_{G})^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "NC1 measures the variation of features with-in the same class: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{N C}1=\\frac{1}{K}\\mathrm{tr}\\left(\\Sigma_{W}\\Sigma_{B}^{\\dag}\\right),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\Sigma_{B}^{\\dagger}$ denotes the pseudo-inverse of $\\pmb{\\Sigma}_{B}$ ", "page_idx": 13}, {"type": "text", "text": "$\\mathcal{N}\\mathcal{C}2$ measures similarity between the mean of learned last-layer features $\\bar{H}$ and the structure of Simplex ETF: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{N C}2=\\left\\|\\frac{\\bar{\\boldsymbol{H}}^{T}\\bar{\\boldsymbol{H}}}{\\|\\bar{\\boldsymbol{H}}^{T}\\bar{\\boldsymbol{H}}\\|_{F}}-\\frac{1}{K-1}(\\boldsymbol{I}_{K}-\\frac{1}{K}\\mathbf{1}_{K}\\mathbf{1}_{K}^{T})\\right\\|_{F}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "NC3 measures similarity of the last-layer feature $\\bar{H}$ and weights of classifier $W$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{N C3}=\\left\\|\\frac{W}{\\|W\\|_{F}}-\\frac{\\bar{H}}{\\|\\bar{H}\\|_{F}}\\right\\|.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Additionally, it is worth noting that all above ${\\mathcal{N C}}$ criteria are exclusively based on the training set. This is because our focus is solely on analyzing learning performance on imbalanced datasets, and generalization is not a primary concern. ", "page_idx": 13}, {"type": "text", "text": "B Proof under balanced setting ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Problem definition ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As different layers in the neural network introduce complexity, the optimization problem is nonconvex, and KKT conditions do not guarantee global optimality. Therefore, we consider applying inequality relaxation to the joint optimization problem, obtaining a lower bound for the loss function. By determining the conditions under which the equality holds, we can derive the requirements for the $\\scriptstyle{\\dot{\\mathcal{N C}}}$ phenomenon. This analysis assumes a balanced setting, where all $\\#\\tau(k)={\\bar{n}}_{1}=n_{2}=\\cdots=$ $n_{K}\\doteq N/K$ . ", "page_idx": 14}, {"type": "text", "text": "We considered the fully connected layers (explicit) and Deep Equilibrium Models (implicit) under the balanced settings respectively, and then derived the detailed proof. ", "page_idx": 14}, {"type": "text", "text": "(Fully Connected Layers) ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "(Deep Equilibrium Models) ", "text_level": 1, "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}{\\underset{W,W_{\\mathrm{ES},H}}{\\operatorname*{min}}}&{\\frac{1}{N}\\displaystyle\\sum_{k=1}^{K}\\sum_{i=1}^{n_{k}}\\mathcal{L}(W W_{\\mathrm{Ex}}h_{k,i}^{0},y_{k})}&&{\\underset{W,W_{\\mathrm{RS},\\mathrm{z}_{k,i}^{*}}}{\\operatorname*{min}}\\ \\ \\frac{1}{N}\\displaystyle\\sum_{k=1}^{K}\\sum_{i=1}^{n_{k}}\\mathcal{L}(W z^{*},y_{k})}\\\\ {\\mathrm{s.t.}}&{h_{k,i}=W_{\\mathrm{Ex}}h_{k,i}^{0},}&&{s.\\mathrm{t.}}\\\\ &{\\|W_{\\mathrm{Ex}}\\|_{F}\\leq{E}H,}&&{\\|W_{\\mathrm{Ex}}\\|_{\\smash{\\mathbb{V}^{\\infty}}}\\xi L}\\\\ &{\\displaystyle\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}\\|w_{k}\\|^{2}\\leq E_{W},}&&{\\displaystyle\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}\\|w_{k}\\|^{2}\\leq E_{W},}\\\\ &{\\displaystyle\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}\\frac{1}{n_{k}}\\sum_{i=1}^{n_{k}}\\|h_{k,i}\\|^{2}\\leq E_{H},}&&{\\displaystyle\\frac{1}{K}\\displaystyle\\sum_{k=1}^{K}\\frac{1}{n_{k}}\\sum_{i=1}^{n_{k}}\\|z_{k,i}^{*}\\|^{2}\\leq\\xi_{H},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that here $n_{1}=n_{2}=\\cdot\\cdot\\cdot=n_{k}=n$ , and $f$ represents the form of Linear DEQ, where we will use $f(\\mathbf{x};W_{\\mathrm{DEQ}})=\\sum_{i=1}^{\\infty}W_{\\mathrm{DEQ}}^{i}x$ for representation in the following proofs. ", "page_idx": 14}, {"type": "text", "text": "In a classification task, cross-entropy loss $\\mathcal{L}(W h_{k,i},y_{k})$ is regarded as the final loss function. Drawing inspiration from [14], our initial efforts revolve around organizing and simplifying the log function to distinguish the logit in class $k$ from other classes. ", "page_idx": 14}, {"type": "text", "text": "First consider the following lemma: ", "page_idx": 14}, {"type": "text", "text": "Lemma B.1. Let there be $K$ variables $\\delta_{1},\\delta_{2},\\cdots\\,,\\delta_{K}$ , and the logit of each variable $\\delta_{k}$ satisfies the inequality: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\log\\left(\\delta_{k}/\\sum_{k=1}^{k}\\delta_{k}\\right)\\leq M_{1}\\left(\\log\\delta_{k}-\\frac{1}{K-1}\\sum_{k^{\\prime}\\neq k}^{K}\\log\\delta_{k}\\right)+M_{2},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $M_{1}$ and $M_{2}$ are predefined constants. ", "page_idx": 14}, {"type": "text", "text": "Proof. Split the sum in the denominator and sequentially introduce weights for each term. Here, define $K$ coefficients such that their sum is 1. Therefore, we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\frac{C_{1}}{C_{1}+C_{2}}}+\\underbrace{C_{3}+\\cdots+C_{3}}_{K-1}=1,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "that is C3 = $C_{3}=\\frac{C_{2}}{(K-1)(C_{1}+C_{2})}$ . Therefore, by Jensen\u2019s inequality, we can derive: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\log\\left(\\delta_{k}/\\displaystyle\\sum_{k^{\\prime}=1}^{K}\\delta_{k^{\\prime}}\\right)=\\log\\delta_{k}-\\log\\left(\\displaystyle\\sum_{k^{\\prime}=1}^{K}\\delta_{k^{\\prime}}\\right)}&{}\\\\ {=\\log\\delta_{k}-\\log\\left(\\displaystyle\\frac{C_{1}}{C_{1}+C_{2}}\\displaystyle\\frac{(C_{1}+C_{2})\\delta_{k}}{C_{1}}+C_{3}\\displaystyle\\sum_{k^{\\prime}\\neq k}\\frac{\\delta_{k^{\\prime}}}{C_{3}}\\right)}&{}\\\\ {\\leq\\log\\delta_{k}-\\displaystyle\\frac{C_{1}}{C_{1}+C_{2}}\\log\\left(\\displaystyle\\frac{(C_{1}+C_{2})\\delta_{k}}{C_{1}}\\right)-C_{3}\\displaystyle\\sum_{k^{\\prime}\\neq k}^{K}\\log\\displaystyle\\frac{\\delta_{k^{\\prime}}}{C_{3}}}&{}\\\\ {=M_{1}\\left(\\log\\delta_{k}-\\displaystyle\\frac{1}{K-1}\\displaystyle\\sum_{k^{\\prime}\\neq k}^{K}\\log\\delta_{k^{\\prime}}\\right)+M_{2},}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where M1 = $M_{1}\\,=\\,{\\frac{C_{2}}{C_{1}+C_{2}}}$ and M2 = ${\\cal M}_{2}\\,=\\,\\frac{C_{2}}{C_{1}+C_{2}}\\log C_{3}-\\frac{C_{1}}{C_{1}+C_{2}}\\log\\left(\\frac{C_{1}+C_{2}}{C_{1}}\\right)$ Therefore the lemma is proved. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Remark B.2. When C2/C1 = $C_{2}/C_{1}\\,=\\,\\frac{1}{K-1}\\exp\\left(\\log\\delta_{k}-\\frac{1}{K-1}\\sum_{k^{\\prime}\\neq k}^{K}\\log\\delta_{k^{\\prime}}\\right)$ , the right term of the inequality in lemma B.1 reaches its maximum. ", "page_idx": 15}, {"type": "text", "text": "Proof. Let $\\mathcal{M}=M_{1}\\left(\\log\\delta_{k}-\\frac{1}{K-1}\\sum_{k^{\\prime}\\neq k}^{K}\\log\\delta_{k^{\\prime}}\\right)+M_{2}$ in Lemma B.1. Then, upon computing the derivatives of $C_{1}$ and $C_{2}$ , we obtain: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial\\mathcal{M}}{\\partial C_{1}}=\\frac{1}{(C_{1}+C_{2})^{2}}\\left(-C_{2}\\frac{\\mathcal{M}-M_{2}}{M_{1}}+C_{2}\\log\\frac{(K-1)C_{2}}{C_{1}}\\right)}\\\\ {\\displaystyle\\frac{\\partial\\mathcal{M}}{\\partial C_{2}}=\\frac{1}{(C_{1}+C_{2})^{2}}\\left(C_{1}\\frac{\\mathcal{M}-M_{2}}{M_{1}}-C_{1}\\log\\frac{(K-1)C_{2}}{C_{1}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Combining these two equations yields the conclusion. ", "page_idx": 15}, {"type": "text", "text": "Next, we substitute the result of each logit into the lemma B.1, from which we can derive: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{L}=-\\frac{1}{N}\\sum_{k=1}^{K}\\sum_{i=1}^{n}y_{k,i}\\log\\frac{\\exp(w_{k,i}^{T}h_{i})}{\\sum_{k^{\\prime}=1}^{K}\\exp(w_{k^{\\prime},i}^{T}h_{i})}}\\\\ {\\displaystyle\\quad\\ge\\frac{C_{1}}{(C_{1}+C_{2})N(K-1)}\\sum_{i=1}^{n}\\left[\\left(\\sum_{k=1}^{K}h_{i,i}\\right)^{T}\\left(\\sum_{k=1}^{K}w_{k}\\right)-K\\sum_{k=1}^{K}h_{k,i}^{T}w_{k}\\right]+C_{4}}\\\\ {\\displaystyle\\quad=\\frac{C_{1}K}{(C_{1}+C_{2})N(K-1)}\\sum_{k=1}^{K}\\sum_{i=1}^{n}(\\bar{h}_{i}-h_{k,i})^{T}w_{k}+C_{4}}\\\\ {\\displaystyle\\quad\\ge\\frac{C_{1}}{(C_{1}+C_{2})N(K-1)}\\left(-\\frac{K}{2}\\sum_{k=1}^{K}\\sum_{i=1}^{n}\\|\\bar{h}_{i}-h_{k,i}\\|^{2}/C_{5}-\\frac{C_{5}N}{2}\\sum_{k=1}^{K}\\|w_{k}\\|^{2}\\right)+C_{4},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the second inequality applies Mean Inequalities, and $C_{4}\\;\\;=\\;\\;\\frac{C_{2}}{N(C_{1}+C_{2})}\\log C_{3}\\;-$ ${\\frac{C_{1}}{N(C_{1}+C_{2})}}\\log{\\left({\\frac{C_{1}+C_{2}}{C_{1}}}\\right)}.$ . For convenience, we denote $\\Tilde{\\mathcal{L}}=-\\frac{K}{2}\\sum_{k=1}^{K}\\sum_{i=1}^{n}\\Vert\\bar{h}_{i}-h_{k,i}\\Vert^{2}/C_{5}-$ $\\frac{C5N}{2}\\sum_{k=1}^{K}\\|\\pmb{w}_{k}\\|^{2}$ . As the corresponding constraints have already been added in (6), specifically the constraint $\\sum_{k=1}^{K}\\|\\pmb{w}_{k}\\|^{2}\\leq E_{W}$ , our focus shifts to discussing the situation concerning the first term. Since it represents the features of the final layer, we separately explore the differences in its extraction when using DEQ and fully connected layers. First suppose the extracted feature by the backbone is $h^{0}$ . ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "B.2 NC analysis ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We separately discuss the representation of ${\\mathcal{N C}}$ in the cases of Explicit NN and DEQ, and compare the lower bounds of the loss function. ", "page_idx": 16}, {"type": "text", "text": "B.2.1 ${\\mathcal{N C}}$ proof in Explicit neural networks ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For convenience, we assume there is only one layer in the feature extractor, that is, $\\boldsymbol{h}=\\boldsymbol{W}_{\\mathrm{EX}}\\boldsymbol{h}^{0}$ , then the first term in $\\tilde{\\mathcal{L}}$ becomes: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\displaystyle\\frac{K}{2}\\displaystyle\\sum_{k=1}^{K}\\sum_{i=1}^{n}\\|\\bar{\\pmb{h}}_{i}-\\pmb{h}_{k,i}\\|^{2}=-\\displaystyle\\frac{K}{2}\\displaystyle\\sum_{k=1}^{K}\\sum_{i=1}^{n}\\|W_{\\mathrm{EX}}(\\bar{\\pmb{h}}_{i}^{0}-\\pmb{h}_{k,i}^{0})\\|^{2}}\\\\ &{\\displaystyle~~~~~~~~~~~~~~~~~~~\\geq-\\displaystyle\\frac{K}{4}\\displaystyle\\sum_{k=1}^{K}\\sum_{i=1}^{n}\\left(\\|{\\pmb{W}}_{\\mathrm{EX}}\\|_{F}^{2}+\\|\\bar{\\pmb{h}}_{i}^{0}-\\pmb{h}_{k,i}^{0}\\|^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Substituting them into the loss function (19), we can observe that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\mathcal{L}}\\geq-\\displaystyle\\frac{N K}{4C_{5}}\\|W_{\\mathrm{EX}}\\|_{F}^{2}-\\displaystyle\\frac{K}{4C_{5}}\\sum_{k=1}^{K}\\sum_{i=1}^{n}\\|\\bar{h}_{i}^{0}-h_{k,i}^{0}\\|^{2}-\\displaystyle\\frac{C_{5}N K}{2}E_{W}}\\\\ &{\\quad=-\\displaystyle\\frac{K^{2}}{4C_{5}}\\sum_{i=1}^{n}\\displaystyle\\frac{1}{K}\\sum_{k=1}^{K}\\left(\\|h_{k,i}^{0}\\|^{2}-\\|\\bar{h}_{i}^{0}\\|^{2}\\right)-\\displaystyle\\frac{N K}{4C_{5}}\\|W_{\\mathrm{EX}}\\|_{F}^{2}-\\displaystyle\\frac{C_{5}N K}{2}E_{W}}\\\\ &{\\quad\\geq-\\displaystyle\\frac{K N}{4C_{5}}E_{H}-\\displaystyle\\frac{C_{5}K N}{2}E_{W}+\\displaystyle\\frac{K^{2}}{4C_{5}}\\sum_{i=1}^{n}\\|\\bar{h}_{i}^{0}\\|^{2}-\\displaystyle\\frac{N K}{4C_{5}}E_{H}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To acquire the lower bound of the loss function, we assign the value $C_{5}=\\sqrt{E_{H}/E_{W}}$ , the lower bound becomes: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{inf}{\\mathcal{L}}_{\\mathrm{EX}}=-{\\frac{C_{1}K}{(C_{1}+C_{2})(K-1)}}{\\sqrt{E_{W}E_{H}}}+C_{4}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Furthermore, the condition $\\|\\bar{h}_{i}^{0}\\|^{2}=0$ should also be satisfied, indicating that the average of the features for the i-th sample, $\\frac{1}{K}\\sum_{k=1}^{K}h_{k,i}^{0}$ , is equal to zero. ", "page_idx": 16}, {"type": "text", "text": "The satisfaction conditions for the inequalities include the following: ", "page_idx": 16}, {"type": "text", "text": "\u2022 In Eq. (19): The first inequality becomes equality when ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{(C_{1}+C_{2})h_{k,i}^{T}w_{k}}{C_{1}}=\\frac{h_{k,i}^{T}w_{k^{\\prime}}}{C_{3}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "that is, ", "page_idx": 16}, {"type": "equation", "text": "$$\nh_{k,i}^{T}\\pmb{w}_{k}=h_{k,i}^{T}\\pmb{w}_{k^{\\prime}}+\\log\\left(\\frac{C_{1}(K-1)}{C_{2}}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The second inequality is reduced to equality when $\\bar{h}_{i}-h_{k,i}=-C_{5}{\\pmb w}_{k}$ . ", "page_idx": 16}, {"type": "text", "text": "\u2022 In Eq. (20): $\\|W_{\\mathrm{EX}}\\|_{F}^{2}=\\sum_{k=1}^{K}\\sum_{i=1}^{n_{k}}\\|\\bar{h}_{i}^{0}-h_{k,i}^{0}\\|^{2}.$ ", "page_idx": 16}, {"type": "text", "text": "\u2022 In Eq. (21): When the following conditio n K1kK =1\u2225wk\u22252 = EW and \u2225W EX\u22252F = $\\frac{1}{K}\\sum_{k=1}^{K}\\sum_{i=1}^{n_{k}}\\|h_{k,i}^{0}\\|^{2}=E_{H}$ holds, the inequality was reduced to equality. ", "page_idx": 17}, {"type": "text", "text": "Since $||\\bar{h}_{i}^{0}||^{2}\\;=\\;0$ , it follows that $\\Vert\\bar{\\pmb{h}}_{i}\\Vert^{2}\\ =\\ \\Vert\\pmb{W}_{\\mathrm{EX}}\\bar{\\pmb{h}}_{i}^{0}\\Vert^{2}\\ =\\ 0$ . Combined with the condition $\\frac{1}{K}\\sum_{k=1}^{K}\\|\\pmb{w}_{k}\\|^{2}=E_{W}$ and $\\frac{1}{K}\\sum_{k=1}^{K}\\sum_{i=1}^{n_{k}}\\|h_{k,i}\\|^{2}=E_{H}$ , therefore, $h_{k}=h_{k,i}$ , for $\\forall k$ , that is, ${\\mathcal{N C}}1$ is proved. ", "page_idx": 17}, {"type": "text", "text": "Consequently, $h_{k,i}=C_{5}{\\pmb w}_{k}$ , demonstrating the validity of ${\\mathcal{N}}{\\mathcal{C}}3$ . For $\\mathcal{N C}2$ , since ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\sqrt{E_{H}/E_{W}}\\|w_{k}\\|^{2}=h_{k}w_{k}=h_{k}w_{k^{\\prime}}+\\log\\left(\\frac{C_{1}(K-1)}{C_{2}}\\right)=W_{\\mathrm{EX}}h_{k}^{0}{W_{k^{\\prime}}}+\\log\\left(\\frac{C_{1}(K-1)}{C_{2}}\\right),}}}\\\\ {{\\displaystyle\\sqrt{E_{H}/E_{W}}\\|w_{k^{\\prime}}\\|^{2}=h_{k^{\\prime}}w_{k^{\\prime}}=h_{k^{\\prime}}w_{k}+\\log\\left(\\frac{C_{1}(K-1)}{C_{2}}\\right)=W_{\\mathrm{EX}}h_{k^{\\prime}}^{0}{W_{k}}+\\log\\left(\\frac{C_{1}(K-1)}{C_{2}}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "holds, by the equality conditions, $\\|\\pmb{w}_{k}\\|^{2}=\\|\\pmb{w}_{k^{\\prime}}\\|^{2}=E_{W}$ . ", "page_idx": 17}, {"type": "text", "text": "Further, $\\sum_{k=1}^{K}h_{k}{w}_{k^{\\prime}}=\\sum_{k=1}^{K}W_{\\mathrm{EX}}h_{k}^{0}{w}_{k^{\\prime}}=0,\\mathrm{as}\\;h_{k}{w}_{k}=\\sqrt{E_{W}E_{H}},\\,\\mathrm{so}\\:h_{k}{w}_{k^{\\prime}}=\\mathrm{wh}\\:h_{k}^{0}{w}_{k^{\\prime}}=\\mathrm{wh}\\:h_{k}^{0}{w}_{k^{\\prime}}=0,\\mathrm{as}\\:h_{k}^{0}{w}_{k^{\\prime}}=\\mathrm{wh}\\:\\ensuremath{\\mathrm{wh}}\\:\\ensuremath{\\mathrm{wh}}\\;\\ensuremath{\\mathrm{in}}\\;\\ensuremath{\\mathrm{wh}}\\;\\ensuremath{\\mathrm{in}}\\;\\ensuremath{\\mathrm{wh}}\\;\\ensuremath{\\mathrm{in}}\\;\\ensuremath{\\mathrm{wh}}\\;\\ensuremath{\\mathrm{in}}\\;\\ensuremath{\\mathrm{wh}}\\;\\ensuremath{\\mathrm{in}}\\;\\ensuremath{\\mathrm{wh}}\\;\\ensuremath{\\mathrm{in}}\\;\\ensuremath{\\mathrm{wh}}\\;\\ensuremath{\\mathrm{in}}\\;\\ensuremath{\\mathrm{wh}}\\;\\ensuremath{\\mathrm{in}}\\;\\ensuremath{\\mathrm{wh}}\\;\\ensuremath{\\mathrm{in}}\\;\\ensuremath{\\mathrm{wh}}\\;\\ensuremath{\\mathrm{in}}\\;\\ensuremath{\\mathrm{wh}}\\;\\ensuremath{\\mathrm{in}}\\;\\ensuremath{\\mathrm{wh}}\\;\\ensuremath{\\mathrm{in}}\\;\\ensuremath{\\mathrm{wh}}\\;\\ensuremath{\\mathrm{in}}\\;\\ensuremath{\\mathrm{wh}}\\;\\ensuremath{\\mathrm{in}}\\;\\ensuremath{\\mathrm{wh}}\\;\\ensuremath{\\mathrm{in}}\\;\\ensuremath{\\mathrm{wh}}\\;\\ensuremath{\\mathrm{in}}\\;\\ensuremath{\\mathrm{wh}}\\;\\ensuremath{\\mathrm{in}}\\;\\ensuremath{\\mathrm{wh}}\\;\\ensuremath{\\mathrm{in}}\\;\\ensuremath{\\mathrm{wh}}\\;\\ensuremath{\\mathrm{wh}}\\;\\ensuremath{\\mathrm{in}}\\;\\ensuremath{\\mathrm{wh}}\\;\\ensuremath{\\mathrm{w1}}\\;\\ensuremath{\\mathrm{w}}\\mathrm{w\\mathrm{w}\\mathrm{w}\\mathrm{w}\\mathrm{in}\\mathrm{w}\\mathrm{w}\\mathrm{w}\\mathrm{w}\\mathrm{w}\\mathrm{w}\\mathrm{w}\\mathrm{c}\\mathrm{w}\\mathrm{w}\\mathrm{w}\\mathrm{w}\\mathrm{w}\\mathrm{c}}\\mathrm\\mathrm{w{w}}\\mathrm\\mathrm{in}$ $h_{k}\\mathbf{w}_{k^{\\prime}}=-\\frac{\\sqrt{E_{W}E_{H}}}{N-1}$ ", "page_idx": 17}, {"type": "text", "text": "Therefore, the $\\mathcal{N C}2$ condition satisfies: ", "page_idx": 17}, {"type": "equation", "text": "$$\nW\\pmb{W}^{T}=\\sqrt{E_{W}/E_{H}}\\pmb{W}\\pmb{H}=\\frac{K E_{W}}{K-1}\\left(\\mathbf{1}_{K}-\\frac{1}{k}\\mathbf{1}_{K}\\mathbf{1}_{K}^{T}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "B.2.2 ${\\mathcal{N C}}$ proof in DEQ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In the blocks for feature extraction, DEQ can be referred as a mapping from the features by backbone to the output $h^{0}\\to h^{\\star}$ , which can be directly solved using the implicit equation: ", "page_idx": 17}, {"type": "equation", "text": "$$\nh^{\\star}=f(W_{\\mathrm{DEQ}};h^{0})=\\sum_{i=1}^{\\infty}W_{\\mathrm{DEQ}}^{i}h^{0}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similar as the explicit case, start with the term: ", "page_idx": 17}, {"type": "equation", "text": "$$\n-\\frac{K}{2}\\sum_{k=1}^{K}\\sum_{i=1}^{n}\\|\\bar{h}_{i}-h_{k,i}\\|^{2}=\\frac{K}{2}\\sum_{k=1}^{K}\\sum_{i=1}^{n}\\left\\|\\sum_{j=0}^{\\infty}W_{\\mathrm{DEQ}}^{j}(\\bar{h}_{i}^{0}-h_{k,i}^{0})\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since the Neumann series can be regarded as a recursive procedure, denote $\\mathcal{G}_{k,i}^{j}=\\sum_{j^{\\prime}=0}^{j}W_{\\mathrm{DEQ}}^{j^{\\prime}}(\\bar{h}_{i}^{0}-$ $h_{k,i}^{0})\\left(j=0,1,\\cdots,\\infty\\right)$ , therefore $\\mathcal{G}_{k,i}^{j}=W_{\\mathrm{DEQ}}\\mathcal{G}_{k,i}^{j-1}+(\\bar{h}_{i}^{0}-h_{k,i}^{0})$ . ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\displaystyle\\frac{K}{2}\\displaystyle\\sum_{k=1}^{K}\\sum_{i=1}^{n}\\left\\|\\boldsymbol{\\mathcal{G}}_{k,i}^{j}\\right\\|^{2}=\\displaystyle\\frac{K}{2}\\sum_{k=1}^{K}\\sum_{i=1}^{n}\\left\\|\\boldsymbol{W}_{\\mathrm{DEQ}}\\boldsymbol{\\mathcal{G}}_{k,i}^{j-1}+(\\bar{h}_{i}^{0}-h_{k,i}^{0})\\right\\|^{2}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\geq-\\displaystyle\\frac{K}{2}\\sum_{k=1}^{K}\\sum_{i=1}^{n}\\left\\|\\boldsymbol{W}_{\\mathrm{DEQ}}\\boldsymbol{\\mathcal{G}}_{k,i}^{j-1}\\right\\|^{2}-\\displaystyle\\frac{K}{2}\\sum_{k=1}^{K}\\sum_{i=1}^{n}\\left\\|\\bar{h}_{i}^{0}-h_{k,i}^{0}\\right\\|^{2}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\geq-\\displaystyle\\frac{K}{4}\\sum_{k=1}^{K}\\sum_{i=1}^{n}\\left(\\|\\boldsymbol{W}_{\\mathrm{DEQ}}\\|_{F}^{2}+\\left\\|\\boldsymbol{\\mathcal{G}}_{k,i}^{j-1}\\right\\|^{2}\\right)-\\displaystyle\\frac{K}{2}\\sum_{k=1}^{K}\\sum_{i=1}^{n}\\left\\|\\bar{h}_{i}^{0}-h_{k,i}^{0}\\right\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Continuing the recursion, we can obtain: ", "page_idx": 17}, {"type": "equation", "text": "$$\n-\\frac{1}{2}\\|\\mathcal{G}_{k,i}^{j}\\|^{2}\\geq-\\left(\\frac{1}{2}\\right)^{j+1}\\left\\|\\mathcal{G}_{k,i}^{0}\\right\\|^{2}-\\left(1-\\frac{1}{2^{j}}\\right)\\left\\|h_{i}^{0}-h_{k,i}^{0}\\right\\|^{2}-\\left(\\frac{1}{2}-\\frac{1}{2^{j+1}}\\right)\\left\\|W_{\\mathrm{DEQ}}\\right\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "So, when $j\\rightarrow\\infty$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{-\\frac{K}{2}\\sum_{k=1}^{K}\\sum_{i=1}^{n}\\left\\|\\mathcal{G}_{k,i}^{0}\\right\\|^{2}=\\frac{K}{2}\\sum_{k=1}^{K}\\sum_{i=1}^{n}\\left\\|\\displaystyle\\sum_{j=0}^{\\infty}W_{\\mathrm{DEQ}}^{j}(\\bar{h}_{i}^{0}-h_{k,i}^{0})\\right\\|^{2}}}\\\\ &{}&{\\displaystyle\\geq-K\\sum_{k=1}^{K}\\sum_{i=1}^{n}\\left(\\left\\|h_{i}^{0}-h_{k,i}^{0}\\right\\|^{2}-\\frac{1}{2}\\left\\|W_{\\mathrm{DEQ}}\\right\\|_{F}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, use a similar proof as a fully connected layer, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\mathcal{L}}\\geq-\\displaystyle\\frac{K}{C_{5}}\\displaystyle\\sum_{k=1}^{K}\\sum_{i=1}^{n}\\left\\|h_{i}^{0}-h_{k,i}^{0}\\right\\|^{2}-\\displaystyle\\frac{N K}{2C_{5}}\\left\\|W_{\\mathrm{DEQ}}\\right\\|_{F}^{2}-\\displaystyle\\frac{C_{5}N K}{2}E_{W}}\\\\ &{\\quad=-\\displaystyle\\frac{K}{C_{5}}\\displaystyle\\sum_{i=1}^{n}\\left(\\frac{1}{K^{2}}\\displaystyle\\sum_{k=1}^{K}\\|h_{k,i}^{0}\\|^{2}-\\|\\bar{h}_{i}^{0}\\|^{2}\\right)-\\displaystyle\\frac{N K}{2C_{5}}\\left\\|W_{\\mathrm{DEQ}}\\right\\|_{F}^{2}-\\displaystyle\\frac{C_{5}N K}{2}E_{W}}\\\\ &{\\quad\\geq-\\displaystyle\\frac{N K}{C_{5}E_{H}}-\\frac{C_{5}N K}{2}E_{W}+\\displaystyle\\frac{K^{2}}{C_{5}}\\displaystyle\\sum_{i=1}^{n}\\|\\bar{h}_{i}^{0}\\|^{2}-\\displaystyle\\frac{N K}{2C_{5}}E_{H}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Set $C_{5}=\\sqrt{E_{H}/E_{W}}$ , the loss bound of the loss function becomes: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{inf}{\\mathcal{L}}_{\\mathrm{DEQ}}=-{\\frac{2C_{1}K}{(C_{1}+C_{2})(K-1)}}{\\sqrt{E_{W}E_{H}}}+C_{4}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In comparison with the lower bound of the loss function (22), it is evident that the loss function of the DEQ layer is significantly lower than that of the explicit neural network. Since the models are identical, according to Remark B.2, the values of $C_{1}$ and $C_{2}$ are nearly the same. This observation highlights the relatively stronger potential of DEQ compared to Explicit Neural Networks. ", "page_idx": 18}, {"type": "text", "text": "Also, the satisfaction conditions for the inequalities in DEQ settings include the following: ", "page_idx": 18}, {"type": "text", "text": "\u2022 In Eq. (19): The first inequality becomes equality when ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{(C_{1}+C_{2})h_{k,i}^{T}w_{k}}{C_{1}}=\\frac{h_{k,i}^{T}w_{k^{\\prime}}}{C_{3}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "that is, ", "page_idx": 18}, {"type": "equation", "text": "$$\nh_{k,i}^{T}\\pmb{w}_{k}=h_{k,i}^{T}\\pmb{w}_{k^{\\prime}}+\\log\\left(\\frac{C_{1}(K-1)}{C_{2}}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The second inequality is reduced to equality when $\\bar{\\pmb{h}}_{i}-\\pmb{h}_{k,i}=-C_{5}\\pmb{w}_{k}$ . This condition is quite similar to explicit fully connected layers. ", "page_idx": 18}, {"type": "text", "text": "\u2022 In Eq. (29): The first inequality: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\boldsymbol{W}_{\\mathrm{DEQ}}\\boldsymbol{\\mathcal{G}}_{k,i}^{j-1}=\\bar{\\boldsymbol{h}}_{i}^{0}-\\boldsymbol{h}_{k,i}^{0},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and the second inequality ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\lVert W_{\\mathrm{DEQ}}\\right\\rVert_{F}^{2}=\\left\\lVert\\mathcal{G}_{k,i}^{j-1}\\right\\rVert^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "\u2022 In Eq. (32): When the following condition ${\\frac{1}{K}}\\sum_{k=1}^{K}\\|\\pmb{w}_{k}\\|^{2}\\;=\\;E_{W}$ and $\\lVert W_{\\mathrm{DEQ}}\\rVert^{2}\\ =$ $\\frac{1}{K}\\sum_{k=1}^{K}\\sum_{i=1}^{n_{k}}\\|h_{k,i}\\|^{2}=E_{H}$ holds, the inequality was reduced to equality. ", "page_idx": 18}, {"type": "text", "text": "To summarize, DEQs are proposed for the memory-saving properties, as the forward passes can leverage any black-box root solvers [3, 5]. However, in terms of forward inference, explicit neural networks have limited learning capacity for data representation since they involve direct expressions computed in a single pass and backward propagation. In contrast, DEQ, lacking a direct explicit form, requires multiple rounds of parameter adjustments for learning. In each iteration, DEQ introduces input data in a sequential manner, allowing more adjustment space for learning parameters specific to the input. Therefore, to compare the two loss functions, we can derive the following theorem: ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Theorem B.3. DEQ achieves a lower bound on the loss function compared to explicit neural network under balanced datasets: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{inf}{\\mathcal{L}}_{D E Q}=-2C_{1}\\frac{K}{K-1}\\sqrt{E_{W}E_{H}}+C_{2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "while the lower bound of loss function of explicit neural network remains: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{inf}{\\mathcal{L}_{E X}}=-C_{1}\\frac{K}{K-1}\\sqrt{E_{W}E_{H}}+C_{2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $C_{1}$ and $C_{2}$ are two given constants. ", "page_idx": 19}, {"type": "text", "text": "Under the balanced dataset, the sample distribution of each class within each batch is relatively even. Therefore, during the fixed-point iteration process, both DEQ and explicit neural network can learn the features of each class relatively well, without showing significant differences. Besides, from a numerical perspective, the penalties $E_{W}$ and $E_{H}$ are generally not set to very large values, especially smaller than 1, so the difference between the two lower bounds in Theorem B.3 may not be substantial. Besides, as analyzed in Remark B.2, we can set $C_{2}$ in this two equations as identical, and once the propotion of logits in the explicit neural network is greater than the DEQ, the lower bound of loss function in DEQ is lower. ", "page_idx": 19}, {"type": "text", "text": "C Proof under imbalanced learning ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "C.1 Lower bound of the loss function ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Consider the loss function: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\underbrace{\\frac{K_{A}n_{A}}{N}\\sum_{k=1}^{K_{A}}\\sum_{i=1}^{n_{A}}\\mathcal{L}(W h,y_{k})}_{\\mathcal{L}_{A}}+\\underbrace{\\frac{K_{B}n_{B}}{N}}_{k=K_{A}+1}\\sum_{i=1}^{K_{B}}\\mathcal{L}(W h,y_{k})}_{\\mathcal{L}_{B}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "First analyze the loss in the majority class $\\mathcal{L}_{A}$ and introduce each term in the loss function. Suppose sample $i$ belongs to category $k$ , where $1\\leq k\\leq K_{A}$ , i.e., $k$ is a majority class. ", "page_idx": 19}, {"type": "text", "text": "By applying Jensen\u2019s inequalities, we can derive: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\log\\left(\\displaystyle\\frac{\\exp(h_{k,i}^{T}w_{k})}{\\sum_{k^{\\prime}=1}^{K}\\exp(h_{k,i}^{T}w_{k^{\\prime}})}\\right)}\\\\ &{=-h_{k,i}^{T}w_{k}+\\log\\left(C_{1}\\exp\\left(\\frac{h_{k,i}^{T}w_{k^{\\prime}}}{C_{1}}\\right)+C_{2}\\displaystyle\\sum_{k^{\\prime}\\neq k}^{K_{A}}\\exp\\left(\\frac{h_{k,i}^{T}w_{k^{\\prime}}}{C_{2}}\\right)+C_{3}\\sum_{k^{\\prime}=K_{A}+1}^{K}\\exp\\left(\\frac{h_{k,i}^{T}w_{k^{\\prime}}}{C_{3}}\\right)\\right)}\\\\ &{\\geq(C_{1}-1)h_{k,i}^{T}w_{k}+C_{2}\\displaystyle\\sum_{k^{\\prime}\\neq k}h_{k,i}^{T}w_{k^{\\prime}}+C_{3}\\displaystyle\\sum_{k^{\\prime}=K_{A}+1}^{K}h_{k,i}^{T}w_{k^{\\prime}}+c o n s t}\\\\ &{=C_{0}C_{a}\\left(\\displaystyle\\frac{1}{K_{A}}\\sum_{k^{\\prime}=1}^{K_{A}}h_{k^{\\prime}}^{T}w_{k^{\\prime}}-h_{k,i}^{T}w_{k}\\right)+C_{0}C_{5}\\left(\\displaystyle\\frac{1}{K_{B}}\\sum_{k^{\\prime}=K_{A}+1}^{K}h_{k,i}^{T}w_{k^{\\prime}}-h_{k,i}^{T}w_{k}\\right)+c o n s t}\\\\ &{=C_{0}C_{a}\\left(h_{k,i^{\\prime}}w_{k,i^{\\prime}}+h_{k,i^{\\prime}}^{T}w_{k^{\\prime}}\\right)+C_{0}C_{5}\\left(h_{k,i^{\\prime}}^{T}w_{B}-h_{k,i^{\\prime}}^{T}w_{k}\\right)+c o n s t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Here the value of const is $-C_{1}\\log C_{1}-(k_{A}\\,-\\,1)C_{2}\\log C_{2}\\,-\\,K_{B}C_{3}\\log C_{3}$ . Besides, $\\pmb{w}_{A}~=$ $\\frac{1}{K_{A}}\\sum_{k^{\\prime}=1}^{K_{A}}{w_{k^{\\prime}}}$ =1wk\u2032 and wB =KBk\u2032=K +1 $\\pmb{w}_{B}=\\frac{1}{K_{B}}\\sum_{k^{\\prime}=K_{A}+1}^{K}\\pmb{w}_{k^{\\prime}}$ represent the mean values of the weights in majority and minority classes, respectively. ", "page_idx": 19}, {"type": "text", "text": "To ensure the equality conditions hold, suppose there are three adaptive constants $a\\ \\mathrm{~>~}\\ 0$ , $b~>~0,~c~>~0$ . Denote $C_{1}~=~\\frac{a}{a+(K_{A}-1)b+K_{B}c}$ , $C_{2}\\ =\\ {\\frac{b}{a+(K_{A}-1)b+K_{B}c}}$ a + (KA \u22121)b + KBc, and $C_{3}~=~\\frac{c}{a+(K_{A}-1)b+K_{B}c}$ Additionally, to ensure $C_{4}+\\,C_{5}\\ =\\ 1$ , introduce a constant C0 =a + (KA \u22121)b + KBc, thus C4 = $C_{4}=\\frac{K_{A}b}{K_{A}b+K_{B}c}$ c and C5 =KAb + KBc. ", "page_idx": 20}, {"type": "text", "text": "After aggregating each term in the loss function, we obtain: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{K_{A}n_{A}}\\sum_{k=1}^{K_{A}}\\sum_{i=1}^{n_{A}}\\mathcal{L}(W h,y_{k})}\\\\ {\\displaystyle\\geq\\frac{1}{K_{A}n_{A}}\\sum_{k=1}^{K_{A}}\\sum_{i=1}^{n_{A}}C_{i}\\left(h_{k,i}^{T}w_{A}-h_{k,i}^{T}w_{k}\\right)+C_{5}\\left(h_{k,i}^{T}w_{B}-h_{k,i}^{T}w_{k}\\right)+c o n s t}\\\\ {\\displaystyle=\\frac{1}{K_{A}}\\sum_{k=1}^{K_{A}}h_{k}^{T}(C_{4}w_{A}+C_{5}w_{B}-w_{k})+c o n s t,}\\\\ {\\displaystyle_{k}=\\frac{1}{n_{A}}\\sum_{i=1}^{n_{B}}h_{k,i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where ", "page_idx": 20}, {"type": "text", "text": "Subsequently, consider the lower bound of ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K_{A}}h_{k}^{T}(C_{4}w_{A}+C_{5}w_{B}-w_{k})\\geq-\\frac{C_{6}}{2}\\sum_{k=1}^{K_{A}}\\|h_{k}\\|^{2}-\\sum_{k=1}^{K_{A}}\\frac{1}{2}\\|C_{4}w_{A}+C_{5}w_{B}-w_{k}\\|^{2}/C_{6}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that this inequality (41) is reduced to equality only when the following equality holds: ", "page_idx": 20}, {"type": "equation", "text": "$$\nC_{4}{\\pmb w}_{A}+C_{5}{\\pmb w}_{B}-{\\pmb w}_{k}=C_{6}{\\pmb h}_{k},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $1\\leq k\\leq K_{A}$ ", "page_idx": 20}, {"type": "text", "text": "Continuing the analysis of inequality (41), the first term on the right-hand side can be bounded as: Case 1: (Explicit fully connected layers) ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle-\\sum_{k=1}^{K_{A}}\\|h_{k}\\|^{2}=-\\displaystyle\\sum_{k=1}^{K_{A}}\\|W_{\\mathrm{EX}}h_{k}^{0}\\|^{2}}\\\\ {\\displaystyle\\geq-\\displaystyle\\frac{1}{2}\\left(K_{A}\\|W_{\\mathrm{EX}}\\|_{F}+\\displaystyle\\sum_{k=1}^{K_{A}}\\|h_{k}^{0}\\|^{2}\\right)}\\\\ {\\displaystyle\\geq-\\displaystyle\\frac{1}{2}\\left(K_{A}\\|W_{\\mathrm{EX}}\\|_{F}+\\displaystyle\\sum_{k=1}^{K_{A}}\\frac{1}{n_{k}}\\displaystyle\\sum_{i=1}^{n_{k}}\\|h_{k,i}^{0}\\|^{2}\\right)}\\\\ {\\displaystyle>-K_{A}E_{H}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Case 2: (Deep Equilibrium Models) ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle-\\sum_{k=1}^{K_{A}}\\|h_{k}\\|^{2}=-\\displaystyle\\sum_{k=1}^{K_{A}}\\big\\|\\big(I-W_{\\mathrm{DEQ}}\\big)^{-1}h_{k}^{0}\\big\\|^{2}}\\\\ &{\\ge-\\displaystyle\\frac{1}{2}\\left(K_{A}\\displaystyle\\sum_{j=0}^{\\infty}\\|W_{\\mathrm{DEQ}}\\|_{F}^{j}+\\displaystyle\\sum_{k=1}^{K_{A}}\\|h_{k}\\|^{2}\\right)}\\\\ &{\\ge-\\displaystyle\\frac{1}{2}\\left(K_{A}\\displaystyle\\sum_{j=0}^{\\infty}E_{H}^{j}+\\displaystyle\\sum_{k=1}^{K_{A}}\\displaystyle\\frac{1}{n_{k}}\\displaystyle\\sum_{i=1}^{n_{k}}\\|h_{k,i}\\|^{2}\\right)}\\\\ &{\\ge-\\displaystyle\\frac{1}{2}\\left(\\displaystyle\\frac{1}{1-E_{H}}+E_{H}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Compared the lower bound of explicit neural network and DEQ, we can find that: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left(-\\sum_{k=1}^{K_{A}}\\|h_{k}\\|^{2}\\right)_{\\mathrm{DEQ}}<\\left(-\\sum_{k=1}^{K_{A}}\\|h_{k}\\|^{2}\\right)_{\\mathrm{EX}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for all $E_{H}\\neq1$ . ", "page_idx": 21}, {"type": "text", "text": "We now shift our attention to the second term (Ref. Eq [82-83] in [14]): ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\frac{1}{K_{A}\\sum_{k=1}^{K}|C_{4}w_{k}+C_{3}w_{k}-w_{k}|^{2}}}\\\\ &{=-\\frac{1}{K_{A}\\sum_{k=1}^{K}|\\mathbf{r}_{k}|}|^{2}+\\frac{2}{K_{A}\\sum_{k=1}^{K}\\mathbf{r}_{k}^{2}(C_{4}w_{k}+C_{5}w_{k})-||C_{4}w_{4}+C_{5}w_{k}|^{2}|^{2}}}\\\\ &{=-\\frac{1}{K_{A}\\sum_{k=1}^{K}|\\mathbf{r}_{k}|}|^{2}+2C_{5}^{2}w_{k}w_{1}+C_{4}(2-C_{4})|w_{4}|^{2}-C_{5}|w_{1}|^{2}}\\\\ &{=-\\frac{1}{K_{A}\\sum_{k=1}^{K}|\\mathbf{r}_{k}|}|^{2}+\\frac{2}{K_{A}\\sum_{k=1}^{K}\\mathbf{r}_{k}|}|\\mathbf{r}_{k}|^{2}+C_{4}(2-C_{4})|w_{1}+\\frac{C_{4}^{2}}{C_{4}(2-C_{4})}w_{1}|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ &{\\qquad-\\left(C_{5}^{2}+\\frac{C_{5}^{2}}{C_{4}(2-C_{4})}\\right)|w_{1}|^{2}}\\\\ &{\\geq-\\frac{K}{K_{A}\\sum_{k=1}^{K}}\\left(\\frac{1}{K_{B}}-C_{5}^{2}-\\frac{C_{5}^{2}}{C_{4}(2-C_{4})}\\right)|w_{1}|^{2}+\\frac{1}{K_{A}\\sum_{k=1}^{K}|\\mathbf{r}_{k}|}|w_{k}-w_{B}|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+c_{1}(2-C_{4})\\left|\\mathbf{r}_{k}+\\frac{C_{5}^{2}}{C_{4}(2-C_{4})}w_{1}\\right|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $K_{R}=K_{A}/K_{B}$ denotes the ratio of the number of majority classes to minority classes. In summary, the lower bound of loss function (40) could be simplified as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{C_{A}=\\frac{1}{K_{A}m_{A}}\\sum_{k=1}^{K_{A}}\\sum_{l=K}^{K_{A}}\\mathcal{L}(W h_{k,i},y_{k})}}\\\\ &{\\geq\\frac{1}{K_{A}}\\sum_{k=1}^{K_{A}}h_{k}^{T}(C_{G}w_{A}+C_{S}w_{B}-w_{k})+c o n s t}\\\\ &{\\geq-\\frac{C_{G}}{2K_{A}}\\sum_{k=1}^{K_{A}}\\|h_{k}\\|^{2}-\\frac{1}{2K_{A}}\\sum_{k=1}^{K_{A}}\\|C_{4}w_{A}+C_{S}w_{B}-w_{k}\\|^{2}/C_{6}+c o n s t}\\\\ &{\\geq\\frac{C_{G}}{2K_{A}}M-\\frac{K E_{W}}{2C_{G}K_{A}}+\\frac{1}{2C_{G}}\\left(\\frac{1}{K_{B}}-C_{S}^{2}-\\frac{C_{S}^{4}}{C_{4}(2-C_{A})}\\right)\\|w_{B}\\|^{2}}\\\\ &{\\qquad+\\frac{C_{4}(2-C_{G})}{C_{G}}\\left\\|w_{A}+\\frac{C_{S}^{2}}{C_{4}(2-C_{G})}w_{B}\\right\\|^{2}+\\frac{1}{2C_{G}K_{A}}\\sum_{k=1}^{K}\\|w_{k}-w_{B}\\|^{2}+c o n s t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $M=-K_{A}E_{H}$ if the network is a fully connected layer and $\\begin{array}{r}{M=-\\frac{K_{A}}{2}\\left(\\frac{1}{1-E_{H}}+E_{H}\\right)}\\end{array}$ if the network is a Deep Equilibrium Model. ", "page_idx": 21}, {"type": "text", "text": "Similarly, the loss function w.r.t the minority classes is bounded as: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{B}=\\displaystyle\\frac{1}{K_{B}n_{B}}\\sum_{k=1}^{K_{B}n_{B}}\\sum_{\\ell=1}^{n_{B}n_{B}}\\mathcal{L}(W h_{k,i},y_{k})}\\\\ &{\\quad=\\displaystyle\\frac{1}{K_{B}}\\sum_{k=1}^{K_{B}}h_{k}^{T}(C_{4}w_{A}+C_{5}w_{B}-w_{k})+c o n s t}\\\\ &{\\quad\\geq-\\displaystyle\\frac{C_{6}}{2K_{B}}\\sum_{k=1}^{K_{B}}\\|h_{k}\\|^{2}-\\frac{1}{2K_{B}}\\sum_{k=1}^{K_{B}}\\|C_{4}w_{A}+C_{5}w_{B}-w_{k}\\|^{2}/C_{6}+c o n s t}\\\\ &{\\quad\\geq\\displaystyle\\frac{C_{6}}{2K_{B}}M-\\frac{K E_{W}}{2C_{6}K_{B}}+\\frac{1}{2C_{6}}\\left(K_{R}-C_{5}^{2}-\\frac{C_{5}^{4}}{C_{4}(2-C_{4})}\\right)\\|w_{A}\\|^{2}}\\\\ &{\\qquad+\\displaystyle\\frac{C_{5}(2-C_{5})}{C_{6}}\\left\\|\\frac{C_{4}^{2}}{C_{5}(2-C_{5})}w_{A}+w_{B}\\right\\|^{2}+\\displaystyle\\frac{1}{2C_{6}K_{B}}\\sum_{k=1}^{K_{A}}\\|w_{k}-w_{A}\\|^{2}+c o n s t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The inequality reduces to equality when the constraints in $\\mathcal{C}$ are treated as equalities, achieving the upper bound. Additionally, the following equalities must hold: ", "page_idx": 22}, {"type": "equation", "text": "$$\nC_{4}{\\pmb w}_{A}+C_{5}{\\pmb w}_{B}-{\\pmb w}_{k}=C_{6}{\\pmb h}_{k},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $K_{A}+1\\leq k\\leq K$ . ", "page_idx": 22}, {"type": "text", "text": "If $K_{R}=1$ , i.e., the number of majority classes is equal to the number of minority classes, the results of (46) and (47) are totally equivalent. ", "page_idx": 22}, {"type": "text", "text": "Therefore, without loss of generality, assuming $K_{A}>K_{B}$ , the lower bound of the loss function (38) can be simplified to: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\mathcal{L}}={\\mathcal{L}}_{A}+{\\mathcal{L}}_{B}}}\\\\ {~~}\\\\ {{\\displaystyle~\\geq\\frac{C_{6}M}{2}\\left(\\frac{1}{K_{A}}+\\frac{1}{K_{B}}\\right)+\\frac{1}{2C_{6}K_{B}}\\sum_{k=1}^{K_{A}}\\|w_{k}-w_{A}\\|^{2}+\\frac{1}{2C_{6}K_{A}}\\sum_{k=1}^{K_{B}}\\|w_{k}-w_{B}\\|^{2}}}\\\\ {~~}\\\\ {{\\displaystyle~~~+\\frac{C_{4}(2-C_{4})}{2C_{6}}\\left\\|w_{A}+\\frac{C_{5}^{2}}{C_{4}(2-C_{4})}w_{B}\\right\\|^{2}+\\frac{C_{5}(2-C_{5})}{2C_{6}}\\left\\|\\frac{C_{4}^{2}}{C_{5}(2-C_{5})}w_{A}+w_{B}\\right\\|^{2}}}\\\\ {~~}\\\\ {{\\displaystyle~~~+\\frac{1}{2C_{6}}\\left(\\frac{1}{K_{R}}-C_{5}^{2}-\\frac{C_{5}^{4}}{C_{4}(2-C_{4})}\\right)\\|w_{B}\\|^{2}+\\frac{1}{2C_{6}}\\left(K_{R}-C_{5}^{2}-\\frac{C_{5}^{4}}{C_{4}(2-C_{4})}\\right)\\|w_{A}\\|^{2}+c o n s t.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "C.2 NC Analysis ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "As analyzed in (43) and (44), when it reaches the minimal value, each $h_{k,i}~=~h_{k}$ for $\\forall k\\;=$ $1,2\\cdots\\,K_{A}$ . Similarly, this holds for minority class with $K_{A}+1\\leq k\\leq K$ . This implies that, in an imbalanced scenario, both DEQ and fully connected layer exhibit feature collapse, i.e., ${\\mathcal{N C}}1$ is still present. ", "page_idx": 22}, {"type": "text", "text": "As we need to calculate the lower bound of the loss function, it is essential to minimize the terms $\\mathcal{L}_{A}+\\mathcal{L}_{B}$ as much as possible. ", "page_idx": 22}, {"type": "text", "text": "Therefore, consider the gradient with respect to $\\pmb{w}_{k}$ for majority class and $\\pmb{w}_{k}$ for minority class, respectively. First compute the case with $1\\leq k\\leq K_{A}$ . ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\frac{\\partial\\mathcal{L}}{\\partial{\\pmb w}_{k}}=\\frac{1}{C_{6}K_{B}}\\left(1-\\frac{1}{K_{A}}\\right)({\\pmb w}_{k}-{\\pmb w}_{A})+\\frac{C_{4}(2-C_{4})}{C_{6}K_{A}}\\left({\\pmb w}_{A}+\\frac{C_{5}^{2}}{C_{4}(2-C_{4})}{\\pmb w}_{B}\\right)}}\\\\ {{+\\,\\frac{C_{4}^{2}}{K_{A}C_{6}}\\left(\\frac{C_{4}^{2}}{C_{5}(2-C_{5})}{\\pmb w}_{A}+{\\pmb w}_{B}\\right)}}\\\\ {{+\\,\\frac{1}{K_{A}C_{6}}\\left(K_{R}-C_{5}^{2}-\\frac{C_{5}^{4}}{C_{4}(2-C_{4})}\\right){\\pmb w}_{A}=0.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "So, we can derive that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\left(K_{R}-\\frac{1}{K_{B}}\\right)w_{k}+\\frac{1}{1-C_{4}^{2}}w_{B}}&&{}\\\\ {\\displaystyle\\qquad\\qquad+\\left(\\frac{1}{K_{B}}+C_{4}(2-C_{4})+\\frac{C_{4}^{4}}{C_{5}(2-C_{5})}-C_{5}^{2}-\\frac{C_{5}^{2}}{C_{4}(2-C_{4})}\\right)w_{A}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "One important note here is that when the proportion of majority class samples approaches infinity, i.e., C4 \u21921, we have1\u22121C2 $\\begin{array}{r}{\\frac{1}{1-C_{4}^{2}}\\to0}\\end{array}$ . In this scenario, the weights $\\pmb{w}_{k}$ belonging to the majority class are almost exclusively related to $\\pmb{w}_{A}$ , and have little dependence on the average of the minority class $\\pmb{w}_{B}$ , which validates the results of Proposition 4.2. ", "page_idx": 23}, {"type": "text", "text": "Similarly, if $K_{A}+1\\,\\le\\,k\\,\\le\\,K$ , the following equality will hold to ensure optimality of $\\pmb{w}_{k}$ in minority classes of (49): ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{\\displaystyle\\left(\\frac{1}{K_{R}}-\\frac{1}{K_{A}}\\right)w_{k}+\\frac{1}{1-C_{5}^{2}}w_{A}}}&{{}}&{{}}\\\\ {{}}&{{\\displaystyle+\\left(\\frac{1}{K_{A}}+C_{5}(2-C_{5})+\\frac{C_{5}^{4}}{C_{4}(2-C_{4})}-C_{4}^{2}-\\frac{C_{4}^{2}}{C_{5}(2-C_{5})}\\right)w_{B}=0.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Next, we consider the conditions for the validity in $\\mathcal{N}\\mathcal{C}2$ and ${\\mathcal{N}}{\\mathcal{C}}3$ , then compare the performance of DEQ and explicit neural network. ", "page_idx": 23}, {"type": "text", "text": "Therefore, for the majority class $1\\leq k\\leq K_{A}$ , suppose it reaches its minimum value, recall the condition (42), and combined with (51), we can derive: ", "page_idx": 23}, {"type": "equation", "text": "$$\nC_{6}h_{k}^{T}h_{k^{\\prime}}=\\left(C_{4}+\\frac{K_{B}C_{A}}{K_{A}-1}\\right)w_{A}^{T}h_{k^{\\prime}}+\\left(C_{5}+\\frac{K_{B}}{(K_{A}-1)(1-C_{4}^{2})}\\right)w_{B}^{T}h_{k^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Similarly, for the minority class $K_{A}+1\\leq k\\leq K$ , combine (48) with (52), we obtain: ", "page_idx": 23}, {"type": "equation", "text": "$$\nC_{6}h_{k}^{T}h_{k^{\\prime}}=\\left(C_{4}+\\frac{K_{A}}{(K_{B}-1)(1-C_{5}^{2})}\\right)w_{A}^{T}h_{k^{\\prime}}+\\left(C_{5}+\\frac{K_{A}C_{B}}{K_{B}-1}\\right)w_{B}^{T}h_{k^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In the above two equations, $k^{\\prime}=1,2,\\cdots\\,,K$ . And we denote the coefficient of $\\pmb{w}_{A}$ in Eq. (51) and the coefficient of $\\pmb{w}_{B}$ in Eq. (52) as $C_{A}$ and $C_{B}$ respectively for simplicity. After this deviation, we can find that both of the coefficients of $w_{A}^{T}h_{k^{\\prime}}$ and $\\pmb{w}_{B}^{T}\\pmb{h}_{k^{\\prime}}$ are constants. ", "page_idx": 23}, {"type": "text", "text": "It can be obviously concluded that $\\mathcal{N}\\mathcal{C}2$ and $\\mathcal{N C}3$ do not hold under imbalanced dataset conditions. However, we can still compare the numerical differences between them under DEQ and fully connected layer. By adaptively specifying parameters $C_{4}$ and $C_{5}$ , we can denote $(h_{k}^{0})^{T}h_{k^{\\prime}}^{0}=m_{k,k^{\\prime}}$ ", "page_idx": 23}, {"type": "text", "text": "Thus, by considering all the equality conditions in (46), we can measure the distance from the features to the Simplex ETF. ", "page_idx": 23}, {"type": "equation", "text": "$$\nC_{6}h_{k^{\\prime}}^{T}h_{k}=C_{4}h_{k^{\\prime}}^{T}w_{A}+C_{5}h_{k^{\\prime}}^{T}w_{B}-h_{k^{\\prime}}^{T}w_{k}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Case 1: (Explicit fully connected layers) ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle C_{6}(h_{k^{\\prime}}^{0})^{T}h_{k}=C_{4}W_{\\mathrm{EX}}(h_{k^{\\prime}}^{0})^{T}w_{A}+C_{5}W_{\\mathrm{EX}}(h_{k^{\\prime}}^{0})^{T}w_{B}-W_{\\mathrm{EX}}(h_{k^{\\prime}}^{0})^{T}w_{k}}&{{}}\\\\ {\\displaystyle=W_{\\mathrm{EX}}\\left(C_{4}h_{k^{\\prime}}^{T}w_{A}+C_{5}h_{k^{\\prime}}^{T}w_{B}-h_{k^{\\prime}}^{T}w_{k}\\right)}&{{}}\\\\ {\\displaystyle}&{{\\le\\frac{1}{2}\\|{\\boldsymbol W}_{\\mathrm{EX}}\\|_{\\mathrm{F}}+\\frac{1}{2}\\left\\|C_{4}h_{k^{\\prime}}^{T}w_{A}+C_{5}h_{k^{\\prime}}^{T}w_{B}-h_{k^{\\prime}}^{T}w_{k}\\right\\|}}\\\\ {{}}&{{=E_{H}+\\frac{1}{2}M.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Case 2: (Deep Equilibrium Models) ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{6}(h_{k^{\\prime}}^{0})^{T}h_{k}=C_{4}(I-W_{\\mathrm{DEQ}})^{-1}(h_{k^{\\prime}}^{0})^{T}w_{A}+C_{5}(I-W_{\\mathrm{DEQ}})^{-1}(h_{k^{\\prime}}^{0})^{T}w_{B}}\\\\ &{\\qquad\\qquad\\qquad\\qquad-(I-W_{\\mathrm{DEQ}})^{-1}(h_{k^{\\prime}}^{0})^{T}w_{k}}\\\\ &{\\qquad=(I-W_{\\mathrm{DEQ}})^{-1}\\left(C_{4}h_{k^{\\prime}}^{T}w_{A}+C_{5}h_{k^{\\prime}}^{T}w_{B}-h_{k^{\\prime}}^{T}w_{k}\\right)}\\\\ &{\\leq\\frac{1}{2}\\|(I-W_{\\mathrm{DEQ}})^{-1}\\|_{F}+\\frac{1}{2}\\left\\|C_{4}h_{k^{\\prime}}^{T}w_{A}+C_{5}h_{k^{\\prime}}^{T}w_{B}-h_{k^{\\prime}}^{T}w_{k}\\right\\|}\\\\ &{=\\frac{1}{2(1-E_{H})}+\\frac{1}{2}M.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, to compare these two models, we consider the case when the distance of these two models from the Simplex ETF is minimized. We denote each element in the Simplex ETF as $s_{i j}$ and compare the differences between them. When the distance of DEQ is relatively smaller than that of explicit neural network, we can obtain: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{2(1-E_{H})}+\\frac{1}{2}m-s\\right|<\\left|\\frac{1}{2}E_{H}+\\frac{1}{2}m-s\\right|.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For simplicity, we only consider the subscripts of $\\pmb{s}$ and $\\mathbf{\\nabla}m$ , and denote $\\begin{array}{r}{\\widehat{\\mathbb{O}}=\\frac{1}{2(1-E_{H})}+\\frac{1}{2}m-s}\\end{array}$ and $\\begin{array}{r}{\\widehat{\\boldsymbol{\\geq}}=\\frac{1}{2}E_{H}+\\frac{1}{2}m-s}\\end{array}$ . We then classify and discuss their magnitudes. ", "page_idx": 24}, {"type": "text", "text": "\u2022 $\\textcircled{1}>0,\\textcircled{2}>0$ :   \nSince $\\textstyle{\\frac{1}{2(1-E_{H})}}>E_{H}$ , there is a contradiction! Therefore, it does not hold.   \n\u2022 $\\textcircled{1}>0,\\textcircled{2}<0$ : $\\begin{array}{r}{\\frac{1}{2(1-E_{H})}+\\frac{1}{2}m-s<0,\\,\\frac{1}{2}E_{H}+\\frac{1}{2}m-s>0}\\end{array}$ , which means $\\begin{array}{r}{\\frac{1}{2(1-E_{H})}<E_{H}}\\end{array}$ . And that is a contradiction!   \n\u2022 $\\textcircled{1}<0,\\textcircled{2}>0$ :   \nSince $\\begin{array}{r}{\\frac{1}{2(1-E_{H})}+\\frac{1}{2}m-s>0,\\,\\frac{1}{2}E_{H}+\\frac{1}{2}m-s<0}\\end{array}$ , we have $E_{H}<2s-m<{\\frac{1}{1-E_{H}}}.$ ", "page_idx": 24}, {"type": "text", "text": "Besides, by the inequality (58), we have 12EH +2(1\u22121EH) $\\textstyle{\\frac{1}{2}}E_{H}\\,+\\,{\\frac{1}{2(1-E_{H})}}\\,<\\,2s\\,-\\,m$ . Then find the intersection, we obtain that: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{2}E_{H}+\\frac{1}{2(1-E_{H})}<2s-m<\\frac{1}{1-E_{H}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "\u2022 $\\textcircled{1}<0,\\textcircled{2}<0$ : ", "page_idx": 24}, {"type": "text", "text": "Since2(1\u2212EH) $\\textstyle{\\frac{1}{2(1-E_{H})}}+\\frac{1}{2}\\pmb{m}-\\pmb{s}<0$ and $\\textstyle{\\frac{1}{2}}E_{H}+{\\frac{1}{2}}m-s<0$ , it implies that $\\textstyle{\\frac{1}{2(1-E_{H})}}>E_{H}$ always holds. ", "page_idx": 24}, {"type": "text", "text": "Therefore, we only need to ensure that ", "page_idx": 24}, {"type": "equation", "text": "$$\n2s-m>\\operatorname*{min}\\left\\{E_{H},\\frac{1}{1-E_{H}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Combining these four cases and finding their intersection, we conclude that when the inequality ", "page_idx": 24}, {"type": "equation", "text": "$$\nE_{H}<2s-m<{\\frac{1}{1-E_{H}}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "is satisfied, the performance of DEQ is better than that of explicit neural network. ", "page_idx": 24}, {"type": "text", "text": "As for ${\\mathcal{N}}{\\mathcal{C}}3$ , consider the cosine distance with the feature $h_{k}$ and $\\pmb{w}_{k}$ . ", "page_idx": 24}, {"type": "text", "text": "Case 1: (Explicit fully connected layers) ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{cos}(h_{k},w_{k})_{\\mathrm{EX}}=\\frac{h_{k}^{T}w_{k}}{\\|w_{k}\\|\\|h_{k}\\|}}&{}\\\\ {=\\frac{W_{\\mathrm{EX}}~\\left(h_{k}^{0}\\right)^{T}w_{k}}{\\|w_{k}\\|\\|W_{\\mathrm{EX}}h_{k}^{0}\\|}}&{}\\\\ {\\geq\\frac{2W_{\\mathrm{EX}}~\\left(h_{k}^{0}\\right)^{T}w_{k}}{\\|w_{k}\\|^{2}+\\frac{1}{2}\\|W_{\\mathrm{EX}}\\|^{2}+\\frac{1}{2}\\|h_{k}^{0}\\|}}&{}\\\\ {\\geq\\frac{2E_{H}\\left(h_{k}^{0}\\right)^{T}w_{k}}{E_{W}+E_{H}}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Case 2: (Deep Equilibrium Model) ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Very similarly, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\cos(h_{k},w_{k})_{\\mathrm{DEQ}}=\\frac{h_{k}^{T}w_{k}}{\\|w_{k}\\|\\|h_{k}\\|}}&{}\\\\ {\\displaystyle=\\frac{\\left(I-W_{\\mathrm{DEQ}}\\right)\\left(h_{k}^{0}\\right)^{T}w_{k}}{\\|w_{k}\\|\\left\\|\\left(I-W_{\\mathrm{DEQ}}\\right)^{-1}h_{k}^{0}\\right\\|}}&{}\\\\ {\\displaystyle\\geq\\frac{2\\left(I-W_{\\mathrm{DEQ}}\\right)\\left(h_{k}^{0}\\right)^{T}w_{k}}{\\|w_{k}\\|^{2}+\\frac{1}{2}\\|\\left(I-W_{\\mathrm{DEQ}}\\right)\\|^{2}+\\frac{1}{2}\\|h_{k}^{0}\\|}}&{}\\\\ {\\displaystyle\\geq\\frac{4E_{H}\\left(h_{k}^{0}\\right)^{T}w_{k}}{1+2(E_{W}+E_{H})(1-E_{H})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "If the performance of DEQ is better than explicit neural network, then we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\cos(h_{k},\\boldsymbol{w}_{k})_{\\mathrm{DEQ}}/\\cos(h_{k},\\boldsymbol{w}_{k})_{\\mathrm{exp}}>1,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which is equivalent to ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{E_{H}}{E_{w}+E_{H}}+E_{H}(1-E_{H})<2.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In summary, though DEQ does not completely mitigate the issue of minority collapse, it shows significant improvement compared to explicit neural network under some conditions that are relatively easy to satisfy in the manifestation of the ${\\mathcal{N C}}$ phenomenon. ", "page_idx": 25}, {"type": "text", "text": "D More experiments ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we provide more experimental results, including the ${\\mathcal{N C}}$ phenomena of Explicit NN and DEQ, and the training results under other imbalanced conditions. ", "page_idx": 25}, {"type": "table", "img_path": "obUXeUMmq1/tmp/301fcc0c9da7bc480d4b81840e818b5fc8be7a9854226254ac0243439a2a72d3.jpg", "table_caption": ["Table 3: Test Accuracy on Cifar-10 and Cifar-100 Dataset with $K_{A}=5$ "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "obUXeUMmq1/tmp/4dac34192c9fac99ceb854d337611900383fbc405aa576be66d2874bd9281b86.jpg", "table_caption": ["Table 4: Test Accuracy on Cifar-10 and Cifar-100 Dataset with $K_{A}=7$ "], "table_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "obUXeUMmq1/tmp/b2f660a030c1b5a869e67bc9ba89cbe2c48068bf0d8b72db44d15924422fc13f.jpg", "img_caption": ["Figure 4: Accuracy and ${\\mathcal{N C}}$ phenomenon on imbalanced dataset with $K_{A}=3$ , $K_{B}=7$ , $R=100$ "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "ResNet-18 DEQ ", "text_level": 1, "page_idx": 27}, {"type": "image", "img_path": "obUXeUMmq1/tmp/d217fcdad0ef99f4dead9f598513ba763faa9654ccaf547fdca85b3a0bfd504e.jpg", "img_caption": ["Figure 5: Accuracy and ${\\mathcal{N C}}$ phenomenon on imbalanced dataset with $K_{A}=7$ , $K_{B}=3$ , $R=100$ "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: As stated in the abstract and introduction, this paper is the first to analyze the representation of the Deep Equilibrium Model from the perspective of Neural Collapse, accurately reflecting the key contributions and scope. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: The limitation analysis is provided in the Conclusion. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The assumptions and proofs are provided in appendix B and C. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: These details are provided in Section 5.1 - Experiment setup. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We will release the code once the paper is accepted. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: These details are provided in Section 5.1 - Experiment setup. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The standard deviation in our experimental results (Table 1-4) shows the statistical significance. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The computer resources are provided in Section 5.1 - Experiment setup. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Our research conformed with the NeurIPS Code of Ethics. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: Our paper primarily focuses on theoretical research in machine learning, comparing two typical neural network algorithms, with no relevance to societal impacts. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our paper uses standard CIFAR-10 and CIFAR-100 datasets, which do not involve such issues. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The datasets CIFAR-10 and CIFAR-100 are cited properly. Other assets are not applied in this paper. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper does not introduce new assets. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]