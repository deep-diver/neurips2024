[{"figure_path": "ZYNYhh3ocW/figures/figures_3_1.jpg", "caption": "Figure 1: (a) DNAS struggles to accurately generate circuits, especially larger ones. (b) The depth of an output node of the circuit in the circuit. DNAS only connects to very shallow layers, while our method learns deeper layers. (c) The visualization of the converged DNAS network. The dark nodes represent the used circuit nodes, indicating very low utilization of deep-layer nodes. (d) The circuits generated by SOP show that the average number of nodes per layer forms a triangular pattern.", "description": "This figure demonstrates the limitations of using Differentiable Neural Architecture Search (DNAS) for generating circuits.  Subfigure (a) shows that DNAS struggles to generate accurate circuits, especially as the circuit size increases. (b) illustrates that DNAS only utilizes shallow layers of the network, while the proposed method utilizes deeper layers.  (c) visualizes the DNAS network, highlighting the underutilization of deeper layers. Finally, (d) shows that the structure of circuits generated by Sum of Products (SOP) exhibits a triangular pattern, unlike the common rectangular shape used by existing DNAS methods.", "section": "4 Rethinking DNAS for Neural Circuit Generation"}, {"figure_path": "ZYNYhh3ocW/figures/figures_4_1.jpg", "caption": "Figure 2: Framework of the Regularized Triangle-Shaped Circuit Network (T-Net). Our proposed T-Net consists of three key modules: 1) Multi-label transformation of training data to decrease generation difficulty. 2) A Triangle-shaped network architecture, designed to align with the structural biases inherent in logic circuits. 3) Regularized training loss for efficient search and exact generation.", "description": "This figure illustrates the overall framework of the proposed T-Net model for logic synthesis.  It highlights three key modules: (1) A multi-label transformation of training data that partitions the input-output examples and merges them into multi-label data to reduce the complexity of the learning process. (2) A triangle-shaped network architecture that aligns with the structural bias in logic circuits, resulting in an efficient search space for circuit generation. (3) A regularized training loss function to avoid overfitting and ensure accurate circuit generation. The figure visually shows how the input truth table is transformed using multi-label transformation, feeding into the triangle-shaped architecture, and finally producing predicted output with regularized training loss.", "section": "5 A Regularized Triangle-Shaped Circuit Network Generation Framework"}, {"figure_path": "ZYNYhh3ocW/figures/figures_18_1.jpg", "caption": "Figure 1: (a) DNAS struggles to accurately generate circuits, especially larger ones. (b) The depth of an output node of the circuit in the circuit. DNAS only connects to very shallow layers, while our method learns deeper layers. (c) The visualization of the converged DNAS network. The dark nodes represent the used circuit nodes, indicating very low utilization of deep-layer nodes. (d) The circuits generated by SOP show that the average number of nodes per layer forms a triangular pattern.", "description": "This figure demonstrates the limitations of Differentiable Neural Architecture Search (DNAS) for neural circuit generation. Subfigure (a) shows the low accuracy of DNAS in generating circuits, especially larger ones. Subfigure (b) illustrates that DNAS tends to use only shallow layers of the network, while the proposed method utilizes deeper layers. Subfigure (c) visualizes a converged DNAS network showing that many nodes are unused due to skip connections. Subfigure (d) shows the triangular structure observed in circuits generated by the Sum of Products (SOP) method, contrasting with the structure learned by DNAS.", "section": "4 Rethinking DNAS for Neural Circuit Generation"}, {"figure_path": "ZYNYhh3ocW/figures/figures_18_2.jpg", "caption": "Figure 4: We present the exploration of two circuits. The height of the empty bars illustrates the total number of nodes in each layer, with the solid bars indicating the number of nodes that have been explored.", "description": "This figure visualizes the exploration degree of each network layer after the DNAS model converges.  The height of each empty bar represents the total number of nodes at each layer. The height of the filled bar in each layer shows the number of nodes that were used during the training process (explored nodes). The results demonstrate a clear bias towards exploring shallow layers, highlighting the problem of underutilization of deeper layers by DNAS methods.", "section": "4.2 A Deep Understanding of These Challenges"}, {"figure_path": "ZYNYhh3ocW/figures/figures_19_1.jpg", "caption": "Figure 5: (a) and (b) Loss of different output in the same circuit. The convergence speed of loss varies among different outputs. (c) and (d) Distribution of convergence iterations for input-output examples. Different examples have different converge speeds. The bar between 10 thousand and Inf means these instances do not converge in 10 thousand iterations.", "description": "This figure demonstrates the challenges in training DNAS for circuit generation.  Subfigures (a) and (b) show that the convergence speed of the loss function varies significantly among different output bits within the same circuit. Subfigures (c) and (d) illustrate that the convergence speed also varies substantially across different input samples for the same output bit. This highlights the imbalance in learning difficulty, making it challenging to train the model effectively.", "section": "4.2 A Deep Understanding of These Challenges"}, {"figure_path": "ZYNYhh3ocW/figures/figures_21_1.jpg", "caption": "Figure 6: Framework of the evolutionary algorithm assisted by RL agent restarting technique for circuit optimization.", "description": "This figure illustrates the optimization framework used in the paper. It uses an evolutionary algorithm combined with reinforcement learning to optimize circuit designs.  The RL agent learns to find optimal sequences of logic synthesis operators to minimize circuit size. The process involves generating an initial population of circuits, which are then iteratively optimized using the RL agent.  A key feature is the agent restart technique, which reinitializes the agent parameters periodically to help escape local optima and continue exploring the search space. The resulting optimized circuit is then used as input for the next generation in the evolutionary process. The diagram shows the flow of data between the components, including truth tables, circuits, RL agent, and environment.", "section": "5.4 Neural Circuit Optimization"}]