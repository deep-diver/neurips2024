[{"figure_path": "w67vRHZF13/figures/figures_1_1.jpg", "caption": "Figure 1: (a) In WebQA [10], the accuracy roughly forms a \"U\" shape curve when the relevant image-text pair for a question appears at different positions. While our model also shows similar trends, it tends to be more stable overall. (b) The accuracy of various types of questions in MMVP-VLM [81], it can be observed that our model's performance improves on such tasks after introducing the discriminative training. Details can be seen in Appendix E.3", "description": "This figure shows two plots. Plot (a) demonstrates the accuracy of a multimodal large language model (MLLM) on the WebQA dataset when the relevant image-text pair is placed at different positions within a sequence. It reveals that the accuracy tends to be lower when the relevant pair is not at the beginning or the end. Plot (b) presents the accuracy of various types of questions in the MMVP-VLM benchmark, showing how the model's performance enhances after incorporating discriminative training. These plots highlight the challenges faced by generative models in performing discriminative tasks and the benefits of the proposed unified generative-discriminative training.", "section": "Introduction"}, {"figure_path": "w67vRHZF13/figures/figures_2_1.jpg", "caption": "Figure 2: Our structure-induced generative and discriminative training joint training strategy.", "description": "This figure illustrates the Sugar framework, a novel approach that integrates generative and discriminative training for multimodal large language models (MLLMs).  It shows how the model processes interleaved image-text sequences, employing dynamic sequence alignment to capture global semantics and a novel kernel for fine-grained semantic differentiation.  The framework balances generative and discriminative tasks by using both a generative loss (Lg) and a discriminative loss (Ld).  The discriminative loss is calculated based on the similarity between the predicted similarity of two input sequences and their actual similarity.  The generative loss is based on the text generation performance. This unified approach aims to leverage the strengths of both generative and discriminative training paradigms, ultimately improving the performance of MLLMs on various tasks.", "section": "Method"}, {"figure_path": "w67vRHZF13/figures/figures_3_1.jpg", "caption": "Figure 3: (a) Dynamic Sequence Alignment. Semantically matched slices are connected with a blue dashed line. The arrows indicate the direction of the ordered temporal alignment path. With these alignments, we can obtain the similarity between two interleaved inputs for training. (b) Sugar Framework. Sugar supports both multi-modal generation and retrieval simultaneously.", "description": "This figure illustrates the core components of the proposed Sugar framework. (a) shows the dynamic sequence alignment, where semantically related segments from two interleaved image-text sequences are connected, allowing for comparison and similarity calculation. The arrows show the direction of the temporal alignment. (b) provides an overview of the Sugar framework itself, highlighting its capability for both multi-modal generation and retrieval. The framework takes interleaved sequences as input, processes them using a language model (LLM) augmented with LoRA, and uses two projectors to handle visual and textual tokens separately, resulting in generated tokens or retrieval outputs.", "section": "3 Method"}, {"figure_path": "w67vRHZF13/figures/figures_5_1.jpg", "caption": "Figure 2: Our structure-induced generative and discriminative training joint training strategy.", "description": "This figure illustrates the Sugar framework, which combines generative and discriminative training to improve the performance of multi-modal large language models.  It shows how interleaved image-text sequences are used as input, and how a structure-induced training strategy imposes semantic relationships between the input samples and the model's hidden state.  This enhances the model's ability to capture global semantics and distinguish fine-grained semantics using dynamic sequence alignment and a novel kernel. The framework balances generative and discriminative tasks, resulting in improved performance on both types of tasks.", "section": "3 Method"}, {"figure_path": "w67vRHZF13/figures/figures_20_1.jpg", "caption": "Figure 5: A Case for WebQA. The index for the useful pair is three.", "description": "This figure shows a sample question from the WebQA dataset, which involves identifying a relevant image-text pair among several distractors. The question asks whether the Atlanta Hawks wore red uniforms during the 2015 NBA season.  The figure highlights several image-text pairs, and the correct one is labeled as \"useful\".  The goal is to illustrate the challenge of capturing global semantics in long sequences, as the performance of LLMs on this task varies depending on the position of the relevant pair in the sequence.", "section": "Introduction Experiment Details"}, {"figure_path": "w67vRHZF13/figures/figures_21_1.jpg", "caption": "Figure 1: (a) In WebQA [10], the accuracy roughly forms a \"U\" shape curve when the relevant image-text pair for a question appears at different positions. While our model also shows similar trends, it tends to be more stable overall. (b) The accuracy of various types of questions in MMVP-VLM [81], it can be observed that our model's performance improves on such tasks after introducing the discriminative training. Details can be seen in Appendix E.3", "description": "This figure shows two plots demonstrating the shortcomings of current vision-language models (VLMs). Plot (a) shows the accuracy of a question-answering model, varying the position of the correct image-text pair within a sequence of candidates.  It reveals that current generative models struggle with centrally located information. Plot (b) demonstrates the performance of VLMs on a fine-grained retrieval task, showing that generative models struggle with distinguishing between visually similar images.", "section": "Introduction"}, {"figure_path": "w67vRHZF13/figures/figures_22_1.jpg", "caption": "Figure 2: Our structure-induced generative and discriminative training joint training strategy.", "description": "This figure illustrates the Sugar framework, a novel approach that unifies generative and discriminative training for multi-modal large language models (MLLMs). It shows how the model integrates both generative and discriminative losses (Lg and Ld), leveraging dynamic sequence alignment within the Dynamic Time Warping framework and a novel triple kernel for fine-grained semantic differentiation. The framework takes interleaved image-text sequences as input, explicitly imposing semantic relationships between samples and the MLLM's hidden states to enhance the model's ability to capture global semantics and distinguish fine-grained details. This unified approach aims to overcome limitations of both generative and discriminative paradigms alone, achieving synergistic improvements in both generative and discriminative tasks.", "section": "3 Method"}, {"figure_path": "w67vRHZF13/figures/figures_23_1.jpg", "caption": "Figure 4: Selected examples for various image-text tasks. The pink background indicates retrieval results, while the blue background indicates generated results. More examples are provided in the Appendix F.2.", "description": "This figure showcases several examples of the model's performance on various image-text tasks.  The examples are categorized into four groups: Sensitivity with Detailed Semantics, World Knowledge, Multimodal Concept Composition, and Fine-grained Image Discrimination. Each example shows the input prompt (image and/or text), the model's output (generated text or retrieved results), and a visual indicator (pink for retrieval, blue for generation). The figure demonstrates the model's ability to handle various complexities in image-text understanding, including detailed semantics, world knowledge, and compositional reasoning. More examples are available in Appendix F.2.", "section": "Experiments"}, {"figure_path": "w67vRHZF13/figures/figures_24_1.jpg", "caption": "Figure 9: Selected examples. Sugar excels at accurately discerning subtle differences between images and identifying detailed objects and their attributes.", "description": "This figure showcases Sugar's ability to distinguish subtle differences between similar images and pinpoint detailed objects and their attributes.  It presents several examples where Sugar's responses are more precise and accurate compared to the responses of another model (VILA), highlighting Sugar's enhanced capacity for fine-grained visual analysis and discrimination.", "section": "Fine-grained Image Discrimination"}, {"figure_path": "w67vRHZF13/figures/figures_24_2.jpg", "caption": "Figure 9: Selected examples. Sugar excels at accurately discerning subtle differences between images and identifying detailed objects and their attributes.", "description": "This figure shows several examples where Sugar is used to identify subtle differences between images.  The examples demonstrate Sugar's superior ability to pinpoint precise differences, unlike VILA which may provide more general descriptions of the image contents. The improved accuracy in detailed object identification and attribute recognition is highlighted.", "section": "Fine-grained Image Discrimination"}, {"figure_path": "w67vRHZF13/figures/figures_24_3.jpg", "caption": "Figure 2: Our structure-induced generative and discriminative training joint training strategy.", "description": "This figure illustrates the Sugar framework, which unifies generative and discriminative training for multi-modal large language models (MLLMs).  It shows how interleaved image-text sequences are processed by the MLLM, with a structure-induced training strategy imposing semantic relationships between input samples and the hidden state. This approach aims to enhance the MLLM's ability to capture global semantics, distinguish fine-grained semantics, and balance generative and discriminative tasks. The figure depicts the input interleaved sequences, dynamic sequence alignment using a novel kernel, generative and discriminative losses (Lg and Ld), and the final generated tokens. ", "section": "Method"}, {"figure_path": "w67vRHZF13/figures/figures_25_1.jpg", "caption": "Figure 2: Our structure-induced generative and discriminative training joint training strategy.", "description": "This figure illustrates the Sugar framework, which integrates generative and discriminative training paradigms. It shows how interleaved image-text sequences are processed by a Multimodal Large Language Model (MLLM) to achieve both generative and discriminative tasks.  The framework uses a dynamic sequence alignment method, a novel kernel for fine-grained semantic differentiation, and a structure-induced training strategy that imposes semantic relationships between input samples and the MLLM's hidden state.  The goal is to balance generative capabilities (like text generation) and discriminative abilities (like image-text retrieval), addressing weaknesses of each approach in isolation.  The resulting model is trained with both generative and discriminative losses.", "section": "3 Method"}, {"figure_path": "w67vRHZF13/figures/figures_26_1.jpg", "caption": "Figure 2: Our structure-induced generative and discriminative training joint training strategy.", "description": "This figure illustrates the proposed Sugar framework, which combines generative and discriminative training for multi-modal large language models.  It shows how the model integrates interleaved image-text sequences, dynamic sequence alignment, and a novel kernel for fine-grained semantic differentiation. The framework aims to balance generative and discriminative tasks, leveraging the strengths of both paradigms to improve performance in various tasks.", "section": "Method"}, {"figure_path": "w67vRHZF13/figures/figures_26_2.jpg", "caption": "Figure 3: (a) Dynamic Sequence Alignment. Semantically matched slices are connected with a blue dashed line. The arrows indicate the direction of the ordered temporal alignment path. With these alignments, we can obtain the similarity between two interleaved inputs for training. (b) Sugar Framework. Sugar supports both multi-modal generation and retrieval simultaneously.", "description": "This figure illustrates the proposed Sugar framework's core components: dynamic sequence alignment and the overall architecture.  (a) Dynamic Sequence Alignment shows how semantically similar parts of interleaved image-text sequences are aligned to capture global context and relationships.  (b) Sugar Framework provides a high-level overview of the model's architecture, showcasing how structure-induced training balances generative and discriminative tasks, enabling both generation and retrieval.", "section": "3.1 Problem Formulation and Architecture Overview"}, {"figure_path": "w67vRHZF13/figures/figures_26_3.jpg", "caption": "Figure 2: Our structure-induced generative and discriminative training joint training strategy.", "description": "This figure illustrates the proposed Sugar framework, which combines generative and discriminative training for multi-modal large language models.  The framework integrates both generative and discriminative losses, leveraging dynamic sequence alignment and a novel kernel for fine-grained semantic differentiation.  The diagram depicts how the model processes interleaved image-text sequences, captures global and fine-grained semantics, and generates or retrieves outputs.", "section": "3 Method"}, {"figure_path": "w67vRHZF13/figures/figures_26_4.jpg", "caption": "Figure 3: (a) Dynamic Sequence Alignment. Semantically matched slices are connected with a blue dashed line. The arrows indicate the direction of the ordered temporal alignment path. With these alignments, we can obtain the similarity between two interleaved inputs for training. (b) Sugar Framework. Sugar supports both multi-modal generation and retrieval simultaneously.", "description": "This figure illustrates the core methodology of the Sugar framework.  Panel (a) shows the dynamic sequence alignment used to capture semantic relationships between interleaved image-text sequences.  The alignment path highlights the similarities between corresponding parts of different sequences. Panel (b) provides a schematic overview of the Sugar framework, showing the integration of generative and discriminative training for vision-language modeling, using both generative and discriminative loss functions.", "section": "3.1 Problem Formulation and Architecture Overview"}, {"figure_path": "w67vRHZF13/figures/figures_26_5.jpg", "caption": "Figure 3: (a) Dynamic Sequence Alignment. Semantically matched slices are connected with a blue dashed line. The arrows indicate the direction of the ordered temporal alignment path. With these alignments, we can obtain the similarity between two interleaved inputs for training. (b) Sugar Framework. Sugar supports both multi-modal generation and retrieval simultaneously.", "description": "This figure shows two parts: (a) illustrates the dynamic sequence alignment method used by the Sugar framework to capture semantic relationships between input samples. Matched parts of the interleaved image-text sequences are connected by lines and arrows show the order of alignment. (b) presents an overview of the Sugar framework architecture, including the interleaved input sequence, visual and text token projections, the MLLM, and the generative and discriminative loss functions.", "section": "3.1 Problem Formulation and Architecture Overview"}, {"figure_path": "w67vRHZF13/figures/figures_26_6.jpg", "caption": "Figure 2: Our structure-induced generative and discriminative training joint training strategy.", "description": "This figure illustrates the Sugar framework, which integrates generative and discriminative training.  Interleaved image-text sequences are fed into a Multimodal Large Language Model (MLLM).  A dynamic sequence alignment module, using a novel kernel, helps to capture global and fine-grained semantic relationships between the input sequences. The model then produces both generative (text generation) and discriminative (similarity prediction) outputs, with losses calculated for both aspects to guide training.  The synergistic benefits are highlighted by the integrated approach, enhancing both generative and discriminative capabilities of the model.", "section": "Method"}, {"figure_path": "w67vRHZF13/figures/figures_26_7.jpg", "caption": "Figure 4: Selected examples for various image-text tasks. The pink background indicates retrieval results, while the blue background indicates generated results. More examples are provided in the Appendix F.2.", "description": "This figure showcases several examples of the model's performance on various image-text tasks.  Each example is categorized into one of four task types: Sensitivity with Detailed Semantics, World Knowledge, Multimodal Concept Composition, and Fine-grained Image Discrimination.  Each example shows the input image(s) and text, followed by the model's response, indicated by a pink background for retrieval results and a blue background for generation results.  The purpose is to visually demonstrate the model's ability to handle a wide range of multimodal tasks with high accuracy and provide both generative and discriminative capabilities within a single model.", "section": "Experiments"}, {"figure_path": "w67vRHZF13/figures/figures_26_8.jpg", "caption": "Figure 3: (a) Dynamic Sequence Alignment. Semantically matched slices are connected with a blue dashed line. The arrows indicate the direction of the ordered temporal alignment path. With these alignments, we can obtain the similarity between two interleaved inputs for training. (b) Sugar Framework. Sugar supports both multi-modal generation and retrieval simultaneously.", "description": "This figure shows two parts: (a) illustrates the Dynamic Sequence Alignment used to compute semantic relationships between different input samples. The alignment path between semantically similar parts of the two sequences is highlighted. (b) shows the overall framework of Sugar, which supports both multimodal generation and retrieval by incorporating a structure-induced constraint.", "section": "3.1 Problem Formulation and Architecture Overview"}, {"figure_path": "w67vRHZF13/figures/figures_26_9.jpg", "caption": "Figure 3: (a) Dynamic Sequence Alignment. Semantically matched slices are connected with a blue dashed line. The arrows indicate the direction of the ordered temporal alignment path. With these alignments, we can obtain the similarity between two interleaved inputs for training. (b) Sugar Framework. Sugar supports both multi-modal generation and retrieval simultaneously.", "description": "This figure illustrates the proposed Sugar framework. Panel (a) shows the dynamic sequence alignment method used to compute semantic relationships between interleaved image-text sequences. The alignment is represented by a path connecting semantically similar parts of the sequences.  Panel (b) is a diagram of the overall Sugar framework, which combines a vision transformer (VIT), a large language model (LLM), and a retrieval module to perform both generation and retrieval tasks. The figure highlights how the structure-induced training strategy integrates generative and discriminative tasks within the MLLM. ", "section": "3 Method"}, {"figure_path": "w67vRHZF13/figures/figures_26_10.jpg", "caption": "Figure 4: Selected examples for various image-text tasks. The pink background indicates retrieval results, while the blue background indicates generated results. More examples are provided in the Appendix F.2.", "description": "This figure showcases several examples of the model's performance on various image-text tasks.  The examples are grouped into categories such as: Sensitivity with Detailed Semantics, World Knowledge, Multimodal Concept Composition, and Retrieval and Dialog. Each example shows the input prompt, the model's output (generated text or retrieved results), and a visual representation of the input images. The pink background highlights retrieval tasks, showing the model's ability to retrieve relevant information based on the prompt. The blue background shows generated results, which demonstrate the model's capacity to generate accurate and coherent text.  The appendix contains further examples not included in the main figure due to space constraints.", "section": "Experiments"}, {"figure_path": "w67vRHZF13/figures/figures_26_11.jpg", "caption": "Figure 7: Selected examples from do retrieval-augmented generation (continued for Figure 6).", "description": "This figure shows more examples of retrieval-augmented generation.  The examples demonstrate Sugar's ability to generate more accurate and detailed answers by leveraging external knowledge retrieved from a knowledge base.  Each example includes a question, an image, the answer provided by Sugar, and the external knowledge that informed Sugar's answer.  The examples illustrate that Sugar can better integrate information from both images and text, retrieving more relevant information for more effective question answering.", "section": "F More Results"}, {"figure_path": "w67vRHZF13/figures/figures_26_12.jpg", "caption": "Figure 2: Our structure-induced generative and discriminative training joint training strategy.", "description": "This figure illustrates the Sugar framework, which combines generative and discriminative training for multi-modal large language models.  It shows how interleaved image-text sequences are fed into the model, and how both generative (Lg) and discriminative (Ld) losses are used to train the model. The Dynamic Sequence Alignment component is highlighted, showing how semantic relationships between input samples are used to induce structure on the model's hidden states. A novel kernel is also mentioned, designed for fine-grained semantic differentiation.  The overall aim is to bridge the gap between generative and discriminative training paradigms by leveraging the strengths of both.", "section": "Method"}, {"figure_path": "w67vRHZF13/figures/figures_26_13.jpg", "caption": "Figure 2: Our structure-induced generative and discriminative training joint training strategy.", "description": "This figure illustrates the Sugar framework, which integrates generative and discriminative training.  The input is an interleaved sequence of image and text tokens.  These are processed by a Multimodal Large Language Model (MLLM), which outputs both generated text tokens and hidden states.  The hidden states are used to compute the discriminative loss, Ld, which measures the similarity between pairs of input sequences.  A generative loss, Lg, based on the generated text, is also calculated. The final loss function is a weighted combination of Ld and Lg. This joint training strategy aims to leverage the strengths of both generative and discriminative paradigms, improving the model's ability to capture global semantics, differentiate fine-grained semantics, and reduce hallucinations.", "section": "Method"}, {"figure_path": "w67vRHZF13/figures/figures_27_1.jpg", "caption": "Figure 2: Our structure-induced generative and discriminative training joint training strategy.", "description": "This figure illustrates the Sugar framework, which integrates generative and discriminative training paradigms. It shows how interleaved image-text sequences are fed into a Multimodal Large Language Model (MLLM).  The model then uses a dynamic sequence alignment mechanism to capture global and fine-grained semantics and utilizes a novel triple kernel to enhance semantic differentiation. The framework outputs both generated text and predicted similarity scores, effectively balancing generative and discriminative tasks.", "section": "Method"}, {"figure_path": "w67vRHZF13/figures/figures_27_2.jpg", "caption": "Figure 2: Our structure-induced generative and discriminative training joint training strategy.", "description": "This figure illustrates the Sugar framework, which combines generative and discriminative training for multi-modal large language models. It shows how the model processes interleaved image-text sequences using dynamic sequence alignment and a novel kernel for fine-grained semantic differentiation. The framework aims to balance generative and discriminative tasks, leveraging the strengths of both paradigms to improve performance on a variety of tasks.", "section": "3 Method"}, {"figure_path": "w67vRHZF13/figures/figures_27_3.jpg", "caption": "Figure 3: (a) Dynamic Sequence Alignment. Semantically matched slices are connected with a blue dashed line. The arrows indicate the direction of the ordered temporal alignment path. With these alignments, we can obtain the similarity between two interleaved inputs for training. (b) Sugar Framework. Sugar supports both multi-modal generation and retrieval simultaneously.", "description": "This figure shows the proposed Sugar framework's dynamic sequence alignment and overall architecture. (a) illustrates how the model aligns semantically related parts of interleaved image-text sequences to capture global semantics.  (b) depicts the framework's ability to handle both generative and retrieval tasks simultaneously, using the aligned sequences as input for an MLLM.", "section": "3 Method"}, {"figure_path": "w67vRHZF13/figures/figures_28_1.jpg", "caption": "Figure 3: (a) Dynamic Sequence Alignment. Semantically matched slices are connected with a blue dashed line. The arrows indicate the direction of the ordered temporal alignment path. With these alignments, we can obtain the similarity between two interleaved inputs for training. (b) Sugar Framework. Sugar supports both multi-modal generation and retrieval simultaneously.", "description": "This figure illustrates the proposed Sugar framework which consists of two parts: dynamic sequence alignment and Sugar framework. Dynamic sequence alignment shows how the model aligns semantically similar parts of two interleaved image-text sequences to capture the global semantics. Sugar framework shows how the model jointly trains generative and discriminative tasks to enhance the model's ability to capture both global and detailed semantics.", "section": "3.1 Problem Formulation and Architecture Overview"}, {"figure_path": "w67vRHZF13/figures/figures_28_2.jpg", "caption": "Figure 4: Selected examples for various image-text tasks. The pink background indicates retrieval results, while the blue background indicates generated results. More examples are provided in the Appendix F.2.", "description": "This figure showcases several examples of the model's performance on various image-text tasks, highlighting its capabilities in both retrieval and generation.  The pink background indicates examples where the model successfully retrieved relevant information, while the blue background indicates examples where the model successfully generated text.  The examples cover a range of complexities and demonstrate the model's ability to handle both simple and complex scenarios.", "section": "Experiments"}]