[{"figure_path": "w67vRHZF13/tables/tables_6_1.jpg", "caption": "Table 1: Comparison with state-of-the-art methods on 11 visual-language benchmarks. We mark the best performance bold and the second-best underlined. Benchmark names are abbreviated due to space limits. VQA-v2 [27]; GQA [35]; VizWiz [28]; SQA\u00b9: ScienceQA-IMG [61]; VQAT: TextVQA [76]; POPE [51]; MMEP, MMEC: MME Perception, MME Cognition [95]; MMB: MMBench [58]; LLaVAWd: LLaVA-Bench(In-the-Wild)-Detail [56]; MM-Vet [99]. * indicates the training images of the datasets are observed during training.", "description": "This table compares the performance of the proposed Sugar model against eleven state-of-the-art models across eleven different visual-language benchmarks.  The benchmarks assess a range of capabilities, including visual question answering, visual reasoning, and multimodal understanding.  The table highlights the best and second-best performing models for each benchmark, showing Sugar's performance relative to existing methods and indicating where it achieves state-of-the-art results.", "section": "4 Experiments"}, {"figure_path": "w67vRHZF13/tables/tables_7_1.jpg", "caption": "Table 2: Comparision with state-of-the-art method on DEMON [47] benchmark.", "description": "This table compares the performance of the proposed Sugar model against several state-of-the-art models on the DEMON benchmark. DEMON evaluates models' ability to handle complicated multimodal comprehension tasks across various datasets and categories.  The table shows the results for each model across seven categories with 29 sub-tasks, highlighting Sugar's superior performance compared to VPG-C, the previous state-of-the-art model, across six out of seven categories.", "section": "4.3 Complicated Multimodal Comprehension on DEMON"}, {"figure_path": "w67vRHZF13/tables/tables_7_2.jpg", "caption": "Table 1: Comparison with state-of-the-art methods on 11 visual-language benchmarks. We mark the best performance bold and the second-best underlined. Benchmark names are abbreviated due to space limits. VQA-v2 [27]; GQA [35]; VizWiz [28]; SQA\u00b9: ScienceQA-IMG [61]; VQAT: TextVQA [76]; POPE [51]; MMEP, MMEC: MME Perception, MME Cognition [95]; MMB: MMBench [58]; LLaVAWd: LLaVA-Bench(In-the-Wild)-Detail [56]; MM-Vet [99]. * indicates the training images of the datasets are observed during training.", "description": "This table compares the performance of the proposed method, Sugar, against 10 other state-of-the-art models across 11 visual language benchmarks.  The benchmarks assess various capabilities, including visual question answering, visual reasoning, and multimodal comprehension.  The table highlights Sugar's performance, indicating where it achieves the best or second-best results, using bold and underline formatting respectively. The asterisk (*) denotes benchmarks where training images are used during the model's training phase.  Abbreviations are used for brevity, and the full names of the benchmarks are given in the caption.", "section": "4.2 Multimodal Comprehension on 11 Benchmarks"}, {"figure_path": "w67vRHZF13/tables/tables_8_1.jpg", "caption": "Table 4: (a) Retrieval-Augmented Generation. (b) Ablation Study. For MSCOCO, we report the R@5 in text-to-image retrieval. For VIST, we report the R@5 of retrieving an image given 5 captions and 4 images. For Winoground, we report the Image score. For other tasks, we report Accuracy (%).", "description": "This table presents the results of two experiments: retrieval-augmented generation and ablation study.  The retrieval-augmented generation experiment shows the improvement of performance in VizWiz and SQA tasks when incorporating retrieval. The ablation study investigates the impact of removing individual components (data for generative tasks, data for discriminative tasks, global alignment kernel, triple kernel, average pooling) on the performance of both generative and discriminative tasks.  The results show that all components contribute positively to the model's performance, highlighting the synergistic benefits of the unified approach.", "section": "4.5 Retrieval-Augmented Generation"}, {"figure_path": "w67vRHZF13/tables/tables_8_2.jpg", "caption": "Table 1: Comparison with state-of-the-art methods on 11 visual-language benchmarks. We mark the best performance bold and the second-best underlined. Benchmark names are abbreviated due to space limits. VQA-v2 [27]; GQA [35]; VizWiz [28]; SQA\u00b9: ScienceQA-IMG [61]; VQAT: TextVQA [76]; POPE [51]; MMEP, MMEC: MME Perception, MME Cognition [95]; MMB: MMBench [58]; LLaVAWd: LLaVA-Bench(In-the-Wild)-Detail [56]; MM-Vet [99]. * indicates the training images of the datasets are observed during training.", "description": "This table compares the performance of the proposed method 'Sugar' against several state-of-the-art methods across eleven established visual-language benchmarks.  Each benchmark assesses different aspects of visual-language understanding, such as visual question answering, image captioning, and multimodal reasoning.  The best and second-best performing models are highlighted for each benchmark, showing Sugar's competitive performance.", "section": "4.2 Multimodal Comprehension on 11 Benchmarks"}, {"figure_path": "w67vRHZF13/tables/tables_20_1.jpg", "caption": "Table 5: Specific accuracy (%) values displayed on WebQA. The index indicates the position of the useful image-text pair, denoting which position it occupies in the sequence.", "description": "This table presents the accuracy of VILA and Sugar models on the WebQA dataset, where the position of the relevant image-text pair is varied.  The index indicates the pair's location in the sequence (1 being the first, 6 being the last).  The results show Sugar consistently outperforming VILA, suggesting it's less prone to positional bias in capturing global semantics.", "section": "4.2 Multimodal Comprehension on 11 Benchmarks"}, {"figure_path": "w67vRHZF13/tables/tables_21_1.jpg", "caption": "Table 1: Comparison with state-of-the-art methods on 11 visual-language benchmarks. We mark the best performance bold and the second-best underlined. Benchmark names are abbreviated due to space limits. VQA-v2 [27]; GQA [35]; VizWiz [28]; SQA\u00b9: ScienceQA-IMG [61]; VQAT: TextVQA [76]; POPE [51]; MMEP, MMEC: MME Perception, MME Cognition [95]; MMB: MMBench [58]; LLaVAWd: LLaVA-Bench(In-the-Wild)-Detail [56]; MM-Vet [99]. * indicates the training images of the datasets are observed during training.", "description": "This table compares the performance of the proposed method, Sugar, against eleven state-of-the-art methods across 11 benchmark datasets for visual language tasks. The table shows the results for each method on various metrics like accuracy, recall@k and other relevant metrics for each benchmark. The best and second-best results are highlighted for each benchmark dataset to enable easy comparison.", "section": "4.2 Multimodal Comprehension on 11 Benchmarks"}, {"figure_path": "w67vRHZF13/tables/tables_29_1.jpg", "caption": "Table 1: Comparison with state-of-the-art methods on 11 visual-language benchmarks. We mark the best performance bold and the second-best underlined. Benchmark names are abbreviated due to space limits. VQA-v2 [27]; GQA [35]; VizWiz [28]; SQA\u00b9: ScienceQA-IMG [61]; VQAT: TextVQA [76]; POPE [51]; MMEP, MMEC: MME Perception, MME Cognition [95]; MMB: MMBench [58]; LLaVAWd: LLaVA-Bench(In-the-Wild)-Detail [56]; MM-Vet [99]. * indicates the training images of the datasets are observed during training.", "description": "This table compares the performance of the proposed model, Sugar, against 10 other state-of-the-art models across 11 visual language benchmarks.  The benchmarks assess different aspects of vision-language understanding, including question answering, image captioning, and multimodal reasoning. The table shows the performance of each model on each benchmark, highlighting Sugar's competitive performance, especially in tasks requiring fine-grained semantic distinctions.  The asterisk (*) indicates models that were trained with access to the images in the datasets during training.", "section": "4.2 Multimodal Comprehension on 11 Benchmarks"}]