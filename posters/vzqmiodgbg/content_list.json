[{"type": "text", "text": "SAFEWORLD: Geo-Diverse Safety Alignment ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Da Yin\u2217 Haoyi Qiu\u2217 UCLA UCLA da.yin@cs.ucla.edu haoyiqiu@cs.ucla.edu ", "page_idx": 0}, {"type": "text", "text": "Kung-Hsiang Huang Kai-Wei Chang Nanyun Peng Salesforce AI Research UCLA UCLA kh.huang@salesforce.com kwchang@cs.ucla.edu violetpeng@cs.ucla.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Content Warning: This paper may contain examples of harmful contents by nature. ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In the rapidly evolving field of Large Language Models (LLMs), ensuring safety is a crucial and widely discussed topic. However, existing works often overlook the geo-diversity of cultural and legal standards across the world. To demonstrate the challenges posed by geo-diverse safety standards, we introduce SAFEWORLD, a novel benchmark specifically designed to evaluate LLMs\u2019 ability to generate responses that are not only helpful but also culturally sensitive and legally compliant across diverse global contexts. SAFEWORLD encompasses 2,775 test user queries, each grounded in high-quality, human-verified cultural norms and legal policies from 50 countries and 493 regions/races. On top of it, we propose a multidimensional automatic safety evaluation framework that assesses the contextual appropriateness, accuracy, and comprehensiveness of responses. Our evaluations reveal that current LLMs struggle to meet these criteria. To enhance LLMs\u2019 alignment with geo-diverse safety standards, we synthesize helpful preference pairs for Direct Preference Optimization (DPO) alignment training. The preference pair construction aims to encourage LLMs to behave appropriately and provide precise references to relevant cultural norms and policies when necessary. Our trained SAFEWORLDLM outperforms all competing models, including GPT-4o on all the three evaluation dimensions by a large margin. Global human evaluators also note a nearly $20\\%$ higher winning rate in helpfulness and harmfulness evaluation. ", "page_idx": 0}, {"type": "image", "img_path": "VZQmIoDGBG/tmp/d49424ab38b4e1d08aa1107080d27efa42176a0020ad89777db86c2ab0c47fb7.jpg", "img_caption": ["Figure 1: Examples of geo-diverse safety standards and the overall introduction of SAFEWORLD benchmark and its multi-dimensional evaluation. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Large Language Models (LLMs), such as LLaMA [36] and GPT [24], are becoming integral to various AI applications, serving tens of millions of users globally. As their use increases, concerns around LLMs safety are rapidly growing. Recently, a wide range of studies focus on evaluating and reducing their toxic and harmful impact on users [14, 38, 12, 21, 43, 31, 20, 2, 5, 3]. Despite significant progress in this area, an essential factor often remains overlooked: geo-diversity. Recognizing and incorporating geographical variations [41, 40, 4, 10, 31, 6] in safety principles is crucial in the global landscape of LLM safety. Cultural norms and legal frameworks vary widely, resulting in diverse definitions of safe and acceptable behavior. As shown in Figure 1, while giving a green hat as a gift might be benign in many cultures, it is considered offensive in China. Likewise, legal ages for drinking and marriage differ significantly between regions. If a model fails to account for these cultural norms and local policies (i.e., cultural-legal guidelines), it can inadvertently cause unnecessary confilcts among individuals or even between nations and pose significant legal risks for local services. Therefore, to be both equitable and effective, universally applicable LLMs must be calibrated to align with diverse cultural norms and legal standards worldwide. ", "page_idx": 1}, {"type": "text", "text": "We introduce SAFEWORLD, the first geo-diverse safety alignment evaluation benchmark, focusing on cultural and legal safety (\u00a73). SAFEWORLD evaluates an LLM\u2019s ability to generate helpful, safe, and appropriate responses in a global context. Constructed based on insights from our global user survey (Appendix A.2), SAFEWORLD comprises 2,775 high-quality diverse queries to simulate realistic, geo-diverse safety scenarios, validated through machine and human validations, which ensures alignment with cultural-legal guidelines from 50 countries and 439 regions/races. ", "page_idx": 1}, {"type": "text", "text": "To assess the quality of LLM responses to geo-diverse safety queries, we establish the three automatic evaluation protocols focusing on contextual appropriateness, accuracy, and comprehensiveness (\u00a74). Our evaluation reveals that LLaMA- and Mistral-series models can achieve comparable performance to GPT-3.5 and GPT-4-turbo on several dimensions. Although the cultural-legal guidelines in the SAFEWORLD benchmark queries are all derived from GPT-4-turbo\u2019s parametric knowledge, GPT4-turbo struggles with queries implicitly related to these guidelines and is even worse at providing appropriate response types than some open-source LLMs. This suggests that additional alignment methods may be necessary to effectively elicit and apply its learned knowledge in model responses. ", "page_idx": 1}, {"type": "text", "text": "This phenomenon motivates us to explore effective approaches for geo-diverse safety alignment. Focusing on the widely used alignment method Direct Preference Optimization (DPO) [26] (\u00a75), we investigate how to synthesize training data for preference pairs that helps LLMs behave appropriately and accurately elicit factual knowledge. Specifically, we first synthesize training queries based on our repository of human-verified cultural-legal guidelines, SAFEWORLD. Positive responses are then synthesized to align with the user queries and their corresponding cultural-legal guidelines. The negative responses in preference pairs are divided into two categories: Negative Response Category 1, which includes responses that correctly reference cultural-legal guidelines but do so inappropriately; Negative Response Category 2, which includes responses that are behaviorally appropriate but contain incorrect references to cultural-legal guidelines. Following the DPO alignment practices suggested by Huggingface Alignment Handbook [37], trained on top of Zephyr-7B-SFT-Full [37], our SAFEWORLDLM model outperforms all competitors, including GPT-4o, across all three evaluated dimensions, along with a nearly $20\\%$ higher winning rate in helpfulness and harmfulness assessments by human evaluators from 9 countries. In addition, our SAFEWORLDALIGN training data proves to be useful for maintaining performance on general NLP and safety evaluation tasks while enhancing geo-diverse safety alignment. ", "page_idx": 1}, {"type": "text", "text": "To summarize, we make the following contributions: (1) We introduce SAFEWORLD, the first geodiverse safety alignment evaluation benchmark for future real-world global AI applications. (2) We propose a multi-dimensional safety evaluation framework to assess the contextual appropriateness, accuracy, and comprehensiveness of responses, crucial for geo-diverse safety alignment. (3) We develop a geo-diverse safety alignment training method that enhances LLMs to outperform the advanced GPT-4o model in generating precise geo-diverse safety knowledge. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Cultural Knowledge Bases and Evaluation Benchmarks. Early efforts to build cultural knowledge bases have primarily followed a top-down approach, extracting norm-related data from web resources like Wikipedia and Reddit, and categorizing them by countries or regions [10, 11, 23, 7]. However, these methods often yield noisy data due to challenges in filtering irrelevant information. CULTUREBANK [32] improved data quality with a cleansing pipeline but did not address the safety aspect of cultural awareness. Our study employs a bottom-up approach, starting with specific countries and regions, followed by norm elicitation, careful data processing, and human validation. This approach ensures high-quality data collection cost-effectively. Additionally, our benchmark incorporates public policies, broadening the applicability of SAFEWORLD to diverse use cases. ", "page_idx": 1}, {"type": "image", "img_path": "VZQmIoDGBG/tmp/f017f73541177cfa5ca0a0962e108aea1127d57a4ce424b42b6775d9e7132fc3.jpg", "img_caption": ["(a) Safety evaluation benchmarks. (b) Cultural understanding evaluation benchmarks. Figure 2: The comparison between SAFEWORLD and other existing benchmarks. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "LLM Safety Evaluation. Safety and ethical concerns about LLMs, including issues like toxicity, bias, and potential disclosure of personal information, have been explored [14, 38, 30, 12, 35, 44]. As LLMs usage grows, new safety evaluation benchmarks are being developed to help researchers better understand and address these issues. Benchmarks such as TOXICCHAT [21] and SAFETYBENCH [43] adopt a binary classification approach, requiring LLMs to determine whether a conversation is toxic, or use a multiple-choice format to prompt LLMs to choose the correct action from a set of answers. Others, like SAFER-INSTRUCT [31], BEAVERTAILS [20] and ANTHROPIC-HH [2] evaluate open-ended generation results. However, these evaluations often overlook important factors: (1) the actual geo-diversity of safety standards, and (2) a fine-grained, multi-dimensional assessment of aspects such as desired response behavior and the accuracy and factuality of references to pertinent policies in the responses. SAFEWORLD represents the first geo-diverse safety alignment benchmark that provides a comprehensive evaluation of LLM responses across key dimensions, focusing on geo-diverse safety topics covering cultural norms and policies. ", "page_idx": 2}, {"type": "text", "text": "Cultural-Awareness and Alignment in Language Models. Previous research has primarily focused on evaluating a language model\u2019s preference when responding to global value surveys [13, 9, 28, 1]. Studies like [9, 28, 33] formalize and quantitatively measure LLMs\u2019 reflection of subjective opinions across nations. Another group of recent works simply query LLMs with the multiple-choice questions about multicultural knowledge [6, 27]. In contrast to these evaluation works, our work investigates novel and more deterministic geo-diverse safety topics that typically enjoy broader consensus among local populations or are documented in official records. Additionally, our work goes beyond examining simple multiple-choice multicultural QA, focusing instead on assessing the safety and helpfulness of LLM responses in real-world open-ended generation settings, and on exploring methods to further enhance response quality through alignment methods. ", "page_idx": 2}, {"type": "text", "text": "3 SAFEWORLD ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we detail the methodology for developing SAFEWORLD evaluation benchmark, designed to evaluate the geo-diverse safety alignment of LLMs. We first define geo-diverse safety (\u00a73.1), followed by the task definition (\u00a73.2), and the dataset construction process (\u00a73.3). ", "page_idx": 2}, {"type": "text", "text": "3.1 Geo-Diverse Safety Definition ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Inspired by the formal taxonomy proposed by [38], we identify three critical safety categories for geo-diverse contexts: (1) discrimination, exclusion, and toxicity; (2) malicious uses; and (3) misinformation harms. Building upon these categories, our SAFEWORLD benchmark emphasizes cultural safety and legal safety and we elaborate on the definition of these two dimensions: ", "page_idx": 2}, {"type": "text", "text": "Cultural safety defines an environment that is spiritually, socially, emotionally, and physically safe for people [39]. It is about adhering to cultural and social norms, which dictate appropriate scenario within a society. For example, in many East Asian countries, it is customary to remove one\u2019s shoes before entering a home, demonstrating respect for the household and ensuring cleanliness. Straying from established norms can compromise both personal and communal harmony, highlighting the importance of respecting cultural boundaries to ensure peaceful interactions within a society. ", "page_idx": 2}, {"type": "text", "text": "Legal safety refers to abiding the policies enacted by governments, with each country having its own set of regulations designed to maintain social order and stability. These rules establish standards for acceptable scenario, resolve confilcts, and protect the rights and well-being of individuals and communities. Violating these policies can jeopardize public harmony [29] in the local area, emphasizing the need to respect geo-diverse legal frameworks to preserve order. ", "page_idx": 2}, {"type": "image", "img_path": "VZQmIoDGBG/tmp/96b407924607938ff7ea155d3f6e668e7d074af55dddfc1ad08b6ec2e9f491af.jpg", "img_caption": ["Figure 3: Overview of queries generation pipeline. Based on GEOSAFEDB, we generated four types of queries. We apply both machine and human validation to ensure high-quality generation. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3.2 The Geo-Diverse Safety Alignment Task ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The SAFEWORLD benchmark aims to evaluate models\u2019 ability to respond appropriately to queries involving a culturally or legally sensitive content. The input to this task is a query $x$ that may adhere to or violate specific cultural-legal guidelines $k^{y}=\\{k_{1}^{y},...,k_{J}^{y}\\}$ , with an expected response type $r^{y}$ . These guidelines and response types vary by query type, detailed in $\\S3.3.2$ . ", "page_idx": 3}, {"type": "text", "text": "3.3 SAFEWORLD Construction ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The benchmark creation involves two key stages: (1) constructing GEOSAFEDB, a cultural and legal geo-diverse safety database (\u00a73.3.1), and (2) formulating SAFEWORLD benchmark queries, each of which corresponds to cultural-legal guidelines in GEOSAFEDB (\u00a73.3.2). ", "page_idx": 3}, {"type": "text", "text": "3.3.1 GEOSAFEDB Development ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The first step towards SAFEWORLD involves creating a cultural and legal geo-diverse safety database, referred to as GEOSAFEDB, composed of the cultural-legal guidelines of various geographic backgrounds. It is beneficial for generating the queries indeed grounded to geo-diverse safety-related topics. We introduce a bottom-up approach, gathering country- and region/race-level guidelines via LLM prompting, followed by validation by native or local annotators. ", "page_idx": 3}, {"type": "text", "text": "We begin by selecting the top 50 most populous countries and use GPT-4-turbo to generate 100 unique, country-specific cultural-legal guidelines for each, ensuring geo-diversity in GEOSAFEDB. These guidelines undergo a rigorous multi-step verification process, combining both automated and human-based methods. Initially, verification is carried out using retrieval-augmented LLMs like Command-R and GPT-4-turbo, which validate the information against web-sourced data and pre-trained knowledge. Following this, geo-diverse human annotators from Amazon Mechanical Turk conduct a final round of validation, addressing common data quality issues encountered in prior research. Additionally, within a country, significant differences may exist between individual races and regions. For example, while India\u2019s national law prohibits cow slaughter due to the sacred status of cows in Hinduism, some states like West Bengal allow it. To capture these nuances, we extend our methodology by generating region/race-specific cultural-legal guidelines. This is achieved by prompting GPT-4-turbo based on the country-level guidelines, followed by another round of stringent machine and human validation to ensure the accuracy and representativeness of these region- and race-specific norms. Ultimately, this process results in a comprehensive collection of 7,447 cultural norms $(D_{C})$ and 6,652 public policies $(D_{L})$ spanning 50 countries and 493 regions and ethnic groups. Appendix A.1 provides more details about database construction. ", "page_idx": 3}, {"type": "text", "text": "3.3.2 SAFEWORLD Queries Generation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "After building GEOSAFEDB, we proceed to construct queries that reflect real-life geo-diverse safety scenarios. To ensure that the queries align with relevant use cases, we conduct surveys with participants from diverse geographical backgrounds (Appendix A.2). Using the survey insights, we design four distinct query types, each tailored to a specific response type. Every query in SAFEWORLD includes a scenario that illustrates a culturally or legally sensitive (or insensitive) context, accompanied by a related question. Figure 3 overviews our query generation process. Below, we detail the steps for designing these queries. Figure 4 show SAFEWORLD query examples. ", "page_idx": 3}, {"type": "text", "text": "SPECIFICANSWER. These queries involve scenarios that have already violated the cultural-legal guidelines of the queried country, race, or region. While the questions themselves might not be culturally or legally unsafe, LLMs should identify the specific guideline that has been violated when providing a response. To generate such SPECIFICANSWER queries, we create norm- or policyviolating scenarios and corresponding questions for each cultural-legal guideline $g_{v}\\in D_{C}\\cup D_{L}$ , using carefully crafted prompts for GPT-4-turbo. ", "page_idx": 3}, {"type": "image", "img_path": "VZQmIoDGBG/tmp/5240671d2382dbc4e97edc0be547e8dfb6bd2d0b16798900077a9890768fff17.jpg", "img_caption": ["Figure 4: SAFEWORLD query examples across four types. Some are paired with their corresponding reference (i.e., ground-truth) cultural-legal guidelines. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "COMPREANSWER. For scenarios where no specific countries, races, or regions are mentioned, but potentially violate norms or laws of some communities, models should provide comprehensive responses covering representative regions where such scenarios might be unsafe. To generate these queries, we cluster $N$ instances of violated cultural-legal guidelines from SPECIFICANSWER queries into $M_{1}$ clusters using K-Means [22], based on the norm and policy embeddings from INSTRUCTOR [34]. This identifies cultural-legal guidelines with shared topics. Based on them, the COMPREANSWER query generation can be more coherent to the shared topics and indeed involve those guidelines. Specifically, for each cluster, we prompt GPT-4-turbo to create $K_{1}$ scenarios and questions integrating the guidelines into contexts where they are violated, producing $K_{1}\\times M_{1}$ queries. ", "page_idx": 4}, {"type": "text", "text": "REFUSETOANSWER. Models should consistently avoid directly addressing certain inappropriate queries, such as those that compare cultural or legal systems or impose one group\u2019s guidelines onto another. To generate scenarios involving two countries, races, or regions, we cluster $N$ instances of violated norms or policies from SPECIFICANSWER queries into $M_{2}$ clusters using INSTRUCTOR, similar to the construction of COMPREANSWER queries. For each cluster, GPT-4-turbo generates $K_{2}$ scenarios and corresponding questions by embedding these norms or policies in various contexts related to specific races or regions, producing a total of $K_{2}\\times M_{2}$ queries. ", "page_idx": 4}, {"type": "text", "text": "DOANSWER. DOANSWER queries consist of scenarios and questions that adhere to cultural-legal guidelines. They evaluate a model\u2019s ability to provide helpful responses without mistakenly raising red flags, similar to assessing a model\u2019s precision. To construct these queries, we synthesize a scenario adhering to a specific cultural-legal guideline $g_{a}\\in D_{C}\\cup D_{L}$ using GPT-4-turbo. Since the scenarios are designed to be safe, we generate relevant questions without any restrictions. ", "page_idx": 4}, {"type": "text", "text": "By construction, this data collection process naturally annotates each instance in the SAFEWORLD benchmark with the query type, the expected response type $r_{y}$ , and the associated cultural-legal guideline $k^{y}$ . For SPECIFICANSWER and COMPREANSWER queries, $k^{y}$ specifies the violated guideline, denoted as $k^{y}=\\{g_{v}\\}$ . In contrast, for DOANSWER queries, $k^{y}$ identifies the guideline that is followed, represented as $k^{y}\\,=\\,\\{g_{a}\\}$ . For REFUSETOANSWER queries, $k^{y}$ is an empty set $(k^{y}\\,=\\,\\emptyset)$ ), indicating that no guidelines are either violated or adhered to, and their responses do not reference any specific guidelines. After generating these queries, we employ a multi-round validation process involving both machines and humans. The final evaluation set consists of 2,775 human-verified queries, while the remaining queries sreve as raw training data, detailed in $\\S5.2$ . For further information on query generation, refer to Appendix A.3. ", "page_idx": 4}, {"type": "image", "img_path": "VZQmIoDGBG/tmp/ba993128ddeb81d86319663108f30db123326de062fac4a7dde4993768195d2c.jpg", "img_caption": ["Figure 5: Overview of our multi-dimensional evaluation framework. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "4 Automatic Evaluation Framework ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our evaluation aims to assess how contextually appropriate, accurate, and comprehensive LLM responses are when addressing the four types of geo-diverse safety queries outlined in $\\S3$ . To achieve this, we implement the following evaluation protocols: (1) Response Type Matching (\u00a74.1); (2) Reference-Based Faithfulness and Coverage (\u00a74.2); (3) Reference-Free Factuality (\u00a74.3). ", "page_idx": 5}, {"type": "text", "text": "4.1 Response Type Matching ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As described in $\\S3$ , each query is associated with an expected response type, denoted as $\\mathcal{R}\\,=$ {SPECIFICANSWER, COMPREANSWER, REFUSETOANSWER, DOANSWER}. This evaluation protocol aims to determine whether the type of a model\u2019s generated response matches the expected response type. For example, in the case of DOANSWER queries, a model\u2019s response is considered a match if it addresses the query directly without raising any violation alerts. On the other hand, a response is deemed unmatched for REFUSETOANSWER queries if the model provides an answer when it is expected to refuse. Formally, for each model response $\\hat{y}$ , GPT-4-turbo classifies its response type $r_{\\hat{y}}\\in\\mathcal{R}$ . We then evaluate the alignment between the generated response type $r_{\\hat{y}}$ and the expected response type $r_{y}$ using the following metric: ALIGNMENT $(r_{\\hat{y}},r_{y})=\\mathbb{1}\\{r_{\\hat{y}}=r_{y}\\}\\in\\{0,1\\}$ . ", "page_idx": 5}, {"type": "text", "text": "4.2 Reference-based Faithfulness and Coverage Evaluation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the second evaluation dimension, we aim to determine whether models accurately identify and reference the norms or laws that are violated. To achieve this, we propose reference-based metrics to eexvtarlaucattien gt htehe norms or p oalnidcies from a  omf omdeold\u2019se lr ersepsponosnes,e sd e[n2o5t, e1d7 a, s1 $\\boldsymbol{k^{\\hat{y}}}=\\{k_{1}^{\\hat{y}},...,k_{L}^{\\hat{y}}\\}$ ,r euqsiunirg e GfPirsTt4-turbo. Faithfulness measures how accurately the model\u2019s response aligns with the ground-truth norms or policies. It is calculated as: FAITHFULNESS $(k^{\\hat{y}},k^{y})\\overset{\\mathtt{~.~}}{=}|k^{\\hat{y}}\\cap k^{\\breve{y}}|/|k^{\\hat{y}}|\\in[0,\\breve{1}]$ . A higher faithfulness score indicates that the model\u2019s response is more precise in referencing the expected norms or policies. Coverage, on the other hand, evaluates the comprehensiveness of the model\u2019s response, indicating how well it captures the entirety of the ground-truth norms embedded in the query. It is defined as: COVE $\\bar{\\mathrm{RAGE}}(k^{\\hat{y}},k^{y})\\,=\\,|k^{\\hat{y}}\\,\\bar{\\cap}\\,k^{y}|/|k^{\\bar{y}}|\\,\\in\\,[0,1]$ . A higher coverage score suggests that the model has referenced a more complete set of relevant norms or policies. ", "page_idx": 5}, {"type": "text", "text": "4.3 Reference-free Factuality Evaluation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To address situations where the norms or policies mentioned in the generated response are accurate but not covered by our annotated ground-truth norms or policies $k^{y}$ , we leverage the state-of-the-art retrieval-augmented LLM, Command-R. This model helps evaluate whether the norms or policies extracted from the model\u2019s response, $k^{\\hat{y}}$ , can be verified using online sources. This process is crucial for assessing the factuality (i.e., factual accuracy) of the generated content, as discussed in prior works [18, 19]. Let $k_{\\mathrm{fact}}^{\\hat{y}}\\subset k^{\\hat{y}}$ represent the subset of norms or policies that can be validated using information found on the web. We define factuality as: FACTUALITY $\\mathit{\\check{\\Psi}}(k^{\\hat{y}},k_{\\mathrm{fact}}^{\\hat{y}})=|k_{\\mathrm{fact}}^{\\hat{y}}|/|k^{\\hat{y}}|\\in[0,1]$ This metric measures the proportion of extracted norms or policies that are verifiable, providing a clearer indication of the response\u2019s factual accuracy. ", "page_idx": 5}, {"type": "text", "text": "4.4 LLM Evaluation Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We conduct a comprehensive evaluation of six open-source (Zephyr, LLaMA-2, LLaMA-3, Mistral) and five proprietary (OpenAI and Cohere families). Detailed information about the model versions can be found in Appendix Table 6. Each model is rigorously tested against our newly proposed SAFEWORLD benchmark. We assess model responses across the three dimensions outlined above. The results of these evaluations are presented in Table 1. ", "page_idx": 5}, {"type": "table", "img_path": "VZQmIoDGBG/tmp/bc7cf33b0ffb2fffb6f2bd27ca1b8ca83080d11b2ec9db5bb73bc6e81bfd8066.jpg", "table_caption": ["Table 1: Performance of different LLMs on our SAFEWORLD benchmark. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "LLaMA & Mistral vs. Proprietary LLMs. The LLaMA and Mistral-series models demonstrate impressive performance against proprietary LLMs. Notably, some models outperform GPT-3.5-turbo and Command- $\\mathbf{\\cdotR/R+}$ in coverage, faithfulness, and factuality. Moreover, they even exceed GPT-4- turbo in response type classification. This success underscores the potential of open-source models to leverage relevant knowledge effectively, especially in addressing geo-diverse safety scenarios, achieving performance levels comparable to leading proprietary models like the GPT series. ", "page_idx": 6}, {"type": "text", "text": "Scrutiny on GPT and Command-R Performance. The query generation method described in $\\S3.3$ uses cultural-legal guidelines generated by GPT-4-turbo to create the basis for test queries. This implies that GPT-4-turbo has internalized much of the cultural norms and policy knowledge present within the test set. However, in real-world scenarios that implicitly involve these cultural-legal guidelines, GPT-4-turbo often struggles to recognize and respond to them appropriately. Additionally, the Command-R models, which utilize web-scale retrieval-augmented generation, do not perform optimally on the SAFEWORLD testing scenarios. This highlights a critical limitation: despite the advantages of web-scale retrieval, LLMs can still struggle to accurately discern and apply the relevant norms and policies in nuanced contexts. ", "page_idx": 6}, {"type": "text", "text": "How can we improve LLMs geo-safety awareness? Despite GPT-4-turbo possessing the knowledge to respond to geo-diverse safety queries, its failures suggest that additional alignment methods might be necessary to effectively elicit and apply this knowledge in model responses. In particular, existing LLMs often struggle to generate the correct type of response and to ensure that their outputs faithfully adhere to the cultural-legal guidelines pertinent to each query. This insight that targeted alignment on these two aspects could enhance overall response quality motivates our subsequent study in $\\S5$ on geo-diverse safety alignment methods. ", "page_idx": 6}, {"type": "text", "text": "In Appendix B.2, we present two sets of evaluation results. Appendix D.1 demonstrates the high correlation with human judgements achieved by the proposed evaluation framework, which validates the effectiveness of our evaluation strategy. ", "page_idx": 6}, {"type": "text", "text": "5 Geo-Diverse Safety Alignment Training ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To align model responses with geo-diverse safety standards, we employ Direct Preference Optimization (DPO) [26], a commonly used alignment method. This method fine-tunes open-source LLMs to effectively address global user queries, ensuring safety and utility. This process requires high-quality simulated user queries and response preference pairs, guidling models to generate more appropriate responses. This section outlines the creation of alignment training data, SAFEWORLDALIGN (\u00a75.2) and details the training settings (\u00a75.3). ", "page_idx": 6}, {"type": "image", "img_path": "VZQmIoDGBG/tmp/968986697ccbdeca4c9038ed33506fbde189148cc6ce8666a273f90df5947a42.jpg", "img_caption": ["Figure 6: Overall framework of geo-diverse safety alignment training. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "5.1 Direct Preference Optimization (DPO) Background ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "DPO is a straightforward alignment training paradigm that uses preference pair data without reinforcement learning. Its main goal is to train an aligned model by optimizing an objective that increases the conditional probability of the positive responses over the negative ones. The DPO training data consists of preference pairs, each containing a user query $Q$ , a positive response $R_{p}$ and a negative response Rn. The entire DPO training annotations can be represented as D = (Q, Rp, Rn)(i) |iD=|1. The optimization objective for DPO minimizes: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{DPO}}(\\pi_{\\theta};\\pi_{\\mathrm{ref}})=-\\mathbb{E}_{(Q,R_{p},R_{n})\\sim\\mathcal{D}}\\Bigg[\\log\\sigma\\Big(\\beta\\log\\frac{\\pi_{\\theta}(R_{p}|Q)}{\\pi_{\\mathrm{ref}}(R_{p}|Q)}-\\beta\\log\\frac{\\pi_{\\theta}(R_{n}|Q)}{\\pi_{\\mathrm{ref}}(R_{n}|Q)}\\Big)\\Bigg],}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\sigma$ is the sigmoid function, $\\beta$ is a hyperparameter, and $\\pi_{\\mathrm{ref}}$ is the initial policy. ", "page_idx": 7}, {"type": "text", "text": "5.2 SAFEWORLDALIGN Alignment Training Data Generation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "DPO training requires preference pair annotations, consisting of a user query $Q$ , a positive response $R_{p}$ , and a negative response $R_{n}$ . We detail how we synthesize these three key components: (1) Training Query Generation: Detailed in $\\S3.3.2$ , this process relies on high-quality, human-verified annotations of cultural-legal guidelines to ensure the generated queries cover geo-diverse safety topics accurately and comprehensively. (2) Positive Response Generation: For a training user query $Q$ regarding cultural norm or policy $I$ , we generate a safe and useful positive response $R_{p}$ that incorporate $I$ using tailored prompts. Specifically, for a query of Type $t$ , we deploy a custom prompt crafted to elicit responses that align with the desired characteristics for that query type. (3) Negative Response Generation: For a Type $t$ training query $Q$ related to a specific cultural norm or policy $I$ , we create two distinct categories of negative responses within the preference pairs: ", "page_idx": 7}, {"type": "text", "text": "Negative Category 1 consists of negative responses that adhere to correct cultural-legal guideline $I$ but correspond to a different response type $t^{\\prime}$ where $t^{\\prime}\\neq t$ . Specifically, for a query of Type $t$ , we utilize a prompt that tailors for generating the response with a different type $t^{\\prime}$ that misaligns with the query type. For example, consider a SPECIFICANSWER query that demands an alerted response to a violated cultural norm or policy. A negative response for this category could be drawn from response DOANSWER, which fails to provide any reminders of the violation. This misalignment between the query and response type further encourages the model to acquire the desired behavior of LLMs when faced with diverse global user queries. ", "page_idx": 7}, {"type": "text", "text": "Negative Category 2 consists of negative responses that match the user query type $t$ but refer to incorrect cultural norms and policies $I^{\\prime}$ where ${\\cal I}^{\\prime}\\ne{\\cal I}$ . For example, if the correct guideline is about infidelity to the wife or girlfriend, a negative response contains a perturbed incorrect guideline $I^{\\prime}$ (e.g., the green hat is offensive to elders). Generating negative responses with the reference of incorrect guidelines $I^{\\prime}$ via LLM prompting ensures these factual errors in the responses while being relevant with the user queries and encourages the model to precisely distinguish and memorize the correct cultural norms and policies. Note that since REFUSETOANSWER queries require only refusal and lack involved cultural norm and policy information, we do not generate responses for this negative response category across all REFUSETOANSWER queries. ", "page_idx": 7}, {"type": "text", "text": "5.3 Alignment Training Settings ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Following the open-source LLM alignment method outlined in the Huggingface Alignment Handbook [37], we employ the DPO training on top of an initial reference policy, Zephyr-7B-SFT-Full, an already supervised fine-tuned (SFT) model. To ensure the integrity of our evaluation, we exclude any training queries that involve cultural-legal guidelines present in the test set. This prevents data leakage and establishes a rigorous testing environment for assessing the model\u2019s capacity to generalize across unfamiliar guidelines during the training process. The final DPO training dataset SAFEWORLDALIGN contains 45,746 preference pairs: 26,382 for Negative Category 1 and 19,364 for Negative Category 2. We refer to our alignment models as SAFEWORLDLM. See Appendix C for parameter details. ", "page_idx": 7}, {"type": "text", "text": "5.4 SAFEWORLDLM Evaluation Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we provide an in-depth evaluation and analysis of the performance of our SAFEWORLDLM on SAFEWORLD. Additionally, we conduct ablation studies to highlight the effectiveness of our specially constructed DPO training data. Our analysis spans both SAFEWORLD and general NLP and safety evaluation benchmarks, demonstrating the robust improvements our approach offers. ", "page_idx": 7}, {"type": "table", "img_path": "VZQmIoDGBG/tmp/940386c48bd53197b32b7ce255416f968ee132f1b67b412baf3aaa160eab9d54.jpg", "table_caption": ["Table 2: Performance of our SAFEWORLDLM on the SAFEWORLD benchmark. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Main Results. Table 1 and Table 2 highlight that our 7B SAFEWORLDLM-series LLMs significantly outperform nearly all competing models, including GPT-4o, across all dimensions. It shows the remarkable efficacy of our geo-diverse safety alignment training. Notably, our leading SAFEWORLDLM model surpasses top-tier proprietary models like GPT-4-turbo and GPT-4o in all dimensions, with especially notable gains in response type classification, showing improvements of $44.7\\%$ and $38.0\\%$ , respectively. These impressive results highlight our model\u2019s unparalleled ability to adapt and respond effectively across a wide range of query types. As we discuss in $\\S4.4$ , GPT-4-turbo often struggles to recognize and respond to the queries appropriately, where the relevant guidelines are embedded in its parametric knowledge. ", "page_idx": 8}, {"type": "text", "text": "What if we enhance GPTs with additional guidance? In Table 2, we compare our SAFEWORLDLMseries LLMs against various prompting baselines that provide explicit instructions for considering regional differences established upon the top-performing GPT-series model, GPT-4-turbo. Additionally, we include baselines that integrate both ground-truth cultural-legal guidelines and relevant guidelines retrieved from SAFEWORLD in the user prompt. We find that even if we provide explicit hints to GPT-4-turbo, our SAFEWORLDLM-series LLMs still demonstrate superior performance, underscoring the substantial beneftis of additional safety alignment training. Although SAFEWORLDLM scores slightly lower in faithfulness compared to GPT-4-turbo w/ Ground-Truth Guidelines, this difference is primarily because the baseline model directly utilizes ground-truth guidelines. We also notice that there are still occasional inconsistencies where GPT-4-turbo might not integrate the provided ground-truth guidelines into its responses, thereby resulting in lower coverage score. ", "page_idx": 8}, {"type": "text", "text": "Ablation Studies To understand the impact of different components in our alignment training, we conduct ablation studies on different variants of SAFEWORLDLM. We tested three variants: (1) SAFEWORLDLM w/o Neg. Category 1 is the variant trained with only the preference pairs containing the negative responses based on incorrect norm and policy knowledge. (2) SAFEWORLDLM w/o Neg. Category 2 is the model trained with only the preference pairs containing the negative responses with incorrect response types. (3) SAFEWORLDLM $(50\\%)$ represents another variant trained using half of the total SAFEWORLD align training dataset, incorporating both types of negative responses, designed for a fair comparison with the previous two variants thanks to the matched amount of training data. As shown in Table 2, the first two variants show distinct advantages. SAFEWORLDLM w/o Neg. Category 1 shows better proficiency in factuality, while SAFEWORLDLM w/o Neg. Category 2 outperforms in response type matching. This can be attributed to the distinct training approaches: SAFEWORLDLM w/o Neg. Category 1 uses preference pair data that emphasizes the contrast between involved norms and policy contents, enabling it to generate more precise and factual responses. On the other hand, SAFEWORLDLM w/o Neg. Category 2 is tailored to better understand and align with the desired behaviors associated with global user query types. This disparity reveals that different negative response generation strategies can significantly enhance model performance in specific key evaluation dimensions critical to the SAFEWORLD benchmark. Furthermore, comparing SAFEWORLDLM $(50\\%)$ with the former two variants shows that it achieves better performance across all evaluation dimensions, indicating that a more holistic improvement in model performance can be achieved by integrating diverse types of preference pairs. ", "page_idx": 8}, {"type": "text", "text": "5.5 General NLP and Safety Benchmark Evaluation Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To further assess the impact of our SAFEWORLD training data on both general NLP and safety benchmarks, we conduct additional experiments to investigate that the geo-diverse safety alignment does not compromise performance on downstream tasks. We select two general NLP tasks, MMLU [15] and HellaSwag [42], from the Open LLM Leaderboard on Huggingface. Following the leaderboard\u2019s few-shot evaluation framework, we provide 5-shot and 10-shot in-context examples for MMLU and HellaSwag, respectively. We also evaluate the models on two general safety benchmarks: Anthropic HH-RLHF [2] and BeaverTails [20]. Using the methodology from [31], we measure the proportion of harmless responses in the test sets as our primary safety metric, implemented through GPT-4-turbo prompting. We compare SAFEWORLDLM with the base model Zephyr-7B-SFT-Full to see the impact of SAFEWORLDALIGN on the general tasks. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "We find that training on SAFEWORLDALIGN can achieve $96.5\\%$ and $80.2\\%$ harmless response ratios, significantly superior to Zephyr-7B-SFT-Full\u2019s performance of $59.3\\%$ and $74.2\\%$ on the two general safety benchmarks, HH-RLHF and BeaverTails. Additionally, we observe that SAFEWORLDALIGN\u2019s performance on two general NLP tasks, MMLU and HellaSwag, is $56.6\\%$ and $78.5\\%$ , matching $56.8\\%$ and $78.5\\%$ performance of Zephyr7B-SFT-Full, even though SAFEWORLDALIGN is designed for geo-diverse safety alignment. These findings suggest that SAFEWORLDALIGN enables models to significantly enhance geodiverse and general safety alignment while maintaining performance on general NLP tasks. In the Appendix D.2, we provide further analysis showing that combining SAFEWORLDALIGN with general alignment data, such as ULTRAFEEDBACK and SAFER-INSTRUCT, enhances performance beyond using ULTRAFEEDBACK and SAFER-INSTRUCT alone, respectively. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "image", "img_path": "VZQmIoDGBG/tmp/ebaafd787eda0e236926c9fbdcf934dd280bf76229813a019f4eb1968c2e6764.jpg", "img_caption": ["Figure 7: Results of comparative evaluation by global annotators. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5.6 Human Evaluation ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We further conduct human evaluation to showcase the effectiveness of SAFEWORLDLM according to the global annotator feedbacks. Following standard settings for evaluating LLMs\u2019 ability to follow instructions [2, 8], we recruit global annotators from 9 different countries as the users to compare and rate model responses to geo-diverse safety queries based on helpfulness and harmlessness. We randomly sample 40 queries from each query type for the human evaluation. From Figure 7, we find that SAFEWORLDLM achieves an $18\u201320\\%$ higher winning rate than GPT-4o in both dimensions, further demonstrating SAFEWORLDLM\u2019s effectiveness and its global acceptability among users. ", "page_idx": 9}, {"type": "text", "text": "5.7 Western vs. Non-Western ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "One of the key objectives of studying geo-diverse safety alignment is to ensure models to perform equitably across both Western and non-Western countries, thereby delivering fair benefits to users worldwide. To achieve this, we analyze performance disparities between instances involving Western and non-Western countries, where smaller disparities indicate greater inclusivity. Notably, apart from the response type alignment dimension, SAFEWORLDLM demonstrates smaller disparities compared to GPT-4o and Command- $\\cdot\\mathbf{R}+$ . We attribute this improvement to the richer emphasis on non-Western knowledge in our training data, as illustrated in Figure 13. This focus likely contributes to the model\u2019s more balanced performance across different regions. These results highlight our commitment to developing inclusive models that cater effectively to a diverse global audience. ", "page_idx": 9}, {"type": "table", "img_path": "VZQmIoDGBG/tmp/4f6aa185696526c92a22f5d3f7130b4643167fe90fb718069a1c53158f8537f4.jpg", "table_caption": ["Table 3: Western vs. Non-Western models performance statistics. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduce SAFEWORLD, a novel benchmark for evaluating safety alignment across diverse global contexts, ensuring LLMs meet the needs of users worldwide. For comprehensively assess LLM response, we propose a holistic multi-dimensional safety evaluation framework focusing on key dimensions needed for address user queries involving geo-diverse safety topics. Beyond the mere evaluation, we also present a geo-diverse safety alignment training method, encouraging the model to acquire the desired behavior and precisely distinguish and memorize the cultural-legal guidelines. We observe that our method significantly enhances geo-diverse safety alignment, outperforming GPT-4o, while also maintaining strong performance on general NLP and safety evaluation tasks. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank the anonymous reviewers for their feedback. This research was supported in part by NSF #2331966, an Amazon AGI Research Award, Google Research Scholar, and a CISCO gift. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Badr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona T. Diab, and Marjan Ghazvininejad. A review on language models as knowledge bases. ArXiv, abs/2204.06031, 2022.   \n[2] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.   \n[3] Faeze Brahman, Sachin Kumar, Vidhisha Balachandran, Pradeep Dasigi, Valentina Pyatkin, Abhilasha Ravichander, Sarah Wiegreffe, Nouha Dziri, Khyathi Chandu, Jack Hessel, et al. The art of saying no: Contextual noncompliance in language models. arXiv preprint arXiv:2407.12043, 2024.   \n[4] Yu-Chu Chang, Xu Wang, Jindong Wang, Yuanyi Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Weirong Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qian Yang, and Xingxu Xie. A survey on evaluation of large language models. ArXiv, abs/2307.03109, 2023.   \n[5] Hailin Chen, Fangkai Jiao, Xingxuan Li, Chengwei Qin, Mathieu Ravaut, Ruochen Zhao, Caiming Xiong, and Shafiq R. Joty. Chatgpt\u2019s one-year anniversary: Are open-source large language models catching up? ArXiv, abs/2311.16989, 2023.   \n[6] Yu Ying Chiu, Liwei Jiang, Maria Antoniak, Chan Young Park, Shuyue Stella Li, Mehar Bhatia, Sahithya Ravi, Yulia Tsvetkov, Vered Shwartz, and Yejin Choi. Culturalteaming: Aiassisted interactive red-teaming for challenging llms\u2019 (lack of) multicultural knowledge. ArXiv, abs/2404.06664, 2024.   \n[7] Awantee V. Deshpande, Dana Ruiter, Marius Mosbach, and Dietrich Klakow. Stereokg: Data-driven knowledge graph construction for cultural knowledge and stereotypes. ArXiv, abs/2205.14036, 2022.   \n[8] Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. Advances in Neural Information Processing Systems, 36, 2024.   \n[9] Esin Durmus, Karina Nyugen, Thomas I Liao, Nicholas Schiefer, Amanda Askell, Anton Bakhtin, Carol Chen, Zac Hatfield-Dodds, Danny Hernandez, Nicholas Joseph, et al. Towards measuring the representation of subjective global opinions in language models. arXiv preprint arXiv:2306.16388, 2023.   \n[10] Yi Ren Fung, Tuhin Chakraborty, Hao Guo, Owen Rambow, Smaranda Muresan, and Heng Ji. Normsage: Multi-lingual multi-cultural norm discovery from conversations on-the-fly. In Conference on Empirical Methods in Natural Language Processing, 2022.   \n[11] Yi Ren Fung, Ruining Zhao, Jae Doo, Chenkai Sun, and Heng Ji. Massively multi-cultural knowledge acquisition & lm benchmarking. ArXiv, abs/2402.09369, 2024.   \n[12] Deep Ganguli, Danny Hernandez, Liane Lovitt, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova Dassarma, Dawn Drain, Nelson Elhage, et al. Predictability and surprise in large generative models. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, pages 1747\u20131764, 2022.   \n[13] Christian Haerpfer, Ronald Inglehart, Alejandro Moreno, Christian Welzel, Kseniya Kizilova, Jaime Diez-Medrano, Marta Lagos, Pippa Norris, Eduard Ponarin, and Bi Puranen. World values survey wave 7 (2017-2022) cross-national data-set. World Values Survey Association, 2022.   \n[14] Peter Henderson, Koustuv Sinha, Nicolas Angelard-Gontier, Nan Rosemary Ke, Genevieve Fried, Ryan Lowe, and Joelle Pineau. Ethical challenges in data-driven dialogue systems. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pages 123\u2013129, 2018.   \n[15] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.   \n[16] Kung-Hsiang Huang, Hou Pong Chan, and Heng Ji. Zero-shot faithful factual error correction. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5660\u20135676, Toronto, Canada, July 2023. Association for Computational Linguistics.   \n[17] Kung-Hsiang Huang, Philippe Laban, A. R. Fabbri, Prafulla Kumar Choubey, Shafiq R. Joty, Caiming Xiong, and Chien-Sheng Wu. Embrace divergence for richer insights: A multidocument summarization benchmark and a case study on summarizing diverse information from news articles. ArXiv, abs/2309.09369, 2023.   \n[18] Kung-Hsiang Huang, Kathleen McKeown, Preslav Nakov, Yejin Choi, and Heng Ji. Faking fake news for real fake news detection: Propaganda-loaded training data generation. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14571\u201314589, Toronto, Canada, July 2023. Association for Computational Linguistics.   \n[19] Kung-Hsiang Huang, ChengXiang Zhai, and Heng Ji. CONCRETE: Improving cross-lingual fact-checking with cross-lingual retrieval. In Nicoletta Calzolari, Chu-Ren Huang, Hansaem Kim, James Pustejovsky, Leo Wanner, Key-Sun Choi, Pum-Mo Ryu, Hsin-Hsi Chen, Lucia Donatelli, Heng Ji, Sadao Kurohashi, Patrizia Paggio, Nianwen Xue, Seokhwan Kim, Younggyun Hahm, Zhong He, Tony Kyungil Lee, Enrico Santus, Francis Bond, and Seung-Hoon Na, editors, Proceedings of the 29th International Conference on Computational Linguistics, pages 1024\u20131035, Gyeongju, Republic of Korea, October 2022. International Committee on Computational Linguistics.   \n[20] Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of llm via a human-preference dataset. Advances in Neural Information Processing Systems, 36, 2023.   \n[21] Zi Lin, Zihan Wang, Yongqi Tong, Yangkun Wang, Yuxin Guo, Yujia Wang, and Jingbo Shang. ToxicChat: Unveiling hidden challenges of toxicity detection in real-world user-AI conversation. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 4694\u20134702, Singapore, December 2023. Association for Computational Linguistics.   \n[22] Stuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):129\u2013137, 1982.   \n[23] Tuan-Phong Nguyen, Simon Razniewski, Aparna S. Varde, and Gerhard Weikum. Extracting cultural commonsense knowledge at scale. Proceedings of the ACM Web Conference 2023, 2022.   \n[24] OpenAI. Gpt-4 technical report. 2023.   \n[25] Haoyi Qiu, Wenbo Hu, Zi-Yi Dou, and Nanyun Peng. Valor-eval: Holistic coverage and faithfulness evaluation of large vision-language models. 2024.   \n[26] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. ArXiv, abs/2305.18290, 2023.   \n[27] Abhinav Rao, Akhila Yerukola, Vishwa Shah, Katharina Reinecke, and Maarten Sap. Normad: A benchmark for measuring the cultural adaptability of large language models. arXiv preprint arXiv:2404.12464, 2024.   \n[28] Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and Tatsunori Hashimoto. Whose opinions do language models reflect? In International Conference on Machine Learning, pages 29971\u201330004. PMLR, 2023.   \n[29] Mortimer N. S. Sellers. What is the rule of law and why is it so important. 2014.   \n[30] Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. Societal biases in language generation: Progress and challenges. In Proceedings of the Conference of the 59th Annual Meeting of the Association for Computational Linguistics (ACL), 2021.   \n[31] Taiwei Shi, Kai Chen, and Jieyu Zhao. Safer-instruct: Aligning language models with automated preference data. NAACL, 2024.   \n[32] Weiyan Shi, Ryan Li, Yutong Zhang, Caleb Ziems, Chunhua yu, Raya Horesh, Rog\u2019erio Abreu de Paula, and Diyi Yang. Culturebank: An online community-driven knowledge base towards culturally aware language technologies. 2024.   \n[33] Taylor Sorensen, Jared Moore, Jillian Fisher, Mitchell Gordon, Niloofar Mireshghallah, Christopher Michael Rytting, Andre Ye, Liwei Jiang, Ximing Lu, Nouha Dziri, et al. A roadmap to pluralistic alignment. ICML, 2024.   \n[34] Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. One embedder, any task: Instruction-finetuned text embeddings. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 1102\u20131121, Toronto, Canada, July 2023. Association for Computational Linguistics.   \n[35] Hao Sun, Guangxuan Xu, Jiawen Deng, Jiale Cheng, Chujie Zheng, Hao Zhou, Nanyun Peng, Xiaoyan Zhu, and Minlie Huang. On the safety of conversational models: Taxonomy, dataset, and benchmark. In Findings of the Conference of the 60th Annual Meeting of the Association for Computational Linguistics (ACL-findings), 2022.   \n[36] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. ArXiv, abs/2302.13971, 2023.   \n[37] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Cl\u00e9mentine Fourrier, Nathan Habib, et al. Zephyr: Direct distillation of lm alignment. arXiv preprint arXiv:2310.16944, 2023.   \n[38] Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359, 2021.   \n[39] Robyn Williams. Cultural safety \u2014 what does it mean for our work practice? Australian and New Zealand Journal of Public Health, 23, 1999.   \n[40] Da Yin, Hritik Bansal, Masoud Monajatipoor, Liunian Harold Li, and Kai-Wei Chang. GeoMLAMA: Geo-diverse commonsense probing on multilingual pre-trained language models. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2039\u20132055, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.   \n[41] Da Yin, Liunian Harold Li, Ziniu Hu, Nanyun Peng, and Kai-Wei Chang. Broaden the vision: Geo-diverse visual commonsense reasoning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2115\u20132129, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.   \n[42] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4791\u20134800, 2019.   \n[43] Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, and Minlie Huang. Safetybench: Evaluating the safety of large language models with multiple choice questions. ArXiv, abs/2309.07045, 2023.   \n[44] Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai-Wei Chang, Minlie Huang, and Nanyun Peng. On prompt-driven safeguarding for large language models. In Proceedings of the Fortieth International Conference on Machine Learning (ICML), 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A SAFEWORLD Construction ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The benchmark creation involves two key stages: (1) constructing GEOSAFEDB, a cultural and legal geo-diverse safety database (Appendix A.1), and (2) formulating SAFEWORLD benchmark queries, each of which corresponds to cultural-legal guidelines in GEOSAFEDB (Appendix A.3). ", "page_idx": 14}, {"type": "image", "img_path": "VZQmIoDGBG/tmp/ec71e123373bf70acc55cf90b9ef761a5716f891240a631cfa3a1ab947e68291.jpg", "img_caption": ["A.1 GEOSAFEDB Development ", "Figure 8: Overview of database generation. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "The initial phase of SAFEWORLD focuses on developing GEOSAFEDB, a culturally and legally geo-diverse safety database. This database includes cultural norms and public policies from various geographic backgrounds. Previous methods face challenges such as limited relevance to safety concerns and compromised data quality, often due to top-down collection methods and insufficient annotation processes. To overcome these limitations, we propose a bottom-up approach that gathers country- and region-level guidelines through LLM prompting, followed by validation by native or local annotators, ensuring both accuracy and cultural and legal sensitivity. ", "page_idx": 14}, {"type": "image", "img_path": "VZQmIoDGBG/tmp/e575f924d102d134d93a0e74691f7352c3ef65b77e755f0c564d161a6f0c761e.jpg", "img_caption": ["Figure 9: Prompts for GPT-4-turbo to country-level cultural-legal guidelines. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "We select the top 50 most populous countries2 and use GPT-4-turbo with prompts from Figure 9 to generate 100 unique, country-specific cultural-legal guidelines for each, ensuring geo-diversity in SAFEWORLD. These guidelines then undergo a multi-step, rigorous verification process involving both machine and human-based validation. We employ retrieval-augmented LLMs like Command- $\\cdot\\mathrm{\\bfR}^{\\breve{3}}$ and GPT-4-turbo to cross-check the guidelines against web-based information and pre-trained knowledge. Specifically, Command-R assesses each norm and policy using the prompt: \u201cDo you think <NORM/POLICY> is a well-known [cultural norm/policy] in <COUNTRY>?\u201d Guidelines that receive a response of \u201cYes\u201d or \u201cUnsure,\u201d are retained, leveraging Command-R\u2019s precision in Retrieval Augmented Generation (RAG) to validate norms and policies using online information. Subsequently, GPT-4 re-evaluates the filtered norms and policies using the same prompt, with only those receiving a \u201cYes\u201d moving forward. For a final layer of validation, global human annotators from the selected 50 countries, sourced through Amazon Mechanical Turk Platform, review the guidelines. nnotators were selected through a qualification test that included a reading comprehension task and a simulated verification exercise. Each guideline was reviewed by three annotators from the respective country, with an \u201cUnsure\u201d option available to accommodate the diversity within countries. Figure 14 displays screenshots of the qualification process and the verification task. Annotators were compensated at a rate of $\\mathbb{S}15$ per hour. Due to budget constraints, human annotators were not recruited for policy validation; instead, we relied on machine-based verification, which demonstrated a high correlation (0.92) with human validation in a pilot test with 50 examples. ", "page_idx": 14}, {"type": "text", "text": "Additionally, within a country, cultural and legal practices can vary significantly between regions and among different racial or ethnic groups. For example, in India, while national laws generally prohibit cow slaughter due to the sacred status of cows in Hinduism, certain states like West Bengal permit it. To capture these nuances, we include region-level cultural-legal guidelines by prompting GPT-4-turbo based on the country-level guidelines: \u201cAre there any variations of the given [norm/policy] in <COUNTRY> related to different regions or races? Please list three to five variations.\u201d Our data collection process incorporates thorough machine and human validation to ensure that each region\u2019s cultural-legal landscape is accurately and comprehensively represented. This approach yields a dataset of 7,447 cultural norms $(D_{C})$ and 6,652 public policies $(D_{L})$ spanning 50 countries and 493 regions and racial groups. ", "page_idx": 15}, {"type": "text", "text": "A.2 Global User Survey Regarding Geo-Diverse Safety User Query Types ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Before finalizing the global user query types for our study, as shown in Figure 15, we conduct a survey to better understand the response types that global users might expect in geo-diverse scenarios. We introduce three candidate response types, labeled as SPECIFICANSWER, COMPREANSWER, and REFUSETOANSWER, for participants to consider. Among the 21 respondents from 8 different countries, 11 expressed a preference for all three response types, while only 2 opted for none. Based on these insights, we decided to include all three query types in our study. Additionally, to enhance the complexity of the safety benchmark and to discourage models from overly frequent alerts about norm or policy violations, we incorporated DOANSWER queries into our evaluation. ", "page_idx": 15}, {"type": "text", "text": "A.3 Cultural Norms and Legal Policies/Laws Queries ", "text_level": 1, "page_idx": 15}, {"type": "image", "img_path": "VZQmIoDGBG/tmp/a6e81dec9926e79e5fd593ca7564fc66a5f77f7576bafcb55cde1a0fa8527a12.jpg", "img_caption": ["Figure 10: Overview of database generation. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "After building GEOSAFEDB we proceed to construct queries that reflect real-life geo-diverse safety situations. To identify the most relevant use cases, we conduct surveys with participants from diverse geographical backgrounds. Based on the survey results (Appendix A.2), we design four distinct query types, each tailored to elicit a specific response type. Each query in SAFEWORLD includes a scenario that presents a culturally or legally sensitive context, accompanied by a relevant question. Figure 10 illustrates our query generation process. For more details on the generation prompts, please refer to the Supplemental Material. Below, we detail the steps involved in creating these queries. Figure 11 show SAFEWORLD query examples. ", "page_idx": 15}, {"type": "table", "img_path": "VZQmIoDGBG/tmp/9907302120e82663be7d31b8c3a81e9718fad1bc9dd6e02a78ae43583b06177d.jpg", "table_caption": ["Table 4: SAFEWORLD queries types. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "SPECIFICANSWER. These queries involve scenarios that have already violated the cultural-legal guidelines of the queried country, race, or region. While the questions themselves might not be culturally or legally unsafe, LLMs should identify the specific guideline that has been violated when providing a response. To generate such SPECIFICANSWER queries, we create norm- or policyviolating scenarios and corresponding questions for each cultural-legal guideline $g_{v}\\in D_{C}\\cup D_{L}$ , using carefully crafted prompts for GPT-4-turbo. ", "page_idx": 15}, {"type": "image", "img_path": "VZQmIoDGBG/tmp/a3d5a5586698306137670bdfcf344f20b15fdefa78f3c34bdb961722c7b41e19.jpg", "img_caption": ["Figure 11: SAFEWORLD query examples across four types. Some are paired with their corresponding reference (i.e., ground-truth) cultural-legal guidelines. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "COMPREANSWER. For scenarios where no specific countries, races, or regions are mentioned, but potentially violate norms or laws of some communities, models should provide comprehensive responses covering representative regions where such scenarios might be unsafe. To generate these queries, we cluster $N$ instances of violated cultural-legal guidelines from SPECIFICANSWER queries into $M_{1}$ clusters using K-Means [22], based on the norm and policy embeddings from INSTRUCTOR [34]. This identifies cultural-legal guidelines with shared topics. Based on them, the COMPREANSWER query generation can be more coherent to the shared topics and indeed involve those guidelines. Specifically, for each cluster, we prompt GPT-4-turbo to create $K_{1}$ scenarios and questions integrating the guidelines into contexts where they are violated, producing $K_{1}\\times M_{1}$ queries. ", "page_idx": 16}, {"type": "text", "text": "REFUSETOANSWER. Models should consistently avoid directly addressing certain inappropriate queries, such as those that compare cultural or legal systems or impose one group\u2019s guidelines onto another. To generate scenarios involving two countries, races, or regions, we cluster $N$ instances of violated norms or policies from SPECIFICANSWER queries into $M_{2}$ clusters using INSTRUCTOR, similar to the construction of COMPREANSWER queries. For each cluster, GPT-4-turbo generates $K_{2}$ scenarios and corresponding questions by embedding these norms or policies in various contexts related to specific races or regions, producing a total of $K_{2}\\times M_{2}$ queries. ", "page_idx": 16}, {"type": "text", "text": "DOANSWER. DOANSWER queries consist of scenarios and questions that adhere to cultural-legal guidelines. They evaluate a model\u2019s ability to provide helpful responses without mistakenly raising red flags, similar to assessing a model\u2019s precision. To construct these queries, we synthesize a scenario adhering to a specific cultural-legal guideline $g_{a}\\in D_{C}\\cup D_{L}$ using GPT-4-turbo. Since the scenarios are designed to be safe, we generate relevant questions without any restrictions. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Empirically, we choose $M_{1}=M_{2}=250$ and $K_{1}=K_{2}=10$ in our query generation. ", "page_idx": 17}, {"type": "text", "text": "By construction, this data collection process naturally annotates each instance in the SAFEWORLD benchmark with the query type, the expected response type $r_{y}$ , and the associated cultural-legal guideline $k^{y}$ . For SPECIFICANSWER and COMPREANSWER queries, $k^{y}$ specifies the violated guideline, denoted as $k^{y}=\\{g_{v}\\}$ . In contrast, for DOANSWER queries, $k^{y}$ identifies the guideline that is followed, represented as $k^{y}\\,=\\,\\{g_{a}\\}$ . For REFUSETOANSWER queries, $k^{y}$ is an empty set $(k^{y}=\\emptyset)$ ), indicating that no guidelines are either violated or adhered to, and their responses do not reference any specific guidelines. After generating these queries, we employ a multi-round validation process involving both machines and humans. Initially, we use GPT-4-turbo to assess the relevance of each query against our established criteria for cultural and legal safety, as outlined in $\\S3.1$ . Queries are rated on a scale from 1 (least relevant) to 5 (most relevant), retaining only those with a score of 4 or higher. The \u201cOriginal\u201d columns in Table 5 display the number of queries that remain after this machine validation step. To ensure a high-quality evaluation set, we randomly sample 500 queries from each category, maintaining a balanced distribution across different countries. These sampled queries are then further validated by two experienced annotators. Only those that receive unanimous approval for both validity and relevance are included in the final evaluation set. This rigorous process results in a dataset of 2,775 human-verified queries, forming the core of our evaluation set. The remaining queries serve as raw training data, providing a robust foundation for further alignment and model training. Detailed statistics of SAFEWORLD are provided in Table 5, highlighting the thoroughness of our validation procedure. ", "page_idx": 17}, {"type": "table", "img_path": "VZQmIoDGBG/tmp/b3cc1627aea41d3e18ba7f326d98b048002b775cf54430d6be9502da4d2f3001.jpg", "table_caption": ["Table 5: SAFEWORLD detailed statistics. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 12 and Figure 13 illustrate the distribution of countries represented in GEOSAFEDB, SAFEWORLD (i.e., test set) and SAFEWORLDALIGN (i.e., train set). We find that the country distribution slightly skews towards non-Western countries, due to the higher agreement rate among the human validators when flitering inaccurate cultural-legal guidelines. TThe coverage of countries for cultural norms is narrower compared to the policy portion, as validating cultural norms requires additional manual effort (see Appendix A.1). Additionally, we faced challenges in finding qualified annotators for some regions. Figure 14 provides screenshots of the Mturk tasks used to select qualified geodiverse annotators and verify cultural-legal guidelines, using the example of the qualification test for US-based annotators and the cultural norm verification tasks. ", "page_idx": 17}, {"type": "text", "text": "B Automatic Evaluation Framework ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 Evaluated Models ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We conduct a comprehensive evaluation of six open-source (Zephyr, LLaMA-2, LLaMA-3, Mistral) and five proprietary (OpenAI and Cohere families). Detailed information about the model versions can be found in Table 6. ", "page_idx": 17}, {"type": "text", "text": "B.2 Response Quality Assessment ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We present evaluation results for two models: SAFEWORLDLM and GPT-4-turbo. The evaluation includes two key components: extraction_list, which consists of norms or policies extracted from the models\u2019 responses using GPT-4-turbo, and response_type_classification, which categorizes the type of responses generated by GPT-4-turbo. As an example (Table 7), we highlight a SPECIFICANSWER query concerning local traditional ceremonies in Egypt, focusing on aspects of public sharing and privacy: ", "page_idx": 17}, {"type": "image", "img_path": "VZQmIoDGBG/tmp/db70d27baba902d2b5862f31a7cd8092c2ee4406bc4730acd81110ee2c28e1bc.jpg", "img_caption": ["Figure 12: GEOSAFEDB country distribution. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "VZQmIoDGBG/tmp/cd1f904da287894e712076975b3a1d8f6388b846a5ae1366373b01e24beec637.jpg", "img_caption": ["Figure 13: SAFEWORLD (i.e., test set) and SAFEWORLDALIGN (i.e., train set) country distributions. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Based on our results, SAFEWORLDLM demonstrates a strong ability to identify and respect cultural norms, guiding users towards culturally appropriate behavior. In contrast, GPT-4-turbo tends to disregard these norms, offering recommendations like direct live-streaming without considering cultural sensitivities. Specifically, SAFEWORLDLM references 8 cultural norms in its response, with 2 of them matching the ground-truth norms, resulting in a faithfulness score of 0.25. Meanwhile, GPT-4-turbo\u2019s response does not align with any ground-truth norms, yielding a score of 0. Moreover, SAFEWORLDLM successfully covers all ground-truth norms, achieving a coverage score of 1, while GPT-4-turbo scores 0 in this area. This example highlights SAFEWORLDLM\u2019s superior performance and validates our evaluation framework for accurately recognizing and respecting cultural norms. ", "page_idx": 18}, {"type": "text", "text": "Geo-Diverse Annotator Qualification Test ", "text_level": 1, "page_idx": 19}, {"type": "image", "img_path": "VZQmIoDGBG/tmp/2a1508765e8267333b48628075069883cf1d11ac5647d633b2484109419dfcda.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Cultural-Legal Guideline Verification Task ", "text_level": 1, "page_idx": 19}, {"type": "image", "img_path": "VZQmIoDGBG/tmp/a18541838b9b40ec6c476f8af99d085bf349aaa9efd63cd4a5c2cc7979fd61b8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 14: Mturk task screenshots for selecting qualified geo-diverse annotators and cultural-legal guideline verification. We take the example about US annotator qualification test and the cultural norm verification tasks. ", "page_idx": 19}, {"type": "text", "text": "C Alignment Training Settings ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Following the open-source LLM alignment method outlined in the Huggingface Alignment Handbook [37], we employ the DPO training on top of an initial reference policy, Zephyr-7B-SFT-Full, an already supervised fine-tuned (SFT) model. Consistent with the handbook\u2019s guidelines, we conduct training with 4 NVIDIA A100 80GB GPUs for one epoch using a batch size of 32, a learning rate of $5\\times10^{-7}$ , a $\\beta$ value of 0.01 in the DPO loss function, and a warmup rate of 0.1. ", "page_idx": 19}, {"type": "text", "text": "D Further Discussions ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we seek to answer a couple of additional research questions. ", "page_idx": 19}, {"type": "table", "img_path": "VZQmIoDGBG/tmp/dc3d25b3e59374b5a5c34321ec875f29ebec17676034392cfe9626d31ca33ed0.jpg", "table_caption": ["Table 6: Specification of the evaluated models. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 7: Evaluation results for two models: SAFEWORLDLM and GPT-4-turbo. Based on our results, SAFEWORLDLM demonstrates a strong ability to identify and respect cultural norms, guiding users towards culturally appropriate behavior. In contrast, GPT-4-turbo tends to disregard these norms, offering recommendations like direct live-streaming without considering cultural sensitivities. Specifically, SAFEWORLDLM references 8 cultural norms in its response, with 2 of them matching the ground-truth norms, resulting in a faithfulness score of 0.25. Meanwhile, GPT-4-turbo\u2019s response does not align with any ground-truth norms, yielding a score of 0. Moreover, SAFEWORLDLM successfully covers all ground-truth norms, achieving a coverage score of 1, while GPT-4-turbo scores 0 in this area. This example highlights SAFEWORLDLM\u2019s superior performance and validates our evaluation framework for accurately recognizing and respecting cultural norms. ", "page_idx": 20}, {"type": "table", "img_path": "VZQmIoDGBG/tmp/76a62a9157196c9af1aafb7a830df94fde79a1e1e2bba5cad4d36cd2dfcc4465.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "D.1 How reliable is our evaluation framework? ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We conduct experiments to assess whether our LLM-based automatic evaluation framework aligns with human evaluations across four dimensions. To this end, we randomly sample 60 responses generated by five models from our evaluation results, calculating Pearson correlation $(\\rho)$ and Kendall\u2019s tau $(\\tau)$ scores. We utilize Llama-3-70B-Instruct and GPT-4-turbo as the base models for the evaluation metric. As shown in Table 8, our results indicate a notably strong correlation $_{(>0.7)}$ across all dimensions between human judgments and our evaluation framework when using GPT-4-turbo. In contrast, Llama-3-70B-Instruct demonstrates only moderate correlation. Given these findings, we prioritize GPT-4-turbo for our evaluators due to its superior alignment with human assessments. ", "page_idx": 20}, {"type": "text", "text": "Al Assistant Expected Behavior in Geo-Diverse ", "text_level": 1, "page_idx": 21}, {"type": "image", "img_path": "VZQmIoDGBG/tmp/132b2dd761cf911aca59a37112e11d5796f2f99d8f1ab5f9cb05fb5ace37e2ad.jpg", "img_caption": ["Figure 15: Screenshot for global user survey to finalize the query types we study. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "D.2 Does using more alignment data yield better performance? ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "General NLP Benchmark Evaluation. We initially explore the effect of integrating SAFEWORLD training data with the commonly used ULTRAFEEDBACK DPO data, which is aimed at enhancing response helpfulness. Our findings indicate that adding SAFEWORLD data to ULTRAFEEDBACK training set results in a $1\\!-\\!2\\%$ performance improvement over using ULTRAFEEDBACK alone on general NLP tasks. Remarkably, even though the SAFEWORLD training data is smaller than ULTRAFEEDBACK, it matches ULTRAFEEDBACK in performance on MMLU. This not only underscores ", "page_idx": 21}, {"type": "text", "text": "Table 8: Comparison of Pearson $\\rho$ and Kendall $\\tau$ correlation scores between our LLM-based evaluation framework and human judgments, using Llama-3-70B-Instruct and GPT-4-turbo as the underlying models for the evaluation metric. ", "page_idx": 22}, {"type": "table", "img_path": "VZQmIoDGBG/tmp/1f7c87850106d48c042458be42c864c03af30002d432a1f41a0d4445b591adc6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "VZQmIoDGBG/tmp/a8839e57d06208b8c90cee15fa76c6210ddb948efc143bb2a28fcc3901ad8b37.jpg", "table_caption": ["Table 9: Model accuracy $(\\%)$ on general NLP tasks and ratio of harmlessness responses $(\\%)$ on general safety benchmarks. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "the quality of the instruction-response pair annotations within the SAFEWORLD dataset but also highlights its potential to enhance both the geo-diverse safety alignment and overall capabilities of LLMs. ", "page_idx": 22}, {"type": "text", "text": "General Safety Benchmark Evaluation. Consistent with findings from general NLP tasks, we found that incorporating SAFEWORLD training data with SAFER-INSTRUCT, which is tailored for general safety alignment, yields beneficial outcomes on general safety evaluation tasks as well. When the training data from both sources are combined, there is a notable $3.5\\%$ improvement over using SAFER-INSTRUCT alone on the Anthropic HH-RLHF benchmark. Moreover, SAFEWORLD training data by itself even outperforms SAFER-INSTRUCT on the same benchmark, which is specifically designed to enhance performance on general safety dimensions. This underscores SAFEWORLD\u2019s capability not only to enhance the general safety of LLMs but also to contribute significantly to the geo-diverse safety aspects. ", "page_idx": 22}, {"type": "text", "text": "D.3 Which types of queries are LLMs better/worse at? ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Figure 16 illustrates the performance breakdown of various models when handling different types of queries. The scores for each dimension represent the average of the scores presented in Table 1 and Table 2. Notably, both open-source and proprietary LLMs, such as Mistral-7B-Instruct, Llama-3-8B-Instruct, Command-R-Plus, GPT-4-turbo, and GPT-4o, generally perform poorly on norm/policy queries, with the exception of NORMDOANSWER. In contrast, our alignment model, SAFEWORLDLM consistently outperforms the other LLMs across all query types. ", "page_idx": 22}, {"type": "text", "text": "E Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Coverage of Countries. Our SAFEWORLD benchmark currently focuses on the top 50 most populous countries, which limits its scope by excluding cultural and legal norms from less populous nations. This narrow coverage may result in the omission of valuable geo-diverse perspectives that are crucial for achieving a truly comprehensive understanding of global safety norms. Expanding the dataset to include a wider range of countries in future work will be essential for ensuring more inclusive and well-rounded geo-diverse safety alignment. ", "page_idx": 22}, {"type": "image", "img_path": "VZQmIoDGBG/tmp/8b966e79b40f03e91672cbda52a984763c291ee50382b2d27e83cf24f65b9ee7.jpg", "img_caption": ["Figure 16: Performance of various LLMs on different types of norm/policy queries. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Limited Scope of Query Types. Although our study encompasses a range of query types, it does not capture the full spectrum of scenarios and subtleties present in real-world geo-diverse safety contexts. This limitation may result in the underrepresentation of certain response types and cultural-legal nuances in our analysis. To enhance the robustness of LLM performance evaluation in geo-diverse safety alignment, future research should focus on expanding the diversity of query types and scenarios considered. ", "page_idx": 23}, {"type": "text", "text": "Addressing Hallucination Challenges. Despite the strong performance of our SAFEWORLDLM it remains susceptible to generating inaccurate or potentially unsafe responses in real-world scenarios. To address this, we will actively warn users against unintended use and prioritize the implementation of robust safeguards. Furthermore, we are committed to ongoing research aimed at developing more reliable strategies to mitigate hallucination risks, while continuously enhancing the model\u2019s geo-diverse safety and accuracy. ", "page_idx": 23}, {"type": "text", "text": "F Supplementary Materials ", "text_level": 1, "page_idx": 24}, {"type": "image", "img_path": "VZQmIoDGBG/tmp/557d0c91f90075df396803b5f67cb69c543a5911b69bb99e861a7f37321a79ca.jpg", "img_caption": ["Table 10: Prompt template for scenarios generation for norms SPECIFICANSWER queries generation. {In-context examples} are in-context examples. {Input norm} refers to norms extracted from GEOSAFEDB. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "VZQmIoDGBG/tmp/4725dd045a7cd6f4f1d932476cbe82b66e1bb4f9db0f140250c830524748b50f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Table 11: Prompt template for scenarios generation for norms DOANSWER queries generation. {In-context examples} are in-context examples. {Input norm} refers to norms extracted from GEOSAFEDB. ", "page_idx": 24}, {"type": "text", "text": "Remove the location and culture/policy entities, while keeping the fluency of the sentences. Start with \u201cScenario:\u201d. No more details and explanations. Here is the given scenario: {Input scenario}. ", "page_idx": 24}, {"type": "text", "text": "Table 12: Prompt template for norms or policies location removal for SPECIFICANSWER or DOANSWER queries generation. {Input scenario} are generated scenario with location described in Table 10, Table 11, Table 17, or Table 18. ", "page_idx": 24}, {"type": "image", "img_path": "VZQmIoDGBG/tmp/7f7a55ad01dc168faa0e0b180ceeff36c0fa79cfc64e5cf191f10b07786ccd38.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Table 13: Prompt template for questions generation for norms SPECIFICANSWER queries generation. {In-context examples} are in-context examples. {Input scenario} are generated scenarios without locations described in Table 12 from norms SPECIFICANSWER queries. ", "page_idx": 25}, {"type": "text", "text": "Can you imagine $K_{1}$ VIVID and RELEVANT scenarios given the norms of different countries? Describe them with a casual, informal and brief way in 2-3 short sentences. Write it using the first-person perspective. The last sentence must be either in future tense or a question. Also, please provide the related specific norms involved in this situation and all the related countries. Each scenario starts with \u201cScenarios [INDEX]:\u201d. Each scenario can cover one to many countries. DO NOT mention ANY SPECIFIC LOCATION OR COUNTRY in the outputs. No more REFLECTION AND RECONSIDERATION (such as \u201cunaware\u201d, \u201cforgetting\u201d, \u201cnot realizing\u201d, \u201coverlooking\u201d, etc.) about the scenario. No explicit offensive words such as \u201cinterrupt\u201d, \u201cblame\u201d, \u201cmake a joke\u201d, etc. DO NOT question ANYTHING ABOUT IF THE scenario IS APPROPRIATE, RESPECTFUL, OR HOW I CAN ENSURE RESPECTFULNESS!!   \nHere is an example: {In-context examples}   \nTest case:   \nNorms of different countries: {Input cluster of violated specific norms} ", "page_idx": 25}, {"type": "text", "text": "Table 14: Prompt template for queries generation for norms COMPREANSWER queries. {In-context examples} are in-context examples. {Input cluster of violated specific norms} are the violated specific norms generated during norms SPECIFICANSWER scenarios generation. ", "page_idx": 25}, {"type": "image", "img_path": "VZQmIoDGBG/tmp/33ae5c90b4d594acee7617cdc5fb1c1751858d82ba71a2d16cbd05350bbecfee.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Table 15: Prompt template for queries generation for norms REFUSETOANSWER queries. {Incontext examples} are in-context examples. {Input cluster of violated specific norms} are the violated specific norms generated during norms SPECIFICANSWER scenarios generation. ", "page_idx": 25}, {"type": "image", "img_path": "VZQmIoDGBG/tmp/663b0f6e491c021b43a1cd8a241b6bf175f517a1fea3afd869a6fe3f2ad5c971.jpg", "img_caption": ["Table 16: Prompt template for questions generation for norms DOANSWER queries generation. {In-context examples} are in-context examples. {Input scenario} are generated scenarios without locations described in Table 12 from norms DOANSWER queries. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "VZQmIoDGBG/tmp/d8a4b5068b23b173a95c8a5c68e5c3002227d9ee37f6e002eda89f3a888bd3f1.jpg", "img_caption": ["Table 17: Prompt template for SPECIFICANSWER scenarios generation for policies SPECIFICANSWER queries generation. {In-context examples} are in-context examples. {Input policy} refers to policies extracted from GEOSAFEDB. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "VZQmIoDGBG/tmp/f9f618e49dae442d453c785c6993891521f2b6403dbea2be23e842a518fbf6dc.jpg", "img_caption": ["Table 18: Prompt template for DOANSWER scenarios generation for policies DOANSWER queries generation. {In-context examples} are in-context examples. {Input policy} refers to policies extracted from GEOSAFEDB. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "VZQmIoDGBG/tmp/60d0a2d49d4d4be28e2a055fc621d0489053961e888bdd658ca42020930fb0ec.jpg", "img_caption": ["Table 19: Prompt template for question generation for policies SPECIFICANSWER queries generation. {In-context examples} are in-context examples. {Input scenario} are generated scenarios without locations described in Table 12 from policies SPECIFICANSWER queries. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "VZQmIoDGBG/tmp/a3bbc0b7d4e6ea26fbd912408830dae2402fffe56947f2d4aecd4d2f332b1270.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 20: Prompt template for queries generation for policies COMPREANSWER queries. {In-context examples} are in-context examples. {Input cluster of violated specific policies} are the violated specific policies generated during policies SPECIFICANSWER queries generation. ", "page_idx": 28}, {"type": "image", "img_path": "VZQmIoDGBG/tmp/4479461f57db1a26d878152f6c13d093f64f3e95745d4da8022b52433b8218db.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 21: Prompt template for queries generation for policies REFUSETOANSWER queries. {Incontext examples} are in-context examples. {Input cluster of violated specific policies} are the violated specific policies generated during policies SPECIFICANSWER queries generation. ", "page_idx": 28}, {"type": "image", "img_path": "VZQmIoDGBG/tmp/4fe5295763f2e7e51c24454470ab5aff0d78fc8f671eaac82b9b5c148d614e94.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 22: Prompt template for questions generation for policies DOANSWER queries generation. {In-context examples} are in-context examples. {Input scenario} are generated scenarios without locations described in Table 12 from policies DOANSWER queries. ", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We clearly state them in the introduction and abstract. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: Yes, we have limitation section Appendix E. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: N/A ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Yes, we provide the details in $\\S3,\\S4,\\S5.$ . ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Yes, we provide the details in \u00a73, \u00a74, \u00a75. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Yes, we provide the details in $\\S3$ and $\\S5$ . Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [No] ", "page_idx": 31}, {"type": "text", "text": "Justification: N/A ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Yes, we provide the details in $\\S5$ . ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Yes. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: Yes, we describe it in the introduction and limitation sections. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 32}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 33}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: N/A ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Yes, we cite the original paper of Huggingface Alignment Handbook. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] Justification: N/A ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] Justification: Yes, we discuss it in Appendix A.1. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] Justification: N/A ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]