[{"figure_path": "bioHNTRnQk/figures/figures_1_1.jpg", "caption": "Figure 1: Demystifying model collapse. Refer to Appendix D for details on the experimental setup.", "description": "This figure displays two plots illustrating model collapse in high-dimensional regression with Gaussian data. Plot (a) shows test error evolution for different sample sizes (T), ridge regularization parameters (\u03bb), and training data generations (n) using an isotropic covariance spectrum (\u03a3 = Id). The U-shaped curves highlight an optimal regularization parameter. Plot (b) uses a power-law covariance spectrum, showing test error for adaptive regularization (\u03bb = T\u2212lcrit) and varying sample sizes. The broken lines represent the theoretical results from Theorem 4.1 and Theorem 5.1. Both plots showcase how test error increases linearly with the number of model iterations (n), as predicted by the theory.", "section": "1 Introduction"}, {"figure_path": "bioHNTRnQk/figures/figures_2_1.jpg", "caption": "Figure 1: Demystifying model collapse. Refer to Appendix D for details on the experimental setup.", "description": "This figure shows the evolution of test error for different sample sizes (T), different levels of ridge-regularization (\u03bb), and training data from different generations (n) of fake data for both isotropic and power-law covariance spectrums.  The plots demonstrate the impact of model collapse and show the existence of an optimal regularization parameter (sweet spot) for large values of n.  The broken lines in the plots correspond to the theoretical results from Theorem 4.1 and Theorem 5.1.  Appendix D provides additional experimental details.", "section": "1 Introduction"}, {"figure_path": "bioHNTRnQk/figures/figures_2_2.jpg", "caption": "Figure 1: Demystifying model collapse. Refer to Appendix D for details on the experimental setup.", "description": "This figure shows the impact of model collapse on test error in two different settings. (a) Isotropic covariance spectrum shows how test error evolves with different ridge regularization parameters and sample sizes, illustrating the existence of an optimal regularization parameter.  (b) Power-law covariance spectrum demonstrates test error changes with adaptive regularization. The figures reveal how test error changes with different generations of synthetic data and highlights the effects of model collapse.", "section": "1 Introduction"}, {"figure_path": "bioHNTRnQk/figures/figures_3_1.jpg", "caption": "Figure 3: Illustration of the theoretical framework. The process begins with the original model \u0175o(wo) and the original dataset (X0, Y0). n synthetic data generators w\u2081 to \u0175n are iteratively fit on data labelled by the previous model with label noise \u03c3\u03bf, using To samples each. We evaluate the test error (with respect to the ground truth labels from wo) of wpred, trained on (X, Y) := (Xn, Yn) using T samples with label noise \u03b5 and a regularization coefficient \u03bb.", "description": "This figure illustrates the theoretical framework of the paper.  It shows how the model is trained iteratively on synthetic data generated from previous generations, which introduces noise at each step.  The process starts with original model parameters \u0175o and original data (X0, Y0). Subsequent models (\u01751 to \u0175n) are trained on data synthesized using the previous generation's model with added noise (\u03c30). Finally, a downstream model (wpred) is trained on the nth generation of synthetic data (Xn, Yn) with added noise (\u03c3) and a regularization parameter (\u03bb). The test error of the downstream model (wpred) is then evaluated against the ground truth labels (from \u0175o).", "section": "Theoretical Setup"}, {"figure_path": "bioHNTRnQk/figures/figures_17_1.jpg", "caption": "Figure 4: Model collapse in kernel ridge regression (power-law covariance spectrum) on MNIST. Here, we use adaptive regularization \u03bb = T\u2212l for different values of the exponent l > 0 (see Section D for full experimental setup). Top row: RBF kernel. Bottom row: polynomial kernel. In each plot, we show test error curves as a function of sample size T, from different generations (n) of fake data. The broken vertical line corresponds to T = T0, where T0 is the number of samples (from the true data distribution) which was used to train the label faker. The value of the exponent regularization l = l\u2217 (broken curves) is the optimal value in the presence of iterative data relabeling, while l = lcrit (solid curves) corresponds to the optimal value without iterative re-labelling (i.e n = 0) proposed in Cui et al. [14] (see (26)). Specifically, we take l\u2217 = (b \u2212 a)lcrit = blcrit, where b = log T0/log T (so that T0 = Tb), as proposed in Theorem 5.1, formula (28). Notice how the effect of fake data makes the test error become non decreasing in sample size T. This is effectively a collapse of the learned model.", "description": "This figure shows the impact of training on synthetic data generated iteratively from previous generations. It compares the test error of kernel ridge regression models trained on real MNIST data versus those trained on synthetic data generated by iteratively retraining a model on its own outputs.  The plots illustrate the 'model collapse' phenomenon, where the test error increases (and does not decrease) as the number of synthetic data generations increases.", "section": "Details of Experiments"}, {"figure_path": "bioHNTRnQk/figures/figures_18_1.jpg", "caption": "Figure 1: Demystifying model collapse. Refer to Appendix D for details on the experimental setup.", "description": "This figure demonstrates the evolution of test error as a function of different hyperparameters such as ridge regularization, sample size, and the number of generations of synthetic data. The plots show the test error for isotropic and power-law covariance spectrum scenarios, highlighting the effects of model collapse.", "section": "1 Introduction"}]