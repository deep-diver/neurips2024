[{"figure_path": "LZV0U6UHb6/tables/tables_6_1.jpg", "caption": "Table 1: Quantitative comparisons for part composition on our constructed benchmark. The left part of the table reports the evaluation results on the inner-ID track. The right part estimates the inter-ID track. MimicBrush demonstrates superior performance for each track with the most simplified interaction form. \u201cI\u201d or \u201cT\u201d denotes the image or text similarity.", "description": "This table presents a quantitative comparison of different image editing methods on a benchmark designed for part composition tasks. The benchmark includes two tracks: inner-ID (using images from the same object) and inter-ID (using images from different objects).  The table shows the performance of each method based on SSIM, PSNR, LPIPS scores, and image/text similarity using DINO and CLIP models. MimicBrush outperforms other methods across all metrics on both tracks.", "section": "4.2 Comparisons with Other Works"}, {"figure_path": "LZV0U6UHb6/tables/tables_6_2.jpg", "caption": "Table 2: User study results. We let annotators rank the results of different methods from the best to the worst from three aspects: fidelity, harmony, and quality. We report both the number of the best picks and the average rank for a comprehensive comparison.", "description": "This table presents the results of a user study comparing MimicBrush with other image editing methods.  Ten annotators ranked the results of each method based on three criteria: fidelity (how well the edited region matches the reference), harmony (how well the edited region blends with the surrounding image), and quality (the overall visual quality of the edited image). The table shows the percentage of times each method received the highest ranking for each criterion, as well as the average ranking across all annotators.  This provides a subjective evaluation of the methods' performance, complementing the objective metrics presented elsewhere in the paper.", "section": "4.2 Comparisons with Other Works"}, {"figure_path": "LZV0U6UHb6/tables/tables_7_1.jpg", "caption": "Table 3: Ablation study for different reference feature extractors. U-Net demonstrates consistent advantages across different evaluation tracks and metrics compared with CLIP and DINOv2.", "description": "This table presents the results of an ablation study comparing different methods for extracting features from reference images in the imitative editing task.  The study compares using a U-Net, CLIP encoder, and DINOv2 encoder.  The performance is evaluated across multiple metrics (SSIM, PSNR, LPIPS, DINO-I, CLIP-I, CLIP-T) and two different tasks (Part Composition and Texture Transfer).  The results show that the U-Net consistently outperforms the other methods.", "section": "4.3 Ablation Studies"}, {"figure_path": "LZV0U6UHb6/tables/tables_7_2.jpg", "caption": "Table 4: Ablation study for training strategies. In the first block, we verify the importance of training data and augmentation. In the second block, we explore different strategies for masking the source image. The performance of our full pipeline is given at the bottom.", "description": "This table presents the results of ablation studies on different training strategies for the proposed image editing method.  The first part shows the impact of using only image data versus incorporating video data and augmentations. The second part investigates different masking strategies for the source image, comparing random masking with a method that leverages feature matching to select more informative regions to mask. The bottom row provides the performance of the complete, fully optimized model as a benchmark.", "section": "4.3 Ablation Studies"}]