[{"type": "text", "text": "Localized Adaptive Risk Control ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Matteo Zecchin Osvaldo Simeone Centre for Intelligent Information Processing Systems Department of Engineering King\u2019s College London London, United Kingdom {matteo.1.zecchin,osvaldo.simeone}@kcl.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Adaptive Risk Control (ARC) is an online calibration strategy based on set prediction that offers worst-case deterministic long-term risk control, as well as statistical marginal coverage guarantees. ARC adjusts the size of the prediction set by varying a single scalar threshold based on feedback from past decisions. In this work, we introduce Localized Adaptive Risk Control (L-ARC), an online calibration scheme that targets statistical localized risk guarantees ranging from conditional risk to marginal risk, while preserving the worst-case performance of ARC. L-ARC updates a threshold function within a reproducing kernel Hilbert space (RKHS), with the kernel determining the level of localization of the statistical risk guarantee. The theoretical results highlight a trade-off between localization of the statistical risk and convergence speed to the long-term risk target. Thanks to localization, L-ARC is demonstrated via experiments to produce prediction sets with risk guarantees across different data subpopulations, significantly improving the fairness of the calibrated model for tasks such as image segmentation and beam selection in wireless networks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Adaptive risk control (ARC), also known as online risk control, is a powerful tool for reliable decision-making in online settings where feedback is obtained after each decision [Gibbs and Candes, 2021, Feldman et al., 2022]. ARC finds applications in domains, such as finance, robotics, and health, in which it is important to ensure reliability in forecasting, optimization, or control of complex systems [Wisniewski et al., 2020, Lekeufack et al., 2023, Zhang et al., 2023, Zecchin et al., 2024]. While providing worst-case deterministic guarantees of reliability, ARC may distribute such guarantees unevenly in the input space, favoring a subpopulation of inputs at the detriment of another subpopulation. ", "page_idx": 0}, {"type": "text", "text": "As an example, consider the tumor segmentation task illustrated in Figure 1. In this setting, the objective is to calibrate a pre-trained segmentation model to generate masks that accurately identify tumor areas according to a user-defined reliability level [Yu et al., 2016]. The calibration process typically involves combining data from various datasets, such as those collected from different hospitals. For an online setting, as visualized in the figure, ARC achieves the desired long-term reliability in terms of false negative ratio. However, it does so by prioritizing certain datasets, resulting in unsatisfactory performance on other data sources. Such behavior is particularly dangerous, as it may result in some subpopulations being poorly diagnosed. This paper addresses this shortcoming of ARC by proposing a novel localized variant of ARC. ", "page_idx": 0}, {"type": "image", "img_path": "fogJgrozu1/tmp/3cb5a084f65c493e643b330124a879d827298870ab750ce05978a57e6001faa6.jpg", "img_caption": ["Figure 1: Calibration of a tumor segmentation model via ARC [Angelopoulos et al., 2024a] and the proposed localized ARC, L-ARC. Calibration data comprises images from multiple sources, namely, the Kvasir data set [Jha et al., 2020] and the ETIS-LaribPolypDB data set [Silva et al., 2014]. Both ARC and L-ARC achieve worst-case deterministic long-term risk control in terms of false negative rate (FNR). However, ARC does so by prioritizing Kvasir samples at the detriment of the Larib data source, for which the model has poor FNR performance. In contrast, L-ARC can yield uniformly satisfactory performance for both data subpopulations. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "1.1 Adaptive Risk Control ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "To elaborate, consider an online decision-making scenario in which inputs are provided sequentially to a pre-trained model. At each time step $t\\geq1$ , the model observes a feature vector $X_{t}$ , and based on a bounded non-conformity scoring function $s:\\mathcal{X}\\times\\mathcal{Y}\\to[0,S_{\\operatorname*{max}}]$ and a threshold $\\lambda_{t}\\in\\mathbb{R}$ , it outputs a prediction set ", "page_idx": 1}, {"type": "equation", "text": "$$\nC_{t}=C(X_{t},\\lambda_{t})=\\{y\\in\\mathcal{Y}:s(X_{t},y)\\leq\\lambda_{t}\\},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\boldsymbol{\\wp}$ is the domain of the target variable $Y$ . After each time step $t$ , the model receives feedback in the form of a loss function ", "page_idx": 1}, {"type": "equation", "text": "$$\nL_{t}=\\mathcal{L}(C_{t},Y_{t})\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "that is assumed to be non-negative, upper bounded by $B<\\infty$ and non-increasing in the predicted set size $\\left|C_{t}\\right|$ . A notable example is the miscoverage loss ", "page_idx": 1}, {"type": "equation", "text": "$$\n{\\mathcal{L}}(C,y)=\\mathbb{1}\\{y\\notin C\\}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Accordingly, for an input-output sequence $\\{(X_{t},Y_{t})\\}_{t=1}^{T}$ the performance of the set predictions $\\{C_{t}\\}_{t=1}^{T}$ in (1) can be gauged via the cumulative risk ", "page_idx": 1}, {"type": "equation", "text": "$$\n{\\bar{L}}(T)={\\frac{1}{T}}\\sum_{t=1}^{T}{\\mathcal{L}}(C_{t},Y_{t})={\\frac{1}{T}}\\sum_{t=1}^{T}L_{t}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "For a user-specified loss level $\\alpha$ and a learning rate sequence $\\{\\eta_{t}\\}_{t=1}^{T}$ , ARC updates the threshold $\\lambda_{t}$ in (1) as [Feldman et al., 2022] ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\lambda_{t+1}=\\lambda_{t}+\\eta_{t}(L_{t}-\\alpha),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $L_{t}-\\alpha$ measures the discrepancy between the current loss (2) and the target $\\alpha$ . For step size decreasing as $\\eta_{t}=\\eta_{1}t^{-1/2}$ for $a\\in(0,1)$ and an arbitrary $\\eta_{1}>0$ , the results in [Angelopoulos et al., 2024b] imply that the update rule (5) guarantees that the cumulative risk (4) for the miscoverage loss (3) converges to target level $\\alpha$ for any data sequence $\\{(X_{t},Y_{t})\\}_{t\\ge1}$ as ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\left|\\bar{L}(T)-\\alpha\\right|\\leq\\frac{S_{\\mathrm{max}}+\\eta_{1}B}{\\sqrt{T}},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "thus offering a worst-case deterministic long-term guarantee. Furthermore when data are generated i.i.d. as $(X_{t},Y_{t})\\sim P_{X Y}$ for all $t\\geq1$ , in the special case of the miscoverage loss (3), the set predictor produced by (5) enjoys the asymptotic marginal coverage guarantee ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}\\operatorname*{Pr}\\left[Y\\notin C_{T}\\right]\\overset{p}{=}\\alpha,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where the probability is computed with respect to the test sample $(X,Y)\\sim P_{X Y}$ , which is independent of the sequence of samples $\\{(X_{t},Y_{t})\\}_{t=1}^{T}$ , and the convergence is in probability with respect to the sequence $\\{(X_{t},Y_{t})\\}_{t\\ge1}$ . Note that in [Angelopoulos et al., 2024b], a stronger version of (7) is provided, in which the limit holds almost surely. ", "page_idx": 1}, {"type": "image", "img_path": "fogJgrozu1/tmp/9819b63e510612831c25a6ca93a95863a68eb590558a643d1cdfd5f127ea1e79.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 2: The degree of localization in L-ARC is dictated by the choice of the reweighting function class $\\mathcal{W}$ via the marginal-to-conditional guarantee (9). At the leftmost extreme, we illustrate constant reweighting functions, for which marginal guarantees are recovered. At the rightmost extreme, reweighting with maximal localization given by Dirac delta functions for which the criterion (9) corresponds to a conditional guarantee. In between the two extremes lie function sets $\\mathcal{W}$ with an intermediate level of localization yielding localized guarantees. ", "page_idx": 2}, {"type": "text", "text": "1.2 Conditional and Localized Risk ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The convergence guarantee (7) for ARC is marginalized over the covariate $X$ . Therefore, there is no guarantee that the conditional miscoverage $\\operatorname*{Pr}\\left[Y\\notin C_{T}|X=x\\right]$ is smaller than the target $\\alpha$ . This problem is particularly relevant for high-stakes applications in which it is important to ensure a homogeneous level of reliability across different regions of the input space, such as across subpopulations. That said, even when the set predictor $C(\\bar{X}|\\mathcal D_{\\mathrm{cal}})$ is obtained based on an offline calibration data set $\\mathcal{D}_{\\mathrm{cal}}$ with i.i.d. data $(X,Y)\\sim P_{X Y}$ , it is generally impossible to control the conditional miscoverage probability as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[Y\\not\\in C(X|\\mathcal{D}_{\\mathrm{cal}})|X=x\\right]\\leq\\alpha{\\mathrm{~for~all~}}x\\in\\mathcal{X}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "without making further assumptions about the distribution $P_{X Y}$ or producing uninformative prediction sets [Vovk, 2012, Foygel Barber et al., 2021]. ", "page_idx": 2}, {"type": "text", "text": "A relaxed marginal-to-conditional guarantee was considered by Gibbs et al. [2023], which relaxed the marginal miscoverage requirement (8) as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X,Y,\\mathcal{D}_{\\mathrm{cal}}}\\left[\\frac{w(X)}{\\mathbb{E}_{X}[w(X)]}\\mathbb{1}\\{Y\\notin C(X|\\mathcal{D}_{\\mathrm{cal}})\\}\\right]\\le\\alpha\\;\\mathrm{for~all~}w(\\cdot)\\in\\mathcal{W},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{W}$ is a set of non-negative reweighting functions, and the expectation is taken over the joint distribution of the calibration data $\\mathcal{D}_{\\mathrm{cal}}$ and the test pair $(X,Y)$ . Note that with a singleton set $\\mathcal{W}$ encompassing a single constant function, e.g., $w(x)=1$ , the criterion (9) reduces to marginal coverage. Furthermore, as illustrated in Figure 2, depending on the degree of localization of the functions in set $\\mathcal{W}$ , the criterion (9) interpolates between marginal and conditional guarantees. ", "page_idx": 2}, {"type": "text", "text": "At the one extreme, a marginal guarantee like (7) is recovered when the reweighting functions are constant. Conversely, at the other extreme, conditional guarantees as in (8) emerge when the reweighting functions are maximally localized, i.e., when ${\\dot{\\mathcal{W}}}=\\{w(x)=\\delta(x-\\mu):\\mu\\in\\mathcal{X}\\}$ , where $\\delta(x)$ denotes the Dirac delta function. In between these two extremes, one obtains an intermediate degree of localization. For example, this can be done by considering reweighting functions such as ", "page_idx": 2}, {"type": "equation", "text": "$$\nW=\\left\\{w(x)\\!=\\!\\sum_{i=1}^{\\infty}\\beta_{i}\\left(\\kappa\\exp\\left(-\\frac{\\Vert x-\\mu_{i}\\Vert^{2}}{l}\\right)+1\\right);\\mathbb{E}_{X}[w(X)]>0,\\mathrm{~and~}w(x)\\ge0\\,\\forall x\\in\\mathcal{X}\\right\\},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $l\\geq0$ is a fixed length scale, $\\kappa\\geq0$ is a fixed scaling parameter, and $\\lVert\\cdot\\rVert$ denotes the Euclidean norm. Furthermore, function $w(x)$ may also depend on the output of the pre-trained model, supporting calibration requirements via constraints of the form (9) [Zhang et al., 2024]. ", "page_idx": 2}, {"type": "text", "text": "In Gibbs et al. [2023], the authors demonstrated that it is possible to design offline set predictors $C(X|\\mathcal{D}_{\\mathrm{cal}})$ that approximately control risk (9), with an approximation gap that depends on the degree of localization of the family $\\mathcal{W}$ of weighting functions. ", "page_idx": 2}, {"type": "text", "text": "1.3 Localized Risk Control ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Motivated by the importance of conditional risk guarantees, we propose Localized ARC (L-ARC), a novel online calibration algorithm that produces prediction sets with localized statistical risk control guarantees as in (9), while also retaining the worst-case deterministic long-term guarantees (6) of ARC. Unlike Gibbs et al. [2023], our work focuses on online settings in which calibration is carried out sequentially based on feedback received on past decisions. ", "page_idx": 3}, {"type": "text", "text": "The key technical innovation of L-ARC lies in the way set predictions are constructed. As detailed in Section 2, L-ARC prediction sets replace the single threshold in (1) with a threshold function $g(\\cdot)$ mapping covariate $X$ to a localized threshold value $g(X)$ . The threshold function is adapted in an online fashion within a reproducing kernel Hilbert space (RKHS) family $\\mathcal{G}$ based on an input data stream and loss feedback. The choice of the RKHS family determines the family $\\mathcal{W}$ of weighting functions in the statistical guarantee of the form (9), thus dictating the desired level of localization. ", "page_idx": 3}, {"type": "text", "text": "The main technical results, presented in Section 2.3, are as follows. ", "page_idx": 3}, {"type": "text", "text": "\u2022 In the case of i.i.d. sequences, $(X_{t},Y_{t})\\sim P_{X Y}$ for all $t\\geq1$ , L-ARC provides localized statistical risk guarantees where the reweighting class $\\mathcal{W}$ corresponds to all non-negative functions $w\\in{\\mathcal{G}}$ with a positive mean under distribution $P_{X Y}$ . More precisely, given a target loss value $\\alpha$ , the time-averaged threshold function ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\bar{g}_{T}(\\cdot)=\\frac{1}{T}\\sum_{t=1}^{T}g_{t}(\\cdot),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "ensures that for any function $w\\in\\mathscr{W}$ , the limit ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}\\operatorname{lim}_{\\mathbb{E}_{X,Y}}\\left[{\\frac{w(X)}{\\mathbb{E}_{X}[w(X)]}}{\\mathcal{L}}(C(X,{\\bar{g}}_{T}),Y)\\right]\\leq\\alpha+A({\\mathcal{G}},w)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "holds, where convergence is in probability with respect to the sequence $\\{(X_{t},Y_{t})\\}_{t\\ge1}$ and the average is over the test pair $(X,Y)$ . The gap $A(\\bar{\\boldsymbol{g}},\\boldsymbol{w})$ depends on both the RKHS $\\mathcal{G}$ and function $w$ ; it increases with the level of localization of the functions in the RKHS $\\mathcal{G}$ ; and it equals zero in the case of constant threshold functions, recovering (7) for the special case of the miscoverage loss. ", "page_idx": 3}, {"type": "text", "text": "\u2022 Furthermore, for an arbitrary sequence $\\{(X_{t},Y_{t})\\}_{t\\ge1}$ L-ARC has a cumulative loss that converges to a neighborhood of the nominal reliability level $\\alpha$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left|{\\frac{1}{T}}\\sum_{t=1}^{T}{\\mathcal{L}}(C(X_{t},g_{t}),Y_{t})-\\alpha\\right|\\leq{\\frac{B({\\mathcal{G}})}{\\sqrt{T}}}+C({\\mathcal{G}}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $B(\\mathcal{G})$ and $C(\\mathcal{G})$ are terms that increase with the level of localization of the function in the RKHS $\\mathcal{G}$ . The quantity $C(\\mathcal{G})$ equals zero in the case of constant threshold functions, recovering the guarantee (6) of ARC. ", "page_idx": 3}, {"type": "text", "text": "In Section 3 we showcase the superior conditional risk control properties of L-ARC as compared to ARC for the task of electricity demand forecasting, tumor segmentation, and beam selection in wireless networks. ", "page_idx": 3}, {"type": "text", "text": "2 Localized Adaptive Risk Control ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "2.1 Setting ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Unlike the ARC prediction set (1), L-ARC adopts prediction sets that are defined based on a threshold function $g_{t}:\\mathcal{X}\\to\\mathbb{R}$ . Specifically, at each time $t\\geq1$ the L-ARC prediction set is obtained based on a non-conformity scoring function $s:\\mathcal{X}\\times\\mathcal{Y}\\to\\mathbb{R}$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\nC_{t}=C(X_{t},g_{t}):=\\left\\{y\\in\\mathcal{y}:s(X_{t},y)\\leq g_{t}(X_{t})\\right\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "By (14), the threshold $g_{t}\\big(X_{t}\\big)$ is localized, i.e., it is selected as a function of the current input $X_{t}$ . In this paper, we consider threshold functions of the form ", "page_idx": 3}, {"type": "equation", "text": "$$\ng_{t}(\\cdot)=f_{t}(\\cdot)+c_{t},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $c_{t}\\in\\mathbb{R}$ is a constant and function $f_{t}(\\cdot)$ belongs to a reproducing kernel Hilbert space (RKHS) $\\mathcal{H}$ associated to a kernel $k(\\cdot,\\cdot):\\mathcal{X}\\times\\mathcal{X}\\stackrel{}{\\rightarrow}\\mathbb{R}$ with inner product $\\langle\\cdot,\\cdot\\rangle_{\\mathcal{H}}$ and norm $\\left\\lVert\\cdot\\right\\rVert_{\\mathcal{H}}$ . Note that the threshold function $g_{t}(\\cdot)$ belongs to the RKHS $\\mathcal{G}$ determined by the kernel $k^{\\prime}(\\cdot,\\cdot)=\\\"\\dot{k}(\\cdot,\\cdot)+1$ . ", "page_idx": 4}, {"type": "text", "text": "We focus on the online learning setting, in which at every time set $t\\geq1$ , the model observes an input feature $X_{t}$ , produces a set $C_{t}$ , and receives as feedback the loss $\\boldsymbol{L}_{t}=\\boldsymbol{\\mathcal{L}}(\\boldsymbol{C}_{t},Y_{t})$ . Note that label $Y_{t}$ may not be directly observed, and only the loss $\\textstyle{\\mathcal{L}}(C_{t},Y_{t})$ may be recorded. Based on the observed sequence of features $X_{t}$ and feedback $L_{t}$ , we are interested in producing prediction sets as in (14) that satisfy the reliability guarantees (12) and (13), with a reweighting function set $\\mathcal{W}$ encompassing all non-negative functions $w(\\cdot)\\in\\mathcal{G}$ with a positive mean $\\mathbb{E}_{X}[w(X)]$ under distribution $P_{X}$ , i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{W}=\\{w(\\cdot)\\in\\mathcal{G}:\\ \\mathbb{E}_{X}[w(X)]>0,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\nw(x)\\geq0\\{{\\mathrm{or~all}}\\;x\\in\\mathcal{X}\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Importantly, as detailed below, the level of localization in guarantee (12) depends on the choice of the kernel $k(\\cdot,\\cdot)$ . ", "page_idx": 4}, {"type": "text", "text": "2.2 L-ARC ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Given a regularization parameter $\\lambda>0$ and a learning rate $\\eta_{t}\\leq1/\\lambda$ , L-ARC updates the threshold function $\\bar{g_{t}(\\cdot)}=f_{t}(\\cdot)\\bar{+}\\,c_{t}$ in (14) based on the recursive formulas ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r l}&{c_{t+1}=c_{t}-\\eta_{t}(\\alpha-L_{t})}\\\\ &{f_{t+1}(\\cdot)=(1-\\lambda\\eta_{t})f_{t}(\\cdot)-\\eta_{t}(\\alpha-L_{t})k(X_{t},\\cdot),}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with $f_{1}(\\cdot)=0$ and $c_{1}=0$ . In order to implement the update (17)-(18), it is useful to rewrite the function $g_{t+1}(\\cdot)$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\ng_{t+1}(\\cdot)=\\sum_{i=1}^{t}a_{t+1}^{i}k(X_{i},\\cdot)+c_{t+1},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the coefficients $\\{a_{t+1}^{i}\\}_{i=1}^{t}$ are recursively defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a_{t+1}^{t}=-\\eta_{t}(\\alpha-L_{t})}\\\\ &{a_{t+1}^{i}=(1-\\eta_{t}\\lambda)a_{t}^{i},\\quad\\mathrm{~for~}i=1,2,\\ldots,t-1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Accordingly, if the loss $L_{t}$ is larger than the long-term target $\\alpha$ , the update rule (20)-(21) increases the function $g_{t+1}(\\cdot)$ around the current input $X_{t}$ , while decreasing it around the previous inputs $X_{1},\\ldots,X_{t-1}$ . Intuitively, this change enhances the reliability for inputs in the neighborhood of $X_{t}$ . ", "page_idx": 4}, {"type": "text", "text": "It is important to note that, at any time $t$ , computing the threshold function (19) requires storing the coefficients $\\{a_{t}^{i}\\}_{i=1}^{t-1}$ and $c_{t}$ , as well as the input data $\\{X_{t}\\}_{i=1}^{t}$ . Consequently, L-ARC has a linear memory requirement in $t$ , which is a known limitation of non-parametric learning in online settings [Koppel et al., 2020]. Previous research has explored methods that trade memory efficiency for accuracy [Kivinen et al., 2004]. In Appendix C.3, we build on these approaches to present a memory-efficient variant of L-ARC that allows for a trade-off between localized risk control and memory requirements. ", "page_idx": 4}, {"type": "text", "text": "2.3 Theoretical Guarantees ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we formalize the theoretical guarantees of L-ARC, which were informally stated in Section 1.3 as (12) and (13). ", "page_idx": 4}, {"type": "text", "text": "Assumption 1 (Stationary and bounded kernel). The kernel function is stationary, i.e., $k(x,x^{\\prime})=$ ${\\tilde{k}}(\\|x-x^{\\prime}\\|)$ , for some non-negative function $\\tilde{k}(\\cdot)$ , which is $\\rho$ -Lipschitz for some $\\rho\\,>\\,0$ , upper bounded by $\\kappa<\\infty$ , and coercive, i.e., $\\begin{array}{r}{\\operatorname*{lim}_{z\\to\\infty}\\tilde{k}(z)=0}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "Many well-known stationary kernels, such as the radial basis function (RBF), Cauchy, and triangular kernels, satisfy Assumption 1. The smoothness parameter $\\rho$ and the maximum value of the kernel function $\\kappa$ determine the localization of the threshold function $g_{t}(\\cdot)\\in\\mathcal G$ . For example, the set of functions $\\mathcal{W}$ defined in (10) corresponds to the function class (16) associated with the RKHS defined by the raised RBF kernel $k(x,x^{\\prime})=\\kappa\\exp(-\\left\\|x-x^{\\prime}\\right\\|^{2}/l)+1$ , with length scale $l=2e(\\kappa/\\rho)^{2}$ . As illustrated in Figure 2, by increasing $\\kappa$ and $\\rho$ , we obtain functions with an increasing level of localization, ranging from constant functions to maximally localized functions. ", "page_idx": 4}, {"type": "text", "text": "Assumption 2 (Bounded non-conformity scores). The non-conformity scoring function is nonnegative and bounded, i.e., $s(x,y)\\leq S_{m a x}<\\infty$ for any pair $(x,y)\\in\\mathcal{X}\\times\\mathcal{Y}$ . ", "page_idx": 5}, {"type": "text", "text": "Assumption 3 (Bounded and monotone loss). The loss function is non-negative; bounded, i.e., $\\mathcal{L}(C,\\bar{Y})\\leq B<\\infty$ for any $C\\subseteq\\mathcal{V}$ and $Y\\in\\mathcal{V}$ ; and monotonic, in the sense that for prediction sets $C^{\\prime}$ and $C$ such that $C^{\\prime}\\subseteq C$ , the inequality $\\begin{array}{r}{\\mathcal{L}(C,Y)\\le\\mathcal{L}(C^{\\prime},Y)}\\end{array}$ holds for any $Y\\in\\mathcal{V}$ . ", "page_idx": 5}, {"type": "text", "text": "2.3.1 Statistical Localized Risk Control ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To prove the localized statistical guarantee (12) we will make the following assumption. ", "page_idx": 5}, {"type": "text", "text": "Assumption 4 (Strictly decreasing loss). For any fixed threshold function $g(\\cdot)\\ \\in\\ {\\mathcal{G}}$ , the loss $\\mathbb{E}_{Y}[\\mathcal{L}(C(X,g),Y)|X=x]$ is strictly decreasing in the threshold $g(x)$ for any $x\\in\\mathscr{X}$ . ", "page_idx": 5}, {"type": "text", "text": "Assumption 5 (Left-continuous loss). For any fixed threshold function $g(\\cdot)\\in\\mathcal{G}$ , the loss $\\mathcal{L}(C(x,g+$ $h),y)$ is left-continuous in $h\\in\\mathbb{R}$ for any $(x,y)\\in\\mathcal{X}\\times\\mathcal{Y}$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. Fix a user-defined target reliability $\\alpha$ . For any regularization parameter $\\lambda>0$ and any learning rate sequence $\\eta_{t}^{'}=\\eta_{1}t^{-\\breve{1}/2}<1/\\lambda,$ , for some $\\eta_{1}>0$ , given a sequence $\\{(X_{t},Y_{t})\\}_{t=1}^{T}$ of i.i.d. samples from $P_{X Y}$ , the time-averaged threshold function $(I I)$ satisfies the limit ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}\\operatorname{lim}_{X,Y}\\left[\\frac{w(X)}{\\mathbb{E}_{X}[w(X)]}\\mathcal{L}(C(X,\\bar{g}_{T}),Y)\\right]\\overset{p}{\\leq}\\alpha+\\kappa B\\frac{\\|f_{w}\\|_{\\mathcal{H}}}{\\mathbb{E}_{X}[w(X)]},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for any weighting function $w(\\cdot)=f_{w}(\\cdot)+c_{w}\\in\\mathcal{W}$ where the expectation is with respect to the test sample $(X,Y)$ . ", "page_idx": 5}, {"type": "text", "text": "Proof. See Appendix A. ", "page_idx": 5}, {"type": "text", "text": "By (22), the average localized loss converges in probability to a quantity that can be bounded by the target $\\alpha$ with a gap $A(\\mathcal{G},w)$ that increases with the level of localization $\\kappa$ . ", "page_idx": 5}, {"type": "text", "text": "2.3.2 Worst-Case Deterministic Long-Term Risk Control ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Theorem 2. Fix a user-defined target reliability $\\alpha$ . For any regularization parameter $\\lambda>0$ and any learning rate sequence $\\bar{\\eta_{t}}=\\eta_{1}t^{-\\bar{1}/2}<1/\\lambda$ with $\\eta_{1}>0$ , given any sequence $\\{(X_{t},Y_{t})\\}_{t=1}^{T}\\ w i t h$ bounded input $\\|X_{t}\\|\\leq D<\\infty,$ , L-ARC produces a sequence of threshold functions $\\{g_{t}\\big(\\cdot\\big)\\}_{t=1}^{T}\\;i n$ (19) that satisfy the inequality ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{T}\\sum_{t=1}^{T}\\mathcal{L}(C(X_{t},g_{t}),Y_{t})-\\alpha\\right|\\leq\\frac{1}{\\sqrt{T}}\\left(\\frac{S_{m a x}}{\\eta_{0}}+\\frac{4B\\sqrt{\\rho\\kappa D}}{\\eta_{0}\\lambda}+2B(2\\kappa+1)\\right)+\\kappa B.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proof. We defer the proof to Appendix B. ", "page_idx": 5}, {"type": "text", "text": "Formalizing the upper bound in (13), Theorem 2 states that the difference between the long-term cumulative risk and the target reliability level $\\alpha$ decreases with a rate $B(\\mathcal{G})T^{-1/2}$ to a value $C(\\mathcal{G})=$ $\\kappa B$ that is increasing with the maximum value of the kernel $\\kappa$ . In the special case, $\\kappa=0$ which corresponds to no localization, the right-hand side of (23) vanishes in $T$ , recovering ARC long-term guarantee (6). ", "page_idx": 5}, {"type": "text", "text": "3 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we explore the worst-case long-term and statistical localized risk control performance of L-ARC as compared to ARC. Firstly, we address the task of electricity demand forecasting, utilizing data from the Elec2 dataset [Harries et al., 1999]. Next, we present an experiment focusing on tumor segmentation, where the data comprises i.i.d. samples drawn from various image datasets [Jha et al., 2020, Bernal et al., 2015, 2012, Silva et al., 2014, V\u00e1zquez et al., 2017]. Finally, we study a problem in the domain of communication engineering by focusing on beam selection, a key task in wireless systems [Ali et al., 2017]. A further example concerning applications with calibration constraints can be found in Appendix C.2. Unless stated otherwise, we instantiate L-ARC with the RBF kernel $k(x,x^{\\prime})\\,=\\,\\kappa\\exp(-\\,\\|x-x^{\\prime}\\|^{2}\\,/l)$ with $\\kappa\\,=\\,1$ , length scale $l\\,=\\,1$ and regularization parameter $\\lambda=10^{-4}$ . With a smaller length scale $l$ , we obtain increasingly localized weighting functions. All the experiments are conducted on a consumer-grade Mac Mini with an M1 chip. The simulation code is available at https://github.com/kclip/localized-adaptive-risk-control.git. ", "page_idx": 5}, {"type": "image", "img_path": "fogJgrozu1/tmp/ebcac0b8a0e4b08f2e07e295e857b565c0f53c622f4cd82726798cb7f816ab7e.jpg", "img_caption": ["Figure 3: Long-term coverage (left) and average miscoverage error (right), marginalized and conditioned on weekdays and weekends. for ARC and L-ARC with varying values of the localization parameter $l$ on the Elec2 dataset. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "fogJgrozu1/tmp/1f1a1d3ece065d6666a1ad48c0ff5b4993815eb0559b7ed6584ce2546d8ecc43.jpg", "img_caption": ["Figure 4: Long-term FNR (left), average FNR across different data sources (center), and average mask size across different data sources (right) for ARC and L-ARC with varying values of the localization parameter $l$ for the task of tumor segmentation [Fan et al., 2020]. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "3.1 Electricity Demand ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The Elec2 dataset comprises $T\\,=\\,45312$ hourly recordings of electricity demands in New South Wales, Australia. The data sequence $\\{Y_{t}\\}_{t=1}^{T}$ is subject to distribution shifts due to fluctuations in demand over time, such as between day and night or between weekdays and weekends. We adopt a setup akin to that of Angelopoulos et al. [2024b], wherein the even-time data samples are used for online calibration while odd-time data samples are used to evaluate coverage after calibration. At time $t$ , the observed covariate $X_{t}$ corresponds to the past time series $Y_{1:t-1}$ , and the forecasted electricity demand $\\hat{Y_{t}}$ is obtained based on a moving average computed from demand data collected within the preceding 24 to 48 hours. We produce prediction sets $C_{t}$ based on the non-conformity score $s(X_{t},Y_{t})\\,=\\,|\\hat{Y}_{t}-Y_{t}|$ and we target a miscoverage rate $\\alpha\\:=\\:0.1$ using the miscoverage loss (3). Both ARC and L-ARC use the learning rate $\\eta_{t}=t^{-1/2}$ . L-ARC is instantiated with the RBF kernel $k(x,x^{\\prime})=\\kappa\\exp(-\\left\\|\\phi(x)-\\phi(x^{\\prime})\\right\\|^{2}/l)$ , where $\\phi(x)$ is a 7-dimensional feature vector corresponding to the daily average electricity demand during the past 7 days. ", "page_idx": 6}, {"type": "text", "text": "In the left panel of Figure 3, we report the cumulative miscoverage error of ARC and L-ARC for different values of the localization parameter $l$ . All algorithms converge to the desired coverage level of 0.9 in the long-term. The right panel of Figure 3, displays the average miscoverage error on the hold-out dataset at convergence. We specifically evaluate both the marginalized miscoverage rate and the conditional miscoverage rate separately over weekdays and weekends. L-ARC is shown to reduce the weekend coverage error rate as compared to ARC providing balanced coverage as the length scale $l$ decreases. ", "page_idx": 6}, {"type": "text", "text": "3.2 Tumor Image Segmentation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we focus on the task of calibrating a predictive model for tumor segmentation. Here, the feature vector $X_{t}$ represents a $d_{\\mathrm{H}}\\times d_{\\mathrm{W}}$ image, while the label $Y_{t}\\subseteq\\mathcal{P}$ identifies a subset of the image pixels $\\mathcal{P}=\\{(1,1),\\dots,(d_{\\mathrm{H}},d_{\\mathrm{W}})\\}$ that encompasses the tumor region. As in Angelopoulos et al. [2022], the dataset is a compilation of samples from several open-source online repositories: Kvasir, CVC-300, CVC-ColonDB, CVC-ClinicDB, and ETIS-LaribDB. We reserve 50 samples from each repository for testing the performance post-calibration, while the remaining $T=2098$ samples are used for online calibration. Predicted sets are obtained by applying a threshold $g(X_{t})$ to the pixel-wise logits $f(p_{\\mathrm{H}},p_{\\mathrm{W}})$ generated by the PraNet segmentation model [Fan et al., 2020], with the objective of controlling the false negative ratio (FNR) $\\bar{\\mathcal{L}}(C_{t},Y_{t})=1-\\vert C_{t}\\cap Y_{t}\\vert/\\vert Y_{t}\\vert$ . Both ARC and L-ARC are run using the same decaying learning rate $\\eta_{t}=0.1t^{-1/2}$ . L-ARC is instantiated with the RBF kernel $k(x,x^{\\prime})=\\kappa\\exp(-\\left\\|\\phi(x)-\\phi(x^{\\prime})\\right\\|^{2}/l)$ , where $\\phi(x)$ is a 5-dimensional feature vector obtained via the principal component analysis (PCA) from the last hidden layer of the ResNet model used in PraNet. ", "page_idx": 7}, {"type": "text", "text": "In the leftmost panel of Figure 4, we report the long-term FNR for varying values of the localization parameter $l$ , targeting an FNR level $\\alpha\\,=\\,0.1$ . All methods converge rapidly to the desired FNR level, ensuring long-term risk control. The calibrated models are then tested on the hold-out data, and the FNR and average predicted set size are separately evaluated across different repositories. In the middle and right panels of Figure 4, we report the average FNR and average prediction set size averaged over 10 trials. ", "page_idx": 7}, {"type": "text", "text": "The model calibrated via ARC has a marginalized FNR error larger than the target value $\\alpha$ . Moreover, the FNR error is unevenly distributed across the different data repositories, ranging from $\\mathrm{FNR}=0.08$ for CVC-300 to $\\mathrm{FNR}=0.32\\$ for ETIS-LaribPolypDB. In contrast, L-ARC can equalize performance across repositories, while also achieving a test FNR closer to the target level. In particular, as illustrated in the rightmost panel, L-ARC improves the FNR for the most challenging subpopulation in the data by increasing the associated prediction set size, while maintaining a similar size for subpopulations that already have satisfactory performance. ", "page_idx": 7}, {"type": "image", "img_path": "fogJgrozu1/tmp/dc631fd740cadeb4cb287c41820ace0ddff430d6edf00890863a541270107c1d.jpg", "img_caption": ["3.3 Beam Selection "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 5: Long-term risk (left-top), average beam set size (left-bottom), and SNR level across the deployment area (right) for ARC, Mondrian ARC, and L-ARC. The transmitter is denoted as a green circle and obstacles to propagation are shown as grey rectangles. ", "page_idx": 7}, {"type": "text", "text": "Motivated by the importance of reliable uncertainty quantification in engineering applications, we address the task of selecting location-specific beams for the initial access procedure in sixth-generation wireless networks [Ali et al., 2017]. Further details regarding the engineering aspects of the problem and the simulation scenario are provided in Appendix C.1.1. In the beam selection task, at each time $t$ , the observed covariate corresponds to the location $X_{t}\\,=\\,[p_{\\mathrm{x}},p_{\\mathrm{y}}]$ of a receiver within the network deployment, where $p_{\\mathrm{x}}$ and $p_{\\mathrm{y}}$ represent the geographical coordinates. Based on the observed covariate, the transmitter chooses a set, denoted as $C_{t}\\subseteq[1,\\cdots\\,,B_{\\operatorname*{max}}]$ , consisting of a subset of the $B_{\\mathrm{max}}$ available communication beams. ", "page_idx": 7}, {"type": "text", "text": "Each communication beam $i$ is associated with a wireless link characterized by a signal-to-noise ratio $Y_{t,i}$ , which follows an unknown distribution depending on the user\u2019s location $X_{t}$ . We represent the vector of signal-to-noise ratios as $Y_{t}=[Y_{t,1},\\ldots,Y_{t,{B_{\\operatorname*{max}}}}]\\in\\mathbb{R}^{B_{\\operatorname*{max}}}$ . For a set $C_{t}$ , the transmitter sweeps over the beam set $C_{t}$ , and the performance is measured by the ratio between the SNR obtained on the best beam in set $C_{t}$ and the best SNR on all the beams, i.e., ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathcal{L}(C_{t},Y_{t})=L_{t}=1-\\frac{\\operatorname*{max}_{i\\in\\mathcal{C}_{t}}Y_{t,i}}{\\operatorname*{max}_{i\\in\\{1,...,B_{\\operatorname*{max}}\\}}Y_{t,i}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Given an SNR predictor $\\hat{Y}_{t}=f_{\\mathrm{SNR}}(X_{t})$ for all beams at location $X_{t}$ , we consider sets that include only beams with a predicted SNR exceeding a threshold $g_{t}(X_{t})$ as ", "page_idx": 8}, {"type": "equation", "text": "$$\nC(X_{t},g_{t})=\\{i\\in[1,\\ldots,B_{\\operatorname*{max}}]:\\hat{Y}_{t,i}>g_{t}(X_{t})\\}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "In this setting, localization refers to the fair provision of service across the entire deployment area. As a benchmark, we thus also consider an additional calibration strategy that divides the deployment area into two regions: one encompassing all locations near the transmitter, which are more likely to experience high SNR levels, and the other including locations far from the transmitter. For each of these regions, we run two separate instances of ARC algorithms. Inspired by the method introduced in Bostr\u00f6m et al. [2021] for offline settings, we refer to this baseline approach as Mondrian ARC. ", "page_idx": 8}, {"type": "text", "text": "In the left panels of Figure 5, we compare the performance of ARC, Mondrian ARC, and L-ARC with an RBF kernel with $l\\,=\\,10$ , using a calibration data sequence of length $T\\,=\\,25000$ . All methods achieve the target long-term SNR regret, but L-ARC achieves this result while selecting sets with smaller sizes, thus requiring less time for beam sweeping. Additionally, as illustrated on the right panel, thanks to the localization of the threshold function, L-ARC ensures a satisfactory communication SNR level across the entire deployment area. In contrast, both ARC and Mondrian ARC produce beam-sweep sets with uneven guarantees over the network deployment area. ", "page_idx": 8}, {"type": "text", "text": "4 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our work contributes to the field of adaptive conformal prediction (CP), originally introduced by Gibbs and Candes [2021]. Adaptive CP extends traditional CP [Vovk et al., 2005] to online settings, where data is non-exchangeable and may be affected by distribution shifts. This extension has found applications in reliable time-series forecasting [Xu and Xie, 2021, Zaffran et al., 2022], control [Lekeufack et al., 2023, Angelopoulos et al., 2024a], and optimization [Zhang et al., 2023, Deshpande et al., 2024]. Adaptive CP ensures that prediction sets generated by the algorithm contain the response variable with a user-defined coverage level on average across the entire time horizon. Recently, Bhatnagar et al. [2023] proposed a variant of adaptive CP based on strongly adaptive online learning, providing coverage guarantees for any subsequence of the data stream. While their approach offers localized guarantees in time, L-ARC provides localized guarantees in the covariate space. More similar to our work is [Bastani et al., 2022], which studies group-conditional coverage. Our work extends beyond coverage guarantees to a more general risk definition, akin to Feldman et al. [2022]. Angelopoulos et al. [2024a] studied the asymptotic coverage properties of adaptive conformal predictions in the i.i.d. setting; and our work extends these results to encompass covariate shifts. Finally, the guarantee provided by L-ARC is similar to that of Gibbs et al. [2023], albeit for an offilne conformal prediction setting. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion and Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We have presented and analyzed L-ARC, a variant of adaptive risk control that produces prediction sets based on a threshold function mapping covariate information to localized threshold values. L-ARC can guarantee both worst-case deterministic long-term risk control and statistical localized risk control. Empirical analysis demonstrates L-ARC\u2019s ability to effectively control risk for different tasks while providing prediction sets that exhibit consistent performance across various data subpopulations. The effectiveness of L-ARC is contingent upon selecting an appropriate kernel function. Furthermore, L-ARC has memory requirements that grow with time due to the need to store the input data $\\{X_{t}\\}_{t>1}$ and coefficients (20)-(21). These limitations of L-ARC motivate future work aimed at optimizing online the kernel function based on hold-out data [Kiyani et al., 2024] or in an online manner [Angelopoulos et al., 2024a], and at studying the statistical guarantees of memory-efficient variants of L-ARC [Kivinen et al., 2004]. ", "page_idx": 8}, {"type": "text", "text": "6 Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the European Union\u2019s Horizon Europe project CENTRIC (101096379). The work of Osvaldo Simeone was also supported by the Open Fellowships of the EPSRC (EP/W024101/1) by the EPSRC project (EP/X011852/1), and by Project REASON, a UK Government funded project under the Future Open Networks Research Challenge (FONRC) sponsored by the Department of Science Innovation and Technology (DSIT). We would also like to express our gratitude to Anastasios Angelopoulos for valuable insights on the technical content of the paper. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Anum Ali, Nuria Gonz\u00e1lez-Prelcic, and Robert W Heath. Millimeter wave beam-selection using outof-band spatial information. IEEE Transactions on Wireless Communications, 17(2):1038\u20131052, 2017.   \nAnastasios Angelopoulos, Emmanuel Candes, and Ryan J Tibshirani. Conformal PID control for time series prediction. Advances in Neural Information Processing Systems, 36, 2024a.   \nAnastasios N Angelopoulos, Stephen Bates, Adam Fisch, Lihua Lei, and Tal Schuster. Conformal risk control. arXiv preprint arXiv:2208.02814, 2022.   \nAnastasios N Angelopoulos, Rina Foygel Barber, and Stephen Bates. Online conformal prediction with decaying step sizes. arXiv preprint arXiv:2402.01139, 2024b.   \nOsbert Bastani, Varun Gupta, Christopher Jung, Georgy Noarov, Ramya Ramalingam, and Aaron Roth. Practical adversarial multivalid conformal prediction. Advances in Neural Information Processing Systems, 35:29362\u201329373, 2022.   \nJorge Bernal, Javier S\u00e1nchez, and Fernando Vilarino. Towards automatic polyp detection with a polyp appearance model. Pattern Recognition, 45(9):3166\u20133182, 2012.   \nJorge Bernal, F Javier S\u00e1nchez, Gloria Fern\u00e1ndez-Esparrach, Debora Gil, Cristina Rodr\u00edguez, and Fernando Vilari\u00f1o. WM-DOVA maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians. Computerized medical imaging and graphics, 43:99\u2013111, 2015.   \nAadyot Bhatnagar, Huan Wang, Caiming Xiong, and Yu Bai. Improved online conformal prediction via strongly adaptive online learning. In International Conference on Machine Learning, pages 2337\u20132363. PMLR, 2023.   \nHenrik Bostr\u00f6m, Ulf Johansson, and Tuwe L\u00f6fstr\u00f6m. Mondrian conformal predictive distributions. In Conformal and Probabilistic Prediction and Applications, pages 24\u201338. PMLR, 2021.   \nNicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line learning algorithms. IEEE Transactions on Information Theory, 50(9):2050\u20132057, 2004.   \nShachi Deshpande, Charles Marx, and Volodymyr Kuleshov. Online calibrated and conformal prediction improves bayesian optimization. In International Conference on Artificial Intelligence and Statistics, pages 1450\u20131458. PMLR, 2024.   \nDeng-Ping Fan, Ge-Peng Ji, Tao Zhou, Geng Chen, Huazhu Fu, Jianbing Shen, and Ling Shao. Pranet: Parallel reverse attention network for polyp segmentation. In International conference on medical image computing and computer-assisted intervention, pages 263\u2013273. Springer, 2020.   \nShai Feldman, Liran Ringel, Stephen Bates, and Yaniv Romano. Achieving risk control in online learning settings. arXiv preprint arXiv:2205.09095, 2022.   \nRina Foygel Barber, Emmanuel J Candes, Aaditya Ramdas, and Ryan J Tibshirani. The limits of distribution-free conditional predictive inference. Information and Inference: A Journal of the IMA, 10(2):455\u2013482, 2021.   \nIsaac Gibbs and Emmanuel Candes. Adaptive conformal inference under distribution shift. Advances in Neural Information Processing Systems, 34:1660\u20131672, 2021. ", "page_idx": 9}, {"type": "text", "text": "Isaac Gibbs, John J Cherian, and Emmanuel J Cand\u00e8s. Conformal prediction with conditional guarantees. arXiv preprint arXiv:2305.12616, 2023. ", "page_idx": 10}, {"type": "text", "text": "Andrea Goldsmith. Wireless communications. Cambridge university press, 2005. ", "page_idx": 10}, {"type": "text", "text": "Michael Harries, New South Wales, et al. Splice-2 comparative evaluation: Electricity pricing. 1999. ", "page_idx": 10}, {"type": "text", "text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \nJakob Hoydis, Fay\u00e7al A\u00eft Aoudia, Sebastian Cammerer, Merlin Nimier-David, Nikolaus Binder, Guillermo Marcus, and Alexander Keller. Sionna RT: Differentiable ray tracing for radio propagation modeling. arXiv preprint arXiv:2303.11103, 2023.   \nDebesh Jha, Pia H Smedsrud, Michael A Riegler, P\u00e5l Halvorsen, Thomas De Lange, Dag Johansen, and H\u00e5vard D Johansen. Kvasir-seg: A segmented polyp dataset. In MultiMedia Modeling: 26th International Conference, MMM 2020, Daejeon, South Korea, January 5\u20138, 2020, Proceedings, Part II 26, pages 451\u2013462. Springer, 2020.   \nJyrki Kivinen, Alexander J Smola, and Robert C Williamson. Online learning with kernels. IEEE transactions on signal processing, 52(8):2165\u20132176, 2004.   \nShayan Kiyani, George Pappas, and Hamed Hassani. Conformal prediction with learned features. arXiv preprint arXiv:2404.17487, 2024.   \nAlec Koppel, Amrit Singh Bedi, Ketan Rajawat, and Brian M Sadler. Optimally compressed nonparametric online learning: Tradeoffs between memory and consistency. IEEE Signal Processing Magazine, 37(3):61\u201370, 2020.   \nJordan Lekeufack, Anastasios A Angelopoulos, Andrea Bajcsy, Michael I Jordan, and Jitendra Malik. Conformal decision theory: Safe autonomous decisions from imperfect predictions. arXiv preprint arXiv:2310.05921, 2023.   \nHorea Muresan and Mihai Oltean. Fruit recognition from images using deep learning. Acta Universitatis Sapientiae, Informatica, 10(1):26\u201342, 2018.   \nJuan Silva, Aymeric Histace, Olivier Romain, Xavier Dray, and Bertrand Granado. Toward embedded detection of polyps in wce images for early diagnosis of colorectal cancer. International journal of computer assisted radiology and surgery, 9:283\u2013293, 2014.   \nDavid V\u00e1zquez, Jorge Bernal, F Javier S\u00e1nchez, Gloria Fern\u00e1ndez-Esparrach, Antonio M L\u00f3pez, Adriana Romero, Michal Drozdzal, and Aaron Courville. A benchmark for endoluminal scene segmentation of colonoscopy images. Journal of healthcare engineering, 2017, 2017.   \nVladimir Vovk. Conditional validity of inductive conformal predictors. In Asian conference on machine learning, pages 475\u2013490. PMLR, 2012.   \nVladimir Vovk, Alexander Gammerman, and Glenn Shafer. Algorithmic learning in a random world, volume 29. Springer, 2005.   \nWojciech Wisniewski, David Lindsay, and Sian Lindsay. Application of conformal prediction interval estimations to market makers\u2019 net positions. In Conformal and probabilistic prediction and applications, pages 285\u2013301. PMLR, 2020.   \nChen Xu and Yao Xie. Conformal prediction interval for dynamic time-series. In International Conference on Machine Learning, pages 11559\u201311569. PMLR, 2021.   \nLequan Yu, Hao Chen, Qi Dou, Jing Qin, and Pheng Ann Heng. Integrating online and offline three-dimensional deep learning for automated polyp detection in colonoscopy videos. IEEE journal of biomedical and health informatics, 21(1):65\u201375, 2016.   \nMargaux Zaffran, Olivier F\u00e9ron, Yannig Goude, Julie Josse, and Aymeric Dieuleveut. Adaptive conformal predictions for time series. In International Conference on Machine Learning, pages 25834\u201325866. PMLR, 2022. ", "page_idx": 10}, {"type": "text", "text": "Matteo Zecchin, Sangwoo Park, and Osvaldo Simeone. Forking uncertainties: Reliable prediction and model predictive control with sequence models via conformal risk control. IEEE Journal on Selected Areas in Information Theory, 2024. Lujing Zhang, Aaron Roth, and Linjun Zhang. Fair risk control: A generalized framework for calibrating multi-group fairness risks. arXiv preprint arXiv:2405.02225, 2024. Yunchuan Zhang, Sangwoo Park, and Osvaldo Simeone. Bayesian optimization with formal safety guarantees via online conformal prediction. arXiv preprint arXiv:2306.17815, 2023. ", "page_idx": 11}, {"type": "text", "text": "A Proof of Theorem 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We are interested in bounding the localized risk in (22) of the threshold function (11) for all weighting functions in set $\\mathcal{W}$ defined in (16). To study the limit in (22) we first note that L-ARC update rule (18) corresponds to an online gradient descent step for a loss function $\\ell(g,x,y)$ , with respect to function $f(\\cdot)$ and constant $c$ in function $g(\\cdot)=f(\\cdot)+c$ as in (15). In particular, interpreting the update rule (17)-(18) as a gradient descent step, we obtain that the partial derivatives of the loss function $\\ell(g,x,y)$ evaluated at $\\check{g(\\cdot)}=f(\\cdot)+c$ are ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{f}\\ell(g)=\\displaystyle\\frac{\\partial\\ell(g,x,y)}{\\partial f}(\\cdot)=(\\alpha-\\mathcal{L}(C(x,g),y))k(x,\\cdot)+\\lambda f(\\cdot)\\in\\mathcal{H},}\\\\ &{\\quad\\nabla_{c}\\ell(g)=\\displaystyle\\frac{\\partial\\ell(g,x,y)}{\\partial c}=(\\alpha-\\mathcal{L}(C(x,g),y))\\in\\mathbb{R},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "so that the first order approximation of the loss $\\ell(g,x,y)$ around $g(\\cdot)$ is given by ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell(g+\\epsilon\\delta_{f},x,y)\\approx\\ell(g,x,y)+\\epsilon(\\alpha-\\mathcal{L}(C(x,g),y))\\langle K_{x},\\delta_{f}\\rangle+\\epsilon\\langle f,\\delta_{f}\\rangle}\\\\ &{\\ell(g+\\epsilon\\delta_{c},x,y)\\approx\\ell(g,x,y)+\\epsilon(\\alpha-\\mathcal{L}(C(x,g),y))\\delta_{c}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "In order to study the convexity of the loss $\\ell(g,x,y)$ in $g(\\cdot)$ , we compute the the derivatives of (26)- (27) with respect to $f(\\cdot)$ and $c$ . The derivative of (26) with respect to $f$ is the operator $A:\\mathcal{H}\\to\\mathcal{H}$ satisfying ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A\\delta_{f}=\\underset{\\epsilon\\rightarrow0}{\\operatorname*{lim}}\\frac{\\nabla_{f}\\ell\\left(g+\\epsilon\\delta_{f}\\right)-\\nabla_{f}\\ell\\left(g\\right)}{\\epsilon}}\\\\ &{\\quad=-\\underset{\\epsilon\\rightarrow0}{\\operatorname*{lim}}\\frac{\\mathcal{L}\\left(C\\left(x,g\\right),y\\right)-\\mathcal{L}\\left(C\\left(x,g+\\epsilon\\delta_{f}\\right),y\\right)}{\\epsilon}K_{x}+\\lambda\\delta_{f}}\\\\ &{\\quad=-\\Big\\langle\\frac{\\partial\\mathcal{L}\\left(C\\left(x,g\\right),y\\right)}{\\partial g(x)}\\frac{\\partial g(x)}{\\partial f},\\delta_{f}\\Big\\rangle K_{x}+\\lambda\\delta_{f}}\\\\ &{\\quad=-\\frac{\\partial\\mathcal{L}\\left(C\\left(x,g\\right),y\\right)}{\\partial g(x)}\\langle K_{x},\\delta_{f}\\rangle K_{x}+\\lambda\\delta_{f}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "It follows that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left\\langle f,A f\\right\\rangle=-\\frac{\\partial\\mathcal{L}(C(x,g),y)}{\\partial g(x)}f(x)^{2}+\\lambda\\left\\|f\\right\\|_{\\mathcal{H}}^{2}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Similarly, the derivative of (26) with respect to $c$ is the operator $B:\\mathbb{R}\\rightarrow\\mathcal{H}$ is given by ", "page_idx": 12}, {"type": "equation", "text": "$$\nB c=-\\frac{\\partial\\mathcal{L}(C(x,g),y)}{\\partial g(x)}K_{x}c,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "which satisfies ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\langle f,B c\\rangle=-{\\frac{\\partial{\\mathcal{L}}(C(x,g),y)}{\\partial g(x)}}f(x)c.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The derivative of (27) with respect to $c$ is given by ", "page_idx": 12}, {"type": "equation", "text": "$$\nD c=-\\frac{\\partial\\mathcal{L}(C(x,g),y)}{\\partial g(x)}c,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and the derivative with respect to to $f$ is the operator $C:\\mathcal{H}\\rightarrow\\mathcal{R}$ given by ", "page_idx": 12}, {"type": "equation", "text": "$$\nC\\delta_{f}=-\\frac{\\partial\\mathcal{L}(C(x,g),y)}{\\partial g(x)}\\langle K_{x},\\delta_{f}\\rangle,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "so that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\langle c,C f\\rangle=-{\\frac{\\partial{\\mathcal{L}}(C(x,g),y)}{\\partial g(x)}}f(x)c.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "From Assumption 4, the inequality \u2202L(C\u2202(gx(,xg)),Y )|X = x \u2265\u03b3 > 0 holds. Thus, the second-order term of the approximation of $\\mathbb{E}_{Y}\\left[\\ell\\bar{(}g,X,Y)|X=x\\right]$ around $g(\\cdot)\\neq0$ satisfies ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{Y}\\left[\\left[f}&{c\\right]\\left[A\\right.}&{B\\right]\\left[f\\right]\\Bigg|X=x\\Bigg]=\\mathbb{E}_{Y}\\left[\\left\\langle f,A f\\right\\rangle+\\left\\langle f,B c\\right\\rangle+\\left\\langle c,C f\\right\\rangle+\\left\\langle c,D c\\right\\rangle\\middle|X=x\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=L^{\\prime}f(x)^{2}+2L^{\\prime}f(x)c+L^{\\prime}c^{2}+\\lambda\\left\\|f\\right\\|_{\\mathcal{H}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=L^{\\prime}(f(x)+c)^{2}+\\lambda\\left\\|f\\right\\|_{\\mathcal{H}}^{2}>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We then conclude that the loss function $\\mathbb{E}_{Y}\\left[\\ell(g,X,Y)|X=x\\right]$ is strongly convex in $g(\\cdot)$ , and that the population loss minimizer ", "page_idx": 13}, {"type": "equation", "text": "$$\ng^{*}(\\cdot)=f^{*}(\\cdot)+c^{*}=\\underset{g\\in\\mathcal{G}}{\\arg\\operatorname*{min}}\\,\\mathbb{E}_{X,Y}\\left[\\ell(g,X,Y)\\right]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "is unique. For any covariate shift $w(\\cdot)\\in\\mathcal{W}$ denote its components $f_{w}(\\cdot)\\in\\mathcal{H}$ and $c_{w}\\in\\mathbb{R}$ such that $w(\\cdot)\\bar{=}f_{w}(\\cdot)+c_{w}$ . From the first order optimality conditions, it holds that the directional derivatives with respect to $f_{w}(\\cdot)$ and $c_{w}$ must satisfy ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{X,Y}\\left[\\nabla_{\\epsilon}\\ell(g^{*}+\\epsilon f_{w},X,Y)|_{\\epsilon=0}\\right]=\\!\\mathbb{E}_{X,Y}\\left[(\\alpha-\\mathcal{L}(C_{t}(X,g^{*}),Y))f_{w}(X)+\\lambda\\langle f_{w},f^{*}\\rangle_{\\mathcal{H}}\\right]=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{X,Y}\\left[\\nabla_{\\epsilon}\\ell(g^{*}+\\epsilon c_{w},X,Y)|_{\\epsilon=0}\\right]=\\!\\mathbb{E}_{X,Y}\\left[(\\alpha-\\mathcal{L}(C_{t}(X,g^{*}),Y))c_{w}\\right]=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which implies that for the optimal solution $g^{\\ast}(\\cdot)$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X,Y}\\left[\\frac{w(X)}{\\mathbb{E}_{X}[w(X)]}\\mathcal{L}(C(X,g^{*}),Y)\\right]=\\alpha+\\lambda\\left\\langle f^{*},\\frac{f_{w}}{\\mathbb{E}_{X}[w(X)]}\\right\\rangle_{\\mathcal{H}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Equality (41) amounts to a localized risk control guarantee for the threshold $g^{\\ast}(\\cdot)$ for covariate shift in $w(\\cdot)\\in\\mathcal{W}$ . The following lemma states that the time-average L-ARC threshold function $\\bar{g}_{T}(\\cdot)$ defined in (11) converges to the population risk minimizer $g^{\\ast}(\\cdot)$ . ", "page_idx": 13}, {"type": "text", "text": "Lemma 1. For any regularization parameter $\\lambda>0$ and any learning rate sequence $\\eta_{t}=\\eta_{1}t^{-1/2}<$ $1/\\lambda,$ , for some $\\eta_{1}>0$ , given a sequence $\\{(X_{t},Y_{t})\\}_{t=1}^{T}$ of i.i.d. samples from $P_{X Y}$ , the time-averaged threshold function $(I I)$ satisfies for any $\\epsilon>0$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}\\operatorname*{Pr}[\\|g^{*}-\\bar{g}_{T}\\|_{\\infty}\\geq\\epsilon]=0\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. To prove convergence in probability, we need to show that the loss function $\\ell(g,X,Y)$ is bounded. To this end, we first show that $\\ell(g,X,Y)$ is Lipschitz in $g(\\cdot)$ by studying the norm of the derivatives (26)-(27). For $g_{t}(\\cdot)=f_{t}(\\cdot)+c_{t}$ returned by the update rule (18), the gradient with respect to $f_{t}(\\cdot)$ satisfies ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\frac{\\partial\\ell(g_{t},x,y)}{\\partial f}(\\cdot)\\right\\|_{\\mathcal{H}}=\\|(\\alpha-\\mathscr{L}(C(x,g_{t}),y)k(x,\\cdot)+\\lambda f_{t}(\\cdot)\\|_{\\mathcal{H}}}\\\\ {\\leq B\\sqrt{\\kappa}+\\lambda\\,\\|f_{t}(\\cdot)\\|_{\\mathcal{H}}\\leq2B\\sqrt{\\kappa},~~~~~~~~~~~~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the first inequality follows from the boundedness on the kernel (Assumption 1) and the boundedness on the loss (Assumption 3), while the last follows from Proposition 1. The gradient with respect to $c$ can be similarly bounded as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left|\\frac{\\partial\\ell(g_{t},x,y)}{\\partial c}(\\cdot)\\right|=|(\\alpha-\\mathcal{L}(C(x,g_{t}),y)|\\leq B.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "From the mean value theorem it follows that for $g(\\cdot)=f(\\cdot)+c$ and $g^{\\prime}(\\cdot)=f^{\\prime}(\\cdot)+c^{\\prime}$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n|\\ell(g,X,Y)-\\ell(g^{\\prime},X,Y)|\\leq\\left(2B\\sqrt{k}+B\\right)\\|f-f^{\\prime}\\|_{\\mathcal{H}}+B|(c-c^{\\prime})|.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since L-ARC returns functions $f_{t}(\\cdot)$ with bounded RKHS norm and infinity norm (Proposition 1), and thresholds function $g_{t}(\\cdot)$ with bounded in infinity norm (Proposition 3), we conclude that there exists a finite $\\ell_{m a x}<\\infty$ such that $|\\ell(g,X,Y)|\\leq\\ell_{m a x}$ . Given that the loss is bounded we can apply ", "page_idx": 13}, {"type": "text", "text": "[Kivinen et al., 2004, Theorem 4] and obtain that for the threshold $\\{g_{t}(\\cdot)\\}_{t\\geq1}$ returned by L-ARC and the population loss minimizer $g^{\\ast}(\\cdot)$ it holds ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=1}^{T}\\ell(g_{t},X_{t},Y_{t})\\le\\frac{1}{T}\\sum_{t=1}^{T}\\ell(g^{*},X_{t},Y_{t})+B^{2}\\kappa^{2}(2\\kappa^{2}+1)^{2}\\left(\\frac{2}{\\sqrt{T}}\\left(2\\eta_{0}+\\frac{1}{\\eta_{0}\\lambda^{2}}\\right)+\\frac{1}{2\\eta_{0}\\lambda^{2}T}\\right)\\!.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By Hoeffding\u2019s inequality the empirical average on the right-hand side of (46) converges to its expected value. Formally, we have that with probability at least $1-\\delta$ with respect to the sequence $\\{(\\dot{X}_{t},Y_{t})\\}_{t=1}^{T}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{T}\\sum_{t=1}^{T}\\ell(g^{*},X_{t},Y_{t})-\\mathbb{E}_{X,Y}[\\ell(g^{*},X,Y)]\\right|\\leq\\ell_{m a x}\\sqrt{\\frac{2}{T}\\log\\left(\\frac{1}{\\delta}\\right)}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Similarly, by [Cesa-Bianchi et al., 2004, Theorem 2] the empirical risk on the left-hand side of (46) converges to the population risk of the time-averaged solution (11). With probability at least $1-\\delta$ with respect to the sequence of samples $\\{(X_{t},Y_{t})\\bar{\\}_{t=1}^{T}$ , it holds ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X,Y}\\left[\\ell(\\bar{g}_{T},X,Y)\\right]\\leq\\frac{1}{T}\\sum_{t=1}^{T}\\ell(g_{t},X_{t},Y_{t})+\\ell_{m a x}\\sqrt{\\frac{2}{T}\\log\\left(\\frac{1}{\\delta}\\right)}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Combining the two inequalities, with probability at least $1-2\\delta$ with respect to $\\{(X_{t},Y_{t})\\}_{t=1}^{T}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{X,Y}\\left[\\ell(\\bar{g}_{T},X,Y)-\\ell(g^{*},X,Y)\\right]\\le B^{2}\\kappa^{2}(2\\kappa^{2}+1)^{2}\\left(\\frac{2}{\\sqrt{T}}\\left(2\\eta_{0}+\\frac{1}{\\eta_{0}\\lambda^{2}}\\right)+\\frac{1}{2\\eta_{0}\\lambda^{2}T}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\ 2\\ell_{m a x}\\sqrt{\\frac{2}{T}\\log\\left(\\frac{1}{\\delta}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since the $\\mathbb{E}_{X,Y}[\\ell(g,X,Y)]$ is strongly convex there exists a value $\\gamma>0$ such that the second order approximation of $\\mathbb{E}_{X,Y}[\\ell(g,X,Y)]$ at $g^{\\ast}(\\cdot)$ satisfies ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\gamma}{2}\\left(\\left\\|f^{*}-\\bar{f}_{T}\\right\\|_{\\mathcal{H}}+(c^{*}-\\bar{c}_{T})\\right)^{2}\\leq\\mathbb{E}_{X,Y}[\\ell(\\bar{g}_{T},X,Y)-\\ell(g^{*},X,Y)]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Combining (50) and (49), and leveraging $\\|f\\|_{\\infty}\\leq{\\sqrt{\\kappa}}\\left\\|f\\right\\|_{\\mathcal{H}}$ , which follows from the Assumption 1, we conclude that with probability $1-2\\delta$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n|g^{*}-\\bar{g}_{T}||_{\\infty}\\leq\\sqrt{\\frac{2B^{2}\\kappa^{2}(2\\kappa^{2}+1)^{2}}{\\gamma(\\kappa+1)}\\bigg(\\frac{2}{\\sqrt{T}}\\bigg(2\\eta_{0}+\\frac{1}{\\eta_{0}\\lambda^{2}}\\bigg)+\\frac{1}{2\\eta_{0}\\lambda^{2}T}\\bigg)+\\frac{4\\ell_{m a x}}{\\gamma(\\kappa+1)}\\sqrt{\\frac{2}{T}\\log\\bigg(\\frac{1}{\\delta}\\bigg)}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Choosing $\\begin{array}{r}{\\delta=\\frac{1}{T}}\\end{array}$ , for any $\\epsilon>0$ , it holds ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}}\\operatorname*{Pr}[\\left\\|g^{*}-\\bar{g}_{T}\\right\\|_{\\infty}\\geq\\epsilon]=0.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By itself, the convergence of the threshold function $\\bar{g}_{T}(\\cdot)$ to the population risk minimizer $g^{\\ast}(\\cdot)$ is not sufficient to provide localized risk control guarantees for L-ARC time-averaged solution. However, under the additional loss regularity assumption in Assumption 5, we can show that set predictor $C(X,{\\bar{g}}_{T})$ enjoys conditional risk control for $T\\to\\infty$ . ", "page_idx": 14}, {"type": "text", "text": "Having assumed that the loss $\\mathcal{L}(C(x,g),y)$ is left-continuous and decreasing for larger prediction sets (Assumption 3 and 5), for any $\\delta^{\\prime}>0$ there exists $\\epsilon>0$ such that for $g(\\cdot)$ such that $\\|g^{*}-g\\|_{\\infty}\\leq\\epsilon$ it holds ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}\\left(C\\left(X,g\\right),Y\\right)\\leq\\mathcal{L}\\left(C\\left(X,g^{*}\\right),Y\\right)+\\delta^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For such $g(\\cdot)$ the following inequality holds ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{w\\in\\mathcal{W}}\\mathbb{E}\\left[\\frac{w(X)}{\\mathbb{E}[w(X)]}\\mathcal{L}\\left(C\\left(X,g\\right),Y\\right)\\right]\\leq\\operatorname*{max}_{w\\in\\mathcal{W}}\\mathbb{E}\\left[\\frac{w(X)}{\\mathbb{E}[w(X)]}\\mathcal{L}\\left(C\\left(X,g^{*}\\right),Y\\right)\\right]+\\delta^{\\prime}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "As stated in Lemma 1, we can always find $T$ large enough, such that $\\|g^{*}-\\bar{g}_{T}\\|_{\\infty}\\leq\\epsilon$ with arbitrary large probability. This implies, that for any $\\delta^{\\prime}>0$ and $w\\in\\mathscr{W}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}}\\mathbb{E}\\left[\\frac{w\\left(X\\right)}{\\mathbb{E}\\left[w\\left(X\\right)\\right]}\\mathcal{L}\\left(C\\left(X,\\bar{g}_{T}\\right),Y\\right)\\right]\\leq\\mathbb{E}\\left[\\frac{w\\left(X\\right)}{\\mathbb{E}\\left[w\\left(X\\right)\\right]}\\mathcal{L}\\left(C\\left(X,g^{*}\\right),Y\\right)\\right]+\\delta^{\\prime}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\alpha+\\lambda\\left\\langle f^{*},\\frac{f_{w}}{\\mathbb{E}_{X}\\left[w\\left(X\\right)\\right]}\\right\\rangle_{\\mathcal{H}}+\\delta^{\\prime}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\alpha+\\kappa B\\frac{\\left\\lVert f_{w}\\right\\rVert_{\\mathcal{H}}}{\\mathbb{E}_{X}\\left[w\\left(X\\right)\\right]}+\\delta^{\\prime},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the inequality (55) follows from (54), the inequality (56) follows from (41) and the inequality (57) from Proposition 1. ", "page_idx": 15}, {"type": "text", "text": "B Proof of Theorem 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We are interested in bounding the absolute difference between the cumulative loss value incurred by the set predictors $\\{C(g_{t},X_{t})\\}_{t=1}^{T}$ produced by L-ARC (18) and the target reliability level $\\alpha$ , i.e., ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{T}\\sum_{t=1}^{T}(\\underbrace{\\mathcal{L}(C_{t},Y_{t})}_{L_{t}}-\\alpha)\\right|.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "From Assumption 1 and having assumed $\\|X_{t}\\|\\leq D$ for $t\\geq1$ , it follows that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\|x\\|\\to\\infty}k(X_{t},x)=0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A bound on the cumulative risk can then be obtained by bounding ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{T}\\sum_{t=1}^{T}(L_{t}-\\alpha)(k(X_{t},\\cdot)+1)\\right\\|_{\\infty},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where for a function $f:\\mathcal{X}\\to\\mathbb{R}$ , the infinity norm $\\|f\\|_{\\infty}$ is defined as $\\operatorname*{max}_{x\\in\\mathcal{X}}|f(x)|$ . In fact, from (60) we directly obtain a bound on the cumulative risk ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\frac{1}{T}}\\sum_{t=1}^{T}(L_{t}-\\alpha){\\Bigg|}=\\operatorname*{lim}_{\\|x\\|\\rightarrow\\infty}\\left|{\\frac{1}{T}}\\sum_{t=1}^{T}(L_{t}-\\alpha)(k(X_{t},x)+1)\\right|\\leq\\left\\|{\\frac{1}{T}}\\sum_{t=1}^{T}(L_{t}-\\alpha)(k(X_{t},\\cdot)+1)\\right\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To this end, we first note that functions $\\{f_{t}(\\cdot)\\}_{t\\in\\mathbb{N}}$ generated by (18) have bounded RKHS norm and are smooth. ", "page_idx": 15}, {"type": "text", "text": "Proposition 1. For every $t\\geq1$ , we have the inequalities $\\begin{array}{r}{\\|f_{t}\\|_{\\mathcal H}\\leq\\frac{B\\sqrt{\\kappa}}{\\lambda}}\\end{array}$ and $\\begin{array}{r}{\\|f_{t}\\|_{\\infty}\\leq\\frac{\\kappa B}{\\lambda}}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "Proof. The proof is by induction, with the base case $\\|f_{1}(\\cdot)\\|_{\\mathcal{H}}\\leq B\\sqrt{\\kappa}/\\lambda$ being satisfied as $f_{1}(\\cdot)=0$ . The induction step is given as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|f_{t+1}\\|_{\\mathcal H}=\\|(1-\\lambda\\eta_{t})f_{t}-\\eta_{t}(\\alpha-L_{t})k(x_{t},\\cdot)\\|_{\\mathcal H}}&{}\\\\ {\\quad\\leq\\|(1-\\lambda\\eta_{t})f_{t}\\|_{\\mathcal H}+\\|\\eta_{t}(\\alpha-L_{t})k(x_{t},\\cdot)\\|_{\\mathcal H}}&{}\\\\ {\\quad\\leq(1-\\lambda\\eta_{t})\\,\\|f_{t}\\|_{\\mathcal H}+\\eta_{t}B\\sqrt{\\kappa}}&{}\\\\ {\\quad\\leq\\frac{B\\sqrt{\\kappa}}{\\lambda},}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the equality (62) follows from the update rule (18); the inequality (63) from the properties of the norm, the inequality\u221a (64) from Assumption 1 and 3, and the inequality (65) from the induction hypothesis \u2225ft\u2225H \u2264B\u03bb \u03ba \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Proposition 2. For $t\\geq1$ and any $(x,x^{\\prime})\\in\\mathcal{X}\\times\\mathcal{X}$ we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n|f(x)-f(x^{\\prime})|\\leq\\frac{B\\sqrt{2\\rho\\kappa D}}{\\lambda}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Denote the evaluation function at $x$ as $K_{x}=k(x,\\cdot)$ . From Proposition 1 and the Lipschitz continuity assumed in Assumption 1, it follows that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{f(x)-f(y)|=|\\langle f,K_{x}\\rangle_{\\mathcal{H}}-\\langle f,K_{y}\\rangle_{\\mathcal{H}}|}}\\\\ &{=|\\langle f,K_{x}-K_{y}\\rangle_{\\mathcal{H}}|}\\\\ &{\\le\\|f\\|_{\\mathcal{H}}\\,\\|K_{x}-K_{y}\\|_{\\mathcal{H}}}\\\\ &{=\\|f\\|_{\\mathcal{H}}\\,\\sqrt{k(x,x)+k(y,y)-2k(x,y)}}\\\\ &{\\le\\|f\\|_{\\mathcal{H}}\\,\\sqrt{2\\rho\\,\\|x-y\\|}}\\\\ &{\\le\\frac{2B\\sqrt{\\rho\\kappa D}}{\\lambda}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the first inequality follows from Cauchy\u2013Schwarz inequality, the second from the Lipschitz continuity of the kernel, and the last one from Proposition 1 together with $\\|x\\|\\leq D$ for $x\\in\\mathscr{X}$ . ", "page_idx": 16}, {"type": "text", "text": "Leveraging the above characterization of the function $f_{t}(\\cdot)$ returned by L-ARC, we now show that the threshold function $g_{t}(\\cdot)$ has maximum and minimum values that are uniformly bounded. ", "page_idx": 16}, {"type": "text", "text": "Proposition 3. For every $t\\geq1$ and $x\\in\\mathscr{X}$ we have $g_{t}(x)\\in[G_{m i n},G_{m a x}]\\;w i t h$ ", "page_idx": 16}, {"type": "equation", "text": "$$\nG_{m a x}=S_{m a x}+\\frac{2B\\sqrt{\\rho\\kappa D}}{\\lambda}+\\eta_{0}B(2\\kappa+1)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and ", "page_idx": 16}, {"type": "equation", "text": "$$\nG_{m i n}=-\\frac{2B\\sqrt{\\rho\\kappa D}}{\\lambda}-\\eta_{0}B(2\\kappa+1).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. We now prove the upper bound (68). The proof is by contradiction and it start by assuming that there exists a $t>1$ and $x\\in\\mathscr{X}$ such that $g_{t}(x)\\geq G_{\\mathrm{max}}$ while $g_{t^{\\prime}}(\\cdot)<G_{\\mathrm{max}}$ for all $t^{i}<t$ . From the update rule (18) we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{t-1}(x)=g_{t}(x)+\\eta_{t-1}(\\alpha-L_{t})(k(X_{t-1},x)+1)-\\lambda\\eta_{t-1}f_{t-1}(x)}\\\\ &{\\qquad\\qquad\\geq G_{\\operatorname*{max}}-\\eta_{0}B(\\kappa+1)-\\lambda\\eta_{0}|f_{t-1}(x)|}\\\\ &{\\qquad\\qquad\\geq G_{\\operatorname*{max}}-\\eta_{0}B(2\\kappa+1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "From Proposition 2 we also have ", "page_idx": 16}, {"type": "equation", "text": "$$\ng_{t-1}(X_{t-1})\\geq g_{t-1}(x)-\\frac{2B\\sqrt{\\rho\\kappa D}}{\\lambda}\\geq G_{\\operatorname*{max}}-\\eta_{0}B(2\\kappa+1)-\\frac{2B\\sqrt{\\rho\\kappa D}}{\\lambda}\\geq S_{\\operatorname*{max}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last inequality follows from $G_{\\mathrm{max}}$ being defined as (68). From Assumption 2, for all $x\\in\\mathscr{X}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\ng_{t-1}(X_{t-1})\\geq S_{\\operatorname*{max}}\\implies\\alpha\\geq L_{t-1}\\implies g_{t}(x)\\leq(1-\\lambda\\eta_{t-1})g_{t-1}(x)\\leq G_{\\operatorname*{max}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which contradicts with the original assumption that there exists $x$ such that $g_{t}(x)\\geq G_{\\mathrm{max}}$ . ", "page_idx": 16}, {"type": "text", "text": "The proof of the lower bound (69) follows similarly. Assume there exists $t>1$ and $x\\in\\mathscr{X}$ such that $g_{t}(x)\\leq G_{\\operatorname*{min}}$ while $g_{t^{\\prime}}(\\cdot)>G_{\\mathrm{min}}$ for $t^{\\prime}<t$ . From the update rule (18) we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{t-1}(x)=g_{t}(x)+\\eta_{t-1}(\\alpha-L_{t})(k(X_{t-1},x)+1)-\\lambda\\eta_{t-1}f_{t-1}(x)}\\\\ &{\\qquad\\qquad\\leq G_{\\operatorname*{min}}+\\eta_{0}B(\\kappa+1)+\\lambda\\eta_{0}|f_{t-1}(x)|}\\\\ &{\\qquad\\qquad\\leq G_{\\operatorname*{min}}+\\eta_{0}B(2\\kappa+1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "From Proposition 2 we also have ", "page_idx": 16}, {"type": "equation", "text": "$$\ng_{t-1}(X_{t-1})\\leq g_{t-1}(x)+\\frac{2B\\sqrt{\\rho\\kappa D}}{\\lambda}\\leq G_{\\operatorname*{min}}+\\eta_{0}B(2\\kappa+1)+\\frac{2B\\sqrt{\\rho\\kappa D}}{\\lambda}\\leq0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last inequality follows from $G_{\\mathrm{min}}$ being defined as (69). From Assumption 2, for all $x\\in\\mathscr{X}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\ng_{t-1}(X_{t-1})\\leq0\\implies L_{t-1}\\geq\\alpha\\implies g_{t}(x)\\geq(1-\\lambda\\eta_{t-1})g_{t-1}(x)\\geq G_{\\operatorname*{min}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which contradicts the assumption that there exists $\\mathrm{min}_{x\\in\\mathcal{X}}\\,g_{t}(x)\\leq G_{\\mathrm{min}}$ . ", "page_idx": 16}, {"type": "text", "text": "Having established an upper and lower bound on the maximum value of the function $g_{t}(\\cdot)$ generated by (18) we can now bound (60). Define $\\Delta_{t}=\\eta_{t}^{-1}-\\eta_{t-1}^{-1}$ and $\\Delta_{1}=\\eta_{1}^{-1}$ and note that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{1}{T}\\sum_{i=1}^{T}(L_{t}-\\alpha)\\Bigg|\\leq\\underset{\\sigma^{\\star}\\in T}{\\operatorname*{sup}}\\Bigg[\\frac{1}{T}\\sum_{i=1}^{T}(L_{t}-\\alpha)(k(X_{t},x_{t})+1)\\Bigg]}\\\\ &{\\quad=\\left\\lVert\\frac{1}{T}\\sum_{i=1}^{T}\\left(\\sum_{s=1}^{T}\\alpha_{s}\\Big)\\right\\rVert\\eta(L_{t}-\\alpha)(k(X_{t},x_{t})+1)\\right\\rVert_{s}}\\\\ &{\\quad=\\left\\lVert\\frac{1}{T}\\sum_{i=1}^{T}\\sum_{s=1}^{T}\\left(\\sum_{u=0}^{T}(L_{u}-\\alpha)(k(X_{t},x_{t})+1)\\right)\\right\\rVert_{s}}\\\\ &{\\quad=\\left\\lVert\\frac{1}{T}\\sum_{i=1}^{T}\\left(\\sum_{s=1}^{T}\\left(\\sum_{u=1}^{T}\\alpha_{u,u}+1-(1-\\alpha)\\beta_{t}\\right)/t_{*}-\\alpha\\right)\\right\\rVert_{s}}\\\\ &{\\quad=\\left\\lVert\\frac{1}{T}\\sum_{i=1}^{T}\\Delta_{t}\\left(p\\ r+\\beta_{u}+\\beta_{u}\\sum_{u=1}^{T}\\beta_{u,u}\\right)\\right\\rVert_{s}}\\\\ &{\\quad\\leq\\left\\lVert\\frac{1}{T}\\sum_{i=1}^{T}\\left(\\sum_{s=1}^{T}\\theta(r+1-\\theta_{u})\\right)_{s}+\\left\\lVert\\frac{1}{T}\\sum_{i=1}^{T}\\sum_{s=1}^{T}\\frac{\\gamma_{s}}{\\mu\\tau^{\\star}}\\right\\rVert_{s}}\\\\ &{\\quad\\leq\\frac{1}{T}\\sum_{i=1}^{T}\\left(\\sum_{s=1}^{T}-\\theta\\Big)_{s}+\\frac{\\Delta}{T}\\sum_{i=1}^{T}\\left\\lVert\\theta(R_{1},\\theta_{u})\\right\\rVert_{s}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The first term can be bounded based on Proposition (3) as ", "page_idx": 17}, {"type": "equation", "text": "$$\nE_{1}\\leq\\frac{1}{T}\\operatorname*{max}_{r}\\|g_{T+1}-g_{r}\\|_{\\infty}\\sum_{r=1}^{T}\\Delta_{r}=\\frac{1}{\\eta_{T}T}\\left(S_{\\mathrm{max}}+\\frac{4B\\sqrt{\\rho\\kappa D}}{\\lambda}+2\\eta_{0}B(2\\kappa+1)\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and similarly, for the second term, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\nE_{2}\\leq\\frac{\\lambda}{T}\\sum_{t=1}^{T}\\frac{\\kappa B}{\\lambda}=\\kappa B.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Fix a decreasing learning rate $\\eta_{t}=\\eta_{0}t^{-\\omega}$ and a regularization parameter $\\lambda=\\lambda_{0}T^{-\\xi}$ , then the $E_{1}$ becomes ", "page_idx": 17}, {"type": "equation", "text": "$$\nE_{1}=\\frac{S_{\\mathrm{max}}}{\\eta_{0}T^{1-\\omega}}+\\frac{4B\\sqrt{\\rho\\kappa D}}{\\eta_{0}\\lambda T^{1-\\omega}}+\\frac{2B(2\\kappa+1)}{T^{1-\\omega}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For any $\\omega<1$ , it follows ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}\\left|\\frac{1}{T}\\sum_{t=1}^{T}(L_{t}-\\alpha)\\right|=\\kappa B.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "C Additional Experiments ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1 Beam Selection ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1.1 Simulation Details ", "text_level": 1, "page_idx": 18}, {"type": "image", "img_path": "fogJgrozu1/tmp/4ea7e66ecd091f15d7ffa720f3c47d79afec89ab7edb24e108276344d09a3ca7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 6: Network deployment assumed in the simulations. A single transmitter (green circle) communicates with receivers that are uniformly distributed in a scene containing multiple buildings (grey rectangles). ", "page_idx": 18}, {"type": "text", "text": "For the beam selection experiment, we consider the network deployment depicted in Figure 6, in which a transmitter (green circle) communicates with users in an urban environment with multiple buildings (grey rectangles). We assume that communication occurs at a frequency $f_{c}=2.14\\:\\mathrm{GHz}$ and that the transmitter is equipped with $N_{t}=8$ transmitting antennas while receiving users have singleantenna equipment. The transmitter adopts a discrete Fourier transform beamforming codebook of size $B_{\\mathrm{max}}=11$ , with each beam $b_{i}$ given by ", "page_idx": 18}, {"type": "equation", "text": "$$\nb_{i}=\\frac{1}{\\sqrt{N_{t}}}[1,e^{j2\\pi\\frac{2\\pi i}{B_{\\operatorname*{max}}}},\\cdot\\cdot\\cdot,e^{j(N_{t}-1)\\frac{2\\pi i}{B_{\\operatorname*{max}}}}]\\in\\mathbb{C}^{N_{t}},\\quad\\mathrm{for~}i\\in\\{0,\\cdot\\cdot\\cdot,B_{\\operatorname*{max}}-1\\},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $j=\\sqrt{-1}$ . The wireless channel response $h^{R}\\in\\mathbb{C}^{N_{t}}$ between the transmitter and a receiver located at $X_{t}=[p_{\\mathrm{x}},p_{\\mathrm{y}}]\\in\\mathbb{R}^{2}$ , is modeled using Sionna ray-tracer [Hoydis et al., 2023], and we account for small scale fading using a Rayleigh noise model [Goldsmith, 2005]. The resulting channel vector is distributed as ", "page_idx": 18}, {"type": "equation", "text": "$$\nh_{t}\\sim h^{R}(X_{t})+R a y l e i g h(\\sigma).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $h^{R}(X_{t})$ is the ray tracer output and $R a y l e i g h(\\sigma)$ is a Rayleigh distributed random variable with parameter $\\sigma=10^{\\circ}{}^{-4}$ . Assuming unit power transmit symbols and receiver noise, for a channel vector $h_{t}$ the communication signal-to-noise ratio (SNR) obtained using the beamformer $b_{i}$ is given by ", "page_idx": 18}, {"type": "equation", "text": "$$\nY_{t,i}=h_{t}^{\\mathrm{T}}b_{i}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Beam sets are obtained calibrating an SNR predictor $\\hat{Y}_{t}=f_{\\mathrm{SNR}}(X_{t})$ realized using a 3-layer fully connected neural network that is trained on 2500 samples with the user location generated uniformly at random within the deployment area. ", "page_idx": 18}, {"type": "text", "text": "C.1.2 Effect of the Length Scale ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Figure 7, we study the effect of the length scale $l$ of the kernel function on the time-averaged threshold function $\\bar{g}_{T}(X)$ returned by L-ARC. We report the value of the L-ARC time-averaged threshold, $\\bar{g}_{T}(X)$ , in (11), for the same experimental set-up as in Section 3.3, and for increasing localization of the kernel function. As the length scale parameter $l$ decreases, corresponding to a more localized kernel, the value of the threshold is allowed to vary more across the deployment area. In particular, the threshold function reduces its value around areas where the beam selection problem becomes more challenging, such as building clusters, in order to create larger beam selection sets. ", "page_idx": 18}, {"type": "image", "img_path": "fogJgrozu1/tmp/3e6525415badac8806225f2eabad78676ef088ad92cbded09f157ababc9c70ae.jpg", "img_caption": ["Figure 7: Time-averaged threshold function $\\bar{g}_{T}$ for different values of localization parameter $l$ . "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "fogJgrozu1/tmp/b4f94ceba9962b40af9b9260d6b5875f862eb0647b7eacb018c4241674473d28.jpg", "img_caption": ["Figure 8: Long-term coverage (left), coverage rate (center), and prediction set size (right) versus model\u2019s confidence for ARC and L-ARC for different values of the localization parameter $l$ . "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "C.2 Image Classification with Calibration Requirements ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we consider an image classification task under calibration requirements based on the fruit-360 dataset [Muresan and Oltean, 2018]. For this problem, the feature vector $X_{t}$ is an image of size $100\\times100$ , and the corresponding label $Y_{t}\\in\\mathcal{y}\\bar{=}\\{1,\\ldots,130\\}$ is one of 130 types of fruit, vegetable, or nut in image $X_{t}$ . We study the online calibration of a pre-trained ResNet18 model [He et al., 2016]. For an input image $X_{t}$ , the prediction set is obtained from the model\u2019s predictive distribution $\\hat{p}(y|X_{t})$ as ", "page_idx": 19}, {"type": "equation", "text": "$$\nC(X_{t},g_{t})=\\{y\\in\\mathcal{Y}:\\hat{p}(y|X_{t})>g_{t}(X_{t})\\},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and we target the miscoverage loss (3) with a target miscoverage rate $\\alpha=0.25$ . In order to capture calibration requirements, we impose coverage constraints that are localized in the model\u2019s confidence. The model\u2019s confidence indicator is given by the maximum value of the model\u2019s predictive distribution $\\hat{p}(y|X_{t})$ , i.e., ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname{Conf}(X_{t})=\\operatorname*{max}_{y\\in\\mathcal{Y}}\\hat{p}(y|X_{t}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Accordingly, we run ARC and L-ARC calibration with a sequence of $T=8000$ samples and we instantiate L-ARC using the exponential kernel $k(x,x^{\\prime})=\\kappa\\exp(-\\left\\|\\phi(x)-\\phi(x^{\\prime})\\right\\|^{2}/l)$ , where the feature vector is given by the model\u2019s uncertainty, i.e., $\\phi(x)=\\operatorname{Conf}(x)$ . ", "page_idx": 19}, {"type": "text", "text": "In the left-most panel of Figure 8 we report the long-term coverage of ARC and L-ARC for an increasing level of localization obtained by decreasing the length scale $l$ . All methods guarantee long-term coverage. In the middle panel, we use hold-out data to evaluate the coverage of the calibrated model conditioned on the model\u2019s confidence level. For small length scale $l$ , L-ARC yields prediction sets that satisfy the coverage requirement across different levels of the model\u2019s confidence. In contrast, ARC, due to its inability to adapt the threshold function, has a large miscoverage rate for small model confidence levels. As illustrated in the right panel, this is achieved by producing a larger set size when the model\u2019s confidence is low. ", "page_idx": 19}, {"type": "image", "img_path": "fogJgrozu1/tmp/c38fc450db209bb512a1f668a7c39fbe92b0a6232a0eda2d7b462d609183edda.jpg", "img_caption": ["Figure 9: FNR obtained by ARC, L-ARC, and L-ARC with limited memory budget $M_{\\mathrm{max}}~\\in$ $\\{500,1000,1500\\}$ . As the memory budget increases, the localized risk control performance of L-ARC interpolates between ARC and L-ARC. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "fogJgrozu1/tmp/d96eb45980393383044230e9e3fdd5127a3b15aac42b30fd65d364412c3a9743.jpg", "img_caption": ["Figure 10: SNR across the deployment attained by L-ARC with limited memory budget $M_{\\mathrm{max}}$ . "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "C.3 On the memory efficiency of L-ARC ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In a manner similar to [Kivinen et al., 2004], it is possible to obtain a memory-efficient version of L-ARC that adopts a truncated version of L-ARC threshold (19) given by ", "page_idx": 20}, {"type": "equation", "text": "$$\ng_{t+1}(\\cdot)=\\sum_{i=\\operatorname*{max}\\{1,t-M_{\\operatorname*{max}}\\}}^{t}a_{t+1}^{i}k(X_{i},\\cdot)+c_{t+1}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Unlike the threshold (19), which has a linear memory requirement, the truncated version (86) requires a constant memory and computational load that are proportional to the number of coefficients $M_{\\mathrm{max}}$ . It is known that in online non-parametric learning, there exists a trade-off between memory efficiency and performance. In the following, we empirically study the trade-off between the localized risk control of L-ARC and its memory requirements by varying the parameteR $M_{\\mathrm{max}}$ . ", "page_idx": 20}, {"type": "text", "text": "C.3.1 Tumor Segmentation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Using the setup described in Section 3.2, we now consider calibrating the image segmentation model using L-ARC with a truncated threshold (86). In Figure 9, we report the average FNR conditioned on different data sources for $M_{\\mathrm{max}}\\in\\{500,1000,1500\\}$ . As a benchmark, we also compare against ARC and L-ARC without truncation. By adjusting the value of $M_{\\mathrm{max}}$ , it is possible to trade off localized risk control for memory efficiency. In fact, the effect of truncation on L-ARC\u2019s performance is minimal when the number of coefficients in the truncation is large $M_{\\mathrm{max}}=1500)$ ). However, for greater memory savings ( $M_{\\mathrm{max}}=500\\}$ ), L-ARC\u2019s performance becomes similar to that of ARC. In all cases, L-ARC provides better localized risk control than ARC. ", "page_idx": 20}, {"type": "text", "text": "C.3.2 Beam Selection ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We consider the beam selection problem discussed in Section 3.3. In Figure 10, we report the SNR levels across the deployment attained by ARC, L-ARC, and L-ARC with a truncated threshold with a maximum number of coefficients $M_{\\mathrm{max}}\\in\\{500,1000\\}$ . As the number of coefficients $M_{\\mathrm{max}}$ and the memory requirement reduce, the localized risk control performance of L-ARC also decreases. Nonetheless, even for small $M_{\\mathrm{max}}$ , L-ARC delivers a more consistent SNR level across the deployment compared to ARC. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The theoretical claims about the proposed calibration algorithm are supported by Theorem 1 and Theorem 2, while experimental results in Section 3 demonstrate its capability to control long-term risk and to improve fairness across data subpopulations. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: In Section 5, we highlight two primary limitations of L-ARC: its memory requirements and the necessity of specifying a suitable kernel. Additionally, we suggest potential directions for future research to address these issues. A memory-efficient version of L-ARC is given in the Appendix. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Both theorems are preceded by a necessary set of assumptions. In the proofs, provided in the appendix, we reference these assumptions when using them. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in the appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: In the main text, we provide details of all the experiments, including factors influencing the proposed solution, such as datasets and algorithm parameters like the choice of kernel, localization parameters, and learning rate. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 22}, {"type": "text", "text": "some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: In the supplementary material, we include the source code of the experiments along with a concise guide containing all the necessary information to replicate the results. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: All the information, such as the type of datasets, the learning rate, the type of kernel function, and localization parameters, is reported in the main text. In the appendix, we provide additional details about the data generation for the beam selection experiment. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Where possible we report $95\\%$ confidence intervals in the form of error bars or shaded areas. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We indicate that the experiments are run on a Mac Mini. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We carefully read and comply with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper provides a new calibration scheme that offers long-term risk control and statistical localized risk guarantees. We do not see any negative societal impacts associated with the proposed algorithm. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This work does not pose such risks. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: All datasets and models used in the experiments are properly credited by citing the corresponding papers. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: We do not release new assets. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: We do not use crowdsourcing or human subjects. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: We do not use crowdsourcing or human subjects. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]