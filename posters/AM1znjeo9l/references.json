{"references": [{"fullname_first_author": "John C Baez", "paper_title": "A noether theorem for markov processes", "publication_date": "2013-01-01", "reason": "This paper provides a theoretical foundation for understanding the relationship between symmetries and the behavior of stochastic systems, which is central to the analysis of SGD in this paper."}, {"fullname_first_author": "Rapha\u00ebl Berthier", "paper_title": "Incremental learning in diagonal linear networks", "publication_date": "2023-01-01", "reason": "This paper offers insights into the behavior of SGD in linear networks, providing a foundation for studying the effects of SGD in deeper and more complex networks."}, {"fullname_first_author": "Pratik Chaudhari", "paper_title": "Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks", "publication_date": "2018-01-01", "reason": "This paper connects SGD to variational inference and explores the convergence properties of SGD, both of which are central to the analysis and implications in this paper."}, {"fullname_first_author": "Jeremy M Cohen", "paper_title": "Gradient descent on neural networks typically occurs at the edge of stability", "publication_date": "2021-01-01", "reason": "This paper discusses stability issues and implicit biases in SGD, offering insights into the dynamics and behavior that are relevant to this paper's analysis."}, {"fullname_first_author": "Simon S Du", "paper_title": "Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced", "publication_date": "2018-01-01", "reason": "This paper addresses the phenomenon of algorithmic regularization in deep learning, explaining how model architecture can influence the behavior of SGD, offering relevant insights for the results of this paper."}]}