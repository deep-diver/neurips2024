[{"figure_path": "DKSI3bULiZ/figures/figures_3_1.jpg", "caption": "Figure 1: Finetuning a model pretrained on large amounts of advection and diffusion data outperforms models trained from scratch on advection-diffusion data across a wide range of data availability (16-100K examples).", "description": "The figure shows a comparison of the test root mean square error (RMSE) for models trained from scratch, models pretrained on advection and diffusion data and then finetuned on advection-diffusion data, and models trained only on advection-diffusion data.  It demonstrates that pretraining on advection and diffusion improves the accuracy of models trained on advection-diffusion, even when the amount of training data is limited.  This supports the hypothesis that learning partially overlapping physics is beneficial for transfer learning.", "section": "4 Scalable Multiple Physics Pretraining"}, {"figure_path": "DKSI3bULiZ/figures/figures_3_2.jpg", "caption": "Figure 1: Finetuning a model pretrained on large amounts of advection and diffusion data outperforms models trained from scratch on advection-diffusion data across a wide range of data availability (16-100K examples).", "description": "This figure demonstrates the effectiveness of the Multiple Physics Pretraining (MPP) approach.  It shows a comparison of three different training methods: training from scratch, training with a pretrained model that only saw advection and diffusion data separately, and a zero-shot approach using the MPP pretrained model. The y-axis represents the test root mean squared error (RMSE), and the x-axis shows the number of training samples used. The results clearly indicate that the MPP pretrained model significantly outperforms the other methods, especially when the number of training samples is limited.  Even with limited data, the MPP-pretrained model performs comparably to models trained with much larger datasets.", "section": "4 Scalable Multiple Physics Pretraining"}, {"figure_path": "DKSI3bULiZ/figures/figures_4_1.jpg", "caption": "Figure 2: (Left) MPP works by individually normalizing each example using Reversible Instance Normalization (RevIN) then embedding each field individually into a shared, normalized space. A single transformer backbone can then predict the next step for multiple sets of physics. We use an AViT backbone which attends over space and time axis sequentially. Spatial attention is further split by axis, though these share linear projection weights. (Right) The embedding and reconstruction matrices are formed by subsampling a larger 1 \u00d7 1 convolutional filter based on input fields.", "description": "This figure illustrates the Multiple Physics Pretraining (MPP) architecture.  The left side shows the data processing pipeline: individual fields from multiple physical systems are normalized using Reversible Instance Normalization (RevIN), embedded into a shared space, and processed by a spatiotemporal transformer (Axial ViT). The transformer predicts the next time step for all systems simultaneously. The right side details the field embedding process: each field is projected into a shared embedding space using a 1x1 convolutional filter.  The weights of this filter are shared across systems, allowing the model to learn generalizable features.", "section": "4.2 Architecture"}, {"figure_path": "DKSI3bULiZ/figures/figures_7_1.jpg", "caption": "Figure 3: NRMSE for transfer learning tasks. Solid lines are one-step error. Dashed lines are averaged error over five step rollouts. The MPP model shows clear performance benefits in both cases. The more turbulent behavior of \u201cfar\u201d seems to be difficult to learn from scratch or from video data, but pretraining on physical data leads to much stronger results.", "description": "The figure displays the Normalized Root Mean Squared Error (NRMSE) for two transfer learning tasks, \"Near\" and \"Far\", comparing the performance of MPP-trained models, models trained from scratch, and VideoMAE models pretrained on two different video datasets (K400 and SSV2).  Both one-step error and error averaged over five steps are shown.  The results indicate that MPP pretraining significantly improves prediction accuracy, especially for the more complex \"Far\" task, even with limited training data. The results demonstrate the beneficial effect of MPP in transferring knowledge to new physical systems.", "section": "5.2 Transfer to Low-data Domains"}, {"figure_path": "DKSI3bULiZ/figures/figures_7_2.jpg", "caption": "Figure 4: Kinetic energy for incompressible pretraining and compressible finetuning examples. The \"near\" compressible snapshot resembles the pretraining snapshot while \"far\" displays new turbulent small scales.", "description": "This figure compares kinetic energy snapshots for incompressible pretraining data and compressible finetuning data in two scenarios: \"near\" and \"far\" transfer.  The \"near\" transfer shows similar patterns between pretraining and finetuning, while \"far\" transfer exhibits significantly different, more turbulent small-scale features in the finetuned data, demonstrating the model's ability to learn and transfer knowledge to new physical behavior.", "section": "5.2 Transfer to Low-data Domains"}, {"figure_path": "DKSI3bULiZ/figures/figures_25_1.jpg", "caption": "Figure 3: NRMSE for transfer learning tasks. Solid lines are one-step error. Dashed lines are averaged error over five step rollouts. The MPP model shows clear performance benefits in both cases. The more turbulent behavior of \u201cfar\u201d seems to be difficult to learn from scratch or from video data, but pretraining on physical data leads to much stronger results.", "description": "This figure compares the performance of MPP-pretrained models against models trained from scratch and video foundation models on transfer learning tasks. The tasks involve predicting the dynamics of previously unseen systems, both similar (\"near\") and dissimilar (\"far\") to the systems used in pretraining. The results demonstrate that the MPP model significantly outperforms the others, especially for the more complex, \"far\", transfer task. The solid lines represent the one-step prediction error and dashed lines show the average error over five steps.", "section": "5.2 Transfer to Low-data Domains"}, {"figure_path": "DKSI3bULiZ/figures/figures_26_1.jpg", "caption": "Figure 3: NRMSE for transfer learning tasks. Solid lines are one-step error. Dashed lines are averaged error over five step rollouts. The MPP model shows clear performance benefits in both cases. The more turbulent behavior of \u201cfar\u201d seems to be difficult to learn from scratch or from video data, but pretraining on physical data leads to much stronger results.", "description": "This figure compares the performance of three different methods on transfer learning tasks: a model pretrained using Multiple Physics Pretraining (MPP), a model trained from scratch, and a VideoMAE model (pretrained on video data).  The x-axis shows the number of training samples, and the y-axis shows the Normalized Root Mean Square Error (NRMSE).  Two transfer scenarios are shown: \"Near\" transfer (less turbulent), and \"Far\" transfer (more turbulent).  The results demonstrate that the MPP model significantly outperforms the other two, especially in the more challenging \"Far\" transfer scenario, highlighting the benefits of MPP for transfer learning, particularly when dealing with systems exhibiting previously unseen or complex physical behaviors.", "section": "5.2 Transfer to Low-data Domains"}, {"figure_path": "DKSI3bULiZ/figures/figures_26_2.jpg", "caption": "Figure 3: NRMSE for transfer learning tasks. Solid lines are one-step error. Dashed lines are averaged error over five step rollouts. The MPP model shows clear performance benefits in both cases. The more turbulent behavior of \u201cfar\u201d seems to be difficult to learn from scratch or from video data, but pretraining on physical data leads to much stronger results.", "description": "This figure compares the performance of MPP-pretrained models, models trained from scratch, and VideoMAE (a video foundation model) on transfer learning tasks.  Two transfer learning scenarios are shown: 'Near' (similar physics to the pretraining data) and 'Far' (different physics). The results demonstrate that MPP-pretrained models significantly outperform both training from scratch and VideoMAE, especially in the 'Far' scenario, highlighting the benefit of MPP for transferring knowledge to unseen physical systems.", "section": "5.2 Transfer to Low-data Domains"}, {"figure_path": "DKSI3bULiZ/figures/figures_27_1.jpg", "caption": "Figure 3: NRMSE for transfer learning tasks. Solid lines are one-step error. Dashed lines are averaged error over five step rollouts. The MPP model shows clear performance benefits in both cases. The more turbulent behavior of \u201cfar\u201d seems to be difficult to learn from scratch or from video data, but pretraining on physical data leads to much stronger results.", "description": "This figure compares the performance of MPP, training from scratch, and VideoMAE (a video foundation model) in a transfer learning setting.  Two transfer tasks are considered, \"Near\" and \"Far\", representing systems with varying degrees of similarity to the training data.  The results show that MPP consistently outperforms the other methods, demonstrating the effectiveness of multi-physics pretraining, especially in low-data regimes.", "section": "5.2 Transfer to Low-data Domains"}, {"figure_path": "DKSI3bULiZ/figures/figures_27_2.jpg", "caption": "Figure 3: NRMSE for transfer learning tasks. Solid lines are one-step error. Dashed lines are averaged error over five step rollouts. The MPP model shows clear performance benefits in both cases. The more turbulent behavior of \u201cfar\u201d seems to be difficult to learn from scratch or from video data, but pretraining on physical data leads to much stronger results.", "description": "This figure compares the performance of three different models on two transfer learning tasks: a \"near\" task and a \"far\" task. The \"near\" task involves a system with physics similar to those seen during pretraining, while the \"far\" task involves a system with significantly different physics.  The results show that the MPP (Multiple Physics Pretraining) model significantly outperforms both a model trained from scratch and a video foundation model (VideoMAE) on both tasks, particularly the more challenging \"far\" task.  This demonstrates the effectiveness of the MPP model in transferring knowledge to systems with previously unseen physics.", "section": "5.2 Transfer to Low-data Domains"}, {"figure_path": "DKSI3bULiZ/figures/figures_27_3.jpg", "caption": "Figure 3: NRMSE for transfer learning tasks. Solid lines are one-step error. Dashed lines are averaged error over five step rollouts. The MPP model shows clear performance benefits in both cases. The more turbulent behavior of \u201cfar\u201d seems to be difficult to learn from scratch or from video data, but pretraining on physical data leads to much stronger results.", "description": "This figure compares the performance of MPP, training from scratch, and VideoMAE on transfer learning tasks.  It shows NRMSE (Normalized Root Mean Squared Error) for both one-step predictions and predictions averaged over five time steps. Two transfer learning scenarios are presented: \"Near\" and \"Far\". The \"Near\" task involves a system with similar physics to those seen during pretraining, while the \"Far\" task involves a system with much more turbulent and complex behavior. The results demonstrate that MPP significantly outperforms both training from scratch and video-based pretraining, especially in the \"Far\" scenario.", "section": "5.2 Transfer to Low-data Domains"}, {"figure_path": "DKSI3bULiZ/figures/figures_28_1.jpg", "caption": "Figure 2: (Left) MPP works by individually normalizing each example using Reversible Instance Normalization (RevIN) then embedding each field individually into a shared, normalized space. A single transformer backbone can then predict the next step for multiple sets of physics. We use an AViT backbone which attends over space and time axis sequentially. Spatial attention is further split by axis, though these share linear projection weights. (Right) The embedding and reconstruction matrices are formed by subsampling a larger 1 \u00d7 1 convolutional filter based on input fields.", "description": "This figure illustrates the Multiple Physics Pretraining (MPP) architecture. The left panel shows the overall workflow: individual data normalization (RevIN), field embedding into a shared space, spatiotemporal transformer processing, and prediction.  The right panel details the embedding and reconstruction process, highlighting the use of 1x1 convolutional filters and subsampling to create the matrices.", "section": "4 Scalable Multiple Physics Pretraining"}]