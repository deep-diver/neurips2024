[{"figure_path": "DKSI3bULiZ/tables/tables_6_1.jpg", "caption": "Table 1: NRMSE comparison between MPP-pretrained models and dedicated baselines on shallow water equations (SWE), 2D Diffusion-Reaction (DiffRe2D), and compressible Navier-Stokes (CNS) at Mach numbers M = .1 and M = 1. Complex parameters counted as two real. Top performing within size range and overall are bolded. Dashes indicate precision not provided by source.", "description": "This table compares the normalized root mean squared error (NRMSE) of different models on three different partial differential equation (PDE) datasets: shallow water equations (SWE), 2D diffusion-reaction (DiffRe2D), and compressible Navier-Stokes (CNS).  The models are compared at two different Mach numbers (M=0.1 and M=1.0).  The table includes both models pretrained using the multiple physics pretraining (MPP) method and task-specific baselines. The best performing models within each size range, and overall, are highlighted in bold. Missing values are indicated by dashes.", "section": "5.1 Pretraining Representations"}, {"figure_path": "DKSI3bULiZ/tables/tables_8_1.jpg", "caption": "Table 2: NRMSE for 2D to 3D inflation. Sub-headings are initial condition type.", "description": "This table compares the normalized root mean squared error (NRMSE) for different models on 3D incompressible Navier-Stokes simulations.  The models are evaluated on two different initial conditions: turbulent and random. The results show that inflating 2D models pretrained with MPP to 3D yields better performance than training from scratch, especially in the turbulent setting.", "section": "5.3 Inflation to 3D"}, {"figure_path": "DKSI3bULiZ/tables/tables_18_1.jpg", "caption": "Table 3: Details of the various model architectures and scales explored.", "description": "This table presents the hyperparameters of the different transformer models used in the experiments.  The table shows the embedding dimension, MLP dimension, number of heads, number of blocks, patch size, and the total number of parameters for each model. Different model sizes (Ti, S, B, L) are used for evaluating how model performance changes with scale.", "section": "5.1 Pretraining Representations"}, {"figure_path": "DKSI3bULiZ/tables/tables_18_2.jpg", "caption": "Table 1: NRMSE comparison between MPP-pretrained models and dedicated baselines on shallow water equations (SWE), 2D Diffusion-Reaction (DiffRe2D), and compressible Navier-Stokes (CNS) at Mach numbers M = .1 and M = 1. Complex parameters counted as two real. Top performing within size range and overall are bolded. Dashes indicate precision not provided by source.", "description": "This table compares the Normalized Root Mean Squared Error (NRMSE) of multiple physics pretraining (MPP) models against several dedicated baselines across three different physical systems: shallow water equations (SWE), 2D diffusion-reaction equations (DiffRe2D), and compressible Navier-Stokes equations (CNS).  The results are shown for Mach numbers of 0.1 and 1.0, highlighting the performance of MPP across different scales and system complexities.  Bold values indicate the top-performing models in each category.", "section": "5.1 Pretraining Representations"}, {"figure_path": "DKSI3bULiZ/tables/tables_21_1.jpg", "caption": "Table 5: Effective learning rate for the finetuning of VideoMAE.", "description": "This table shows the effective learning rates used for fine-tuning two pre-trained VideoMAE models (K400 and SSV2) on two different transfer learning tasks: \"Near\" and \"Far\".  The learning rates were determined empirically, likely through hyperparameter search, to achieve optimal performance on each task. The values indicate that the optimal learning rate for each model varies depending on the specific transfer task.", "section": "5.2 Transfer to Low-data Domains"}, {"figure_path": "DKSI3bULiZ/tables/tables_22_1.jpg", "caption": "Table 1: NRMSE comparison between MPP-pretrained models and dedicated baselines on shallow water equations (SWE), 2D Diffusion-Reaction (DiffRe2D), and compressible Navier-Stokes (CNS) at Mach numbers M = .1 and M = 1. Complex parameters counted as two real. Top performing within size range and overall are bolded. Dashes indicate precision not provided by source.", "description": "The table compares the Normalized Root Mean Squared Error (NRMSE) of different models on three different tasks: Shallow Water Equations (SWE), 2D Diffusion-Reaction (DiffRe2D), and Compressible Navier-Stokes (CNS) at two different Mach numbers (0.1 and 1).  The models are categorized into MPP-pretrained models (Multiple Physics Pretraining) and dedicated baselines.  For each task and Mach number, the NRMSE is shown for each model.  The best performing model for each task and Mach number is bolded.", "section": "5.1 Pretraining Representations"}, {"figure_path": "DKSI3bULiZ/tables/tables_23_1.jpg", "caption": "Table 7: Ablation table showing changes in NRMSE as our proposed modifications to the architecture and model are removed.", "description": "This ablation study analyzes the impact of removing different components of the proposed architecture and training procedure.  The table shows the NRMSE for four different PDE systems (SWE, DiffRe2D, CNS M1.0, CNS M0.1) under different conditions: the full model (MPP-AVIT-B), removing the normalized MSE training loss, removing the reversible instance normalization (RevIN), and removing both.  The results highlight the contributions of each component to the overall performance of the model. Removing either the normalized loss or RevIN significantly degrades performance, and removing both leads to a drastic decrease in accuracy.", "section": "5.1 Pretraining Representations"}, {"figure_path": "DKSI3bULiZ/tables/tables_23_2.jpg", "caption": "Table 1: NRMSE comparison between MPP-pretrained models and dedicated baselines on shallow water equations (SWE), 2D Diffusion-Reaction (DiffRe2D), and compressible Navier-Stokes (CNS) at Mach numbers M = .1 and M = 1. Complex parameters counted as two real. Top performing within size range and overall are bolded. Dashes indicate precision not provided by source.", "description": "This table compares the normalized root mean squared error (NRMSE) of different models on three different partial differential equation (PDE) datasets: shallow water equations (SWE), 2D Diffusion-Reaction (DiffRe2D), and compressible Navier-Stokes (CNS) at two different Mach numbers (M=0.1 and M=1).  The models include a variety of transformer-based architectures, both pretrained using the proposed Multiple Physics Pretraining (MPP) method and those trained specifically for each task. The table highlights the performance of a single MPP-pretrained model across different tasks, demonstrating its ability to match or outperform task-specific baselines, particularly in situations with limited data. The number of parameters in each model is also provided.", "section": "5.1 Pretraining Representations"}, {"figure_path": "DKSI3bULiZ/tables/tables_24_1.jpg", "caption": "Table 9: RMSE for inverse problem tasks. Error from constant prediction included for context.", "description": "This table presents the results of an experiment that evaluates the ability of MPP-pretrained models to solve inverse problems, specifically identifying forcing and buoyancy parameters from simulation data.  The RMSE (Root Mean Squared Error) values are shown for the MPP, scratch (training from scratch), and a baseline method from Mialon et al. (2023). A constant prediction baseline is also included to provide a context for comparison.  The results demonstrate that models pretrained using the MPP approach achieve lower RMSE than training from scratch and the Mialon et al. baseline for both the forcing and buoyancy inverse problems.", "section": "5 Experiments"}, {"figure_path": "DKSI3bULiZ/tables/tables_24_2.jpg", "caption": "Table 1: NRMSE comparison between MPP-pretrained models and dedicated baselines on shallow water equations (SWE), 2D Diffusion-Reaction (DiffRe2D), and compressible Navier-Stokes (CNS) at Mach numbers M = .1 and M = 1. Complex parameters counted as two real. Top performing within size range and overall are bolded. Dashes indicate precision not provided by source.", "description": "This table compares the performance of Multiple Physics Pretraining (MPP) against several baselines on three different Partial Differential Equation (PDE) datasets: Shallow Water Equations (SWE), 2D Diffusion-Reaction (DiffRe2D), and Compressible Navier-Stokes (CNS).  The table shows the Normalized Root Mean Squared Error (NRMSE) for each model and dataset.  The models are grouped by architecture and size, allowing for comparison of performance across different model capacities.  The best performing model within each size range and overall are highlighted in bold.  Missing values are indicated by dashes.", "section": "5.1 Pretraining Representations"}, {"figure_path": "DKSI3bULiZ/tables/tables_25_1.jpg", "caption": "Table 1: NRMSE comparison between MPP-pretrained models and dedicated baselines on shallow water equations (SWE), 2D Diffusion-Reaction (DiffRe2D), and compressible Navier-Stokes (CNS) at Mach numbers M = .1 and M = 1. Complex parameters counted as two real. Top performing within size range and overall are bolded. Dashes indicate precision not provided by source.", "description": "This table compares the performance of Multiple Physics Pretraining (MPP) against other methods on three different PDEs: Shallow Water Equations (SWE), 2D Diffusion-Reaction (DiffRe2D), and Compressible Navier-Stokes (CNS).  Results are shown for different Mach numbers (M = 0.1 and M = 1). The table highlights the NRMSE (Normalized Root Mean Square Error) achieved by each method and shows that MPP generally performs competitively, especially when considering its task-agnostic nature.", "section": "5.1 Pretraining Representations"}, {"figure_path": "DKSI3bULiZ/tables/tables_25_2.jpg", "caption": "Table 12: Test NRMSE for \u201cNear\u201d Compressible Navier-Stokes M0.1, \u03b7 = .01.", "description": "This table presents the test NRMSE results for the \"Near\" Compressible Navier-Stokes dataset with a Mach number of 0.1 and a viscosity of 0.01.  It compares the performance of VideoMAE (K400), VideoMAE (SSV2), and MPP-AVIT-B models across different numbers of training samples (100, 200, 400, 600, 800) for both one-step prediction (T+1) and five-step prediction (T+5). The results show the normalized root mean squared error, demonstrating how the accuracy of each model improves with increasing training data and comparing the models' ability to extrapolate over multiple timesteps.", "section": "5.2 Transfer to Low-data Domains"}, {"figure_path": "DKSI3bULiZ/tables/tables_26_1.jpg", "caption": "Table 1: NRMSE comparison between MPP-pretrained models and dedicated baselines on shallow water equations (SWE), 2D Diffusion-Reaction (DiffRe2D), and compressible Navier-Stokes (CNS) at Mach numbers M = .1 and M = 1. Complex parameters counted as two real. Top performing within size range and overall are bolded. Dashes indicate precision not provided by source.", "description": "This table compares the Normalized Root Mean Squared Error (NRMSE) of various models on three different partial differential equation (PDE) datasets: Shallow Water Equations (SWE), 2D Diffusion-Reaction (DiffRe2D), and Compressible Navier-Stokes (CNS).  The models are categorized into MPP-pretrained models (Multiple Physics Pretraining) and dedicated baselines.  The table shows the performance of models with different numbers of parameters, highlighting the best performing model for each dataset and overall. The bolded values indicate the best results for each dataset and size range. Dashes represent instances where precision was not provided in the original source.", "section": "5.1 Pretraining Representations"}]