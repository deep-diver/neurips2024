{"references": [{"fullname_first_author": "M. Abdin", "paper_title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone", "publication_date": "2024-04-01", "reason": "This paper is cited as evidence that data quality trumps quantity in large language models, a key concept in the context of data distillation."}, {"fullname_first_author": "J. Ba", "paper_title": "Do deep nets really need to be deep?", "publication_date": "2013-12-01", "reason": "This paper is referenced in the context of knowledge distillation, another important concept relating to the efficiency of data use in machine learning."}, {"fullname_first_author": "G. Cazenavette", "paper_title": "Dataset Distillation by Matching Training Trajectories", "publication_date": "2022-03-01", "reason": "This paper is one of the main methods benchmarked against, and as such is central to the evaluation of soft labels in dataset distillation."}, {"fullname_first_author": "J. Cui", "paper_title": "Scaling Up Dataset Distillation to ImageNet-1K with Constant Memory", "publication_date": "2022-11-01", "reason": "This paper is among the top performing methods benchmarked against and highlights the importance of soft labels in scaling to larger datasets."}, {"fullname_first_author": "Y. Feng", "paper_title": "Embarassingly Simple Dataset Distillation", "publication_date": "2023-11-01", "reason": "This paper is another of the top performing methods benchmarked against, and provides a further comparison for the importance of soft labels in distillation."}]}