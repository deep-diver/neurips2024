[{"figure_path": "ncYGjx2vnE/figures/figures_1_1.jpg", "caption": "Figure 1: The Overview of Contributions and Architecture of Chimera. We present a 2-dimensional SSM with careful and expressive parameterization. It uses different learnable discretization processes to learn seasonal and long-term progression patterns, and leverages a parallelizable and fast training process by re-formulating the 2D input dependent recurrence as a 2D prefix sum problem.", "description": "This figure provides a high-level overview of the Chimera architecture, highlighting its key components and how they work together.  It shows the input multivariate time series, which is processed by a two-headed 2-dimensional state-space model (SSM). Each head uses a learnable discretization process (Zero-Order Hold) to capture seasonal and long-term patterns.  The two SSM heads produce outputs that are combined to create a final, expressive representation of the time series.  The figure also illustrates how the 2D recurrence is reformulated as a 2D prefix sum problem to enable efficient parallel training.", "section": "3 Chimera: A Three-headed 2-Dimensional State Space Model"}, {"figure_path": "ncYGjx2vnE/figures/figures_4_1.jpg", "caption": "Figure 1: The Overview of Contributions and Architecture of Chimera. We present a 2-dimensional SSM with careful and expressive parameterization. It uses different learnable discretization processes to learn seasonal and long-term progression patterns, and leverages a parallelizable and fast training process by re-formulating the 2D input dependent recurrence as a 2D prefix sum problem.", "description": "This figure provides a high-level overview of the Chimera model, a novel 2-dimensional state space model (SSM) for multivariate time series.  The diagram showcases the key components:  two SSM heads (one for seasonal and one for long-term patterns), learnable discretization processes to capture complex time dependencies, and the use of a parallel 2D scan for efficient training. It highlights Chimera's ability to model seasonal and long-term patterns simultaneously while leveraging an efficient training process.", "section": "3 Chimera: A Three-headed 2-Dimensional State Space Model"}, {"figure_path": "ncYGjx2vnE/figures/figures_9_1.jpg", "caption": "Figure 2: Different forms of Chimera. (Top-Left) Chimera has a recurrence form (bi-directional along the variates), which also can be computed as a global convolution in training. (Top-Right) In forecasting, we present the multivariate closed-loop to improve the performance for long horizons. (Bottom) Using data-dependent parameters, Chimera training can be done as a parallel 2D scan.", "description": "This figure illustrates three different perspectives of the Chimera model: a recurrence form showing the bi-directional flow of information along the variate axis, which can be efficiently computed as a convolution; a closed-loop architecture for multivariate forecasting which handles long time horizons; and a parallel 2D scan implementation enabled by data-dependent parameters for efficient training.", "section": "3 Chimera: A Three-headed 2-Dimensional State Space Model"}, {"figure_path": "ncYGjx2vnE/figures/figures_9_2.jpg", "caption": "Figure 4: Wall-clock scaling.", "description": "This figure demonstrates how the training time of different models scales with the length of the input time series.  It shows that Chimera (using the 2D parallel scan method) scales linearly with sequence length, significantly outperforming other models like Transformer, S4, LSTM, and SpaceTime.  The near-linear scaling of Chimera highlights the efficiency of its algorithm.  The figure is used to support the claim that Chimera is efficient even for very long sequences.", "section": "4.2 Ablation Study and Efficiency"}, {"figure_path": "ncYGjx2vnE/figures/figures_9_3.jpg", "caption": "Figure 2: Different forms of Chimera. (Top-Left) Chimera has a recurrence form (bi-directional along the variates), which also can be computed as a global convolution in training. (Top-Right) In forecasting, we present the multivariate closed-loop to improve the performance for long horizons. (Bottom) Using data-dependent parameters, Chimera training can be done as a parallel 2D scan.", "description": "This figure illustrates three different perspectives of the Chimera model. The top-left panel shows the model's architecture, highlighting its bi-directional recurrence along variates, which can be efficiently computed as a global convolution during training. The top-right panel depicts the multivariate closed-loop used for forecasting, designed to improve long-horizon prediction accuracy.  Finally, the bottom panel illustrates how the training process can be parallelized using a 2D scan when employing data-dependent parameters.", "section": "3 Chimera: A Three-headed 2-Dimensional State Space Model"}, {"figure_path": "ncYGjx2vnE/figures/figures_9_4.jpg", "caption": "Figure 2: Different forms of Chimera. (Top-Left) Chimera has a recurrence form (bi-directional along the variates), which also can be computed as a global convolution in training. (Top-Right) In forecasting, we present the multivariate closed-loop to improve the performance for long horizons. (Bottom) Using data-dependent parameters, Chimera training can be done as a parallel 2D scan.", "description": "This figure illustrates different aspects of the Chimera model.  The top-left panel shows the model's recurrent structure, highlighting its bi-directional processing of variates and its equivalence to a global convolution during training for efficiency.  The top-right panel depicts the multivariate closed-loop architecture used in forecasting, enabling improved long-horizon predictions. The bottom panel demonstrates how data-dependent parameters allow for efficient parallel 2D scanning during Chimera's training process.", "section": "3 Chimera: A Three-headed 2-Dimensional State Space Model"}]