{"references": [{"fullname_first_author": "Bartlett, P. L.", "paper_title": "Benign overfitting in linear regression", "publication_date": "2020-MM-DD", "reason": "This paper introduced the concept of benign overfitting, which is central to the current work's investigation of the same phenomenon in Vision Transformers."}, {"fullname_first_author": "Allen-Zhu, Z.", "paper_title": "Towards understanding ensemble, knowledge distillation and self-distillation in deep learning", "publication_date": "2020-MM-DD", "reason": "This paper contributes to the theoretical framework of feature learning theory which is employed in the current study."}, {"fullname_first_author": "Cao, Y.", "paper_title": "Benign overfitting in two-layer convolutional neural networks", "publication_date": "2022-MM-DD", "reason": "This paper provides a theoretical analysis of benign overfitting in CNNs, offering a comparison point for the current work's analysis of Vision Transformers."}, {"fullname_first_author": "Zhang, M.", "paper_title": "An empirical study of training self-supervised vision transformers", "publication_date": "2020-MM-DD", "reason": "This paper offers empirical insights into the training dynamics of Vision Transformers, providing a valuable context for the current work's theoretical study."}, {"fullname_first_author": "Dosovitskiy, A.", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-MM-DD", "reason": "This paper introduced the Vision Transformer (ViT), a foundational model that is the focus of the current study's investigation into benign overfitting."}]}