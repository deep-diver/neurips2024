[{"figure_path": "NTWXVvIXJM/tables/tables_6_1.jpg", "caption": "Table 2: Results of applying our Meta-DiffuB (De = a specific S2S-Diffusion model) to other S2S-Diffusion models [8, 45, 44]. The specific S2S-Diffusion model used in the exploiter model is indicated by the assignment of De. Outcomes where Meta-DiffuB outperforms previous S2S-Diffusion models are highlighted in bold. A star (+) indicates results reported directly from previous studies, while a dagger (\u2020) signifies that we reproduced the results because the original studies did not report them using the same metrics on these datasets.", "description": "This table presents the results of applying the Meta-DiffuB framework to enhance three existing S2S-Diffusion models: DiffuSeq, SeqDiffuSeq, and Dinoiser, on two benchmark datasets, QQP and WA.  It compares the performance of each original model to the performance after applying Meta-DiffuB.  Improved performance after applying Meta-DiffuB is indicated in bold.  The table also notes whether the reported results are taken directly from previous studies or reproduced by the authors due to inconsistencies in the original reporting.", "section": "5 Model-Agnostic Characteristics of Meta-DiffuB"}, {"figure_path": "NTWXVvIXJM/tables/tables_8_1.jpg", "caption": "Table 3: We present the results of our Meta-DiffuB (De = DiffuSeq) compared with other models across four Seq2Seq datasets. We report the scores of DiffuSeq and PLMs from [8]. A star (+) indicates results reported directly from previous studies, while a dagger (\u2020) signifies that we reproduced the results because the previous studies did not report them using the same metrics on these datasets. The best results among S2S-Diffusion models are underlined, and the overall best results are in bold.", "description": "This table compares the performance of Meta-DiffuB (with DiffuSeq as the exploiter model) against other state-of-the-art S2S-Diffusion models and fine-tuned pre-trained language models (PLMs) across four benchmark Seq2Seq datasets.  The results are presented using standard evaluation metrics including BLEU, ROUGE-L, BERTScore, Distinct-1, Self-BLEU and Mean-Rank.  Meta-DiffuB achieves state-of-the-art performance in most cases, demonstrating improved generation quality and diversity.", "section": "6.1 Experiment with Seq2Seq Benchmark Datasets"}, {"figure_path": "NTWXVvIXJM/tables/tables_9_1.jpg", "caption": "Table 4: The results of our Meta-DiffuB (De = DiffuSeq) and other S2S-Diffusion models for generating sentences (E) and (H) on the WA dataset. The best result in each group is highlighted in bold.", "description": "This table presents a comparison of the performance of Meta-DiffuB (with DiffuSeq as the exploiter model) against other S2S-Diffusion models (DiffuSeq, SeqDiffuSeq, and Dinoiser) on the WA dataset.  The results are separated into two groups: (E) representing easier sentences and (H) representing harder sentences.  The table shows BLEU scores (higher is better) and Self-BLEU scores (lower is better), indicating the quality and diversity of the generated sentences, respectively.  The best-performing model for each metric and sentence difficulty level is highlighted in bold.", "section": "6.2 Contextualized Noise Scheduling of Meta-DiffuB"}, {"figure_path": "NTWXVvIXJM/tables/tables_9_2.jpg", "caption": "Table 5: Results of the plug-and-play experiment for our scheduler model. The \u2018Scheduler\u2019 field indicates the dataset used to train our scheduler model, while the \u2018DiffuSeq\u2019 field indicates the dataset used to train DiffuSeq. If the \u2018DiffuSeq\u2019 field is \u2018Null\u2019, DiffuSeq generates sentences using its own noise. Results that outperform those where DiffuSeq uses its own noise scheduling are highlighted in bold.", "description": "This table presents the results of a plug-and-play experiment using a pre-trained scheduler model with DiffuSeq on various datasets. The scheduler model's capability to enhance the DiffuSeq model's performance without the need for fine-tuning is evaluated across four Seq2Seq benchmark datasets.  The results are shown in terms of BLEU, ROUGE-L, BERTScore, Dist-1, and Self-BLEU scores, with bolded values indicating cases where the plug-and-play approach outperforms DiffuSeq using its own noise scheduling.", "section": "6.3 Plug-and-Play Experiments with the Scheduler Model"}, {"figure_path": "NTWXVvIXJM/tables/tables_14_1.jpg", "caption": "Table 6: Results of Meta-DiffuB on Machine Translation datasets (DE-EN). Results where Meta-DiffuB combined with different models show improved performance are indicated in bold.", "description": "This table compares the performance of Meta-DiffuB when combined with three different S2S-Diffusion models (DiffuSeq, SeqDiffuSeq, and Dinoiser) on two machine translation datasets (IWSLT14 DE-EN and WMT14 DE-EN).  The results are presented as SacreBLEU scores, a metric for evaluating machine translation quality.  Higher scores indicate better performance. The table highlights cases where using Meta-DiffuB leads to a performance improvement compared to the baseline S2S-Diffusion model alone.", "section": "E Experiments of Meta-DiffuB on Machine Translation and Other Datasets"}, {"figure_path": "NTWXVvIXJM/tables/tables_14_2.jpg", "caption": "Table 2: Results of applying our Meta-DiffuB (De = a specific S2S-Diffusion model) to other S2S-Diffusion models [8, 45, 44]. The specific S2S-Diffusion model used in the exploiter model is indicated by the assignment of De. Outcomes where Meta-DiffuB outperforms previous S2S-Diffusion models are highlighted in bold. A star (+) indicates results reported directly from previous studies, while a dagger (\u2020) signifies that we reproduced the results because the original studies did not report them using the same metrics on these datasets.", "description": "This table compares the performance of Meta-DiffuB when used with different S2S diffusion models (DiffuSeq, SeqDiffuSeq, and Dinoiser) on two datasets (QQP and WA).  It shows the BLEU, BERTScore, and Dist-1 scores for each model and dataset combination. Bold results highlight cases where Meta-DiffuB outperforms the original S2S diffusion model.  The table also notes where results are taken directly from previous studies (*) or were reproduced by the authors (\u2020).", "section": "5 Model-Agnostic Characteristics of Meta-DiffuB"}, {"figure_path": "NTWXVvIXJM/tables/tables_15_1.jpg", "caption": "Table 8: The sample output of our Meta-DiffuB (De = DiffuSeq) and other S2S-Diffusion models [8, 44, 45] on hardest generated sentences (H) of WA dataset. The conditional sentence is the same.", "description": "This table presents a comparison of the text generated by Meta-DiffuB and three other S2S-Diffusion models in response to the same input sentence.  The focus is on the hardest sentences to generate (H),  highlighting the differences in quality and diversity of the generated text. Meta-DiffuB shows better performance in producing effective and diverse sentences compared to the other models.", "section": "Showcase of Generated Sentences"}, {"figure_path": "NTWXVvIXJM/tables/tables_15_2.jpg", "caption": "Table 9: The sample output of Meta-DiffuB (De = DiffuSeq) and other S2S-Diffusion models [8, 44, 45] on hardest generated sentence (H) of QQP dataset. The conditional sentence is the same.", "description": "This table presents example outputs from the Meta-DiffuB model and three other S2S-Diffusion models (DiffuSeq, Dinoiser, and SeqDiffuSeq) for a difficult sentence from the QQP dataset.  It demonstrates the differences in the quality and diversity of generated sentences by each model when faced with a challenging input.", "section": "6.2 Contextualized Noise Scheduling of Meta-DiffuB"}, {"figure_path": "NTWXVvIXJM/tables/tables_16_1.jpg", "caption": "Table 4: The results of our Meta-DiffuB (De = DiffuSeq) and other S2S-Diffusion models for generating sentences (E) and (H) on the WA dataset. The best result in each group is highlighted in bold.", "description": "This table presents the performance comparison of Meta-DiffuB (with DiffuSeq as the exploiter model) against other S2S-Diffusion models (DiffuSeq, SeqDiffuSeq, and Dinoiser) on the WA dataset.  The comparison is done for both easy (E) and hard (H) sentences.  The 'easy' and 'hard' sentences are determined based on their BLEU scores, where lower scores indicate harder sentences to generate. The table shows the BLEU score, Self-BLEU score, ROUGE-L score, BERTScore, and Dist-1 for each model and sentence type.  Higher BLEU, ROUGE-L, and BERTScore indicate better quality, while lower Self-BLEU and higher Dist-1 indicate better diversity.  Meta-DiffuB shows consistently superior results compared to the other models, demonstrating effectiveness in generating both easy and hard sentences with higher quality and diversity.", "section": "6.2 Contextualized Noise Scheduling of Meta-DiffuB"}, {"figure_path": "NTWXVvIXJM/tables/tables_16_2.jpg", "caption": "Table 11: Results of the plug-and-play experiment for our scheduler model. The 'Scheduler' field indicates the dataset used to train our scheduler model, while the 'DiffuSeq' field indicates the dataset used to train DiffuSeq. If the 'DiffuSeq' field is 'Null', DiffuSeq generates sentences using its own noise. Results that outperform those where DiffuSeq uses its own noise scheduling are highlighted in bold.", "description": "This table presents the results of experiments demonstrating the plug-and-play functionality of the Meta-DiffuB scheduler model.  It shows how using a pre-trained scheduler model (trained on a specific dataset) with a separate DiffuSeq model (trained on a different or no dataset) impacts performance.  The table reports various metrics (BLEU, ROUGE-L, BERTScore, Dist-1, Self-BLEU) across multiple datasets (WA, QQP, QT), comparing scenarios with and without a pre-trained scheduler.  Bold values indicate cases where the scheduler improves performance over the DiffuSeq model operating without a pre-trained scheduler.", "section": "6.3 Plug-and-Play Experiments with the Scheduler Model"}, {"figure_path": "NTWXVvIXJM/tables/tables_17_1.jpg", "caption": "Table 12: Plug-and-play experiments on SeqDiffuSeq integrated with our scheduler. The field \u2018SeqDiffuSeq\u2019 indicates which dataset this model is trained on. When the \u2018Scheduler\u2019 field is \u2018Null\u2019, it indicates the use of the model\u2019s own noise scheduling. Results where the model performs better with its own noise are indicated in bold.", "description": "This table presents plug-and-play experiment results using the proposed scheduler model with the SeqDiffuSeq model.  It shows the performance improvements achieved by integrating the scheduler model trained on different datasets (WA, QQP, QT) with a pre-trained SeqDiffuSeq model on the same datasets. The 'Null' entry represents the baseline performance of the SeqDiffuSeq model using its own noise scheduling mechanism. Results are compared using BLEU, BERTScore and Dist-1 metrics.", "section": "I. Additional Plug-and-Play Experiments with the Scheduler Model"}, {"figure_path": "NTWXVvIXJM/tables/tables_17_2.jpg", "caption": "Table 13: Plug-and-play experiments on Dinoiser integrated with our scheduler. The field \u2018Dinoiser\u2019 indicates which dataset this model is trained on. When the \u2018Scheduler\u2019 field is \u2018Null\u2019, it indicates the use of the model\u2019s own noise scheduling. Results where the model performs better with its own noise are indicated in bold.", "description": "This table presents the results of plug-and-play experiments using the proposed scheduler model with the Dinoiser model for sequence-to-sequence tasks.  It shows the performance improvements when integrating the scheduler trained on different datasets (WA, QQP, QT). The \u2018Null\u2019 row indicates the Dinoiser model using its own internal noise scheduling, serving as a baseline for comparison. The metrics used to evaluate performance are BLEU, BERTScore, and Dist-1.", "section": "I Additional Plug-and-Play Experiments with the Scheduler Model"}]