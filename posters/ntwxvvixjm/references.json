{"references": [{"fullname_first_author": "Jacob Austin", "paper_title": "Structured denoising diffusion models in discrete state-spaces", "publication_date": "2021-12-01", "reason": "This paper is foundational to the understanding of diffusion models in discrete spaces, a key concept underlying the core methodology of the current paper."}, {"fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-12-01", "reason": "This work introduces denoising diffusion probabilistic models, a fundamental concept that forms the basis for the current paper's approach to text generation."}, {"fullname_first_author": "Xiang Li", "paper_title": "Diffusion-LM improves controllable text generation", "publication_date": "2022-12-01", "reason": "This paper adapts diffusion models to text generation, providing a direct foundation for the sequence-to-sequence diffusion models explored in the current research."}, {"fullname_first_author": "Shansan Gong", "paper_title": "Diffuseq: Sequence to sequence text generation with diffusion models", "publication_date": "2023-01-01", "reason": "This paper is highly relevant as it directly addresses sequence-to-sequence text generation using diffusion models, a problem that the current paper aims to improve upon."}, {"fullname_first_author": "Alec Radford", "paper_title": "Language models are unsupervised multitask learners", "publication_date": "2019-12-01", "reason": "This work is important because it introduces GPT-2, a large language model used as a baseline and comparison point for the current paper's novel diffusion model."}]}