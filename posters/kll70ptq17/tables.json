[{"figure_path": "KLL70pTQ17/tables/tables_4_1.jpg", "caption": "Table 1: Hyperparameters for MaxIteration", "description": "This table lists the hyperparameters used in the MaxIteration algorithm.  These include optimizer settings (Adam), learning rates for the value function, the number of rounds and gradient steps per round, batch size, and the discount factor (\u03b3).  These settings are crucial for the performance of the algorithm in the experiments.", "section": "C.1 Hyperparameters"}, {"figure_path": "KLL70pTQ17/tables/tables_15_1.jpg", "caption": "Table 1: Hyperparameters for MaxIteration", "description": "This table lists the hyperparameters used in the MaxIteration algorithm.  These include the optimizer used (Adam), its associated hyperparameters (beta1, beta2, epsilon), the learning rate for the value function, the number of rounds for the algorithm, the number of gradient steps per round, the batch size, and the gamma discount factor.", "section": "C.1 Hyperparameters"}, {"figure_path": "KLL70pTQ17/tables/tables_15_2.jpg", "caption": "Table 2: Hyperparameters for Implicit Q-Learning", "description": "This table lists the hyperparameters used for the Implicit Q-Learning algorithm in the paper's experiments.  It includes settings for the Adam optimizer (beta1, beta2, epsilon), learning rates for both the actor and critic networks, batch size, n_steps, gamma (discount factor), tau (soft update parameter), number of critics, and parameters related to the algorithm's quantile regression function (expectile, weight_temp, max_weight). These hyperparameters are crucial for the algorithm's performance and reproducibility.", "section": "C.1 Hyperparameters"}]