[{"figure_path": "KLL70pTQ17/figures/figures_6_1.jpg", "caption": "Figure 1: Examples of MDPs with max-following policy performance comparison", "description": "This figure shows two examples of Markov Decision Processes (MDPs).  The first MDP (a) illustrates a scenario where two policies, one always going left and the other always going right, each yield low cumulative rewards individually. However, a max-following policy (which, at each state, selects the action yielding the highest expected cumulative reward from among the base policies) would achieve optimal performance by cleverly switching between left and right actions at appropriate times. The second MDP (b) highlights that max-following policies may not always be optimal, and different max-following policies can have different values, depending on how ties are broken between equally valued actions. This example shows that the max-following policy can be arbitrarily worse than an optimal policy depending on the starting state.", "section": "4 The approximate max-following benchmark"}, {"figure_path": "KLL70pTQ17/figures/figures_7_1.jpg", "caption": "Figure 1: Examples of MDPs with max-following policy performance comparison", "description": "This figure shows two examples of Markov Decision Processes (MDPs) to illustrate the performance of max-following policies compared to optimal policies and individual constituent policies.  The first MDP (a) demonstrates a scenario where two policies (one going left, one going right) perform poorly individually, but a max-following policy combining them achieves optimality. The second MDP (b) highlights cases where max-following policies can be significantly worse than the optimal policy depending on the starting state and how ties are broken between policies.", "section": "4 The approximate max-following benchmark"}, {"figure_path": "KLL70pTQ17/figures/figures_7_2.jpg", "caption": "Figure 2: Examples for Observation 4.6 and Observation 4.7", "description": "This figure consists of two subfigures. Subfigure (a) shows an MDP where small value approximation errors at s0 hinder max-following. The transition dynamics are color-coded to indicate actions taken by \u03c0\u2070 (red) and \u03c0\u00b9 (blue). Subfigure (b) shows an MDP where the max-following value function is piecewise linear, unlike the constituent policy values which are affine functions of the state for fixed actions.", "section": "4 The approximate max-following benchmark"}, {"figure_path": "KLL70pTQ17/figures/figures_9_1.jpg", "caption": "Figure 3: Policies 0 and 1 correspond to the pre-trained policies using IQL on the initial tasks above the arrow in each graph. That is, in the left most subfigure, Policy 0 corresponds to the policy of picking and placing a dumbbell, whereas Policy 1 corresponds to the policy of moving a box into the trashcan. Mean return and success rate over 5 seeds of Maxlteration compared to fine-tuning IQL on selected tasks. Error-bars correspond to standard error. Full bars correspond to returns and red lines indicate the success rate of each algorithm. Maxlteration can yield improvements in return but increased return does not always yield success.", "description": "This figure compares the performance of Maxlteration against IQL fine-tuning on three different robotic manipulation tasks from the CompoSuite benchmark.  For each task, two pre-trained policies (Policy 0 and Policy 1) serve as input to Maxlteration.  The figure displays the mean return and success rate for each method across five independent runs, with error bars representing the standard error. The results show that Maxlteration sometimes outperforms IQL fine-tuning in terms of return, but this increased return does not always translate into higher success rates.", "section": "5 Experiments"}, {"figure_path": "KLL70pTQ17/figures/figures_16_1.jpg", "caption": "Figure 3: Policies 0 and 1 correspond to the pre-trained policies using IQL on the initial tasks above the arrow in each graph. That is, in the left most subfigure, Policy 0 corresponds to the policy of picking and placing a dumbbell, whereas Policy 1 corresponds to the policy of moving a box into the trashcan. Mean return and success rate over 5 seeds of Maxlteration compared to fine-tuning IQL on selected tasks. Error-bars correspond to standard error. Full bars correspond to returns and red lines indicate the success rate of each algorithm. Maxlteration can yield improvements in return but increased return does not always yield success.", "description": "This figure shows the experimental results comparing MaxIteration's performance against fine-tuning IQL on several robotic manipulation tasks from the CompoSuite benchmark.  Each pair of bars represents a different task, with the constituent policies trained using IQL on simpler subtasks. The figure compares the mean return and success rate of Maxlteration with IQL's fine-tuning capabilities. The error bars represent the standard error across five trials.  Maxlteration often improves returns over the base IQL policies but doesn't always improve success rates; highlighting that improved returns do not guarantee improved task success.", "section": "5 Experiments"}, {"figure_path": "KLL70pTQ17/figures/figures_17_1.jpg", "caption": "Figure 5: Mean return over 5 seeds of Maxlteration on DM Control tasks [Tunyasuvunakool et al., 2020]. Error-bars correspond to standard error. MaxIteration always selects the best performing constituent policy.", "description": "This figure shows the results of the MaxIteration algorithm on two tasks from the DeepMind Control Suite: Cheetah-run and Walker-walk.  The algorithm is tested against three constituent policies (Policy 0, Policy 1, Policy 2) and compared to the results of using only the best performing constituent policy. The error bars represent the standard error across five different runs. The key finding is that MaxIteration consistently selects and performs as well as the best of the three constituent policies.", "section": "C.3 Results on DM Control"}]