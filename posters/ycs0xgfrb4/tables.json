[{"figure_path": "YCS0xGFrb4/tables/tables_7_1.jpg", "caption": "Table 1: Performance comparison in D4RL benchmarks with scripted preferences. The subscript \u25c7 indicates the baseline with access to true reward functions, while and indicate the reported scores and our re-implementation with default parameters, respectively.", "description": "This table compares the performance of different methods on several D4RL benchmark tasks.  The methods include behavior cloning (BC), Implicit Q-learning (IQL), Preference Transformer (PT), Offline Preference-based Reinforcement Learning (OPRL), Offline Preference Optimization via Policy Optimization (OPPO), and the proposed CAMP method.  The table shows average performance across different tasks with and without access to the true reward functions. The subscript \u25c7 denotes baselines using the true reward function. The subscripts and show the performance reported by the original authors and the performance after re-implementation with default settings, respectively. The results highlight the superior performance of CAMP, especially when comparing with preference-based methods.", "section": "5.2 Performance Comparison"}, {"figure_path": "YCS0xGFrb4/tables/tables_8_1.jpg", "caption": "Table 2: Generalization performance on five unseen tasks. CAMP exhibits superior performance.", "description": "This table presents the generalization performance of the proposed CAMP method and several baseline methods on five unseen tasks. The unseen tasks are not included in the training data for the models. The table shows the average success rate for each method on each unseen task, demonstrating that CAMP achieves significantly better performance than the baseline methods in this generalization setting.", "section": "5.4 Analysis on Generalization Ability"}, {"figure_path": "YCS0xGFrb4/tables/tables_18_1.jpg", "caption": "Table 3: Ablation results on the MI regularization term.", "description": "This table presents the ablation study results focusing on the impact of the mutual information (MI) regularization term. It compares the performance of the model with and without the MI regularization term across various tasks (MT-10, walker2d-medium-expert, walker2d-medium-replay, hopper-medium-expert, hopper-medium-replay, halfcheetah-medium-expert). The results demonstrate the significance of the MI regularization term in improving the model's performance by enhancing alignment between the representation conditions and the generated trajectories.", "section": "5.5 Ablation Study"}, {"figure_path": "YCS0xGFrb4/tables/tables_19_1.jpg", "caption": "Table 4: Ablations on the number of tasks.", "description": "This table presents the ablation study results on varying the number of tasks (K) used in the experiments.  It shows the average success rates across five MetaWorld tasks for three different values of K: 3, 5, and 10.  The results demonstrate the impact of the number of tasks on the overall performance of the model, showing improvements with a larger number of tasks (and thus a greater number of training samples).", "section": "5.5 Ablation Study"}, {"figure_path": "YCS0xGFrb4/tables/tables_19_2.jpg", "caption": "Table 6: Average success rates given sub-optimal datasets.", "description": "This table presents the average success rates achieved by different methods (MTBC, MTIQL, MTDiff, MT-OPPO-p, MT-OPPO-w, and CAMP) across various tasks in the MetaWorld MT-10 benchmark.  The results are based on sub-optimal datasets, meaning that only a portion of the training data is used. Each row represents a specific task, and the values indicate the average success rate with the standard deviation. This shows the performance of each algorithm when provided with limited training data.", "section": "5.2 Performance Comparison"}, {"figure_path": "YCS0xGFrb4/tables/tables_19_3.jpg", "caption": "Table 6: Average success rates given sub-optimal datasets.", "description": "This table presents the average success rates for various multi-task learning methods on the MetaWorld MT-10 benchmark dataset.  The dataset used is described as 'sub-optimal', meaning it contains a limited amount of training data. The table compares the performance of several methods, including MTBC (Multi-task Behavior Cloning), MTIQL (Multi-task Implicit Q-Learning), MTDiff (Multi-task Diffusion), MT-OPPO-p and MT-OPPO-w (modified versions of OPPO for multi-task settings), and CAMP (the authors' proposed method).  The success rate is the primary evaluation metric, indicating the percentage of successfully completed tasks.  The results show the average success rate across ten different tasks.  Error bars are provided to indicate the variability in performance.", "section": "5.2 Performance Comparison"}, {"figure_path": "YCS0xGFrb4/tables/tables_22_1.jpg", "caption": "Table 1: Performance comparison in D4RL benchmarks with scripted preferences. The subscript \u25c7 indicates the baseline with access to true reward functions, while and indicate the reported scores and our re-implementation with default parameters, respectively.", "description": "This table compares the performance of the proposed CAMP method against several baselines on D4RL benchmarks.  It shows the average success rates for different tasks and datasets (medium, medium-replay, and medium-expert). The baselines include behavior cloning (BC), Implicit Q-learning (IQL), Preference Transformer (PT), Offline Preference-based Reinforcement Learning (OPRL), and Offline Preference-based Policy Optimization (OPPO).  The table highlights CAMP's improved performance, especially compared to preference-based methods in some tasks.", "section": "5.2 Performance Comparison"}]