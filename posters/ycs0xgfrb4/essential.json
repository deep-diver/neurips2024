{"importance": "This paper is crucial because **it tackles the challenge of aligning sequential decision-making agents with human preferences across multiple tasks**.  This is a significant advancement in offline reinforcement learning, offering a more versatile solution than previous reward-based approaches. The proposed method, with its focus on preference representations and mutual information maximization, opens exciting new avenues for research in both single and multi-task settings.", "summary": "A novel regularized conditional diffusion model enables effective multi-task preference alignment in sequential decision-making by learning unified preference representations and maximizing mutual information between conditions and generated trajectories.", "takeaways": ["A novel multi-task preference learning framework unifies single- and multi-task scenarios.", "A mutual information regularization method effectively aligns generated trajectories with learned preference representations.", "The proposed approach outperforms existing methods in both single- and multi-task settings, demonstrating effectiveness and strong generalization ability."], "tldr": "Current methods for aligning sequential decision-making agents with human preferences often rely on well-defined reward functions. This is difficult, especially in multi-task scenarios, as it requires substantial human effort and faces challenges in balancing alignment and versatility. Moreover, existing return-conditioned diffusion models often struggle with ensuring consistency between conditions and generated trajectories. This paper addresses these limitations by adopting multi-task preferences as a unified framework. \nThe proposed method, CAMP, learns preference representations aligned with preference labels, guiding the conditional generation process of diffusion models. It introduces an auxiliary regularization objective to maximize the mutual information between conditions and generated trajectories, effectively improving their alignment with preferences. Experiments demonstrate CAMP's superior performance in single- and multi-task settings across various benchmarks, showcasing the favorable performance and generalization ability.", "affiliation": "Institute of Artificial Intelligence (TeleAI), China Telecom", "categories": {"main_category": "Machine Learning", "sub_category": "Reinforcement Learning"}, "podcast_path": "YCS0xGFrb4/podcast.wav"}