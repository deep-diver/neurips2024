[{"figure_path": "fyYrZbWtNz/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of (a) the data distribution from the widely used DIV2k [1] training set, (b) the reconstruction results of RCAN [51] model, and (c) the proposed weight-balancing framework.", "description": "This figure illustrates three key aspects of the paper's approach to addressing imbalance in image super-resolution. (a) Shows the imbalanced distribution of the DIV2k dataset, highlighting the disproportionate number of easy and hard patches. (b) Presents reconstruction results of a state-of-the-art model (RCAN) revealing that the model overfits easy patches (smooth areas) and underfits hard patches (texture-rich areas). (c) Outlines the proposed Weight-Balancing framework (WBSR), which aims to address data and model optimization imbalances by balancing the learning process.  The framework uses Hierarchical Equalization Sampling (HES) for data balancing and Balanced Diversity Loss (BDLoss) for model optimization. This ultimately leads to more efficient and accurate super-resolution.", "section": "1 Introduction"}, {"figure_path": "fyYrZbWtNz/figures/figures_4_1.jpg", "caption": "Figure 2: Illustration of the proposed weight-balancing framework. (a) The training stage combines hierarchical equalization sampling and balanced diversity loss to jointly train a supernet model with balanced weights. (b) The testing stage adopts the gradient projection dynamic inference with a gradient projection map and multiple dynamic subnets for efficient inference.", "description": "This figure illustrates the WBSR framework.  The training stage (a) shows how hierarchical equalization sampling addresses data imbalance by sampling patches, leading to balanced weight updates using the Balanced Diversity Loss. The testing stage (b) details the dynamic inference, where input is chopped into patches, gradients are calculated and projected onto a map to route patches to appropriate subnets for processing, followed by combining results to produce the final high-resolution image. The framework aims to efficiently address data and model optimization imbalances for efficient inference.", "section": "4 Methodology"}, {"figure_path": "fyYrZbWtNz/figures/figures_6_1.jpg", "caption": "Figure 3: Visualizations of (a) the edge detection results, (b) the gradient magnitude results, and (c) the projected subnet selection. For ease of observation, we visualize three assigned subnets with small, medium, and large computational costs as green, yellow, and red, respectively.", "description": "This figure visualizes the process of gradient projection dynamic inference.  (a) shows edge detection results, highlighting the boundaries of image regions. (b) displays the gradient magnitude, indicating the complexity of each region; higher magnitudes represent more complex areas. (c) demonstrates subnet selection based on gradient magnitude. Patches with low gradients (smooth areas) are assigned to small subnets, while patches with high gradients (texture-rich regions) are assigned to larger subnets.  The color-coding helps visualize this dynamic allocation of resources during inference, illustrating how the model adapts its computational resources to the input image.", "section": "4.2 Gradient Projection Dynamic Inference"}, {"figure_path": "fyYrZbWtNz/figures/figures_8_1.jpg", "caption": "Figure 2: Illustration of the proposed weight-balancing framework. (a) The training stage combines hierarchical equalization sampling and balanced diversity loss to jointly train a supernet model with balanced weights. (b) The testing stage adopts the gradient projection dynamic inference with a gradient projection map and multiple dynamic subnets for efficient inference.", "description": "This figure illustrates the WBSR framework.  The training stage uses Hierarchical Equalization Sampling (HES) and Balanced Diversity Loss (BDLoss) to train a supernet model. The testing stage uses gradient projection dynamic inference for efficiency.  It shows how imbalanced LR patches are processed through HES to create balanced weights, then processed through the supernet and BDLoss to create a balanced and efficient model for inference. ", "section": "4 Methodology"}, {"figure_path": "fyYrZbWtNz/figures/figures_9_1.jpg", "caption": "Figure 5: Illustration of the gain of our weight-balancing framework relative to the baseline model and its weight rectification diagram.", "description": "This figure illustrates the performance improvement achieved by the proposed Weight-Balancing framework (WBSR) compared to a baseline model.  The bar chart shows the PSNR (Peak Signal-to-Noise Ratio) for different categories of image patches. The light blue bars represent the baseline model's performance, while the dark blue bars show the additional gain achieved using the WBSR.  The diagram on the right visually explains how the WBSR method rebalances the weights across different patch categories, leading to improved performance on challenging texture regions.  The categories are based on the complexity of the image patches.", "section": "6 Conclusion"}, {"figure_path": "fyYrZbWtNz/figures/figures_15_1.jpg", "caption": "Figure 6: Visual comparison of error map from SR model trained by L\u2081 and our Lbd.", "description": "This figure shows a visual comparison of error maps generated using L1 loss and the proposed Balanced Diversity Loss (BDLoss). The error maps are displayed side-by-side for two different images. The rightmost image shows the subnet allocation map, which visualizes how different subnets are assigned to process various parts of the image.  The comparison highlights the improved accuracy and efficiency of the BDLoss in handling complex textures and details, as evidenced by the reduced errors in the textured regions.", "section": "4.2 Gradient Projection Dynamic Inference"}, {"figure_path": "fyYrZbWtNz/figures/figures_15_2.jpg", "caption": "Figure 7: Effectiveness of different class values k of samples.", "description": "This figure shows the performance (PSNR) and computational cost (GFLOPs) of the model under different numbers of sample categories (K).  The plot indicates that using a small number of categories (K=5) results in less accurate subnet selection due to concentrated gradient vector projections. Increasing the value of K improves the accuracy but also increases the computational cost.  The optimal value of K balances the performance gain and computational cost trade-off, which is not explicitly stated in the figure but implied by the plotted trends.", "section": "5.3 Ablation Studies"}, {"figure_path": "fyYrZbWtNz/figures/figures_16_1.jpg", "caption": "Figure 4: Qualitative comparison results of our method with other methods for \u00d74 SR on the four testing datasets. Please zoom in for details.", "description": "This figure displays a qualitative comparison of super-resolution (SR) results from four different methods (ClassSR, ARM, WBSR (Ours), and a high-resolution (HR) ground truth) on four different test datasets (B100, Urban100, Test2K, and Test4K). Each dataset contains images with varying levels of complexity and texture. The results show that the proposed WBSR method generally produces more visually appealing and accurate reconstructions compared to the other three methods, especially in areas with complex textures. The WBSR method preserves more details and finer textures compared to other methods. In regions with simpler details, all four methods produce similar results.", "section": "5 Experiments"}, {"figure_path": "fyYrZbWtNz/figures/figures_16_2.jpg", "caption": "Figure 9: Qualitative comparison results of \u00d72 on Urban100 [16] dataset between our method and other methods. Please zoom in for details.", "description": "This figure compares the super-resolution results of different methods on the Urban100 dataset.  The top row shows the low-resolution (LR) image, followed by the results from ClassSR, ARM, the proposed WBSR method, and finally the high-resolution (HR) ground truth. The red boxes highlight regions of detail.  The figure aims to visually demonstrate the superior performance of the WBSR method in recovering fine details and textures compared to existing approaches.", "section": "More Qualitative Results"}, {"figure_path": "fyYrZbWtNz/figures/figures_17_1.jpg", "caption": "Figure 3: Visualizations of (a) the edge detection results, (b) the gradient magnitude results, and (c) the projected subnet selection. For ease of observation, we visualize three assigned subnets with small, medium, and large computational costs as green, yellow, and red, respectively.", "description": "This figure visualizes the process of gradient projection dynamic inference.  It shows three stages: edge detection, gradient magnitude calculation, and subnet selection based on gradient magnitude.  Each stage highlights how the complexity of image patches is evaluated, ultimately leading to the appropriate subnet selection for efficient inference.  Green, yellow, and red boxes represent different subnet choices based on computational cost.", "section": "4.2 Gradient Projection Dynamic Inference"}, {"figure_path": "fyYrZbWtNz/figures/figures_17_2.jpg", "caption": "Figure 4: Qualitative comparison results of our method with other methods for \u00d74 SR on the four testing datasets. Please zoom in for details.", "description": "This figure displays visual comparisons of super-resolution (SR) results from four different methods (ClassSR, ARM, the proposed WBSR, and a ground truth HR image) on four different test datasets (B100, Urban100, Test2k, and Test4k).  The comparisons showcase the quality of reconstruction at a scale factor of x4, highlighting the improved detail and sharpness achieved with the proposed WBSR approach in comparison to the baseline methods.", "section": "5 Experiments"}]