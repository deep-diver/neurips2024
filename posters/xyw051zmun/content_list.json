[{"type": "text", "text": "How does Gradient Descent Learn Features - A Local Analysis for Regularized Two-Layer Neural Networks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "MoZhou\\* Rong Ge University of Washington Duke University mozhou17@cs.washington.edu rongge@cs.duke.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The ability of learning useful features is one of the major advantages of neural networks. Although recent works show that neural network can operate in a neural tangent kernel (NTK) regime that does not allow feature learning, many works also demonstrate the potential for neural networks to go beyond NTK regime and perform feature learning. Recently, a line of work highlighted the feature learning capabilities of the early stages of gradient-based training. In this paper we consider another mechanism for feature learning via gradient descent through a local convergence analysis. We show that once the loss is below a certain threshold, gradient descent with a carefully regularized objective will capture ground-truth directions. We further strengthen this local convergence analysis by incorporating early-stage feature learning analysis. Our results demonstrate that feature learning not only happens at the initial gradient steps, but can also occur towards the end of training. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Feature learning has long been considered to be a major advantage of neural networks. However, how gradient-based training algorithms can learn useful features is not well-understood. In particular, the most widely applied analysis for overparametrized neural networks is the neural tangent kernel (NTK) (Jacot et al., 2018; Du et al., 2019; Allen-Zhu et al., 2019b). In this setting, the neurons don't move far from their initialization and the features are determined by the network architecture and random initialization (Chizat et al., 2019). ", "page_idx": 0}, {"type": "text", "text": "While there are empirical and theoretical evidence on the limitation of NTK regime (Chizat et al., 2019; Arora et al., 2019), extending the analysis beyond the NTK regime has been challenging. For 2-layer networks, an alternative framework for analyzing overparametrized neural networks called mean-field analysis was introduced. Earlier mean-field analysis (e.g., Chizat and Bach, 2018; Mei et al., 2018) require either infinite or exponentially many neurons. Later works (e.g., Li et al., 2020; Ge et al., 2021; Bietti et al., 2022; Mahankali et al., 2024) can analyze the training dynamics of mildly overparametrized networks with polynomially many neurons with stronger assumptions on the ground-truth function. ", "page_idx": 0}, {"type": "text", "text": "Recently, a growing line of works (Daniely and Malach, 2020; Damian et al., 2022; Abbe et al., 2021, 2022, 2023; Yehudai and Shamir, 2019; Shi et al., 2022; Ba et al., 2022; Mousavi-Hosseini et al. 2023; Barak et al., 2022; Dandi et al., 2023; Wang et al., 2024; Nichani et al., 2024a,b) showed that early stages of gradient training (either one/a few steps of gradient descent or a small amount of time of gradient flow) can be useful in feature learning. These works show that after the early stages of gradient training, the first layer in a 2-layer neural network already captures useful features (usually in the form of a low dimensional subspace), and continuing training the second layer weights will give performance guarantees that are stronger than any kernel or random feature based models. In this work, we consider the natural follow-up question: ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We show that this is not the case by demonstrating feature learning capability for the final stage of gradient training - local convergence. In particular, we prove the following result: ", "page_idx": 1}, {"type": "text", "text": "Theorem 1 (Informal). If the data is generated by a 2-layer teacher network $f_{*}$ aslongasthewidth of student network m is at least some quantity mo that only depends on $f_{*}$ avariantofgradient descent algorithm (Algorithm 1, roughly gradient descent with decreasing weight decay) can recover the target network within polynomial time. Moreover, the student neurons align with the teacher neuronsattheend. ", "page_idx": 1}, {"type": "text", "text": "Our result highlights the different mechanisms of feature learning: previous works show that in the early stages of gradient descent, the network learns the subspace spanned by the neurons in the teacher network. Our local convergence result shows that at later stages, gradient descent is able to learn the exact directions of the teacher neurons, which are much more informative compared to the subspace and lead to stronger guarantes. ", "page_idx": 1}, {"type": "text", "text": "Analyzing the entire training dynamics is still challenging so in our algorithm (see Algorithm 1) we use a convex second stage to \u201cfast-forward\" to the local analysis. Our technique for local convergence is similar to the earlier work (Zhou et al., 2021), however we consider a more complicated setting with ReLU activation and allow second-layer weights to be both positive or negative. This change requires additional regularization in the form of standard weight decay and new dual certificate analysis. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Neural Tangent Kernel _ Early works often studied neural network optimization using NTK theory (Jacot et al., 2018; Allen-Zhu et al., 2019b; Du et al., 2019). It is shown that highly-overparametrized neural nets are essentially kernel methods under certain initialization scale. However, NTK theory cannot explain the performance of neural nets in practice (Arora et al., 2019) and leads to lazy training dynamics that neurons stay close to their initialization (Chizat et al., 2019). Hence, later research efforts (e.g., Allen-Zhu et al., 2019a; Bai and Lee, 2020; Li et al., 2020), as well as current paper, focus on feature learning regime where neural nets can learn features and outperform kernel methods. ", "page_idx": 1}, {"type": "text", "text": "Early stage feature learning Researchers have recently tried to understand how neural networks trained with gradient descent (GD) can learn features, going beyond the kernel/lazy regime (Jacot et al., 2018; Chizat et al., 2019). A typical setup is to use 2-layer neural networks to learn certain target function, often equipped with low-dimensional structure. Examples include learning polynomials (Yehudai and Shamir, 2019; Damian et al., 2022), single-index models (Ba et al., 2022; MousaviHosseini et al., 2023; Moniri et al., 2024; Cui et al., 2024), multi-index models (Dandi et al., 2023), sparse boolean functions (Abbe et al., 2021, 2022, 2023), sparse parity functions (Daniely and Malach, 2020; Shi et al., 2022; Barak et al., 2022) and causal graph (Nichani et al., 2024b). Also, few works use 3-layer networks as learner model (Nichani et al., 2024a; Wang et al., 2024). These works essentially showed that feature learning happens in the early stage of training. Specifically, they often use 2-stage layer-wise training procedure: first-layer weights/features are only trained with one or few steps of gradient descent/flow and only update the second-layer afterwards. Our results give a complementary view that feature learning can also happen in the final stage training that leading student neurons eventually align with ground-truth directions. This cannot be achieved if first-layer weights are fixed after few steps. ", "page_idx": 1}, {"type": "text", "text": "Learning single/multi-index models with neural networks  Single/Multi-index models are the functions that only depend on one or few directions of the high dimensional input. Many recent works have studied the problem of using 2-layer networks to learn single-index models (Soltanolkotabi, 2017; Yehudai and Ohad, 2020; Frei et al., 2020; Wu, 2022; Bietti et al., 2022; Xu and Du, 2023; Berthier et al., 2023; Mahankali et al., 2024) and multi-index models (Damian et al., 2022; Bietti et al., 2023; Suzuki et al., 2024; Glasgow, 2024). These works show the advantages of feature learning over fixed random features in various settings. In this paper, we consider target multi-index function that can be represented by a small 2-layer network, and show a variant of GD with weight decay can learn it and, moreover, recover the ground-truth directions. ", "page_idx": 1}, {"type": "text", "text": "Local loss landscape  Safran et al. (2021) showed that in the overparametrized case with orthogonal teacher neurons, even around the local region of global minima, the landscape neither is convex nor satisfies PL condition. Chizat (2022) considered square loss with $\\ell_{2}$ regularization similar to our setup and showed the local loss landscape is strongly-convex under certain non-degenerate assumptions. However, it is not known when such assumptions actually hold and the proof cannot handle ReLU. Later Akiyama and Suzuki (2021) gives a result for ReLU, but the non-degeneracy assumption is still required (and also focus on effective $\\ell_{1}$ regularization instead of $\\ell_{2}$ regularization). Zhou et al. (2021) studies a similar local convergence setting but restricts second-layer weights to be positive and uses absolute activation. In this paper, we focus on a more natural but technically challenging case that second-layer can be positive and negative and using ReLU activation. We develop new techniques to overcome the above challenges (additional assumption, ReLU, standard second-layer, etc). ", "page_idx": 2}, {"type": "text", "text": "2 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation  Let $[n]$ be set $\\{1,\\ldots,n\\}$ . For vector $\\pmb{w}$ , we use ${\\|\\pmb{w}\\|}_{2}$ for its 2-norm and $\\overline{{\\pmb{w}}}=\\pmb{w}/\\lVert\\pmb{w}\\rVert_{2}$ as its normalized version. For two vectors $\\mathbf{\\nabla}w\\,,v$ we use $\\bar{\\angle}(\\pmb{w},\\pmb{v})\\overset{=}{=}\\operatorname{arccos}(|\\pmb{w}^{\\top}\\pmb{v}|/(\\|\\pmb{w}\\|_{2}\\,\\|\\pmb{v}\\|_{2})]\\,\\bar{\\in}$ $[0,\\pi/2]$ as the angle between them (up to a sign).For matrix $\\pmb{A}$ let $\\|\\mathbf{A}\\|_{F}$ be its Frobenius norm. We use standard $O,\\Omega,\\Theta$ to hide constants and $\\widetilde{O},\\widetilde{\\Omega},\\widetilde{\\Theta}$ to hide polylog factors. We use $O_{*}$ $\\Omega_{\\ast},\\Theta_{\\ast}$ to hide problem dependent parameters that only depend on the target network (see paragraph above (1)). ", "page_idx": 2}, {"type": "text", "text": "Teacher-student setup  We will consider the teacher-student setup for two-layer neural networks with Gaussian input $\\bar{\\pmb{x}^{}}\\sim N(\\mathbf{0},\\pmb{I}_{d})$ . The goal is to learn the teacher network of size $m_{*}$ ", "page_idx": 2}, {"type": "equation", "text": "$$\nf_{*}(\\pmb{x})=\\sum_{i=1}^{m^{*}}a_{i}^{*}\\sigma(\\pmb{w}_{i}^{*\\top}\\pmb{x})+\\pmb{w}_{0}^{*\\top}\\pmb{x}+b_{0}^{*},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\sigma(x):=\\operatorname*{max}\\{0,x\\}$ is ReLU activation, $S_{*}:=\\operatorname{span}\\{w_{1}^{*},\\ldots,w_{m^{*}}^{*}\\}$ is the target subspace.   \nWithout loss of generality, we will assume $\\lVert\\pmb{w}_{i}^{*}\\rVert_{2}=1$ due to the homogeneity of ReLU. ", "page_idx": 2}, {"type": "text", "text": "Following the recent line of works in learning single/multi-index models (Ba et al., 2022; Damian et al., 2022), we assume the target network has low dimensional structure. ", "page_idx": 2}, {"type": "text", "text": "Assumption 1. Teacher neurons form a low dimensional subspace in $\\mathbb{R}^{d}$ that is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\dim(S_{*})=\\dim(\\operatorname{span}\\{w_{1}^{*},\\dots,w_{m^{*}}^{*}\\})=r\\ll d.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We will also assume the teacher neurons are non-degenerate in the following sense: ", "page_idx": 2}, {"type": "text", "text": "Assumption 2. Teacher neurons are $\\Delta$ -separated, that is angle $\\angle(\\pmb{w}_{i}^{*},\\pmb{w}_{j}^{*})\\geq\\Delta$ for all $i\\neq j$ ", "page_idx": 2}, {"type": "text", "text": "Assumption 3. $\\begin{array}{r}{\\pmb{H}:=\\sum_{i=1}^{m^{*}}a_{i}^{*}\\pmb{w}_{i}^{*}\\pmb{w}_{i}^{*\\top}}\\end{array}$ is non-degenerate in taret subspace $S_{*}$ i.e, ran $k(H)=r$ Denote $\\kappa:=|\\lambda_{r}(H)|$ ", "page_idx": 2}, {"type": "text", "text": "Assumption 2 simply requires all teacher neurons pointing to different directions, which is crucial for identifiability (Zhou et al., 2021). ", "page_idx": 2}, {"type": "text", "text": "Assumption 3 says the target network contains low-order (second-order) information, which is related with the notion of information exponent (Arous et al., 2021). In our setting, the information exponent is at most 2 due to Assumption 3. Indeed, one can show $\\mathbb{E}_{\\pmb{x}}[f_{*}(\\pmb{x})h_{2}(\\pmb{\\breve{v}}^{\\top}\\pmb{x})]=\\hat{\\sigma}_{2}\\pmb{v}^{\\top}\\pmb{H}\\pmb{v}$ where $h_{2}(x)$ is the 2nd-order normalized Hermite polynomial and $\\hat{\\sigma}_{2}$ is the 2nd Hermite coefficient of ReLU. See Appendix A for more details. Many previous works also rely on same or similar assumption to show neural networks can learn features to perform better than kernels (Damian et al., 2022; Abbe et al., 2022; Ba et al., 2022). ", "page_idx": 2}, {"type": "text", "text": "In this paper, we are interested in the case where the complexity of target network is small. Therefore,we will use $O_{*},\\Omega_{*},\\Theta_{*}$ tohidepo $\\mathrm{{ly}}(r,m_{*},\\Delta,|a_{1}|,\\cdot\\cdot\\cdot,|a_{m_{*}}|,\\kappa)$ , which is the polynomial dependency on relevant parameters of target $f_{*}$ (does not depend on student network). ", "page_idx": 2}, {"type": "text", "text": "We will use the following overparametrized student network: ", "page_idx": 2}, {"type": "equation", "text": "$$\nf(\\pmb{x};\\pmb{\\theta})=\\sum_{i=1}^{m}a_{i}\\sigma(\\pmb{w}_{i}^{\\top}\\pmb{x})+\\alpha+\\beta^{\\top}\\pmb{x},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{\\boldsymbol{a}}=(a_{1},\\ldots,a_{m})^{\\top}\\in\\mathbb{R}^{m}$ \uff0c $W=(\\pmb{w}_{1}\\cdot\\cdot\\cdot\\pmb{w}_{m})^{\\top}\\in\\mathbb{R}^{m\\times d}$ and $\\pmb{\\theta}=(\\pmb{a},\\pmb{W},\\alpha,\\beta)$ ", "page_idx": 2}, {"type": "text", "text": "Loss and algorithm  Consider the square loss function with $\\ell_{2}$ regularization under Gaussian input ", "page_idx": 3}, {"type": "equation", "text": "$$\nL_{\\lambda}(\\pmb\\theta)=\\mathbb{E}_{\\pmb x\\sim N(0,I_{d})}[(f(\\pmb x;\\pmb\\theta)-\\widetilde y)^{2}]+\\frac{\\lambda}{2}\\left\\lVert\\pmb\\alpha\\right\\rVert_{2}^{2}+\\frac{\\lambda}{2}\\left\\lVert\\pmb W\\right\\rVert_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We will use $L$ to denote the square loss for simplicity. The $\\ell_{2}$ regularization is the same as the commonly used weight decay in practice. Our goal is to find the minima of unregularized problem $\\left.\\lambda=0\\right]$ ) to recover teacher network $f_{*}$ . However, directly analyzing the unregularized problem is challenging so instead we choose to analyze the regularized problem and will gradually let $\\lambda\\to0$ ", "page_idx": 3}, {"type": "text", "text": "In above, we use preprocessed data $(x,\\widetilde{y})$ in the loss function as in Damian et al. (2022). Specifically, given any $(\\pmb{x},y)$ With $y=f_{*}(x)$ ,denote $\\alpha_{*}=\\mathbb{E}_{\\mathbf{x}}[y]$ and $\\beta_{*}=\\mathbb{E}_{\\mathbf{x}}[y\\mathbf{x}]$ ,weget ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widetilde f_{*}(\\pmb x)=\\widetilde y=y-\\alpha_{*}-\\beta_{*}^{\\top}\\pmb x.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This preprocessing process essentially removes the O-th and 1-st order term in the Hermite expansion Oof $\\sigma$ . See Appendix A for a brief introduction of Hermite polynomials and Claim B.1. ", "page_idx": 3}, {"type": "text", "text": "Our algorithm is shown in Algorithm 1. It is roughly the standard GD following a given schedule of weightdecay $\\lambda_{t}$ that goes to O. Due to the difficulty in analyzing gradient descent training beyond early and final stage, we choose to only train the norms in Stage 2 as a tractable way to reach the local convergence regime. ", "page_idx": 3}, {"type": "text", "text": "We will use symmetric initialization that $a_{i}=-a_{i+m/2}$ \uff0c ${\\pmb w}_{i}={\\pmb w}_{i+m/2}$ with $a_{i}\\sim\\operatorname{Unif}\\{\\pm{\\sqrt{d}}\\}$ $\\pmb{w}_{i}\\sim\\mathrm{Unif}\\big((1/\\sqrt{m})\\mathbb{S}^{d-1}\\big)$ \uff0c $\\alpha=0$ $\\beta=0$ . Our analysis is not sensitive to the initialization scale we choose here. The choice is just for the simplicity of the proof. ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1: Learning 2-layer neural networks ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Input: initialization ${\\pmb\\theta}^{(0)}$ weight decay $\\lambda_{t}$ and stepsize $\\eta_{t}$   \nData preprocess: get $(x,\\widetilde{y})$ according to (3)   \nStage 1: one step gradient update $\\pmb{\\theta}^{(1)}\\leftarrow\\pmb{\\theta}^{(\\bar{0})}\\overset{\\bar{}}{-}\\eta_{0}\\nabla_{\\pmb{\\theta}}\\bar{L_{\\lambda_{0}}}(\\pmb{\\theta}^{(0)})$   \nStage 2: norm adjustment by convex program $\\begin{array}{r}{a^{(T_{2})},\\alpha^{(T_{2})},\\beta^{(T_{2})}\\gets\\operatorname*{min}_{a,\\alpha,\\beta}L(a,W^{(1)},\\alpha,\\beta)+\\lambda\\sum_{i}\\|\\pmb{w}_{i}\\|_{2}\\left|a_{i}\\right|}\\end{array}$ Balancing norm between two layers s.t. $|a_{i}|=\\|\\pmb{w}_{i}\\|_{2}$ for all $i$ ", "page_idx": 3}, {"type": "text", "text": "Stage 3: local convergence for $k\\leq K$ do // for each epoch, run GD until convergence for $T_{3,k-1}\\leq t\\leq T_{3,k}$ do $\\pmb{\\theta}^{(t+1)}\\leftarrow\\pmb{\\theta}^{(t)}-\\bar{\\eta}\\nabla_{\\pmb{\\theta}}L_{\\lambda_{3,k}}(\\pmb{\\theta}^{(t)})$ ", "page_idx": 3}, {"type": "text", "text": "Output: $\\pmb{\\theta}^{(T_{3,K})}=\\left(\\pmb{a}^{(T_{3,K})},\\pmb{W}^{(T_{3,K})},\\alpha^{(T_{3,K})},\\beta^{(T_{3,K})}\\right)$ ", "page_idx": 3}, {"type": "text", "text": "3 Main results ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we give our main result that shows training student network using Algorithm 1 can recover the target network within polynomial time. We will focus on the case that $d\\geq\\Omega_{*}(1)$ when the complexity of target function is small. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2 (Main result). Under Assumption 1, 2, 3, consider Algorithm 1 on loss (2). There exists a scheduleofweight decay $\\lambda_{t}$ and step size $\\eta_{t}$ such that given $m\\ge m_{0}=\\widetilde O_{*}(1)\\cdot(1/\\varepsilon_{0})^{O(r)}$ neurons withsmallenough $\\varepsilon_{0}=\\Theta_{\\ast}(1)$ ,withhigh probabilitywewill recover the target network $L(\\pmb\\theta)\\leq\\varepsilon$ within time $T=\\^{\\cdot}O_{*}(1/\\eta\\varepsilon^{2})$ where $\\eta=\\mathrm{poly}(\\varepsilon,1/d,1/m)$ ", "page_idx": 3}, {"type": "text", "text": "Moreover, when $\\varepsilon\\rightarrow0$ every student neuron $\\pmb{w}_{i}$ either aligns with one of teacher neuron $\\pmb{w}_{j}^{*}$ as $\\angle(\\pmb{w}_{i},\\pmb{w}_{j}^{*})=0$ or vanishes as $|a_{i}|=\\|\\pmb{w}_{i}\\|=0$ ", "page_idx": 3}, {"type": "text", "text": "Note that our results can be extended to only have access to polynomial number of samples by using standard concentration tools. We omit the sample complexity for simplicity. See more discussion in Appendix J. We emphasize that the required width $m_{0}$ only depends on the complexity of target function $f_{*}$ (only quantities that are related to $f_{*}$ , not student network $f$ or error $\\varepsilon$ ), so any mildly overparametrized networks can learn $f_{*}$ efficiently to arbitrary small error. ", "page_idx": 3}, {"type": "text", "text": "The analysis consists of three stages: early-stage feature learning (Stage 1 and 2) and final-stage feature learning/local convergence (Stage 3). It will be clear in the later section that $\\varepsilon_{0}$ is in fact the threshold to enter the local convergence regime. See Section 4 for more details. ", "page_idx": 4}, {"type": "text", "text": "Our result improves the previous works that only train the first layer weight with small number of gradient steps at the beginning (Damian et al., 2022; Ba et al., 2022; Abbe et al., 2021, 2022, 2023). In these works, neural networks only learn the target subspace and do random features within it (see Section 4.1 for more details). Intuitively, these random features need to span the whole space of the target function class to perform well, which means its number (the width) should be on the order of the dimension of target function class. For 2-layer networks, random features in the target subspace need $(1/\\varepsilon)^{O(r)}$ neurons to achieve desired accuracy $\\varepsilon$ In contrast, continue training both layer at the last phase of training allows us to learn not only subspace but also exactly the ground-truth directions. Moreover, we only use $(1/\\varepsilon_{0})^{O(r)}$ neurons that only depends on the complexity of target network. This highlights the benefit of continue training first layer weights instead of fixing them after first step. ", "page_idx": 4}, {"type": "text", "text": "4 Proof overview ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we give the proof overview of these three stages separately. ", "page_idx": 4}, {"type": "text", "text": "Denote the optimality gap $\\zeta$ at time $t$ as the difference between current loss and the best loss one could achieve with networks of any size (including infinite-width networks) ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\zeta_{t}=\\!L_{\\lambda_{t}}(\\pmb\\theta^{(t)})-\\operatorname*{min}_{\\mu\\in\\mathcal{M}(\\mathbb{S}^{d-1})}L_{\\lambda_{t}}(\\mu),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{M}(\\mathbb{S}^{d-1})$ is the set of measures on the sphere $\\mathbb{S}^{d-1}$ . As an example, if $\\begin{array}{r}{\\mu=\\sum_{i}a_{i}\\left\\|\\pmb{\\{w}}_{i}\\right\\|\\delta\\pmb{\\overline{{\\mathbf{w}}}}_{i}}\\end{array}$ then $L_{\\lambda}(\\mu)$ recovers $L_{\\lambda}(\\pmb\\theta)$ when linear term $\\alpha,\\beta$ are perfectly fitted and norms are balanced $|a_{i}|=\\|{\\pmb w}_{i}\\|$ . We defer the precise definition of $L_{\\lambda}(\\mu)$ to (6) in appendix. ", "page_idx": 4}, {"type": "text", "text": "4.1  Stage 1 ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For Stage 1, we show in the lemma below that the first step of gradient descent identifies the target subspace and ensures there always exists student neuron that is close to every teacher neuron. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3 (Stage 1). Under Assumption 1,2,3, consider Algorithm $^{\\,l}$ with $\\lambda_{0}~=~\\eta_{0}~=~1$ and $m\\ge m_{0}=\\widetilde{O}_{*}(\\bar{1})\\cdot(1/\\varepsilon_{0})^{O(r)}$ with any $\\varepsilon_{0}=\\Theta_{\\ast}(1)$ Afterfrst st, withprobabily $1-\\delta$ we have (i) for every teacher neuron $\\pmb{w}_{i}^{*}$ , there exists at least one student neuron ${\\pmb w}_{j}$ s.t. $\\angle(\\pmb{w}_{i}^{*},\\pmb{w}_{j})\\leq\\varepsilon_{0}$ (i) $\\left\\|\\pmb{w}_{i}^{(1)}\\right\\|_{2}=\\Theta_{*}(1).$ $|a_{i}^{(1)}|\\leq O_{*}(1/\\sqrt{m})$ for all $i\\in[m_{*}]$ $\\alpha_{1}=0$ and $\\beta_{1}=\\mathbf{0}$ ", "page_idx": 4}, {"type": "text", "text": "The key observation here s similar toDamian et al 2022) that $\\pmb{w}_{i}^{(1)}\\approx-2\\eta_{0}a_{i}^{(0)}\\left(\\hat{\\sigma}_{2}^{2}\\pmb{H}\\overline{{\\pmb{w}}}_{i}\\right)$ so that given $\\pmb{H}$ isnn-etresp $S_{*}$ we essentialy sample $\\pmb{w}_{i}^{(1)}$ from the target subspaece. It is then natural to expect that the neurons form an $\\varepsilon_{0}$ -net in the target subspace given $m_{0}$ neurons. ", "page_idx": 4}, {"type": "text", "text": "4.2  Stage 2 ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Given the learned features (first-layer weights) in Stage 1, we now perform least squares to adjust the norms and reach a low loss solution in Stage 2. ", "page_idx": 4}, {"type": "text", "text": "Lemma4 (Stage 2). Under Assumption 1,2,3, consider Algorithm $^{\\,l}$ With $\\lambda_{t}=\\sqrt{\\varepsilon_{0}}$ .Given Stage 1 in Lemma 3, we have Stage 2 ends within time $T_{2}=\\widetilde{O}_{*}(1/\\eta\\varepsilon_{0})$ such that optimality gap $\\zeta_{T_{2}}=O_{*}(\\varepsilon_{0})$ ", "page_idx": 4}, {"type": "text", "text": "It remains an open problem to prove the convergence when training both layers simultaneously beyond early and final stage. To overcome this technical challenge, we choose to use a simple least square for Stage 2. We use the simple (sub)gradient descent to optimize this loss. There exist many other algorithms that can solve this Lasso-type problem, but we omit it for simplicity as this is not the main focus of this paper. ", "page_idx": 4}, {"type": "text", "text": "Note that the regularization in Algorithm 1 is the same as standard weight decay when we train both layers. This regularization leads to several desired properties at the end of Stage 2: (1) prevent norm cancellation between neurons: neurons with similar direction but different sign of second layer weights cancel with each other; (2) neurons mostly concentrate around ground-truth directions. As we will see later, these nice properties continue to hold in Stage 3, thanks to the regularization. ", "page_idx": 4}, {"type": "text", "text": "4.3Stage 3 ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "After Stage 2 we are in the local convergence regime. The following lemma shows that we could recover the target network within polynomial time using a multi-epoch gradient descent with decreasingweightdecay $\\lambda$ at every epoch. Note that this result only requires the initial optimality gap is small and width $m\\geq m_{*}$ (target network width, not $m_{0}$ ", "page_idx": 5}, {"type": "text", "text": "Lemma 5 (Stage 3). Under Assumption 1,2,3, consider Algorithm $^{\\,l}$ on loss (2). Given Stage 2 in Lemma ,if the inial optimality gap $\\zeta_{3,0}\\leq O_{*}(\\lambda_{3,0}^{9/5})$ weight decay $\\lambda$ follows the schedule of initial value $\\lambda_{3,0}=O_{*}(1)$ and $k$ -th epoch $\\lambda_{3,k}=\\lambda_{3,k-1}/2$ and stepsize $\\eta_{3k}=\\eta\\le O_{*}(\\lambda_{3,k}^{12}d^{-3})$ for all $T_{3,k}\\leq t\\leq T_{3,k+1}$ in epoch $k$ then within $K=O_{*}(\\log(1/\\varepsilon))$ epochs and total $T_{3}-T_{2}=$ $O_{*}(\\lambda_{3,0}^{-4}\\eta^{-1}\\varepsilon^{-2})$ time we recover the ground-truth network $L(\\pmb\\theta)\\leq\\varepsilon$ ", "page_idx": 5}, {"type": "text", "text": "The lemma above relies on the following result that shows the local landscape is benign in the sense that it satisfies a special case of Lojasiewicz property (Lojasiewicz, 1963). This means GD can always make progress until the optimality gap $\\zeta$ is small. ", "page_idx": 5}, {"type": "text", "text": "Lemma 6 (Gradient lower bound). When $\\Omega_{*}(\\lambda^{2})\\le\\zeta\\le{\\cal O}_{*}(\\lambda^{9/5})$ and $\\lambda\\leq O_{*}(1)$ we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left\\|\\nabla_{\\theta}L_{\\lambda}\\right\\|_{F}^{2}\\geq\\Omega_{*}(\\zeta^{4}/\\lambda^{2}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that this generalizes previous result in Zhou et al. (2021) that only focuses on 2-layer networks with positive second layer weights. This turns out to be technically challenging as two neurons with different signs can cancel each other. We discuss how to deal with this challenge in the next section. ", "page_idx": 5}, {"type": "text", "text": "5 Descent direction in local convergence (Stage 3): the benefit of weight decay ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we give the high-level proof ideas for the most technical challenging part of our results \u2014- characterize the local landscape in Stage 3 (Lemma 6). ", "page_idx": 5}, {"type": "text", "text": "The key idea is to construct descent direction \u2014\u2014 a direction that has positive correlation with the gradient direction. The gradient lower bound follows from the existence of such descent direction. ", "page_idx": 5}, {"type": "text", "text": "It turns out that the existence of both positive and negative second-layer weights introduces significant challenge for the analysis: there might exist neurons with similar directions (e.g., $(a,w)$ and $(-a,w))$ that can cancel with each other to have no effect on the output of network. Intuitively, we would hope all of them to move towards O, but they have no incentive to do so. Moreover, if they are not exactly symmetric it's hard to characterize which directions these neurons will move. ", "page_idx": 5}, {"type": "text", "text": "We use standard weight decay to address the above challenge. Specifically, weight decay helps us to \u00b7 Balance norm between neurons. When norm between two layers are balanced, the $\\ell_{2}$ regularization $\\begin{array}{r}{\\sum_{i}|a_{i}|^{2}+\\|\\pmb{w}_{i}\\|^{2}}\\end{array}$ would become the effective $\\ell_{1}$ regularization $\\begin{array}{r l r}{\\mathrm{~}}&{{}}&{2\\sum_{i}\\left|a_{i}\\right|\\left|\\pmb{\\left|\\boldsymbol{w}_{i}\\right|}\\right|}\\end{array}$ overthe distribution of neurons. Such sparsity penalty ensures most neurons concentrate around the ground-truth directions, especially preventing norm cancellation between far-away neurons. \u00b7 Reduce cancellation between close-by neurons. For close-by neurons, weight decay helps to reduce the norm of neurons with the \u2018incorrect' sign (different sign with the ground-truth neuron). This is because weight decay prefers low norm solutions, and reducing cancellations between neurons can reduce total norm (regularization term) while keeping the square loss same. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "We will group the neurons (i.e., partitioning $\\mathbb{S}^{d-1},$ 0 based on their distance to the closest teacher neurons: denote $\\bar{T_{i}}=\\{\\pmb{w}:\\angle(\\pmb{w},\\pmb{\\dot{w}}_{i}^{*})\\leq\\angle(\\pmb{\\dot{w}},\\pmb{w}_{j}^{*})$ for any $j\\neq i\\}$ (break the tie arbitrarily) so that $\\cup_{i}\\mathcal{T}_{i}=\\mathbb{S}^{d-1}$ . We will also use $\\delta_{j}$ to denote $\\angle({\\pmb w}_{j},{\\pmb w}_{i}^{*})$ for $j\\in\\mathcal{T}_{i}$ ", "page_idx": 5}, {"type": "text", "text": "As described above, weight decay can always lead to descent direction when norms are not balanced or norm cancellation happens (see Lemma F.15 and Lemma F.16). The following lemma shows that in other scenarios we can always improve features towards the ground-truth directions. ", "page_idx": 5}, {"type": "text", "text": "Lemma 7 (Feature improvement descent direction, informal). When norms are balanced and no norm cancellation hapens, there exists properly chosen $q_{i j}\\geq0$ and $\\textstyle\\sum_{j\\in{\\mathcal{T}}_{i}}a_{j}q_{i j}=a_{i}^{*}$ suchthat ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\sum_{i\\in[m_{*}]}\\sum_{j\\in\\mathcal{T}_{i}}\\langle\\nabla_{w_{i}}L_{\\lambda},w_{j}-q_{i j}\\pmb{w}_{i}^{*}\\rangle=\\Omega(\\zeta).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "image", "img_path": "XYw051ZmUn/tmp/dda52e0a3262b9fcc088ee3c15b4afb5e811a9e23217ed28dc10eeba02369581.jpg", "img_caption": ["Figure 1: Illustration of descent direction "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "In words, this descent direction is the following: we move neuron $w_{j}\\in{\\mathcal{T}}_{i}$ toward either ground-truth direction $\\pmb{w}_{i}^{*}$ or O depending on whether it is in the neighborhood of teacher neuron $\\pmb{w}_{i}^{*}$ . Specifically, we move far-away neurons towards O (and thus setting $q_{i j}=0$ ) and move close-by neurons towards its \u2032closest' minima $q_{i j}w_{i}^{*}$ (the fraction of $\\pmb{w}_{i}^{*}$ that neuron ${\\pmb w}_{j}$ should target to approximate). See Figure 1 for an illustration. ", "page_idx": 6}, {"type": "text", "text": "The proof of the above lemma requires a dedicated characterization of the low loss solution's structure, which we describe inSection 6. ", "page_idx": 6}, {"type": "text", "text": "6  Structure of (approximated) minima ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we first highlight the importance of understanding local geometry by showing the challenges in proving the existence of descent direction (Lemma 7). Then after presenting the main result of this section to show the structure of (approximated) minima (Lemma 8), we discuss several proof ideas such as dual certificate analysis in the remaining part. ", "page_idx": 6}, {"type": "text", "text": "6.1   Constructing descent direction requires better understanding of local geometry ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To show the existence of descent direction in Lemma 7, we compute the inner product between gradient and constructed descent direction. We can lower bound it by (assuming norms are balanced) ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\zeta+2\\sum_{i\\in[m_{*}]}\\sum_{j\\in{\\mathcal{T}}_{i}}\\mathbb{E}_{{\\pmb x}}[R({\\pmb x})a_{j}q_{i j}{\\pmb w}_{i}^{*\\top}{\\pmb x}(\\sigma^{\\prime}({\\pmb w}_{i}^{*\\top}{\\pmb x})-\\sigma^{\\prime}({\\pmb w}_{j}^{\\top}{\\pmb x}))],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $R({\\pmb x})=f({\\pmb x})-\\widetilde{f}_{*}({\\pmb x})$ is the residual. Thus, in order to get a lower bound, the goal is to show second term above is small than $\\zeta$ . As we can see, this term is quite complicated and can be viewed as the inner product between $R(x)$ and $\\begin{array}{r}{h(\\pmb{x})=\\sum_{i\\in[m_{*}]}\\sum_{j\\in\\mathcal{T}_{i}}\\dot{a}_{j}q_{i j}\\pmb{w}_{i}^{*\\dagger}\\pmb{x}(\\sigma^{\\prime}(\\pmb{w}_{i}^{*\\top}\\pmb{x})-\\sigma^{\\prime}(\\pmb{w}_{j}^{\\top}\\pmb{x}))}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "Average neuron and residual decomposition  To deal with above challenge, we use the idea of average neuron and residual decomposition. For each teacher neuron $\\pmb{w}_{i}^{*}$ denote $\\begin{array}{r}{\\pmb{v}_{i}=\\sum_{j\\in{T}_{i}}a_{j}\\pmb{w}_{j}}\\end{array}$ as the average neuron. Intuitively, this average neuron $\\pmb{v}_{i}$ stands for an idealize case where all neurons belong to $\\mathcal{T}_{i}$ (closer to $\\pmb{w}_{i}^{*}$ than other $\\pmb{w}_{j}^{*}$ ) collapse into a single neuron. ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{I}_{1}(x)=\\displaystyle\\frac{1}{2}\\sum_{i\\in[m_{*}]}\\hat{v}_{i}^{\\top}x\\operatorname{sign}({w_{i}^{*}}^{\\top}x),R_{2}(x)=\\displaystyle\\frac{1}{2}\\sum_{i\\in[m_{*}],j\\in T_{i}}a_{j}w_{j}^{\\top}x(\\operatorname{sign}(w_{j}^{\\top}x)-\\operatorname{sign}({w_{i}^{*}}^{\\top}x)),}\\\\ &{\\mathfrak{I}_{3}(x)=\\displaystyle\\frac{1}{\\sqrt{2\\pi}}\\left(\\sum_{i\\in[m_{*}]}a_{i}^{*}\\,\\|w_{i}^{*}\\|_{2}-\\sum_{i\\in[m]}a_{i}\\,\\|w_{i}\\|_{2}\\right)+\\alpha-\\hat{\\alpha}+(\\beta-\\hat{\\beta})^{\\top}x.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "$R_{1}$ can be thought as the exact-parametrization setting (use $m_{*}$ neurons to learn $m_{*}$ neurons), where the average neurons $\\{{v}_{i}\\}_{i=1}^{\\bar{m}_{*}}$ are the efective neurons. The difference between this exactparametrization and overparametrization setting is then characterized by the term $R_{2}$ , which captures the difference in nonlinear activation pattern. This term in fact suggests the loss landscape is degenerate in overparametrized case and slows down the convergence (Zhou et al., 2021; Xu and Du, 2023). Overall, this residual decomposition is similar to Zhou et al. (2021), with additional modification of $R_{3}$ to deal with ReLU activation and linear term $\\alpha,\\beta$ ", "page_idx": 6}, {"type": "text", "text": "To some extent, our residual decomposition can be viewed as a kind of \u201cbias-variance? decomposition in the sense that the \u2018bias\u2019 term $R_{1}$ captures the overall average contribution of all neurons, and the 'variance'term $R_{2}$ captures the individual contributions of each neuron that are not reflected in $R_{1}$ ", "page_idx": 7}, {"type": "text", "text": "High-level proof plan of Lemma 7 We now are ready to give a proof plan for Lemma 7. The key is to show properties of minima that can help us to bound $\\langle R,h\\rangle$ ", "page_idx": 7}, {"type": "text", "text": "1. Show that neurons mostly concentrate around ground-truth directions.   \n2. Show that average neuron $\\pmb{v}_{i}$ is close to teacher neuron $\\pmb{w}_{i}^{*}$ for all $i\\in[m]$   \n3. Use above structure to bound $\\langle R_{i},h\\rangle$ . Specifically, bounding $\\langle R_{1},h\\rangle$ relies on the fact that average neuron is close to teacher neuron (step 2); a bound on $\\langle R_{2},h\\rangle$ follows from far-away neurons are small (step 1); third term $\\langle R_{3},h\\rangle$ can be directly bounded using the loss. Detailed calculations are deferred into Appendix H.3. ", "page_idx": 7}, {"type": "text", "text": "We give main result of this section that shows the desired local geometry properties more precisely ((i)(ii) corresponding to step 1 and (ii) corresponding to step 2 above). ", "page_idx": 7}, {"type": "text", "text": "Lemma 8 (Informal). Suppose the optimality gap is $\\zeta$ wehave (i) Total norm of far-away neurons is small: $\\begin{array}{r}{\\sum_{i\\in[m_{*}]}\\sum_{j\\in{\\mathcal{T}}_{i}}|a_{j}|\\,\\|{\\pmb w}_{j}\\|_{2}\\,\\delta_{j}^{2}=O_{*}(\\zeta/\\lambda)}\\end{array}$ where angle $\\delta_{j}=\\angle(\\pmb{w}_{j},\\pmb{w}_{i}^{*})\\,f o r\\,\\pmb{w}_{j}$ that $j\\in\\mathcal T_{i}$ (i) For every $\\pmb{w}_{i}^{*}$ , there exists at least one close-by neuron w s.t. $\\angle(\\pmb{w},\\pmb{w}_{i}^{*})\\leq\\delta_{c l o s e}=O_{*}(\\zeta^{1/3}).$ (i Average neuron is close to teach neurons: we have $\\|\\pmb{v}_{i}-\\pmb{w}_{i}^{*}\\|_{2}\\leq O_{*}((\\zeta/\\lambda)^{3/4})$ ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "These properties give us a sense of what the network should look like when loss is small: neurons have large norm only if they are around the ground-truth directions. Moreover, when $\\zeta/\\lambda\\to0$ student neuron must align with one of teacher neurons $(\\delta_{j}=0)$ ) or norm becomes 0 $(|a_{j}|\\,\\|w_{j}\\|=0)$ This can be understood from the $\\ell_{1}$ regularized loss (equivalent to $\\ell_{2}$ regularization on both layers) that promotes the sparsity over the distribution of neurons. In the rest of this section, we discuss new techniques such as dual certificate that we develop for the proof. ", "page_idx": 7}, {"type": "text", "text": "6.2 Neurons concentrate around teacher neurons: dual certificate analysis and test function ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We focus on Lemma 8(i)(ii) here. We will use a dual certificate technique similar to Poon et al. (2023) to prove Lemma 8(i), and a more general construction of test function to prove Lemma 8(ii). In below, we consider a relaxed version of original optimization problem (2) by allowing infinite number of neurons, i.e., distribution of neurons, with $\\sigma_{\\geq2}(x)=\\mathrm{ReLU}(x)-1/\\sqrt{2\\pi}-x/2$ instead ofReLU: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mu\\in\\mathcal{M}(\\mathbb{S}^{d-1})}L_{\\lambda}(\\mu):=L(\\mu;\\sigma_{\\geq2})+\\lambda|\\mu|_{1},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Wwhere $\\mu_{\\lambda}^{*}$ is the minimizer. We use $\\sigma_{\\geq2}$ activation because this is the effective activation when linear terms $\\alpha,\\beta$ are perfectly fitted (remove Oth and 1st order Hermite expansion of ReLU, see Claim B.1 and (6) in appendix). ", "page_idx": 7}, {"type": "text", "text": "This is the loss function we would have in the idealized setting: (1) linear term $\\alpha,\\beta$ reach their global minima (this is easy to achieve as loss is convex in them); (2) use $\\ell_{1}$ regularization instead of $\\ell_{2}$ regularization, since this is the case when the first and second layer norm are balanced (weight decay encourages this to happen). Note that the results in this part can handle almost all activation as long as its Hermite expansion is well-defined, generalizing Zhou et al. (2021) that can only handle absolute/ReLU activation. In below we will focus on the activation $\\sigma_{\\geq2}$ for simplicity. ", "page_idx": 7}, {"type": "text", "text": "Dual certificate  This optimization problem (5) can be viewed as a natural extension of the classical compressed sensing problem (Donoho, 2006; Candes et al., 2006) and Lasso-type problem (Tibshirani, 1996) in the infinite dimensional space, which has been studied in recent years (Bach, 2017; Poon et al., 2023). One common way is to study its dual problem. The dual solution $p_{0}(x)$ (maps $\\mathbb{R}^{d}$ to $\\mathbb{R}$ of (5) when $\\lambda=0$ satisfies $\\mathbb{E}_{\\pmb{x}}[p_{0}(\\pmb{x})\\sigma_{\\geq2}(\\pmb{w}^{\\top}\\pmb{x})]\\in\\partial|\\mu_{*}|(\\mathbb{S}^{d-1})$ (more detailed discussions on this dual problem can be found in e.g., Poon et al. (2023)). Here $\\eta(\\pmb{w})=\\mathbb{E}_{\\pmb{x}}[p(\\pmb{x})\\sigma_{\\geq2}(\\pmb{w}^{\\top}\\pmb{x})]$ is often called dual certificate, as it serves as a certificate of whether a solution $\\mu$ is optimal. Its meaning will be clear in the discussions below. ", "page_idx": 7}, {"type": "text", "text": "We now introduce the notion of non-degenerate dual certificate, motivated by Poon et al. (2023). Note that the condition $\\eta(\\pmb{w})\\in\\partial|\\mu_{*}|(\\mathring{\\mathbb{S}^{d-1}})$ implies that $\\eta(\\pmb{w}_{i}^{*})=\\mathrm{sign}(a_{i}^{*})$ and $\\|\\eta\\|_{\\infty}\\leq1$ .The following definition is a slightly stronger version of the above implications as it requires $\\eta$ to decay at least quadratic when moves away from $\\pmb{w}_{i}^{*}$ ", "page_idx": 8}, {"type": "text", "text": "Definition 1 (Non-degenerate dual certificate). $\\eta(w)$ is called a non-degenerate dual certificate if there exists $p(x)$ suchthat $\\eta(\\pmb{w})=\\mathbb{E}_{\\pmb{x}}[p(\\pmb{x})\\sigma_{\\ge2}(\\pmb{w}^{\\top}\\pmb{x})]_{J}$ for $\\pmb{w}\\in\\mathbb{S}^{d-1}$ and ", "page_idx": 8}, {"type": "image", "img_path": "XYw051ZmUn/tmp/ea1b42ea9113c5df5b7f6ab1e4c85058de9d2737e5e0b343dec186808b8f59d2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "The existence and construction of the non-degenerate dual certificate is deferred to Appendix G. We focus on the implications of such non- Figure 2: Dual certificate degenerate dual certificate below. $\\eta$ ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Roughly speaking, the dual certificate only focuses on the position of ground-truth directions $\\pmb{w}_{i}^{*}$ as it decays fast when moving away from these directions (Figure 2). Thus, if $\\mu$ exactly recovers ground-truth $\\mu_{*}$ , then we have $\\langle\\eta,\\dot{\\mu}_{*}\\rangle=|\\mu_{*}|_{1}$ . The gap between $\\langle\\eta,\\mu\\rangle$ and $|\\mu|_{1}$ is large when $\\mu$ is away from $\\mu_{*}$ . Therefore, $\\eta$ can be viewed as a certificate to test the optimality of $\\mu$ . The lemma below makes it more precise. ", "page_idx": 8}, {"type": "text", "text": "Lemma 9. Given a non-degenerate dual certificate $\\eta,$ then ", "page_idx": 8}, {"type": "equation", "text": "$\\begin{array}{r}{\\mu\\in\\mathcal{M}(\\mathbb{S}^{d-1}),\\,|\\langle\\eta,\\mu\\rangle|\\leq|\\mu|_{1}-\\rho_{\\eta}\\sum_{i\\in[m_{*}]}\\int_{\\mathcal{T}_{i}}\\delta(\\boldsymbol{w},\\boldsymbol{w}_{i}^{*})^{2}\\,\\mathrm{d}|\\mu|(\\boldsymbol{w}).}\\end{array}$ ", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "In the fnite width case, we have $\\begin{array}{r}{\\sum_{i\\in[m_{*}]}\\int_{\\mathcal{T}_{i}}\\delta(\\pmb{w},\\pmb{w}_{i}^{*})^{2}\\,\\mathrm{d}|\\mu|(\\pmb{w})=\\sum_{i}|a_{i}|\\,\\|\\pmb{w}_{i}\\|\\,\\delta_{i}^{2}}\\end{array}$ . This is exactly the quantity that we are interested in Lemma 8. ", "page_idx": 8}, {"type": "text", "text": "To see thusefules ofLmma9we showa proof fortotal nrmboudof theoptmal olut $\\mu_{\\lambda}^{\\ast}$ The proof for general $\\mu$ with optimality gap $\\zeta$ is similar (Lemma F.5). ", "page_idx": 8}, {"type": "text", "text": "Claim 1 Lemma 8(i) for $\\mu_{\\lambda}^{*}$ $\\begin{array}{r}{\\sum_{i\\in[m_{*}]}\\int_{\\mathcal T_{i}}\\delta(\\pmb{w},\\pmb{w}_{i}^{*})^{2}\\,\\mathrm{d}|\\mu_{\\lambda}^{*}|(\\pmb{w})\\leq O_{*}(\\lambda)}\\end{array}$ ", "page_idx": 8}, {"type": "text", "text": "Proof. It is not hard to show $|\\mu_{\\lambda}^{*}|_{1}\\leq|\\mu^{*}|_{1}$ (Lemma F.3) so we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\lvert\\mu_{\\lambda}^{*}\\rvert_{1}-\\lvert\\mu^{*}\\rvert_{1}-\\langle\\eta,\\mu_{\\lambda}^{*}-\\mu^{*}\\rangle\\leq-\\langle\\eta,\\mu_{\\lambda}^{*}-\\mu^{*}\\rangle.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Using Lemma 9 and the fact $L(\\mu_{\\lambda}^{*})=O_{*}(\\lambda^{2})$ from Lemma F.3, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathrm{LHS}=|\\mu_{\\lambda}^{*}|_{1}-\\langle\\eta,\\mu_{\\lambda}^{*}\\rangle\\geq\\rho_{\\eta}\\sum_{i\\in[m_{*}]}\\int_{T_{i}}\\delta(\\pmb{w},\\pmb{w}_{i}^{*})^{2}\\,\\mathrm{d}|\\mu_{\\lambda}^{*}|(\\pmb{w}),\\quad\\mathrm{RHS}\\leq\\|p\\|_{2}\\,\\sqrt{L(\\mu_{\\lambda}^{*})}=O_{*}(\\lambda).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Test function  The idea of using test function is to identify certain properties of the target function/distribution that we are interested in. Specifically, we construct test function so that it only correlates well with the target function that has the desired property. Generally speaking, the dual certificate above can be consider as a specific case of a test function: the correlation between dual certificate $\\eta$ and distribution of neurons $\\mu$ is large (reach $|\\mu|_{1})$ Only when $\\mu\\approx\\mu_{*}$ ", "page_idx": 8}, {"type": "text", "text": "In below, we use this test function idea to show that every ground-truth direction has close-by neuron (Lemma 8(ii)). Denote $T_{i}(\\delta):=\\{j:\\angle({\\pmb w}_{j},{\\pmb w}_{i})\\leq\\delta\\}\\cap\\dot{T}_{i}$ as the neurons that are $\\delta$ -closeto $\\pmb{w}_{i}^{*}$ ", "page_idx": 8}, {"type": "text", "text": "Lemma 10 (Lemma 8(i), informal). Given the optimality gap $\\zeta$ we have the total mass near each target direction is large, i.e., $\\mu(\\mathcal{T}_{i}(\\delta))\\,\\mathrm{sign}(a_{i}^{*})\\geq|a_{i}^{*}|/2$ forall $i\\in[m_{*}]$ andany $\\delta\\geq\\Theta_{*}\\left(\\zeta^{1/3}\\right)$ ", "page_idx": 8}, {"type": "text", "text": "Note that although the results in the dual certificate part (Lemma 9(i)) can imply that there are neurons close to teacher neurons, the bound we get here using carefully designed test function are sharper( $\\zeta^{1/3}$ Vs. $\\zeta^{1/4}$ ). This is in fact important to the descent direction construction (Lemma 7). ", "page_idx": 8}, {"type": "text", "text": "In the proof, we view the residual $R({\\pmb x})=f_{\\mu}({\\pmb x})-f_{*}({\\pmb x})$ as the target function and construct test function that will only have large correlation if there is a teacher neuron that have no close student neurons. Specifically, the test function $g$ only consists of high-order Hermite polynomial such that it is large around the ground-truth direction and decays fast when moving away (Figure 3). It looks like a single spike in dual certificate n hnt in fart decavc much facter than $\\eta$ when movino awavItic more flexible to choose test function than dual certificate, so test function $g$ can focus only on a local region of one ground-truth direction and give a better guarantee than dual certificate analysis. ", "page_idx": 9}, {"type": "image", "img_path": "XYw051ZmUn/tmp/d6acb87e4dc2eaa1093222b54f2ba697f12efe8b85abbba66c32f3bf37e5262e.jpg", "img_caption": ["Figure 3: Test function $g$ "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6.3 Average neuron is close to teacher neuron: residual decomposition and average neuron ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We give the proof idea for Lemma 8(i) that shows average neuron $\\pmb{v}_{i}$ is close to teacher neuron $\\pmb{w}_{i}^{*}$ using the residual decomposition $R=R_{1}+R_{2}+R_{3}$ ", "page_idx": 9}, {"type": "text", "text": "The key is to observe that $R_{1}$ is an analogue to exact-parametrization case where loss is often strongly-convex, so we have $\\begin{array}{r}{\\|R_{1}\\|_{2}^{2}=\\Omega_{*}(1)\\stackrel{}{\\sum}_{i}\\|v_{i}-\\pmb{w}_{i}^{\\Bar{*}}\\|_{2}^{2}}\\end{array}$ Then the goal is to upper bound $\\|R_{1}\\|$ Given the decomposition $R=R_{1}+R_{2}+R_{3}$ , it is easy to bound $\\|{\\bar{R_{1}}}\\|\\leq\\|R\\|{\\bar{+}}\\|R_{2}\\|+\\|R_{3}\\|$ We focus on $\\|R_{2}\\|$ as the other two are not hard to bound (loss is small in local regime). $R_{2}$ is in fact closely related with the total weighted norm bound in Lemma 8: we show $\\|R_{2}\\|\\;=$ $\\begin{array}{r}{O_{*}(1)\\left(\\sum_{j\\in\\mathcal{T}_{i}}|a_{j}|\\,\\|w_{j}\\|_{2}\\,\\delta_{j}^{2}\\right)^{3/2}\\,=\\,O_{*}((\\zeta/\\lambda)^{3/2})}\\end{array}$ . Thus, we get a bound for $\\lVert\\pmb{v}_{i}-\\pmb{w}_{i}^{*}\\rVert$ . See Appendix F.1.4 for details. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper we showed that gradient descent converges in a large local region depending on the complexity of the teacher network, and the local convergence allows 2-layer networks to perform a strong notion of feature learning (matching the directions of ground-truth teacher networks). We hope our result gives a better understanding of why gradient-based training is important for feature learning in neural networks. Our results rely on adding standard weight decay and new constructions of dual certificate and test functions, which can be helpful in understanding local optimization landscape in other problems. A natural but challenging next step is to understand whether the intermediate steps are also important for feature learning. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Rong Ge and Mo Zhou are supported by NSF Award DMS-2031849 and CCF-1845171 (CAREER). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Emmanuel Abbe, Enric Boix-Adsera, Matthew S Brennan, Guy Bresler, and Dheeraj Nagaraj. The staircase property: How hierarchical structure can guide deep learning. Advances in Neural Information Processing Systems, 34:26989-27002,2021. ", "page_idx": 9}, {"type": "text", "text": "Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks. In Conference on Learning Theory, pages 4782-4887. PMLR, 2022. ", "page_idx": 9}, {"type": "text", "text": "Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. Sgd learning on neural networks: leap complexity and saddle-to-saddle dynamics. In The Thirty Sixth Annual Conference on Learning Theory, pages 2552-2623. PMLR, 2023. ", "page_idx": 9}, {"type": "text", "text": "P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix manifolds. Princeton University Press, 2008. ", "page_idx": 9}, {"type": "text", "text": "P-A Absil, Robert Mahony, and Jochen Trumpf. An extrinsic look at the riemannian hessian. In International conference on geometric science of information, pages 361-368. Springer, 2013. ", "page_idx": 9}, {"type": "text", "text": "Shunta Akiyama and Taiji Suzuki. On learnability via gradient method for two-layer relu neural networks in teacher-student setting. In International Conference on Machine Learning, pages 152-162.PMLR, 2021. ", "page_idx": 9}, {"type": "text", "text": "Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neural networks, going beyond two layers. Advances in neural information processing systems, 32, 2019a.   \nZeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via overparameterization. In International conference on machine learning, pages 242-252, 2019b.   \nSanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact computation with an infnitely wide neural net. Advances in neural information processing systems, 32, 2019.   \nGerard Ben Arous, Reza Gheissari, and Aukosh Jagannath. Online stochastic gradient descent on non-convex losses from high-dimensional inference. The Journal of Machine Learning Research, 22(1):4788-4838, 2021.   \nJimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang. Highdimensional asymptotics of feature learning: How one gradient step improves the representation. Advances in Neural Information Processing Systems, 35:37932-37946, 2022.   \nFrancis Bach. Breaking the curse of dimensionality with convex neural networks. Journal of Machine Learning Research, 18(19):1-53, 2017.   \nYu Bai and Jason D. Lee. Beyond linearization: On quadratic and higher-order approximation of wide neural networks. In International Conference on Learning Representations, 2020.   \nBoaz Barak, Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang. Hidden progress in deep learning: Sgd learns parities near the computational limit. Advances in Neural Information Processing Systems, 35:21750-21764, 2022.   \nRaphael Berthier, Andrea Montanari, and Kangjie Zhou. Learning time-scales in two-layers neural networks. arXiv preprint arXiv:2303.00055, 2023.   \nAlberto Bietti, Joan Bruna, Clayton Sanford, and Min Jae Song. Learning single-index models with shallow neural networks. Advances in Neural Information Processing Systems, 35:9768-9783, 2022.   \nAlberto Bietti,Joan Bruna, and Loucas Pillaud-Vivien. On learning gaussian multi-index models with gradient flow. arXiv preprint arXiv:2310.19793, 2023.   \nEmmanuel J Candes, Justin Romberg, and Terence Tao. Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information. IEEE Transactions on information theory, 52(2):489-509, 2006.   \nLenaic Chizat. Sparse optimization on measures with over-parameterized gradient descent. Mathematical Programming, 194(1):487-532, 2022.   \nLenaic Chizat and Francis Bach.On the global convergenceof gradient descent forover-parameterized models using optimal transport. In Advances in neural information processing systems, pages 3036-3046, 2018.   \nLenaic Chizat, Edouard Oyallon, and Francis Bach On lazy training in diffeentiable programmng. In Advances in Neural Information Processing Systems,pages 2933-2943, 2019.   \nHugo Cui, Luca Pesce, Yatin Dandi, Florent Krzakala, Yue Lu, Lenka Zdeborova, and Bruno Loureiro. Asymptotics of feature learning in two-layer networks after one gradient-step. In Forty-first International Conference on Machine Learning, 2024.   \nAlexandru Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent. In Conference on Learning Theory, pages 5413-5452. PMLR, 2022.   \nYatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. How two-layer neural networks learn, one (giant) step at a time. In NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning, 2023.   \nAmit Daniely and Eran Malach. Learning parities with neural networks. Advances in Neural Information Processing Systems, 33:20356-20365, 2020.   \nDavidL Donoho. Compressed sensing. IEEE Transactions on information theory, 52(4):1289-1306, 2006.   \nSimon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. In International Conference on Learning Representations, 2019.   \nSpencer Frei, Yuan Cao, and Quanquan Gu. Agnostic learning of a single neuron with gradient descent. Advances in Neural Information Processing Systems, 33:5417-5428, 2020.   \nRong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landcape design. In International Conference on Learning Representations, 2018.   \nRong Ge, Yunwei Ren, Xiang Wang, and Mo Zhou. Understanding deflation process in overparametrized tensor decomposition. Advances in Neural Information Processing Systems, 34: 1299-1311, 2021.   \nMargalit Glasgow. SGD finds then tunes features in two-layer neural networks with near-optimal sample complexity: A case study in the XOR problem. In The Twelfth International Conference on Learning Representations, 2024.   \nSurbhi Goel, Aravind Gollakota, Zhihan Jin, Sushrut Karmalkar, and Adam Klivans. Superpolynomial lower bounds for learning one-layer neural networks using gradient descent. In International Conference on Machine Learning, pages 3587-3596. PMLR, 2020.   \nArthur Jacot, Franck Gabriel, and Clment Hongler. Neural tangent kernel: Convergence and generalization in eural networks. InAdvances in neural information processing systems, pages 8571-8580, 2018.   \nYuanzhi Li, Tengyu Ma, and Hongyang R Zhang. Learning over-parametrized two-layer neural networks beyond ntk. In Conference on learning theory, pages 2613-2682. PMLR, 2020.   \nStanislaw Lojasiewicz. Une propriete topologique des sous-ensembles analytiques r\u00e9els. Les \u00e9quations aux derivees partielles, 117:87-89, 1963.   \nArvind Mahankali, Haochen Zhang, Kefan Dong, Margalit Glasgow, and Tengyu Ma. Beyond ntk with vanilla gradient descent: A mean-field analysis of neural networks with polynomial width, samples, and time. Advances in Neural Information Processing Systems, 36, 2024.   \nSong Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean feld view of the landscape of twolayer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665-E7671, 2018.   \nBehrad Moniri, Donghwan Lee, Hamed Hassani, and Edgar Dobriban. A theory of non-linear feature learning with one gradient step in two-layer neural networks. In Forty-first International Conference on Machine Learning, 2024.   \nAlireza Mousavi-Hosseini, Sejun Park, Manuela Girotti, Ioannis Mitliagkas, and Murat A Erdogdu. Neural networks efficiently learn low-dimensional representations with SGD. In The Eleventh International Conference on Learning Representations, 2023.   \nEshan Nichani, Alex Damian, and Jason D Lee. Provable guarantees for nonlinear feature learning in three-layer neural networks. Advances in Neural Information Processing Systems, 36, 2024a.   \nEshaan Nichani, Alex Damian, and Jason D Lee. How transformers learn causal structure with gradient descent. arXiv preprint arXiv:2402.14735, 2024b.   \nRyan O'Donnell. Analysis of boolean functions. arXiv preprint arXiv:2105.10386, 2021.   \nClarice Poon, Nicolas Keriven, and Gabriel Peyre. The geomety of offthe-grid compressed sensing. Foundations of Computational Mathematics, 23(1):241-327, 2023.   \nItay M Safran, Gilad Yehudai, and Ohad Shamir. The effects of mild over-parameterization on the optimization landcapeof shallow relu neural networks. In Conference on Learning Theory, paes 3889-3934. PMLR, 2021.   \nZhenmei Shi, Junyi Wei, and Yingyu Liang. A theoretical analysis on feature learning in neural networks: Emergence from inputs and advantage over fixed features. In International Conference on Learning Representations, 2022.   \nMahdi Soltanolkotabi. Learning relus via gradient descent. Advances in neural information processing systems, 30, 2017.   \nTaiji Suzuki, Denny Wu, Kazusato Oko, and Atsushi Nitanda. Feature learning via mean-field langevin dynamics: classifying sparse parities and beyond. Advances in Neural Information Processing Systems, 36, 2024.   \nRobert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society Series B: Statistical Methodology, 58(1):267-288, 1996.   \nZihao Wang, Eshaan Nichani, and Jason D. Lee. Learning hierarchical polynomials with three-layer neural networks. In The Twelfth International Conference on Learning Representations, 2024.   \nLei Wu. Learning a single neuron for non-monotonic activation functions. In International Conference on Artificial Intelligence and Statistics, pages 4178-4197. PMLR, 2022.   \nWeihang Xu and Simon Du. Over-parameterization exponentially slows down gradient descent for learning a single neuron. In The Thirty Sixth Annual Conference on Learning Theory, pages 1155-1198. PMLR, 2023.   \nGilad Yehudai and Shamir Ohad. Learning a single neuron with gradient methods. In Conference on Learning Theory, pages 3756-3786. PMLR, 2020.   \nGilad Yehudai and Ohad Shamir. On the power and limitations of random features for understanding neural networks. Advances in Neural Information Processing Systems, 32, 2019.   \nMo Zhou, Rong Ge, and Chi Jin. A local convergence theory for mildly over-parameterized two-layer neural network. In Conference on Learning Theory, pages 4577-4632. PMLR, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A  Some properties of Hermite polynomials ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we give several properties of Hermite polynomials that are useful in our analysis. See O'Donnell (2021) for a more complete discussion on Hermite polynomials. Let $H_{k}$ be the probabilists\u2019 Hermite polynomial where ", "page_idx": 13}, {"type": "equation", "text": "$$\nH_{k}(x)=(-1)^{k}e^{x^{2}/2}{\\frac{\\mathrm{d}^{k}}{\\mathrm{d}x^{k}}}(e^{-x^{2}/2})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and $\\begin{array}{r}{h_{k}=\\frac{1}{\\sqrt{k!}}H_{k}}\\end{array}$ be the normalized Hermite polynomials. ", "page_idx": 13}, {"type": "text", "text": "Hermite polynomials are classical orthogonal polynomials, which means $\\mathbb{E}_{x\\sim N(0,1)}[h_{m}(x)h_{n}(x)]=$ 1if $m\\,=\\,n$ and otherwise O. Given a function $\\sigma$ , we call $\\begin{array}{r}{\\sigma(x)=\\sum_{k=0}^{\\infty}\\hat{\\sigma}_{k}\\dot{h}_{k}(x)}\\end{array}$ as the Hermit expansion of $\\sigma$ and $\\hat{\\sigma}_{k}=\\mathbb{E}_{x\\sim N(0,1)}[\\sigma(x)h_{k}(x)]$ as the $k$ -th Hermite coefficient of $\\sigma$ ", "page_idx": 13}, {"type": "text", "text": "The following is a useful property of Hermite polynomial ", "page_idx": 13}, {"type": "text", "text": "Claim A.1 (O'Donnell, 2021), Section 11.2). Let $(x,y)$ be $\\rho$ -correlatedstandardnormal variables (that is, both $x,y$ have marginal distribution $N(0,1)$ and $\\mathbb{E}[x y]\\,=\\,\\rho)$ .Then, $\\mathbb{E}[h_{m}(x)h_{n}(y)]\\,=$ $\\rho^{n}\\delta_{m n}$ ,where $\\delta_{m n}=1\\;i f m=n$ andotherwise $\\boldsymbol{O}$ ", "page_idx": 13}, {"type": "text", "text": "The following lemma gives the Hermite coefficients for absolute value function and ReLU. ", "page_idx": 13}, {"type": "text", "text": "Lemma A.1. Let $\\hat{\\sigma}_{k}=\\mathbb{E}_{x\\sim N(0,1)}[\\sigma(x)h_{k}(x)]$ be the Hermite coeffcient of $\\sigma$ .For $\\sigma$ is ReLU or absolute function, we have $|\\hat{\\sigma}_{k}|=\\Theta(k^{-5/4})$ ", "page_idx": 13}, {"type": "text", "text": "Proof. From Goel et al. (2020); Zhou et al. (2021) we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\sigma}_{a b s,k}=\\left\\{\\begin{array}{l l}{0}&{,k\\mathrm{~is~odd}}\\\\ {\\sqrt{2/\\pi}}&{,k=0}\\\\ {(-1)^{\\frac{k}{2}-1}\\sqrt{\\frac{2}{\\pi}}\\frac{(k-2)!}{\\sqrt{k!}2^{k/2-1}(k/2-1)!}}&{,k\\mathrm{~is~even~and~}k\\geq2}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\hat{\\sigma}_{r e l u,k}=\\left\\{\\begin{array}{l l}{0}&{,k\\mathrm{~is~odd~and~}k\\geq3}\\\\ {\\sqrt{1/2\\pi}}&{,k=0}\\\\ {1/2}&{,k=1}\\\\ {(-1)^{\\frac{k}{2}-1}\\sqrt{\\frac{1}{2\\pi}}\\frac{(k-2)!}{\\sqrt{k!2^{k/2-1}(k/2-1)!}}}&{,k\\mathrm{~is~even~and~}k\\geq2}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Using Stirling's formula, we get $|\\hat{\\sigma}_{a b s,k}|,|\\hat{\\sigma}_{r e l u,k}|=\\Theta(k^{-5/4})$ ", "page_idx": 13}, {"type": "text", "text": "B  Useful facts and proof of Theorem 2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section we provide several useful facts and present the proof of Theorem 2. ", "page_idx": 13}, {"type": "text", "text": "The following claim shows that the square loss can be decomposed into 3 terms, where $\\alpha,\\beta$ are corresponding to Oth and 1st order of Hermite expansion. The effective activation is in fact $\\sigma_{\\geq2}$ as definedbelow. ", "page_idx": 13}, {"type": "text", "text": "Claim B.1. Denote $\\begin{array}{r}{\\hat{\\alpha}=-(1/\\sqrt{2\\pi})\\sum_{i=1}^{m}a_{i}\\,\\|\\pmb{w}_{i}\\|_{2},\\,\\hat{\\pmb{\\beta}}=-(1/2)\\sum_{i=1}^{m}a_{i}\\pmb{w}_{i}.}\\end{array}$ We have square loss ", "page_idx": 13}, {"type": "equation", "text": "$$\nL(\\pmb\\theta)=\\lvert\\alpha-\\hat{\\alpha}\\rvert^{2}+\\left\\lVert\\beta-\\hat{\\beta}\\right\\rVert_{2}^{2}+\\mathbb{E}_{\\pmb x}[(f_{\\geq2}(\\pmb x)-\\widetilde{f}_{*}(\\pmb x))^{2}]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Where $\\begin{array}{r}{f_{\\geq2}(\\pmb{x};\\pmb{\\theta})=\\sum_{i\\in[m]}a_{i}\\sigma_{\\geq2}(\\pmb{w}_{i}^{\\top}\\pmb{x})}\\end{array}$ and $\\sigma_{\\geq2}(x)=\\sigma(x)-1/\\sqrt{2\\pi}-x/2$ is the activation that after removing Oth and ist order term in Hermite expansion. ", "page_idx": 13}, {"type": "text", "text": "As a result, when $\\alpha,\\beta$ are perfectly fitted and norms are balanced we have ", "page_idx": 13}, {"type": "equation", "text": "$$\nL_{\\lambda}(\\pmb\\theta)=\\mathbb{E}_{\\pmb x}[(f_{\\geq2}(\\pmb x)-\\widetilde{f}_{*}(\\pmb x))^{2}]+\\lambda\\sum_{i\\in[m]}.|a_{i}|\\,\\|\\pmb w_{i}\\|_{2}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. Following Ge et al. (2018), we can write the loss $L(\\theta)$ as a sum of tensor decomposition problem using Hermite expansion as in Section A (recall $\\lVert\\pmb{w}_{i}^{*}\\rVert_{2}=1$ and preprocessing procedure removes the O-th and 1-st order term in the Hermite expansion of $\\sigma$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\Sigma}_{\\mathbf{\\theta}}(\\pmb{\\theta})=\\mathbb{E}_{x}\\left[\\left(\\displaystyle\\sum_{i\\in[m]}a_{i}\\,\\|\\pmb{w}_{i}\\|_{2}\\sum_{k\\geq0}\\hat{\\sigma}_{k}h_{k}(\\overline{{\\boldsymbol{w}}}_{i}^{\\top}\\pmb{x})+\\alpha+h_{1}(\\beta^{\\top}\\pmb{x})-\\displaystyle\\sum_{i\\in[m]}a_{i}^{*}\\,\\|\\pmb{w}_{i}^{*}\\|_{2}\\sum_{k\\geq2}\\hat{\\sigma}_{k}h_{k}(\\pmb{w}_{i}^{*\\top}\\pmb{x})\\right.\\right.}\\\\ &{\\left.\\left.\\quad=\\left|\\alpha+\\hat{\\sigma}_{0}\\sum_{i\\in[m]}a_{i}\\,\\|\\pmb{w}_{i}\\|_{2}\\right|^{2}+\\left\\|\\beta+\\hat{\\sigma}_{1}\\sum_{i\\in[m]}a_{i}\\pmb{w}_{i}\\right\\|_{2}^{2}\\right.\\right.}\\\\ &{\\left.\\left.\\quad\\quad+\\sum_{k\\geq2}\\hat{\\sigma}_{k}^{2}\\left\\|\\sum_{i\\in[m]}a_{i}\\,\\|\\pmb{w}_{i}\\|_{2}\\,\\overline{{\\boldsymbol{w}}}_{i}^{\\beta\\pmb{k}}-\\displaystyle\\sum_{i\\in[m]}a_{i}^{*}\\,\\|\\pmb{w}_{i}^{*}\\|_{2}\\,\\pmb{w}_{i}^{*\\beta}\\right\\|_{F}^{2}.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that $\\hat{\\sigma}_{0}=1/\\sqrt{2\\pi},\\,\\hat{\\sigma}_{1}=1/2$ as in Lemma A.1, we get the result. ", "page_idx": 14}, {"type": "text", "text": "The proof of main result Theorem 2 is simply a combination of few lemmas appear in other sections.   \nWe refer the detailed proof and discussion to their corresponding sections. ", "page_idx": 14}, {"type": "text", "text": "Theorem 2 (Main result). Under Assumption 1, 2, 3, consider Algorithm $^{\\,l}$ on loss (2). There exists $a$ schedule of weight decay $\\lambda_{t}$ and step size $\\eta_{t}$ such that given $m\\ge m_{0}=\\widetilde O_{*}(1)\\cdot(1/\\varepsilon_{0})^{O(r)}$ neurons withsmallenough $\\varepsilon_{0}=\\Theta_{\\ast}(1)$ , with high probability we will recover the target network $L(\\pmb\\theta)\\leq\\varepsilon$ within time $T=O_{*}(1/\\eta\\varepsilon^{2})$ where $\\eta=\\mathrm{poly}(\\varepsilon,1/d,1/m)$ ", "page_idx": 14}, {"type": "text", "text": "Moreover, when $\\varepsilon\\rightarrow0$ every student neuron $\\pmb{w}_{i}$ either aligns with one of teacher neuron $\\pmb{w}_{j}^{*}$ as $\\angle(\\pmb{w}_{i},\\pmb{w}_{j}^{*})=0$ orvanishes as $|a_{i}|=\\|\\pmb{w}_{i}\\|=0$ ", "page_idx": 14}, {"type": "text", "text": "Proof. Combine Lemma 3 (Stage 1), Lemma 4 (Stage 2) and Lemma 5 (Stage 3) together and follow thechoiceof $\\lambda_{t}$ and $\\eta_{t}$ weget theresult. ", "page_idx": 14}, {"type": "text", "text": "For the student neurons? alignment, it is a direct corollary from Lemma F.6 and Lemma F.5. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "C  Stage 1: first gradient step ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we show that after the first gradient update the first layer weights $w_{1},\\dots,w_{m}$ form a $\\varepsilon_{0}$ -net of the target subspace $S_{*}$ ,given $m=(1/\\varepsilon_{0})^{O(r)}$ neurons. The proof is deferred to Section C.1. ", "page_idx": 14}, {"type": "text", "text": "Lemma 3 (Stage 1). Under Assumption 1,2,3, consider Algorithm 1 with $\\lambda_{0}~=~\\eta_{0}~=~1$ and $m\\ge m_{0}=\\widetilde O_{*}(1)\\cdot(1/\\varepsilon_{0})^{O(r)}$ withany $\\varepsilon_{0}=\\Theta_{\\ast}(1)$ After first step, with probability $1-\\delta$ wehave (i) for every teacher neuron $\\pmb{w}_{i}^{*}$ , there exists at least one student neuron ${\\pmb w}_{j}$ s.t. $\\angle(\\pmb{w}_{i}^{*},\\pmb{w}_{j})\\leq\\varepsilon_{0}$ (i) l = 0(1), l|\u2264 0(1/m) foral $i\\in[m_{*}],$ $\\alpha_{1}=0$ and $\\beta_{1}=\\mathbf{0}$ ", "page_idx": 14}, {"type": "text", "text": "The proof relies on the following lemma from Damian et al. (2022) that shows after the first step update $\\boldsymbol{w}_{i}$ 's are located at positions as if they are sampled within the target subspace $S_{*}$ ", "page_idx": 14}, {"type": "text", "text": "Lemma C.1 (Lemma 4, Damian et al. (2022)). Under Assumption $^3$ wehavewithhighprobability inthe $\\ell_{2}$ normsense ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\pmb{w}_{i}^{(1)}=-\\eta_{0}\\nabla_{\\pmb{w}_{i}}L(\\pmb{a}^{(0)},\\pmb{W}^{(0)})=-2\\eta_{0}\\pmb{a}_{i}^{(0)}\\left(\\hat{\\sigma}_{2}^{2}\\pmb{H}\\pmb{\\overline{{{w}}}}_{i}\\pm\\widetilde{O}(\\frac{\\sqrt{r}}{d})\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\hat{\\sigma}_{k}:=\\mathbb{E}_{{\\pmb x}}[\\sigma({\\pmb x})h_{k}({\\pmb x})]$ is the $k$ -th Hermite polynomial coeffcient. ", "page_idx": 14}, {"type": "text", "text": "C.1 Proofs in Section C ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We now are ready to give the proof of Lemma 3. ", "page_idx": 14}, {"type": "text", "text": "Lemma 3 (Stage 1). Under Assumption 1,2,3, consider Algorithm $^{\\,l}$ with $\\lambda_{0}~=~\\eta_{0}~=~1$ and $m\\ge m_{0}=\\widetilde{O}_{*}(\\bar{1})\\cdot(1/\\varepsilon_{0})^{O(r)}$ with any $\\varepsilon_{0}=\\Theta_{\\ast}(1)$ .After first step, with probability $1-\\delta$ we have (i) for every teacher neuron $\\pmb{w}_{i}^{*}$ , there exists at least one student neuron ${\\pmb w}_{j}$ s.t. $\\angle(\\pmb{w}_{i}^{*},\\pmb{w}_{j})\\leq\\varepsilon_{0}$ (i) $\\left\\|\\pmb{w}_{i}^{(1)}\\right\\|_{2}=\\Theta_{*}(1),$ $|a_{i}^{(1)}|\\leq O_{*}(1/\\sqrt{m})$ for all $i\\in[m_{*}]$ $\\alpha_{1}=0$ and $\\beta_{1}=\\mathbf{0}$ ", "page_idx": 15}, {"type": "text", "text": "Proof. We show them one by one. ", "page_idx": 15}, {"type": "text", "text": "Part (i)  From Lemma C.1 and the fact that $\\overline{{\\pmb{w}}}_{i}^{(0)}$ samples uniformly from unit sphere, we know the probability of $\\angle(\\overline{{\\pmb{w}}}_{i}^{(1)},\\pmb{w})$ for any given $\\mathbf{\\nabla}w$ is at least $\\Omega_{\\ast}\\left(\\varepsilon_{0}^{r}\\right)$ . Applying union bound we get the desired result. ", "page_idx": 15}, {"type": "text", "text": "Part (i)  We have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pmb{w}_{i}^{(1)}=-\\eta_{0}\\nabla_{\\pmb{w}_{i}}L(\\pmb{a}^{(0)},\\pmb{W}^{(0)})=\\pmb{a}_{i}^{(0)}\\mathbb{E}_{\\pmb{x}}[\\widetilde{f}_{*}(\\pmb{x})\\sigma^{\\prime}(\\pmb{w}_{i}^{\\top}\\pmb{x})\\pmb{x}]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For the norm bound, using Lemma C.1 we know ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sqrt{d}\\left(\\left\\|H\\overline{{\\pmb{w}}}_{i}^{(0)}\\right\\|_{2}-\\widetilde{O}(\\frac{\\sqrt{r}}{d})\\right)\\leq\\left\\|\\pmb{w}_{i}^{(1)}\\right\\|_{2}\\leq\\sqrt{d}\\left(\\left\\|\\pmb{H}\\overline{{\\pmb{w}}}_{i}^{(0)}\\right\\|_{2}+\\widetilde{O}(\\frac{\\sqrt{r}}{d})\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since ${\\pmb w}_{i}^{(0)}$ initaliefaiiswll can bound $|a_{i}^{(1)}|$ ", "page_idx": 15}, {"type": "text", "text": "Since we use a symmetric initialization and have preprocessed the data, it is easy to see $\\alpha,\\beta$ remains atO. ", "page_idx": 15}, {"type": "text", "text": "D Stage 2: reaching low loss ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Stage 2, we show that given the features learned in Stage 1 one can adjust the norms on top of it to reach low loss that enters the local convergence regime in Stage 3. ", "page_idx": 15}, {"type": "text", "text": "Procedure  We first specify the procedure to solve $\\begin{array}{r}{\\operatorname*{min}_{\\pmb{a}}\\operatorname*{min}_{\\alpha,\\beta}L(\\pmb{\\theta})+\\lambda\\sum_{i}\\|\\pmb{w}_{i}\\|_{2}\\left|a_{i}\\right|}\\end{array}$ .For $\\textbf{\\em a}$ at current point, we first solve the inner optimization problem, which is a linear regression on $\\alpha,\\beta$ From Claim B.1 we know the global minima is $({\\hat{\\alpha}},{\\hat{\\beta}})$ . For simplicity of the proof, we just directly set $(\\alpha,\\beta)=({\\hat{\\alpha}},{\\hat{\\beta}})$ . Then given the $\\alpha,\\beta$ the outer optimization is a convex optimization for $\\textbf{\\em a}$ which can also be solved efficiently. Specifically, we perform 1 step of (sub)gradient on the loss function. We repeat the above 2 steps until convergence. ", "page_idx": 15}, {"type": "text", "text": "From Claim B.1 we know the actual objective that we optimize is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\widetilde{L}_{1,\\lambda}(\\pmb{a})=\\mathbb{E}_{\\pmb{x}}[(\\pmb{a}^{\\top}\\sigma_{\\geq2}(\\pmb{W}\\pmb{x})-\\widetilde{\\pmb{y}})^{2}]+\\lambda\\sum_{i}\\|\\pmb{w}_{i}\\|_{2}\\,|a_{i}|.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The following lemma shows that after Stage 2 we reach a low loss solution given the first layer features learned after first gradient step. The proof requires $\\eta$ to be small enough that depends on $1/m$ , mostly due to the large gradient norm. We believe using more advance algorithm for this type of problem can alleviate this issue. However, as this is not the focus of this paper, we omit it for simplicity. ", "page_idx": 15}, {"type": "text", "text": "Lemma 4 (Stage 2). Under Assumption 1,2,3, consider Algorithm $^{\\,l}$ With $\\lambda_{t}=\\sqrt{\\varepsilon_{0}}$ Given Stage 1 in Lemma 3, we have Stage 2 ends within time $T_{2}=\\widetilde{O}_{*}(1/\\eta\\varepsilon_{0})$ such that optimality gap $\\zeta_{T_{2}}=O_{*}(\\varepsilon_{0})$ ", "page_idx": 15}, {"type": "text", "text": "Proof. Denote $\\widetilde{\\pmb{a}}_{*}$ as the minima of $\\widetilde{L}_{1,\\lambda}$ . Then, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|a^{(t+1)}-\\widetilde{a}_{*}\\right\\|_{2}^{2}=\\left\\|a^{(t)}-\\widetilde{a}_{*}\\right\\|_{2}^{2}-2\\eta\\langle\\nabla_{a}\\widetilde{L}_{1,\\lambda}(a^{(t)}),a^{(t)}-\\widetilde{a}_{*}\\rangle+\\eta^{2}\\left\\|\\nabla_{a}\\widetilde{L}_{1,\\lambda}(a^{(t)})\\right\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\overset{\\mathrm{(a)}}{\\leq}\\left\\|a^{(t)}-\\widetilde{a}_{*}\\right\\|_{2}^{2}-2\\eta\\langle\\widetilde{L}_{1,\\lambda}(a^{(t)})-\\widetilde{L}_{1,\\lambda}(\\widetilde{a}_{*}))+\\eta^{2}O_{*}(m)}\\\\ &{\\qquad\\qquad\\qquad=\\left\\|a^{(t)}-\\widetilde{a}_{*}\\right\\|_{2}^{2}-2\\eta\\langle\\widetilde{L}_{1,\\lambda}(a^{(t)})-\\widetilde{L}_{1,\\lambda}(\\widetilde{a}_{*}))+\\eta\\varepsilon_{0}/2,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where (a) we use idea loss $\\widetilde{L}_{1,\\lambda}$ is convex in $\\textbf{\\em a}$ ", "page_idx": 16}, {"type": "text", "text": "Iterating the above inequality over all $t$ wehave ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Big\\|\\boldsymbol{a}^{(T)}-\\widetilde{\\boldsymbol{a}}_{*}\\Big\\|_{2}^{2}\\le\\Big\\|\\boldsymbol{a}^{(1)}-\\widetilde{\\boldsymbol{a}}_{*}\\Big\\|_{2}^{2}-2\\eta\\sum_{t\\le T}(\\widetilde{L}_{1,\\lambda}(\\boldsymbol{a}^{(t)})-\\widetilde{L}_{1,\\lambda}(\\widetilde{\\boldsymbol{a}}_{*}))+\\eta T\\varepsilon_{0}/2,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which means ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{t\\leq T}\\widetilde{L}_{1,\\lambda}(\\pmb{a}^{(t)})-\\widetilde{L}_{1,\\lambda}(\\widetilde{\\pmb{a}}_{*})\\leq\\frac{1}{T}\\sum_{t\\leq T}(\\widetilde{L}_{1,\\lambda}(\\pmb{a}^{(t)})-\\widetilde{L}_{1,\\lambda}(\\widetilde{\\pmb{a}}_{*}))\\leq\\frac{\\|\\pmb{a}^{(1)}-\\widetilde{\\pmb{a}}_{*}\\|_{2}^{2}}{\\eta T}+\\varepsilon_{0}/2.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "It is easy to see $\\left\\|\\pmb{\\mathscr{a}}^{(1)}\\right\\|_{2},\\|\\widetilde{\\pmb{\\mathscr{a}}}_{*}\\|_{1}=O_{*}(1)$ . Thus, when $T\\geq O_{*}(1/\\eta\\varepsilon_{0})$ we know $\\widetilde{L}_{1,\\lambda}(\\pmb{a}^{(T_{2})})~-$ $\\widetilde{L}_{1,\\lambda}(\\widetilde{\\pmb{a}}_{\\ast})\\leq3\\varepsilon_{0}/4$ ", "page_idx": 16}, {"type": "text", "text": "This suggests the optimality gap after balancing the norm (so that $L_{\\lambda}(\\pmb\\theta^{(T_{2})})=\\widetilde{L}_{1,\\lambda}(\\pmb a^{(T_{2})}))$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\zeta_{T_{2}}=\\!\\!L_{\\lambda}(\\theta^{(T_{2})})-\\underset{\\mu\\in\\mathcal{M}(\\mathbb{S}^{d-1})}{\\operatorname*{min}}L_{\\lambda}(\\mu)}\\\\ &{\\quad\\quad\\!=\\!\\!\\widetilde{L}_{1,\\lambda}(\\!a^{(T_{2})})-\\widetilde{L}_{1,\\lambda}(\\widetilde{\\boldsymbol{a}}_{*})+\\widetilde{L}_{1,\\lambda}(\\widetilde{\\boldsymbol{a}}_{*})-\\underset{\\mu\\in\\mathcal{M}(\\mathbb{S}^{d-1})}{\\operatorname*{min}}L_{\\lambda}(\\mu).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For $\\widetilde{L}_{1,\\lambda}\\big(\\pmb{a}^{(T_{2})}\\big)-\\widetilde{L}_{1,\\lambda}\\big(\\pmb{\\widetilde{a}}_{\\ast}\\big)$ wejust show abve that it sls $3\\varepsilon_{0}/4$ ", "page_idx": 16}, {"type": "text", "text": "For $\\widetilde{L}_{1,\\lambda}(\\widetilde{\\pmb{a}}_{*})-\\operatorname*{min}_{\\mu\\in\\mathcal{M}(\\mathbb{S}^{d-1})}L_{\\lambda}(\\mu)$ we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{L}_{1,\\lambda}(\\widetilde{\\boldsymbol{a}}_{*})-\\underset{\\boldsymbol{\\mu}\\in\\mathcal{M}(\\mathbb{S}^{d-1})}{\\operatorname*{min}}L_{\\lambda}(\\boldsymbol{\\mu})\\le\\!\\!\\widetilde{L}_{1,\\lambda}(\\widehat{\\boldsymbol{a}}_{*})-\\underset{\\boldsymbol{\\mu}\\in\\mathcal{M}(\\mathbb{S}^{d-1})}{\\operatorname*{min}}L_{\\lambda}(\\boldsymbol{\\mu})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\le\\!O_{*}(\\varepsilon_{0}^{2})+\\lambda\\,\\|\\boldsymbol{a}_{*}\\|_{1}-\\lambda|\\mu_{\\lambda}^{*}|_{1}\\le O_{*}(\\lambda^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where in the last inequality we use Lemma F.3 and $\\mu_{\\lambda}^{\\ast}=\\arg\\operatorname*{min}_{\\mu\\in\\mathcal{M}(\\mathbb{S}^{d-1})}L_{\\lambda}(\\mu)$ . Here $\\hat{\\pmb{a}}_{*}$ is a rescaled version of $\\mathbf{a}_{*}$ and is constructed as: for every teacher neuron $\\pmb{w}_{i}^{*}$ choose the closest neuron ${\\pmb w}_{j}$ s.t. $\\angle(\\pmb{w}_{j},\\pmb{w}_{i}^{*})\\leq\\varepsilon_{0}$ and set $\\hat{\\pmb{a}}_{\\ast,j}=a_{i}^{\\ast}/\\left\\lVert\\pmb{w}_{j}\\right\\rVert_{2}$ . Set all other $\\hat{\\pmb{a}}_{*,k}=0$ ", "page_idx": 16}, {"type": "text", "text": "Together with above calculations, we have $\\zeta_{T_{2}}\\leq O_{*}(\\varepsilon_{0})$ ", "page_idx": 16}, {"type": "text", "text": "E  Stage 3: local convergence for regularized 2-layer neural networks ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section we show the local convergence that loss eventually goes to O within polynomial time and recovers teacher neurons? direction. ", "page_idx": 16}, {"type": "text", "text": "The results in this section only need the width $m\\geq m_{*}$ as long as its initial loss is small. ", "page_idx": 16}, {"type": "text", "text": "Lemma 5 (Stage 3). Under Assumption 1,2,3, consider Algorithm 1 on loss (2). Given Stage 2 inLemma,ifteintalaly $\\zeta_{3,0}\\leq O_{*}(\\lambda_{3,0}^{9/5})$ weight decay $\\lambda$ follows the schedule of initial value $\\lambda_{3,0}=O_{*}(1)$ ,and $k$ -th epoch $\\lambda_{3,k}=\\lambda_{3,k-1}/2$ and stepsize $\\eta_{3k}=\\eta\\le O_{*}(\\lambda_{3,k}^{12}d^{-3})$ for all $T_{3,k}\\leq t\\leq T_{3,k+1}$ in epoch $k$ thenwithin $K=O_{*}(\\log(1/\\varepsilon))$ epochs and total $T_{3}-T_{2}=$ $O_{*}(\\lambda_{3,0}^{-4}\\eta^{-1}\\varepsilon^{-2})$ time we recover the ground-truth network $L(\\pmb\\theta)\\leq\\varepsilon$ ", "page_idx": 16}, {"type": "text", "text": "The goal of each epoch is to minimize the loss $L_{\\lambda}$ with a fix $\\lambda$ . The lemma below shows that as long as the initial optimality gap is $O_{*}(\\lambda^{9/5})$ , then at the end of each epoch, $L_{\\lambda}$ could decrease to ${O_{*}}(\\lambda^{2})$ Therefore, using a slow decay of weight decay parameter $\\lambda$ for each epoch we could stay in the local convergence regime for each epoch and eventually recovers the target network. ", "page_idx": 16}, {"type": "text", "text": "Lemma E.1 (Loss improve within one epoch). Suppose $|a_{i}^{(0)}|\\,\\leq\\,\\left\\lVert{\\pmb{w}}_{i}^{(0)}\\right\\rVert_{2}$ for all $i\\,\\in\\,[m]$ .If $\\zeta_{0}\\leq O_{*}(\\lambda^{9/5})$ and $\\lambda\\leq O_{*}(1)$ and $\\eta\\leq O_{*}(\\lambda^{12}d^{-3})$ then within $O_{*}(\\lambda^{-4}\\eta^{-1})$ time the optimality gapbecomes $\\dot{L}_{\\lambda}-L_{\\lambda}(\\mu_{\\lambda}^{\\ast})=\\bar{O}_{\\ast}(\\lambda^{2})$ ", "page_idx": 17}, {"type": "text", "text": "The above result relies on the following characterization of local landscape of regularized loss. We show the gradient is large whenever the optimality gap is large. This is the main contribution of this paper, see Section F for detailed proofs. ", "page_idx": 17}, {"type": "text", "text": "Lemma 6 (Gradient lower bound). When $\\Omega_{*}(\\lambda^{2})\\le\\zeta\\le{\\cal O}_{*}(\\lambda^{9/5})$ and $\\lambda\\leq O_{*}(1)$ wehave ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\|\\nabla_{\\theta}L_{\\lambda}\\right\\|_{F}^{2}\\geq\\Omega_{*}(\\zeta^{4}/\\lambda^{2}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In order to use the above landscape result with standard descent lemma, we also need certain smoothness condition on the loss function. We show below that this regularized loss indeed satisfies certain smoothness condition (though weaker than standard smoothness condition) to allow the convergenceanalysis. ", "page_idx": 17}, {"type": "text", "text": "Lemma E.2 (Smoothness). Suppose $|a_{i}|\\leq\\|\\pmb{w}_{i}\\|_{2}$ and $\\left\\|\\mathbb{E}_{\\mathbf{x}}[R(\\mathbf{x})\\sigma^{\\prime}(\\mathbf{\\overline{{w}}}_{i}^{(t)^{\\top}}\\mathbf{x})\\mathbf{x}]\\right\\|_{2}^{2}=O_{\\ast}(d)\\,f$ orall $i\\in[m]$ If $\\eta=O_{\\ast}(1/d)$ then ", "page_idx": 17}, {"type": "equation", "text": "$$\nL_{\\lambda}(\\pmb\\theta-\\eta\\nabla_{\\pmb\\theta}L_{\\lambda})\\leq L_{\\lambda}(\\pmb\\theta)-\\eta\\left\\|\\nabla_{\\pmb\\theta}L_{\\lambda}\\right\\|_{F}^{2}+O_{*}(\\eta^{3/2}d^{3/2})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "E.1Proofs in Section $\\mathbf{E}$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We now are ready to show the convergence of Stage 3 by using Lemma E.1 to show the loss makes progress every epoch. ", "page_idx": 17}, {"type": "text", "text": "Lemma 5 (Stage 3). Under Assumption 1,2,3, consider Algorithm 1 on loss (2). Given Stage 2 in Lemma 4, ifthe initial optimality gap $\\zeta_{3,0}\\leq O_{*}(\\lambda_{3,0}^{9/5})$ weight decay $\\lambda$ follows the schedule of initial value $\\lambda_{3,0}=O_{*}(1)$ , and $k$ -th epoch $\\lambda_{3,k}=\\lambda_{3,k-1}/2$ and stepsize $\\eta_{3k}=\\eta\\le O_{*}(\\lambda_{3,k}^{12}d^{-3})$ for all $T_{3,k}\\leq t\\leq T_{3,k+1}$ in epoch $k$ then within $K=O_{*}(\\log(1/\\varepsilon))$ epochs and total $T_{3}-T_{2}=$ $O_{*}(\\lambda_{3,0}^{-4}\\eta^{-1}\\varepsilon^{-2})$ time we recover the ground-truth network $L(\\pmb\\theta)\\leq\\varepsilon$ ", "page_idx": 17}, {"type": "text", "text": "Proof. Since $\\left|a_{i}^{(0)}\\right|\\leq\\left\\|\\pmb{w}_{i}^{(0)}\\right\\|_{2}$ for all $i\\in[m]$ at the beginning of Stage 3, from Lemma E.3 we know they will remain hold for all epoch and all time $t$ ", "page_idx": 17}, {"type": "text", "text": "From Lemma E.1 we know for epoch $k$ it finishes within $O_{*}(\\lambda_{k}^{-4}\\eta^{-1})$ time and achieves $L_{\\lambda_{k}}\\mathrm{~-~}$ $L_{\\lambda_{k}}(\\mu_{\\lambda_{k}}^{*})=O_{*}(\\lambda_{k}^{2})$ . To proceed to next epoch $k+1$ , we only need to show the solution at the end of epoch $k\\,\\pmb{\\theta}^{(k)}$ gives the optimality gap $\\zeta=O_{*}(\\lambda_{k+1}^{9/5})$ for the next $\\lambda_{k+1}$ .We have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\lambda}_{\\lambda_{k+1}}(\\theta^{(k)})-L_{\\lambda_{k+1}}(\\mu_{\\lambda_{k+1}}^{*})=L(\\theta^{(k)})-L(\\mu_{\\lambda_{k+1}}^{*})+\\frac{\\lambda_{k+1}}{2}\\left\\|\\boldsymbol{a}^{(k)}\\right\\|_{2}^{2}+\\frac{\\lambda_{k+1}}{2}\\left\\|\\boldsymbol{W}^{(k)}\\right\\|_{F}^{2}-\\lambda_{k+1}|\\mu_{\\lambda_{k+1}}^{*}}\\\\ &{\\overset{\\mathrm{(i)}}{\\leq}O_{*}(\\lambda_{k}^{2})+\\frac{\\lambda_{k+1}}{\\lambda_{k}}\\left(\\frac{\\lambda_{k}}{2}\\left\\|\\boldsymbol{a}^{(k)}\\right\\|_{2}^{2}+\\frac{\\lambda_{k}}{2}\\left\\|\\boldsymbol{W}^{(k)}\\right\\|_{F}^{2}-\\lambda_{k}|\\mu_{\\lambda_{k+1}}^{*}|_{1}\\right)}\\\\ &{\\overset{\\mathrm{(b)}}{\\leq}O_{*}(\\lambda_{k}^{2})+\\frac{\\lambda_{k+1}}{\\lambda_{k}}\\left(O_{*}(\\lambda_{k}^{2})+L(\\mu_{\\lambda_{k}}^{*})-L(\\theta^{(k)})\\right)}\\\\ &{\\overset{\\mathrm{(c)}}{\\leq}O_{*}(\\lambda_{k}^{2})\\leq O_{*}(\\lambda_{k+1}^{9/5})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where (a) due to Lemma F.4 that $L(\\pmb{\\theta}^{(k)})$ is small; (b) the optimality gap at the end of epoch $k$ is $O_{*}(\\lambda_{k}^{2})$ and $|\\mu_{\\lambda_{k}}^{\\ast}|_{1}-|\\mu_{\\lambda_{k+1}}^{\\ast}|_{1}=\\dot{O_{\\ast}}(\\lambda_{k})$ from Lemma F.3; (c) due to Lemma F.3 that $L(\\mu_{\\lambda_{k}}^{*})$ is small. In this way, we can apply Lemma E.1 again for epoch $k+1$ ", "page_idx": 17}, {"type": "text", "text": "From Lemma F.4 we know at the end of epoch $k$ the square loss $L(\\pmb\\theta^{(k)})=O_{*}(\\lambda_{k}^{2})$ . Thus, to reach $\\varepsilon$ square loss, we need $\\lambda_{k}=O_{*}(\\varepsilon^{1/2})$ , which means we need to take ${\\cal O}_{\\ast}(\\log(1/\\varepsilon))$ epoch. Since epoch $k$ it finishes within $O_{*}(\\lambda_{k}^{-4}\\eta^{-1})$ time, we know the total time is at most $\\overset{\\cdot}{O}_{*}(\\lambda_{0}^{-4}\\eta^{-1}\\varepsilon^{-2})$ time. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "To show the lemma below that loss makes progress within every epoch, we rely on the gradient lower bound (Lemma 6) and smoothness condition of loss function (Lemma E.2). ", "page_idx": 18}, {"type": "text", "text": "Lemma E.1 (Loss improve within one epoch). Suppose $\\left|a_{i}^{(0)}\\right|\\,\\leq\\,\\left\\|\\pmb{w}_{i}^{(0)}\\right\\|_{2}$ for all $i\\,\\in\\,[m]$ f $\\zeta_{0}\\leq O_{*}(\\lambda^{9/5})$ and $\\lambda\\leq O_{*}(1)$ and $\\eta\\leq O_{*}(\\lambda^{12}d^{-3})$ then within $O_{*}(\\lambda^{-4}\\eta^{-1})$ time the optimality gap becomes $\\dot{L}_{\\lambda}-L_{\\lambda}(\\mu_{\\lambda}^{\\ast})=\\bar{O}_{\\ast}(\\lambda^{2})$ ", "page_idx": 18}, {"type": "text", "text": "Proof. Since $\\left|a_{i}^{(0)}\\right|\\leq\\left\\|\\pmb{w}_{i}^{(0)}\\right\\|_{2}$ for all $i\\in[m]$ at the beginning of current epoch, from Lemma E.3 we know they will remain hoid for all time $t$ . Then combine Lemma E.4 and Lemma E.2 we know ", "page_idx": 18}, {"type": "equation", "text": "$$\nL_{\\lambda}(\\pmb\\theta-\\eta\\nabla_{\\pmb\\theta}L_{\\lambda})\\leq L_{\\lambda}(\\pmb\\theta)-\\eta\\left\\|\\nabla_{\\pmb\\theta}L_{\\lambda}\\right\\|_{F}^{2}+O_{*}(\\eta^{3/2}d^{3/2}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Recall $\\zeta_{t}=L_{\\lambda}(\\pmb\\theta^{(t)})-L_{\\lambda}(\\mu_{\\lambda}^{*})$ . Using gradient lower bound Lemma 6 and consider the time before $\\zeta_{t}$ reach ${O_{*}(\\lambda^{2})}$ wehave ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\zeta_{t+1}\\leq\\zeta_{t}-\\eta\\Omega_{*}(\\zeta_{t}^{4}/\\lambda^{2})+O_{*}(\\eta^{3/2}d^{3/2})\\leq\\zeta_{t}-\\Omega_{*}(\\eta\\zeta_{t}^{4}/\\lambda^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we use $\\eta={\\cal O}_{*}(\\lambda^{12}d^{-3})$ to be small enough. ", "page_idx": 18}, {"type": "text", "text": "The above recursion implies that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\zeta_{t}=O_{*}((t/\\lambda^{2}+\\zeta_{0}^{-3})^{-1/3}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, within $O_{*}(1/\\lambda^{4})$ the optimality gap $\\zeta_{t}$ reaches ${O_{*}}(\\lambda^{2})$ ", "page_idx": 18}, {"type": "text", "text": "The lemma below shows a regularity condition on the norm between two layers. ", "page_idx": 18}, {"type": "text", "text": "Lemma E.3. If we start at $\\left|a_{i}^{(0)}\\right|\\leq\\left\\|\\pmb{w}_{i}^{(0)}\\right\\|_{2}$ and $\\eta=O_{*}(1)$ then we have ${|a_{i}^{(t)}|^{2}\\leq\\left\\|\\pmb{w}_{i}^{(t)}\\right\\|_{2}^{2}f o r}$ all $i\\in[m_{*}]$ and all time $t$ ", "page_idx": 18}, {"type": "text", "text": "Proof. Denote $R({\\pmb x})=f({\\pmb x})-f_{*}({\\pmb x})$ . Assume $\\big|a_{i}^{(t)}\\big|^{2}-\\left\\|\\pmb{w}_{i}^{(t)}\\right\\|_{2}^{2}\\leq0$ we show it remains at $t+1$ We have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|a_{i}^{(t+1)}|^{2}-\\left\\|w_{i}^{(t+1)}\\right\\|_{2}^{2}}\\\\ &{=\\!\\left|a_{i}^{(t)}-\\eta\\nabla_{a_{i}}L_{\\lambda}(\\pmb{\\theta}^{(t)})\\right|^{2}-\\left\\|w_{i}^{(t)}-\\eta\\nabla_{w_{i}}L_{\\lambda}(\\pmb{\\theta}^{(t)})\\right\\|_{2}^{2}}\\\\ &{=\\!\\left|a_{i}^{(t)}\\right|^{2}-\\left\\|w_{i}^{(t)}\\right\\|_{2}^{2}+\\eta^{2}|\\nabla_{a_{i}}L_{\\lambda}(\\pmb{\\theta}^{(t)})|^{2}-\\eta^{2}\\left\\|\\nabla_{w_{i}}L_{\\lambda}(\\pmb{\\theta}^{(t)})\\right\\|_{2}^{2}}\\\\ &{=\\!\\left|a_{i}^{(t)}|^{2}-\\left\\|w_{i}^{(t)}\\right\\|_{2}^{2}+\\eta^{2}|2\\mathbb{E}_{x}[R(\\pmb{x})\\sigma(\\pmb{w}_{i}^{(t)}\\tau_{x})]+\\lambda a_{i}^{(t)}|^{2}-\\eta^{2}\\left\\|2\\mathbb{E}_{x}[R(\\pmb{x})a_{i}^{(t)}\\sigma^{\\prime}(\\overline{{\\pmb{w}}}_{i}^{(t)\\top}\\pmb{x})\\pmb{x}]+\\lambda w_{i}^{(t)}\\right\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We first focus on the last 2 terms. We have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~|2\\mathbb{E}_{x}[R(x)\\sigma({w_{i}^{(t)}}^{\\top}x)]+\\lambda{a_{i}^{(t)}}|^{2}-\\left\\|2\\mathbb{E}_{x}[R(x){a_{i}^{(t)}}\\sigma^{\\prime}(\\overline{{w_{i}^{(t)}}}^{\\top}x)x]+\\lambda{w_{i}^{(t)}}\\right\\|_{2}^{2}}\\\\ &{=\\left\\|{w_{i}^{(t)}}\\right\\|_{2}^{2}|2\\mathbb{E}_{x}[R(x)\\sigma(\\overline{{w_{i}^{(t)}}}^{\\top}x)]|^{2}+\\lambda^{2}|{a_{i}^{(t)}}|^{2}-|{a_{i}^{(t)}}|^{2}\\left\\|2\\mathbb{E}_{x}[R(x)\\sigma^{\\prime}(\\overline{{w_{i}^{(t)}}}^{\\top}x)x]\\right\\|_{2}^{2}-\\lambda^{2}\\left\\|{w_{i}^{(t)}}\\right\\|_{2}^{2}}\\\\ &{\\overset{\\mathrm{s}}{\\leq}\\left(|{a_{i}^{(t)}}|^{2}-\\left\\|{w_{i}^{(t)}}\\right\\|_{2}^{2}\\right)\\left(\\lambda^{2}-\\left\\|2\\mathbb{E}_{x}[R(x)\\sigma^{\\prime}(\\overline{{w_{i}^{(t)}}}^{\\top}x)x]\\right\\|_{2}^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where (a) due to $|2\\mathbb{E}_{\\mathbf{x}}[R({\\boldsymbol x})\\sigma(\\overline{{{\\mathbf{w}}}}_{i}^{(t)\\top}{\\boldsymbol x})]|^{2}\\leq\\Big\\|2\\mathbb{E}_{\\mathbf{x}}[R({\\boldsymbol x})\\sigma^{\\prime}(\\overline{{{\\mathbf{w}}}}_{i}^{(t)\\top}{\\boldsymbol x}){\\boldsymbol x}]\\Big\\|_{2}^{2}.$ ", "page_idx": 18}, {"type": "text", "text": "Therefore, plug it back to the above equation, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|a_{i}^{(t+1)}|^{2}-\\left\\|w_{i}^{(t+1)}\\right\\|_{2}^{2}\\leq\\left(|a_{i}^{(t)}|^{2}-\\left\\|w_{i}^{(t)}\\right\\|_{2}^{2}\\right)\\left(1+\\eta^{2}\\lambda^{2}-\\eta^{2}\\left\\|2\\mathbb{E}_{x}[R(x)\\sigma^{\\prime}(\\overline{{w}}_{i}^{(t)\\top}x)x]\\right\\|_{2}^{2}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\underset{\\leq0,0}{\\overset{\\mathrm{(a)}}{\\leq}}0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where (a) dueto ${|a_{i}^{(t)}|^{2}-\\left\\|\\pmb{w}_{i}^{(t)}\\right\\|_{2}^{2}\\leq0}$ and we use $\\left\\|2\\mathbb{E}_{\\pmb{x}}[R(\\pmb{x})\\sigma^{\\prime}(\\pmb{\\overline{{w}}}_{i}^{(t)\\top}\\pmb{x})]\\pmb{x}\\right\\|_{2}^{2}=O_{\\ast}(d)$ from Lemma E.4 and $\\eta$ is small enough. ", "page_idx": 19}, {"type": "text", "text": "Therefore we can ethat $\\big|a_{i}^{(t)}\\big|^{2}-\\Big|\\Big|\\pmb{w}_{i}^{(t)}\\Big|\\Big|_{2}^{2}\\leq0$ remains fo al $t$ ", "page_idx": 19}, {"type": "text", "text": "This lemma shows the smoothness of loss function. The proof requires a careful calculations to bound the error terms. ", "page_idx": 19}, {"type": "text", "text": "Lemma E.2 (Smoothness). Suppose $|a_{i}|\\leq\\|\\pmb{w}_{i}\\|_{2}$ and $\\begin{array}{r}{\\left\\|\\mathbb{E}_{\\mathbf{x}}[R(\\pmb{x})\\sigma^{\\prime}(\\overline{{\\pmb{w}}}_{i}^{(t)\\top}\\pmb{x})\\pmb{x}]\\right\\|_{2}^{2}=O_{*}(d).}\\end{array}$ for all $i\\in[m]$ If $\\eta=O_{\\ast}(1/d)$ ,then ", "page_idx": 19}, {"type": "equation", "text": "$$\nL_{\\lambda}(\\pmb\\theta-\\eta\\nabla_{\\pmb\\theta}L_{\\lambda})\\leq L_{\\lambda}(\\pmb\\theta)-\\eta\\left\\|\\nabla_{\\pmb\\theta}L_{\\lambda}\\right\\|_{F}^{2}+O_{*}(\\eta^{3/2}d^{3/2})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Denote $R_{\\theta}({\\pmb x})\\,=\\,f_{\\theta}({\\pmb x})\\,-\\,f_{*}({\\pmb x})$ todenotethedependency on $\\pmb{\\theta}$ . For simplicity, we will use $\\widetilde{\\nabla}_{\\theta}\\,=\\,-\\eta\\nabla_{\\theta}L_{\\lambda}$ and same for others. Since $\\left\\|\\mathbb{E}_{\\mathbf{x}}[R(\\mathbf{x})\\sigma^{\\prime}(\\overline{{\\pmb{w}}}_{i}^{(t)\\top}\\pmb{x})\\pmb{x}]\\right\\|_{2}^{2}=O_{*}(d)$ we know $|\\widetilde{\\nabla}_{a_{i}}|=O_{*}(\\eta\\,\\|\\pmb{w}_{i}\\|_{2}\\,d)$ and $\\left\\|\\widetilde{\\nabla}_{\\pmb{w}_{i}}\\right\\|_{2}=O_{\\ast}(\\eta|a_{i}|d)$ ", "page_idx": 19}, {"type": "text", "text": "We have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad L_{3}(\\theta-\\eta\\nabla_{\\theta})-L_{3}(\\theta)+\\eta\\nabla_{\\theta}(\\log\\vert\\nabla_{\\theta}\\vert)}\\\\ &{=L_{3}(\\theta-\\eta\\nabla_{\\theta})-L_{3}(\\theta)-\\langle\\nabla_{\\theta},-\\eta\\nabla_{\\theta}\\rangle}\\\\ &{=\\mathbb{E}_{\\mathbf{R}}\\big[R_{\\theta}+\\eta\\nabla_{\\theta}(\\alpha)^{2}\\big]+\\frac{\\lambda}{2}\\left\\|{\\mathbf{a}}+\\overline{{\\nabla}}_{\\theta}\\right\\|_{F}^{2}+\\frac{\\lambda}{2}\\left\\|{\\mathbf{W}}+\\overline{{\\nabla}}_{\\theta}\\right\\|_{F}^{2}-\\mathbb{E}_{\\mathbf{R}}\\big[R_{\\theta}(\\alpha)^{2}\\big]-\\frac{\\lambda}{2}\\left\\|{\\mathbf{a}}\\right\\|_{2}^{2}-\\frac{\\lambda}{2}\\left\\|{\\mathbf{W}}\\right\\|_{F}^{2}}\\\\ &{\\quad-\\underbrace{\\sum_{\\theta}}_{\\theta}\\mathbb{E}_{\\theta}\\big[R_{\\theta}(\\eta,\\eta)\\nabla_{\\theta}^{\\pi}\\big]-\\underbrace{\\sum_{\\theta}\\nabla_{\\theta}[R_{\\theta}(\\alpha)^{2}(\\mathbf{w}^{\\pi})^{2}\\mathbf{w}^{\\pi}]-\\nabla_{\\theta}[R_{\\theta}(\\eta,\\eta)\\nabla_{\\theta}^{\\pi}]-\\mathbb{E}_{\\mathbf{R}}}_{\\theta}[R_{\\theta}(\\eta,\\eta)\\nabla_{\\theta}^{\\pi}]-\\mathbb{E}_{\\mathbf{x}}}\\\\ &{\\quad\\quad+\\frac{\\lambda}{2}\\big\\|\\overline{{\\mathbf{a}}}_{\\theta}\\big[R_{\\theta}(\\eta,\\eta)-\\nabla_{\\theta}(\\theta)\\nabla_{\\theta}\\big]}\\\\ &{=\\frac{\\mathbb{E}_{\\theta}}{\\theta}\\Big[R_{\\theta}+\\frac{\\eta}{\\sqrt{\\theta}}\\big(R_{\\theta}+\\eta\\big)^{2}\\Big]}\\\\ &{\\quad+\\underbrace{2\\mathbb{E}_{\\mathbf{w}}\\left[R_{\\theta}(\\eta,\\eta)\\left(R_{\\theta}+\\eta\\big)-R_{\\theta}(\\eta,\\eta)-\\sum_{\\theta}\\sigma(w_{\\theta}^{\\pi})\\tilde{\\nabla}_{\\theta}-\\sum_{\\phi}\\mathbf{a}_{\\phi}^{\\pi}(w_{\\theta}^{\\pi})^{2}\\mathbf{w}^{\\pi}\\nabla_{\\theta \n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The last line is easy to see on $O_{*}(\\eta^{2}d^{2})$ using norm bound in Lemma F.12, so in below we are going to bound (I) and (ll) one by one. The goal is to show they are small in the sense of on order $o(\\eta)$ ", "page_idx": 19}, {"type": "text", "text": "Bound (I) For (I), we can write out the expression as ", "text_level": 1, "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle R_{\\theta+\\widetilde{\\nabla}_{\\theta}}(x)-R_{\\theta}(x))^{2}\\rangle=\\!\\mathbb{E}_{x}\\left[\\left(\\displaystyle\\sum_{i\\in[m]}\\big(a_{i}+\\widetilde{\\nabla}_{a_{i}}\\big)\\sigma((w_{i}+\\widetilde{\\nabla}_{w_{i}})^{\\top}x)-a_{i}\\sigma(w_{i}^{\\top}x)+\\widetilde{\\nabla}_{\\alpha}+x^{\\top}\\widetilde{\\nabla}_{\\beta}\\right)\\!\\!\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\!2\\,\\mathbb{E}_{x}\\left[\\left(\\displaystyle\\sum_{i\\in[m]}\\big(a_{i}+\\widetilde{\\nabla}_{a_{i}}\\big)\\sigma((w_{i}+\\widetilde{\\nabla}_{w_{i}})^{\\top}x)-a_{i}\\sigma(w_{i}^{\\top}x)\\right)^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For (I.i), we can split into 2 terms as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\boldsymbol{x}}\\left[\\left(\\displaystyle\\sum_{i\\in[m]}\\left(a_{i}+\\tilde{\\nabla}_{a_{i}})\\sigma((w_{i}+\\tilde{\\nabla}_{w_{i}})^{\\top}\\boldsymbol{x})-a_{i}\\sigma(w_{i}^{\\top}\\boldsymbol{x})\\right)^{2}\\right]}\\\\ &{\\le2\\mathbb{E}_{\\boldsymbol{x}}\\left[\\left(\\displaystyle\\sum_{i\\in[m]}\\tilde{\\nabla}_{a_{i}}\\sigma((w_{i}+\\tilde{\\nabla}_{w_{i}})^{\\top}\\boldsymbol{x})\\right)^{2}\\right]+2\\mathbb{E}_{\\boldsymbol{x}}\\left[\\left(\\displaystyle\\sum_{i\\in[m]}a_{i}\\sigma((w_{i}+\\tilde{\\nabla}_{w_{i}})^{\\top}\\boldsymbol{x})-a_{i}\\sigma(w_{i}^{\\top}\\boldsymbol{x})\\right)^{2}\\right]}\\\\ &{\\le2\\mathbb{E}_{\\boldsymbol{x}}\\left[\\left(\\displaystyle\\sum_{i\\in[m]}|\\tilde{\\nabla}_{a_{i}}||(w_{i}+\\tilde{\\nabla}_{w_{i}})^{\\top}\\boldsymbol{x}|\\right)^{2}\\right]+2\\mathbb{E}_{\\boldsymbol{x}}\\left[\\left(\\displaystyle\\sum_{i\\in[m]}|a_{i}||\\tilde{\\nabla}_{w_{i}}^{\\top}\\boldsymbol{x}|\\right)^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We then can bound them separately as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(I.i)\\overset{\\mathrm{(a)}}{\\leq}O(1)\\left(\\displaystyle\\sum_{i\\in[m]}|\\widetilde{\\nabla}_{a_{i}}|\\,\\Big\\Vert w_{i}+\\widetilde{\\nabla}_{w_{i}}\\Big\\Vert_{2}\\right)^{2}+O(1)\\left(\\displaystyle\\sum_{i\\in[m]}|a_{i}|\\,\\Big\\Vert\\widetilde{\\nabla}_{w_{i}}\\Big\\Vert_{2}\\right)^{2}}\\\\ &{\\quad\\overset{\\mathrm{(b)}}{\\leq}O_{*}(d^{2})\\left(\\displaystyle\\sum_{i\\in[m]}\\eta\\,\\Vert w_{i}\\Vert_{2}^{2}+\\eta^{2}|a_{i}|\\,\\Vert w_{i}\\Vert_{2}\\,d\\right)^{2}+O_{*}(d^{2})\\left(\\displaystyle\\sum_{i\\in[m]}\\eta a_{i}^{2}\\right)^{2}}\\\\ &{\\quad\\overset{\\mathrm{(c)}}{\\leq}O_{*}(\\eta^{2}d^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where (a) we use Lemma E.5; (b) recall $|\\widetilde{\\nabla}_{a_{i}}|=O_{*}(\\eta\\,\\|\\pmb{w}_{i}\\|_{2}\\,d)$ and $\\left\\|\\widetilde{\\nabla}_{\\mathbf{\\boldsymbol{w}}_{i}}\\right\\|_{2}=O_{*}(\\eta|a_{i}|d)$ (c) $\\begin{array}{r}{\\left\\|\\pmb{a}\\right\\|,\\left\\|\\pmb{W}\\right\\|_{F},\\sum_{i\\in[m]}\\left|a_{i}\\right|\\left\\|\\pmb{w}_{i}\\right\\|_{2}=O_{*}(1)}\\end{array}$ from Lemma F.12 and Lemma F.4, as well as $\\eta$ is small enough. ", "page_idx": 20}, {"type": "text", "text": "For (I.ii), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x}\\left[\\left(\\widetilde{\\nabla}_{\\alpha}+\\pmb{x}^{\\top}\\widetilde{\\nabla}_{\\beta}\\right)^{2}\\right]\\leq O(|\\widetilde{\\nabla}_{\\alpha}|^{2}+\\left\\|\\widetilde{\\nabla}_{\\beta}\\right\\|_{2}^{2})=O_{\\ast}(\\eta^{2}d^{2}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we use Lemma F.4. ", "page_idx": 20}, {"type": "text", "text": "Combine (I.i) and (I.i) we know $(\\mathrm{I}){=}O_{*}(\\eta^{2}d^{2})$ ", "page_idx": 20}, {"type": "text", "text": "Bound $(\\mathbf{II})$ For (II), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{x}\\displaystyle\\left[R_{\\theta}(x)\\left(R_{\\theta+\\Tilde{\\nabla}_{\\theta}}(x)-R_{\\theta}(x)-\\sum_{i\\in[m]}\\sigma(w_{i}^{\\top}x)\\Tilde{\\nabla}_{a_{i}}-\\sum_{i\\in[m]}a_{i}\\sigma^{\\prime}(w_{i}^{\\top}x)x^{\\top}\\Tilde{\\nabla}_{w_{i}}-\\Tilde{\\nabla}_{\\alpha}-x^{\\top}\\Tilde{\\nabla}_{\\theta}\\right)\\right]}\\\\ &{=\\!\\mathbb{E}_{x}\\displaystyle\\left[R_{\\theta}(x)\\left(\\sum_{i\\in[m]}(\\underline{{a_{i}+\\Tilde{\\nabla}_{a_{i}}}})\\sigma((w_{i}+\\Tilde{\\nabla}_{w_{i}})^{\\top}x)-a_{i}\\sigma(w_{i}^{\\top}x)-\\sigma(w_{i}^{\\top}x)\\Tilde{\\nabla}_{a_{i}}-a_{i}\\sigma^{\\prime}(w_{i}^{\\top}x)x^{\\top}\\Tilde{\\nabla}_{a_{i}}\\right)\\right]}\\\\ &{\\underset{i\\in[m]}{\\leq}\\displaystyle\\sum_{i\\in[m]}\\|R_{\\theta}\\|\\,\\|I_{i}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We focus on bound $\\|I_{i}\\|$ below. The goal is to show it is $o(\\eta)$ . For $I_{i}(x)$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{i}\\|_{2}^{2}=\\mathbb{E}_{x}\\left[\\Big((a_{i}+\\widetilde{\\nabla}_{a_{i}})\\sigma\\big((w_{i}+\\widetilde{\\nabla}_{w_{i}})^{\\top}x\\big)-a_{i}\\sigma(w_{i}^{\\top}x\\big)-\\sigma(w_{i}^{\\top}x)\\widetilde{\\nabla}_{a_{i}}-a_{i}\\sigma^{\\prime}(w_{i}^{\\top}x)x^{\\top}\\widetilde{\\nabla}_{w_{i}}\\Big)^{2}\\right]}\\\\ &{\\qquad\\le\\mathbb{E}_{x}\\left[2\\left(\\widetilde{\\nabla}_{a_{i}}\\big(\\sigma\\big((w_{i}+\\widetilde{\\nabla}_{w_{i}})^{\\top}x\\big)-\\sigma(w_{i}^{\\top}x)\\big)\\right)^{2}+2\\left(a_{i}\\big(\\sigma\\big((w_{i}+\\widetilde{\\nabla}_{w_{i}})^{\\top}x\\big)-\\sigma(w_{i}^{\\top}x)-\\sigma^{\\prime}(w_{i}^{\\top}x)\\big)^{2}\\right)\\right.}\\\\ &{\\qquad\\le2\\underbrace{\\mathbb{E}_{x}\\left[\\vert\\widetilde{\\nabla}_{a_{i}}\\vert^{2}\\vert\\widetilde{\\nabla}_{w_{i}}^{\\top}x\\vert^{2}\\right]}_{(I I.i)}+2a_{i}^{2}\\underbrace{\\mathbb{E}_{x}\\left[\\vert(w_{i}+\\widetilde{\\nabla}_{w_{i}})^{\\top}x\\vert^{2}\\big(\\sigma^{\\prime}\\big((w_{i}+\\widetilde{\\nabla}_{w_{i}})^{\\top}x\\big)-\\sigma^{\\prime}(w_{i}^{\\top}x)\\big)^{2}\\right]}_{(I I.i i)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For (IIi), recall $|\\widetilde{\\nabla}_{a_{i}}|=O_{\\ast}(\\eta\\,\\|\\pmb{w}_{i}\\|_{2}\\,d)$ and $\\left\\|\\widetilde{\\nabla}_{\\pmb{w}_{i}}\\right\\|_{2}=O_{*}(\\eta|a_{i}|d)$ we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{x}\\left[|\\widetilde{\\nabla}_{a_{i}}|^{2}|\\widetilde{\\nabla}_{\\pmb{w}_{i}}^{\\top}\\pmb{x}|^{2}\\right]\\leq|\\widetilde{\\nabla}_{a_{i}}|^{2}\\left\\|\\widetilde{\\nabla}_{\\pmb{w}_{i}}\\right\\|^{2}=O_{*}(\\eta^{4}|a_{i}|^{2}\\left\\|\\pmb{w}_{i}\\right\\|_{2}^{2}d^{4}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For (II.ii), we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb E_{x}\\left[|\\pmb{w}_{i}+\\widetilde{\\nabla}_{\\pmb{w}_{i}}^{\\top}\\pmb{x}|^{2}(\\sigma^{\\prime}((\\pmb{w}_{i}+\\widetilde{\\nabla}_{\\pmb{w}_{i}})^{\\top}\\pmb{x})-\\sigma^{\\prime}(\\pmb{w}_{i}^{\\top}\\pmb{x}))^{2}\\right]}\\\\ &{=\\!\\mathbb E_{x}\\left[|(\\pmb{w}_{i}+\\widetilde{\\nabla}_{\\pmb{w}_{i}})^{\\top}\\pmb{x}|^{2}\\mathbb{1}_{\\mathrm{sign}((\\pmb{w}_{i}+\\widetilde{\\nabla}_{\\pmb{w}_{i}})^{\\top}\\pmb{x})\\neq\\mathrm{sign}(\\pmb{w}_{i}^{\\top}\\pmb{x})}\\right]}\\\\ &{\\le\\!O(\\left\\|\\pmb{w}_{i}+\\widetilde{\\nabla}_{\\pmb{w}_{i}}\\right\\|_{2}^{2}\\delta^{3}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\delta=\\angle({\\pmb w}_{i}\\!+\\!\\widetilde\\nabla_{{\\pmb w}_{i}},{\\pmb w}_{i})$ is the angle between $\\pmb{w}_{i}+\\widetilde{\\nabla}_{\\pmb{w}_{i}}$ and $\\pmb{w}_{i}$ . Since $\\left\\|\\widetilde{\\nabla}_{\\mathbf{\\boldsymbol{w}}_{i}}\\right\\|_{2}=O_{*}(\\eta|{a}_{i}|d)=$ $O_{*}(\\eta\\,\\vert\\vert\\,\\pmb{w}_{i}\\vert\\vert_{2}\\,d)$ , we know $\\delta=O(\\lVert\\widetilde{\\nabla}_{\\mathbf{\\boldsymbol{w}}_{i}}\\rVert)$ given $\\eta=O_{\\ast}(1/d)$ to be smallenough. ", "page_idx": 21}, {"type": "text", "text": "Combine (II.i) and (II.i) we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|I_{i}\\|_{2}^{2}\\leq O_{*}(\\eta^{4}a_{i}^{2}\\left\\|\\pmb{w}_{i}\\right\\|_{2}^{2}d^{4})+O(a_{i}^{2}\\left\\|\\pmb{w}_{i}+\\widetilde{\\nabla}_{\\pmb{w}_{i}}\\right\\|_{2}^{2}\\left\\|\\widetilde{\\nabla}_{\\pmb{w}_{i}}\\right\\|_{2}^{3})\\leq O_{*}(\\eta^{3}a_{i}^{2}\\left\\|\\pmb{w}_{i}\\right\\|_{2}^{2}d^{3}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $\\|R_{\\theta}\\|=O_{*}(1)$ , this implies ", "page_idx": 21}, {"type": "equation", "text": "$$\n(I I)\\leq\\sum_{i\\in[m]}O_{*}(\\eta^{3/2}a_{i}\\left\\|{\\pmb w}_{i}\\right\\|_{2}d^{3/2})=O_{*}(\\eta^{3/2}d^{3/2}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Combine $(\\bf{I})(\\bf{I I})$ Finally, combing (I) and (Il) we have ", "page_idx": 21}, {"type": "equation", "text": "$$\nL_{\\lambda}(\\pmb\\theta-\\eta\\nabla\\pmb\\theta)-L_{\\lambda}(\\pmb\\theta)+\\eta\\left\\|\\nabla_{\\pmb\\theta}\\right\\|_{F}^{2}=O_{*}(\\eta^{3/2}d^{3/2}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Going back to the beginning of this proof, we get the desired result. ", "page_idx": 21}, {"type": "text", "text": "E.2 Technical Lemma ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We present technical lemmas that are used in the proof of this section. They mostly follow from direct calculations. ", "page_idx": 21}, {"type": "text", "text": "Lemma E.4. We have $\\left\\|\\mathbb{E}_{\\mathbf{x}}[R(\\mathbf{x})\\sigma^{\\prime}(\\overline{{\\pmb{w}}}_{i}^{(t)\\top}\\pmb{x})\\pmb{x}]\\right\\|_{2}^{2}=O_{\\ast}(d)$ ", "page_idx": 21}, {"type": "text", "text": "Proof. It is easy to see given $\\|R\\|=O_{*}(1)$ ", "page_idx": 21}, {"type": "text", "text": "Lemma E.5 (Lemma D.4 in Zhou et al. (2021). Consider $\\alpha_{i}\\in\\mathbb{R}^{d}$ for $i\\in[n]$ .Wehave ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x\\sim N(0,I)}\\left[\\left(\\sum_{i=1}^{n}|\\alpha_{i}^{\\top}x|\\right)^{2}\\right]\\leq c_{0}\\left(\\sum_{i=1}^{n}\\|\\alpha_{i}\\|\\right)^{2},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $c_{\\mathrm{0}}$ is a constant. ", "page_idx": 21}, {"type": "text", "text": "F  Local landscape of population loss ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we are going to show Lemma 6 that characterizing the population local landscape with a fixed $\\lambda$ by giving the lower bound of gradient. ", "page_idx": 21}, {"type": "text", "text": "Outline _ We generally follow the high-level proof plan that outlines in Section 6. In Section F.1 and Section F.2, we characterize the local geometry as in Lemma 8. Then, we use it to construct descent direction in Section F.3. Finally we give the proof of Lemma 6 in Section F.4. ", "page_idx": 22}, {"type": "text", "text": "We start by identifying the structure of (approximated) solution of a closely-related problem in Section F.1 (rewrite (5)): ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mu\\in\\mathcal{M}(\\mathbb{S}^{d-1})}{\\operatorname*{min}}\\,L_{\\lambda}(\\mu):=\\!L(\\mu)+\\lambda|\\mu|_{1}:=\\mathbb{E}_{\\pmb{x},\\tilde{y}}[(f_{\\mu}(\\pmb{x})-\\tilde{y})^{2}]+\\lambda|\\mu|_{1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\!\\mathbb{E}_{\\pmb{x}}\\left[\\left(\\displaystyle\\int_{\\pmb{w}}\\sigma_{\\geq2}(\\pmb{w}^{\\top}\\pmb{x})\\mathbf{d}\\,\\mu-\\mu_{*}\\right)^{2}\\right]+\\lambda|\\mu|_{1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\mathcal{M}(\\mathbb{S}^{d-1})$ is the measure space over unit sphere $\\mathbb{S}^{d-1}$ \uff0c $\\begin{array}{r}{\\mu_{*}=\\sum_{i\\in[m_{*}]}a_{i}^{*}\\delta_{\\pmb{w}_{i}^{*}}}\\end{array}$ and $\\sigma_{\\geq2}(x)=$ $\\sigma(x)\\!-\\!1/\\sqrt{2\\pi}-x/2$ is the activation that after removing Oth and 1st order term in Hermite expansion. Note that when $\\mu$ represents a finite-wdith network, we have $\\begin{array}{r}{\\mu=\\sum_{i\\in[m]}a_{i}\\left\\lVert\\pmb{w}_{i}\\right\\rVert_{2}\\delta\\mathbf{\\overline{{w}}}_{i}}\\end{array}$ is a empirical measure over the neurons. In particular, when $\\mu=\\mu_{*}$ , model $f_{\\mu}$ recovers the target $\\widetilde{f}_{*}$ ", "page_idx": 22}, {"type": "text", "text": "We call (5) as the ideal loss because the original problem (2) would become the above (5) when we balance thenorms $\\left(\\left\\|\\pmb{\\mathnormal{w}}_{i}\\right\\|_{2}=\\left|a_{i}\\right|\\right)$ ,perfectly fit $\\alpha,\\beta$ and relax the finite-width constraints to allow infinite-width (see Ciaim B.1). This is why we slightly abused the notation to use $L_{\\lambda}$ in both (2) and (5). ", "page_idx": 22}, {"type": "text", "text": "In Section F.3 we will use the solution structure to construct descent direction that are positively correlated with gradient and also handle the case when norms are not balanced or $\\alpha,\\beta$ arenot fitted Well. ", "page_idx": 22}, {"type": "text", "text": "Notation  Denote the optimality gap between the loss at $\\mu$ and the optimal distribution $\\mu_{\\lambda}^{*}$ as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\zeta(\\mu):=L_{\\lambda}(\\mu)-L_{\\lambda}(\\mu_{\\lambda}^{\\ast}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\mu_{\\lambda}^{*}$ is the optimal measure that minimize (5). For simplicity denote $\\widetilde{\\boldsymbol{a}}_{i}=\\boldsymbol{a}_{i}\\left\\|\\pmb{{w}}_{i}\\right\\|_{2}$ so that $|\\mu|_{1}=\\|\\dot{\\widetilde{\\pmb{a}}}\\|_{1}$ when $\\begin{array}{r}{\\mu=\\sum_{i\\in[m]}a_{i}\\left\\lVert\\pmb{w}_{i}\\right\\rVert_{2}\\delta\\pmb{\\overline{{w}}}_{i}}\\end{array}$ . Often we use $\\zeta_{t}=\\zeta(\\mu_{t})$ to denote the optimality gap at time $t$ and just $\\zeta$ for simplicity. We slightly abuse the notation to also use $\\zeta=L_{\\lambda}(\\theta)-L_{\\lambda}(\\mu_{\\lambda}^{*})$ Finally denote $\\begin{array}{r}{\\mu^{*}=\\sum_{i\\in[m_{*}]}a_{i}^{*}\\delta_{\\pmb{w}_{i}^{*}}}\\end{array}$ (assuming $\\lVert\\pmb{w}_{i}^{*}\\rVert_{2}=1\\rangle$ so that $f_{\\mu^{*}}(\\pmb{x})=\\mathbb{E}_{\\pmb{w}\\sim\\mu^{*}}[\\sigma_{\\geq2}(\\pmb{w}^{\\top}\\pmb{x})]$ ", "page_idx": 22}, {"type": "text", "text": "F.1  Structure of the ideal loss solution ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we will focus on the structure of approximated solution for the $\\ell_{1}$ regularized regression problem (5). ", "page_idx": 22}, {"type": "text", "text": "In the rest of this section, we will first introduce the idea of non-degenerate dual certificate and then use it as a tool to characterize the structure of the solutions. The proofs are deferred to Section H. ", "page_idx": 22}, {"type": "text", "text": "F.1.1  Non-degenerate dual certificate ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We first recall the definition of non-degenerate dual certificate, which is similar as in (Poon et al., 2023) but slightly adapted for fit our need. ", "page_idx": 22}, {"type": "text", "text": "Definition 1 (Non-degenerate dual certificate). $\\eta(w)$ is called a non-degenerate dual certificate if there exists $p(x)$ suchthat $\\eta(\\pmb{w})=\\mathbb{E}_{\\pmb{x}}[p(\\pmb{x})\\sigma_{\\ge2}(\\pmb{w}^{\\top}\\pmb{x})].$ for $\\pmb{w}\\in\\mathbb{S}^{d-1}$ and ", "page_idx": 22}, {"type": "text", "text": "(i) $\\eta(\\pmb{w}_{i}^{*})=\\mathrm{sign}(a_{i}^{*}).$ for $i=1,\\dots,m_{*}.$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n|\\eta(\\pmb{w})|\\leq1-\\rho_{\\eta}\\delta(\\pmb{w},\\pmb{w}_{i}^{*})^{2}\\,i f\\,\\pmb{w}\\in\\mathcal{T}_{i},\\,w h e r e\\,\\delta(\\pmb{w},\\pmb{w}_{i}^{*})=\\angle(\\pmb{w},\\pmb{w}_{i}^{*}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We first show that there exist such non-degenerate dual certificate. More discussion and a detailed proof are deferred to Section G. ", "page_idx": 22}, {"type": "text", "text": "Lemma F.1. There exists a non-degenerate dual certificate $\\eta=\\mathbb{E}_{\\pmb{x}}[p(\\pmb{x})\\sigma_{\\ge2}(\\pmb{w}^{\\top}\\pmb{x})]\\,w i t h\\,\\rho_{\\eta}=\\Theta(1)$ and $\\left\\|p\\right\\|_{2}\\leq\\mathrm{poly}(m_{*},\\Delta)$ ", "page_idx": 22}, {"type": "text", "text": "The following lemma (restate of Lemma 9) gives the properties that will be used in the later proofs: the non-degenerate dual certificate $\\eta$ allows us to capture the gap between the current position $\\mu$ and the target $\\mu^{*}$ ", "page_idx": 22}, {"type": "text", "text": "Lemma F.2. Given a non-degenerate dual certificate $\\eta_{:}$ then ", "page_idx": 23}, {"type": "text", "text": "(i) For any measure $\\begin{array}{r}{\\mu\\in\\mathcal{M}(\\mathbb{S}^{d-1}),\\,|\\langle\\eta,\\mu\\rangle|\\leq|\\mu|_{1}-\\rho_{\\eta}\\sum_{i\\in[m_{*}]}\\int_{\\mathcal{T}_{i}}\\delta(\\boldsymbol{w},\\boldsymbol{w}_{i}^{*})^{2}\\,\\mathrm{d}|\\mu|(\\boldsymbol{w}).}\\end{array}$ (ii) $\\langle\\eta,\\mu\\,-\\,\\mu^{*}\\rangle\\;=\\;\\langle p,f_{\\mu}\\,-\\,f_{\\mu^{*}}\\rangle$ where $f_{\\mu}(\\pmb{x})\\;=\\;\\mathbb{E}_{\\pmb{w}\\sim\\mu}[\\sigma_{\\geq2}(\\pmb{w}^{\\top}\\pmb{x})]$ .Then $|\\langle\\eta,\\mu\\mathrm{~-~}\\mu^{*}\\rangle|\\;\\leq$ $\\|p\\|_{2}\\,\\sqrt{L(\\mu)}$ ", "page_idx": 23}, {"type": "text", "text": "F.1.2Properties of $\\mu_{\\lambda}^{*}$ ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Given the non-degenerate dual certificate $\\eta$ , we now are ready to identify several useful properties of $\\mu_{\\lambda}^{*}$ . The lemma below essentially says that $\\mu_{\\lambda}^{*}$ is similar to $\\mu^{*}$ in the sense that most of the norm are concentrated in the ground-truth direction and the square loss is small. The proof relies on comparing $\\mu_{\\lambda}^{*}$ with $\\mu^{*}$ using the optimality conditions. ", "page_idx": 23}, {"type": "text", "text": "Lemma F.3. We have the following hold (i) $|\\mu_{*}|_{1}-\\lambda\\,\\|p\\|_{2}^{2}\\leq|\\mu_{\\lambda}^{*}|_{1}\\leq|\\mu^{*}|_{1}=\\|a^{*}\\|_{1}$ (ii) $L(\\mu_{\\lambda}^{*})\\leq\\lambda^{2}\\left\\|p\\right\\|_{2}^{2}=O_{*}(\\lambda^{2})$ (ii) $\\begin{array}{r}{\\sum_{i\\in[m_{*}]}\\int_{\\mathcal{T}_{i}}\\delta(\\pmb{w},\\pmb{w}_{i}^{*})^{2}\\,\\mathrm{d}|\\mu_{\\lambda}^{*}|(\\pmb{w})\\leq\\lambda\\,\\|p\\|_{2}^{2}\\,/\\rho_{\\eta}=O_{*}(\\lambda)}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "F.1.3  Properties of $\\mu$ with optimality gap $\\zeta$ ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We now characterize the structure of $\\mu$ when the optimality gap is $\\zeta$ . The proof mostly relies on comparing $\\mu$ with $\\mu_{\\lambda}^{*}$ and the structure of $\\mu_{\\lambda}^{*}$ in previous section. ", "page_idx": 23}, {"type": "text", "text": "The following lemma shows the square loss is bounded by the optimality gap and norms are always bounded. Note that the conditions are true under Lemma 6. ", "page_idx": 23}, {"type": "text", "text": "Lemma F.4. Recall the optimality gap $\\zeta=L_{\\lambda}(\\mu)-L_{\\lambda}(\\mu_{\\lambda}^{\\ast})$ . Then, the following holds: ", "page_idx": 23}, {"type": "text", "text": "(i) $L(\\mu)\\leq5\\lambda^{2}\\left\\|p\\right\\|^{2}+4\\zeta=O_{*}(\\lambda^{2}+\\zeta).$ (i) $i f\\zeta\\leq\\lambda|\\mu^{*}|_{1}$ and $\\lambda\\leq|\\mu^{*}|_{1}/\\,\\|p\\|_{2}^{2},\\,t h e n\\,|\\mu|_{1}\\leq3|\\mu^{*}|_{1}=3\\,\\|a^{*}\\|_{1}.$ ", "page_idx": 23}, {"type": "text", "text": "The following two lemma characterize the structure of $\\mu$ using the fact that the square loss is small in previous lemma. The lemma below says that the total norm of far away neuron is small. ", "page_idx": 23}, {"type": "text", "text": "Lemma F.5. Recall the optimality gap $\\zeta=L_{\\lambda}(\\mu)-L_{\\lambda}(\\mu_{\\lambda}^{\\ast})$ . Then, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{i\\in[m_{*}]}\\int_{\\mathcal{T}_{i}}\\delta(\\boldsymbol{w},\\boldsymbol{w}_{i}^{*})^{2}\\,\\mathrm{d}|\\mu|(\\boldsymbol{w})\\leq(\\zeta/\\lambda+2\\lambda\\,\\|\\boldsymbol{p}\\|_{2}^{2})/\\rho_{\\eta}=O_{*}(\\zeta/\\lambda+\\lambda).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In particular, when $\\begin{array}{r}{\\mu=\\sum_{i\\in[m]}a_{i}\\left\\lVert\\pmb{w}_{i}\\right\\rVert_{2}\\delta_{\\pmb{\\overline{{w}}}_{i}}}\\end{array}$ represents fnite number of nerons,we hav ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{i\\in[m_{*}]}\\sum_{j\\in{\\mathcal{T}}_{i}}|a_{j}|\\,\\|\\pmb{w}_{j}\\|_{2}\\,\\delta_{j}^{2}\\leq(\\zeta/\\lambda+2\\lambda\\,\\|p\\|_{2}^{2})/\\rho_{\\eta}=O_{*}(\\zeta/\\lambda+\\lambda),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\delta_{j}=\\angle(\\pmb{w}_{j},\\pmb{w}_{i}^{*})$ for $j\\in\\mathcal T_{i}$ ", "page_idx": 23}, {"type": "text", "text": "The lemma below shows there are neurons close to the teacher neurons once the gap is small. The proof idea is similar to Section 5.3 in Zhou et al. (2021) that use test function to lower bound the loss, but nowwe can handle almost all activation. ", "page_idx": 23}, {"type": "text", "text": "Lemma F.6. Under Lemma $\\theta.$ if the Hermite coefficient of $\\sigma$ decays as $\\left|\\hat{\\sigma}_{k}\\right|=\\Theta(k^{-c_{\\sigma}})$ with some constant $c_{\\sigma}>0$ then the total mass near each target direction is large, i.., $\\mu({\\bar{T}}_{i}(\\delta))\\,\\mathrm{sign}(a_{i}^{*})\\geq$ $|a_{i}^{*}|/2$ foral $i\\in[m_{*}]$ $\\begin{array}{r}{\\delta_{c l o s e}\\,\\geq\\,\\widetilde\\Omega\\,\\left((\\frac{L(\\mu)}{a_{\\operatorname*{min}}^{2}})^{1/(4c_{\\sigma}-2)}\\right)}\\end{array}$   \nIn particular, for $\\sigma$ is ReLU or absolute function, $\\begin{array}{r}{\\delta_{c l o s e}\\ge\\widetilde\\Omega\\left((\\frac{L(\\mu)}{a_{\\mathrm{min}}^{2}})^{1/3}\\right)}\\end{array}$ .Here $a_{\\mathrm{min}}=\\operatorname*{min}\\left|a_{i}\\right|$ .is the smallest entry of $\\pmb{a}_{*}$ in absolute value. ", "page_idx": 23}, {"type": "text", "text": "As a corollary, if the optimality gap $\\zeta=L_{\\lambda}(\\mu)-L_{\\lambda}(\\mu_{\\lambda}^{\\ast}),$ then $\\delta_{c l o s e}\\,\\geq\\,\\widetilde{\\Omega}_{\\ast}\\,\\big((\\zeta+\\lambda^{2})^{1/(4c_{\\sigma}-2)}\\big)$ and for ReLU or absolute $\\delta_{c l o s e}\\geq\\widetilde{\\Omega}_{*}\\left((\\zeta+\\lambda^{2})^{1/3}\\right)$ ", "page_idx": 23}, {"type": "text", "text": "F.1.4  Residual decomposition and average neuron ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we introduce the residual decomposition and average neuron as in (Zhou et al., 2021) that will be used when proving the existence of descent direction. ", "page_idx": 24}, {"type": "text", "text": "Denote the decomposition $R({\\pmb x})=f_{\\mu}({\\pmb x})-f_{\\mu^{*}}({\\pmb x})=R_{1}({\\pmb x})+R_{2}({\\pmb x})+R_{3}({\\pmb x})$ (this can be directly verified noticing that $\\sigma_{\\geq2}(x)=|x|/2-1/{\\sqrt{2\\pi}})$ \uff0c ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle R_{1}({\\pmb x})=\\frac{1}{2}\\sum_{i\\in[m_{*}]}\\left(\\sum_{j\\in{\\mathcal{T}}_{i}}a_{j}{\\pmb w}_{j}-{\\pmb w}_{i}^{*}\\right)^{\\top}\\boldsymbol{x}\\mathrm{\\sign}({\\pmb w}_{i}^{*\\top}{\\pmb x})},}\\\\ {{\\displaystyle R_{2}({\\pmb x})=\\frac{1}{2}\\sum_{i\\in[m_{*}]}\\sum_{j\\in{\\mathcal{T}}_{i}}a_{j}{\\pmb w}_{j}^{\\top}\\boldsymbol{x}(\\mathrm{sign}({\\pmb w}_{j}^{\\top}{\\pmb x})-\\mathrm{sign}({\\pmb w}_{i}^{*\\top}{\\pmb x})),}}\\\\ {{\\displaystyle R_{3}({\\pmb x})=\\frac{1}{\\sqrt{2\\pi}}\\left(\\sum_{i\\in[m_{*}]}a_{i}^{*}\\,\\|{\\pmb w}_{i}^{*}\\|_{2}-\\sum_{i\\in[m]}a_{i}\\,\\|{\\pmb w}_{i}\\|_{2}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In the following we characterize $R_{1},R_{2},R_{3}$ separately. In Lemma F.7 we relate $R_{1}$ with the average neuron. In Lemma F.8 and Lemma F.9 we bound $R_{2}$ and $R_{3}$ respectively. ", "page_idx": 24}, {"type": "text", "text": "Lemma F.7 (Zhou et al. (2021), Lemma 11) $\\begin{array}{r}{\\big\\|\\boldsymbol R_{1}\\big\\|_{2}^{2}=\\Omega(\\Delta^{3}/m_{*}^{3})\\sum_{i\\in[m_{*}]}\\Big\\|\\sum_{j\\in\\mathcal T_{i}}a_{j}\\boldsymbol w_{j}-\\boldsymbol w_{i}^{*}\\Big\\|_{2}^{2}.}\\end{array}$ ", "page_idx": 24}, {"type": "text", "text": "Lemma F8. Under Lemma $^{6}$ recall the optimality gap $\\zeta=L_{\\lambda}(\\mu)-L_{\\lambda}(\\mu_{\\lambda}^{\\ast})$ Then ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|{R_{2}}\\right\\|_{2}^{2}=O_{*}((\\zeta/\\lambda+\\lambda)^{3/2}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lemma F9. Under Lemma $^{6}$ and recall the optimality gap $\\zeta=L_{\\lambda}(\\mu)-L_{\\lambda}(\\mu_{\\lambda}^{*})$ If $\\hat{\\sigma}_{0}=0$ and $\\hat{\\sigma}_{k}>0$ with some $k=\\Theta((1/\\Delta^{2})\\log(\\zeta/\\left\\|a_{*}\\right\\|_{1}))$ ,then ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|R_{3}\\|_{2}\\!=\\!\\!\\widetilde{\\cal O}_{\\ast}((\\zeta+\\lambda^{2})^{1/2}/\\hat{\\sigma}_{k}+(\\zeta/\\lambda+\\lambda)+\\zeta).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now we are ready to bound the difference between average neuron with its corresponding ground-truth neuron. ", "page_idx": 24}, {"type": "text", "text": "Lemma F.10. Under Lemma $^{6}$ recall the optimality gap $\\zeta\\,=\\,L_{\\lambda}(\\mu)\\,-\\,L_{\\lambda}(\\mu_{\\lambda}^{*})$ Thenfor any $i\\in[m_{*}]$ $\\zeta=\\Omega(\\lambda^{2})$ and $\\zeta,\\lambda\\leq1/\\mathrm{poly}(m_{*},\\Delta,\\|\\pmb{a}_{*}\\|_{1})$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{j\\in\\mathcal{T}_{i}}a_{j}\\boldsymbol{w}_{j}-\\boldsymbol{w}_{i}^{*}\\right\\|_{2}\\leq\\left(\\sum_{i\\in[m_{*}]}\\left\\|\\sum_{j\\in\\mathcal{T}_{i}}a_{j}\\boldsymbol{w}_{j}-\\boldsymbol{w}_{i}^{*}\\right\\|_{2}^{2}\\right)^{1/2}=O_{*}((\\zeta/\\lambda)^{3/4}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "F.2From ideal loss solution to real loss solution ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In previous section, we consider the ideal loss solution that assumes the norms are perfectly balanced $(\\bar{|\\boldsymbol{a}_{i}|}=\\bar{||\\boldsymbol{w}_{i}||_{2}})$ and $\\alpha,\\beta$ are perfectly fitted. However, during the training we are not able to guarantee achieve these exactly but only approximately. This section is devoted to show that the results in previous section still hold though the conditions are only approximately satisfied. Recall that the Originalloss ", "page_idx": 24}, {"type": "equation", "text": "$$\nL_{\\lambda}(\\pmb\\theta)=L(\\pmb\\theta)+\\frac{\\lambda}{2}\\left\\|\\pmb\\alpha\\right\\|_{2}^{2}+\\frac{\\lambda}{2}\\left\\|\\pmb{W}\\right\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "so that when norm are balanced and $\\alpha,\\beta$ are perfectly fitted, $\\begin{array}{r}{L_{\\lambda}(\\pmb\\theta)=L(\\pmb\\theta)+\\lambda\\sum_{i}|a_{i}|\\,\\|\\pmb w_{i}\\|_{2}=}\\end{array}$ $L_{\\lambda}(\\mu)$ ", "page_idx": 24}, {"type": "text", "text": "The lemma below shows that the properties of ideal loss solution in previous section still hold for the solution of original loss, when $\\alpha,\\beta$ are approximately fitted. ", "page_idx": 24}, {"type": "text", "text": "Lemma F.11. Given any $\\pmb{\\theta}=(\\pmb{a},\\pmb{W},\\alpha,\\beta)$ satisfying $|\\alpha-\\hat{\\alpha}|^{2}=O(\\zeta),$ $\\left\\|\\beta-\\hat{\\beta}\\right\\|_{2}^{2}=O(\\zeta),$ where $\\begin{array}{r}{\\hat{\\alpha}\\,=\\,-(1/\\sqrt{2\\pi})\\sum_{i=1}^{m}a_{i}\\,\\|\\pmb{w}_{i}\\|_{2}}\\end{array}$ and $\\begin{array}{r}{\\hat{\\beta}\\,=\\,-(1/2)\\sum_{i=1}^{m}a_{i}{\\pmb w}_{i}}\\end{array}$ .Let its corresponding balanced ", "page_idx": 24}, {"type": "text", "text": "version $\\begin{array}{r}{\\mathfrak{h}_{o l}=(\\mathbf{a}_{b a l},W_{b a l},\\alpha_{b a l},\\beta_{b a l})\\;a s\\;\\mathfrak{a}_{b a l,i}=\\mathrm{sign}(a_{i})\\sqrt{|a_{i}|\\left\\|w_{i}\\right\\|_{2}},\\;\\mathfrak{w}_{b a l,i}=\\overline{{\\mathfrak{w}}}_{i}\\sqrt{|a_{i}|\\left\\|w_{i}\\right\\|_{2}},}\\end{array}$ $\\alpha_{b a l}=\\hat{\\alpha}$ and $\\beta_{b a l}=\\hat{\\beta}$ Then, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\nL_{\\lambda}(\\pmb\\theta)-L_{\\lambda}(\\theta_{b a l})=|\\alpha-\\hat{\\alpha}|^{2}+\\left\\|\\pmb\\beta-\\hat{\\beta}\\right\\|_{2}^{2}+\\frac{\\lambda}{2}\\sum_{i\\in[m]}(|a_{i}|-\\|\\pmb w_{i}\\|_{2})^{2}\\geq0.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Moreover, let the optimality gap $\\zeta=L_{\\lambda}(\\pmb\\theta)-L_{\\lambda}(\\mu_{\\lambda}^{*}),$ we have results in Lemma $F.$ 4, Lemma F.5, Lemma F.6, Lemma F.7, Lemma F.8, Lemma $F.9$ andLemma $F.I O$ still hold for ${\\cal L}_{\\lambda}(\\pmb\\theta)$ with the change of $R_{3}$ in (8) as ", "page_idx": 25}, {"type": "equation", "text": "$$\nR_{3}(\\pmb{x})=\\frac{1}{\\sqrt{2\\pi}}\\left(\\sum_{i\\in[m*]}a_{i}^{*}\\left\\|\\pmb{w}_{i}^{*}\\right\\|_{2}-\\sum_{i\\in[m]}a_{i}\\left\\|\\pmb{w}_{i}\\right\\|_{2}\\right)+\\alpha-\\hat{\\alpha}+(\\beta-\\hat{\\beta})^{\\top}\\pmb{x}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The following lemma shows the norm remains bounded. ", "page_idx": 25}, {"type": "text", "text": "Lemma F12. Under Lemma $6$ suppose optimalitygap $\\zeta=L_{\\lambda}(\\pmb\\theta)\\!-\\!L_{\\lambda}(\\mu_{\\lambda}^{*})$ Then $\\left\\|\\mathbfit{a}\\right\\|_{2}^{2}+\\left\\|W\\right\\|_{F}^{2}\\leq$ $3\\left\\|a_{*}\\right\\|_{1}$ ", "page_idx": 25}, {"type": "text", "text": "F.3 Descent direction ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we show that there is a descent direction as long as the optimality gap is small until it reaches ${\\cal O}(\\lambda^{2})$ .Wewill assume $\\zeta=\\Omega(\\lambda^{2})$ in this section for simplicity. ", "page_idx": 25}, {"type": "text", "text": "We first show gradient is always large whenever $\\alpha,\\beta$ are not ftted well. This is a direct corollary of Claim B.1. ", "page_idx": 25}, {"type": "text", "text": "Lemma F.13 (Descent direction, $\\alpha$ and $\\beta$ ).We have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left|\\nabla_{\\alpha}L_{\\lambda}\\right|^{2}=4(\\alpha-\\hat{\\alpha})^{2},\\quad\\left\\|\\nabla_{\\beta}L_{\\lambda}\\right\\|_{2}^{2}=4\\left\\|\\beta-\\hat{\\beta}\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Before proceeding to the following descent direction, we first make a simplification assumption that Assumption F.1. For every $\\mathcal{T}_{i}$ forallneuron $w_{j}\\in{\\mathcal{T}}_{i}$ assume $\\pmb{w}_{j}^{\\top}\\pmb{w}_{i}^{*}\\geq0$ ", "page_idx": 25}, {"type": "text", "text": "This is because due to the linear term $\\beta$ , the effective activation is symmetry $\\sigma_{\\geq2}(x)=\\sigma_{\\geq2}(-x)$ This introduce the ambiguity of the sign of neurons. Such assumption clarifies the ambiguity of neurons' direction. ", "page_idx": 25}, {"type": "text", "text": "As the lemma below shows, there always exists a set of parameter (by flipping the sign of neurons) that satisfy the assumption and gives almost same gradient norm. Thus, making such assumption will not cause any issue when $\\alpha,\\beta$ are perfectly fitted. ", "page_idx": 25}, {"type": "text", "text": "Lemma F.14. Suppose $(\\alpha-{\\hat{\\alpha}})^{2}$ \uff0c $\\left\\|\\beta-\\hat{\\beta}\\right\\|_{2}^{2}\\leq\\tau$ to be small enough and $\\|\\pmb{a}\\|_{2}\\,,\\|\\pmb{W}\\|_{F}=O_{*}(1)$ Then, given any parameter $\\pmb{\\theta}$ there exists another set of parameter $\\widetilde{\\pmb{\\theta}}$ that satisfies Assumption $F.I$ such that $f_{\\pmb{\\theta}}=f_{\\widetilde{\\pmb{\\theta}}}$ and $|\\|\\nabla_{\\theta}L_{\\lambda}\\|-\\left\\|\\nabla_{\\widetilde{\\theta}}L_{\\lambda}\\right\\|_{F}|\\leq O_{*}(\\sqrt{\\tau})$ ", "page_idx": 25}, {"type": "text", "text": "Proof. Denote $\\pmb{\\theta}=(\\pmb{a},\\pmb{w}_{1},\\dots,\\pmb{w}_{m},\\alpha,\\beta)$ . We first construct $\\widetilde{\\pmb{\\theta}}=(\\widetilde{\\pmb{a}},\\widetilde{\\pmb{w}}_{1},\\dots,\\widetilde{\\pmb{w}}_{m},\\widetilde{\\alpha},\\widetilde{\\beta})$ ", "page_idx": 25}, {"type": "text", "text": "Let $\\widetilde{\\pmb{a}}=\\pmb{a}$ . For $\\widetilde{\\pmb{w}}_{i}$ ,there exists such signvector $\\pmb{s}=(s_{1},\\dots,s_{m})\\in\\{\\pm1\\}^{m}$ so that by flipping the_sign of neuronswehave $\\widetilde{\\pmb{w}}_{i}~=~s_{i}\\pmb{w}_{i}$ satisfiesAssumption F.1.Let $\\widetilde{\\alpha}\\;=\\;\\alpha$ and $\\widetilde{\\beta}\\,=\\,\\beta\\,+$ $+\\textstyle\\sum_{i:s_{i}=-1}a_{i}\\pmb{w}_{i}$ ", "page_idx": 25}, {"type": "text", "text": "One can verify that $f_{\\pmb{\\theta}}=f_{\\widetilde{\\pmb{\\theta}}}$ . Moreover, for the gradient of $\\alpha,\\beta$ we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\nabla_{\\alpha}L_{\\lambda}=\\nabla_{\\tilde{\\alpha}}L_{\\lambda},\\nabla_{\\beta}L_{\\lambda}=\\nabla_{\\tilde{\\beta}}L_{\\lambda},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For gradient of $\\mathbf{\\Delta}_{\\mathbf{a},\\mathbf{\\nabla}w_{i}}$ ,when $s_{i}=1$ we know they are the same. When $s_{i}=-1$ , note that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{a_{i}}L_{\\lambda}-\\nabla\\widetilde{a}_{i}L_{\\lambda}=2\\mathbb{E}_{x}[R(x)(\\sigma(w_{i}^{\\top}x)-\\sigma(\\widetilde{w}_{i}^{\\top}x))]=2(\\beta-\\hat{\\beta})^{\\top}w_{i}}\\\\ &{\\nabla_{w_{i}}L_{\\lambda}+\\nabla\\widetilde{w}_{i}L_{\\lambda}=2a_{i}\\mathbb{E}_{x}[R(x)(\\sigma^{\\prime}(w_{i}^{\\top}x)+\\sigma^{\\prime}(\\widetilde{w}_{i}^{\\top}x))x]=2a_{i}(\\beta-\\hat{\\beta}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore, we get the desired result by noting the norm bound. ", "page_idx": 25}, {"type": "text", "text": "We then show that if norms are not balanced or norm cancellation happens for neurons with similar direction, then one can always adjust the norm to decrease the loss due to the regularization term. ", "page_idx": 26}, {"type": "text", "text": "Lemma F.15 (Descent direction, norm balance). We have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{i}\\sum_{j\\in T_{i}}\\left|\\langle\\nabla_{a_{j}}L_{\\lambda},-a_{j}\\rangle+\\langle\\nabla_{w_{j}}L_{\\lambda},w_{j}\\rangle\\right|=\\lambda\\displaystyle\\sum_{i\\in[m_{*}]}\\left|a_{i}^{2}-\\|w_{i}\\|_{2}^{2}\\right|}&{}\\\\ {\\displaystyle\\geq\\operatorname*{max}\\left\\{\\lambda\\|a\\|_{2}^{2}-\\|W\\|_{F}^{2}\\,|\\,,\\lambda\\displaystyle\\sum_{i\\in[m_{*}]}(|a_{i}|-\\|w_{i}\\|_{2})^{2}\\right\\}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Lemma F.16 (Descent direction, norm cancellation). Under Lemma $^{6}$ andAssumption $F.I$ suppose the optimality gap $\\zeta=L_{\\lambda}(\\pmb\\theta)-L_{\\lambda}(\\mu_{\\lambda}^{\\ast})$ . For any $\\pmb{w}_{i}^{*}$ , consider $\\delta_{\\mathrm{sign}}$ such that $\\delta_{c l o s e}\\,<\\,\\delta_{\\mathrm{sign}}\\,=$ $O(\\lambda/\\zeta^{1/2})$ with small enough hidden constant( $\\delta_{c l o s e}$ definedinLemma $F.6$ ), then ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{\\in\\{+,-\\}}\\sum_{j\\in T_{i,s}\\left(\\delta_{\\mathrm{sign}}\\right)}\\left\\langle\\nabla_{a_{j}}L_{\\lambda},\\frac{a_{j}}{\\sum_{j\\in T_{i,s}\\left(\\delta_{\\mathrm{sign}}\\right)}\\left|a_{j}\\right|\\left\\|w_{j}\\right\\|_{2}}\\right\\rangle+\\left\\langle\\nabla_{w_{j}}L_{\\lambda},\\frac{w_{j}}{\\sum_{j\\in T_{i,s}\\left(\\delta_{\\mathrm{sign}}\\right)}\\left|a_{j}\\right|\\left\\|w_{j}\\right\\|_{2}}\\right\\rangle=0,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\begin{array}{r}{\\ddot{\\iota}_{i,+}(\\delta_{\\mathrm{sign}})=\\{j\\in T_{i}:\\delta({\\boldsymbol w}_{j},{\\boldsymbol w}_{i}^{*})\\leq\\delta_{\\mathrm{sign}},\\mathrm{sign}(a_{j})=\\mathrm{sign}(a_{i}^{*})\\},\\,T_{i,-}(\\delta_{\\mathrm{sign}})=\\{j\\in T_{i}:\\delta({\\boldsymbol w}_{i},{\\boldsymbol w}_{i}^{*})\\}.}\\end{array}$ $\\delta(\\pmb{w}_{j},\\pmb{w}_{i}^{*})\\leq\\bar{\\delta_{\\mathrm{sign}}}$ \uff0c $\\mathrm{sign}(a_{j})\\ne\\mathrm{sign}(a_{i}^{*})\\}$ are theset of neurons that close to $\\pmb{w}_{i}^{*}$ with/without same sign of $a_{i}^{*}$ ", "page_idx": 26}, {"type": "text", "text": "As a result, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\|\\nabla_{a}L_{\\lambda}\\right\\|_{2}^{2}+\\left\\|\\nabla_{W}L_{\\lambda}\\right\\|_{F}^{2}\\geq\\lambda^{2}\\sum_{j\\in T_{i,-}(\\delta_{\\mathrm{sign}})}\\left|a_{j}\\right|\\left\\|\\pmb{w}_{j}\\right\\|_{2}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Now given the above lemmas, it suffices to consider the remaining case that $\\alpha,\\beta$ are well fitted, norms are balanced and no cancellation. In this case, the loss landscape is roughly the same as the ideal loss (5) from Lemma F.11. Thus, we could leverage these detailed characterization of the solution (far-away neurons are small and average neuron is close to corresponding ground-truth neuron) to construct descent direction. ", "page_idx": 26}, {"type": "text", "text": "Lemma E.17 (Descent direction). Under Lemma $^{6}$ and Assumption $F.I$ suppose the optimality gap $\\zeta=L_{\\lambda}(\\pmb\\theta)-L_{\\lambda}(\\mu_{\\lambda}^{*})$ Suppose ", "page_idx": 26}, {"type": "equation", "text": "$\\begin{array}{r}{|\\|\\pmb{W}\\|_{F}^{2}-\\|\\pmb{a}\\|_{2}^{2}|\\leq\\zeta/\\lambda,\\sum_{i\\in[m]}(|a_{j}|-\\|\\pmb{w}_{j}\\|_{2})^{2}=O_{*}(\\zeta^{2}/\\lambda^{2})}\\end{array}$ ", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "(i) (almost) no norm cancellation: consider all neurons ${\\pmb w}_{j}$ that are $\\delta_{\\mathrm{sign}}$ -close w.rt. teacher neuron $\\pmb{w}_{i}^{*}$ but has a different sign, i.e., $\\mathrm{sign}(a_{j})\\ne\\mathrm{sign}(a_{i}^{*})$ with $\\delta_{\\mathrm{sign}}\\,=\\,\\Theta_{\\ast}(\\lambda/\\zeta^{1/2})$ we have $\\begin{array}{r}{\\sum_{j\\in T_{i,-}(\\delta_{\\mathrm{sign}})}|a_{j}|\\,\\|{\\boldsymbol w}_{j}\\|_{2}\\le\\tau=O_{*}(\\zeta^{5/6}/\\lambda)}\\end{array}$ with smalleough hiden constant, where $T_{i,-}(\\delta)$ definedinLemmaF.16. ", "page_idx": 26}, {"type": "text", "text": "(ii) $\\alpha,\\beta$ are wellftted: $\\lvert\\alpha-\\hat{\\alpha}\\rvert^{2}=O_{*}(\\zeta),\\,\\Big\\lVert\\beta-\\hat{\\beta}\\Big\\rVert_{2}^{2}=O_{*}(\\zeta)$ with smallenough hidden factor ", "page_idx": 26}, {"type": "text", "text": "Then, we can construct the following descent direction ", "page_idx": 26}, {"type": "equation", "text": "$$\n(\\alpha+\\alpha_{\\ast})\\nabla_{\\alpha}L_{\\lambda}+\\langle\\nabla_{\\beta}L_{\\lambda},\\beta+\\beta_{\\ast}\\rangle+\\sum_{i\\in[m_{\\ast}]}\\sum_{j\\in{\\mathcal{T}}_{i}}\\langle\\nabla_{w_{i}}L_{\\lambda},w_{j}-q_{i j}w_{i}^{\\ast}\\rangle=\\Omega(\\zeta),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Where $q_{i j}$ satisfy the following conditions with $\\delta_{c l o s e}\\,\\,<\\,\\,\\delta_{\\mathrm{sign}}$ and $\\delta_{c l o s e}\\ =\\ O_{*}(\\zeta^{1/3}).$ (1) $\\textstyle\\sum_{j\\in{\\mathcal{T}}_{i}}a_{j}q_{i j}\\;=\\;a_{i}^{*},$ (2) $q_{i j}~\\geq~0$ (3) $q_{i j}~=~0$ when $\\mathrm{sign}(a_{j})\\ \\stackrel{\\sim}{\\neq}\\ \\mathrm{sign}(a_{i}^{*})$ or $\\delta_{j}~>~\\delta_{c l o s e}$ (4) $\\begin{array}{r}{\\sum_{i\\in[m_{*}]}\\sum_{j\\in{\\mathcal{T}}_{i}}q_{i j}^{2}=O_{*}(1).}\\end{array}$ ", "page_idx": 26}, {"type": "text", "text": "F.4 Proof of Lemma 6 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Now we are ready to prove the gradient lower bound (Lemma 6) by combining all descent direction lemma in the previous section together. ", "page_idx": 26}, {"type": "text", "text": "Lemma 6 (Gradient lower bound). When $\\Omega_{*}(\\lambda^{2})\\le\\zeta\\le{\\cal O}_{*}(\\lambda^{9/5})$ and $\\lambda\\leq O_{*}(1)$ wehave ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left\\|\\nabla_{\\theta}L_{\\lambda}\\right\\|_{F}^{2}\\geq\\Omega_{*}(\\zeta^{4}/\\lambda^{2}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. We check the assumption of Lemma F.17 one by one. We first assume Assumption F.1 holds to get a gradient lower bound. ", "page_idx": 27}, {"type": "text", "text": "For assumption (i) (norm balance) in Lemma F.17, whenever $\\begin{array}{r}{\\sum_{i\\in[m_{*}]}\\left|a_{i}^{2}-\\|\\pmb{w}_{i}\\|_{2}^{2}\\right|=\\Omega_{*}(\\zeta^{2}/\\lambda^{2}),}\\end{array}$ by Lemma F.15 we know ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{i}\\sum_{j\\in T_{i}}\\left|\\langle\\nabla_{a_{j}}L_{\\lambda},-a_{j}\\rangle+\\langle\\nabla_{{\\pmb w}_{j}}L_{\\lambda},{\\pmb w}_{j}\\rangle\\right|\\ge\\!\\Omega_{\\ast}(\\zeta^{2}/\\lambda).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "With Lemma F.12, this implies ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\langle\\|\\nabla_{a}L_{\\lambda}\\|_{2}^{2}+\\|\\nabla_{W}L_{\\lambda}\\|_{F}^{2}\\cdot O(\\|a_{*}\\|_{1})\\ge\\sqrt{\\|\\nabla_{a}L_{\\lambda}\\|_{2}^{2}+\\|\\nabla_{W}L_{\\lambda}\\|_{F}^{2}}\\sqrt{\\|a\\|_{2}^{2}+\\|W\\|_{F}^{2}}=\\Omega_{*}(\\zeta^{2}/\\lambda)\\right\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which means ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla_{\\theta}L_{\\lambda}\\|_{F}^{2}\\geq\\|\\nabla_{a}L_{\\lambda}\\|_{2}^{2}+\\|\\nabla_{W}L_{\\lambda}\\|_{F}^{2}\\geq\\Omega_{*}(\\zeta^{4}/\\lambda^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For assumption (i) (norm cancellation) in Lemma F.17, whenever it does not hold, by Lemma F.16 weknow ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\theta}L_{\\lambda}\\|_{F}^{2}\\geq\\|\\nabla_{a}L_{\\lambda}\\|_{2}^{2}+\\|\\nabla_{W}L_{\\lambda}\\|_{F}^{2}\\geq\\lambda^{2}\\sum_{j\\in T_{i,-}(\\delta_{\\mathrm{sign}})}|a_{j}|\\,\\|w_{j}\\|_{2}\\geq\\Omega_{*}(\\zeta^{5/6}\\lambda).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For assumption (i) $(\\alpha,\\beta)$ in Lemma F.17, whenever it does not hold, by Lemma F.13 we know ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\nabla_{\\alpha}L_{\\lambda}|^{2}=(\\alpha-\\hat{\\alpha})^{2}=\\Omega_{\\ast}(\\zeta^{2}),\\quad\\|\\nabla_{\\beta}L_{\\lambda}\\|_{2}^{2}=4\\left\\|\\beta-\\hat{\\beta}\\right\\|_{2}^{2}=\\Omega_{\\ast}(\\zeta^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which implies ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla_{\\theta}L_{\\lambda}\\|_{F}^{2}\\geq|\\nabla_{\\alpha}L_{\\lambda}|^{2}+\\|\\nabla_{\\beta}L_{\\lambda}\\|_{2}^{2}=\\Omega_{*}(\\zeta^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, the remaining case is the one that all assumption (i)-(i) in Lemma F.17 hold and also $\\begin{array}{r}{\\sum_{i\\in[m_{*}]}\\left|a_{i}^{2}-\\|\\pmb{w}_{i}\\|_{2}^{2}\\right|=O_{*}(\\zeta^{2}/\\lambda^{2})}\\end{array}$ ,wechoose ", "page_idx": 27}, {"type": "equation", "text": "$$\nq_{i j}=\\left\\{\\begin{array}{l l}{\\frac{a_{j}a_{i}^{*}}{\\sum_{j\\in{\\cal T}_{i,+}(\\delta_{c l o s e})}a_{j}^{2}}}&{\\mathrm{,if~}j\\in{\\cal T}_{i,+}(\\delta_{c l o s e})}\\\\ {0}&{\\mathrm{,otherwise}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "so that condition (1)-(4) on $q_{i j}$ all hold: condition (1)-(3) are easy to check, Lemma H.4 shows condition (4) holds. Now we know from Lemma F.17 that ", "page_idx": 27}, {"type": "equation", "text": "$$\n(\\alpha+\\alpha_{\\ast})\\nabla_{\\alpha}L_{\\lambda}+\\langle\\nabla_{\\beta}L_{\\lambda},\\beta+\\beta_{\\ast}\\rangle+\\sum_{i\\in[m_{\\ast}]}\\sum_{j\\in{\\mathcal{T}}_{i}}\\langle\\nabla_{w_{i}}L_{\\lambda},w_{j}-q_{i j}w_{i}^{\\ast}\\rangle=\\Omega(\\zeta).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Note that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle(\\alpha+\\alpha_{*})\\nabla_{\\alpha}L_{\\lambda}+\\langle\\nabla_{\\beta}L_{\\lambda},\\beta+\\beta_{*}\\rangle+\\sum_{i\\in[m_{*}]}\\sum_{j\\in{\\mathcal{T}}_{i}}\\langle\\nabla_{w_{i}}L_{\\lambda},w_{j}-q_{i j}w_{i}^{*}\\rangle}}\\\\ {{\\displaystyle\\leq\\sqrt{\\left|\\nabla_{\\alpha}L_{\\lambda}\\right|^{2}+\\|\\nabla_{\\beta}L_{\\lambda}\\right|_{2}^{2}+\\|\\nabla_{\\alpha}L_{\\lambda}\\|_{2}^{2}}+\\|\\nabla_{W}L_{\\lambda}\\|_{F}^{2}\\sqrt{\\left(\\alpha+\\alpha_{*}\\right)^{2}+\\|\\beta+\\beta_{*}\\|_{2}^{2}+\\sum_{i\\in[m_{*}]}\\sum_{j\\in{\\mathcal{T}}_{i}}\\|w_{j}-q_{i j}\\|_{F}^{2}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\alpha+\\alpha_{*}|\\leq|\\hat{\\alpha}|+|\\alpha_{*}|+O_{*}(\\zeta)\\overset{\\mathrm{(a)}}{\\leq}O_{*}(1)}\\\\ &{\\|\\beta+\\beta_{*}\\|_{2}\\leq\\left\\|\\hat{\\beta}\\right\\|_{2}+\\left\\|\\beta_{*}\\right\\|_{2}+O_{*}(\\zeta)\\overset{\\mathrm{(b)}}{\\leq}O_{*}(1)}\\\\ &{\\displaystyle\\sum_{i\\in[m_{*}]}\\sum_{j\\in\\mathcal{T}_{i}}\\|w_{j}-q_{i j}w_{i}^{*}\\|_{2}^{2}\\leq2\\displaystyle\\sum_{i\\in[m_{*}]}\\sum_{j\\in\\mathcal{T}_{i}}\\|w_{j}\\|_{2}^{2}+q_{i j}^{2}\\left\\|w_{i}^{*}\\right\\|_{2}^{2}\\leq O_{*}(1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where (a)(b) by Lemma F.4; (c) we use Lemma F.12 and condition (4) on $q_{i j}$ ", "page_idx": 28}, {"type": "text", "text": "Therefore, we get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\theta}L_{\\lambda}\\|_{F}^{2}=|\\nabla_{\\alpha}L_{\\lambda}|^{2}+\\|\\nabla_{\\beta}L_{\\lambda}\\|_{2}^{2}+\\|\\nabla_{a}L_{\\lambda}\\|_{2}^{2}+\\|\\nabla_{W}L_{\\lambda}\\|_{F}^{2}=\\Omega_{*}(\\zeta^{2}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Combine all cases above, we know ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|\\nabla_{a}L_{\\lambda}\\|_{2}^{2}+\\|\\nabla_{W}L_{\\lambda}\\|_{F}^{2}=\\Omega_{*}(\\operatorname*{min}\\{\\zeta^{4}/\\lambda^{2},\\zeta^{5/6}\\lambda,\\zeta^{2}\\})=\\Omega_{*}(\\zeta^{4}/\\lambda^{2}),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "ao ivug as s -U(^' / puiy(', ll\\*,\u2192, llw\\*ll1 ,umin)). ", "page_idx": 28}, {"type": "text", "text": "We now use Lemma F.14 to show when Assumption F.1 is not true, we can get similar gradient lower   \nbound. Denote the above gradient lower bound as $\\tau_{0}=\\Omega_{*}(\\zeta^{4}/\\lambda^{2})$ . Let $\\tau=\\tau_{0}/2$   \nWhen $(\\alpha-\\hat{\\alpha})^{2}\\geq\\tau$ or $\\left\\lVert\\beta-\\hat{\\beta}\\right\\rVert_{2}^{2}\\geq\\tau$ from Lemma F.13 we know $\\|\\nabla_{\\theta}L_{\\lambda}\\|_{F}^{2}\\geq\\tau$   \nWhen $\\left.(\\alpha-\\hat{\\alpha})^{2},\\left\\|\\beta-\\hat{\\beta}\\right\\|_{2}^{2}\\leq\\tau$ using Lemma F.14 we know there exists $\\widetilde{\\pmb{\\theta}}$ such hat $\\left\\Vert\\nabla_{\\tilde{\\theta}}L_{\\lambda}\\right\\Vert_{F}^{2}\\geq\\tau_{0}$   \nand $|\\left\\lVert\\nabla_{\\widetilde{\\theta}}L_{\\lambda}\\right\\rVert_{F}-\\left\\lVert\\nabla_{\\theta}L_{\\lambda}\\right\\rVert_{F}|\\le\\sqrt{\\tau}$ Thus, we know $\\left\\lVert\\nabla_{\\theta}L_{\\lambda}\\right\\rVert_{F}^{2}\\geq0.1\\tau$   \nTherefore, combine above we can show $\\left\\|\\nabla_{\\theta}L_{\\lambda}\\right\\|_{F}^{2}=\\Omega_{*}(\\zeta^{4}/\\lambda^{2})$ ", "page_idx": 28}, {"type": "text", "text": "G   Non-degenerate dual certificate ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we show that there indeed exists a non-degenerate dual certificate that satisfies Definition 1 and therefore proving Lemma F.1. ", "page_idx": 28}, {"type": "text", "text": "Lemma F.1. There exists a non-degenerate dual certificate $\\eta=\\mathbb{E}_{\\pmb{x}}[p(\\pmb{x})\\sigma_{\\ge2}(\\pmb{w}^{\\top}\\pmb{x})]\\,w i t h\\,\\rho_{\\eta}=\\Theta(1)$ and $\\|p\\|_{2}\\leq\\mathrm{poly}(m_{*},\\Delta)$ ", "page_idx": 28}, {"type": "text", "text": "Recall that we want to use the dual certificate $\\eta$ to characterize the (approximate) solution for the following regression problem: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mu\\in\\mathcal{M}(\\mathbb{S}^{d-1})}L_{\\lambda}(\\mu)=\\mathbb{E}_{\\mathbf{x},\\tilde{y}}[(f_{\\mu}(\\mathbf{x})-\\tilde{y})^{2}]+\\lambda|\\mu|_{1}=\\mathbb{E}_{\\mathbf{x}}\\left[\\left(\\int_{w}\\sigma_{\\geq2}(w^{\\top}x)\\mathrm{d}\\,\\mu-\\mu_{*}\\right)^{2}\\right]+\\lambda|\\mu|_{1},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\sigma{_{\\ge2}}$ is the ReLU activation after removing Oth and 1st order (corresponding to $\\alpha$ and $\\beta$ terms) and $\\begin{array}{r}{\\mu_{*}\\equiv\\sum_{i\\in[m_{*}]}a_{i}^{*}\\delta_{\\pmb{w}_{i}^{*}}}\\end{array}$ is the ground-truth. ", "page_idx": 28}, {"type": "text", "text": "Notation We need to first introduce few notations before proceeding to the proof. Denote the kernel $K_{\\geq\\ell}(\\pmb{w},\\pmb{u})=\\mathbb{E}_{\\pmb{x}\\sim N(0,\\pmb{I})}[\\overline{{\\sigma_{\\geq\\ell}}}(\\pmb{\\overline{{w}}}^{\\top}\\pmb{x})\\overline{{\\sigma_{\\geq\\ell}}}(\\pmb{\\overline{{u}}}^{\\top}\\pmb{x})]$ as the kernel induced by activation $\\sigma_{\\geq\\ell}(x)$ , where \u53ef\u2265e(x)=\u2211k\u2265ekhk(x)/Zo, $\\begin{array}{r}{Z_{\\sigma}=\\|\\sigma_{\\geq\\ell}\\|_{2}=\\sqrt{\\sum_{k\\geq\\ell}\\hat{\\sigma}_{k}^{2}}=\\Theta(\\ell^{-3/4})}\\end{array}$ is the normalizing factor, $h_{k}(x)$ is the normalized $k$ -th (probabilistic) Hermite polynomial and $\\hat{\\sigma}_{k}$ is the corresponding Hermite coefficient. We will specify the value of $\\ell$ later and use $K$ instead of $K_{\\ge\\ell}$ for simplicity. ", "page_idx": 28}, {"type": "text", "text": "We will construct the dual certificate $\\eta$ following the proof strategy in Poon et al. (2023) with the form below (the difference is that we now only keep high order terms that are at least $\\ell$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\eta(\\pmb{w})=\\sum_{j\\in[m_{*}]}\\alpha_{1,j}K(\\pmb{w}_{j}^{*},\\pmb{w})+\\sum_{j\\in[m_{*}]}\\alpha_{2,j}^{\\top}\\nabla_{1}K(\\pmb{w}_{j}^{*},\\pmb{w})\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "such that it satisfies ", "page_idx": 28}, {"type": "text", "text": "Here $\\alpha_{1}=(\\alpha_{1},\\ldots,\\alpha_{m_{*}})^{\\top}\\in\\mathbb{R}^{m_{*}},\\alpha_{2}=(\\alpha_{2,1}^{\\top},\\ldots,\\alpha_{2,m_{*}}^{\\top})^{\\top}\\in\\mathbb{R}^{m_{*}d}$ are the parameters that we are going to solve and $\\nabla_{i}$ means the gradient w.r.t. $i$ -th variable (for example, $\\nabla_{1}K({\\pmb x},{\\pmb y})$ means gradient with respect to $\\textbf{\\em x}$ ", "page_idx": 28}, {"type": "text", "text": "One can rewrite the above constraints (9) into the matrix form: ", "page_idx": 28}, {"type": "equation", "text": "$$\n{\\Upsilon}\\left({\\alpha_{1}\\atop\\alpha_{2}}\\right)=b,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\begin{array}{r c l r c l c l}{b}&{=}&{(\\mathrm{sign}(a_{1}^{*}),\\ldots,\\mathrm{sign}(a_{m_{*}}^{*}),\\mathbf{0}_{m^{*}d}^{\\top})^{\\top}}&{\\in}&{\\mathbb{R}^{m_{*}(d+1)},}&{\\!\\!\\!\\!\\mathbf{T}}&{=}&{\\mathbb{E}_{x}[\\gamma(x)\\gamma(\\pmb{x})^{\\top}]}&{\\in}\\end{array}$ $\\mathbb{R}^{m_{*}(d+1)\\times m_{*}(d+1)}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\gamma(x)=(\\overline{{\\sigma_{\\Sigma\\ell}}}(w_{1}^{*\\top}x),\\hdots,\\overline{{\\sigma_{\\Sigma\\ell}}}(w_{m_{*}}^{*\\top}x),\\nabla_{w}\\overline{{\\sigma_{\\Sigma\\ell}}}(\\overline{{w}}_{1}^{*\\top}x)^{\\top},\\hdots,\\nabla_{w}\\overline{{\\sigma_{\\Sigma\\ell}}}(\\overline{{w}}_{m_{*}}^{*\\top}x)^{\\top})^{\\top}\\in\\mathbb{R}^{m_{*}(d+1)}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Here $\\nabla_{w}\\overline{{\\sigma_{\\geq\\ell}}}(\\overline{{\\pmb{w}}}_{i}^{*\\,\\top}{\\pmb{x}})\\,=\\,P_{{\\pmb{w}}_{i}^{*}}\\overline{{\\sigma_{\\geq\\ell}}}^{\\prime}({\\pmb{w}}_{i}^{*\\,\\top}{\\pmb{x}}){\\pmb{x}}\\,\\in\\,\\mathbb{R}^{d}$ ,where $P_{w_{i}^{*}}$ is the projection matix defined below. ", "page_idx": 29}, {"type": "text", "text": "Notions on the unit sphere As we could see, the kernel $K$ is invariant under the change of norms, so it suffices to focus on the input on the unit sphere $\\mathbb{S}^{d-1}$ . On the unite sphere, we could compute the gradient and hessian of a function $f(w)$ on the sphere (e.g., Absil et al. (2013)) ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{grad}\\,f(\\boldsymbol{w})=P_{w}\\nabla f(\\boldsymbol{w}),}\\\\ &{\\mathrm{H}\\,f(\\boldsymbol{w})[\\boldsymbol{z}]=P_{w}(\\nabla^{2}f(\\boldsymbol{w})-\\overline{{\\boldsymbol{w}}}^{\\top}\\nabla f(\\boldsymbol{w})I)\\boldsymbol{z}\\quad\\mathrm{for~all~tangent~vector~}\\boldsymbol{z}\\mathrm{\\that~}\\boldsymbol{z}^{\\top}\\boldsymbol{w}=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $P_{w}=I-w w^{\\top}$ is the projection matrix. ", "page_idx": 29}, {"type": "text", "text": "Then, we could define the derivative as in Poon et al. (2023); Absil et al. (2008): for tangent vectors $z,z^{\\prime}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\operatorname{D}_{0}f(\\pmb{w}):=f(\\pmb{w})}\\\\ &{\\qquad\\operatorname{D}_{1}f(\\pmb{w})[\\pmb{z}]:=\\langle\\pmb{z},\\mathrm{grad}~f(\\pmb{w})\\rangle=\\pm^{\\top}P_{\\pmb{w}}\\nabla f(\\pmb{w})}\\\\ &{\\qquad\\operatorname{D}_{2}f(\\pmb{w})[\\pmb{z},\\pmb{z}^{\\prime}]:=\\langle\\mathrm{H}~f(\\pmb{w})[\\pmb{z}],\\pmb{z}^{\\prime}\\rangle=\\pm^{\\top}P_{\\pmb{w}}(\\nabla^{2}f(\\pmb{w})-\\overline{{\\pmb{w}}}^{\\top}\\nabla f(\\pmb{w})I)P_{\\pmb{w}}\\pmb{z}^{\\prime},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and their associated norms ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathrm{D}_{1}\\,f(\\pmb{w})\\right\\|_{\\pmb{w}}:=\\underset{\\|\\pmb{z}\\|_{\\pmb{w}}=1}{\\operatorname*{sup}}\\,\\mathrm{D}_{1}\\,f(\\pmb{w})[\\pmb{z}]=\\left\\|P_{\\pmb{w}}\\nabla f(\\pmb{w})\\right\\|_{2},}\\\\ &{\\left\\|\\mathrm{D}_{2}\\,f(\\pmb{w})\\right\\|_{\\pmb{w}}:=\\underset{\\|\\pmb{z}\\|_{\\pmb{w}},\\|\\pmb{z}^{\\prime}\\|_{\\pmb{w}}=1}{\\operatorname*{sup}}\\,\\mathrm{D}_{2}\\,f(\\pmb{w})[\\pmb{z},\\pmb{z}^{\\prime}]=\\left\\|P_{\\pmb{w}}\\mathrm{\\,H}\\,f(\\pmb{w})P_{\\pmb{w}}\\right\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\|\\pmb{z}\\|_{\\pmb{w}}=\\|P_{\\pmb{w}}\\pmb{z}\\|_{2}$ ", "page_idx": 29}, {"type": "text", "text": "For simplicity, we will use $K^{(i j)}(\\pmb{w},\\pmb{u})$ to denote $\\nabla_{1}^{i}\\nabla_{2}^{j}K(\\pmb{w},\\pmb{u})$ . One can check that this is in fact the same as the one defined Poon et al. (2023) under our specific kernel $K,i+j\\leq3$ and $i,j\\le2$ Let ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|{K^{(i j)}(\\pmb{w},\\pmb{u})}\\right\\|_{\\pmb{w},\\pmb{u}}:=\\operatorname*{sup}_{\\left\\|\\pmb{z}_{\\pmb{w}}^{(p)}\\right\\|_{\\pmb{w}}=\\left\\|\\pmb{z}_{\\pmb{u}}^{(q)}\\right\\|_{\\pmb{u}}=1,\\pmb{\\cal N}}{K^{(i j)}(\\pmb{w},\\pmb{u})[\\pmb{z}_{\\pmb{w}}^{(1)},\\pmb{\\ldots},\\pmb{z}_{\\pmb{u}}^{(j)}]},}\\\\ {\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad w^{\\top}\\pmb{z}_{\\pmb{w}}^{(p)}\\!=\\!\\pmb{u}^{\\top}\\pmb{z}_{\\pmb{u}}^{(q)}\\!=\\!0\\,\\forall p\\!\\in\\![i],q\\!\\in\\![j]}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $z_{w}^{(p)}$ applies to the dimension corresponding to $\\mathbf{\\nabla}w$ and similarly $z_{u}^{(q)}$ for $\\textbf{\\em u}$ ", "page_idx": 29}, {"type": "text", "text": "Before solving (10), we first present some useful proprieties of kernel $K$ that will be used later (see Section I for the proofs). The lemma below shows that kernel $K(w,u)$ is non-degenerate in the sense that it decays at least quadratic at each ground-truth direction $(\\boldsymbol{w}\\approx\\boldsymbol{u}\\approx\\boldsymbol{w}_{i}^{*}$ )and contributes almost nothing when $\\mathbf{\\nabla}w\\,,\\,u$ areaway. ", "page_idx": 29}, {"type": "text", "text": "Lemma G.1 (Non-degeneracy of kernel $K$ ).For any $h>0,$ let $\\ell\\geq\\Theta(\\Delta^{-2}\\log(m_{*}\\ell/h\\Delta))$ ,kernel $K_{\\ge\\ell}$ is non-degenerate in the sense that there exists $r=\\Theta(\\ell^{-1/2}),\\rho_{1}=\\Theta(1),\\rho_{2}=\\Theta(\\ell)$ such that following hold: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\|K^{(i j)}(w_{1}^{*},w_{k}^{*})\\right\\|_{w_{i}^{*},w_{k}^{*}}\\leq h/m_{*}^{2}f o r\\left(i,j\\right)\\in\\{0,1\\}\\times\\{0,1,2\\}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The following lemma shows that $K$ and its derivatives are bounded. ", "page_idx": 29}, {"type": "text", "text": "LemmaG.2 Regulartycondiions onkeel $K$ 0. Let $\\begin{array}{r}{B_{i j}\\;:=\\;\\operatorname*{sup}_{w,u}\\left\\|K^{(i j)}(w,u)\\right\\|_{w,u}}\\end{array}$ and $B_{0}=B_{00}+B_{10}+1,$ $B_{2}=B_{20}+B_{21}+1$ We have $B_{00}=O(1)$ \uff0c $B_{10}=O(\\ell^{1/2})$ \uff0c $B_{11}=O(\\ell)$ \uff0c $B_{20}=O(\\ell)$ \uff0c $B_{21}=O(\\ell^{3/2})$ , and therefore $B_{0}=O(\\ell^{1/2})$ \uff0c $B_{2}=O(\\ell^{3/2})$ ", "page_idx": 29}, {"type": "text", "text": "The following lemma from Poon et al. (2023) connects the non-degeneracy of kernel $K$ to the dual certificate $\\eta$ that we are interested in. ", "page_idx": 30}, {"type": "text", "text": "Lemma G.3 (Lemma 2, Poon et al. (2023), adapted in our setting). Let $a\\in\\{\\pm1\\}$ .Suppose that for some $\\rho>0$ \uff0c $B>0$ and $0<r\\le B^{-1/2}$ we have: for all $\\delta(w,w_{0})$ and $z\\in\\mathbb{R}^{d}$ with $z^{\\top}w=0$ $i t$ holds that $-K^{(02)}({\\pmb w}_{0},{\\pmb w})[z,z]>\\rho\\left\\|z\\right\\|_{2}^{2}$ and $\\left\\|K^{(02)}(\\pmb{w}_{0},\\pmb{w})\\right\\|_{\\pmb{w}}\\leq B$ Let nbea smoothfunction. f $\\eta(w_{0})=a_{;}$ $\\nabla\\eta(\\pmb{w}_{0})=0$ and $\\left\\|a\\,\\mathrm{D}_{2}\\,\\eta(\\pmb{w})-K^{(02)}(\\pmb{w}_{0},\\pmb{w})\\right\\|_{\\pmb{w}}\\leq\\tau$ for all $\\delta({\\pmb w},{\\pmb w}_{0})\\,\\leq\\,r$ with $\\tau<\\rho/2$ then we have $|\\eta(\\pmb{w})|\\leq1-((\\rho-2\\tau)/2)\\delta(\\pmb{w},\\pmb{w}_{0})^{2}$ for all $\\delta({\\pmb w},{\\pmb w}_{0})\\leq r$ ", "page_idx": 30}, {"type": "text", "text": "We now are ready to proof the main result in this section Lemma F.1 that shows the non-degenerate dual certificate exists. Roughly speaking, following the same proof as in Poon et al. (2023), we can showthat $\\begin{array}{r}{\\alpha\\approx\\mathrm{sign}(a_{*})}\\end{array}$ and $\\alpha_{2}\\approx0$ and therefore we can transfer the non-degeneracy of kernel $K$ tothe dual certificate $\\eta$ with LemmaG.3. ", "page_idx": 30}, {"type": "text", "text": "Lemma F.1. There exists a non-degenerate dual certificate $\\eta=\\mathbb{E}_{\\mathbf{x}}[p(\\mathbf{x})\\sigma_{\\geq2}(\\pmb{w}^{\\top}\\pmb{x})]$ with $\\rho_{\\eta}=\\Theta(1)$ and $\\|p\\|_{2}\\leq\\mathrm{poly}(m_{*},\\Delta)$ ", "page_idx": 30}, {"type": "text", "text": "Proof. Note that $\\mathbf{\\tilde{T}}=S D\\tilde{\\mathbf{r}}D S$ where ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\boldsymbol{D}=\\left(\\begin{array}{l l l l}{\\boldsymbol{I}_{m*}}&&&\\\\ &{\\boldsymbol{P}_{w_{1}^{*}}}&&\\\\ &&{\\ddots}&\\\\ &&&{\\boldsymbol{P}_{w_{m*}^{*}}}\\end{array}\\right),\\quad\\boldsymbol{S}=\\left(\\begin{array}{l l l l}{\\boldsymbol{I}_{m*}}&&&\\\\ &{(\\boldsymbol{Z}_{\\sigma^{\\prime}}/\\boldsymbol{Z}_{\\sigma})\\boldsymbol{I}_{m_{*}}}&&\\\\ &&{\\ddots}&\\\\ &&&{(\\boldsymbol{Z}_{\\sigma^{\\prime}}/\\boldsymbol{Z}_{\\sigma})\\boldsymbol{I}_{m_{*}}}\\end{array}\\right)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "are block diagonal matrices, Y = Ea[(\u03b1)(\u03b1)] E Rm\\*(d+1)\u00d7m(d+1), ", "page_idx": 30}, {"type": "text", "text": "(x)=(e(wTa),..,\u2265e(w\u03b1),(Za/Za)\u2265(wTe)T,.,(Za/Za)\u2265(wTa)T)T\u2208Rm(d+1), $\\begin{array}{r}{Z_{\\sigma^{\\prime}}=\\sqrt{\\sum_{k\\ge\\ell}\\hat{\\sigma}_{k}^{2}k}=\\Theta(\\ell^{-1/4})}\\end{array}$ is the normalizing factor so that the diagonal of $\\widetilde{\\Upsilon}$ are all 1. ", "page_idx": 30}, {"type": "text", "text": "Thus, to solve (10), it is sufficient to solve the following: denote $\\widetilde{\\kappa}=D\\widetilde{\\Upsilon}D$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\widetilde{K}\\left(\\stackrel{\\widetilde{\\alpha}_{1}}{\\widetilde{\\alpha}_{2}}\\right)=b,\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and let $\\alpha_{1}=\\widetilde{\\alpha}_{1}$ \uff0c $\\pmb{\\alpha}_{2,i}=(Z_{\\sigma}/Z_{\\sigma^{\\prime}})\\widetilde{\\pmb{\\alpha}}_{2,i}$ to get the solution of (10)) ", "page_idx": 30}, {"type": "text", "text": "In the following, we are going to first show that $\\widetilde{K}\\approx D D$ because all the off-diagonal terms of $\\widetilde{\\Upsilon}$ are small due to Lemma G.1 (ii) (we can choose $h$ to be small enough, and we will choose it later). Specifically, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\widetilde{K}-D D\\right\\|_{2}=\\underset{\\|\\boldsymbol{z}\\|_{2}=1}{\\operatorname*{sup}}|\\boldsymbol{z}^{\\top}(\\widetilde{K}-D D)\\boldsymbol{z}|}\\\\ &{=\\underset{\\|\\boldsymbol{z}\\|_{2}=1}{\\operatorname*{sup}}\\left|\\sum_{\\boldsymbol{z}\\|_{2}=1}\\!\\!\\!\\!\\!\\!\\sum_{\\boldsymbol{i},\\boldsymbol{i}}\\!\\!\\!K(\\boldsymbol{w}_{i}^{*},\\boldsymbol{w}_{j}^{*})\\boldsymbol{z}_{1,j}+2(Z_{\\sigma}/Z_{\\sigma^{\\prime}})\\sum_{\\boldsymbol{i},j}\\!\\!\\!\\!\\!z_{1,i}\\nabla_{1}K(\\boldsymbol{w}_{i}^{*},\\boldsymbol{w}_{j}^{*})^{\\top}\\boldsymbol{z}_{2,j}\\right.}\\\\ &{\\quad+\\left.(Z_{\\sigma}/Z_{\\sigma^{\\prime}})^{2}\\underset{\\textit{i},j}{\\sum}\\!\\!\\!\\!z_{2,i}^{\\top}\\nabla_{1}\\nabla_{2}K(\\boldsymbol{w}_{i}^{*},\\boldsymbol{w}_{j}^{*})^{\\top}\\boldsymbol{z}_{2,j}\\right|}\\\\ &{\\le\\underset{\\boldsymbol{i},j}{\\sum}|K(\\boldsymbol{w}_{i}^{*},\\boldsymbol{w}_{j}^{*})|+\\Theta(\\ell^{-1/2})\\left\\|K^{(10)}(\\boldsymbol{w}_{i}^{*},\\boldsymbol{w}_{j}^{*})\\right\\|_{\\boldsymbol{w}_{i}^{*}}+\\Theta(\\ell^{-1})\\left\\|K^{(11)}(\\boldsymbol{w}_{i}^{*},\\boldsymbol{w}_{j}^{*})\\right\\|_{\\boldsymbol{w}_{i}^{*},\\boldsymbol{w}_{j}^{*}}\\le2h,}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Wwhere $z=(z_{1}^{\\top},z_{2}^{\\top})^{\\top}$ $z_{1}=(z_{1,1},\\ldots,z_{1,m_{*}})^{\\top}$ and $z_{2}=(z_{2,1}^{\\top},\\ldots,z_{2,m_{*}}^{\\top})^{\\top}$ has the same block structure as $(\\alpha_{1},\\alpha_{2})$ and we use Lemma G.1 and Lemma G.2 in the last line. ", "page_idx": 30}, {"type": "text", "text": "Note that $_{D D}$ has exactly $m_{*}d$ eigenvalues of 1 and $m_{*}$ eigenvalues of $0$ , and $\\widetilde{\\kappa}$ also has $m_{*}$ eigenvalues ofO.ByWeyl's inequalty, we kw $|\\gamma_{i}\\!-\\!1|\\le2h$ where $\\begin{array}{r}{\\widetilde{\\pmb{K}}=\\sum_{i\\in[m_{*}d]}\\gamma_{i}\\pmb{v}_{i}\\pmb{v}_{i}^{\\top}}\\end{array}$ is its eigendecomposition. Here $\\pmb{v}_{i}^{\\top}\\pmb{v}_{\\bot}=0$ for all $\\pmb{v}_{\\bot}\\in V_{\\bot}=\\mathrm{span}\\{(\\pmb{0},\\pmb{w}_{1}^{\\ast},\\pmb{0},\\ldots,\\pmb{0})^{\\top},\\ldots(\\pmb{0},\\ldots,\\pmb{0},\\pmb{w}_{m_{\\ast}}^{\\ast})^{\\top}\\}$ ", "page_idx": 30}, {"type": "text", "text": "in the null space of $_{D}$ . Since $\\pmb{b}^{\\top}\\pmb{v}_{\\bot}=0$ for all ${\\pmb v}_{\\perp}\\in V_{\\perp}$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\binom{\\widetilde{\\alpha}_{1}}{\\widetilde{\\alpha}_{2}}=\\widetilde{\\pmb K}^{\\dagger}b=\\sum_{i\\in[m_{*}d]}\\gamma_{i}^{-1}v_{i}v_{i}^{\\top}b=\\sum_{i\\in[m_{*}d]}\\big(\\gamma_{i}^{-1}-1\\big)v_{i}v_{i}^{\\top}b+\\sum_{i\\in[m_{*}d]}v_{i}v_{i}^{\\top}b}\\\\ &{\\quad\\quad\\quad=\\displaystyle\\sum_{i\\in[m_{*}d]}\\big(\\gamma_{i}^{-1}-1\\big)v_{i}v_{i}^{\\top}b+b.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Therefore, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left\\|\\left({\\widetilde{\\alpha}}_{1}\\right)-b\\right\\|_{2}\\leq\\left\\|\\sum_{i\\in[m_{*}d]}\\left(\\gamma_{i}^{-1}-1\\right)\\boldsymbol{v}_{i}\\boldsymbol{v}_{i}^{\\top}b\\right\\|_{2}\\leq\\operatorname*{max}_{i}\\left|\\gamma_{i}^{-1}-1\\right|{\\sqrt{m_{*}}}=O(h{\\sqrt{m_{*}}})=:h^{\\prime}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "This implies $\\begin{array}{r}{\\|\\alpha_{1}-\\mathrm{sign}(a_{*})\\|_{\\infty}\\,=\\,\\|\\widetilde{\\alpha}_{1}-\\mathrm{sign}(a_{*})\\|_{\\infty}\\,\\le\\,h^{\\prime},}\\end{array}$ $\\|\\pmb{\\alpha}_{1}\\|_{\\infty}\\,=\\,\\|\\widetilde{\\pmb{\\alpha}}_{1}\\|_{\\infty}\\,\\le\\,1+h^{\\prime}$ and $\\|\\pmb{\\alpha}_{2}\\|_{2}=(Z_{\\sigma}/Z_{\\sigma^{\\prime}})\\,\\|\\widetilde{\\pmb{\\alpha}}_{2,i}\\|_{2}\\le\\Theta(h^{\\prime}\\ell^{-1/2})$ ", "page_idx": 31}, {"type": "text", "text": "Now, given the $\\alpha_{1},\\;\\alpha_{2}$ , we can show the corresponding $\\eta$ is non-degenerate. Choosing $h\\,=$ $O(m_{*}^{-1/2})$ and $\\ell=\\Theta(\\Delta^{-2}\\log(m_{*}/\\Delta))$ so that the condition in Lemma G.1 holds. ", "page_idx": 31}, {"type": "text", "text": "Consider $w\\in\\tau_{i}$ ,when $\\delta(\\pmb{w},\\pmb{w}_{i}^{*})\\geq r=\\Theta(\\ell^{-1/2})$ , using Lemma G.1 and Lemma G.2 we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\eta(\\boldsymbol{w})|=\\displaystyle\\left|\\sum_{\\boldsymbol{j}\\in[m_{*}]}\\alpha_{1,j}K(\\boldsymbol{w}_{j}^{*},\\boldsymbol{w})+\\sum_{\\boldsymbol{j}\\in[m_{*}]}\\alpha_{2,j}^{\\top}\\nabla_{1}K(\\boldsymbol{w}_{j}^{*},\\boldsymbol{w})\\right|}\\\\ &{\\qquad\\le\\displaystyle\\sum_{\\boldsymbol{j}\\in[m_{*}]}|\\alpha_{1,j}||K(\\boldsymbol{w}_{j}^{*},\\boldsymbol{w})|+\\sum_{\\boldsymbol{j}\\in[m_{*}]}\\|\\alpha_{2,j}\\|_{\\boldsymbol{w}_{j}^{*}}\\left\\|\\nabla_{1}K(\\boldsymbol{w}_{j}^{*},\\boldsymbol{w})\\right\\|_{\\boldsymbol{w}_{j}^{*}}}\\\\ &{\\qquad\\le(1+h^{\\prime})(1-\\rho_{1}+h)+\\Theta(h^{\\prime}\\ell^{-1/2})(B_{10}+h)\\le1-\\rho_{1}/2\\le1-\\Theta(\\rho_{1})\\delta(\\boldsymbol{w},\\boldsymbol{w}_{i}^{*})^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where we choose $h=O(m_{*}^{-1/2})$ to be small enough. ", "page_idx": 31}, {"type": "text", "text": "When $\\delta({\\pmb w},{\\pmb w}_{i}^{*})\\leq r=\\Theta(\\ell^{-1/2})$ , again using Lemma G.1 and Lemma G.2 we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|a_{i}^{*}\\,\\mathrm{D}_{2}\\,\\eta(\\boldsymbol{w})-K^{(02)}(\\boldsymbol{w}_{i}^{*},\\boldsymbol{w})\\right\\|_{\\boldsymbol{w}}}\\\\ &{\\le\\left\\|\\alpha_{1,i}K^{(02)}(\\boldsymbol{w}_{i}^{*},\\boldsymbol{w})-K^{(02)}(\\boldsymbol{w}_{i}^{*},\\boldsymbol{w})\\right\\|_{\\boldsymbol{w}}+\\displaystyle\\sum_{j\\neq i}\\left\\|\\alpha_{1,j}K^{(02)}(\\boldsymbol{w}_{j}^{*},\\boldsymbol{w})\\right\\|_{\\boldsymbol{w}}+\\sum_{j\\in[m_{*}]}\\|\\alpha_{2,j}\\|_{\\boldsymbol{w}_{j}^{*}}\\left\\|K^{(12)}(\\boldsymbol{w}_{i}^{*},\\boldsymbol{w})\\right\\|_{\\boldsymbol{w}}}\\\\ &{\\le h^{\\prime}B_{02}+(1+h^{\\prime})h+\\Theta(h^{\\prime}\\ell^{-1/2})(B_{21}+h)\\le\\rho_{2}/16,}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where again due to our choice of small $h$ .Using Lemma G.3 we know that $|\\eta(\\pmb{w})|~\\leq~1~-$ $(\\rho_{2}/4)\\delta(\\mathbf{\\bar{w}},\\mathbf{w}_{i}^{*})^{2}$ ", "page_idx": 31}, {"type": "text", "text": "Combine the above two cases, we have $|\\eta(\\pmb{w})|\\leq1\\!-\\!\\Theta(1)\\delta(\\pmb{w},\\pmb{w}_{i}^{*})^{2}$ and $\\eta(\\pmb{w})=\\mathbb{E}_{\\pmb{x}}[p(\\pmb{x})\\sigma(\\pmb{w}^{\\top}\\pmb{x})]$ with ", "page_idx": 31}, {"type": "equation", "text": "$$\np(\\mathbf{x})=\\frac{1}{Z_{\\sigma}^{2}}\\left(\\sum_{j\\in[m_{*}]}\\alpha_{1,j}\\sigma_{\\geq\\ell}(w_{j}^{*\\top}\\pmb{x})+\\sum_{j\\in[m_{*}]}\\alpha_{2,j}^{\\top}(I-w_{i}^{*}w_{i}^{*\\top})\\pmb{x}\\sigma_{\\geq\\ell}^{\\prime}(w_{i}^{*\\top}\\pmb{x})\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We have $\\|p\\|=O(\\ell^{3/4}m_{*}+m_{*}h^{\\prime}\\ell^{-1/2}\\ell^{5/4})=\\widetilde O(\\Delta^{-3/2}m_{*}).$ ", "page_idx": 31}, {"type": "text", "text": "H Proofs in Section $\\mathbf{F}$ ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section, we give the omitted proofs in Section F. ", "page_idx": 31}, {"type": "text", "text": "H.1  Omitted proofs in Section F.1 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We give the proofs for these results that characterize the structure of ideal loss solution. The following proof follows from the definition of non-degenerate dual certificate $\\eta$ ", "page_idx": 31}, {"type": "text", "text": "Lemma F.2. Given a non-degenerate dual certificate $\\eta_{:}$ then ", "page_idx": 32}, {"type": "text", "text": "(i) For any measure $\\begin{array}{r}{\\mu\\in\\mathcal{M}(\\mathbb{S}^{d-1}),\\,|\\langle\\eta,\\mu\\rangle|\\leq|\\mu|_{1}-\\rho_{\\eta}\\sum_{i\\in[m_{*}]}\\int_{\\mathcal{T}_{i}}\\delta(\\boldsymbol{w},\\boldsymbol{w}_{i}^{*})^{2}\\,\\mathrm{d}|\\mu|(\\boldsymbol{w}).}\\end{array}$ ", "page_idx": 32}, {"type": "text", "text": "Proof. We show the results one by one. ", "page_idx": 32}, {"type": "text", "text": "Part (i)(ii) We have ", "text_level": 1, "page_idx": 32}, {"type": "equation", "text": "$$\n\\langle\\eta,\\mu\\rangle|\\leq\\int_{\\mathbb{S}^{d-1}}|\\eta(\\pmb{w})|\\,\\mathrm{d}|\\mu|(\\pmb{w})=\\sum_{i\\in[m_{*}]}\\int_{\\mathcal{T}_{i}}|\\eta(\\pmb{w})|\\,\\mathrm{d}|\\mu|(\\pmb{w})\\leq|\\mu|_{1}-\\rho_{\\eta}\\sum_{i\\in[m_{*}]}\\int_{\\mathcal{T}_{i}}\\delta(\\pmb{w},\\pmb{w}_{i}^{*})^{2}\\,\\mathrm{d}|\\mu|(\\pmb{w})|>|\\mu|_{1}+\\frac{1}{\\delta}|\\mu|_{1}+\\frac{1}{\\delta}|\\mu|_{2}\\leq1\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the last inequality follows the property of non-degenerate dual certificate (Definition 1). The other part then follows directly by the definition of $\\mu^{*}$ ", "page_idx": 32}, {"type": "text", "text": "Part (ii)  We have ", "text_level": 1, "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\eta,\\mu-\\mu^{*}\\rangle=\\displaystyle\\int_{\\mathbb S^{d-1}}\\eta(w)\\,\\mathrm{d}(\\mu-\\mu^{*})(w)=\\displaystyle\\int_{\\mathbb S^{d-1}}\\mathbb E_{x}[p(x)\\sigma_{\\geq2}(w^{\\top}x)]\\,\\mathrm{d}(\\mu-\\mu^{*})(w)}\\\\ &{\\qquad\\qquad=\\mathbb E_{x}\\displaystyle\\left[p(x)\\int_{\\mathbb S^{d-1}}\\sigma_{\\geq2}(w^{\\top}x)\\,\\mathrm{d}(\\mu-\\mu^{*})(w)\\right]}\\\\ &{\\qquad\\qquad=\\mathbb E_{x}[p(x)(f_{\\mu}(x)-f_{\\mu^{*}}(x))].}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Note that $L(\\mu)=\\|f_{\\mu}-f_{\\mu^{*}}\\|_{2}^{2}$ , this leads to $\\lvert\\langle\\eta,\\mu-\\mu^{*}\\rangle\\rvert\\leq\\lvert\\lvert p\\rvert\\rvert_{2}\\,\\sqrt{L(\\mu)}$ ", "page_idx": 32}, {"type": "text", "text": "Given the above lemma and the optimality of $\\mu_{\\lambda}^{*}$ , we are able to characterize the structure of $\\mu_{\\lambda}^{*}$ as below: norm is bounded, square loss is small and far-away neurons are small. ", "page_idx": 32}, {"type": "text", "text": "Lemma F.3. We have the following hold ", "text_level": 1, "page_idx": 32}, {"type": "equation", "text": "$$\n|\\mu_{*}|_{1}-\\lambda\\,\\|p\\|_{2}^{2}\\leq|\\mu_{\\lambda}^{*}|_{1}\\leq|\\mu^{*}|_{1}=\\|a^{*}\\|_{1}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{i\\in[m_{*}]}\\int_{\\mathcal{T}_{i}}\\delta(\\pmb{w},\\pmb{w}_{i}^{*})^{2}\\,\\mathrm{d}|\\mu_{\\lambda}^{*}|(\\pmb{w})\\leq\\lambda\\,\\|p\\|_{2}^{2}\\,/\\rho_{\\eta}=O_{*}(\\lambda)}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. We show the results one by one. ", "page_idx": 32}, {"type": "text", "text": "Part (i)  Due to the optimality of $\\mu_{\\lambda}^{*}$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\nL(\\mu_{\\lambda}^{*})+\\lambda|\\mu_{\\lambda}^{*}|_{1}=L_{\\lambda}(\\mu_{\\lambda}^{*})\\leq L_{\\lambda}(\\mu^{*})=L(\\mu^{*})+\\lambda|\\mu^{*}|_{1}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Rearranging the terms, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\lambda|\\mu_{\\lambda}^{*}|_{1}-\\lambda|\\mu^{*}|_{1}\\leq L(\\mu^{*})-L(\\mu_{\\lambda}^{*})=-L(\\mu_{\\lambda}^{*})\\leq0.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "For the lower bound, with Lemma F.2 we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n0\\leq|\\mu_{\\lambda}^{*}|_{1}-|\\mu^{*}|_{1}-\\langle\\eta,\\mu_{\\lambda}^{*}-\\mu^{*}\\rangle\\leq|\\mu_{\\lambda}^{*}|_{1}-|\\mu^{*}|_{1}+\\|p\\|_{2}\\,\\sqrt{L(\\mu_{\\lambda}^{*})}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Using part (i) we get the desired lower bound. ", "page_idx": 32}, {"type": "text", "text": "Part (i)  We first have the following inequality due to the optimality of $\\mu_{\\lambda}^{\\ast}$ and adding $\\lambda\\langle\\eta,\\mu_{\\lambda}^{*}-\\mu^{*}\\rangle$ on both side: ", "page_idx": 33}, {"type": "equation", "text": "$$\nL(\\mu_{\\lambda}^{*})+\\underbrace{\\lambda(|\\mu_{\\lambda}^{*}|_{1}-|\\mu^{*}|_{1})-\\lambda\\langle\\eta,\\mu_{\\lambda}^{*}-\\mu^{*}\\rangle}_{(I)}\\leq L(\\mu^{*})-\\lambda\\langle\\eta,\\mu_{\\lambda}^{*}-\\mu^{*}\\rangle.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "For $(I)$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n(I)=\\lambda(|\\mu_{\\lambda}^{*}|_{1}-\\langle\\eta,\\mu_{\\lambda}^{*}\\rangle)+\\lambda(\\langle\\eta,\\mu^{*}\\rangle-|\\mu^{*}|_{1})\\ge0,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where we use Lemma F.2 in the last inequality. ", "page_idx": 33}, {"type": "text", "text": "Therefore, the above inequality leads to ", "page_idx": 33}, {"type": "equation", "text": "$$\nL(\\mu_{\\lambda}^{*})\\leq L(\\mu^{*})-\\lambda\\langle\\eta,\\mu_{\\lambda}^{*}-\\mu^{*}\\rangle\\leq\\lambda\\left\\lVert p\\right\\rVert_{2}\\sqrt{L(\\mu_{\\lambda}^{*})},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where we again use Lemma F.2. This further leads to $L(\\mu_{\\lambda}^{*})\\leq\\lambda^{2}\\left\\lVert p\\right\\rVert_{2}^{2}$ ", "page_idx": 33}, {"type": "text", "text": "Part (ii)  Using part (i) we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\lvert\\mu_{\\lambda}^{*}\\rvert_{1}-\\lvert\\mu^{*}\\rvert_{1}-\\langle\\eta,\\mu_{\\lambda}^{*}-\\mu^{*}\\rangle\\leq-\\langle\\eta,\\mu_{\\lambda}^{*}-\\mu^{*}\\rangle.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "With Lemma F.2, LHS and RHS become ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{LHS}=\\lvert\\mu_{\\lambda}^{*}\\rvert_{1}-\\langle\\eta,\\mu_{\\lambda}^{*}\\rangle\\geq\\rho_{\\eta}\\displaystyle\\sum_{i\\in[m_{*}]}\\int_{T_{i}}\\delta(\\pmb{w},\\pmb{w}_{i}^{*})^{2}\\,\\mathrm{d}\\lvert\\mu_{\\lambda}^{*}\\rvert(\\pmb{w})}\\\\ &{\\mathrm{RHS}\\leq\\lVert p\\rVert_{2}\\,\\sqrt{L(\\mu_{\\lambda}^{*})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Then using part (ii) we have the desired result. ", "page_idx": 33}, {"type": "text", "text": "We are now ready to characterize the approximated solution by comparing $\\mu$ and $\\mu_{\\lambda}^{*}$ ", "page_idx": 33}, {"type": "text", "text": "Lemma F4. Recall the optimality gap $\\zeta=L_{\\lambda}(\\mu)-L_{\\lambda}(\\mu_{\\lambda}^{\\ast})$ . Then, the following holds: ", "page_idx": 33}, {"type": "equation", "text": "$$\nL(\\mu)\\leq5\\lambda^{2}\\left\\|p\\right\\|^{2}+4\\zeta=O_{*}(\\lambda^{2}+\\zeta).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. We show the results one by one. ", "page_idx": 33}, {"type": "text", "text": "Part (i)  By the definition of the optimality gap $\\zeta$ and adding $-\\lambda\\langle\\eta,\\mu-\\mu^{*}\\rangle$ on both side, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\nL(\\mu)+\\lambda(|\\mu|_{1}-|\\mu_{\\lambda}^{*}|_{1})-\\lambda\\langle\\eta,\\mu-\\mu^{*}\\rangle\\leq L(\\mu_{\\lambda}^{*})+\\zeta-\\lambda\\langle\\eta,\\mu-\\mu^{*}\\rangle.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Note that on LHS, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda(|\\mu|_{1}-|\\mu_{\\lambda}^{*}|_{1})-\\lambda\\langle\\eta,\\mu-\\mu^{*}\\rangle=\\lambda(|\\mu|_{1}-\\langle\\eta,\\mu\\rangle)+\\lambda(|\\mu^{*}|_{1}-|\\mu_{\\lambda}^{*}|_{1})\\geq0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where we use Lemma F.2 and Lemma F.3. ", "page_idx": 33}, {"type": "text", "text": "Therefore, with Lemma F.2 and Lemma F.3 we get ", "page_idx": 33}, {"type": "equation", "text": "$$\nL(\\mu)\\leq L(\\mu_{\\lambda}^{*})+\\zeta-\\lambda\\langle\\eta,\\mu-\\mu^{*}\\rangle\\leq\\lambda^{2}\\left\\lVert p\\right\\rVert_{2}^{2}+\\zeta+\\lambda\\left\\lVert p\\right\\rVert_{2}\\sqrt{L(\\mu)}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Solving the above inequality on $L(\\mu)$ gives $L(\\mu)\\leq5\\lambda^{2}\\left\\|p\\right\\|_{2}^{2}+4\\zeta$ ", "page_idx": 33}, {"type": "text", "text": "Part (i)  Again from the definition of the optimality gap $\\zeta$ wehave ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\lambda|\\mu|_{1}\\leq L(\\mu_{\\lambda}^{*})+\\lambda|\\mu_{\\lambda}^{*}|_{1}+\\zeta-L(\\mu)\\leq\\lambda^{2}\\left\\|p\\right\\|_{2}^{2}+\\lambda|\\mu^{*}|_{1}+\\zeta,}\\\\ &{\\mathrm{mma~F.3.~Thus,}\\,|\\mu|_{1}\\leq\\lambda\\left\\|p\\right\\|_{2}^{2}+|\\mu^{*}|_{1}+\\zeta/\\lambda\\leq3|\\mu^{*}|_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where we use Ler ", "page_idx": 33}, {"type": "text", "text": "The lemma below shows that far-away neurons are still small even for the approximated solution. Intutively, we use the non-degenerate dual certificate to certify the gap between $\\mu$ and $\\mu_{\\lambda}^{\\ast}$ and give a bound for it. ", "page_idx": 33}, {"type": "text", "text": "Lemma F.5. Recall the optimality gap $\\zeta=L_{\\lambda}(\\mu)-L_{\\lambda}(\\mu_{\\lambda}^{\\ast})$ . Then, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\sum_{i\\in[m_{*}]}\\int_{\\mathcal{T}_{i}}\\delta(\\boldsymbol{w},\\boldsymbol{w}_{i}^{*})^{2}\\,\\mathrm{d}|\\mu|(\\boldsymbol{w})\\leq(\\zeta/\\lambda+2\\lambda\\,\\|\\boldsymbol{p}\\|_{2}^{2})/\\rho_{\\eta}=O_{*}(\\zeta/\\lambda+\\lambda).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "In particular, when $\\begin{array}{r}{\\mu=\\sum_{i\\in[m]}a_{i}\\left\\lVert\\pmb{w}_{i}\\right\\rVert_{2}\\delta_{\\pmb{\\overline{{w}}}_{i}}}\\end{array}$ represents fnite number of neurons, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\sum_{i\\in[m_{*}]}\\sum_{j\\in{\\mathcal{T}}_{i}}|a_{j}|\\,\\|\\pmb{w}_{j}\\|_{2}\\,\\delta_{j}^{2}\\leq(\\zeta/\\lambda+2\\lambda\\,\\|p\\|_{2}^{2})/\\rho_{\\eta}=O_{*}(\\zeta/\\lambda+\\lambda),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\delta_{j}=\\angle(\\pmb{w}_{j},\\pmb{w}_{i}^{*})$ for $j\\in\\mathcal T_{i}$ ", "page_idx": 34}, {"type": "text", "text": "Proof. By the definition of the optimality gap $\\zeta$ ,wehave ", "page_idx": 34}, {"type": "equation", "text": "$$\nL(\\mu)+\\lambda|\\mu|_{1}=L(\\mu_{\\lambda}^{*})+\\lambda|\\mu_{\\lambda}^{*}|_{1}+\\zeta.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Rearranging the terms and adding $-\\langle\\eta,\\mu-\\mu^{*}\\rangle$ on both side, we get ", "page_idx": 34}, {"type": "equation", "text": "$$\n|\\mu|_{1}-|\\mu_{\\lambda}^{*}|_{1}-\\langle\\eta,\\mu-\\mu^{*}\\rangle=\\frac{1}{\\lambda}(L(\\mu_{\\lambda}^{*})-L(\\mu)+\\zeta)-\\langle\\eta,\\mu-\\mu^{*}\\rangle.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "For LHS, with Lemma F.2 and Lemma F.3 we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathrm{LHS}=|\\mu|_{1}-\\langle\\eta,\\mu\\rangle-|\\mu_{\\lambda}^{\\ast}|_{1}+|\\mu^{\\ast}|_{1}\\geq\\rho_{\\eta}\\sum_{i\\in[m_{*}]}\\int_{\\mathcal{T}_{i}}\\delta(\\pmb{w},\\pmb{w}_{i}^{\\ast})^{2}\\,\\mathrm{d}|\\mu|(\\pmb{w}).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "For RHS, with Lemma F.2 and Lemma F.3 we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathrm{RHS}\\leq\\frac{1}{\\lambda}(\\lambda^{2}\\left\\|p\\right\\|_{2}^{2}-L(\\mu)+\\zeta)+\\left\\|p\\right\\|_{2}\\sqrt{L(\\mu)}=\\frac{\\zeta}{\\lambda}+\\lambda\\left\\|p\\right\\|_{2}^{2}-\\frac{L(\\mu)}{\\lambda}+\\left\\|p\\right\\|_{2}\\sqrt{L(\\mu)}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "When $L(\\mu)\\geq\\lambda^{2}\\left\\|p\\right\\|_{2}^{2}$ we have $\\mathrm{RHS}\\leq\\zeta/\\lambda+\\lambda\\left\\|p\\right\\|_{2}^{2}$ When $L(\\mu)\\leq\\lambda^{2}\\left\\|p\\right\\|_{2}^{2}$ we have $\\mathrm{RHS}\\leq$ $\\zeta/\\lambda+2\\lambda\\left\\|p\\right\\|_{2}^{2}$ . Thus, in summary $\\mathrm{RHS}\\leq\\zeta/\\lambda+2\\lambda\\left\\|p\\right\\|_{2}^{2}$ ", "page_idx": 34}, {"type": "text", "text": "Combine the bounds on LHS and RHS we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\rho_{\\eta}\\sum_{i\\in[m_{*}]}\\int_{\\mathcal{T}_{i}}\\delta(\\pmb{w},\\pmb{w}_{i}^{*})^{2}\\,\\mathrm{d}\\vert\\mu\\vert(\\pmb{w})\\leq\\zeta/\\lambda+2\\lambda\\,\\Vert p\\Vert_{2}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The following lemma shows that every teacher neuron must have at least one close-by student neuron within angle $\\bar{O}_{*}(\\zeta^{1/3})$ . This generalize and greatly simplify the previous results Lemma 9 in Zhou et al. (2021). In particular, we design a new test function using the Hermite expansion to achieve this. ", "page_idx": 34}, {"type": "text", "text": "Lemma F.6. Under Lemma $^{6}$ if the Hermite coeffcient of o decays as $\\left|\\hat{\\sigma}_{k}\\right|=\\Theta(k^{-c_{\\sigma}})$ with some constant $c_{\\sigma}>0$ then the total mass near each target direction is large, i.e., $\\mu({\\mathcal{T}}_{i}(\\delta))\\,\\mathrm{sign}(a_{i}^{*})\\geq$ $|a_{i}^{*}|/2$ for all $i\\in[m_{*}]$ and any $\\begin{array}{r}{\\delta_{c l o s e}\\,\\geq\\,\\widetilde\\Omega\\,\\left((\\frac{L(\\mu)}{a_{\\operatorname*{min}}^{2}})^{1/\\left(4c_{\\sigma}-2\\right)}\\right)}\\end{array}$ with large enough hidden constant. In particular, for $\\sigma$ is ReLU or absolute function, $\\begin{array}{r}{\\delta_{c l o s e}\\ge\\widetilde\\Omega\\left((\\frac{L(\\mu)}{a_{\\mathrm{min}}^{2}})^{1/3}\\right)}\\end{array}$ . Here $a_{\\mathrm{min}}=\\operatorname*{min}\\left|a_{i}\\right|$ .is the smallest entry of $\\pmb{a}_{*}$ in absolute value. ", "page_idx": 34}, {"type": "text", "text": "As a corollary, if the optimality gap. $\\zeta=L_{\\lambda}(\\mu)-L_{\\lambda}(\\mu_{\\lambda}^{\\ast}),$ then $\\delta_{c l o s e}\\,\\geq\\,\\widetilde{\\Omega}_{\\ast}\\,\\big((\\zeta+\\lambda^{2})^{1/(4c_{\\sigma}-2)}\\big)$ and for ReLU or absolute $\\delta_{c l o s e}\\geq\\widetilde{\\Omega}_{*}\\left((\\zeta+\\lambda^{2})^{1/3}\\right)$ ", "page_idx": 34}, {"type": "text", "text": "Proof. Assume towards contradiction that there exists some $i\\ \\in\\ [m_{*}]$ with some $\\delta_{c l o s e}~~\\ge$ $\\widetilde{\\Omega}\\left((\\frac{L(\\mu)}{a_{\\mathrm{min}}^{2}})^{1/(4c_{\\sigma}-2)}\\right)$ with large enough hidden constant such that $\\mu(\\mathcal{T}_{i}(\\delta))\\operatorname{sign}(a_{i}^{*})\\ \\leq\\ |a_{i}^{*}|/2$ For simplicity, we will use $\\delta$ for $\\delta_{c l o s e}$ in the following. ", "page_idx": 34}, {"type": "text", "text": "Let $\\begin{array}{r}{g(x)\\;=\\;\\sum_{\\ell\\le k<2\\ell}\\mathrm{sign}(a_{i}^{*})\\,\\mathrm{sign}(\\hat{\\sigma}_{k})h_{k}({\\pmb w}_{i}^{*\\,\\top}{\\pmb x})}\\end{array}$ be a test function, where $h_{k}(x)$ is the $k$ th normalized probabilistic Hermite polynomial and $\\ell$ will be chosen later. ", "page_idx": 34}, {"type": "text", "text": "Denote $R({\\pmb x})=f_{\\mu}({\\pmb x})-f_{\\mu^{*}}({\\pmb x})$ so that $\\left\\|R\\right\\|_{2}^{2}=L(\\mu)$ . We have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{L(\\boldsymbol{\\mu})}\\left\\|g\\right\\|_{2}\\geq\\cdots\\boldsymbol{\\left[-R,g\\right\\rangle}}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}_{\\boldsymbol{x}}\\left[\\left(a_{i}^{\\ast}\\boldsymbol{\\sigma}(\\boldsymbol{w}_{i}^{\\ast\\intercal}\\boldsymbol{x})-\\int_{T_{i}(\\boldsymbol{\\delta})}\\boldsymbol{\\sigma}(\\boldsymbol{w}^{\\intercal}\\boldsymbol{x})\\,\\mathrm{d}\\mu(\\boldsymbol{w})\\right)g(\\boldsymbol{x})\\right]}\\\\ &{\\quad\\quad\\quad\\quad+\\mathbb{E}_{\\boldsymbol{x}}\\left[\\left(\\sum_{j\\neq i}a_{j}^{\\ast}\\boldsymbol{\\sigma}(\\boldsymbol{w}_{j}^{\\ast\\intercal}\\boldsymbol{x})-\\int_{\\mathbb{S}^{d-1}\\setminus T_{i}(\\boldsymbol{\\delta})}\\boldsymbol{\\sigma}(\\boldsymbol{w}^{\\intercal}\\boldsymbol{x})\\,\\mathrm{d}\\mu(\\boldsymbol{w})\\right)g(\\boldsymbol{x})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Recall the Hermite expansion of $\\begin{array}{r}{\\sigma(x)=\\sum_{k\\geq0}\\hat{\\sigma}_{k}h_{k}(x)}\\end{array}$ and its property in Claim A.1. For the first term, it becomes ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sum_{\\ell\\leq k<2\\ell}\\left(|a_{i}^{*}||\\hat{\\sigma}_{k}|-\\int_{\\mathcal T_{i}(\\delta)}|\\hat{\\sigma}_{k}|\\,\\mathrm{sign}(a_{i}^{*})(w^{\\top}w_{i}^{*})^{k}\\,\\mathrm{d}\\mu(w)\\right)\\geq\\frac12|a_{i}^{*}|\\sum_{\\ell\\leq k<2\\ell}|\\hat{\\sigma}_{k}|.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "For the second term, it becomes ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{\\ell\\leq k<2\\ell}\\left(\\sum_{j\\neq i}a_{j}^{*}|\\hat{\\sigma}_{k}|\\sin(a_{i}^{*})(w_{j}^{*\\top}w_{i}^{*})^{k}-\\int_{\\mathbb{S}^{d-1}\\setminus\\mathcal{T}_{i}(\\delta)}|\\hat{\\sigma}_{k}|\\sin(a_{i}^{*})(w^{\\top}w_{i}^{*})^{k}\\,\\mathrm{d}\\mu(w)\\right)}\\\\ &{\\leq\\!(\\|a^{*}\\|_{1}+|\\mu|_{1})\\displaystyle\\sum_{\\ell\\leq k\\leq2\\ell}|\\hat{\\sigma}_{k}|\\,\\displaystyle\\operatorname*{max}_{\\ell(w,w_{i}^{*})\\geq\\delta}(w^{\\top}w_{i}^{*})^{k}}\\\\ &{\\leq\\!(\\|a^{*}\\|_{1}+|\\mu|_{1})\\displaystyle\\sum_{\\ell\\leq k<2\\ell}|\\hat{\\sigma}_{k}|(1-\\delta^{2}/5)^{\\ell}}\\\\ &{\\leq\\!4\\,\\|a^{*}\\|_{1}\\,(1-\\delta^{2}/5)^{\\ell}\\displaystyle\\sum_{\\ell\\leq k<2\\ell}|\\hat{\\sigma}_{k}|\\leq\\frac{1}{4}|a_{i}^{*}|\\displaystyle\\sum_{\\ell\\leq k<2\\ell}|\\hat{\\sigma}_{k}|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where (i) in the third line we use $\\cos\\delta\\le1-\\delta^{2}/5$ for $\\delta\\in[0,\\pi/2]$ and (i) in the last line we use Lemma F.4 and choose $\\ell=\\lceil(5/\\delta^{2})\\log(16\\,\\|a^{*}\\|_{1}^{}\\,/|a_{i}^{*}|)\\rceil$ ", "page_idx": 35}, {"type": "text", "text": "Thus, given $\\left|\\hat{\\sigma}_{k}\\right|=\\Theta(k^{-c_{\\sigma}})$ we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n{\\sqrt{L(\\mu)}}{\\sqrt{\\ell}}={\\sqrt{L(\\mu)}}\\,\\|g\\|_{2}\\geq{\\frac{1}{4}}|a_{i}^{*}|\\sum_{\\ell\\leq k<2\\ell}|{\\hat{\\sigma}}_{k}|={\\frac{1}{4}}|a_{i}^{*}|\\sum_{\\ell\\leq k<2\\ell}\\Theta(k^{-c_{\\sigma}})=|a_{i}^{*}|\\Theta(\\ell^{1-c_{\\sigma}}).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "With the choice of $\\begin{array}{r l r}{\\ell}&{{}=}&{\\widetilde\\Theta(1/\\delta^{2})}\\end{array}$ , we have $\\begin{array}{r l r}{\\delta}&{=}&{\\widetilde{O}\\left(\\left(\\frac{L(\\mu)}{|a_{i}^{*}|^{2}}\\right)^{1/\\left(4c_{\\sigma}-2\\right)}\\right)}\\end{array}$ Since $\\delta\\ \\geq$ $\\widetilde{\\Omega}\\left((\\frac{L(\\mu)}{a_{\\mathrm{min}}^{2}})^{1/(4c_{\\sigma}-2)}\\right)$ with alarge nough hidden constant, we know this is a contradiction. As a corollary, with Lemma F.4 that $\\begin{array}{r l r}{L(\\mu)}&{{}=}&{4\\zeta~+~5\\lambda^{2}\\left\\|p\\right\\|_{2}^{2}}\\end{array}$ \uff0cwe have $\\delta\\quad\\geq$ $\\begin{array}{r}{\\widetilde\\Omega\\left((\\frac{4\\zeta+5\\lambda^{2}\\|p\\|_{2}^{2}}{a_{\\operatorname*{min}}^{2}})^{1/\\left(4c_{\\sigma}-2\\right)}\\right)}\\end{array}$ ", "page_idx": 35}, {"type": "text", "text": "For the activation $\\sigma$ is ReLU or absolute function, by Lemma A.1 we know $c_{\\sigma}=5/4$ , which gives the desired result. \u53e3 ", "page_idx": 35}, {"type": "text", "text": "The lemma below bounds $R_{2}$ using the fact that it is spiky (has small non-zero support). ", "page_idx": 35}, {"type": "text", "text": "Lemma F8. Under Lemma $^{6}$ recall the optimality gap $\\zeta=L_{\\lambda}(\\mu)-L_{\\lambda}(\\mu_{\\lambda}^{*})$ .Then ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left\\|{R_{2}}\\right\\|_{2}^{2}=O_{*}((\\zeta/\\lambda+\\lambda)^{3/2}).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. Using the same calculation as in Lemma 12 in Zhou et al. (2021), we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left\\|{R_{2}}\\right\\|_{2}^{2}\\leq O(m_{*})\\sum_{i\\in[m_{*}]}\\left(\\sum_{j\\in{\\mathcal{T}}_{i}}|a_{j}|\\left\\|{\\boldsymbol{w}}_{j}|\\right\\|_{2}\\right)^{1/2}\\left(\\sum_{j\\in{\\mathcal{T}}_{i}}|a_{j}|\\left\\|{\\boldsymbol{w}}_{j}\\right\\|_{2}\\delta_{j}^{2}\\right)^{3/2}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "With Lemma F.4 and Lemma F.5, we have $\\|R_{2}\\|_{2}^{2}=O(m_{*}^{2}|\\mu^{*}|^{1/2}(\\zeta/\\lambda+\\lambda)^{3/2})$ ", "page_idx": 35}, {"type": "text", "text": "The following lemma bounds $R_{3}$ . In fact, in the view of expressing the loss as a sum of tensor decomposition problem, $R_{3}$ corresponds to the O-th order term in the expansion. It would become small when high-order terms become small, as shown in the proof below. ", "page_idx": 36}, {"type": "text", "text": "Lemma F.9. Under Lemma $^{6}$ and recall the optimality gap $\\zeta=L_{\\lambda}(\\mu)-L_{\\lambda}(\\mu_{\\lambda}^{*})$ f $\\hat{\\sigma}_{0}=0$ and $\\hat{\\sigma}_{k}>0$ with some $k=\\Theta((1/\\Delta^{2})\\log(\\zeta/\\left\\|a_{*}\\right\\|_{1}))$ ,then ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\|R_{3}\\|_{2}\\!=\\!\\!\\widetilde{\\cal O}_{\\ast}((\\zeta+\\lambda^{2})^{1/2}/\\hat{\\sigma}_{k}+(\\zeta/\\lambda+\\lambda)+\\zeta).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. As shown in Ge et al. (2018); Li et al. (2020), we can write the loss $L(\\mu)$ as sum of tensor decomposition problem (recall $\\|\\pmb{w}_{i}^{*}\\|_{2}=1\\}$ ", "page_idx": 36}, {"type": "equation", "text": "$$\nL(\\mu)=\\sum_{k\\geq0}\\hat{\\sigma}_{k}^{2}\\left\\|\\int_{w\\in\\mathbb{S}^{d-1}}{w^{\\otimes k}}\\,\\mathrm{d}\\mu(\\pmb{w})-\\sum_{i\\in[m_{*}]}a_{i}^{*}\\,\\|\\pmb{w}_{i}^{*}\\|_{2}\\,\\pmb{w}_{i}^{*\\otimes k}\\right\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Thus, we know for any $k\\geq1$ \uff0c ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left\\|\\int_{w\\in\\mathbb{S}^{d-1}}{\\pmb w}^{\\otimes k}\\,\\mathrm{d}\\mu({\\pmb w})-\\sum_{i\\in[m_{*}]}a_{i}^{*}\\,\\|{\\pmb w}_{i}^{*}\\|_{2}\\,{\\pmb w}_{i}^{*\\otimes k}\\right\\|_{F}^{2}\\leq L(\\mu)/\\hat{\\sigma}_{k}^{2}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Given any $\\pmb{w}_{j}^{*}$ and even $k$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\displaystyle\\int_{\\omega\\in\\mathbb{R}^{d-1}}w^{\\otimes k}\\,{\\mathrm{d}}\\mu(w)-\\sum_{i\\in[m_{i}]}a_{i}^{*}\\,{\\mathrm{\\large|}\\,}w_{i}^{*}\\|_{2}\\,w_{i}^{**}\\mathrm{\\large|}\\right|_{F}}\\\\ &{\\geq\\left|\\displaystyle\\left\\langle\\sum_{i\\in[m_{i}]}a_{i}^{*}\\,{\\mathrm{\\large|}\\,}[w_{i}^{*}]_{2}\\,w_{i}^{*}\\,z^{k}-\\int_{w\\in\\mathbb{R}^{d-1}}w^{\\otimes k}\\,{\\mathrm{\\large|}\\,}w^{*}\\,z^{k}\\,{\\mathrm{\\large|}\\,}\\right\\rangle\\right|}\\\\ &{\\geq\\left|a_{i}^{*}\\,\\displaystyle\\left\\{w_{i}^{*}\\,{\\mathrm{\\large|}\\,}[2_{1}-\\int_{\\gamma_{i}}(w,w_{j}^{*})^{k}\\,{\\mathrm{d}}\\mu(w)-\\left|\\sum_{i\\in[j]}a_{i}^{*}\\,{\\mathrm{\\large|}\\,}[w_{i}^{*}]_{2}\\,{\\mathrm{\\large|}\\,}w_{i}^{*}\\,w_{j}^{*}]^{k}-\\int_{\\mathbb{R}^{d-1}\\setminus\\gamma_{i}}\\langle w,w_{j}^{*}\\,z^{k}\\,{\\mathrm{d}}\\mu(w)\\right|\\right.}\\\\ &{\\left.\\geq\\left|a_{j}^{*}\\,\\displaystyle\\left\\{w_{i}^{*}\\,{\\mathrm{\\large|}\\,}[2_{2}-\\int_{\\gamma_{i}}\\mathrm{\\large{\\large|}\\,}w(w)]-\\left|\\int_{\\gamma_{i}}\\,{\\mathrm{d}}\\mu(w)-\\int_{\\gamma_{i}}\\,\\langle w,w_{j}^{*}\\,\\rangle^{k}\\,{\\mathrm{d}}\\mu(w)\\right|\\right.}\\\\ &{\\qquad-\\left.\\left|\\sum_{i\\in[j]}a_{i}^{*}\\,{\\mathrm{\\large|}\\,}w_{i}^{*}\\,z^{k}-\\int_{\\gamma_{i}-1\\backslash\\gamma_{i}}\\langle w,w_{j}^{*}\\,\\rangle^{k}\\,{\\mathrm{d}}\\mu(w)\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We show the last 2 terms are small. ", "page_idx": 36}, {"type": "text", "text": "For the second term on RHS, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int_{T_{j}}\\,\\mathrm{d}\\mu(\\boldsymbol{w})-\\int_{T_{j}}\\langle\\boldsymbol{w},\\boldsymbol{w}_{j}^{*}\\rangle^{k}\\,\\mathrm{d}\\mu(\\boldsymbol{w})\\Bigg\\vert\\le\\int_{T_{j}}\\,\\big(1-\\langle\\boldsymbol{w},\\boldsymbol{w}_{j}^{*}\\rangle^{k}\\big)\\,\\,\\mathrm{d}\\vert\\mu\\vert(\\boldsymbol{w})\\overset{\\mathrm{(a)}}{\\le}\\int_{T_{j}}1-(1-\\delta(\\boldsymbol{w},\\boldsymbol{w}_{j}^{*})^{2}/2)^{k}}}\\\\ &{}&{\\stackrel{\\mathrm{(b)}}{\\le}\\int_{T_{j},\\delta(\\boldsymbol{w},\\boldsymbol{w}_{j}^{*})^{2}\\le1}O(k)\\cdot\\delta(\\boldsymbol{w},\\boldsymbol{w}_{j}^{*})^{2}\\,\\mathrm{d}\\vert\\mu\\vert(\\boldsymbol{w})+\\int_{T_{j},\\delta(\\boldsymbol{w},\\boldsymbol{w}_{j}^{*})^{2}>1}O(k)\\,\\mathrm{d}\\mu}\\\\ &{}&{\\le{\\cal O}(k)\\int_{T_{j}}\\delta(\\boldsymbol{w},\\boldsymbol{w}_{j}^{*})^{2}\\,\\mathrm{d}\\vert\\mu\\vert(\\boldsymbol{w}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where (a) $\\cos\\delta\\ge1-\\delta^{2}/2$ for $\\delta\\in[0,\\pi/2]$ ; (b) $(1-x)^{k}\\geq1-k x$ for $x\\in[0,1]$ ", "page_idx": 36}, {"type": "text", "text": "For the third term on RHS, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\left\\lvert\\sum_{i\\neq j}a_{i}^{*}\\parallel\\!w_{i}^{*}\\!\\parallel_{2}\\langle w_{i}^{*},w_{j}^{*}\\rangle^{k}-\\int_{\\mathbb S^{d-1}\\backslash T_{j}}\\langle w,w_{j}^{*}\\rangle^{k}\\,\\mathrm{d}\\mu(w)\\right\\rvert\\leq\\!(\\|a_{*}\\|_{1}+|\\mu|_{1})\\operatorname*{max}_{\\angle(w,w_{j}^{*})\\geq\\Delta/2}(w^{\\top}w_{j}^{*})^{k}}\\\\ {\\overset{()}{\\leq}(\\|a_{*}\\|_{1}+|\\mu|_{1})(1-\\Delta^{2}/10)^{k}\\overset{()}{\\leq}O(\\zeta),}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where (a) $\\cos\\delta\\,\\leq\\,1\\,-\\,\\delta^{2}/5$ for $\\delta\\,\\in\\,[0,\\pi/2]$ ; (b) we choose $k\\,=\\,\\Theta((1/\\Delta^{2})\\log(\\zeta/\\left\\|a_{*}\\right\\|_{1}))$ and Lemma F.4. ", "page_idx": 37}, {"type": "text", "text": "Therefore, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~\\left\\|\\displaystyle\\int_{w\\in\\mathbb{S}^{d-1}}w^{\\otimes k}\\mu(w)-\\sum_{i\\in[m_{*}]}a_{i}^{*}\\,\\|w_{i}^{*}\\|_{2}\\,{w}_{i}^{*\\otimes k}\\right\\|_{F}}\\\\ &{\\geq\\left|a_{j}^{*}\\,\\|{w}_{j}^{*}\\|_{2}-\\displaystyle\\int_{T_{j}}\\mu({w})\\right|-O(k)\\displaystyle\\int_{T_{j}}\\delta({w},{w}_{j}^{*})^{2}|\\mu|(w)-O(\\zeta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "This implies that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{m_{*}\\sqrt{L(\\mu)}/\\hat{\\sigma}_{k}\\geq\\displaystyle\\sum_{j\\in[m_{*}]}\\left|a_{j}^{*}\\left\\|{\\pmb w}_{j}^{*}\\right\\|_{2}-\\int_{\\mathcal{T}_{j}}\\mu({\\pmb w})\\right|-O(k)\\sum_{j\\in[m_{*}]}\\int_{\\mathcal{T}_{j}}\\delta({\\pmb w},{\\pmb w}_{j}^{*})^{2}|\\mu|({\\pmb w})-O(m_{*}\\zeta)}\\\\ &{\\qquad\\qquad\\geq\\left|\\displaystyle\\sum_{i\\in[m_{*}]}a_{i}^{*}\\left\\|{\\pmb w}_{i}^{*}\\right\\|_{2}-\\int_{\\mathbb{S}^{d-1}}\\mu({\\pmb w})\\right|-\\widetilde O_{*}(\\zeta/\\lambda+\\lambda)-O(m_{*}\\zeta),}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where we use Lemma F.5. Rearranging the terms and recalling $L(\\mu)=O_{*}(\\zeta+\\lambda^{2})$ from Lemma F.4, we get the bound. ", "page_idx": 37}, {"type": "text", "text": "The following lemma gives the bound on the average neuron to its corresponding teacher neuron. It follows directly from the residual decomposition and previous lemmas that characterize $R_{1},R_{2},R_{3}$ respectively. ", "page_idx": 37}, {"type": "text", "text": "LemmaF.10.UnderLemma $^{6}$ recall theoptimalitygap $\\zeta\\,=\\,L_{\\lambda}(\\mu)\\,-\\,L_{\\lambda}(\\mu_{\\lambda}^{*})$ Thenfor any $i\\in[m_{*}]$ $\\zeta=\\Omega(\\lambda^{2})$ and $\\zeta,\\lambda\\leq1/\\mathrm{poly}(m_{*},\\Delta,\\|\\pmb{a}_{*}\\|_{1})$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left\\|\\sum_{j\\in\\mathcal{T}_{i}}a_{j}\\boldsymbol{w}_{j}-\\boldsymbol{w}_{i}^{*}\\right\\|_{2}\\leq\\left(\\sum_{i\\in[m_{*}]}\\left\\|\\sum_{j\\in\\mathcal{T}_{i}}a_{j}\\boldsymbol{w}_{j}-\\boldsymbol{w}_{i}^{*}\\right\\|_{2}^{2}\\right)^{1/2}=O_{*}((\\zeta/\\lambda)^{3/4}).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. With the relation of residual decomposition, Lemma F.7, Lemma F.8 and Lemma F.9, we have for any $i\\in[m_{*}]$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Omega(\\Delta^{3/2}/m_{*}^{3/2})\\left(\\displaystyle\\sum_{i\\in[m_{*}]}\\left\\|\\sum_{j\\in\\mathcal{T}_{i}}a_{j}w_{j}-w_{i}^{*}\\right\\|_{2}^{2}\\right)^{1/2}\\leq\\|R_{1}\\|_{2}\\leq\\|R\\|_{2}+\\|R_{2}\\|_{2}+\\|R_{3}\\|_{2}}\\\\ &{=\\!O_{*}((\\zeta+\\lambda^{2})^{1/2}+(\\zeta/\\lambda+\\lambda)^{3/4})+\\widetilde O_{*}((\\zeta+\\lambda^{2})^{1/2}+(\\zeta/\\lambda+\\lambda)+\\zeta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Rearranging the terms, we get the result. ", "page_idx": 37}, {"type": "text", "text": "H.2  Omitted proofs in Section F.2 ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "In this section, we give the omitted proofs in Section F.2. The key observation used in the proofs is that balancing the norm and setting $\\alpha,\\beta$ perfectly to their target values only decrease the optimality gap. ", "page_idx": 37}, {"type": "text", "text": "Lemma F.11.Givenany $\\pmb{\\theta}=(\\pmb{a},\\pmb{W},\\alpha,\\beta)$ satisfying $|\\alpha-\\hat{\\alpha}|^{2}=O(\\zeta),$ $\\left\\|\\beta-\\hat{\\beta}\\right\\|_{2}^{2}=O(\\zeta),$ where $\\begin{array}{r}{\\hat{\\alpha}\\,=\\,-(1/\\sqrt{2\\pi})\\sum_{i=1}^{m}a_{i}\\left\\|\\pmb{w}_{i}\\right\\|_{2}}\\end{array}$ and $\\begin{array}{r}{\\hat{\\beta}\\,=\\,-(1/2)\\sum_{i=1}^{m}a_{i}{\\pmb w}_{i}}\\end{array}$ Letitscorrespondingbalanced version $\\pmb{\\theta}_{b a l}=\\left(\\pmb{a}_{b a l},\\pmb{W}_{b a l},\\alpha_{b a l},\\beta_{b a l}\\right)$ as $a_{b a l,i}=\\mathrm{sign}(a_{i})\\sqrt{\\left|a_{i}\\right|\\left\\|\\pmb{w}_{i}\\right\\|_{2}},\\,\\pmb{w}_{b a l,i}=\\overline{{\\pmb{w}}}_{i}\\sqrt{\\left|a_{i}\\right|\\left\\|\\pmb{w}_{i}\\right\\|_{2}},$ $\\alpha_{b a l}=\\hat{\\alpha}$ and $\\beta_{b a l}=\\hat{\\beta}$ Then,wehave ", "page_idx": 37}, {"type": "equation", "text": "$$\nL_{\\lambda}(\\pmb\\theta)-L_{\\lambda}(\\theta_{b a l})=|\\alpha-\\hat{\\alpha}|^{2}+\\left\\|\\pmb\\beta-\\hat{\\beta}\\right\\|_{2}^{2}+\\frac{\\lambda}{2}\\sum_{i\\in[m]}(|a_{i}|-\\|\\pmb w_{i}\\|_{2})^{2}\\geq0.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Moreover, let the optimality gap $\\zeta=L_{\\lambda}(\\pmb\\theta)-L_{\\lambda}(\\mu_{\\lambda}^{*}),$ we have results in Lemma F.4, Lemma F.5, Lemma F.6, Lemma F.7, Lemma F.8, Lemma $F.9$ andLemma $F.I O$ still hold for ${\\cal L}_{\\lambda}(\\pmb\\theta)$ with the change of $R_{3}$ in (8) as ", "page_idx": 38}, {"type": "equation", "text": "$$\nR_{3}(\\pmb{x})=\\frac{1}{\\sqrt{2\\pi}}\\left(\\sum_{i\\in[m_{*}]}a_{i}^{*}\\left\\|\\pmb{w}_{i}^{*}\\right\\|_{2}-\\sum_{i\\in[m]}a_{i}\\left\\|\\pmb{w}_{i}\\right\\|_{2}\\right)+\\alpha-\\hat{\\alpha}+(\\beta-\\hat{\\beta})^{\\top}\\pmb{x}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. Recall in Claim B.1 we have ", "page_idx": 38}, {"type": "equation", "text": "$$\nL(\\pmb\\theta)=|\\alpha-\\hat{\\alpha}|^{2}+\\left\\|\\pmb\\beta-\\hat{\\beta}\\right\\|_{2}^{2}+\\sum_{k\\geq2}\\hat{\\sigma}_{k}^{2}\\left\\|\\sum_{i\\in[m]}a_{i}\\left\\|\\pmb{w}_{i}\\right\\|_{2}\\overline{{\\pmb w}}_{i}^{\\otimes k}-\\sum_{i\\in[m*]}a_{i}^{*}\\left\\|\\pmb{w}_{i}^{*}\\right\\|_{2}\\pmb w_{i}^{*\\otimes k}\\right\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Note that $|a_{i}|\\,\\|\\pmb{w}_{i}\\|_{2}=\\left|a_{b a l,i}\\right|\\|\\pmb{w}_{b a l,i}\\|_{2}$ so that $L(\\pmb\\theta)=L(\\pmb\\theta_{b a l})+|\\alpha-\\hat{\\alpha}|^{2}+\\left\\|\\pmb\\beta-\\hat{\\beta}\\right\\|_{2}^{2}$ We then have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal L}_{\\lambda}({\\pmb\\theta})-{\\cal L}_{\\lambda}({\\pmb\\theta}_{b a l})=}\\vert\\alpha-\\hat{\\alpha}\\vert^{2}+\\left\\|\\beta-\\hat{\\beta}\\right\\|_{2}^{2}+\\frac{\\lambda}{2}\\left\\|{\\pmb a}\\right\\|_{2}^{2}+\\frac{\\lambda}{2}\\left\\|{\\pmb W}\\right\\|_{2}^{2}-\\frac{\\lambda}{2}\\left\\|{\\pmb a}_{b a l}\\right\\|_{2}^{2}-\\frac{\\lambda}{2}\\left\\|{\\pmb W}_{b a l}\\right\\|_{2}^{2}}\\\\ {{\\displaystyle=}\\vert\\alpha-\\hat{\\alpha}\\vert^{2}+\\left\\|\\beta-\\hat{\\beta}\\right\\|_{2}^{2}+\\frac{\\lambda}{2}\\displaystyle\\sum_{i\\in[m]}(\\vert a_{i}\\vert-\\left\\|{\\pmb w}_{i}\\right\\|_{2})^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Therefore, we have the optimality gap $\\zeta=L_{\\lambda}(\\pmb\\theta)-L_{\\lambda}(\\mu_{\\lambda}^{*})\\geq L_{\\lambda}(\\pmb\\theta_{b a l})-L_{\\lambda}(\\mu_{\\lambda}^{*})=\\zeta_{b a l}$ . Note that $\\theta_{b a l}$ corresponds to a network that has perfect balanced norms and fitted $\\alpha,\\beta$ , thus all results in Lemma F.4, Lemma F.5, Lemma F.6, Lemma F.7, Lemma F.8, Lemma F.9 and Lemma F.10 hold for $\\pmb{\\theta}_{b a l}$ . Since $\\geq\\zeta_{b a l},\\left|a_{i}\\right|\\left\\|\\pmb{w}_{i}\\right\\|_{2}=\\left|a_{b a l,i}\\right|\\left\\|\\pmb{w}_{b a l,i}\\right\\|_{2}$ and $L(\\pmb\\theta)=L(\\pmb\\theta_{b a l})+O(\\zeta)$ ,wecan easily check that all of them also hold for $\\pmb{\\theta}$ . For the bound on $R_{3}$ , note that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\left\\|{\\cal R}_{3}\\right\\|_{2}\\leq\\frac{1}{\\sqrt{2\\pi}}\\left|\\sum_{i\\in[m_{*}]}a_{i}^{*}\\left\\|{\\pmb{w}}_{i}^{*}\\right\\|_{2}-\\sum_{i\\in[m]}a_{i}\\left\\|{\\pmb{w}}_{i}\\right\\|_{2}\\right|+\\left|\\alpha-\\hat{\\alpha}\\right|+\\left\\|\\beta-\\hat{\\beta}\\right\\|_{2}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "so that the same bound still hold for $R_{3}$ ", "page_idx": 38}, {"type": "text", "text": "Lemma F.12. Under Lemma $6$ suppose optimalitygap $\\zeta=L_{\\lambda}(\\pmb\\theta)\\!-\\!L_{\\lambda}(\\mu_{\\lambda}^{*})$ Then $\\left\\|\\mathbfit{a}\\right\\|_{2}^{2}+\\left\\|W\\right\\|_{F}^{2}\\leq$ $3\\left\\|a_{*}\\right\\|_{1}$ ", "page_idx": 38}, {"type": "text", "text": "Proof. We have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\frac{\\lambda}{2}\\left\\|\\pmb{\\alpha}\\right\\|_{2}^{2}+\\frac{\\lambda}{2}\\left\\|\\pmb{W}\\right\\|_{F}^{2}=\\zeta+L(\\mu_{\\lambda}^{\\ast})+\\lambda|\\mu_{\\lambda}^{\\ast}|_{1}-L(\\pmb{\\theta})\\leq\\zeta+\\lambda^{2}\\left\\|p\\right\\|_{2}^{2}+\\lambda|\\mu_{\\lambda}^{\\ast}|_{1},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where we use Lemma F.3. Rearranging the terms, we get the result by noting that $|\\mu_{\\lambda}^{*}|_{1}\\leq\\|\\pmb{\\alpha}_{*}\\|_{1}$ .\u53e3 ", "page_idx": 38}, {"type": "text", "text": "H.3 Omitted proofs in Section F.3 ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "In this section, we give the omitted proofs in Section F.3. We will consider them case by case.   \nThe lemma below says that one can always decrease the loss if norms are not balanced. ", "page_idx": 38}, {"type": "text", "text": "Lemma F.15 (Descent direction, norm balance). We have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{i}\\sum_{j\\in T_{i}}\\left|\\langle\\nabla_{a_{j}}L_{\\lambda},-a_{j}\\rangle+\\langle\\nabla_{w_{j}}L_{\\lambda},w_{j}\\rangle\\right|=\\lambda\\displaystyle\\sum_{i\\in[m_{+}]}\\left|a_{i}^{2}-\\|w_{i}\\|_{2}^{2}\\right|}&{}\\\\ {\\displaystyle\\geq\\operatorname*{max}\\left\\{\\lambda\\|\\|a\\|_{2}^{2}-\\|W\\|_{F}^{2}\\,|\\,,\\lambda\\displaystyle\\sum_{i\\in[m_{+}]}(|a_{i}|-\\|w_{i}\\|_{2})^{2}\\right\\}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. We have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{i\\in[m]}\\left|\\langle\\nabla_{a_{j}}L_{\\lambda},-a_{j}\\rangle+\\langle\\nabla_{w_{j}}L_{\\lambda},w_{j}\\rangle\\right|}\\\\ &{=\\displaystyle\\sum_{i\\in[m]}\\left|-2\\mathbb{E}_{\\alpha}[(f(\\pmb{x})-f_{*}(\\pmb{x}))a_{j}\\sigma(\\pmb{w}_{j}^{\\top}\\pmb{x})]-\\lambda a_{j}^{2}+2\\mathbb{E}_{\\alpha}[(f(\\pmb{x})-f_{*}(\\pmb{x}))a_{j}\\sigma(\\pmb{w}_{j}^{\\top}\\pmb{x})]+\\lambda\\left\\|\\pmb{w}_{i}\\right\\|_{2}^{2}}\\\\ &{=\\lambda\\displaystyle\\sum_{i}\\left|a_{i}^{2}-\\left\\|\\pmb{w}_{i}\\right\\|_{2}^{2}\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Note that $|a_{i}|+\\left\\|\\pmb{w}_{i}\\right\\|_{2}\\geq||a_{i}|-\\|\\pmb{w}_{i}\\|_{2}\\,|$ , we get the result. ", "page_idx": 39}, {"type": "text", "text": "The following lemma shows that one can always decrease the loss if there are close-by neurons that cancels with others. Intuitively, reducing such norm cancellation decrease the regularization term while keeping the square loss term, which decreasing the total loss as a whole. ", "page_idx": 39}, {"type": "text", "text": "Lemma F.16 (Descent direction, norm cancellation). Under Lemma $^{6}$ and Assumption $F l$ suppose the optimality gap $\\zeta=L_{\\lambda}(\\pmb\\theta)-L_{\\lambda}(\\mu_{\\lambda}^{\\ast})$ . For any $\\pmb{w}_{i}^{*}$ , consider $\\delta_{\\mathrm{sign}}$ such that $\\delta_{c l o s e}\\,<\\,\\delta_{\\mathrm{sign}}\\,=$ $O(\\lambda/\\zeta^{1/2})$ with small enough hidden constant ( $\\delta_{c l o s e}$ defined inLemma $F.6$ ), then ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\sum_{\\mathbf{c}\\{+,-\\}}\\sum_{j\\in T_{i,s}\\left(\\delta_{\\mathrm{sign}}\\right)}\\left\\langle\\nabla_{a_{j}}L_{\\lambda},\\frac{a_{j}}{\\sum_{j\\in T_{i,s}\\left(\\delta_{\\mathrm{sign}}\\right)}\\left|a_{j}\\right|\\left\\|w_{j}\\right\\|_{2}}\\right\\rangle+\\left\\langle\\nabla_{w_{j}}L_{\\lambda},\\frac{w_{j}}{\\sum_{j\\in T_{i,s}\\left(\\delta_{\\mathrm{sign}}\\right)}\\left|a_{j}\\right|\\left\\|w_{j}\\right\\|_{2}}\\right\\rangle=0,\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where $\\begin{array}{r}{\\ddot{\\iota}_{i,+}(\\delta_{\\mathrm{sign}})=\\{j\\in T_{i}:\\delta({\\boldsymbol w}_{j},{\\boldsymbol w}_{i}^{*})\\leq\\delta_{\\mathrm{sign}},\\mathrm{sign}(a_{j})=\\mathrm{sign}(a_{i}^{*})\\},\\,T_{i,-}(\\delta_{\\mathrm{sign}})=\\{j\\in T_{i}:\\delta({\\boldsymbol w}_{i},{\\boldsymbol w}_{i}^{*})\\}.}\\end{array}$ $\\begin{array}{r}{\\delta(\\pmb{w}_{j},\\pmb{w}_{i}^{*})\\leq\\delta_{\\mathrm{sign}},\\mathrm{sign}(a_{j})\\neq\\mathrm{sign}(a_{i}^{*})\\}}\\end{array}$ are the set of neurons that close to $\\pmb{w}_{i}^{*}$ with/without same signof $a_{i}^{*}$ ", "page_idx": 39}, {"type": "text", "text": "As a result, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left\\|\\nabla_{a}L_{\\lambda}\\right\\|_{2}^{2}+\\left\\|\\nabla_{W}L_{\\lambda}\\right\\|_{F}^{2}\\geq\\lambda^{2}\\sum_{j\\in T_{i,-}(\\delta_{\\mathrm{sign}})}\\left|a_{j}\\right|\\left\\|\\pmb{w}_{j}\\right\\|_{2}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Proof. Denote $R({\\pmb x})=f({\\pmb x})-\\widetilde{f}_{*}({\\pmb x})$ . We have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s\\in\\{+,-\\}}\\sum_{j\\in T_{i,s}(\\delta_{\\mathrm{sign}})}\\Biggl\\langle\\nabla_{a_{j}}L_{\\lambda,\\frac{a_{j}}{\\sum_{j\\in T_{i,s_{\\mathrm{sign}}}}|a_{j}|\\|w_{j}\\|_{2}}\\Biggr\\rangle+\\left\\langle\\nabla_{w_{j}}L_{\\lambda,\\frac{w_{j}}{\\sum_{j\\in T_{i,s}(\\delta_{\\mathrm{sign}})}|a_{j}|\\|w_{j}\\|_{2}}\\right\\rangle}\\\\ &{\\displaystyle=\\sum_{s\\in\\{+,-\\}}\\sum_{j\\in T_{i,s}(\\delta_{\\mathrm{sign}})}\\frac{a_{j}\\left\\|w_{j}\\right\\|_{2}}{\\sum_{j\\in T_{i,s}(\\delta_{\\mathrm{sign}})}|a_{j}|\\left\\|w_{j}\\right\\|_{2}}\\cdot2\\mathbb{E}_{x}[R(s)\\sigma(\\overline{{w}}_{j}^{\\top}x)]+\\frac{\\lambda a_{j}^{2}}{\\sum_{j\\in T_{i,s}(\\delta_{\\mathrm{sign}})}\\left|a_{j}\\right|\\left\\|w_{j}\\right\\|_{2}}}\\\\ &{\\displaystyle+\\sum_{s\\in\\{+,-\\}}\\sum_{j\\in T_{i,s}(\\delta_{\\mathrm{sign}})}\\frac{a_{j}\\left\\|w_{j}\\right\\|_{2}}{\\sum_{j\\in T_{i,s}(\\delta_{\\mathrm{sign}})}\\left|a_{j}|\\left\\|w_{j}\\right\\|_{2}}\\cdot2\\mathbb{E}_{x}[R(x)\\sigma(\\overline{{w}}_{j}^{\\top}x)]+\\frac{\\lambda\\left\\|w_{j}\\right\\|_{2}}{\\sum_{j\\in T_{i,s}(\\delta_{\\mathrm{sign}})}\\left|a_{j}\\right|\\left\\|w_{j}\\right\\|_{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We split the above into two terms (depending on square loss or regularization). WLOG, assume $\\mathrm{sign}(a_{i}^{*})=1$ . For the first term that depends on gradient on square loss, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(I)=4\\displaystyle\\sum_{s\\in\\{+,-\\}}\\sum_{j\\in T_{i,s}(\\delta_{q,\\varepsilon})}\\frac{a_{j}\\|\\log|j_{2}|}{\\sum_{j\\in T_{i,s}(\\delta_{q,\\varepsilon})}\\sum_{\\alpha\\in T_{i,s,\\varepsilon\\_{\\theta(a)}}}\\|a_{j}\\|\\log_{j}\\|_{2}}\\cdot\\mathbb{E}_{\\alpha}[R(x)\\sigma(\\overline{{w}}_{j}^{\\top}x)]}\\\\ &{\\quad=4\\displaystyle\\sum_{j\\in T_{i,s}(\\delta_{q,\\varepsilon\\_{\\theta(a)}})}\\frac{|a_{j}|\\|\\log_{j}\\|_{2}}{\\sum_{j\\in T_{i,+}(\\delta_{q,\\varepsilon\\_{\\theta(a)}})}\\|a_{j}\\|\\prod_{2}}\\mathbb{E}_{\\alpha}[R(x)\\sigma(\\overline{{w}}_{j}^{\\top}x)]}\\\\ &{\\quad\\quad-4\\displaystyle\\sum_{j\\in T_{i,-}(\\delta_{q,\\varepsilon\\_{\\theta(a)}})}\\frac{|a_{j}|\\|\\log_{j}\\|_{2}}{\\sum_{j\\in T_{i,-}(\\delta_{q,\\varepsilon\\_{\\theta(a)}})}\\|a_{j}\\|\\prod_{\\theta(p)}\\|_{2}}\\mathbb{E}_{\\alpha}[R(x)\\sigma(\\overline{{w}}_{j}^{\\top}x)]}\\\\ &{\\quad\\quad=4\\displaystyle\\sum_{j\\in T_{i,-}(\\delta_{q,\\varepsilon\\_{\\theta(a)}})}\\frac{|a_{j}|\\|\\pi_{j}\\|_{2}}{\\sum_{j\\in T_{i,+}(\\delta_{q,\\varepsilon\\_{\\theta(a)}})}\\|a_{j}\\|\\prod_{\\theta(p)}\\|_{2}}\\mathbb{E}_{\\alpha}[R(x)\\sigma(\\overline{{w}}_{j}^{\\top}x)-\\sigma(\\overline{{w}}_{i}^{\\top}x)]\\}}\\\\ &{\\quad\\quad-4\\displaystyle\\sum_{j\\in T_{i,-}(\\delta_{q,\\varepsilon\\_{\\theta(a)}})}\\frac{|a_{j}|\\|\\ W_{j}\\|_{2}}{\\sum_{j\\in T_{i,-}(\\delta_{q,\\varepsilon\\_{\\theta(a)}})}\\|a_{j}\\|\\prod_{\\theta(p)}\\|_{2}}\\mathbb{E}_{\\alpha}[R(x)(\\sigma(\\overline{{w}}_{j}^{\\top}x)-\\sigma(\\overline{{w}}_{i} \n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Since $\\overline{{\\pmb{w}}}_{j}$ is $\\delta_{\\mathrm{sign}}$ -close to $\\pmb{w}_{i}^{*}$ and $\\left\\|R\\right\\|_{2}^{2}=L(\\pmb{\\theta})$ , we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|(I)|\\le O(\\delta_{\\mathrm{sign}})\\,\\|R\\|_{2}=O_{*}(\\delta_{\\mathrm{sign}}\\zeta^{1/2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where we use Lemma F.11 that $L(\\pmb\\theta)=O_{*}(\\zeta)$ ", "page_idx": 40}, {"type": "text", "text": "For the second term that depends on regularization, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n(I I)=\\!\\lambda\\sum_{s\\in\\{+,-\\}}\\frac{\\sum_{j\\in T_{i,s}(\\delta_{\\mathrm{sign}})}a_{j}^{2}+\\|\\pmb{w}_{j}\\|_{2}^{2}}{\\sum_{j\\in T_{i,s}(\\delta_{\\mathrm{sign}})}|a_{j}|\\,\\|\\pmb{w}_{j}\\|_{2}}\\geq2\\lambda+2\\lambda=4\\lambda.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Therefore, when $(I)\\leq2\\lambda$ i.e., $\\delta_{\\mathrm{sign}}=O_{*}(\\lambda/\\zeta^{1/2})$ , we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s\\in\\{+,-\\}}\\sum_{j\\in T_{i,s}(\\delta_{\\mathrm{ign}})}\\left\\langle\\nabla_{a_{j}}L_{\\lambda},\\frac{\\mathrm{sign}(a_{j})|a_{j}|}{\\sum_{j\\in T_{i,s}(\\delta_{\\mathrm{ign}})}|a_{j}|\\,\\|w_{j}\\|_{2}}\\right\\rangle+\\left\\langle\\nabla_{w_{j}}L_{\\lambda},\\frac{w_{j}}{\\sum_{j\\in T_{i,s}(\\delta_{\\mathrm{ign}})}|a_{j}|\\,\\|w_{j}\\|_{2}}\\right\\rangle}\\\\ &{\\displaystyle\\geq\\frac{\\lambda}{2}\\sum_{s\\in\\{+,-\\}}\\frac{\\sum_{j\\in T_{i,s}(\\delta_{\\mathrm{ign}})}a_{j}^{2}+\\|w_{j}\\|_{2}^{2}}{\\sum_{j\\in T_{i,s}(\\delta_{\\mathrm{sign}})}|a_{j}|\\,\\|w_{j}\\|_{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "We compute a upper bound for LHS. Note that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{s\\in\\{+,-\\}}\\sum_{j\\in T_{s,\\varepsilon}(\\delta_{\\mathrm{sion}})}\\left\\langle\\nabla_{a_{j}}L_{\\lambda_{\\varepsilon}},\\frac{a_{j}}{\\sum_{j\\in T_{i,s}(\\delta_{\\mathrm{sion}})}|a_{j}|\\,\\|w_{j}\\|_{2}}\\right\\rangle+\\left\\langle\\nabla_{w_{j}}L_{\\lambda_{\\varepsilon}},\\frac{w_{j}}{\\sum_{j\\in T_{i,s}(\\delta_{\\mathrm{sion}})}|a_{j}|\\,\\|w_{j}\\|_{2}}\\right\\rangle}\\\\ {\\displaystyle\\xi\\sqrt{\\sum_{s\\in\\{+,-\\}}\\sum_{j\\in T_{s,\\varepsilon}(\\delta_{\\mathrm{sion}})}\\left(\\nabla_{a_{j}}L_{\\lambda}\\right)^{2}+\\left\\|\\nabla_{w_{j}}L_{\\lambda}\\right\\|_{2}^{2}}\\sqrt{\\sum_{s\\in\\{+,-\\}}\\sum_{j\\in T_{s,\\varepsilon}(\\delta_{\\mathrm{sion}})}\\frac{a_{j}^{2}}{(\\sum_{j\\in T_{i,s}(\\delta_{\\mathrm{sion}})}|a_{j}|\\,\\|w_{j}\\|_{2})^{2}}}}\\\\ {\\displaystyle\\xi\\sqrt{\\|\\nabla_{a}L_{\\lambda}\\|_{2}^{2}}+\\|\\nabla_{W}L_{\\lambda}\\|_{F}^{2}\\sqrt{\\sum_{s\\in\\{+,-\\}}\\frac{\\sum_{j\\in T_{s,\\varepsilon}(\\delta_{\\mathrm{sion}})}a_{j}^{2}+\\|w_{j}\\|_{2}^{2}}{(\\sum_{j\\in T_{i,s}(\\delta_{\\mathrm{sion}})}|a_{j}|\\,\\|w_{j}\\|_{2})^{2}}}}\\\\ {\\displaystyle\\xi\\sqrt{\\|\\nabla_{a}L_{\\lambda}\\|_{2}^{2}+\\|\\nabla_{W}L_{\\lambda}\\|_{F}^{2}}\\sqrt{\\sum_{s\\in\\{+,-\\}}\\frac{\\sum_{j\\in T_{s,\\varepsilon}(\\delta_{\\mathrm{sion}})}a_{j}^{2}+\\|w_{j}\\|_{2}^{2}}{\\sum_{j\\in T_{s,\\varepsilon}(\\delta_{\\mathrm{sion}})}\\|a_{j}\\|\\,\\|w_{j}\\|_{2}}}\\frac{1}{\\sqrt{\\sum_{j\\in T_{s,\\varepsilon}(\\delta_{\\mathrm{sion}})\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Wwherethelast line we useLema R6i $\\begin{array}{r}{\\sum_{j\\in T_{i,-}(\\delta_{\\mathrm{sign}})}\\left|a_{j}\\right|\\left\\|\\pmb{w}_{j}\\right\\|_{2}\\;<\\;\\sum_{j\\in T_{i,+}(\\delta_{\\mathrm{sign}})}\\left|a_{j}\\right|\\left\\|\\pmb{w}_{j}\\right\\|_{2}}\\end{array}$because $\\begin{array}{r}{\\mu(T_{i}(\\delta))=\\sum_{j\\in T_{i}(\\delta_{\\mathrm{sign}})}a_{j}\\:\\|\\pmb{w}_{j}\\|_{2}>0}\\end{array}$", "page_idx": 40}, {"type": "text", "text": "Combine with the above descent direction, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{\\|\\nabla_{a}L_{\\lambda}\\|_{2}^{2}+\\|\\nabla_{W}L_{\\lambda}\\|_{F}^{2}}\\sqrt{\\displaystyle\\sum_{s\\in\\{+,-\\}}\\frac{\\sum_{j\\in T_{i,s}(\\delta_{\\mathrm{sign}})}a_{j}^{2}+\\|w_{j}\\|_{2}^{2}}{\\sum_{j\\in T_{i,s}(\\delta_{\\mathrm{sign}})}\\|a_{j}\\|\\,\\|w_{j}\\|_{2}}}\\frac{1}{\\sqrt{\\sum_{j\\in T_{i,-\\}(\\delta_{\\mathrm{sign}})}|a_{j}|\\,\\|w_{j}\\|_{2}}}}\\\\ &{\\geq\\displaystyle\\frac{\\lambda}{2}\\sum_{s\\in\\{+,-\\}}\\frac{\\sum_{j\\in T_{i,s}(\\delta_{\\mathrm{sign}})}a_{j}^{2}+\\|w_{j}\\|_{2}^{2}}{\\sum_{j\\in T_{i,s}(\\delta_{\\mathrm{sign}})}|a_{j}|\\,\\|w_{j}\\|_{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "which implies ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left\\|\\nabla_{a}L_{\\lambda}\\right\\|_{2}^{2}+\\left\\|\\nabla_{W}L_{\\lambda}\\right\\|_{F}^{2}\\geq\\lambda^{2}\\sum_{j\\in T_{i,-}(\\delta_{\\mathrm{sign}})}\\left|a_{j}\\right|\\left\\|\\pmb{w}_{j}\\right\\|_{2}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "The lemma below shows that when all previous cases are not hold, then there is a descent direction that move all close-by neurons towards their corresponding teacher neuron. The proof relies on calculations that generalize Lemma 8 in Zhou et al. (2021). ", "page_idx": 40}, {"type": "text", "text": "Lemma E.17 (Descent direction). Under Lemma $^{6}$ and Assumption $F.I$ suppose the optimality gap $\\zeta=L_{\\lambda}(\\pmb\\theta)-L_{\\lambda}(\\mu_{\\lambda}^{*})$ .Suppose ", "page_idx": 41}, {"type": "equation", "text": "$\\begin{array}{r}{|\\|\\pmb{W}\\|_{F}^{2}-\\|\\pmb{a}\\|_{2}^{2}|\\leq\\zeta/\\lambda,\\sum_{i\\in[m]}(|a_{j}|-\\|\\pmb{w}_{j}\\|_{2})^{2}=O_{*}(\\zeta^{2}/\\lambda^{2})}\\end{array}$ ", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "(i) (almost) no norm cancellation: consider all neurons $\\pmb{w}_{j}$ that are $\\delta_{\\mathrm{sign}}$ -close w.rt. teacher neuron $\\pmb{w}_{i}^{*}$ but has a different sign, i.e., $\\mathrm{sign}(a_{j})\\ne\\mathrm{sign}(a_{i}^{*})$ with $\\bar{\\delta_{\\mathrm{sign}}}=\\Theta_{\\ast}(\\lambda/\\zeta^{1/2})$ we have $\\begin{array}{r}{\\sum_{j\\in T_{i,-}(\\delta_{\\mathrm{sign}})}|a_{j}|\\,\\|{\\boldsymbol w}_{j}\\|_{2}\\le\\tau=O_{*}(\\zeta^{5/6}/\\lambda)}\\end{array}$ with smallenough hiden constat, where $T_{i,-}(\\delta)$ defined in Lemma F.16. ", "page_idx": 41}, {"type": "text", "text": "(ii) $\\alpha,\\beta$ are wellftted: $\\lvert\\alpha-\\hat{\\alpha}\\rvert^{2}=O_{*}(\\zeta),\\,\\Big\\lVert\\beta-\\hat{\\beta}\\Big\\rVert_{2}^{2}=O_{*}(\\zeta)$ with small enough hidden factor. Then, we can construct the following descent direction ", "page_idx": 41}, {"type": "equation", "text": "$$\n(\\alpha+\\alpha_{\\ast})\\nabla_{\\alpha}L_{\\lambda}+\\langle\\nabla_{\\beta}L_{\\lambda},\\beta+\\beta_{\\ast}\\rangle+\\sum_{i\\in[m_{\\ast}]}\\sum_{j\\in{\\mathcal{T}}_{i}}\\langle\\nabla_{w_{i}}L_{\\lambda},w_{j}-q_{i j}w_{i}^{\\ast}\\rangle=\\Omega(\\zeta),\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Where $q_{i j}$ satisfy the following conditions with $\\delta_{c l o s e}\\,\\,<\\,\\,\\delta_{\\mathrm{sign}}$ and $\\delta_{c l o s e}\\ =\\ O_{*}(\\zeta^{1/3}).$ (1) $\\textstyle\\sum_{j\\in{\\mathcal{T}}_{i}}a_{j}q_{i j}\\;=\\;a_{i}^{*},$ (2) $q_{i j}~\\geq~0$ (3) $q_{i j}~=~0$ when $\\mathrm{sign}(a_{j})\\ \\ne\\ \\mathrm{sign}(a_{i}^{*})$ or $\\delta_{j}~>~\\delta_{c l o s e}$ (4) $\\begin{array}{r}{\\sum_{i\\in[m_{*}]}\\sum_{j\\in\\mathcal{T}_{i}}q_{i j}^{2}=O_{*}(1)}\\end{array}$ ", "page_idx": 41}, {"type": "text", "text": "Proof. Recall residual $R({\\pmb x})=f({\\pmb x})-\\widetilde{f}_{*}({\\pmb x})$ We have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad(n+\\alpha_{1})\\nabla_{v}E_{1}+(\\nabla_{v}E_{1},\\beta_{1}+\\beta_{2})+\\underset{v\\in\\mathbb{R}_{n}}{\\sum}\\sum_{\\rho_{1}}^{v}(\\nabla_{v}E_{1},\\beta_{1}-\\rho_{1}-\\rho_{1}w_{1}^{\\prime})}\\\\ &{\\quad(n\\geq)_{2}\\Big\\langle H_{2}|\\hat{\\sigma}_{v}\\Big(\\alpha_{1}\\Big)+2\\kappa_{2}\\Big|H_{3}\\Big(\\rho_{1}\\beta_{2}+\\beta_{2}\\Big)\\tau_{1}}\\\\ &{\\quad+\\frac{2}{\\mathcal{N}}\\sum_{\\rho_{1},\\rho_{1}^{\\prime}\\in\\mathbb{R}_{n}}\\sum_{\\phi=1}^{v}[\\phi(x)_{p,j}\\sigma(w^{\\prime})^{\\prime}]+2\\sum_{s=1}^{v}\\sum_{\\rho_{1},\\rho_{1}^{\\prime}\\in\\mathbb{R}_{n}}|\\mathcal{H}(\\lambda_{2})\\rho_{\\rho,j}\\sigma(w^{\\prime})^{\\prime}|}\\\\ &{\\quad+\\frac{2}{\\mathcal{N}}\\sum_{\\rho_{1},\\rho_{1}^{\\prime}\\in\\mathbb{R}_{n}}\\sum_{\\phi=1}^{v}[\\phi(x)_{p,j}w_{\\rho^{\\prime}}^{\\prime}]\\sigma(w^{\\prime})^{\\prime}(\\beta^{\\prime})^{\\prime}\\frac{1}{\\mathcal{N}}-\\rho\\big(w^{\\prime})^{\\prime}\\alpha_{1}^{\\prime}\\Big\\rangle}\\\\ &{\\quad+\\lambda\\sum_{\\rho_{1},\\rho_{1}^{\\prime}\\in\\mathbb{R}_{n}}\\Big\\{\\|w(\\lambda_{2})_{p,j}\\sigma^{\\prime}\\|^{2}\\Big\\}}\\\\ &{\\quad+\\lambda\\sum_{\\rho_{1}}^{v}\\Big\\{\\|w\\|_{2}^{2}-\\lambda_{2}\\sum_{p=1}^{v}\\sum_{\\phi=1}^{v}w^{\\prime}\\Big\\}w^{\\prime}}\\\\ &{\\quad+\\lambda\\frac{\\epsilon_{1}}{\\mathcal{N}}\\Big\\{\\|w\\|_{2}^{2}-\\lambda_{1}\\|w\\|_{2}^{2}\\Big\\}+\\sum_{s=1}^{v}\\sum_{\\rho_{1}=1}^{v}w^{\\prime}\\Big\\}w^{\\prime}}\\\\ &{\\quad+\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where  (a)_ we  plug  in  the  gradient   expression   and  ad   and   minus  the  term $\\begin{array}{r}{2\\sum_{i\\in[m_{*}]}\\sum_{j\\in\\mathcal{T}_{i}}\\bar{\\mathbb{E_{x}}}[\\bar{R}(\\pmb{x})a_{j}q_{i j}\\sigma(\\pmb{w}_{i}^{\\mp\\top}\\pmb{x})]}\\end{array}$ ;(b) rearranging the terms; (c) using $L_{\\lambda}(\\pmb\\theta)\\quad=$ $\\left\\|{R}\\right\\|_{2}^{2}+(\\lambda/2)\\left\\|{W}\\right\\|_{F}^{2}+\\left(\\lambda/2\\right)\\left\\|{a}\\right\\|_{2}^{2}=L_{\\lambda}(\\mu_{\\lambda}^{*})+\\zeta$ ", "page_idx": 41}, {"type": "text", "text": "For the first line on RHS of (12), we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad L_{\\lambda}(\\mu_{\\lambda}^{*})+\\zeta+\\frac{\\lambda}{2}(\\|W\\|_{F}^{2}-\\|a\\|_{2}^{2})-\\lambda\\displaystyle\\sum_{i\\in[m_{*}]}\\sum_{j\\in\\mathcal{T}_{i}}q_{i j}\\left\\|w_{j}\\right\\|_{2}}\\\\ &{\\stackrel{\\mathrm{(a)}}{\\geq}\\zeta/2+L(\\mu_{\\lambda}^{*})+\\lambda|\\mu_{\\lambda}^{*}|-\\lambda\\displaystyle\\sum_{i\\in[m_{*}]}\\sum_{j\\in\\mathcal{T}_{i}}q_{i j}\\left\\|w_{j}\\right\\|_{2}}\\\\ &{\\stackrel{\\mathrm{(b)}}{\\geq}\\zeta/2+\\lambda|\\mu_{\\lambda}^{*}|-\\lambda\\left\\|a_{*}\\right\\|_{1}+\\lambda\\displaystyle\\sum_{i\\in[m_{*}]}\\sum_{j\\in\\mathcal{T}_{i}}q_{i j}(|a_{j}|-\\|w_{j}\\|_{2})}\\\\ &{\\stackrel{\\mathrm{(c)}}{\\geq}\\zeta/2-O_{*}(\\lambda^{2})-\\lambda\\left(\\displaystyle\\sum_{i\\in[m_{*}]}\\sum_{j\\in\\mathcal{T}_{i}}q_{i j}^{2}\\right)^{1/2}\\left(\\displaystyle\\sum_{i\\in[m]}(|a_{j}|-\\|w_{j}\\|_{2})^{2}\\right)^{1/2}\\stackrel{\\mathrm{(d)}}{\\geq}\\zeta/4,}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where (a) due to assumption that norms are balanced; (b) we ignore $L(\\mu_{\\lambda}^{*})$ and add and minus $\\lambda\\left\\Vert\\pmb{a}_{*}\\right\\Vert_{1}$ ; (c) due to Lemma F.3; (d) due to assumption that norms are balanced and the choice of $q_{i j}$ In the following, we will lower bound the last term of (12) to show it is no smaller than $-\\zeta/8$ that we get the desired lower bound. Recall the residual decomposition (8) that $R({\\bf x})=R_{1}({\\bf x})+$ $R_{2}({\\pmb x})+R_{3}({\\pmb x})$ ,wehave ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{i\\in[m_{i}]}{\\sum\\sum}\\underset{j\\in\\mathcal{T}_{i}}{\\sum}[R_{x}[R(x)a_{j}{q_{i j}}_{i j}w_{i}^{*\\top}x(\\sigma^{\\prime}(w_{j}^{*\\top}x)-\\sigma^{\\prime}(w_{i}^{*\\top}x))]}\\\\ &{=\\underset{i\\in[m_{i}]}{\\sum\\sum}\\underset{j\\in\\mathcal{T}_{i}}{\\sum}[R_{x}[R_{i}(x)a_{j}{q_{i j}}_{i j}w_{i}^{*\\top}x(\\sigma^{\\prime}(w_{i}^{*\\top}x)-\\sigma^{\\prime}(w_{j}^{\\top}x))]}\\\\ &{\\quad+\\underset{i\\in[m_{i}]}{\\sum\\sum}\\underset{j\\in\\mathcal{T}_{i}}{\\sum}[R_{x}[R_{2}(x)a_{j}{q_{i j}}_{i j}w_{i}^{*\\top}x(\\sigma^{\\prime}(w_{i}^{*\\top}x)-\\sigma^{\\prime}(w_{j}^{\\top}x))]}\\\\ &{\\quad+\\underset{i\\in[m_{i}]}{\\sum\\sum}\\underset{j\\in\\mathcal{T}_{i}}{\\sum}[\\overline{{\\Omega}}_{i}[R_{3}(x){q_{i j}}_{i j}w_{i}^{*\\top}x(\\sigma^{\\prime}(w_{i}^{*\\top}x)-\\sigma^{\\prime}(w_{j}^{\\top}x))]}\\\\ &{\\quad+\\underset{i\\in[m_{i}]}{\\sum\\sum}\\underset{j\\in\\mathcal{T}_{i}}{\\sum}[\\overline{{\\Omega}}_{i}[R_{3}(x){q_{i j}}_{i j}w_{i}^{*\\top}x(\\sigma^{\\prime}(w_{i}^{*\\top}x)-\\sigma^{\\prime}(w_{j}^{\\top}x))]}\\\\ &{\\quad+\\underset{i\\in[m_{i}]}{\\sum\\sum}\\underset{j\\in\\mathcal{T}_{i}}{\\sum}[\\overline{{\\Omega}}_{i}[R_{3}(x){q_{i j}}_{i j}w_{i}^{*\\top}x(\\sigma^{\\prime}(w_{i}^{*\\top}x)-\\sigma^{\\prime}(w_{j}^{\\top}x))]}\\\\ &{\\quad+\\underset{i\\in[m_{i}]}{\\sum}(\\overline{{\\Omega}}_{i}[I I I]}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Bound I) For (I), recall $\\begin{array}{r l r}{R_{1}(\\pmb{x})}&{{}=}&{(1/2)\\sum_{i\\in[m_{*}]}\\pmb{v}_{i}^{\\top}\\pmb{x}\\operatorname{sign}(\\pmb{w}_{i}^{*\\top}\\pmb{x}),}\\end{array}$ where $\\begin{array}{r l}{\\pmb{v}_{i}}&{{}=}\\end{array}$ $\\textstyle\\sum_{j\\in\\mathcal{T}_{i}}a_{j}\\mathbf{w}_{j}-\\mathbf{w}_{i}^{*}$ is the difference between average neuron and corresponding ground-truth and $\\begin{array}{r}{(\\sum_{i\\in[m_{*}]}\\|v_{i}\\|_{2}^{2})^{1/2}=O_{*}((\\zeta/\\lambda)^{3/4})}\\end{array}$ from Lemma F.10 and Lemma F.11. We have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{i\\in[m_{i}],j\\in\\mathcal{T}_{h}}{\\sum\\sum}\\mathbb{E}_{\\alpha}[R_{1}(\\alpha)\\mu_{j}\\mathrm{f}_{i j}\\mathrm{t}\\omega_{i}^{\\top}\\mathbf{x}(\\sigma^{\\prime}(\\mathbf{u}_{i}^{*}\\cdot\\mathbf{\\bar{x}})-\\sigma^{\\prime}(\\mathbf{u}_{j}^{\\top}\\mathbf{\\bar{x}}))]}\\\\ &{\\overset{(a)}{\\geq}-\\frac{1}{2}\\underset{i\\in[m_{i}],j\\in\\mathcal{T}_{h}}{\\sum\\sum}\\underset{k\\in[m_{i}],k\\in[m_{i}]}{\\sum}\\mathbb{E}_{\\alpha}[|\\mathbf{v}_{k}^{T}\\mathbf{x}||a_{j}\\mathrm{f}_{i j}]|\\mathbf{u}_{i}^{\\top}\\mathbb{\\bar{x}}[\\mathrm{1}_{s\\oplus(w_{i}^{\\top}\\mathbf{\\bar{x}})\\neq\\mathrm{i}\\phi(\\mathbf{u}_{i}^{\\top}\\mathbf{\\bar{x}})}]}\\\\ &{\\overset{(b)}{=}-\\frac{1}{2}\\underset{i\\in[m_{i}],j\\in\\mathcal{T}_{h}}{\\sum\\sum}\\underset{k\\in[m_{i}]}{\\sum}|a_{j}\\mathrm{f}_{i j}|\\|\\mathbf{v}_{k}\\|\\|\\mathbb{g}_{\\bar{x}}[|\\mathbf{\\bar{x}}_{i}^{T}\\bar{\\mathbf{\\bar{x}}}||\\mathrm{w}_{i k}^{\\top}\\bar{\\mathbf{x}}]|_{s\\oplus\\mathrm{i}\\phi(\\mathbf{u}_{i}^{\\top}\\mathbf{\\bar{x}})\\neq\\mathrm{i}\\phi(\\mathbf{u}_{i}^{\\top}\\mathbf{\\bar{x}})}}\\\\ &{\\overset{(c)}{\\geq}-\\frac{1}{2}\\underset{i\\in[m_{i}],j\\in\\mathcal{T}_{h}}{\\sum\\sum}\\underset{k\\in[m_{i}]}{\\sum}|a_{j}\\mathrm{f}_{i j}|\\|\\mathbf{v}_{k}\\|_{2}\\delta\\mathbb{E}_{\\alpha}[\\|\\bar{\\mathbf{x}}|_{2}^{2}]\\mathbf{1}_{s\\oplus(w_{i}^{\\top}\\mathbf{\\bar{x}})\\neq\\mathrm{i}\\phi(\\mathbf{u}_{i}^{\\top}\\mathbf{\\bar{x}})}}\\\\ &{\\overset{(d)}{\\geq}-\\frac{1}{2}\\underset{i\\in[m_{i}],j\\in\\mathcal{T}_{\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where in (a) we plug in the definition of $R_{1}$ and using the fact that $\\pmb{w}_{i}^{\\ast\\top}\\pmb{x}(\\sigma^{\\prime}(\\pmb{w}_{i}^{\\ast\\top}\\pmb{x})-\\sigma^{\\prime}(\\pmb{w}_{j}^{\\top}\\pmb{x}))=$ $|{\\pmb w}_{i}^{*\\top}{\\pmb x}|\\,\\mathbb{1}_{\\mathrm{sign}({\\pmb w}_{j}^{\\top}{\\pmb x})\\neq\\mathrm{sign}({\\pmb w}_{i}^{*\\top}{\\pmb x})}$ (b) $\\widetilde{\\mathbf{\\Omega}}$ is a3-dimensional Gaussan since theexpectationonlyepends on $\\pmb{v}_{k},\\pmb{w}_{i}^{*},\\pmb{w}_{j}$ ; (c) $|{\\pmb w}_{i}^{*\\top}{\\widetilde x}|\\leq\\delta_{j}\\,\\|{\\widetilde x}\\|_{2}$ when $\\mathrm{sign}(\\pmb{w}_{j}^{\\top}\\widetilde{\\pmb{x}})\\neq\\mathrm{sign}(\\pmb{w}_{i}^{\\ast\\top}\\widetilde{\\pmb{x}})$ ; (d) a direct calculation bound as Lemma H.2; (e) definition of qij . ", "page_idx": 43}, {"type": "text", "text": "Bound (II) For (II), recall ", "text_level": 1, "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathfrak{L}_{2}(\\boldsymbol{x})=\\frac{1}{2}\\sum_{i\\in[m_{*}]}\\sum_{j\\in\\mathcal{T}_{i}}a_{j}w_{j}^{\\top}\\boldsymbol{x}(\\mathrm{sign}(\\boldsymbol{w}_{j}^{\\top}\\boldsymbol{x})-\\mathrm{sign}(\\boldsymbol{w}_{i}^{*\\top}\\boldsymbol{x}))=\\sum_{i\\in[m_{*}]}\\sum_{j\\in\\mathcal{T}_{i}}a_{j}|w_{j}^{\\top}\\boldsymbol{x}|\\mathbb{1}_{\\mathrm{sign}(\\boldsymbol{w}_{j}^{\\top}\\boldsymbol{x})\\neq\\mathrm{sign}(\\boldsymbol{w}_{i}^{*}\\setminus\\mathcal{T}_{i})},\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "For each term in (II) with $j\\in\\mathcal T_{i}$ , we can split it into two terms that corresponding to $\\mathcal{T}_{i}$ and other $\\tau_{k}$ 's. ", "page_idx": 43}, {"type": "text", "text": "$\\begin{array}{r l}&{\\quad\\mathbb{E}_{\\mathbf{x}}[R_{2}(\\mathbf{x})a_{j}q_{i j}{w_{i}^{*}}^{\\top}\\mathbf{x}(\\sigma^{\\prime}({w_{i}^{*}}^{\\top}\\mathbf{x})-\\sigma^{\\prime}({w_{j}^{*}}\\mathbf{x}))]}\\\\ &{=\\displaystyle\\sum_{k\\in[m_{*}]}\\sum_{\\ell\\in\\mathcal{T}_{k}}\\mathbb{E}_{\\mathbf{x}}[a_{\\ell}|{w_{\\ell}^{\\top}}^{\\top}\\mathbf{x}|\\mathbb{I}_{\\mathrm{sign}({w_{\\ell}^{\\top}}^{\\top}\\mathbf{x})\\neq\\mathrm{sign}({w_{i}^{*}}^{\\top}\\mathbf{x})}\\cdot a_{j}{q_{i j}}{w_{i}^{*}}^{\\top}\\mathbf{x}(\\sigma^{\\prime}({w_{i}^{*}}^{\\top}\\mathbf{x})-\\sigma^{\\prime}({w_{j}^{\\top}}\\mathbf{x})]}\\\\ &{=\\displaystyle\\sum_{k\\in[m_{*}]}\\sum_{\\ell\\in\\mathcal{T}_{k}}a_{\\ell}a_{j}q_{i j}\\mathbb{E}_{\\mathbf{x}}[|{w_{\\ell}^{\\top}}^{\\top}\\mathbf{x}|\\mathbb{I}_{\\mathrm{sign}({w_{\\ell}^{\\top}}^{\\top}\\mathbf{x})\\neq\\mathrm{sign}({w_{i}^{*}}^{\\top}\\mathbf{x})}\\cdot|{w_{i}^{*}}^{\\top}\\mathbf{x}|\\mathbb{I}_{\\mathrm{sign}({w_{i}^{*}}^{\\top}\\mathbf{x})\\neq\\mathrm{sign}({w_{j}^{\\top}}\\mathbf{x})}]}\\\\ &{=\\displaystyle\\sum_{\\ell\\in\\mathcal{T}_{i}}a_{\\ell}a_{j}q_{i j}\\mathbb{E}_{\\mathbf{x}}[|{w_{\\ell}^{\\top}}^{\\top}\\mathbf{x}||{w_{i}^{*}}^{\\top}\\mathbf{x}|\\mathbb{I}_{\\mathrm{sign}({w_{\\ell}^{\\top}}^{\\top}\\mathbf{x})\\neq\\mathrm{sign}({w_{i}^{*}}^{\\top}\\mathbf{x})}\\cdot\\mathbb{I}_{\\mathrm{sign}({w_{i}^{*}}^{\\top}\\mathbf{x})\\neq\\mathrm{sign}({w_{j}^{\\top}}\\mathbf{x})}]}\\end{array}$ (I11.i)   \nktilETk (I1.ii) ", "page_idx": 43}, {"type": "text", "text": "For (II.i), we further split neurons into $\\mathcal{T}_{i}(\\delta_{\\mathrm{sign}})$ and others: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I I.i)=\\displaystyle\\sum_{\\ell\\in\\mathcal{T}_{i}(\\delta_{\\mathrm{sign}})}a_{\\ell}a_{j}q_{i j}\\mathbb{E}_{x}[|w_{\\ell}^{\\top}x||w_{i}^{*^{\\top}}x|\\mathbb{1}_{\\mathrm{sign}(w_{\\ell}^{\\top}x)\\neq\\mathrm{sign}(w_{i}^{*^{\\top}}x)}\\cdot\\mathbb{1}_{\\mathrm{sign}(w_{i}^{*^{\\top}}x)\\neq\\mathrm{sign}(w_{j}^{\\top}x)}]}\\\\ &{\\qquad+\\displaystyle\\sum_{\\ell\\in\\mathcal{T}_{i}\\setminus\\mathcal{T}_{i}(\\delta_{\\mathrm{sign}})}a_{\\ell}a_{j}q_{i j}\\mathbb{E}_{x}[|w_{\\ell}^{\\top}x||w_{i}^{*^{\\top}}x|\\mathbb{1}_{\\mathrm{sign}(w_{\\ell}^{\\top}x)\\neq\\mathrm{sign}(w_{i}^{*^{\\top}}x)}\\cdot\\mathbb{1}_{\\mathrm{sign}(w_{i}^{*^{\\top}}x)\\neq\\mathrm{sign}(w_{j}^{\\top}x)}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Consider the first line of (14), from the choice of $q_{i j}$ we know $a_{j}q_{i j}a_{i}^{*}\\geq0$ For $\\ell\\in\\mathcal{T}_{i,+}(\\delta_{\\mathrm{sign}})$ we know $\\mathrm{sign}(a_{\\ell})=\\mathrm{sign}(a_{i}^{*})$ which implies $a_{\\ell}a_{j}q_{i j}\\geq0$ for these terms. We thus only need to deal with neurons in $T_{i,-}(\\delta_{\\mathrm{sign}})$ , we have the first line is bounded as ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\varepsilon\\in\\mathbb Z_{T}}{\\sum}\\underset{(\\varepsilon_{i},\\ldots,(\\delta_{\\delta_{\\delta}}))}{\\sum}a_{\\varepsilon\\in[0,\\varepsilon_{i}]}\\mathbb{E}_{\\pi}[\\|w_{t}^{\\varepsilon}\\|_{H^{1}}^{-1}\\chi_{\\operatorname*{sign}(w_{t}^{\\varepsilon},\\pi)\\in\\mathrm{bign}(w_{t}^{\\varepsilon},\\pi)}\\cdot1_{\\mathrm{sign}(w_{t}^{\\varepsilon},\\pi)\\neq\\mathrm{sign}(w_{j}^{\\varepsilon},\\pi)}]}\\\\ &{\\geq\\underset{\\varepsilon\\in\\mathbb Z_{T}-(\\delta_{\\delta_{\\delta}})}{\\sum}a_{\\varepsilon\\in[0,\\varepsilon_{i}]}y_{\\varepsilon_{i}}[w_{t}^{\\varepsilon}]x|w_{t}^{\\varepsilon_{i}}\\pi|_{s i g n(w_{t}^{\\varepsilon},\\pi)\\neq\\mathrm{sign}(w_{t}^{\\varepsilon},\\pi)}\\cdot1_{\\mathrm{sign}(w_{t}^{\\varepsilon},\\pi)\\neq\\mathrm{sign}(w_{j}^{\\varepsilon},\\pi)}]}\\\\ &{\\overset{(a)}{\\geq}-|a_{\\varepsilon_{i}}\\pi_{i}|_{\\varepsilon}\\sum_{\\varepsilon_{i},\\ldots,(\\delta_{\\delta})}\\left|a_{\\varepsilon}\\right|\\,\\|w_{t}\\|_{2}\\mathbb{E}_{\\pi}[\\|w_{t}^{\\varepsilon}\\widehat{x}\\|_{W^{1}}\\widehat{x}]_{(\\mathbb{R})(\\pi_{t}^{1},\\pi)(w_{t}^{\\varepsilon},\\widehat{x})\\neq\\mathrm{sign}(w_{t}^{\\varepsilon},\\pi)}\\cdot1_{\\mathrm{sign}(w_{t}^{\\varepsilon},\\pi)\\neq\\mathrm{sign}(w_{t}^{\\varepsilon},\\pi)}}\\\\ &{\\overset{(b)}{\\geq}-|a_{\\varepsilon_{i}}\\pi_{i}|_{\\varepsilon}\\sum_{\\substack{\\varepsilon\\in[0,\\varepsilon_{i}]}}|a_{\\varepsilon}|\\,\\|w_{t}\\|_{2}\\delta_{\\delta}\\delta_{\\varepsilon}\\mathbb{E}_{\\pi}[\\|\\widehat{x}\\|_{2}^{2}]\\,\\underset{\\mathrm{sign}(w_{t}^{\\varepsilon},\\pi)\\neq\\mathrm{sign}(w_{t}^{\\varepsilon},\\pi)}{\\sum_{1\\leq a\\leq\\lfloor\\pi\\rfloor}}\\cdot1_{\\mathrm{sign}(w_{t}^{\\varepsilon}, \n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where (a) $\\widetilde{\\mathbf{\\Lambda}}_{x}$ is a 3-dimensional Gaussian since the expectation only depends on $\\pmb{w}_{\\ell},\\pmb{w}_{j},\\pmb{w}_{i}^{*}$ ;(b) $|\\overline{{\\pmb{w}}}_{\\ell}^{\\top}\\widetilde{\\pmb{x}}|\\leq\\delta_{\\ell}\\,\\|\\widetilde{\\pmb{x}}\\|_{2}$ when $\\mathrm{sign}(\\pmb{w}_{i}^{*\\top}\\widetilde{\\pmb{x}})\\neq\\mathrm{sign}(\\pmb{w}_{\\ell}^{\\top}\\widetilde{\\pmb{x}})$ and $|{\\pmb w}_{i}^{*\\top}{\\widetilde x}|\\leq\\delta_{j}\\,\\|{\\widetilde x}\\|_{2}$ when $\\mathrm{sign}(\\pmb{w}_{i}^{*\\top}\\widetilde{\\pmb{x}})\\neq$ $\\mathrm{sign}(\\pmb{w}_{j}^{\\top}\\widetilde{\\pmb{x}})$ ; (c) a direct calculation as in Lemma H.2; (d) assumption that norm cancellation is small. For the cecond term of (14) cimilor ac ahove we have ", "page_idx": 44}, {"type": "text", "text": "", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{2}{\\varepsilon\\in\\mathbb{Z}_{\\neq}\\cap_{i}\\cap_{\\varepsilon=0}}\\sum_{\\substack{\\ell\\in\\mathbb{Z}_{\\neq}}}\\left|w_{\\ell}\\middle|\\,w_{\\varepsilon}^{\\top}\\middle{x}\\middle||w_{i}^{*\\top}\\middle{x}\\middle||\\operatorname*{sign}(w_{\\ell}^{\\top}\\alpha)\\neq\\operatorname*{sign}(w_{i}^{*\\top}\\tau)\\cdot\\cdot\\mathbb{I}_{\\operatorname*{sign}(w_{i}^{*}\\tau)\\neq\\operatorname*{sign}(w_{j}^{\\top}\\alpha)}\\right|}\\\\ &{\\overset{(a)}{\\geq}-2|a_{j}q_{i j}|\\qquad\\sum_{\\substack{\\ell\\in\\mathbb{Z}_{\\neq}\\setminus\\mathcal{T}_{\\ell}(i,\\ell_{\\mathrm{sign}})}}|a_{\\ell}|\\left\\|w_{\\ell}\\middle|\\right\\|\\mathbb{I}w_{i}^{*}\\middle{\\widetilde{x}}\\middle|\\left||w_{i}^{*\\top}\\widetilde{x}\\middle|\\right\\|\\operatorname*{sign}(w_{\\ell}^{\\top}\\overline{{x}})\\neq\\operatorname*{sign}(w_{i}^{*\\top}\\overline{{x}})\\cdot\\mathbb{I}_{\\operatorname*{sign}(w_{\\ell}^{*}\\tau)}\\cdot\\mathbb{I}_{\\operatorname*{sign}(w_{\\ell}^{*}\\tau)}}\\\\ &{\\overset{(b)}{\\geq}-2|a_{j}q_{i j}|\\qquad\\sum_{\\substack{\\ell\\in\\mathcal{T}_{\\ell}\\setminus\\mathcal{T}_{\\ell}(i,\\ell_{\\mathrm{sign}})}}|a_{\\ell}|\\left\\|w_{\\ell}\\middle|\\right\\|\\mathbb{I}_{\\ell}\\delta_{\\ell}\\delta_{\\ell}\\mathbb{E}_{\\widetilde{x}}\\big[\\big\\|\\widetilde{x}\\big\\|_{2}^{2}\\operatorname*{lign}(w_{i}^{*\\top}\\tau)\\neq\\operatorname*{sign}(w_{j}^{*}\\overline{{x}})\\big]}\\\\ &{\\overset{(c)}{\\geq}-2|a_{j}q_{i j}|\\sigma(\\delta_{j}^{2})\\underset{\\ell\\in\\mathcal{T}_{\\ell}\\setminus\\mathcal{T}_{\\ell}(i,\\ell_{\\mathrm{sign}})}{\\leq}\\Big|a_{\\ell}|\\left\\|w_{\\ell}\\middle\\|_{2}\\delta_{\\ell}\\right\\|}\\\\ &{\\overset{(b)}{\\geq}-2|a_{j}q_{i j}|\\sigma(\\delta_{j}^{2}) \n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where (a) $\\widetilde{\\mathbf{\\Lambda}}_{x}$ is 3-dimensional Gaussian vector since the expectation only depends on $\\pmb{w}_{\\ell},\\pmb{w}_{j},\\pmb{w}_{i}^{*}$ (b) $|\\overline{{\\pmb{w}}}_{\\ell}^{\\top}\\widetilde{\\pmb{x}}|\\leq\\delta_{\\ell}\\,\\|\\widetilde{\\pmb{x}}\\|_{2}$ when $\\mathrm{sign}(\\pmb{w}_{i}^{*\\top}\\widetilde{\\pmb{x}})\\neq\\mathrm{sign}(\\pmb{w}_{\\ell}^{\\top}\\widetilde{\\pmb{x}})$ and $|{\\pmb w}_{i}^{*\\top}{\\widetilde x}|\\leq\\delta_{j}\\,\\|{\\widetilde x}\\|_{2}$ when $\\mathrm{sign}(\\pmb{w}_{i}^{*\\top}\\widetilde{\\pmb{x}})\\neq$ $\\mathrm{sign}(\\pmb{w}_{j}^{\\top}\\widetilde{\\pmb{x}})$ ; (c) a direct calculation as in Lemma H.2; (d) choice of $q_{i j}$ and Lemma F.5 and Lemma F.11 that far-away neurons are small. ", "page_idx": 44}, {"type": "text", "text": "Thus, for (II.i) we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n(I I.i)\\ge-2|a_{j}q_{i j}|O_{*}(\\tau\\delta_{\\mathrm{sign}}\\delta_{c l o s e}^{2}+\\delta_{c l o s e}^{2}\\zeta\\lambda^{-1}\\delta_{\\mathrm{sign}}^{-1}).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "For (II.i), we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|(I I.i i)|\\leq2\\displaystyle\\sum_{k\\neq i}\\sum_{\\ell=\\ell\\neq h}|a_{\\ell}||a_{j}q_{i j}|\\,\\|\\mathbf{z}_{\\mathbf{x}}[|\\mathbf{w}_{\\ell}^{\\top}\\mathbf{x}||w_{i}^{*\\top}\\mathbf{x}|]\\log(w_{\\ell}^{\\top}\\mathbf{x})^{*}\\mathrm{sign}(w_{k}^{*\\top}\\mathbf{x})\\cdot\\mathbb{1}_{\\mathrm{sign}(w_{i}^{*\\top}\\mathbf{\\bar{x}})\\neq\\mathrm{sign}(w_{j}^{\\top}\\mathbf{x})}}\\\\ &{\\overset{(a)}{\\leq}2\\displaystyle\\sum_{k\\neq i}\\sum_{\\ell=\\ell\\neq h}|a_{\\ell}||a_{j}q_{i j}|\\,\\|\\mathbf{w}_{\\ell}\\|_{2}\\,\\delta_{\\ell}\\delta_{j}\\mathbb{E}_{\\bar{\\mathbf{x}}}[\\|\\tilde{\\mathbf{x}}\\|_{2}^{2}\\,\\mathbf{1}_{\\mathrm{sign}(w_{\\ell}^{\\top}\\widetilde{\\mathbf{x}})\\neq\\mathrm{sign}(w_{k}^{*\\top}\\widetilde{\\mathbf{x}})}\\cdot\\mathbb{1}_{\\mathrm{sign}(w_{i}^{*\\top}\\widetilde{\\mathbf{x}})\\neq\\mathrm{sign}(w_{j}^{\\top}\\widetilde{\\mathbf{x}})}}\\\\ &{\\overset{(b)}{\\leq}2\\displaystyle\\sum_{k\\neq i}\\sum_{\\ell=\\ell}|a_{\\ell}||a_{j}q_{i j}|\\,\\|\\mathbf{w}_{\\ell}\\|_{2}\\,\\delta_{\\ell}\\delta_{j}\\mathbb{E}_{\\bar{\\mathbf{x}}}[\\|\\tilde{\\mathbf{x}}\\|_{2}^{2}\\,\\mathbf{1}_{|\\mathbf{w}_{k}^{*\\top}\\widetilde{\\mathbf{x}}|\\leq\\delta_{\\ell}\\|\\widetilde{\\mathbf{x}}\\|_{2}}\\cdot\\mathbf{1}_{|\\mathbf{w}_{i}^{*\\top}\\widetilde{\\mathbf{x}}|\\leq\\delta_{j}|\\widetilde{\\mathbf{x}}\\|_{2}}]}\\\\ &{\\overset{(c)}{\\leq}2|a_{j}q_{i j}|\\delta_{j}\\displaystyle\\sum_{k\\neq i}\\sum_{\\ell\\neq\\ell}|a_{\\ell}|\\,\\|\\mathbf{w}_{\\ell}\\|_{2}\\,\\delta_{\\ell}\\cdot O(\\delta_{\\ell}\\delta_{j}/\\Delta)}\\\\ &{\\overset{(d)}{=}2|a_{j\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where (a)(b) $\\widetilde{\\mathbf{\\Omega}}$ is a 4-dimensional Gaussian vector, $|\\overline{{{\\pmb w}}}_{\\ell}^{\\top}\\widetilde{\\pmb x}|\\ \\leq\\ \\delta_{\\ell}\\,\\|\\widetilde{\\pmb x}\\|_{2}$ when $\\mathrm{sign}(\\pmb{w}_{i}^{*\\top}\\widetilde{\\pmb{x}})\\ \\neq$ $\\mathrm{sign}(\\pmb{w}_{\\ell}^{\\top}\\widetilde{\\pmb{x}})$ and $|{\\pmb w}_{i}^{*\\top}{\\widetilde x}|\\,\\leq\\,\\delta_{j}\\,\\,\\|{\\widetilde x}\\|_{2}$ when $\\mathrm{sign}({\\pmb w}_{i}^{*\\top}{\\widetilde x})\\,\\neq\\,\\mathrm{sign}({\\pmb w}_{j}^{\\top}{\\widetilde x})$ ; (c) by Lemma H.1; (d) choice of $q_{i j}$ and Lemma F.5 and Lemma F.11 that far-away neurons are small. ", "page_idx": 44}, {"type": "text", "text": "Combine (I1.i) (II.i), we have for (13) ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\boldsymbol{x}}[R_{2}(\\boldsymbol{x})a_{j}q_{i j}w_{i}^{*\\top}\\boldsymbol{x}(\\sigma^{\\prime}(w_{i}^{*\\top}\\boldsymbol{x})-\\sigma^{\\prime}(w_{j}^{\\top}\\boldsymbol{x}))]\\ge-2|a_{j}q_{i j}|O(\\tau\\delta_{\\mathrm{sign}}\\delta_{c l o s e}^{2}+\\delta_{c l o s e}^{2}\\zeta\\lambda^{-1}\\delta_{\\mathrm{sign}}^{-1}).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "This further gives the lower bound on (II): ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i\\in[m_{*}]}\\sum_{j\\in\\mathcal{T}_{i}}\\mathbb{E}_{\\boldsymbol{x}}[R_{2}(\\boldsymbol{x})a_{j}q_{i j}\\boldsymbol{w}_{i}^{*\\top}\\boldsymbol{x}(\\sigma^{\\prime}(\\boldsymbol{w}_{i}^{*\\top}\\boldsymbol{x})-\\sigma^{\\prime}(\\boldsymbol{w}_{j}^{\\top}\\boldsymbol{x}))]}\\\\ &{\\displaystyle\\geq-\\sum_{i\\in[m_{*}]}\\sum_{j\\in\\mathcal{T}_{i}}|a_{j}q_{i j}|O(\\tau\\delta_{\\mathrm{sign}}\\delta_{c l o s e}^{2}+\\delta_{c l o s e}^{2}\\zeta\\lambda^{-1}\\delta_{\\mathrm{sign}}^{-1})}\\\\ &{\\displaystyle=-\\sum_{\\delta_{*}\\left(\\tau\\delta_{\\mathrm{sign}}\\delta_{c l o s e}^{2}+\\delta_{c l o s e}^{2}\\zeta\\lambda^{-1}\\delta_{\\mathrm{sign}}^{-1}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Bound $\\left(\\mathbf{III}\\right)$ For (II), recall $\\begin{array}{r}{{\\cal R}_{3}({\\pmb x})=\\frac{1}{\\sqrt{2\\pi}}\\left(\\sum_{i\\in[m_{*}]}a_{i}^{*}\\,\\|{\\pmb w}_{i}^{*}\\|_{2}-\\sum_{i\\in[m]}a_{i}\\,\\|{\\pmb w}_{i}\\|_{2}\\right)+\\alpha-\\hat{\\alpha}+}\\end{array}$ $(\\beta-\\hat{\\beta})^{\\top}x$ Wehave ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{(b)\\leq t}{\\sum}\\underset{i\\in[1,1)}{\\sum}\\underset{j\\in\\mathcal{T}_{i}}{\\sum}|\\mathbb{B}_{z}|\\mathbb{H}_{\\partial^{\\tau}}\\boldsymbol{\\mathrm{B}}_{i j}^{\\top}\\boldsymbol{\\mathrm{B}}_{i j}^{\\top}\\boldsymbol{\\mathrm{B}}_{i j}^{\\top}\\boldsymbol{\\mathrm{B}}_{i j}^{\\top}\\boldsymbol{\\mathrm{B}}_{i j}^{\\top}\\boldsymbol{\\mathrm{B}}_{i j}^{\\top}\\boldsymbol{\\mathrm{B}}_{i j}^{\\top}\\boldsymbol{\\mathrm{B}}_{i j}^{\\top}}\\\\ &{\\overset{(c)}{\\geq}-O_{i}(\\zeta/3)\\underset{i\\in[1,1]}{\\sum}\\underset{i\\in[1,1]}{\\sum}|\\alpha_{i j}\\psi_{i j}|\\mathbb{B}_{i j}^{\\top}|\\boldsymbol{\\mathrm{B}}_{i j}^{\\top}\\boldsymbol{\\mathrm{B}}_{i j}^{\\top}|\\boldsymbol{\\mathrm{B}}_{i j}^{\\top}\\boldsymbol{\\mathrm{B}}_{i j}^{\\top}\\boldsymbol{\\mathrm{B}}_{i j}^{\\top}\\boldsymbol{\\mathrm{B}}_{i j}^{\\top}\\boldsymbol{\\mathrm{B}}_{i j}^{\\top}}\\\\ &{\\quad-\\underset{(b)\\leq t}{\\sum}\\underset{i\\in[1,1]}{\\sum}\\left[|\\partial_{t}\\psi_{i j}|\\mathbb{B}_{i j}^{\\top}|\\left|\\left(\\beta-\\beta\\right)^{\\top}\\mathbb{B}_{i}^{\\top}\\right|\\left|\\alpha_{i j}^{\\top}\\mathbb{B}_{i j}^{\\top}\\boldsymbol{\\mathrm{B}}_{i j}^{\\top}\\boldsymbol{\\mathrm{B}}_{i j}^{\\top}\\boldsymbol{\\mathrm{B}}_{i j}^{\\top}\\boldsymbol{\\mathrm{B}}_{i j}^{\\top}\\boldsymbol{\\mathrm{B}}_{i j}^{\\top}\\boldsymbol{\\mathrm{B}}_{i j}^{\\top}\\boldsymbol{\\mathrm{B}}_{i j}^{\\top}\\boldsymbol{\\mathrm{B}}_{i j}^{\\top}\\boldsymbol{\\mathrm{B}}_{i j}^{\\top}\\boldsymbol{\\mathrm{B}}_{i j}^{\\top}\\boldsymbol{\\mathrm{B}}_{i j}^{\\top}\\boldsymbol{\\mathrm{B}}_{i j}^{\\top}\\right.}\\\\ &{\\overset{(b)}{\\geq}-O_{i}(\\zeta/3)\\underset{i\\in[1,1]}{\\sum}\\underset{i\\in[1,1]}{\\sum}|\\alpha_{i j}\\psi_{i j}|\\mathcal{O \n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where (a) plugging in the expression of $R_{3}$ and using Lemma F.9 and Lemma F.11; (b) using Lemma H.3 and the fact that $\\widetilde{\\mathbf{\\Omega}}$ is a 3-dimensional Gaussian vector and $|{\\pmb w}_{i}^{*\\top}{\\widetilde x}|\\leq\\delta_{j}\\,\\|{\\widetilde x}\\|_{2}$ when $\\mathrm{sign}(\\pmb{w}_{i}^{*\\top}\\widetilde{\\pmb{x}})\\neq\\mathrm{sign}(\\pmb{w}_{j}^{\\top}\\widetilde{\\pmb{x}})$ ; (c) Lemma H.2; (d) choice of $q_{i j}$ ", "page_idx": 45}, {"type": "text", "text": "Combine all bounds Combine (I) (II) (IIl) we now get the last term of (12) ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\sum_{\\substack{\\in[m_{*}]\\,j\\in\\mathcal{T}_{i}}}\\mathbb{E}_{x}[R(x)a_{j}q_{i j}w_{i}^{*\\top}x(\\sigma^{\\prime}(w_{j}^{\\top}x)-\\sigma^{\\prime}(w_{i}^{*\\top}x))]\\ge-O_{*}((\\zeta/\\lambda)^{3/4}\\delta_{c l o s e}^{2}+\\tau\\delta_{\\mathrm{sign}}\\delta_{c l o s e}^{2}+\\delta_{c l o s e}^{4})\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "From Lemma F.6 we can choose $\\delta_{c l o s e}=O_{*}(\\zeta^{1/3})$ and from Lemma F.16 we can choose $\\delta_{\\mathrm{sign}}=$ $\\Theta_{*}(\\lambda/\\zeta^{1/2})$ . Also with $\\tau=O(\\zeta^{5/6}/\\lambda)$ , we finally get ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\sum_{i\\in[m_{*}]}\\sum_{j\\in{\\mathcal{T}}_{i}}{\\mathbb{E}}_{x}[R(x)a_{j}{q_{i j}}{w_{i}^{*}}^{\\top}x(\\sigma^{\\prime}({w_{j}^{\\top}}x)-\\sigma^{\\prime}({w_{i}^{*}}^{\\top}x))]\\ge\\zeta/8,\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "as long as $\\zeta=O(\\lambda^{9/5}/\\operatorname{poly}(r,m_{*},\\Delta,\\|\\pmb{\\mathscr{a}}_{*}\\|_{1}\\,,a_{\\operatorname*{min}}))$ with small enough hidden constant. ", "page_idx": 45}, {"type": "text", "text": "Thus, we eventually get the lower bound of (12) ", "page_idx": 45}, {"type": "equation", "text": "$$\n(\\alpha+\\alpha_{*})\\nabla_{\\alpha}L_{\\lambda}+\\langle\\nabla_{\\beta}L_{\\lambda},\\beta+\\beta_{*}\\rangle+\\sum_{i\\in[m_{*}]}\\sum_{j\\in{\\mathcal{T}}_{i}}\\langle\\nabla_{w_{i}}L_{\\lambda},w_{j}-q_{i j}w_{i}^{*}\\rangle\\ge\\zeta/4-\\zeta/8=\\zeta/8.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "H.4  Technical Lemma ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "In this section, we collect several technical lemmas that are useful in the proof. ", "page_idx": 45}, {"type": "text", "text": "Lemma H.1. Consider $\\alpha,\\beta\\,\\in\\,\\mathbb{R}^{4}$ with $\\phi\\;=\\;\\angle(\\pmb{\\alpha},\\beta)\\;\\in\\;[0,\\pi]$ and $\\|\\pmb{\\alpha}\\|_{2}\\;=\\;\\|\\pmb{\\beta}\\|_{2}\\;=\\;1$ and $\\pmb{x}\\sim N(\\mathbf{0},\\pmb{I})$ . Then, for any $0<\\delta_{1},\\delta_{2}\\le\\phi$ we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{x}}[\\|\\mathbf{x}\\|_{2}^{2}\\,\\mathbb{1}_{|\\alpha^{\\top}\\mathbf{x}|\\leq\\delta_{1}\\|\\mathbf{x}\\|_{2},|\\beta^{\\top}\\mathbf{x}|\\leq\\delta_{2}\\|\\mathbf{x}\\|_{2}}]=O(\\delta_{1}\\delta_{2}/\\sin\\phi).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Proof. We first consider the case when at least one of $\\delta_{1},\\delta_{2}\\geq c\\phi$ for a fixed small enough constant. WLOG, suppose $\\delta_{2}\\geq c\\phi$ . In this case, it suffices to show a bound $O(\\delta_{1})$ .Wehave ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x}[\\|x\\|_{2}^{2}\\,\\mathbb{1}_{|\\alpha^{\\top}x|\\leq\\delta_{1}\\|x\\|_{2},|\\beta^{\\top}x|\\leq\\delta_{2}\\|x\\|_{2}}]\\leq\\mathbb{E}_{x}[\\|x\\|_{2}^{2}\\,\\mathbb{1}_{|\\alpha^{\\top}x|\\leq\\delta_{1}\\|x\\|_{2}}]=O(\\delta_{1}).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Then, we focus on the case when $\\delta_{1},\\delta_{2}\\leq c\\phi$ for a fixed small enough constant. WLOG, assume $\\pmb{\\alpha}=(1,0,0,0)^{\\top}$ $\\beta=(\\cos\\phi,\\sin\\phi,0,0)$ and $\\phi\\in[0,\\pi/2]$ . Then we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad{\\mathbb E}_{\\mathbf{z}}[\\|x\\|_{2}^{2}\\|_{{\\boldsymbol{\\alpha}}}\\tau_{1}\\omega_{1}\\|\\omega_{1}]_{2}\\|^{\\mathcal{O}_{\\mathbf{z}}}[\\omega_{1}]_{2}\\|\\omega_{1}\\|_{2}^{2}\\Big]}\\\\ &{=\\displaystyle\\frac{1}{(2\\pi)^{2}}\\int_{0}^{\\infty}r^{5}e^{-r^{2}/2}\\,\\mathrm{d}r}\\\\ &{\\quad\\displaystyle\\int_{0\\leq\\theta_{1}\\leq\\pi,\\,|\\cos\\theta_{1}|\\leq\\beta_{1}}\\sin^{2}\\theta_{1}\\int_{0\\leq\\theta_{2}\\leq\\pi,\\,|\\cos\\theta_{1}\\cos\\phi+\\sin\\theta_{1}\\cos\\theta_{2}\\sin\\phi|\\leq\\delta_{2}}\\sin\\theta_{2}\\,\\mathrm{d}\\theta_{2}\\,\\mathrm{d}\\theta_{1}\\int_{0}^{2\\pi}1\\,\\mathrm{d}\\theta_{3}}\\\\ &{=O(1)\\cdot\\displaystyle\\int_{0\\leq\\theta_{1}\\leq\\pi,\\,|\\cos\\theta_{1}|\\leq\\beta_{1}}\\sin^{2}\\theta_{1}\\int_{0\\leq\\theta_{2}\\leq\\pi,\\,\\frac{-\\delta_{2}-\\cos\\theta_{1}\\cos\\phi}{\\sin\\theta_{1}\\sin\\phi}\\leq\\cos\\theta_{2}\\leq\\frac{\\delta_{2}-\\cos\\theta_{1}\\cos\\phi}{\\sin\\theta_{1}\\sin\\phi}}\\sin\\theta_{2}\\,\\mathrm{d}\\theta_{2}\\,\\mathrm{d}\\theta_{1}}\\\\ &{=\\displaystyle\\int_{0\\leq\\theta_{1}\\leq\\pi,\\,|\\cos\\theta_{1}|\\leq\\beta_{1}}\\sin^{2}\\theta_{1}\\cdot O\\left(\\frac{\\delta_{2}}{\\sin\\theta_{1}\\sin\\phi}\\right)\\,\\mathrm{d}\\theta_{1}}\\\\ &{=O\\left(\\frac{\\delta_{1}\\delta_{2}}{\\sin\\phi}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Lemma H.2 (Lemma C.9 in Zhou et al. (2021)). Consider $\\alpha,\\beta\\,\\in\\,\\mathbb{R}^{3}$ With $\\angle(\\alpha,\\beta)\\,=\\,\\phi$ and $\\alpha^{\\top}\\beta\\geq0$ Wehave ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathbf{x}}[\\left\\|\\mathbf{x}\\right\\|^{2}\\mathbb{1}_{\\mathrm{sign}(\\alpha^{\\top}\\mathbf{x})\\neq\\mathrm{sign}(\\beta^{\\top}\\mathbf{x})}]=O(\\phi).}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Lemma H.3. Consider $\\alpha,\\beta\\in\\mathbb{R}^{d}\\,w i t h\\angle(\\alpha,\\beta)=\\phi,\\,\\|\\alpha\\|_{2}=\\|\\beta\\|_{2}=1$ and $\\alpha^{\\top}\\beta\\geq0$ We have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathbf{x}}[|\\alpha^{\\top}x|\\mathbb{1}_{\\mathrm{sign}({\\pmb\\alpha}^{\\top}{\\pmb x})\\neq\\mathrm{sign}({\\pmb\\beta}^{\\top}{\\pmb x})}]=O(\\phi^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Proof. It suffices to consider $\\alpha,\\beta,x\\in\\mathbb{R}^{2}$ . WLOG, assume $\\pmb{\\alpha}=(1,0)^{\\top}$ and $\\beta=(\\cos\\phi,\\sin\\phi)^{\\top}$ We have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\boldsymbol{x}}\\big[|\\alpha^{\\top}x|\\mathbb{1}_{\\mathrm{sign}(\\alpha^{\\top}x)\\neq\\mathrm{sign}(\\beta^{\\top}x)}\\big]=\\!\\frac{1}{2\\pi}\\int_{0}^{\\infty}r e^{-r^{2}/2}\\,\\mathrm{d}r\\displaystyle\\int_{0}^{2\\pi}\\cos\\theta\\mathbb{1}_{\\mathrm{sign}(\\cos\\theta)\\neq\\mathrm{sign}(\\cos(\\theta-\\phi))}\\,\\mathrm{d}\\theta}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\!O(\\phi^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Lemma H.4. Under Lemma $^{6}$ let ", "page_idx": 46}, {"type": "equation", "text": "$$\nq_{i j}=\\left\\{\\begin{array}{l l}{\\frac{a_{j}a_{i}^{*}}{\\sum_{j\\in{\\cal T}_{i,+}({\\delta_{c l o s e}})}a_{j}^{2}}}&{,i f j\\in{\\cal T}_{i,+}({\\delta_{c l o s e}})}\\\\ {0}&{,o t h e r w i s e}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r}{I f\\sum_{i\\in[m_{*}]}\\left|a_{i}^{2}-\\|\\pmb{w}_{i}\\|_{2}^{2}\\right|\\leq a_{\\operatorname*{min}}/2,\\,t h e n\\sum_{i\\in[m_{*}]}\\sum_{j\\in\\mathcal{T}_{i}}q_{i j}^{2}=O(\\|\\pmb{a}_{*}\\|_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Proof. We have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\sum_{i\\in[m_{*}]}\\sum_{j\\in{\\cal T}_{i}}q_{i j}^{2}=\\sum_{i\\in[m_{*}]}\\sum_{j\\in{\\cal T}_{i,+}(\\delta_{c l o s e})}\\frac{a_{j}^{2}a_{i}^{*2}}{(\\sum_{j\\in{\\cal T}_{i,+}(\\delta_{c l o s e})}a_{j}^{2})^{2}}=\\sum_{i\\in[m_{*}]}\\frac{a_{i}^{*2}}{\\sum_{j\\in{\\cal T}_{i,+}(\\delta_{c l o s e})}a_{j}^{2}}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Inthe folowing, we aim to lower bound $\\begin{array}{r}{\\sum_{j\\in T_{i,+}(\\delta_{c l o s e})}a_{j}^{2}}\\end{array}$ Given $\\begin{array}{r}{\\sum_{j\\in T_{i,+}(\\delta_{c l o s e})}|a_{j}^{2}-\\|\\pmb{w}_{j}\\|_{2}^{2}|\\leq}\\end{array}$ $|a_{i}^{*}|/2$ ,wehave ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\langle\\sum_{j\\in T_{i,+}(\\delta_{c l o s e})}a_{j}^{2}\\geq\\sum_{j\\in T_{i,+}(\\delta_{c l o s e})}a_{j}^{2}+\\|w_{j}\\|_{2}^{2}-|a_{i}^{*}|/2\\geq2\\sum_{j\\in T_{i,+}(\\delta_{c l o s e})}|a_{j}|\\,\\|w_{j}\\|_{2}-|a_{i}^{*}|/2\\geq|a_{i}^{*}|/2\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where the last inequality is due to Lemma F.6: $\\sum_{j\\in T_{i,+}}(\\delta_{c l o s e})\\left|a_{j}\\right|\\left|\\pmb{w}_{j}\\right|\\right|_{2}$ $\\begin{array}{r}{|\\sum_{j\\in\\mathcal{T}_{i}(\\delta_{c l o s e})}a_{j}\\,\\|\\pmb{w}_{j}\\|_{2}\\,|\\geq|a_{i}^{*}|/2}\\end{array}$ . Thus, we have $\\begin{array}{r}{\\sum_{i\\in[m_{*}]}\\sum_{j\\in\\mathcal{T}_{i}}q_{i j}^{2}=O(\\|\\pmb{a}_{*}\\|_{1})}\\end{array}$ \u53e3 ", "page_idx": 46}, {"type": "text", "text": "1 Proofs in Section $\\mathbf{G}$ (non-degenerate dual certificate) ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "In this section, we give the omitted proofs in Section G. The proofs are mostly direct computations with the properties of Hermite polynomials in Claim A.1. ", "page_idx": 47}, {"type": "text", "text": "Lemma G.1 (Non-degeneracy of kernel $K$ ). For any $h>0,$ let $\\ell\\geq\\Theta(\\Delta^{-2}\\log(m_{*}\\ell/h\\Delta))$ , kernel $K_{\\ge\\ell}$ is non-degenerate inthe sense that there exists $r=\\Theta(\\ell^{-1/2}),\\rho_{1}=\\Theta(1),\\rho_{2}=\\Theta(\\ell)$ such that following hold: ", "page_idx": 47}, {"type": "text", "text": "(i) $K(\\pmb{w},\\pmb{u})\\leq1-\\rho_{1}$ for all $\\delta({\\pmb w},{\\pmb u}):=\\angle({\\pmb w},{\\pmb u})\\geq r.$ (i) $K^{(20)}({\\pmb w},{\\pmb u})[z,z]\\leq-\\rho_{2}\\,\\|z\\|^{2}$ for tangent vector $_{\\textit{z}}$ that $z^{\\top}w=0$ and $\\delta({\\pmb w},{\\pmb u})\\leq r$ ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\left\\|K^{(i j)}(w_{1}^{*},w_{k}^{*})\\right\\|_{w_{i}^{*},w_{k}^{*}}\\leq h/m_{*}^{2}f o r\\left(i,j\\right)\\in\\{0,1\\}\\times\\{0,1,2\\}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Proof. With the property of Hermite polynomials in Claim A.1, we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K^{(1)}(w,u)\\:\\frac{1}{2}\\sum_{i,j\\in\\mathcal{E}^{\\prime}}|(\\Delta_{t}u)^{\\theta_{\\theta_{\\theta}}}|^{2}\\:\\frac{1}{|\\alpha^{i}-u^{\\theta_{\\theta}}|^{2}}\\:u^{T}|,}\\\\ &{\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:}\\\\ &{K^{(1)}(w,u)\\:\\frac{1}{2}\\sum_{i,j\\in\\mathcal{E}^{\\prime}}|\\hat{u}^{\\theta_{\\theta}}|^{2}\\:\\mathrm{foross}|^{2}\\:\\frac{1}{|\\alpha^{i}-u^{\\theta_{\\theta}}|^{2}}|\\frac{1}{|\\alpha^{i}-u^{\\theta_{\\theta}}|^{2}}\\:\\mathrm{foross}|^{2}\\:\\mathrm{foross}|^{2}}\\\\ &{\\:\\:\\:\\:\\:\\quad\\quad\\:\\:\\:\\:\\:+\\frac{1}{2}\\sum_{i,j\\in\\mathcal{E}^{\\prime}}|\\hat{u}_{i,j}^{\\theta_{\\theta}}|^{2}\\:\\mathrm{foross}|^{2}\\:\\mathrm{foross}|^{2}\\:\\mathrm{foross}|^{2}\\:\\mathrm{foross}|^{2}}\\\\ &{\\:\\:\\:\\:\\:\\:\\forall\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:\\:}\\\\ &{K^{(2)}(w,u)\\:\\frac{1}{2}\\sum_{i,j\\in\\mathcal{E}^{\\prime}}|\\hat{u}_{i,j}^{\\theta_{\\theta}}|^{2}\\:\\mathrm{foross}|^{2}\\:\\frac{1}{|\\alpha^{i}-u^{\\theta_{\\theta}}|^{2}}|(I-w^{\\theta_{\\theta}})^{2}\\:\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where $\\theta=\\operatorname{arccos}(\\overline{{\\boldsymbol{w}}}^{\\top}\\overline{{\\boldsymbol{u}}})$ ", "page_idx": 47}, {"type": "text", "text": "Part (i  Given that $r=\\Theta(1/\\sqrt{\\ell})$ with a small enough hidden constant, we know for $\\delta({\\pmb w},{\\pmb u})\\geq r$ ", "page_idx": 47}, {"type": "equation", "text": "$$\nK(\\pmb{w},\\pmb{u})=\\frac{1}{Z_{\\sigma}^{2}}\\sum_{k\\geq\\ell}\\hat{\\sigma}_{k}^{2}\\cos^{k}\\theta\\leq\\frac{1}{Z_{\\sigma}^{2}}\\sum_{k\\geq\\ell}\\hat{\\sigma}_{k}^{2}\\cdot(1-r^{2}/5)^{\\ell}=c<1,\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where $c$ is a constant less than 1. Thus, $\\rho_{1}=\\Theta(1)$ ", "page_idx": 47}, {"type": "text", "text": "Part (i)  For tangent vector $_{z}$ that $z^{\\top}w=0$ we have $(\\|\\pmb{w}\\|_{2}=\\|\\pmb{u}\\|_{2}=1$ $\\delta({\\pmb w},{\\pmb u})\\leq r_{\\|}$ ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle K^{(20)}(w,u)[z,z]=\\displaystyle\\frac{1}{Z_{\\sigma}^{2}}\\sum_{k\\geq\\ell}\\hat{\\sigma}_{k}^{2}k(k-1)\\cos^{k-2}\\theta\\cdot(\\overline{{{u}}}^{\\top}z)^{2}-\\displaystyle\\frac{1}{Z_{\\sigma}^{2}}\\sum_{k\\geq\\ell}\\hat{\\sigma}_{k}^{2}k\\cos^{k-1}\\theta\\cdot\\overline{{{w}^{\\top}\\overline{{{u}}}}}\\,\\|z\\|_{2}^{2}}}\\\\ {{\\displaystyle=\\frac{\\|z\\|_{2}^{2}}{Z_{\\sigma}^{2}}\\,\\left(\\sum_{k\\geq\\ell}\\hat{\\sigma}_{k}^{2}k(k-1)\\cos^{k-2}\\theta\\cdot(\\overline{{{u}}}^{\\top}\\overline{{{z}}})^{2}-\\displaystyle\\sum_{k\\geq\\ell}\\hat{\\sigma}_{k}^{2}k\\cos^{k-1}\\theta\\cdot\\overline{{{w}^{\\top}\\overline{{{u}}}}}\\right)}}\\\\ {{\\displaystyle\\ \\ \\ \\leq\\frac{\\|z|_{2}^{2}}{Z_{\\sigma}^{2}}\\,\\left(\\sum_{k\\geq\\ell}\\hat{\\sigma}_{k}^{2}k(k-1)\\cos^{k-2}\\theta\\sin^{2}\\theta-\\displaystyle\\sum_{\\ell\\leq k\\leq2\\ell}\\hat{\\sigma}_{k}^{2}k\\cos^{k}\\theta\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "For the first term, we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{k\\geq\\ell}\\hat{\\sigma}_{k}^{2}k(k-1)\\cos^{k-2}\\theta\\sin^{2}\\theta}\\\\ &{\\leq\\displaystyle\\sum_{k\\geq1/r^{2}}\\hat{\\sigma}_{k}^{2}k(k-1)\\cdot\\Theta(1/k)+\\displaystyle\\sum_{\\ell\\leq k\\leq1/r^{2}}\\Theta(k^{-1/2})r^{2}}\\\\ &{\\leq\\displaystyle\\sum_{k\\geq1/r^{2}}\\Theta(k^{-3/2})+\\Theta(r)=\\Theta(r),}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where we use Lemma I.1 and $\\hat{\\sigma}_{k}^{2}=\\Theta(k^{-5/2})$ in Lemma A.1. ", "page_idx": 48}, {"type": "text", "text": "For the second term, we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\sum_{\\ell\\leq k\\leq2\\ell}\\hat{\\sigma}_{k}^{2}k\\cos^{k}\\theta\\geq\\Theta(\\ell^{-1/2})(1-r^{2})^{2\\ell}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Given that $r=\\Theta(1/\\sqrt{\\ell})$ with a small enough hidden constant, we know ", "page_idx": 48}, {"type": "equation", "text": "$$\nK^{(20)}({\\pmb w},{\\pmb u})[{\\pmb z},{\\pmb z}]\\leq-\\frac{\\|{\\pmb z}\\|_{2}^{2}}{Z_{\\sigma}^{2}}\\Theta(\\ell^{-1/2})=-\\Theta(\\ell)\\,\\|{\\pmb z}\\|_{2}^{2}\\,,\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "since $Z_{\\sigma}^{2}=\\Theta(\\ell^{-3/2})$ ", "page_idx": 48}, {"type": "text", "text": "Part (iil  Recall that $\\delta(\\pmb{w}_{i}^{*},\\pmb{w}_{j}^{*})\\geq\\Delta$ for $i\\neq j$ . It sufices to bound $\\left\\|\\boldsymbol{K}^{(i j)}(\\pmb{w},\\pmb{u})\\right\\|_{2}\\leq h/m_{*}^{2}$ for $\\theta=\\delta({\\pmb w},{\\pmb u})\\geq\\Delta$ Given that $\\bar{\\ell}\\geq\\Theta(\\Delta^{-2}\\log(m_{*}\\ell/h\\Delta))$ with large enough hidden constant, from (15) we have for $\\|\\pmb{w}\\|=\\|\\pmb{u}\\|=1$ ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad K(w,u)\\le\\displaystyle\\frac{1}{Z_{\\sigma}^{2}}\\displaystyle\\sum_{k\\ge\\ell}\\partial_{k}^{2}(1-\\Delta^{2}/5)^{\\ell}\\le h/m_{*}^{2},}\\\\ &{\\left\\|K^{(10)}(w,u)\\right\\|_{w}\\le\\displaystyle\\frac{1}{Z_{\\sigma}^{2}}\\displaystyle\\sum_{k\\ge\\ell}\\partial_{k}^{2}k\\cos^{k-1}\\theta\\sin\\theta\\le\\Theta(\\ell)(1-\\Delta^{2}/5)^{\\ell-1}\\le h/m_{*}^{2},}\\\\ &{\\left|K^{(11)}(w,u)\\right|_{w,u}=\\displaystyle\\frac{1}{Z_{\\sigma}^{2}}\\displaystyle\\sum_{\\le\\,\\frac{1}{\\ell}\\equiv z_{*}^{2}\\,u=z_{*}^{2}\\,u=0,\\,k\\ge\\ell}\\displaystyle\\sum_{k\\ge\\ell}\\partial_{\\ell}^{2}k(k-1)\\cos^{k-2}\\theta\\displaystyle\\overline{{u}}^{\\top}z_{1}\\cdot\\overline{{w}}^{\\top}z_{2}+\\displaystyle\\sum_{k\\ge\\ell}\\partial_{k}^{2}k\\cos^{k-1}\\theta z}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad\\times\\displaystyle\\frac{1}{Z_{\\sigma}^{2}}\\displaystyle\\sum_{k\\ge\\ell}\\partial_{k}^{2}k(k-1)\\cos^{k-2}\\theta\\sin^{2}\\theta+\\displaystyle\\frac{1}{Z_{\\sigma}^{2}}\\displaystyle\\sum_{k\\ge\\ell}\\partial_{k}^{2}k\\cos^{k-1}\\theta}\\\\ &{\\qquad\\qquad\\qquad\\quad\\le\\Theta(\\ell^{3/2})\\displaystyle\\sum_{k\\ge\\ell}\\Theta(k^{-1/2})(1-\\Delta^{2}/5)^{k-2}+\\Theta(\\ell)(1-\\Delta^{2}/5)^{\\ell-1}\\le h/m_{*}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|K^{(20)}(w,u)\\right|\\Big|_{w}=\\!\\frac{1}{Z_{\\sigma}^{2}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "$\\begin{array}{r l}&{\\left\\|K^{(21)}(w,u)\\right\\|_{w,u}}\\\\ &{=\\underset{\\varepsilon_{1}^{\\prime}\\in\\mathbb R_{+}^{2}}{=}\\underset{\\varepsilon_{2}^{\\prime}\\in\\mathbb R_{+}^{2}}{\\operatorname*{sup}}\\underset{\\varepsilon_{3}^{\\prime}\\in\\mathbb R_{+}^{2}}{\\prod_{\\tilde{\\sigma}_{\\varepsilon}}}\\frac{1}{\\varepsilon_{2}^{2}}\\underset{k\\in\\mathbb R^{3}}{\\sum_{\\tilde{\\sigma}_{\\varepsilon}^{\\varepsilon}}}\\mu_{\\varepsilon_{1}^{\\prime}}^{-2}\\underset{\\varepsilon_{2}^{\\prime}}{\\cos^{2}}\\varepsilon_{\\varepsilon_{1}^{\\prime}}^{-3}\\,\\theta_{\\varepsilon_{1}^{\\prime}}^{-1}\\,\\tilde{\\sigma}_{z_{2}}^{-1}\\,\\tilde{\\mu}_{\\varepsilon_{2}}^{-1}}\\\\ &{\\ \\ \\ \\frac{1}{\\sum_{\\tilde{\\sigma}_{\\varepsilon}^{\\prime}\\in\\mathbb R^{3}}}\\biggl|\\mathscr{S}_{i}^{2}\\mathscr{S}_{k}^{4}[k-1)\\cos^{k-2}\\theta\\left(\\sum_{\\tilde{\\sigma}^{\\varepsilon}}\\mathscr{Q}_{i}^{\\varepsilon}\\overline{{\\Psi}}(I-\\overline{{u\\pi}}^{\\tau})\\mathrm{e}_{\\varepsilon}\\cdot\\overline{{\\Psi}}^{\\tau}\\mathscr{z}_{2}+\\sum_{\\tilde{\\sigma}_{\\varepsilon}^{\\prime}}\\mathscr{Q}_{i}^{\\varepsilon}\\overline{{\\Psi}}^{\\tau}(I-\\overline{{u\\pi}}^{\\tau})\\mathrm{e}_{\\varepsilon}\\cdot\\overline{{\\Psi}}^{\\tau}\\right.}\\\\ &{\\left.-\\frac{1}{\\sum_{\\tilde{\\sigma}_{\\varepsilon}^{\\prime}}}\\sum_{\\tilde{\\sigma}_{\\varepsilon}^{\\prime}}^{\\tilde{\\sigma}}\\mathscr{S}_{k}^{2}[k-1)\\cos^{k-2}\\theta\\sum_{\\tilde{\\sigma}}\\varphi_{i}^{\\varepsilon}\\,\\theta_{\\varepsilon}^{\\prime}(I-\\overline{{u\\pi}}^{\\tau})\\overline{{\\Psi}}^{\\prime}\\cdot\\overline{{\\Psi}}^{\\tau}\\overline{{\\Psi}}^{\\tau}\\mathscr{z}}\\\\ &{-\\frac{1}{\\sum_{\\tilde{\\sigma}_{\\varepsilon}^{\\prime}}}\\sum_{\\tilde{\\sigma}_{\\varepsilon}^{\\prime}}^{\\tilde{\\$ 2   \n(a)   \n\u2264h/m\u00b2, ", "page_idx": 49}, {"type": "text", "text": "where we use $\\hat{\\sigma}_{k}^{2}=\\Theta(k^{-5/2})$ in Lemma A.1 and (a) the last two terms bound similarly as in $K^{(20)}$ and frst term $\\begin{array}{r}{\\frac{1^{s^{\\prime}}}{Z_{\\sigma}^{2}}\\sum_{k\\geq\\ell}\\hat{\\sigma}_{k}^{2}k(\\dot{k}-1)(k-2)\\cos^{k-3}\\theta\\sin^{3}\\theta\\leq\\Theta(\\ell^{3/2})\\sum_{k\\geq\\ell}\\Theta(k^{1/2})(\\dot{1}-\\Delta^{2}/5)^{k}\\leq\\ell}\\end{array}$ $h/3m_{*}^{2}$ \u53e3 ", "page_idx": 49}, {"type": "text", "text": "Lemma G.2 (Regularity conditions on kerel $K$ 0. Let $\\begin{array}{r}{B_{i j}\\;:=\\;\\operatorname*{sup}_{w,u}\\big\\|K^{(i j)}(w,u)\\big\\|_{w,u}}\\end{array}$ and $B_{0}=B_{00}+B_{10}+1,$ $B_{2}=B_{20}+B_{21}+1.$ We have $B_{00}=O(1)$ \uff0c $B_{10}=O(\\ell^{1/2})$ $B_{11}=O(\\ell)$ \uff0c $B_{20}=O(\\ell)$ $B_{21}=O(\\ell^{3/2})$ , and therefore $B_{0}=O(\\ell^{1/2})$ \uff0c $B_{2}=O(\\ell^{3/2})$ ", "page_idx": 49}, {"type": "text", "text": "Proof.We compute $B_{i j}$ one by one from (15) (see part (i) proof in Lemma G.1). Using Lemma I.1 wehave ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle B_{00}=\\operatorname*{sup}_{w,u}\\left\\vert\\frac{1}{Z_{\\sigma}^{2}}\\sum_{k\\geq\\ell}\\hat{\\sigma}_{k}^{2}\\cos^{k}\\theta\\right\\vert\\leq1,}}\\\\ {{\\displaystyle B_{10}\\leq\\frac{1}{Z_{\\sigma}^{2}}\\sum_{k\\geq\\ell}\\hat{\\sigma}_{k}^{2}k\\cos^{k-1}\\theta\\sin\\theta\\leq\\Theta(\\ell^{3/2})\\sum_{k\\geq\\ell}\\Theta(k^{-5/2})k\\frac{1}{\\sqrt{k}}=O(\\ell^{1/2}),}}\\\\ {{\\displaystyle B_{11}\\leq\\frac{1}{Z_{\\sigma}^{2}}\\sum_{k\\geq\\ell}\\hat{\\sigma}_{k}^{2}k(k-1)\\cos^{k-2}\\theta\\sin^{2}\\theta+\\frac{1}{Z_{\\sigma}^{2}}\\sum_{k\\geq\\ell}\\hat{\\sigma}_{k}^{2}k\\cos^{k-1}\\theta}}\\\\ {{\\displaystyle\\leq\\Theta(\\ell^{3/2})\\sum_{k\\geq\\ell}\\Theta(k^{-5/2})k^{2}\\frac{1}{k}+\\Theta(\\ell^{3/2})\\sum_{k\\geq\\ell}\\Theta(k^{-5/2})k=O(\\ell),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\frac{1}{320}\\leq\\frac{1}{Z_{\\sigma}^{2}}\\sum_{k\\geq\\ell}\\hat{\\sigma}_{k}^{2}k(k-1)\\cos^{k-2}\\theta\\sin^{2}\\theta+\\frac{1}{Z_{\\sigma}^{2}}\\sum_{k\\geq\\ell}\\hat{\\sigma}_{k}^{2}k\\cos^{k}\\theta}}\\\\ {{\\displaystyle\\qquad\\leq\\Theta(\\ell^{3/2})\\sum_{k\\geq\\ell}\\Theta(k^{-5/2})k^{2}\\frac{1}{k}+\\Theta(\\ell^{3/2})\\sum_{k\\geq\\ell}\\Theta(k^{-5/2})k={\\cal O}(\\ell)},}\\\\ {{\\displaystyle\\frac{3}{21}\\leq\\frac{1}{Z_{\\sigma}^{2}}\\sum_{k\\geq\\ell}\\hat{\\sigma}_{k}^{2}k(k-1)(k-2)\\cos^{k-3}\\theta\\sin^{3}\\theta+\\frac{2}{Z_{\\sigma}^{2}}\\sum_{k\\geq\\ell}\\hat{\\sigma}_{k}^{2}k(k-1)\\cos^{k-1}\\theta\\sin\\theta+\\frac{1}{Z_{\\sigma}^{2}}\\sum_{k\\geq\\ell}\\hat{\\sigma}_{k}^{2}k}}\\\\ {{\\displaystyle\\qquad\\leq\\Theta(\\ell^{3/2})\\sum_{k\\geq\\ell}\\Theta(k^{1/2})(1-\\theta^{2}/5)^{k-3}\\theta^{3}+\\Theta(\\ell^{3/2})\\sum_{k\\geq\\ell}\\Theta(k^{-1/2})(1-\\theta^{2}/5)^{k-1}\\theta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "For first term above $\\begin{array}{r}{\\sum_{k\\geq\\ell}\\Theta(k^{1/2})(1-\\theta^{2}/5)^{k-3}\\theta^{3}}\\end{array}$ , using Lemma I.2 we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{k\\geq\\ell}\\Theta(k^{1/2})(1-\\theta^{2}/5)^{k-3}\\theta^{3}\\leq\\displaystyle\\sum_{k\\geq\\ell}\\Theta(\\frac{1}{\\sqrt{\\ln(1/(1-\\theta^{2}))}})(1-\\theta^{2}/5)^{k/2-3}\\theta^{3}}&{}\\\\ {\\leq\\displaystyle\\sum_{k\\geq\\ell}\\Theta(\\theta^{2})(1-\\theta^{2}/5)^{k/2-3}=\\Theta(\\theta^{2})\\frac{(1-\\theta^{2}/5)^{\\ell}}{\\theta^{2}}=O(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "For second term above $\\begin{array}{r}{\\sum_{k\\geq\\ell}\\Theta(k^{-1/2})(1-\\theta^{2}/5)^{k-1}\\theta}\\end{array}$ we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\sum_{\\infty}\\Theta(k^{-1/2})(1-\\theta^{2}/5)^{k-1}\\theta\\le\\Theta(\\theta)\\int_{\\ell}^{\\infty}x^{-1/2}(1-\\theta^{2}/5)^{x}\\le\\Theta(\\theta)\\Theta(\\frac{1}{\\sqrt{\\ln(1/(1-\\theta^{2}))}})=O(1)\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Therefore, we have $B_{21}=O(\\ell^{3/2})$ ", "page_idx": 50}, {"type": "text", "text": "1.1  Technical lemma ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "We collect few lemma here used in the proof. They mostly rely on direct calculations. ", "page_idx": 50}, {"type": "text", "text": "Lemma I.1. For large enough integer $k$ wehave ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{max}|\\cos^{k}\\theta\\sin\\theta|\\leq\\Theta(1/\\sqrt{k}),}\\\\ &{\\,\\,\\operatorname*{max}|\\cos^{k}\\theta\\sin^{2}\\theta|\\leq\\Theta(1/k),}\\\\ &{\\operatorname*{max}|\\cos^{k}\\theta\\sin^{3}\\theta|=\\Theta(1/k^{3/2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Proof. We only compute the first one max $\\mid\\cos^{k}\\theta\\sin\\theta\\mid={1}/{\\sqrt{k}}$ . Others are similar. ", "page_idx": 50}, {"type": "text", "text": "We compute the gradient of $f(\\theta)=\\cos^{k}\\theta\\sin\\theta$ and get $f^{\\prime}(\\theta)=\\cos^{k-1}\\theta(\\cos^{2}\\theta-k\\sin^{2}\\theta)$ We only need to consider $\\theta\\,\\in\\,[0,2\\pi]$ . So the maximum is achieved either at boundary $\\theta\\,=\\,0,\\pi$ or $f^{\\prime}({\\bar{\\boldsymbol{\\theta}}})=0$ . Then one can verify that the bound is true. \u53e3 ", "page_idx": 50}, {"type": "text", "text": "LemmaI.2.For $\\beta<1$ andl $k>0$ wehave $\\begin{array}{r}{k^{1/2}\\beta^{k/2}\\le\\frac{1}{\\sqrt{2\\ln(2/\\beta)}}.}\\end{array}$ ", "page_idx": 50}, {"type": "text", "text": "Proof. Let $f(k)=k^{1/2}\\beta^{k/2}$ . We have $f^{\\prime}(k)=\\textstyle{\\frac{1}{2}}k^{-1/2}\\beta^{k/2}+k^{1/2}\\beta^{k/2}\\ln(\\beta/2)$ . Set $f^{\\prime}(k_{0})=0$ we have $\\begin{array}{r}{k_{0}=\\frac{1}{2\\ln(2/\\beta)}}\\end{array}$ Itis asy to see max $\\begin{array}{r}{f(k)=f(k_{0})\\le\\frac{1}{\\sqrt{2\\ln(2/\\beta)}}}\\end{array}$ \u53e3 ", "page_idx": 50}, {"type": "text", "text": "J  Notes on Sample Complexity ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "The current paper focuses on the analysis on population loss, which is already highly non-trivial and requires new ideas that we developed in the paper. The finite-sample analysis is not our focus, so we Omit it in the current paper. ", "page_idx": 50}, {"type": "text", "text": "For sample complexity, we believe the following strategy would work to get a polynomial sample complexity. We can break down the analysis into 2 parts: early-stage feature learning (Stage 1 and 2) and final-stage feature learning (Stage 3). ", "page_idx": 50}, {"type": "text", "text": "\u00b7 Stage 1 and 2: This should follow the results in Damian et al. (2022). The most important step is to show the concentration of first-step gradient (Stage 1). As shown in Damian et al. (2022), using concentration tools we can get sample complexity $n=\\Theta_{\\ast}(d^{2})$ , where $n$ is the number of sample and $d$ is input dimension.   \n\u00b7 Stage 3: In local convergence regime, all weights have norms bounded in $O_{*}(1)$ due to $\\ell_{2}$ regularization we have. Thus, we can apply standard concentration tools to show the empirical gradients are close to population gradients given a large enough polynomial number of samples. ", "page_idx": 51}, {"type": "text", "text": "Achieving a tight sample complexity is an interesting and challenging open problem that is beyond the scope of current work. ", "page_idx": 51}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 52}, {"type": "text", "text": "Justification: The main contribution is our Theorem 2, which matches the claims in abstract and introduction. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 52}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Justification: This is a theory paper so all assumptions are clearly listed and discussed. The limitations, for example Stage 2 in Algorithm 1, are clearly discussed in the paper. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should refect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 52}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: All assumptions are clearly listed in the main text and full proofs are given in the appendix. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 53}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 53}, {"type": "text", "text": "Justification: No experiments in the paper. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 53}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 54}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 54}, {"type": "text", "text": "Justification: No experiments in the paper. Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips . cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so ^No\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 54}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 54}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 54}, {"type": "text", "text": "Justification: No experiments in the paper. Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 54}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 54}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 54}, {"type": "text", "text": "Justification: No experiments in the paper. Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 54}, {"type": "text", "text": "", "page_idx": 55}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 55}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 55}, {"type": "text", "text": "Justification: No experiments in the paper. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 55}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: The paper follows the NeurIPs Code of Ethics. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 55}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 55}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 55}, {"type": "text", "text": "Justification: This is a theory paper and has no foresee direct societal impacts. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 55}, {"type": "text", "text": "", "page_idx": 56}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 56}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 56}, {"type": "text", "text": "Justification: This is a theory paper. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 56}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 56}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 56}, {"type": "text", "text": "Justification: Not applicable. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 56}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 57}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA]   \nJustification: Not applicable. Guidelines:   \n\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 57}, {"type": "text", "text": "", "page_idx": 57}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] Justification: Not applicable. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 57}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] Justification: Not applicable. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 57}]