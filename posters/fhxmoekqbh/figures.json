[{"figure_path": "fHxmoekQBh/figures/figures_1_1.jpg", "caption": "Figure 1: We compared the performance of the classic single-image task trained MLLM LLaVA1.5 [22] and our model in three multi-image scenarios including Multi Image Reasoning, Knowledge Based VQA and Visual Relation Inference. LLaVA1.5 exhibits significant limitations in multi-image scenarios.", "description": "This figure showcases a comparison between the performance of a classic single-image trained multi-modal large language model (MLLM) called LLaVA1.5 and the authors' proposed model, MaVEn, across three different multi-image scenarios: Multi-Image Reasoning, Knowledge-Based VQA, and Visual Relation Inference.  The results visually demonstrate that LLaVA1.5 struggles significantly with multi-image tasks, highlighting the limitations of single-image trained models on such scenarios and the improved performance of MaVEn in these complex, multi-image situations.", "section": "1 Introduction"}, {"figure_path": "fHxmoekQBh/figures/figures_3_1.jpg", "caption": "Figure 2: Subfigure (a) illustrates the structural schematic of our proposed Multi-Granularity Hybrid Encoding, while subfigure (b) demonstrates the mechanism for the reduction of continuous visual tokens under the guidance of discrete visual information.", "description": "This figure shows the architecture of MaVEn, a multi-granularity hybrid visual encoding framework.  (a) illustrates the hybrid encoding process combining discrete and continuous visual representations.  Images are encoded using both a Vision Transformer (ViT) for continuous features and a discrete visual encoder (SEED) for discrete symbols. These are then aligned to a unified vocabulary before being input to the large language model. (b) details the dynamic reduction of continuous visual features. A patch selector, guided by text semantics, identifies and retains only the most relevant patches, reducing computational overhead and improving efficiency.", "section": "3 Method"}, {"figure_path": "fHxmoekQBh/figures/figures_4_1.jpg", "caption": "Figure 3: The diagram illustrates the training schematic for MaVEn. We divide the training of MaVEn into four stages, where the snowflake icon indicates that the model parameters are frozen during training, and the flame icon indicates that the model parameters are updated during training.", "description": "This figure illustrates the four-stage training process of the MaVEn model.  Each stage focuses on training specific components while freezing others. Stage 1 trains the patch selector using Grounding SAM and image-text data. Stage 2 fine-tunes the LLM embedding layer to adapt to a unified multimodal vocabulary. Stage 3 trains the visual projector. Stage 4 performs instruction fine-tuning on the entire model.", "section": "3.3 Training Paradigm of MaVEn"}, {"figure_path": "fHxmoekQBh/figures/figures_7_1.jpg", "caption": "Figure 4: Evaluation Results of MaVEn on different benchmarks with varying Keeping Ratios.", "description": "This figure shows the performance of MaVEn on different benchmarks (SEED-Bench, DEMONBench, VQA, and MMBench) while varying the \"Keeping Ratio\" parameter. The Keeping Ratio controls the number of continuous visual tokens retained after a reduction mechanism.  The graph displays how the model's performance on various benchmarks changes as this ratio changes.  Different colors represent different benchmarks, and the x-axis represents the Keeping Ratio, while the y-axis represents the performance metric (e.g., accuracy).  It demonstrates the impact of the dynamic reduction mechanism on the efficiency and accuracy of MaVEn.", "section": "4.3 Ablation Study"}, {"figure_path": "fHxmoekQBh/figures/figures_8_1.jpg", "caption": "Figure 5: This figure visualizes the distribution of discrete tokens in an image containing index 4568 discrete tokens, along with the relevant score computed based on the Patch Selector and the patches chosen according to the relevant score that are most semantically related to the discrete visual tokens.", "description": "This figure shows the distribution of discrete tokens (visual vocabulary) in three example images.  For each image, it displays the original image, the patches selected by the Patch Selector based on their semantic relevance to the discrete tokens, the relevance scores, and finally the distribution of those discrete tokens across the image's patches.  This illustrates the model's ability to select semantically relevant patches based on the discrete token vocabulary, highlighting the multi-granularity approach of MaVEn.", "section": "4.4 Qualitative Analysis"}, {"figure_path": "fHxmoekQBh/figures/figures_8_2.jpg", "caption": "Figure 6: Visualization of the attention maps with and without the visual discrete tokens. We demonstrate the attention maps for the 31-st layers, where the range of visual tokens is indicated by orange and the range of text tokens is indicated by blue.", "description": "This figure compares attention maps from the 31st layer of the LLM when using both discrete and continuous visual tokens (top) versus only continuous visual tokens (bottom).  The visualizations show how attention weights are distributed across image tokens (Image1 and Image2) and text tokens. The top image shows that with multi-granularity hybrid visual encoding, the model attends to both discrete and continuous visual tokens when answering multi-image questions. In contrast, the bottom image demonstrates that when using only continuous visual tokens, the model primarily focuses its attention on text tokens and largely ignores visual information.", "section": "4.4 Qualitative Analysis"}]