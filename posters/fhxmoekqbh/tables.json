[{"figure_path": "fHxmoekQBh/tables/tables_5_1.jpg", "caption": "Table 1: Average results of zero-shot evaluation on each task of DEMON Benchmark [17].", "description": "This table presents the performance of different multimodal large language models (MLLMs) on seven different tasks within the DEMON benchmark.  The benchmark focuses on multi-image visual understanding and reasoning,  covering scenarios such as Visual Relation Inference, Multi-Modal Dialogue, Knowledge Grounded QA, and Multi-Image Reasoning. The results are reported as average scores for each model across the seven tasks, providing a comparative analysis of their capabilities in complex multi-image scenarios.", "section": "4.2 Main Results"}, {"figure_path": "fHxmoekQBh/tables/tables_5_2.jpg", "caption": "Table 2: Average results of zero-shot evaluation on each task category of SEED Benchmark [17].", "description": "This table presents a comparison of different multimodal large language models (MLLMs) on the SEED benchmark, focusing on zero-shot evaluation.  The benchmark assesses video understanding capabilities across various tasks. The table lists each model's vision encoder, language model, average score across all tasks, average image understanding score, and average video understanding score.  It shows MaVEn outperforms other models, particularly in video understanding.", "section": "4 Experiments"}, {"figure_path": "fHxmoekQBh/tables/tables_6_1.jpg", "caption": "Table 3: Performance comparison on visual question answering and zero-shot multi-modal benchmarks. For VQA, accuracy is reported. Note that specialists are fine-tuned on each individual dataset. \u2020 denotes OCR inputs are utilized. \u2021 indicates the model has trained on the dataset.", "description": "This table compares the performance of MaVEn against other state-of-the-art models on several visual question answering (VQA) benchmarks and multi-modal benchmarks.  It shows the accuracy scores achieved by each model on various tasks, including VQAv2, GQA, VizWizQA, TextVQA, SciQA, MME, MMBench, and MM-Vet.  The table also indicates whether OCR inputs were used or if the model was trained on the specific dataset.", "section": "4.2 Main Results"}, {"figure_path": "fHxmoekQBh/tables/tables_7_1.jpg", "caption": "Table 4: Ablation evaluation on multi-modal benchmarks We evaluated the performance of various ablation targets on both multi-image (SEED-Bench, DEMONBench ) and single-image (VQA, MMBench) benchmarks.", "description": "This table presents the ablation study results, comparing the performance of MaVEn with different configurations of visual encoding (discrete only, continuous only, and both). The performance is evaluated on four benchmarks: SEED-Bench and DEMONBench for multi-image scenarios, and VQA and MMBench for single-image scenarios.  It shows the impact of using discrete and/or continuous visual features on model performance.", "section": "4.3 Ablation Study"}]