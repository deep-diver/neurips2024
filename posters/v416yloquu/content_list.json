[{"type": "text", "text": "Adam with model exponential moving average is effective for nonconvex optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kwangjun Ahn Microsoft Research Cambridge, MA kwangjunahn@microsoft.com ", "page_idx": 0}, {"type": "text", "text": "Ashok Cutkosky   \nBoston University   \nBoston, MA   \nashok@cutkosky.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this work, we offer a theoretical analysis of two modern optimization techniques for training large and complex models: (i) adaptive optimization algorithms, such as Adam, and (ii) the model exponential moving average (EMA). Specifically, we demonstrate that a clipped version of Adam with model EMA achieves the optimal convergence rates in various nonconvex optimization settings, both smooth and nonsmooth. Moreover, when the scale varies significantly across different coordinates, we demonstrate that the coordinate-wise adaptivity of Adam is provably advantageous. Notably, unlike previous analyses of Adam, our analysis crucially relies on its core elements\u2014momentum and discounting factors\u2014as well as model EMA, motivating their wide applications in practice. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In neural network training, the training loss $F:\\mathbb{R}^{d}\\,\\rightarrow\\,\\mathbb{R}$ is often optimized using an iterative optimization algorithm which starts with the initial iterate $\\mathbf{x}_{\\mathrm{0}}$ and then updates during each iteration $t=1,2,\\ldots$ as follows: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t}=\\mathbf{x}_{t-1}+\\mathbf{z}_{t}\\,,\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $\\mathbf{z}_{t}$ denotes the increment chosen by the algorithm during the $t$ -th iteration. One of the most popular optimization algorithms is Adam [Kingma and Ba, 2014]. Adam has gained significant attention due to its effectiveness in training Transformer-based language models [Zhang et al., 2020a, Kunstner et al., 2023, Jiang et al., 2023, Pan and Li, 2023, Ahn et al., 2024a, Kunstner et al., 2024, Zhang et al., 2024a]. ", "page_idx": 0}, {"type": "text", "text": "The model exponential moving average (EMA) [Polyak and Juditsky, 1992, Ruppert, 1988] is an optimization technique that has gained popularity in conjunction with Adam for various recent applications. EMA maintains an exponential moving average of the model iterates, $\\mathbf{x}_{t}$ , which contributes to the stabilization of these iterates. There has been a resurgence of interest in this technique due to its effectiveness in training high-quality generative models [Yaz et al., 2018, Karras et al., 2019, Song et al., 2021b, Dhariwal and Nichol, 2021, Nichol and Dhariwal, 2021, Song et al., 2021a, Balaji et al., 2022, Karras et al., 2022, Rombach et al., 2022, Kang et al., 2023, Karras et al., 2023]. Moreover, a recent work by Block et al. [2024] demonstrates the effectiveness of EMA for both language modeling and imitation learning applications. ", "page_idx": 0}, {"type": "text", "text": "In this work, we theoretically study the effectiveness of these two modern optimization techniques.   \nOur main results can be informally summarized as follows. ", "page_idx": 0}, {"type": "text", "text": "Our main results are based on the online-to-nonconvex conversion framework of Cutkosky et al. [2023], which chooses the increment $\\mathbf{z}_{t}$ based on an online learner of choice. In particular, our approach is quite different than the previous analyses of Adam (see Section 1.1 below). Notably, our analysis relies on the key components of Adam (momentum and adaptive learning rate) as well as EMA of the iterates, offering new, theoretical insight into their success. See Section 7 for a more detailed discussion. ", "page_idx": 1}, {"type": "text", "text": "At a high level, our analysis combines the main insights from the two recent works: Zhang and Cutkosky [2024] and Ahn et al. [2024b]. We first carefully modify the discounted-to-nonconvex conversion framework (Lemma 7) of Zhang and Cutkosky [2024] which converts an online learner that achieves a good discounted regret (Definition 6) into a good noncovex optimizer. We then combine it with the main insight of Ahn et al. [2024b] that an effective discounted online learner can be designed based on scale-free Follow-the-Regularized-Leader (FTRL) [Orabona and P\u00e1l, 2018]. In particular, the way we arrive at Adam is similar to Ahn et al. [2024b]: choosing a discounted version of FTRL in the discounted-to-nonconvex conversion leads to Adam. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Even though Adam is widely used in deep learning, our theoretical understanding of its inner workings, especially the importance of its core components\u2014momentum and discounting factors\u2014remains incomplete, as pointed out by Ahn et al. [2024b]. Most theoretical work on Adam and its variations focus on characterizing the convergence rate for convex or smooth nonconvex functions, where methods like SGD already achieve the minimax optimal convergence rate. [Reddi et al., 2018, Zhou et al., 2019, Chen et al., 2019, Zou et al., 2019, Alacaoglu et al., 2020, Guo et al., 2021, D\u00e9fossez et al., 2022, Zhang et al., 2022, Li et al., 2023, Wang et al., 2023]. In fact, even the most recent results [Li et al., 2023, Wang et al., 2023] are not reflective of practice, in the sense that Adam\u2019s convergence rate worsens with momentum [Wang et al., 2023, \u00a76] or is no better than that of SGD [Li et al., 2023, \u00a77]. A notable exception is Crawshaw et al. [2022], which demonstrates the advantages of momentum under the generalized smoothness conditions of Zhang et al. [2020a]. However, the algorithm they analyze is signSGD, which differs significantly from the original Adam. In contrast, as we will explore in the subsequent sections, momentum and discounting factors are important in our analysis. See Section 7 for more details. ", "page_idx": 1}, {"type": "text", "text": "We also highlight that our analysis relies on model EMA, a technique widely used in practice as mentioned above (also see a recent work by Block et al. [2024]). It is worth noting that EMA (or model averaging in general) has shown to have generalization benefits in practice [Tarvainen and Valpola, 2017, Izmailov et al., 2018]. In this paper, we study EMA from an optimization perspective, and show that the use of EMA leads to optimal guarantees for nonconvex optimization. Interestingly, EMA naturally derives from the discounted-to-online conversion (see Algorithm 1), which, we believe, provides new theoretical insights into this practical method. ", "page_idx": 1}, {"type": "text", "text": "The use of EMA also represents a significant departure from most non-convex optimization analyses. While EMA is a classical technique in the convex setting, theoretical analyses in the non-convex setting typically randomly select an iterate as the \u201cfinal output\u201d of the optimizer, rather than using EMA. This random selection is intuitively extremely impractical (indeed, on average it actually wastes half of the computation), and is never performed in real implementations. ", "page_idx": 1}, {"type": "text", "text": "Our analysis follows a line of work studying convergence guarantees for non-smooth non-convex optimization. Our particular convergence criterion is similar to finding the Goldstein stationary points [Goldstein, 1977] that were first studied in the context of modern machine learning by [Zhang et al., 2020b], and has seen much subsequent interest [Tian and So, 2022, Jordan et al., 2023, Davis et al., 2020]. Other notions of convergence are also reasonable\u2014common alternatives involve the Moreau envelope, or imposing a weak convexity condition [Davis et al., 2018, 2022a]. ", "page_idx": 1}, {"type": "text", "text": "2 Setting for nonconvex and nonsmooth optimization ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Throughout this paper, unless specified otherwise, $\\lVert\\cdot\\rVert$ denotes the $L_{2}$ norm. Following [Cutkosky et al., 2023], we consider optimizing a loss function $F$ that satisfies the following conditions, accessing information about $F$ through a stochastic gradient oracle STOGRAD : $\\mathbb{R}^{d}\\times\\bar{\\mathcal{Z}}\\rightarrow\\mathbb{R}^{d}$ , for the set of randomness $\\mathcal{Z}$ . ", "page_idx": 1}, {"type": "text", "text": "Assumption 2. Let $F:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ be a differentiable function with the following properties: ", "page_idx": 2}, {"type": "text", "text": "\u2022 Let $\\Delta:=F(\\mathbf{x}_{0})-\\operatorname*{inf}_{\\mathbf{x}}F(\\mathbf{x})$ . ", "page_idx": 2}, {"type": "equation", "text": "$\\begin{array}{r}{F(\\mathbf{y})-F(\\mathbf{x})=\\int_{0}^{1}\\left\\langle\\nabla F(\\mathbf{x}+t(\\mathbf{y}-\\mathbf{x})),\\mathbf{y}-\\mathbf{x}\\right\\rangle\\mathrm{d}t.}\\end{array}$ ", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "\u2022 Lipschitzness. $F$ is $G$ -Lipshitz, i.e., for any point $\\mathbf{x}_{:}$ , $\\|\\nabla F(\\mathbf{x})\\|\\leq G$ . ", "page_idx": 2}, {"type": "text", "text": "\u2022 Stochastic gradient variance. For any point $\\mathbf{x}$ , the stochastic gradient $\\mathbf{g}\\gets\\operatorname{SToGRAD}(\\mathbf{x},r)$ for randomness $r\\in\\mathcal{Z}$ satisfies $\\begin{array}{r}{\\mathbb{E}[\\mathbf{g}]=\\nabla F(\\mathbf{x})\\,a n d\\,\\mathbb{E}\\left\\|\\mathbf{g}-\\nabla F(\\mathbf{x})\\right\\|^{2}\\leq\\sigma^{2},}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "The Lipschitz continuity condition is a standard assumption in nonconvex nonsmooth settings. However, as we will discuss in Section 6, one of the key insights from our results is that Adam enables us to adapt to the Lipschitz constants coordinate-wise without requiring prior knowledge of these constants. Note that we almost certainly need some form of structural assumption on the \u201cdifficulty\u201d of the loss function; thus, relaxing the Lipschitz assumption would likely come at the cost of another assumption, such as smoothness. ", "page_idx": 2}, {"type": "text", "text": "The second condition, called well-behavedess in [Cutkosky et al., 2023, Definition 1], is a mild regularity condition. For any locally Lipschitz function $F$ , applying an arbitrarily small perturbation to the function is sufficient to ensure this condition [Cutkosky et al., 2023, Proposition 2]. ", "page_idx": 2}, {"type": "text", "text": "For the notion of optimality, we follow Zhang and Cutkosky [2024] and consider the following notion of stationarity for nonconvex and nonsmooth functions. This notion is a slight relaxation of the notion of a Goldstein stationarity point [Goldstein, 1977], which was further studied by recent works [Zhang et al., 2020a, Davis et al., 2022b, Tian et al., 2022, Jordan et al., 2023]. ", "page_idx": 2}, {"type": "text", "text": "Definition 3 ( $\\left(\\lambda,\\varepsilon\\right)$ -stationary point). Suppose $F:\\mathbb{R}^{d}\\,\\rightarrow\\,\\mathbb{R}$ is differentiable. We say x is a $(\\lambda,\\varepsilon)$ -stationary point of $F$ if $\\|\\nabla F(\\mathbf{x})\\|^{[\\lambda]}\\leq\\varepsilon,$ , where ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla F({\\mathbf x})\\|^{[\\lambda]}:=\\underset{p\\in{\\mathcal P}(\\mathbb{R}^{d}),}{\\operatorname*{inf}}\\left\\lbrace\\|\\mathbb{E}[\\nabla F({\\mathbf y})]\\|+\\lambda\\cdot\\mathbb{E}\\left\\|{\\mathbf y}-{\\mathbf x}\\right\\|^{2}\\right\\rbrace\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "To further motivate this definition, we remark that $(\\lambda,\\epsilon)$ -stationary points retain the desirable properties of Goldstein stationary points. Specifically, the following result [Zhang and Cutkosky, 2024, Lemma 2.3] demonstrates that, akin to Goldstein stationary points, $(\\lambda,\\epsilon)$ -stationary points can be reduced to first-order stationary points with appropriate choices of $\\lambda$ when the objective function is smooth or second-order smooth. ", "page_idx": 2}, {"type": "text", "text": "Lemma 4. If $F$ is $L$ -smooth, then an $(L^{2}\\varepsilon^{-1},\\varepsilon)$ -stationary point $\\mathbf{x}$ of $F$ satisfies $\\|\\nabla F(\\mathbf{x})\\|\\leq$ $2\\varepsilon$ . Moreover, if $F$ is $H$ -second-order-smooth, then an $(H/2,\\varepsilon)$ -stationary point $\\mathbf{x}$ of $F$ satisfies $\\|\\nabla F(\\mathbf{x})\\|\\leq2\\varepsilon$ . ", "page_idx": 2}, {"type": "text", "text": "Moreover, as shown by [Zhang and Cutkosky, 2024, Lemma 2.4], $(\\lambda,\\varepsilon)$ -stationary points can also be reduced to Goldstein stationary points when $F$ is Lipschitz. ", "page_idx": 2}, {"type": "text", "text": "Lemma 5. Suppose $F$ is $G$ -Lipschitz. For any $\\lambda,\\varepsilon,\\delta\\,>\\,0,$ , a $(\\lambda,\\varepsilon)$ -stationary point is a $(\\delta,\\varepsilon^{\\prime})$ - Goldstein stationary point, where $\\begin{array}{r}{\\varepsilon^{\\prime}=(1+\\frac{2G}{\\lambda\\delta^{2}})\\cdot\\varepsilon}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "Now we design algorithms that find $(\\lambda,\\varepsilon)$ -stationary points efficiently. ", "page_idx": 2}, {"type": "text", "text": "3 Discounted-to-nonconvex conversion: online learning of increments ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our main results are built on the online-to-nonconvex conversion framework of Cutkosky et al. [2023]. At its core, this framework involves selecting the increment $\\mathbf{z}_{t}$ using an online learner, as discussed by Ahn et al. [2024b]. Specifically, we follow a variant developed by Zhang and Cutkosky [2024], which carefully incorporates the discounting factor in the conversion process. Note that we make slight modifications to the version proposed by Zhang and Cutkosky [2024] as follows. Here Exp(1) denotes the exponential random variable with mean 1. ", "page_idx": 2}, {"type": "text", "text": "Given Algorithm 1, it turns out we need to design an online learner that minimizes the discounted regret, formally defined below. It is worth noting that discounted regret has been recently studied with the goal of better adapting online learners to dynamic environments [Ahn et al., 2024b, Zhang et al., 2024b, Jacobsen and Cutkosky, 2024]. ", "page_idx": 2}, {"type": "text", "text": "Input: Initial point $\\mathbf{x}_{\\mathrm{0}}$ , $T\\in\\mathbb N$ , online learning algorithm $\\boldsymbol{\\mathcal{A}}$ , and discounting factor $\\beta\\in(0,1)$   \nfor $t=1,2\\ldots,T$ do Receive $\\mathbf{z}_{t}$ from ${A}\\,/{}/{$ choose the increment using an online learner Update $\\mathbf{x}_{t}\\gets\\mathbf{x}_{t-1}+\\alpha_{t}\\mathbf{z}_{t}$ , where $\\alpha_{t}{\\sim}\\mathrm{Exp}(1)$ i.i.d. Compute $\\mathbf{g}_{t}\\gets\\operatorname{SToGRAD}(\\mathbf{x}_{t},r_{t})$ with freshly sampled randomness $r_{t}$ Send $\\ell_{t}^{[\\beta]}(\\mathbf{z}):=\\langle\\beta^{-t}\\mathbf{g}_{t},\\mathbf{z}\\rangle$ to $\\boldsymbol{\\mathcal{A}}$ // Maintain exponential moving average (for output only): Update $\\begin{array}{r}{\\overline{{\\mathbf{x}}}_{t}\\gets\\frac{\\beta-\\beta^{t}}{1-\\beta^{t}}\\overline{{\\mathbf{x}}}_{t-1}+\\frac{1-\\beta}{1-\\beta^{t}}\\mathbf{x}_{t}}\\end{array}$ (Equivalently, $\\begin{array}{r}{\\overline{{\\mathbf{x}}}_{t}\\xleftarrow{}\\frac{1-\\beta}{1-\\beta^{t}}\\sum_{s=1}^{t}\\beta^{t-s}\\mathbf{x}_{s})}\\end{array}$   \nend for ", "page_idx": 3}, {"type": "text", "text": "Definition 6 (Discounted regret). For a comparator u, the $\\beta$ -discounted regret is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{Regret}_{t}^{[\\beta]}(\\mathbf{u}):=\\beta^{t}\\cdot\\sum_{s=1}^{t}(\\ell_{s}^{[\\beta]}(\\mathbf{z}_{s})-\\ell_{s}^{[\\beta]}(\\mathbf{u}))=\\sum_{s=1}^{t}\\beta^{t-s}\\left\\langle\\mathbf{g}_{s},\\mathbf{z}_{s}-\\mathbf{u}\\right\\rangle\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The discounted regret of an online learner $\\boldsymbol{\\mathcal{A}}$ can be used to upper bound the norm of averaged gradients, as shown in the following result. ", "page_idx": 3}, {"type": "text", "text": "Lemma 7 (Discounted-to-nonconvex conversion). Suppose that $F$ satisfies Assumption 2. Then for the comparator sequence chosen as $\\begin{array}{r}{\\mathbf{u}_{t}:=-D\\frac{\\sum_{s=1}^{t}\\beta^{-s}\\nabla F(\\mathbf{x}_{s})}{\\left\\|\\sum_{s=1}^{t}\\beta^{-s}\\nabla F(\\mathbf{x}_{s})\\right\\|}}\\end{array}$ , Algorithm 1 gives $\\begin{array}{r l}{\\underset{t\\sim[T]}{\\mathbb{E}}\\mathbb{E}\\left\\|\\underset{\\mathbf{y}_{t}}{\\mathbb{E}}\\nabla F(\\mathbf{y}_{t})\\right\\|\\leq\\frac{\\displaystyle\\Delta}{\\displaystyle{D T}}+\\frac{2G+\\sigma}{(1-\\beta)T}+\\sigma\\sqrt{1-\\beta}}&{}\\\\ {+\\,\\frac{\\displaystyle1}{\\displaystyle{D T}}\\left[\\beta\\cdot\\mathbb{E}\\left[\\mathsf{R e g r e t}_{T}^{[\\beta]}(\\mathbf{u}_{T})\\right]+(1-\\beta)\\cdot\\underset{t=1}{\\overset{T}{\\sum}}\\mathbb{E}\\left[\\mathsf{R e g r e t}_{t}^{[\\beta]}(\\mathbf{u}_{t})\\right]\\right]\\,,}\\end{array}$ where $\\mathbf{y}_{t}$ is distributed over $\\begin{array}{r}{\\{\\mathbf{x}_{s}\\}_{s=1}^{t}\\,a s\\,\\mathbb{P}(\\mathbf{y}_{t}=\\mathbf{x}_{s})=\\beta^{t-s}\\cdot\\frac{1-\\beta}{1-\\beta^{t}}\\,f o r\\,s=1,2,\\dotsc,t.}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "The proof combines the techniques of [Cutkosky et al., 2023, Theorem 7] and [Zhang and Cutkosky, 2024, Theorem 3.3]. See Appendix A for details. ", "page_idx": 3}, {"type": "text", "text": "We briefly explain how Lemma 7 can be used to find a $(\\lambda,\\epsilon)$ -stationary point (Definition 3). Recall that $(\\lambda,\\epsilon)$ -stationarity essentially requires producing a point $\\mathbf{x}=\\mathbb{E}[\\mathbf{y}]$ such that both $\\|\\mathbb{E}[\\nabla F(\\mathbf{y})]\\|$ and $\\dot{\\mathbb{E}}\\|\\mathbf{y}-\\mathbf{x}\\|^{2}$ are small. ", "page_idx": 3}, {"type": "text", "text": "Given this context, Lemma 7 states that as long as the discounted regret of the online learner $\\boldsymbol{\\mathcal{A}}$ is small, we can ensure that the EMA iterates $\\overline{{\\bf x}}_{t}=\\mathbb{E}[{\\bf y}_{t}]$ serve as good candidates for $(\\lambda,\\epsilon)$ -stationarity, since the term $\\mathbb{E}\\left\\|\\mathbb{E}_{\\mathbf{y}_{t}}\\nabla F(\\mathbf{y}_{t})\\right\\|$ can be kept small. The remaining task is to bound the variance term, $\\mathbb{E}\\left\\|\\mathbf{y}_{t}-\\overline{{\\mathbf{x}}}_{t}\\right\\|^{2}$ , which will be addressed later in Lemma 10. ", "page_idx": 3}, {"type": "text", "text": "Moreover, the comparator $\\mathbf{u}_{t}$ roughly models the update direction that an oracle algorithm with perfect knowledge of the loss would select. In the proof of Lemma 7, we demonstrate that moving along the $\\mathbf{u}_{t}$ direction effectively decreases the loss value, which forms the basis for establishing our convergence guarantee. ", "page_idx": 3}, {"type": "text", "text": "Thanks to the discounted-to-nonconvex conversion, the task now reduces to designing an online learner that achieves low discounted regret. ", "page_idx": 3}, {"type": "text", "text": "4 Scale-free Follow-the-Regularized-Leader (FTRL) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we introduce an algorithmic component, called the Followed-The-Regularized-Leader (FTRL), a powerful online learning technique with various applications [Gordon, 1999, Kalai and Vempala, 2005, Shalev-Shwartz and Singer, 2006, Abernethy et al., 2008, Nesterov, 2009, Hazan and Kale, 2010]. ", "page_idx": 3}, {"type": "text", "text": "For the setting, consider the online linear optimization (OLO) setting, where during each round $t=1,\\dots,T$ , and online learner chooses $\\mathbf{z}_{t}$ , and then the linear loss $\\ell_{t}(\\bar{\\cdot})=\\langle\\mathbf{v}_{t},\\cdot\\rangle$ is revealed by the environment. Here the goal of the online learner is to minimize the regret defined as $\\sum_{t}\\left\\langle\\mathbf{v}_{t},\\mathbf{z}_{t}-\\mathbf{u}\\right\\rangle$ , where $\\mathbf{u}$ is the comparator in hindsight. For this setting, FTRL is presented in Algorithm 2. ", "page_idx": 4}, {"type": "table", "img_path": "v416YLOQuU/tmp/80a82457306e33012c5a151f35303fe7adf752e066da6d60afc54d6938a6f48f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "The key insight of Ahn et al. [2024b] and Zhang et al. [2024b] is that in order to design an online learner for discounted regret, it is important that the online learner is scale-free as described below. In particular, following Ahn et al. [2024b], we consider a gradient adaptive scale-free FTRL algorithm called scale-free FTRL [Orabona and P\u00e1l, 2018]. ", "page_idx": 4}, {"type": "text", "text": "We will focus on the case where $\\mathcal{D}=\\mathbb{B}_{D}$ , the $d$ -dimensional $L_{2}$ -ball of radius $D>0$ . Scale-free FTRL is given by Algorithm 2 with the following choie: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\psi_{t}(\\cdot)=\\frac{1}{\\eta_{t}}\\left\\|\\cdot\\right\\|^{2}\\quad\\mathrm{and}\\quad\\eta_{t}=\\frac{D}{\\sqrt{\\sum_{s=1}^{t-1}\\left\\|\\mathbf{v}_{s}\\right\\|^{2}}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then using the clipping operator $\\mathrm{clip}_{D}(\\mathbf x):=\\mathbf x\\operatorname*{min}({\\cal D}/\\|\\mathbf x\\|,1)$ , we can write down the update rule more explicitly as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{z}_{t}\\leftarrow\\operatorname{argmin}_{\\mathbf{z}\\in\\mathbb{B}_{D}}\\left[\\frac{1}{\\eta_{t}}\\left\\|\\mathbf{z}\\right\\|^{2}+\\sum_{s=1}^{t-1}\\left\\langle\\mathbf{v}_{s},\\mathbf{z}\\right\\rangle\\right]=-\\mathrm{clip}_{D}\\left(D\\frac{\\sum_{s=1}^{t-1}\\mathbf{v}_{s}}{\\sqrt{\\sum_{s=1}^{t-1}\\left\\|\\mathbf{v}_{s}\\right\\|^{2}}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, if the denominator is zero, i.e., $\\mathbf{v}_{1}=\\cdots=\\mathbf{v}_{t-1}=\\mathbf{0}$ , then we set $\\mathbf z_{t}\\gets0$ . Note that this algorithm is scale-free in the sense that when the loss sequence is scaled by a scalar $c>0$ , the updates remain the same. ", "page_idx": 4}, {"type": "text", "text": "Let us now present the regret bound of SCALE-FREE FTRL ", "page_idx": 4}, {"type": "text", "text": "Lemma 8 (Gradient-adaptive regret bound). For any $T>0$ , loss sequence $\\mathbf{v}_{1:T}$ and comparator $\\mathbf{u}\\in\\mathbb{R}^{d}$ s.t. $\\|\\mathbf{u}\\|\\leq D$ , SCALE-FREE FTRL guarantees the following regret bound: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\left\\langle\\mathbf{v}_{t},\\mathbf{z}_{t}-\\mathbf{u}\\right\\rangle\\leq4D\\sqrt{\\sum_{t=1}^{T}\\left\\Vert\\mathbf{v}_{t}\\right\\Vert^{2}}\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We note that Lemma 8 follows (with a slightly worse constant) from [Orabona and P\u00e1l, 2018, Theorem 1], and the version we invoke is here due to [Ahn et al., 2024b, Theorem A.1]. ", "page_idx": 4}, {"type": "text", "text": "Recall from Lemma 7 that an online learner for the discounted-to-nonconvex conversion (Algorithm 1) needs to have a low discounted regret. To achieve this, following Ahn et al. [2024b] and Zhang et al. [2024b], we simply substitute $\\mathbf{v}_{t}\\gets\\beta^{-t}\\mathbf{g}_{t}$ into SCALE-FREE FTRL, resulting in the update ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{z}_{t}\\gets-\\mathrm{clip}_{D}\\left(D\\frac{\\sum_{s=1}^{t-1}\\beta^{-s}\\mathbf{g}_{s}}{\\sqrt{\\sum_{s=1}^{t-1}\\beta^{-2s}\\left\\Vert\\mathbf{g}_{s}\\right\\Vert^{2}}}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here again, if the denominator is zero, i.e., $\\mathbf{g}_{1}=\\cdots=\\mathbf{g}_{t-1}=\\mathbf{0}$ , then we set $\\mathbf z_{t}\\gets0$ . Then, the following result characterizes the discounted regret guarantee of $\\beta$ -FTRL. ", "page_idx": 4}, {"type": "text", "text": "Theorem 9 (Discounted regret bound). Let $\\beta\\in(0,1]$ . For any $T>0$ , loss sequence $\\mathbf{g}_{1:T}$ and comparator $\\mathbf{u}\\in\\mathbb{R}^{d}$ s.t. $\\|\\mathbf{u}\\|\\leq D$ , $\\beta$ -FTRL guarantees the following static regret bound ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{Regret}_{T}^{[\\beta]}(\\mathbf{u})\\leq4D\\sqrt{\\sum_{t=1}^{T}\\beta^{2(T-t)}\\left\\lVert\\mathbf{g}_{t}\\right\\rVert^{2}}\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We next use this result to design an algorithm for nonconvex optimization. ", "page_idx": 4}, {"type": "text", "text": "5 Discounted-FTRL leads to adaptive nonconvex optimization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, as a warm-up, let us see the implications of choosing $\\mathcal{A}=\\beta$ -FTRL in Algorithm 1. First, let us obtain a bound on the expected discounted regret. By Theorem 9 together with Jensen\u2019s inequality, we have the following regret bound for any $t=1,2,\\ldots,T$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[{\\mathrm{Regret}}_{t}^{[\\beta]}(\\mathbf{u}_{t})\\right]\\leq4D\\,\\mathbb{E}\\,\\sqrt{\\sum_{t=1}^{T}\\beta^{2(T-t)}\\left\\Vert\\mathbf{g}_{t}\\right\\Vert^{2}}\\leq4D\\sqrt{\\sum_{t=1}^{T}\\beta^{2(T-t)}\\,\\mathbb{E}\\left\\Vert\\mathbf{g}_{t}\\right\\Vert^{2}}\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Since $\\mathbb{E}\\left\\|\\mathbf{g}_{t}\\right\\|^{2}\\leq G^{2}+\\sigma^{2}$ and $\\begin{array}{r}{\\frac{1}{\\sqrt{1-\\beta^{2}}}\\leq\\frac{1}{\\sqrt{1-\\beta}}}\\end{array}$ 2 \u2264 \u221a11\u2212\u03b2 , it follows that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[{\\mathrm{Regret}}_{t}^{[\\beta]}(\\mathbf{u}_{t})\\right]\\leq{\\frac{4D(G+\\sigma)}{\\sqrt{1-\\beta^{2}}}}\\leq{\\frac{4D(G+\\sigma)}{\\sqrt{1-\\beta}}}\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "5.1 From gradient-adaptive regret to nonconvex optimization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In order to obtain nonconvex optimization guarantees in terms of the $(\\lambda,\\varepsilon)$ -stationarity (Definition 3), we need to handle the variance term. Following [Zhang and Cutkosky, 2024, Lemma 3.2], the variance term can be bounded as follows. ", "page_idx": 5}, {"type": "text", "text": "Lemma 10 (Variance bound). Using the notations of Lemma 7, for any $t=1,2,\\dots,T,$ $\\beta$ -FTRL satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname{\\mathbb{E}}_{t\\sim[T]}\\mathbb{E}\\left\\|\\mathbf{y}_{t}-\\overline{{\\mathbf{x}}}_{t}\\right\\|^{2}\\leq12\\frac{D^{2}}{(1-\\beta)^{2}}\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proof. From [Zhang and Cutkosky, 2024, Lemma 3.2], it follows that $\\begin{array}{r l}{\\mathbb{E}\\sum_{t=1}^{T}\\left\\|\\mathbf{y}_{t}-\\overline{{\\mathbf{x}}}_{t}\\right\\|^{2}\\ \\leq}\\end{array}$ $\\begin{array}{r}{\\frac{12}{(1-\\beta)^{2}}\\mathop{\\mathbb{E}}\\sum_{t=1}^{T}\\left\\|\\mathbf{z}_{t}\\right\\|^{2}}\\end{array}$ . Now since $\\|\\mathbf{z}_{t}\\|\\,\\leq\\,D$ for all $t\\,=\\,1,2,\\ldots,T$ , after dividing each side by $\\scriptstyle{T}$ , we get the desired inequality. \u53e3 ", "page_idx": 5}, {"type": "text", "text": "Plugging the regret bound (1) into Lemma 7 and combining it with Lemma 10, we arrive at the following optimization guarantee in terms of the $(\\lambda,\\varepsilon)$ -stationarity. See Section B.1 for a proof. ", "page_idx": 5}, {"type": "text", "text": "Theorem 11. Suppose that $F$ satisfies Assumption 2 and consider any $\\lambda>0$ . For $C>0$ , choose $\\mathcal{A}=\\beta$ -FTRL in Algorithm $^{\\,l}$ with the following parameters: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\beta=1-\\left(\\frac{\\varepsilon}{10C}\\right)^{2},\\,\\,\\,D=\\frac{(1-\\beta)\\varepsilon^{1/2}}{4\\lambda^{1/2}},\\,\\,\\,a n d\\,\\,\\,T=\\frac{1}{1-\\beta}\\cdot\\operatorname*{max}\\left\\{\\frac{4\\Delta\\lambda^{1/2}}{\\varepsilon^{3/2}},\\,\\,\\frac{12C}{\\varepsilon}\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then we have $\\begin{array}{r}{\\mathbb{E}_{t\\sim[T]}\\,\\mathbb{E}\\,\\|\\nabla F(\\overline{{\\mathbf{x}}}_{t})\\|^{[\\lambda]}\\leq(1+\\frac{G+\\sigma}{C})\\varepsilon}\\end{array}$ . In other words, a randomly chosen model EMA $\\overline{{\\mathbf{x}}}_{t}$ is a $\\textstyle(\\lambda,(1+\\frac{G+\\sigma}{C})\\varepsilon)$ -stationary point, in expectation. ", "page_idx": 5}, {"type": "text", "text": "5.2 Optimality and gradient adaptivity ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Here, we discuss several notable aspects of the guarantee provided in Theorem 11. ", "page_idx": 5}, {"type": "text", "text": "5.2.1 Optimality ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As shown in [Zhang and Cutkosky, 2024, Corollary 5.1], the lower bound on the iteration complexity for finding a $(\\lambda,\\varepsilon)$ -stationary point is $\\Omega((G+\\sigma)^{2}\\Delta\\lambda^{1/2}\\varepsilon^{-7/2})$ , provided that $\\begin{array}{r}{\\lambda\\le\\frac{G^{4}}{\\Delta^{2}}\\varepsilon^{-1}}\\end{array}$ . Theorem 11 implies that setting $C=G+\\sigma$ achieves this optimal iteration complexity. ", "page_idx": 5}, {"type": "text", "text": "Corollary 12. In Theorem $_{l l}$ , choosing $C=G+\\sigma$ leads to the following iteration complexity for finding a $(\\lambda,\\varepsilon)$ -stationary point: ", "page_idx": 5}, {"type": "equation", "text": "$$\nO\\left(\\operatorname*{max}\\left\\{{\\frac{(G+\\sigma)^{2}\\Delta\\lambda^{1/2}}{\\varepsilon^{7/2}}},\\,{\\frac{(G+\\sigma)^{3}}{\\varepsilon^{3}}}\\right\\}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In particular, treating $G,\\,\\sigma_{;}$ , and $\\Delta$ as constants, as long as $\\lambda\\gtrsim\\varepsilon,$ , this leads to the optimal complexity of $O((G+\\sigma)^{2}\\Delta\\lambda^{1/2}\\varepsilon^{-7/2})$ . ", "page_idx": 5}, {"type": "text", "text": "In light of Lemma 4, the above optimal complexity can be converted into the optimal complexities for smooth settings. ", "page_idx": 6}, {"type": "text", "text": "Corollary 13 (Smooth settings). Corollary 12 implies the following optimal iteration complexity for smooth settings. Choosing $\\lambda=O(\\varepsilon^{-1})$ , it implies the optimal complexity of $O(\\varepsilon^{-4})$ for smooth loss functions [Arjevani et al., 2023]. Similarly, with $\\lambda={\\cal O}(1)$ , it achieves the optimal iteration complexitiy of $O(\\varepsilon^{-7/2})$ for second-order smooth loss functions [Arjevani et al., 2020]. ", "page_idx": 6}, {"type": "text", "text": "We next discuss the beneftis of using the gradient-adaptive regret bound (Theorem 9) by considering the case where we do not have knowledge of $G,\\sigma$ . ", "page_idx": 6}, {"type": "text", "text": "5.2.2 Gradient adaptivity ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "A remarkable consequence of Theorem 11 is that, due to the gradient-adaptive regret bound of Theorem 9, the final convergence guarantee has a better dependence on $G,\\sigma$ in the case when we do not have knowledge of them. For concreteness, in the following discussion, we treat $G,\\sigma,\\Delta$ as constants, and focus on the regime $\\lambda\\gtrsim\\varepsilon$ . ", "page_idx": 6}, {"type": "text", "text": "First, our Theorem 11 with $C=1$ (since we do not know $G,\\sigma)$ leads to the following iteration complexity for finding a $(\\lambda,\\varepsilon)$ -stationary point: ", "page_idx": 6}, {"type": "equation", "text": "$$\nO\\left(\\left(G+\\sigma\\right)^{7/2}\\Delta\\lambda^{1/2}\\varepsilon^{-7/2}\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The price we pay for not knowing $G,\\sigma$ relative to the lower bound is a multiplicative factor of $(G+\\sigma)^{3/2}$ . To see the benefit of this adaptive regret approach, let us consider the guarantees given by Zhang and Cutkosky [2024]. Their approach is based on choosing online gradient descent for $\\boldsymbol{\\mathcal{A}}$ in Algorithm 1, when the learning rate is not properly tuned with the knowledge of $G$ and $\\sigma$ , it would lead to the following (suboptimal) discounted regret bound: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[{\\mathrm{Regret}}_{t}^{[\\beta]}(\\mathbf{u}_{t})\\right]\\leq O\\left({\\frac{D(G+\\sigma)^{2}}{\\sqrt{1-\\beta}}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then, the resulting iteration complexity becomes $\\begin{array}{r}{O(\\Delta\\lambda^{1/2}(\\frac{\\varepsilon}{(G+\\sigma)^{2}})^{-7/2})}\\end{array}$ , which is equal to $O\\left((G+\\sigma)^{7}\\Delta\\lambda^{1/2}\\varepsilon^{-7/2}\\right)$ . This is larger than the complexity due to our adaptive approach by a multiplicative factor of $(G+\\sigma)^{7/2}$ . ", "page_idx": 6}, {"type": "text", "text": "Next, we build on the results from this section and consider a better approach to design an adaptive nonconvex optimizer. ", "page_idx": 6}, {"type": "text", "text": "6 Coordinate-wise adaptivity via (clipped-)Adam ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we consider the setting where the Lipschitzness constants vary across different coordinates, which is empirically observed to be reflective of practical neural network training (see, e.g. [Crawshaw et al., 2022, Zhuang et al., 2022]). Formally, we consider the following setting. ", "page_idx": 6}, {"type": "text", "text": "Assumption 14. Under the same setting as Assumption 2, we replace the last two conditions with the following coordinate-wise version: ", "page_idx": 6}, {"type": "text", "text": "\u2022 For each coordinate $i\\,=\\,1,2,\\ldots,d,$ , there is a Lipschitzness constant $G_{i}\\,>\\,0$ and a variance constant $\\sigma_{i}>0$ such that $\\forall\\mathbf{x},$ , $|\\partial_{i}F(\\mathbf{x})|\\le G_{i}$ and the stochastic gradient $\\mathbf{g}\\gets\\operatorname{STOGRAD}(\\mathbf{x},r)$ satisfies $\\mathbb{E}[\\mathbf{g}[i]]=\\partial_{i}F(\\mathbf{x}_{i})$ and $\\mathbb{E}\\left|\\mathbf{g}[i]-\\partial_{i}F(\\mathbf{x})\\right|^{2}\\leq\\sigma_{i}^{2}$ . (Here, $\\partial_{i}F$ denotes the partial derivative of $F$ w.r.t. the i-th coordinate.) ", "page_idx": 6}, {"type": "text", "text": "Let $G:=(G_{1},\\ldots,G_{d})$ and $\\pmb{\\sigma}:=\\,(\\sigma_{1},\\ldots,\\sigma_{d})$ . Then, the above condition implies the last two conditions in Assumption 2 with $G=\\|G\\|_{2}$ and $\\sigma=\\lVert{\\pmb{\\sigma}}\\rVert_{2}$ . ", "page_idx": 6}, {"type": "text", "text": "As we mentioned before, the previous approaches [Cutkosky et al., 2023, Zhang and Cutkosky, 2024] choose the online learner $\\boldsymbol{\\mathcal{A}}$ to be online gradient descent, and hence choosing the learning rate requires the knowledge of $G_{i},\\sigma_{i}$ for all $i$ . However, for neural network training, $d$ is equal to the number of parameters in the network, so tuning them individually is computationally infeasible. We instead consider running $\\beta$ -FTRL coordinate-wise in Algorithm 1, which will automatically adapt to each coordinate. We begin with an important observation that such an approach in fact leads to a popular optimizer widely used in practice. ", "page_idx": 6}, {"type": "text", "text": "6.1 Coordinate-wise discounted FTRL corresponds to (clipped-)Adam ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "For notational simplicity, fix a coordinate among $i=1,2,\\ldots,d.$ , and let us denote the iterate by $x_{t}$ , the stochastic gradient by $g_{t}$ , and the update by $z_{t}$ . Then the resulting optimizer becomes: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\boxed{z_{t+1}=-\\mathrm{clip}_{D}\\left(D\\frac{\\sum_{s=1}^{t}\\beta_{1}^{t-s}g_{s}}{\\sqrt{\\sum_{s=1}^{t}\\beta_{2}^{t-s}g_{s}^{2}}}\\right)\\;,}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\beta_{1}\\,=\\,\\beta$ and $\\beta_{2}\\,=\\,\\beta^{2}$ . Here, again if the denominator is zero, i.e., if $g_{1}\\,=\\,\\cdot\\,\\cdot\\,=\\,g_{t}\\,=\\,0$ , then we set the update to be zero, i.e., $z_{t+1}\\,=\\,0$ . Note that this is almost exactly the Adam optimizer [Kingma and Ba, 2014], except that now we add clipping to control the variance of the iterates relative to their EMA. Notably, CLIPPED-ADAM retains one of the most important properties of Adam: it is scale-invariant. The scale invariance causes the optimizer to make updates of the same magnitude on each coordinate even when the scale differs across different coordinates. ", "page_idx": 7}, {"type": "text", "text": "In practice, we expect that the clipping operation will effectively be a no-op. This is because, when the algorithm is converging (even if the convergence is somewhat slow), the gradients are likely to behave as approximately mean-zero random variables (due to factors such as stochastic noise, unstable training trajectories, etc.). In such cases, standard concentration inequalities imply that ", "page_idx": 7}, {"type": "text", "text": "We also remark that CLIPPED-ADAM does not consider the \u201cbias correction\u201d terms in the original updates of Adam [Kingma and Ba, 2014]. However, note that the bias correction terms are coordinateindependent, and they can be merged into the scalar $D$ . ", "page_idx": 7}, {"type": "text", "text": "6.2 Nonconvex optimization guarantees of CLIPPED-ADAM ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We next discuss the theoretical guarantees of CLIPPED-ADAM for nonconvex and nonsmooth optimizaton. Inspired by [Duchi et al., 2010, McMahan and Streeter, 2010], where the coordinate-wise online learners lead to regret bounds with respect to the $L_{1}$ norms of stochastic gradients, we consider the following variant of Definition 3, in the same vein as [Cutkosky et al., 2023, Section 4]. ", "page_idx": 7}, {"type": "text", "text": "Definition 15 $(\\lambda,\\varepsilon){\\bf-}L_{1}$ -stationary point). Suppose $F:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ is differentiable. We say x is a $(\\lambda,\\varepsilon)$ - $L_{1}$ -stationary point of $F\\;i f\\,\\|\\nabla F(\\mathbf{x})\\|_{1}^{[\\lambda]}\\leq\\varepsilon$ , where ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla F({\\mathbf x})\\|_{1}^{[\\lambda]}:=\\operatorname*{inf}_{p\\in{\\mathcal P}(\\mathbb{R}^{d}),\\atop\\mathbb{E}_{\\mathbf{y}\\sim p}[{\\mathbf y}]={\\mathbf x}}\\left\\{||\\mathbb{E}[\\nabla F({\\mathbf y})]||_{1}+\\lambda\\cdot\\mathbb{E}\\,||{\\mathbf y}-{\\mathbf x}||_{2}^{2}\\right\\}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Using the fact $\\left\\|\\cdot\\right\\|_{1}\\leq\\sqrt{d}\\left\\|\\cdot\\right\\|_{2}$ , one can connect the two notions of $(\\lambda,\\varepsilon)$ -stationary points. ", "page_idx": 7}, {"type": "text", "text": "Lemma 16. A $(\\lambda/\\sqrt{d},\\varepsilon/\\sqrt{d})$ -stationary point is a $(\\lambda,\\varepsilon)$ - $L_{1}$ -stationary point. ", "page_idx": 7}, {"type": "text", "text": "In order to obtain the guarantee in terms of $L_{1}$ -norm, we consider the coordinate-wise version of discounted-to-online conversion, in the same vein as [Cutkosky et al., 2023, Appendix G]. See Section A.1 for details. ", "page_idx": 7}, {"type": "text", "text": "Lemma 17 $\\mathrm{~}L_{1}$ -variant of Lemma 7). Suppose that $F$ satisfies Assumption 14. Consider the comparator sequence chosen as $\\mathbf{u}_{t}$ defined as $\\begin{array}{r}{\\mathbf u_{t}[i]:=-D\\frac{\\sum_{s=1}^{t}\\beta^{-s}\\partial_{i}F(\\mathbf x_{s})}{\\left|\\sum_{s=1}^{t}\\beta^{-s}\\partial_{i}F(\\mathbf x_{s})\\right|}}\\end{array}$ tts=1 \u03b2\u03b2\u2212\u2212ss\u2202\u2202iFF  ((xxs))| for i = 1, 2, . . . , d. Then, Algorithm 1 gives ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{t\\sim\\left[T\\right]}{\\mathbb{E}}\\mathbb{E}\\left\\|\\underset{\\mathbf{y}_{t}}{\\mathbb{E}}\\nabla F(\\mathbf{y}_{t})\\right\\|_{1}\\leq\\frac{\\Delta}{D T}+\\frac{2\\left\\Vert G+\\sigma\\right\\Vert_{1}}{(1-\\beta)T}+\\left\\Vert\\sigma\\right\\Vert_{1}\\sqrt{1-\\beta}}&{}\\\\ {+\\left.\\frac{1}{D T}\\left[\\beta\\cdot\\mathbb{E}\\left[\\mathrm{Regre}_{T}^{\\left[\\beta\\right]}(\\mathbf{u}_{T})\\right]+(1-\\beta)\\cdot\\underset{t=1}{\\overset{T}{\\sum}}\\mathbb{E}\\left[\\mathrm{Regre}_{t}^{\\left[\\beta\\right]}(\\mathbf{u}_{t})\\right]\\right]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\mathbf{y}_{t}$ is distributed over $\\begin{array}{r}{\\{\\mathbf{x}_{s}\\}_{s=1}^{t}\\,a s\\,\\mathbb{P}(\\mathbf{y}_{t}=\\mathbf{x}_{s})=\\beta^{t-s}\\cdot\\frac{1-\\beta}{1-\\beta^{t}}.}\\end{array}$ for $s=1,2,\\ldots,t$ . ", "page_idx": 7}, {"type": "text", "text": "Next, let us consider the (expected) regret bound. Fix a coordinate $i\\,=\\,1,\\ldots,d$ . Then, by the one-dimensional version of Theorem 9 together with Jensen\u2019s inequality, we have the following regret bound for any $t=1,2,\\ldots,T$ : ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[{\\mathrm{Regret}}_{t}^{[\\beta]}(\\mathbf{u}_{t}[i])\\right]\\leq4D\\sqrt{\\sum_{t=1}^{T}\\beta^{2(T-t)}\\mathbb{E}\\left|\\mathbf{g}_{t}[i]\\right|^{2}}\\leq\\frac{4D(G_{i}+\\sigma_{i})}{\\sqrt{1-\\beta}}\\,.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Hence, taking the sum over all coordinates $i=1,\\ldots,d$ , we obtain ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathrm{Regret}_{t}^{[\\beta]}(\\mathbf{u}_{t})\\right]\\leq\\frac{4D\\left\\lVert G+\\pmb{\\sigma}\\right\\rVert_{1}}{\\sqrt{1-\\beta}}\\,.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Combining these together, one get the following guarantee in terms of the $L_{1}$ norm. See Section B.2 for a proof. ", "page_idx": 8}, {"type": "text", "text": "Theorem 18. Suppose that $F$ satisfies Assumption 14 and consider any $\\lambda>0$ . For $C>0$ , choose the coordinate-wise optimizer CLIPPED-ADAM in Algorithm $^{\\,l}$ with the following parameters: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\beta=1-\\left(\\frac{\\varepsilon}{10C}\\right)^{2},\\,\\,\\,D=\\frac{(1-\\beta)\\varepsilon^{1/2}}{4d^{1/2}\\lambda^{1/2}},\\,\\,\\,a n d\\,\\,\\,T=\\frac{1}{1-\\beta}\\cdot\\operatorname*{max}\\left\\{\\frac{4\\Delta d^{1/2}\\lambda^{1/2}}{\\varepsilon^{3/2}},\\,\\,\\frac{12C}{\\varepsilon}\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "TEhMenA  wxet  ihsa av e $\\begin{array}{r}{\\mathbb{E}_{t\\sim[T]}\\mathbb{E}\\left\\|\\nabla F(\\overline{{\\mathbf{x}}}_{t})\\right\\|_{1}^{[\\lambda]}\\leq(1+\\frac{\\|G+\\pmb{\\sigma}\\|_{1}}{C})\\varepsilon}\\end{array}$ .n  Ienx potehcteart iwoonr.ds, a randomly chosen model $\\begin{array}{r}{(\\lambda,(1+\\frac{\\|G+\\pmb{\\sigma}\\|_{1}}{C})\\varepsilon)-1}\\end{array}$ $L_{1}$ ", "page_idx": 8}, {"type": "text", "text": "6.3 Benefits of coordinate-wise adaptivity of CLIPPED-ADAM ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we discuss the beneftis of coordinate-wise adaptivity by examining the guarantee from Theorem 18 and compare it with that of Theorem 11. We begin with the $(\\lambda,\\varepsilon)\\!-\\!L_{1}$ -stationary point guarantee due to Theorem 18. We consider the scenario where $\\beta$ is carefully tuned by making the optimal choice of $C$ . ", "page_idx": 8}, {"type": "text", "text": "Corollary 19. In Theorem $I\\&$ , choosing $C=\\left\\|G+\\pmb{\\sigma}\\right\\|_{1}$ leads to the following iteration complexity for finding a $(\\lambda,\\varepsilon)\\!-\\!L_{1}$ -stationary point: ", "page_idx": 8}, {"type": "equation", "text": "$$\nO\\left(\\operatorname*{max}\\left\\{\\frac{\\left\\Vert G+\\pmb{\\sigma}\\right\\Vert_{1}^{2}\\Delta d^{1/2}\\lambda^{1/2}}{\\varepsilon^{7/2}},\\,\\frac{\\left\\Vert G+\\pmb{\\sigma}\\right\\Vert_{1}^{3}}{\\varepsilon^{3}}\\right\\}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "In order to better appreciate the benefits of coordinate-wise adaptivity, let us compare the above iteration complexity with that of Theorem 11. ", "page_idx": 8}, {"type": "text", "text": "For concreteness, we treat $G=\\|G\\|_{2}$ and $\\sigma=\\lVert{\\pmb{\\sigma}}\\rVert_{2}$ as constants throughout, and more importantly, we assume that the coordinates are heterogeneous in the sense that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\lVert G+\\pmb{\\sigma}\\rVert_{1}\\approx\\lVert G+\\pmb{\\sigma}\\rVert_{2}~.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The assumption (3) roughly says that a few coordinates of $G+\\sigma$ take much large\u221ar values than the rest; if all the coordinates of $G+\\sigma$ have similar magnitudes, then $\\left\\|G+\\pmb{\\sigma}\\right\\|_{1}\\approx\\sqrt{d}\\left\\|\\pmb{G}+\\pmb{\\sigma}\\right\\|_{2}$ . In the case $\\lambda\\gtrsim\\varepsilon$ , Corollary 19 implies that the iteration complexity is ", "page_idx": 8}, {"type": "equation", "text": "$$\nO(\\|\\pmb{G}+\\pmb{\\sigma}\\|_{1}^{2}\\,\\Delta d^{1/2}\\lambda^{1/2}\\varepsilon^{-7/2})\\,.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Next, let us consider the counterpart that does not adapt to each coordinate separately. In this case, we appl\u221ay Lem\u221ama 16, which tells us that to find a $(\\lambda,\\varepsilon)\\!-\\!L_{1}$ stationary point it suffices to find a $(\\lambda/\\sqrt{d},\\varepsilon/\\sqrt{d})$ -statio\u221anary point. Then, from Corollary 12, the iteration complexity is $O(\\|\\pmb{G}+\\pmb{\\sigma}\\|_{2}^{2}\\,\\Delta(\\lambda/\\sqrt{d})^{1/2}(\\varepsilon/\\sqrt{d})^{-7/2})$ , i.e., ", "page_idx": 8}, {"type": "equation", "text": "$$\nO(\\|\\pmb{G}+\\pmb{\\sigma}\\|_{2}^{2}\\,\\Delta d^{3/2}\\lambda^{1/2}\\varepsilon^{-7/2})\\,.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Hence, when (3) holds, (4) can be lower than (5) by a multiplicative factor of $d$ , showing the beneftis of coordinate-wise adaptivity. ", "page_idx": 8}, {"type": "text", "text": "7 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our analyses of Adam based on the discounted-to-online conversion is quite different than the previous ones. As discussed in Section 1.1, the previous analyses often result in guarantees that are not quite reflective of practice\u2014e.g., the rates get better without momentum and the rates are no better than that of non-adaptive methods. In contrast, our analyses and results highlight the role of the practical components as highlighted below. ", "page_idx": 9}, {"type": "text", "text": "\u2022 Momentum. In order to obtain a low discounted regret, any sensible online learner should integrate the past history of stochastic gradients $\\mathbf{g}_{1:t}$ to make the decision $\\mathbf{z}_{t+1}$ . Such online learners under the discounted-to-online conversion lead to momentum methods that integrate $\\mathbf{g}_{1:t}$ to obtain the next increment $\\mathbf{z}_{t+1}$ . In particular, non-momentum methods would correspond to aggressive online learners that only use the last gradient ${\\bf g}_{t}$ to make the decision $\\mathbf{z}_{t+1}$ . This perspective provides new insights into understanding the role of momentum, as echoed by Ahn et al. [2024b]. ", "page_idx": 9}, {"type": "text", "text": "\u2022 Adaptive learning rates. The adaptive learning rate due to $\\beta$ -FTRL leads to a gradient-adaptive regret bound (Theorem 9), which is important to obtain a better Lipshitzness dependence (Section 5.2) as well as the coordinate-wise adaptivity for high-dimension settings (Section 6.3). Our analysis offers theoretical benefits of adaptive learning rate from a discounted regret perspective. ", "page_idx": 9}, {"type": "text", "text": "\u2022 Model EMA. Lastly, the discounted-to-nonconvex conversion (Algorithm 1) naturally leads to guarantees in terms of the model EMA, $\\overline{{\\mathbf{x}}}_{t}$ . At a high level (see Appendix A precise details), this is because for a dynamic environment, it is important to discount the losses such that online learners adapt to changing environments. The appearance of model EMA in the discounted-to-nonconvex conversion provides a new perspective on its role. ", "page_idx": 9}, {"type": "text", "text": "Our analyses and results have several limitations and raise several interesting questions. Firstly, CLIPPED-ADAM does not precisely match the original Adam algorithm, warranting further investigation into the original Adam update. Specifically, our analysis suggests choosing $\\beta_{1}=\\beta$ and $\\beta_{2}=\\bar{\\beta}^{2}$ , which does not align with the commonly used practical choices. Understanding the exact roles of these practical choices for $\\beta_{1}$ and $\\beta_{2}$ would be valuable. ", "page_idx": 9}, {"type": "text", "text": "In Section 5.2, we observed that our iteration complexity for finding a $(\\lambda,\\varepsilon)$ -stationary point is $O(\\Delta(G+\\sigma)^{7/2}\\lambda^{1/2}\\varepsilon^{-7/2})$ when $G$ and $\\sigma$ are unknown. Investigating whether this complexity is optimal presents another intriguing direction for future research. ", "page_idx": 9}, {"type": "text", "text": "Lastly, from a practical standpoint, developing a more advanced online learner for discounted regret and designing an algorithm that surpasses Adam in practicality would have significant practical implications. ", "page_idx": 9}, {"type": "text", "text": "Funding Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "AC is supported by NSF grant number CCF-2211718. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Jacob D. Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efficient algorithm for bandit linear optimization. In Conference on Learning Theory (COLT), pages 263\u2013274. Omnipress, 2008. ", "page_idx": 9}, {"type": "text", "text": "Kwangjun Ahn, Xiang Cheng, Minhak Song, Chulhee Yun, Ali Jadbabaie, and Suvrit Sra. Linear attention is (maybe) all you need (to understand transformer optimization). In The Twelfth International Conference on Learning Representations, 2024a. URL https://openreview. net/forum?id=0uI5415ry7. ", "page_idx": 9}, {"type": "text", "text": "Kwangjun Ahn, Zhiyu Zhang, Yunbum Kook, and Yan Dai. Understanding Adam optimizer via online learning of updates: Adam is FTRL in disguise. In International Conference on Machine Learning. PMLR, 2024b. ", "page_idx": 9}, {"type": "text", "text": "Ahmet Alacaoglu, Yura Malitsky, Panayotis Mertikopoulos, and Volkan Cevher. A new regret analysis for Adam-type algorithms. In International conference on machine learning (ICML), pages 202\u2013210. PMLR, 2020. ", "page_idx": 9}, {"type": "text", "text": "Yossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Ayush Sekhari, and Karthik Sridharan. Second-order information in non-convex stochastic optimization: Power and limitations. In Conference on Learning Theory, pages 242\u2013299, 2020.   \nYossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Woodworth. Lower bounds for non-convex stochastic optimization. Mathematical Programming, 199(1): 165\u2013214, 2023.   \nYogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, et al. eDiff-I: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022.   \nAdam Block, Dylan J Foster, Akshay Krishnamurthy, Max Simchowitz, and Cyril Zhang. Butterfly effects of SGD noise: Error amplification in behavior cloning and autoregression. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/ forum?id $\\equiv$ CgPs04l9TO.   \nXiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of A class of Adam-type algorithms for non-convex optimization. In International Conference on Learning Representations (ICLR), 2019.   \nMichael Crawshaw, Mingrui Liu, Francesco Orabona, Wei Zhang, and Zhenxun Zhuang. Robustness to unbounded smoothness of generalized signsgd. Advances in neural information processing systems, 35:9955\u20139968, 2022.   \nAshok Cutkosky, Harsh Mehta, and Francesco Orabona. Optimal stochastic non-smooth non-convex optimization through online-to-non-convex conversion. In International Conference on Machine Learning (ICML), volume 202, pages 6643\u20136670. PMLR, 2023.   \nDamek Davis, Dmitriy Drusvyatskiy, Kellie J MacPhee, and Courtney Paquette. Subgradient methods for sharp weakly convex functions. Journal of Optimization Theory and Applications, 179:962\u2013982, 2018.   \nDamek Davis, Dmitriy Drusvyatskiy, Sham Kakade, and Jason D Lee. Stochastic subgradient method converges on tame functions. Foundations of computational mathematics, 20(1):119\u2013154, 2020.   \nDamek Davis, Mateo D\u00edaz, and Dmitriy Drusvyatskiy. Escaping strict saddle points of the moreau envelope in nonsmooth optimization. SIAM Journal on Optimization, 32(3):1958\u20131983, 2022a.   \nDamek Davis, Dmitriy Drusvyatskiy, Yin Tat Lee, Swati Padmanabhan, and Guanghao Ye. A gradient sampling method with complexity guarantees for lipschitz functions in high and low dimensions. In Advances in Neural Information Processing Systems, volume 35, pages 6692\u20136703. Curran Associates, Inc., 2022b.   \nAlexandre D\u00e9fossez, Leon Bottou, Francis Bach, and Nicolas Usunier. A simple convergence proof of Adam and AdaGrad. Transactions on Machine Learning Research (TMLR), 2022.   \nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780\u20138794, 2021.   \nJ. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. In Conference on Learning Theory (COLT), pages 257\u2013269, 2010.   \nAA Goldstein. Optimization of lipschitz continuous functions. Mathematical Programming, 13: 14\u201322, 1977.   \nGeoffrey J. Gordon. Regret bounds for prediction problems. In Conference on Learning Theory (COLT), pages 29\u201340. ACM, 1999.   \nZhishuai Guo, Yi Xu, Wotao Yin, Rong Jin, and Tianbao Yang. A novel convergence analysis for algorithms of the Adam family. arXiv preprint arXiv:2112.03459, 2021.   \nElad Hazan and Satyen Kale. Extracting certainty from uncertainty: Regret bounded by variation in costs. Machine Learning, 80:165\u2013188, 2010.   \nPavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. In 34th Conference on Uncertainty in Artificial Intelligence 2018, UAI 2018, pages 876\u2013885. Association For Uncertainty in Artificial Intelligence (AUAI), 2018.   \nAndrew Jacobsen and Ashok Cutkosky. Online linear regression in dynamic environments via discounting. In International Conference on Machine Learning. PMLR, 2024.   \nKaiqi Jiang, Dhruv Malik, and Yuanzhi Li. How does adaptive optimization impact local neural network geometry? In Advances in Neural Information Processing Systems (NeurIPS), 2023. URL https://openreview.net/forum?id=gIG8LvTLuc.   \nMichael Jordan, Guy Kornowski, Tianyi Lin, Ohad Shamir, and Manolis Zampetakis. Deterministic nonsmooth nonconvex optimization. In The Thirty Sixth Annual Conference on Learning Theory, pages 4570\u20134597. PMLR, 2023.   \nAdam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. Journal of Computer and System Sciences (JCSS), 71(3):291\u2013307, 2005.   \nMinguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10124\u201310134, 2023.   \nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4401\u20134410, 2019.   \nTero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. Advances in Neural Information Processing Systems, 35:26565\u201326577, 2022.   \nTero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. arXiv preprint arXiv:2312.02696, 2023.   \nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2014.   \nFrederik Kunstner, Jacques Chen, Jonathan Wilder Lavington, and Mark Schmidt. Noise is not the main factor behind the gap between SGD and Adam on Transformers, but sign descent might be. In International Conference on Learning Representations (ICLR), 2023.   \nFrederik Kunstner, Robin Yadav, Alan Milligan, Mark Schmidt, and Alberto Bietti. Heavy-tailed class imbalance and why adam outperforms gradient descent on language models. arXiv preprint arXiv:2402.19449, 2024.   \nHaochuan Li, Alexander Rakhlin, and Ali Jadbabaie. Convergence of Adam under relaxed assumptions. In Advances in Neural Information Processing Systems (NeurIPS), 2023. URL https://openreview.net/forum?id $\\equiv$ yEewbkBNzi.   \nH. Brendan McMahan and Matthew Streeter. Adaptive bound optimization for online convex optimization. In Proceedings of the 23rd Annual Conference on Learning Theory (COLT), pages 244\u2013256, 2010.   \nYurii Nesterov. Primal-dual subgradient methods for convex problems. Mathematical programming, 120(1):221\u2013259, 2009.   \nAlexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International conference on machine learning, pages 8162\u20138171. PMLR, 2021.   \nFrancesco Orabona and D\u00e1vid P\u00e1l. Scale-free online learning. Theoretical Computer Science, 716: 50\u201369, 2018.   \nYan Pan and Yuanzhi Li. Toward understanding why Adam converges faster than SGD for Transformers. arXiv preprint arXiv:2306.00204, 2023.   \nBoris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM journal on control and optimization, 30(4):838\u2013855, 1992.   \nSashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In International Conference on Learning Representations, 2018.   \nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \nDavid Ruppert. Efficient estimations from a slowly convergent robbins-monro process. Technical report, Cornell University Operations Research and Industrial Engineering, 1988.   \nShai Shalev-Shwartz and Yoram Singer. Online learning meets optimization in the dual. In Conference on Learning Theory (COLT), volume 4005, pages 423\u2013437. Springer, 2006.   \nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021a. URL https://openreview.net/ forum?id $\\equiv$ St1giarCHLP.   \nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021b. URL https://openreview.net/forum?id $=$ PxTIG12RRHS.   \nAntti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems, 30, 2017.   \nLai Tian and Anthony Man-Cho So. No dimension-free deterministic algorithm computes approximate stationarities of lipschitzians. arXiv preprint arXiv:2210.06907, 2022.   \nLai Tian, Kaiwen Zhou, and Anthony Man-Cho So. On the finite-time complexity and practical computation of approximate stationarity concepts of lipschitz functions. In International Conference on Machine Learning, pages 21360\u201321379. PMLR, 2022.   \nBohan Wang, Jingwen Fu, Huishuai Zhang, Nanning Zheng, and Wei Chen. Closing the gap between the upper bound and lower bound of Adam\u2019s iteration complexity. In Advances in Neural Information Processing Systems (NeurIPS), 2023.   \nYasin Yaz, Chuan-Sheng Foo, Stefan Winkler, Kim-Hui Yap, Georgios Piliouras, Vijay Chandrasekhar, et al. The unusual effectiveness of averaging in GAN training. In International Conference on Learning Representations, 2018.   \nJingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates training: A theoretical justification for adaptivity. In International Conference on Learning Representations (ICLR), 2020a.   \nJingzhao Zhang, Hongzhou Lin, Stefanie Jegelka, Suvrit Sra, and Ali Jadbabaie. Complexity of finding stationary points of nonconvex nonsmooth functions. In International Conference on Machine Learning, 2020b.   \nQinzi Zhang and Ashok Cutkosky. Random scaling and momentum for non-smooth non-convex optimization. In International Conference on Machine Learning. PMLR, 2024.   \nYushun Zhang, Congliang Chen, Naichen Shi, Ruoyu Sun, and Zhi-Quan Luo. Adam can converge without any modification on update rules. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages 28386\u201328399, 2022.   \nYushun Zhang, Congliang Chen, Tian Ding, Ziniu Li, Ruoyu Sun, and Zhi-Quan Luo. Why transformers need adam: A hessian perspective. arXiv preprint arXiv:2402.16788, 2024a.   \nZhiyu Zhang, David Bombara, and Heng Yang. Discounted adaptive online prediction. In International Conference on Machine Learning. PMLR, 2024b.   \nZhiming Zhou, Qingru Zhang, Guansong Lu, Hongwei Wang, Weinan Zhang, and Yong Yu. Adashift: Decorrelation and convergence of adaptive learning rate methods. In International Conference on Learning Representations (ICLR), 2019.   \nZhenxun Zhuang, Mingrui Liu, Ashok Cutkosky, and Francesco Orabona. Understanding AdamW through proximal methods and scale-freeness. Transactions on Machine Learning Research, 2022.   \nFangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, and Wei Liu. A sufficient condition for convergences of Adam and RMSProp. In The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11127\u201311135, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Proof of discounted-to-nonconvex conversion (Lemma 7) 1 ", "page_idx": 14}, {"type": "text", "text": "A.1 Proof of the coordinate-wise version (Lemma 17) . 17 ", "page_idx": 14}, {"type": "text", "text": "B Proof of main theorems ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "18 ", "page_idx": 14}, {"type": "text", "text": "B.1 Proof of Theorem 11 18   \nB.2 Proof of Theorem 18 19 ", "page_idx": 14}, {"type": "text", "text": "A Proof of discounted-to-nonconvex conversion (Lemma 7) ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Note first that via a change of summation, we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{n=1}^{T}\\displaystyle\\sum_{t=1}^{n}\\beta^{n-t}(1-\\beta)(F(\\mathbf{x}_{t})-F(\\mathbf{x}_{t-1}))=\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{n=t}^{T}\\beta^{n-t}(1-\\beta)(F(\\mathbf{x}_{t})-F(\\mathbf{x}_{t-1}))}&{}\\\\ {\\displaystyle=\\sum_{t=1}^{T}(1-\\beta^{T-t+1})(F(\\mathbf{x}_{t})-F(\\mathbf{x}_{t-1}))}&{}\\\\ {\\displaystyle=F(\\mathbf{x}_{T})-F(\\mathbf{x}_{0})-\\displaystyle\\sum_{t=1}^{T}\\beta^{T-t+1}(F(\\mathbf{x}_{t})-F(\\mathbf{x}_{t-1}))\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Rearranging the above together with the fact $\\begin{array}{r}{F(\\mathbf{x}_{0})-F(\\mathbf{x}_{T})\\leq F(\\mathbf{x}_{0})-\\operatorname*{inf}_{\\mathbf{x}}F(\\mathbf{x})=:\\Delta}\\end{array}$ , we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n-\\Delta\\leq\\underbrace{\\mathbb{E}\\left[\\sum_{n=1}^{T}\\sum_{t=1}^{n}\\beta^{n-t}(1-\\beta)(F(\\mathbf{x}_{t})-F(\\mathbf{x}_{t-1}))\\right]}_{\\mathrm{(A)}}+\\underbrace{\\mathbb{E}\\left[\\sum_{t=1}^{T}\\beta^{T-t+1}(F(\\mathbf{x}_{t})-F(\\mathbf{x}_{t-1}))\\right]}_{\\mathrm{(B)}}\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We also recall the following fact about exponential random variable due to [Zhang and Cutkosky, 2024, Lemma 3.1]. ", "page_idx": 14}, {"type": "text", "text": "Lemma 20. Let $\\alpha\\sim E x p(\\lambda)$ for some $\\lambda>0$ , then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[F({\\mathbf x}+\\alpha{\\mathbf z})-F({\\mathbf x})]=\\mathbb{E}[\\langle\\nabla F({\\mathbf x}+\\alpha{\\mathbf z}),{\\mathbf z}\\rangle]/\\lambda\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By Lemma 20 with $\\lambda=1$ , together with $\\mathbf{x}_{t}=\\mathbf{x}_{t-1}+\\alpha_{t}\\mathbf{z}_{t}$ , it follows that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}[F({\\bf x}_{t})-F({\\bf x}_{t-1})]=\\mathbb{E}\\left[\\langle{\\bf g}_{t},{\\bf z}_{t}\\rangle\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This identity indicates that the function gap is exactly equal to the linearization of the function gap.   \nIn this sense, the randomization renders the first-order Taylor approximation perfectly accurate. ", "page_idx": 14}, {"type": "text", "text": "Now, with this result, we will address each term separately. ", "page_idx": 14}, {"type": "text", "text": "Analysis of ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Note that for each $t\\leq n$ , since $\\mathbf{x}_{t}=\\mathbf{x}_{t-1}+\\alpha_{t}\\mathbf{z}_{t}$ for $\\alpha_{t}{\\sim}\\mathrm{Exp}(1)$ , Lemma 20 yields ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[F(\\mathbf{x}_{t})-F(\\mathbf{x}_{t-1})]=\\mathbb{E}\\left\\langle\\nabla F(\\mathbf{x}_{t}),\\mathbf{z}_{t}\\right\\rangle=\\mathbb{E}\\left\\langle\\nabla F(\\mathbf{x}_{t}),\\mathbf{u}_{n}\\right\\rangle+\\mathbb{E}\\left\\langle\\nabla F(\\mathbf{x}_{t}),\\mathbf{z}_{t}-\\mathbf{u}_{n}\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad=\\mathbb{E}\\left\\langle\\nabla F(\\mathbf{x}_{t}),\\mathbf{u}_{n}\\right\\rangle+\\mathbb{E}\\left\\langle\\nabla F(\\mathbf{x}_{t})-\\mathbf{g}_{t},\\mathbf{z}_{t}-\\mathbf{u}_{n}\\right\\rangle+\\mathbb{E}\\left\\langle\\mathbf{g}_{t},\\mathbf{z}_{t}-\\mathbf{u}_{n}\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad=\\underbrace{\\mathbb{E}\\left\\langle\\nabla F(\\mathbf{x}_{t}),\\mathbf{u}_{n}\\right\\rangle}_{\\textcircled{1}}+\\underbrace{\\mathbb{E}\\left\\langle\\nabla F(\\mathbf{x}_{t})-\\mathbf{g}_{t},-\\mathbf{u}_{n}\\right\\rangle}_{\\textcircled{2}}+\\underbrace{\\mathbb{E}\\left\\langle\\mathbf{g}_{t},\\mathbf{z}_{t}-\\mathbf{u}_{n}\\right\\rangle}_{\\textcircled{3}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the last line follows from the fact $\\mathbb{E}[\\langle\\nabla F({\\mathbf{x}}_{t})-{\\mathbf{g}}_{t},{\\mathbf{z}}_{t}\\rangle]\\,=\\,0$ . More specifically, note that the randomness in the stochastic gradient oracle is independent of the randomness due to $\\alpha_{t}$ . Since $\\mathbb{E}[\\nabla F({\\mathbf x}_{t})-\\mathbf{g}_{t}]=0$ , it follows that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\langle\\nabla F({\\mathbf x}_{t})-\\mathbf{g}_{{\\mathbf t}},\\mathbf{z}_{{\\mathbf t}}\\rangle]=\\mathbb{E}[\\langle\\mathbb{E}[\\nabla F({\\mathbf x}_{t})-\\mathbf{g}_{{\\mathbf t}}],\\mathbf{z}_{{\\mathbf t}}\\rangle]=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the inner expectation is with respect to the randomness in the stochastic gradient oracle and the outer is with respect to all other quantities. ", "page_idx": 15}, {"type": "text", "text": "Now let us handle each term. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\sum_{t=1}^{n}\\beta^{n-t}(1-\\beta)\\left\\langle\\nabla F({\\mathbf x}_{t}),{\\mathbf u}_{n}\\right\\rangle=(1-\\beta)\\operatorname{\\mathbb{E}}\\left\\langle\\sum_{t=1}^{n}\\beta^{n-t}\\nabla F({\\mathbf x}_{t}),-D\\frac{\\sum_{t=1}^{n}\\beta^{n-t}\\nabla F({\\mathbf x}_{t})}{\\|\\sum_{t=1}^{n}\\beta^{n-t}\\nabla F({\\mathbf x}_{t})\\|}\\right\\rangle\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=(1-\\beta^{n})\\mathbb{E}\\left\\langle\\sum_{t=1}^{n}\\frac{1-\\beta}{1-\\beta^{n}}\\beta^{n-t}\\nabla F({\\mathbf x}_{t}),-D\\frac{\\sum_{t=1}^{n}\\frac{1-\\beta}{1-\\beta^{n}}\\beta^{n-t}\\nabla F({\\mathbf x}_{t})}{\\left\\|\\sum_{t=1}^{n}\\frac{1-\\beta}{1-\\beta^{n}}\\beta^{n-t}\\nabla F({\\mathbf x}_{t})\\right\\|}\\right\\rangle}\\\\ &{=-D(1-\\beta^{n})\\,\\mathbb{E}\\left\\|\\mathbb{E}_{n}\\,\\nabla F({\\mathbf y}_{n})\\right\\|\\leq-D\\,\\mathbb{E}\\left\\|\\mathbb{E}_{n}\\,\\nabla F({\\mathbf y}_{n})\\right\\|+D G\\beta^{n}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, summing over $n=1,\\ldots,T$ , we obtain: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\sum_{n=1}^{T}\\sum_{t=1}^{n}\\beta^{n-t}(1-\\beta)\\left\\langle\\nabla F(\\mathbf{x}_{t}),\\mathbf{u}_{n}\\right\\rangle\\leq-D\\,\\mathbb{E}\\sum_{t=1}^{T}\\left\\|\\mathbb{E}_{t}\\,\\nabla F(\\mathbf{y}_{t})\\right\\|+\\frac{D G}{1-\\beta}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "$\\textcircled{2}$ : For the second term, using Cauchy-Schwartz inequality, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\sum_{t=1}^{n}\\beta^{n-t}\\left\\langle\\nabla F(\\mathbf{x}_{t})-\\mathbf{g}_{t},-\\mathbf{u}_{n}\\right\\rangle\\leq\\sqrt{\\mathbb{E}\\left\\|\\sum_{t=1}^{n}\\beta^{n-t}(\\nabla F(\\mathbf{x}_{t})-\\mathbf{g}_{t})\\right\\|^{2}\\mathbb{E}\\left\\|\\mathbf{u}_{n}\\right\\|^{2}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using the bounded variance assumption on the stochastic gradient oracle, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\Vert\\sum_{t=1}^{n}\\beta^{n-t}(\\nabla F(\\mathbf{x}_{t})-\\mathbf{g}_{t})\\right\\Vert^{2}=\\mathbb{E}\\sum_{t=1}^{n}\\beta^{2(n-t)}\\left\\Vert\\nabla F(\\mathbf{x}_{t})-\\mathbf{g}_{t}\\right\\Vert^{2}\\leq\\frac{\\sigma^{2}}{1-\\beta^{2}}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, summing over $n\\,=\\,1,\\ldots,T$ , and using the fact that $\\begin{array}{r}{\\frac{1}{1-\\beta^{2}}\\,\\leq\\,\\frac{1}{1-\\beta}}\\end{array}$ , we get the following bound on the second term: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\sum_{n=1}^{T}\\sum_{t=1}^{n}\\beta^{n-t}(1-\\beta)\\left\\langle\\nabla F(\\mathbf{x}_{t})-\\mathbf{g}_{t},-\\mathbf{u}_{n}\\right\\rangle\\leq\\sum_{n=1}^{T}(1-\\beta)\\cdot\\frac{\\sigma D}{\\sqrt{1-\\beta^{2}}}\\leq\\sigma D T\\sqrt{1-\\beta}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "${\\bigcirc}.$ : Lastly, for the third term, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\displaystyle\\sum_{n=1}^{T}\\sum_{t=1}^{n}\\beta^{n-t}(1-\\beta)\\left<\\mathbf{g}_{t},\\mathbf{z}_{t}-\\mathbf{u}_{n}\\right>=\\left(1-\\beta\\right)\\mathbb{E}\\displaystyle\\sum_{n=1}^{T}\\left[\\sum_{t=1}^{n}\\mathbb{E}\\left<\\beta^{n-t}\\mathbf{g}_{t},\\mathbf{z}_{t}-\\mathbf{u}_{n}\\right>\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\left(1-\\beta\\right)\\mathbb{E}\\displaystyle\\sum_{t=1}^{T}\\mathrm{Regret}_{t}^{\\left[\\beta\\right]}(\\mathbf{u}_{t})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Analysis of $\\textcircled{\\textbf{B}}$ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Note that for each $t$ , since $\\mathbf{x}_{t}\\gets\\mathbf{x}_{t-1}+\\alpha_{t}\\mathbf{z}_{t}$ for $\\alpha_{t}{\\sim}\\mathrm{Exp}(1)$ , Lemma 20 yields ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[F({\\mathbf x}_{t})-F({\\mathbf x}_{t-1})]=\\mathbb{E}\\left\\langle\\nabla F({\\mathbf x}_{t}),{\\mathbf z}_{t}\\right\\rangle=\\mathbb{E}\\left\\langle{\\mathbf g}_{t},{\\mathbf z}_{t}\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad=\\mathbb{E}\\left\\langle{\\mathbf g}_{t},{\\mathbf z}_{t}-{\\mathbf u}_{T}\\right\\rangle+\\mathbb{E}\\left\\langle{\\mathbf g}_{t},{\\mathbf u}_{T}\\right\\rangle\\leq\\mathbb{E}\\left\\langle{\\mathbf g}_{t},{\\mathbf z}_{t}-{\\mathbf u}_{T}\\right\\rangle+D(G+\\sigma)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb E\\left[\\displaystyle\\sum_{t=1}^{T}\\beta^{T-t+1}(F(\\mathbf x_{t})-F(\\mathbf x_{t-1}))\\right]=\\beta\\mathbb E\\displaystyle\\sum_{t=1}^{T}\\left[\\langle\\beta^{T-t}\\mathbf g_{t},\\mathbf z_{t}-\\mathbf u_{T}\\rangle+\\beta^{T-t}D(G+\\sigma)\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\beta\\mathbb E[\\mathrm{Regret}_{T}^{[\\beta]}(\\mathbf u_{T})]+\\frac{D(G+\\sigma)}{1-\\beta}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combining A and B ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Combining the above analyses and rearranging, it follows that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D\\mathbb{E}\\displaystyle\\sum_{t=1}^{T}\\left\\|\\underline{{\\mathbb{E}}}\\,\\nabla F(\\mathbf{y}_{t})\\right\\|\\leq\\Delta+\\displaystyle\\frac{D G}{1-\\beta}+\\sigma D T\\sqrt{1-\\beta}+(1-\\beta)\\,{\\mathbb{E}}\\displaystyle\\sum_{t=1}^{T}\\left[\\mathrm{Regret}_{t}^{[\\beta]}(\\mathbf{u}_{t})\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\beta\\,{\\mathbb{E}}[\\mathrm{Regret}_{T}^{[\\beta]}(\\mathbf{u}_{T})]+\\displaystyle\\frac{D(G+\\sigma)}{1-\\beta}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Dividing both sides by $D T$ , we get the desired result. ", "page_idx": 16}, {"type": "text", "text": "A.1 Proof of the coordinate-wise version (Lemma 17) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The proof closely follows that of Lemma 7. In particular, with $\\Delta:=F(\\mathbf{x}_{0})-\\operatorname*{inf}_{\\mathbf{x}}F(\\mathbf{x})$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n-\\Delta\\leq\\underbrace{\\mathbb{E}\\left[\\sum_{n=1}^{T}\\sum_{t=1}^{n}\\beta^{n-t}(1-\\beta)(F(\\mathbf{x}_{t})-F(\\mathbf{x}_{t-1}))\\right]}_{\\mathrm{(A)}}+\\underbrace{\\mathbb{E}\\left[\\sum_{t=1}^{T}\\beta^{T-t+1}(F(\\mathbf{x}_{t})-F(\\mathbf{x}_{t-1}))\\right]}_{\\mathrm{(B)}}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We begin with the term $\\textcircled{\\scriptsize{\\mathbf{B}}}$ . Using the same decomposition as before, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbb{E}[F(\\mathbf{x}_{t})-F(\\mathbf{x}_{t-1})]=\\mathbb{E}\\left\\langle\\mathbf{g}_{t},\\mathbf{z}_{t}-\\mathbf{u}_{T}\\right\\rangle+\\mathbb{E}\\left\\langle\\mathbf{g}_{t},\\mathbf{u}_{T}\\right\\rangle=\\mathbb{E}\\left\\langle\\mathbf{g}_{t},\\mathbf{z}_{t}-\\mathbf{u}_{T}\\right\\rangle+\\displaystyle\\sum_{i=1}^{d}\\mathbb{E}\\,\\mathbf{g}_{t}[i]\\mathbf{u}_{T}[i]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\mathbb{E}\\left\\langle\\mathbf{g}_{t},\\mathbf{z}_{t}-\\mathbf{u}_{T}\\right\\rangle+D\\displaystyle\\sum_{i=1}^{d}(G_{i}+\\sigma_{i})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\beta^{T-t+1}(F(\\mathbf{x}_{t})-F(\\mathbf{x}_{t-1}))\\right]=\\beta\\,\\mathbb{E}\\displaystyle\\sum_{t=1}^{T}\\left[\\langle\\beta^{T-t}\\mathbf{g}_{t},\\mathbf{z}_{t}-\\mathbf{u}_{T}\\rangle+\\beta^{T-t}\\displaystyle\\sum_{i=1}^{d}D(G_{i}+\\sigma_{i})\\right]}\\\\ &{\\phantom{=}\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\beta\\,\\mathbb{E}[\\mathrm{Regret}_{T}^{[\\beta]}(\\mathbf{u}_{T})]+\\frac{D\\sum_{i=1}^{d}(G_{i}+\\sigma_{i})}{1-\\beta}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Moving onto the term $\\textcircled{\\mathrm{A}}.$ , we again use the same decomposition: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}[F(\\mathbf{x}_{t})-F(\\mathbf{x}_{t-1})]=\\underbrace{\\mathbb{E}\\left\\langle\\nabla F(\\mathbf{x}_{t}),\\mathbf{u}_{n}\\right\\rangle}_{\\textcircled{1}}+\\underbrace{\\mathbb{E}\\left\\langle\\nabla F(\\mathbf{x}_{t})-\\mathbf{g}_{t},-\\mathbf{u}_{n}\\right\\rangle}_{\\textcircled{2}}+\\underbrace{\\mathbb{E}\\left\\langle\\mathbf{g}_{t},\\mathbf{z}_{t}-\\mathbf{u}_{n}\\right\\rangle}_{\\textcircled{3}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As before, let us handle each term one by one separately. ", "page_idx": 16}, {"type": "text", "text": "$\\textcircled{1}$ : Note that using the definition of ${\\bf y}_{n}$ , for each coordinate $i=1,\\ldots,d,$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\boldsymbol{\\zeta}\\sum_{t=1}^{n}\\beta^{n-t}(1-\\beta)\\partial_{t}F(\\mathbf{x}_{t})\\mathbf{u}_{n}[i]=(1-\\beta)\\,\\mathbb{E}\\left[\\left(\\sum_{t=1}^{n}\\beta^{n-t}\\nabla F(\\mathbf{x}_{t})\\right)\\left(-D\\frac{\\sum_{t=1}^{n}\\beta^{n-t}\\partial_{i}F(\\mathbf{x}_{t})}{|\\sum_{t=1}^{n}\\beta^{s-t}\\partial_{i}F(\\mathbf{x}_{t})|}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=(1-\\beta^{n})\\mathbb{E}\\left[\\left(\\displaystyle\\sum_{t=1}^{n}\\frac{1-\\beta}{1-\\beta^{n}}\\beta^{n-t}\\nabla F(\\mathbf{x}_{t})\\right)\\left(-D\\displaystyle\\frac{\\sum_{t=1}^{n}\\frac{1-\\beta}{1-\\beta^{n}}\\beta^{n-t}\\nabla F(\\mathbf{x}_{t})}{\\left|\\sum_{t=1}^{n}\\frac{1-\\beta}{1-\\beta^{n}}\\beta^{n-t}\\nabla F(\\mathbf{x}_{t})\\right|}\\right)\\right]}\\\\ &{=-D(1-\\beta^{n})\\,\\mathbb{E}\\left|\\displaystyle\\mathbb{E}_{n}\\,\\partial_{t}F(\\mathbf{y}_{n})\\right|\\leq-D\\,\\mathbb{E}\\left|\\displaystyle\\mathbb{E}_{n}\\,\\partial_{t}F(\\mathbf{y}_{n})\\right|+D G_{i}\\beta^{n}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, summing over $i=1,\\ldots,d$ and then $n=1,\\ldots,T$ , we obtain: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\sum_{n=1}^{T}\\sum_{t=1}^{n}\\beta^{n-t}(1-\\beta)\\left\\langle\\nabla F(\\mathbf{x}_{t}),\\mathbf{u}_{n}\\right\\rangle\\leq-D\\,\\mathbb{E}\\sum_{t=1}^{T}\\left\\|\\mathbb{E}_{t}\\,\\nabla F(\\mathbf{y}_{t})\\right\\|_{1}+\\frac{D\\sum_{i=1}G_{i}}{1-\\beta}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "$\\textcircled{2}$ : For each coordinate $i=1,\\ldots,d$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\sum_{t=1}^{n}\\beta^{n-t}(\\partial_{i}F(\\mathbf{x}_{t})-\\mathbf{g}_{t}[i])(-\\mathbf{u}_{n}[i])\\leq\\sqrt{\\mathbb{E}\\left|\\sum_{t=1}^{n}\\beta^{n-t}(\\partial_{i}F(\\mathbf{x}_{t})-\\mathbf{g}_{t}[i])\\right|^{2}\\mathbb{E}\\left|\\mathbf{u}_{n}[i]\\right|^{2}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using the coordinate-wise bounded variance assumption on the stochastic gradient oracle, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left|\\sum_{t=1}^{n}\\beta^{n-t}(\\partial_{i}F(\\mathbf{x}_{t})-\\mathbf{g}_{t}[i])\\right|^{2}=\\mathbb{E}\\sum_{t=1}^{n}\\beta^{2(n-t)}\\left|\\partial_{i}F(\\mathbf{x}_{t})-\\mathbf{g}_{t}[i]\\right|^{2}\\leq\\frac{\\sigma_{i}^{2}}{1-\\beta^{2}}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, summing over $n\\,=\\,1,\\ldots,T$ , and using the fact that $\\begin{array}{r}{\\frac{1}{1-\\beta^{2}}\\,\\leq\\,\\frac{1}{1-\\beta}}\\end{array}$ , we get the following bound on the second term: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\displaystyle\\sum_{n=1}^{T}\\sum_{t=1}^{n}\\beta^{n-t}(1-\\beta)\\left<\\nabla F(\\mathbf{x}_{t})-\\mathbf{g}_{t},-\\mathbf{u}_{n}\\right>\\leq\\displaystyle\\sum_{n=1}^{T}(1-\\beta)\\cdot\\frac{D\\sum_{i=1}^{d}\\sigma_{i}}{\\sqrt{1-\\beta^{2}}}}\\\\ {\\leq D T\\left(\\displaystyle\\sum_{i=1}^{d}\\sigma_{i}\\right)\\sqrt{1-\\beta}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "$\\textcircled{3}$ We use the same manipulation as before: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\sum_{n=1}^{T}\\sum_{t=1}^{n}\\beta^{n-t}(1-\\beta)\\left\\langle\\mathbf{g}_{t},\\mathbf{z}_{t}-\\mathbf{u}_{n}\\right\\rangle=(1-\\beta)\\,\\mathbb{E}\\sum_{t=1}^{T}\\mathrm{Regret}_{t}^{[\\beta]}(\\mathbf{u}_{t})\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Combining the above, we get the desired result in Lemma 17. ", "page_idx": 17}, {"type": "text", "text": "B Proof of main theorems ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 Proof of Theorem 11 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "By Definition 3, since $\\mathbb{E}[\\mathbf{y}_{t}]=\\overline{{\\mathbf{x}}}_{t}$ , it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{t\\sim\\left[T\\right]}{\\mathbb{E}}\\left\\|\\nabla F(\\overline{{\\mathbf{x}}}_{t})\\right\\|^{[\\lambda]}\\leq\\underset{t\\sim\\left[T\\right]}{\\mathbb{E}}\\left[\\left\\|\\underset{\\mathbf{y}_{t}}{\\mathbb{E}}\\nabla F(\\mathbf{y}_{t})\\right\\|+\\lambda\\underset{\\mathbf{y}_{t}}{\\mathbb{E}}\\left\\|\\mathbf{y}_{t}-\\overline{{\\mathbf{x}}}_{t}\\right\\|^{2}\\right]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We begin with the second term (the variance term). By Lemma 10, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\lambda\\operatorname*{\\mathbb{E}}_{t\\sim[T]}\\mathbb{E}_{t}\\left\\|\\mathbf{y}_{t}-\\overline{{\\mathbf{x}}}_{t}\\right\\|^{2}\\leq12\\frac{\\lambda D^{2}}{(1-\\beta)^{2}}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence, by choosing D = (14\u2212\u03bb\u03b21)/\u03b521/2, it follows that $\\lambda\\cdot\\mathbb{E}_{t\\sim[T]}\\,\\mathbb{E}_{\\mathbf{y}_{t}}\\left\\|\\mathbf{y}_{t}-\\overline{{\\mathbf{x}}}_{t}\\right\\|^{2}\\leq\\varepsilon.$ . ", "page_idx": 18}, {"type": "text", "text": "Next consider the first term (the norm of the averaged gradients). Plugging the regret bound (1) into Lemma 7, we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{t\\sim[T]}{\\mathbb{E}}\\left\\|\\underset{\\mathbf{y}_{t}}{\\mathbb{E}}\\nabla F(\\mathbf{y}_{t})\\right\\|\\leq\\frac{\\Delta}{D T}+\\frac{2G+\\sigma}{(1-\\beta)T}+\\sigma\\sqrt{1-\\beta}+\\frac{4(G+\\sigma)}{T\\sqrt{1-\\beta}}+4(G+\\sigma)\\sqrt{1-\\beta}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\frac{4\\Delta\\lambda^{1/2}}{(1-\\beta)\\varepsilon^{1/2}T}+\\frac{6G+5\\sigma}{(1-\\beta)T}+(4G+5\\sigma)\\sqrt{1-\\beta}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last line follows since\u221a11\u2212\u03b2 \u22641\u22121\u03b2 and D = (14\u2212\u03bb\u03b21)/\u03b521/2. Choosing $\\begin{array}{r}{\\beta=1-(\\frac{\\varepsilon}{10C})^{2}}\\end{array}$ , the last term is bounded by $\\frac{G+\\sigma}{2C}\\varepsilon$ . Moreover, choosing $T=(1-\\beta)^{-1}\\cdot\\operatorname*{max}\\left\\{4\\Delta\\lambda^{1/2}\\varepsilon^{-3/2},\\,12C\\varepsilon^{-1}\\right\\}$ , the first and second terms are bounded by $\\varepsilon$ and $\\frac{G+\\sigma}{2C}\\varepsilon$ , respectively. This concludes the proof. ", "page_idx": 18}, {"type": "text", "text": "B.2 Proof of Theorem 18 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "By Definition 15, since $\\mathbb{E}[\\mathbf{y}_{t}]=\\overline{{\\mathbf{x}}}_{t}$ , it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{t\\sim[T]}{\\mathbb{E}}\\,\\|\\nabla F(\\overline{{\\mathbf{x}}}_{t})\\|_{1}^{[\\lambda]}\\leq\\underset{t\\sim[T]}{\\mathbb{E}}\\left[\\left\\|\\underline{{\\mathbb{E}}}\\,\\nabla F(\\mathbf{y}_{t})\\right\\|_{1}+\\lambda\\underset{\\mathbf{y}_{t}}{\\mathbb{E}}\\,\\|\\mathbf{y}_{t}-\\overline{{\\mathbf{x}}}_{t}\\|_{2}^{2}\\right]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We begin with the second term (the variance term). This time, given that now each coordinate of update $\\mathbf{z}_{t}$ is bounded by $D$ , i.e., $|\\mathbf{z}_{t}[i]|\\,\\le\\,D$ , applying the variance bound due to Lemma 10 coordinate-wise implies: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\lambda\\operatorname*{\\mathbb{E}}_{t\\sim[T]}\\mathbb{E}_{t}\\left\\|\\mathbf{y}_{t}-\\overline{{\\mathbf{x}}}_{t}\\right\\|_{2}^{2}\\leq12\\frac{\\lambda d D^{2}}{(1-\\beta)^{2}}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hence, by choosing $\\begin{array}{r}{D=\\frac{(1-\\beta)\\varepsilon^{1/2}}{4d^{1/2}\\lambda^{1/2}}}\\end{array}$ (41d\u22121/\u03b22)\u03bb\u03b51/2 , it follows that \u03bb Et\u223c[T ] Eyt \u2225yt \u2212xt\u22252 \u2264\u03b5. ", "page_idx": 18}, {"type": "text", "text": "Next consider the first term (the $L_{1}$ -norm of the averaged gradients). Plugging the regret bound (2) into Lemma 17, and doing similar manipulations as the proof of Theorem 11, we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{t\\sim[T]}{\\mathbb{E}}\\left\\|\\underset{\\mathbf{y}_{t}}{\\mathbb{E}}\\nabla F(\\mathbf{y}_{t})\\right\\|_{1}\\leq\\frac{\\Delta}{D T}+\\frac{\\left\\|6G+5\\pmb{\\sigma}\\right\\|_{1}}{(1-\\beta)T}+\\left\\|4G+5\\pmb{\\sigma}\\right\\|_{1}\\sqrt{1-\\beta}}&{}\\\\ {=\\frac{4\\Delta d^{1/2}\\lambda^{1/2}}{(1-\\beta)\\varepsilon^{1/2}T}+\\frac{\\left\\|6G+5\\pmb{\\sigma}\\right\\|_{1}}{(1-\\beta)T}+\\left\\|4G+5\\pmb{\\sigma}\\right\\|_{1}\\sqrt{1-\\beta}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last line follows since D = (41d\u22121/\u03b22)\u03bb\u03b511//22 . Choosing \u03b2 = 1 \u2212(10\u03b5C )2, the last term is bounded by $\\begin{array}{r}{\\frac{\\|G+\\pmb{\\sigma}\\|_{1}}{2C}\\cdot\\varepsilon}\\end{array}$ . Moreover, choosing $T=(1-\\beta)^{-1}\\cdot\\operatorname*{max}\\left\\{4\\Delta d^{1/2}\\lambda^{1/2}\\varepsilon^{-3/2},\\,12C\\varepsilon^{-1}\\right\\}$ , the first and second terms are bounded by $\\varepsilon$ an d \u2225G2+C\u03c3\u22251\u00b7 \u03b5, respectively. This concludes the proof. ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The abstract and introduction accurately reflect the paper\u2019s contributions regarding the analysis of Adam with EMA for nonconvex and nonsmooth optimization. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: The paper discuss the limitations of the work in Section 7. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The assumptions and the proofs are provided both in the main text and the appendix. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This paper is a theory paper and does not have any experiments. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] Justification: This paper is a theory paper and does not have any experiments. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper is a theory paper and does not have any experiments. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper is a theory paper and does not have any experiments. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper is a theory paper and does not have any experiments. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The research adheres to the NeurIPS Code of Ethics. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper is a theory paper, and we do not forsee any direct societal implications arising from the research. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 22}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ", "page_idx": 22}, {"type": "text", "text": "\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper is a theory paper and does not have any experiments. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper accurately credits the existing results by citing the papers that inspired the techniques used in this work. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not introduce any new assets. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]