[{"figure_path": "aNHEqFMS0N/tables/tables_4_1.jpg", "caption": "Table 1: Results (%) on \u201cLost in the Middle\u201d. \u201cPosition\u201d indicates the correct answers' index, and each index comprises 500 samples. All results are fine-tuned on Llama-2-7B with 4K length data.", "description": "This table presents the results of the experiment on the \"Lost-in-the-Middle\" task.  The task evaluates the ability of different models to retrieve information from various positions within a long sequence.  The table compares the performance of PoSE and CREAM with different interpolation methods (Linear, NTK, YaRN) across five key positions (0, 18, 37, 54, 74) for sequences of approximately 5K tokens and five key positions (0, 34, 69, 104, 139) for sequences of approximately 10K tokens. The average performance across all positions is also provided for each model and interpolation method.  The results highlight the improvement of CREAM over PoSE.", "section": "3.2 Effective Context Window Size Evaluation on CREAM-Base"}, {"figure_path": "aNHEqFMS0N/tables/tables_5_1.jpg", "caption": "Table 1: Results (%) on \u201cLost in the Middle\u201d. \u201cPosition\u201d indicates the correct answers' index, and each index comprises 500 samples. All results are fine-tuned on Llama-2-7B with 4K length data.", "description": "This table presents the performance of different models on the \"Lost in the Middle\" task, a key-value retrieval task assessing a model's ability to accurately retrieve information located at various positions within an extended context.  The table shows the accuracy of POSE and CREAM models with different positional interpolation methods (Linear, NTK, YaRN) at various key positions (0, 18, 37, 54, 74 for the ~5K token length; 0, 34, 69, 104, 139 for the ~10K token length). The average accuracy across all positions is also provided.  The results highlight CREAM's improved performance over POSE, particularly in retrieving information from the middle of the context.", "section": "3.2 Effective Context Window Size Evaluation on CREAM-Base"}, {"figure_path": "aNHEqFMS0N/tables/tables_6_1.jpg", "caption": "Table 2: Results (%) on LongBench. * indicates results reported by Bai et al. [2023]. CREAM-7B-32k is instruction-tuned for 100 steps using 4K length data on LLaMa2-7B-Chat.", "description": "This table presents the performance of different language models on the LongBench benchmark.  The models are evaluated on various subtasks, including single-document question answering, multi-document question answering, summarization, few-shot learning, code completion, and synthetic tasks. The results show CREAM-7B-32k outperforms other models, particularly in code completion, highlighting its capability in handling extended context lengths.", "section": "3.3 Long Context Understanding Evaluation on CREAM-Chat"}, {"figure_path": "aNHEqFMS0N/tables/tables_6_2.jpg", "caption": "Table 2: Results (%) on LongBench. * indicates results reported by Bai et al. [2023]. CREAM-7B-32k is instruction-tuned for 100 steps using 4K length data on LLaMa2-7B-Chat.", "description": "This table presents the results of evaluating the CREAM-7B-32k model on the LongBench benchmark, comparing its performance to other models such as Llama2-7B-chat, XGen-7B-8k, Mistral models, InternLM-7B-8k, Vicuna-v1.5-7B-16k, and LongChat-v1.5-7B-32k. The evaluation covers various subtasks including single-document QA, multi-document QA, summarization, few-shot learning, code completion, and synthetic tasks.  The CREAM-7B-32k model shows competitive or superior performance compared to other models, especially considering it only underwent 100 steps of instruction tuning.", "section": "3.3 Long Context Understanding Evaluation on CREAM-Chat"}, {"figure_path": "aNHEqFMS0N/tables/tables_7_1.jpg", "caption": "Table 1: Results (%) on \u201cLost in the Middle\u201d. \u201cPosition\u201d indicates the correct answers' index, and each index comprises 500 samples. All results are fine-tuned on Llama-2-7B with 4K length data.", "description": "This table presents the results of the experiment on the \"Lost in the Middle\" task.  The task evaluates the ability of different models to retrieve information located in the middle of a long sequence. The table shows the accuracy of different models (POSE and CREAM with various interpolation methods) at different positions within the sequence.  The data is from Llama-2-7B models fine-tuned with a 4K context window.", "section": "3.2 Effective Context Window Size Evaluation on CREAM-Base"}, {"figure_path": "aNHEqFMS0N/tables/tables_7_2.jpg", "caption": "Table 1: Results (%) on \u201cLost in the Middle\u201d. \u201cPosition\u201d indicates the correct answers' index, and each index comprises 500 samples. All results are fine-tuned on Llama-2-7B with 4K length data.", "description": "This table presents the results of evaluating different models on the \"Lost in the Middle\" task.  The task assesses a model's ability to retrieve information from the middle of an extended context. The table shows the accuracy (%) of different models (POSE-Linear, CREAM-Linear, POSE-NTK, CREAM-NTK, POSE-YaRN, CREAM-YaRN) at retrieving information located at various positions (0, 18, 37, 54, 74, and 0, 34, 69, 104, 139) within the context. The average accuracy across all positions is also provided for each model. All models were fine-tuned on Llama-2-7B with a context window size of 4K tokens.", "section": "3.2 Effective Context Window Size Evaluation on CREAM-Base"}, {"figure_path": "aNHEqFMS0N/tables/tables_8_1.jpg", "caption": "Table 1: Results (%) on \u201cLost in the Middle\u201d. \u201cPosition\u201d indicates the correct answers' index, and each index comprises 500 samples. All results are fine-tuned on Llama-2-7B with 4K length data.", "description": "This table presents the results of the \"Lost in the Middle\" experiment.  It shows the accuracy of retrieving information from different positions within a long sequence (75 and 140 keys, corresponding to approximately 5K and 10K tokens).  The models were fine-tuned on Llama-2-7B with a 4K context window. The table compares the performance of several positional interpolation methods (Linear, NTK, YaRN) applied to both the PoSE and CREAM models across various key positions within the sequence. The average accuracy for each method across all positions is also reported.", "section": "3.2 Effective Context Window Size Evaluation on CREAM-Base"}, {"figure_path": "aNHEqFMS0N/tables/tables_15_1.jpg", "caption": "Table 7: Perplexity results of GovReport and Proof-pile. Each experiment is the average perplexity of 50 samples, and all results are based on Baichuan2-7B fine-tuned on 4K data length.", "description": "This table presents the perplexity scores achieved by the original model and CREAM-Linear model on the GovReport and Proof-pile datasets.  The perplexity is calculated for different context window sizes (4K, 8K, 16K, 32K tokens). Lower perplexity indicates better performance. The results demonstrate the effectiveness of CREAM-Linear in reducing perplexity, especially as the context window size increases.", "section": "3.5 Language Modeling and Standard Benchmark"}, {"figure_path": "aNHEqFMS0N/tables/tables_15_2.jpg", "caption": "Table 1: Results (%) on \u201cLost in the Middle\u201d. \u201cPosition\u201d indicates the correct answers' index, and each index comprises 500 samples. All results are fine-tuned on Llama-2-7B with 4K length data.", "description": "This table presents the results of the \"Lost in the Middle\" experiment, which evaluates the ability of different models to retrieve information from various positions within a long context.  The experiment uses Llama-2-7B models fine-tuned with a 4K context window. Each position index contains 500 samples. The table compares the performance of POSE and CREAM models using different interpolation methods (Linear, NTK, and YaRN).  The results demonstrate CREAM's superior performance, particularly in retrieving information from the middle of the context.", "section": "3.2 Effective Context Window Size Evaluation on CREAM-Base"}, {"figure_path": "aNHEqFMS0N/tables/tables_16_1.jpg", "caption": "Table 9: Experimental results (%) of the LongBench subtasks selected in Zhang et al. [2024]. \u2020 indicates results quoted from Zhang et al. [2024]. Len represents the context length during fine-tuning. All results are based on Llama 2-7B.", "description": "This table presents the experimental results on 12 selected subtasks from the LongBench benchmark. The results are compared across different models and different context lengths during fine-tuning. The table helps to understand the performance of various models on different subtasks, showing how the context length impacts their results.  The models compared include methods such as NTK-by-Parts, Yarn, ABF, EABF, and CREAM, all based on the Llama 2-7B model. ", "section": "3.3 Long Context Understanding Evaluation on CREAM-Chat"}, {"figure_path": "aNHEqFMS0N/tables/tables_16_2.jpg", "caption": "Table 1: Results (%) on \u201cLost in the Middle\u201d. \u201cPosition\u201d indicates the correct answers' index, and each index comprises 500 samples. All results are fine-tuned on Llama-2-7B with 4K length data.", "description": "This table presents the results of the \"Lost-in-the-Middle\" experiment.  It evaluates the ability of different models to retrieve information from various positions within a long sequence.  The table shows the accuracy of each model at retrieving information from different positions (0, 18, 37, 54, 74 for the shorter sequence and 0, 34, 69, 104, 139 for the longer sequence),  all of which were fine-tuned using a 4K token context window.  The models compared include POSE with different interpolation methods (Linear, NTK, YaRN) and CREAM with the same interpolation methods. The average accuracy across all positions is also provided for each model and interpolation method.", "section": "3.2 Effective Context Window Size Evaluation on CREAM-Base"}, {"figure_path": "aNHEqFMS0N/tables/tables_16_3.jpg", "caption": "Table 11: Experimental results (%) of the LongBench subtasks.", "description": "This table presents the performance of various LLMs on the LongBench benchmark's subtasks, specifically focusing on few-shot learning, code completion, and synthetic tasks.  The models compared include Llama2-7B-chat-4k, XGen-7B-8k, InternLM-7B-8k, Vicuna-v1.5-7B-16k, LongChat-v1.5-7B-32k, and CREAM. The results are shown as percentages for each subtask, offering a comprehensive comparison of their performance across different capabilities.", "section": "3.3 Long Context Understanding Evaluation on CREAM-Chat"}]