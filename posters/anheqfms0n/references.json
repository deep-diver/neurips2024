{"references": [{"fullname_first_author": "Dawei Zhu", "paper_title": "POSE: Efficient context window extension of LLMs via positional skip-wise training", "publication_date": "2023-00-00", "reason": "This paper proposes a method for extending the context window of LLMs, which is a key problem addressed in the target paper."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-00", "reason": "This paper introduces Llama 2, the base language model used in the target paper's experiments, providing the foundation for the research."}, {"fullname_first_author": "Shouyuan Chen", "paper_title": "Extending context window of large language models via positional interpolation", "publication_date": "2023-06-00", "reason": "This paper is highly relevant because it explores positional interpolation methods to extend context windows, a core technique that is directly compared to and improved upon by the target paper."}, {"fullname_first_author": "Bowen Peng", "paper_title": "NTK-aware scaled RoPE allows Llama models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation", "publication_date": "2023-00-00", "reason": "This paper is cited for its approach to enhancing Llama models, and thus provides crucial background and a comparison point for the target paper's methodology."}, {"fullname_first_author": "Anian Ruoss", "paper_title": "Randomized positional encodings boost length generalization of transformers", "publication_date": "2023-00-00", "reason": "This work is important as it investigates positional encoding, a technique central to the target paper's approach to extending context windows, offering another key comparison point."}]}