[{"figure_path": "opdiIAHfBr/figures/figures_0_1.jpg", "caption": "Figure 1: Transform the hidden channel activation of deep-nets into visual brain voxels' response.", "description": "This figure illustrates the core concept of the AlignedCut method. It shows how the hidden channel activations (768D) from a deep neural network are linearly transformed into the visual brain voxels' response. The transformation enables a mapping between the network's feature space and the brain's activity, which is used to guide channel alignment and visual concept discovery.", "section": "Introduction"}, {"figure_path": "opdiIAHfBr/figures/figures_1_1.jpg", "caption": "Figure 2: From the 768D feature on CLIP layer-6, we extract different levels of segmentation by restricting the use of a subset of channels. Left: Channel activation on example image patches. The ordering of channels is sorted from the early brain to the late brain by their weights for brain voxels. Right: Spectral clustering on each subset of channels filtered by each brain ROI (V1, V4, EBA), image pixels colored by 3D spectral-tSNE of top 10 eigenvectors.", "description": "This figure shows how the authors extract different levels of image segmentation by selecting subsets of channels from the 768-dimensional feature space of CLIP layer 6.  The left panel displays channel activations on example image patches, with channels ordered from early brain regions to later brain regions based on their fMRI weights. The right panel shows the results of spectral clustering applied to channel subsets filtered by brain regions of interest (ROIs) - V1, V4, and EBA.  The image pixels are colored using 3D spectral t-distributed Stochastic Neighbor Embedding (t-SNE) of the top 10 eigenvectors, visualizing the clusters generated by the spectral clustering.", "section": "3 Results"}, {"figure_path": "opdiIAHfBr/figures/figures_2_1.jpg", "caption": "Figure 3: Cosine similarity of channel activation on the same image inputs.", "description": "This figure shows the cosine similarity of channel activations on the same image inputs before and after channel alignment.  The before-alignment plot reveals distinct blocks representing different models (CLIP, DINO, MAE), indicating a lack of initial alignment between their channel activations. After alignment, the diagonal shows high similarity, illustrating the effectiveness of the channel alignment process in creating a shared feature space across different models. This alignment is crucial for subsequent visual concept discovery using spectral clustering.", "section": "2 Methods: AlignedCut"}, {"figure_path": "opdiIAHfBr/figures/figures_4_1.jpg", "caption": "Figure 4: Spectral clustering in the universal channel aligned feature space. The image pixels are colored by our approach AlignedCut, the pixel RGB value is assigned by the 3D spectral-tSNE of the top 20 eigenvectors. The coloring is consistent across all images, layers, and models.", "description": "This figure shows the results of spectral clustering performed on a universal channel aligned feature space.  The input images are processed through three different models (CLIP, DINO, MAE) across twelve layers each.  The pixel colors are determined using a 3D t-SNE representation of the top 20 eigenvectors obtained from spectral clustering.  The consistent coloring across models and layers highlights the shared visual concepts discovered by the AlignedCut method.", "section": "3 Results"}, {"figure_path": "opdiIAHfBr/figures/figures_5_1.jpg", "caption": "Figure 5: Unsupervised segmentation scores from spectral clustering on each CLIP layer. ImageNet-segmentation dataset is used with binary figure-ground labels, and the mIoU score peaks plateau from layer-4 to layer-10. In PASCAL VOC with 20 class labels, the mIoU score peaks at layer-9.", "description": "This figure shows the results of unsupervised image segmentation using spectral clustering on different layers of the CLIP model.  Two datasets are used for evaluation: ImageNet-segmentation (binary figure-ground labels) and PASCAL VOC (20 class labels). The mIoU (mean Intersection over Union) score, a measure of segmentation accuracy, is plotted for each layer.  The results indicate that figure-ground segmentation is well-encoded from layer 4 onwards, while object category representation peaks at layers 9 and 10.", "section": "3.1 Figure-ground representation emerge before categories"}, {"figure_path": "opdiIAHfBr/figures/figures_5_2.jpg", "caption": "Figure 6: The figure-ground visual concepts in CLIP layer-5. Left: Mean activation of foreground or background pixels, linearly transformed to the brain\u2019s space. Right: Cosine similarity from one reference pixel marked. The figure-ground visual concepts are agnostic to image categories.", "description": "This figure shows the results of analyzing figure-ground visual concepts in CLIP layer 5.  The left panel displays the mean brain activation for foreground and background pixels after a linear transformation to brain space.  Different brain regions show different activation patterns for foreground and background pixels. The right panel shows a cosine similarity heatmap computed from a single reference pixel. The similarity patterns reveal that the figure-ground visual concept is consistent across different image categories (class-agnostic).", "section": "3.2 Visual concepts: class-agnostic figure-ground"}, {"figure_path": "opdiIAHfBr/figures/figures_6_1.jpg", "caption": "Figure 7: The same figure-ground visual concepts are found in CLIP, DINO and MAE. Left: Mean activation of all foreground (top) and background (bottom) pixels; the three models exhibit similar activation patterns. Right: AlignedCut, pixels colored by 3D spectral-tSNE of the top 20 eigenvectors; the three models show similar grouping colors for foreground pixels.", "description": "This figure demonstrates the consistency of figure-ground visual concepts across different models (CLIP, DINO, MAE).  The left panel shows that the average channel activations for foreground and background pixels in the brain space are similar across the three models. The right panel uses AlignedCut to visualize the concepts with pixels colored according to their 3D spectral-tSNE coordinates based on top 20 eigenvectors; similar color groupings indicate similar visual concepts across models.", "section": "3.2 Visual concepts: class-agnostic figure-ground"}, {"figure_path": "opdiIAHfBr/figures/figures_6_2.jpg", "caption": "Figure 17: Category visual concepts in CLIP Layer 9. Left: Mean activation of all pixels within an Euclidean sphere centered at the visual concept in the 3D spectral-tSNE space; the concepts activate different brain regions. Middle: The standard deviation negatively correlates with absolute mean activations. Right: Spectral clustering, colored by 3D spectral-tSNE of the top 20 eigenvectors.", "description": "This figure shows the results of category visual concept analysis using CLIP layer 9. The left panel displays the mean activation of pixels within a 3D spectral-tSNE space, revealing that different visual concepts activate distinct brain regions. The middle panel demonstrates a negative correlation between the standard deviation and the absolute mean of channel activations, suggesting that more consistent visual concepts have higher channel activation values. Finally, the right panel presents the results of spectral clustering, with pixels colored according to their 3D spectral-tSNE coordinates using the top 20 eigenvectors.", "section": "3.3 Visual concepts: categories"}, {"figure_path": "opdiIAHfBr/figures/figures_7_1.jpg", "caption": "Figure 9: Trajectory of feature progression in layers for six example pixels. Left: 2D spectral-tSNE plot of the top 20 eigenvectors, jointly clustered across all models; the foreground and background pixels bifurcate at CLIP layer-4 and DINO layer-5. Right: Pixels colored by unsupervised segmentation.", "description": "This figure visualizes the transition of pixel features across different layers in CLIP and DINO models using 2D spectral t-SNE. The left panel shows the trajectory of six example pixels in the 2D spectral t-SNE space, revealing a bifurcation of foreground and background pixels at specific layers. The right panel displays the same pixels colored according to unsupervised segmentation results, enhancing the visualization of the separation.", "section": "3.4 Transition of visual concepts over layers"}, {"figure_path": "opdiIAHfBr/figures/figures_8_1.jpg", "caption": "Figure 10: Transition probability of visual concepts from CLIP layer-3 to layer-4. Left: Five visual concepts sampled from CLIP layer-3 and layer-4. Right: Transition probability measured separately for foreground and background pixels; a bifurcation occurs where foreground pixels have more traffic to concept B0, while background pixels have more traffic to concepts B3 and B4.", "description": "This figure visualizes the transition probabilities of visual concepts between CLIP layers 3 and 4.  The left panel shows a 2D t-SNE plot of the top 20 eigenvectors, illustrating the distribution of visual concepts in both layers.  Five example visual concepts (A0-A4) are highlighted in layer 3, and their corresponding concepts (B0-B4) in layer 4 are also marked. The right panel presents transition probability matrices for both foreground and background pixels. Each cell in these matrices shows the probability of a pixel transitioning from a specific concept in layer 3 to another concept in layer 4.  The color intensity represents the probability, with darker colors indicating higher probabilities. The figure demonstrates that the transitions of foreground and background pixels exhibit different patterns, indicating a bifurcation or separation of these two classes in the higher layers.", "section": "3.4 Transition of visual concepts over layers"}, {"figure_path": "opdiIAHfBr/figures/figures_12_1.jpg", "caption": "Figure 11: Brain Region of Interests (ROIs). V1v: ventral stream, V1d: dorsal stream.", "description": "This figure shows the brain regions of interests (ROIs) used in the study.  It's a visual representation of different brain areas and their locations, including V1v (ventral stream of V1), V1d (dorsal stream of V1), V2v, V2d, V3v, V3d, hV4, EBA (extrastriate body area), FBA-1, FBA-2 (fusiform body areas), OFA (occipital face area), FFA-1, FFA-2 (fusiform face areas), OPA (occipital place area), PPA (parahippocampal place area), OWFA (occipital word form area), and VWFA-1, VWFA-2 (visual word form areas). These areas are known to be involved in different aspects of visual processing, from early visual perception (V1-V4) to higher-level object recognition (FFA, EBA) and scene understanding (PPA, OPA). The figure provides a visual map of the brain regions that are critical for processing and integrating visual information. ", "section": "B Brain Region Background Knowledge"}, {"figure_path": "opdiIAHfBr/figures/figures_15_1.jpg", "caption": "Figure 4: Spectral clustering in the universal channel aligned feature space. The image pixels are colored by our approach AlignedCut, the pixel RGB value is assigned by the 3D spectral-tSNE of the top 20 eigenvectors. The coloring is consistent across all images, layers, and models.", "description": "This figure shows the results of spectral clustering applied to a universal channel aligned feature space.  The images are processed and represented by colors from a 3D spectral-tSNE (t-distributed Stochastic Neighbor Embedding) of the top 20 eigenvectors. The consistent coloring across images, layers, and models from different neural networks (CLIP, DINO, MAE) illustrates a shared representation of visual concepts that are consistent regardless of model architecture or layer.", "section": "3 Results"}, {"figure_path": "opdiIAHfBr/figures/figures_16_1.jpg", "caption": "Figure 16: Mean activation of foreground or background pixels at each layer of CLIP, DINO and MAE. Channel activations are linearly transformed to the brain\u2019s space. Large absolute activation value means more consistent visual concepts.", "description": "This figure shows the mean activation of foreground and background pixels for each layer of three different models (CLIP, DINO, and MAE). The activations are linearly transformed into the brain\u2019s space, and the color intensity represents the magnitude of the activation. The figure helps to visualize how consistent the visual concepts are across different layers and models. Strong activations (intense colors) indicate more consistent visual concepts.", "section": "Figure-ground Channel Activation from All Layers and Models"}, {"figure_path": "opdiIAHfBr/figures/figures_17_1.jpg", "caption": "Figure 12: Spectral clustering in the universal channel aligned feature space. The image pixels are colored by our approach AlignedCut, the pixel RGB value is assigned by the 3D spectral-tSNE of the top 20 eigenvectors. The coloring is consistent across all images, layers, and models.", "description": "This figure shows the results of spectral clustering applied to a universal channel aligned feature space.  The pixel colors represent a 3D spectral t-SNE embedding of the top 20 eigenvectors obtained from spectral clustering.  The consistent coloring across all images, layers, and models highlights the stability and reproducibility of the discovered visual concepts.", "section": "3 Results"}, {"figure_path": "opdiIAHfBr/figures/figures_18_1.jpg", "caption": "Figure 4: Spectral clustering in the universal channel aligned feature space. The image pixels are colored by our approach AlignedCut, the pixel RGB value is assigned by the 3D spectral-tSNE of the top 20 eigenvectors. The coloring is consistent across all images, layers, and models.", "description": "This figure shows the results of spectral clustering applied to a universal channel-aligned feature space, visualizing the results using a 3D spectral-tSNE of the top 20 eigenvectors. The colors assigned to pixels by the AlignedCut method remain consistent across different images, layers, and models, showcasing how visual concepts are shared and consistently represented across various levels of abstraction and different network architectures.", "section": "3 Results"}, {"figure_path": "opdiIAHfBr/figures/figures_19_1.jpg", "caption": "Figure 16: Mean activation of foreground or background pixels at each layer of CLIP, DINO and MAE. Channel activations are linearly transformed to the brain\u2019s space. Large absolute activation value means more consistent visual concepts.", "description": "This figure displays the mean activation of foreground and background pixels for each layer of three different models (CLIP, DINO, and MAE).  The activations are linearly transformed into the brain's space, providing a representation of how these models process figure-ground information. The intensity of the color (red/blue) indicates the strength of activation in the brain region; a stronger intensity signifies more consistent visual concepts.", "section": "E Figure-ground Channel Activation from All Layers and Models"}, {"figure_path": "opdiIAHfBr/figures/figures_20_1.jpg", "caption": "Figure 17: Category visual concepts in CLIP Layer 9. Left: Mean activation of all pixels within an Euclidean sphere centered at the visual concept in the 3D spectral-tSNE space; the concepts activate different brain regions. Middle: The standard deviation negatively correlates with absolute mean activations. Right: Spectral clustering, colored by 3D spectral-tSNE of the top 20 eigenvectors.", "description": "This figure presents an analysis of category visual concepts in layer 9 of the CLIP model.  The left panel shows the mean activation of pixels grouped around visual concepts in a 3D spectral-tSNE space, demonstrating that these concepts activate distinct brain regions. The middle panel illustrates the negative correlation between the standard deviation and the absolute mean of channel activations for each concept, indicating that concepts with higher activation values tend to be more consistent. Finally, the right panel displays the results of spectral clustering, with pixels colored according to their position in the 3D spectral-tSNE space based on the top 20 eigenvectors.", "section": "3.3 Visual concepts: categories"}, {"figure_path": "opdiIAHfBr/figures/figures_21_1.jpg", "caption": "Figure 9: Trajectory of feature progression in layers for six example pixels. Left: 2D spectral-tSNE plot of the top 20 eigenvectors, jointly clustered across all models; the foreground and background pixels bifurcate at CLIP layer-4 and DINO layer-5. Right: Pixels colored by unsupervised segmentation.", "description": "This figure visualizes how the representation of foreground and background pixels changes across different layers of three different models (CLIP, DINO, MAE).  The left panel shows a 2D spectral t-SNE plot, illustrating the trajectory of six example pixels as they progress through the network layers. The plot reveals a clear separation between foreground and background pixels starting from layer 4 in CLIP and layer 5 in DINO, indicating that the models start to distinctly represent these two elements at these layers.  The right panel displays the same pixels, colored according to an unsupervised segmentation. This provides a visual confirmation of the separation shown in the left panel.", "section": "3.4 Transition of visual concepts over layers"}, {"figure_path": "opdiIAHfBr/figures/figures_22_1.jpg", "caption": "Figure 18: Trajectory of feature progression in from layer to layer, in the 2D spectral-tSNE space. Arrows displayed for 10 randomly sampled example pixels. Top Right: Pixels are colored by unsupervised segmentation.", "description": "This figure visualizes the trajectory of feature progression from layer to layer using 2D spectral-tSNE.  It shows the movement of 10 randomly selected pixels across different layers in CLIP, DINO, and MAE models. The top right shows the pixels colored by unsupervised segmentation results, illustrating how different parts of the image are processed and separated throughout the layers.  The arrows trace the path each pixel takes through the feature space as the network processes the image.", "section": "3.4 Transition of visual concepts over layers"}, {"figure_path": "opdiIAHfBr/figures/figures_23_1.jpg", "caption": "Figure 9: Trajectory of feature progression in layers for six example pixels. Left: 2D spectral-tSNE plot of the top 20 eigenvectors, jointly clustered across all models; the foreground and background pixels bifurcate at CLIP layer-4 and DINO layer-5. Right: Pixels colored by unsupervised segmentation.", "description": "This figure visualizes the movement of image features across layers in different models using 2D spectral t-SNE. The left panel shows the trajectory of six example pixels in the 2D spectral t-SNE space, highlighting the separation of foreground and background pixels at specific layers (CLIP layer-4 and DINO layer-5). The right panel displays the same pixels colored according to unsupervised segmentation results, providing visual context to the feature trajectories.", "section": "3.4 Transition of visual concepts over layers"}, {"figure_path": "opdiIAHfBr/figures/figures_24_1.jpg", "caption": "Figure 9: Trajectory of feature progression in layers for six example pixels. Left: 2D spectral-tSNE plot of the top 20 eigenvectors, jointly clustered across all models; the foreground and background pixels bifurcate at CLIP layer-4 and DINO layer-5. Right: Pixels colored by unsupervised segmentation.", "description": "This figure visualizes how the representation of foreground and background pixels changes across different layers of three different models (CLIP, DINO, and MAE).  The left panel shows a 2D spectral t-SNE plot, illustrating the trajectory of six example pixels through the layers of each model.  The points represent the pixels' features, and the lines trace their evolution.  A key observation is the separation of foreground and background pixels in the later layers of CLIP and DINO. The right panel displays the same pixels, but colored according to the unsupervised segmentation result to emphasize the grouping.", "section": "3.4 Transition of visual concepts over layers"}, {"figure_path": "opdiIAHfBr/figures/figures_25_1.jpg", "caption": "Figure 9: Trajectory of feature progression in layers for six example pixels. Left: 2D spectral-tSNE plot of the top 20 eigenvectors, jointly clustered across all models; the foreground and background pixels bifurcate at CLIP layer-4 and DINO layer-5. Right: Pixels colored by unsupervised segmentation.", "description": "This figure visualizes the transition of feature representations across layers for six example pixels using 2D spectral-tSNE.  The left panel shows the trajectory of each pixel's representation across layers in a 2D spectral-tSNE space, demonstrating that foreground and background pixels separate at different layers (CLIP layer 4 and DINO layer 5). The right panel shows the same pixels colored by their unsupervised segmentation results, providing a visual context to the spectral-tSNE trajectories.", "section": "3.4 Transition of visual concepts over layers"}]