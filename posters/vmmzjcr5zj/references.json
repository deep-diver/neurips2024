{"references": [{"fullname_first_author": "Rob J Hyndman", "paper_title": "Forecasting: principles and practice", "publication_date": "2018", "reason": "This book is a foundational text in the field of time series analysis, providing a comprehensive overview of forecasting methods and techniques."}, {"fullname_first_author": "Rishi Bommasani", "paper_title": "On the opportunities and risks of foundation models", "publication_date": "2021-08-07", "reason": "This paper provides a foundational understanding of the opportunities and risks associated with large pre-trained models, which are the basis of the current work."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020", "reason": "This influential paper demonstrated the capabilities of large language models to perform well on various tasks with minimal fine-tuning, providing motivation for exploring similar pre-trained models for time series data."}, {"fullname_first_author": "Abhimanyu Das", "paper_title": "A decoder-only foundation model for time-series forecasting", "publication_date": "2023-10-10", "reason": "This is a state-of-the-art paper on pre-trained time series models that the current work builds upon and improves on."}, {"fullname_first_author": "Nate Gruver", "paper_title": "Large language models are zero-shot time series forecasters", "publication_date": "2024", "reason": "This paper explores using large language models for time series forecasting, directly motivating the current research by demonstrating the potential of this approach."}]}