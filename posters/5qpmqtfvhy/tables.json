[{"figure_path": "5qPmQtfvhy/tables/tables_4_1.jpg", "caption": "Table 1: Attribution of progress to pre-training algorithmic progress and compute scaling between model pairs based on Shapley decomposition in linear space. Numbers may not all add up to 100% due to rounding. These Shapley values are based on point estimates from our preferred model and as such are meant for illustrative purposes only. We omit parameter efficiency improvements from the table since these are almost always 0% and not very informative. The Transformer here is by Baevski and Auli [2018] (the earliest decoder-only transformer we have in our dataset), who modify the original transformer architecture by Vaswani et al. [2017] to be decoder-only.", "description": "This table shows the Shapley decomposition of progress in pre-training language models between pairs of models.  It breaks down the contributions of algorithmic improvements and compute scaling to the overall performance gains between each model pair.  Note that the Shapley values are based on point estimates and are for illustration only, and parameter efficiency is omitted due to its insignificance. The table focuses on the earliest decoder-only transformer model in the dataset.", "section": "3.2 Most recent performance gains in next-token prediction have been from compute-scaling"}, {"figure_path": "5qPmQtfvhy/tables/tables_6_1.jpg", "caption": "Table 1: Attribution of progress to pre-training algorithmic progress and compute scaling between model pairs based on Shapley decomposition in linear space. Numbers may not all add up to 100% due to rounding. These Shapley values are based on point estimates from our preferred model and as such are meant for illustrative purposes only. We omit parameter efficiency improvements from the table since these are almost always 0% and not very informative. The Transformer here is by Baevski and Auli [2018] (the earliest decoder-only transformer we have in our dataset), who modify the original transformer architecture by Vaswani et al. [2017] to be decoder-only.", "description": "This table shows the decomposition of progress in language models between pairs of models into the contributions of algorithmic progress and compute scaling using the Shapley value decomposition method. The table focuses on the relative contributions of these two factors and omits parameter efficiency due to its negligible contribution.", "section": "3.2 Most recent performance gains in next-token prediction have been from compute-scaling"}, {"figure_path": "5qPmQtfvhy/tables/tables_14_1.jpg", "caption": "Table 1: Attribution of progress to pre-training algorithmic progress and compute scaling between model pairs based on Shapley decomposition in linear space. Numbers may not all add up to 100% due to rounding. These Shapley values are based on point estimates from our preferred model and as such are meant for illustrative purposes only. We omit parameter efficiency improvements from the table since these are almost always 0% and not very informative. The Transformer here is by Baevski and Auli [2018] (the earliest decoder-only transformer we have in our dataset), who modify the original transformer architecture by Vaswani et al. [2017] to be decoder-only.", "description": "This table shows the decomposition of progress into pre-training algorithmic progress and compute scaling for different model pairs.  The Shapley value decomposition method is used to quantify the relative contributions of each factor.  Note that parameter efficiency improvements are omitted due to their consistently negligible values. The table uses the earliest decoder-only transformer model available in the dataset as a reference point for comparison.", "section": "3.2 Most recent performance gains in next-token prediction have been from compute-scaling"}, {"figure_path": "5qPmQtfvhy/tables/tables_15_1.jpg", "caption": "Table 1: Attribution of progress to pre-training algorithmic progress and compute scaling between model pairs based on Shapley decomposition in linear space. Numbers may not all add up to 100% due to rounding. These Shapley values are based on point estimates from our preferred model and as such are meant for illustrative purposes only. We omit parameter efficiency improvements from the table since these are almost always 0% and not very informative. The Transformer here is by Baevski and Auli [2018] (the earliest decoder-only transformer we have in our dataset), who modify the original transformer architecture by Vaswani et al. [2017] to be decoder-only.", "description": "This table presents the results of a Shapley value decomposition analysis of the relative contributions of pre-training algorithmic progress and compute scaling to performance improvements between pairs of language models.  It shows the percentage contribution of each factor for various model pairs, highlighting the increasing importance of compute scaling over time. Note that some values may not sum to 100% due to rounding.", "section": "3.2 Most recent performance gains in next-token prediction have been from compute-scaling"}, {"figure_path": "5qPmQtfvhy/tables/tables_18_1.jpg", "caption": "Table 1: Attribution of progress to pre-training algorithmic progress and compute scaling between model pairs based on Shapley decomposition in linear space. Numbers may not all add up to 100% due to rounding. These Shapley values are based on point estimates from our preferred model and as such are meant for illustrative purposes only. We omit parameter efficiency improvements from the table since these are almost always 0% and not very informative. The Transformer here is by Baevski and Auli [2018] (the earliest decoder-only transformer we have in our dataset), who modify the original transformer architecture by Vaswani et al. [2017] to be decoder-only.", "description": "This table presents the results of a Shapley value decomposition analysis showing the relative contributions of algorithmic progress and compute scaling to the improvements in language model performance between different pairs of models.  The analysis decomposes the progress into percentage contributions from improvements in algorithmic efficiency and the scaling up of compute resources.  It is important to note that the Shapley values are based on point estimates from the authors' preferred model and are intended for illustrative purposes only.", "section": "3.2 Most recent performance gains in next-token prediction have been from compute-scaling"}, {"figure_path": "5qPmQtfvhy/tables/tables_19_1.jpg", "caption": "Table 1: Attribution of progress to pre-training algorithmic progress and compute scaling between model pairs based on Shapley decomposition in linear space. Numbers may not all add up to 100% due to rounding. These Shapley values are based on point estimates from our preferred model and as such are meant for illustrative purposes only. We omit parameter efficiency improvements from the table since these are almost always 0% and not very informative. The Transformer here is by Baevski and Auli [2018] (the earliest decoder-only transformer we have in our dataset), who modify the original transformer architecture by Vaswani et al. [2017] to be decoder-only.", "description": "This table shows the relative contributions of pre-training algorithmic progress and compute scaling to performance gains between pairs of language models.  The values are calculated using a Shapley decomposition, which is a technique for fairly assigning the contribution of each factor when their effects are intertwined. Note that parameter efficiency improvements are not shown because they are nearly always zero. The table uses the earliest decoder-only transformer in the dataset as a reference point.", "section": "3.2 Most recent performance gains in next-token prediction have been from compute-scaling"}, {"figure_path": "5qPmQtfvhy/tables/tables_21_1.jpg", "caption": "Table 1: Attribution of progress to pre-training algorithmic progress and compute scaling between model pairs based on Shapley decomposition in linear space. Numbers may not all add up to 100% due to rounding. These Shapley values are based on point estimates from our preferred model and as such are meant for illustrative purposes only. We omit parameter efficiency improvements from the table since these are almost always 0% and not very informative. The Transformer here is by Baevski and Auli [2018] (the earliest decoder-only transformer we have in our dataset), who modify the original transformer architecture by Vaswani et al. [2017] to be decoder-only.", "description": "This table presents the results of a Shapley value decomposition analysis, which is used to attribute the improvements in language models to either algorithmic progress or compute scaling. The table shows the relative contribution of each factor for several model pairs, considering different starting points and comparing performance gains. Note that due to rounding the values may not always add up to 100%.", "section": "3.2 Most recent performance gains in next-token prediction have been from compute-scaling"}, {"figure_path": "5qPmQtfvhy/tables/tables_21_2.jpg", "caption": "Table 1: Attribution of progress to pre-training algorithmic progress and compute scaling between model pairs based on Shapley decomposition in linear space. Numbers may not all add up to 100% due to rounding. These Shapley values are based on point estimates from our preferred model and as such are meant for illustrative purposes only. We omit parameter efficiency improvements from the table since these are almost always 0% and not very informative. The Transformer here is by Baevski and Auli [2018] (the earliest decoder-only transformer we have in our dataset), who modify the original transformer architecture by Vaswani et al. [2017] to be decoder-only.", "description": "This table presents a Shapley decomposition of the relative contributions of algorithmic progress and compute scaling to the performance gains observed between pairs of language models.  It shows the percentage of performance improvement attributed to each factor for various pairs of models, illustrating the changing balance of these two drivers over time. Note that the Shapley values are based on point estimates and may not always sum to 100% due to rounding.", "section": "3.2 Most recent performance gains in next-token prediction have been from compute-scaling"}, {"figure_path": "5qPmQtfvhy/tables/tables_23_1.jpg", "caption": "Table 1: Attribution of progress to pre-training algorithmic progress and compute scaling between model pairs based on Shapley decomposition in linear space. Numbers may not all add up to 100% due to rounding. These Shapley values are based on point estimates from our preferred model and as such are meant for illustrative purposes only. We omit parameter efficiency improvements from the table since these are almost always 0% and not very informative. The Transformer here is by Baevski and Auli [2018] (the earliest decoder-only transformer we have in our dataset), who modify the original transformer architecture by Vaswani et al. [2017] to be decoder-only.", "description": "This table presents a Shapley value decomposition of the contributions of algorithmic progress and compute scaling to the overall progress between different pairs of language models. The Shapley values show the relative contribution of each factor, with parameter efficiency improvements omitted as they are consistently near 0%.  The table also notes that the transformer used refers to a specific decoder-only version from 2018 that modifies the original 2017 transformer.", "section": "3.2 Most recent performance gains in next-token prediction have been from compute-scaling"}, {"figure_path": "5qPmQtfvhy/tables/tables_24_1.jpg", "caption": "Table 1: Attribution of progress to pre-training algorithmic progress and compute scaling between model pairs based on Shapley decomposition in linear space. Numbers may not all add up to 100% due to rounding. These Shapley values are based on point estimates from our preferred model and as such are meant for illustrative purposes only. We omit parameter efficiency improvements from the table since these are almost always 0% and not very informative. The Transformer here is by Baevski and Auli [2018] (the earliest decoder-only transformer we have in our dataset), who modify the original transformer architecture by Vaswani et al. [2017] to be decoder-only.", "description": "This table shows the decomposition of progress in pre-training language models into compute scaling and algorithmic progress for several model pairs.  Shapley values are used to attribute the improvements between models to either compute scaling or algorithmic progress.  Parameter efficiency improvements are not shown because they are consistently near 0%. The earliest decoder-only transformer model is used as a reference point for the transformer architecture.", "section": "3.2 Most recent performance gains in next-token prediction have been from compute-scaling"}, {"figure_path": "5qPmQtfvhy/tables/tables_28_1.jpg", "caption": "Table 1: Attribution of progress to pre-training algorithmic progress and compute scaling between model pairs based on Shapley decomposition in linear space. Numbers may not all add up to 100% due to rounding. These Shapley values are based on point estimates from our preferred model and as such are meant for illustrative purposes only. We omit parameter efficiency improvements from the table since these are almost always 0% and not very informative. The Transformer here is by Baevski and Auli [2018] (the earliest decoder-only transformer we have in our dataset), who modify the original transformer architecture by Vaswani et al. [2017] to be decoder-only.", "description": "This table shows the relative contributions of pre-training algorithmic progress and compute scaling to the performance gains between pairs of language models.  The Shapley decomposition method is used to attribute the improvements. Note that parameter efficiency gains are omitted due to near-zero values in most cases. The table also specifies the specific transformer model used as a reference point.", "section": "3.2 Most recent performance gains in next-token prediction have been from compute-scaling"}, {"figure_path": "5qPmQtfvhy/tables/tables_29_1.jpg", "caption": "Table 1: Attribution of progress to pre-training algorithmic progress and compute scaling between model pairs based on Shapley decomposition in linear space. Numbers may not all add up to 100% due to rounding. These Shapley values are based on point estimates from our preferred model and as such are meant for illustrative purposes only. We omit parameter efficiency improvements from the table since these are almost always 0% and not very informative. The Transformer here is by Baevski and Auli [2018] (the earliest decoder-only transformer we have in our dataset), who modify the original transformer architecture by Vaswani et al. [2017] to be decoder-only.", "description": "This table shows the relative contribution of algorithmic progress and compute scaling to the performance improvement between pairs of language models.  The Shapley decomposition method is used to fairly allocate the progress between the two factors.  Parameter efficiency is omitted because it's consistently negligible. The table highlights the significant impact of the Transformer architecture.", "section": "3.2 Most recent performance gains in next-token prediction have been from compute-scaling"}, {"figure_path": "5qPmQtfvhy/tables/tables_31_1.jpg", "caption": "Table 1: Attribution of progress to pre-training algorithmic progress and compute scaling between model pairs based on Shapley decomposition in linear space. Numbers may not all add up to 100% due to rounding. These Shapley values are based on point estimates from our preferred model and as such are meant for illustrative purposes only. We omit parameter efficiency improvements from the table since these are almost always 0% and not very informative. The Transformer here is by Baevski and Auli [2018] (the earliest decoder-only transformer we have in our dataset), who modify the original transformer architecture by Vaswani et al. [2017] to be decoder-only.", "description": "This table shows the relative contributions of algorithmic progress and compute scaling to the performance improvements observed between pairs of language models.  The Shapley decomposition method is used to attribute the gains. Note that parameter efficiency improvements are omitted because they are consistently near 0%. The table uses the earliest decoder-only transformer model as a reference point.", "section": "3.2 Most recent performance gains in next-token prediction have been from compute-scaling"}]