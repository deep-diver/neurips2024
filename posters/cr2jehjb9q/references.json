{"references": [{"fullname_first_author": "A. Zeng", "paper_title": "Are transformers effective for time series forecasting?", "publication_date": "2022", "reason": "This paper is foundational to the work presented, investigating the effectiveness of transformers for time series forecasting, a core topic of the current research."}, {"fullname_first_author": "H. Zhou", "paper_title": "Informer: Beyond efficient transformer for long sequence time-series forecasting", "publication_date": "2021", "reason": "This paper introduces the Informer model, a highly influential architecture for long sequence time series forecasting that is directly compared in the current research."}, {"fullname_first_author": "H. Wu", "paper_title": "Autoformer: Decomposition transformers with autocorrelation for long-term series forecasting", "publication_date": "2022", "reason": "The Autoformer model, presented here, offers another significant architecture for long-term time series forecasting, directly relevant to the current work."}, {"fullname_first_author": "J. Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020", "reason": "This work establishes scaling laws for neural language models, which serves as a theoretical foundation for understanding the scaling behavior in other domains such as time series forecasting."}, {"fullname_first_author": "Y. Bahri", "paper_title": "Explaining neural scaling laws", "publication_date": "2024", "reason": "This paper provides a theoretical explanation for neural scaling laws, a crucial concept explored and extended in this work's investigation of scaling laws in time series forecasting."}]}