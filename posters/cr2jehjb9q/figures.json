[{"figure_path": "Cr2jEHJB9q/figures/figures_6_1.jpg", "caption": "Figure 1: Data Scaling. The proposed formula loss(D) = A + B/D\u00ba fits well. More comparison with other formulas can be found at Appendix I.", "description": "This figure shows the results of experiments on data scaling.  The x-axis represents the percentage of data used for training, and the y-axis represents the loss.  The figure shows that the proposed formula, loss(D) = A + B/D^\u03b1, provides a good fit for the data, where D represents the dataset size and \u03b1 is a scaling exponent. Different lines on the graph represent different time series forecasting models and datasets. The caption indicates that additional comparisons with other formulas can be found in Appendix I of the paper.", "section": "4.1 Scaling Law for dataset size and model width do exist for Time Series Forecasting"}, {"figure_path": "Cr2jEHJB9q/figures/figures_6_2.jpg", "caption": "Figure 2: Width Scaling. When the model is not powerful enough, loss(W) = A + B/W\u00ba fits well for these situations. When data is scarce, a large model may lead to overfitting, as observed with ModernTCN on ETTm1.", "description": "This figure shows the results of experiments on the impact of model width on the loss function for time series forecasting.  It confirms that increasing model width generally reduces loss, following a power law relationship (loss(W) = A + B/W^\u03b1) when the model is not too large.  However, when data is scarce (as with the ModernTCN model on the ETTm1 dataset), very large models can lead to overfitting and increased loss, indicating an optimal model width exists.", "section": "4.1 Scaling Law for dataset size and model width do exist for Time Series Forecasting"}, {"figure_path": "Cr2jEHJB9q/figures/figures_7_1.jpg", "caption": "Figure 3: Loss v.s. Horizon for a certain amount of training data, for different datasets and different models.", "description": "This figure displays the relationship between loss and horizon for various datasets and models, each with a specific amount of training data.  It demonstrates how the optimal horizon (the point at which loss is minimized) changes depending on the amount of available training data.  The figure helps visualize the impact of the look-back horizon on model performance in time series forecasting. Different lines represent different amounts of training data used for each model.", "section": "4.2 The Impact of Horizon to Final Loss"}, {"figure_path": "Cr2jEHJB9q/figures/figures_7_2.jpg", "caption": "Figure 1: Data Scaling. The proposed formula loss(D) = A + B/D\u00ba fits well. More comparison with other formulas can be found at Appendix I.", "description": "This figure shows the results of experiments on data scaling in time series forecasting.  It validates the scaling law theory proposed in the paper by demonstrating a relationship between dataset size (D) and loss. The formula  loss(D) = A + B/D\u00ba, where A and B are constants and \u03b1 is the scaling exponent, is shown to provide a good fit for the data.  Appendix I contains additional comparisons of this formula with other possible formulas.", "section": "4.1 Scaling Law for dataset size and model width do exist for Time Series Forecasting"}, {"figure_path": "Cr2jEHJB9q/figures/figures_8_1.jpg", "caption": "Figure 1: Data Scaling. The proposed formula loss(D) = A + B/D\u00ba fits well. More comparison with other formulas can be found at Appendix I.", "description": "This figure shows the results of experiments on data scaling for time series forecasting.  The x-axis represents the percentage of data used, and the y-axis represents the loss.  Multiple lines represent different models tested on various datasets. The caption indicates that the proposed formula, loss(D) = A + B/D^\u03b1, provides a good fit to the data, and additional comparisons using different formulas are available in Appendix I.  The figure demonstrates that performance improves with increasing data size, which is a key finding of the scaling law.", "section": "4.1 Scaling Law for dataset size and model width do exist for Time Series Forecasting"}, {"figure_path": "Cr2jEHJB9q/figures/figures_8_2.jpg", "caption": "Figure 1: Data Scaling. The proposed formula loss(D) = A + B/D\u00ba fits well. More comparison with other formulas can be found at Appendix I.", "description": "This figure displays the results of experiments on data scaling in time series forecasting. It shows the relationship between the size of the dataset (D) and the loss for different models on various datasets (Traffic, Weather, ETTh1, ETTh2, ETTm1, ETTm2). The results demonstrate that the proposed formula loss(D) = A + B/D^\u03b1 provides a good fit for the observed data.  Appendix I contains a more detailed comparison with other formulas.", "section": "4.1 Scaling Law for dataset size and model width do exist for Time Series Forecasting"}, {"figure_path": "Cr2jEHJB9q/figures/figures_21_1.jpg", "caption": "Figure 7: MSE loss v.s. interpolate-to-len (length after downsampling) for Traffic and Weather dataset for a certain amount of training data.", "description": "This figure shows the impact of downsampling on the MSE loss for Traffic and Weather datasets.  The x-axis represents the length after downsampling, while the y-axis represents the MSE loss. Different colored lines represent different amounts of training data used.  The figure visually demonstrates how downsampling affects model performance, which is explained in Section F of the paper.", "section": "F Downsampling May Improve Performance: power of patch, low-pass-filter, and downsampling"}, {"figure_path": "Cr2jEHJB9q/figures/figures_21_2.jpg", "caption": "Figure 1: Data Scaling. The proposed formula loss(D) = A + B/D\u00ba fits well. More comparison with other formulas can be found at Appendix I.", "description": "This figure displays the results of data scaling experiments.  It shows that the relationship between dataset size (D) and loss follows a power law, as indicated by the proposed formula loss(D) = A + B/D^\u03b1. The figure includes multiple subplots, each showing the results for a different time series dataset (Traffic, Weather, ETTh1, ETTh2, ETTm1, ETTm2).  Each subplot shows the loss on the y-axis and the percentage of data used on the x-axis.  The strong fit of the proposed power law formula suggests a scaling law behavior for dataset size in time series forecasting exists.", "section": "4.1 Scaling Law for dataset size and model width do exist for Time Series Forecasting"}, {"figure_path": "Cr2jEHJB9q/figures/figures_22_1.jpg", "caption": "Figure 1: Data Scaling. The proposed formula loss(D) = A + B/D\u00ba fits well. More comparison with other formulas can be found at Appendix I.", "description": "This figure shows the results of experiments on data scaling.  The x-axis represents the percentage of data used, while the y-axis represents the loss. Different lines represent different models (MLP, iTransformer, ModernTCN) applied to different datasets (Traffic, Weather, ETTh1, ETTh2, ETTm1, ETTm2). The figure demonstrates that the proposed formula loss(D) = A + B/D^\u03b1 provides a good fit to the experimental data, validating the scaling law for dataset size in time series forecasting. Appendix I contains further comparisons with other formulas.", "section": "4.1 Scaling Law for dataset size and model width do exist for Time Series Forecasting"}, {"figure_path": "Cr2jEHJB9q/figures/figures_23_1.jpg", "caption": "Figure 10: PCA obtained with MLP features trained on Mixed Dataset. The 2-layer 512-dimension MLP is trained under Channel Independent setting on a mixed dataset of Traffic, Weather, Exchange, ETTh1, ETTh2, ETTm1, ETTm2 and ECL. The mixed feature shows a well-aligned Zip-f law for features of higher rankings.", "description": "This figure displays the results of Principal Component Analysis (PCA) performed on features extracted from a Multilayer Perceptron (MLP) model trained on a mixed dataset. The dataset combines various time series datasets, including Traffic, Weather, Exchange, ETTh1, ETTh2, ETTm1, ETTm2, and ECL.  The analysis reveals that the resulting feature distribution closely follows a Zipf's law, particularly for features with higher rankings. This finding is consistent with the paper's theoretical framework, which postulates that data distribution within the intrinsic space follows a Zipf's law.", "section": "H Experiment results for Zip-f distribution for Mixed Datasets"}]