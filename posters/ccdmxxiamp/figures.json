[{"figure_path": "cCDMXXiamP/figures/figures_6_1.jpg", "caption": "Figure 1: Auditing models with average-case vs worst-case initial parameters at various levels of \u025b.", "description": "This figure compares the empirical privacy leakage (\u03b5_emp) obtained using average-case and worst-case initial model parameters for MNIST and CIFAR-10 datasets at different theoretical privacy levels (\u03b5).  The worst-case parameters, crafted to maximize privacy leakage, result in significantly higher \u03b5_emp values, demonstrating the impact of parameter initialization on the tightness of black-box DP-SGD audits. Error bars represent the standard deviation across multiple runs of the audit procedure.", "section": "5 Experiments"}, {"figure_path": "cCDMXXiamP/figures/figures_6_2.jpg", "caption": "Figure 2: Comparing the average gradient norms to empirical privacy leakage, \nEemp, for models trained on MNIST at \u03b5 = 10.0 with increasing number of pre-training epochs.", "description": "This figure shows the relationship between the average gradient norm and the empirical privacy leakage (\u03b5emp) for models trained on the MNIST dataset with a theoretical privacy parameter \u03b5 of 10.0.  The x-axis represents the number of pre-training epochs used to craft the worst-case initial model parameters.  The y-axis shows both the average gradient norm (in maroon) and the empirical privacy leakage (in light blue). The figure demonstrates that as the number of pre-training epochs increases, the average gradient norm decreases, while the empirical privacy leakage increases. This suggests that minimizing the gradients of normal samples through pre-training makes the target sample's gradient more distinguishable, leading to tighter privacy audits.", "section": "5 Experiments"}, {"figure_path": "cCDMXXiamP/figures/figures_7_1.jpg", "caption": "Figure 3: Auditing models trained on varying dataset sizes (n) at different values of \u03b5. The full dataset size |D| is 30,000 for MNIST and 25,000 for CIFAR-10.", "description": "This figure displays the results of auditing models trained on different dataset sizes (n = 100, n = 1000, n = |D|, where |D| represents the full dataset size) at various privacy levels (\u03b5).  The plot shows the empirical privacy leakage (\u03b5_emp) estimated using the auditing procedure.  The results are shown separately for the MNIST and CIFAR-10 datasets.  The purpose is to analyze how the size of the training dataset impacts the tightness of the privacy audit, comparing smaller subsets to the complete datasets.  The theoretical privacy level (\u03b5) is also shown for comparison.", "section": "5 Experiments"}, {"figure_path": "cCDMXXiamP/figures/figures_8_1.jpg", "caption": "Figure 4: Auditing models trained with varying gradient clipping norm (C) at  \u03b5 = 1.0, 2.0, 4.0, 10.0.", "description": "This figure shows the result of auditing models trained with different gradient clipping norms (C = 0.1, 1.0, 10.0) at various privacy levels (\u03b5). It compares the empirical privacy leakage (\u03b5_emp) against the theoretical privacy guarantee (\u03b5) for both MNIST and CIFAR-10 datasets.  The error bars represent the standard deviation across multiple runs. The figure aims to demonstrate the impact of the gradient clipping norm on the tightness of the black-box audits, showing how smaller clipping norms lead to tighter audits.", "section": "5.2 Full Model Training"}, {"figure_path": "cCDMXXiamP/figures/figures_8_2.jpg", "caption": "Figure 1: Auditing models with average-case vs worst-case initial parameters at various levels of \u03b5.", "description": "This figure compares the empirical privacy leakage (\u03b5_emp) obtained using average-case and worst-case initial model parameters for different theoretical privacy levels (\u03b5).  The worst-case parameters were crafted to maximize the privacy leakage.  The figure shows that using worst-case initial parameters leads to significantly tighter audits (\u03b5_emp closer to the theoretical \u03b5) than using average-case parameters, especially at higher values of \u03b5. Error bars represent standard deviation across five independent runs.", "section": "5 Experiments"}, {"figure_path": "cCDMXXiamP/figures/figures_12_1.jpg", "caption": "Figure 6: Test accuracies (%) on CIFAR-10 for models trained for varying number of iterations, T and learning rates, \u03b7 at a fixed \u03b5 = 10.0.", "description": "This figure shows the test accuracy of models trained on the CIFAR-10 dataset with different learning rates (\u03b7 = 2.0, 4.0, and 10.0) and varying numbers of iterations (T). The privacy parameter (\u03b5) was fixed at 10.0 for all experiments.  The plot illustrates how the model's performance improves with increasing training iterations, and how the optimal learning rate may change depending on the number of iterations.  Higher learning rates can lead to faster initial improvement but might result in lower accuracy at convergence.", "section": "5.2 Full Model Training"}, {"figure_path": "cCDMXXiamP/figures/figures_13_1.jpg", "caption": "Figure 7: Comparing the tightness of  \u03b5emp for different factors at different values of \u03b5", "description": "This figure compares the empirical privacy leakage (\u03b5emp) against the theoretical privacy parameter (\u03b5) for different experimental settings.  Subfigure (a) shows the effect of varying the gradient clipping norm (C) on the tightness of the audits.  Subfigure (b) shows the impact of different dataset sizes (n) on \u03b5emp, comparing results from using 100, 1000 and the full dataset size. The results are presented for four different values of the theoretical privacy parameter \u03b5 (1.0, 2.0, 4.0, 10.0). Error bars represent the standard deviation across five independent runs.", "section": "5 Experiments"}]