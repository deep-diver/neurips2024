[{"figure_path": "4sueqIwb4o/tables/tables_23_1.jpg", "caption": "Table 1: Result of episode reward, step size = 0.1. The columns correspond to \u03b7, and rows correspond to number of tiles.", "description": "This table presents the results of an experiment on the Mountain Car environment.  The experiment compares the performance of the RegQ algorithm with different values of the regularization coefficient (\u03b7) and varying numbers of tiles used in tile coding to discretize the state space.  The episode reward is shown, averaged over 100 test runs for each condition, demonstrating the effect of hyperparameter tuning on performance.", "section": "B.6 Mountain car [Sutton and Barto, 2018] experiment"}, {"figure_path": "4sueqIwb4o/tables/tables_25_1.jpg", "caption": "Table 1: Result of episode reward, step size = 0.1. The columns correspond to \u03b7, and rows correspond to number of tiles.", "description": "This table presents the results of episode rewards from the Mountain Car experiment.  The step size used was 0.1. Each row represents a different number of tiles used in the tile coding for state discretization, while each column shows results for different values of the regularization coefficient \u03b7.  The values are averages \u00b1 standard deviations from 100 runs. The table demonstrates how the reward (which is negative in this case, reflecting penalty for each step) varies with differing levels of state discretization and regularization.", "section": "B.6 Mountain car [Sutton and Barto, 2018] experiment"}]