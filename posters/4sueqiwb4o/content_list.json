[{"type": "text", "text": "Regularized Q-Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Han-Dong Lim Electrical Engineering, KAIST limaries30@kaist.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Donghwan Lee Electrical Engineering, KAIST donghwan@kaist.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Q-learning is widely used algorithm in reinforcement learning (RL) community. Under the lookup table setting, its convergence is well established. However, its behavior is known to be unstable with the linear function approximation case. This paper develops a new Q-learning algorithm, called RegQ, that converges when linear function approximation is used. We prove that simply adding an appropriate regularization term ensures convergence of the algorithm. Its stability is established using a recent analysis tool based on switching system models. Moreover, we experimentally show that RegQ converges in environments where Q-learning with linear function approximation was known to diverge. An error bound on the solution where the algorithm converges is also given. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recently, RL has shown great success in various fields. For instance, Mnih et al. [2015] achieved human level performance in several video games in the Atari benchmark [Bellemare et al., 2013]. Since then, researches on deep RL algorithms have shown significant progresses [Lan et al.]. Although great success has been achieved in practice, there is still gap between theory and the practical success. Especially when off-policy, function approximation, and bootstrapping are used together, the algorithm may show unstable behaviors. This phenomenon is called the deadly triad [Sutton and Barto, 2018]. Famous counter-examples are given in Baird [1995], Tsitsiklis and Van Roy [1997]. ", "page_idx": 0}, {"type": "text", "text": "For policy evaluation, especially for temporal-difference (TD) learning algorithm, there has been several algorithms to resolve the deadly triad issue. Bradtke and Barto [1996] uses the least-square method to compute a solution of TD-learning, but it suffers from $O(h^{2})$ time complexity, where $h$ is number of features. Maei [2011], Sutton et al. [2009] developed gradient descent based methods which minimize the mean square projected Bellman error. Ghiassian et al. [2020] added regularization term to TD Correction (TDC) algorithm, which uses a single time scale step-size. Lee et al. [2022] introduced several variants of the gradient TD (GTD) algorithm under control theoretic frameworks. Sutton et al. [2016] re-weights some states to match the on-policy distribution to stabilize the off-policy TD-learning. Bharadwaj Diddigi et al. [2020] uses $l_{2}$ regularization to propose a new convergent off-policy TD-learning algorithm. Mahadevan et al. [2014] studied regularization on the off-policy TD-learning through the lens of primal dual method. ", "page_idx": 0}, {"type": "text", "text": "First presented by Watkins and Dayan [1992], Q-learning also suffers from divergence issues under the deadly triad. While there are convergence results under the look-up table setting [Watkins and Dayan, 1992, Jaakkola et al., 1994, Borkar and Meyn, 2000, Lee and He, 2020], even with the simple linear function approximation, the convergence is only guaranteed under strong assumptions [Melo et al., 2008, Lee and He, 2020, Yang and Wang, 2019]. ", "page_idx": 0}, {"type": "text", "text": "The main goal of this paper is to propose a practical Q-learning algorithm, called regularized Qlearning (RegQ), that guarantees convergence under linear function approximation. We prove its convergence using the ordinary differential equation (O.D.E) analysis framework in Borkar and Meyn [2000] together with the switching system approach developed in Lee and He [2020]. As in Lee and He [2020], we construct upper and lower comparison systems, and prove its global asymptotic stability based on switching system theories. Compared to the standard Q-learning in Watkins and Dayan [1992], a difference lies in the additional $l_{2}$ regularization term, which makes the algorithm relevantly simple. Moreover, compared to the previous works in Carvalho et al. [2020], Maei et al. [2010], our algorithm is single time-scale, and hence, shows faster convergence rates experimentally. Our algorithm directly uses bootstrapping rather than circumventing the issue in the deadly triad. Therefore, it could give a new insight into training reinforcement learning algorithms with function approximation without using the so-called target network technique introduced in Mnih et al. [2015]. The main contributions of this paper are summarized as follows: ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "1. A new single time-scale Q-learning algorithm with linear function approximation is proposed.   \n2. We provide a theoretical analysis on the solution of the projected Bellman equation where a regularization term is included.   \n3. We prove the convergence of the proposed algorithm based on the O.D.E approach together with the switching system model in Lee and He [2020].   \n4. We experimentally show that our algorithm performs faster than other two time-scale Qlearning algorithms in Carvalho et al. [2020], Maei et al. [2010]. ", "page_idx": 1}, {"type": "text", "text": "Related works: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Several works [Melo et al., 2008, Lee and He, 2020, Yang and Wang, 2019] have relied on strong assumptions to guarantee convergence of Q-learning under linear function approximation. Melo et al. [2008] adopts an assumption on relation between behavior policy and target policy to guarantee convergence, which is not practical in general. Lee and He [2020] assumes a similar assumption to that of Melo et al. [2008] to ensure the convergence with the so-called switching system approach. Yang and Wang [2019] considered a transition matrix that can be represented by the feature values, which restricts the class of Markov chain. ", "page_idx": 1}, {"type": "text", "text": "Motivated by the empirical success of the deep Q-learning in Mnih et al. [2015], recent works in Zhang et al. [2021], Carvalho et al. [2020], Agarwal et al., Chen et al. [2023] use the target network to circumvent the bootstrapping issue and guarantee convergence. Carvalho et al. [2020] designed a two time-scale learning method motivated by the target network method. Zhang et al. [2021] uses $l_{2}$ regularization with the target network, while a projection step is involved, which makes it difficult to implement practically. Moreover, it also relies on a two time-scale learning method. Chen et al. [2023] used target network and truncation method to address the divergence issue. Agarwal et al. additionally uses the so-called experience replay technique with the target network. Furthermore, the optimality is only guaranteed under a specific type of Markov chain. Even though, the target network update can guarantee stability, it often leads to slow convergence rate [Kim et al., 2019]. ", "page_idx": 1}, {"type": "text", "text": "Maei et al. [2010] suggested the so-called Greedy-GQ (gradient Q-learning) algorithm, but due to non-convexity of the objective function, it could converge to a local optima. Lu et al. [2021] used linear programming approach [Manne, 1960] to design convergent Q-learning algorithm under deterministic control systems. Devraj and Meyn [2017] proposed a Q-learning algorithm that minimizes asymptotic variance. However, it requires the assumption that the number of changes of policy are finite, and involves matrix inversion at each iteration. Meyn [2023] introduced an optimistic training scheme with modified Gibbs policy for Q-learning with linear function approximation, which guarantees existence of a solution of the projected Bellman equation, but not the convergence. Geist et al. [2019], Xi et al. [2024] considered regularization on the policy which address a different scenario than the regularization in our work. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries and Notations ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Markov Decision Process ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We consider an infinite horizon Markov Decision Process (MDP), which consists of a tuple $\\mathcal{M}=$ $(S,{\\mathcal{A}},P,r,\\gamma)$ , where the state space $\\boldsymbol{S}$ and action space $\\boldsymbol{\\mathcal{A}}$ are finite sets, $P$ denotes the transition probability, $\\boldsymbol{r}:S\\times\\mathcal{A}\\times\\mathcal{S}\\rightarrow\\mathbb{R}$ is the reward, and $\\gamma\\,\\in\\,(0,1)$ is the discount factor. Given a stochastic policy $\\pi:S\\to{\\mathcal{P}}(A)$ , where $\\mathcal{P}(\\mathcal{A})$ is the set of probability distributions over $\\boldsymbol{\\mathcal{A}}$ , agent at the current state $s_{k}$ selects an action $a_{k}\\sim\\pi(\\cdot|s_{k})$ , then the agent\u2019s state changes to the next state $s_{k+1}\\sim P(\\cdot|s_{k},a_{k})$ , and receives reward $r_{k+1}:=r\\big(s_{k},a_{k},s_{k+1}\\big)$ . A deterministic policy is a special stochastic policy, which can be defined simply as a mapping $\\pi:{\\mathcal{S}}\\rightarrow A$ . ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "The objective of MDP is to find a deterministic optimal policy, denoted by $\\pi^{*}$ , such that the cumulative discounted rewards over infinite time horizons is maximized, i.e., $\\pi^{*}:=$ $\\begin{array}{r l}{\\operatorname{arg\\,max}_{\\pi}\\operatorname{\\mathbb{E}}\\left[\\left.\\sum_{k=0}^{\\infty}\\gamma^{k}r_{k}\\right\\rvert\\pi\\right]}\\end{array}$ , where $(s_{0},a_{0},s_{1},a_{1},\\ldots)$ is a state-action trajectory generated by the Markov chain under policy $\\pi$ , and $\\mathbb{E}[\\cdot|\\pi]$ is an expectation conditioned on the policy $\\pi$ . The Q-function under policy $\\pi$ is defined as $\\begin{array}{r}{Q^{\\pi}(s,\\stackrel{.}{a})=\\mathbb{E}\\left[\\sum_{k=0}^{\\infty}\\gamma^{k}r_{k}\\big|\\,s_{0}=s,a_{0}=a,\\pi\\right]}\\end{array}$ , $(s,a)\\in S\\times A$ , and the optimal Q-function is defined as $Q^{*}(s,a)=Q^{\\pi^{*}}(s,a)$ for all $(s,a)\\in S\\times A$ . Once $Q^{*}$ is known, then an optimal policy can be retrieved by the greedy action, i.e., $\\pi^{*}(s)=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}Q^{*}(s,a)$ . Throughout, we assume that the Markov chain is time homogeneous so that the MDP is well posed, which is standard in the literature. It is known that the optimal Q-function satisfies the so-called Bellman equation expressed as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\nQ^{*}(s,a)=\\mathbb{E}\\left[r_{k+1}+\\operatorname*{max}_{a_{k+1}\\in A}\\gamma Q^{*}(s_{k+1},a_{k+1})\\bigg\\vert\\left(s_{k},a_{k}\\right)=(s,a)\\right]:={\\mathcal{T}}Q^{*}(s,a),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\tau$ is called the Bellman operator. ", "page_idx": 2}, {"type": "text", "text": "2.2 Notations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this paper, we will use an O.D.E. model [Borkar and Meyn, 2000] of Q-learning to analyze its convergence. To this end, it is useful to introduce some notations in order to simplify the overall expressions. Throughout the paper, $e_{a}$ and $e_{s}$ denote $a$ -th and $s$ -th canonical basis vectors in $\\mathbb{R}^{|A|}$ and $\\mathbb{R}^{|\\bar{S}|}$ , respectively, and $\\otimes$ stands for the Kronecker product. Let us introduce the following notations: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P:=\\left[\\begin{array}{l}{P_{1}}\\\\ {\\vdots}\\\\ {P_{|A|}}\\end{array}\\right]\\in\\mathbb{R}^{|\\mathcal{S}||\\mathcal{A}|\\times|\\mathcal{S}|},\\quad R:=\\left[\\begin{array}{l}{R_{1}}\\\\ {\\vdots}\\\\ {R_{|A|}}\\end{array}\\right]\\in\\mathbb{R}^{|\\mathcal{S}||\\mathcal{A}|},\\quad Q:=\\left[\\begin{array}{l}{Q_{1}}\\\\ {\\vdots}\\\\ {Q_{|A|}}\\end{array}\\right]\\in\\mathbb{R}^{|\\mathcal{S}||\\mathcal{A}|},}\\\\ &{D_{a}:=\\left[\\begin{array}{l l l l}{\\!d(1,a)}&&\\\\ &{\\ddots}&\\\\ &&{\\!d(|\\mathcal{S}|,a)\\!\\right]\\in\\mathbb{R}^{|\\mathcal{S}|\\times|\\mathcal{S}|},\\quad D:=\\left[\\begin{array}{l l l l}{\\!D_{1}}&&\\\\ &{\\ddots}&\\\\ &&{\\!D_{|A|}}\\end{array}\\right]\\in\\mathbb{R}^{|\\mathcal{S}||\\mathcal{A}|\\times|\\mathcal{S}||\\mathcal{A}|},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $P_{a}\\in\\mathbb{R}^{|S|\\times|S|},a\\in\\mathcal{A}$ is the state transition matrix whose $i$ -th row and $j$ -th column component denotes the probability of transition to state $j$ when action $a$ is taken at state $i$ , $P^{\\pi}\\in\\mathbb{R}^{|S||A|\\bar{\\times}|S||A|}$ represents the state-action transition matrix under policy $\\pi$ , i.e., ", "page_idx": 2}, {"type": "equation", "text": "$$\n(e_{s}\\otimes e_{a})^{T}P^{\\pi}(e_{s^{\\prime}}\\otimes e_{a^{\\prime}})=\\mathbb{P}[s_{k+1}=s^{\\prime},a_{k+1}=a^{\\prime}|s_{k}=s,a_{k}=a,\\pi],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "$Q_{a}=Q(\\cdot,a)\\in\\mathbb{R}^{|S|},a\\in\\mathcal{A}$ and $R_{a}(s):=\\mathbb{E}[r(s,a,s^{\\prime})|s,a],s\\in\\mathcal{S}$ . Moreover, $d(\\cdot,\\cdot)$ is the stateaction visit distribution, where i.i.d. random variables $\\{(s_{k},a_{k})\\}_{k=0}^{\\infty}$ are sampled, i.e., $d(s,a)=$ ${\\mathbb P}[s_{k}=s,a_{k}=a],\\;(s,a)\\in{\\mathcal S}\\times{\\mathcal A}$ . With a slight abuse of notation, $d$ will be also used to denote the vector $d\\in\\mathbb{R}^{|S||A|}$ such that $d^{T}(e_{s}\\otimes e_{a})=d(s,a)$ , $\\forall(s,a)\\in{\\mathcal{S}}\\times{\\mathcal{A}}$ . In this paper, we represent a policy in a matrix form in order to formulate a switching system model. In particular, for a given policy \u03c0, define the matrix \u03a0\u03c0 \u2208R|S|\u00d7|S||A|: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Pi^{\\pi}:=\\left[\\big(e_{\\pi(1)}\\otimes e_{1}\\big)\\quad\\big(e_{\\pi(2)}\\otimes e_{2}\\big)\\quad\\cdot\\cdot\\cdot\\quad\\big(e_{\\pi(|S|)}\\otimes e_{|S|}\\big)\\right]^{\\top}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Then, we can prove that for any deterministic policy, $\\pi$ , we have $\\begin{array}{r l}{\\Pi^{\\pi}Q}&{{}=}\\end{array}$ $\\begin{array}{r l l l}{[Q(1,\\pi(1))}&{{}Q(2,\\pi(2))}&{\\cdots}&{{}Q(|S|,\\pi(|S|))]^{T}}\\end{array}$ . For simplicity, let $\\Pi_{Q}\\quad:=\\quad\\Pi^{\\pi}$ when $\\pi(s)\\;\\;\\;=\\;\\;\\arg\\operatorname*{max}_{a\\in{\\cal A}}Q(s,a)$ . Moreover, we can prove that for any deterministic policy \u03c0, $\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\$ , where $P^{\\pi}$ is the state-action transition probability matrix. Using the notations introduced, the Bellman equation in (1) can be compactly written as $Q^{*}\\ =\\ \\gamma P\\Pi_{Q^{*}}Q^{*}\\;+\\;R\\;=:\\;{\\mathcal T}Q^{*}$ , where $\\pi_{Q^{*}}$ is the greedy policy defined as $\\pi_{Q^{*}}(s)=\\arg\\operatorname*{max}_{a\\in\\mathcal{A}}Q^{*}\\tilde{(}s,a)$ . ", "page_idx": 2}, {"type": "text", "text": "2.3 Q-learning with linear function approximation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Q-learning is widely used model-free learning to find $Q^{*}$ , whose updates are given as ", "page_idx": 2}, {"type": "equation", "text": "$$\nQ_{k+1}(s_{k},a_{k})\\gets Q_{k}(s_{k},a_{k})+\\alpha_{k}\\delta_{k},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\delta_{k}=r_{k+1}+\\gamma\\operatorname*{max}_{a\\in{\\cal A}}Q_{k}(s_{k+1},a)-Q_{k}(s_{k},a_{k})$ is called the TD error. Each update uses an i.i.d. sample $\\left(s_{k},a_{k},r_{k+1},s_{k+1}\\right)$ , where $(s_{k},a_{k})$ is sampled from a state-action distribution $d(\\cdot,\\cdot)$ . ", "page_idx": 3}, {"type": "text", "text": "Here, we assume that the step-size is chosen to satisfy the so-called the Robbins-Monro condition [Robbins and Monro, 1951], $\\alpha_{k}>0$ , $\\textstyle\\sum_{k=0}^{\\infty}\\alpha_{k}={\\overset{}{\\infty}}$ , $\\textstyle\\sum_{k=0}^{\\infty}\\alpha_{k}^{2}<\\infty$ . When the state-spaces and action-spaces are too large, then the memory and computational complexities usually become intractable. In such a case, function approximation is commonly used to approximate Q-function [Mnih et al., 2015, Hessel et al., 2018]. Linear function approximation is one of the simplest function approximation approaches. In particular, we use the feature matrix $X\\in\\mathbb{R}^{|S||A|\\times h}$ and parameter vector $\\boldsymbol\\theta\\in\\mathbb{R}^{h}$ to approximate Q-function, i.e., $Q\\simeq X\\theta$ , where the feature matrix is expressed as $X:=[x(1,1)\\quad\\cdot\\cdot\\cdot\\quad x(1,|A|)\\quad\\cdot\\cdot\\cdot\\quad x(|S|,|A|)]^{T}\\in\\mathbb{R}^{|S||A|\\times h}$ . Here, $\\boldsymbol{x}(\\cdot,\\cdot)\\in\\mathbb{R}^{h}$ is called the feature vector, and $h$ is a positive integer with $h\\ll|{\\cal S}||{\\cal A}|$ . The corresponding greedy policy becomes $\\pi_{X\\theta}(s)=\\arg\\operatorname*{max}_{a\\in{\\cal{A}}}^{\\;\\;\\cdot}x(s,a)^{T}{\\bar{\\theta}}$ . Note that the number of policies characterized by the greedy policy is finite. This is because the policy is invariant under constant multiplications, and there exists a finite number of sectors on which the policy is invariant. Next, we summarize some standard assumptions adapted throughout this paper. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.1. The state-action visit distribution is positive, i.e., $d(s,a)>0$ for all $(s,a)\\in S\\times A$ . Assumption 2.2. The feature matrix, $X$ , has full column rank, and is a non-negative matrix. Moreover, columns of $X$ are orthogonal. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.3 (Boundedness on feature matrix and reward matrix). There exists constants, $X_{\\operatorname*{max}}>$ 0 and $R_{\\mathrm{max}}>0,$ , such that $\\operatorname*{max}(||X||_{\\infty},||X^{T}||_{\\infty})<X_{\\operatorname*{max}}$ and $||R||_{\\infty}<R_{\\mathrm{max}}$ . ", "page_idx": 3}, {"type": "text", "text": "We note that except for the orthogonality of the feature matrix in Assumption 2.2, the assumptions in the above are commonly adopted in the literature, e.g. Carvalho et al. [2020], Lee and He [2020]. Moreover, under Assumption 2.1, $D$ is a nonsingular matrix with strictly positive diagonal elements. ", "page_idx": 3}, {"type": "text", "text": "Lemma 2.4 (Gosavi [2006]). Under Assumption 2.3, $Q^{*}$ , is bounded, i.e., $\\begin{array}{r}{||Q^{*}||_{\\infty}\\leq\\frac{R_{\\operatorname*{max}}}{1-\\gamma}}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "The proof of Lemma 2.4 comes from the fact that under the discounted infinite horizon setting, $Q^{*}$ can be expressed as an infinite sum of a geometric sequence. ", "page_idx": 3}, {"type": "text", "text": "2.4 Switching System ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this paper, we consider a particular system, called the switched linear system [Liberzon, 2003], ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\dot{x}}_{t}=A_{\\sigma_{t}}x_{t},\\quad x_{0}=z\\in\\mathbb{R}^{n},\\quad t\\in\\mathbb{R}_{+},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $x_{t}\\in\\mathbb{R}^{n}$ is the state, $\\mathcal{M}:=\\{1,2,\\dots,M\\}$ is called the set of modes, $\\sigma_{t}\\in\\mathcal{M}$ is called the switching signal, and $\\{A_{\\sigma},\\sigma\\in\\mathcal{M}\\}$ are called the subsystem matrices. The switching signal can be either arbitrary or controlled by the user under a certain switching policy. ", "page_idx": 3}, {"type": "text", "text": "Stability and stabilization of (3) have been widely studied for decades. Still, finding a practical and effective condition for them is known to be a challenging open problem. Contrary to linear time-invariant systems, even if each subsystem matrix $A_{\\sigma}$ is Hurwitz, the overall switching system may not be stable in general. This tells us that tools in linear system theories cannot be directly applied to conclude the stability of the switching system. ", "page_idx": 3}, {"type": "text", "text": "Another approach is to use the Lyapunov theory [Khalil, 2002]. From standard results in control system theories, finding a Lyapunov function ensures stability of the switching system. If the switching system consists of matrices with strictly negatively row dominant diagonals, defined in Definiiton A.5 in the Appendix, or negative-definite matrices, we can always find a common (piecewise) quadratic Lyapunov function to ensure its stability. We use this fact to prove the convergence of the proposed algorithm. ", "page_idx": 3}, {"type": "text", "text": "Lemma 2.5. Consider a switched system in (3). Suppose one of the following two conditions hold: ", "page_idx": 3}, {"type": "text", "text": "Then, the origin of $(3)$ is asymptotically stable. ", "page_idx": 3}, {"type": "text", "text": "The proof is given in Appendix A.4 ", "page_idx": 3}, {"type": "text", "text": "3 Projected Bellman equation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we introduce the notion of projected Bellman equation with a regularization term, and establish connections between it and the proposed algorithm. Moreover, we briefly discuss the existence and uniqueness of the solution of the projected Bellman equation. We will also provide an example to illustrate the existence and uniqueness of the solution. ", "page_idx": 4}, {"type": "text", "text": "3.1 Projected Bellman equation (PBE) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "When using the linear function approximation, since the true action value may not lie in the subspace spanned by the feature vectors, a solution of the Bellman equation may not exist in general. To resolve this issue, a standard approach is to consider the projected Bellman equation (PBE) defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\nX\\theta^{*}=\\Gamma\\tau X\\theta^{*},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\Gamma:=X(X^{T}D X)^{-1}X^{T}D$ is the weighted Euclidean projection with respect to state-action visit distribution onto the subspace spanned by the feature vectors, and $T X\\theta^{*}=\\gamma P\\Pi_{X\\theta^{*}}X\\theta^{*}+R$ In this case, there are more chances for a solution satisfying the PBE to exist. Still, there may exist cases where the PBE does not admit a solution. To proceed, letting ", "page_idx": 4}, {"type": "equation", "text": "$$\nA_{\\pi_{X\\theta^{*}}}:=X^{T}D X-\\gamma X^{T}D P\\Pi_{X\\theta^{*}}X,\\quad b=X^{T}D R,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "we can rewrite (4) equivalently as ", "page_idx": 4}, {"type": "equation", "text": "$$\nX\\theta^{*}=X(X^{T}D X)^{-1}X^{T}D(\\gamma P\\Pi_{X\\theta^{*}}X\\theta^{*}+R)\\Leftrightarrow A_{\\pi_{X\\theta^{*}}}\\theta^{*}=b,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Furthermore, we use the simplified notation $C:=X^{T}D X$ . A potential deterministic algorithm to solve the above equation is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\theta_{k+1}=\\theta_{k}+\\alpha_{k}(b-A_{\\pi_{X\\theta_{k}}}\\theta_{k}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "It iteratively solves the linear or nonlinear equation, which is a widely used algorithm called a Richardson iteration [Kelley, 1995]. If it converges, i.e., $\\theta_{k}\\to\\theta^{*}$ as $k\\rightarrow\\infty$ , then it is clear that $\\theta^{*}$ solves (5). In this paper, the proposed algorithm is a stochastic algorithm that solves the modified equation ", "page_idx": 4}, {"type": "equation", "text": "$$\nb-(A_{\\pi_{X\\theta_{\\eta}^{*}}}+\\eta I)\\theta_{\\eta}^{*}=0,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $I$ is the $h\\times h$ identity matrix, and $\\eta\\geq0$ is a weight on the regularization term. We can use $\\eta C$ instead of $\\eta I$ as the regularization term but $\\eta C$ is known to solve a MDP with modified discount factor Chen et al. [2023]. Similar to (6), the corresponding deterministic algorithm is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\theta_{k+1}=\\theta_{k}+\\alpha_{k}(b-(A_{\\pi_{X\\theta_{k}}}+\\eta I)\\theta_{k}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "If it converges, i.e., $\\theta_{k}\\to\\theta_{\\eta}^{*}$ as $k\\rightarrow\\infty$ , then it is clear that $\\theta_{\\eta}^{*}$ solves (7). ", "page_idx": 4}, {"type": "text", "text": "3.2 Regularized projected Bellman equation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The equation (7) can be written as the regularized projected Bellman equation (RPBE) ", "page_idx": 4}, {"type": "equation", "text": "$$\nX\\theta_{\\eta}^{*}=\\Gamma_{\\eta}\\tau X\\theta_{\\eta}^{*},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Gamma_{\\eta}:={X}({X}^{\\top}{D}{X}+\\eta{I})^{-1}{X}^{\\top}{D}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The proof of the equivalence between (7) and (9) are given in Lemma A.12 in the Appendix Section A.3. The matrix $\\Gamma_{\\eta}$ can be viewed as a modified projection operator which will be called the regularized projection. It can be interpreted as the projection with a regularization term $\\begin{array}{r}{\\Gamma_{\\eta}(x)=\\arg\\operatorname*{min}_{\\theta\\in\\mathbb{R}^{h}}\\left(\\frac{1}{2}\\left\\|x-X\\theta\\right\\|_{D}^{2}+\\frac{\\eta}{2}\\left\\|\\theta\\right\\|_{2}^{2}\\right)}\\end{array}$ . The concept is illustrated in Figure 1a. Before moving forward, some natural questions that arise here are as follows: How does $\\theta^{*}$ and $\\theta_{\\eta}^{*}$ differ? Furthermore, which conditions can determine the existence and uniqueness of the solution of (4) and (9)? Partial answers are given in the sequel. ", "page_idx": 4}, {"type": "text", "text": "First, let us assume that the solution of (4) and (9), $\\theta^{*}$ and $\\theta_{\\eta}^{*}$ , respectively, exist and are unique. To understand the difference between $\\theta^{*}$ and $\\theta_{\\eta}^{*}$ , an important property of $\\Gamma_{\\eta}$ is introduced: ", "page_idx": 4}, {"type": "image", "img_path": "4sueqIwb4o/tmp/9b4d462ed8afb4ae887a336c8ba5f39df271d5aa5ac6293c3593cd81cca64832.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "(a) Regularized projection: $C(X)$ (b) Regularized projection: One di- (c) Boundedness of the projection means the range space of $X$ mensional case ", "page_idx": 5}, {"type": "text", "text": "Figure 1: Illustrative explanation on the regularized projection. The Figure 1c implies that as $\\eta\\rightarrow\\infty$ , $\\gamma\\Gamma_{\\eta}$ can potentially move outside of the unit ball satisfying $||x||_{\\infty}\\leq1$ , and this phase is indicated with the term \u201cblowing up\u201d phase. The quantity $\\|\\gamma\\Gamma_{\\eta}\\|_{\\infty}$ actually blows up initially as $\\eta\\rightarrow\\infty$ . However, since $\\begin{array}{r}{\\operatorname*{lim}_{\\eta\\to\\infty}\\|\\gamma\\Gamma_{\\eta}\\|_{\\infty}=0}\\end{array}$ , we know that $\\gamma\\Gamma_{\\eta}$ will eventually converge to the origin and move inside the unit ball. This behavior is indicated by the \u201cshrinking\u201d phase in the figure. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.1. (a) The projection $\\Gamma_{\\eta}$ satisfies the following properties: $\\operatorname*{lim}_{\\eta\\to\\infty}\\Gamma_{\\eta}=0$ and $\\operatorname*{lim}_{\\eta\\rightarrow0}\\Gamma_{\\eta}=\\Gamma$ . ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\cdot\\left\\|\\Gamma_{\\eta}\\right\\|_{\\infty}\\leq\\left\\|X^{\\top}D\\right\\|_{2}\\left\\|X\\right\\|_{2}\\left\\|(X^{\\top}D X)^{-1}\\right\\|_{2}\\sqrt{|S\\times A|}\\,f o r\\,a l l\\,\\eta\\geq0.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The proof is given in Appendix A.5. From the above result, one can observe that as $\\eta\\rightarrow\\infty$ , the projection is attracted to the origin as illustrated in Figure 1b. Moreover, as $\\eta\\rightarrow0$ , we will expect that $\\theta_{\\eta}^{\\ast}\\to\\theta^{\\ast}$ . Furthermore, one can observe that the bound in item (b) of Lemma 3.1 cannot be controlled by simply scaling the feature function, and therefore, it more depends on the inherent structures of the feature matrix $X$ . The concept is illustrated in Figure 1c. We will provide a more in-depth discussion on the error bound of $\\theta_{\\eta}^{\\ast}-\\theta^{\\ast}$ in Section 3.3. ", "page_idx": 5}, {"type": "text", "text": "Now, we will discuss the existence and uniqueness of the solutions. Considering the non-existence of the solution of (4) [De Farias and Van Roy, 2000], (9) may not also have a solution. However, for RPBE in (9), we can prove that under mild conditions, its solution exists and is unique. We provided an example where the solution does not exist for (4) but does exist for (9) in Appendix A.14. Let us first state a general condition such that the solution of (9) exists and is unique: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\gamma||\\Gamma_{\\eta}||_{\\infty}<1,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Lemma 3.2. Suppose that $(I I)$ holds. Then the solution of RPBE in (9) exists and is unique. ", "page_idx": 5}, {"type": "text", "text": "The proof is given in Appendix A.6, which uses Banach fixed-point theorem [Agarwal et al., 2018]. From Lemma 3.2, we can see that the condition, $\\gamma||\\Gamma_{\\eta}||_{\\infty}<1$ , is important to guarantee the uniqueness and existence of the solution. We will clarify under what situations the condition (11) can be met, and provide related discussions in Lemma 3.3, 3.4, and 3.5, where each lemma illustrate different scenarios when (11) is met. In particular, Lemma 3.3 shows that with simple feature scaling, $\\eta$ can be easily chosen such that (11) holds. Furthermore, Lemma 3.4 considers a case when $\\eta$ is in a small neighborhood of zero, and Lemma 3.5 considers the case when (11) should hold for all $\\eta\\geq0$ . ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\cdot\\eta>\\gamma||X^{\\top}D||_{\\infty}||X||_{\\infty}+||X^{\\top}D X||_{\\infty},w e\\,h a\\nu e\\,\\gamma\\,\\|\\Gamma_{\\eta}\\|_{\\infty}<1.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The proof is given in Lemma A.11 in Appendix. From Lemma 3.3, we can satisfy the condition in (11) with scaling the values of the feature matrix $X$ . For example, if $\\operatorname*{max}(||X||_{\\infty},\\cdot||X^{\\top}||_{\\infty})<1$ , it is enough to choose $\\eta>2$ to meet the condition in Lemma 3.3. It is worth noting that scaling the values of feature matrix is a commonly employed technique in the both theoretical literature or in practice. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.4. Suppose $\\gamma||\\Gamma||_{\\infty}<1$ so that the solution point of PBE in (4) exists and is unique. Then, the condition $\\begin{array}{r}{0\\leq\\eta<\\frac{(1-\\gamma||\\Gamma||_{\\infty})||(X^{T}D X)^{-1}||_{\\infty}^{-1}}{\\gamma||(X^{T}D X)^{-1}||_{\\infty}||X||_{\\infty}||X^{T}D||_{\\infty}+(1-\\gamma||\\Gamma||_{\\infty})}}\\end{array}$ implies \u03b3||\u0393\u03b7||\u221e< 1. ", "page_idx": 5}, {"type": "text", "text": "The proof is in Appendix A.7. We note that the condition, $\\gamma\\|\\Gamma\\|_{\\infty}<1$ in Lemma 3.4 is used to guarantee the existence and uniqueness of the solution of PBE in (4), which is provided in Melo et al. [2008]. Therefore, without any special conditions, we can guarantee the existence and uniqueness of the solution in the neighborhood of $\\eta=0$ . ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.5. Suppose the feature vector satisfies $X^{\\top}D X=a I$ for a positive real number a such that $a|S||A|\\geq1$ . Assume that $||X||_{2}\\leq1$ and $D=1/(|S||A|)I$ . Then, $(I I)$ holds for all $\\eta>0$ . ", "page_idx": 6}, {"type": "text", "text": "The proof is given in Appendix A.8. A simple example where the above statement holds is by letting $X=I$ . This is only a conceptual example and there could be many other examples in existence. ", "page_idx": 6}, {"type": "text", "text": "3.3 Error analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As promised in the previous section, we provide discussion on the behavior and quality of $\\theta_{\\eta}^{*}$ , i.e., the error bound analysis in $\\theta^{*}-\\theta_{\\eta}^{*}$ depending on $\\eta$ . ", "page_idx": 6}, {"type": "text", "text": "Let us first examine the case when $\\eta\\rightarrow0$ and $\\eta\\rightarrow\\infty$ . As discussed in Section 3.2, we can consider $\\eta\\rightarrow0$ if we can guarantee the existence of $\\theta_{\\eta}^{*}$ and $\\theta^{*}$ when $\\eta$ is nearby the origin, for example in the case of Lemma 3.4 and 3.5. As $\\eta\\rightarrow0$ , (4) and (9) coincide, implying that $\\theta_{\\eta}^{\\ast}\\to\\theta^{\\ast}$ . ", "page_idx": 6}, {"type": "text", "text": "Furthermore, as $\\eta$ gets larger, by Lemma 3.3, we can always guarantee existence and uniqueness of $\\theta_{\\eta}^{*}$ after a certain threshold. As from the discussion of Lemma 3.1, we expect $\\theta_{\\eta}^{*}\\to0$ , which is stated in the following lemma whose proof is given in Appendix A.9: ", "page_idx": 6}, {"type": "text", "text": "Lemma 3.6. We have $\\operatorname*{lim}_{\\eta\\to\\infty}\\theta_{\\eta}^{*}=0$ . ", "page_idx": 6}, {"type": "text", "text": "Note that even if a solution satisfying (7) exists, $X{\\theta_{\\eta}^{*}}$ may be different from $Q^{*}$ . However, we can derive a bound on the error, $X\\theta_{\\eta}^{*}-Q^{*}$ , using simple algebraic inequalities and contraction property of the Bellman operator. We present the error bound of the solution in the following lemma: ", "page_idx": 6}, {"type": "text", "text": "Lemma 3.7. Suppose $(I I)$ holds. Then, we have : $\\begin{array}{r}{\\cdot\\|X\\theta_{\\eta}^{*}-Q^{*}\\|_{\\infty}\\leq\\frac{1}{1-\\gamma||\\Gamma_{\\eta}||_{\\infty}}||\\Gamma_{\\eta}Q^{*}-Q^{*}||_{\\infty}.}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "The proof is given in Appendix A.10. We provide a discussion on the error bound in the following: ", "page_idx": 6}, {"type": "text", "text": "1) $\\eta\\:\\rightarrow\\:0$ : Consider the case when $\\theta_{\\eta}^{*}$ and $\\theta^{*}$ exists and unique, for example the condition in Lemma 3.4 is satisfied. Since $\\Gamma_{\\eta}\\to\\Gamma$ from Lemma 3.1, we exactly recover the error bound by fixed point of original projected Bellman equation $\\left.\\eta=0\\right.$ ) in (4), which is $\\begin{array}{r}{\\frac{||\\Gamma Q^{*}-Q^{*}||_{\\infty}}{1-\\gamma||\\Gamma||_{\\infty}}}\\end{array}$ provided in Melo et al. [2008]. Thus, our bound in Lemma 3.7 is tight when $\\eta\\rightarrow0$ . ", "page_idx": 6}, {"type": "text", "text": "2) $\\eta\\rightarrow\\infty$ : As from Lemma 3.3, $\\theta_{\\eta}^{*}$ always exist when $\\eta$ gets larger than certain value. Noting that $\\Gamma_{\\eta}\\rightarrow0$ , we have $||X\\theta_{\\eta}^{*}-Q^{*}||_{\\infty}\\leq||Q^{*}||_{\\infty}$ . Considering that $\\theta_{\\eta}^{\\ast}\\to0$ as $\\eta\\rightarrow\\infty$ from Lemma 3.6, we should have $||X\\cdot0-Q^{*}||_{\\infty}=||Q^{*}||_{\\infty}.$ . Thus, our bound in Lemma 3.7 is tight when $\\eta\\rightarrow\\infty$ . ", "page_idx": 6}, {"type": "text", "text": "3) The error bound is close to zero: An upper bound on Lemma 3.7 can be obtained by simple algebraic manipulation: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\|X\\theta_{\\eta}^{*}-Q^{*}\\|_{\\infty}\\leq\\frac{\\|\\Gamma_{\\eta}Q^{*}-Q^{*}\\|_{\\infty}}{1-\\gamma\\|\\Gamma_{\\eta}\\|_{\\infty}}\\leq\\frac{1}{1-\\gamma\\|\\Gamma_{\\eta}\\|_{\\infty}}\\left(\\underbrace{\\|\\Gamma_{\\eta}Q^{*}-\\Gamma Q^{*}\\|_{\\infty}}_{(\\Gamma1)}+\\underbrace{\\|\\Gamma Q^{*}-Q^{*}\\|_{\\infty}}_{(\\Gamma2)}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Suppose that the features are well designed such that (T2) in (12) will be small. For example, if $Q^{*}$ is in the range space of $X$ , then the error term in (T2) vanishes. Moreover, we can make (T1) arbitrarily small as follows: as $\\eta\\rightarrow0$ , we have $||\\boldsymbol{\\Gamma}_{\\eta}-\\boldsymbol{\\Gamma}||_{\\infty}\\to0$ while $1-\\gamma||\\Gamma_{\\eta}||_{\\infty}>0$ . This yields (T1) in (12) to be sufficiently small. In the end, we will have $||X\\theta_{\\eta}^{*}-Q^{*}||_{\\infty}\\leq\\epsilon$ for any $\\epsilon\\geq0$ . ", "page_idx": 6}, {"type": "text", "text": "4) When the PBE does not admit a fixed point around $\\eta\\:=\\:0$ : In this case, we should always choose $\\eta>0$ greater than a certain number, and (T1) cannot be entirely vanished, while (T2) can be arbitrarily close to zero when $Q^{*}$ is close to the range space of $X$ . The error in (T1) cannot be overcame because it can be seen as a fundamental error caused by the regularization for PBE. However, (T1) can be still small enough in many cases when $||\\Gamma-\\Gamma_{\\eta}||_{\\infty}$ is small. ", "page_idx": 6}, {"type": "text", "text": "4 Algorithm ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we will introduce our main algorithm, called RegQ, and elaborate the condition on the regularization term to make the algorithm convergent. The proposed algorithm is motivated by TD-learning. In particular, for on-policy TD-learning, one can establish its convergence using the property of the stationary distribution. On the other hand, for an off-policy case, the mismatch between the sampling distribution and the stationary distribution could cause its divergence [Sutton et al., 2016]. To address this problem, Bharadwaj Diddigi et al. [2020] adds a regularization term to TD-learning in order to make it convergent. Since Q-learning can be interpreted as an off-policy TD-learning, we add a regularization term to Q-learning update motivated by Bharadwaj Diddigi et al. [2020]. This modification leads to the proposed RegQ algorithm as follows: ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\theta_{k+1}=\\theta_{k}+\\alpha_{k}(x(s_{k},a_{k})\\delta_{k}-\\eta\\theta_{k})\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The pseudo-code is given in Appendix A.16. Note that it can be viewed as a gradient descent step applied to the TD-loss $\\begin{array}{r}{L(\\theta)\\ :=\\ \\frac{1}{2}(y_{k}-Q_{\\theta}(s_{k},a_{k}))^{2}\\,+\\,\\frac{1}{2}\\eta\\,\\|\\theta\\|_{2}^{2},}\\end{array}$ , where $y_{k}~=~r_{k+1}+$ $\\operatorname{\\gammamax}_{a\\in A}^{}Q_{\\theta_{k}}(s_{k+1},a)$ is the TD-target, and $Q_{\\theta_{k}}=X\\theta_{k}$ . Furthermore, letting $\\eta=0$ , the above update is reduced to the standard Q-learning with linear function approximation in (2). The proposed RegQ is different from Bharadwaj Diddigi et al. [2020] in the sense that a regularization term is applied to Q-learning instead of TD-learning. Rewriting the stochastic update in a deterministic manner, it can be written as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\theta_{k+1}=\\theta_{k}+\\alpha_{k}(b-(A_{\\pi_{X\\theta_{k}}}+\\eta I)\\theta_{k}+m_{k+1}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $m_{k+1}=\\delta_{k}x(s_{k},a_{k})-\\eta\\theta_{k}-(b-(A_{\\pi\\chi_{\\theta_{k}}}+\\eta I)\\theta_{k})$ is a Martingale difference sequence. Without $m_{k+1}$ , (14) is reduced to the deterministic version in (8). In our convergence analysis, we will apply the O.D.E. approach, and in this case, $A_{\\pi_{X\\theta_{k}}}+\\eta I$ will determine the stability of the corresponding O.D.E. model, and hence, convergence of (13). Note that (14) can be interpreted as a switching system defined in (3) with stochastic noises. As mentioned earlier, proving the stability of a general switching system is challenging in general. However, we can find a common Lyapunov function to prove its asymptotic stability. In particular, we can make $-(A_{\\pi_{X\\theta_{k}}}+\\eta I)$ to have a strictly negatively row dominant diagonal or negative-definite under the following condition: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|>\\operatorname*{min}\\left\\{\\underset{(S1)}{\\underbrace{\\gamma||X^{\\top}D||_{\\infty}||X||_{\\infty}+||X^{\\top}D X||_{\\infty}}},~\\underset{(S1)\\in S\\times A}{\\underbrace{\\operatorname*{max}(C)\\left(\\underset{(s,a)\\in S\\times A}{\\operatorname*{max}}\\frac{\\gamma d^{T}P^{\\pi}(e_{a}\\otimes e_{s})}{2d(s,a)}-\\frac{2-\\gamma}{2}\\right)}}\\right\\}}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(S2)}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The conditions in $(S1)$ and $(S2)$ , which make $-(A_{\\pi_{X\\theta_{k}}}+\\eta I)$ to have strictly negatively row dominant diagonal or negative definite matrix , respectively, do not necessarily imply each others, which are discussed in Appendix A.15. Now, we can use the Lyapunov argument to establish stability of the overall system. Building on the fact, in the next section, we prove that under the stochastic update (13), we have $\\theta_{k}\\to\\theta_{\\eta}^{*}$ as $k\\rightarrow\\infty$ with probability one, where $\\theta_{\\eta}^{*}$ satisfies RPBE in (7). If $\\eta=0$ satisfies (15), we can guarantee convergence to an optimal policy without errors. ", "page_idx": 7}, {"type": "text", "text": "5 Convergence Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Recently, Lee and He [2020] suggested a switching system framework to prove the stability of Q-learning in the linear function approximation cases. However, its assumption on the behavior policy and feature matrix seems too stringent to check in practice. Here, we develop more practical Q-learning algorithm by adding an appropriately preconditioned regularization term. We prove the convergence of the proposed Q-learning with regularization term (13) following lines similar to Lee and He [2020]. Our proof mainly relies on Borkar-Meyn theorem. Therefore, we first discuss about the corresponding O.D.E. for the proposed update in (13), which is ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\dot{\\theta}_{t}=-(X^{T}D X+\\eta I)\\theta_{t}+\\gamma X^{T}D P\\Pi_{X\\theta_{t}}X\\theta_{t}+X^{T}D R:=f(\\theta_{t}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Then, using changes of coordinates, the above O.D.E. can be rewritten as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{d}{d t}(\\theta_{t}-\\theta_{\\eta}^{*})=(-A_{\\pi x\\theta_{t}}-\\eta I)(\\theta_{t}-\\theta_{\\eta}^{*})+\\gamma X^{T}D P(\\Pi_{X\\theta_{t}}-\\Pi_{X\\theta_{\\eta}^{*}})X\\theta_{\\eta}^{*},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\theta_{\\eta}^{*}$ satisfies (7). Here, we assume that an equilibrium point exists and is unique. We later prove that if an equilibrium exists, then it is unique. To apply Borkar-Meyn theorem in Lemma A.1, we discuss about the asymptotic stability of the O.D.E. in (17). Note that (17) includes an affine term, i.e., it cannot be expressed as a matrix times vector $\\theta_{t}-\\theta_{\\eta}^{*}$ . It is in general hard to establish asymptotic stability of switched linear system with affine term compared to switched linear system (3). To circumvent this difficulty, Lee and He [2020] proposed upper and lower comparison systems, which upper bounds and lower bounds the original system. Then, the stability of the original system can be established by proving the stability of the upper and lower systems, which are easier to analyze. Following similar lines, to check global asymptotic stability of the original system, we also introduce upper and lower comparison systems. Then, we prove global asymptotic stability of the two bounding systems. Since upper and lower comparison systems can be viewed as switched linear system and linear system, respectively, the global asymptotic stability is easier to prove. We stress that although the switching system approach in Lee and He [2020] is applied in this paper, the detailed proof is entirely different and nontrivial. In particular, the upper and lower comparison systems are given as follows: ", "page_idx": 7}, {"type": "image", "img_path": "4sueqIwb4o/tmp/0d62c80d303789f30f3d26acee8a3c9e2889ce982629c610cf0c35e2dd88e001.jpg", "img_caption": ["Figure 2: Experiment results ", ""], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "equation", "text": "$$\n\\dot{\\theta}_{t}^{u}=(-X^{T}D X-\\eta I+\\gamma X^{T}D P\\Pi_{X\\theta_{t}^{u}}X)\\theta_{t}^{u},\\quad\\dot{\\theta}_{t}^{l}=(-X^{T}D X-\\eta I+\\gamma X^{T}D P\\Pi_{X\\theta_{\\eta}^{*}}X)\\theta_{t}^{l},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\theta_{t}^{u}$ and $\\theta_{t}^{l}$ denote the states of the upper and lower systems, respectively. We defer the detailed construction of each system to Appendix A.12. The stability of overall system can be proved by establishing stability of the upper and lower comparison systems. ", "page_idx": 8}, {"type": "text", "text": "Theorem 5.1. Suppose \u03b7 satisfies (15), and Assumption 2.1, 2.2, and 2.3 hold. Moreover, assume that a solution of RPBE in (7) exists. Then, it is also unique, and the origin is the unique globally asymptotically stable equilibrium point of (17). ", "page_idx": 8}, {"type": "text", "text": "The detailed proof is given in Appendix A.12. Building on the previous results, we now use Borkar and Meyn\u2019s theorem in Lemma A.1 to establish the convergence of RegQ. The full proof of the following theorem is given in Appendix A.13. ", "page_idx": 8}, {"type": "text", "text": "Theorem 5.2. Suppose \u03b7 satisfies (15), then with Assumption 2.1, 2.2, and 2.3 holds. Assume that solution of RPBE in (7) exists. Then, $\\theta_{n}^{*}$ is unique, and under the stochastic update (13), $\\theta_{k}\\to\\theta_{\\eta}^{*}$ as $k\\rightarrow\\infty$ with probability one, where $\\theta_{\\eta}^{*^{'}}$ satisfies (7). ", "page_idx": 8}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we briefly present the experimental results under well-known environments in Tsitsiklis and Van Roy [1996], Baird [1995], where Q-learning with linear function approximation diverges. As from Figure 2b, our algorithm shows faster convergence rate than other algorithms. Further details on the experiments are deferred to Appendix B. In Appendix B.6, we also compare performance under the Mountain Car environment [Sutton and Barto, 2018] where Q-learning performs well. In Appendix B.5, we show experimental results under various step-size and $\\eta$ . Moreover, the trajectories of upper and lower systems to illustrate the theoretical results are given in Appendix B.7. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we presented a new convergent Q-learning with linear function approximation (RegQ), which is simple to implement. We provided theoretical analysis on the proposed RegQ, and demonstrated its performance on several experiments, where the original Q-learning with linear function approximation diverges. Developing a new Q-learning algorithm with linear function approximation without bias would be one interesting future research topic. Moreover, considering the great success of deep learning, it would be interesting to develop deep reinforcement learning algorithms with appropriately preconditioned regularization term instead of using the target network. ", "page_idx": 9}, {"type": "text", "text": "8 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The work was supported by the Institute of Information Communications Technology Planning Evaluation (IITP) funded by the Korea government under Grant 2022-0-00469, and the BK21 FOUR from the Ministry of Education (Republic of Korea). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Naman Agarwal, Syomantak Chaudhuri, Prateek Jain, Dheeraj Mysore Nagaraj, and Praneeth Netrapalli. Online Target Q-learning with Reverse Experience Replay: Efficiently finding the Optimal Policy for Linear MDPs. In International Conference on Learning Representations. ", "page_idx": 9}, {"type": "text", "text": "Praveen Agarwal, Mohamed Jleli, and Bessem Samet. Fixed Point Theory in Metric Spaces. Recent Advances and Applications, 2018.   \nLeemon Baird. Residual algorithms: Reinforcement learning with function approximation. In Machine Learning Proceedings 1995, pages 30\u201337. Elsevier, 1995.   \nMarc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47: 253\u2013279, 2013.   \nRaghuram Bharadwaj Diddigi, Chandramouli Kamanchi, and Shalabh Bhatnagar. A convergent off-policy temporal difference algorithm. In ECAI 2020, pages 1103\u20131110. IOS Press, 2020.   \nVivek S Borkar and Sean P Meyn. The ODE method for convergence of stochastic approximation and reinforcement learning. SIAM Journal on Control and Optimization, 38(2):447\u2013469, 2000.   \nSteven J Bradtke and Andrew G Barto. Linear least-squares algorithms for temporal difference learning. Machine learning, 22(1):33\u201357, 1996.   \nDiogo Carvalho, Francisco S Melo, and Pedro Santos. A new convergent variant of Q-learning with linear function approximation. Advances in Neural Information Processing Systems, 33: 19412\u201319421, 2020.   \nZaiwei Chen, John-Paul Clarke, and Siva Theja Maguluri. Target network and truncation overcome the deadly triad in-learning. SIAM Journal on Mathematics of Data Science, 5(4):1078\u20131101, 2023.   \nDaniela Pucci De Farias and Benjamin Van Roy. On the existence of fixed points for approximate value iteration and temporal-difference learning. Journal of Optimization theory and Applications, 105(3):589\u2013608, 2000.   \nAdithya M Devraj and Sean Meyn. Zap Q-learning. Advances in Neural Information Processing Systems, 30, 2017.   \nMatthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision processes. In International Conference on Machine Learning, pages 2160\u20132169. PMLR, 2019.   \nSina Ghiassian, Andrew Patterson, Shivam Garg, Dhawal Gupta, Adam White, and Martha White. Gradient temporal-difference learning with regularized corrections. In International Conference on Machine Learning, pages 3524\u20133534. PMLR, 2020. ", "page_idx": 9}, {"type": "text", "text": "Abhijit Gosavi. Boundedness of iterates in Q-learning. Systems & control letters, 55(4):347\u2013349, 2006. ", "page_idx": 10}, {"type": "text", "text": "William W Hager. Updating the inverse of a matrix. SIAM review, 31(2):221\u2013239, 1989. ", "page_idx": 10}, {"type": "text", "text": "Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. In Thirty-second AAAI conference on artificial intelligence, 2018. ", "page_idx": 10}, {"type": "text", "text": "Morris W Hirsch and Hal Smith. Monotone dynamical systems. In Handbook of differential equations: ordinary differential equations, volume 2, pages 239\u2013357. Elsevier, 2006. ", "page_idx": 10}, {"type": "text", "text": "RA Horn and CR Johnson. Matrix analysis second edition, 2013. ", "page_idx": 10}, {"type": "text", "text": "Tommi Jaakkola, Michael I Jordan, and Satinder P Singh. On the convergence of stochastic iterative dynamic programming algorithms. Neural computation, 6(6):1185\u20131201, 1994. ", "page_idx": 10}, {"type": "text", "text": "Carl T Kelley. Iterative methods for linear and nonlinear equations. SIAM, 1995. ", "page_idx": 10}, {"type": "text", "text": "Hassan K Khalil. Nonlinear systems; 3rd ed. Prentice-Hall, Upper Saddle River, NJ, 2002. URL https://cds.cern.ch/record/1173048. The book can be consulted by contacting: PH-AID: Wallet, Lionel. ", "page_idx": 10}, {"type": "text", "text": "Seungchan Kim, Kavosh Asadi, Michael Littman, and George Konidaris. Deepmellow: removing the need for a target network in deep q-learning. In Proceedings of the twenty eighth international joint conference on artificial intelligence, 2019. ", "page_idx": 10}, {"type": "text", "text": "Qingfeng Lan, Yangchen Pan, Alona Fyshe, and Martha White. Maxmin Q-learning: Controlling the Estimation Bias of Q-learning. In International Conference on Learning Representations. ", "page_idx": 10}, {"type": "text", "text": "Donghwan Lee and Niao He. A unified switching system perspective and convergence analysis of Q-learning algorithms. Advances in Neural Information Processing Systems, 33:15556\u201315567, 2020. ", "page_idx": 10}, {"type": "text", "text": "Donghwan Lee, Han-Dong Lim, Jihoon Park, and Okyong Choi. New Versions of Gradient TemporalDifference Learning. IEEE Transactions on Automatic Control, 68(8):5006\u20135013, 2022. ", "page_idx": 10}, {"type": "text", "text": "Daniel Liberzon. Switching in systems and control. Springer Science & Business Media, 2003. ", "page_idx": 10}, {"type": "text", "text": "Fan Lu, Prashant G Mehta, Sean P Meyn, and Gergely Neu. Convex Q-learning. In 2021 American Control Conference (ACC), pages 4749\u20134756. IEEE, 2021. ", "page_idx": 10}, {"type": "text", "text": "Hamid Reza Maei. Gradient temporal-difference learning algorithms. 2011. ", "page_idx": 10}, {"type": "text", "text": "Hamid Reza Maei, Csaba Szepesv\u00e1ri, Shalabh Bhatnagar, and Richard S Sutton. Toward off-policy learning control with function approximation. In ICML, 2010. ", "page_idx": 10}, {"type": "text", "text": "Sridhar Mahadevan, Bo Liu, Philip Thomas, Will Dabney, Steve Giguere, Nicholas Jacek, Ian Gemp, and Ji Liu. Proximal reinforcement learning: A new theory of sequential decision making in primal-dual spaces. arXiv preprint arXiv:1405.6757, 2014. ", "page_idx": 10}, {"type": "text", "text": "Alan S Manne. Linear programming and sequential decisions. Management Science, 6(3):259\u2013267, 1960. ", "page_idx": 10}, {"type": "text", "text": "Francisco S Melo, Sean P Meyn, and M Isabel Ribeiro. An analysis of reinforcement learning with function approximation. In Proceedings of the 25th international conference on Machine learning, pages 664\u2013671, 2008. ", "page_idx": 10}, {"type": "text", "text": "Sean Meyn. Stability of Q-learning Through Design and Optimism. arXiv preprint arXiv:2307.02632, 2023. ", "page_idx": 10}, {"type": "text", "text": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529\u2013533, 2015. ", "page_idx": 10}, {"type": "text", "text": "Alexander P Molchanov and Ye S Pyatnitskiy. Criteria of asymptotic stability of differential and difference inclusions encountered in control theory. Systems & Control Letters, 13(1):59\u201364, 1989.   \nHerbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics, pages 400\u2013407, 1951.   \nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.   \nRichard S Sutton, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba Szepesv\u00e1ri, and Eric Wiewiora. Fast gradient-descent methods for temporal-difference learning with linear function approximation. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 993\u20131000, 2009.   \nRichard S Sutton, A Rupam Mahmood, and Martha White. An emphatic approach to the problem of off-policy temporal-difference learning. The Journal of Machine Learning Research, 17(1): 2603\u20132631, 2016.   \nJohn N Tsitsiklis and Benjamin Van Roy. Feature-based methods for large scale dynamic programming. Machine Learning, 22(1):59\u201394, 1996.   \nJohn N Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with function approximation. IEEE transactions on automatic control, 42(5):674\u2013690, 1997.   \nChristopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279\u2013292, 1992.   \nJiachen Xi, Alfredo Garcia, and Petar Momcilovic. Regularized Q-learning with Linear Function Approximation. arXiv preprint arXiv:2401.15196, 2024.   \nLin Yang and Mengdi Wang. Sample-optimal parametric Q-learning using linearly additive features. In International Conference on Machine Learning, pages 6995\u20137004. PMLR, 2019.   \nShangtong Zhang, Hengshuai Yao, and Shimon Whiteson. Breaking the deadly triad with a target network. In International Conference on Machine Learning, pages 12621\u201312631. PMLR, 2021. ", "page_idx": 11}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 O.D.E analysis ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The dynamic system framework has been widely used to prove convergence of reinforcement learning algorithms, e.g., Sutton et al. [2009], Maei et al. [2010], Borkar and Meyn [2000], Lee and He [2020]. Especially, Borkar and Meyn [2000] is one of the most widely used techniques to prove stability of stochastic approximation using O.D.E. analysis. Consider the following stochastic algorithm with a nonlinear mapping $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{n}$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\theta_{k+1}=f(\\theta_{k})+m_{k},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $m_{k}\\in\\mathbb{R}^{n}$ is an i.i.d. noise vector. For completeness, results in Borkar and Meyn [2000] are briefly reviewed in the sequel. Under Assumption A.2 given in Appendix A.2, we now introduce Borkar and Meyn theorem below. ", "page_idx": 12}, {"type": "text", "text": "Lemma A.1 (Borkar and Meyn theorem). Suppose that Assumption A.2 in the Appendix A.2 holds, and consider the stochastic algorithm in (18). Then, for any initial $\\theta_{0}\\in\\mathbb{R}^{n}$ , $\\mathrm{sup}_{k\\ge0}\\left|\\left|\\theta_{k}\\right|\\right|<\\infty$ with probability one. In addition , $\\theta_{k}\\rightarrow\\theta^{e}$ as $k\\rightarrow\\infty$ with probability one, where $\\theta^{e}$ satisfies $f(\\theta^{e})=0$ . ", "page_idx": 12}, {"type": "text", "text": "The main idea of Borkar and Meyn theorem is as follows: iterations of a stochastic recursive algorithm follow the solution of its corresponding O.D.E. in the limit when the step-size satisfies the RobbinsMonro condition. Hence, by proving the asymptotic stability of the O.D.E., we can induce the convergence of the original algorithm. In this paper, we will use an O.D.E. model of Q-learning, which is expressed as a special nonlinear system called a switching system. ", "page_idx": 12}, {"type": "text", "text": "A.2 Assumption for Borkar and Meyn Theorem ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Assumption A.2. ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "1. The mapping $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{n}$ is globally Lipschitz continuous, and there exists a function $f_{\\infty}$ : $\\mathbb{R}^{n}\\to\\mathbb{R}^{n}$ such that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{c\\to\\infty}{\\frac{f(c x)}{c}}=f_{\\infty}(x),\\quad\\forall x\\in\\mathbb{R}^{n}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "2. The origin in $\\mathbb{R}^{n}$ is an asymptotically stable equilibrium for the O.D.E. $\\dot{x}_{t}=f_{\\infty}(x_{t})$ . ", "page_idx": 12}, {"type": "text", "text": "3. There exists a unique globally asymptotically stable equilibrium $\\theta^{e}~\\in~\\mathbb{R}^{n}$ for the O.D.E.   \n${\\dot{x}}_{t}=f(x_{t})$ , i.e., $x_{t}\\rightarrow\\theta^{e}$ as $t\\to\\infty$ . ", "page_idx": 12}, {"type": "text", "text": "4. The sequence $\\{m_{k},\\mathcal{G}_{k}\\}_{k\\ge1}$ where $\\mathcal{G}_{k}$ is sigma-algebra generated by $\\{(\\theta_{i},m_{i},k~\\geq~i\\},$ , is a Martingale difference sequence. In addition , there exists a constant $C_{0}<\\infty$ such that for any initial $\\theta_{0}\\in\\mathbb{R}^{n}$ , we have $\\begin{array}{r}{\\mathbb{E}[||m_{k+1}||^{2}|\\mathcal{G}_{k}]\\leq C_{0}(1+||\\theta_{k}||^{2}),\\forall k\\geq0.}\\end{array}$ . ", "page_idx": 12}, {"type": "text", "text": "5. The step-sizes satisfies the Robbins-Monro condition [Robbins and Monro, $I95I J$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\sum_{k=0}^{\\infty}\\alpha_{k}=\\infty,\\quad\\sum_{k=0}^{\\infty}\\alpha_{k}^{2}<\\infty.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "A.3 Auxiliary lemmas ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Lemma A.3 (Woodbury matrix identity [Hager, 1989]). For $A,B\\in\\mathbb{R}^{n\\times n}$ , suppose $A$ and $I{+}A^{-1}B$ is invertible, then $A+B$ is invertible and we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n(A+B)^{-1}=A^{-1}-A^{-1}B(I+A^{-1}B)^{-1}A^{-1}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Lemma A.4 (Gelfand\u2019s formula, Corollay 5.6.14 in Horn and Johnson [2013]). For any matrix norm $||\\cdot||$ , for $A\\in\\mathbb{R}^{n\\times n}$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\rho(A)=\\operatorname*{lim}_{k\\to\\infty}||A^{k}||^{\\frac{1}{k}},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\rho(\\cdot)$ denotes the spectral radius of a given matrix. ", "page_idx": 12}, {"type": "text", "text": "Definition A.5 (Theorem 3 in Molchanov and Pyatnitskiy [1989]). A matrix $A\\in\\mathbb{R}^{n\\times n}$ is said to have strictly negatively row dominating diagonal if $\\begin{array}{r}{\\cdot[A]_{i i}+\\sum_{j\\in\\{1,2,\\dots,n\\}\\backslash\\{i\\}}[A]_{i j}\\,<\\,0}\\end{array}$ for all $1\\leq i\\leq n$ . ", "page_idx": 13}, {"type": "text", "text": "Lemma A.6 (Theorem 3 in Molchanov and Pyatnitskiy [1989]). Consider a switched system in (3) where $\\mathcal{M}=:\\{1,2,\\dots,M\\}$ is the set for switching modes. If there exists a number $m\\geq n$ , a full-row rank matrix $L\\in\\mathbb{R}^{n\\times m}$ and a set of matrices $\\{\\bar{\\mathcal{L}_{\\sigma}}\\in\\mathbb{R}^{m\\times\\dot{m}}\\}_{\\sigma\\in\\mathcal{M}}$ such that ", "page_idx": 13}, {"type": "text", "text": "1) Each ${\\mathcal{L}}_{\\sigma}$ for $\\sigma\\in\\mathcal{M}$ has a strictly negatively row dominating diagonal: ", "page_idx": 13}, {"type": "equation", "text": "$$\n[\\mathcal{L}]_{i i}+\\sum_{j\\in\\{1,2,...,m\\}\\backslash\\{i\\}}|[\\mathcal{L}]_{i j}|<0.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "2) The following holds for all $\\sigma\\in\\mathcal{M}$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A_{\\sigma}^{\\top}L=L\\mathcal{L}_{\\sigma}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then, the origin of (3) is asymptotically stable. ", "page_idx": 13}, {"type": "text", "text": "Lemma A.7 (Gerschgorin circle theorem [Horn and Johnson, 2013]). Let $A\\in\\mathbb{R}^{n\\times m}$ whose $i$ -th row and $j$ -th column element is $a_{i j}$ . Let $\\begin{array}{r l r l r l}{R_{i}(A)=}&{{}}&{\\sum}&{{}}&{a_{i j}}\\end{array}$ . Consider the Gerschgorin circles j\u2208{1,2,...,m}\\{i} ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\{z\\in\\mathbb{C}:|z-a_{i i}|\\leq R_{i}(A)\\},\\quad i=1,\\ldots,n.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The eigenvalues of $A$ are in the union of Gerschgorin discs ", "page_idx": 13}, {"type": "equation", "text": "$$\nG(A)=\\cup_{i=1}^{n}\\{z\\in\\mathbb{C}:|z-a_{i i}|\\leq R_{i}(A)\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Now, we state the lemma to guarantee positive definiteness of $A_{\\pi_{X\\theta}}+\\eta I$ . Instead we prove positive definiteness of $\\begin{array}{r}{A_{\\pi_{X\\theta}}+\\frac{\\bar{\\eta^{}}}{\\lambda_{\\operatorname*{max}}(C)}C}\\end{array}$ . We follow the similar lines in Bharadwaj Diddigi et al. [2020]. ", "page_idx": 13}, {"type": "text", "text": "Lemma A.8. Let ", "page_idx": 13}, {"type": "equation", "text": "$$\nM^{\\pi x\\theta}:=D\\left(\\left(1+\\frac{\\eta}{\\lambda_{\\operatorname*{max}}(C)}\\right)I-\\gamma P^{\\pi x\\theta}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Under the following condition: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\eta>\\lambda_{\\mathrm{max}}(C)\\operatorname*{max}_{(s,a)\\in\\mathcal{S}\\times\\mathcal{A}}\\left(\\frac{\\gamma d^{T}P^{\\pi_{X\\theta}}(e_{a}\\otimes e_{s})}{2d(s,a)}-\\frac{2-\\gamma}{2}\\right)\\quad,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\Theta$ is the set of all deterministic policies, and $\\otimes$ is the Kronecker product, $M^{\\pi_{X\\theta}}$ is positive definite. ", "page_idx": 13}, {"type": "text", "text": "Proof. For simplicity of the notation, we will denote $d_{i}\\,=\\,d(s,a)$ and $\\boldsymbol{e}_{i}\\,=\\,\\boldsymbol{e}_{a}\\otimes\\boldsymbol{e}_{a}$ for some $i\\in\\{1,2,\\ldots,|\\bar{S}||A|\\}$ where $i$ corresponds to some $s,a\\in{\\mathcal{S}}\\times{\\mathcal{A}}$ . ", "page_idx": 13}, {"type": "text", "text": "We use Gerschgorin circle theorem for the proof. First, denote $m_{i j}=[M^{\\pi_{X\\theta}}]_{i j}$ . Then, one gets ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{m_{i i}=d_{i}\\left(\\left(1+\\frac{\\eta}{\\lambda_{\\operatorname*{max}}(C)}\\right)-\\gamma e_{i}^{T}P^{\\pi_{X\\theta}}e_{i}\\right),}\\\\ &{m_{i j}=-d_{i}\\gamma e_{i}^{T}P^{\\pi_{X\\theta}}e_{j}\\quad\\mathrm{for}\\quad i\\neq j.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Except for the diagonal element, the row and column sums, respectively, become ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{j\\in S_{i}}|m_{i j}|=\\gamma d_{i}(1-e_{i}^{T}P^{\\pi_{X\\theta}}e_{i}),}\\\\ &{\\displaystyle\\sum_{j\\in S_{i}}|m_{j i}|=\\gamma d^{T}P^{\\pi_{X\\theta}}e_{i}-\\gamma d_{i}e_{i}^{T}P^{\\pi_{X\\theta}}e_{i},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $S_{i}=\\{1,2,...\\,,|S||A|\\}\\setminus\\{i\\}$ . We need to show that $M^{\\pi_{X\\theta}}+M^{\\pi_{X\\theta}^{T}}$ is positive definite. To this end, we use Lemma A.7 to have the following inequality: ", "page_idx": 14}, {"type": "equation", "text": "$$\n|\\lambda-2m_{i i}|\\leq\\sum_{j\\in S_{i}}|m_{i j}|+\\sum_{j\\in S_{i}}|m_{j i}|.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Considering the lower bound of $\\lambda$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathrm{\\Upsilon}:\\geq2m_{i i}-\\sum_{j\\in S_{i}}|m_{i j}|-\\sum_{j\\in S_{i}}|m_{j i}|}}\\\\ &{=2d_{i}\\left(\\bigg(1+\\frac{\\eta}{\\lambda_{\\operatorname*{max}}(C)}\\bigg)-\\gamma e_{i}^{T}P^{\\pi x\\theta}e_{i}\\right)-\\gamma d_{i}(1-e_{i}^{T}P^{\\pi x\\theta}e_{i})-(\\gamma d^{T}P^{\\pi x\\theta}e_{i}-\\gamma d_{i}e_{i}^{T}P^{\\pi x\\theta}e_{i})}\\\\ &{=\\eta\\frac{2d_{i}}{\\lambda_{\\operatorname*{max}}(C)}+(2-\\gamma)d_{i}-\\gamma d^{T}P^{\\pi x\\theta}e_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Hence, for $\\lambda>0$ , we should have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\eta>\\lambda_{\\operatorname*{max}}(C)\\left(\\frac{\\gamma d^{T}P^{\\pi_{X\\theta}}e_{i}}{2d_{i}}-\\frac{2-\\gamma}{2}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Taking $\\eta\\,>\\,\\lambda_{\\mathrm{max}}(C)$ $\\begin{array}{r l}&{\\underset{\\pi\\in\\Theta}{\\operatorname*{max}}\\quad\\quad\\left(\\frac{\\gamma d^{T}P^{\\pi_{X\\theta}}e_{i}}{2d_{i}}-\\frac{2-\\gamma}{2}\\right)}\\\\ &{2,\\dots,|S||A|\\}}\\end{array}$ we can make $M^{\\pi_{X\\theta}}$ always positive definite. This completes the proof. ", "page_idx": 14}, {"type": "text", "text": "We first introduce a lemma to bound the inverse of a matrix norm: ", "page_idx": 14}, {"type": "text", "text": "Lemma A.9. [Page 351 in Horn and Johnson [2013]] If $M\\in\\mathbb{R}^{n\\times n}$ satisfies $||M||<1$ for some matrix norm $||\\cdot||$ , then $I-M$ is non-singular, and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\|(I-M)^{-1}\\right\\|\\leq{\\frac{1}{1-\\|M\\|}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma A.10. Suppose that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\|X^{\\top}D X\\right\\|_{\\infty}<\\eta.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\big\\|(X^{\\top}D X+\\eta I)^{-1}\\big\\|_{\\infty}\\leq\\frac{1}{\\eta-\\|X^{\\top}D X\\|_{\\infty}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. We have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|{(X^{\\top}D X+\\eta I)^{-1}}\\right\\|_{\\infty}=\\left\\|{\\frac{1}{\\eta}\\left(\\frac{1}{\\eta}X^{\\top}D X+I\\right)^{-1}}\\right\\|_{\\infty}}&{}\\\\ {=\\frac{1}{\\eta}\\left\\|{\\left(\\frac{1}{\\eta}X^{\\top}D X+I\\right)^{-1}}\\right\\|_{\\infty}}&{}\\\\ {\\ \\ \\leq\\frac{1}{\\eta}\\frac{1}{1-\\left\\|{\\frac{1}{\\eta}X^{\\top}D X}\\right\\|_{\\infty}}}&{}\\\\ {\\ \\ =\\frac{1}{\\eta-\\|X^{\\top}D X\\|_{\\infty}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The first inequality follows from Lemma A.9. This completes the proof. ", "page_idx": 14}, {"type": "text", "text": "Lemma A.11. For $\\eta>\\gamma||X^{\\top}D||_{\\infty}||X||_{\\infty}+||X^{\\top}D X||_{\\infty},$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\gamma\\left\\|\\Gamma_{\\eta}\\right\\|_{\\infty}<1.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. From the definition of $\\Gamma_{\\eta}$ in (10), we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma\\left\\|\\Gamma_{\\eta}\\right\\|_{\\infty}=\\gamma\\left\\|X^{\\top}D(X^{\\top}D X+\\eta I)^{-1}X\\right\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\leq\\gamma\\left\\|X^{\\top}D\\right\\|_{\\infty}||X||_{\\infty}\\frac{1}{\\eta-\\|X^{\\top}D X\\|_{\\infty}}}\\\\ &{\\qquad\\qquad<1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The first inequality follows from Lemma A.10. The last inequality follows from the condition $\\eta>\\gamma||X^{\\top}D||_{\\infty}||\\bar{X}||_{\\infty}+||X^{\\top}D X||_{\\infty}.$ This completes the proof. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Lemma A.12. The equation (7) can be written as ", "page_idx": 15}, {"type": "equation", "text": "$$\nX\\theta_{\\eta}^{*}=\\Gamma_{\\eta}\\tau X\\theta_{\\eta}^{*}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Let us expand the terms in (7): ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad X^{\\top}D R=(\\eta I+X^{\\top}D X)\\theta_{\\eta}^{*}-\\gamma X^{\\top}D P\\Pi_{X\\theta_{\\eta}^{*}}X\\theta_{\\eta}^{*}}\\\\ &{\\Longleftrightarrow X^{\\top}D(R+\\gamma P\\Pi_{X\\theta_{\\eta}^{*}}X\\theta_{\\eta}^{*})=(\\eta I+X^{\\top}D X)\\theta_{\\eta}^{*}}\\\\ &{\\Longleftrightarrow X(\\eta I+X^{\\top}D X)^{-1}X^{\\top}D(R+\\gamma P\\Pi_{X\\theta_{\\eta}^{*}}X\\theta_{\\eta}^{*})=X\\theta_{\\eta}^{*}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The last line follows from that $X$ is full-column rank matrix. This completes the proof. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.13. For any $\\boldsymbol\\theta\\in\\mathbb{R}^{h}$ , $f\\eta>\\gamma||\\boldsymbol{X}^{\\top}\\boldsymbol{D}||_{\\infty}||\\boldsymbol{X}||_{\\infty}+||\\boldsymbol{X}^{\\top}\\boldsymbol{D}\\boldsymbol{X}||_{\\infty}$ , then $-A_{\\pi_{X\\theta}}-\\eta I$ has strictly negatively row dominating diagonal. ", "page_idx": 15}, {"type": "text", "text": "Proof. For $1\\leq i\\leq h$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle[-A_{\\pi\\chi\\theta}-\\eta I]_{i i}+\\sum_{j\\in\\{1,2,\\dots,h\\}\\setminus\\{i\\}}|[A_{\\pi\\chi\\theta}+\\eta I]_{i j}|\\leq-\\eta+\\displaystyle\\sum_{j=1}^{h}|[A_{\\pi\\chi\\theta}]_{i j}|}&{}\\\\ {\\displaystyle\\leq-\\eta+\\|A_{\\pi\\chi\\theta}\\|_{\\infty}}&{}\\\\ {\\displaystyle\\leq-\\eta+\\|X^{\\top}D X\\|_{\\infty}+\\gamma\\left\\|X^{\\top}D\\right\\|_{\\infty}\\|X\\|_{\\infty}}&{}\\\\ {\\displaystyle<0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The second last inequality follows the fact that $\\|P\\Pi_{X_{\\theta}}||_{\\infty}\\le1$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma A.14 (Continuity of $\\theta_{\\eta}^{*}$ with respect to $\\eta$ ). Let $\\eta_{0}$ be a non-negative real valued constant.   \nSuppose $\\gamma||\\Gamma_{\\eta_{0}}||_{\\infty}<1$ . Then, $\\theta_{\\eta}^{*}$ is continuous at $\\eta_{0}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. Note that $\\Gamma_{\\eta}$ is continuous function of $\\eta$ , and we have, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Gamma_{\\eta_{0}+\\eta}=\\Gamma_{\\eta_{0}}+{\\cal O}(\\eta),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $O(\\cdot)$ stands for the big O notation. Therefore, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lVert X\\theta_{\\eta_{0}+\\eta}^{*}-X\\theta_{\\eta_{0}}^{*}\\rVert_{\\infty}=\\lVert\\Gamma_{\\eta_{0}+\\eta}\\mathcal{T}X\\theta_{\\eta_{0}+\\eta}^{*}-\\Gamma_{\\eta_{0}}\\mathcal{T}X\\theta_{\\eta_{0}}^{*}\\rVert_{\\infty}}&{}\\\\ {\\leq\\lVert\\Gamma_{\\eta_{0}}\\mathcal{T}X(\\theta_{\\eta_{0}+\\eta}^{*}-\\theta_{\\eta_{0}}^{*})\\rVert_{\\infty}+O(\\eta)}&{}\\\\ {\\leq\\gamma\\lVert\\Gamma_{\\eta_{0}}\\rVert_{\\infty}\\,\\lVert X(\\theta_{\\eta_{0}+\\eta}^{*}-\\theta_{\\eta_{0}}^{*})\\rVert_{\\infty}+O(\\eta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The first equality follows from the definition of \u03b8\u03b7\u22170+\u03b7 and \u03b8\u03b7\u22170. The second inequality follows from triangle inequality. The last inequality follows from the contraction property of the Bellman operator. Therefore, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n||{\\theta_{\\eta_{0}+\\eta}^{*}}-\\theta_{\\eta_{0}}^{*}||_{\\infty}\\leq C||X{\\theta_{\\eta_{0}+\\eta}^{*}}-X{\\theta_{\\eta_{0}}^{*}}||_{\\infty}\\leq O(\\eta),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the first inequality holds because $X$ is full-column rank matrix, and $C$ is a universal constant. This completes the proof. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "A.4 Proof of Lemma 2.5 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. The first item follows from Lemma A.6. The second item follows from the fact that if all the subsystem matrices are negative-definite, then it will have $V(x)=||x||_{2}^{2}$ as a common Lyapunov function. This completes the proof. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "A.5 Proof of Lemma 3.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. Let us prove the first item. When, $\\eta\\rightarrow0$ , we have $(X^{\\top}D X+\\eta I)^{-1}\\,\\to\\,(X^{\\top}D X)^{-1}.$ .   \nTherefore, we have $\\Gamma_{\\eta}\\to\\Gamma$ as $\\eta\\rightarrow0$ . ", "page_idx": 16}, {"type": "text", "text": "Moreover, note that from Lemma A.10, for sufficiently large $\\eta$ , we have $(X^{\\top}D X+\\eta I)^{-1}$ is invertible. Therefore, we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\Gamma_{\\eta}\\right\\|_{\\infty}=\\left\\|X^{\\top}D(X^{\\top}D X+\\eta I)^{-1}X\\right\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\leq\\left\\|X^{\\top}D\\right\\|_{\\infty}||X||_{\\infty}\\frac{1}{\\eta-\\left\\|X^{\\top}D X\\right\\|_{\\infty}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the first inequality follows from Lemma A.10. As $\\eta\\,\\rightarrow\\,\\infty$ , we get $\\|\\Gamma_{\\eta}\\|_{\\infty}\\to\\,0$ . This completes the proof of the first item. ", "page_idx": 16}, {"type": "text", "text": "Now, we will prove the second item. First of all, using Woodbury matrix identity [Hager, 1989] in Lemma A.3, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(X^{\\top}D X+\\eta I)^{-1}=(X^{\\top}D X)^{-1}-(X^{\\top}D X+\\eta^{-1}(X^{\\top}D X)(X^{\\top}D X))^{-1}}\\\\ &{\\qquad\\qquad\\qquad\\preceq(X^{\\top}D X)^{-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the inequality comes from the fact that $(X^{\\top}D X+\\eta^{-1}(X^{\\top}D X)(X^{\\top}D X))^{-1}$ is positive semidefinite. Then, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\Gamma_{\\eta}\\right\\|_{\\infty}=\\left\\|X^{\\top}D(X^{\\top}D X+\\eta I)^{-1}X\\right\\|_{\\infty}}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~=\\sqrt{|{\\cal S}\\times{\\cal A}|}\\left\\|X^{\\top}D(X^{\\top}D X+\\eta I)^{-1}X\\right\\|_{2}}\\\\ &{~~~~~~~~~~~~~~~~~~\\leq\\sqrt{|{\\cal S}\\times{\\cal A}|}\\left\\|X^{\\top}D\\right\\|_{2}\\left\\|X\\right\\|_{2}\\left\\|(X^{\\top}D X+\\eta I)^{-1}\\right\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Next, since the spectral norm is monotone, for any two symmetric positive semidefinite matrices $A$ and $B$ , $,A\\succeq B$ implies $\\|A\\|_{2}\\geq\\|B\\|_{2}$ , which comes from the properties of the spectral norm for symmetric positive semidefinite matrices. Therefore, one gets ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\Gamma_{\\eta}\\right\\|_{\\infty}\\leq\\!\\!\\left\\|X^{\\top}D\\right\\|_{2}\\!\\left\\|X\\right\\|_{2}\\left\\|(X^{\\top}D X+\\eta I)^{-1}\\right\\|_{2}\\sqrt{|{\\cal S}\\times{\\cal A}|}}\\\\ &{\\qquad\\quad\\leq\\!\\!\\left\\|X^{\\top}D\\right\\|_{2}\\left\\|X\\right\\|_{2}\\left\\|(X^{\\top}D X)^{-1}\\right\\|_{2}\\sqrt{|{\\cal S}\\times{\\cal A}|},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which is the desired conclusion. ", "page_idx": 16}, {"type": "text", "text": "A.6 Proof of Lemma 3.2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. To show the existence and uniqueness of the solution of (9), we use Banach fixed-point theorem. Note that it is enough show the existence and uniqueness of the solution of the following equation: ", "page_idx": 16}, {"type": "equation", "text": "$$\ny=\\Gamma_{\\eta}(R+\\gamma P\\Pi_{y}y),\\quad y\\in\\mathbb{R}^{h}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This is because a solution $\\boldsymbol{y}^{*}\\in\\mathbb{R}^{h}$ satisfying the above equation is in the image of $X$ . We can find a unique $\\theta$ such that $X\\theta=y^{*}$ because $X$ is a full-column rank matrix. To this end, we will apply the Banach fixed point theorem: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{||y_{1}-y_{2}||_{\\infty}=||X(X^{T}D X+\\eta I)^{-1}(\\gamma X^{T}D P\\Pi_{y_{1}}y_{1}-\\gamma X^{T}D P\\Pi_{y_{2}}y_{2})||_{\\infty}}\\\\ &{\\qquad\\qquad\\leq\\gamma||X(X^{T}D X+\\eta I)^{-1}X^{\\top}D||_{\\infty}||\\Pi_{y_{1}}y_{1}-\\Pi_{y_{2}}y_{2}||_{\\infty}}\\\\ &{\\qquad\\qquad\\leq\\gamma||X(X^{T}D X+\\eta I)^{-1}X^{\\top}D||_{\\infty}||y_{1}-y_{2}||_{\\infty}}\\\\ &{\\qquad\\qquad<||y_{1}-y_{2}||_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The second inequality follows from the non-expansiveness property of the max-operator. Now, we can use Banach fixed-point theorem to conclude existence and uniqueness of (20). This completes the proof. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "A.7 Proof of Lemma 3.4 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For the proof, suppose that $\\gamma\\|\\Gamma\\|_{\\infty}\\quad<\\quad1$ . If the condition $\\begin{array}{r l r l r}{0}&{{}\\le}&{\\eta}&{{}<}&{}\\end{array}$ < \u03b3\u2225(XT DX)\u22121\u2225\u221e\u2225X\u2225\u221e\u2225XT D\u2225\u221e+(\u221e1\u2212\u03b3\u2225\u0393\u2225\u221e) holds, it ensures (1\u2212\u03b3\u2225\u0393\u2225\u221e)\u2225(XT DX)\u22121\u2225\u22121 $\\left\\|\\eta(X^{T}D X)^{-1}\\right\\|_{\\infty}\\ \\ <\\ \\ 1$ since \u03b3\u2225(XT DX)\u22121\u2225\u221e\u2225(1X\u2212\u2225\u03b3\u221e\u2225\u2225\u0393\u2225X\u221eT )D\u2225\u221e+(1\u2212\u03b3\u2225\u0393\u2225\u221e) < 1. Then, using Gelfand\u2019s formula in Lemma A.4, we can easily prove that the spectral radius of $\\eta(X^{T}D X)^{-1}$ is less than one. Next, note that for any two square matrices $A$ and $B$ , $\\textstyle(A-B)^{-1}=\\sum_{i=0}^{\\infty}(A^{-1}B)^{i}A^{-1}$ if the spectral radius of $A^{-1}B$ is less than one. Using this fact, one has ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|\\Gamma_{\\eta}\\|_{\\infty}=\\left\\|\\gamma X(X^{T}D X+\\eta I)^{-1}X^{T}D\\right\\|_{\\infty}}}\\\\ &{=\\gamma\\left\\|X\\displaystyle\\sum_{i=0}^{\\infty}\\left(-\\eta(X^{T}D X)^{-1}\\right)^{i}(X^{T}D X)^{-1}X^{T}D\\right\\|_{\\infty}}\\\\ &{\\leq\\gamma\\left\\|X(X^{T}D X)^{-1}X^{T}D\\right\\|_{\\infty}+\\gamma\\left\\|(X^{T}D X)^{-1}\\displaystyle\\sum_{i=1}^{\\infty}\\eta^{i}X(-X^{T}D X)^{-i}X^{T}D\\right\\|_{\\infty}}\\\\ &{\\leq\\gamma\\left\\|X(X^{T}D X)^{-1}X^{T}D\\right\\|_{\\infty}+\\gamma\\eta\\left\\|(X^{T}D X)^{-1}\\right\\|_{\\infty}^{2}\\|X\\|_{\\infty}\\left\\|X^{T}D\\right\\|_{\\infty}\\displaystyle\\sum_{i=0}^{\\infty}\\left\\|\\eta(X^{T}D X)^{-1}\\right\\|_{\\infty}}\\\\ &{\\leq\\gamma\\|\\Gamma\\|_{\\infty}+\\frac{\\gamma\\eta\\left\\|(X^{T}D X)^{-1}\\right\\|_{\\infty}^{2}\\|X\\|_{\\infty}\\left\\|X^{T}D\\right\\|_{\\infty}}{1-\\eta\\left\\|(X^{T}D X)^{-1}\\right\\|_{\\infty}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the second line uses the matrix inverse property. Therefore, $\\gamma\\left\\|\\Gamma_{\\eta}\\right\\|_{\\infty}<1$ holds if ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\gamma\\|\\Gamma\\|_{\\infty}+\\frac{\\gamma\\eta\\left\\|\\left(X^{T}D X\\right)^{-1}\\right\\|_{\\infty}^{2}\\left\\|X\\right\\|_{\\infty}\\left\\|X^{T}D\\right\\|_{\\infty}}{1-\\eta\\left\\|\\left(X^{T}D X\\right)^{-1}\\right\\|_{\\infty}}<1.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Rearranging terms, one gets the desired conclusion. ", "page_idx": 17}, {"type": "text", "text": "A.8 Proof of Lemma 3.5 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "From the definition of $\\Gamma_{\\eta}$ in (10), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma\\left\\|\\Gamma_{\\eta}\\right\\|_{\\infty}\\leq\\gamma\\left\\|X\\right\\|_{\\infty}\\left\\|(X^{\\top}D X+\\eta I)^{-1}\\right\\|_{\\infty}\\left\\|X^{\\top}\\right\\|_{\\infty}\\left\\|D\\right\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\leq\\gamma\\frac{1}{|S||A|}\\frac{1}{a+\\eta}}\\\\ &{\\qquad\\qquad<1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The second inequality follows from the assumption that $X^{\\top}D X=a I$ and $||X||_{2}\\leq1$ . The last inequality follows from the condition $a|S||A|\\bar{\\geq}1$ . ", "page_idx": 17}, {"type": "text", "text": "A.9 Proof of Lemma 3.6 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. Since we are going to consider the case $\\eta~\\rightarrow~\\infty$ , assume that $\\eta~>~\\left\\|X^{\\top}D X\\right\\|_{\\infty}~+$ $\\gamma\\left\\|X^{\\top}D\\right\\|_{\\infty}\\left\\|X\\right\\|_{\\infty}$ . From (7), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\theta_{\\eta}^{*}\\right\\|_{\\infty}=\\left\\|(X^{\\top}D X+\\eta I)^{-1}(X^{\\top}D R+\\gamma X^{\\top}D P\\Pi_{X\\theta_{\\eta}^{*}}X\\theta_{\\eta}^{*})\\right\\|_{\\infty}}\\\\ &{\\qquad\\quad\\leq\\frac{1}{\\eta-\\left\\|X^{\\top}D X\\right\\|_{\\infty}}\\left\\|X^{\\top}D R+\\gamma X^{\\top}D P\\Pi_{X\\theta_{\\eta}^{*}}X\\theta_{\\eta}^{*}\\right\\|_{\\infty}}\\\\ &{\\qquad\\quad\\leq\\frac{1}{\\eta-\\left\\|X^{\\top}D X\\right\\|_{\\infty}}\\left\\|X^{\\top}D R\\right\\|_{\\infty}+\\frac{1}{\\eta-\\left\\|X^{\\top}D X\\right\\|_{\\infty}}\\left\\|X^{\\top}D\\right\\|_{\\infty}\\left\\|X\\right\\|_{\\infty}\\left\\|\\theta_{\\eta}^{*}\\right\\|_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The first inequality follows from Lemma A.10. Therefore, considering that $\\eta>\\left\\|X^{\\top}D X\\right\\|_{\\infty}+$ $\\gamma\\left\\|X^{\\top}D\\right\\|_{\\infty}\\left\\|X\\right\\|_{\\infty}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\eta-\\left\\|X^{\\top}D X\\right\\|_{\\infty}-\\gamma\\left\\|X^{\\top}D\\right\\|_{\\infty}\\left\\|X\\right\\|_{\\infty}}{\\eta-\\left\\|X^{\\top}D X\\right\\|_{\\infty}}\\left\\|\\theta_{\\eta}^{*}\\right\\|_{\\infty}<\\frac{1}{\\eta-\\left\\|X^{\\top}D X\\right\\|_{\\infty}}\\left\\|X^{\\top}D R\\right\\|_{\\infty},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which leads to ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\|\\theta_{\\eta}^{*}\\right\\|_{\\infty}\\leq\\frac{1}{\\eta-\\left\\|X^{\\top}D X\\right\\|_{\\infty}-\\gamma\\left\\|X^{\\top}D\\right\\|_{\\infty}\\left\\|X\\right\\|_{\\infty}}\\left\\|X^{\\top}D R\\right\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "As $\\eta\\rightarrow\\infty$ , the right-hand side of the above equation goes to zero, i.e., $\\theta_{\\eta}^{*}\\to0$ . ", "page_idx": 18}, {"type": "text", "text": "A.10 Proof of Lemma 3.7 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. The bias term of the solution can be obtained using simple algebraic inequalities. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lVert X\\theta_{\\eta}^{*}-Q^{*}\\rVert_{\\infty}\\leq\\left\\lVert\\Gamma_{\\eta}\\mathcal{T}(X\\theta_{\\eta}^{*})-\\Gamma Q^{*}\\right\\rVert_{\\infty}+\\left\\lVert\\Gamma_{\\eta}Q^{*}-Q^{*}\\right\\rVert_{\\infty}}&{}\\\\ {\\leq\\left\\lVert\\Gamma_{\\eta}\\right\\rVert_{\\infty}\\left\\lVert\\mathcal{T}(X\\theta_{\\eta}^{*})-Q^{*}\\right\\rVert_{\\infty}+\\left\\lVert\\Gamma_{\\eta}Q^{*}-Q^{*}\\right\\rVert_{\\infty}}&{}\\\\ {=\\left\\lVert\\Gamma_{\\eta}\\right\\rVert_{\\infty}\\left\\lVert\\mathcal{T}(X\\theta_{\\eta}^{*})-\\mathcal{T}(Q^{*})\\right\\rVert_{\\infty}+\\left\\lVert\\Gamma_{\\eta}Q^{*}-Q^{*}\\right\\rVert_{\\infty}}&{}\\\\ {\\leq\\gamma\\left\\lVert\\Gamma_{\\eta}\\right\\rVert_{\\infty}\\left\\lVert X\\theta_{\\eta}^{*}-Q^{*}\\right\\rVert_{\\infty}+\\left\\lVert\\Gamma_{\\eta}Q^{*}-Q^{*}\\right\\rVert_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The first inequality follows from triangle inequality. The third equality follows from the fact that $Q^{*}$ is the solution of optimal Bellman equation. The last inequality follows from the contraction property of the Bellman operator. Noting that $\\gamma\\left\\|\\Gamma_{\\eta}\\right\\|_{\\infty}<1$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\|X\\theta_{\\eta}^{*}-Q^{*}\\right\\|_{\\infty}\\leq\\frac{1}{1-\\gamma\\left\\|\\Gamma_{\\eta}\\right\\|_{\\infty}}\\left\\|\\Gamma_{\\eta}Q^{*}-Q^{*}\\right\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This finishes the proof. ", "page_idx": 18}, {"type": "text", "text": "A.11 Proofs to check Assumption A.2 for Theorem 5.2. ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we provide omitted proofs to check Assumption A.2 in Appendix Section A.2 to apply the Borkar and Meyn Theorem in Lemma A.1 in the Appendix. ", "page_idx": 18}, {"type": "text", "text": "First of all, Lipschitzness of $f(\\theta)$ ensures the unique solution of the O.D.E.. ", "page_idx": 18}, {"type": "text", "text": "Lemma A.15 (Lipschitzness). Let ", "page_idx": 18}, {"type": "equation", "text": "$$\nf(\\theta)=-(X^{T}D X+\\eta I)\\theta+\\gamma X^{T}D P\\Pi_{X\\theta}X\\theta+X^{T}D R.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, $f(\\theta)$ is globally Lipschitzness continuous. ", "page_idx": 18}, {"type": "text", "text": "Proof. Lipschitzness of $f(\\theta)$ can be proven as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{||f(\\theta)-f(\\theta^{\\prime})||_{\\infty}\\leq||(X^{T}D X+\\eta I)(\\theta-\\theta^{\\prime})||_{\\infty}+\\gamma||X^{T}D P(\\Pi_{X\\theta}X_{\\theta}-\\Pi_{X\\theta^{\\prime}}X\\theta^{\\prime})||_{\\infty}}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~\\leq||X^{T}D X+\\eta I||_{\\infty}||\\theta-\\theta^{\\prime}||_{\\infty}+\\gamma||X^{T}D P||_{\\infty}||\\Pi_{X\\theta}X\\theta-\\Pi_{X\\theta^{\\prime}}X\\theta^{\\prime}||_{\\infty}}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\leq(||X^{T}D X+\\eta I||_{\\infty}+\\gamma||X^{T}D P|||_{\\infty}||X||_{\\infty})||\\theta-\\theta^{\\prime}||_{\\infty}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The last inequality follows from non-expansiveness property of max-operator. Therefore $f(\\theta)$ is Lipschitz continuous with respect to the $||\\cdot||_{\\infty}$ , \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Next, the existence of limiting O.D.E. of (16) can be proved using the fact that policy is invariant under constant multiplication when linear function approximation is used. ", "page_idx": 18}, {"type": "text", "text": "Lemma A.16 (Existence of limiting O.D.E. and stability). Let ", "page_idx": 18}, {"type": "equation", "text": "$$\nf(\\theta)=(-X^{T}D X-\\eta I)\\theta+\\gamma X^{T}D P\\Pi_{X\\theta}X\\theta+X^{T}D R.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "If $\\eta$ satisfies (15), there exists limiting O.D.E. of (22) and its origin is asymptotically stable. ", "page_idx": 18}, {"type": "text", "text": "Proof. The existence of limiting O.D.E. can be obtained using the homogeneity of policy, $\\Pi_{X(c\\theta)}=$ \u03a0X\u03b8. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{f(c\\theta)=-(X^{T}D X+\\eta I)(c\\theta)+\\gamma X^{T}D P\\Pi_{X(c\\theta)}X(c\\theta)+X^{T}D R,}}\\\\ {{\\displaystyle\\operatorname*{lim}_{c\\to\\infty}\\frac{f(c x)}{c}=(-X^{T}D X-\\eta I+\\gamma X^{T}D P\\Pi_{X\\theta}X)\\theta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This can be seen as switching system and, from Lemma A.8 and Lemma A.13, we can apply Lemma 2.5. Therefore, the origin is asymptotically stable. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Lastly, we check conditions for martingale difference sequences. ", "page_idx": 19}, {"type": "text", "text": "Lemma A.17 (Martingale difference sequence, $m_{k}$ , and square integrability). We have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~~\\mathbb{E}[m_{k+1}|\\mathcal{F}_{k}]=0,}\\\\ &{\\mathbb{E}[||m_{k+1}||_{2}^{2}|\\mathcal{F}_{k}]<C_{0}(1+||\\theta_{k}||_{2}^{2}),}\\\\ &{~~~~~~~~~\\frac{1}{2}\\textmd\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $C_{0}:=\\operatorname*{max}(12X_{\\operatorname*{max}}^{2}R_{\\operatorname*{max}}^{2},12\\gamma X_{\\operatorname*{max}}^{4}+4\\eta^{2}).$ ", "page_idx": 19}, {"type": "text", "text": "Proof. To show $\\{m_{k},k\\in\\mathbb{N}\\}$ is a martingale difference sequence with respect to the sigma-algebra generated by $\\mathcal{G}_{k}$ , we first prove expectation of $m_{k+1}$ is zero conditioned on $\\mathcal{G}_{k}$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}[m_{k+1}|\\mathcal{G}_{k}]=0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This follows from definition of $b,C$ and $A_{\\pi_{X\\theta}}$ ", "page_idx": 19}, {"type": "text", "text": "The boundedness $\\mathbb{E}[||m_{k}||_{2}]\\;<\\;\\infty$ for $k\\ \\in\\ \\mathbb{N}$ also follows from simple algebraic inequalities.   \nTherefore $\\{m_{k},k\\in\\tilde{\\mathbb{N}}\\}$ is martingale difference sequence. ", "page_idx": 19}, {"type": "text", "text": "Now, we show that the following hods: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}[||m_{k+1}||_{2}^{2}|\\mathcal{G}_{k}]\\le C_{0}(||\\theta_{k}||_{2}^{2}+1).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using simple algebraic inequalities, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\|m_{k+1}\\|_{2}^{2}|\\mathcal{G}_{k}]=\\mathbb{E}[\\|\\delta_{k}x(s_{k},a_{k})+\\eta\\theta_{k}-\\mathbb{E}_{\\mu}[\\delta_{k}x(s_{k},a_{k})+\\eta\\theta_{k}]\\|_{2}^{2}|\\mathcal{G}_{t}]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\mathbb{E}[\\|\\delta_{k}x(s_{k},a_{k})+\\eta\\theta_{k}\\|_{2}^{2}+|\\|\\mathbb{E}_{\\mu}[\\delta_{k}x(s_{k},a_{k})+\\eta\\theta_{k}]\\|_{2}^{2}|\\mathcal{G}_{t}]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq2\\mathbb{E}[\\|\\delta_{k}x(s_{k},a_{k})+\\eta\\theta_{k}\\|_{2}^{1}\\|_{2}^{2}|\\mathcal{G}_{t}]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\leq4\\mathbb{E}[\\|\\delta_{k}x(s_{k},a_{k})\\|_{2}^{2}|\\mathcal{G}_{t}]+4\\eta^{2}\\mathbb{E}[\\|\\theta_{k}\\|_{2}^{2}|\\mathcal{G}_{t}]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq12X_{\\operatorname*{max}}^{2}\\mathbb{E}[\\|r_{k}\\|_{2}^{2}+\\|\\gamma\\operatorname*{max}x(s_{k},a_{k})\\theta_{k}\\|_{2}^{2}+||x(s_{k},a_{k})\\theta_{k}||_{2}^{2}|\\mathcal{G}_{t}]+4\\eta^{2}||\\theta_{k}||_{2}^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\leq12X_{\\operatorname*{max}}^{2}R_{\\operatorname*{max}}^{2}+12\\gamma X_{\\operatorname*{max}}^{4}||\\theta_{k}||_{2}^{2}+||\\theta_{k}||_{2}^{2}+4\\eta^{2}||\\theta_{k}||_{2}^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq C_{0}(1+\\|\\theta_{k} \n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $C_{0}:=\\operatorname*{max}(12X_{\\operatorname*{max}}^{2}R_{\\operatorname*{max}}^{2},12\\gamma X_{\\operatorname*{max}}^{4}+4\\eta^{2})$ . The fourth inequality follows from the fact that $||a+b+c||_{2}^{2}\\leq3||a||_{2}^{2}+3||b||_{2}^{2}+3||c||_{2}^{2}$ . Together with the above inequality, the square integrability of $m_{k}$ for $k\\in\\mathbb{N}$ follows from the recursive update of $\\theta_{k}$ in (14). This completes the proof. ", "page_idx": 19}, {"type": "text", "text": "A.12 Proof of Theorem 5.1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Before moving onto the proof of Theorem 5.1, in order to prove the stability using the upper and lower systems, we need to introduce some notions such as the quasi-monotone function and vector comparison principle. We first introduce the notion of quasi-monotone increasing function, which is a necessary prerequisite for the comparison principle for multidimensional vector system. ", "page_idx": 19}, {"type": "text", "text": "Definition A.18 (Quasi-monotone function). Consider a vector-valued function $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{n}$ with $f:=\\left[f_{1}\\quad f_{2}\\quad\\cdot\\cdot\\quad f_{n}\\right]^{T}$ where $f_{i}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ for $i\\in\\{1,2,\\dots,n\\}$ . $f$ is said to be quasi-monotone increasing if $f_{i}(x)\\,\\leq\\,f_{i}(y)$ holds for all $i\\,\\in\\,\\{1,2,\\dots,n\\}$ and $\\boldsymbol{x},\\boldsymbol{y}\\in\\mathbb{R}^{n}$ such that $x_{i}=y_{i}$ and $x_{j}\\leq y_{j}$ for all $j\\neq i$ . ", "page_idx": 19}, {"type": "text", "text": "Based on the notion of quasi-monotone function, we introduce the vector comparison principle. ", "page_idx": 20}, {"type": "text", "text": "Lemma A.19 (Vector Comparison Principle, Hirsch and Smith [2006]). Suppose that $\\bar{f},f$ are globally Lipschitz continuous. Let $x_{t}$ be a solution of the system ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{d}{d t}x_{t}=\\bar{f}(x_{t}),\\quad x_{0}\\in\\mathbb{R}^{n},\\quad\\forall t\\geq0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Assume that $\\bar{f}$ is quasi-monotone increasing, and let $v_{t}$ be a solution of the system ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{d}{d t}v_{t}=f(v_{t}),\\quad v_{0}<x_{0},\\quad\\forall t\\geq0,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $f(v)\\leq\\bar{f}(v)$ holds for any $\\boldsymbol{v}\\in\\mathbb{R}^{n}$ . Then, $v_{t}\\leq x_{t}$ for all $t\\geq0$ . ", "page_idx": 20}, {"type": "text", "text": "The vector comparison lemma can be used to bound the state trajectory of the original system by those of the upper and lower systems. Then, proving global asymptotic stability of the upper and lower systems leads to global asymptotic stability of original system. We now give the proof of Theorem 5.1. ", "page_idx": 20}, {"type": "text", "text": "Proof. First we construct the upper comparison part. Noting that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\gamma X^{T}D P\\Pi_{X\\theta_{\\eta}^{*}}X\\theta_{\\eta}^{*}\\geq\\gamma X^{T}D P\\Pi_{X\\theta}X\\theta_{\\eta}^{*}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\gamma X^{T}D P\\Pi_{X(\\theta-\\theta_{\\eta}^{*})}X(\\theta-\\theta_{\\eta}^{*})\\geq\\gamma X^{T}D P\\Pi_{X\\theta}X(\\theta-\\theta_{\\eta}^{*}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "we define $\\bar{f}(y)$ and $\\ensuremath{\\underline{{f}}}(y)$ as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{f}(y)=(-X^{T}D X-\\eta I+\\gamma X^{T}D P\\Pi_{X y}X)y,}\\\\ &{f(y)=(-X^{T}D X-\\eta I+\\gamma X^{T}D P\\Pi_{X(y+\\theta_{\\eta}^{*})}X)y+\\gamma X^{T}D P(\\Pi_{X(y+\\theta_{\\eta}^{*})}-\\Pi_{X\\theta_{\\eta}^{*}})X\\theta_{\\eta}^{*}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using (23) and (24), we have $f(y)\\leq{\\bar{f}}(y)$ . ", "page_idx": 20}, {"type": "text", "text": "$\\underline{{f}}$ is the corresponding O.D.E. of original system in (17) and $\\bar{f}$ becomes O.D.E. of the upper system.   \n$\\bar{f}$ becomes switched linear system. ", "page_idx": 20}, {"type": "text", "text": "Now, consider the O.D.E. systems ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\displaystyle{\\frac{d}{d t}}\\theta_{t}^{u}=\\bar{f}(\\theta_{t}^{u}),\\quad}}&{{\\theta_{0}^{u}>\\theta_{0},}}\\\\ {{\\displaystyle{\\frac{d}{d t}}\\theta_{t}=f(\\theta_{t}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Next, we prove quasi-monotone increasing property of $\\bar{f}$ . For any $y\\in\\mathbb{R}^{h}$ , consider a non-negative vector $p\\in\\mathbb{R}^{h}$ such that its $i$ -th element is zero. Then, for any $1\\leq i\\leq h$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{e_{i}^{T}\\bar{f}(y+p)=e_{i}^{T}(-X^{T}D X-\\eta I+\\gamma X^{T}D P\\Pi_{X(y+p)}X)(y+p)}\\\\ &{\\qquad\\qquad=-e_{i}^{T}(X^{T}D X+\\eta I)y-\\eta e_{i}^{T}p+\\gamma e_{i}^{T}X^{T}D P\\Pi_{X(y+p)}X(y+p)}\\\\ &{\\qquad\\qquad\\geq-e_{i}^{T}(X^{T}D X+\\eta I)y+\\gamma e_{i}^{T}X^{T}D P\\Pi_{X y}X y}\\\\ &{\\qquad\\qquad=e_{i}^{T}\\bar{f}(y),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the inequality comes from $e_{i}^{T}X^{T}D X p=0$ due to Assumption 2.2 and $e_{i}^{T}p=0$ since $i$ -th element of $p$ is zero. ", "page_idx": 20}, {"type": "text", "text": "Therefore by Lemma A.19, we can conclude that $\\theta_{t}\\leq\\theta_{t}^{u}$ . The condition $(S1)$ in (15) ensures the switching matrices to have strictly negatively row dominating diagonal. Therefore, from Lemma A.13, and from Lemma 2.5, the global asymptotically stability of the origin follows. Likewise, the condition $(S2)$ in (15) ensures that the matrices are all negative-definite, implying that the switching system shares $V(\\theta)=||\\theta||_{2}^{2}$ as common Lyapunov function. Therefore, we can conclude that the upper comparison system is globally asymptotically stable. ", "page_idx": 20}, {"type": "text", "text": "For the lower comparison part, noting that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\gamma X^{T}D P\\Pi_{X\\theta}X\\theta\\geq\\gamma X^{T}D P\\Pi_{X\\theta_{\\eta}^{*}}X\\theta,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "we can define $\\mathcal{L}(y)$ and $\\bar{f}(y)$ such that $f(y)\\leq{\\bar{f}}(y)$ as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{f}(y)=-X^{T}D X y-\\eta y+\\gamma X^{T}D P\\Pi_{X y}X y+X^{T}D R,}\\\\ &{f(y)=-X^{T}D X y-\\eta y+\\gamma X^{T}D P\\Pi_{X\\theta_{\\eta}^{*}}X y+X^{T}D R.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The corresponding O.D.E. system becomes ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{d}{d t}\\theta_{t}=\\bar{f}(\\theta_{t}),}\\\\ {\\displaystyle\\frac{d}{d t}\\theta_{t}^{l}=f(\\theta_{t}^{l}),\\quad\\theta_{0}^{l}<\\theta_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proving the quasi-monotonicity of $\\bar{f}$ is similar to previous step. Consider a non-negative vector $p\\in\\mathbb{R}^{\\bar{h}}$ such that its $i$ -th element is zero. Then, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{e_{i}^{T}\\bar{f}(y+p)=e_{i}^{T}(-(X^{T}D X+\\eta I)(y+p)+\\gamma X^{T}D P\\Pi_{X(y+p)}X(y+p)+X^{T}D R)}\\\\ &{\\qquad\\qquad=e_{i}^{T}(-(X^{T}D X+\\eta I)y+\\gamma X^{T}D P\\Pi_{X(y+p)}X(y+p)+X^{T}D R)}\\\\ &{\\qquad\\qquad\\geq e_{i}^{T}(-(X^{T}D X+\\eta I)y+\\gamma X^{T}D P\\Pi_{X y}X y+X^{T}D R)}\\\\ &{\\qquad\\qquad=e_{i}^{T}\\bar{f}(y).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The second equality holds since $X^{T}D X$ is diagonal matrix and $p_{i}=0$ . Therefore by Lemma A.19, we can conclude that $\\theta_{t}^{l}\\leq\\theta_{t}$ . The lower comparison part is linear system without affine term. Hence, following the similar lines as in proving the stability of upper comparison system, we can conclude that (25) is globally asymptotically stable. ", "page_idx": 21}, {"type": "text", "text": "To prove uniqueness of the equilibrium point, assume there exists two different equilibrium points $\\theta_{1}^{e}$ and $\\theta_{2}^{e}$ . The global asymptotic stability implies that regardless of initial state, $\\theta_{t}\\to\\theta_{1}^{e}$ and $\\theta_{t}\\rightarrow\\theta_{2}^{e}$ . However this becomes contradiction if $\\theta_{1}^{e}\\neq\\theta_{2}^{e}$ . Therefore, the equilibrium point is unique. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "A.13 Proof of Theorem 5.2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof. To apply Lemma A.1, let us check Assumption A.2. ", "page_idx": 21}, {"type": "text", "text": "1. First and second statement of Assumption A.2 follows from Lemma A.16.   \n2. Third statement of Assumption A.2 follows from Theorem 5.1.   \n3. Fourth statement of Assumption A.2 follows from Lemma A.17. ", "page_idx": 21}, {"type": "text", "text": "Since we assumed Robbins Monro step-size, we can now apply Lemma A.1 to complete the proof. ", "page_idx": 21}, {"type": "text", "text": "A.14 Example for non-existence of solution of PBE ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Let us define a MDP whose state transition diagram is given as in Figure 3. The cardinality of state space and action space are $|{\\cal S}|=3$ , $|{\\mathcal{A}}|=2$ respectively. The corresponding state transition matrix, and other parameters are given as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{X}=\\left[\\begin{array}{l l}{1}&{0}\\\\ {2}&{0}\\\\ {0}&{1}\\\\ {0}&{1}\\\\ {2}&{0}\\end{array}\\right]\\,,\\;\\boldsymbol{R}_{1}=\\left[\\begin{array}{l}{-2}\\\\ {0}\\\\ {0}\\end{array}\\right]\\,,\\;\\boldsymbol{R}_{2}=\\left[\\begin{array}{l}{1}\\\\ {0}\\\\ {0}\\end{array}\\right]\\,,}\\\\ &{\\boldsymbol{P}_{1}=\\left[\\begin{array}{l l}{0}&{1}\\\\ {\\frac{1}{4}}&{\\frac{1}{4}}\\end{array}\\right]\\,,\\;\\boldsymbol{P}_{2}=\\left[\\begin{array}{l l l}{0}&{0}&{1}\\\\ {\\frac{1}{4}}&{\\frac{1}{4}}&{\\frac{1}{2}}\\\\ {\\frac{1}{4}}&{\\frac{1}{2}}&{\\frac{1}{4}}\\end{array}\\right]\\,,}\\\\ &{\\boldsymbol{\\gamma}=0.\\boldsymbol{99},\\quad\\boldsymbol{d}(s,a)=\\frac{1}{6}\\,,\\,\\forall s\\in\\mathcal{S},\\forall a\\in\\mathcal{A}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "image", "img_path": "4sueqIwb4o/tmp/f9981c80f230a9994f9167a2bce86c551321f792ad6ee3103e42c49678bf2abb.jpg", "img_caption": ["Figure 3: State transition diagram "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "where the order of elements of each column follows the orders of the corresponding definitions. Note that for this Markov decision process, taking action $a=1$ and action $a=2$ at state $s=2$ have the same transition probabilities and reward. It is similar for the state $s=3$ . In this MDP, there are only two deterministic policies available, denoted by $\\pi_{1}$ and $\\pi_{2}$ , that selects action $a=1$ and action $a=2$ at state $s=1$ , respectively, i.e., $\\pi_{1}(1)=1$ and $\\pi_{2}(1)=2$ . The actions at state $s=2$ and $s=3$ do not affect the overall results. ", "page_idx": 22}, {"type": "text", "text": "The motivation of this MDP is as follows. Substitute $\\pi_{X\\theta^{*}}$ in (5) with $\\pi_{1}$ and $\\pi_{2}$ . Then each of its solution becomes ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\theta^{e1}:=\\left[\\!\\!\\begin{array}{c}{\\!\\!\\theta_{1}^{e1}\\!}\\\\ {\\!\\!\\theta_{2}^{e1}\\!}\\end{array}\\!\\!\\right]\\approx\\left[\\!\\!\\begin{array}{c}{\\!\\!-0.85\\!}\\\\ {\\!\\!-0.72\\!}\\end{array}\\!\\!\\right],\\quad\\theta^{e2}:=\\left[\\!\\!\\begin{array}{c}{\\!\\!\\theta_{1}^{e2}\\!}\\\\ {\\!\\!\\theta_{2}^{e2}\\!}\\end{array}\\!\\!\\right]\\approx\\left[\\!\\!\\begin{array}{c}{\\!\\!-1.26\\!}\\\\ {\\!\\!-1.46\\!}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "If $\\pi_{1}$ is the corresponding policy to the solution of (5), it means that action $a\\,=\\,1$ is greedily selected at state $s\\ =\\ 1$ . Therefore, $Q^{\\pi_{1}}(1,1)\\,>\\,Q^{\\pi_{1}}(1,2)$ should be satisfied. However, since $Q^{\\pi_{1}}(1,1)\\,=\\,x(1,1)^{T}\\theta^{e1}\\,\\approx\\,-0.85$ and $\\dot{Q}^{\\pi_{1}}(1,2)\\,=\\,\\dot{x}(1,\\dot{2})^{T}\\theta^{e1}\\,\\approx\\,-0.72$ , this is contradiction. The same logic applies to the case for $\\pi_{2}$ . Therefore, neither of them becomes a solution of (5). On the other hand, considering (7) with $\\eta\\,=\\,4$ which satisfies (15), the solution for each policy becomes $\\theta_{1}^{e1}\\approx-0.069,\\theta_{2}^{e1}\\approx\\stackrel{\\circ}{0.032}$ and $\\dot{\\theta}_{1}^{e2}\\approx-0.069,\\theta_{2}^{e2}\\approx0.035$ , respectively. For $\\pi_{1}$ and $\\pi_{2}$ , we have $Q^{\\pi_{1}}(1,1)<Q^{\\pi_{2}}(1,2)$ and $Q^{\\pi_{1}}(1,1)<Q^{\\pi_{1}}(1,2)$ respectively. Hence, $\\theta^{e\\bar{2}}$ satisfies (7) and becomes the unique solution. ", "page_idx": 22}, {"type": "text", "text": "A.15 Discussion on (15) ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we provide further discussion on (15). The two conditions $(S1)$ and $(S2)$ are to make a matrix to have strictly negatively row dominating diagonal or negative definite, respectively. We first provide a case where a matrix with strictly negatively row dominating diagonal is not necessarily a negative definite matrix, and vice versa. We will consider MDPs with $\\gamma=0.99$ . ", "page_idx": 22}, {"type": "text", "text": "A matrix with strictly negatively row dominating diagonal but not negative-definite: Consider the following MDP with only single action for each state: ", "page_idx": 22}, {"type": "equation", "text": "$$\nX=\\left[\\!\\!\\begin{array}{c c}{{13}}&{{-4}}\\\\ {{1}}&{{8}}\\end{array}\\!\\!\\right],\\quad P=\\left[\\!\\!\\begin{array}{c c}{{0}}&{{1}}\\\\ {{0}}&{{1}}\\end{array}\\!\\!\\right],\\quad D=\\left[\\!\\!\\begin{array}{c c}{{{\\frac{1}{2}}}}&{{0}}\\\\ {{0}}&{{{\\frac{1}{2}}}}\\end{array}\\!\\!\\right],\\quad\\Pi=\\left[\\!\\!\\begin{array}{c c}{{1}}&{{0}}\\\\ {{0}}&{{1}}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the matrix $\\Pi$ represents the policy. Then, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\nM:=-X^{\\top}D X+\\gamma X^{\\top}D P\\Pi X\\approx\\left[\\!\\!\\begin{array}{c c}{-78}&{-77}\\\\ {24}&{-24.2}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This is a matrix with strictly negatively row dominating diagonal but $M+M^{\\top}$ is not negative definite matrix. ", "page_idx": 22}, {"type": "text", "text": "A negative-definite matrix but not with a strictly negatively row dominating diagonal: Consider the following MDP with single action for each state: ", "page_idx": 22}, {"type": "equation", "text": "$$\nX=\\left[\\!\\!\\begin{array}{c c}{{-1}}&{{-4}}\\\\ {{0}}&{{5}}\\end{array}\\!\\!\\right],\\quad P=\\left[\\!\\!\\begin{array}{c c}{{\\frac12}}&{{\\frac12}}\\\\ {{\\frac12}}&{{\\frac12}}\\end{array}\\!\\!\\right],\\quad D=\\left[\\!\\!\\begin{array}{c c}{{\\frac12}}&{{0}}\\\\ {{0}}&{{\\frac12}}\\end{array}\\!\\!\\right],\\quad\\Pi=\\left[\\!\\!\\begin{array}{c c}{{1}}&{{0}}\\\\ {{0}}&{{1}}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\nM:=-X^{\\top}D X+\\gamma X^{\\top}D P\\Pi X\\approx\\left[-0.25\\quad-2.25\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "$M+M^{\\top}$ is negative definite matrix but $M$ does not have strictly negatively row dominating diagonal. Now, we provide an example where the condition $(S1)$ and $(S2)$ in (15) does not imply each other, i.e., there are cases such that $(S1)\\ge(S2)$ or $(S2)\\geq(S1)$ : ", "page_idx": 23}, {"type": "text", "text": "As for the condition in (15), consider the following MDP with single action for each state: ", "page_idx": 23}, {"type": "equation", "text": "$$\nX=\\left[\\!\\!\\begin{array}{c}{{1}}\\\\ {{2}}\\end{array}\\!\\!\\right],\\quad D=\\left[\\!\\!\\begin{array}{c c}{{{\\frac{1}{100}}}}&{{0}}\\\\ {{0}}&{{{\\frac{99}{100}}}}\\end{array}\\!\\!\\right],\\quad P_{1}=\\left[\\!\\!\\begin{array}{c c}{{{\\frac{1}{2}}}}&{{{\\frac{1}{2}}}}\\\\ {{{\\frac{1}{2}}}}&{{{\\frac{1}{2}}}}\\end{array}\\!\\!\\right],\\quad P_{2}=\\left[\\!\\!\\begin{array}{c c}{{0}}&{{1}}\\\\ {{0}}&{{1}}\\end{array}\\!\\!\\right],\\quad\\Pi=\\left[\\!\\!\\begin{array}{c c}{{1}}&{{0}}\\\\ {{0}}&{{1}}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $P_{1}$ and $P_{2}$ are two different transition matrices and the matrix $\\Pi$ represents the policy. First, considering $P_{1}$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n(S1)\\approx7.9,\\quad(S2)\\approx196.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Meanwhile, considering $P_{2}$ as the transition matrix, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n(S1)\\approx7.9,\\quad(S2)\\approx2.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, $(S1)$ and $(S2)$ does not necessarily imply each other. ", "page_idx": 23}, {"type": "text", "text": "A.16 Pseudo-code ", "text_level": 1, "page_idx": 23}, {"type": "table", "img_path": "4sueqIwb4o/tmp/86b8de3f415c5a1195ab1c8e5d4ea0865867af7415e1f9613260a02eec33e3e0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "B Experiment ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "B.1 Experiments ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we present experimental results under well-known environments in Tsitsiklis and Van Roy [1996], Baird [1995], where Q-learning with linear function approximation diverges. In Appendix B.6, we also compare performance under the Mountain Car environment [Sutton and Barto, 2018] where Q-learning performs well. In Appendix B.5, we show experimental results under various step-sizes and $\\eta$ . We also show trajectories of the O.D.E. of upper and lower comparison systems to illustrate the theoretical results. ", "page_idx": 23}, {"type": "text", "text": "B.2 $\\theta\\rightarrow2\\theta$ , Tsitsiklis and Van Roy [1996] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Even when there are only two states, Q-learning with linear function approximation could diverge [Tsitsiklis and Van Roy, 1996]. Depicted in Figure 4a in Appendix B.4, from state one $\\left(\\theta\\right)$ , the transition is deterministic to absorbing state two (2\u03b8), and reward is zero at every time steps. Therefore, the episode length is fixed to be two. Learning rate for Greedy GQ (GGQ) and Coupled Q Learning (CQL), which have two learning rates, are set as 0.05 and 0.25, respectively as in Carvalho et al. [2020], Maei et al. [2010]. Since CQL requires normalized feature values, we scaled the feature value with $\\bar{\\frac12}$ as in Carvalho et al. [2020], and initialized weights as one. We implemented Q-learning with target network [Zhang et al., 2021], which also have two learning rates, without projection for practical reason (Qtarget). We set the learning rate as 0.25 and 0.05 respectively, and the weight $\\eta$ as two. For RegQ, we set the learning rate as 0.25, and the weight $\\eta$ as two. It is averaged over five runs. In Figure 2a, we can see that RegQ achieves the fastest convergence rate. ", "page_idx": 23}, {"type": "text", "text": "B.3 Baird Seven Star Counter Example, Baird [1995] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Baird [1995] considers an overparameterized example, where Q-learning with linear function approximation diverges. The overall state transition is depicted in Figure 4b given in Appendix B.4. ", "page_idx": 23}, {"type": "text", "text": "There are seven states and two actions for each state, which are solid and dash action. The number of features are $h=15$ . At each episode, it is initialized at random state with uniform probability. Solid action leads to seventh state while dashed action makes transition uniformly random to states other than the seventh state. At seventh state, the episode ends with probability $\\frac{1}{100}$ . The behavior policy selects dashed action with probability $\\frac{5}{6}$ , and solid action with probability $\\frac{1}{6}$ . Since CQL in Carvalho et al. [2020] converges under normalized feature values, we scaled the feature matrix with $\\textstyle{\\frac{1}{\\sqrt{5}}}$ . The weights are set as one except for . The learning rates and the weight $\\eta$ are set as same as the previous experiment. As in Figure 2b, Our RegQ shows the fastest convergence compared to other convergent algorithms. ", "page_idx": 24}, {"type": "text", "text": "B.4 Diagrams for $\\theta\\rightarrow2\\theta$ and Baird Seven Star Counter Example ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The state transition diagrams of $\\theta\\rightarrow2\\theta$ and Baird seven-star example are depicted in Figure 4a and Figure 4b respectively. ", "page_idx": 24}, {"type": "image", "img_path": "4sueqIwb4o/tmp/69d88aeeb21d8b534b3c57052550744db641184f20d458d64cca620bb734f686.jpg", "img_caption": ["Figure 4: Counter-examples where Q-learning with linear function approximation diverges ", "(b) Baird seven star counter example "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "B.5 Experiments with varying hyperparameters ", "text_level": 1, "page_idx": 24}, {"type": "image", "img_path": "4sueqIwb4o/tmp/fbef90bc84f06421168a5051053a46ce97f41dc7c4039edb04631995ded1c666.jpg", "img_caption": ["Figure 5: Learning curve under different learning rate and regularization coefficient "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "In Figure 5, we have ran experiments under $\\eta\\,\\in\\,\\{2^{-2},2^{-1},1,2\\}$ , and learning rate 0..01, 0.05.   \nOverall, we can see that the convergence rate gets faster as $\\eta$ increases. ", "page_idx": 25}, {"type": "text", "text": "B.6 Mountain car [Sutton and Barto, 2018] experiment ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Mountain Car is environment where state consists of position, and velocity, which are both continuous values. The actions are discrete, accelerating to left, staying neutral, and accelerating to the right. The goal is to reach the top of the mountain quickly as agent gets $^-1$ reward every time step. We use tile-coding [Sutton and Barto, 2018] to discretize the states. We experimented under various tiling numbers and with appropriate $\\eta$ , it achieves performance as Q-learning does. We ran 1000 episodes for the training process, and the episode reward was averaged for 100 runs during test time. From Table 1, with appropriate $\\eta$ , RegQ performs comparable to Q-learning. ", "page_idx": 25}, {"type": "table", "img_path": "4sueqIwb4o/tmp/a6adeb91be831adc8a3ce309c62edd90c93478928316a831a8aa0c9054671d32.jpg", "table_caption": ["Table 1: Result of episode reward, step size $=0.1$ . The columns correspond to $\\eta$ , and rows correspond to number of tiles. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "B.7 O.D.E. experiment ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Let us consider a MDP with $|S|=2,|A|=2$ , and the following parameters: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{X=\\left[\\begin{array}{l l}{1}&{0}\\\\ {0}&{2}\\\\ {1}&{0}\\\\ {0}&{2}\\end{array}\\right],\\quad D=\\left[\\begin{array}{l l l l}{\\frac{1}{4}}&{0}&{0}&{0}\\\\ {0}&{\\frac{1}{4}}&{0}&{0}\\\\ {0}&{0}&{\\frac{1}{4}}&{0}\\\\ {0}&{0}&{0}&{\\frac{1}{4}}\\end{array}\\right],}\\\\ &{P=\\left[\\begin{array}{l l}{0.5}&{0.5}\\\\ {1}&{0}\\\\ {0.5}&{0.5}\\\\ {0.25}&{0.75}\\end{array}\\right],\\quad R=\\left[\\begin{array}{l}{1}\\\\ {1}\\\\ {1}\\end{array}\\right],\\quad\\gamma=0.99.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For this MDP, we will illustrate trajectories of the upper and lower system. Each state action pair is sampled uniformly random and reward is one for every time step. $\\eta=2.25$ is chosen to satisfy conditions of Theorem 5.1. From Figure 6, we can see that the trajectory of the original system is bounded by the trajectories of lower and upper system. ", "page_idx": 26}, {"type": "image", "img_path": "4sueqIwb4o/tmp/60f4824c4ed5e1926a2316a271b48b5fb45ea432d0ab3b733b31e634e9dea994.jpg", "img_caption": ["Figure 6: O.D.E. results "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We have rigorously proved the result of a convergent Q-learning algorithm, and provided thorough analysis on the quality of its solution. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We have cleary stated the Assumptions used througout the paper. Moreover, as a limitation, we discuseed that the convergence solution is biased, and provided thorough analysis on its error bound. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We have provided thorough analysis in the Appendix. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The experimantal envirnoments are fairly simple and well-known in the community, thus making it easy to reproduce. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We have attached the code in the supplementary files. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We have stated the choice of step-size, which is the only hyper-parameter. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We have plotted the error bar. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: NA ", "page_idx": 30}, {"type": "text", "text": "Justification: Our experiments can simply run on normal computer because we do not require any heavy computation including using GPU. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The authors have checked NeurIPS Code of Ethics. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper is a theoretical paper. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This is a theoretical paper. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification:We did not use existing assets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 31}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: There are no such risks. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]