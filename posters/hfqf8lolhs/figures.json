[{"figure_path": "HfQF8LoLhs/figures/figures_8_1.jpg", "caption": "Figure 1: Trajectories to approximate a Gaussian mixture with a Gaussian family, represented under different parameterizations.", "description": "This figure displays the trajectories of five different variational inference algorithms (MAX, UNB, SGE, SGM, NAT) while approximating a Gaussian mixture target using a single Gaussian family.  The three subplots represent the trajectories in three different parameter spaces: the original parameters (\u03b8), the natural parameters (\u03b7), and the mean parameters (\u03bc). Each subplot shows how the algorithms converge to a local minimum of the alpha-divergence. The different colors represent the different algorithms, and the shaded surface represents the alpha-divergence landscape.  The figure illustrates the impact of parameterization on algorithm behavior and convergence paths.", "section": "6 Experiments"}, {"figure_path": "HfQF8LoLhs/figures/figures_9_1.jpg", "caption": "Figure 1: Trajectories to approximate a Gaussian mixture with a Gaussian family, represented under different parameterizations.", "description": "This figure visualizes the performance of five different variational inference algorithms (MAX, UNB, SGE, SGM, NAT) in approximating a Gaussian mixture distribution using a Gaussian variational family.  The trajectories of each algorithm's parameters (mean and variance) are plotted across iterations, showcasing their convergence behavior toward a local minimum of the alpha-divergence. The three subplots represent different parameterizations (Gaussian mixture parameters, Gaussian family parameters using natural parameters \u03b7, and Gaussian family parameters using mean parameters \u03bc). The figure highlights the different convergence behaviors of the algorithms under different parameterizations and loss landscapes, with MAX and UNB demonstrating more direct paths towards local minima compared to the other algorithms, which exhibit more erratic trajectories due to unfavorable landscapes. ", "section": "6 Experiments"}, {"figure_path": "HfQF8LoLhs/figures/figures_19_1.jpg", "caption": "Figure 1: Trajectories to approximate a Gaussian mixture with a Gaussian family, represented under different parameterizations.", "description": "This figure compares the performance of five different algorithms (MAX, UNB, SGE, SGM, NAT) in approximating a Gaussian mixture using a Gaussian family. Each algorithm's trajectory in the parameter space is shown for both parameterizations (\u03b7 and \u03bc). The alpha-divergence (a=0.5) is used as the objective function. The figure highlights the different convergence behaviors and paths toward a local minimum of the alpha-divergence for each algorithm.  The variations in trajectories reflect the algorithms' sensitivity to the loss landscape and the chosen parameterization.", "section": "6 Experiments"}, {"figure_path": "HfQF8LoLhs/figures/figures_19_2.jpg", "caption": "Figure 1: Trajectories to approximate a Gaussian mixture with a Gaussian family, represented under different parameterizations.", "description": "This figure visualizes the trajectories of five different variational inference algorithms (MAX, UNB, SGE, SGM, NAT) while approximating a Gaussian mixture using a Gaussian family.  Each algorithm's path through the parameter space is shown for three different parameterizations (par. 0, par. \u03b7, par. \u03bc), providing a visual comparison of their convergence behavior. The alpha-divergence (with \u03b1=0.5) is used as the optimization criterion. The different trajectories highlight the impact of algorithm choice and parameterization on convergence speed and final result.", "section": "6 Experiments"}]