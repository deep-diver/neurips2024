[{"figure_path": "l5SbrtvSRS/tables/tables_1_1.jpg", "caption": "Table 1: Comparison of different model merging methods. A merging method is deemed self-aware if it manages parameter competition within individual task models, and cross-aware if it balances competition within a population of task models. For more details, please refer to App. A.", "description": "This table compares different model merging methods based on their ability to manage parameter competition within individual tasks (self-aware) and across multiple tasks (cross-aware).  It also shows the granularity level at which the method operates (parameter, task, or layer). The table helps to understand the differences between various existing approaches and highlights the unique aspects of PCB-MERGING.", "section": "1 Introduction"}, {"figure_path": "l5SbrtvSRS/tables/tables_6_1.jpg", "caption": "Table 2: Comparison of different model merging methods across various fine-tuning configurations and modalities, with average performance reported for different tasks.", "description": "This table compares the performance of various model merging methods across different tasks and modalities (NLP, PEFT, LLMs, and Vision). It shows the average performance improvement achieved by PCB-MERGING compared to several baseline methods, including simple averaging, Fisher merging, RegMean, Task Arithmetic, and Ties-Merging.  The table highlights PCB-MERGING's consistent performance gains across various tasks and model types, demonstrating its effectiveness in different settings.", "section": "5 Results"}, {"figure_path": "l5SbrtvSRS/tables/tables_6_2.jpg", "caption": "Table 3: Comparison of the performance of different methods on 3 datasets after merging LLMs.", "description": "This table compares the performance of various model merging methods on three different tasks (Chinese language proficiency, mathematical reasoning, and code generation) after merging three large language models.  The methods compared include averaging, Task Arithmetic, TIES-Merging, and PCB-MERGING (with and without DARE preprocessing).  The table shows the average performance across the three tasks, highlighting PCB-MERGING's improved performance, especially with the inclusion of DARE preprocessing.", "section": "5 Results"}, {"figure_path": "l5SbrtvSRS/tables/tables_7_1.jpg", "caption": "Table 2: Comparison of different model merging methods across various fine-tuning configurations and modalities, with average performance reported for different tasks.", "description": "This table compares the performance of various model merging methods across different tasks and modalities (NLP, PEFT, LLMs, and Vision).  It shows the average performance of fine-tuned models, multi-task learning models and several advanced merging techniques including averaging, Fisher Merging, RegMean, Task Arithmetic, TIES-Merging, and PCB-MERGING.  The results are presented for different model sizes (T5-base, T5-large, ViT-B/32, ViT-L/14) and configurations, and highlight the superior performance of the proposed PCB-MERGING method across various settings, both with and without a validation set.", "section": "5 Results"}, {"figure_path": "l5SbrtvSRS/tables/tables_8_1.jpg", "caption": "Table 5: Ablation study on individual components of PCB-MERGING.", "description": "This table presents the results of ablation experiments conducted to evaluate the impact of individual components of the PCB-MERGING method.  It shows the performance of the method when different components are removed, such as intra-balancing, inter-balancing, the drop mechanism, and rescaling. The results highlight the importance of each component for achieving optimal performance. The ablation experiments were performed on two model architectures: ViT-B/32 and T5-base.", "section": "6.1 Ablation of PCB-MERGING Components"}, {"figure_path": "l5SbrtvSRS/tables/tables_17_1.jpg", "caption": "Table 2: Comparison of different model merging methods across various fine-tuning configurations and modalities, with average performance reported for different tasks.", "description": "This table compares the average performance of various model merging methods across different tasks and modalities (NLP, PEFT, LLM, and Vision).  It shows the performance gains achieved by PCB-MERGING over baseline methods such as simple averaging, Fisher Merging, and others, for both fine-tuned models and when using a validation set.  The results demonstrate improved performance from PCB-MERGING across multiple modalities, task types, and training configurations.", "section": "5 Results"}, {"figure_path": "l5SbrtvSRS/tables/tables_18_1.jpg", "caption": "Table 7: Compare the performance of different merging methods after applying unsupervised training with AdaMerging.", "description": "This table compares the performance of different model merging methods, specifically AdaMerging, AdaMerging combined with TIES-Merging, and AdaMerging combined with PCB-Merging.  The comparison is done for both task-wise and layer-wise coefficient application on ViT-B/32 and ViT-L/14 models. It highlights how PCB-Merging improves upon AdaMerging by further enhancing performance.", "section": "C.2 Compare with Adamerging"}, {"figure_path": "l5SbrtvSRS/tables/tables_19_1.jpg", "caption": "Table 2: Comparison of different model merging methods across various fine-tuning configurations and modalities, with average performance reported for different tasks.", "description": "This table compares the performance of several model merging methods across different tasks and modalities (NLP, PEFT, LLMs, and Vision).  It shows the average performance of each method for multiple tasks using several different models (T5-Base, T5-Large, (IA)\u00b3, LLaMa2, ViT-B/32, and ViT-L/14).  The numbers in parentheses indicate performance improvements compared to the \"Fine-tuned\" baseline. The table highlights the superior performance of the PCB-MERGING method across various tasks and model types.", "section": "5 Results"}, {"figure_path": "l5SbrtvSRS/tables/tables_19_2.jpg", "caption": "Table 2: Comparison of different model merging methods across various fine-tuning configurations and modalities, with average performance reported for different tasks.", "description": "This table compares the performance of various model merging methods across different tasks and modalities. It shows the average performance for each method on seven NLP tasks using T5-base and T5-large models, eleven PEFT tasks using (IA)\u00b3 models, three LLM tasks using Llama2 models, and eight vision tasks using ViT-B/32 and ViT-L/14 models.  The results are presented with and without access to a validation set, demonstrating the impact of validation data on the methods' performance.  The table also highlights the improvements achieved by PCB-MERGING compared to existing methods.", "section": "5 Results"}, {"figure_path": "l5SbrtvSRS/tables/tables_19_3.jpg", "caption": "Table 2: Comparison of different model merging methods across various fine-tuning configurations and modalities, with average performance reported for different tasks.", "description": "This table compares the performance of various model merging methods across different tasks and modalities, considering various fine-tuning configurations.  The average performance is shown for different sets of tasks (NLP, PEFT, LLMs, and Vision).  It provides a comparison to baselines such as fine-tuned models, multi-task learning, simple averaging, Fisher merging, RegMean, Task Arithmetic, and Ties-Merging. The table highlights the improved performance of PCB-MERGING, particularly when a validation set is not available.  Parenthetical values show percentage improvements over baseline methods for T5-base and T5-large models. ", "section": "5 Results"}, {"figure_path": "l5SbrtvSRS/tables/tables_20_1.jpg", "caption": "Table 2: Comparison of different model merging methods across various fine-tuning configurations and modalities, with average performance reported for different tasks.", "description": "This table compares the performance of various model merging methods across different tasks and modalities (NLP and Vision).  It shows the average performance for each method on several tasks, comparing fine-tuned, multitask, and different model merging methods.  The results highlight the performance gains achieved by PCB-MERGING compared to baselines, particularly in cross-task, cross-domain, and out-of-domain scenarios. The table also indicates if a validation set was used and provides the average improvement obtained by PCB-MERGING compared to the top performing baseline.", "section": "5 Results"}, {"figure_path": "l5SbrtvSRS/tables/tables_20_2.jpg", "caption": "Table 2: Comparison of different model merging methods across various fine-tuning configurations and modalities, with average performance reported for different tasks.", "description": "This table compares the average performance of different model merging methods across various tasks and modalities.  It shows the performance improvements achieved by PCB-MERGING (the authors' method) compared to various baselines, such as simple averaging, Fisher merging, and Task Arithmetic.  The results are broken down by modality (NLP, PEFT, LLM, Vision) and whether a validation set was used.  The table highlights the superior performance of PCB-MERGING across multiple tasks, models, modalities, and training settings.", "section": "5 Results"}, {"figure_path": "l5SbrtvSRS/tables/tables_20_3.jpg", "caption": "Table 2: Comparison of different model merging methods across various fine-tuning configurations and modalities, with average performance reported for different tasks.", "description": "This table compares the performance of various model merging methods across different tasks and modalities (NLP and Vision).  It shows the average performance of each method for several tasks after fine-tuning using different configurations.  The table includes baseline methods (simple averaging, Fisher merging, RegMean, Task Arithmetic, Ties-Merging) and the proposed PCB-MERGING method, allowing for a comparison of performance improvements.", "section": "5 Results"}, {"figure_path": "l5SbrtvSRS/tables/tables_21_1.jpg", "caption": "Table 2: Comparison of different model merging methods across various fine-tuning configurations and modalities, with average performance reported for different tasks.", "description": "This table compares the performance of various model merging methods across different tasks and model types (NLP, PEFT, LLM, Vision).  It shows the average performance of each method for each set of tasks, comparing against fine-tuned and multitask baselines. The table highlights the improvements achieved by PCB-MERGING, particularly on the T5-base and T5-large models.", "section": "5 Results"}, {"figure_path": "l5SbrtvSRS/tables/tables_21_2.jpg", "caption": "Table 2: Comparison of different model merging methods across various fine-tuning configurations and modalities, with average performance reported for different tasks.", "description": "This table compares the performance of various model merging methods across different tasks and modalities (NLP and Vision).  It shows the average performance for several baseline methods (simple averaging, Fisher merging, RegMean, Task Arithmetic, Ties-Merging) and the proposed PCB-MERGING method.  The table includes results with and without using a validation set. The results demonstrate that PCB-MERGING consistently outperforms existing methods in multiple scenarios.", "section": "5 Results"}, {"figure_path": "l5SbrtvSRS/tables/tables_22_1.jpg", "caption": "Table 2: Comparison of different model merging methods across various fine-tuning configurations and modalities, with average performance reported for different tasks.", "description": "This table compares the performance of various model merging methods across different tasks and modalities.  It shows the average performance (accuracy or F1 score, depending on the task) for each method across seven NLP tasks (using T5-base and T5-large models), eleven PEFT tasks (using (IA)3 adapters and T0-3B models), three LLM tasks (using Llama2 models), and eight vision tasks (using ViT-B/32 and ViT-L/14 models).  Results are shown both with and without a validation set, highlighting the impact of validation data on model performance.", "section": "5 Results"}, {"figure_path": "l5SbrtvSRS/tables/tables_24_1.jpg", "caption": "Table 2: Comparison of different model merging methods across various fine-tuning configurations and modalities, with average performance reported for different tasks.", "description": "This table compares the performance of PCB-MERGING with other model merging methods across various fine-tuning configurations and modalities.  It shows the average performance across multiple tasks for several different models (T5-Base, T5-Large, IA\u00b3 and Llama2 for NLP, and ViT-B/32 and ViT-L/14 for vision).  The results demonstrate the improvement achieved by PCB-MERGING compared to baselines, highlighting its effectiveness across different scenarios and model types.", "section": "5 Results"}]