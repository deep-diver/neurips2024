[{"figure_path": "OuQYWNuNxm/figures/figures_2_1.jpg", "caption": "Figure 1: An illustrative comparison between the standard REC algorithm and REC with space partitioning. We illustrate the prior P's density in blue and Q's density in orange. (a) In a standard REC algorithm, we may draw numerous samples (colored in red) before identifying one that aligns well with Q (colored in green). The majority of these samples do not directly contribute to the desired result. (b) In the method we propose, we first divide the search space into smaller grids and then reweight each grid. This amounts to adjusting the prior P to a search heuristic P', which can align better with Q. The samples from P' will thus be more relevant to Q, potentially reducing the runtime.", "description": "This figure compares the standard REC algorithm with the proposed space partitioning approach.  The standard approach involves a random search (red dots) to find samples aligning with the target distribution Q (green dot). The proposed method partitions the search space and reweights each partition, resulting in a more efficient search that reduces runtime by focusing on more relevant areas.", "section": "3 Relative Entropy Coding with Space Partitioning"}, {"figure_path": "OuQYWNuNxm/figures/figures_3_1.jpg", "caption": "Figure 1: An illustrative comparison between the standard REC algorithm and REC with space partitioning. We illustrate the prior P's density in blue and Q's density in orange. (a) In a standard REC algorithm, we may draw numerous samples (colored in red) before identifying one that aligns well with Q (colored in green). The majority of these samples do not directly contribute to the desired result. (b) In the method we propose, we first divide the search space into smaller grids and then reweight each grid. This amounts to adjusting the prior P to a search heuristic P', which can align better with Q. The samples from P' will thus be more relevant to Q, potentially reducing the runtime.", "description": "This figure illustrates the difference between standard REC and REC with space partitioning.  The standard REC algorithm randomly samples from the prior distribution P, often wasting time on samples that don't contribute to finding a sample that matches the target distribution Q.  The proposed space partitioning method divides the search space and adjusts the prior P to a new distribution P', which is more closely aligned to Q. This results in faster encoding because fewer samples are needed to find a sample similar to the target distribution.", "section": "Relative Entropy Coding with Space Partitioning"}, {"figure_path": "OuQYWNuNxm/figures/figures_8_1.jpg", "caption": "Figure 2: Comparing standard PFR and PFR with our proposed space partitioning algorithm on toy examples. Solid lines and the shadow areas represent the mean and IQR.", "description": "This figure compares the performance of the standard Poisson Functional Representation (PFR) algorithm with the proposed space-partitioning PFR algorithm on synthetic 5D Gaussian examples.  The left panel shows the runtime (number of steps) versus the R\u00e9nyi-\u221e divergence between the target and coding distributions. The middle panel displays codelength versus mutual information. The right panel illustrates runtime versus R\u00e9nyi-\u221e divergence for approximate sampling, categorized by the maximum mean discrepancy (MMD) between the encoded samples and the target distribution. The plots demonstrate that the proposed space-partitioning approach significantly reduces runtime while maintaining comparable codelength in both exact and approximate sampling scenarios.", "section": "5 Experiments"}, {"figure_path": "OuQYWNuNxm/figures/figures_14_1.jpg", "caption": "Figure 1: An illustrative comparison between the standard REC algorithm and REC with space partitioning. We illustrate the prior P's density in blue and Q's density in orange. (a) In a standard REC algorithm, we may draw numerous samples (colored in red) before identifying one that aligns well with Q (colored in green). The majority of these samples do not directly contribute to the desired result. (b) In the method we propose, we first divide the search space into smaller grids and then reweight each grid. This amounts to adjusting the prior P to a search heuristic P', which can align better with Q. The samples from P' will thus be more relevant to Q, potentially reducing the runtime.", "description": "This figure illustrates the difference between standard REC and the proposed space partitioning REC. In standard REC, many samples are drawn from the prior distribution P before finding a sample aligning with target distribution Q. The proposed method partitions the space and adjusts the prior, which leads to more relevant samples and faster runtime.", "section": "Relative Entropy Coding with Space Partitioning"}, {"figure_path": "OuQYWNuNxm/figures/figures_15_1.jpg", "caption": "Figure 1: An illustrative comparison between the standard REC algorithm and REC with space partitioning. We illustrate the prior P's density in blue and Q's density in orange. (a) In a standard REC algorithm, we may draw numerous samples (colored in red) before identifying one that aligns well with Q (colored in green). The majority of these samples do not directly contribute to the desired result. (b) In the method we propose, we first divide the search space into smaller grids and then reweight each grid. This amounts to adjusting the prior P to a search heuristic P', which can align better with Q. The samples from P' will thus be more relevant to Q, potentially reducing the runtime.", "description": "This figure illustrates the difference between the standard REC algorithm and the proposed space partitioning method. The standard method randomly samples from the prior distribution P until a sample is found that is sufficiently close to the target distribution Q. The proposed method first partitions the space into smaller grids and then reweights the grids based on the target distribution. This results in the samples being drawn from a modified prior distribution that is better aligned with the target distribution, leading to faster encoding times.", "section": "3 Relative Entropy Coding with Space Partitioning"}, {"figure_path": "OuQYWNuNxm/figures/figures_16_1.jpg", "caption": "Figure 7: Visualizing approximation error of standard ORC and our proposed methods with different partitioning strategies when executed with the same number of samples (20). We use the same setup as the toy experiments in the main text (details in Appendix D.1), but here we set dimension 1 to have zero mutual information (i.e., collapsed dimension).", "description": "This figure compares the performance of standard ORC and three variations of the proposed space partitioning method for encoding samples from a 5D Gaussian distribution.  The key difference between methods is how space is partitioned: only partitioning the 'collapsed' dimension (dimension with zero mutual information), random assignment of partitions, and assigning partitions according to mutual information. The figure shows histograms of the empirical densities of the encoded samples across 5000 runs for each method, with the target density overlaid in orange for comparison. The results show that assigning intervals according to mutual information outperforms other approaches.", "section": "B.2.1 Examples and Ablations on Choosing the Number of Intervals Assigned to Each Axis"}, {"figure_path": "OuQYWNuNxm/figures/figures_16_2.jpg", "caption": "Figure 2: Comparing standard PFR and PFR with our proposed space partitioning algorithm on toy examples. Solid lines and the shadow areas represent the mean and IQR.", "description": "This figure compares the runtime and codelength of the standard Poisson Functional Representation (PFR) algorithm with the proposed space partitioning PFR algorithm.  The experiment uses 5D synthetic Gaussian examples. The plots show that the space partitioning method significantly reduces the runtime while maintaining similar codelength compared to the standard PFR, especially as the KL divergence between the target and coding distributions increases.", "section": "Experiments"}, {"figure_path": "OuQYWNuNxm/figures/figures_17_1.jpg", "caption": "Figure 9: Rate-distortion of RECOMBINER (on 100 CIFAR-10 test images) by our space-partitioning algorithm using different partition strategies: randomly assigning intervals to dimensions; first removing axis with MI \u2248 0 and then randomly assigning intervals; assigning intervals according to MI (our proposed strategy). We include standard ORC and theoretical RD for reference. Our proposed partition strategy is better than randomly assigning intervals per axis. Surprisingly, the results achieved by first removing uninformative dimensions (MI \u2248 0) and then randomly assigning intervals to other dimensions are only slightly worse than our proposed partition strategy. This indicates that our algorithm is not very sensitive to how we construct the intervals, as long as we avoid partitioning along uninformative dimensions.", "description": "This figure compares the rate-distortion performance of the proposed space-partitioning algorithm with different partitioning strategies on the CIFAR-10 dataset using the RECOMBINER codec.  It shows that the proposed method which assigns intervals according to mutual information performs best.  The results highlight the algorithm's robustness to the choice of partitioning strategy as long as uninformative dimensions are handled appropriately.", "section": "5 Experiments"}, {"figure_path": "OuQYWNuNxm/figures/figures_27_1.jpg", "caption": "Figure 10: Dimension-wise mutual information and KL divergence for each test image along all dimensions. We can see the KL divergences concentrate around mutual information. And if the mutual information is zero for some dimension, the test KL divergence will also be zero. This ensures our space partitioning strategy can reduce the runtime.", "description": "This figure shows the relationship between the dimension-wise mutual information and KL divergence for each test image. The KL divergence values are concentrated around the mutual information values.  A zero mutual information value in a dimension results in a zero KL divergence in that dimension. This concentration is key for the space partitioning strategy's runtime reduction because a close alignment between the mutual information and KL divergence leads to more efficient searching within the relevant regions of the space.", "section": "D.2.2 Space partitioning details"}, {"figure_path": "OuQYWNuNxm/figures/figures_27_2.jpg", "caption": "Figure 11: The distribution of KL for each group, estimated from 60,000 training images. Here, we take the case where we split the latent embeddings into 2 blocks as an example. Different colors represent different blocks. We note that the difference between these 2 blocks is solely due to randomness in the splitting.", "description": "This histogram shows the distribution of the KL divergence for each block in a 2-block setup estimated from 60,000 MNIST training images.  The slight difference between the two distributions is solely due to the random splitting of the latent space into blocks, highlighting the randomness in this process.", "section": "D.2.3 Encoding details"}, {"figure_path": "OuQYWNuNxm/figures/figures_30_1.jpg", "caption": "Figure 12: Comparing RECOMBINER with our proposed space partitioning algorithm (w. fine-tuning) with other codecs on CIFAR-10. We use solid lines to denote INR-based codecs, dotted lines to denote VAE-based codecs, and dashed lines to denote classical codecs.", "description": "This figure compares the rate-distortion performance of the proposed algorithm with RECOMBINER and other codecs (classical, VAE-based, and INR-based) on CIFAR-10.  The x-axis represents the bitrate, and the y-axis represents the PSNR.  Solid lines indicate INR-based codecs, dotted lines represent VAE-based codecs, and dashed lines show classical codecs. The result demonstrates that the proposed method improves the performance of RECOMBINER.", "section": "More Results"}]