[{"figure_path": "RwgNbIpCpk/tables/tables_6_1.jpg", "caption": "Table 1: Test accuracy on the Long Range Arena Benchmarks. We follow the standard training procedures introduced in [23]. Bold scores indicate the highest performing model on a given task and underlined the second best performing. X indicates a model did not do better than random guessing and - that a result was not available. In this table we only include results from other non-input-dependent models.", "description": "This table presents the test accuracy results on the Long Range Arena (LRA) benchmark for various sequence models.  The models are categorized into Transformers, State Space Models, and Convolutional Models, with several ablations of MRConv also included.  The results are shown for six different tasks within the LRA benchmark: ListOps, Text Retrieval, Image, Pathfinder, Path-X, and an average across all tasks.  The table highlights the state-of-the-art performance of MRConv among the attention-free models.", "section": "5.1 Long Range Arena"}, {"figure_path": "RwgNbIpCpk/tables/tables_7_1.jpg", "caption": "Table 2: MRConv design ablations. Effect of MRConv modifications on ListOps and Image tasks from LRA. For reference we note that S4-LegS uses 815K and 3.6M parameters and Liquid-S4 uses 333K and 11M parameters for each task respectively.", "description": "This table presents the ablation study results for the MRConv model on ListOps and Image tasks from the Long Range Arena (LRA) benchmark.  It shows how different design choices (adding multi-resolution, Fourier kernels, BatchNorm, and increasing model depth) impact the model's performance in terms of accuracy and the number of parameters.  The table also provides a comparison to the parameter counts of similar models, S4-LegS and Liquid-S4.", "section": "3 Reparameterized Multi-Resolution Convolutions"}, {"figure_path": "RwgNbIpCpk/tables/tables_8_1.jpg", "caption": "Table 1: Test accuracy on the Long Range Arena Benchmarks. We follow the standard training procedures introduced in [23]. Bold scores indicate the highest performing model on a given task and underlined the second best performing. X indicates a model did not do better than random guessing and - that a result was not available. In this table we only include results from other non-input-dependent models.", "description": "This table presents the test accuracy results on various tasks from the Long Range Arena (LRA) benchmark.  It compares the performance of the proposed MRConv model against several state-of-the-art baselines, including various transformer models and other convolutional models. The table shows accuracy scores for different sequence lengths and various data modalities.  Bold scores highlight the best-performing model for each task, while underlined scores indicate the second-best.", "section": "5.1 Long Range Arena"}, {"figure_path": "RwgNbIpCpk/tables/tables_8_2.jpg", "caption": "Table 14: Test accuracy on 35-way Speech Commands classification task [47]. Each model is trained on one-second 16kHz audio waveforms and then tested at 16kHz and 0-shot at 8kHz. Results for the baselines are reported in [23].", "description": "This table presents the test accuracy results for a 35-way speech command classification task.  The models were trained using one-second audio waveforms sampled at 16kHz.  The table shows the performance at both the original 16kHz sampling rate and a zero-shot evaluation at 8kHz (obtained by downsampling). The results are compared against several baselines from the literature.", "section": "D.5 Raw Speech Classification"}, {"figure_path": "RwgNbIpCpk/tables/tables_8_3.jpg", "caption": "Table 1: Test accuracy on the Long Range Arena Benchmarks. We follow the standard training procedures introduced in [23]. Bold scores indicate the highest performing model on a given task and underlined the second best performing. X indicates a model did not do better than random guessing and - that a result was not available. In this table we only include results from other non-input-dependent models.", "description": "This table presents the test accuracy results on the Long Range Arena benchmark dataset for various sequence models.  It compares the performance of MRConv against several baseline models across six different tasks (ListOps, Text Retrieval, Image, Pathfinder, Path-X, and an average across all tasks).  The table highlights MRConv's state-of-the-art performance, particularly when compared to other non-input-dependent models.", "section": "5.1 Long Range Arena"}, {"figure_path": "RwgNbIpCpk/tables/tables_8_4.jpg", "caption": "Table 4: Left: Top-1 test accuracy on ImageNet classification [41]. Right: Inference time speed comparison between Base and Large versions of MRConv and linear and quadratic attention versions of MEGA [34]. We denote Rep as reparameterized models. By scaling MRConv up with more parameters we match the performance of MEGA with quadratic attention, whilst also being more efficient.", "description": "This table compares the performance and efficiency of MRConv with other models on the ImageNet classification task.  The left side shows the Top-1 accuracy for various models, highlighting that MRConv achieves state-of-the-art results. The right side focuses on inference time speed, demonstrating that MRConv's speed is competitive and efficient compared to state-of-the-art models.", "section": "5.2 Pixel-Level 1D Image Classification"}, {"figure_path": "RwgNbIpCpk/tables/tables_14_1.jpg", "caption": "Table 1: Test accuracy on the Long Range Arena Benchmarks. We follow the standard training procedures introduced in [23]. Bold scores indicate the highest performing model on a given task and underlined the second best performing. X indicates a model did not do better than random guessing and - that a result was not available. In this table we only include results from other non-input-dependent models.", "description": "This table presents the test accuracy results on the Long Range Arena (LRA) benchmark for various sequence models.  The models are categorized into Transformer-based models, linear-time Transformers, state-space models, and convolutional models.  The table compares the performance of the proposed MRConv model against state-of-the-art baselines across six different tasks within the LRA benchmark: ListOps, Text, Retrieval, Image, Pathfinder, and Path-X.  Results are shown for different input sequence lengths and different model variants.  Bold scores highlight the best performing model for each task, and underlined scores show the second-best performing model.", "section": "5.1 Long Range Arena"}, {"figure_path": "RwgNbIpCpk/tables/tables_14_2.jpg", "caption": "Table 1: Test accuracy on the Long Range Arena Benchmarks. We follow the standard training procedures introduced in [23]. Bold scores indicate the highest performing model on a given task and underlined the second best performing. X indicates a model did not do better than random guessing and - that a result was not available. In this table we only include results from other non-input-dependent models.", "description": "This table presents the test accuracy results on the Long Range Arena (LRA) benchmark for various sequence models.  It compares MRConv (the proposed model) against several state-of-the-art baselines, including different transformer and convolutional models, across six different tasks. The tasks involve various data modalities and sequence lengths to assess long-range dependency modeling capabilities.  The table highlights the performance of MRConv against these baselines, demonstrating its state-of-the-art performance in various scenarios.", "section": "5.1 Long Range Arena"}, {"figure_path": "RwgNbIpCpk/tables/tables_14_3.jpg", "caption": "Table 1: Test accuracy on the Long Range Arena Benchmarks. We follow the standard training procedures introduced in [23]. Bold scores indicate the highest performing model on a given task and underlined the second best performing. X indicates a model did not do better than random guessing and - that a result was not available. In this table we only include results from other non-input-dependent models.", "description": "This table presents the test accuracy results on the Long Range Arena (LRA) benchmark for various sequence models.  It compares the performance of MRConv with different kernel parameterizations (Dilated, Fourier, and Fourier+Sparse) against various baselines, including Transformers and other convolutional models.  The table highlights the average performance across multiple tasks within the benchmark (ListOps, Text Retrieval, Image, Pathfinder, Path-X), showing MRConv's competitive performance compared to existing state-of-the-art models.", "section": "5.1 Long Range Arena"}, {"figure_path": "RwgNbIpCpk/tables/tables_16_1.jpg", "caption": "Table 5: Experiment hyperparameters for MRConv-Base variants", "description": "This table presents the hyperparameters used for the MRConv-Base model variants in the Long Range Arena experiments.  It shows the dataset used, kernel type, depth, number of features, kernel size, whether bidirectional convolutions were used, normalization type (Batch Normalization or Layer Normalization), whether pre-normalization was applied, dropout rate, kernel learning rate, overall learning rate, weight decay, batch size, and number of epochs.", "section": "D.2 Default Hyperparameters"}, {"figure_path": "RwgNbIpCpk/tables/tables_16_2.jpg", "caption": "Table 6: Experiment hyperparameters for MRConv-Large variants", "description": "This table presents the hyperparameters used for the large variants of the MRConv model in the Long Range Arena experiments.  It shows the kernel type, depth, number of features, kernel size, whether bidirectional convolutions were used, the type of normalization, whether pre-normalization was applied, the dropout rate, learning rate for the kernel, overall learning rate, weight decay, batch size, and number of epochs used for training. The hyperparameters were chosen to ensure the computational resources used for the large MRConv models were comparable to those of baseline methods that use quadratic attention.", "section": "D.2 Default Hyperparameters"}, {"figure_path": "RwgNbIpCpk/tables/tables_18_1.jpg", "caption": "Table 1: Test accuracy on the Long Range Arena Benchmarks. We follow the standard training procedures introduced in [23]. Bold scores indicate the highest performing model on a given task and underlined the second best performing. X indicates a model did not do better than random guessing and - that a result was not available. In this table we only include results from other non-input-dependent models.", "description": "This table presents the test accuracy results on the Long Range Arena (LRA) benchmark for various sequence models.  It compares different model types (Transformers, State Space Models, Convolutional Models) and variations of the proposed MRConv model with different kernel parameterizations (Dilated, Fourier, Fourier+Sparse).  The table highlights the best-performing model for each task and includes an average performance across all tasks.  Results are presented for both base and large model sizes of MRConv.", "section": "5.1 Long Range Arena"}, {"figure_path": "RwgNbIpCpk/tables/tables_18_2.jpg", "caption": "Table 1: Test accuracy on the Long Range Arena Benchmarks. We follow the standard training procedures introduced in [23]. Bold scores indicate the highest performing model on a given task and underlined the second best performing. X indicates a model did not do better than random guessing and - that a result was not available. In this table we only include results from other non-input-dependent models.", "description": "This table presents the test accuracy results on the Long Range Arena (LRA) benchmark.  It compares the performance of MRConv against various baseline models on several tasks, including ListOps, Text Retrieval, Image, Pathfinder, and Path-X.  The results highlight MRConv's state-of-the-art performance in several tasks.", "section": "5.1 Long Range Arena"}, {"figure_path": "RwgNbIpCpk/tables/tables_19_1.jpg", "caption": "Table 9: Initial kernel size albations. Effect of the initial kernel size lo on ListOps and Image tasks from LRA. We note that the initial kernel size also effects the number of independent resolutions N = log2(L/lo) + 1 which we include in the table for clarity.", "description": "This table shows the results of an ablation study on the effect of different initial kernel sizes on the ListOps and Image tasks from the Long Range Arena (LRA) benchmark.  It demonstrates how the choice of initial kernel size impacts the final accuracy, with smaller kernel sizes performing better on more discrete datasets like ListOps and larger kernel sizes performing better on smoother image data.", "section": "D.3.5 Initial Resolution Ablation"}, {"figure_path": "RwgNbIpCpk/tables/tables_20_1.jpg", "caption": "Table 10: Normalization albations. Effect of different normalization layers. We find that there is very little difference between using LayerNorm of BatchNorm but emphasize the benefit of using BatchNorm which enables structural reparameterization.", "description": "This table shows the result of ablations on different normalization techniques.  It compares the performance of using BatchNorm vs LayerNorm in the MRConv model on the ListOps and Image tasks from the Long Range Arena benchmark.  The results indicate minimal performance difference between the two normalization methods, highlighting BatchNorm's advantage due to its compatibility with the model's structural reparameterization.", "section": "D.3.6 LayerNorm Ablation"}, {"figure_path": "RwgNbIpCpk/tables/tables_20_2.jpg", "caption": "Table 11: Kernel parameterization albations. Performance of different kernel parameterizations on ListOps and Image tasks from LRA.", "description": "This table shows the results of an ablation study comparing different kernel parameterizations (Dilated, Fourier, Fourier+Sparse, and MLP) on two tasks from the Long Range Arena (LRA) benchmark: ListOps and Image.  The table reports the number of parameters and the accuracy achieved by each parameterization on each task. This allows the reader to assess the impact of the different kernel types on the performance of the MRConv model.", "section": "5 Experiments"}, {"figure_path": "RwgNbIpCpk/tables/tables_21_1.jpg", "caption": "Table 1: Test accuracy on the Long Range Arena Benchmarks. We follow the standard training procedures introduced in [23]. Bold scores indicate the highest performing model on a given task and underlined the second best performing. X indicates a model did not do better than random guessing and - that a result was not available. In this table we only include results from other non-input-dependent models.", "description": "This table presents the test accuracy results on the Long Range Arena (LRA) benchmark for various sequence models.  It compares the performance of the proposed MRConv model against several state-of-the-art baselines, including transformers and other convolutional models, across different tasks within the LRA benchmark.  The table highlights the best-performing model for each task and indicates when a model's performance was no better than random guessing.  Only non-input-dependent models are included in the comparison.", "section": "5.1 Long Range Arena"}, {"figure_path": "RwgNbIpCpk/tables/tables_21_2.jpg", "caption": "Table 1: Test accuracy on the Long Range Arena Benchmarks. We follow the standard training procedures introduced in [23]. Bold scores indicate the highest performing model on a given task and underlined the second best performing. X indicates a model did not do better than random guessing and - that a result was not available. In this table we only include results from other non-input-dependent models.", "description": "This table presents the test accuracy results on various tasks from the Long Range Arena (LRA) benchmark.  It compares the performance of the proposed MRConv model against several other state-of-the-art models, including linear-time transformers and state-space models.  The table highlights MRConv's superior performance across different tasks and input lengths, showcasing its effectiveness in long-sequence modeling.", "section": "5.1 Long Range Arena"}, {"figure_path": "RwgNbIpCpk/tables/tables_22_1.jpg", "caption": "Table 14: Test accuracy on 35-way Speech Commands classification task [47]. Each model is trained on one-second 16kHz audio waveforms and then tested at 16kHz and 0-shot at 8kHz. Results for the baselines are reported in [23].", "description": "This table presents the test accuracy results of various models on the Speech Commands dataset [47], a task that involves classifying 35 spoken words from 1-second audio recordings.  The models are evaluated at two sampling rates: 16 kHz (the original rate) and 8 kHz (zero-shot, downsampled). The table compares the performance of MRConv (with different kernel parameterizations) against several baseline models, including CNNs and state-space models.  It demonstrates MRConv's ability to perform well, especially at 16 kHz and even at 8 kHz for the Fourier kernel.", "section": "5 Experiments"}, {"figure_path": "RwgNbIpCpk/tables/tables_23_1.jpg", "caption": "Table 1: Test accuracy on the Long Range Arena Benchmarks. We follow the standard training procedures introduced in [23]. Bold scores indicate the highest performing model on a given task and underlined the second best performing. X indicates a model did not do better than random guessing and - that a result was not available. In this table we only include results from other non-input-dependent models.", "description": "This table presents the test accuracy results on the Long Range Arena benchmark for various sequence models.  The models are categorized into Transformers, State Space Models, and Convolutional Models.  The table highlights the performance of MRConv (the proposed model) against these baselines across different tasks within the benchmark (ListOps, Text Retrieval, Image, Pathfinder, Path-X).  Bold scores represent the best-performing model for each task, while underlined scores indicate the second-best performance.  'X' denotes models that did not perform better than random guessing, and '-' indicates missing results.  The table focuses specifically on non-input-dependent models for a fair comparison.", "section": "5.1 Long Range Arena"}, {"figure_path": "RwgNbIpCpk/tables/tables_23_2.jpg", "caption": "Table 1: Test accuracy on the Long Range Arena Benchmarks. We follow the standard training procedures introduced in [23]. Bold scores indicate the highest performing model on a given task and underlined the second best performing. X indicates a model did not do better than random guessing and - that a result was not available. In this table we only include results from other non-input-dependent models.", "description": "This table presents the test accuracy results on the Long Range Arena (LRA) benchmark.  It compares various sequence modeling methods, including different versions of transformers and convolutional models, across six different tasks with various input lengths. The table highlights the top-performing models for each task, taking into account computational complexities and avoiding input-dependent models for a fair comparison.", "section": "5.1 Long Range Arena"}, {"figure_path": "RwgNbIpCpk/tables/tables_24_1.jpg", "caption": "Table 1: Test accuracy on the Long Range Arena Benchmarks. We follow the standard training procedures introduced in [23]. Bold scores indicate the highest performing model on a given task and underlined the second best performing. X indicates a model did not do better than random guessing and - that a result was not available. In this table we only include results from other non-input-dependent models.", "description": "This table compares the performance of MRConv against various other models on six different tasks from the Long Range Arena benchmark dataset.  The tasks test the ability of models to handle long-range dependencies in various data modalities (text, images, paths, etc.). The table shows the test accuracy achieved by each model on each task.  The best performing model on each task is highlighted in bold, while the second best is underlined.  'X' indicates models that performed no better than random guessing on the given task. The table focuses specifically on non-input dependent models.", "section": "5.1 Long Range Arena"}]