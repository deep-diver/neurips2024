{"references": [{"fullname_first_author": "E. J. Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "publication_date": "2021", "reason": "This paper introduces the Low-Rank Adaptation (LoRA) technique, a highly influential parameter-efficient fine-tuning method for large language models, which is directly relevant to the core methodology of the current paper."}, {"fullname_first_author": "S. Chen", "paper_title": "Adaptformer: Adapting vision transformers for scalable visual recognition", "publication_date": "2022", "reason": "This paper proposes AdaptFormer, a significant advancement in parameter-efficient fine-tuning for Vision Transformers (ViTs), providing a strong baseline for comparison and context for the current research."}, {"fullname_first_author": "W. Dong", "paper_title": "Efficient adaptation of large vision transformer via adapter re-composing", "publication_date": "2023", "reason": "This paper, also by the current authors, explores a related approach to parameter-efficient fine-tuning in ViTs, offering valuable context and comparison for the proposed method."}, {"fullname_first_author": "A. Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020", "reason": "This foundational paper introduces the Vision Transformer (ViT) architecture, which is the subject of adaptation in the current paper, making it a crucial reference for understanding the background and context."}, {"fullname_first_author": "Z. Liu", "paper_title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows", "publication_date": "2021", "reason": "This paper presents the Swin Transformer, a highly effective and widely used ViT architecture that is used as a backbone in the current work's experiments, providing a strong empirical basis for evaluating the proposed method."}]}