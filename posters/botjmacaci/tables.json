[{"figure_path": "BOtjMacACI/tables/tables_6_1.jpg", "caption": "Table 1: Performance comparisons on the VTAB-1k benchmark with ViT-B/16 models pre-trained on ImageNet-21K. * denotes leveraging the augmented ViT backbone by AugReg [34]. The bold font shows the best accuracy of all methods and the underline font shows the second best accuracy.", "description": "This table presents a comparison of different parameter-efficient fine-tuning (PEFT) methods on the VTAB-1k benchmark using the ViT-B/16 architecture pretrained on ImageNet-21K.  It compares the performance (top-1 accuracy) of various methods, including full fine-tuning, linear probing, and several state-of-the-art PEFT techniques like LoRA, Adapter, and the proposed HTA method.  The table also indicates the number of parameters used by each method, highlighting the parameter efficiency of the proposed HTA approach. The use of an augmented ViT backbone (AugReg) is also noted for some methods.", "section": "4 Experiments"}, {"figure_path": "BOtjMacACI/tables/tables_6_2.jpg", "caption": "Table 2: Performance comparisons on five FGVC datasets with ViT-B/16 models pre-trained on ImageNet-21K. * denotes leveraging the augmented ViT backbone by AugReg [34].", "description": "This table presents the performance comparison of different methods on five fine-grained visual classification (FGVC) datasets using a ViT-B/16 model pre-trained on ImageNet-21K.  The results show the accuracy achieved by each method on each dataset, along with the total number of trainable parameters (in millions). The table includes results for both standard ViT-B/16 and augmented ViT-B/16 backbones.", "section": "4 Experiments"}, {"figure_path": "BOtjMacACI/tables/tables_7_1.jpg", "caption": "Table 3: Performance comparison on VTAB-1k using ViT-Large pre-trained on ImageNet-21k as the backbone. Detailed results are presented in the Appendix.", "description": "This table presents a comparison of the proposed HTA method against various baselines and state-of-the-art parameter-efficient fine-tuning (PEFT) methods on the VTAB-1k benchmark.  The ViT-Large model, pre-trained on ImageNet-21k, is used as the backbone. The table shows the average Top-1 accuracy across seven natural, four specialized, and eight structured vision tasks in VTAB-1k.  The number of trainable parameters (in millions) for each method is also provided. Detailed results for each individual task are available in the appendix.", "section": "4 Experiments"}, {"figure_path": "BOtjMacACI/tables/tables_7_2.jpg", "caption": "Table 4: Performance comparison on VTAB-1k using Swin Transformer pre-trained on ImageNet-21k as the backbone. Detailed results are presented in the Appendix.", "description": "This table presents a comparison of different parameter-efficient fine-tuning (PEFT) methods on the VTAB-1k benchmark using the Swin Transformer architecture pre-trained on ImageNet-21k.  It shows the mean accuracy across seven natural, four specialized, and eight structured vision tasks, along with the total number of trainable parameters for each method.  The results allow for a comparison of performance and parameter efficiency between different PEFT approaches, including full fine-tuning and linear probing,  as baselines.", "section": "4 Experiments"}, {"figure_path": "BOtjMacACI/tables/tables_8_1.jpg", "caption": "Table 5: Ablation study on using HTA as alternative to the low-rank based adaptation matrices in LORA and Adapter on VTAB-1k. Following the configurations in FacT [26], LoRA and Adapter are applied to {Wq, Wv} and {WFC1, WFC2} projection matrices, separately.", "description": "This table presents the ablation study results on using HTA as a replacement for low-rank adaptation matrices in LoRA and Adapter methods. It compares the performance (Top-1 Test Accuracy) and the number of parameters of different configurations on the VTAB-1k benchmark. The configurations include applying LoRA and HTA to different sets of projection matrices ({Wq, Wv} and {WFC1, WFC2}).", "section": "4.3 Ablation studies"}, {"figure_path": "BOtjMacACI/tables/tables_13_1.jpg", "caption": "Table 1: Dataset statistics for FGVC. \u201c*\u201d denotes the train/val split of datasets following the dataset setting in VPT [24].", "description": "This table shows the statistics of five fine-grained visual classification (FGVC) datasets used in the paper.  For each dataset, it provides the description, the number of classes, the size of the training set, the size of the validation set, and the size of the test set. The train/val split follows the settings in the VPT [24] paper, as indicated by the asterisk (*) symbol.", "section": "4.1 Experimental settings"}, {"figure_path": "BOtjMacACI/tables/tables_14_1.jpg", "caption": "Table 2: Dataset statistics for VTAB-1k [42].", "description": "This table presents detailed statistics for the 24 datasets used in the VTAB-1k benchmark.  It breaks down each dataset by its description (Natural, Specialized, or Structured), the number of classes, and the sizes of the training, validation, and test sets.  This information is crucial for understanding the scale and characteristics of the experimental setup and evaluating the generalizability of the proposed method.", "section": "4.1 Experimental settings"}, {"figure_path": "BOtjMacACI/tables/tables_14_2.jpg", "caption": "Table 3: The implementation details of configurations such as optimizer and hyper-parameters. We select the best hyper-parameters for each download task via using grid search.", "description": "This table details the settings used for the experiments in the paper. It lists the optimizer used (AdamW), the range of learning rates tested, the weight decay values, the batch sizes, the dropout rates for adapter layers, the learning rate schedule (cosine decay), and the number of training and warmup epochs.", "section": "4.1 Experimental settings"}, {"figure_path": "BOtjMacACI/tables/tables_15_1.jpg", "caption": "Table 1: Performance comparisons on the VTAB-1k benchmark with ViT-B/16 models pre-trained on ImageNet-21K. * denotes leveraging the augmented ViT backbone by AugReg [34]. The bold font shows the best accuracy of all methods and the underline font shows the second best accuracy.", "description": "This table presents a comparison of different parameter-efficient fine-tuning (PEFT) methods on the VTAB-1k benchmark using the ViT-B/16 architecture pre-trained on ImageNet-21k.  It shows the top-1 test accuracy for each method across various downstream tasks within the VTAB-1k benchmark. The table also indicates the number of parameters used by each method, highlighting the parameter efficiency of the proposed HTA method compared to existing PEFT techniques.", "section": "4.1 Experimental settings"}, {"figure_path": "BOtjMacACI/tables/tables_15_2.jpg", "caption": "Table 1: Performance comparisons on the VTAB-1k benchmark with ViT-B/16 models pre-trained on ImageNet-21K. * denotes leveraging the augmented ViT backbone by AugReg [34]. The bold font shows the best accuracy of all methods and the underline font shows the second best accuracy.", "description": "This table presents a comparison of different parameter-efficient fine-tuning (PEFT) methods on the VTAB-1k benchmark using the ViT-B/16 architecture pre-trained on ImageNet-21K.  It shows the Top-1 accuracy for each method across various downstream tasks within the VTAB-1k benchmark.  The table also indicates the number of parameters used by each method, highlighting the efficiency of different PEFT approaches.  The best and second-best accuracy results are emphasized using bold and underlined fonts, respectively.  The use of an augmented ViT backbone (* indicates methods that leveraged AugReg) is noted.", "section": "4 Experiments"}, {"figure_path": "BOtjMacACI/tables/tables_15_3.jpg", "caption": "Table 2: Performance comparisons on five FGVC datasets with ViT-B/16 models pre-trained on ImageNet-21K. * denotes leveraging the augmented ViT backbone by AugReg [34].", "description": "This table presents a comparison of different parameter-efficient fine-tuning (PEFT) methods on five fine-grained visual classification (FGVC) datasets using a Vision Transformer (ViT) with a base model and 16 layers (ViT-B/16) pretrained on ImageNet-21K.  The results show the classification accuracy achieved by each method.  An augmented ViT backbone is also included in some experiments, indicated by an asterisk (*). The methods compared include full fine-tuning (as a baseline), linear probing (as another baseline), and various PEFT methods: Adapter, Bias, VPT-Shallow, VPT-Deep, LoRA, ARC, RLRR, HTA, and SSF.  The table also shows the total number of trainable parameters (in millions) for each method.", "section": "4.1 Experimental settings"}]