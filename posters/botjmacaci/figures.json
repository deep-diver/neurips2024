[{"figure_path": "BOtjMacACI/figures/figures_3_1.jpg", "caption": "Figure 1: Underpinned by (a) LoRA [1] and (c) Adapter [22], we utilize Householder matrix to construct Householder transformation-based adaptations, involving (b) LoRA-based method with HTA and (d) Adapter-based method with HTA.", "description": "This figure illustrates the architecture of the proposed Householder Transformation-based Adaptor (HTA) method and its comparison with existing parameter-efficient fine-tuning (PEFT) methods, LoRA and Adapter.  It shows how HTA integrates Householder transformations into the LoRA and Adapter frameworks to create adaptation matrices with varying ranks across layers. (a) and (c) depict the original LoRA and Adapter architectures, respectively. (b) and (d) showcase the integration of HTA into LoRA and Adapter, highlighting the use of Householder vectors and a diagonal matrix of scaling values to construct the orthogonal matrices.", "section": "3 Methodology"}, {"figure_path": "BOtjMacACI/figures/figures_14_1.jpg", "caption": "Figure 1: Underpinned by (a) LoRA [1] and (c) Adapter [22], we utilize Householder matrix to construct Householder transformation-based adaptations, involving (b) LoRA-based method with HTA and (d) Adapter-based method with HTA.", "description": "This figure illustrates the architectural differences between the original LoRA and Adapter methods and their counterparts incorporating the Householder Transformation-based Adaptor (HTA).  Panels (a) and (c) show the standard LoRA and Adapter architectures, respectively. Panels (b) and (d) depict the modified architectures where the original unitary matrices from the SVD decomposition have been replaced with Householder transformations.  The key change is that Householder transformations only require a vector, making HTA more parameter-efficient than traditional methods while allowing for flexibility in adapting pre-trained models.", "section": "3 Methodology"}]