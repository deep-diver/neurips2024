[{"figure_path": "QyR1dNDxRP/figures/figures_1_1.jpg", "caption": "Figure 1: Types of overfitting behaviors. Consider a binary classification problem of learning a realizable distribution D<sub>0</sub>. Let D be the distribution induced by adding an \u03b5*-probability for a data point's label to be flipped relative to D<sub>0</sub>. Suppose a model is trained with data from D. Then, assuming the classes are balanced, the trivial generalization performance is 0.5 (in gray; e.g., with a constant predictor). Left. Evaluating the model on D, a Bayes-optimal hypothesis (in red) obtains a generalization error of \u03b5*. For large enough training sets, our results (Section 4) dictate a tempered overfitting behavior illustrated above. For arbitrary noise, the error is approximately bounded by 1-8\u03b5*(1-\u03b5*)<sup>1-\u03b5*</sup> (blue). For independent noise, the error is concentrated around the tighter 2\u03b5*(1-\u03b5*) (yellow). A similar figure was previously shown in Manoj and Srebro [58] for shortest-program interpolators. Right. Assuming independent noise, the left figure can be transformed into the error of the model on D<sub>0</sub> (see Lemma A.9). The linear behavior in the independent setting (yellow) is similar to the behavior observed empirically in Mallinar et al. [57, Figures 2, 3, and 6].", "description": "This figure compares different overfitting behaviors (benign, tempered, catastrophic) of a binary classification model trained on a noisy dataset. The left panel shows the generalization error on the noisy dataset (D), while the right panel shows the generalization error on a clean dataset (D<sub>0</sub>).  The results illustrate how different noise models affect the generalization error and how the proposed models exhibit tempered overfitting.", "section": "4 Tempered overfitting of min-size and random interpolators"}, {"figure_path": "QyR1dNDxRP/figures/figures_1_2.jpg", "caption": "Figure 1: Types of overfitting behaviors. Consider a binary classification problem of learning a realizable distribution D<sub>0</sub>. Let D be the distribution induced by adding an \u03b5*-probability for a data point's label to be flipped relative to D<sub>0</sub>. Suppose a model is trained with data from D. Then, assuming the classes are balanced, the trivial generalization performance is 0.5 (in gray; e.g., with a constant predictor). Left. Evaluating the model on D, a Bayes-optimal hypothesis (in red) obtains a generalization error of \u03b5*. For large enough training sets, our results (Section 4) dictate a tempered overfitting behavior illustrated above. For arbitrary noise, the error is approximately bounded by 1-8\u03b5*(1 \u2013 \u03b5*)<sup>1-\u03b5*</sup> (blue). For independent noise, the error is concentrated around the tighter 2\u03b5*(1 \u2013 \u03b5*) (yellow). A similar figure was previously shown in Manoj and Srebro [58] for shortest-program interpolators. Right. Assuming independent noise, the left figure can be transformed into the error of the model on D<sub>0</sub> (see Lemma A.9). The linear behavior in the independent setting (yellow) is similar to the behavior observed empirically in Mallinar et al. [57, Figures 2, 3, and 6].", "description": "This figure illustrates different overfitting behaviors (benign, tempered, catastrophic) for deep neural networks trained on noisy datasets.  The left panel shows generalization error on a noisy dataset (D), while the right panel shows generalization error on a clean dataset (D<sub>0</sub>) derived from D.  The results demonstrate that the generalization error is neither optimal nor trivial, indicating tempered overfitting, and that the type of overfitting behavior depends on the statistical properties of the label noise.", "section": "4 Tempered overfitting of min-size and random interpolators"}, {"figure_path": "QyR1dNDxRP/figures/figures_5_1.jpg", "caption": "Figure 2: Interpolating a dataset. To memorize the training set, we use a subset of the parameters to match those of the teacher and another subset to memorize the noise (label flips). Then, we \"merge\" these subsets to interpolate the noisy training set. In our figure, (1) blue edges represent weights identical to the teacher's; (2) yellow edges memorize the noise; (3) red edges are set to 0; and two additional layers implement the XOR between outputs, thus memorizing the training set.", "description": "This figure illustrates how a neural network can interpolate a noisy dataset.  It shows three networks: a teacher network (a), a network memorizing label flips (b), and a wider student network that combines the previous two networks and uses an XOR construction to perfectly memorize the training data.  The color-coding of the edges helps to visualize how the parameters of the teacher and label flip memorization networks are integrated into the student network.", "section": "3 Interpolating a noisy training set"}, {"figure_path": "QyR1dNDxRP/figures/figures_7_1.jpg", "caption": "Figure 2: Interpolating a dataset. To memorize the training set, we use a subset of the parameters to match those of the teacher and another subset to memorize the noise (label flips). Then, we \"merge\" these subsets to interpolate the noisy training set. In our figure, (1) blue edges represent weights identical to the teacher's; (2) yellow edges memorize the noise; (3) red edges are set to 0; and two additional layers implement the XOR between outputs, thus memorizing the training set.", "description": "This figure illustrates how a neural network can interpolate a noisy dataset by combining the teacher network's weights with a network that memorizes the noise.  Blue edges represent weights identical to the teacher network, yellow edges represent weights memorizing the label flips, red edges have zero weight, and two XOR layers combine the outputs to create the interpolator.", "section": "3 Interpolating a noisy training set"}, {"figure_path": "QyR1dNDxRP/figures/figures_53_1.jpg", "caption": "Figure 2: Interpolating a dataset. To memorize the training set, we use a subset of the parameters to match those of the teacher and another subset to memorize the noise (label flips). Then, we \"merge\" these subsets to interpolate the noisy training set. In our figure, (1) blue edges represent weights identical to the teacher's; (2) yellow edges memorize the noise; (3) red edges are set to 0; and two additional layers implement the XOR between outputs, thus memorizing the training set.", "description": "This figure illustrates how a noisy dataset can be interpolated using a neural network.  It decomposes the task of interpolation into two subtasks: (1) matching the teacher network's weights and (2) memorizing the noise (label flips). The figure shows how to use different subsets of the parameters (blue edges for teacher, yellow for noise) and how to use an XOR operation to merge these subsets. The result is a wider network that perfectly interpolates the noisy training data.", "section": "3 Interpolating a noisy training set"}]