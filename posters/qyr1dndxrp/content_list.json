[{"type": "text", "text": "Provable Tempered Overfitting of Minimal Nets and Typical Nets ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Itamar Harel William M. Hoza Gal Vardi Technion The University of Chicago Weizmann Institute of Science itamarharel01@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Itay Evron Nathan Srebro Daniel Soudry Technion Toyota Technological Institute at Chicago Technion ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the overftiting behavior of fully connected deep Neural Networks (NNs) with binary weights fitted to perfectly classify a noisy training set. We consider interpolation using both the smallest NN (having the minimal number of weights) and a random interpolating NN. For both learning rules, we prove overfitting is tempered. Our analysis rests on a new bound on the size of a threshold circuit consistent with a partial function. To the best of our knowledge, ours are the first theoretical results on benign or tempered overfitting that: (1) apply to deep NNs, and (2) do not require a very high or very low input dimension. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Neural networks (NNs) famously exhibit strong generalization capabilities, seemingly in defiance of traditional generalization theory. Specifically, NNs often generalize well empirically even when trained to interpolate the training data perfectly [97]. This motivated an extensive line of work attempting to explain the overftiting behavior of NNs, and particularly their generalization capabilities when trained to perfectly fit a training set with corrupted labels (e.g., [5, 28, 57, 48]). ", "page_idx": 0}, {"type": "text", "text": "In an attempt to better understand the aforementioned generalization capabilities of NNs, Mallinar et al. [57] proposed a taxonomy of benign, tempered, and catastrophic overftiting. An algorithm that perfectly interpolates a training set with corrupted labels, i.e., an interpolator, is said to have tempered overftiting if its generalization error is neither benign nor catastrophic \u2014 not optimal but much better than trivial. However, the characterization of overftiting in NNs is still incomplete, especially in deep NNs when the input dimension is neither very high nor very low. In this paper, we aim to understand the overfitting behavior of deep NNs in this regime. ", "page_idx": 0}, {"type": "text", "text": "We start by analyzing tempered overfitting in \u201cmin-size\u201d NN interpolators, i.e., whose neural layer widths are selected to minimize the total number of weights. The number of parameters in a model is a natural complexity measure in learning theory and practice. For instance, it is theoretically well understood that $L_{1}$ regularization in a sparse linear regression setting yields a sparse regressor. Practically, finding small-sized deep models is a common objective used in pruning (e.g., [33]) and neural architecture search (e.g., [54]). Recently, Manoj and Srebro [58] proved that the shortest program (Turing machine) that perfectly interpolates noisy datasets exhibits tempered overftiting, illustrating how a powerful model can avoid catastrophic overfitting by returning a min-size interpolator. ", "page_idx": 0}, {"type": "text", "text": "Furthermore, we study tempered overfitting in random (\u201ctypical\u201d) interpolators \u2014 NNs sampled uniformly from the set of parameters that perfectly fti the training set. Given a narrow teacher model and no label noise, Buzaglo et al. [13] recently proved that such typical interpolators, which may be highly overparameterized, generalize well. This is remarkable since these interpolators do not rely on explicit regularization or the implicit bias of any gradient algorithm. An immediate question arises \u2014 what kind of generalization behavior do typical interpolators exhibit in the presence of label noise? This is especially interesting in light of theoretical and empirical findings that typical NNs implement low-frequency functions [70, 83], while interpolating noisy training sets may require high frequencies. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "For both the min-size and typical NN interpolators, we study the generalization behavior under an underlying noisy teacher model. We focus on deep NNs with binary weights and activations (similar NNs are used in resource-constrained environments; e.g., [42]). Our analysis reveals that these models exhibit a tempered overftiting behavior that depends on the statistical properties of the label noise. For independent noise, in addition to an upper bound we also find a lower bound on the expected generalization error. Our results are illustrated in Figure 1 below, in which the yellow line in the right panel is similar to empirically observed linear behavior [e.g., 57, Figures 2, 3, and 6]. ", "page_idx": 1}, {"type": "image", "img_path": "QyR1dNDxRP/tmp/d6c864de16198247a09a4ea694cbc31afd924e75578a7a3f09a10fe7cd672653.jpg", "img_caption": ["(a) With noisy labels distribution $_D$ "], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "QyR1dNDxRP/tmp/0945711c4a890e293d72ae27c3b56a271592df8d5de20486e5a81dcc5f5b8930.jpg", "img_caption": ["(b) With clean labels distribution $D_{0}$ "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Types of overfitting behaviors. Consider a binary classification problem of learning a realizable distribution $\\mathcal{D}_{0}$ . Let $\\mathcal{D}$ be the distribution induced by adding an $\\varepsilon^{\\star}$ -probability for a data point\u2019s label to be flipped relative to $\\mathcal{D}_{0}$ . Suppose a model is trained with data from $\\mathcal{D}$ . Then, assuming the classes are balanced, the trivial generalization performance is 0.5 (in gray; e.g., with a constant predictor). Left. Evaluating the model on $\\mathcal{D}$ , a Bayes-optimal hypothesis (in red) obtains a generalization error of $\\varepsilon^{\\star}$ . For large enough training sets, our results (Section 4) dictate a tempered overfitting behavior illustrated above. For arbitrary noise, the error is approximately bounded by $1-\\varepsilon^{\\star\\varepsilon^{\\star}}\\,\\bar{(}1-\\varepsilon^{\\star})^{1-\\varepsilon^{\\star}}$ (blue). For independent noise, the error is concentrated around the tighter $2\\varepsilon^{\\star}\\left(1-\\varepsilon^{\\star}\\right)$ (yellow). A similar figure was previously shown in Manoj and Srebro [58] for shortest-program interpolators. Right. Assuming independent noise, the left figure can be transformed into the error of the model on $\\mathcal{D}_{0}$ (see Lemma A.9). The linear behavior in the independent setting (yellow) is similar to the behavior observed empirically in Mallinar et al. [57, Figures 2, 3, and 6]. ", "page_idx": 1}, {"type": "text", "text": "The contributions of this paper are: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Returning a min-size NN interpolator is a natural learning rule that follows the Occam\u2019s-razor principle. We show that this learning rule exhibits tempered overfitting (Section 4.1).   \n\u2022 We prove that overparameterized random NN interpolators typically exhibit tempered overftiting with generalization close to a min-size NN interpolator (Section 4.2).   \n\u2022 To the best of our knowledge, ours are the first theoretical results on benign or tempered overftiting that: (1) apply to deep NNs, and (2) do not require a very high or very low input dimension.   \n\u2022 The above results rely on a key technical result \u2014 datasets generated by a constant-size teacher model with label noise can be interpolated1 using a NN of constant depth with threshold activations, binary weights, a width sublinear in $N$ , and roughly $H(\\varepsilon^{\\star})\\cdot N$ weights, where $H(\\varepsilon^{\\star})$ is the binary entropy function of the fraction of corrupted labels (Section 3). ", "page_idx": 1}, {"type": "text", "text": "2 Setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation. We reserve bold lowercase characters for vectors, bold uppercase characters for matrices, and regular uppercase characters for random elements. We use log to denote the base 2 logarithm, and ln to denote the natural logarithm. For a pair of vectors $\\underline{{d}}=\\left(d_{1},\\,.\\,.\\\\,.\\,,d_{L}\\right),\\underline{{d}}^{\\prime}=\\left(d_{1}^{\\prime},\\,.\\,.\\,.\\,,d_{L}^{\\prime}\\right)\\in\\mathbb{N}^{L}$ we denote $\\underline{d}\\leq\\underline{d}^{\\prime}$ if for all $l\\in[L]$ , $d_{l}\\leq d_{l}^{\\prime}$ . We use $\\bigoplus$ to denote the XOR between two binary $\\{0,1\\}$ values, and $\\odot$ to denote the Hadamard (elementwise) product between two vectors. We use $H\\left(\\mathcal{D}\\right)$ to denote the entropy of some distribution $\\mathcal{D}$ . Finally, we use ${\\mathrm{Ber}}\\left(\\varepsilon\\right)$ for the Bernoulli distribution with parameter $\\varepsilon$ , and $H\\left(\\varepsilon\\right)$ for its entropy, which is the binary entropy function. ", "page_idx": 2}, {"type": "text", "text": "2.1 Model: Fully connected threshold NNs with binary weights ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Similarly to Buzaglo et al. [13], we define the following model. ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 (Binary threshold networks). For a depth $L$ , widths $\\underline{d}=(d_{1},\\ldots,d_{L})$ , input dimension $d_{0}$ , a scaled-neuron fully connected binary threshold NN, or binary threshold network, is a mapping $\\theta\\mapsto h_{\\theta}$ such that $h_{\\pmb{\\theta}}:\\{0,1\\}^{d_{0}}\\to\\{0,1\\}^{d_{L}}$ , parameterized by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{\\theta}=\\left\\{\\mathbf{W}^{(l)},\\mathbf{b}^{(l)},\\pmb{\\gamma}^{(l)}\\right\\}_{l=1}^{L}\\;,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where for every layer $l\\in[L]$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{W}^{(l)}\\in\\mathcal{Q}_{l}^{W}=\\{0,1\\}^{d_{l}\\times d_{l-1}}\\,,\\,\\,\\gamma^{(l)}\\in\\mathcal{Q}_{l}^{\\gamma}\\!=\\!\\{-1,0,1\\}^{d_{l}}\\,,\\,\\,\\mathbf{b}^{(l)}\\!\\in\\mathcal{Q}_{l}^{b}\\!=\\!\\{-d_{l-1}+1,\\dots,d_{l-1}\\}^{d_{l}}\\,\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This mapping is defined recursively as $h_{\\pmb\\theta}\\left(\\mathbf{x}\\right)=h^{(L)}\\left(\\mathbf{x}\\right)$ where ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h^{(0)}\\left(\\mathbf{x}\\right)=\\mathbf{x}\\,,}\\\\ {\\forall l\\in[L]\\quad h^{(l)}\\left(\\mathbf{x}\\right)=\\mathbb{I}\\left\\{\\left(\\gamma^{(l)}\\odot\\left(\\mathbf{W}^{(l)}h^{(l-1)}\\left(\\mathbf{x}\\right)\\right)+\\mathbf{b}^{(l)}\\right)>\\mathbf{0}\\right\\}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We denote the number of weights by $\\begin{array}{r}{w(\\underline{d})\\;=\\;\\sum_{l=1}^{L}d_{l}d_{l-1}}\\end{array}$ , and the total number of neurons by $\\begin{array}{r}{n\\left(\\underline{{d}}\\right)\\,=\\,\\sum_{l=1}^{L}d_{l}}\\end{array}$ . The total number of parameters in such a NN is $M\\left(\\underline{{d}}\\right)=w\\left(\\underline{{d}}\\right)+2n\\left(\\underline{{d}}\\right)$ . We denote t he set of functions represen table as binary networks of widths $\\underline{d}$ by $\\mathcal{H}_{\\underline{{d}}}^{\\mathrm{BTN}}$ and their corresponding parameter space by . ", "page_idx": 2}, {"type": "text", "text": "Remark 2.2. Our generalization results are for the above formulation of neuron scalars $\\gamma$ , i.e., ternary scaling before the activation. However, we could have derived similar results if, instead, we changed the scale $\\gamma$ to appear after the activation and also adjusted the range of the biases (see Appendix $\\mathrm{G}$ ). Although we chose the former for simplicity, the latter is similar to the ubiquitous phenomenon in neuroscience known as \u201cDale\u2019s Law\u201d [82]. This law, in a simplified form, means that all outgoing synapses of a neuron have the same effect, e.g., are all excitatory (positive) or all inhibitory (negative). Remark 2.3 (Simple counting argument). Let ${\\underline{{d}}}_{\\operatorname*{max}}\\triangleq\\operatorname*{max}\\left\\{{d_{1},\\dots,{d_{L-1}}}\\right\\}$ be the maximal hiddenlayer width. Then, combinatorially, it holds that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underbrace{\\log\\left|\\mathcal{H}_{\\underline{{d}}}^{\\mathrm{BTN}}\\right|}_{\\#\\mathrm{\\,hypotheses}}\\leq\\log\\underbrace{\\left|\\Theta^{\\mathrm{BTN}}\\left(\\underline{{d}}\\right)\\right|}_{\\mathrm{\\#\\,paraments}}\\leq\\underbrace{w\\left(\\underline{{d}}\\right)}_{\\#\\mathrm{\\,weights}}+\\underbrace{n\\left(\\underline{{d}}\\right)}_{\\#\\mathrm{\\,neurons}}\\left(\\log(3)+\\log\\big(\\underbrace{2d_{\\operatorname*{max}}}_{\\mathrm{\\upmaximal}}\\big)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This implies, using classical PAC bounds [75], that the sample complexity of learning with the finite hypothesis class $\\bar{\\mathcal{H}}_{\\underline{{d}}}^{\\mathrm{BTN}}$ is $O\\left(w\\left(\\underline{{{d}}}\\right)+n\\left(\\underline{{{d}}}\\right)\\log\\underline{{{d}}}_{\\operatorname*{max}}\\right)$ (a more refined bound on $|\\mathcal{H}_{\\underline{{d}}}^{\\mathrm{{BTN}}}|$ is given in Lemma F.1). In Section 4 we show how this generalization bound can be improved in our setting. ", "page_idx": 2}, {"type": "text", "text": "2.2 Data model: A teacher network and label-flip noise ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Data distribution. Let $\\mathcal{X}=\\left\\{0,1\\right\\}^{d_{0}}$ and let $\\mathcal{D}$ be some joint distribution over a finite sample space ${\\mathcal{X}}\\times\\{0,1\\}$ of features and labels. ", "page_idx": 2}, {"type": "text", "text": "Assumption 2.4 (Teacher assumption). We assume a \u201cteacher NN\u201d $h^{\\star}$ generating the labels. A label flipping noise is then added with a noise level of $\\varepsilon^{\\star}=\\mathbb{P}_{(X,Y)\\sim{\\mathcal{D}}}\\left(Y\\neq h^{\\star}(X)\\right)$ , or equivalently ", "page_idx": 2}, {"type": "equation", "text": "$$\nY\\oplus h^{\\star}\\left(X\\right)\\sim\\operatorname{Ber}\\left(\\varepsilon^{\\star}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The label noise is independent when $Y\\oplus h^{\\star}\\left(X\\right)$ is independent of the features $X$ (in Section 4 it leads to stronger generalization results compared to ones for arbitrary noise). ", "page_idx": 2}, {"type": "text", "text": "2.3 Learning problem: Classification with interpolators ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We consider the problem of binary classification over a training set ${\\cal{S}}\\,=\\,\\{({\\bf{x}}_{i},y_{i})\\}_{i=1}^{N}$ with $N$ data points, sampled from the noisy joint distribution $\\mathcal{D}$ described above. We always assume that $S$ is sampled i.i.d., and therefore, with some abuse of notation, we use ${\\mathcal{D}}\\left(S\\right)={\\dot{\\mathcal{D}}}^{N}\\left(S\\right)=$ $\\begin{array}{r}{\\prod_{i=1}^{N}\\mathcal{D}\\left(\\mathbf{x}_{i},y_{i}\\right)}\\end{array}$ . For a hypothesis $h:\\mathcal{X}\\to\\{0,1\\}$ , we define the risk, i.e., the generalization error w.r.t. $\\mathcal{D}$ , as ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{\\mathcal{D}}\\left(h\\right)\\triangleq\\mathbb{P}_{(X,Y)\\sim{\\mathcal{D}}}\\left(h(X)\\neq Y\\right)\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We also define the empirical risk, $i.e.$ ., the training error, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{S}\\left(h\\right)\\triangleq\\frac{1}{N}\\sum_{n=1}^{N}\\mathbb{I}\\left\\{h(\\mathbf{x}_{n})\\neq y_{n}\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We say a hypothesis is an interpolator if $\\mathcal{L}_{S}\\left(h\\right)=0$ . ", "page_idx": 3}, {"type": "text", "text": "In this paper, we are specifically interested in consistent datasets that can be perfectly fit. This is formalized in the following definition. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.5 (Consistent datasets). A dataset ${\\cal{S}}=\\{({\\bf x}_{i},y_{i})\\}_{i=1}^{N}$ is consistent if ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\forall i,j\\in[N]\\ \\mathbf{x}_{i}=\\mathbf{x}_{j}\\Longrightarrow y_{i}=y_{j}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Motivated by modern NNs which are often extremely overparameterized, we are interested in the generalization behavior of interpolators, i.e., models that fit a consistent training set perfectly. Specifically, we consider Framework 1. While this framework is general enough to fit any minimal training error models, we shall be interested in the generalization of $A\\left(S\\right)$ in cases where the training set is most likely consistent (Def. 2.5). ", "page_idx": 3}, {"type": "text", "text": "Framework 1 Learning interpolators Input: A training set $S$ . Algorithm: if $S$ is consistent: return an interpolator $A\\left(S\\right)=h$ (such that $\\begin{array}{r}{\\mathcal{L}_{S}\\left(h\\right)=0,}\\end{array}$ ) else: return an arbitrary hypothesis $A\\left(S\\right)=h$ (e.g., $h(\\mathbf{x})=0,\\forall\\mathbf{x})$ ", "page_idx": 3}, {"type": "text", "text": "In Section 4, we analyze the generalization of two learning rules that fall under this framework: (1) learning min-size NN interpolators and (2) sampling random NN interpolators. Our analysis reveals a tempered overfitting behavior in both cases. ", "page_idx": 3}, {"type": "text", "text": "3 Interpolating a noisy training set ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our main generalization results rely on a key technical result, which shows how to memorize any consistent training set generated according to our noisy teacher model. We prove that the memorizing \u201cstudent\u201d NN can be small enough to yield meaningful generalization bounds in the next sections. ", "page_idx": 3}, {"type": "text", "text": "We begin by noticing that under a teacher model $h^{\\star}$ (Assumption 2.4), the labels of a consistent dataset $S$ (Def. 2.5) can be decomposed as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\forall i\\in[N]\\,\\,y_{i}=h^{\\star}\\left(\\mathbf{x}_{i}\\right)\\oplus f\\left(\\mathbf{x}_{i}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $f:\\{0,1\\}^{d_{0}}\\to\\{0,1\\}$ indicates a label flip in the $i^{\\mathrm{th}}$ example, and can be defined arbitrarily for $\\textbf{x}\\notin{S}$ . Motivated by this observation, we now show an upper bound for the dimensions of a network interpolating $S$ , by bounding the dimensions of an NN implementing an arbitrary \u201cpartial\u201d function $f$ defined on $N$ points. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1 (Memorizing the label flips). Let $f\\colon\\{0,1\\}^{d_{0}}\\ \\to\\ \\{0,1,\\star\\}$ be any function.2 Let $N\\,=\\,|f^{-1}(\\{0,1\\})|$ and $N_{1}\\,=\\,|f^{-1}(\\mathbf{\\dot{1}})|$ . There exists $a$ depth-14 binary threshold network $\\tilde{h}$ : $\\{0,1\\}^{d_{0}}\\rightarrow\\{0,1\\}$ , with widths $\\tilde{\\underline{d}},$ satisfying the following. ", "page_idx": 4}, {"type": "text", "text": "2. The total number of weights in $\\tilde{h}$ is at most $\\displaystyle(1+o(1))\\cdot\\log\\binom{N}{N_{1}}+\\mathrm{poly}(d_{0})$ . More precisely, ", "page_idx": 4}, {"type": "equation", "text": "$$\nw\\left(\\tilde{\\underline{d}}\\right)=\\log\\binom{N}{N_{1}}+\\left(\\log\\binom{N}{N_{1}}\\right)^{3/4}\\cdot\\mathrm{polylog}\\,N+O(d_{0}^{2}\\cdot\\log N)\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3. Every layer of $\\tilde{h}$ has width at most $(\\log{\\binom{N}{N_{1}}})^{3/4}\\cdot\\mathrm{poly}(d_{0})$ . More precisely, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{\\underline{{d}}}_{\\mathrm{max}}=\\left(\\log\\binom{N}{N_{1}}\\right)^{3/4}\\cdot\\mathrm{polylog}\\,N+O(d_{0}\\cdot\\log N)\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The main takeaway from Theorem 3.1 is that label flips can be memorized with networks with a number of parameters that is optimal in the leading order $N\\cdot{\\mathcal{L}}_{S}\\left(h^{\\star}\\right)$ , i.e., not far from the minimal information-theoretical value. The proofs for this section are given in Appendix D. ", "page_idx": 4}, {"type": "text", "text": "Proof idea. Denote $S=f^{-1}\\left(\\{0,1\\}\\right)$ . We employ established techniques from the pseudorandomness literature to construct an efficient hitting set generator $\\mathrm{(HSG)}^{3}$ for the class of all conjunctions of literals. The HSG definition implies that there exists a seed on which the generator outputs a truth table that agrees with $f$ on $S$ . The network \u02dch computes any requested bit of that truth table. ", "page_idx": 4}, {"type": "text", "text": "Remark 3.2 (Dependence on $d_{0}$ ). In Appendix E we show that the $O\\left(d_{0}^{2}\\cdot\\log N\\right)$ term is nearly tight, yet it can be relaxed when using s\u221aome closely related NN architectures. For example, with a single additional layer of width $\\tilde{\\Omega^{\\prime}}(\\sqrt{d_{0}}\\cdot\\log N)$ with ternary weights in the first layer, i.e., $\\mathcal{Q}_{1}^{W}\\,{=}\\,\\{-1,0,1\\}$ instead of $\\{0,1\\}$ , the $O\\left(d_{0}^{2}\\cdot\\log N\\right)$ term of Theorem 3.1 can be improved to $O\\left(d_{0}^{3/2}\\cdot\\log N+d_{0}\\cdot\\log^{3}N\\right)$ . ", "page_idx": 4}, {"type": "text", "text": "Next, with the bound on the dimensions of a NN implementing $f$ , we can bound the dimensions of a min-size interpolating NN by bounding the dimensions of a NN implementing the XOR of $\\tilde{h}$ and $h^{\\star}$ . ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.3 (XOR of two NNs). Let $h_{1},h_{2}$ be two binary NNs with depths $L_{1}\\leq L_{2}$ and widths $\\underline{d}^{(1)},\\underline{d}^{(2)}$ , respectively. Then, there exists a NN h with depth $L_{\\mathrm{XOR}}\\triangleq L_{2}+2$ and widths ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{d_{\\mathrm{XOR}}}{d_{\\mathrm{XOR}}}\\triangleq\\left(d_{1}^{(1)}+d_{1}^{(2)},\\,\\cdot\\,.\\,.\\,,\\,d_{L_{1}}^{(1)}+d_{L_{1}}^{(2)},\\,d_{L_{1}+1}^{(2)}+1,\\,.\\,.\\,.\\,,\\,d_{L_{2}}^{(2)}+1,\\,2,\\,1\\right)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "such that for all inputs $\\mathbf{x}\\in\\{0,1\\}^{d_{0}}$ , $h\\left(\\mathbf{x}\\right)=h_{1}\\left(\\mathbf{x}\\right)\\oplus h_{2}\\left(\\mathbf{x}\\right).$ ", "page_idx": 4}, {"type": "text", "text": "Combining Theorem 3.1 and Lemma 3.3 results in the following corollary. ", "page_idx": 4}, {"type": "text", "text": "Corollary 3.4 (Memorizing a consistent dataset). For any teacher $h^{\\star}$ of depth $L^{\\star}$ and dimensions $\\underline{d}^{\\star}$ and any consistent training set $S$ generated from it, there exists an interpolating NN $h$ (i.e., $\\begin{array}{r}{\\mathcal{L}_{S}\\left(h\\right)=0,}\\end{array}$ ) of depth $L=\\operatorname*{max}\\left\\{L^{\\star},14\\right\\}+2$ and dimensions $\\underline{{d}},$ , such that the number of weights is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w\\left(\\underline{d}\\right)\\leq w\\left(\\underline{d}^{\\star}\\right)+N\\cdot H\\left(\\mathcal{L}_{S}\\left(h^{\\star}\\right)\\right)+2n\\left(\\underline{d}^{\\star}\\right)N^{3/4}H\\left(\\mathcal{L}_{S}\\left(h^{\\star}\\right)\\right)^{3/4}\\mathrm{polylog}N}\\\\ &{\\qquad\\qquad+\\ O\\left(d_{0}\\left(d_{0}+n\\left(\\underline{d}^{\\star}\\right)\\right)\\cdot\\log N\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and the maximal width is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d_{\\mathrm{max}}\\leq\\underline{{d}}_{\\mathrm{max}}^{\\star}+N^{3/4}\\cdot H\\left(\\mathcal{L}_{S}\\left(h^{\\star}\\right)\\right)^{3/4}\\cdot\\mathrm{polylog}\\left(N\\right)+O\\left(d_{0}\\cdot\\log\\left(N\\right)\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Proof idea. We explicitly construct a NN with the desired properties. We can choose a subset of neurons to implement the teacher NN and another subset to implement the NN memorizing the label filps. Furthermore, we zero the weights between the two subsets. Two additional layers compute the XOR of the outputs, thus yielding the labels as in (1). This is illustrated in Figure 2. ", "page_idx": 4}, {"type": "image", "img_path": "QyR1dNDxRP/tmp/56ffd43b78f5b9f233427f1eeb71323542e8c45e7eb05e5b9e7b7ed7976b3bc3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 2: Interpolating a dataset. To memorize the training set, we use a subset of the parameters to match those of the teacher and another subset to memorize the noise (label filps). Then, we \u201cmerge\u201d these subsets to interpolate the noisy training set. In our figure, (1) blue edges represent weights identical to the teacher\u2019s; (2) yellow edges memorize the noise; (3) red edges are set to 0; and two additional layers implement the XOR between outputs, thus memorizing the training set. ", "page_idx": 5}, {"type": "text", "text": "4 Tempered overfitting of min-size and random interpolators ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we provide our main results on the overfitting behavior of interpolating NNs. We consider min-size NN interpolators and random NN interpolators. For both learning rules, we prove tempered overfitting. Namely, we show that the test performance of the learned interpolators is not much worse than the Bayes optimal error. ", "page_idx": 5}, {"type": "text", "text": "First, for the sake of readability, let us define the marginal peak probability of the distribution. ", "page_idx": 5}, {"type": "text", "text": "Definition 4.1 (Peak marginal probability). $\\begin{array}{r}{\\mathcal{D}_{\\operatorname*{max}}\\triangleq\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{X}}\\mathbb{P}_{(X,Y)\\sim\\mathcal{D}}\\left(X=\\mathbf{x}\\right)\\!.}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Our results in this section focus on cases where the number of training samples is $N=\\omega\\left(d_{0}^{2}\\log d_{0}\\right)$ and $N=o\\left(1/\\sqrt{D_{\\mathrm{max}}}\\right)$ . In such regimes, the data consistency probability is $\\mathrm{\\high^{4}}$ and our bounds are meaningful. Note that given the binarization of the data, $N=o\\left(1/\\sqrt{D_{\\mathrm{max}}}\\right)$ implies an exponential upper bound of $N=o\\left(2^{d_{0}/2}\\right)$ , achieved by the uniform distribution, i.e., when $\\mathcal{D}_{\\mathrm{max}}=2^{-d_{0}}$ . Due to the exponential growth of the sample space w.r.t. the input dimension, we find this assumption to be reasonable. Also, $N=\\omega\\left(d_{0}^{2}\\log\\dot{d}_{0}\\right)$ implies that the input dimension cannot be arbitrarily large, but may still be non trivially small (see comparison to previous work in Section 5). ", "page_idx": 5}, {"type": "text", "text": "4.1 Min-size interpolators ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We consider min-size NN interpolators of a fixed depth, i.e., networks with the smallest number of weights for a certain depth that interpolate a given training set. In realizable settings, achieving good generalization performance by restricting the number of parameters in the learned interpolating model is a natural and well-understood approach. Indeed, in such cases, generalization follows directly from standard VC-dimension bounds [4, 75]. However, when interpolating noisy data, the size of the returned model increases with the number of samples (in order to memorize the noise; see e.g., Vardi et al. [88]), making it challenging to guarantee generalization. In what follows, we prove that even when interpolating noisy data, min-size NNs exhibit good generalization performance. ", "page_idx": 5}, {"type": "text", "text": "Learning rule: Min-size NN interpolator. Given a consistent dataset $S$ and a fixed depth $L$ , a min-size NN interpolator, or min-#weights interpolator, is a binary threshold network $h$ (see Def. 2.1) that achieves $\\mathcal{L}_{S}\\left(h\\right)=0$ using a minimal number of weights. Recall that $\\begin{array}{r}{w(\\underline{{d}})=\\sum_{l=1}^{L}d_{l}d_{l-1}}\\end{array}$ and define the minimal number of weights required to implement a given hypothesis $h$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{w_{L}\\left(h\\right)\\triangleq\\operatorname*{min}_{\\underline{{d}}\\in\\mathbb{N}^{L}}w\\left(\\underline{{d}}\\right)\\ \\mathrm{s.t.}\\ h\\in\\mathcal{H}_{\\underline{{d}}}^{\\mathrm{BTN}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The learning rule is then defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\nA_{L}\\left(S\\right)\\in\\operatorname{argmin}_{h}w_{L}\\left(h\\right){\\mathrm{~s.t.~}}{\\mathcal{L}}_{S}\\left(h\\right)=0\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Theorem 4.2 (Tempered overftiting of min-size NN interpolators). Let $\\mathcal{D}$ be a distribution induced by a noisy teacher of depth $L^{\\star}$ , widths $\\underline{d}^{\\star}$ , $n(\\underline{d}^{\\star})$ neurons, and a noise level of $\\varepsilon^{\\star}<{}^{1/2}$ (Assumption 2.4). There exists $c~>~0$ such that the following holds. Let $S\\ \\sim\\ D^{N}$ be a training set such that $N=\\omega\\big(n\\left(\\underline{{d}}^{\\star}\\right)^{4}H\\left(\\varepsilon^{\\star}\\right)^{3}\\log\\left(n\\left(\\underline{{d}}^{\\star}\\right)\\right)^{c}+d_{0}^{2}\\log d_{0}\\big)$ and $N=o(\\sqrt{1/D_{\\operatorname*{max}}})$ . Then, for any fixed depth $L\\geq\\operatorname*{max}\\left\\{L^{\\star},14\\right\\}+2,$ , the generalization error of the min-size depth- $L$ NN interpolator satisfies the following. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Under arbitrary label noise, ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "\u2022 Under independent label noise, ", "text_level": 1, "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{S}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A_{L}\\left(S\\right)\\right)\\right]\\leq1-2^{-H\\left(\\varepsilon^{\\star}\\right)}+o\\left(1\\right)\\!.}\\\\ &{\\left|\\mathbb{E}_{S}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A_{L}\\left(S\\right)\\right)\\right]-2\\varepsilon^{\\star}\\left(1\\!-\\!\\varepsilon^{\\star}\\right)\\right|=o\\left(1\\right)\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Here, $o\\left(1\\right)$ indicates terms that become insignificant when the number of samples $N$ is large. We illustrate these behaviors in Figure 1. Moreover, we discuss these results and the proof idea in Section 4.3 after presenting the corresponding results for posterior sampling. The complete proof with detailed characterization of the $o(1)$ terms is given in Appendix F.1. ", "page_idx": 6}, {"type": "text", "text": "4.2 Random NN interpolators (posterior sampling) ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Recent empirical [87, 20] and theoretical [13] works have shown that, somewhat surprisingly, randomly sampled deep NNs that interpolate a training set often generalize well. We now turn to analyzing such random interpolators under our teacher assumption and noisy labels (Assumption 2.4). As with min-size NN interpolators, our analysis here reveals a tempered overfitting behavior. ", "page_idx": 6}, {"type": "text", "text": "Prior distribution. A distribution over parameters induces a prior distribution over hypotheses by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{P}\\left(h\\right)=\\mathbb{P}_{\\pmb{\\theta}}\\left(h_{\\pmb{\\theta}}=h\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We focus on the prior induced by the uniform prior over the parameters of binary threshold networks. Specifically, for a fixed depth $L$ and dimensions $\\underline{d}$ , we consider $\\theta\\sim$ Uniform $\\left(\\Theta^{\\mathrm{BTN}}\\left(\\underline{{d}}\\right)\\right)$ . In other words, to generate $h\\sim\\mathcal{P}$ , each weight, bias, and neuron scalar in the NN is sampled independently and uniformly from its respective domain. ", "page_idx": 6}, {"type": "text", "text": "Learning rule: Posterior sampling. For any training set $S$ , denote the probability to sample an interpolating NN by $p_{S}\\triangleq\\,\\mathcal{P}\\left(\\mathcal{L}_{S}\\left(h\\right)=0\\right)$ . When $p_{S}>0$ , define the posterior distribution $\\mathcal{P}_{S}$ as ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\mathcal P}_{S}\\left(h\\right)\\triangleq{\\mathcal P}\\left(h\\mid{\\mathcal L}_{S}\\left(h\\right)=0\\right)=\\frac{{\\mathcal P}(h)}{p_{S}}\\mathbb{I}\\left\\{{\\mathcal L}_{S}\\left(h\\right)=0\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "When $p_{S}\\!=\\!0$ , use an arbitrary $\\mathcal{P}_{S}$ . Finally, the posterior sampling rule is $A_{\\underline{{d}}}\\left(S\\right)\\sim\\mathcal{P}_{S}$ . ", "page_idx": 6}, {"type": "text", "text": "Remark 4.3 (Hypothesis expressivity). The following result requires that the student NN is large enough to interpolate any consistent $S$ (see Corollary 3.4), thus, $p_{S}>0$ and $\\mathcal{P}_{S}$ is defined as in (2). ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.4 (Tempered overftiting of random NN interpolators). Let $\\mathcal{D}$ be a distribution induced by a noisy teacher of depth $L^{\\star}$ , widths $\\underline{d}^{\\star}$ , $n(\\underline{d}^{\\star})$ neurons, and a noise level of $\\varepsilon^{\\star}<{}^{1}\\!/2$ (Assumption 2.4). There exists a constant $c>0$ such that the following holds. Let $S\\sim\\mathcal{D}^{N}$ be a training set such that $N=\\omega\\big(n\\left(\\underline{{d}}^{\\star}\\right)^{4}\\log\\left(n\\left(\\underline{{d}}^{\\star}\\right)\\right)^{c}+d_{0}^{2}\\log d_{0}\\big)$ and $N=o(\\sqrt{1/D_{\\operatorname*{max}}})$ . Then, for any student network of depth $L\\ge\\operatorname*{max}\\left\\{L^{\\star},14\\right\\}+2$ and widths $\\underline{d}\\in\\mathbb{N}^{L}$ holding ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\forall l=1,\\ldots,L^{\\star}\\!-\\!1\\quad d_{l}\\geq d_{l}^{\\star}+N^{3/4}\\cdot\\left(\\log N\\right)^{c}+c\\cdot d_{0}\\cdot\\log\\left(N\\right)\\,,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "the generalization error of posterior sampling satisfies the following. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Under arbitrary label noise, ", "text_level": 1, "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}_{S,A_{\\underline{{d}}}(S)}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A_{\\underline{{d}}}\\left(S\\right)\\right)\\right]\\leq1-2^{-H(\\varepsilon^{\\star})}+O\\left(\\frac{n\\left(\\underline{{d}}\\right)\\cdot\\log\\left(\\underline{{d}}_{\\operatorname*{max}}+d_{0}\\right)}{N}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "\u2022 Under independent label noise, ", "text_level": 1, "page_idx": 6}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{S,A_{\\underline{{d}}}(S)}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A_{\\underline{{d}}}\\left(S\\right)\\right)\\right]-2\\varepsilon^{\\star}\\left(1\\!-\\!\\varepsilon^{\\star}\\right)\\right|\\leq O\\left(\\sqrt{\\frac{n\\left(\\underline{{d}}\\right)\\cdot\\log\\left(\\underline{{d}}_{\\operatorname*{max}}+d_{0}\\right)}{N}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The proof and a detailed description of the error terms are given in Appendix F.2. ", "page_idx": 7}, {"type": "text", "text": "Remarkably, note that the interpolating NN in the theorem might be highly overparameterized, and that for such NNs good generalization is not guaranteed by standard generalization bounds [4, 75]. This theorem complements a similar result by Buzaglo et al. [13] for the realizable setting. ", "page_idx": 7}, {"type": "text", "text": "4.3 Discussion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The overfitting behaviors described in this section are illustrated in Figure 1. ", "page_idx": 7}, {"type": "text", "text": "Proof idea. We extend the information-theoretical generalization bounds from [58] to this paper\u2019s setting in which label collisions in the datasets have a non-zero probability. In particular, we bound the interpolator\u2019s complexity from below by the mutual information between the model and the training set. Since the model is interpolating, we can further bound the mutual information by a quantity dependent on the population error. From the other direction, we bound the model\u2019s complexity from above by (1) its size in the min-size setting of Section 4.1, and (2) by the negative log interpolation probability for the posterior sampling of Section 4.2. Together with Corollary 3.4 we obtain the bounds above on the expected generalization error. ", "page_idx": 7}, {"type": "text", "text": "In Figure 2 we illustrated the construction of a memorizing network used to bound the complexity of the min-size interpolator. In the following Figure 3 we illustrate how the interpolation probability $p_{S}$ can be bounded to induce a meaningful generalization bound. ", "page_idx": 7}, {"type": "text", "text": "Figure 3: Interpolating a dataset with an overparameterized student. We build on the construction from Figure 2 that memorizes a dataset using a subset of the parameters (blue, yellow, and red edges). Then, redundant neurons (gray) can be effectively ignored by setting their neuron scaling parameters $(\\gamma)$ to 0, leaving the redundant weights (gray edges) unconstrained. Thus, the interpolation probability $p_{S}$ can be bounded by a quantity exponentially decaying in the number of neurons $n\\left(\\underline{{\\dot{d}}}\\right)$ rather than in the number of weights $w\\left(\\underline{{d}}\\right)=\\omega\\left(N\\right)$ . ", "page_idx": 7}, {"type": "image", "img_path": "QyR1dNDxRP/tmp/b560aa4d12d01a7a7f8303790e31c73c62ee26a1d2ef659f412826b334feb25d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Following Remark 3.2, the assumption $N=\\omega\\left(d_{0}^{2}\\log d_{0}\\right)$ can be relaxed in some related architectures. For example, with a single additional layer of width $O\\left(\\sqrt{d_{0}}\\cdot\\log N\\right)$ and ternary weights in the first layer $\\mathcal{Q}_{1}^{W}=\\{-1,0,1\\}$ , the requirement can be relaxed to $N=\\omega\\big(d_{0}^{3/2}\\log d_{0}\\big)$ . ", "page_idx": 7}, {"type": "text", "text": "Remark 4.5 (Higher weight quantization). The bounds in the arbitrary noise setting can easily be extended to NNs with higher quantization levels. For example, letting $\\dot{\\mathcal{Q}}_{l}^{W}$ such that $\\left\\lbrack\\boldsymbol{Q}_{l}^{W}\\right\\vert=\\boldsymbol{Q}\\mathrm{\\bar{~}}$ and $\\{0,1\\}\\subseteq\\mathcal{Q}_{l}^{W}$ , under the appropriate assumptions, we get that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{({\\cal S},{\\cal A}({\\cal S}))}\\left[{\\mathcal{L}}_{\\mathcal{D}}\\left({\\cal A}\\left({\\cal S}\\right)\\right)\\right]\\lessapprox1-{\\cal Q}^{-H(\\varepsilon^{\\star})}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "which is a meaningful bound for noise levels $\\varepsilon^{\\star}\\leq\\varepsilon\\left(Q\\right)$ for some $\\varepsilon\\left(Q\\right)<{^1\\!/2}$ .5 Tighter results would require utilizing the additional quantization levels to achieve smaller dimensions of the interpolating network, and are left to future work. ", "page_idx": 7}, {"type": "text", "text": "5 Related work ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Benign and tempered overfitting. The benign overfitting phenomenon has been extensively studied in recent years. Previous works analyzed the conditions in which benign overfitting occurs in linear regression [34, 11, 5, 65, 67, 21, 47, 93, 86, 98, 44, 90, 19, 3, 76, 31], kernel regression [51, 61, 53, 57, 72, 10, 60, 8, 50, 99, 6], and linear classification [18, 91, 14, 66, 64, 76, 52, 85, 92, 25]. Moreover, several works proved benign overfitting in classification using nonlinear NNs [28, 29, 15, 49, 94, 95, 62, 48, 30, 46]. All the aforementioned benign overfitting results require high-dimensional settings, namely, the input dimension is larger than the number of training samples. ", "page_idx": 7}, {"type": "text", "text": "Mallinar et al. [57] suggested the taxonomy of benign, tempered, and catastrophic overftiting, which we use in this work. They demonstrated empirically that nonlinear NNs in classification tasks exhibit tempered overfitting. As mentioned in the introduction, our theoretical results for the independent noise case closely resemble these empirical findings (see Figure 1). Tempered overfitting in kernel ridge regression was theoretically studied in Mallinar et al. [57], Zhou et al. [99], Barzilai and Shamir [6]. In univariate ReLU NNs (namely, for input dimension 1), tempered overftiting was obtained for both classification [48] and regression [43]. Manoj and Srebro [58] proved tempered overftiting for a learning rule returning short programs in some programming language. Finally, tempered overftiting is well understood for the 1-nearest-neighbor learning rule, where the asymptotic risk is roughly twice the Bayes risk [23]. ", "page_idx": 8}, {"type": "text", "text": "Circuit complexity. Theorem 3.1 (our NN for memorizing label filps) is in a similar spirit as several prior theorems in the area of circuit complexity. For example, Lupanov famously proved that every function $f\\colon\\{0,1\\}^{d_{0}}\\rightarrow\\{0,1\\}$ can be computed by a circuit consisting of $(1+\\dot{o(1)})\\cdot2^{d_{0}}/d_{0}$ many AND/OR/NOT gates, where the AND/OR gates have fan-in two [55]. Lupanov\u2019s bound, which is tight [77], is analogous to Theorem 3.1, because a NN can be considered a type of circuit. ", "page_idx": 8}, {"type": "text", "text": "Even more relevant is a line of work that analyzes the circuit complexity of an arbitrary partial function $f\\colon\\{0,1\\}^{d_{0}}\\to\\{0,1,\\star\\}$ with a given domain size $N$ and a given number of 1-inputs $N_{1}$ , similar to the setup of Theorem 3.1. See Jukna\u2019s textbook for an overview [45, Section 1.4.2]. We highlight the work of Chashkin, who showed that every such function can be computed by a circuit (of unbounded depth and bounded fan-in) with (1 + o(1)) \u00b7lolgo lgo(gN(1NN)) gates [17]. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "To the best of our knowledge, prior to our work, nothing analogous to Chashkin\u2019s theorem [17] was known regarding constant-depth threshold networks. It is conceivable that one could adapt Chashkin\u2019s construction [17] to the binary threshold network setting as a method of proving Theorem 3.1, but our proof of Theorem 3.1 uses a different approach. Our proof relies on shallow threshold networks computing $k$ -wise independent generators [36] and an error-reduction technique that was developed in the context of space-bounded derandomization [38], among other ingredients. ", "page_idx": 8}, {"type": "text", "text": "Memorization. Our construction shows how noisy data can be interpolated using a small threshold NN with binary weights. It essentially requires memorizing the noisy examples. The task of memorization, namely, finding a smallest NN that allows for interpolation of arbitrary data points, has been extensively studied in recent decades. Memorization of $N$ arbitrary points in general position in $\\mathbb{R}^{d}$ with a two-layer NN can be achieved using $\\begin{array}{r}{O\\left(\\lceil\\frac{N}{d}\\rceil\\right)}\\end{array}$ hidden neurons [7, 81, 12]. Memorizing arbitrary $N$ points, even if they are not in general position, can be done using two-layer networks with $O(N)$ neurons [41, 74, 40, 97]. With three-layer networks, $O(\\sqrt{N})$ neurons suffice, but the number of parameters is still linear in $N$ [39, 96, 89, 71]. Using deeper networks allows for memorization with a sublinear number of parameters [68, 88]. For example, memorization with networks of depth $\\sqrt{N}$ requires only $\\tilde{O}(\\sqrt{N})$ parameters [88]. However, we note that in the aforementioned results, the number of quantization levels is not constant, namely, the number of bits in the representation of each weight depends on $N$ .6 Moreover, even in the sublinear constructions of [68, 88], the number of bits required to represent the network is $\\omega(N)$ . As a result, in this work we cannot rely on these constructions to obtain meaningful bounds. ", "page_idx": 8}, {"type": "text", "text": "Posterior sampling and guess and check. The generalization of random interpolating neural networks has previously been studied, both empirically and theoretically [87, 63, 84, 20, 13]. Theisen et al. [84] studied the generalization of interpolating random linear and random features classifiers. Valle-Perez et al. [87], Mingard et al. [63] considered the Gaussian process approximation to random NNs which typically requires networks with infinite width. Buzaglo et al. [13] provided a method to obtain generalization results for quantized random NNs of general architectures \u2014 possibly deep and with finite width, under the assumption of a narrow teacher model. A variant of this approach was used to prove our generalization results of posterior sampling, with the XOR network (Lemma 3.3) used in the role of the teacher. ", "page_idx": 8}, {"type": "text", "text": "6 Extensions, limitations, and future work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we focused on binary (fully connected) threshold networks of depth $L\\protect\\geq16$ (Section 2.1) with binary input features (Section 2.2), for which we were able to derive nontrivial generalization bounds. ", "page_idx": 9}, {"type": "text", "text": "Our results can be extended with simple modifications to derive bounds in other settings. For instance, to NNs with higher weight quantization (see Remark 4.5), or to ReLU networks (since any threshold network with binary weights can be computed by a not-much-larger ReLU network with a constant quantization level). Unfortunately, without more sophisticated arguments these extensions result in looser generalization bounds. The \u201cbottleneck\u201d of our approach is the reliance on (nearly) tight bounds on the widths of interpolating NNs. ", "page_idx": 9}, {"type": "text", "text": "Extending the results to other architectures (e.g., CNNs or fully connected without neuron scaling) and other quantization schemes (e.g., floating point representations) will mainly require utilizing their specific structure to derive tighter bounds on the complexity (e.g., number of weights or number of bits) needed to interpolate consistent datasets. Furthermore, our bounds require the depth of the networks to be at least 16, and the width to be $\\omega\\left(N^{3/4}\\right)$ , which might be deemed impractical for real datasets.7 The key to alleviating these requirements is, again, obtaining tighter complexity results. ", "page_idx": 9}, {"type": "text", "text": "Our paper focused on consistent training sets (Def. 2.5), in order to allow perfect interpolation. Realistically, models do not always perfectly interpolate the training set, and therefore it is interesting to find generalization bounds for non-interpolating models, depending on the training error. In addition, it is interesting to relate the generalization to the training loss, and not just to the training accuracy. Such extensions will require either broadening our generalization results or deriving new ones. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank Alexander Chashkin for generously providing English-language expositions of some results from his work [17] as well as some results from Lupanov\u2019s work [56] (personal communication). The research of DS was Funded by the European Union (ERC, A-B-C-Deep, 101039436). Views and opinions expressed are however those of the author only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency (ERCEA). Neither the European Union nor the granting authority can be held responsible for them. DS also acknowledges the support of the Schmidt Career Advancement Chair in AI. GV is supported by research grants from the Center for New Scientists at the Weizmann Institute of Science, and the Shimon and Golde Picker \u2013 Weizmann Annual Grant. Part of this work was done as part of the NSF-Simons funded Collaboration on the Mathematics of Deep Learning. NS was partially supported by the NSF TRIPOD Institute on Data Economics Algorithms and Learning (IDEAL) and an NSF-IIS award. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] N. Alon, L. Babai, and A. Itai. A fast and simple randomized parallel algorithm for the maximal independent set problem. J. Algorithms, 7(4):567\u2013583, 1986. ISSN 0196-6774. doi: 10.1016/0196-6774(86)90019-2.   \n[2] N. Alon, O. Goldreich, J. H\u00e5 stad, and R. Peralta. Simple constructions of almost $k$ -wise independent random variables. Random Structures Algorithms, 3(3):289\u2013304, 1992. ISSN 1042-9832. doi: 10.1002/rsa.3240030308.   \n[3] P. L. Bartlett and P. M. Long. Failures of model-dependent generalization bounds for least-norm interpolation. The Journal of Machine Learning Research, 22(1):9297\u20139311, 2021.   \n[4] P. L. Bartlett, N. Harvey, C. Liaw, and A. Mehrabian. Nearly-tight vc-dimension and pseudodimension bounds for piecewise linear neural networks. Journal of Machine Learning Research, 20(63):1\u201317, 2019.   \n[5] P. L. Bartlett, P. M. Long, G. Lugosi, and A. Tsigler. Benign overfitting in linear regression. Proceedings of the National Academy of Sciences, 117(48):30063\u201330070, 2020.   \n[6] D. Barzilai and O. Shamir. Generalization in kernel regression under realistic assumptions. In Forty-first International Conference on Machine Learning, 2024.   \n[7] E. B. Baum. On the capabilities of multilayer perceptrons. Journal of complexity, 4(3):193\u2013215, 1988.   \n[8] D. Beaglehole, M. Belkin, and P. Pandit. On the inconsistency of kernel ridgeless regression in fixed dimensions. SIAM Journal on Mathematics of Data Science, 5(4):854\u2013872, 2023. doi: 10.1137/22M1499819.   \n[9] P. W. Beame, S. A. Cook, and H. J. Hoover. Log depth circuits for division and related problems. SIAM J. Comput., 15(4):994\u20131003, 1986. ISSN 0097-5397. doi: 10.1137/0215070.   \n[10] M. Belkin, D. J. Hsu, and P. Mitra. Overftiting or perfect ftiting? Risk bounds for classification and regression rules that interpolate. In Advances in Neural Information Processing Systems (NeurIPS), 2018.   \n[11] M. Belkin, D. Hsu, and J. Xu. Two models of double descent for weak features. SIAM Journal on Mathematics of Data Science, 2(4):1167\u20131180, 2020.   \n[12] S. Bubeck, R. Eldan, Y. T. Lee, and D. Mikulincer. Network size and size of the weights in memorization with two-layers neural networks. In Neural Information Processing Systems, 2020.   \n[13] G. Buzaglo, I. Harel, M. S. Nacson, A. Brutzkus, N. Srebro, and D. Soudry. How uniform random weights induce non-uniform bias: Typical interpolating neural networks generalize with narrow teachers. In International Conference on Machine Learning (ICML), 2024.   \n[14] Y. Cao, Q. Gu, and M. Belkin. Risk bounds for over-parameterized maximum margin classification on sub-gaussian mixtures. In Advances in Neural Information Processing Systems (NeurIPS), 2021.   \n[15] Y. Cao, Z. Chen, M. Belkin, and Q. Gu. Benign overfitting in two-layer convolutional neural networks. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural Information Processing Systems, 2022.   \n[16] A. K. Chandra, L. Stockmeyer, and U. Vishkin. Constant depth reducibility. SIAM J. Comput., 13(2):423\u2013439, 1984. ISSN 0097-5397. doi: 10.1137/0213028.   \n[17] A. Chashkin. On the realization of partial boolean functions. In Proceedings of the 7th International Conference on Discrete Models in the Theory of Control Systems, pages 390\u2013404, 2006. In Russian.   \n[18] N. S. Chatterji and P. M. Long. Finite-sample analysis of interpolating linear classifiers in the overparameterized regime. Journal of Machine Learning Research, 22(129):1\u201330, 2021.   \n[19] N. S. Chatterji, P. M. Long, and P. L. Bartlett. The interplay between implicit bias and benign overfitting in two-layer linear networks. Journal of machine learning research, 23(263):1\u201348, 2022.   \n[20] P. Chiang, R. Ni, D. Y. Miller, A. Bansal, J. Geiping, M. Goldblum, and T. Goldstein. Loss landscapes are all you need: Neural network generalization can be explained without the implicit bias of gradient descent. In The Eleventh International Conference on Learning Representations, 2023.   \n[21] G. Chinot and M. Lerasle. On the robustness of the minimum $\\ell_{2}$ interpolator. arXiv preprint arXiv:2003.05838, 2020.   \n[22] B. Chor and O. Goldreich. On the power of two-point based sampling. J. Complexity, 5(1): 96\u2013106, 1989. ISSN 0885-064X. doi: 10.1016/0885-064X(89)90015-0.   \n[23] T. Cover and P. Hart. Nearest neighbor pattern classification. IEEE transactions on information theory, 13(1):21\u201327, 1967.   \n[24] M. Dietzfelbinger. Universal hashing and $k$ -wise independent random variables via integer arithmetic without primes. In STACS 96 (Grenoble, 1996), volume 1046 of Lecture Notes in Comput. Sci., pages 569\u2013580. Springer, Berlin, 1996.   \n[25] K. Donhauser, N. Ruggeri, S. Stojanovic, and F. Yang. Fast rates for noisy interpolation require rethinking the effect of inductive bias. In International Conference on Machine Learning (ICML), 2022.   \n[26] W. Eberly. Very fast parallel polynomial arithmetic. SIAM J. Comput., 18(5):955\u2013976, 1989. ISSN 0097-5397. doi: 10.1137/0218066.   \n[27] G. Even, O. Goldreich, M. Luby, N. Nisan, and B. Veli\u02c7ckovi\u00b4c. Efficient approximation of product distributions. Random Structures & Algorithms, 13(1):1\u201316, 1998.   \n[28] S. Frei, N. S. Chatterji, and P. L. Bartlett. Benign overftiting without linearity: Neural network classifiers trained by gradient descent for noisy linear data. In Conference on Learning Theory (COLT), 2022.   \n[29] S. Frei, G. Vardi, P. Bartlett, and N. Srebro. Benign overfitting in linear classifiers and leaky relu networks from kkt conditions for margin maximization. In Proceedings of Thirty Sixth Conference on Learning Theory, volume 195 of Proceedings of Machine Learning Research, pages 3173\u20133228. PMLR, 12\u201315 Jul 2023.   \n[30] E. George, M. Murray, W. Swartworth, and D. Needell. Training shallow relu networks on noisy data using hinge loss: when do we overfit and is it benign? Advances in Neural Information Processing Systems, 36, 2024.   \n[31] N. Ghosh and M. Belkin. A universal trade-off between the model size, test loss, and training loss of linear predictors. SIAM Journal on Mathematics of Data Science, 5(4):977\u20131004, 2023.   \n[32] A. Hajnal, W. Maass, P. Pudl\u00e1k, M. Szegedy, and G. Tur\u00e1n. Threshold circuits of bounded depth. J. Comput. System Sci., 46(2):129\u2013154, 1993. ISSN 0022-0000. doi: 10.1016/0022-0000(93) 90001-D.   \n[33] S. Han, J. Pool, J. Tran, and W. Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015.   \n[34] T. Hastie, A. Montanari, S. Rosset, and R. J. Tibshirani. Surprises in high-dimensional ridgeless least squares interpolation. Annals of statistics, 50(2):949, 2022.   \n[35] P. Hatami and W. Hoza. Paradigms for unconditional pseudorandom generators. Foundations and Trends\u00ae in Theoretical Computer Science, 16(1-2):1\u2013210, 2024. ISSN 1551-305X. doi: 10.1561/0400000109.   \n[36] A. Healy and E. Viola. Constant-depth circuits for arithmetic in finite fields of characteristic two. In STACS 2006, volume 3884 of Lecture Notes in Comput. Sci., pages 672\u2013683. Springer, Berlin, 2006. doi: 10.1007/11672142\\_55.   \n[37] T. Hofmeister, W. Hohberg, and S. K\u00f6hling. Some notes on threshold circuits, and multiplication in depth 4. Inform. Process. Lett., 39(4):219\u2013225, 1991. ISSN 0020-0190. doi: 10.1016/ 0020-0190(91)90183-I.   \n[38] W. M. Hoza and D. Zuckerman. Simple optimal hitting sets for small-success RL. SIAM J. Comput., 49(4):811\u2013820, 2020. ISSN 0097-5397. doi: 10.1137/19M1268707.   \n[39] G.-B. Huang. Learning capability and storage capacity of two-hidden-layer feedforward networks. IEEE transactions on neural networks, 14(2):274\u2013281, 2003.   \n[40] G.-B. Huang and H. A. Babri. Upper bounds on the number of hidden neurons in feedforward networks with arbitrary bounded nonlinear activation functions. IEEE transactions on neural networks, 9(1):224\u2013229, 1998.   \n[41] S.-C. Huang, Y.-F. Huang, et al. Bounds on the number of hidden neurons in multilayer perceptrons. IEEE transactions on neural networks, 2(1):47\u201355, 1991.   \n[42] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio. Binarized neural networks. Advances in neural information processing systems, 29, 2016.   \n[43] N. Joshi, G. Vardi, and N. Srebro. Noisy interpolation learning with shallow univariate reLU networks. In The Twelfth International Conference on Learning Representations, 2024.   \n[44] P. Ju, X. Lin, and J. Liu. Overfitting can be harmless for basis pursuit, but only to a degree. Advances in Neural Information Processing Systems, 33:7956\u20137967, 2020.   \n[45] S. Jukna. Boolean function complexity, volume 27 of Algorithms and Combinatorics. Springer, Heidelberg, 2012. ISBN 978-3-642-24507-7. doi: 10.1007/978-3-642-24508-4. Advances and frontiers.   \n[46] K. Karhadkar, E. George, M. Murray, G. Mont\u00fafar, and D. Needell. Benign overftiting in leaky relu networks with moderate input dimension. arXiv preprint arXiv:2403.06903, 2024.   \n[47] F. Koehler, L. Zhou, D. J. Sutherland, and N. Srebro. Uniform convergence of interpolators: Gaussian width, norm bounds and benign overftiting. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems, 2021.   \n[48] G. Kornowski, G. Yehudai, and O. Shamir. From tempered to benign overfitting in relu neural networks. Advances in Neural Information Processing Systems, 36, 2024.   \n[49] Y. Kou, Z. Chen, Y. Chen, and Q. Gu. Benign overfitting for two-layer relu networks. arXiv preprint arXiv:2303.04145, 2023.   \n[50] J. Lai, M. Xu, R. Chen, and Q. Lin. Generalization ability of wide neural networks on $\\mathbb{R}$ . arXiv preprint arXiv:2302.05933, 2023.   \n[51] T. Liang and A. Rakhlin. Just interpolate: Kernel \u201cridgeless\" regression can generalize. Annals of Statistics, 48(3):1329\u20131347, 2020.   \n[52] T. Liang and B. Recht. Interpolating classifiers make few mistakes. Journal of Machine Learning Research, 24(20):1\u201327, 2023.   \n[53] T. Liang, A. Rakhlin, and X. Zhai. On the multiple descent of minimum-norm interpolants and restricted lower isometry of kernels. In Conference on Learning Theory (COLT), 2020.   \n[54] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang. Learning efficient convolutional networks through network slimming. In Proceedings of the IEEE international conference on computer vision, pages 2736\u20132744, 2017.   \n[55] O. B. Lupanov. A method of circuit synthesis. Izvesitya VUZ, Radiofizika, 1, 1958. In Russian.   \n[56] O. B. Lupanov. On a certain approach to the synthesis of control systems \u2013 the principle of local coding. Problemy Kibernetiki, 14:31\u2013110, 1965. In Russian.   \n[57] N. R. Mallinar, J. B. Simon, A. Abedsoltan, P. Pandit, M. Belkin, and P. Nakkiran. Benign, tempered, or catastrophic: Toward a refined taxonomy of overfitting. In Advances in Neural Information Processing Systems, 2022.   \n[58] N. S. Manoj and N. Srebro. Interpolation learning with minimum description length. arXiv preprint arXiv:2302.07263, 2023.   \n[59] Y. Mansour, N. Nisan, and P. Tiwari. The computational complexity of universal hashing. Theoret. Comput. Sci., 107(1):121\u2013133, 1993. ISSN 0304-3975. doi: 10.1016/0304-3975(93) 90257-T.   \n[60] A. D. McRae, S. Karnik, M. Davenport, and V. K. Muthukumar. Harmless interpolation in regression and classification with structured features. In International Conference on Artificial Intelligence and Statistics, pages 5853\u20135875. PMLR, 2022.   \n[61] S. Mei and A. Montanari. The generalization error of random features regression: Precise asymptotics and the double descent curve. Communications on Pure and Applied Mathematics, 2019.   \n[62] X. Meng, D. Zou, and Y. Cao. Benign overftiting in two-layer relu convolutional neural networks for xor data. arXiv preprint arXiv:2310.01975, 2023.   \n[63] C. Mingard, G. Valle-P\u00e9rez, J. Skalse, and A. A. Louis. Is sgd a bayesian sampler? well, almost. The Journal of Machine Learning Research, 22(1):3579\u20133642, 2021.   \n[64] A. Montanari, F. Ruan, Y. Sohn, and J. Yan. The generalization error of max-margin linear classifiers: High-dimensional asymptotics in the overparametrized regime. Preprint, arXiv:1911.01544, 2020.   \n[65] V. Muthukumar, K. Vodrahalli, V. Subramanian, and A. Sahai. Harmless interpolation of noisy data in regression. IEEE Journal on Selected Areas in Information Theory, 2020.   \n[66] V. Muthukumar, A. Narang, V. Subramanian, M. Belkin, D. Hsu, and A. Sahai. Classification vs regression in overparameterized regimes: Does the loss function matter? Journal of Machine Learning Research, 22(222):1\u201369, 2021.   \n[67] J. Negrea, G. K. Dziugaite, and D. Roy. In defense of uniform convergence: Generalization via derandomization with an application to interpolating predictors. In International Conference on Machine Learning, pages 7263\u20137272, 2020.   \n[68] S. Park, J. Lee, C. Yun, and J. Shin. Provable memorization via deep neural networks using sub-linear parameters. In Conference on Learning Theory, pages 3627\u20133661. PMLR, 2021.   \n[69] N. Pippenger. The complexity of computations by networks. IBM J. Res. Develop., 31(2): 235\u2013243, 1987. ISSN 0018-8646. doi: 10.1147/rd.312.0235.   \n[70] N. Rahaman, A. Baratin, D. Arpit, F. Draxler, M. Lin, F. Hamprecht, Y. Bengio, and A. Courville. On the spectral bias of neural networks. In International conference on machine learning, pages 5301\u20135310. PMLR, 2019.   \n[71] S. Rajput, K. Sreenivasan, D. Papailiopoulos, and amin karbasi. An exponential improvement on the memorization capacity of deep threshold networks. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems, 2021.   \n[72] A. Rakhlin and X. Zhai. Consistency of interpolation with laplace kernels is a high-dimensional phenomenon. In Conference on Learning Theory, pages 2595\u20132623. PMLR, 2019.   \n[73] J. H. Reif and S. R. Tate. On threshold circuits and polynomial computation. SIAM J. Comput., 21(5):896\u2013908, 1992. ISSN 0097-5397. doi: 10.1137/0221053.   \n[74] M. A. Sartori and P. J. Antsaklis. A simple method to derive bounds on the size and to train multilayer neural networks. IEEE transactions on neural networks, 2(4):467\u2013471, 1991.   \n[75] S. Shalev-Shwartz and S. Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.   \n[76] O. Shamir. The implicit bias of benign overfitting. In Conference on Learning Theory, pages 448\u2013478. PMLR, 2022.   \n[77] C. E. Shannon. The synthesis of two-terminal switching circuits. Bell System Tech. J., 28:59\u201398, 1949. ISSN 0005-8580. doi: 10.1002/j.1538-7305.1949.tb03624.x.   \n[78] K.-Y. Siu and J. Bruck. On the power of threshold circuits with small weights. SIAM J. Discrete Math., 4(3):423\u2013435, 1991. ISSN 0895-4801. doi: 10.1137/0404038.   \n[79] K.-Y. Siu and V. P. Roychowdhury. On optimal depth threshold circuits for multiplication and related problems. SIAM J. Discrete Math., 7(2):284\u2013292, 1994. ISSN 0895-4801. doi: 10.1137/S0895480192228619.   \n[80] K.-Y. Siu, J. Bruck, T. Kailath, and T. Hofmeister. Depth efficient neural networks for division and related problems. IEEE Trans. Inform. Theory, 39(3):946\u2013956, 1993. ISSN 0018-9448. doi: 10.1109/18.256501.   \n[81] D. Soudry and E. Hoffer. Exponentially vanishing sub-optimal local minima in multilayer neural networks. arXiv preprint arXiv:1702.05777, 2017.   \n[82] P. Strata and R. Harvey. Dale\u2019s principle. Brain Research Bulletin, 50(5):349\u2013350, 1999. ISSN 0361-9230. doi: https://doi.org/10.1016/S0361-9230(99)00100-8. URL https://www. sciencedirect.com/science/article/pii/S0361923099001008.   \n[83] D. Teney, A. M. Nicolicioiu, V. Hartmann, and E. Abbasnejad. Neural redshift: Random networks are not random functions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4786\u20134796, 2024.   \n[84] R. Theisen, J. Klusowski, and M. Mahoney. Good classifiers are abundant in the interpolating regime. In International Conference on Artificial Intelligence and Statistics, pages 3376\u20133384. PMLR, 2021.   \n[85] C. Thrampoulidis, S. Oymak, and M. Soltanolkotabi. Theoretical insights into multiclass classification: A high-dimensional asymptotic view. Advances in Neural Information Processing Systems, 33:8907\u20138920, 2020.   \n[86] A. Tsigler and P. L. Bartlett. Benign overftiting in ridge regression. Journal of Machine Learning Research, 24(123):1\u201376, 2023.   \n[87] G. Valle-Perez, C. Q. Camargo, and A. A. Louis. Deep learning generalizes because the parameter-function map is biased towards simple functions. In International Conference on Learning Representations, 2019.   \n[88] G. Vardi, G. Yehudai, and O. Shamir. On the optimal memorization power of ReLU neural networks. In International Conference on Learning Representations, 2022.   \n[89] R. Vershynin. Memory capacity of neural networks with threshold and relu activations. arXiv preprint arXiv:2001.06938, 2020.   \n[90] G. Wang, K. Donhauser, and F. Yang. Tight bounds for minimum l1-norm interpolation of noisy data. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2022.   \n[91] K. Wang and C. Thrampoulidis. Binary classification of gaussian mixtures: Abundance of support vectors, benign overftiting, and regularization. SIAM Journal on Mathematics of Data Science, 4(1):260\u2013284, 2022.   \n[92] K. Wang, V. Muthukumar, and C. Thrampoulidis. Benign overfitting in multiclass classification: All roads lead to interpolation. In Advances in Neural Information Processing Systems (NeurIPS), 2021.   \n[93] D. Wu and J. Xu. On the optimal weighted $\\ell_{2}$ regularization in overparameterized linear regression. Advances in Neural Information Processing Systems, 33:10112\u201310123, 2020.   \n[94] X. Xu and Y. Gu. Benign overfitting of non-smooth neural networks beyond lazy training. In International Conference on Artificial Intelligence and Statistics, pages 11094\u201311117. PMLR, 2023.   \n[95] Z. Xu, Y. Wang, S. Frei, G. Vardi, and W. Hu. Benign overftiting and grokking in reLU networks for XOR cluster data. In The Twelfth International Conference on Learning Representations, 2024.   \n[96] C. Yun, S. Sra, and A. Jadbabaie. Small relu networks are powerful memorizers: a tight analysis of memorization capacity. In Advances in Neural Information Processing Systems, pages 15558\u201315569, 2019.   \n[97] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107\u2013115, 2021.   \n[98] L. Zhou, F. Koehler, P. Sur, D. J. Sutherland, and N. Srebro. A non-asymptotic moreau envelope theory for high-dimensional generalized linear models. In Advances in Neural Information Processing Systems, 2022.   \n[99] L. Zhou, J. B. Simon, G. Vardi, and N. Srebro. An agnostic view on the cost of overfitting in (kernel) ridge regression. In The Twelfth International Conference on Learning Representations, 2024. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Preliminaries and Auxiliary Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A.1 Preliminaries ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Before moving to the proofs of the main results, we recall and introduce some notation that will be used throughout the supplementary material. ", "page_idx": 16}, {"type": "text", "text": "Notation. We denote a (possibly random) learning algorithm by $A\\left(S\\right)$ , and assume that it takes values in some hypothesis class $\\mathcal{H}$ . We use $\\mathcal{D}$ to denote the joint distribution over a finite sample space ${\\mathcal{X}}\\times\\{0,1\\}$ of the features and labels, $\\nu$ to denote the marginal distribution of the algorithm, and $p$ to denote the joint distribution of a training set $S\\sim\\mathcal{D}^{N}$ and the algorithm $A\\left(S\\right)$ . Specifically, the training set is a random element ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\boldsymbol{S}=\\left\\{(\\boldsymbol{X}_{1},Y_{1})\\,,\\dots,(\\boldsymbol{X}_{N},Y_{N})\\right\\}\\sim\\mathcal{D}^{N}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $(X_{i},Y_{i})$ is reserved for the $i$ -th example in $S$ . That is $(X_{i},Y_{i})$ is always a sample in $S$ , whereas $(X,Y)$ is used to denote a data point which is independent of $S$ . We use $d D\\left(x,\\bar{y}\\right),\\,d\\nu\\left(h\\right)$ and $d p\\,(s,h)\\,\\,=\\,\\,d p\\,(\\{(x_{1},y_{1})\\,,\\dots,(x_{N}\\overset{\\cdot}{,}y_{N})\\}\\,,h)$ to denote the corresponding probability mass functions. With some abuse of notation, we use $\\,d D\\left(x\\right)$ for the probability mass function of the marginal of $\\mathcal{D}$ over $\\mathcal{X}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\nd\\mathscr{D}\\left(x\\right)=\\mathbb{P}_{(X,Y)\\sim\\mathscr{D}}\\left(X=x\\right)\\,,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and $d p\\left(\\left(x_{1},y_{1}\\right),h\\right)$ for the marginal of the joint probability of a single point from $S$ and the output of the algorithm, $i.e.$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\nd p\\left(\\left(x_{1},y_{1}\\right),h\\right)=\\mathbb{P}_{\\left(S,A(S)\\right)\\sim p}\\left(X_{1}=x_{1},Y_{1}=y_{1},A\\left(S\\right)=h\\right)\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Similarly, we use $d p\\left(x_{1},h\\right),d p\\left(y_{1}\\mid x_{1},h\\right)$ , etc., for the probability mass functions of the appropriate marginal and conditional distributions. ", "page_idx": 16}, {"type": "text", "text": "Interpolating algorithm. In order to simplify the analysis, we introduce a framework of interpolation learning related to the one introduced in Framework 1. ", "page_idx": 16}, {"type": "text", "text": "Let $\\tilde{A}\\left(S\\right)$ be a learning rule satisfying Framework 1, and let $\\star$ be some arbitrary token distinct from any hypothesis the algorithm may produce. We define a modified learning rule $A\\left(S\\right)^{8}$ such that ", "page_idx": 16}, {"type": "text", "text": "\u2022 If $S$ is inconsistent then $A\\left(S\\right)={\\star}$ . ", "page_idx": 16}, {"type": "text", "text": "\u2022 Otherwise, if $S$ is consistent then $A\\left(S\\right)=\\tilde{A}\\left(S\\right)$ , so in particular $\\mathcal{L}_{S}\\left(A\\left(S\\right)\\right)=0$ . ", "page_idx": 16}, {"type": "text", "text": "Notice that since the $A\\left(S\\right)=\\tilde{A}\\left(S\\right)$ when $S$ is consistent ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname{\\mathbb{E}}\\left[{\\mathcal{L}}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\mid{\\mathrm{consistent~}}S\\right]=\\operatorname{\\mathbb{E}}\\left[{\\mathcal{L}}_{\\mathcal{D}}\\left({\\tilde{A}}\\left(S\\right)\\right)\\mid{\\mathrm{consistent~}}S\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and therefore we can find bounds for the generalization error of $\\tilde{A}\\left(S\\right)$ by analyzing $A\\left(S\\right)$ . In addition, when it can be inferred from context we use $A\\left(S\\right)$ to denote the min-size and posterior sampling interpolators (instead of $A_{L}(S)$ or $A_{\\underline{{d}}}(S)$ , respectively). ", "page_idx": 16}, {"type": "text", "text": "For ease of exposition, throughout the appendix\u221a, we rephrase the assumptions made in Section 4, namely, that $\\dot{N}=\\omega\\left(d_{0}^{2}\\log d\\bar{\\theta_{0}}\\right)$ and $N=\\dot{\\sigma}\\left(1/\\sqrt{\\mathcal{D}_{\\operatorname*{max}}}\\right)$ , as follows. ", "page_idx": 16}, {"type": "text", "text": "Assumption A.1 (Bounded input dimension). $d_{0}=o\\left(\\sqrt{N/\\log N}\\right).$ ", "page_idx": 16}, {"type": "text", "text": "Assumption A.2 (Data distribution flatness). $\\ensuremath{\\mathcal{D}_{\\mathrm{max}}}=o\\left(1/N^{2}\\right)$ . ", "page_idx": 16}, {"type": "text", "text": "A.2 Auxiliary results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We start by citing several standard results from information theory and lemmas from Manoj and Srebro [58] which will be useful throughout our supplementary materials. ", "page_idx": 17}, {"type": "text", "text": "Lemma A.3 (Chain rule of mutual information). For any random variables $A_{1},A_{2}$ and $B$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\cal I}\\left((A_{1},A_{2})\\,;B\\right)={\\cal I}\\left(A_{2};B\\mid A_{1}\\right)+{\\cal I}\\left(A_{1};B\\right)\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma A.4. Let $A$ and $B$ be any two random variables with associated marginal distributions $p_{A}$ , $p_{B}$ , and joint $p_{A,B}$ . Let $q_{A\\mid B}$ be any conditional distribution (i.e. such that for any $b$ , $q_{A|B}\\left(\\cdot,b\\right)$ is $a$ normalized non-negative measure). Then: ", "page_idx": 17}, {"type": "equation", "text": "$$\nI\\left(A;B\\right)\\geq\\mathbb{E}_{A,B\\sim p_{A,B}}\\left[\\log\\left(\\frac{d q_{A|B}\\left(A|B\\right)}{d p_{A}\\left(A\\right)}\\right)\\right]\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma A.5. Let $A_{1},A_{2},B$ be random variables where $A_{1}$ and $A_{2}$ are independent. Then ", "page_idx": 17}, {"type": "equation", "text": "$$\nI\\left(\\left(A_{1},A_{2}\\right);B\\right)\\ge I\\left(A_{1};B\\right)+I\\left(A_{2};B\\right)\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma A.6 (Lemma A.4 from Manoj and Srebro [58]). For $C\\geq0$ and $0\\leq\\alpha\\leq1$ it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{1-2^{-H(\\alpha)-C}\\leq1-2^{-H(\\alpha)}+C\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma A.7. Let $\\varepsilon\\in\\left(0,\\frac{1}{2}\\right)$ and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\phi\\left(t\\right)\\triangleq\\phi_{\\varepsilon}\\left(t\\right)=\\frac{\\varepsilon^{t}}{\\varepsilon^{t}+\\left(1-\\varepsilon\\right)^{t}}=\\frac{1}{1+\\left(\\frac{1}{\\varepsilon}-1\\right)^{t}}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, $\\phi$ is monotonically decreasing as a function of $t$ , and convex in $(0,\\infty)$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. Denote $\\begin{array}{r}{\\alpha\\triangleq\\frac{1}{\\varepsilon}-1}\\end{array}$ then ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\phi\\left(t\\right)={\\cfrac{1}{1+\\alpha^{t}}}}\\\\ &{{\\boldsymbol{\\phi}}^{\\prime}\\left(t\\right)={\\cfrac{-\\ln\\left(\\alpha\\right)\\alpha^{t}}{\\left(1+\\alpha^{t}\\right)^{2}}}=-\\ln\\left(\\alpha\\right)\\cdot{\\cfrac{\\alpha^{t}}{1+2\\alpha^{t}+\\alpha^{2t}}}}\\\\ &{{\\boldsymbol{\\phi}}^{\\prime\\prime}\\left(t\\right)=-\\ln\\left(\\alpha\\right)\\cdot{\\cfrac{\\ln\\left(\\alpha\\right)\\alpha^{t}\\left(1+\\alpha^{t}\\right)^{2}-\\alpha^{t}\\cdot2\\left(1+\\alpha^{t}\\right)\\cdot\\ln\\left(\\alpha\\right)\\alpha^{t}}{\\left(1+\\alpha^{t}\\right)^{4}}}}\\\\ &{\\qquad=-\\ln\\left(\\alpha\\right)^{2}\\cdot\\alpha^{t}\\cdot{\\cfrac{\\left(1+\\alpha^{t}\\right)-2\\alpha^{t}}{\\left(1+\\alpha^{t}\\right)^{3}}}=\\ln\\left(\\alpha\\right)^{2}\\cdot\\alpha^{t}\\cdot{\\cfrac{\\alpha^{t}-1}{\\left(1+\\alpha^{t}\\right)^{3}}}\\,.}\\end{array}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Notice that for any $\\varepsilon\\in\\left(0,\\frac{1}{2}\\right)$ , $\\begin{array}{r}{\\alpha=\\frac{1}{\\varepsilon}-1>1}\\end{array}$ so for all $t>0$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\alpha^{t}-1>0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and $\\phi^{\\prime\\prime}\\left(t\\right)>0$ so the function is indeed convex, and $-\\ln\\left(\\alpha\\right)<0$ so $\\phi$ is decreasing. ", "page_idx": 17}, {"type": "text", "text": "Corollary A.8. For all $t>0$ it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\phi\\left(t\\right)\\geq\\phi\\left(1\\right)+\\phi^{\\prime}\\left(1\\right)\\left(t-1\\right)=\\varepsilon+\\ln2\\left(\\varepsilon\\log\\left(\\varepsilon\\right)+\\varepsilon H\\left(\\varepsilon\\right)\\right)\\left(t-1\\right)\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Substituting $t=1$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi\\left(1\\right)=\\frac{\\varepsilon}{\\varepsilon+\\left(1-\\varepsilon\\right)}=\\varepsilon}\\\\ &{\\phi^{\\prime}\\left(1\\right)=-\\ln\\left(\\alpha\\right)\\cdot\\frac{\\alpha}{\\left(1+\\alpha\\right)^{2}}=-\\ln\\left(\\frac{1}{\\varepsilon}-1\\right)\\cdot\\frac{\\frac{1}{\\varepsilon}-1}{\\left(1+\\left(\\frac{1}{\\varepsilon}-1\\right)\\right)^{2}}=-\\ln\\left(\\frac{1-\\varepsilon}{\\varepsilon}\\right)\\cdot\\frac{\\frac{1}{\\varepsilon}-1}{\\left(\\frac{1}{\\varepsilon}\\right)^{2}}}\\\\ &{\\qquad=-\\left(\\ln\\left(1-\\varepsilon\\right)-\\ln\\left(\\varepsilon\\right)\\right)\\cdot\\left(\\varepsilon-\\varepsilon^{2}\\right)=\\varepsilon\\left(1-\\varepsilon\\right)\\ln\\left(\\varepsilon\\right)-\\varepsilon\\left(1-\\varepsilon\\right)\\ln\\left(1-\\varepsilon\\right)}\\\\ &{\\qquad=\\varepsilon\\ln\\left(\\varepsilon\\right)-\\varepsilon\\left(\\varepsilon\\ln\\left(\\varepsilon\\right)+\\left(1-\\varepsilon\\right)\\ln\\left(1-\\varepsilon\\right)\\right)}\\\\ &{\\qquad=\\varepsilon\\ln2\\left(\\log\\left(\\varepsilon\\right)-\\left(\\varepsilon\\log\\left(\\varepsilon\\right)+\\left(1-\\varepsilon\\right)\\log\\left(1-\\varepsilon\\right)\\right)\\right)}\\\\ &{\\qquad=\\varepsilon\\ln2\\left(\\log\\left(\\varepsilon\\right)+H\\left(\\varepsilon\\right)\\right)=\\ln2\\left(\\varepsilon\\log\\left(\\varepsilon\\right)+\\varepsilon H\\left(\\varepsilon\\right)\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The inequality then holds due to convexity. ", "page_idx": 18}, {"type": "text", "text": "Finally, for completeness, we derive the relationship between the generalization error with respect to the noisy distribution $\\mathcal{L}_{\\mathcal{D}}\\left(h\\right)$ , and the generalization error with respect to the clean distribution $\\mathcal{L}_{\\mathcal{D}_{0}}\\left(h\\right)$ . ", "page_idx": 18}, {"type": "text", "text": "Lemma A.9. Let $\\mathcal{D}$ be a distribution as in Section 2.2, with independent noise with label flipping probability $\\varepsilon^{\\star}\\in\\left(0,\\frac{1}{2}\\right)$ . Let $\\mathcal{D}_{0}$ be the clean distribution, i.e., the distribution with label flipping probability 0. If ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{\\mathcal{D}}\\left(h\\right)=\\mathbb{P}_{\\left(X,Y\\right)\\sim{\\mathcal{D}}}\\left(h\\left(X\\right)\\neq Y\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathcal{D}_{0}}\\left(h\\right)=\\mathbb{P}_{\\left(X,Y\\right)\\sim\\mathcal{D}_{0}}\\left(h\\left(X\\right)\\neq Y\\right)=\\mathbb{P}_{\\left(X\\right)\\sim\\mathcal{D}}\\left(h\\left(X\\right)\\neq h^{\\star}\\left(X\\right)\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathcal{D}_{0}}\\left(h\\right)=\\frac{\\mathcal{L}_{\\mathcal{D}}\\left(h\\right)-\\varepsilon^{\\star}}{1-2\\varepsilon^{\\star}}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. By definition, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathcal{D}}\\left(h\\right)=\\mathbb{P}_{\\left(X,Y\\right)\\sim\\mathcal{D}}\\left(h\\left(X\\right)\\neq Y\\right)}\\\\ &{\\qquad\\qquad=\\mathbb{P}_{\\left(X,Y\\right)\\sim\\mathcal{D}}\\left(h\\left(X\\right)\\neq Y\\mid h^{\\star}\\left(X\\right)\\oplus Y=0\\right)\\mathbb{P}_{\\left(X,Y\\right)\\sim\\mathcal{D}}\\left(h^{\\star}\\left(X\\right)\\oplus Y=0\\right)}\\\\ &{\\qquad\\qquad+\\left.\\mathbb{P}_{\\left(X,Y\\right)\\sim\\mathcal{D}}\\left(h\\left(X\\right)\\neq Y\\mid h^{\\star}\\left(X\\right)\\oplus Y=1\\right)\\mathbb{P}_{\\left(X,Y\\right)\\sim\\mathcal{D}}\\left(h^{\\star}\\left(X\\right)\\oplus Y=1\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since we assume that the noise is independent, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathcal{D}}\\left(h\\right)=\\left(1-\\varepsilon^{\\star}\\right)\\mathbb{P}_{\\left(X,Y\\right)\\sim\\mathcal{D}}\\left(h\\left(X\\right)\\neq Y\\mid h^{\\star}\\left(X\\right)\\oplus Y=0\\right)}\\\\ &{\\quad\\quad\\quad+\\varepsilon^{\\star}\\mathbb{P}_{\\left(X,Y\\right)\\sim\\mathcal{D}}\\left(h\\left(X\\right)\\neq Y\\mid h^{\\star}\\left(X\\right)\\oplus Y=1\\right)}\\\\ &{\\quad\\quad\\quad=\\left(1-\\varepsilon^{\\star}\\right)\\mathbb{P}_{\\left(X,Y\\right)\\sim\\mathcal{D}}\\left(h\\left(X\\right)\\neq h^{\\star}\\left(X\\right)\\right)+\\varepsilon^{\\star}\\mathbb{P}_{\\left(X,Y\\right)\\sim\\mathcal{D}}\\left(h\\left(X\\right)=h^{\\star}\\left(X\\right)\\right)}\\\\ &{\\quad\\quad=\\left(1-\\varepsilon^{\\star}\\right)\\mathcal{L}_{\\mathcal{D}_{0}}\\left(h\\right)+\\varepsilon^{\\star}\\left(1-\\mathcal{L}_{\\mathcal{D}_{0}}\\left(h\\right)\\right)}\\\\ &{\\quad\\quad=\\varepsilon^{\\star}+\\left(1-2\\varepsilon^{\\star}\\right)\\mathcal{L}_{\\mathcal{D}_{0}}\\left(h\\right)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "or equivalently, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathcal{D}_{0}}\\left(h\\right)=\\frac{\\mathcal{L}_{\\mathcal{D}}\\left(h\\right)-\\varepsilon^{\\star}}{1-2\\varepsilon^{\\star}}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Remark A.10. In particular, under the assumptions of the lemma, $\\mathcal{L}_{\\mathcal{D}}\\left(h\\right)=2\\varepsilon^{\\star}\\left(1-\\varepsilon^{\\star}\\right)$ is equivalent to $\\mathcal{L}_{\\mathcal{D}_{0}}\\left(h\\right)=\\varepsilon^{\\star}$ . ", "page_idx": 18}, {"type": "text", "text": "B Data consistency ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Before moving on to generalization, we address some key properties of the training set\u2019s consistency.   \nLemma B.1. For any distribution over the data $\\begin{array}{r}{D,\\mathbb{P}\\left(i n c o n s i s t e n t\\,S\\right)\\le\\frac{1}{2}N^{2}\\mathcal{D}_{\\operatorname*{max}}}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Using the union bound, ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{^{\\mathrm{9}}{\\bigl(}{\\mathrm{inconsistent}}\\,S{\\bigr)}=\\mathbb{P}\\,(\\exists i\\neq j\\in[N]\\,:\\,X_{i}=X_{j},Y_{i}\\neq Y_{j}{\\bigr)}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathbb{P}\\,(\\exists i\\neq j\\in[N]\\,:\\,X_{i}=X_{j})}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\sum_{i\\neq j}\\mathbb{P}\\,(X_{i}=X_{j})={\\binom{N}{2}}\\mathbb{P}\\,(X_{1}\\!=\\!X_{2})={\\binom{N}{2}}\\sum_{x\\in X}\\mathbb{P}\\,(X_{1}=x)\\,\\mathbb{P}\\,(X_{2}=x)}\\\\ &{\\qquad\\qquad\\qquad\\leq{\\binom{N}{2}}\\sum_{x\\in X}{\\mathcal{D}}_{\\mathrm{max}}\\mathbb{P}\\,(X=x)={\\binom{N}{2}}{\\mathcal{D}}_{\\mathrm{max}}\\leq{\\frac{1}{2}}N^{2}{\\mathcal{D}}_{\\mathrm{max}}\\,.}\\end{array}}\n$$$\\mathbb{P}$ ", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, under Assumption A.2 we have $\\mathbb{P}$ (inconsistent $S)=o\\left(1\\right)$ , i.e., the inconsistency probability is asymptotically small. ", "page_idx": 19}, {"type": "text", "text": "B.1 Independent label noise ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We now focus on the case of independent label noise, i.e., $Y\\oplus h^{\\star}\\left(X\\right)\\mid\\left\\{X=x\\right\\}\\sim\\operatorname{Ber}\\left(\\varepsilon^{\\star}\\right)$ for any $x\\in\\mathscr{X}$ . Recall the noise level ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\varepsilon^{\\star}=\\mathbb{P}_{(X,Y)\\sim\\mathcal{D}}\\left(Y\\neq h^{\\star}\\left(X\\right)\\right)=\\mathbb{P}_{S}\\left(Y_{1}\\neq h^{\\star}\\left(X_{1}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and we define the \u201ceffective\u201d noise level in a consistent training set ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{\\varepsilon}_{\\mathrm{tr}}\\triangleq\\mathbb{P}_{S}\\left(Y_{1}\\neq h^{\\star}\\left(X_{1}\\right)\\mid\\mathsf{c o n s i s t e n t}\\;S\\right)\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We relate $\\hat{\\varepsilon}_{\\mathrm{tr}}$ to $\\varepsilon^{\\star}$ in the following lemma. ", "page_idx": 19}, {"type": "text", "text": "Lemma B.2. In the independent noise setting, it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left|\\hat{\\varepsilon}_{t r}-\\varepsilon^{\\star}\\right|\\leq\\left|\\ln2\\left(\\varepsilon^{\\star}\\log\\left(\\varepsilon^{\\star}\\right)+\\varepsilon^{\\star}H\\left(\\varepsilon^{\\star}\\right)\\right)\\right|\\cdot\\left(N-1\\right)\\frac{\\mathcal{D}_{\\operatorname*{max}}}{\\mathbb{P}\\left(\\mathrm{consistent}\\,S\\right)}\\,,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and moreover, $\\hat{\\varepsilon}_{t r}\\le\\varepsilon^{\\star}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Conditioning on $S$ being consistent (having no label \u201ccollisions\u201d), all occurrences of $x$ in $S$ must have the same label so ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}_{S}\\left(Y_{1}\\neq h^{\\star}\\left(X_{1}\\right)\\big|\\left(X_{1},Y_{1}\\right)\\mathrm{appears~}k\\mathrm{\\times~in\\}S,\\mathrm{\\consistent\\}S\\right)=\\frac{\\varepsilon^{\\star k}}{\\varepsilon^{\\star k}+\\left(1-\\varepsilon^{\\star}\\right)^{k}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{s}_{\\mathrm{tr}}=\\mathbb{P}_{S}\\left(Y_{1}\\neq h^{\\star}\\left(X_{1}\\right)\\,|\\,\\mathrm{consistent}\\,S\\right)}\\\\ &{\\quad=\\displaystyle\\sum_{k=1}^{N}\\mathbb{P}_{S}\\left(Y_{1}\\neq h^{\\star}\\left(X_{1}\\right)\\,|\\left(X_{1},Y_{1}\\right)\\,\\mathrm{appears}\\,k\\,\\mathrm{times~in}\\,S,\\,\\mathrm{consistent}\\,S\\right)}\\\\ &{\\quad\\quad\\quad\\cdot\\mathbb{P}\\left(\\left(X_{1},Y_{1}\\right)\\,\\mathrm{appears}\\,k\\,\\mathrm{times~in}\\,S\\,|\\,\\mathrm{consistent}\\,S\\right)}\\\\ &{\\quad=\\displaystyle\\sum_{k=1}^{N}\\frac{\\varepsilon^{\\star k}}{\\varepsilon^{\\star k}+\\left(1-\\varepsilon^{\\star}\\right)^{k}}\\cdot\\mathbb{P}\\left(\\left(X_{1},Y_{1}\\right)\\,\\mathrm{appears}\\,k\\,\\mathrm{times~in}\\,S\\,|\\,\\,\\mathrm{consistent}\\,S\\right)}\\\\ &{\\quad\\quad\\le\\displaystyle\\sum_{k=1}^{N}\\frac{\\varepsilon^{\\star1}}{\\varepsilon^{\\star1}+\\left(1-\\varepsilon^{\\star}\\right)^{1}}\\cdot\\mathbb{P}\\left(\\left(X_{1},Y_{1}\\right)\\,\\mathrm{appears}\\,k\\,\\mathrm{times~in}\\,S\\,|\\,\\mathrm{consistent}\\,S\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "On the other hand, define ", "page_idx": 20}, {"type": "equation", "text": "$$\nK\\left(S\\right)\\triangleq\\left|\\left\\{i\\in\\left[N\\right]\\mid X_{i}=X_{1}\\right\\}\\right|=\\sum_{i=1}^{N}\\mathbb{I}\\left\\{X_{i}=X_{1}\\right\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{S}\\left[K\\left(S\\right)\\mid\\mathrm{consistent}\\;S\\right]=1+\\displaystyle\\sum_{i=2}^{N}\\mathbb{E}_{S}\\left[\\mathbb{I}\\left\\{X_{i}=X_{1}\\right\\}\\mid\\mathrm{consistent}\\;S\\right]\\quad}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=1+(N-1)\\,\\mathbb{P}_{S}\\left(X_{2}=X_{1}\\mid\\mathrm{consistent}\\;S\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Next, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(X_{2}=X_{1}\\ |\\ {\\mathrm{consistent}}\\,S\\right)={\\frac{\\mathbb{P}\\left(X_{2}=X_{1},{\\mathrm{consistent}}\\,S\\right)}{\\mathbb{P}\\left({\\mathrm{consistent}}\\,S\\right)}}\\leq{\\frac{\\mathbb{P}\\left(X_{1}=X_{2}\\right)}{\\mathbb{P}\\left({\\mathrm{consistent}}\\,S\\right)}}\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $d D\\left(x\\right)\\leq D_{\\mathrm{max}}$ for all $x\\in\\mathscr{X}$ , as in the proof of Lemma B.1 ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{S}\\left[K\\left(S\\right)\\mid{\\mathrm{consistent~}}S\\right]\\leq1+\\left(N-1\\right){\\frac{\\mathbb{P}\\left(X_{1}=X_{2}\\right)}{\\mathbb{P}\\left({\\mathrm{consistent~}}S\\right)}}\\leq1+\\left(N-1\\right){\\frac{\\mathcal{D}_{\\operatorname*{max}}}{\\mathbb{P}\\left({\\mathrm{consistent~}}S\\right)}}\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, using Lemma A.7 we get, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\hat{\\varepsilon}_{\\mathrm{tr}}=\\sum_{k=1}^{N}\\frac{\\varepsilon^{\\star k}}{\\varepsilon^{\\star k}+\\left(1-\\varepsilon^{\\star}\\right)^{k}}\\cdot\\mathbb{P}\\left(X_{1}\\ \\mathrm{appears}\\ k\\ \\mathrm{times}\\ \\mathrm{in}\\ S\\ |\\ \\mathrm{consistent}\\ S\\right)}}\\\\ {\\displaystyle{\\qquad=\\sum_{k=1}^{N}\\phi_{\\varepsilon^{\\star}}\\left(k\\right)\\cdot\\mathbb{P}\\left(X_{1}\\ \\mathrm{appears}\\ k\\ \\mathrm{times}\\ \\mathrm{in}\\ S\\ |\\ \\mathrm{consistent}\\ S\\right)}}\\\\ {\\displaystyle{\\qquad=\\mathbb{E}_{S}\\left[\\frac{\\phi_{\\varepsilon^{\\star}}\\left(K\\left(S\\right)\\right)}{\\mathrm{concain}\\ k}\\ |\\ \\mathrm{consistent}\\ S\\right]}}\\\\ {\\left[\\mathrm{fonsen}\\right]\\ge\\phi_{\\varepsilon^{\\star}}\\left(\\mathbb{E}_{S}\\left[K\\left(S\\right)|\\ \\mathrm{consistent}\\ S\\right]\\right)}\\\\ {\\left[\\mathrm{decreasing}\\right]\\ge\\phi_{\\varepsilon^{\\star}}\\left(1+\\left(N-1\\right)\\cdot\\frac{\\mathcal{D}_{\\operatorname*{max}}}{\\mathbb{P}\\left(\\mathrm{consistent}\\ S\\right)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Corollary A.8 implies that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\varepsilon}_{\\mathrm{tr}}\\geq\\phi_{\\varepsilon^{\\star}}\\left(1+(N-1)\\cdot\\frac{\\mathcal{D}_{\\mathrm{max}}}{\\mathbb{P}\\left(\\mathrm{consistent}\\,S\\right)}\\right)}\\\\ &{\\quad\\geq\\varepsilon^{\\star}+\\ln2\\left(\\varepsilon^{\\star}\\log\\left(\\varepsilon^{\\star}\\right)+\\varepsilon^{\\star}H\\left(\\varepsilon^{\\star}\\right)\\right)\\cdot\\left(N-1\\right)\\frac{\\mathcal{D}_{\\mathrm{max}}}{\\mathbb{P}\\left(\\mathrm{consistent}\\,S\\right)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Combining the bounds we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\hat{\\varepsilon}_{\\mathrm{tr}}-\\varepsilon^{\\star}\\right|\\leq\\left|\\ln2\\left(\\varepsilon^{\\star}\\log\\left(\\varepsilon^{\\star}\\right)+\\varepsilon^{\\star}H\\left(\\varepsilon^{\\star}\\right)\\right)\\right|\\cdot\\left(N-1\\right)\\frac{\\mathcal{D}_{\\operatorname*{max}}}{\\mathbb{P}\\left(\\mathrm{consistent}\\,S\\right)}\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "C Generalization bounds ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We present two generalization bounds for the population error of an interpolating algorithm in terms of the mutual information of it with the training set. ", "page_idx": 21}, {"type": "text", "text": "Remark C.1 (High consistency probability). Throughout the appendix we assume that the consistency satisfies $\\mathbb{P}_{S}$ (consistent $S)\\geq{\\frac{1}{2}}$ . While this assumption is not without loss of generality, it is a weaker version of Assumption A.2 and implied by it (asymptotically). As Assumption A.2 is assumed in all \u201cdownstream results\u201d that this appendix aims to support, we find it is reasonable to assume here. ", "page_idx": 21}, {"type": "text", "text": "C.1 Arbitrary label noise ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this subsection, we provide a generalization bound for interpolating algorithms without any assumptions on the distribution of the noise $Y\\oplus h^{\\star}\\left(X\\right)\\mid\\{X={\\bar{x}}\\}$ , other than $\\mathcal{L}_{\\mathcal{D}}\\left(h^{\\star}\\right)=\\varepsilon^{\\star}$ . ", "page_idx": 21}, {"type": "text", "text": "Lemma C.2. For any interpolating learning algorithm $A\\left(S\\right)$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n-\\log\\left(1-\\mathbb{E}_{S,A\\left(S\\right)}\\left[{\\mathcal{L}}_{{\\mathcal{D}}}\\left(A\\left(S\\right)\\right)\\mid{\\mathrm{consistent}}\\,S\\right]\\right)\\leq{\\frac{I\\left(S;A\\left(S\\right)\\right)}{N\\cdot\\mathbb{P}_{S}\\left({\\mathrm{consistent}}\\,S\\right)}}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. We rely on Lemma A.4. Specifically, we shall use the following suggested conditional distribution. For $h\\neq\\star$ let ", "page_idx": 21}, {"type": "equation", "text": "$$\nd q\\left(s|h\\right)=\\frac{1}{Z_{h}}\\mathbb{I}\\left\\{\\mathcal{L}_{s}\\left(h\\right)=0\\right\\}d\\mathcal{D}^{N}\\left(s\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle Z_{h}=\\sum_{s}\\mathbb{I}\\left\\{\\mathcal{L}_{s}\\left(h\\right)=0\\right\\}d\\mathcal{D}^{N}\\left(s\\right)}}\\\\ {{\\displaystyle\\quad=\\mathbb{E}_{S}\\mathbb{I}\\left\\{\\mathcal{L}_{S}\\left(h\\right)=0\\right\\}}}\\\\ {{\\displaystyle\\quad=\\mathbb{P}_{S}\\left(\\mathcal{L}_{S}\\left(h\\right)=0\\right)}}\\\\ {{\\displaystyle=\\left(1-\\mathcal{L}_{\\mathcal{D}}\\left(h\\right)\\right)^{N}\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For $h=\\star$ let ", "page_idx": 21}, {"type": "equation", "text": "$$\nd q\\left(s\\right)\\kappa\\right)=\\frac{\\mathbb{I}\\left\\{\\mathrm{inconsistent}\\;s\\right\\}d\\mathcal{D}^{N}\\left(s\\right)}{\\sum_{s^{\\prime}}\\mathbb{I}\\left\\{\\mathrm{inconsistent}\\;s^{\\prime}\\right\\}d\\mathcal{D}^{N}\\left(s^{\\prime}\\right)}=\\mathbb{I}\\left\\{\\mathrm{inconsistent}\\;s\\right\\}\\frac{d\\mathcal{D}^{N}\\left(s\\right)}{\\mathbb{P}_{S}\\left(\\mathrm{inconsistent}\\;S\\right)}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Clearly, if $h\\neq\\star$ and $l q\\left(s|h\\right)=0$ then either $d\\mathcal{D}^{N}\\left(s\\right)=0$ so $d p\\left(s,h\\right)=0$ as well, or $\\mathcal{L}_{s}\\left(h\\right)\\neq0$ So, since $h\\ \\neq\\ \\star$ , $s$ can be interpolated and $d p\\left(s\\mid h\\right)\\;=\\;0$ . That is, the proposed conditional distribution is absolutely continuous w.r.t. the true conditional distribution. When $h=\\star$ , $q$ is the true conditional distribution given that $h=\\star{\\bf{s o}}$ it is also absolutely continuous w.r.t. it. That is, the proposed distribution is ", "page_idx": 21}, {"type": "equation", "text": "$$\nd q\\left(s\\mid h\\right)={\\frac{{\\mathbb{I}}\\left\\{{\\mathrm{inconsistent~}}s\\right\\}}{\\mathbb{P}_{S}\\left({\\mathrm{inconsistent~}}S\\right)}}\\mathbb{I}\\left\\{h=\\star\\right\\}d{\\mathcal{D}}^{N}\\left(s\\right)+{\\frac{{\\mathbb{I}}\\left\\{{\\mathcal{L}}_{s}\\left(h\\right)=0\\right\\}}{\\left(1-{\\mathcal{L}}_{D}\\left(h\\right)\\right)^{N}}}\\mathbb{I}\\left\\{h\\neq\\star\\right\\}d{\\mathcal{D}}^{N}\\left(s\\right)\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "From Lemma A.4 ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I\\left(S;A\\left(S\\right)\\right)\\geq\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\log\\left(\\frac{d q\\left(S\\middle|A\\left(S\\right)\\right)}{d\\mathcal{D}^{N}\\left(S\\right)}\\right)\\right]}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\log\\left(\\frac{\\mathbb{I}\\{\\mathrm{inconsistent}\\;S\\}}{\\mathbb{P}_{S^{\\prime}}\\left(\\mathrm{inconsistent}\\;S^{\\prime}\\right)}\\mathbb{I}\\left\\{A\\left(S\\right)=\\star\\right\\}+\\frac{\\mathbb{I}\\{C_{S}\\left(A\\left(S\\right)\\right)=0\\}}{\\left(1-C_{D}\\left(A\\left(S\\right)\\right)\\right)^{N}}\\mathbb{I}\\left\\{A\\left(S\\right)\\neq\\star\\right\\}\\right)\\right]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "$\\mathbb{I}\\left\\{A\\left(S\\right)=\\star\\right\\}$ and $\\mathbb{I}\\left\\{A\\left(S\\right)\\neq\\star\\right\\}$ are mutually exclusive so ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I\\left(S;A\\left(S\\right)\\right)}\\\\ &{\\quad\\ge\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\log\\left(\\frac{\\mathbb{I}\\left\\{\\operatorname{inconsistent}S\\right\\}}{\\mathbb{P}_{S^{\\prime}}\\left(\\operatorname{inconsistent}S^{\\prime}\\right)}\\right)\\mathbb{I}\\left\\{A\\left(S\\right)=\\star\\right\\}+\\log\\left(\\frac{\\mathbb{I}\\left\\{\\mathcal{L}_{S}\\left(A\\left(S\\right)\\right)=0\\right\\}}{\\left(1-\\mathcal{L}_{D}\\left(A\\left(S\\right)\\right)\\right)^{N}}\\right)\\mathbb{I}\\left\\{A\\left(S\\right)\\ne\\star\\right\\}\\right]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The first term is 0 when $A\\left(S\\right)\\;\\neq\\;\\star$ and positive when $A\\left(S\\right)\\;=\\;\\star$ (and so always non-negative). Furthermore, whenever $d p\\left(S,A\\left(S\\right)\\right)>0$ and $\\mathbb{I}\\left\\{A\\left(S\\right)\\neq\\star\\right\\}=1$ hold together, they imply that ", "page_idx": 21}, {"type": "text", "text": "$\\mathbb{I}\\left\\{\\mathcal{L}_{S}\\left(A\\left(S\\right)\\right)=0\\right\\}=1$ so we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I\\left(S;A\\left(S\\right)\\right)\\geq\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\log\\left(\\frac{1}{\\left(1-\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\right)^{N}}\\right)\\mathbb{I}\\left\\{A\\left(S\\right)\\neq\\star\\right\\}\\right]}\\\\ &{\\quad\\quad\\quad\\quad=-\\mathbb{E}_{S,A\\left(S\\right)}\\left[N\\log\\left(1-\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\right)\\mathbb{I}\\left\\{A\\left(S\\right)\\neq\\star\\right\\}\\right]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using Jensen\u2019s inequality, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\mathbb{E}_{S,A\\left(S\\right)}\\left[N\\log\\left(1-\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\right){\\mathbb{I}\\left\\{A\\left(S\\right)\\neq\\star\\right\\}}\\right]}\\\\ &{\\qquad\\qquad=-N\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\log\\left(1-\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\right)\\mid{\\mathbb{I}\\left\\{A\\left(S\\right)\\neq\\star\\right\\}}\\right]\\mathbb{P}_{S,A\\left(S\\right)}\\left(A\\left(S\\right)\\neq\\star\\right)}\\\\ &{\\qquad\\qquad\\geq-N\\log\\left(1-\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\mid{\\mathbb{I}\\left\\{A\\left(S\\right)\\neq\\star\\right\\}}\\right]\\right)\\mathbb{P}_{S,A\\left(S\\right)}\\left(A\\left(S\\right)\\neq\\star\\right)}\\\\ &{\\qquad\\qquad=-N\\log\\left(1-\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\mid\\mathrm{consistent}\\;S\\right]\\right)\\mathbb{P}_{S}\\left(\\mathrm{consistent}\\;S\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "so ", "page_idx": 22}, {"type": "text", "text": "$I\\left(S;A\\left(S\\right)\\right)\\ge-{\\,N\\log\\left({1-\\mathbb{E}_{S,A\\left(S\\right)}\\left[{\\mathcal{L}_{D}\\left(A\\left(S\\right)\\right)\\mid\\mathrm{{consistent}}\\,S}\\right]}\\right)\\mathbb{P}_{S}\\left({\\mathrm{consistent}}\\,S\\right)\\,.$ Rearranging the inequality ", "page_idx": 22}, {"type": "equation", "text": "$$\n-\\log\\left(1-\\mathbb{E}_{S,A\\left(S\\right)}\\left[{\\mathcal{L}}_{{\\mathcal{D}}}\\left(A\\left(S\\right)\\right)\\mid{\\mathrm{consistent}}\\,S\\right]\\right)\\leq{\\frac{I\\left(S;A\\left(S\\right)\\right)}{N\\cdot\\mathbb{P}_{S}\\left({\\mathrm{consistent}}\\,S\\right)}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "C.2 Independent Noise ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Lemma C.3. Assuming independent noise, the generalization error of interpolating learning rules satisfies the following. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{E}_{S,A(S)}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\mid\\mathrm{consistent}\\,S\\right]-2\\varepsilon^{\\star}\\left(1-\\varepsilon^{\\star}\\right)\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\le\\left(1-2\\varepsilon^{\\star}\\right)\\sqrt{C\\left(N\\right)}+\\frac{\\left(N-1\\right)\\mathcal{D}_{\\operatorname*{max}}}{3}\\,,}\\\\ &{\\,\\,\\gamma\\left(N\\right)\\triangleq\\frac{I\\left(S;A(S)\\right)-N\\cdot\\left(H\\left(\\varepsilon^{\\star}\\right)-\\mathbb{P}(\\mathrm{inconsistent}\\,S)\\right)}{N\\left(1-\\mathbb{P}(\\mathrm{inconsistent}\\,S)\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. As in the proof of Lemma 4.2 in Manoj and Srebro [58], since $S$ is sampled i.i.d., we have ", "page_idx": 23}, {"type": "equation", "text": "$$\nI\\left(S;A\\left(S\\right)\\right)\\stackrel{A.5}{\\geq}\\sum_{i=1}^{N}I\\left(\\left(X_{i},Y_{i}\\right);A\\left(S\\right)\\right)=N\\cdot I\\left(\\left(X_{1},Y_{1}\\right);A\\left(S\\right)\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Using properties of conditional mutual information, ", "page_idx": 23}, {"type": "equation", "text": "$$\nI\\left(Y_{1};A\\left(S\\right)\\mid X_{1}\\right)=H\\left(Y_{1}\\mid X_{1}\\right)-H\\left(Y_{1}\\mid A\\left(S\\right),X_{1}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For the first term in (7), we employ the fact that for any $x\\in\\mathscr{X}$ , either $Y_{1}\\mid X_{1}=x\\sim\\operatorname{Ber}\\left(\\varepsilon^{\\star}\\right)$ or $Y_{1}\\mid X_{1}=x\\sim\\operatorname{Ber}\\left(1-\\varepsilon^{\\star}\\right)$ to get ", "page_idx": 23}, {"type": "equation", "text": "$$\nH\\left(Y_{1}\\mid X_{1}\\right)=\\mathbb{E}_{(X,Y)\\sim{\\mathcal{D}}}\\left[H\\left(Y_{1}\\mid X_{1}=X\\right)\\right]=\\mathbb{E}_{(X,Y)\\sim{\\mathcal{D}}}\\left[H\\left({\\varepsilon^{\\star}}\\right)\\right]=H\\left({\\varepsilon^{\\star}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For the second term in (7), we again employ the definition of conditional entropy, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{H\\left(Y_{1}\\mid A\\left(S\\right),X_{1}\\right)=-\\displaystyle\\sum_{x_{1}\\in\\mathcal{X}}\\sum_{h\\in\\mathcal{H}\\cup\\left\\{\\star\\right\\}}\\left[d p\\left(\\left(x_{1},0\\right),h\\right)\\log\\left(\\frac{d p\\left(\\left(x_{1},0\\right),h\\right)}{d p\\left(x_{1},h\\right)}\\right)\\right.\\qquad}\\\\ {\\displaystyle\\left.+\\,d p\\left(\\left(x_{1},1\\right),h\\right)\\log\\left(\\frac{d p\\left(\\left(x_{1},1\\right),h\\right)}{d p\\left(x_{1},h\\right)}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "When $h\\neq\\star$ , the marginal distribution of a training data point and a hypothesis is ", "page_idx": 23}, {"type": "equation", "text": "$$\nd p\\left(\\left(x,y\\right),h\\right)=d p\\left(y\\mid x,h\\right)d p\\left(x,h\\right)=\\mathbb{I}\\left\\{y=h\\left(x\\right)\\right\\}d p\\left(x,h\\right),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and the inner sum becomes a sum over expressions of the form: ", "page_idx": 23}, {"type": "equation", "text": "$$\nd p\\left(\\left(x_{1},h\\left(x_{1}\\right)\\right),h\\right)\\log\\left(\\frac{d p\\left(\\left(x_{1},h\\left(x_{1}\\right)\\right),h\\right)}{d p\\left(x_{1},h\\right)}\\right)=d p\\left(x_{1},h\\right)\\underbrace{\\log\\left(\\frac{d p\\left(x_{1},h\\right)}{d p\\left(x_{1},h\\right)}\\right)}_{=0}=0\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, we have that, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{H\\left(Y_{1}\\mid A\\left(S\\right),X_{1}\\right)}\\\\ {\\quad=-\\displaystyle\\sum_{x_{1}\\in\\mathcal{X}}\\left[d p\\left(\\left(x_{1},0\\right),\\star\\right)\\log\\left(\\frac{d p\\left(\\left(x_{1},0\\right),\\star\\right)}{d p\\left(x_{1},\\star\\right)}\\right)+d p\\left(\\left(x_{1},1\\right),\\star\\right)\\log\\left(\\frac{d p\\left(\\left(x_{1},1\\right),\\star\\right)}{d p\\left(x_{1},\\star\\right)}\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Employing conditional probabilities (notice that $\\begin{array}{r}{\\frac{d p\\left(\\left(x_{1},0\\right),\\star\\right)}{d p\\left(x_{1},\\star\\right)}=\\frac{d p\\left(0\\left|x_{1},\\star\\right)d p\\left(x_{1},\\star\\right)}{d p\\left(x_{1},\\star\\right)}=d p\\left(0\\mid x_{1},\\star\\right)),}\\end{array}$ we get, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H\\left(Y_{1}\\mid A\\left(S\\right),X_{1}\\right)}\\\\ &{\\;=\\;-\\displaystyle\\sum_{x_{1}\\in\\mathcal{X}}d p\\left(x_{1},\\star\\right)\\left[d p\\left(0\\mid x_{1},\\star\\right)\\log\\left(d p\\left(0\\mid x_{1},\\star\\right)\\right)+d p\\left(1\\mid x_{1},\\star\\right)\\log\\left(d p\\left(1\\mid x_{1},\\star\\right)\\right)\\right]}\\\\ &{\\;=\\displaystyle\\sum_{x_{1}\\in\\mathcal{X}}d p\\left(x_{1},\\star\\right)H\\left(d p\\left(0\\mid x_{1},\\star\\right)\\right)=d p\\left(\\star\\right)\\sum_{x_{1}\\in\\mathcal{X}}d p\\left(x_{1}\\mid\\star\\right)H\\left(d p\\left(0\\mid x_{1},\\star\\right)\\right)}\\\\ &{\\;=\\mathbb{P}\\left(A\\left(S\\right)=\\star\\right)\\mathbb{E}_{\\left(S,A\\left(S\\right)\\right)\\sim p}\\left[\\underbrace{H\\left(d p\\left(0\\mid X_{1},\\star\\right)\\right)}_{\\leq1}\\mid\\mathrm{inconsistent}\\;S\\right]\\leq\\mathbb{P}_{S}\\left(\\mathrm{inconsistent}\\;S\\right)\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Overall, the right term in (6) is lower bounded by, ", "page_idx": 24}, {"type": "equation", "text": "$$\nI\\left(Y_{1};A\\left(S\\right)\\mid X_{1}\\right)=H\\left(Y_{1}\\mid X_{1}\\right)-H\\left(Y_{1}\\mid A\\left(S\\right),X_{1}\\right)\\geq H\\left(\\varepsilon^{\\star}\\right)-{\\mathbb P}\\left({\\mathrm{inconsistent}}\\;S\\right)\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For the left term, i.e., $I\\left(X_{1};A\\left(S\\right)\\right)$ , we use the variational bound Lemma A.4 with the following suggested conditional distribution. ", "page_idx": 24}, {"type": "text", "text": "\u2022 For $h=\\star$ , choose $d q\\left(x_{1}\\mid\\star\\right)=d p\\left(x_{1}\\right)$ (notice that $\\begin{array}{r}{\\sum_{x_{1}\\in\\mathcal{X}}d q\\left(x_{1}\\mid\\star\\right)=\\sum_{x_{1}}d p\\left(x_{1}\\right)=1.}\\end{array}$ ", "page_idx": 24}, {"type": "text", "text": "\u2022 Otherwise, if $h\\neq\\star.$ , denote $q_{\\varepsilon}=\\mathrm{Ber}\\left(\\varepsilon\\right)$ , and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{~}\\hat{\\varepsilon}_{\\mathrm{tr}}=\\mathbb{P}_{S}\\left(Y_{1}\\neq h^{\\star}\\left(X_{1}\\right)\\mid\\sf{c o n s i s t e n t}\\;S\\right)}\\\\ &{\\hat{\\varepsilon}_{\\mathrm{gen}}=\\mathbb{E}_{(S,A(S))\\sim p}\\left[\\mathbb{P}_{X\\sim\\mathcal{D}}\\left(A\\left(S\\right)(X)\\neq h^{\\star}\\left(X\\right)\\right)\\mid A\\left(S\\right)\\neq\\star\\right]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that $\\hat{\\varepsilon}_{\\mathrm{tr}}$ may differ from $\\varepsilon^{\\star}$ . We choose the following conditional distribution ", "page_idx": 24}, {"type": "equation", "text": "$$\nd q\\left(x_{1}\\mid h\\right)=\\frac{1}{Z_{h}}\\cdot\\frac{d q_{\\hat{\\varepsilon}_{\\mathrm{tr}}}\\left(h\\left(x_{1}\\right)\\oplus h^{\\star}\\left(x_{1}\\right)\\right)}{d q_{\\hat{\\varepsilon}_{\\mathrm{gen}}}\\left(h\\left(x_{1}\\right)\\oplus h^{\\star}\\left(x_{1}\\right)\\right)}d p\\left(x_{1}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In total, we choose, ", "page_idx": 24}, {"type": "equation", "text": "$$\nd q\\left(x_{1}\\mid h\\right)=\\frac{1}{Z_{h}}\\cdot\\frac{d q_{\\hat{\\varepsilon}_{\\mathrm{t}}}\\left(h\\left(x_{1}\\right)\\oplus h^{\\star}\\left(x_{1}\\right)\\right)}{d q_{\\hat{\\varepsilon}_{\\mathrm{gen}}}\\left(h\\left(x_{1}\\right)\\oplus h^{\\star}\\left(x_{1}\\right)\\right)}\\cdot\\mathbb{I}\\left\\{h\\neq\\star\\right\\}d p\\left(x_{1}\\right)+\\mathbb{I}\\left\\{h=\\star\\right\\}d p\\left(x_{1}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $Z_{h}$ is the corresponding partition function. ", "page_idx": 24}, {"type": "text", "text": "Then, we use (A.4) and properties of logarithms and indicators to show that, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell\\left(X_{1};A\\left(S\\right)\\right)\\geq\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\log\\left(\\frac{d q\\left(X_{1}\\vert A\\left(S\\right)\\right)}{d p\\left(X_{1}\\right)}\\right)\\right]}\\\\ &{=\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\log\\left(\\frac{\\sum_{k=0}^{1}\\frac{d q_{k}\\left(N_{1}\\right)\\sin^{\\ast}\\left(X_{1}\\right)}d p\\left(X_{1}\\right)\\right)\\mathbb{I}\\left\\{A\\left(S\\right)\\neq\\star\\right\\}+d p\\left(X_{1}\\right)\\mathbb{I}\\left\\{A\\left(S\\right)=\\star\\right\\}\\right)}{d p\\left(X_{1}\\right)}\\right]}\\\\ &{=\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\log\\left(\\frac{1}{Z_{4\\left(S\\right)}}\\frac{d q_{k}\\left(N_{1}\\right)\\oplus\\hslash\\left(X_{1}\\right)}{d q_{\\infty}\\left(h\\left(X_{1}\\right)\\right)\\oplus\\hslash\\left(X_{1}\\right)}\\right)\\left\\{A\\left(S\\right)\\neq\\star\\right\\}+\\left\\{\\left[A\\left(S\\right)=\\star\\right\\}\\right)\\right]}\\\\ &{=\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\log\\left(\\frac{1}{Z_{4\\left(S\\right)}}\\frac{d q_{k}\\left(N_{1}\\right)\\oplus\\hslash\\left(X_{1}\\right)}{d q_{\\infty}\\left(h\\left(X_{1}\\right)\\right)\\oplus\\hslash\\left(X_{1}\\right)}\\right)\\right]\\left\\{A\\left(S\\right)\\neq\\star\\right\\}\\right]+}\\\\ &{\\ \\ \\ \\mathbb{E}_{S,A\\left(S\\right)}\\left[\\log\\left(\\frac{1}{Z_{4\\left(S\\right)}}\\frac{d q_{k}\\left(N_{1}\\right)\\oplus\\hslash\\left(X_{1}\\right)}{d q_{\\infty}\\left(h\\left(X_{1}\\right)\\right)\\oplus\\hslash\\left(X_{1}\\right)}\\right)\\right]\\left\\{A\\left(S\\right)\\neq\\star\\right\\}\\right]+}\\\\ &{=\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\log\\left(\\frac{1}{Z_{4\\left(S\\right)}}\\log\\left(N_{1}\\right)\\oplus\\hslash\\left(X_{1}\\right)\\right)\\right]}\\\\ &{=\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\log\\left(\\frac{1}{Z_{4\\left(S\\right)}}\\frac{d q_{k}\\left(N_{1}\\right)\\oplus\\hslash\\left(X_{1}\\right)}{d q_{\\infty}\\left(h\\left(X_{1}\\right)\\right)\\oplus\\hslash\\left(X_{1}\\right)}\\right)\\right\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Using the law of total expectation, the above becomes, ", "page_idx": 24}, {"type": "equation", "text": "$$\n=\\mathbb{P}\\left(A\\left(S\\right)\\neq\\star\\right)\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\log\\left(\\frac{1}{Z_{A\\left(S\\right)}}\\cdot\\frac{d q_{\\delta_{\\mathrm{tr}}}\\left(h\\left(X_{1}\\right)\\oplus h^{\\star}\\left(X_{1}\\right)\\right)}{d q_{\\delta_{\\mathrm{gen}}}\\left(h\\left(X_{1}\\right)\\oplus h^{\\star}\\left(X_{1}\\right)\\right)}\\right)\\biggm|A\\left(S\\right)\\neq\\star\\right]\\,,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we also use Jensen\u2019s inequality to show, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\log\\left(\\frac{1}{Z_{A\\left(S\\right)}}\\cdot\\frac{d q_{\\widehat{\\varepsilon}_{\\mathrm{te}}}\\left(h\\left(X_{1}\\right)\\oplus h^{\\star}\\left(X_{1}\\right)\\right)}{d q_{\\widehat{\\varepsilon}_{\\mathrm{ge}}}\\left(h\\left(X_{1}\\right)\\oplus h^{\\star}\\left(X_{1}\\right)\\right)}\\right)\\biggm|A\\left(S\\right)\\neq\\star\\right]}\\\\ &{=\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\log\\left(\\frac{d q_{\\widehat{\\varepsilon}_{\\mathrm{te}}}\\left(h\\left(X_{1}\\right)\\oplus h^{\\star}\\left(X_{1}\\right)\\right)}{d q_{\\widehat{\\varepsilon}_{\\mathrm{ge}}}\\left(h\\left(X_{1}\\right)\\oplus h^{\\star}\\left(X_{1}\\right)\\right)}\\right)\\biggm|A\\left(S\\right)\\neq\\star\\right]-\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\log\\left(Z_{A\\left(S\\right)}\\right)\\bigm|A\\left(S\\right)\\neq\\star\\right]}\\\\ &{\\geq\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\log\\left(\\frac{d q_{\\widehat{\\varepsilon}_{\\mathrm{te}}}\\left(h\\left(X_{1}\\right)\\oplus h^{\\star}\\left(X_{1}\\right)\\right)}{d q_{\\widehat{\\varepsilon}_{\\mathrm{ge}}}\\left(h\\left(X_{1}\\right)\\oplus h^{\\star}\\left(X_{1}\\right)\\right)}\\right)\\biggm|A\\left(S\\right)\\neq\\star\\right]-\\log\\left(\\mathbb{E}_{S,A\\left(S\\right)}\\left[Z_{A\\left(S\\right)}\\right|A\\left(S\\right)\\neq\\star\\right]\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The partition function satisfies for all $h\\neq\\star$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{Z_{h}=\\displaystyle\\sum_{x_{1}\\in\\mathcal{X}}\\frac{d q_{\\hat{\\varepsilon}_{\\mathrm{tr}}}\\left(h\\left(x_{1}\\right)\\oplus h^{\\star}\\left(x_{1}\\right)\\right)}{d q_{\\xi_{\\mathrm{gen}}}\\left(h\\left(x_{1}\\right)\\oplus h^{\\star}\\left(x_{1}\\right)\\right)}d p\\left(x_{1}\\right)=\\mathbb{E}_{X\\sim\\mathcal{D}}\\left[\\frac{d q_{\\hat{\\varepsilon}_{\\mathrm{tr}}}\\left(h\\left(X\\right)\\oplus h^{\\star}\\left(X\\right)\\right)}{d q_{\\hat{\\varepsilon}_{\\mathrm{gen}}}\\left(h\\left(X\\right)\\oplus h^{\\star}\\left(X\\right)\\right)}\\right]}}\\\\ {{=\\mathbb{P}_{X\\sim\\mathcal{D}}\\left(h\\left(X\\right)=h^{\\star}\\left(X\\right)\\right)\\cdot\\frac{1-\\hat{\\varepsilon}_{\\mathrm{tr}}}{1-\\hat{\\varepsilon}_{\\mathrm{gen}}}+\\mathbb{P}_{X\\sim\\mathcal{D}}\\left(h\\left(X\\right)\\neq h^{\\star}\\left(X\\right)\\right)\\cdot\\frac{\\hat{\\varepsilon}_{\\mathrm{tr}}}{\\hat{\\varepsilon}_{\\mathrm{gen}}}\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Taking the expectation w.r.t. $(S,A\\left(S\\right))\\sim p$ , we get ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\left(S,A\\left(S\\right)\\right)\\sim p}\\left[Z_{A\\left(S\\right)}\\mid A\\left(S\\right)\\neq\\star\\right]}\\\\ &{=\\frac{1-\\hat{\\varepsilon}_{\\mathtt{t e m}}}{1-\\hat{\\varepsilon}_{\\mathtt{g e m}}}\\cdot\\underbrace{\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\mathbb{P}\\left(A\\left(S\\right)\\left(X\\right)=h^{\\star}\\left(X\\right)\\right)\\mid A\\left(S\\right)\\neq\\star\\right]}_{=1-\\hat{\\varepsilon}_{\\mathtt{g e m}}}+}\\\\ &{\\phantom{=\\times\\underbrace{\\hat{\\varepsilon}_{\\mathtt{t e m}}}\\cdot\\underbrace{\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\mathbb{P}\\left(A\\left(S\\right)\\neq\\kappa\\right)+\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\mathbb{P}\\left(A\\left(S\\right)\\left(X\\right)\\neq h^{\\star}\\left(X\\right)\\right)\\mid A\\left(S\\right)\\neq\\star\\right]}_{=\\hat{\\varepsilon}_{\\mathtt{g e m}}}=1\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combining the above, we have that, ", "page_idx": 25}, {"type": "equation", "text": "$$\nI\\left(X_{1};A\\left(S\\right)\\right)\\geq\\mathbb{P}\\left(A\\left(S\\right)\\neq\\star\\right)\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\log\\left(\\frac{d q_{\\mathcal{E}_{\\pi}}\\left(A\\left(S\\right)\\left(X_{1}\\right)\\oplus h^{\\star}\\left(X_{1}\\right)\\right)}{d q_{\\mathcal{E}_{\\pi\\infty}}\\left(A\\left(S\\right)\\left(X_{1}\\right)\\oplus h^{\\star}\\left(X_{1}\\right)\\right)}\\right)\\mid A\\left(S\\right)\\neq\\star\\right].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Notice that ", "page_idx": 25}, {"type": "equation", "text": "$$\n(A\\left(S\\right)(X_{1})\\oplus h^{\\star}\\left(X_{1}\\right)\\mid\\{A\\left(S\\right)\\neq\\star\\})=(Y_{1}\\oplus h^{\\star}\\left(X_{1}\\right)\\mid\\{\\mathrm{consistent}\\,S\\})~,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "so $4\\left({\\cal S}\\right)\\left(X_{1}\\right)\\oplus h^{\\star}\\left(X_{1}\\right)|\\,\\{A\\left({\\cal S}\\right)\\neq\\star\\}\\sim\\mathrm{Ber}\\left(\\hat{\\varepsilon}_{\\mathrm{tr}}\\right)\\!,$ , and thus ", "page_idx": 25}, {"type": "equation", "text": "$$\nI\\left(X_{1};A\\left(S\\right)\\right)\\geq\\mathbb{P}\\left(A\\left(S\\right)\\neq\\star\\right)D_{K L}\\left(q_{\\varepsilon_{\\mathrm{t}}}||q_{\\varepsilon_{\\mathrm{gn}}}\\right)=\\left(1-\\mathbb{P}\\left({\\mathrm{inconsistent}}\\;S\\right)\\right)D_{K L}\\left(q_{\\varepsilon_{\\mathrm{t}}}||q_{\\varepsilon_{\\mathrm{gn}}}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Putting this all together, (6) is lower bounded by, ", "page_idx": 25}, {"type": "equation", "text": "$$\nI\\left(S;A\\left(S\\right)\\right)\\geq N{\\left(1-\\mathbb{P}\\left({\\mathrm{inconsistent}}\\,S\\right)\\right)}D_{K L}\\left(q_{\\varepsilon_{\\mathrm{w}}}||q_{\\varepsilon_{\\mathrm{gn}}}\\right)+N{\\left(H\\left(\\varepsilon^{\\star}\\right)-\\mathbb{P}\\left({\\mathrm{inconsistent}}\\,S\\right)\\right)}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Rearranging the inequality ", "page_idx": 25}, {"type": "equation", "text": "$$\nD_{K L}\\left(q_{\\hat{\\varepsilon}_{\\mathrm{t}}}||q_{\\hat{\\varepsilon}_{\\mathrm{gen}}}\\right)\\le\\frac{I\\left(S;A\\left(S\\right)\\right)-N\\cdot\\left(H\\left(\\varepsilon^{\\star}\\right)-\\mathbb{P}\\left(\\mathrm{inconsistent}\\;S\\right)\\right)}{N\\left(1-\\mathbb{P}\\left(\\mathrm{inconsistent}\\;S\\right)\\right)}\\triangleq C\\left(N\\right)\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Using Pinsker\u2019s inequality, we have, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left|\\hat{\\varepsilon}_{\\mathrm{tr}}-\\hat{\\varepsilon}_{\\mathrm{gen}}\\right|\\leq\\sqrt{\\frac{1}{2}D_{K L}\\left(q_{\\hat{\\varepsilon}_{\\mathrm{tr}}}||q_{\\hat{\\varepsilon}_{\\mathrm{gen}}}\\right)}\\leq\\sqrt{C\\left(N\\right)}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We proceed to bound $\\mathbb{E}_{S,A(S)}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\mid\\right.$ consistent $S]$ in terms of $\\lvert\\hat{\\varepsilon}_{\\mathrm{tr}}-\\hat{\\varepsilon}_{\\mathrm{gen}}\\rvert$ . Notice that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\mid\\mathrm{consistent}\\;S\\right]}\\\\ &{\\ =\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\mathbb{P}_{\\left(X,Y\\right)\\sim\\mathcal{D}}\\left(A\\left(S\\right)\\left(X\\right)\\neq Y\\right)\\mid\\mathrm{consistent}\\;S\\right]}\\\\ &{\\ =\\mathbb{E}_{S,A\\left(S\\right)}\\Big[\\mathbb{P}\\left(A\\left(S\\right)\\left(X\\right)\\neq Y\\mid Y=h^{\\star}\\left(X\\right)\\right)\\underbrace{\\mathbb{P}\\left(Y=h^{\\star}\\left(X\\right)\\right)}_{\\mathrm{~=~}\\mathcal{S}}\\Big|\\;\\mathrm{consistent}\\;S\\Big]+}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n+\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\mathbb{P}\\left(A\\left(S\\right)\\left(X\\right)\\neq Y\\mid Y\\neq h^{\\star}\\left(X\\right)\\right)\\underbrace{\\mathbb{P}\\left(Y\\neq h^{\\star}\\left(X\\right)\\right)}_{\\mathrm{~}}\\;\\Big|\\;{\\mathrm{consistent}}\\;S\\right]\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\mathbb{P}\\left(A\\left(S\\right)\\left(X\\right)\\neq h^{\\star}\\left(X\\right)\\right)\\left(1-\\varepsilon^{\\star}\\right)+\\mathbb{P}\\left(A\\left(S\\right)\\left(X\\right)=h^{\\star}\\left(X\\right)\\right)\\varepsilon^{\\star}\\;\\middle|\\;\\mathrm{consistent}\\;S\\right]}\\\\ &{=\\left(1-\\varepsilon^{\\star}\\right)\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\mathbb{P}\\left(A\\left(S\\right)\\left(X\\right)\\neq h^{\\star}\\left(X\\right)\\right)\\mid\\mathrm{consistent}\\;S\\right]+}\\\\ &{\\phantom{=}+\\varepsilon^{\\star}\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\mathbb{P}\\left(A\\left(S\\right)\\left(X\\right)=h^{\\star}\\left(X\\right)\\right)\\mid\\mathrm{consistent}\\;S\\right]}\\\\ &{=\\left(1-\\varepsilon^{\\star}\\right)\\hat{\\varepsilon}_{\\mathrm{gen}}+\\varepsilon^{\\star}\\left(1-\\hat{\\varepsilon}_{\\mathrm{gen}}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then, using the triangle inequality, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{E}_{S,A(S)}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\mid\\mathrm{consistent}\\;S\\right]-2\\varepsilon^{\\star}\\left(1-\\varepsilon^{\\star}\\right)\\right|}\\\\ &{=\\left|\\left(1-\\varepsilon^{\\star}\\right)\\hat{\\varepsilon}_{\\mathrm{gen}}+\\varepsilon^{\\star}\\left(1-\\hat{\\varepsilon}_{\\mathrm{gen}}\\right)-2\\varepsilon^{\\star}\\left(1-\\varepsilon^{\\star}\\right)\\right|=\\left|\\hat{\\varepsilon}_{\\mathrm{gen}}-\\hat{\\varepsilon}_{\\mathrm{gen}}\\varepsilon^{\\star}+\\varepsilon^{\\star}-\\hat{\\varepsilon}_{\\mathrm{gen}}\\varepsilon^{\\star}-2\\varepsilon^{\\star}+2\\varepsilon^{\\star2}\\right|}\\\\ &{=\\left|\\hat{\\varepsilon}_{\\mathrm{gen}}-2\\hat{\\varepsilon}_{\\mathrm{gen}}\\varepsilon^{\\star}-\\varepsilon^{\\star}+2\\varepsilon^{\\star2}\\right|=\\left|\\hat{\\varepsilon}_{\\mathrm{gen}}\\left(1-2\\varepsilon^{\\star}\\right)-\\varepsilon^{\\star}\\left(1-2\\varepsilon^{\\star}\\right)\\right|=\\left(1-2\\varepsilon^{\\star}\\right)\\left|\\hat{\\varepsilon}_{\\mathrm{gen}}-\\varepsilon^{\\star}\\right|}\\\\ &{\\leq\\left(1-2\\varepsilon^{\\star}\\right)\\left(\\left|\\hat{\\varepsilon}_{\\mathrm{gen}}-\\hat{\\varepsilon}_{\\mathrm{t}}\\right|+\\left|\\hat{\\varepsilon}_{\\mathrm{t}}-\\varepsilon^{\\star}\\right|\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Combining with the result from Lemma B.2 and Remark C.1 ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\hat{\\varepsilon}_{\\mathrm{tr}}-\\varepsilon^{\\star}\\right|\\leq\\left|\\ln2\\left(\\varepsilon^{\\star}\\log\\left(\\varepsilon^{\\star}\\right)+\\varepsilon^{\\star}H\\left(\\varepsilon^{\\star}\\right)\\right)\\right|\\cdot\\left(N-1\\right)\\frac{\\mathcal{D}_{\\mathrm{max}}}{\\mathbb{P}\\left(\\mathrm{consistent}\\,S\\right)}}\\\\ &{\\qquad\\qquad\\leq\\left|\\ln2\\left(\\varepsilon^{\\star}\\log\\left(\\varepsilon^{\\star}\\right)+\\varepsilon^{\\star}H\\left(\\varepsilon^{\\star}\\right)\\right)\\right|\\cdot2\\left(N-1\\right)\\mathcal{D}_{\\mathrm{max}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "we conclude that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\mid\\mathrm{consistent}\\;S\\right]-2\\varepsilon^{\\star}\\left(1-\\varepsilon^{\\star}\\right)\\right|}\\\\ &{\\leq\\left(1-2\\varepsilon^{\\star}\\right)\\left(\\sqrt{C\\left(N\\right)}+\\ln2\\left|\\left(\\varepsilon^{\\star}\\log\\left(\\varepsilon^{\\star}\\right)+\\varepsilon^{\\star}H\\left(\\varepsilon^{\\star}\\right)\\right)\\right|\\cdot2\\left(N-1\\right)\\mathcal{D}_{\\operatorname*{max}}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Finally, we can use the algebraic property that $\\begin{array}{r}{(1-2\\varepsilon^{\\star})\\left|\\ln2\\left(\\varepsilon^{\\star}\\log\\left(\\varepsilon^{\\star}\\right)+\\varepsilon^{\\star}H\\left(\\varepsilon^{\\star}\\right)\\right)\\right|\\ \\leq\\ \\frac{1}{6},}\\end{array}$ to get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\mid\\mathrm{consistent}\\;S\\right]-2\\varepsilon^{\\star}\\left(1-\\varepsilon^{\\star}\\right)\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\left(1-2\\varepsilon^{\\star}\\right)\\sqrt{C\\left(N\\right)}+\\frac{\\left(N-1\\right)\\mathcal{D}_{\\operatorname*{max}}}{3}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We can now bound the expected generalization error without conditioning on the consistency of the training set. ", "page_idx": 26}, {"type": "text", "text": "Lemma C.4. It holds that, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{E}_{S,A(S)}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\right]-2\\varepsilon^{\\star}\\left(1-\\varepsilon^{\\star}\\right)\\right|}\\\\ &{\\qquad\\qquad\\leq\\left|\\mathbb{E}_{S,A(S)}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\mid\\mathrm{consistent}\\;S\\right]-2\\varepsilon^{\\star}\\left(1-\\varepsilon^{\\star}\\right)\\right|+\\mathbb{P}_{S}\\left(\\mathrm{inconsistent}\\;S\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. Let $X$ be an arbitrary RV in $[0,1]$ and $Y$ be a binary RV. Then, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad\\mathbb{E}\\left[X\\right]=\\mathbb{P}\\left(Y\\right)\\mathbb{E}\\left[X\\middle|Y\\right]+\\mathbb{P}\\left(\\neg Y\\right)\\mathbb{E}\\left[X\\middle|\\neg Y\\right]}\\\\ &{\\mathbb{E}\\left[X\\right]-\\mathbb{E}\\left[X\\middle|Y\\right]=\\mathbb{P}\\left(Y\\right)\\mathbb{E}\\left[X\\middle|Y\\right]-\\mathbb{E}\\left[X\\middle|Y\\right]+\\mathbb{P}\\left(\\neg Y\\right)\\mathbb{E}\\left[X\\middle|\\neg Y\\right]}\\\\ &{=\\mathbb{E}\\left[X\\middle|Y\\right]\\left(\\mathbb{P}\\left(Y\\right)-1\\right)+\\mathbb{P}\\left(\\neg Y\\right)\\mathbb{E}\\left[X\\middle|\\neg Y\\right]}\\\\ &{=-\\mathbb{E}\\left[X\\middle|Y\\right]\\mathbb{P}\\left(\\neg Y\\right)+\\mathbb{P}\\left(\\neg Y\\right)\\mathbb{E}\\left[X\\middle|\\neg Y\\right]=\\mathbb{P}\\left(\\neg Y\\right)\\left(\\mathbb{E}\\left[X\\middle|\\neg Y\\right]-\\mathbb{E}\\left[X\\middle|Y\\right]\\right)}\\\\ &{\\left|\\mathbb{E}\\left[X\\right]-\\mathbb{E}\\left[X\\middle|Y\\right]\\right|=\\mathbb{P}\\left(\\neg Y\\right)\\underbrace{\\mathbb{E}\\left[X\\middle|\\neg Y\\right]-\\mathbb{E}\\left[X\\middle|Y\\right]}_{\\leq1}\\leq\\mathbb{P}\\left(\\neg Y\\right)\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "As a result ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\right]-\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\mid\\mathrm{consistent}\\;S\\right]\\right|\\leq\\mathbb{P}_{S}(\\mathrm{inconsistent}\\;S)\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then, the required inequality is obtained by simply using the triangle inequality on ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\mathbb{E}_{S,A(S)}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\right]-2\\varepsilon^{\\star}\\left(1-\\varepsilon^{\\star}\\right)\\right|\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "D Memorizing the label flips (proofs for Section 3) ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we prove Theorem 3.1. We begin with an informal outline of the proof idea. Inspired by Manoj and Srebro\u2019s analysis [58], our proof of Theorem 3.1 is based on the concept of a pseudorandom generator, defined below. ", "page_idx": 27}, {"type": "text", "text": "Definition D.1 (Pseudorandom generator). Let $G\\colon\\{0,1\\}^{r}\\to\\{0,1\\}_{\\phantom{\\infty}-}^{R}$ be a function, let $\\mathcal{V}$ be a class of functions $V\\colon\\{0,1\\}^{R}\\rightarrow\\{0,\\mathbf{\\bar{1}}\\}$ , let $\\mathcal{D}$ be a distribution over $\\{0,1\\}^{R}$ , and let $\\epsilon>0$ . We say that $G$ is an $\\epsilon$ -pseudorandom generator $\\mathbf{\\chi}_{\\epsilon}$ -PRG) for $\\mathcal{V}$ with respect to $\\mathcal{D}$ if for every $V\\in\\mathcal{V}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\mathbb{P}_{\\mathbf{y}\\sim\\mathcal{D}}(V(\\mathbf{y})=1)-\\mathbb{P}_{\\mathbf{u}\\in\\{0,1\\}^{r}}(V(G(\\mathbf{u}))=1)\\right|\\le\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\mathbf{u}$ is sampled uniformly at random from $\\{0,1\\}^{r}$ . ", "page_idx": 27}, {"type": "text", "text": "To connect Definition D.1 to Theorem 3.1, let $R=2^{d_{0}}$ . Let $\\nu$ be the class of all conjunctions of literals, such as $V(\\mathbf{y})=\\mathbf{y}_{1}\\wedge\\bar{\\mathbf{y}}_{2}\\wedge\\mathbf{y}_{4}$ . Let $\\hat{\\mathcal{X}}=f^{-1}(\\{0,1\\})$ . There is a function $V_{f}\\in\\mathcal{V}$ such that given the entire truth table of a $\\mathrm{NN}\\,\\tilde{h}$ , the function $V_{f}$ verifies that $\\tilde{h}$ agrees with $f$ on $\\hat{\\chi}$ . This function $V_{f}$ is a conjunction of $N_{1}$ many variables and $\\left(\\dot{N}-N_{1}\\right)$ many negated variables. ", "page_idx": 27}, {"type": "text", "text": "Let $\\alpha=N_{1}/N$ , and let $\\mathcal{D}=\\mathop{\\mathrm{Ber}}(\\alpha)^{R}$ . Suppose $G$ is an $\\epsilon$ -PRG for $\\nu$ with respect to $\\mathcal{D}$ , where $\\boldsymbol{\\epsilon}<\\mathbb{P}_{\\mathbf{y}\\sim\\mathcal{D}}(V_{f}(\\mathbf{y})=1)$ . Then $\\mathbb{P}_{\\mathbf{u}\\in\\{0,1\\}^{r}}(V_{f}(G(\\mathbf{u}))=1)\\neq0$ , i.e., there exists some $\\mathbf{u}^{\\star}\\in\\{0,1\\}^{r}$ such that $V_{f}(G(\\mathbf{u}^{\\star}))=1$ . Therefore, if we let $\\tilde{h}$ be a NN that computes the function ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\tilde{h}(\\mathbf{x})=G(\\mathbf{u}^{\\star})_{\\mathbf{x}},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "then $\\tilde{h}$ agrees with $f$ on $\\hat{\\chi}$ . In the equation above, $G(\\mathbf{u}^{\\star})_{\\mathbf{x}}$ denotes the $\\mathbf{x}_{}$ -th bit of $G(\\mathbf{u}^{\\star})$ , thinking of $\\mathbf{x}$ as a number from 0 to $R-1$ represented by its binary expansion. ", "page_idx": 27}, {"type": "text", "text": "There is a large body of well-established techniques for constructing PRGs. (See, for example, Hatami and Hoza\u2019s recent survey [35].) Therefore, constructing a suitable PRG might seem like a promising approach to proving Theorem 3.1. However, this approach is flawed. The issue concerns the seed length $(r)$ . According to the plan outlined above, the seed $\\mathbf{u}^{\\star}$ is effectively hard-coded into the neural network $\\tilde{h}$ , which means that, realistically, the number of weights in $\\tilde{h}$ will be at least $r$ Meanwhile, for the plan above to make sense, our PRG\u2019s error parameter (\u03f5) must satisfy ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\epsilon<\\mathbb{P}_{\\mathbf{y}\\sim\\mathcal{D}}(V_{f}(\\mathbf{y})=1)=2^{-H(\\alpha)\\cdot N}\\approx2^{-{\\binom{N}{N_{1}}}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Comparing (9) to Theorem 3.1, we see that we would need a PRG with seed length ", "page_idx": 27}, {"type": "equation", "text": "$$\nr=(1+o(1))\\cdot\\log(1/\\epsilon).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "But this is too much to ask. There is no real reason to expect such a PRG to exist, even if we ignore explicitness considerations. Indeed, in some cases, it is provably impossible to achieve a seed length smaller than $(2-o(1))\\cdot\\log(1/\\epsilon)$ [2]. ", "page_idx": 27}, {"type": "text", "text": "To circumvent this issue, we will work with a more flexible variant of the PRG concept called a hitting set generator (HSG). ", "page_idx": 27}, {"type": "text", "text": "Definition D.2 (Hitting set generator). Let $G\\colon\\{0,1\\}^{r}\\to\\{0,1\\}^{R}$ be a function, let $\\nu$ be a class of functions $V\\colon\\{0,1\\}^{\\overline{{R}}}\\to\\ \\{0,1\\}$ , let $\\mathcal{D}$ be a distribution over $\\{0,1\\}^{R}$ , and let $\\epsilon\\,>\\,0$ . We say that $G$ is an $\\epsilon$ -hitting set generator $\\scriptstyle\\epsilon-\\mathrm{HSG})$ for $\\nu$ with respect to $\\mathcal{D}$ if for every $V\\in\\mathcal{V}$ such that $\\mathbb{P}_{\\mathbf{y}\\sim\\mathcal{D}}(V(\\mathbf{y})=1)>\\epsilon$ , there exists $\\mathbf{u}^{\\star}\\in\\{0,1\\}^{r}$ such that $V(G(\\mathbf{u}^{\\star}))=1$ . ", "page_idx": 27}, {"type": "text", "text": "Definition D.2 is weaker than Definition D.1, but an HSG is sufficient for our purposes. Crucially, one can show nonconstructively that for every $\\mathcal{V},\\mathcal{D}$ , and $\\epsilon$ , there exists an HSG with seed length ", "page_idx": 27}, {"type": "equation", "text": "$$\n1\\cdot\\log(1/\\epsilon)+\\log\\log|\\mathcal{V}|+O(1),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "whereas the nonconstructive PRG seed length is $2\\cdot\\log(1/\\epsilon)+\\cdots.$ . To prove Theorem 3.1, we will construct an explicit HSG for conjunctions of literals with respect to $\\bar{\\mathrm{Ber}}(\\alpha)^{R}$ with a seed length of $(1+o(1))\\cdot\\log(1/\\epsilon)+\\mathrm{polylog}\\,R$ . We will ensure that our HSG is \u201cexplicit enough\u201d to enable computing the function $\\tilde{h}$ defined by (8) using a constant-depth NN with approximately $r$ many weights. ", "page_idx": 27}, {"type": "text", "text": "Our HSG construction uses established techniques from the pseudorandomness literature. In brief, we use $k$ -wise independence to construct an initial HSG with a poor dependence on $\\epsilon$ , and then we apply an error reduction technique due to Hoza and Zuckerman [38]. Details follow. ", "page_idx": 27}, {"type": "text", "text": "D.1 Preprocessing the input to reduce the dimension ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Before applying an HSG as outlined above, the first step of the proof of Theorem 3.1 is actually a preprocessing step that reduces the dimension to approximately $2\\log N$ . This step is not completely essential, but it helps to improve the dependence on $d_{0}$ in Theorem 3.1. The preprocessing step is based on a standard trick, namely, we treat the input as a vector in $\\mathbb{F}_{2}^{d_{0}}$ and apply a random matrix, where $\\mathbb{F}_{2}$ denotes the field with two elements. That is: ", "page_idx": 28}, {"type": "text", "text": "Definition D.3 $\\mathbb{F}_{2}$ -linear and $\\mathbb{F}_{2}$ -affine functions). A function $C\\colon\\{0,1\\}^{d}\\rightarrow\\{0,1\\}^{d^{\\prime}}$ is $\\mathbb{F}_{2}$ -linear if it has the form ", "page_idx": 28}, {"type": "equation", "text": "$$\nC(\\mathbf{x})=\\mathbf{W}\\mathbf{x},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\mathbf{W}\\in\\{0,1\\}^{d^{\\prime}\\times d}$ and the arithmetic is mod 2. More generally, we say that $C$ is $\\mathbb{F}_{2}$ -affine if it has the form ", "page_idx": 28}, {"type": "equation", "text": "$$\nC(\\mathbf{x})=\\mathbf{W}\\mathbf{x}+\\mathbf{b},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\mathbf{W}\\in\\{0,1\\}^{d^{\\prime}\\times d}$ , $\\mathbf{b}\\in\\{0,1\\}^{d^{\\prime}}$ , and the arithmetic is mod 2. ", "page_idx": 28}, {"type": "text", "text": "The following fact is standard; we include the proof only for completeness. ", "page_idx": 28}, {"type": "text", "text": "Lemma D.4 (Preprocessing to reduce the dimension). Let $d_{0}~\\in~\\mathbb{N}$ , let $\\hat{\\mathcal X}\\;\\subseteq\\;\\{0,1\\}^{d_{0}}$ , and let $N=|\\hat{\\mathcal{X}}|$ . There exists an $\\mathbb{F}_{2}$ -linear function $C_{0}\\colon\\{0,1\\}^{d_{0}}\\to\\{0,1\\}^{2\\lceil\\log N\\rceil}$ that is injective on $\\hat{\\mathcal X}$ . ", "page_idx": 28}, {"type": "text", "text": "Proof. Pick $\\mathbf{W}\\,\\in\\,\\{0,1\\}^{2\\lceil\\log N\\rceil\\,\\times\\,d_{0}}$ uniformly at random and let $C_{0}({\\bf x})={\\bf W}{\\bf x}$ . For each pair of distinct points $\\mathbf{x},\\mathbf{x}^{\\prime}\\in\\hat{\\mathcal{X}}$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathbf{W}\\mathbf{x}=\\mathbf{W}\\mathbf{x}^{\\prime})=\\mathbb{P}(\\mathbf{W}(\\mathbf{x}-\\mathbf{x}^{\\prime})=\\mathbf{0})=2^{-2\\lceil\\log N\\rceil}<1/N^{2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Therefore, by the union bound over all pairs $\\mathbf{x},\\mathbf{x}^{\\prime}$ , there is a nonzero chance that $C_{0}$ is injective on $\\hat{\\mathcal X}$ . Therefore, there is some fixing of $\\mathbf{W}$ such that $C_{0}$ is injective on $\\hat{\\chi}$ . \u53e3 ", "page_idx": 28}, {"type": "text", "text": "We will choose $C_{0}$ to be injective on the domain of $f$ . That way, after applying $C_{0}$ to the input, our remaining task is to compute some other partial function $f^{\\prime}\\colon\\{0,1\\}^{2\\lceil\\log N\\rceil}\\to\\{0,1,\\star\\}$ , namely, the function $f^{\\prime}$ such that $f^{\\prime}\\,\\overline{{\\circ}}\\,C_{0}=f$ . This function $f^{\\prime}$ has the same domain size $(N)$ , and it takes the value 1 on the same number of points $(N_{1})$ , so the net effect is that we have decreased the dimension from $d_{0}$ down to $2\\lceil\\log N\\rceil$ . This same technique appears in the circuit complexity literature, along with more sophisticated variants. For example, see Jukna\u2019s textbook [45, Section 1.4.2]. ", "page_idx": 28}, {"type": "text", "text": "To apply Lemma D.4 in our setting, we rely on the well-known fact that $\\mathbb{F}_{2}$ -linear functions, and more generally $\\mathbb{F}_{2}$ -affine functions, can be computed by depth-two binary threshold networks. More precisely, we have the following. ", "page_idx": 28}, {"type": "text", "text": "Lemma D.5 (Binary threshold networks computing $\\mathbb{F}_{2}$ -linear functions). If $C\\colon\\{0,1\\}^{d}\\rightarrow\\{0,1\\}$ is the parity function or its negation, then there exists a depth-one binary threshold network $C_{0}\\colon\\{\\overset{\\cdot}{0},1\\}^{\\tilde{d}}\\stackrel{\\cdot}{\\rightarrow}\\{0,1\\}^{(d+2)}$ and a number $b\\in\\mathbb{R}$ such that for every $\\mathbf{x}\\in\\{0,1\\}^{d}$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\nC(x)=\\mathbf{1}^{T}C_{0}(\\mathbf{x})+b,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where 1 denotes the all-ones vector. Moreover, every affine function $C\\colon\\{0,1\\}^{d}\\rightarrow\\{0,1\\}^{d^{\\prime}}$ can be computed by a depth-two binary threshold network with $d^{\\prime}\\cdot(d+2)$ nodes in the hidden layer. ", "page_idx": 28}, {"type": "text", "text": "Proof. First, suppose $C$ is the parity function. For each $i\\in[d]$ , let $\\phi_{\\leq i}\\colon\\{0,1\\}^{d}\\rightarrow\\{0,1\\}$ be the function ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\phi_{\\leq i}(\\mathbf{x})=1\\iff\\sum_{j=1}^{d}\\mathbf{x}_{j}\\leq i,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and similarly define $\\phi_{\\geq i}\\colon\\{0,1\\}^{d}\\rightarrow\\{0,1\\}$ by ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\phi_{\\geq i}(\\mathbf{x})=1\\iff\\sum_{j=1}^{d}\\mathbf{x}_{j}\\geq i.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\phi_{\\leq1}(\\mathbf{x})+\\phi_{\\geq1}(\\mathbf{x})+\\phi_{\\leq3}(\\mathbf{x})+\\phi_{\\geq3}(\\mathbf{x})+\\cdot\\cdot\\cdot={\\\\\\left\\{\\begin{array}{l l}{[d/2]+1}&{{\\mathrm{if~}}\\mathsf{P A R I T V}(\\mathbf{x})=1}\\\\ {\\lceil d/2\\rceil}&{{\\mathrm{if~}}\\mathsf{P A R I T Y}(\\mathbf{x})=0,}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "so we can take $b=-\\lceil d/2\\rceil$ . Now, suppose $C$ is the negation of the parity function. This reduces to the case of the parity function because $1-\\mathsf{P A R I T y}(\\bar{\\mathbf{x}})=\\mathsf{P A R I T y}(\\mathbf{x},\\bar{\\mathbf{\\xi}})$ . Finally, the \u201cmoreover\u201d statement follows because if $C$ is $\\mathbb{F}_{2}$ -affine, then every output bit of $C$ is either the parity function or the negated parity function applied to some subset of the inputs. \u53e3 ", "page_idx": 29}, {"type": "text", "text": "Lemma D.5 can be generalized to the case of any symmetric function instead of PARITY. This technique is well-known in the circuit complexity literature; for example, see the work of Hajnal, Maass, Pudl\u00e1k, Szegedy, and Tur\u00e1n [32]. ", "page_idx": 29}, {"type": "text", "text": "D.2 Threshold networks computing $k$ -wise independent generators ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "One of the ingredients of our HSG will be a family of pairwise independent hash functions. We will use the following family, notable for its low computational complexity. ", "page_idx": 29}, {"type": "text", "text": "Lemma D.6 (Affine pairwise independent hash functions). For every $a,r\\in\\mathbb{N},$ , there is a family $\\mathcal{H}$ of hash functions hash: $\\{0,1\\}^{a}\\to\\bar{\\{0,1\\}}^{r}$ with the following properties. ", "page_idx": 29}, {"type": "text", "text": "\u2022 |H| \u22642O(a+r).   \n\u2022 $\\mathcal{H}$ is pairwise independent. That is, for every two distinct w, $\\mathbf{w}^{\\prime}\\in\\{0,1\\}^{a}$ , if we pick hash $\\in\\mathcal{H}$ uniformly at random, then hash(w) and hash $\\left(\\mathbf{w}^{\\prime}\\right)$ are independent and uniformly distributed over $\\{0,1\\}^{r}$ .   \n\u2022 Each function hash $\\in\\mathcal{H}$ is $\\mathbb{F}_{2}$ -affine. ", "page_idx": 29}, {"type": "text", "text": "Proof. See the work of Mansour, Nisan, and Tiwari [59, Claim 2.2]. ", "page_idx": 29}, {"type": "text", "text": "Remark D.7 (Alternative hash families). By Lemma D.5, each function hash $\\in{\\mathcal{H}}$ can be computed by a depth-two binary threshold network with $O(r^{2}a+r a^{2})$ wires (weights). There exist alternative pairwise independent hash function families with lower wire complexity. In particular, one could use hash functions based on integer arithmetic [24], which can be implemented with wire complexity $(a+r)^{1+\\gamma}$ for any arbitrarily small constant $\\gamma>0$ [73]. This would lead to slightly better width and wire complexity bounds in Theorem 3.1: each occurrence of $3/4$ could be replaced with $2/3+\\gamma$ . However, the downside of this approach is that the depth of the network would increase to a very large constant depending on $\\gamma$ . ", "page_idx": 29}, {"type": "text", "text": "Another ingredient of our HSG will be a threshold network computing a \u201c $k$ -wise uniform generator,\u201d defined below. ", "page_idx": 29}, {"type": "text", "text": "Definition D.8 ( $k$ -wise uniform generator). A $k$ -wise uniform generator is a function $G\\colon\\{0,1\\}^{r}\\rightarrow$ $\\{0,1\\}^{R}$ such that if we sample $\\mathbf{u}~\\in~\\{0,1\\}^{r}$ uniformly at random, then every $k$ of the output coordinates of $G(\\mathbf{u})$ are independent and uniform. In other words, $G$ is a 0-PRG for $\\nu$ with respect to the uniform distribution, where $\\mathcal{V}$ consists of all Boolean functions that only depend on $k$ bits. ", "page_idx": 29}, {"type": "text", "text": "Prior work has shown that $k$ -wise uniform generators can be implemented by constant-depth threshold networks [36]. We will need to re-analyze the construction to get sufficiently fine-grained bounds. In the remainder of this subsection, we will prove the following. ", "page_idx": 29}, {"type": "text", "text": "Lemma D.9 (Constant-depth $k$ -wise uniform generator). Let $k,R\\in\\mathbb{N}$ where $R$ is a power of two. There exists a $k$ -wise uniform generator $G\\colon\\{\\bar{0},1\\}^{r}\\rightarrow\\{0,1\\}^{R}$ , where $r=O(k\\cdot\\log R)$ , such that for every $\\mathbb{F}_{2}$ -affine function hash: $\\{0,1\\}^{a}\\to\\{0,1\\}^{r}$ , there exists a depth-5 binary threshold network $C\\colon\\{0,1\\}^{a+\\log R}\\to\\{0,1\\}^{k}$ \u00b7polylog $R$ with widths $\\underline{d}$ satisfying the following. ", "page_idx": 30}, {"type": "text", "text": "1. For every w $r\\in\\{0,1\\}^{a}$ and every $\\mathbf{z}\\in\\{0,1\\}^{\\log R}$ , we have $G(\\mathrm{hash}(\\mathbf{w}))_{\\mathbf{z}}=\\mathsf{P A R I T V}(C(\\mathbf{w},\\mathbf{z})),$ thinking of z as a number in $\\{0,1,\\ldots,R-1\\}$ .   \n2. The maximum width $\\underline{{d}}_{\\mathrm{max}}$ is at most ak \u00b7 polylog $R$ .   \n3. The total number of weights $w\\left(\\underline{d}\\right)$ is at most $(a+k)\\cdot a k\\cdot\\mathrm{polylog}\\,R.$ ", "page_idx": 30}, {"type": "text", "text": "Remark D.10 (The role of the parity functions). One can combine Lemma D.9 with Lemma D.5 to obtain threshold networks computing the function $(\\mathbf{u},\\mathbf{z})\\mapsto G(\\mathbf{u})_{\\mathbf{z}}$ . In Lemma D.9, instead of describing a network that computes the function $(\\mathbf{u},\\mathbf{z})\\mapsto G(\\mathbf{u})_{\\mathbf{z}}$ , we describe a network $C$ satisfying $G(\\mathrm{hash}(\\mathbf{\\bar{w}}))_{\\mathbf{z}}=\\mathsf{P A R}|\\mathsf{T}\\mathsf{Y}(C(\\mathbf{w},\\mathbf{z}))$ . The only reason for this more complicated statement is that it leads to a slightly better depth complexity in Theorem 3.1. ", "page_idx": 30}, {"type": "text", "text": "We reiterate that the proof of Lemma D.9 heavily relies on prior work. For the most part, this prior work studies a Boolean circuit model that is closely related to, but distinct from, the \u201cbinary threshold network\u201d model in which we are interested. We introduce the circuit model next. ", "page_idx": 30}, {"type": "text", "text": "Definition D.11 $\\widehat{\\mathrm{LT}}_{L}$ circuits). An $\\widehat{\\mathrm{LT}}_{L}$ circuit is defined just like a depth- $L$ binary threshold network (Definition 2.1), except that we allow arbitrary integer weights $(\\mathbf{W}^{(l)}\\in\\mathbb{Z}^{d_{l}\\times d_{l-1}})$ ; we allow arbitrary integer thresholds $(\\bar{\\mathbf{b}}^{(l)}\\in\\mathbb{Z}^{d_{l}})$ ); and we do not allow any scaling $(\\gamma^{(l)}=\\mathbf{1}^{d_{l}})$ ). The size of the circuit is the sum of the absolute values of the weights, i.e., ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{l=1}^{L}\\sum_{i=1}^{d_{l}}\\sum_{j=1}^{d_{l-1}}|\\mathbf{W}_{i j}^{(l)}|.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Remark D.12 (Parallel wires). In the context of circuit complexity, it is perhaps more natural to stipulate that the weights are always $\\{\\pm1\\}$ ; there can be any number of parallel wires between two nodes, including zero; and the size of the circuit is the total number of wires. This is completely equivalent to Definition D.11. ", "page_idx": 30}, {"type": "text", "text": "The proof of Lemma D.9 relies on circuits performing arithmetic. A long line of research investigated the depth complexity of (iterated) integer multiplication [16, 9, 69, 78, 37, 73, 32, 80, 79], culminating in the following result by Siu and Roychowdhury [79]. ", "page_idx": 30}, {"type": "text", "text": "Theorem D.13 (Iterated multiplication in depth four [79]). For every $n\\in\\mathbb{N},$ , there exists an $\\widehat{\\mathrm{LT_{4}}}$ circuit of size poly $(n)$ that computes the product of n given n-bit integers. ", "page_idx": 30}, {"type": "text", "text": "By a standard trick [26], Theorem D.13 implies circuits of the same complexity that compute the iterated product of polynomials over $\\mathbb{F}_{2}$ . We include a proof sketch for completeness. ", "page_idx": 30}, {"type": "text", "text": "Corollary D.14 (Iterated multiplication of polynomials over $\\mathbb{F}_{2}$ ). For every $n\\in\\mathbb N$ , there exists an $\\widehat{\\mathrm{LT}}_{4}$ circuit of size poly $(n)$ that computes the product of $n$ given polynomials in $\\mathbb{F}_{2}[x]$ , each of which has degree less than n and is represented by an $n$ -bit vector of coefficients. ", "page_idx": 30}, {"type": "text", "text": "Proof sketch. Think of the given polynomials as polynomials over $\\mathbb{Z}$ , say $q_{1}(x),\\ldots,q_{n}(x)$ . If we evaluate one of these polynomials on a power of two, say $q_{i}(2^{s})$ , and then write the output in binary, the resulting string consists of the coefficients of $q_{i}$ , with $s-1$ zeroes inserted between every two bits. Therefore, by using the poly $(n s)$ -size circuit of Theorem D.13 (with some of its inputs fixed to zeroes), we can compute the product $q_{1}{\\bigl(}2^{s}{\\bigr)}\\cdot q_{2}{\\bigl(}2^{s}{\\bigr)}\\cdot\\cdot\\cdot q_{n}{\\bigl(}2^{s}{\\bigr)}=q{\\bigl(}2^{s}{\\bigr)}$ , where $q=q_{1}\\cdot q_{2}\\cdot\\cdot\\cdot q_{n}$ . Every coefficient of $q$ is a nonnegative integer bounded by $n^{n}$ , so if we choose $s=\\lceil n\\log n\\rceil$ , then the binary expansion of $q(2^{s})$ is the concatenation of all of the binary expansions of the coefficients of $q$ . To reduce mod 2, we simply discard all but the lowest-order bit of each of those coefficients. ", "page_idx": 30}, {"type": "text", "text": "At this point, we are ready to construct a circuit that computes a $k$ -wise uniform generator. The construction is based on the work of Healy and Viola [36]. ", "page_idx": 31}, {"type": "text", "text": "Lemma D.15 (A $k$ -wise uniform generator in the $\\widehat{\\mathrm{LT}}_{L}$ model). Let $k,R\\in\\mathbb{N}$ where $R$ is a power of two. There exists a $k$ -wise uniform generator $G\\colon\\{0,1\\}^{r}\\,\\to\\,\\{0,1\\}^{R}$ , an $\\mathbb{F}_{2}$ -linear function $C_{0}\\colon\\{0,1\\}^{\\log R}\\to\\{0,1\\}^{O(\\log R\\cdot\\log k)}$ , and an $\\widehat{\\mathrm{LT_{4}}}$ circuit $C_{1}\\colon\\{0,1\\}^{O(k\\cdot\\log R)}\\to\\{0,1\\}^{O(k\\cdot\\log R)}$ with the following properties. ", "page_idx": 31}, {"type": "text", "text": "\u2022 The seed length is $r=O(k\\cdot\\log R)$ .   \n\u2022 For every seed $\\mathbf{u}\\in\\{0,1\\}^{r}$ and every $\\mathbf{z}\\in\\{0,1\\}^{\\log R}$ , we have $G(\\mathbf{u})_{\\mathbf{z}}=\\mathsf{P A R I T V}(C_{1}(\\mathbf{u},C_{0}(\\mathbf{z}))),$ thinking of z as a number in $\\{0,1,\\ldots,R-1\\}$ .   \n\u2022 The circuit $C_{1}$ has size k \u00b7 polylog $R$ . ", "page_idx": 31}, {"type": "text", "text": "Proof. If $k\\geq R$ , the lemma is trivial, so assume $k<R$ . We use the following standard example of a $k$ -wise independent generator [22, 1]. Let $n=\\log R$ , let $E(x)\\in\\mathbb{F}_{2}[x]$ be an irreducible polynomial of degree $n$ , and let $\\mathbb{F}_{2^{n}}$ be the finite field consisting of all polynomials in $\\mathbb{F}_{2}[x]$ modulo $E(x)$ . The seed of the generator is interpreted as a list of field elements: $\\mathbf{u}=(p_{0},p_{1},\\dots,p_{k-1})\\in\\mathbb{F}_{2^{n}}^{k}$ . Each index $\\mathbf{z}\\in\\{0,1,\\ldots,R\\!-\\!1\\}$ can be interpreted as a field element $\\mathbf{z}\\in\\mathbb{F}_{2^{n}}$ . The output of the generator is given by ", "page_idx": 31}, {"type": "equation", "text": "$$\nG(\\mathbf{u})_{\\mathbf{z}}={\\mathrm{the~lowest~order~bit~of~}}p_{0}\\cdot\\mathbf{z}^{0}+p_{1}\\cdot\\mathbf{z}^{1}+\\cdot\\cdot\\cdot+p_{k-1}\\cdot\\mathbf{z}^{k-1},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the arithmetic takes place in $\\mathbb{F}_{2^{n}}$ . ", "page_idx": 31}, {"type": "text", "text": "To study the circuit complexity of this generator, let us first focus on a single term $p_{i}\\cdot\\mathbf{z}^{i}$ . Since we are thinking of $\\mathbf{z}$ as a field element $\\mathbf{z}\\in\\mathbb{F}_{2^{n}}$ , we can also think of it as a polynomial $\\mathbf{z}(x)\\in\\mathbb{F}_{2}[x]$ of degree less than $n$ . Write $\\begin{array}{r}{{\\bf z}(x)=\\sum_{j=0}^{n-1}{\\bf z}_{j}\\cdot x^{j}}\\end{array}$ . We compute the power $\\mathbf{z}^{i}$ by a \u201crepeated squaring\u201d approach. Write $\\textstyle i=\\sum_{m\\in M}2^{m}$ , where $M\\subseteq\\{0,1,\\ldots,\\lfloor\\log i\\rfloor\\}$ . Then ", "page_idx": 31}, {"type": "equation", "text": "$$\np_{i}(x)\\cdot\\mathbf{z}(x)^{i}=p_{i}(x)\\cdot\\prod_{m\\in M}\\left(\\sum_{j=0}^{n-1}\\mathbf{z}_{j}\\cdot x^{j}\\right)^{2^{m}}=p_{i}(x)\\cdot\\prod_{m\\in M}\\sum_{j=0}^{n-1}\\mathbf{z}_{j}\\cdot x^{j\\cdot2^{i}},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "since we are working in characteristic two. For each $m\\:\\in\\:M$ and each $j~<~n$ , let $e_{m,j}(x)\\,=$ xj\u00b72 mod E(x), a polynomial of degree less than n. That way, ", "page_idx": 31}, {"type": "equation", "text": "$$\np_{i}(x)\\cdot\\mathbf{z}(x)^{i}\\equiv p_{i}(x)\\cdot\\prod_{m\\in M}\\sum_{j=0}^{n-1}\\mathbf{z}_{j}\\cdot e_{m,j}(x){\\pmod{E(x)}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The function $C_{0}(\\mathbf{z})$ computes $\\textstyle\\sum_{j=0}^{n-1}\\mathbf{z}_{j}\\cdot e_{m,j}$ for every $m\\in\\{0,1,\\ldots,\\lfloor\\log k\\rfloor\\}$ . This function is $\\mathbb{F}_{2}$ -linear, because for each $m\\in\\{0,1,\\ldots,\\lfloor\\log k\\rfloor\\}$ and each $s<n$ , the $s$ -th bit of $\\textstyle\\sum_{j=0}^{n-1}\\mathbf{z}_{j}\\cdot e_{m,j}$ is given by ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\bigoplus_{j:e_{m,j,s}=1}\\mathbf{z}_{j},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $e_{m,j,s}$ denotes the $s$ -th coefficient of the polynomial $e_{m,j}$ . ", "page_idx": 31}, {"type": "text", "text": "Next, the circuit $C_{1}$ applies $k$ copies of the iterated multiplication circuit from Corollary D.14, in parallel, to compute the polynomial on the right-hand side of (10) for each $0\\leq i<k$ . Each iterated multiplication circuit has size polylog $R$ , so altogether, $C_{1}$ has size $k\\cdot\\operatorname{polylog}R$ . ", "page_idx": 31}, {"type": "text", "text": "At this point, we have computed polynomials $r_{0},r_{1},\\dots,r_{k-1}\\in\\mathbb{F}_{2}[x]$ , each of degree $O(n\\log k)$ , such that $r_{i}(x)\\equiv p_{i}(x)\\cdot\\bar{\\mathbf{z}}(x)^{i}$ (mod $E(x)$ ). Next, we need to sum these terms up, reduce mod $E(x)$ , and output the lowest-order bit. For each $j\\leq O(n\\log k)$ , let $r_{i j}$ be the $x^{j}$ coefficient of $r_{i}$ . Our circuit needs to output the lowest-order bit of ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{k-1}r_{i}\\;\\mathrm{mod}\\;E(x)=\\sum_{i=0}^{k-1}\\sum_{j=0}^{O(n\\log k)}r_{i j}\\cdot e_{0,j}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Now, we are working over characteristic two, so $\\displaystyle\\sum$ means bitwise XOR. In other words, the output is given by ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\bigoplus_{j:e_{0,j,0}=1}{\\bigoplus_{i=0}^{k-1}{r_{i j}}},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "i.e., it is the parity function applied to some subset of the output bits of $C_{1}$ . To complete the proof, modify $C_{1}$ by deleting the unused output gates. \u53e3 ", "page_idx": 32}, {"type": "text", "text": "We have almost completed the proof of Lemma D.9. The last step is to bridge the gap between $\\widehat{\\mathrm{LT}}_{L}$ circuits and binary threshold networks. We do so via the following lemma. ", "page_idx": 32}, {"type": "text", "text": "Lemma D.16 (Simulating $\\widehat{\\mathrm{LT}}_{L}$ circuits using binary threshold networks). Let $L\\geq1$ be a constant. Let $C_{0}\\colon\\{0,1\\}^{d_{0}}\\to\\{0,1\\}^{d_{1}}$ be an $\\mathbb{F}_{2}$ -affine function, and let $C_{1}\\colon\\{0,1\\}^{d_{1}}\\to\\{0,1\\}^{d_{2}}$ be an $\\widehat{\\mathrm{LT}}_{L}$ circuit of size $S$ . Then the composition $C_{1}\\circ C_{0}$ can be computed by a depth- $(L+1)$ binary threshold network with widths $\\underline{d}$ satisfying the following. ", "page_idx": 32}, {"type": "text", "text": "1. The maximum width $\\underline{{d}}_{\\mathrm{max}}$ is at most $S\\cdot(d_{0}+2)$ .   \n2. The total number of weights $w\\left(\\underline{{d}}\\right)$ is at most $O(S^{2}d_{0}+S d_{0}^{2})$ . ", "page_idx": 32}, {"type": "text", "text": "Proof. Let us define the cost of a layer in an $\\widehat{\\mathrm{LT}}_{L}$ circuit to be the sum of the absolute values of the weights in that layer, so the size of the circuit is the sum of the costs. Lemma D.5 implies that $C_{1}\\circ C_{0}=C_{1}^{\\prime}\\circ C_{0}^{\\prime}$ , where $C_{0}^{\\prime}$ is a depth-one binary threshold network and $C_{1}^{\\prime}$ is an $\\widehat{\\mathrm{LT}}_{L}$ circuit in which the first layer has cost at most $\\bar{S}\\cdot(d_{0}+2)$ and all subsequent layers have cost  at most $S$ . ", "page_idx": 32}, {"type": "text", "text": "To complete the proof, let us show by induction on $L$ that in general, if $C_{0}^{\\prime}$ is a depth-one binary threshold network and $C_{1}^{\\prime}$ is an $\\widehat{\\mathrm{LT}}_{L}$ circuit in which the layers have costs $S_{1},S_{2},\\ldots,S_{L}$ , then $C_{1}^{\\prime}\\circ C_{0}^{\\prime}$ can be computed by a depth- $(L+1)$ binary threshold network in which the layers after the input layer have widths $S_{1},S_{2},\\ldots,S_{L}$ . Let us write $C_{1}^{\\prime}$ as $C_{3}\\circ C_{2}$ , where $C_{3}$ is the last layer of $C_{1}^{\\prime}$ . By induction, $C_{2}\\circ C_{0}^{\\prime}$ can be computed by a depth- $L$ binary threshold network $C$ in which the layers after the input layer have widths $S_{1},S_{2},\\ldots,S_{L-1}$ . Now let us modify $C_{3}$ and $C$ so that every wire in $C_{3}$ has weight either 0 or 1. If a wire in $C_{3}$ has an integer weight $w\\notin\\{0,1\\}$ , then we make $|w|$ many copies of the appropriate output gate of $C$ , negate them if $w<0$ , and then split the wire into $|w|$ many wires, each with weight $+1$ . This process has no effect on the cost of $C_{3}$ . The process could potentially increase the width of the output layer of $C$ , but its width will not exceed $S_{L}$ , the cost of $C_{3}$ . After this modification, we can simply think of $C_{3}$ as one more layer in our binary threshold network. \u53e3 ", "page_idx": 32}, {"type": "text", "text": "Lemma D.9 follows immediately from Lemmas D.15 and D.16. ", "page_idx": 32}, {"type": "text", "text": "D.3 A hitting set generator with a non-optimal dependence on $\\epsilon$ ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In this subsection, we will use the $k$ -wise independent generators that we developed in the previous subsection to construct our first HSG: ", "page_idx": 33}, {"type": "text", "text": "Lemma D.17 (Non-optimal HSG for conjunctions of literals). Let $R$ be a power of two and let $\\alpha,\\epsilon\\in(0,1)$ . Assume that $1/R\\leq\\alpha\\leq1\\,{-}\\,\\mathrm{i}/R.$ . Let $\\mathcal{V}$ be the class of functions $\\stackrel{\\cdot}{V}:\\{0,\\dot{1}\\}^{R}\\rightarrow\\{0,1\\}$ that can be expressed as a conjunction of literals. There exists a generator $G\\colon\\{\\dot{0},1\\}^{\\dot{r}}\\rightarrow\\{\\dot{0_{,}}1\\}^{R}$ satisfying the following. ", "page_idx": 33}, {"type": "text", "text": "2. The seed length is $r=O(\\log(1/\\epsilon)\\cdot\\log^{2}R)$ . ", "page_idx": 33}, {"type": "text", "text": "3. For every $\\mathbb{F}_{2}$ -affine function hash: $\\{0,1\\}^{a}\\rightarrow\\{0,1\\}^{r}$ , the function $C(\\mathbf{w},\\mathbf{z})=G(\\mathrm{hash}(\\mathbf{w}))_{\\mathbf{z}}$ can be computed by a depth-8 binary threshold network with widths $\\underline{d}$ such that the maximum width $\\underline{{d}}_{\\mathrm{max}}$ at most $a\\cdot\\log(1/\\epsilon)$ \u00b7 polylog $R$ and the total number of weights $w\\left(\\underline{{d}}\\right)$ is at most $(\\log(1/\\epsilon)\\cdot a^{2}+\\log^{2}(1/\\epsilon)\\cdot a)\\cdot\\mathrm{polylog}\\,I$ $R$ . ", "page_idx": 33}, {"type": "text", "text": "Remark D.18. The parameters of Lemma D.17 are not yet sufficient to prove Theorem 3.1. Remember, we need the number of weights to be only $(1+o(1))\\cdot\\log(1/\\epsilon)$ . On the other hand, Item 1 is stronger than what the HSG definition requires. This will enable us to improve the seed length of the generator later. ", "page_idx": 33}, {"type": "text", "text": "The proof of Lemma D.17 is based on the work of Even, Goldreich, Luby, Nisan, and Velic\u02d8kovic\u00b4 [27].   \nIn particular, we use the following lemma from their work. ", "page_idx": 33}, {"type": "text", "text": "Lemma D.19 (Implications of $k$ -wise independence [27]). Let $X_{1},\\ldots,X_{R}$ be independent $\\{0,1\\}$ - valued random variables. Let $\\tilde{X}_{1},\\ldots,\\tilde{X}_{R}$ be $k$ -wise independent $\\{0,1\\}$ -valued random variables such that $\\tilde{X}_{i}$ is distributed identically to $X_{i}$ for every $i$ . Then ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(X_{1}=X_{2}=\\cdot\\cdot\\cdot=X_{R}=1)-\\mathbb{P}(\\tilde{X}_{1}=\\tilde{X}_{2}=\\cdot\\cdot\\cdot=\\tilde{X}_{R}=1)|\\leq2^{-\\Omega(k)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof of Lemma $D.I7.$ . Let $Q$ be the smallest positive integer such that $Q\\geq4R^{2}$ and $\\log\\log Q$ is an integer. Let $\\phi\\colon\\{0,1,\\ldots,Q-1\\}\\to\\{0,1\\}$ be the function ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\phi(x)=1\\iff x\\leq\\alpha\\cdot Q.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We think of $\\phi$ as a function $\\phi\\colon\\{0,1\\}^{\\log Q}\\rightarrow\\{0,1\\}$ by representing $x$ in binary. ", "page_idx": 33}, {"type": "text", "text": "Let $\\vec{\\phi}$ $\\vec{\\phi}\\colon\\{0,1\\}^{R\\log Q}\\ \\to\\ \\{0,1\\}^{R}$ be $R$ copies of $\\phi$ applied to $R$ disjoint input blocks. Let $G_{0}\\colon\\{0,1\\}^{r}\\to\\{0,1\\}^{R\\log Q}$ be a $k$ -wise independent generator for a suitable value $k=O(\\log(1/\\epsilon)$ \u00b7 $\\log R)$ . Our generator $G$ is the composition $\\vec{\\phi}\\circ G_{0}$ . ", "page_idx": 33}, {"type": "text", "text": "Now let us prove that $G$ has the claimed properties. The seed length bound is clear. Now let us analyze the computational complexity of $G$ . To compute $G(\\mathrm{hash}(\\mathbf{w}))_{\\mathbf{z}}$ , we begin by computing $C_{1}(\\mathbf{w},\\mathbf{z}\\log Q+i)$ for every $i^{'}\\!\\in\\{0,1,\\ldots,\\log Q-\\mathrm{{\\bar{1}}}\\}$ , all in parallel, where $C_{1}$ is the depth-5 network from Lemma D.9. Since $\\log Q$ is a power of two, the binary expansions of the numbers $\\mathbf{z}\\log\\mathrm{Q}$ , z $\\log Q+1,\\mathbf{z}\\log Q+2,\\ldots,\\mathbf{z}\\log Q+\\log Q\\,-\\,1$ simply consist of $\\mathbf{z}$ followed by all possible bitstrings of length $\\log\\log Q$ . The maximum width of one of these layers is bounded by $a k$ \u00b7 polylog $\\bar{R}\\bar{=}\\,a\\cdot\\log(\\bar{1}/\\epsilon)$ \u00b7 polylog $R$ , and the total number of weights among these layers is at most $(a+k)\\cdot a k\\cdot\\mathrm{polylog}\\,R=(a+\\log(1/\\epsilon))\\cdot a\\cdot\\log(1/\\epsilon)\\cdot\\mathrm{polylog}\\,R$ . Furthermore, the number of output bits is $\\log(1/\\epsilon)\\cdot\\mathrm{polylog}\\,R$ . ", "page_idx": 33}, {"type": "text", "text": "Next, recall that to compute a single bit of the output of $G_{0}$ , we need to apply the parity function to the outputs of $C_{1}$ . Therefore, to compute an output bit of our generator $G$ , we need to apply an $\\mathbb{F}_{2}$ -linear function followed by $\\phi$ . Observe that $\\phi$ can be computed by a depth-two ${}^{\\bullet\\bullet}\\mathsf{A C}^{0}$ circuit,\u201d i.e., a circuit consisting of unbounded-fan-in AND and OR gates applied to literals, in which the total number of gates is $O(\\log Q)=O(\\log R)$ . This can be viewed as a special case of an $\\widehat{\\mathrm{LT}}_{2}$ circuit of size $O(\\log^{2}R)$ . Therefore, by Lemma D.16, the $\\mathbb{F}_{2}$ -linear function followed by $\\phi$ can be computed by a depth-3 binary threshold network in which every layer has width at most $\\operatorname{log}(1/\\epsilon)\\cdot\\mathrm{polylog}\\,R$ and the total number of weights is at most $\\log^{2}(1/\\epsilon)\\cdot\\mathrm{polylog}\\,R$ . This completes the analysis of the computational complexity of $G$ . ", "page_idx": 33}, {"type": "text", "text": "Next, let us prove the correctness of $G$ , i.e., let us prove Item 1 of Lemma D.17. Let $V\\,\\in\\,\\mathcal{V}$ and assume $\\mathbf{\\dot{P}_{y\\sim B e r(}}_{\\alpha})\\mathbf{\\mathbf{\\Psi}}(V(\\mathbf{y})\\;=\\;1)\\;\\geq\\;2\\epsilon$ . Since $V$ is a conjunction of literals, we can write $V(\\mathbf{y})=V_{1}(\\mathbf{y}_{1})\\cdot V_{2}(\\mathbf{y}_{2})\\cdot\\cdot\\cdot V_{R}(\\mathbf{y}_{R})$ for some functions $V_{1},V_{2},\\ldots,V_{R}\\colon\\{0,1\\}\\rightarrow\\{0,1\\}$ . ", "page_idx": 34}, {"type": "text", "text": "We will analyze $\\mathbb{P}_{\\mathbf{u}\\in\\{0,1\\}^{r}}(V(G(\\mathbf{u}))\\ =\\ 1)$ in two stages. First, we compare $V(\\vec{\\phi}(G_{0}(\\mathbf{u})))$ to $V(\\vec{\\phi}(\\bar{\\mathbf{y}}))$ , where $\\bar{\\mathbf{y}}\\in\\{0,1\\}^{R\\log Q}$ is uniform random. Then, in the second stage, we will compare $V(\\vec{\\phi}(\\bar{\\mathbf{y}}))$ to $V(\\mathbf{y})$ , where $\\mathbf{y}\\sim\\mathop{\\mathrm{Ber}}(\\alpha)^{R}$ . ", "page_idx": 34}, {"type": "text", "text": "For the first stage, we are in the situation of Lemma D.19, because the $R$ many $(\\log Q)$ -bit blocks of $G_{0}(\\mathbf{u})$ are $(k/\\log Q)$ -wise independent. Therefore, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left|\\mathbb{P}_{\\mathbf{u}\\in\\{0,1\\}^{r}}(V(G(\\mathbf{u}))=1)-\\mathbb{P}_{\\bar{\\mathbb{Y}}\\in\\{0,1\\}^{R\\log Q}}(V(\\vec{\\phi}(\\bar{\\mathbb{y}}))=1)\\right|\\leq\\exp\\left(-\\Omega\\left(\\frac{k}{\\log Q}\\right)\\right)\\leq0.5\\epsilon,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "provided we choose a suitable value $k=O(\\log(1/\\epsilon)\\cdot\\log R)$ . ", "page_idx": 34}, {"type": "text", "text": "Now, for the second stage, observe that if we sample $\\bar{\\mathbf{y}}\\,\\in\\,\\{0,1\\}^{\\log Q}$ uniformly at random, then $\\begin{array}{r}{|\\mathbb{P}(\\phi(\\bar{\\bf y})=1)-\\alpha|\\leq\\frac{1}{Q}\\overset{\\leftarrow}{\\leq}\\frac{1}{4R^{2}}}\\end{array}$ . For each $i$ , since $1/R\\leq\\alpha\\leq1-1/R$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{\\bar{\\mathbf{y}}\\in\\{0,1\\}^{\\log{Q}}}(V_{i}(\\phi(\\bar{\\mathbf{y}}))=1)\\ge\\mathbb{P}_{\\mathbf{y}\\sim\\mathrm{Ber}(\\alpha)}(V_{i}(\\mathbf{y})=1)-\\frac{1}{4R^{2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\ge\\left(1-\\displaystyle\\frac{1}{4R}\\right)\\cdot\\mathbb{P}_{\\mathbf{y}\\sim\\mathrm{Ber}(\\alpha)}(V_{i}(\\mathbf{y})=1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Therefore, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\mathbb{P}_{\\bar{\\mathbf{y}}\\in\\{0,1\\}^{R\\log Q}}(V(\\vec{\\phi}(\\bar{\\mathbf{y}}))=1)\\geq\\left(1-\\frac{1}{4R}\\right)^{R}\\cdot\\mathbb{P}_{\\mathbf{y}\\sim\\mathrm{Ber}(\\alpha)^{R}}(V(\\mathbf{y})=1)}}\\\\ {{\\displaystyle\\geq1.5\\epsilon}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "by Bernoulli\u2019s inequality. Combining the bounds from the two stages completes the proof. ", "page_idx": 34}, {"type": "text", "text": "D.4 Networks for computing functions that are constant on certain intervals ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "At this point, we have constructed an HSG for conjunctions of literals with a non-optimal dependence on the threshold parameter $\\epsilon$ (Lemma D.17). To improve the dependence on $\\epsilon$ , we will use a technique introduced by Hoza and Zuckerman [38]. They introduced this \u201cerror-reduction\u201d technique in the context of derandomizing general space-bounded algorithms, but it is simpler in our context (conjunctions of literals). ", "page_idx": 34}, {"type": "text", "text": "The basic idea is as follows. Let $V$ be a conjunction of literals with a low acceptance probability: $\\mathbb{P}_{\\mathbf{y}\\sim\\mathrm{Ber}(\\alpha)^{R}}(V(\\mathbf{y})=1)=\\epsilon$ . We will split $V$ up as a product, ", "page_idx": 34}, {"type": "equation", "text": "$$\nV(\\mathbf{y})=V^{(0)}(\\mathbf{y}^{(0)})\\cdot V^{(1)}(\\mathbf{y}^{(1)})\\cdot\\cdot\\cdot V^{(T-1)}(\\mathbf{y}^{(T-1)}),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where each $V^{(i)}$ is a conjunction of literals with a considerably higher acceptance probability: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathbf{y}^{(i)}\\sim\\mathrm{Ber}(\\alpha)^{R_{i}}}(V^{(i)}(\\mathbf{y}^{(i)})=1)\\approx\\epsilon_{0}\\gg\\epsilon.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We choose $V^{(0)}$ to be the conjunction of the first few literals in $V$ ; $V^{(1)}$ is the conjunction of the next few literals; etc. To hit a single $V^{(i)}$ , we can use our initial HSG with a relatively high threshold parameter $\\mathbf{\\epsilon}(\\epsilon_{0})$ . Then, we use pairwise independent hash functions to \u201crecycle\u201d the seed of our initial HSG from one $V^{(i)}$ to the next. ", "page_idx": 34}, {"type": "text", "text": "To implement this technique, one of the ingredients we need is a network that figures out which block $\\bar{V}^{(i)}$ contains a particular given index $\\mathbf{z}\\in\\{0,1,\\ldots,R-1\\}$ . In this subsection, we describe networks that handle that key ingredient. The constructions are elementary and straightforward. ", "page_idx": 34}, {"type": "text", "text": "First, we review standard circuits for integer comparisons. ", "page_idx": 34}, {"type": "text", "text": "Lemma D.20 (DNFs for comparing integers). Let $R$ be a power of two, let $I\\subseteq[0,R)$ be an interval, and let $g_{I}\\colon\\{0,1\\}^{\\log R}\\to\\{0,1\\}$ be the indicator function for $I\\cap\\left\\{0,1,\\ldots,R-1\\right\\}$ (identifying numbers with their binary expansions). Then gI can be expressed as a DNF formula consisting of $O(\\log^{2}R)$ terms. ", "page_idx": 34}, {"type": "text", "text": "Proof. First, consider the case that $I=[0,r)$ for some $r\\in\\{1,2,\\ldots,R\\}$ . If $r=R$ , then the lemma is trivial, so assume $r<R$ . Let $S$ be the set of indices at which the binary expansion of $r$ has a one. For each $i\\in S$ , we introduce a term that asserts that the input ${\\bf z}$ agrees with the binary expansion of $r$ prior to position $i$ , and then $\\mathbf{z}$ has a zero in position $i$ . The disjunction of these $|S|$ many terms computes $g_{I}$ . ", "page_idx": 35}, {"type": "text", "text": "The case $I=[\\ell,R)$ for some $\\ell\\in\\{0,1,\\ldots,R-1\\}$ is symmetric. Finally, in the general case, we can assume that $I$ is an intersection of an interval of the form $[\\ell,R)$ with an interval of the form $[0,r)$ . Therefore, $g_{I}$ can be expressed in the form $\\mathsf{A N D}_{2}\\circ{\\mathsf{O R}}_{\\log R}\\circ\\mathsf{A N D}_{\\log R}$ , where $\\mathsf{A N D}_{k}\\,/\\,\\mathsf{O R}_{k}$ denotes an AND / OR gate with fan-in $k$ . To complete the proof, observe that every $\\mathsf{A N D}_{a}\\circ\\mathsf{O R}_{b}$ formula can be re-expressed as an $\\mathsf{O R}_{b^{a}}\\circ\\mathsf{A N D}_{a}$ formula. \u53e3 ", "page_idx": 35}, {"type": "text", "text": "Lemma D.21 (Computing a function that is constant on intervals). Let $T$ and $R$ be powers of two. Suppose the interval $[0,R)$ has been partitioned into $T$ subintervals, say $[0,R)=I_{0}\\cup I_{1}\\cup\\cdot\\cdot\\cup I_{T-1}$ . Let $g\\colon\\{0,1,\\ldots,R\\stackrel{\\cdot}{-}1\\}\\stackrel{\\cdot}{\\rightarrow}\\{0,1\\}^{a}$ be a function that is constant on each subinterval $I_{j}$ . Then for every $\\mathbb{F}_{2}$ -affine function $C_{0}\\colon\\{0,1\\}^{d_{0}}\\to\\{0,1\\}^{\\log R}$ , there is a depth-6 binary threshold network $C\\colon\\{0,1\\}^{d_{0}}\\to\\{0,1\\}^{a+\\log R}$ with widths $\\underline{d}$ satisfying the following. ", "page_idx": 35}, {"type": "text", "text": "1. For every $\\mathbf{x}\\in\\{0,1\\}^{d_{0}}$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\nC(\\mathbf{x})=(g(C_{0}(\\mathbf{x})),C_{0}(\\mathbf{x})).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "2. The maximum width $d_{\\mathrm{max}}$ is at most $O(T\\cdot\\log^{3}R+a+d_{0}\\cdot\\log R).$ . ", "page_idx": 35}, {"type": "text", "text": "3. The total number of weights $w\\left(\\underline{d}\\right)$ is at most ", "page_idx": 35}, {"type": "equation", "text": "$$\na T+O(T\\cdot\\log^{4}R+d_{0}^{2}\\cdot\\log R+d_{0}\\cdot\\log^{2}R+a\\cdot\\log R).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "We emphasize that the leading term in the weights bound is $a T$ , with a coefficient of 1. This is crucial.   \nIt is also important that the weights bound has only a linear dependence on $T$ , the number of intervals. ", "page_idx": 35}, {"type": "text", "text": "Proof. We begin by computing $C_{0}(\\mathbf{x})$ and the negations of all of those bits. By Lemma D.5, we can compute these bits using a depth-two network where the hidden layer has width $O(d_{0}\\cdot\\log R)$ and the output layer has width $O(\\log R)$ . ", "page_idx": 35}, {"type": "text", "text": "Let $\\mathbf{z}=C_{0}(\\mathbf{x})\\in\\{0,1\\}^{\\log R}$ , and think of $\\mathbf{z}$ as a number $\\mathbf{z}\\in\\{0,1,\\ldots,R-1\\}$ . Our next goal is to compute the $(\\log T)$ -bit binary expansion of the unique $j_{*}\\in\\{0,1,\\ldots,T-1\\}$ such that $\\mathbf{z}\\in I_{j_{*}}$ . To do so, for each position $i\\,\\in\\,\\{0,1,\\ldots,\\log T-1\\}$ , let $S_{i}$ be the set of $j\\in\\{0,1,\\ldots,T-\\bar{1}\\}$ such that $j$ has a 1 in position $i$ of its binary expansion. We have a disjunction, over all $j\\in S_{i}$ , of the DNF computing $g_{I_{j}}$ from Lemma D.20. We also compute all the negations of the bits of $j_{*}$ , and we also copy $\\mathbf{z}$ . Altogether, this is a depth-two network where the hidden layer has width $O(T\\cdot\\log T\\cdot\\log^{2}\\bar{R})=O(\\bar{T}\\cdot\\log^{3}R)$ and the output layer has width $O(\\log R)$ . ", "page_idx": 35}, {"type": "text", "text": "Our final goal is to compute $g(\\mathbf{z})$ , which can be written in the form $g^{\\prime}(j_{*})$ since $g$ is constant on each subinterval. We use a \u201cbrute-force DNF\u201d to compute $g^{\\prime}$ . First, for every $j\\in\\{0,1,\\ldots,T-1\\}$ , we have an AND gate that checks whether $j_{*}=j$ . Then each output bit of $g^{\\prime}$ is a disjunction of some of those AND gates. We also copy ${\\bf z}$ . Altogether, this is a depth-two network where the hidden layer has width $T+\\log R$ and the output layer has width $a+\\log R$ . \u53e3 ", "page_idx": 35}, {"type": "text", "text": "D.5 Error reduction ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "In this subsection, we improve our HSG\u2019s dependence on $\\epsilon$ , as described in the previous subsection. The following theorem should be compared to Lemma D.17. As discussed previously, the proof is based on a technique due to Hoza and Zuckerman [38]. ", "page_idx": 36}, {"type": "text", "text": "Theorem D.22 (HSG with near-optimal dependence on $\\epsilon$ ). Let $R$ be a power of two and let $\\alpha,\\epsilon\\in$ $(0,1)$ . Assume that $1/R\\leq\\alpha\\leq\\bar{1}-1/R$ . Let $\\mathcal{V}$ be the class of functions $V\\colon\\dot{\\{0,1\\}}^{R}$ that can be expressed as a conjunction of literals. There exists a generator $\\dot{G}\\colon\\{0,1\\}^{r}\\rightarrow\\{\\dot{0,}1\\}^{\\dot{R}}$ satisfying the following. ", "page_idx": 36}, {"type": "text", "text": "1. $G$ is an $\\epsilon$ -HSG for $\\nu$ with respect to $\\mathrm{Ber}(\\alpha)^{R}$ . That is, $i f\\mathbb{P}_{\\mathbf{y}\\sim\\mathrm{Ber}(\\alpha)^{R}}(V(\\mathbf{y})=1)>\\epsilon$ for every $V\\in\\mathcal{V}$ , then there exists a seed $\\sigma\\in\\{0,1\\}^{r}$ such that $V(G(\\sigma))=1$ . ", "page_idx": 36}, {"type": "text", "text": "2. The seed length is $r=\\log(1/\\epsilon)+\\log^{3/4}(1/\\epsilon)\\cdot\\mathrm{polylog}\\,R.$ ", "page_idx": 36}, {"type": "text", "text": "3. For every $\\mathbb{F}_{2}$ -affine function $C_{0}\\colon\\{0,1\\}^{d_{0}}\\to\\{0,1\\}^{\\log R}$ and every fixed seed $\\sigma\\in\\{0,1\\}^{r}$ , the function $\\tilde{h}(\\mathbf{x})\\,=\\,G(\\sigma)_{C_{0}(\\mathbf{x})}$ can be computed by a depth-14 binary threshold network with widths d such that the maximum width dmax is at most ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\log^{3/4}(1/\\epsilon)\\cdot\\mathrm{polylog}\\,R+O(d_{0}\\cdot\\log R),\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and the total number of weights $w\\left(\\underline{{d}}\\right)$ is at most ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\log(1/\\epsilon)+\\log^{3/4}(1/\\epsilon)\\cdot\\mathrm{polylog}\\,R+O(d_{0}^{2}\\cdot\\log R+d_{0}\\cdot\\log^{2}R).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. First we will describe the construction of $G$ ; then we will verify its seed length and computational complexity; and finally we will verify its correctness. ", "page_idx": 36}, {"type": "text", "text": "Construction. Let $T$ be the smallest power of two such that $T\\geq\\log^{3/4}(1/\\epsilon)$ . Let ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\epsilon_{0}=\\frac{\\epsilon^{1/T}}{2R},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and note that $\\log(1/\\epsilon_{0})=\\Theta(\\log^{1/4}(1/\\epsilon)+\\log R)$ . Let $G_{0}\\colon\\{0,1\\}^{r_{0}}\\rightarrow\\{0,1\\}^{R}$ be the generator of Lemma D.17 with error parameter $\\epsilon_{0}$ , i.e., for every $V\\in\\mathcal{V}$ , if $\\mathbb{P}_{\\mathbf{y}\\sim\\mathrm{Ber}(\\alpha)^{R}}(V(\\mathbf{y})=1)\\geq2\\epsilon_{0}$ , then $\\mathbb{P}_{\\mathbf{u}\\in\\{0,1\\}^{r_{0}}}(V(G_{0}(\\mathbf{u}))=1)\\geq\\epsilon_{0}$ . Let $a$ be the smallest positive integer such that $2^{a}>R/\\epsilon_{0}$ . Let $\\mathcal{H}$ be the family of $\\mathbb{F}_{2}$ -affine hash functions hash: $\\{0,1\\}^{a}\\rightarrow\\{0,1\\}^{r_{0}}$ from Lemma D.6. ", "page_idx": 36}, {"type": "text", "text": "A seed for our generator $G$ consists of a function hash $\\in\\mathcal{H}$ , inputs $\\mathbf{w}^{0},\\ldots,\\mathbf{w}^{T-1}\\,\\in\\,\\{0,1\\}^{a}$ , and nonnegative integers $0\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!=\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\ell_{0}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\leq\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\leq\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\$ . Given this data $\\sigma\\mathrm{~\\,~}=$ ( $\\mathrm{hash},\\mathbf{w}^{0},\\bar{\\dots}\\,,\\mathbf{w}^{T-1},\\bar{\\ell}_{0},\\dots,\\ell_{T})$ , the output of the generator is given by ", "page_idx": 36}, {"type": "equation", "text": "$$\nG(\\sigma)=G_{0}(\\mathrm{hash}(\\mathbf{w}^{0}))_{0\\cdots\\ell_{1}-1}\\;\\&\\;G_{0}(\\mathrm{hash}(\\mathbf{w}^{1}))_{\\ell_{1}\\cdots\\ell_{2}-1}\\;\\&\\cdots\\;\\&\\;G_{0}(\\mathrm{hash}(\\mathbf{w}^{T-1}))_{\\ell_{T-1}\\cdots\\ell_{T}-1}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "In the equation above, $\\mathbf{y}_{a\\cdots b}$ denotes the substring of y consisting of the bits at positions $a,a+1,a+$ $2,\\ldots,b$ , and $\\&$ denotes concatenation. ", "page_idx": 36}, {"type": "text", "text": "Seed length and computational complexity. Since $|\\mathcal{H}|\\,\\le\\,2^{O(a+r_{0})}$ , the description length of hash is ${\\bar{O}}(a+r_{0})$ . The description length of $\\mathbf{w}^{0},\\dots,\\mathbf{\\dot{w}}^{T-1}$ is $a T$ , and the description length of $\\ell^{0},\\ldots,\\ell^{T}$ is $O(T\\log R)$ . By our choices of $a$ and $\\epsilon_{0}$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\na\\le\\log(1/\\epsilon_{0})+O(\\log R)=\\frac{\\log(1/\\epsilon)}{T}+O(\\log R).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Furthermore, by Lemma D.17, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\nr_{0}=O(\\log(1/\\epsilon_{0})\\cdot\\log^{2}R).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Therefore, the overall seed length of our generator is ", "page_idx": 36}, {"type": "equation", "text": "$$\na T+O(r_{0}+T\\log R+a)\\leq\\log(1/\\epsilon)+\\log^{3/4}(1/\\epsilon)\\cdot\\mathrm{polylog}\\,R.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "To analyze the computational complexity, fix an arbitrary seed ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sigma=(\\mathrm{hash},\\mathbf{w}^{0},\\dots,\\mathbf{w}^{T-1},\\ell_{0},\\dots,\\ell_{T}).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The numbers $\\ell_{0},\\dots,\\ell_{T}$ partition the interval $[0,R)$ into subintervals, namely $[0,R)=[\\ell_{0},\\ell_{1})\\cup$ $[\\ell_{1},\\ell_{2})\\cup\\cdots\\cup[\\ell_{T-1},\\ell_{T})$ . Define $g\\colon\\{0,1,\\ldots\\},R-1\\}\\to\\{0,1\\}^{a}$ by the rule ", "page_idx": 37}, {"type": "text", "text": "Then $g$ is constant on each subinterval $[\\ell_{j},\\ell_{j+1})$ , so we may apply Lemma D.21 to obtain a depth-6 binary threshold network $C_{1}\\colon\\{0,1\\}^{d_{0}}\\to\\{0,1\\}^{a+\\log R}$ computing the function $C(\\mathbf{x})=$ $(g(C_{0}(\\mathbf{x})),C_{0}^{\\phantom{\\dagger}}(\\mathbf{x}))$ . In this network, every layer has width at most ", "page_idx": 37}, {"type": "equation", "text": "$$\nO(T\\cdot\\log^{3}R+a+d_{0}\\cdot\\log R)=\\log^{3/4}(1/\\epsilon)\\cdot\\mathrm{polylog}\\,R+O(d_{0}\\cdot\\log R),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and the total number of weights is at most ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a T+O(T\\cdot\\log^{4}R+d_{0}^{2}\\cdot\\log R+d_{0}\\cdot\\log^{2}R+a\\cdot\\log R)}\\\\ &{\\qquad\\leq\\log(1/\\epsilon)+\\log^{3/4}(1/\\epsilon)\\cdot\\mathrm{polylog}\\,R+O(d_{0}^{2}\\cdot\\log R+d_{0}\\cdot\\log^{2}R).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Let ${\\mathbf z}\\,=\\,C_{0}({\\mathbf x})$ , and let $\\mathbf{w}\\,=\\,g(\\mathbf{z})$ . Our remaining goal is to compute $G(\\sigma)_{\\mathbf{z}}$ , which is equal to $G_{0}(\\mathrm{hash}(\\mathbf{w}))_{\\mathbf{z}}$ . To do so, we use the network guaranteed to exist by Lemma D.17. This network, which we call $C_{2}$ , has depth 8. Every layer in this network has width at most ", "page_idx": 37}, {"type": "equation", "text": "$$\na\\cdot\\log(1/\\epsilon_{0})\\cdot\\mathrm{polylog}\\,R=\\sqrt{\\log(1/\\epsilon)}\\cdot\\mathrm{polylog}\\,R.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "The total number of weights in this network is at most ", "page_idx": 37}, {"type": "equation", "text": "$$\n(\\log(1/\\epsilon_{0})\\cdot a^{2}+\\log^{2}(1/\\epsilon_{0})\\cdot a)\\cdot\\mathrm{polylog}\\,R=\\log^{3/4}(1/\\epsilon)\\cdot\\mathrm{polylog}\\,R.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Composing $C_{2}$ with $C_{1}$ completes the analysis of the computational complexity of our HSG. ", "page_idx": 37}, {"type": "text", "text": "Correctness. Finally, let us prove the correctness of our HSG. For convenience, for any $n\\in\\mathbb N$ and any function $V\\colon\\{0,\\bar{1}\\}^{n}\\rightarrow\\{\\bar{0},1\\}$ , we write $\\mathbb{E}(V)$ to denote the quantity $\\mathbb{P}_{\\mathbf{y}\\sim\\mathrm{Ber}(\\alpha)^{n}}(V(\\mathbf{y})=1)$ . ", "page_idx": 37}, {"type": "text", "text": "Fix any $V\\in\\mathcal{V}$ such that $\\mathbb{E}(V)>\\epsilon$ . Since $V$ is a conjunction of literals, we can write $V$ in the form ", "page_idx": 37}, {"type": "text", "text": "for some functions $V_{0},V_{1},\\ldots,V_{R-1}\\colon\\{0,1\\}\\rightarrow\\{0,1\\}$ . For each $0\\leq a\\leq b\\leq R-1$ , define ", "page_idx": 37}, {"type": "equation", "text": "$$\nV_{a\\cdots b}=V_{a}\\cdot V_{a+1}\\cdot\\cdot\\cdot V_{b}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We inductively define numbers $0=\\ell_{0}\\leq\\ell_{1}\\leq\\cdot\\cdot\\leq\\ell_{T}$ as follows. Assume that we have already defined $\\ell_{0},\\ldots,\\ell_{i}$ . Let $\\ell_{i+1}$ be the smallest integer in $\\{\\ell_{i}+1,\\ldots,R-1\\}$ such that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{E}(V_{\\ell_{i}\\cdots\\ell_{i+1}-1})\\le\\epsilon^{1/T},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "or let $\\ell_{i+1}=R$ if no such $\\ell_{i+1}$ exists. Define $V^{(i)}=V_{\\ell_{i}\\ldots\\ell_{i+1}-1}$ . Observe that $\\ell_{T}=R$ , because otherwise we would have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\epsilon<\\mathbb{E}(V)\\leq\\prod_{i=0}^{T-1}\\mathbb{E}(V_{i})\\leq(\\epsilon^{1/T})^{T}=\\epsilon,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "a contradiction. Furthermore, $\\mathbb{E}(V_{i})>\\epsilon^{1/T}/R=2\\epsilon_{0}$ , because each literal in $V$ is satisfied with probability at least $\\operatorname*{min}\\{\\alpha,1-\\alpha\\}\\geq1/R$ . Therefore, if we define ", "page_idx": 37}, {"type": "equation", "text": "$$\nS_{i}=\\{\\mathbf{u}\\in\\{0,1\\}^{r_{0}}:V_{i}(G_{0}(\\mathbf{u})_{\\ell_{i}\\cdots\\ell_{i+1}-1})=1\\}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and $\\rho_{i}=|S_{i}|/2^{r_{0}}$ , then the correctness of $G_{0}$ ensures that $\\rho_{i}\\geq\\epsilon_{0}$ . ", "page_idx": 37}, {"type": "text", "text": "Next, we will show that there exist hash, $\\mathbf{w}^{0},\\dots,\\mathbf{w}^{T-1}$ such that for every $i$ , we have $\\mathrm{hash}(\\mathbf{w}^{i})\\in$ $S_{i}$ . To prove it, pick hash $\\in{\\mathcal{H}}$ at random. For each $i\\in\\{0,1,\\ldots,T-\\dot{1}\\}$ and each $\\mathbf{w}\\in\\{0,1\\}^{a}$ , let $X_{i,\\mathbf{w}}$ be the indicator random variable for the \u201cgood\u201d event $\\mathrm{hash}(\\mathbf{w})~\\in~S_{i}$ . Define $X_{i}\\ =$ $\\sum_{\\mathbf{w}\\in\\{0,1\\}^{a}}X_{i,\\mathbf{w}}$ . Then for every $i$ , by pairwise independence, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{E}(X_{i})=2^{a}\\cdot\\rho_{i}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Therefore, by Chebyshev\u2019s inequality, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{P}(X_{i}=0)\\le\\frac{1}{2^{a}\\cdot\\rho_{i}}\\le\\frac{1}{2^{a}\\cdot\\epsilon_{0}}<\\frac{1}{R}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Consequently, by the union bound over all $i$ , there is a nonzero chance that $X_{0}\\,=\\,X_{1}\\,=\\,\\cdot\\,\\cdot\\,=$ $X_{T-1}=0$ , in which case there exist $\\mathbf{w}^{0},\\dots,\\mathbf{w}^{T-1}$ such that $\\operatorname{hash}(\\mathbf{w}^{i})\\in S_{i}$ for every $i$ . ", "page_idx": 37}, {"type": "text", "text": "At this point, we have constructed our seed $\\sigma=(\\mathrm{hash},\\mathbf{w}^{0},\\dots,\\mathbf{w}^{T-1},\\ell_{0},\\dots,\\ell_{T})$ . By the construction of $G$ , we have $V(G(\\sigma))=1$ . \u53e3 ", "page_idx": 37}, {"type": "text", "text": "Theorem 3.1 readily follows from Theorem D.22, as we now explain. ", "page_idx": 38}, {"type": "text", "text": "Recall Theorem 3.1. Let $f\\colon\\{0,1\\}^{d_{0}}\\to\\{0,1,\\star\\}$ be any function.9 Let $N=|f^{-1}(\\{0,1\\})|$ and $N_{1}=|f^{-1}(1)|$ . There exists a depth-14 binary threshold network $\\tilde{h}\\colon\\{0,1\\}^{d_{0}}\\to\\{0,1\\}$ , with widths $\\tilde{\\underline{d}}$ , satisfying the following. ", "page_idx": 38}, {"type": "text", "text": "1. $\\tilde{h}$ is consistent with $f,i.e.$ , for every $\\mathbf{x}\\in\\{0,1\\}^{d_{0}}$ , if $f(\\mathbf{x})\\in\\{0,1\\}$ , then $\\tilde{h}({\\bf x})=f({\\bf x})$ . ", "page_idx": 38}, {"type": "text", "text": "2. The total number of weights in $\\tilde{h}$ is at most $\\begin{array}{r}{(1+o(1))\\cdot\\log\\binom{N}{N_{1}}+\\mathrm{poly}(d_{0})}\\end{array}$ . More precisely, ", "page_idx": 38}, {"type": "equation", "text": "$$\nw\\left(\\tilde{\\underline{d}}\\right)=\\log\\binom{N}{N_{1}}+\\left(\\log\\binom{N}{N_{1}}\\right)^{3/4}\\cdot\\mathrm{polylog}\\,N+O(d_{0}^{2}\\cdot\\log N)\\,.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "3. Every layer of $\\tilde{h}$ has width at most $(\\log\\big({\\textstyle\\bigwedge_{N_{1}}^{N}}\\big))^{3/4}\\cdot\\mathrm{poly}(d_{0})$ . More precisely, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\tilde{\\underline{{d}}}_{\\mathrm{max}}=\\left(\\log\\binom{N}{N_{1}}\\right)^{3/4}\\cdot\\mathrm{polylog}\\,N+O(d_{0}\\cdot\\log N)\\,.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. Let $R=2^{2\\lceil\\log N\\rceil}$ . Let $C_{0}\\colon\\{0,1\\}^{d_{0}}\\to\\{0,1\\}^{\\log R}$ be an $\\mathbb{F}_{2}$ -affine function that is injective on $\\mathcal{X}$ ; such a function is guaranteed to exist by Lemma D.4. Define $V\\colon\\{0,1\\}^{R}\\rightarrow\\{0,1\\}$ by the rule ", "page_idx": 38}, {"type": "equation", "text": "$$\nV(\\mathbf{y})=1\\iff\\forall\\mathbf{x}\\in\\mathcal{X},\\ \\mathbf{y}_{C_{0}(\\mathbf{x})}=f(x).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "This function $V$ is a conjunction of $N_{1}$ variables and $N-N_{1}$ negated variables. ", "page_idx": 38}, {"type": "text", "text": "If $N_{1}~\\in~\\{0,N\\}$ , then the theorem is trivial, because we can take $\\tilde{h}$ to be a constant function. Assume, therefore, that $0<N_{1}<N$ . Let $\\alpha=N_{1}/N$ , and note that $1/R\\leq\\alpha\\leq1-1/R$ . Let $\\begin{array}{r}{\\epsilon=\\frac{1}{2}\\alpha^{N_{1}}\\cdot(1-\\alpha)^{N-N_{1}}=2^{-H(\\alpha)\\cdot N-1}}\\end{array}$ , and note that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathbf{y}\\sim\\mathrm{Ber}(\\alpha)^{R}}(V(\\mathbf{y})=1)=2\\epsilon.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Let $G\\colon\\{0,1\\}^{r}\\to\\{0,1\\}^{R}$ be the HSG from Theorem D.22. There exists a seed $\\sigma\\in\\{0,1\\}^{r}$ such that $V(G(\\sigma))=1$ . Our network $\\tilde{h}$ computes the function $\\tilde{h}(\\mathbf{x})=G(\\sigma)_{C_{0}(\\mathbf{x})}$ . Since $V(G(\\sigma))=1$ , we must have $\\tilde{h}({\\bf x})=f({\\bf x})$ for every $x\\in\\mathscr{X}$ . ", "page_idx": 38}, {"type": "text", "text": "To bound the computational complexity, observe that $\\begin{array}{r}{\\log(1/\\epsilon)\\,=\\,H(\\alpha)\\cdot N+1\\,\\le\\,\\log\\binom{N}{N_{1}}\\,+}\\end{array}$ ${\\cal O}(\\log N)$ . Therefore, every layer of $\\tilde{h}$ has width at most ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\left(\\log{\\binom{N}{N_{1}}}\\right)^{3/4}\\cdot\\mathrm{polylog}\\,N+O(d_{0}\\cdot\\log N),\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and the total number of weights in $\\tilde{h}$ is at most ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\log\\binom{N}{N_{1}}+\\left(\\log\\binom{N}{N_{1}}\\right)^{3/4}\\cdot\\mathrm{polylog}\\,N+O(d_{0}^{2}\\cdot\\log N+d_{0}\\cdot\\log^{2}N).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Finally, we have $N\\leq2^{d_{0}}$ , so the last term above can be simplified to $O(d_{0}^{2}\\cdot\\log N)$ . ", "page_idx": 38}, {"type": "text", "text": "Remark D.23. In Theorem 3.1, the weights bound has an $O(d_{0}^{2}\\cdot\\log N)$ term. This term is close to optimal; see Appendix $\\boldsymbol{\\mathrm E}$ for further details. ", "page_idx": 38}, {"type": "text", "text": "D.6 XOR networks ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "In what follows, we denote the activation function $\\sigma\\left(x\\right)=\\mathbb{I}\\left\\{x>0\\right\\}$ . ", "page_idx": 39}, {"type": "text", "text": "Lemma D.24 (XOR NN). The XOR function can be implemented with a single-hidden-layer fully connected binary threshold network with input dimension 2 and cXOR parameters by ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{\\mathrm{XOR}}\\left(\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right)=\\sigma\\left(1\\odot\\left(\\begin{array}{l l}{1}&{1}\\end{array}\\right)\\cdot\\sigma\\left(\\left(\\begin{array}{l}{1}\\\\ {-1}\\end{array}\\right)\\odot\\left(\\begin{array}{l l}{1}&{1}\\\\ {1}&{1}\\end{array}\\right)\\left(\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right)+\\left(\\begin{array}{l}{0}\\\\ {2}\\end{array}\\right)\\right)-1\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Proof. We can simplify $h_{\\mathrm{{XOR}}}$ as ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{h_{\\mathrm{XOR}}\\left(\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right)=\\sigma\\left(\\left(\\begin{array}{l}{1}&{1}\\end{array}\\right)\\cdot\\sigma\\left(\\left(\\begin{array}{l}{1}\\\\ {-1}\\end{array}\\right)\\odot\\left(\\begin{array}{l l}{1}&{1}\\end{array}\\right)\\left(\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right)+\\left(\\begin{array}{l}{0}\\\\ {2}\\end{array}\\right)\\right)-1\\right)}\\\\ &{=\\sigma\\left(\\left(\\begin{array}{l l}{1}&{1}\\end{array}\\right)\\cdot\\sigma\\left(\\left(\\begin{array}{l}{\\begin{array}{l}{x_{1}+x_{2}}\\\\ {-x_{1}-x_{2}+2}\\end{array}\\right)\\right)-1\\right)}\\\\ &{=\\sigma\\left(\\sigma\\left(x_{1}+x_{2}\\right)+\\sigma\\left(2-x_{1}-x_{2}\\right)-1\\right)}\\\\ &{=1\\{\\left\\{\\begin{array}{l}{\\{x_{1}+x_{2}>0\\}+\\mathord{\\mathbb{I}}\\left\\{x_{1}+x_{2}<2\\right\\}>1\\}\\end{array}\\right.\\}}\\\\ &{=1\\{\\left\\{\\begin{array}{l}{\\left\\{\\left(\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right)\\neq\\left(\\begin{array}{l}{0}\\\\ {0}\\end{array}\\right)\\right\\}+\\mathord{\\mathbb{I}}\\left\\{\\left(\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right)\\neq\\left(\\begin{array}{l}{1}\\\\ {1}\\end{array}\\right)\\right\\}>1\\}}\\\\ &{=1\\left\\{\\left(\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right)\\neq\\left(\\begin{array}{l}{0}\\\\ {0}\\end{array}\\right)\\mathrm{~and~}\\left(\\begin{array}{l}{x_{1}}\\\\ {x_{2}}\\end{array}\\right)\\neq\\left(\\begin{array}{l}{1}\\\\ {1}\\end{array}\\right)\\right\\}}\\\\ &{=\\mathrm{XOR}\\left(x_{1},x_{2}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Remark D.25. Notice that the function $\\mathrm{Id}:\\{0,1\\}\\rightarrow\\{0,1\\}$ defined as $\\mathrm{Id}\\left(0\\right)=0$ and $\\operatorname{Id}\\left(1\\right)=1$ can be implemented using any depth $L$ network with a single input dimension and $c_{\\mathrm{Id}}\\cdot L$ parameters. ", "page_idx": 39}, {"type": "text", "text": "Following this remark, for simplicity we shall assume that $h_{1}$ and $h_{2}$ in the following Lemma are of the same depth, as they can be elongated with $O\\left(L\\right)$ additional parameters, which are negligible in the subsequent analysis. ", "page_idx": 39}, {"type": "text", "text": "Recall Lemma 3.3. Let $h_{1},h_{2}$ be two binary networks with depths $L_{1}\\leq L_{2}$ and widths $\\underline{d}^{(1)},\\underline{d}^{(2)}$ , respectively. Then, there exists a network $h$ with depth $L_{\\mathrm{XOR}}\\triangleq L_{2}+2$ and widths ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{d_{\\mathrm{XOR}}}{d_{\\mathrm{XOR}}}\\triangleq\\left(d_{1}^{(1)}+d_{1}^{(2)},\\,\\cdot\\,.\\,.\\,,\\,d_{L_{1}}^{(1)}+d_{L_{1}}^{(2)},\\,d_{L_{1}+1}^{(2)}+1,\\,.\\,.\\,.\\,,\\,d_{L_{2}}^{(2)}+1,\\,2,\\,1\\right)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "such that for all inputs $\\mathbf{x}\\in\\{0,1\\}^{d_{0}},h\\,(\\mathbf{x})=h_{1}\\,(\\mathbf{x})\\oplus h_{2}\\,(\\mathbf{x}).$ ", "page_idx": 40}, {"type": "text", "text": "The lemma above is given immediately by the lemma we state and prove next. ", "page_idx": 40}, {"type": "text", "text": "Lemma D.26 (XOR of Two NNs). Let $h_{1},h_{2}:\\mathcal{X}\\rightarrow\\{0,1\\}$ be quantized fully connected binary threshold networks with depths $L^{\\prime}$ and widths $\\underline{d}^{(1)},\\underline{d}^{(2)}$ , respectively. Let $L\\ge2+L^{\\prime}$ and $\\underline{{d}}\\geq\\underline{{d}}_{\\mathrm{XOR}}$ . Let $\\Theta^{B T N}\\left(\\underline{{d}};h_{1},h_{2}\\right)$ be the subset of $\\Theta^{B T N}\\left(\\underline{{d}}\\right)$ such that for all $\\pmb{\\theta}\\,\\in\\,\\Theta^{B T N}\\,(\\underline{{d}};h_{1},h_{2}),$ , $\\pmb{\\theta}$ has the following form: ", "page_idx": 40}, {"type": "text", "text": "\u2022 For $l=1$ : ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbf{W}_{1}=\\left(\\begin{array}{c}{\\mathbf{W}_{1}^{(1)}}\\\\ {\\mathbf{W}_{1}^{(2)}}\\\\ {\\hat{\\mathbf{W}}_{1}}\\end{array}\\right),\\;\\mathbf{b}_{1}=\\left(\\begin{array}{c}{\\mathbf{b}_{1}^{(1)}}\\\\ {\\mathbf{b}_{1}^{(2)}}\\\\ {\\hat{\\mathbf{b}}_{1}}\\end{array}\\right),\\;\\gamma_{1}=\\left(\\begin{array}{c}{\\mathbf{1}_{d_{l}^{(1)}}}\\\\ {\\mathbf{1}_{d_{l}^{(2)}}}\\\\ {\\mathbf{0}_{d_{l}-d_{l}^{(1)}-d_{l}^{(2)}}}\\end{array}\\right)\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "with arbitrary $\\tilde{\\mathbf{W}}_{1},\\tilde{\\mathbf{b}}_{1}$ . ", "page_idx": 40}, {"type": "text", "text": "\u2022 For $l=2,\\ldots,L^{\\prime}$ : ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{W}_{l}=\\left(\\begin{array}{l l l}{\\mathbf{W}_{l}^{(1)}}&{\\mathbf{0}_{d_{i}^{(1)}\\times d_{i-1}^{(2)}}}&{\\tilde{\\mathbf{W}}_{l}^{1}}\\\\ {0_{d_{i}^{(2)}\\times d_{i-1}^{(1)}}}&{\\mathbf{W}_{l}^{(2)}}&{\\tilde{\\mathbf{W}}_{l}^{2}}\\\\ {\\tilde{\\mathbf{W}}_{l}^{(3)}}&{\\tilde{\\mathbf{W}}_{l}^{4}}&{\\tilde{\\mathbf{W}}_{l}^{5}}\\end{array}\\right)\\in\\{0,1\\}^{d_{l}\\times d_{l-1}}\\,,}\\\\ &{\\mathbf{b}_{l}=\\left(\\begin{array}{l}{\\mathbf{b}_{l}^{(1)}}\\\\ {\\mathbf{b}_{l}^{(2)}}\\\\ {\\mathbf{b}_{l}}\\end{array}\\right)\\in\\{-d_{l-1},\\ldots,-1,0,1,\\ldots,d_{l-1}-1\\}^{d_{l}}\\,,}\\\\ &{\\gamma_{l}=\\left(\\begin{array}{l}{\\mathbf{1}_{d_{i}^{(1)}}}\\\\ {\\mathbf{1}_{d_{i}^{(2)}}}\\\\ {\\mathbf{0}_{d_{i-}d_{i}^{(1)}-d_{i}^{(2)}}}\\end{array}\\right)\\in\\{0,1\\}^{d_{l}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "with arbitrary $\\tilde{\\mathbf{W}}_{l}^{1},\\tilde{\\mathbf{W}}_{l}^{2},\\tilde{\\mathbf{W}}_{l}^{3},\\tilde{\\mathbf{W}}_{l}^{4},\\tilde{\\mathbf{W}}_{l}^{5},\\tilde{\\mathbf{b}}_{l}$ ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{W}_{l}=\\left(\\begin{array}{l l}{\\mathbf{W}_{k}^{\\mathrm{XOR}}}&{\\tilde{\\mathbf{W}}_{l}^{1}}\\\\ {\\hat{\\mathbf{W}}_{l}^{2}}&{\\tilde{\\mathbf{W}}_{l}^{3}}\\end{array}\\right)\\in\\left\\{0,1\\right\\}^{d_{l}\\times d_{l-1}},}\\\\ &{\\mathbf{b}_{l}=\\left(\\begin{array}{l}{\\mathbf{b}_{k}^{\\mathrm{XOR}}}\\\\ {\\hat{\\mathbf{b}}_{l}}\\end{array}\\right)\\in\\left\\{-d_{l-1},\\ldots,-1,0,1,\\ldots,d_{l-1}-1\\right\\}^{d_{l}},\\gamma_{l}=\\left(\\begin{array}{l}{\\gamma_{k}^{\\mathrm{XOR}}}\\\\ {\\mathbf{0}}\\end{array}\\right)\\in\\left\\{0,\\pm1\\right\\}^{d_{l}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "\u2022 And for $l>L^{\\prime}+2$ : ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{W}_{l}=\\left(\\begin{array}{l l}{\\mathbf{W}^{I d}}&{\\tilde{\\mathbf{W}}_{l}^{1}}\\\\ {\\tilde{\\mathbf{W}}_{l}^{2}}&{\\tilde{\\mathbf{W}}_{l}^{3}}\\end{array}\\right)\\in\\left\\{0,1\\right\\}^{d_{l}\\times d_{l-1}},}\\\\ &{\\mathbf{b}_{l}=\\left(\\begin{array}{l}{\\mathbf{b}^{I d}}\\\\ {\\tilde{\\mathbf{b}}_{l}}\\end{array}\\right)\\in\\left\\{-d_{l-1},\\ldots,-1,0,1,\\ldots,d_{l-1}-1\\right\\}^{d_{l}},\\gamma_{l}=\\left(\\begin{array}{l}{\\gamma^{I d}}\\\\ {\\mathbf{0}}\\end{array}\\right)\\in\\left\\{0,\\pm1\\right\\}^{d_{l}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Then for all $\\pmb{\\theta}\\in\\Theta^{B T N}\\left(\\underline{{d}};h_{1},h_{2}\\right)h_{\\pmb{\\theta}}=h_{1}\\oplus h_{2}$ . ", "page_idx": 40}, {"type": "text", "text": "An illustration of this construction is given in Figure 3. ", "page_idx": 40}, {"type": "text", "text": "Proof. We prove the claim by induction. For $l=1$ we have $d_{0}=d_{0}^{(1)}=d_{0}^{(2)}$ and ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{\\theta}^{(1)}\\left(\\mathbf{x}\\right)=\\gamma_{1}\\odot\\sigma\\left(\\mathbf{W}_{1}h_{\\theta}^{(0)}\\left(\\mathbf{x}\\right)+\\mathbf{b}_{1}\\right)}\\\\ &{\\hphantom{=}\\left(\\begin{array}{c}{\\mathbf{1}_{d_{1}^{*}}}\\\\ {\\mathbf{1}_{d_{1}^{'}}}\\\\ {\\mathbf{0}_{d_{1}-d_{1}^{*}-d_{1}^{'}}}\\end{array}\\right)\\odot\\sigma\\left(\\left(\\begin{array}{c}{\\mathbf{W}_{1}^{(1)}}\\\\ {\\mathbf{W}_{1}^{(2)}}\\\\ {\\hat{\\mathbf{W}}_{1}^{1}}\\end{array}\\right)\\mathbf{x}+\\left(\\begin{array}{c}{\\mathbf{b}_{1}^{(1)}}\\\\ {\\mathbf{b}_{1}^{(2)}}\\\\ {\\hat{\\mathbf{b}}_{1}}\\end{array}\\right)\\right)}\\\\ &{\\hphantom{=}\\left(\\begin{array}{c}{\\sigma\\left(\\mathbf{W}_{1}^{(1)}\\mathbf{x}+\\mathbf{b}_{1}^{(1)}\\right)}\\\\ {\\sigma\\left(\\mathbf{W}_{1}^{(2)}\\mathbf{x}+\\mathbf{b}_{1}^{(2)}\\right)}\\\\ {\\mathbf{0}_{d_{1}-d_{1}^{'}-d_{1}^{'}}}\\end{array}\\right)=\\left(\\begin{array}{c}{h_{1}^{(1)}\\left(\\mathbf{x}\\right)}\\\\ {h_{2}^{(1)}\\left(\\mathbf{x}\\right)}\\\\ {\\mathbf{0}_{d_{1}-d_{1}^{*}-d_{1}^{'}}}\\end{array}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Assume that for some $l\\leq L^{\\prime}$ we have ", "page_idx": 41}, {"type": "equation", "text": "$$\nh_{\\theta}^{\\left(l-1\\right)}\\left(\\mathbf{x}\\right)=\\left(\\begin{array}{l}{h_{1}^{\\left(l-1\\right)}\\left(\\mathbf{x}\\right)}\\\\ {h_{2}^{\\left(l-1\\right)}\\left(\\mathbf{x}\\right)}\\\\ {\\mathbf{0}_{d_{l}-d_{l}^{\\star}-d_{l}^{f}}}\\end{array}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Then, ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{\\Phi}_{\\theta}^{(l)}\\left(\\mathbf{x}\\right)=\\gamma_{l}\\odot\\sigma\\left(\\mathbf{W}_{l}h_{\\theta}^{(l-1)}\\left(\\mathbf{x}\\right)+\\mathbf{b}_{l}\\right)}\\\\ &{=\\left(\\begin{array}{l}{\\mathbf{1}_{d_{l}^{(1)}}}\\\\ {\\mathbf{1}_{d_{l}^{(2)}}}\\\\ {\\mathbf{0}_{d_{l}-d_{l}^{(1)}-d_{l}^{(2)}}}\\end{array}\\right)\\odot\\sigma\\left(\\left(\\begin{array}{l l l}{\\mathbf{W}_{l}^{(1)}}&{\\mathbf{0}_{d_{l}^{(1)}\\times d_{l-1}^{(2)}}}&{\\tilde{\\mathbf{W}}_{l}^{1}}\\\\ {\\mathbf{0}_{d_{l}^{(2)}\\times d_{l-1}^{(1)}}}&{\\mathbf{W}_{l}^{(2)}}&{\\tilde{\\mathbf{W}}_{l}^{2}}\\\\ {\\tilde{\\mathbf{W}}_{l}^{3}}&{\\tilde{\\mathbf{W}}_{l}^{4}}&{\\tilde{\\mathbf{W}}_{l}^{5}}\\end{array}\\right)\\left(\\begin{array}{l}{h_{1}^{(l-1)}\\left(\\mathbf{x}\\right)}\\\\ {h_{2}^{(l-1)}\\left(\\mathbf{x}\\right)}\\\\ {\\mathbf{0}_{d_{l}-d_{l}^{(1)}-d_{l}^{(2)}}}\\end{array}\\right)+\\left(\\begin{array}{l}{\\mathbf{b}_{l}^{(1)}}\\\\ {\\mathbf{b}_{l}^{(2)}}\\\\ {\\mathbf{b}_{l}}\\end{array}\\right)\\right)}\\\\ &{=\\left(\\begin{array}{l}{\\sigma\\left(\\mathbf{W}_{l}^{(1)}h_{1}^{(l-1)}\\left(\\mathbf{x}\\right)+\\mathbf{b}_{l}^{(1)}\\right)}\\\\ {\\sigma\\left(\\mathbf{W}_{l}^{(2)}h_{2}^{(l-1)}\\left(\\mathbf{x}\\right)+\\mathbf{b}_{l}^{(2)}\\right)}\\\\ {\\mathbf{0}_{d_{l}-d_{l}^{(1)}-d_{l}^{(2)}}}\\end{array}\\right)=\\left(\\begin{array}{l}{h_{1}^{(l)}\\left(\\mathbf{x}\\right)}\\\\ {h_{2}^{(l)}\\left(\\mathbf{x}\\right)}\\\\ {\\mathbf{0}_{d_{l}-d_{l \n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "It is left to show that the claim holds for $l>L^{\\prime}$ . By the previous steps, $h_{\\theta}^{\\left(L^{\\prime}\\right)}\\left(\\mathbf{x}\\right)=\\left(\\begin{array}{c}{h_{1}\\left(\\mathbf{x}\\right)}\\\\ {h_{2}\\left(\\mathbf{x}\\right)}\\\\ {\\mathbf{0}_{d_{L^{\\prime}}-2}}\\end{array}\\right).$ Under the assumptions on ${\\bf W}_{L^{\\prime}+k},{\\bf b}_{L^{\\prime}+k}$ and $\\gamma_{L^{\\prime}+k}$ , $k=1,2$ it holds that ", "page_idx": 41}, {"type": "equation", "text": "$$\nh_{\\theta}^{\\left(L^{\\prime}+2\\right)}\\left(\\mathbf{x}\\right)=\\left(\\begin{array}{c}{h_{1}\\left(\\mathbf{x}\\right)\\oplus h_{2}\\left(\\mathbf{x}\\right)}\\\\ {\\mathbf{0}_{d_{L^{\\prime}}-1}}\\end{array}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Under the assumptions on layers $l>L^{\\prime}+2$ , ", "page_idx": 41}, {"type": "equation", "text": "$$\nh_{\\theta}^{(l)}\\left(\\mathbf{x}\\right)=\\left(\\begin{array}{c}{h_{1}\\left(\\mathbf{x}\\right)\\oplus h_{2}\\left(\\mathbf{x}\\right)}\\\\ {\\mathbf{0}_{d_{l}-1}}\\end{array}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "In particular, assuming that $d_{L}=1,h_{\\theta}\\left(\\mathbf{x}\\right)=h_{1}\\left(\\mathbf{x}\\right)\\oplus h_{2}\\left(\\mathbf{x}\\right).$ ", "page_idx": 41}, {"type": "text", "text": "Corollary D.27. Let $h_{1},h_{2}$ be networks with depths $L_{1},L_{2}$ and widths $\\underline{d}^{(1)},\\underline{d}^{(2)}$ . Then $h_{1}\\oplus h_{2}$ can be implemented with a network $h$ of depth $L=\\operatorname*{max}\\left\\{L_{1},L_{2}\\right\\}+2$ and widths $\\underline{d}$ such that ", "page_idx": 42}, {"type": "equation", "text": "$$\nw\\left(\\underline{{d}}\\right)\\leq w\\left(\\underline{{d}}^{\\left(1\\right)}\\right)+w\\left(\\underline{{d}}^{\\left(2\\right)}\\right)+2\\underline{{d}}_{\\operatorname*{max}}^{\\left(2\\right)}\\cdot n\\left(\\underline{{d}}^{\\left(1\\right)}\\right)+O(1)\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "and ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underline{{d}}_{\\mathrm{max}}\\leq\\underline{{d}}_{\\mathrm{max}}^{(1)}+\\underline{{d}}_{\\mathrm{max}}^{(2)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof. Following D.25 we assume shall assume that $L_{1}=L_{2}=L$ . We know from D.26 that there exists a network $h$ with dimensions $\\underline{d}=\\left(\\underline{d}^{(1)}+\\underline{d}^{(2)},2,1\\right)$ such that $h=h_{1}\\oplus h_{2}$ . Therefore ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w\\left(d\\right)=\\left(d_{1}^{(1)}+d_{1}^{(2)}\\right)d_{0}+\\displaystyle\\sum_{l=2}^{L}\\left(d_{l}^{(1)}+d_{l}^{(2)}\\right)\\left(d_{l-1}^{(1)}+d_{l-1}^{(2)}\\right)+{\\cal O}(1)}\\\\ &{\\qquad=d_{1}^{(1)}d_{0}+\\displaystyle\\sum_{l=2}^{L}d_{l}^{(1)}d_{l-1}^{(1)}+d_{l}^{(2)}d_{0}+\\displaystyle\\sum_{l=2}^{L}d_{l}^{(2)}d_{l-1}^{(2)}+\\displaystyle\\sum_{l=2}^{L}\\left[d_{l}^{(1)}d_{l-1}^{(2)}+d_{l}^{(2)}d_{l-1}^{(1)}\\right]+{\\cal O}(1)}\\\\ &{\\qquad=w\\left(d^{(1)}\\right)+w\\left(d^{(2)}\\right)+\\displaystyle\\sum_{l=2}^{L}\\left[d_{l}^{(1)}d_{l-1}^{(2)}+d_{l}^{(2)}d_{l-1}^{(1)}\\right]+{\\cal O}(1)}\\\\ &{\\qquad\\leq w\\left(d^{(1)}\\right)+w\\left(d^{(2)}\\right)+\\displaystyle\\sum_{l=2}^{L}\\left[d_{l}^{(1)}d_{0}^{(2)}+d_{l-1}^{(2)}d_{l-1}^{(1)}\\right]+{\\cal O}(1)}\\\\ &{\\qquad=w\\left(d^{(1)}\\right)+w\\left(d^{(2)}\\right)+\\displaystyle\\sum_{l=0}^{L}\\sum_{l=2}^{L}\\left[d_{l}^{(1)}+d_{l-1}^{(1)}\\right]+{\\cal O}(1)}\\\\ &{\\qquad\\leq w\\left(d^{(1)}\\right)+w\\left(d^{(2)}\\right)+2d_{0}^{(2)}\\displaystyle\\sum_{l=2}^{L}\\left(d_{l}^{(1)}+d_{l-1}^{(2)}\\right)+{\\cal O}(1)}\\\\ &{\\qquad\\leq w\\left(d^{(1)}\\right)+w\\left(d^{(2)}\\right)+2d_{0}^{(2)}\\alpha_{0}\\cdot\\left(d^{(1)}\\right)+{\\cal O}(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "In addition, $\\underline{{d}}_{\\mathrm{max}}\\leq\\underline{{d}}_{\\mathrm{max}}^{(1)}+\\underline{{d}}_{\\mathrm{max}}^{(2)}$ and $n\\left(\\underline{{{d}}}\\right)=n\\left(\\underline{{{d}}}^{(1)}\\right)+n\\left(\\underline{{{d}}}^{(2)}\\right).$ ", "page_idx": 42}, {"type": "text", "text": "Recall Corollary 3.4. For any teacher $h^{\\star}$ of depth $L^{\\star}$ and dimensions $\\underline{d}^{\\star}$ and any consistent training set $S$ generated from it, there exists an interpolating network $h$ (i.e., $\\begin{array}{r}{\\mathcal{L}_{S}\\left(h\\right)=0,}\\end{array}$ ) of depth $L=\\operatorname*{max}{\\{L^{\\star},14\\}}+2$ and dimensions $\\underline{d}_{\\cdot}$ , such that the number of edges is ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w\\left(\\underline{d}\\right)\\leq w\\left(\\underline{d}^{\\star}\\right)+N\\cdot H\\left(\\mathcal{L}_{S}\\left(h^{\\star}\\right)\\right)+2n\\left(\\underline{d}^{\\star}\\right)N^{3/4}H\\left(\\mathcal{L}_{S}\\left(h^{\\star}\\right)\\right)^{3/4}\\mathrm{polylog}N}\\\\ &{\\qquad\\qquad+\\ O\\left(d_{0}\\left(d_{0}+n\\left(\\underline{d}^{\\star}\\right)\\right)\\cdot\\log N\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "and the dimensions are ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underline{{d}}_{\\mathrm{max}}\\leq\\underline{{d}}_{\\mathrm{max}}^{\\star}+N^{3/4}\\cdot H\\left(\\mathcal{L}_{S}\\left(h^{\\star}\\right)\\right)^{3/4}\\cdot\\mathrm{polylog}\\left(N\\right)+O\\left(d_{0}\\cdot\\log\\left(N\\right)\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof. We use Corollary D.27 with $h_{1}\\,=\\,h^{\\star}$ and $h_{2}=\\tilde{h}_{S}$ , the noise memorizing network from Theorem 3.1, to get ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w\\left(\\underline{d}\\right)\\leq w\\left(\\underline{d}^{\\star}\\right)+w\\left(\\tilde{\\underline{d}}_{S}\\right)+2\\tilde{\\underline{d}}_{S,\\operatorname*{max}}\\cdot n\\left(\\underline{d}^{\\star}\\right)+O(1)}\\\\ &{\\qquad\\leq w\\left(\\underline{d}^{\\star}\\right)+\\log\\binom{N}{N_{1}}+\\left(\\log\\binom{N}{N_{1}}\\right)^{3/4}\\cdot\\mathrm{polylog}\\,N+O(\\underline{d}_{0}^{2}\\cdot\\log N)}\\\\ &{\\qquad+\\,2n\\left(\\underline{d}^{\\star}\\right)\\left(\\left(\\log\\binom{N}{N_{1}}\\right)\\right)^{3/4}\\cdot\\mathrm{polylog}\\,N+O(d_{0}\\cdot\\log N)\\right)+O(1)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Using Stirling\u2019s approximation ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\log\\left({N\\atop N_{1}}\\right)=N\\cdot H\\left({\\frac{N_{1}}{N}}\\right)+O\\left(\\log\\left(N\\right)\\right)=N\\cdot H\\left({\\mathcal{L}}_{S}\\left(h^{\\star}\\right)\\right)+O\\left(\\log\\left(N\\right)\\right)\\,.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Therefore ", "text_level": 1, "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w\\left(\\underline{d}\\right)\\leq w\\left(\\underline{d}^{\\star}\\right)+N\\cdot H\\left(\\mathcal{L}_{S}\\left(h^{\\star}\\right)\\right)+O\\left(\\log\\left(N\\right)\\right)+N^{3/4}\\cdot H\\left(\\mathcal{L}_{S}\\left(h^{\\star}\\right)\\right)^{3/4}\\cdot\\mathrm{polylog}\\,N}\\\\ &{\\qquad\\qquad+\\,O\\left(d_{0}^{2}\\cdot\\log N\\right)+2n\\left(\\underline{d}^{\\star}\\right)\\left(N^{3/4}\\cdot\\mathrm{polylog}\\,N+O\\left(d_{0}\\cdot\\log N\\right)\\right)}\\\\ &{\\qquad=w\\left(\\underline{d}^{\\star}\\right)+N\\cdot H\\left(\\mathcal{L}_{S}\\left(h^{\\star}\\right)\\right)+2n\\left(\\underline{d}^{\\star}\\right)N^{3/4}H\\left(\\mathcal{L}_{S}\\left(h^{\\star}\\right)\\right)^{3/4}\\mathrm{polylog}N}\\\\ &{\\qquad+\\,O\\left(d_{0}\\left(d_{0}+n\\left(\\underline{d}^{\\star}\\right)\\right)\\cdot\\log N\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "The bound of $d_{\\mathrm{max}}$ is derived similarly. ", "page_idx": 43}, {"type": "text", "text": "E The label-flip-memorization network\u2019s dependence on the dimension ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "In Theorem 3.1, the wire bound has an $O(d_{0}^{2}\\cdot\\log N)$ term. (Recall that $d_{0}$ is the input dimension and $N$ is the domain size.) In this section, we discuss (a) approaches for improving this term and (b) a lower bound showing that it cannot be significantly improved. ", "page_idx": 44}, {"type": "text", "text": "E.1 Improving the $O(d_{0}^{2}\\cdot\\log N)$ Term ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "The $O(d_{0}^{2}\\cdot\\log N)$ term in Theorem 3.1 can be improved by using the following fact. ", "page_idx": 44}, {"type": "text", "text": "Lemma E.1 (Using a s\u221aign matrix for preprocessing). Let $d_{0}\\in\\mathbb{N},$ , let $\\hat{\\mathcal{X}}\\subseteq\\{0,1\\}^{d_{0}}$ , and let $N=|\\hat{\\mathcal{X}}|$ . There exists $d_{1}=O({\\sqrt{d_{0}}}\\cdot\\log N)\\,$ and there exists a matrix $\\mathbf{W}\\in\\{\\pm1\\}^{d_{1}\\times d_{0}}$ such that the function $C_{0}\\colon\\{0,1\\}^{d_{0}}\\to\\{0,1\\}^{d_{1}}$ defined by $C_{0}(\\mathbf{x})=\\mathbb{I}\\left\\{\\mathbf{W}\\mathbf{x}>0\\right\\}$ is injective on $\\hat{\\chi}$ . ", "page_idx": 44}, {"type": "text", "text": "Proof. Pick $\\mathbf{W}\\in\\{\\pm1\\}^{d_{1}\\times d_{0}}$ uniformly at random. We will show that there is a nonzero chance that $C_{0}$ is injective on $\\hat{\\chi}$ . ", "page_idx": 44}, {"type": "text", "text": "Let $\\mathbf{x},\\mathbf{x}^{\\prime}$ be any two distinct points in $\\hat{\\chi}$ . Consider a single row $\\mathbf{W}_{i}$ of $\\mathbf{W}$ . Let $E$ be the good event that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathbf{W}_{i}\\cdot(\\mathbf{x}\\odot\\mathbf{x}^{\\prime})\\in\\{0,1\\}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Then $\\mathrm{Pr}[E]\\geq\\Omega(1/\\sqrt{d_{0}})$ , because we are taking a simple one-dimensional random walk of length at most $d_{0}$ . Conditioned on $E$ , there is an $\\Omega(1)$ chance that $\\mathbb{I}\\left\\{\\mathbf{W}_{i}\\cdot\\mathbf{x}>0\\right\\}\\neq\\mathbb{I}\\left\\{\\mathbf{W}_{i}\\cdot\\mathbf{x}^{\\prime}>\\bar{0}\\right\\},$ because we are taking two independent one-dimensional random walks starting from either 0 or $1$ , at least one of which has nonzero length, and asking whether they land on\u221a the same side of $1/2$ . Therefore, unconditionally, $\\operatorname*{Pr}[\\mathbb{I}\\left\\{\\mathbf{W}_{i}\\cdot\\mathbf{x}>0\\right\\}\\neq\\mathbb{I}\\left\\{\\mathbf{W}_{i}\\cdot\\mathbf{x}^{\\prime}>0\\right\\}]\\ge\\Omega(1/\\sqrt{d_{0}})$ . Consequently, by independence, ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathrm{Pr}[C_{0}(\\mathbf{x})=C_{0}(\\mathbf{x}^{\\prime})]\\leq(1-\\Omega(1/\\sqrt{d_{0}}))^{d_{1}}<1/N^{2},\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "provided we choose a suitable value $d_{1}=O(\\sqrt{d_{0}}\\cdot\\log N)$ . By the union bound over all pairs $\\mathbf{x},\\mathbf{x}^{\\prime}$ , it follows that there is a nonzero chance that $C_{0}$ is injective on $\\hat{\\chi}$ . \u53e3 ", "page_idx": 44}, {"type": "text", "text": "There are two approaches to using Lemma E.1 for the sake of improving the $O(d_{0}^{2}\\cdot\\log N)$ term in Theorem 3.1. ", "page_idx": 44}, {"type": "text", "text": "\u2022 One approach would be to start with a trivial layer that copies the input $\\mathbf{x}\\in\\{0,1\\}^{d_{0}}$ as well as computing all the negations of the bits of $\\mathbf{x}$ ; then we have a layer that applies the function $C_{0}$ from Lemma E.1 (using negated variables to implement $-1$ weights); and then we continue with the network of Theorem 3.1. The net effect is that the depth has increased by two (so the network now has depth 16 instead of 14), and in the weights bound, the $O(d_{0}^{2}\\cdot\\log{N})$ term has been slightly improved to $O(d_{0}^{2}+d_{0}^{3/2}\\cdot\\log N+d_{0}\\cdot\\log^{3}N)$ .   \n\u2022 A second approach would be to change the model. If we permit ternary edge weights (i.e., weights in the set $\\{-1,0,1\\})$ , then the function $C_{0}$ of Lemma E.1 can be implemented as the very first layer of our network, and then we can continue with the network of Theorem 3.1. Note that we need ternary edge weights only in the first layer; the edge weights in all subsequent layers are binary. The benefit of this approach is in the weights bound, the $O(d_{0}^{2}\\cdot\\log\\dot{N})$ term of Theorem 3.1 would be improved to $O(d_{0}^{3/2}\\cdot\\log N+d_{0}\\cdot\\log^{3}N)$ . ", "page_idx": 44}, {"type": "text", "text": "E.2 A $d_{0}^{2}$ Lower Bound on the Number of Weights ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "We now show that the $O(d_{0}^{2}\\cdot\\log N)$ term in Theorem 3.1 cannot be improved to something better than $d_{0}^{2}$ , if we insist on using the \u201cbinary threshold network\u201d model. The argument is elementary. ", "page_idx": 45}, {"type": "text", "text": "Proposition E.2 ( $\\dot{d}_{0}^{2}$ wire lower bound). For every $d_{0}\\in\\mathbb{N}$ , there exists a partial Boolean function $f\\colon\\{0,1\\}^{d_{0}}\\to\\{0,1,\\star\\}$ , defined on a domain $\\hat{\\chi}$ of size $d_{0}+1$ , such that for every binary threshold network ${\\tilde{h}},\\,i f{\\tilde{h}}$ agrees with $f$ everywhere in its domain and $\\underline{d}$ is the widths of $\\tilde{h},$ then $w\\left(\\underline{{d}}\\right)\\geq d_{0}^{2}$ . ", "page_idx": 45}, {"type": "text", "text": "Proof. For each $i\\in\\{0,1,\\ldots,d_{0}\\}$ , let $\\mathbf{x}^{(i)}$ be the vector consisting of $i$ zeroes followed by $d_{0}-i$ ones. Let $\\hat{\\mathcal{X}}=\\{\\mathbf{x}^{(i)}:0\\leq i\\leq d_{0}\\}$ , and let ", "page_idx": 45}, {"type": "equation", "text": "$$\nf(\\mathbf{x})=\\left\\{\\Gamma^{\\mathsf{P A R I T V}(\\mathbf{x})}\\quad{\\mathrm{if~}}\\mathbf{x}\\in{\\hat{\\mathcal{X}}}\\right.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "For the analysis, let $\\tilde{h}$ be a fully connected binary threshold network that agrees with $f$ on all points in $\\hat{\\mathcal X}$ . Consider the layer immediately following the input layer. Each node $g$ in this layer computes either a monotone Boolean function or an anti-monotone Boolean function of the input variables. Therefore, there is at most one value $i\\in\\{1,2,\\ldots,d_{0}\\}$ such that $g(\\mathbf{x}^{(i-1)})\\neq g(\\mathbf{x}^{(i)})$ . On the other hand, for every $i\\in\\{1,2,\\ldots,d_{0}\\}$ , we have $\\tilde{h}(\\mathbf{x}^{(i-1)})\\neq\\tilde{h}(\\mathbf{x}^{(i)})$ , and hence there must be at least one node $g$ in this layer such that $g(\\mathbf{x}^{(i-1)})\\neq g(\\mathbf{x}^{(i)})$ . Therefore, there are at least $d_{0}$ many nodes $g$ . ", "page_idx": 45}, {"type": "text", "text": "Thus, the first two layers of $\\tilde{h}$ both have widths of at least $d_{0}$ , demonstrating that $\\tilde{h}$ has at least $d_{0}^{2}$ many weights. ", "page_idx": 45}, {"type": "text", "text": "F Generalization results (Proofs for Section 4) ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Denote by $\\mathcal{H}_{\\underline{{d}}}^{\\mathrm{BTN}}$ the set of functions representable as binary threshold networks with dimensions $\\underline{d}$ (given a fixed depth $L$ ). We start by bounding the cardinality $\\left|\\mathcal{H}_{\\underline{{d}}}^{\\mathrm{BTN}}\\right|$ in terms of the number of edges $w\\left(\\underline{{d}}\\right)$ . ", "page_idx": 46}, {"type": "text", "text": "Lemma F.1. Let d be the dimensions of a binary threshold network with $w\\triangleq w\\left(\\underline{{d}}\\right)$ edges. Then there are $2^{w+O\\left({\\sqrt{w}}\\log\\left(w\\right)\\right)}$ functions representable as networks with dimensions $\\underline{d},$ . ", "page_idx": 46}, {"type": "text", "text": "Proof. We bound the number of function representable as binary threshold networks with dimensions $\\underline{d}$ having $w$ edges by suggesting a way to encode them, and then bounding the number of bits in the encoding. First, permute each layer so the neurons are sorted by the bias and neuron scaling terms $(b_{l i},\\gamma_{l i})$ . As NNs are invariant to permutations, this does not change the function. Now, at each layer we encode the bias term based on one of two encodings. ", "page_idx": 46}, {"type": "text", "text": "\u2022 If $d_{l}\\,<\\,d_{l-1}$ , then list each of the bias terms as a number with $O\\left(\\log\\left(d_{l-1}\\right)\\right)$ bits plus 2 bits for the scaling term for a total of $O\\left(d_{l}\\left(\\log\\left(d_{l-1}\\right)+2\\right)\\right)\\le O\\left(\\sqrt{d_{l}d_{l-1}}\\log\\left(d_{l-1}\\right)\\right)$ , where the inequality is due to $d_{l}<d_{l-1}$ . \u2022 If $d_{l}\\geq d_{l-1}$ , then we encode the bias and scaling terms by listing the number of times each pair $(b_{l i},\\gamma_{l i})\\in\\{-d_{l-1},\\ldots,d_{l-1}-1\\}\\times\\{-1,0,1\\}$ appears in $(\\mathbf{b}_{l},\\gamma_{l})$ (recall that the neurons are ordered according to these pairs). Each pair can appear at most $d_{l}$ times and so requires $O\\left(\\log\\left(d_{l}\\right)\\right)$ bits to encode for a total of $O\\left(6d_{l-1}\\log\\left(d_{l}\\right)\\right)=O\\left(d_{l-1}\\log\\left(d_{l}\\right)\\right)\\le O\\left(\\sqrt{d_{l}d_{l-1}}\\log\\left(d_{l}d_{l-1}\\right)\\right)$ . ", "page_idx": 46}, {"type": "text", "text": "By encoding each weight with a single bit, this means that for all layers, we can encode the weights, biases and scaling terms using $d_{l}d_{l-1}+O\\left(\\sqrt{d_{l}d_{l-1}}\\log\\left(d_{l}d_{l-1}\\right)\\right)$ bits for a total of ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{L}d_{t}d_{t-1}+O\\left(\\sqrt{d_{t}d_{t-1}}\\log\\left(d_{t}d_{t-1}\\right)\\right)=w+O\\left(\\sum_{t=1}^{L}\\sqrt{d_{t}d_{t-1}}\\log\\left(d_{t}d_{t-1}\\right)\\right)}\\\\ &{\\quad\\le w+O\\left(\\sum_{t=1}^{L}\\sqrt{d_{t}d_{t-1}}\\log\\left(\\sum_{l=1}^{L}d_{t-1}\\right)\\right)}\\\\ &{\\quad\\le w+O\\left(\\sum_{t=1}^{L}\\sqrt{d_{t}d_{t-1}}\\log\\left(w\\right)\\right)=w+O\\left(\\log\\left(w\\right)\\cdot L\\sum_{l=1}^{L}\\frac{1}{L}\\sqrt{d_{l}d_{l-1}}\\right)}\\\\ &{\\mathrm{susenl}\\le w+O\\left(\\log\\left(w\\right)\\cdot L\\sqrt{\\sum_{t=1}^{L}\\frac{1}{L}d_{t}d_{t-1}}\\right)=w+O\\left(\\log\\left(w\\right)\\cdot\\sqrt{L}\\sqrt{\\sum_{t=1}^{L}d_{t}d_{t-1}}\\right)}\\\\ &{\\quad=w+O\\left(\\log\\left(w\\right)\\cdot\\sqrt{L}\\sqrt{w}\\right)}\\\\ &{\\quad=w+O\\left(\\log\\left(w\\right)\\cdot\\sqrt{w}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Corollary F.2. Assuming that the depth $L$ is fixed and known, a bin\u221aary threshold network of depth $L$ with unknown number of weights $w$ , can be encoded with $w+O\\left(\\sqrt{w}\\log\\left(w\\right)\\right)$ bits. ", "page_idx": 46}, {"type": "text", "text": "Proof. After specifying the architecture $\\underline{d}_{\\cdot}$ , from Lemma F.1 we require $w+O\\left(\\sqrt{w}\\log\\left(w\\right)\\right)$ bits. Therefore it remains to bound the length of the encoding of $\\underline{d}$ . We first use $O\\left(\\log\\left(w\\right)\\right)$ bits to encode the number of weights, then, since $\\underline{d}\\in[w]^{L}$ , we only need $O\\left(\\log\\left(w^{L}\\right)\\right)=O\\left(\\log\\left(w\\right)\\right)$ additional bits for a total of $w+O\\left({\\sqrt{w}}\\log\\left(w\\right)\\right)+O\\left(\\log\\left(w\\right)\\right)=w+O\\left({\\sqrt{w}}\\log\\left(w\\right)\\right)$ . \u53e3 ", "page_idx": 46}, {"type": "text", "text": "F.1 Derivation of the min-size generalization bounds (Proofs for Section 4.1) ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Throughout this subsection, we use $A\\left(S\\right)$ to denote the min-size interpolating NN of depth $L$ , $A_{L}\\left(S\\bar{\\right)}$ . ", "page_idx": 47}, {"type": "text", "text": "Lemma F.3. Let $L\\protect\\geq16$ be fixed. Then ", "page_idx": 47}, {"type": "equation", "text": "$$\nI\\left(S;A\\left(S\\right)\\right)\\le w\\left(\\underline{{d}}^{\\star}\\right)+N\\cdot H\\left(\\varepsilon^{\\star}\\right)+O\\left(\\delta\\left(N,d_{0},\\underline{{d}}^{\\star}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\delta\\left(N,d_{0},\\underline{{d}}^{\\star}\\right)=n\\left(\\underline{{d}}^{\\star}\\right)\\cdot N^{3/4}H\\left(\\varepsilon^{\\star}\\right)^{3/4}\\cdot\\mathrm{polylog}\\left(N+n\\left(\\underline{{d}}^{\\star}\\right)+d_{0}\\right)}\\\\ {+\\,d_{0}^{2}\\cdot\\log N+d_{0}n\\left(\\underline{{d}}^{\\star}\\right)\\log\\left(n\\left(\\underline{{d}}^{\\star}\\right)+N+d_{0}\\right)^{3/2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Proof. Using Shannon\u2019s source coding theorem: ", "page_idx": 47}, {"type": "equation", "text": "$$\nI\\left(S;A\\left(S\\right)\\right)\\leq H\\left(A\\left(S\\right)\\right)\\leq\\mathbb{E}\\left|A\\left(S\\right)\\right|\\,,\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where $|A\\left(S\\right)|$ denotes the number of bits in the encoding of $A\\left(S\\right)$ . Following Corollary 3.4, for a consistent $S$ , $A\\left(S\\right)$ is a network with fixed depth and at most ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{w\\stackrel{\\Delta}{=}w\\left(\\underline{{d}}^{\\star}\\right)+N\\cdot H\\left(\\mathcal{L}_{S}\\left(h^{\\star}\\right)\\right)+2n\\left(\\underline{{d}}^{\\star}\\right)N^{3/4}H\\left(\\mathcal{L}_{S}\\left(h^{\\star}\\right)\\right)^{3/4}\\mathrm{polylog}N}}\\\\ {{+\\cal O}\\left(d_{0}\\left(d_{0}+n\\left(\\underline{{d}}^{\\star}\\right)\\right)\\cdot\\log N\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "weights and therefore, using the result from Corollary F.2 and $\\sqrt{w\\left(\\underline{{d}}^{\\star}\\right)}\\leq d_{0}+n\\left(\\underline{{d}}^{\\star}\\right)$ , ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|A\\left(S\\right)|\\leq w+O\\left(\\sqrt{w}\\log\\left(w\\right)\\right)}\\\\ &{=w\\left(\\underline{{d}}^{\\star}\\right)+N\\cdot H\\left(\\mathcal{L}_{S}\\left(h^{\\star}\\right)\\right)+O\\Big(n\\left(\\underline{{d}}^{\\star}\\right)\\cdot N^{3/4}H\\left(\\mathcal{L}_{S}\\left(h^{\\star}\\right)\\right)^{3/4}\\cdot\\mathrm{polylog}\\left(N+n(\\underline{{d}}^{\\star})+d_{0}\\right)\\Big)}\\\\ &{\\quad+O\\Big(d_{0}^{2}\\cdot\\log N+d_{0}n\\left(\\underline{{d}}^{\\star}\\right)\\log\\left(n\\left(\\underline{{d}}^{\\star}\\right)+N+d_{0}\\right)^{3/2}\\Big)}\\\\ &{=w\\left(\\underline{{d}}^{\\star}\\right)+N\\cdot H\\left(\\mathcal{L}_{S}\\left(h^{\\star}\\right)\\right)+O\\left(\\tilde{\\delta}\\left(N,d_{0},\\underline{{d}}^{\\star}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where we grouped all lower order terms in $\\tilde{\\delta}$ . In case $S$ is inconsistent, $A\\left(S\\right)={\\star}$ so $\\left|A\\left(S\\right)\\right|=O\\left(1\\right)$ . Taking the expected value and using Jensen\u2019s inequality gives ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left|A\\left(S\\right)\\right|=\\mathbb{E}\\left[\\left|A\\left(S\\right)\\right|\\cdot\\mathbb{I}\\left\\{\\mathrm{inconsistent}\\;S\\right\\}\\right]+\\mathbb{E}\\left[\\left|A\\left(S\\right)\\right|\\cdot\\mathbb{I}\\left\\{\\mathrm{consistent}\\;S\\right\\}\\right]}\\\\ &{\\qquad\\qquad\\leq O(1)+\\mathbb{E}\\left[\\underbrace{\\mathbb{I}\\left\\{\\mathrm{f(consistent}\\;S)\\right\\}}_{\\leq1}\\underbrace{\\Big(w\\left(\\underline{{d}}^{\\star}\\right)+N\\cdot H\\left(\\mathcal{L}_{S}\\left(h^{\\star}\\right)\\right)+O\\left(\\tilde{\\delta}\\left(N,d_{0},\\underline{{d}}^{\\star}\\right)\\right)\\Big)}_{\\geq0}\\right]}\\\\ &{\\qquad\\qquad\\leq O(1)+\\mathbb{E}\\left[w\\left(\\underline{{d}}^{\\star}\\right)+N\\cdot H\\left(\\mathcal{L}_{S}\\left(h^{\\star}\\right)\\right)+O\\left(\\tilde{\\delta}\\left(N,d_{0},\\underline{{d}}^{\\star}\\right)\\right)\\right]}\\\\ &{\\qquad\\qquad\\leq w\\left(\\underline{{d}}^{\\star}\\right)+N\\cdot H\\left(\\mathbb{E}\\left[\\mathcal{L}_{S}\\left(h^{\\star}\\right)\\right]\\right)+O\\left(\\delta\\left(N,d_{0},\\underline{{d}}^{\\star}\\right)\\right)}\\\\ &{\\qquad\\qquad=w\\left(\\underline{{d}}^{\\star}\\right)+N\\cdot H\\left(\\varepsilon^{\\star}\\right)+O\\left(\\delta\\left(N,d_{0},\\underline{{d}}^{\\star}\\right)\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "With this result, we are ready to derive the generalization results. ", "page_idx": 48}, {"type": "text", "text": "Recall Theorem 4.2. Consider a distribution $\\mathcal{D}$ induced by a noisy teacher model of depth $L^{\\star}$ and widths $\\underline{d}^{\\star}$ (Assumption 2.4) with a noise level of $\\varepsilon^{\\star}<1/2$ . Let $S\\stackrel{*}{\\sim}\\mathcal{D}^{N}$ be a training set such that $N=o(\\sqrt{1/D_{\\operatorname*{max}}})$ . Then, for any fixed depth $L\\ge\\operatorname*{max}\\left\\{L^{\\star},14\\right\\}+2$ , the generalization error of the min-size depth- $L$ NN interpolator satisfies the following. ", "page_idx": 48}, {"type": "text", "text": "\u2022 Under arbitrary label noise, ", "text_level": 1, "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{S}\\left[\\mathcal{L}_{D}\\left(A\\left(S\\right)\\right)\\right]\\leq1-2^{-H\\left(\\varepsilon^{*}\\right)/\\mathbb{P}_{S}\\left(\\mathrm{consistent}\\;S\\right)}+\\mathbb{P}\\left(\\mathrm{inconsistent}\\;S\\right)+O\\left(C_{\\mathrm{min}}\\left(N,d_{0},\\underline{{d}}^{\\star}\\right)\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "\u2022 Under independent label noise, ", "text_level": 1, "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{E}_{S}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\right]-2\\varepsilon^{\\star}\\left(1\\!-\\!\\varepsilon^{\\star}\\right)\\right|}\\\\ &{\\leq(1-2\\varepsilon^{\\star})\\,\\sqrt{\\frac{O(C_{\\mathrm{min}}\\left(N,d_{0},d^{\\star}\\right))+\\mathbb{P}(\\mathrm{inconsistent}\\,S)}{\\mathbb{P}(\\mathrm{consistent}\\,S)}}+\\frac{\\left(N-1\\right)\\mathcal{D}_{\\mathrm{max}}}{3}+\\mathbb{P}\\left(\\mathrm{inconsistent}\\,S\\right)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where ", "page_idx": 48}, {"type": "equation", "text": "$$\nC_{\\mathrm{min}}\\left(N,d_{0},\\underline{{d}}^{\\star}\\right)=\\frac{w\\left(\\underline{{d}}^{\\star}\\right)+\\delta\\left(N,d_{0},\\underline{{d}}^{\\star}\\right)}{N}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "with $\\delta\\left(N,d_{0},\\underline{{d}}^{\\star}\\right)$ as defined in Lemma F.3. ", "page_idx": 48}, {"type": "text", "text": "Remark F.4. The bound shown in Section 4.1 is found by bounding $\\mathbb{P}$ (inconsistent $S)\\begin{array}{r}{S)\\leq\\frac{1}{2}N^{2}D_{\\operatorname*{max}}}\\end{array}$ as in Lemma B.1. Then using the Taylor approximation with small $N^{2}\\mathcal{D}_{\\mathrm{max}}$ ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1-2^{-\\frac{H\\left(\\varepsilon^{\\star}\\right)}{\\mathbb{P}\\left(\\mathrm{consistent}\\,S\\right)}}\\leq1-2^{-\\frac{H\\left(\\varepsilon^{\\star}\\right)}{1-\\frac{1}{2}N^{2}\\mathcal{D}_{\\mathrm{max}}}}}\\\\ &{\\phantom{1-2^{-\\frac{H\\left(\\varepsilon^{\\star}\\right)}{\\mathbb{P}\\left(\\mathrm{consistent}\\,S\\right)}}}=1-2^{-H\\left(\\varepsilon^{\\star}\\right)\\left(1+O\\left(N^{2}\\mathcal{D}_{\\mathrm{max}}\\right)\\right)}}\\\\ &{\\phantom{1-2^{-H\\left(\\varepsilon^{\\star}\\right)}}=1-2^{-H\\left(\\varepsilon^{\\star}\\right)}\\left(1+O\\left(N^{2}\\mathcal{D}_{\\mathrm{max}}\\right)\\right)}\\\\ &{\\phantom{1-2^{-H\\left(\\varepsilon^{\\star}\\right)}}=1-2^{-H\\left(\\varepsilon^{\\star}\\right)}+O\\left(N^{2}\\mathcal{D}_{\\mathrm{max}}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Lemma B.1 is used similarly to bound the error in the independent noise case. Assuming that $N=\\omega\\left(n\\left(\\underline{{{d}}}^{\\star}\\right)^{4}H\\left(\\varepsilon^{\\star}\\right)^{3}\\mathrm{polylog}\\left(n\\left(\\underline{{{d}}}^{\\star}\\right)\\right)+d_{0}^{2}\\log d_{0}\\right)$ when $\\varepsilon^{\\star}\\ >\\ 0$ we can deduce that $N\\,=$ $\\omega\\left(w\\left(\\underline{{d}}^{\\star}\\right)\\right)$ as well since ", "page_idx": 48}, {"type": "equation", "text": "$$\nw\\left(\\underline{{d}}^{\\star}\\right)\\leq\\left(n\\left(\\underline{{d}}^{\\star}\\right)+d_{0}\\right)^{2}\\leq4\\left(\\operatorname*{max}\\left\\{n\\left(\\underline{{d}}^{\\star}\\right),d_{0}\\right\\}\\right)^{2}\\,.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Together with $N=o\\left(\\sqrt{1/D_{\\mathrm{max}}}\\right)$ we get the desired form of the bounds. Finally, note that when $\\varepsilon^{\\star}=0$ , the convergence rate of $\\tilde{O}\\left(1/N\\right)$ instead of $\\tilde{O}\\left(1/\\sqrt[4]{N}\\right)$ , where $\\tilde{O}$ hides logarithmic terms arising as artifacts of our analysis, and dependence on other parameters such as the input dimension $d_{0}$ . ", "page_idx": 48}, {"type": "text", "text": "Proof. Starting with the bound in the arbitrary noise setting, we combine C.2 with F.3 ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\log\\left(1-{\\mathbb{E}}_{S}\\left[{\\mathcal{L}}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\mid\\mathrm{consistent}\\;S\\right]\\right)\\leq\\frac{I\\left(S;A\\left(S\\right)\\right)}{N\\cdot\\mathbb{P}_{S}\\left(\\mathrm{consistent}\\;S\\right)}}\\\\ &{\\qquad\\leq\\frac{w\\left(\\underline{{d}}^{\\star}\\right)+N\\cdot H\\left(\\varepsilon^{\\star}\\right)+O\\left(\\delta\\left(N,d_{0},\\underline{{d}}^{\\star}\\right)\\right)}{N\\cdot\\mathbb{P}_{S}\\left(\\mathrm{consistent}\\;S\\right)}}\\\\ &{\\qquad=\\frac{1}{\\mathbb{P}_{S}\\left(\\mathrm{consistent}\\;S\\right)}\\cdot\\left(H\\left(\\varepsilon^{\\star}\\right)+O\\left(C_{\\mathrm{min}}\\left(N,d_{0},\\underline{{d}}^{\\star}\\right)\\right)\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Rearranging the above inequality and recalling Remark C.1, we have, ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{S}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\mid\\mathrm{consistent}\\;S\\right]\\leq1-2^{-\\frac{H\\left(\\varepsilon^{\\star}\\right)}{\\mathbb{P}_{S}\\left(\\mathrm{consistent}\\;S\\right)}-O\\left(C_{\\mathrm{min}}\\left(N,d_{0},\\underline{{d}}^{\\star}\\right)\\right)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Then, using Lemma A.6, we get, ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{S}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\mid\\mathrm{consistent}\\;S\\right]\\leq1-2^{-\\frac{H\\left(\\varepsilon^{\\star}\\right)}{\\mathbb{P}_{S}\\left(\\mathrm{consistent}\\;S\\right)}}+O\\left(C_{\\mathrm{min}}\\left(N,d_{0},\\underline{{d}}^{\\star}\\right)\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "The bound is derived using the following observation. Since for a RV $X$ in $[0,1]$ and a binary RV $Y$ we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\operatorname{\\mathbb{E}}[X]=\\operatorname{\\mathbb{E}}[X\\mid Y]\\underbrace{\\operatorname{\\mathbb{P}}(Y)}_{\\leq1}+\\underbrace{\\operatorname{\\mathbb{E}}[X\\mid\\neg Y]}_{\\leq1}\\operatorname{\\mathbb{P}}(\\neg Y)\\leq\\operatorname{\\mathbb{E}}[X\\mid Y]+\\operatorname{\\mathbb{P}}[\\neg Y]\\,,\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "we conclude the proof as ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{S}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\right]\\le\\mathbb{E}_{S}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\mid\\mathrm{\\consistent{\\}}S\\right]+\\mathbb{P}\\left(\\mathrm{inconsistent{\\}}S\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "For the independent noise setting, we combine Lemma C.3 and Lemma F.3 to get ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{E}_{S}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\mid\\mathrm{consistent}\\;S\\right]-2\\varepsilon^{\\star}\\left(1-\\varepsilon^{\\star}\\right)\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\left(1-2\\varepsilon^{\\star}\\right)O\\left(\\sqrt{C\\left(N\\right)}\\right)+\\frac{\\left(N-1\\right)\\mathcal{D}_{\\operatorname*{max}}}{3}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C\\left(N\\right)=\\frac{I\\left(S;A\\left(S\\right)\\right)-N\\cdot\\left(H\\left(\\varepsilon^{\\star}\\right)-\\mathbb{P}\\left(\\mathrm{inconsistent\\;}S\\right)\\right)}{N\\left(1-\\mathbb{P}\\left(\\mathrm{inconsistent\\;}S\\right)\\right)}}\\\\ &{\\quad\\leq\\frac{w\\left(d^{\\star}\\right)+N\\cdot H\\left(\\varepsilon^{\\star}\\right)+O\\left(\\delta\\left(N,d_{0},d^{\\star}\\right)\\right)-N\\cdot\\left(H\\left(\\varepsilon^{\\star}\\right)-\\mathbb{P}\\left(\\mathrm{inconsistent\\;}S\\right)\\right)}{N\\left(1-\\mathbb{P}\\left(\\mathrm{inconsistent\\;}S\\right)\\right)}}\\\\ &{\\quad=\\frac{O\\left(\\frac{w\\left(d^{\\star}\\right)+\\delta\\left(N,d_{0},d^{\\star}\\right)}{N}\\right)+\\mathbb{P}\\left(\\mathrm{inconsistent\\;}S\\right)}{\\mathbb{P}\\left(\\mathrm{consistent\\;}S\\right)}}\\\\ &{\\quad=\\frac{O\\left(C_{\\mathrm{min}}\\left(N,d_{0},\\underline{{d}}^{\\star}\\right)\\right)+\\mathbb{P}\\left(\\mathrm{inconsistent\\;}S\\right)}{\\mathbb{P}\\left(\\mathrm{consistent\\;}S\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Finally, using the inequality from Lemma C.4, we have, ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\right]-2\\varepsilon^{\\star}\\left(1-\\varepsilon^{\\star}\\right)\\right|}\\\\ &{\\qquad\\qquad\\leq\\left|\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\mid\\mathrm{consistent}\\;S\\right]-2\\varepsilon^{\\star}\\left(1-\\varepsilon^{\\star}\\right)\\right|+\\mathbb{P}(\\mathrm{inconsistent}\\;S)}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "F.2 Derivation of the posterior sampling generalization bounds (Section 4.2) ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Lemma F.5. For the posterior sampling algorithm ", "page_idx": 50}, {"type": "equation", "text": "$$\nI\\left(S;A\\left(S\\right)\\right)\\leq\\mathbb{E}_{S}\\left[\\log\\left(\\frac{1}{p_{S}}\\right)\\middle|\\mathrm{consistent}\\,S\\right]\\mathbb{P}_{S}\\left(\\mathrm{consistent}\\,S\\right)+\\frac{2}{e\\ln2}\\,.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Proof. Recall the definition of the marginal distribution of the algorithm\u2019s output (a hypothesis $h$ ) is ", "page_idx": 50}, {"type": "equation", "text": "$$\nd\\nu\\left(h\\right)=\\sum_{s}d p\\left(s,h\\right)\\,,\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where $s$ are all possible realizations of a (training) sample of size $N$ . ", "page_idx": 50}, {"type": "text", "text": "For $h=\\star$ , we have $d\\nu\\left(\\star\\right)=\\mathbb{P}_{S}$ (inconsistent $S$ ). ", "page_idx": 50}, {"type": "text", "text": "For $h\\neq\\star$ , since $\\mathcal{L}_{s}\\left(h\\right)=0$ implies that $s$ is consistent, we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle d\\nu\\left(h\\right)\\triangleq\\sum_{s}d p\\left(s,h\\right)=\\sum_{s}\\frac{\\mathbb{I}\\left\\{\\mathcal{L}_{s}\\left(h\\right)=0\\right\\}}{p_{s}}d P\\left(h\\right)d\\mathcal{D}^{N}\\left(s\\right)}\\\\ {\\displaystyle}&{=\\sum_{s:p_{s}>0}\\frac{\\mathbb{I}\\left\\{\\mathcal{L}_{s}\\left(h\\right)=0\\right\\}}{p_{s}}d P\\left(h\\right)d\\mathcal{D}^{N}\\left(s\\right)}\\\\ {\\displaystyle}&{=d\\mathcal{P}\\left(h\\right)\\sum_{s:p_{s}>0}\\frac{\\mathbb{I}\\left\\{\\mathcal{L}_{s}\\left(h\\right)=0\\right\\}}{p_{s}}d\\mathcal{D}^{N}\\left(s\\right)}\\\\ {\\displaystyle}&{=d\\mathcal{P}\\left(h\\right)\\mathbb{E}_{S\\sim\\mathcal{D}^{N}}\\left[\\frac{\\mathbb{I}\\left\\{p_{S}>0\\right\\}}{p_{S}}\\mathbb{I}\\left\\{\\mathcal{L}_{S}\\left(h\\right)=0\\right\\}\\right]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where, for ease of notation, we use the convention that $\\begin{array}{r}{\\frac{\\mathbb{I}\\{p_{s}>0\\}}{p_{s}}=0}\\end{array}$ when $p_{s}=0$ . Denoting ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\pi\\left(h\\right)\\triangleq\\mathbb{E}_{S\\sim\\mathcal{D}^{N}}\\left[\\frac{\\mathbb{I}\\left\\{p_{S}>0\\right\\}}{p_{S}}\\mathbb{I}\\left\\{\\mathcal{L}_{S}\\left(h\\right)=0\\right\\}\\right]\\,,\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "we get ", "page_idx": 50}, {"type": "equation", "text": "$$\nd\\nu\\left(h\\right)=d\\mathcal{P}\\left(h\\right)\\pi\\left(h\\right)\\,.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Notice that if there exists some $s\\,\\in\\,\\mathrm{supp}\\left({\\mathcal{D}}^{N}\\right)$ such that $\\mathcal{L}_{s}\\left(h\\right)=0$ then $\\pi\\left(h\\right)>0$ . Using the definition of the mutual information: ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I\\left({S};{A}\\left({S}\\right)\\right)=\\displaystyle\\sum_{s}\\displaystyle\\sum_{h\\in\\mathcal{H}\\left(\\nu\\right)\\cap\\left(\\star\\right)}d p\\left({s},h\\right)\\log\\left(\\frac{d p\\left(s,h\\right)}{d r\\left(h\\right)d r\\left(h\\right)d r\\left(s\\right)}\\right)}\\\\ &{\\quad=\\displaystyle\\sum_{s\\geq\\nu_{s}=0}d p\\left(s,\\star\\right)\\log\\left(\\frac{d p\\left(s,\\star\\right)}{d r\\left(s\\right)d\\mathcal{D}\\left(s\\right)}\\right)+\\sum_{s\\geq\\nu_{s}>0}\\sum_{h\\in\\mathcal{H}\\left(\\star\\right)}d p\\left(s,h\\right)\\log\\left(\\frac{d p\\left(s,h\\right)}{d\\nu\\left(h\\right)d\\mathcal{D}\\left(s\\right)}\\right)}\\\\ &{\\quad=\\displaystyle\\sum_{s\\geq\\nu_{s}=0}d D\\left(s\\right)\\log\\left(\\frac{d D\\left(s\\right)}{\\mathbb{P}_{S}\\left(\\mathrm{in}\\mathrm{consistent}\\,S\\right)d\\mathcal{D}\\left(s\\right)}\\right)+}\\\\ &{\\quad\\quad\\displaystyle\\sum_{s\\geq\\nu_{s}>0}\\sum_{h\\in\\mathcal{L}_{(s)}\\left(\\nu\\right)=0}\\frac{1}{p}d\\mathcal{P}_{s}d\\mathcal{P}\\left(h\\right)d\\mathcal{D}\\left(s\\right)\\log\\left(\\frac{\\frac{1}{p}d\\mathcal{P}\\left(h\\right)d\\mathcal{D}\\left(s\\right)}{d\\mathcal{P}_{s}\\left(h\\right)\\pi\\left(h\\right)d\\mathcal{D}\\left(s\\right)}\\right)}\\\\ &{\\quad=\\displaystyle\\sum_{s\\geq\\nu_{s}=0}d\\mathcal{D}\\left(s\\right)\\log\\left(\\frac{1}{\\mathcal{P}_{s}\\left(\\mathrm{in}\\mathrm{consistent}\\,S\\right)}\\right)+\\sum_{s\\geq\\nu_{s}\\geq0}\\sum_{h\\in\\mathcal{L}_{(s)}\\left(\\nu\\right)=0}^{\\infty}\\frac{1}{p_{s}}d\\mathcal{P}\\left(h\\right)d\\mathcal{D}\\left(s\\right)\\log\\left(\\frac{1}{p_{s}\\pi\\left(h\\right)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Simplifying each term separately, the first sum immediately simplifies to ", "page_idx": 50}, {"type": "equation", "text": "$$\n-\\mathbb{P}_{S}\\left({\\mathrm{inconsistent~}}S\\right)\\log\\left(\\mathbb{P}_{S}\\left({\\mathrm{inconsistent~}}S\\right)\\right)\\leq{\\frac{1}{e\\ln2}}\\,,\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "and ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{n\\leq n\\leq1\\leq n}\\displaystyle\\sum_{i\\in\\{1,2,n\\}}\\frac{1}{n}\\overline{{f}}(\\mathscr{P}(i);n)\\overline{{(n)}}(\\frac{1}{n\\leq\\gamma(n)})}\\\\ &{=-\\displaystyle\\sum_{s\\geq n\\leq1,\\ldots}\\frac{1}{n}\\overline{{f}}(\\mathscr{P}(i))\\overline{{(n)}}(\\{n\\}(n)\\geq\\sum_{i,j\\leq n,\\ldots,j\\leq n}\\displaystyle\\sum_{\\ell\\leq m\\leq n}\\frac{1}{n}\\overline{{f}}(\\rho)|\\mathcal{D}(\\ell)|)\\overline{{(n)}}(\\psi(i)|)}\\\\ &{=-\\displaystyle\\sum_{s\\geq n\\leq1}\\frac{1}{n}\\overline{{f}}(\\mathscr{D}(i))\\overline{{(n)}}(\\sum_{i,j\\leq n,\\ldots,j\\leq n}\\overline{{f}}(\\ell)|)}\\\\ &{\\quad-\\displaystyle\\sum_{s\\geq n\\leq m\\leq n}\\frac{1}{n}\\overline{{f}}(\\sum_{i,j\\leq n}|n)|\\overline{{f}}(i)|)\\overline{{(n)}}(\\chi(i)|)}\\\\ &{=-\\displaystyle\\sum_{s\\geq n\\leq m\\leq n}\\frac{1}{n}\\overline{{f}}(\\sum_{i,j\\leq n}|n)|\\overline{{f}}(i)|\\mathcal{D}(\\ell)||)\\overline{{(n)}}(\\langle n)|}\\\\ &{=-\\displaystyle\\sum_{s\\geq n\\leq m\\leq n}\\frac{1}{n}\\overline{{f}}(\\gamma)|\\mathcal{D}(i)|\\gamma(s)|\\sum_{i,j\\leq n\\leq m\\leq n}\\sum_{s\\geq0}\\log(\\tau|)|\\mathcal{D}(i)|}\\\\ &{\\quad-\\displaystyle\\sum_{s\\geq n\\leq m\\leq n}\\frac{1}{n}\\overline{{f}}(\\gamma)|\\mathcal{D}(i)|}\\\\ &{=-\\displaystyle\\sum_{s\\geq n\\leq m\\leq n}\\frac{\\alpha}{n}\\overline{{f}}(i)|\\mathcal{D}(i)|-\\displaystyle\\sum_{s\\geq n\\leq m\\leq n}\\psi(i)|\\mathcal{D}(i)|}\\\\ &{=-\\displaystyle\\sum_{s\\geq n\\leq m\\leq n}\\sum_{i\\in\\{1,2,n\\}}\\psi_{i}|\\left(\\tau|)|\\mathcal{D}(i)|}\\\\ &{=-\\displaystyle\\sum_{s\\geq n\\leq m\\leq n}(\\gamma)|\\mathcal{D}(s)|-\\displaystyle\\sum_{s\\geq n\\leq m\\leq n}|\\nabla(x)||\\mathcal{D}(i)|}\\\\ &{=\\mathbb{E}_{i}\\left[\\bigg(\\frac{1}{\\gamma}\\bigg)|\\left\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Putting all of this together, ", "page_idx": 51}, {"type": "equation", "text": "$$\nI\\left(S;A\\left(S\\right)\\right)\\leq\\mathbb{E}_{S}\\left[\\log\\left({\\frac{1}{p_{S}}}\\right)\\middle|{\\mathrm{consistent}}\\;S\\right]\\mathbb{P}_{S}\\left({\\mathrm{consistent}}\\;S\\right)+{\\frac{2}{e\\ln2}}\\,.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Corollary F.6. The generalization of posterior sampling satisfies ", "text_level": 1, "page_idx": 52}, {"type": "equation", "text": "$$\n-\\log\\left(1-\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\mid\\mathrm{consistent}\\,S\\right]\\right)\\leq\\frac{\\mathbb{E}_{S}\\left[\\log\\left(\\frac{1}{p_{S}}\\right)\\middle|\\mathrm{consistent}\\,S\\right]+3}{N}\\,.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Proof. Combining Lemma C.2 and Lemma F.5 we get ", "page_idx": 52}, {"type": "equation", "text": "$$\nI\\left(S;A\\left(S\\right)\\right)\\ge-N\\log\\left(1-\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\mid\\mathrm{consistent}\\;S\\right]\\right)\\mathbb{P}_{S}\\left(\\mathrm{consistent}\\;S\\right)\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "and ", "page_idx": 52}, {"type": "equation", "text": "$$\nI\\left(S;A\\left(S\\right)\\right)\\leq\\mathbb{E}_{S}\\left[\\log\\left({\\frac{1}{p_{S}}}\\right)\\left|{\\mathrm{consistent~}}S\\right]\\mathbb{P}_{S}\\left({\\mathrm{consistent~}}S\\right)+{\\frac{2}{e\\ln2}}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "so ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-N\\log\\left(1-{\\mathbb{E}}_{S,A(S)}\\left[{\\mathcal{L}}_{{\\mathcal{D}}}\\left(A\\left(S\\right)\\right)\\mid\\mathrm{consistent}\\;S\\right]\\right)\\mathbb{P}_{S}\\left(\\mathrm{consistent}\\;S\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq{\\mathbb{E}}_{S}\\left[\\log\\left(\\frac{1}{p_{S}}\\right)\\middle|\\mathrm{consistent}\\;S\\right]\\mathbb{P}_{S}\\left(\\mathrm{consistent}\\;S\\right)+\\frac{2}{e\\ln2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "and finally, using $2/e\\ln2\\leq1.5$ and recalling C.1 we get ", "page_idx": 52}, {"type": "equation", "text": "$$\n-\\log\\left(1-\\mathbb{E}_{S,A\\left(S\\right)}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\mid\\mathsf{c o n s i s t e n t}\\;S\\right]\\right)\\leq\\frac{\\mathbb{E}_{S}\\left[\\log\\left(\\frac{1}{p s}\\right)\\middle|\\mathsf{c o n s i s t e n t}\\;S\\right]+3}{N}\\,.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Let $\\bar{h}$ be a network with depth $L$ , dimensions $\\bar{\\underline{d}}$ , and parameters $\\bar{\\pmb{\\theta}}=\\left\\{\\bar{\\mathbf{W}}_{l},\\bar{\\mathbf{b}}_{l},\\bar{\\gamma}_{l}\\right\\}\\in\\Theta^{\\mathrm{BTN}}\\left(\\bar{\\underline{{d}}}\\right)$ . Let $\\underline{{d}}\\geq\\bar{\\underline{{d}}}$ . Similar to $\\Theta^{\\mathrm{BTN}}\\left(\\underline{{d}};h_{1},h_{2}\\right)$ introduced in Lemma D.26, let $\\Theta^{\\mathrm{BTN}}\\left(\\underline{{d}};\\bar{h}\\right)\\subset\\Theta^{\\mathrm{BTN}}\\left(\\underline{{d}}\\right)$ be the set of parameters $\\pmb{\\theta}$ that implement $\\bar{h}$ by setting a subset of the parameters to be equal to $\\bar{\\pmb\\theta}$ , and zero the effect of redundant neurons by setting their bias and neuron scaling terms to be 0. This is illustrated in Figure 4. In particular, in our notation, $\\Theta^{\\mathtt{B T N}}\\left(\\underline{{d}};h_{1},h_{2}\\right)=\\Theta^{\\mathtt{B T N}}\\left(\\underline{{d}};h_{1}\\oplus h_{2}\\right)$ . ", "page_idx": 53}, {"type": "image", "img_path": "QyR1dNDxRP/tmp/085e47e21a8b3ec09a9c4f48053d6e3e0ffac0304146f0144eccc0b7b0275500.jpg", "img_caption": [], "img_footnote": [], "page_idx": 53}, {"type": "text", "text": "Figure 4: Implementing a narrow network with a wider network. Blue edges represent parameters set to equal the parameters of $\\bar{h}$ , gray nodes represent zero neuron scaling, and gray edges represent unconstrained parameters. ", "page_idx": 53}, {"type": "text", "text": "Lemma F.7. Let h be a network with depth $L$ and dimensions ${\\bar{d}}.$ . Let $\\underline{{d}}\\geq\\bar{\\underline{{d}}}.$ . Then ", "page_idx": 53}, {"type": "equation", "text": "$$\n-\\log\\left(\\frac{\\left|\\Theta^{B T N}\\left(\\underline{{d}};\\bar{h}\\right)\\right|}{\\left|\\Theta^{B T N}\\left(\\underline{{d}}\\right)\\right|}\\right)\\leq w\\left(\\bar{\\underline{{d}}}\\right)+O\\left(n\\left(\\underline{{d}}\\right)\\cdot\\log\\left(\\underline{{d}}_{\\operatorname*{max}}+d_{0}\\right)\\right)\\,.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Proof. We prove this by counting the number of constrained parameters in $\\Theta^{\\mathrm{BTN}}\\left(\\underline{{d}};\\bar{h}\\right)$ . The number of constrained weights is ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\bar{d}_{1}d_{0}+\\sum_{l=2}^{L}\\bar{d}_{l}\\bar{d}_{l-1}\\,,\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "which is exactly $w\\left(\\bar{\\underline{{d}}}\\right)$ . In addition, there are $n\\left(\\underline{d}\\right)$ constrained bias terms, and $n\\left(\\underline{d}\\right)$ constrained scaling terms. In total, after accounting for the quantization of each parameter, this means that ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\frac{\\left|\\Theta^{\\mathrm{BTN}}\\left(\\underline{{d}};\\bar{h}\\right)\\right|}{\\left|\\Theta^{\\mathrm{BTN}}\\left(\\underline{{d}}\\right)\\right|}\\ge\\left(\\underbrace{2^{w\\left(\\underline{{d}}\\right)}}_{\\mathrm{weights}}\\cdot\\underbrace{3^{n\\left(\\underline{{d}}\\right)}}_{\\mathrm{scaling\\;terms}}\\cdot\\prod_{l=1}^{L}\\left(2d_{l-1}\\right)^{d_{l}}\\right)^{-1}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "so ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle-\\log\\left(\\frac{\\left|\\Theta^{\\mathrm{BTN}}\\left(\\underline{{d}};\\bar{h}\\right)\\right|}{\\left|\\Theta^{\\mathrm{BTN}}\\left(\\underline{{d}}\\right)\\right|}\\right)}\\\\ {\\displaystyle\\leq w\\left(\\bar{\\underline{{d}}}\\right)+n(\\underline{{d}})\\cdot\\log3+\\sum_{l=1}^{L}\\bar{d}_{l}\\cdot\\log\\left(2d_{l-1}\\right)}\\\\ {\\displaystyle\\leq w\\left(\\bar{\\underline{{d}}}\\right)+n(\\underline{{d}})\\cdot\\log3+n\\left(\\underline{{d}}\\right)\\cdot\\log\\left(2\\underline{{d}}_{\\mathrm{max}}+2d_{0}\\right)}\\\\ {\\displaystyle=w\\left(\\bar{\\underline{{d}}}\\right)+O\\left(n\\left(\\underline{{d}}\\right)\\cdot\\log\\left(\\underline{{d}}_{\\mathrm{max}}+d_{0}\\right)\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Combining Lemma F.7 with Assumption 2.4, and Corollary 3.4 gives the following lemma. ", "page_idx": 54}, {"type": "text", "text": "Lemma F.8. Consider a distribution $\\mathcal{D}$ induced by a noisy teacher model of depth $L^{\\star}$ and widths $\\underline{d}^{\\star}$ (Assumption 2.4) with a noise level of $\\varepsilon^{\\star}<1/2$ . Let $S\\sim\\dot{\\mathcal{D}}^{N}$ be a training set with effective training set label noise $\\hat{\\varepsilon}_{\\mathrm{tr}}$ as defined in (4). Then there exist constants $c_{1},c_{2}>0$ such that for any student network of depth $L\\ge\\operatorname*{max}\\left\\{L^{\\star},14\\right\\}+2$ and widths $\\underline{d}\\in\\mathbb{N}^{L}$ satisfying ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\forall l=1,\\ldots,L^{\\star}\\!-\\!1\\quad d_{l}\\geq d_{l}^{\\star}+N^{3/4}\\cdot\\left(\\log N\\right)^{c_{1}}+c_{2}\\cdot d_{0}\\cdot\\log\\left(N\\right)\\,,\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "it holds for posterior sampling with a uniform prior over parameters that ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}_{S}\\left[\\log\\left(\\frac{1}{p_{S}}\\right)\\mid c o n s i s t e n t\\,S\\right]}\\quad}&{}\\\\ &{\\leq w\\left(\\underline{{d}}^{\\star}\\right)+N\\cdot H\\left(\\hat{\\varepsilon}_{\\mathrm{tr}}\\right)+2n\\left(\\underline{{d}}^{\\star}\\right)N^{3/4}\\mathrm{polylog}N}\\\\ &{\\quad\\quad+\\,O\\left(d_{0}\\left(d_{0}+n\\left(\\underline{{d}}^{\\star}\\right)\\right)\\cdot\\log\\left(N\\right)+n\\left(\\underline{{d}}\\right)\\cdot\\log\\left(\\underline{{d}}_{\\operatorname*{max}}+d_{0}\\right)\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Remark F.9. Unlike the bounds for min-size interpolators, there is no $H\\left(\\hat{\\varepsilon}_{\\mathrm{tr}}\\right)^{3/4}$ term multiplying the $N^{3/4}$ term. This is because the architecture of random interpolators is fixed, so in our setting we must assume that it is wide enough in order to guarantee interpolation of any noisy training set. ", "page_idx": 54}, {"type": "text", "text": "Proof. Notice that for posterior sampling with uniform distribution over parameters, the interpolation probability $p_{S}$ can be lower bounded as ", "page_idx": 54}, {"type": "equation", "text": "$$\np_{S}\\geq\\frac{\\left|\\Theta^{\\mathrm{BTN}}\\left(\\underline{{d}};h^{\\star}\\oplus\\widetilde{h}_{S}\\right)\\right|}{\\left|\\Theta^{\\mathrm{BTN}}\\left(\\underline{{d}}\\right)\\right|}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "and therefore ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\log\\left(\\frac{1}{p_{S}}\\right)\\leq-\\log\\left(\\frac{\\left|\\Theta^{\\mathrm{BTN}}\\left(\\underline{{d}};h^{\\star}\\oplus\\tilde{h}_{S}\\right)\\right|}{\\left|\\Theta^{\\mathrm{BTN}}\\left(\\underline{{d}}\\right)\\right|}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Then, using the bounds from Lemma F.7 with the one from Corollary 3.4 ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log\\left(\\frac{1}{p_{S}}\\right)\\leq w\\left(\\underline{{d}}^{\\star}\\right)+N\\cdot H\\left(\\mathcal{L}_{S}\\left(h^{\\star}\\right)\\right)+2n\\left(\\underline{{d}}^{\\star}\\right)N^{3/4}\\mathrm{polylog}N}\\\\ &{\\qquad\\qquad\\qquad+\\left.O\\left(d_{0}\\left(d_{0}+n\\left(\\underline{{d}}^{\\star}\\right)\\right)\\cdot\\log N+n\\left(\\underline{{d}}\\right)\\cdot\\log\\left(\\underline{{d}}_{\\operatorname*{max}}+d_{0}\\right)\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "By taking the expectation and using Jensen\u2019s inequality with the concave $H$ we arrive at ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\mathbb{E}_{S}\\left[H\\left(\\mathcal{L}_{S}\\left(h^{\\star}\\right)\\right)\\mid\\mathrm{consistent}\\;S\\right]\\le H\\left(\\mathbb{E}_{S}\\left[\\mathcal{L}_{S}\\left(h^{\\star}\\right)\\mid\\mathrm{consistent}\\;S\\right]\\right)=H\\left(\\hat{\\varepsilon}_{\\mathrm{tr}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Hence ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{S}\\left[\\log\\left(\\frac{1}{p_{S}}\\right)\\bigg|\\,\\mathrm{consistent}\\,S\\right]}\\\\ &{\\leq w\\left(\\underline{{d}}^{\\star}\\right)+N\\cdot H\\left(\\hat{\\varepsilon}_{\\mathrm{tr}}\\right)+2n\\left(\\underline{{d}}^{\\star}\\right)N^{3/4}\\mathrm{polylog}N}\\\\ &{\\quad+\\,O\\left(d_{0}\\left(d_{0}+n\\left(\\underline{{d}}^{\\star}\\right)\\right)\\cdot\\log\\left(N\\right)+n\\left(\\underline{{d}}\\right)\\cdot\\log\\left(\\underline{{d}}_{\\operatorname*{max}}+d_{0}\\right)\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Recall Theorem 4.4. Consider a distribution $\\mathcal{D}$ induced by a noisy teacher model of depth $L^{\\star}$ and widths $\\underline{d}^{\\star}$ (Assumption 2.4) with a noise level of $\\varepsilon^{\\star}<1/2$ . Let $S\\stackrel{*}{\\sim}\\mathcal{D}^{N}$ be a training set such that $N=o(\\sqrt{1/D_{\\operatorname*{max}}})$ . Then, there exist constants $c_{1},c_{2}>0$ such that for any student network of depth $L\\ge\\operatorname*{max}\\left\\{L^{\\star},14\\right\\}+2$ and widths $\\underline{d}\\in\\mathbb{N}^{L}$ holding ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\forall l=1,\\ldots,L^{\\star}\\!-\\!1\\quad d_{l}\\geq d_{l}^{\\star}+N^{3/4}\\cdot\\left(\\log N\\right)^{c_{1}}+c_{2}\\cdot d_{0}\\cdot\\log\\left(N\\right)\\,,\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "the generalization error of posterior sampling satisfies the following. ", "page_idx": 55}, {"type": "text", "text": "\u2022 Under arbitrary label noise, ", "text_level": 1, "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{S,A(S)}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\right]\\leq1-2^{-H(\\varepsilon^{\\star})}+2N^{2}\\mathcal{D}_{\\operatorname*{max}}+O\\left(C_{\\operatorname{rand}}\\left(N\\right)\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "\u2022 Under independent label noise, ", "text_level": 1, "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{E}_{S,A(S)}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\right]-2\\varepsilon^{\\star}\\left(1-\\varepsilon^{\\star}\\right)\\right|}\\\\ &{\\leq\\left(1-2\\varepsilon^{\\star}\\right)\\sqrt{\\frac{O\\left(C_{\\mathrm{rand}}\\left(N\\right)\\right)+\\mathbb{P}\\left(\\mathrm{inconsistent}\\;S\\right)}{\\mathbb{P}\\left(\\mathrm{consistent}\\;S\\right)}}+\\frac{\\left(N-1\\right)\\mathcal{D}_{\\mathrm{max}}}{3}+\\mathbb{P}\\left(\\mathrm{inconsistent}\\;S\\right)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where ", "page_idx": 55}, {"type": "equation", "text": "$$\nC_{\\mathrm{rand}}\\left(N\\right)=\\frac{n\\left(\\underline{{d}}^{\\star}\\right)\\cdot\\mathrm{polylog}\\left(N\\right)}{\\sqrt[4]{N}}+\\frac{w\\left(\\underline{{d}}^{\\star}\\right)+d_{0}\\left(d_{0}+n\\left(\\underline{{d}}^{\\star}\\right)\\right)\\cdot\\log\\left(N\\right)+n\\left(\\underline{{d}}\\right)\\cdot\\log\\left(\\underline{{d}}_{\\mathrm{max}}+d_{0}\\right)}{N}\\,.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Remark F.10. The bound shown in Section 4.2 is found by bounding $\\mathbb{P}$ (inconsistent $S$ ) as in Lemma B.1. Assuming that $N=\\omega\\left(n\\left(\\underline{{d}}^{\\star}\\right)^{4}\\mathrm{polylog}\\left(n\\left(\\underline{{d}}^{\\star}\\right)\\right)+d_{0}^{2}\\log d_{0}\\right)$ we can deduce that $N=\\omega\\left(w\\left(\\underline{{d}}^{\\star}\\right)\\right)$ as well since ", "page_idx": 55}, {"type": "equation", "text": "$$\nw\\left(\\underline{{d}}^{\\star}\\right)\\leq\\left(n\\left(\\underline{{d}}^{\\star}\\right)+d_{0}\\right)^{2}\\leq4\\left(\\operatorname*{max}\\left\\{n\\left(\\underline{{d}}^{\\star}\\right),d_{0}\\right\\}\\right)^{2}\\,.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Together with $N=o\\left(\\sqrt{1/D_{\\mathrm{max}}}\\right)$ we get the desired form of the bounds. ", "page_idx": 55}, {"type": "text", "text": "Proof. Corollary 3.4 implies that there exist $c_{1},c_{2}>0$ such that a student NN satisfying (11) can interpolate any consistent dataset, and so posterior sampling is interpolating for all consistent datasets. We start by proving the bound for arbitrary label noise. First, we notice that ", "page_idx": 55}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{{\\hat{\\varepsilon}}_{\\mathrm{tr}}=\\mathbb{P}(Y_{1}\\neq h^{\\star}(X_{1})\\mid{\\mathrm{consistent}}\\,S)={\\frac{\\mathbb{P}(Y_{1}\\neq h^{\\star}(X_{1}),{\\mathrm{consistent}}\\,S)}{\\mathbb{P}\\,({\\mathrm{consistent}}\\,S)}}}\\\\ &{\\quad\\leq{\\frac{\\mathbb{P}(Y_{1}\\neq h^{\\star}(X_{1}))}{\\mathbb{P}\\,({\\mathrm{consistent}}\\,S)}}={\\frac{\\varepsilon^{\\star}}{\\mathbb{P}\\,({\\mathrm{consistent}}\\,S)}}\\,.}\\end{array}}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "The entropy function $H$ is increasing in $\\textstyle\\left[0,{\\frac{1}{2}}\\right]$ and achieves its maximum at $\\frac{1}{2}$ , so together with the inequality above, we get, ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H\\left(\\hat{\\varepsilon}_{\\mathrm{tr}}\\right)\\leq H\\left(\\operatorname*{min}\\left\\{\\frac{\\varepsilon^{\\star}}{\\mathbb{P}(\\mathrm{consistent}\\,S)},\\frac{1}{2}\\right\\}\\right)=H\\left(\\varepsilon^{\\star}+\\operatorname*{min}\\left\\{\\frac{\\varepsilon^{\\star}}{\\mathbb{P}(\\mathrm{consistent}\\,S)}-\\varepsilon^{\\star},\\frac{1}{2}-\\varepsilon^{\\star}\\right\\}\\right)}\\\\ &{\\qquad\\quad=H\\Big(\\varepsilon^{\\star}+\\underbrace{\\operatorname*{min}\\left\\{\\varepsilon^{\\star}\\frac{\\mathbb{P}(\\mathrm{inconsistent}\\,S)}{\\mathbb{P}(\\mathrm{consistent}\\,S)},\\frac{1}{2}-\\varepsilon^{\\star}\\right\\}}_{\\triangleq\\Delta}\\Big)=H\\left(\\varepsilon^{\\star}+\\Delta\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Employing the concavity of the entropy function, we get, ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r}{H\\left(\\hat{\\varepsilon}_{\\mathrm{t}}\\right)\\leq H\\left(\\varepsilon^{\\star}+\\Delta\\right)\\leq H(\\varepsilon^{\\star})+H^{\\prime}(\\varepsilon^{\\star})\\cdot\\Delta\\leq H(\\varepsilon^{\\star})+\\underbrace{H^{\\prime}(\\varepsilon^{\\star})\\cdot\\varepsilon^{\\star}}_{\\leq\\frac{1}{2},\\mathrm{agebrateally}}\\cdot\\frac{\\mathbb{P}\\left(\\mathrm{inconsistent}\\,S\\right)}{\\mathbb{P}\\left(\\mathrm{consistent}\\,S\\right)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "By combining the above with Corollary F.6, Lemma F.8, we have that ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\log\\left(1-\\mathbb{E}_{(S,A(S))}\\left[Z_{D}\\left(A\\left(S\\right)\\right)\\mid\\mathrm{consistent}\\,S\\right]\\right)\\leq\\frac{\\mathbb{E}_{S}\\left[\\log\\left(1/p_{S}\\right)\\mid\\mathrm{consistent}\\,S\\right]+3}{N}}\\\\ &{\\qquad\\leq H\\left(\\hat{\\varepsilon}_{\\mathrm{t}}\\right)+\\frac{1}{N}\\bigg(w\\left(\\underline{{\\hat{a}}}^{\\dagger}\\right)+N\\cdot H\\left(\\hat{\\varepsilon}_{\\mathrm{t}}\\right)+2n\\left(\\underline{{\\hat{a}}}^{\\dagger}\\right)N^{3/4}\\mathrm{polylog}N}\\\\ &{\\qquad\\quad+O\\left(d_{0}\\left(d_{0}+n\\left(\\underline{{\\hat{a}}}^{\\dagger}\\right)\\right)\\cdot\\log\\left(N\\right)+n\\left(\\underline{{d}}\\right)\\cdot\\log\\left(d_{\\mathrm{max}}+d_{0}\\right)\\right)\\bigg)}\\\\ &{\\qquad\\leq H\\left(\\hat{\\varepsilon}_{\\mathrm{t}}\\right)+O\\left(\\frac{n\\left(\\underline{{\\hat{a}}}^{\\dagger}\\right)\\cdot\\mathrm{polylog}\\left(N\\right)}{\\sqrt{N}}+\\frac{w\\left(\\underline{{\\hat{a}}}^{\\dagger}\\right)+d_{0}\\left(d_{0}+n\\left(\\underline{{\\hat{a}}}^{\\dagger}\\right)\\right)\\cdot\\log\\left(N\\right)+n\\left(\\underline{{\\hat{a}}}\\right)\\cdot\\log\\left(d_{\\mathrm{max}}+d_{0}\\right)}{N}\\right)}\\\\ &{\\qquad\\leq H\\left(\\varepsilon^{*}\\right)+\\frac{\\mathbb{P}\\left(\\operatorname{inconsistent}S\\right)}{2\\sqrt{N}}}\\\\ &{\\qquad\\quad+O\\left(\\frac{n\\left(\\underline{{\\hat{a}}}^{\\dagger}\\right)\\cdot\\mathrm{polylog}\\left(N\\right)}{\\sqrt{N}}+\\frac{w\\left(\\underline{{\\hat{a}}}^{\\dagger}\\right)+d_{0}\\left(d_{0}+n\\left(\\underline{{\\hat{a}}}^{\\dagger}\\right)\\right)\\cdot\\log\\left(N\\right)+n\\left(\\underline{{\\hat{a}}}\\right)\\cdot\\log\\left(d_{\\operatorname*{max}}+d_{0}\\right)}{N}\\right)}\\\\ &{\\qquad=H\\left(\\varepsilon^{*}\\right)+\\frac{\\mathbb{P}\\left(\\operatorname{inconsistent}S\\right)}{\\sqrt{N}}+O\\left(C_{\\operatorname*{max}}\\left(N\\right)\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Rearranging the inequality results in ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{(S,A(S))}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\mid\\mathrm{consistent}\\;S\\right]}\\\\ &{\\qquad\\qquad\\leq1-2^{-H(\\varepsilon^{\\star})-\\frac{\\mathbb{P}(\\mathrm{inconsistent}\\;S)}{2\\mathbb{P}(\\mathrm{consistent}\\;S)}-O\\left(C_{\\mathrm{rand}}\\left(N\\right)\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Then, using Lemma A.6, we get, ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{(S,A(S))}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\mid\\mathrm{consistent}\\;S\\right]}\\\\ &{\\leq1-2^{-H(\\varepsilon^{\\star})}+\\displaystyle\\frac{\\mathbb{P}\\left(\\mathrm{inconsistent}\\;S\\right)}{2\\mathbb{P}\\left(\\mathrm{consistent}\\;S\\right)}+O\\left(C_{\\mathrm{rand}}\\left(N\\right)\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Repeating the argument from the proof of Theorem 4.2, since for an RV $X$ in $[0,1]$ and a binary RV $Y$ we have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\operatorname{\\mathbb{E}}[X]=\\operatorname{\\mathbb{E}}[X\\mid Y]\\underbrace{\\operatorname{\\mathbb{P}}(Y)}_{\\leq1}+\\underbrace{\\operatorname{\\mathbb{E}}[X\\mid\\neg Y]}_{\\leq1}\\operatorname{\\mathbb{P}}(\\neg Y)\\leq\\operatorname{\\mathbb{E}}[X\\mid Y]+\\operatorname{\\mathbb{P}}(\\neg Y)\\,,\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "we have, ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\left(S,A(S)\\right)}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\right]\\leq\\mathbb{E}_{\\left(S,A(S)\\right)}\\left[\\mathcal{L}_{\\mathcal{D}}\\left(A\\left(S\\right)\\right)\\mid\\mathrm{consistent}\\;S\\right]+\\mathbb{P}\\left(\\mathrm{inconsistent}\\;S\\right)}\\\\ &{\\leq1-2^{-H\\left(\\varepsilon^{\\star}\\right)}+\\frac{\\mathbb{P}\\left(\\mathrm{inconsistent}\\;S\\right)}{2\\mathbb{P}\\left(\\mathrm{consistent}\\;S\\right)}+\\mathbb{P}\\left(\\mathrm{inconsistent}\\;S\\right)+O\\left(C_{\\mathrm{rand}}\\left(N\\right)\\right)}\\\\ &{\\leq1-2^{-H\\left(\\varepsilon^{\\star}\\right)}+2\\frac{\\mathbb{P}\\left(\\mathrm{inconsistent}\\;S\\right)}{\\mathbb{P}\\left(\\mathrm{consistent}\\;S\\right)}+O\\left(C_{\\mathrm{rand}}\\left(N\\right)\\right)}\\\\ &{\\leq1-2^{-H\\left(\\varepsilon^{\\star}\\right)}+2\\frac{\\frac{1}{2}N^{2}\\mathcal{D}_{\\mathrm{max}}}{1-\\frac{1}{2}N^{2}\\mathcal{D}_{\\mathrm{max}}}+O\\left(C_{\\mathrm{rand}}\\left(N\\right)\\right)}\\\\ &{\\leq1-2^{-H\\left(\\varepsilon^{\\star}\\right)}+2N^{2}\\mathcal{D}_{\\mathrm{max}}+O\\left(C_{\\mathrm{rand}}\\left(N\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where in the last inequality we used $t/\\left(1-t\\right)\\leq2t$ for $t\\in[0,1/2]$ . ", "page_idx": 56}, {"type": "text", "text": "Moving on to the independent noise setting, we combine Lemma F.5, Lemma F.8, and $\\begin{array}{r}{\\hat{\\varepsilon}_{\\mathrm{tr}}\\le\\varepsilon^{\\star}<\\frac{1}{2}}\\end{array}$ from Lemma B.2, to bound the mutual information as ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I\\left(S;A\\left(S\\right)\\right)\\leq\\ \\mathbb{E}_{S}\\left[\\log\\left(\\frac{1}{p_{S}}\\right)\\mid\\mathrm{consistent}\\ S\\right]\\overbrace{\\mathbb{P}_{S}\\left(\\mathrm{consistent}\\ S\\right)}^{\\leq1}+\\frac{2}{e\\ln2}}\\\\ &{\\leq\\ \\mathbb{E}_{S}\\left[\\log\\left(\\frac{1}{p_{S}}\\right)\\mid\\mathrm{consistent}\\ S\\right]+1.1}\\\\ &{\\leq\\ N\\cdot H\\left(\\varepsilon^{\\star}\\right)+O\\left(N\\cdot C_{\\mathrm{rand}}\\left(N\\right)\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Plugging the above into $C(N)$ of Lemma C.3, we get, ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C\\left(N\\right)=\\frac{I\\left(S;A\\left(S\\right)\\right)-N\\cdot H\\left(\\varepsilon^{\\star}\\right)+N\\cdot\\mathbb{P}_{S\\sim\\mathcal{D}^{N}}\\left(\\mathrm{inconsistent}\\;S\\right)}{N\\cdot\\mathbb{P}\\left(\\mathrm{consistent}\\;S\\right)}}\\\\ &{\\leq\\frac{N\\cdot H\\left(\\varepsilon^{\\star}\\right)+O\\left(N\\cdot C_{\\mathrm{rand}}\\left(N\\right)\\right)-N\\cdot H\\left(\\varepsilon^{\\star}\\right)+N\\cdot\\mathbb{P}\\left(\\mathrm{inconsistent}\\;S\\right)}{N\\cdot\\mathbb{P}\\left(\\mathrm{consistent}\\;S\\right)}}\\\\ &{=\\frac{O\\left(C_{\\mathrm{rand}}\\left(N\\right)\\right)+\\mathbb{P}\\left(\\mathrm{inconsistent}\\;S\\right)}{\\mathbb{P}\\left(\\mathrm{consistent}\\;S\\right)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Then we continue as in the arbitrary noise setting to get the desired bound. ", "page_idx": 57}, {"type": "text", "text": "G Alignment with Dale\u2019s Law ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "In this section, we show that our results apply to a model resembling \u201cDale\u2019s Law\u201d [82], i.e., such that for each neuron, all outgoing weights have the same sign. To this end, we define the following model, in which the main difference from Def. 2.1 is that neuron scaling is applied after the threshold activation. ", "page_idx": 58}, {"type": "text", "text": "Definition G.1 (Binary threshold networks with outgoing scaling). For a depth $L$ , widths $\\underline{d}=(d_{1},\\ldots,d_{L})$ , input dimension $d_{0}$ , a scaled-neuron fully connected binary threshold NN with outgoing weight scaling (oBTN), is a mapping $\\theta\\mapsto g_{\\theta}$ such that $g_{\\pmb\\theta}:\\{0,1\\}^{d_{0}}\\rightarrow\\{-1,0,1\\}^{d_{L}}$ , parameterized by ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\pmb{\\theta}=\\left\\{\\mathbf{W}^{(l)},\\mathbf{b}^{(l)},\\pmb{\\gamma}^{(l)}\\right\\}_{l=1}^{L}\\;,\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "where for every layer $l\\in[L]$ , ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\mathbf{W}^{(l)}\\in\\mathcal{Q}_{l}^{W}=\\{0,1\\}^{d_{l}\\times d_{l-1}}\\,,\\,\\,\\gamma^{(l)}\\in\\mathcal{Q}_{l}^{\\gamma}\\!=\\!\\{-1,0,1\\}^{d_{l}}\\,,\\,\\,\\mathbf{b}^{(l)}\\!\\in\\mathcal{Q}_{l}^{b}\\!=\\!\\{-d_{l-1}+1,\\dots,d_{l-1}\\}^{d_{l}}\\,\\,.\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "This mapping is defined recursively as $g_{\\theta}\\left(\\mathbf{x}\\right)=g^{(L)}\\left(\\mathbf{x}\\right)$ where ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g^{(0)}\\left(\\mathbf{x}\\right)=\\mathbf{x}\\,,}\\\\ &{\\forall l\\in[L]\\quad g^{(l)}\\left(\\mathbf{x}\\right)=\\gamma^{(l)}\\odot\\mathbb{I}\\left\\{\\mathbf{W}^{(l)}g^{(l-1)}\\left(\\mathbf{x}\\right)+\\mathbf{b}^{(l)}>\\mathbf{0}\\right\\}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Lemma G.2. Let g\u03b8 be an oBTN as in Def. G.1. Then there exists a BTN h\u03b8\u2032 with the same dimensions and $\\mathbf{b}^{\\prime(l)}\\in\\mathcal{Q}_{l}^{2b}\\triangleq\\{-2d_{l-1}+1,\\dots,2d_{l}\\}$ such that $h_{\\pmb{\\theta}^{\\prime}}\\equiv g_{\\pmb{\\theta}}+s$ for $s\\in\\{0,1\\}^{d_{L}}$ such that for all $i=1,\\ldots,d_{L}$ , $s_{i}=1$ only if $\\gamma_{i}^{(L)}=-1$ . ", "page_idx": 58}, {"type": "text", "text": "Proof. We prove the lemma by induction on depth. As we will see, the base case is a particular case of the step of the induction, so we start with the latter. Let $l=1,\\dots,L$ . For ease of notation, we denote $\\bar{C}=g^{(l)}$ and $A=g^{(l-1)}$ , as well as $C^{\\prime}=h^{(l)}$ , $A^{\\prime}=h^{(l-1)}$ . In addition, we omit the superscripts from the $l^{t h}$ layer\u2019s parameters. Let $i=1,\\ldots,d_{l}$ , then by the induction hypothesis there exists some $a\\in\\{0,1\\}^{d_{l-1}}$ such that $A\\left(\\mathbf{x}\\right)=A^{\\prime}\\left(\\mathbf{x}\\right)-a$ , for all inputs $\\mathbf{x}$ , ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{C\\left(\\mathbf{x}\\right)_{i}=\\gamma_{i}\\cdot\\mathbb{I}\\left\\{b_{i}+\\sum_{j=1}^{d_{l-1}}w_{i j}A\\left(\\mathbf{x}\\right)_{j}>0\\right\\}=\\gamma_{i}\\cdot\\mathbb{I}\\left\\{b_{i}+\\sum_{j=1}^{d_{l-1}}w_{i j}\\left(A^{\\prime}\\left(\\mathbf{x}\\right)_{j}-a_{j}\\right)>0\\right\\}}}\\\\ {\\displaystyle{\\qquad=\\gamma_{i}\\cdot\\mathbb{I}\\left\\{\\left(b_{i}-\\sum_{j=1}^{d_{l-1}}w_{i j}a_{j}\\right)+\\sum_{j=1}^{d_{l-1}}w_{i j}A^{\\prime}\\left(\\mathbf{x}\\right)_{j}>0\\right\\}\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "If $\\gamma_{i}=+1$ , choose $\\gamma_{i}^{\\prime}=+1$ , and $\\begin{array}{r}{b_{i}^{\\prime}=b_{i}-\\sum_{j=1}^{d_{l-1}}w_{i j}a_{j}}\\end{array}$ . Clearly, since $w_{i j},a_{j}\\in\\{0,1\\}$ , it holds that $\\begin{array}{r}{\\left|\\sum_{j=1}^{d_{l-1}}w_{i j}a_{j}\\right|\\leq d_{l-1}}\\end{array}$ so $b_{i}^{\\prime}\\in\\mathcal{Q}_{l}^{2b}$ . Then ", "page_idx": 58}, {"type": "equation", "text": "$$\nC\\left(\\mathbf{x}\\right)_{i}=\\mathbb{I}\\left\\{b_{i}^{\\prime}+\\gamma_{i}^{\\prime}\\cdot\\sum_{j=1}^{d_{l-1}}w_{i j}A^{\\prime}\\left(\\mathbf{x}\\right)_{j}>0\\right\\}=C^{\\prime}\\left(\\mathbf{x}\\right)_{i}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "i.e., the claim holds with $s_{i}=0$ . If $\\gamma_{i}=-1$ then ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{C\\left(\\mathbf{x}\\right)_{i}=\\gamma_{i}\\cdot\\mathbb{I}}&{\\left\\{\\left({\\phi}_{i}-\\sum_{j=1}^{d-{1}}w_{i j}a_{j}\\right)+\\sum_{j=1}^{d-{1}}w_{i j}A^{\\prime}\\left(\\mathbf{x}\\right)_{j}>0\\right\\}}\\\\ &{\\qquad=-1\\left\\{\\left({b}_{i}-\\sum_{j=1}^{d-{1}}w_{i j}a_{j}\\right)+\\sum_{j=1}^{d-{1}}w_{i j}A^{\\prime}\\left(\\mathbf{x}\\right)_{j}>0\\right\\}}\\\\ &{\\qquad=-1+\\mathbb{I}\\left\\{\\left({b}_{i}-\\sum_{j=1}^{d-{1}}w_{i j}a_{j}\\right)+\\sum_{j=1}^{d-{1}}w_{i j}A^{\\prime}\\left(\\mathbf{x}\\right)_{j}\\leq0\\right\\}}\\\\ &{\\qquad=-1+\\mathbb{I}\\left\\{-\\left({b}_{i}-\\sum_{j=1}^{d-{1}}w_{i j}a_{j}\\right)+\\sum_{j=1}^{d-{1}}w_{i j}A^{\\prime}\\left(\\mathbf{x}\\right)_{j}\\leq0\\right\\}}\\\\ &{\\qquad=-1+\\mathbb{I}\\left\\{-\\left({b}_{i}-\\sum_{j=1}^{d-{1}}w_{i j}a_{j}\\right)-\\sum_{j=1}^{d-{1}}w_{i j}A^{\\prime}\\left(\\mathbf{x}\\right)_{j}\\geq0\\right\\}}\\\\ &{\\qquad=-1+\\mathbb{I}\\left\\{1-\\left({b}_{i}-\\sum_{j=1}^{d-{1}}w_{i j}a_{j}\\right)-\\sum_{j=1}^{d-{1}}w_{i j}A^{\\prime}\\left(\\mathbf{x}\\right)_{j}>0\\right\\}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Thus, we can construct the $l^{t h}$ layer of $h$ by choosing $\\gamma_{i}^{\\prime}=-1$ and $\\begin{array}{r}{b_{i}^{\\prime}=1-\\Big(b_{i}-\\sum_{j=1}^{d_{l-1}}w_{i j}a_{j}\\Big)}\\end{array}$ so ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{C\\left(\\mathbf{x}\\right)_{i}=-1+\\mathbb{I}\\left\\{1-\\displaystyle\\left(b_{i}-\\sum_{j=1}^{d_{l-1}}w_{i j}a_{j}\\right)-\\sum_{j=1}^{d_{l-1}}w_{i j}A^{\\prime}\\left(\\mathbf{x}\\right)_{j}>0\\right\\}}}\\\\ {{=-1+\\mathbb{I}\\left\\{b_{i}^{\\prime}+\\gamma_{i}^{\\prime}\\sum_{j=1}^{d_{l-1}}w_{i j}A^{\\prime}\\left(\\mathbf{x}\\right)_{j}>0\\right\\}}}\\\\ {{=C^{\\prime}\\left(\\mathbf{x}\\right)_{i}-s_{i}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "with $s_{i}=1$ . Finally, if $\\gamma_{i}=0$ , then $C_{i}$ is identically 0, so we can choose $\\gamma_{i}^{\\prime}=b_{i}^{\\prime}=s_{i}=0$ . Notice that this construction also proves the base case $l=1$ where $a={\\bf0}$ . \u53e3 ", "page_idx": 59}, {"type": "text", "text": "Corollary G.3. Let $\\Theta^{\\prime}$ be the set of oBTN parameters such that for all $\\pmb\\theta\\in\\Theta^{\\prime}$ , $g_{\\theta}:\\{0,1\\}^{d_{0}}\\rightarrow$ $\\{0,1\\}^{d_{L}}$ . Then there exists a BTN, $h$ as in Lemma $G.2$ such that $g_{\\theta}\\equiv h$ . ", "page_idx": 59}, {"type": "text", "text": "Proof. Let $\\pmb\\theta\\in\\Theta^{\\prime}$ . Since $g_{\\theta}\\left(\\mathbf{x}\\right)\\neq-1$ for all $\\mathbf{x}$ , there exist parameters $\\pmb{\\theta}^{\\prime}\\in\\Theta^{\\prime}$ such that $g_{\\pmb\\theta^{\\prime}}\\equiv g_{\\pmb\\theta}$ , and $\\gamma^{\\prime(L)}\\geq0$ . Hence, by Lemma G.2 there exists a BTN $h$ such that $h\\equiv g\\pmb{\\theta}^{\\prime}$ , i.e., with $s={\\bf0}$ . ", "page_idx": 59}, {"type": "text", "text": "Remark G.4. Similar results can be shown in the other direction. That is, that BTNs can be represented as slightly larger oBTNs. ", "page_idx": 59}, {"type": "text", "text": "Finally, recall from Appendix F, that the cardinality of the hypothesis class is related to the error terms of Theorem 4.2 and Theorem 4.4 only logarithmically, meaning that we can apply the results to Def. G.1 without qualitatively changing them. ", "page_idx": 59}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: We accurately state the general setup and the essential results. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 60}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Justification: We discuss the limitations of our theory in Section 6. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 60}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: General assumptions on the label-generating process are clearly stated in Section 2.2. Additional assumptions are stated at the beginning of each theorem. Rigorous proofs are given in our supplementary materials and are referred to in the main body of the paper. ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 61}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 61}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 61}, {"type": "text", "text": "Justification: No experiments. ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 61}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 62}, {"type": "text", "text": "Answer: [NA] Justification: No experiments. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 62}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 62}, {"type": "text", "text": "Answer: [NA] Justification: No experiments. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 62}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 63}, {"type": "text", "text": "Answer: [NA]   \nJustification: No experiments. Guidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 63}, {"type": "text", "text": "", "page_idx": 63}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 63}, {"type": "text", "text": "Answer: [NA] Justification: No experiments. ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 63}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 63}, {"type": "text", "text": "Answer: [Yes]   \nJustification: Our work adheres to NeurIPS\u2019 ethical guidelines. Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. \u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 63}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 64}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 64}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 64}, {"type": "text", "text": "Justification: Our work is theoretical in nature, and we do not see a direct social impact. ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 64}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 64}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 64}, {"type": "text", "text": "Justification: No data or models are released. ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 64}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 65}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 65}, {"type": "text", "text": "Justification: Not using external assets. ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 65}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 65}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 65}, {"type": "text", "text": "Justification: This is a theoretical papers; we do not release new assets. ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 65}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 65}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 65}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 65}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 66}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 66}, {"type": "text", "text": "Justification: Our research did not involve human subjects. ", "page_idx": 66}, {"type": "text", "text": "Guidelines: ", "page_idx": 66}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 66}]