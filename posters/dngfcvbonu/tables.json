[{"figure_path": "DNGfCVBOnU/tables/tables_2_1.jpg", "caption": "Table 1: Performance of the 2-layer model for each dataset (MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100, STL-10). Each performance value (%) is presented as the mean \u00b1 standard deviation from three trials. Extended results with various model depths can be found in the Appendix.", "description": "This table presents the classification accuracy of a two-layer neural network model on five different image datasets: MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100, and STL-10.  The accuracy is shown for three different training methods: backpropagation (BP), feedback alignment without random noise pretraining (w/o), and feedback alignment with random noise pretraining (w/).  Each result is the average of three trials, with the standard deviation provided to indicate variability.  The table also notes that more extensive results using networks of varying depths are available in the Appendix.", "section": "4.3 Pre-regularization by random noise training enables robust generalization"}, {"figure_path": "DNGfCVBOnU/tables/tables_5_1.jpg", "caption": "Table 1: Performance of the 2-layer model for each dataset (MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100, STL-10). Each performance value (%) is presented as the mean \u00b1 standard deviation from three trials. Extended results with various model depths can be found in the Appendix.", "description": "This table presents the classification accuracy for a two-layer neural network on five different image datasets: MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100, and STL-10.  The results are shown for three different training methods: Backpropagation (BP), Feedback Alignment (FA) without random noise pretraining (w/o), and Feedback Alignment with random noise pretraining (w/).  Each accuracy value represents the mean \u00b1 standard deviation from three independent trials.  The table highlights the improvement in accuracy achieved by incorporating the random noise pretraining technique, particularly for more complex datasets.", "section": "4.3 Pre-regularization by random noise training enables robust generalization"}]