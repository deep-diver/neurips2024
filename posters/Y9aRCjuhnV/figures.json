[{"figure_path": "Y9aRCjuhnV/figures/figures_6_1.jpg", "caption": "Figure 1: Average rewards for the respective algorithms on 3 datasets, averaged across 30 runs. Shaded regions represent standard error.", "description": "The figure compares the performance of five different algorithms (EduQate, TW, WIQL, Myopic, and Random) across three datasets (Simulated, Junyi, and OLI) in terms of average rewards.  Each algorithm's average reward is plotted against timesteps. Shaded regions indicate the standard error of the mean, representing the variability in the results.  This allows for a visual comparison of the algorithms' effectiveness in maximizing rewards over time and across different datasets.", "section": "5 Experiment"}, {"figure_path": "Y9aRCjuhnV/figures/figures_8_1.jpg", "caption": "Figure 2: This visualization compares network complexities from our experiments. The synthetic dataset (left) shows simpler, isolated groups, while the real-world datasets (Junyi, middle; OLI,right) displays more intricate and interconnected relationships amongst items.", "description": "This figure visualizes the network structures of three datasets used in the experiments: a synthetic dataset and two real-world datasets (Junyi and OLI).  The synthetic dataset's network exhibits simpler, more isolated groups of interconnected nodes, whereas the Junyi and OLI datasets show far more complex and interconnected network structures. This difference highlights the varying degrees of interdependency between learning concepts in the different datasets. The figure helps to contextualize the challenges and opportunities presented by different learning environments. ", "section": "5 Experiment"}, {"figure_path": "Y9aRCjuhnV/figures/figures_14_1.jpg", "caption": "Figure 3: Average rewards for the respective algorithms, on the last episode of training. Note that as Ntopics increase, the network effects are reduced, and most algorithms are not better than a random policy.", "description": "This figure displays the average rewards obtained by different algorithms across three different network setups, with the number of topics varying from 20 to 40. The algorithms tested include EduQate (with and without Replay Buffer), TW, WIQL, Myopic, and Random. The x-axis shows timesteps (0-50) and the y-axis shows the average rewards. The figure demonstrates the impact of network complexity (represented by the number of topics) on the performance of various algorithms.  As the number of topics increases, the performance of most algorithms gets closer to that of the random policy, indicating a reduced impact of network effects on the learning process and a weakening of the advantage of algorithms that explicitly model those effects, such as EduQate.", "section": "E.1 Comparing Different Network Setups"}, {"figure_path": "Y9aRCjuhnV/figures/figures_15_1.jpg", "caption": "Figure 2: This visualization compares network complexities from our experiments. The synthetic dataset (left) shows simpler, isolated groups, while the real-world datasets (Junyi, middle; OLI,right) displays more intricate and interconnected relationships amongst items.", "description": "This figure compares the network structures of three datasets used in the paper's experiments.  The left panel shows a synthetic dataset's network, which is relatively simple with clearly defined, isolated groups of nodes. The middle panel displays the network from the Junyi dataset, a real-world dataset with more complex interconnections between the nodes. The right panel shows the network structure from the OLI dataset, another real-world dataset which is even more complex than the Junyi dataset, demonstrating even more intricate relationships between the nodes. The differences in complexity illustrate the varying levels of interdependency in the learning materials represented in the different datasets.", "section": "5 Experiment"}, {"figure_path": "Y9aRCjuhnV/figures/figures_16_1.jpg", "caption": "Figure 5: Average rewards across 800 episodes of training, across 30 seeds. EduQate- (orange) refers to the EduQate algorithm without replay buffer.", "description": "This figure displays the average rewards obtained by the EduQate algorithm (with and without experience replay buffer) across 800 training episodes. The results are averaged over 30 different random seeds, for three different datasets: Simulated, Junyi, and OLI. It shows the learning curves of EduQate with and without the replay buffer, highlighting the impact of the replay buffer on the performance and stability of the learning process.", "section": "E.2 Ablation of Replay Buffer"}]