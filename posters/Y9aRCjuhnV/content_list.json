[{"type": "text", "text": "EduQate: Generating Adaptive Curricula through RMABs in Education Settings ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 There has been significant interest in the development of personalized and adaptive   \n2 educational tools that cater to a student\u2019s individual learning progress. A crucial   \n3 aspect in developing such tools is in exploring how mastery can be achieved across   \n4 a diverse yet related range of content in an efficient manner. While Reinforcement   \n5 Learning and Multi-armed Bandits have shown promise in educational settings,   \n6 existing works often assume the independence of learning content, neglecting   \n7 the prevalent interdependencies between such content. In response, we introduce   \n8 Education Network Restless Multi-armed Bandits (EdNetRMABs), utilizing a   \n9 network to represent the relationships between interdependent arms. Subsequently,   \n0 we propose EduQate, a method employing interdependency-aware Q-learning to   \n11 make informed decisions on arm selection at each time step. We establish the   \n12 optimality guarantee of EduQate and demonstrate its efficacy compared to baseline   \n13 policies, using students modeled from both synthetic and real-world data. ", "page_idx": 0}, {"type": "text", "text": "14 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "15 The COVID-19 pandemic has accelerated the adoption of educational technologies, especially   \n16 on eLearning platforms. Despite abundant data and advancements in modeling student learning,   \n17 effectively capturing the learning process with interdependent content remains a significant challenge   \n18 [9]. The conventional rules-based approach to creating personalized learning curricula is impractical   \n19 due to its labor-intensive nature and need for expert knowledge. Machine learning-based systems offer   \n20 a scalable alternative, automatically generating personalized content to optimize learning [22, 24].   \n21 One possible approach to model the learning process is the Restless Multi-Armed Bandits (RMAB,   \n22 [26]), where a teacher agent selects a subset of arms (concepts) to teach each round. However,   \n23 RMAB\u2019s assumption that arms are independent is unrealistic in educational settings. For example,   \n24 solving a math question on the area of a triangle requires knowledge of algebra, arithmetic, and   \n25 geometry. Practicing this question should enhance proficiency in all three areas. Models that ignore   \n26 such interdependencies may inaccurately predict knowledge levels by assuming each exercise impacts   \n27 only a single area.   \n28 In response to this challenge, we introduce an interdependency-aware RMAB model to the education   \n29 setting. We posit that by acknowledging and modeling the learning dynamics of interdependent   \n30 content, both teachers and algorithms can strategically leverage overlapping utility to foster mastery   \n31 over a broader range of topics within a curriculum. We advocate for RMABs as a fitting model for   \n32 this context, as the inherent dynamics of such a model align closely with the learning process.   \n33 In this study, our objective is to derive a teacher policy that effectively recommends educational   \n34 content to students, accounting for interdependencies among the content to enhance overall utility   \n35 (that characterizes understanding and retention of content). Our contributions are as follows:   \n36 1. We introduce Restless Multi-armed Bandits for Education (EdNetRMABs), enabling the   \n37 modeling of learning processes with interdependent educational content.   \n38 2. We propose EduQate, a Whittle index-based heuristic algorithm that uses Q-learning to   \n39 compute an inter-dependency-aware teacher policy. Unlike previous methods, EduQate does   \n40 not require knowledge of the transition matrix to compute an optimal policy.   \n41 3. We provide a theoretical analysis of EduQate, demonstrating guarantees of optimality.   \n42 4. We present empirical results on simulated students and real-world datasets, showing the   \n43 effectiveness of EduQate over other teacher policies. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "44 2 Related Work and Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "45 2.1 Restless Multi-Armed Bandits ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "46 The selection of the right time and manner for limited interventions is a problem of great practical im  \n47 portance across various domains, including health intervention [17, 5], anti-poaching operations [20],   \n48 education [13, 6, 2], etc. These problems share a common characteristic of having multiple arms   \n49 in a Multi-armed Bandit (MAB) problem, representing entities such as patients, regions of a forest,   \n50 or students\u2019 mastery of concepts. These arms evolve in an uncertain manner, and interventions are   \n51 required to guide them from \"bad\" states to \"good\" states. The inherent challenge lies in the limited   \n52 number of interventions, dictated by the limited resources (e.g., public health workers, the number of   \n53 student interactions). RMAB, a generalization of MAB, offers an ideal model for representing the   \n54 aforementioned problems of interest. RMAB allows non-active bandits to also undergo the Markovian   \n55 state transition, effectively capturing uncertainty in arm state transitions (reflecting uncertain state   \n56 evolution), actions (representing interventions), and budget constraints (illustrating limited resources).   \nRMABs and the associated Markov Decision Processes (MDP) for each arm offer a valuable model for   \n58 representing the learning process. Firstly, leveraging the MDPs associated with each arm provides the   \n59 flexibility to adopt nuanced modeling of learning content, accommodating different learning curves   \nfor various content based on students\u2019 strengths and weaknesses. Secondly, the transition probabilities   \nserve as a useful mechanism to model forgetting (through state decay due to passivity or negligence)   \n62 and learning (through state transitions to the positive state from repeated practice). Considering   \n63 these aspects, RMABs prove to be a beneficial framework for personalizing and generating adaptive   \n64 curricula across a diverse range of students.   \n65 In general, computing the optimal policy for a given set of restless arms in RMABs is recognized as a   \n66 PSPACE-hard problem [18]. The Whittle index [26] provides an approach with a tractable solution   \n67 that is provably optimal, especially when each arm is indexable. However, proving indexability can   \n68 be challenging and often requires specification of the problem\u2019s structure, such as the optimality of   \n69 threshold policies [17, 16]. Moreover, much of the research on Whittle Index policies has focused   \n70 on two-action settings or requires prior knowledge of the transition matrix of the RMABs. Meeting   \n71 these conditions proves challenging in the educational context, where diverse students interact with   \n72 educational systems, each possessing different prior knowledge and distinct learning curves for   \n73 various topics.   \n74 WIQL [5], on the other hand, employs a Q-learning-based method to estimate the Whittle Index and   \n75 has demonstrated provable optimality without requiring prior knowledge of the transition matrix. We   \n76 utilize WIQL as a baseline method in our subsequent experiments.   \n77 In a recent investigation by [12], RMABs were explored within a network framework, requiring the   \n78 agent to manage a budget while allocating a high-cost, high-benefti resource to one arm to \u201cunlock\"   \n79 potential lower-cost, intermediate-benefit resources for the arm\u2019s neighbors. The network effects   \n80 emphasized in their work are triggered by an intentional, active action, enabling the agent to choose   \n81 to propagate positive externalities to a selected arm\u2019s neighbors within budget constraints. In contrast,   \n82 our study delves into scenarios where network effects are indirect results of an active action, and the   \n83 agent lacks direct control over such effects. Thus, the challenge lies in accurately modeling these   \n84 network effects and leveraging them when beneficial.   \n86 In the realm of education, numerous researchers have explored optimizing the sequencing of in  \n87 structional activities and content, assuming that optimal sequencing can significantly impact student   \n88 learning. RL is a natural approach for making sequential decisions under uncertainty [1]. While RL   \n89 has seen success in various educational applications, effectively sequencing interdependent content in   \n90 a personalized and adaptive manner has yielded mixed or insignificant results compared to baseline   \n91 teacher policies [11, 21, 8]. In general, these RL works focus on data-driven methods using student   \n92 activity logs to estimate students\u2019 knowledge states and progress, assuming that the interdependencies   \n93 between learning content are encapsulated in students\u2019 learning histories [9, 3, 19]. In contrast, our   \n94 work focuses on modelling these interdependencies directly.   \n95 Of particular relevance are factored MDPs applied to skill acquisition introduced by [11]. While fac  \n96 tored MDPs account for interdependencies amongst skills, decentralized policy learning is infeasible   \n97 as policies must consider the joint state space. Our work leverages the advantage of decentralized   \n98 policy learning provided by RMABs and introduces a novel decentralized learning approach that   \n99 exploits interdependencies between arms.   \n100 Complementary to RL methods in education is the utilization of knowledge graphs to uncover   \n101 relationships between learning content [9]. Existing research primarily focuses on establishing these   \n102 relationships through data-driven methods (e.g. [7, 23]) often leveraging student-activity logs. In this   \n103 work, we complement such research by presenting an approach where bandit methods can effectively   \n104 operate with knowledge graphs derived by such methods. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "105 3 Model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "106 In this section, we introduce the Restless Multi-Armed Bandits for Education (EdNetRMABs). It   \n107 is important to note that while we specifically apply EdNetRMABs to the education setting, the   \n108 framework can be seamlessly translated to other scenarios where modeling the effects of active   \n109 actions within a network is critical. For ease of access, a table of notations is provided in Table 2.   \n110 In education, a teacher recommends learning content, or items, to maximize student education, often   \n111 with content from online platforms. Items are grouped by topics, such as \u201cGeometry,\" where exposure   \n112 to one piece of content can enhance knowledge across others in the same group. This cumulative   \n113 learning effect which we refer to as \u201cnetwork effects\", implies that exposure to an item is likely   \n114 to positively impact the student\u2019s success on items within the same group. A successful teacher   \n115 accurately estimates a student\u2019s knowledge state over repeated interactions, leveraging these network   \n116 effects to promote both breadth and depth of understanding through recommendations. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "117 3.1 EdNetRMABs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "118 The RMAB model tasks an agent with selecting $k$ arms from $N$ arms, constrained by a limit on the   \n119 number of arms that can be pulled at each time step. The objective is to find a policy that maximizes   \n120 the total expected discounted reward, assuming that the state of each arm evolves independently   \n121 according to an underlying MDP.   \n122 The EdNetRMABs model extends RMABs by allowing for active actions to propagate to other arms   \n123 dependent on the current arm when it is being pulled, thus relaxing the assumption of independent   \n124 arms. This is operationalized by organising the arms in a network, and pulling of an arm results in   \n125 changes for its neighbors, or members in the same group. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "126 When applied to education setting, the EdNetRMABs is formalized as follows: ", "page_idx": 2}, {"type": "text", "text": "127 Arms Each arm, denoted as $i\\,\\in\\,1,...,N$ , signifies an item. In the context of this networked   \n128 environment, each arm belongs to a group $\\phi\\,\\in\\,\\{1,...,L\\}$ representing the overarching topic that   \n129 encompasses related items. It\u2019s important to note that arm membership is not mutually exclusive,   \n130 allowing arms to be part of multiple groups. This flexibility enables a more nuanced modeling of   \n131 interdependencies among educational content. For instance, a question involving the calculation of   \n132 the area of a triangle may span both arithmetic and geometry groups.   \n133 State space In this framework, each arm possesses a binary latent state, denoted as $s_{i}\\in\\{0,1\\}$ ,   \n134 where $\\\"0\"$ represents an \u201cunlearned\" state, and \u201c1\" indicates a \u201clearned\" state. Considering all arms   \n135 collectively, these states serve as a representation of the student\u2019s overall knowledge state. In the   \n136 current work, it is assumed that the states of all arms are fully observable, providing a comprehensive   \n137 model of the student\u2019s understanding of the various educational concepts.   \n138 Action space To capture the network effects associated with arm pulls, we depart from the conven  \n139 tional RMAB framework with a binary action space $A=\\{0,1\\}$ by introducing a pseudo-action. In   \n140 this modified setup, the action space is extended to $A=\\{0,1,2\\}$ , where actions 0 and 2 represent   \n141 \u201cno-pull\" and \u201cpull\", as commonly used in bandit literature. Notably, in EdNetRMABs, a third action   \n142 1 is introduced to simulate the network effects resulting from pulling another arm within the same   \n143 group. It is important to clarify that agents do not directly engage with action 1 but we employ it   \n144 solely for modeling network effects, hence the term \u201cpseudo-action\".   \n145 Transition function For a given arm $i$ , let $P_{s,s^{\\prime}}^{a,i}$ represent the probability of the arm transitioning   \n146 from state $s$ to $s^{\\prime}$ under action $a$ . It\u2019s noteworthy that, in typical real-world educational settings, the   \n147 actual transition functions governing the states of the arms are often unknown and, even for the same   \n148 concept, may vary among students due to differences in prior knowledge. To address this challenge,   \n149 we adopt model-free approaches in this study, devising methods to compute the teacher policy without   \n150 relying on explicit knowledge of these transition functions. In the following experiments, we maintain   \n151 the assumption of non-zero transition probabilities, and enforce constraints that are aligned with the   \n115523 cnuergraetinvt ed sotamtaei: n $\\bar{P}_{0,1}^{0}<\\bar{P}_{1,1}^{0}$ h, $P_{0,1}^{1}<P_{1,1}^{1}$ m aonred $P_{0,1}^{2}<P_{1,1}^{2}$ ;y  (iini)  tTheh ep aorsimti tvee nsdtsa tteo  tihmapn rcohvae ntghee  tloa ttehnet   \n154 state if more efforts is spent on that arm, i.e., it is active or semi-active: $P_{0,1}^{0}<P_{0,1}^{1}<P_{0,1}^{2}$ and   \n155 $P_{1,1}^{0}<P_{1,1}^{1}<P_{1,1}^{2}$ .   \n156 With the formalization of the EdNetRMABs model provided, we now apply it to an educational   \n157 context. In this scenario, the agent assumes the role of a teacher and takes actions during each time   \n158 step $t\\in\\{1,...,T\\}$ . Specifically, at each time step, the teacher recommends an item for the student to   \n159 study. We represent the vector of actions taken by the teacher at time step $t$ as $\\mathbf{a}^{t}\\in\\{0,1,2\\}^{N}$ . Here,   \n160 arm $i$ is considered to be active at time $t$ if $a^{t}(\\dot{i})=2$ and passive when $a^{t}(i)=0$ . When arm $i$ is   \n161 pulled, the set of arms that share the same group membership as arm $i$ , denoted as $\\phi_{i}^{-}$ under goes   \n162 the pseudo-action, represented as $a^{t}(j)=\\bar{1}$ for all $j\\in\\phi^{-}$ . In our framework, the teacher agent   \n163 acts on exactly one arm per time step to simulate the real-world constraint that the teacher can only   \n164 recommend one concept to students $(\\textstyle\\sum_{i}I_{a^{t}(i)=2}=1,\\forall t\\;)$ . Subsequent to taking action, the teacher   \n165 receives $\\mathbf{s}^{t}\\in\\{0,1\\}^{N}$ , a vector refle cting the state of all arms, and reward $\\begin{array}{r}{r_{t}=\\sum_{i=1}^{N}s^{t}(i)}\\end{array}$ . The   \n166 vector $\\mathbf{s}^{\\mathbf{t}}$ represents the overall knowledge state of the student. The teacher agent\u2019s goal, therefore, is   \n167 to maximize the long term rewards, either discounted or averaged. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "168 4 EduQate ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "169 Q-learning [25] is a popular reinforcement learning method that enables an agent to learn optimal   \n170 actions in an environment by iteratively updating its estimate of state-action value, $Q(s,a)$ , based on   \n171 the rewards it receives. At each time step $t$ , the agent takes an action $a$ using its current estimate of $Q$   \n172 values and current state $s$ , thus received a reward of $r(s)$ and new state $s^{\\prime}$ . We provide an abridged   \n173 introduction to Q-learning in the Appendix F.   \n174 Expanding upon Q-learning, we introduce EduQate, a tailored Q-learning approach designed for   \n175 learning Whittle-index policies in EdNetRMABs. In the interaction with the environment, the agent   \n176 chooses a single item, represented by arm $i$ , to recommend to the student. In this context, the agent   \n177 possesses knowledge of the group membership $\\phi_{i}$ of the selected arm and observes the rewards   \n178 generated by activating arm $i$ and semi-activating arms in $\\phi_{i}^{-}$ . EduQate utilizes this interaction to   \n179 learn the Q-values for all arms and actions.   \n180 To adapt Q-learning to EdNetRMABs, we propose leveraging the learned Q-values to select the arm   \n181 with the highest estimate of the Whittle index, defined as: ", "page_idx": 3}, {"type": "text", "text": "Input: Number of arms $N$   \nInitialize $Q_{i}(s,a)\\gets0$ and $\\lambda_{i}(s)\\gets0$ for each state $s\\in S$ and each action $a\\in\\{0,1,2\\}$ , for each   \narm $i\\in{1,...,N}$ .   \nInitialize replay buffer $D$ with capacity $C$ .   \nfor $t$ in $1,...,T$ do $\\epsilon\\gets\\frac{N}{N+t}$ With probability $\\epsilon$ , select one arm uniformly at random. Otherwise, select arm with highest Whittle Index, $i=\\arg\\operatorname*{max}_{i}\\lambda_{i}$ . for arm $n$ in $1,...,N$ do if $n\\not=i$ then Set arm $n$ to passive, $a_{n}^{t}=0$ else Set arm $n$ to active, $a_{n}^{t}=2$ for $j\\in\\phi_{i}^{-}$ do Set arms in same group as $i$ to semi-active, $a_{j}^{t}=1$ end for end if end for Execute actions $\\mathbf{a}^{\\mathbf{t}}$ and observe reward $r^{t}$ and next state $s^{t+1}$ for all arms Store experience $(s^{t},\\mathbf{a}^{\\mathbf{t}},\\mathbf{r}^{\\mathbf{t}},\\mathbf{s}^{\\mathbf{t}+\\mathbf{1}})$ in replay buffer $D$ . Sample minibatch $B$ of Experience from replay buffer $D$ . for Experience in minibatch $B$ do Update $Q_{n}(s,a)$ using Q-learning update in Equation 11. Compute $\\lambda_{n}$ using Equation 1 end for   \nend for ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\lambda_{i}=Q(s_{i},a_{i}=2)-Q(s_{i},a_{i}=0)+\\sum_{j\\in\\phi_{i}^{-}}(Q(s_{j},a_{j}=1)-Q(s_{j},a_{j}=0))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "182 Here, $\\lambda_{i}$ is the Whittle Index estimate for arm $i$ . In essence, the Whittle Index of arm $i$ is computed as   \n183 the linear combination of the value associated with taking action on arm $i$ over passivity and the value   \n184 of associated with semi-actively engaging with members from same group, compared to passivity.   \n185 To improve the convergence of Q-learning, we incorporate Experience Replay [15]. This involves   \n186 saving the teacher algorithm\u2019s previous experiences in a replay buffer and drawing mini-batches   \n187 of samples from this buffer during updates to enhance convergence. In Section 4.1, we prove that   \n188 EduQate will converge to the optimal policy. However, in practice, we may not have enough episodes   \n189 to fully train EduQate. Therefore, we propose Experience Replay to mitigate the cold-start problem   \n190 common in RL applications, a common problem where initial student interactions with sub-optimal   \n191 teachers can lead to poor learning experiences [3].   \n192 The pseudo-code is provided in Algorithm 1. Similar to WIQL [5], we employ a $\\epsilon$ -decay policy that   \n193 facilitates exploration and learning in the early steps, and proceeds to exploit the learned Q-values in   \n194 later stages. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "195 4.1 Analysis of EduQate ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "196 In this section, we analyze EduQate closely, and show that EduQate does not alter the optimality   \n197 guarantees of Q-learning under the constraint that $\\boldsymbol{\\mathrm{k}}=1$ (Theorem 1). Our method relies on the   \n198 assumption that teachers are limited to assign 1 item to the student at each time step. Theorem 2   \n199 analyzes EduQate under the conditions that $k>1$ . Since our setting involves the semi-active actions,   \n200 we should compute Equation 1. To reiterate, $\\phi_{i}$ here refers to the group that arm $i$ belongs to, and   \n201 $\\phi_{i}^{-}$ is the same group but does not include arm $i$ . If arm $i$ is selected, then all the remaining arms in   \n202 group $\\phi_{i}^{-}$ should be semi-active.   \n203 Theorem 1 Choosing the top arm with the largest $\\lambda$ value in Equation $^{\\,l}$ is equivalent to maximizing   \n204 the cumulative long-term reward.   \n205 Proof. According to the approach, we select the arm according to the $\\lambda$ value. Assume arm $i$ has   \n206 the highest $\\lambda$ value, then for any arm $j$ where $j\\neq i$ , we have ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\lambda_{i}\\geq\\lambda_{j}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "207 According to the definition of $\\lambda$ in Equation 1, we move the negative part to the other side, and the   \n208 left side becomes: ", "page_idx": 5}, {"type": "equation", "text": "$$\nQ(s_{i},a_{i}=1)+\\sum_{i\\in\\phi_{i}^{-}}(Q(s_{i},a_{i}=1))+Q(s_{j},a_{j}=0)+\\sum_{j\\in\\phi_{j}^{-}}(Q(s_{j},a_{j}=0))\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "209 and the right side is similar. There are three cases: ", "page_idx": 5}, {"type": "text", "text": "210 \u2022 arm $i$ and arm $j$ are not connected, and group $\\phi_{i}$ and $\\phi_{j}$ has no overlap, i.e., $\\phi_{i}\\cap\\phi_{j}=\\emptyset$ . We add   \n211 $\\begin{array}{r l}{\\sum}&{{}\\ Q(s_{z},a_{z}=0)}\\end{array}$ on both sides. This denotes the addition of $Q(s_{z},a_{z}=0)$ for all arm $z$ $z\\notin\\!\\!\\phi_{i}\\!\\land\\!z\\notin\\!\\!\\!\\phi_{j}$   \n212 that are not included in the set of $\\phi_{i}$ or $\\phi_{j}$ . We have the left side: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle Q(s_{i},a_{i}=1)+\\sum_{i\\in\\phi_{i}^{-}}(Q(s_{i},a_{i}=1))+Q(s_{j},a_{j}=0)+\\sum_{j\\in\\phi_{j}^{-}}(Q(s_{j},a_{j}=0))+\\sum_{z\\notin\\phi_{i}\\land z\\notin\\phi_{j}}Q(s_{z},a_{z}=0)}\\\\ &{\\displaystyle=Q(s_{i},a_{i}=1)+\\sum_{i\\in\\phi_{i}^{-}}(Q(s_{i},a_{i}=1))+\\sum_{j\\notin\\phi_{i}}(Q(s_{j},a_{j}=0))}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "213 Similarly, we do the same for the right side and thus, the equation 2 becomes ", "page_idx": 5}, {"type": "equation", "text": "$$\nQ(\\mathbf{s},\\mathbf{a}=\\mathbb{I}_{i})\\geq Q(\\mathbf{s},\\mathbf{a}=\\mathbb{I}_{j})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "214 \u2022 arm $i$ and arm $j$ are not connected, but group $\\phi_{i}$ and $\\phi_{j}$ has overlap, i.e., $\\phi_{i}\\cap\\phi_{j}\\neq\\emptyset$ . In this case, 215 we add $\\sum_{j_{i}\\wedge z\\notin\\phi_{j}}^{-}Q(s_{z},a_{z}=0)-\\sum_{z\\in\\phi_{i}\\cap\\phi_{j}}^{-}\\!\\!\\!\\!\\!Q(s_{z},a_{z}=0)$ on both sides. z \u2208/ ", "page_idx": 5}, {"type": "text", "text": "216 \u2022 arm $i$ and arm $j$ are connected, and group $\\phi_{i}$ and $\\phi_{j}$ has overlap, i.e., $\\phi_{i}\\cap\\phi_{j}\\neq\\emptyset$ , and $\\{i,j\\}\\subset\\phi_{i}\\cap$ 217 $\\phi_{j}$ . This case is similar to the previous one, we add $\\sum_{\\substack{\\gamma\\,d\\neq\\cdots\\,d\\neq\\infty\\,.}}Q(s_{z},a_{z}=\\bar{0})-\\sum_{\\substack{\\gamma\\,\\in\\,\\phi\\cdot\\cap\\,\\Lambda}}Q(s_{z},a_{z}=$ z \u2208/\u03d5i\u2227z \u2208/\u03d5j z\u2208\u03d5i\u2229\u03d5j 218 0) on both sides. ", "page_idx": 5}, {"type": "text", "text": "219 The detailed proof is provided in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "220 Thus when $k=1$ , selecting the top arm according to the $\\lambda$ value is equivalent to maximizing the   \n221 cumulative long-term reward, and is guaranteed to be optimal.   \n222 Theorem 2 When $k>1$ , selecting the k arms is a NP-hard problem. The non-asymptotic tight   \n223 upper bound and non-asymptotic tight lower bound for getting the optimal solution are $o(C(n,k))$   \n224 and $\\omega(N)$ , respectively.   \n225 Proof Sketch. This problem can be considered as a variant of the knapsack problem. If we disregard   \n226 the influence of the shared neighbor nodes for two selected arms, then selecting arm $i$ will not   \n227 influence the future selection of arm $j$ . In such instances, the problem of selecting the $k$ arms is   \n228 simplified to the traditional 0/1 knapsack problem, a classic NP-hard problem. Therefore, when   \n229 considering the effect of shared neighbor nodes for two selected arms, this problem is at least as   \n230 challenging as the $0/1$ knapsack problem. \u25a1   \n231 When $k>1$ , it is difficult to compute the optimal solution, we provide a heuristic greedy algorithm   \n232 with the complexity of $O\\big(\\frac{(2N-k)*k}{2}\\big)$ in Section C in the appendix. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "233 5 Experiment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "234 In this section, we demonstrate the effectiveness of EduQate against benchmark algorithms on   \n235 synthetic students and students derived from a real-world dataset, the Junyi Dataset and the OLI   \n236 Statics dataset. All experiments are run on CPU only. In our experiments, we compare EduQate with   \n237 the following policies:   \n238 \u2022 Threshold Whittle (TW): This algorithm, proposed by [17], utilizes an efficient closed-form   \n239 approach to compute the Whittle index, considering only the pull action as active. It operates under   \n240 the assumption that transition probabilities are known and stands as the state-of-the-art in RMABs.   \n241 \u2022 WIQL: This algorithm employs a Q-learning-based Whittle Index approach [5]. It learns Q-values   \n242 using the pull action as the only active strategy and calculates the Whittle Index based on the   \n243 acquired Q-values.   \n244 \u2022 Myopic: This strategy disregards the impact of the current action on future rewards, concentrating   \n245 solely on predicted immediate rewards. It selects the arm that maximizes the expected reward at   \n246 the immediate time step.   \n247 \u2022 Random: This strategy randomly selects arms with uniform probability, irrespective of the under  \n248 lying state.   \n249 Inspired by work in healthcare settings [12, 14], we compare the policies by the Intervention Benefti   \n250 $(I B)$ , as shown in the following equation: ", "page_idx": 5}, {"type": "image", "img_path": "Y9aRCjuhnV/tmp/4d43991bb7795010d27c0a8fa3ef565111cc26101116ab0fd466f5f13755cea4.jpg", "img_caption": ["Figure 1: Average rewards for the respective algorithms on 3 datasets, averaged across 30 runs. Shaded regions represent standard error. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\nI B_{R a n d o m,E Q}(\\pi)=\\frac{\\mathbb{E}_{\\pi}(R(.))-\\mathbb{E}_{R a n d o m}(R(.))}{\\mathbb{E}_{E Q}(R(.))-\\mathbb{E}_{R a n d o m}(R(.))}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "251 where $E Q$ represents EduQate, and Random represents a policy where the arms are selected at random.   \n252 Prior work in educational settings has demonstrated that random policies can yield robust learning   \n253 outcomes through spaced repetition [9, 10]. Therefore, to establish efficacy, successful algorithms   \n254 must demonstrate superiority over random policies. Our chosen metric, $I B$ , effectively compares   \n255 the extent to which a challenger algorithm $\\pi$ outperforms a random policy in comparison to our   \n256 algorithm. ", "page_idx": 6}, {"type": "text", "text": "257 5.1 Experiment setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "258 In all experiments, we commence by initializing all arms in state 0 and permit the teacher algorithms   \n259 to engage with the student for a total of 50 actions, pulling exactly 1 arm (i.e. $k=1$ ) at each time step.   \n260 Following the completion of these actions, the episode concludes, and the student state is reset. This   \n261 process is iterated across 800 episodes, for a total of 30 seeds. The datasets used in our experiment   \n262 are described below:   \n263 Synthetic dataset. Given the domain-motivated constraints on the transition functions highlighted   \n264 in Section 3.1, we create a simulator based on $N=50$ , $S\\in\\{0,1\\}$ , $N_{\\mathrm{topics}}=20$ . We randomly   \n265 assign arms to topic groups, and allow arms to be assigned to be more than one topic. Under this   \n266 method, number of arms under each group may not be equal. For each trial, a new transition matrix   \n267 is generated to simulate distinct student scenarios.   \n268 Junyi dataset. The Junyi dataset [7] is an extensive dataset collected from the Junyi Academy 1,   \n269 an eLearning platform established in 2012 on the basis of the open-source code released by Khan   \n270 Academy. In this dataset, there are nearly 26 million student-exercise interactions across 250 000   \n271 students in its mathematics curriculum. For this experiment, we selected the top 100 exercises with   \n272 the most student interactions to create our student models. Using our method to generate groups, the   \n273 resultant EdNetRMAB has $N=100$ and $N_{t o p i c s}=21$ .   \n274 OLI Statics dataset. The OLI Statics dataset [4] comprises student interactions with an online   \n275 Engineering Statics course2. In this dataset, each item is assigned one or more Knowledge Compo  \n276 nents (KCs) based on the related topics. After filtering for the top 100 items with the most student   \n277 interactions, the resultant EdNetRMAB includes $N=100$ items and $N_{t o p i c s}=76$ distinct topics. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "Y9aRCjuhnV/tmp/82c83a10b048f59604f7afad574b1e64fc4ee36196c7185cd2c83e3c94263ad2.jpg", "table_caption": ["Table 1: Comparison of policies on synthetic, Junyi, and OLI datasets. $\\mathbb{E}[R]$ represents the average reward obtained in the final episode of training. Statistic after $\\pm$ represents standard error across 30 trials. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "278 5.2 Creating student models ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "279 In this section, we outline the procedure for generating student models aimed at simulating the   \n280 learning process. To clarify, a student model in this context is defined as a set of transition matrices   \n281 for all items. These matrices are employed with EdNetRMABs to simulate the learning dynamics.   \n282 We employ various strategies to model transitions within the RMAB framework. Active transitions   \n283 are determined by assessing the average success rate on a question before and after a learning   \n284 intervention. Passive transitions are influenced by difficulty ratings, with more challenging questions   \n285 more prone to rapid forgetting. Semi-active transitions, on the other hand, are computed as proportion   \n286 of active transition, guided by similarity scores. Here, we provide an outline and the full details can   \n287 be found in Appendix D.   \n288 Active Transitions. We use data on students\u2019 correct response rate after interacting with an item to   \n289 create the transition matrix for action 2, based on the change in correctness rates before and after a   \n290 learning intervention.   \n291 Passive Transitions. To construct passive transitions for items, we use relative difficulty scores to   \n292 determine transitions based on difficulty levels. We assume that higher difficulty correlates with a   \n293 greater likelihood of forgetting, resulting in higher failure rates. Specifically, higher difficulty values   \n294 correspond to higher $P_{1,0}^{\\bar{0}}$ values, indicating a greater likelihood of forgetting. The transition matrix   \n295 for the passive action $a=0$ is then randomly generated, with values influenced by difficulty levels.   \n296 Semi-active Transitions. To derive semi-active transitions, we use similarity scores between exercises   \n297 from the Junyi dataset. We first normalize these scores to the range $[0,1]$ . Then, for any chosen arm,   \n298 we compute its transition matrix under the semi-active action $a=1$ as a proportion of its active   \n299 action transitions, $P_{0,1}^{1}=\\sigma(P_{0,1}^{2})$ , where $\\sigma$ signifies the similarity proportion.   \n300 The arm\u2019s transition matrix for the semi-active action varies due to different similarity scores between   \n301 pairs in the same group. To address this, we use the average similarity score to determine the   \n302 proportion. Since the OLI dataset does not contain similarity ratings, we assume a constant similarity   \n303 rating of $\\sigma=0.8$ for all pairs. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "304 6 Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "305 The experimental results for the synthetic, Junyi, and OLI datasets are shown in Table 1. We report   \n306 the average intervention benefit $I B$ and final episode rewards from thirty independent runs for five   \n307 algorithms: EduQate, TW, WIQL, Myopic, and Random. EduQate consistently outperforms the other   \n308 policies across all datasets, demonstrating higher intervention benefits and average rewards.   \n309 In terms of $I B$ , we note that all challenger policies do not exceed $50\\%$ , indicating two key points.   \n310 First, as noted in prior works [9], our results confirm that random policies in educational settings are   \n311 robust and difficult to surpass, even when algorithms are equipped with knowledge of the learning   \n312 dynamics. Second, our interdependency-aware EduQate performs well over random policies and   \n313 other algorithms, highlighting the importance of considering network effects and interdependencies   \n314 in EdNetRMABs.   \n315 Notably, WIQL, which relies solely on Q-learning for active and passive actions, performs worse   \n316 than a random policy, likely due to misattributing positive network effects to passive actions. Despite   \n317 having access to the transition matrix, TW does not perform as well as the interdependency-aware   \n318 EduQate. While it has demonstrated effectiveness in traditional RMABs, TW weaknesses become   \n319 evident in the current setting, where pulling an arm has wider implications to other arms. Overall,   \n320 EduQate has demonstrated robust and effective performance in maximizing rewards across different   \n321 datasets. Figure 1 shows the average rewards obtained in the final episode for each algorithm.   \n322 Figure 2 provides visualizations of the networks generated from synthetic students and mined from   \n323 real-world datasets. The synthetic dataset produces networks with distinct isolated groups, contrasting   \n324 with the more intricate and interconnected networks from the Junyi and OLI datasets, reflecting   \n325 real-world complexities. Despite these differing topologies and levels of interdependency, EduQate   \n326 performs well under all network setups. In Appendix E.1, we explore the effects of different network   \n327 topologies by varying the number of topics while limiting the membership of each item. We find that   \n328 as network interdependencies are reduced, the network effects diminish, and such EdNetRMABs   \n329 can be approximated to traditional RMABs with independent arms. Under these conditions, our   \n330 algorithm does not perform as well as other baseline policies.   \n331 Finally, an ablation study detailed in Appendix E.2 examines the effectiveness of the replay buffer in   \n332 EduQate. The study shows that the replay buffer helps overcome the cold-start problem, where initial   \n333 learning episodes provide sub-optimal experiences for students [3]. ", "page_idx": 7}, {"type": "image", "img_path": "Y9aRCjuhnV/tmp/45290bad8aa62f3c30c5cadd13cf1f018a64914396765665f70f4a11be2f47cb.jpg", "img_caption": ["Figure 2: This visualization compares network complexities from our experiments. The synthetic dataset (left) shows simpler, isolated groups, while the real-world datasets (Junyi, middle; OLI,right) displays more intricate and interconnected relationships amongst items. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "334 7 Conclusion and Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "335 In this paper, we introduced EdNetRMABs to the education setting, a variant of MAB designed to   \n336 model interdependencies in educational content. We also proposed EduQate, a novel Whittle-based   \n337 learning algorithm tailored for EdNetRMABs. Unlike other Whittle-based algorithms, EduQate com  \n338 putes an optimal policy without requiring knowledge of the transition matrix, while still accounting   \n339 for the network effects of pulling an arm. We demonstrated the guaranteed optimality of a policy   \n340 trained under EduQate and showcased its effectiveness on synthetic and real-world datasets, each   \n341 with its own characteristic.   \n342 Our work assumes that student knowledge states are fully observable and available at all times, which   \n343 is a limitation. Despite this, we believe our work is significant and can inspire further research to   \n344 improve efficiencies in education. For future work, we aim to extend EduQate to handle partially   \n345 observable states and address the cold-start problem in education systems by minimizing the initial   \n346 exploratory phase. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "347 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "348 [1] Richard C Atkinson. Ingredients for a theory of instruction. American Psychologist, 27(10):   \n349 921, 1972.   \n350 [2] Aqil Zainal Azhar, Avi Segal, and Kobi Gal. Optimizing representations and policies for   \n351 question sequencing using reinforcement learning. International Educational Data Mining   \n352 Society, 2022.   \n353 [3] Jonathan Bassen, Bharathan Balaji, Michael Schaarschmidt, Candace Thille, Jay Painter, Dawn   \n354 Zimmaro, Alex Games, Ethan Fast, and John C Mitchell. Reinforcement learning for the   \n355 adaptive scheduling of educational activities. In Proceedings of the 2020 CHI conference on   \n356 human factors in computing systems, pages 1\u201312, 2020.   \n357 [4] Norman Bier. Oli engineering statics - fall 2011 (114 students), 2011. URL https://   \n358 pslcdatashop.web.cmu.edu/DatasetInfo?datasetId $\\mathtt{1=}590$ .   \n359 [5] Arpita Biswas, Gaurav Aggarwal, Pradeep Varakantham, and Milind Tambe. Learn to intervene:   \n360 An adaptive learning policy for restless bandits in application to preventive healthcare. arXiv   \n361 preprint arXiv:2105.07965, 2021.   \n362 [6] Colton Botta, Avi Segal, and Kobi Gal. Sequencing educational content using diversity aware   \n363 bandits. 2023.   \n364 [7] Haw-Shiuan Chang, Hwai-Jung Hsu, and Kuan-Ta Chen. Modeling exercise relationships in   \n365 e-learning: A unified approach. In EDM, pages 532\u2013535, 2015.   \n366 [8] Shayan Doroudi, Vincent Aleven, and Emma Brunskill. Robust evaluation matrix: Towards a   \n367 more principled offilne exploration of instructional policies. In Proceedings of the fourth (2017)   \n368 ACM conference on learning $@$ scale, pages 3\u201312, 2017.   \n369 [9] Shayan Doroudi, Vincent Aleven, and Emma Brunskill. Where\u2019s the reward? a review of rein  \n370 forcement learning for instructional sequencing. International Journal of Artificial Intelligence   \n371 in Education, 29:568\u2013620, 2019.   \n372 [10] Hermann Ebbinghaus. \u00dcber das ged\u00e4chtnis: untersuchungen zur experimentellen psychologie.   \n373 Duncker & Humblot, 1885.   \n374 [11] Derek Green, Thomas Walsh, Paul Cohen, and Yu-Han Chang. Learning a skill-teaching   \n375 curriculum with dynamic bayes nets. In Proceedings of the AAAI Conference on Artificial   \n376 Intelligence, volume 25, pages 1648\u20131654, 2011.   \n377 [12] Christine Herlihy and John P. Dickerson. Networked restless bandits with positive externalities,   \n378 2022.   \n379 [13] Andrew S Lan and Richard G Baraniuk. A contextual bandits framework for personalized   \n380 learning action selection. In EDM, pages 424\u2013429, 2016.   \n381 [14] Dexun Li and Pradeep Varakantham. Avoiding starvation of arms in restless multi-armed bandits.   \n382 In Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent   \n383 Systems, pages 1303\u20131311, 2023.   \n384 [15] Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and   \n385 teaching. Machine learning, 8:293\u2013321, 1992.   \n386 [16] Keqin Liu and Qing Zhao. Indexability of restless bandit problems and optimality of whittle   \n387 index for dynamic multichannel access. IEEE Transactions on Information Theory, 56(11):   \n388 5547\u20135567, 2010.   \n389 [17] Aditya Mate, Jackson A Killian, Haifeng Xu, Andrew Perrault, and Milind Tambe. Collapsing   \n390 bandits and their application to public health interventions. arXiv preprint arXiv:2007.04432,   \n391 2020.   \n392 [18] Christos H Papadimitriou and John N Tsitsiklis. The complexity of optimal queueing network   \n393 control. In Proceedings of IEEE 9th Annual Conference on Structure in Complexity Theory,   \n394 pages 318\u2013322. IEEE, 1994.   \n395 [19] Chris Piech, Jonathan Bassen, Jonathan Huang, Surya Ganguli, Mehran Sahami, Leonidas J   \n396 Guibas, and Jascha Sohl-Dickstein. Deep knowledge tracing. Advances in neural information   \n397 processing systems, 28, 2015.   \n398 [20] Yundi Qian, Chao Zhang, Bhaskar Krishnamachari, and Milind Tambe. Restless poachers:   \n399 Handling exploration-exploitation tradeoffs in security domains. In Proceedings of the 2016   \n400 International Conference on Autonomous Agents & Multiagent Systems, pages 123\u2013131, 2016.   \n401 [21] Avi Segal, Yossi Ben David, Joseph Jay Williams, Kobi Gal, and Yaar Shalom. Combining   \n402 difficulty ranking with multi-armed bandits to sequence educational content. In Artificial   \n403 Intelligence in Education: 19th International Conference, AIED 2018, London, UK, June 27\u201330,   \n404 2018, Proceedings, Part II 19, pages 317\u2013321. Springer, 2018.   \n405 [22] Shitian Shen, Markel Sanz Ausin, Behrooz Mostafavi, and Min Chi. Improving learning &   \n406 reducing time: A constrained action-based reinforcement learning approach. In Proceedings of   \n407 the 26th conference on user modeling, adaptation and personalization, pages 43\u201351, 2018.   \n408 [23] Anni Siren and Vassilios Tzerpos. Automatic learning path creation using oer: a systematic   \n409 literature mapping. IEEE Transactions on Learning Technologies, 2022.   \n410 [24] Utkarsh Upadhyay, Abir De, and Manuel Gomez Rodriguez. Deep reinforcement learning of   \n411 marked temporal point processes. Advances in Neural Information Processing Systems, 31,   \n412 2018.   \n413 [25] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3):279\u2013292, 1992.   \n414 [26] Peter Whittle. Restless bandits: Activity allocation in a changing world. Journal of applied   \n415 probability, pages 287\u2013298, 1988. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "416 Appendix/Supplementary Materials ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "417 A Table of Notations ", "text_level": 1, "page_idx": 11}, {"type": "table", "img_path": "Y9aRCjuhnV/tmp/2a71bb2929d6f709b1939ac1601fec6d21f3046d2f6cd7de54089677101a920f.jpg", "table_caption": ["Table 2: Notations "], "table_footnote": [], "page_idx": 11}, {"type": "text", "text": "418 B Proof for the theorem ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "419 We rewrite the theorem here for ease of explanation. ", "page_idx": 11}, {"type": "text", "text": "420 Theorem 3 Choose top arms according to the $\\lambda$ value in Equation 1 is equivalent to maximize the   \n421 cumulative long-term reward.   \n422 Proof. According to the approach, we select the arm according to the $\\lambda$ value. Assume arm $i$ has   \n423 the highest $\\lambda$ value, then for any arm $j$ , where $i\\neq j$ , we have ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\lambda_{i}\\geq\\lambda_{j}}\\\\ {\\displaystyle Q(s_{i},a_{i}=1)-Q(s_{i},a_{i}=0)+\\sum_{i\\in\\mathfrak{S}_{i}^{-}}(Q(s_{i},a_{i}=1)-Q(s_{i},a_{i}=0))\\geq Q(s_{j},a_{j}=1)-Q(s_{j},a_{j}=0)+\\sum_{j\\in\\mathfrak{S}_{j}^{-}}(Q(s_{j},a_{j}=1)-Q(s_{j},a_{j}=0))}\\\\ {\\displaystyle(s_{i},a_{i}=1)+\\sum_{i\\in\\mathfrak{S}_{i}^{-}}(Q(s_{i},a_{i}=1))+Q(s_{j},a_{j}=0)+\\sum_{j\\in\\mathfrak{S}_{j}^{-}}(Q(s_{j},a_{j}=0))\\geq Q(s_{j},a_{j}=1)+\\sum_{j\\in\\mathfrak{S}_{j}^{-}}(Q(s_{j},a_{j}=1))+Q(s_{i},a_{i}=0)+\\sum_{i\\in\\mathfrak{S}_{i}^{-}}(Q(s_{i},a_{i}=0))}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "424 There are two cases: ", "page_idx": 11}, {"type": "text", "text": "425 \u2022 arm $i$ and arm $j$ are not connected, and group $\\phi_{i}$ and $\\phi_{j}$ has no overlap, i.e., $\\phi_{i}\\cap\\phi_{j}=\\emptyset$ . We   \n426 add $\\begin{array}{r l}{\\sum}&{{}\\ Q(s_{z},a_{z}=0)}\\end{array}$ on both sides, we can have the left side: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle Q(s_{i},a_{i}=1)+\\sum_{i\\in\\phi_{i}^{-}}(Q(s_{i},a_{i}=1))+Q(s_{j},a_{j}=0)+\\sum_{j\\in\\phi_{j}^{-}}(Q(s_{j},a_{j}=0))+\\sum_{z\\notin\\phi_{i}\\land z\\notin\\phi_{j}}Q(s_{z},a_{z}=0)}\\\\ &{=\\!Q(s_{i},a_{i}=1)+\\sum_{i\\in\\phi_{i}^{-}}(Q(s_{i},a_{i}=1))+\\sum_{j\\notin\\phi_{i}^{-}}(Q(s_{j},a_{j}=0))}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "427 Similarly, the right side becomes ", "page_idx": 11}, {"type": "equation", "text": "$$\nQ(s_{j},a_{j}=1)+\\sum_{j\\in\\phi_{j}^{-}}(Q(s_{j},a_{j}=1))+\\sum_{i\\notin\\phi_{j}}(Q(s_{i},a_{i}=0))=Q(\\mathbf{s},\\mathbf{a}=\\mathbb{I}_{j})\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "428 Thus, the equation 2 becomes ", "page_idx": 11}, {"type": "equation", "text": "$$\nQ(\\mathbf{s},\\mathbf{a}=\\mathbb{I}_{i})\\geq Q(\\mathbf{s},\\mathbf{a}=\\mathbb{I}_{j})\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "429 \u2022 arm $i$ and arm $j$ are not connected, but group $\\phi_{i}$ and $\\phi_{j}$ has overlap, i.e., $\\phi_{i}\\cap\\phi_{j}\\neq\\emptyset$ . In this   \n430 case, we add z \u2208/ $\\begin{array}{r}{\\sum_{\\stackrel{\\scriptstyle\\wedge}{_{i}\\wedge z\\notin\\phi_{j}}}Q(s_{z},a_{z}=0)-\\sum_{z\\in\\phi_{i}\\cap\\phi_{j}}Q(s_{z},\\stackrel{\\cdot}{a}_{z}=0)}\\end{array}$ on both sides, we can have the ", "page_idx": 11}, {"type": "text", "text": "431 left side: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle Q(s_{i},a_{i}=1)+\\sum_{i\\in\\partial_{i}^{-}}(Q(s_{i},a_{i}=1))+Q(s_{j},a_{j}=0)+\\sum_{j\\in\\phi_{j}^{-}}(Q(s_{j},a_{j}=0))+\\sum_{z\\notin\\phi_{i}\\land z\\notin\\phi_{j}}Q(s_{z},a_{z}=0)-\\sum_{z\\in\\phi_{i}\\cap\\phi_{j}}Q(s_{z},a_{z}=0)}\\\\ &{\\displaystyle=Q(s_{i},a_{i}=1)+\\sum_{i\\in\\phi_{i}^{-}}(Q(s_{i},a_{i}=1))+\\sum_{j\\in\\phi_{j}}(Q(s_{j},a_{j}=0))+\\sum_{z\\notin\\phi_{i}\\land z\\notin\\phi_{j}}Q(s_{z},a_{z}=0)-\\sum_{z\\in\\phi_{i}\\cap\\phi_{j}}Q(s_{z},a_{z}=0)}\\\\ &{\\displaystyle=Q(s_{i},a_{i}=1)+\\sum_{i\\in\\phi_{i}^{-}}(Q(s_{i},a_{i}=1))+\\sum_{j\\notin\\phi_{i}^{-}}(Q(s_{j},a_{j}=0))}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "432 Similarly, the right side becomes ", "page_idx": 12}, {"type": "equation", "text": "$$\nQ(s_{j},a_{j}=1)+\\sum_{j\\in\\phi_{j}^{-}}(Q(s_{j},a_{j}=1))+\\sum_{i\\notin\\phi_{j}}(Q(s_{i},a_{i}=0))=Q(\\mathbf{s},\\mathbf{a}=\\mathbb{I}_{j})\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "433 \u2022 arm $i$ and arm $j$ are connected, and group $\\phi_{i}$ and $\\phi_{j}$ has overlap, i.e., $\\phi_{i}\\cap\\phi_{j}\\neq\\emptyset$ , and   \n434 $\\{i,j\\}\\subset\\phi_{i}\\cap\\phi_{j}$ . This case is similar to the previous one, we add $\\sum_{\\lambda\\,.\\,\\wedge\\,\\neq\\,\\not\\alpha\\,.}Q(s_{z},a_{z}\\,=\\,0)\\,-$   \n$z\\notin\\phi_{i}\\land z\\notin\\phi_{j}$   \n435 $\\begin{array}{r}{\\sum_{z\\in\\phi_{i}\\cap\\phi_{j}}Q(s_{z},a_{z}=0)}\\end{array}$ on both sides, we can have the left side: $Q(\\mathbf{s},\\mathbf{a}=\\mathbb{I}_{i})$ and the right side   \n436 $Q(\\mathbf{s},\\mathbf{a}=\\mathbb{I}_{j})$ . ", "page_idx": 12}, {"type": "text", "text": "437 ", "page_idx": 12}, {"type": "text", "text": "438 We show that, using Theorem 1, selecting the top arms according to the $\\lambda$ value is guaranteed to   \n439 maximize the cumulative long-term reward, thus proving it to be optimal.   \n440 However when it comes to the case where $k>1$ , selecting the top $k$ arms according to the $\\lambda$ value   \n441 is not guaranteed to be optimal. Let the $\\Phi$ denote the set of arms that are selected, i.e., $a_{i}=2$ if   \n442 $i\\in\\Phi$ . Because once the arm $i$ is added to the selected arm set $\\Phi$ , the benefti of selecting arm $j$ will   \n443 also be influenced if the arm $j$ has the shared connected neighbor arms with arm $i$ , i.e., $\\phi_{i}\\cap\\phi_{j}\\neq\\emptyset$   \n444 To this end, finding the optimal solution is difficult, as we need to list all the possible solution sets.   \n445 The non-asymptotic tight upper bound and non-asymptotic tight lower bound for getting the optimal   \n446 solution are $o(\\bar{C}(n,k))$ and $\\omega(N)$ , respectively.   \n447 We provide the proof for Theorem 2: Proof. When considering the influence of the shared neighbor   \n448 nodes for two selected arms, then selecting arm $i$ will influence the future benefit of selecting arm   \n449 $j$ if arm $i$ and arm $j$ have the overlapped neighbor nodes, i.e., $\\phi_{i}\\cap\\phi_{j}\\neq\\emptyset$ . This is because the   \n450 calculation of $\\lambda_{j}$ , as some arms $z\\in\\phi_{i}\\cap\\phi_{j}$ already receive the semi-active action $a=1$ due to the   \n451 selection of arm $i$ , the subsequent selection of arm $j$ would not double introduce the benefit from   \n452 those arms $z$ who already included in $\\phi_{i}$ . However, if the top $k$ arms ranked according to their $\\lambda$   \n453 value do not have any overlaps in their connected neighbor nodes, i.e, $\\phi_{i}\\cap\\phi_{j}=\\emptyset$ for $\\forall i,j$ , where   \n454 arm $i$ and arm $j$ are top $k$ arms according to $\\lambda$ value. We can directly add those top $k$ arms to the   \n455 action set $\\Phi$ , and the solution is guaranteed to be optimal. Then we have the non-asymptotic tight   \n456 lower bound for getting the optimal solution which is $\\omega(N)$ . Otherwise, if the top $k$ arms ranked   \n457 according to their $\\lambda$ value have any overlaps in their connected neighbor nodes, to get the optimal   \n458 solutions, we need to list all possible combinations of the $k$ arms, which have the $C(n,k)$ cases, and   \n459 computing the corresponding sum of the $\\lambda$ value. In this case, we can derive that the non-asymptotic   \n460 tight upper bound for getting the optimal solution is $o(C(n,k))$ . \u25a1 ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "461 C Greedy algorithm when $k>1$ ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "462 When $k>1$ , it is difficult to compute the optimal solution as we might list all possible solutions, and   \n463 the complexity is $O(C(n,k))$ , Thus we provide a heuristic greedy algorithm to find the near-optimal   \n464 solutions. The process to decide the selected arm set $\\Phi$ is as follows:   \n465   \n466   \n467   \n468   \n469 ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "1. We first compute the independent $\\lambda$ value for each arm $i$ , where $i\\in\\{1,\\ldots,N\\}$ , where $\\begin{array}{r}{\\lambda_{i}=Q(s_{i},a_{i}^{-}=1)-Q(s_{i},a_{i}=0)+\\sum_{j\\in\\phi_{i}^{-}}(Q(s_{i},a_{i}=2)-Q(s_{i},\\dot{a_{i}}=0))}\\end{array}$ ;   \n2. We add the arm with the top $\\lambda$ value to the set $\\Phi$ ;   \n3. We recompute the $\\lambda$ value for the each arm, note that we will remove $Q(s_{j},a_{j})$ in the $\\lambda$ equation if $j\\in\\Phi$ or $j\\in\\phi_{j}$ for $\\forall i\\in\\Phi$ ; ", "page_idx": 12}, {"type": "text", "text": "470 4. we add the arm with the top $\\lambda$ value to the set $\\Phi$ , and repeat the step 3 and 4 until we add $k$   \n471 arms to set $\\Phi$ .   \n472 The intuition of such a heuristic greedy algorithm is to add the arm that maximizes the marginal gain   \n473 to the action. And the complexity for the greedy algorithm is $O\\big(\\frac{(2N-k)*k}{2}\\big)$ . ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "474 D Generating Student Models from Junyi and OLI Dataset ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "475 In this section, we describe the features in Junyi and OLI dataset which we use in developing the   \n476 transition matrices.   \n477 The datasets contain the following features which we use in various aspects to generate the student   \n478 models and the network:   \n479 \u2022 Topic & Knowledge Component Classification: Items are classified into topics (Junyi) or   \n480 KCs (OLI). This classification is employed to group items and establish the initial network.   \n481 \u2022 Similarity: The Junyi dataset offers expert ratings for exercise similarity, enabling a nuanced   \n482 approach to form richer group memberships. High similarity scores group exercises together,   \n483 irrespective of topic tags.   \n484 \u2022 Difficulty: The Junyi dataset provides expert ratings to determine the relative difficulty of   \n485 exercise pairs. In the OLI dataset, we use the overall correct response rate as a measure of   \n486 difficulty.   \n487 \u2022 Rate of Correctness: By analyzing student-exercise interactions, we calculate the frequency   \n488 of correct answers for each question, offering insights into the improvement of knowledge   \n489 over time. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "490 D.1 Active Transitions ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "491 Junyi Dataset The Junyi dataset contains earned_proficiency feature which indicates if the   \n492 student has achieved mastery of the topic based on Khan Academy\u2019s algorithm3. Thus, we take the   \n493 number of attempts before earned_proficiency $=\\!1$ as $P_{0,1}^{2}$ , and the errors made during mastery as   \n494 $P_{1,0}^{2}$   \n495 OLI Dataset We possess records of students\u2019 accuracy on quiz questions after studying specific   \n496 topics. To derive the transition matrix for the student with the corresponding action 2, we utilize the   \n497 change in correctness rate before and after a learning intervention.   \n498 Given that proportion of correct attempts at time $t$ as $a^{t}$ , then $a^{t+1}=P_{0,1}^{2}(1-a^{t})+P_{1,1}^{2}(a^{t})$ . We   \n499 use a linear regressor to estimate the respective $P^{2}$ , constraining it to produce positive values and   \n500 clipping the values to 0.99 when required. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "501 D.2 Passive Transitions ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "502 To construct passive transitions for exercises, we utilize relative difficulty scores to determine   \n503 transitions based on difficulty levels. We operate under the assumption that the difficulty of an   \n504 exercise is linked to its likelihood of being forgotten, thereby resulting in a higher failure rate. More   \n505 precisely, higher difficulty values of an exercise correspond to higher $\\bar{P}_{1,0}^{0}$ values, indicating a greater   \n506 likelihood of forgetting. The transition matrix for the passive action $a=0$ is then randomly generated,   \n507 with the values influenced by the difficulty levels. ", "page_idx": 13}, {"type": "text", "text": "508 D.3 Semi-active Transitions ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "509 To derive semi-active transitions, the Junyi dataset contains similarity scores between two distinct   \n510 exercises, quantifying their similarity on a 9-point Likert scale. Once the transition matrices are   \n511 computed under the active action $a=2$ for all arms, we proceed to calculate the transition matrix   \n512 for the semi-active action $a=1$ . This involves normalizing the similarity scores to the range [0, 1],   \n513 denoted as $\\sigma$ . For any chosen arm/topic, we can then compute its neighbor\u2019s transition matrix under   \n514 the semi-active action a = 1 with P 01,1 $P_{0,1}^{\\mathrm{i}}=\\sigma(P_{0,1}^{2})$ , where $\\sigma$ signifies the similarity proportion. It is   \n515 worth noting that an arm\u2019s transition matrix for the semi-active action varies due to different neighbors   \n516 being selected \u2014 different neighbors correspond to different similarity scores.   \n517 To address this, we can store the transition matrix of semi-active actions for different neighbor   \n518 selection scenarios, preserving the flexibility of our algorithm. In this work, for simplicity, we opt   \n519 not to distinguish the impact of different neighbors being selected. Instead, we calculate the average   \n520 similarity for all arms in a group average them, and use the resultant average as $\\sigma$ .   \n521 For the OLI Statics dataset, we use a constant value of $\\sigma=0.8$ since there are no similarity scores   \n522 available. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "523 E Additional Experiment Results and Discussion ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "524 E.1 Comparing Different Network Setups ", "text_level": 1, "page_idx": 14}, {"type": "image", "img_path": "Y9aRCjuhnV/tmp/3fbf2e36dbfdc6bebe841c55e00c175624074dabb6bcf40232b448d5987c1b85.jpg", "img_caption": ["Figure 3: Average rewards for the respective algorithms, on the last episode of training. Note that as $N_{t o p i c s}$ increase, the network effects are reduced, and most algorithms are not better than a random policy. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Table 3: Comparison of policies on synthetic dataset, with different network setups. Note that that as $N_{t o p i c s}$ increase, the reliability of any algorithms decreases, as seen by the standard deviations of their average $I B$ . EduQate- here refers to the EduQate algorithm without replay buffer. ", "page_idx": 15}, {"type": "table", "img_path": "Y9aRCjuhnV/tmp/8de77eafef6a3e0719b15054ba318edaf92a138218c769e4a85728f2d01ebb7f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "525 We present the results for different network setups in Table 3. We note that as the number of topics   \n526 approach the number of arms (i.e. $N_{t o p i c s}=\\{30,40\\}$ , all algorithms perform in a highly unstable   \n527 manner, as reflected in the standard deviations presented. We emphasizes here that the performance   \n528 of EduQate is dependent on the quality of the network it is working on, and tends to thrive in more   \n529 complex, yet realistic scenarios, such as the Junyi dataset presented in Figure 2. We present an   \n530 example of a graph generated when $N_{t o p i c s}=40$ in Figure 4, where we notice that many arms do   \n531 not belong to a group. Under this network, the EdNetRMAB can be approximated to a traditional   \nRMAB, where the arms are independent of each other. ", "page_idx": 15}, {"type": "image", "img_path": "Y9aRCjuhnV/tmp/bb3d99e3b090f5483aa6875647091370cb6bda3d497479eab6a9f51f61ef95c4.jpg", "img_caption": ["Figure 4: Synthetic network when $N_{t o p i c s}=40$ . Note that some arms are without group members, and do not receive benefits from networks. Node colors represent topic groups. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "Y9aRCjuhnV/tmp/a779fecec4005d98db79d91390275b3ce69145eca538d025d6b79b88ef763a73.jpg", "img_caption": ["Figure 5: Average rewards across 800 episodes of training, across 30 seeds. EduQate- (orange) refers to the EduQate algorithm without replay buffer. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "533 E.2 Ablation of Replay Buffer ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Table 4: Comparison of EduQate with and without (EduQate-) Experience Replay Buffer policies across different datasets. Results reported are of the final episode of training. ", "page_idx": 16}, {"type": "table", "img_path": "Y9aRCjuhnV/tmp/b34f404d9a334ae7688009851780b4d79b01fd8524b9d3975aa5efc11b0a3803.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "534 We investigate the importance of the Experience Replay buffer in EduQate, as shown in Figure 5 and   \n535 Table 4. For the Simulated and Junyi datasets, EduQate without Experience Replay (EduQate-) does   \n536 not achieve the performance levels of the full EduQate algorithm within 800 episodes, highlighting the   \n537 importance of methods that aid Q-learning convergence. In real-world applications, slow convergence   \n538 can result in students experiencing a curriculum similar to a random policy, leading to sub-optimal   \n539 learning experiences during the early stages. This issue is known as the cold-start problem [3].   \n540 Future work in EdNetRMABs should explore methods to overcome cold-start problems and improve   \n541 convergence in Q-learning-based methods. ", "page_idx": 16}, {"type": "text", "text": "542 F Q-Learning ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "543 Q-learning [25] is a popular reinforcement learning method that enables an agent to learn optimal   \n544 actions in an environment by iteratively updating its estimate of state-action value, $Q(s,a)$ , based on   \n545 the rewards it receives. The objective, therefore, to learn $Q^{*}(s,a)$ for each state-action pair of an   \n546 MDP, given by: ", "page_idx": 16}, {"type": "equation", "text": "$$\nQ^{*}(s,a)=r(s)+\\sum_{s^{\\prime}\\in S}P(s,a,s^{\\prime})\\cdot V^{*}(s^{\\prime})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "547 where $V^{*}\\!\\left(s^{\\prime}\\right)$ is the optimal expected value of a state, is given by: ", "page_idx": 16}, {"type": "equation", "text": "$$\nV^{*}(s)=m a x_{a\\in A}(Q(s,a))\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "548 Q-learning estimates $Q^{*}$ through repeated interactions with the environment. At each time step $t$ ,   \n549 the agent takes an action $a$ using its current estimate of $Q$ values and current state $s$ , thus received a   \n550 reward of $r(s)$ and new state $s^{\\prime}$ . Q-learning then updates the current estimate using the following: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{n e w}(s,a)\\leftarrow(1-\\alpha)\\cdot Q_{o l d}(s,a)}\\\\ &{\\qquad\\qquad\\qquad+\\,\\alpha\\cdot(r(s)}\\\\ &{\\qquad\\qquad\\qquad+\\,\\gamma\\cdot m a x_{a\\in A}Q_{o l d}(s^{\\prime},a))}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "551 where $\\alpha\\in[0,1]$ is the learning rate that controls updates, and $\\gamma$ is the discount on future rewards   \n552 associated with the MDP. ", "page_idx": 17}, {"type": "text", "text": "553 G Experiment Details and Hyperparameters ", "text_level": 1, "page_idx": 17}, {"type": "table", "img_path": "Y9aRCjuhnV/tmp/3c23de20a3a4d7e5d12ecacd3655d64497cf5d56b47f02ef83d0b21ff71185f2.jpg", "table_caption": [], "table_footnote": ["Table 5: Hyperparameters for Replay Buffer and Q-learning "], "page_idx": 17}, {"type": "text", "text": "554 H NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "556 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n557 paper\u2019s contributions and scope?   \n558 Answer: [Yes]   \n559 Justification: We summarize our contributions and provide the scope of the paper in the   \n560 abstract and introduction.   \n561 Guidelines:   \n562 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n563 made in the paper.   \n564 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n565 contributions made in the paper and important assumptions and limitations. A No or   \n566 NA answer to this question will not be perceived well by the reviewers.   \n567 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n568 much the results can be expected to generalize to other settings.   \n569 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n570 are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "75 Guidelines:   \n76 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n77 the paper has limitations, but those are not discussed in the paper.   \n78 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n79 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n0 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n81 model well-specification, asymptotic approximations only holding locally). The authors   \n2 should reflect on how these assumptions might be violated in practice and what the   \n83 implications would be.   \n4 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n85 only tested on a few datasets or with a few runs. In general, empirical results often   \n86 depend on implicit assumptions, which should be articulated.   \n7 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n88 For example, a facial recognition algorithm may perform poorly when image resolution   \n89 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n90 used reliably to provide closed captions for online lectures because it fails to handle   \n91 technical jargon.   \n92 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n93 and how they scale with dataset size.   \n4 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n95 address problems of privacy and fairness.   \n96 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n97 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n98 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n99 judgment and recognize that individual actions in favor of transparency play an impor  \n00 tant role in developing norms that preserve the integrity of the community. Reviewers   \n01 will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "602 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "603 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n604 a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Proofs are provided in Appendix 4.1. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided inappendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "618 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "9 Question: Does the paper fully disclose all the information needed to reproduce the main ex0 perimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Experriment details are provided in both the main body and the appendix. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Justification: Code and the transition matrices are provided as supplementary materials.   \nGuidelines: \u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). \u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. \u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. \u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. \u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Relevant details are provided in the main body, as well as the appendix. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: In our experiments, we report and display the standard error across all seeds. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean. ", "page_idx": 20}, {"type": "text", "text": "712 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n713 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n714 of Normality of errors is not verified.   \n715 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n716 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n717 error rates).   \n718 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n719 they were calculated and reference the corresponding figures or tables in the text.   \n720 8. Experiments Compute Resources   \n721 Question: For each experiment, does the paper provide sufficient information on the com  \n722 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n723 the experiments?   \n724 Answer: [Yes]   \n725 Justification: The current paper only requires CPU-level of compute and is mentioned in the   \n726 Experiment section.   \n727 Guidelines:   \n728 \u2022 The answer NA means that the paper does not include experiments.   \n729 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n730 or cloud provider, including relevant memory and storage.   \n731 \u2022 The paper should provide the amount of compute required for each of the individual   \n732 experimental runs as well as estimate the total compute.   \n733 \u2022 The paper should disclose whether the full research project required more compute   \n734 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n735 didn\u2019t make it into the paper).   \n736 9. Code Of Ethics   \n737 Question: Does the research conducted in the paper conform, in every respect, with the   \n738 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n739 Answer: [Yes]   \n740 Justification: All datasets used were anonymized by the respective authors.   \n741 Guidelines:   \n742 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n743 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n744 deviation from the Code of Ethics.   \n745 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n746 eration due to laws or regulations in their jurisdiction).   \n747 10. Broader Impacts   \n748 Question: Does the paper discuss both potential positive societal impacts and negative   \n749 societal impacts of the work performed?   \n750 Answer: [Yes]   \n751 Justification: The current work has positive implications for applied machine learning in   \n752 education settings, and is discussed in the Introduction section. As far as we can see, we   \n753 don\u2019t think there are negative impacts for education.   \n754 Guidelines:   \n755 \u2022 The answer NA means that there is no societal impact of the work performed.   \n756 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n757 impact or why the paper does not address societal impact.   \n758 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n759 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n760 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n761 groups), privacy considerations, and security considerations.   \n762 \u2022 The conference expects that many papers will be foundational research and not tied   \n763 to particular applications, let alone deployments. However, if there is a direct path to   \n764 any negative applications, the authors should point it out. For example, it is legitimate   \n765 to point out that an improvement in the quality of generative models could be used to   \n766 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n767 that a generic algorithm for optimizing neural networks could enable people to train   \n768 models that generate Deepfakes faster.   \n769 \u2022 The authors should consider possible harms that could arise when the technology is   \n770 being used as intended and functioning correctly, harms that could arise when the   \n771 technology is being used as intended but gives incorrect results, and harms following   \n772 from (intentional or unintentional) misuse of the technology.   \n773 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n774 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n775 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n776 feedback over time, improving the efficiency and accessibility of ML).   \n777 11. Safeguards   \n778 Question: Does the paper describe safeguards that have been put in place for responsible   \n779 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n780 image generators, or scraped datasets)?   \n781 Answer: [NA]   \n782 Justification: The current paper does not release any new assets.   \n783 Guidelines:   \n784 \u2022 The answer NA means that the paper poses no such risks.   \n785 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n786 necessary safeguards to allow for controlled use of the model, for example by requiring   \n787 that users adhere to usage guidelines or restrictions to access the model or implementing   \n788 safety filters.   \n789 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n790 should describe how they avoided releasing unsafe images.   \n791 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n792 not require this, but we encourage authors to take this into account and make a best   \n793 faith effort.   \n794 12. Licenses for existing assets   \n795 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n796 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n797 properly respected?   \n798 Answer: [Yes]   \n799 Justification: Code [17] and datasets [7, 4] were appropriately cited.   \n800 Guidelines:   \n801 \u2022 The answer NA means that the paper does not use existing assets.   \n802 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n803 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n804 URL.   \n805 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n806 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n807 service of that source should be provided.   \n808 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n809 package should be provided. For popular datasets, paperswithcode.com/datasets   \n810 has curated licenses for some datasets. Their licensing guide can help determine the   \n811 license of a dataset.   \n812 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n813 the derived asset (if it has changed) should be provided. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to ", "page_idx": 23}, {"type": "text", "text": "815 the asset\u2019s creators.   \n816 13. New Assets   \n817 Question: Are new assets introduced in the paper well documented and is the documentation   \n818 provided alongside the assets?   \n819 Answer: [NA]   \n820 Justification: [NA]   \n821 Guidelines:   \n822 \u2022 The answer NA means that the paper does not release new assets.   \n823 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n824 submissions via structured templates. This includes details about training, license,   \n825 limitations, etc.   \n826 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n827 asset is used.   \n828 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n829 create an anonymized URL or include an anonymized zip file.   \n830 14. Crowdsourcing and Research with Human Subjects   \n831 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n832 include the full text of instructions given to participants and screenshots, if applicable, as   \n833 well as details about compensation (if any)?   \n834 Answer: [NA]   \n835 Justification: [NA]   \n836 Guidelines:   \n837 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n838 human subjects.   \n839 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n840 tion of the paper involves human subjects, then as much detail as possible should be   \n841 included in the main paper.   \n842 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n843 or other labor should be paid at least the minimum wage in the country of the data   \n844 collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "46   \n47 Question: Does the paper describe potential risks incurred by study participants, whether   \n48 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n49 approvals (or an equivalent approval/review based on the requirements of your country or   \n50 institution) were obtained?   \n51 Answer: [NA]   \n52 Justification: [NA]   \n53 Guidelines:   \n54 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n55 human subjects.   \n56 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n57 may be required for any human subjects research. If you obtained IRB approval, you   \n58 should clearly state this in the paper.   \n59 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n60 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n61 guidelines for their institution.   \n62 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n63 applicable), such as the institution conducting the review. ", "page_idx": 23}]