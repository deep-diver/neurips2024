[{"figure_path": "JxOQeg1NkH/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of RoboMamba. RoboMamba is an efficient robotic VLA model that combines reasoning and manipulation capabilities. First, we integrate and align a vision encoder with the Mamba LLM, endowing our model with common sense and robotic-related reasoning abilities. Subsequently, we introduce an efficient fine-tuning strategy to equip RoboMamba with pose prediction abilities, requiring a few dozen minutes to fine-tune a simple policy head (3.7M parameters). In terms of inference speed, RoboMamba achieves the highest control frequency, surpassing other VLA models, running on an NVIDIA A100 GPU without any quantization or inference acceleration techniques. More real-world downstream tasks are displayed in Figure 4 and Figure 5.", "description": "This figure provides a high-level overview of the RoboMamba architecture and its workflow. It shows how RoboMamba integrates a vision encoder with the Mamba language model to achieve both reasoning and manipulation capabilities.  The diagram highlights the efficient fine-tuning strategy used, the speed advantage over other VLA models, and previews the real-world applications shown in later figures.", "section": "3 RoboMamba"}, {"figure_path": "JxOQeg1NkH/figures/figures_4_1.jpg", "caption": "Figure 2: Overall framework of RoboMamba. RoboMamba projects images onto Mamba's language embedding using a vision encoder and projection layer, which is then concatenated with text tokens and fed into the Mamba model. To predict the position and rotation of the end-effector pose, we inject simple MLP policy heads and use the global token as input, which is generated through a pooling operation from the language output tokens. Training strategy of RoboMamba. For model training, we divide our training pipeline into two stages. In Stage 1, we introduce alignment pre-training (Stage 1.1) and instruction co-training (Stage 1.2) to equip RoboMamba with both common sense and robotic-related reasoning abilities. In Stage 2, we propose robotic manipulation fine-tuning to efficiently empower RoboMamba with low-level manipulation skills.", "description": "This figure illustrates the architecture of RoboMamba, a robotic VLA model.  It shows how visual and language information are processed through a vision encoder, projection layer, and Mamba language model to generate reasoning and actions.  The figure details the model's two-stage training process: Stage 1 focuses on reasoning abilities through alignment and co-training, while Stage 2 adds manipulation capabilities via fine-tuning a policy head.  The diagram highlights the flow of information and the key components involved in each stage.", "section": "3 RoboMamba"}, {"figure_path": "JxOQeg1NkH/figures/figures_8_1.jpg", "caption": "Figure 1: Overview of RoboMamba. RoboMamba is an efficient robotic VLA model that combines reasoning and manipulation capabilities. First, we integrate and align a vision encoder with the Mamba LLM, endowing our model with common sense and robotic-related reasoning abilities. Subsequently, we introduce an efficient fine-tuning strategy to equip RoboMamba with pose prediction abilities, requiring a few dozen minutes to fine-tune a simple policy head (3.7M parameters). In terms of inference speed, RoboMamba achieves the highest control frequency, surpassing other VLA models, running on an NVIDIA A100 GPU without any quantization or inference acceleration techniques. More real-world downstream tasks are displayed in Figure 4 and Figure 5.", "description": "This figure provides a high-level overview of the RoboMamba architecture, highlighting its key components: a vision encoder, the Mamba language model, a reasoning policy head, and a manipulation module.  It emphasizes the model's efficiency in terms of both fine-tuning and inference speed, comparing it to existing VLA models (OpenVLA and ManipLLM).  The diagram also shows examples of how RoboMamba processes questions and generates answers, showcasing its reasoning and manipulation capabilities. Finally, it points to further examples of real-world applications in Figures 4 and 5.", "section": "3 RoboMamba"}, {"figure_path": "JxOQeg1NkH/figures/figures_8_2.jpg", "caption": "Figure 1: Overview of RoboMamba. RoboMamba is an efficient robotic VLA model that combines reasoning and manipulation capabilities. First, we integrate and align a vision encoder with the Mamba LLM, endowing our model with common sense and robotic-related reasoning abilities. Subsequently, we introduce an efficient fine-tuning strategy to equip RoboMamba with pose prediction abilities, requiring a few dozen minutes to fine-tune a simple policy head (3.7M parameters). In terms of inference speed, RoboMamba achieves the highest control frequency, surpassing other VLA models, running on an NVIDIA A100 GPU without any quantization or inference acceleration techniques. More real-world downstream tasks are displayed in Figure 4 and Figure 5.", "description": "This figure provides a high-level overview of the RoboMamba architecture, highlighting its key components: a vision encoder, the Mamba language model, a reasoning policy head, and a manipulation module.  It emphasizes the model's efficiency in terms of both inference speed and fine-tuning requirements.  The diagram also shows examples of the reasoning and manipulation tasks RoboMamba can handle.", "section": "3 RoboMamba"}, {"figure_path": "JxOQeg1NkH/figures/figures_9_1.jpg", "caption": "Figure 1: Overview of RoboMamba. RoboMamba is an efficient robotic VLA model that combines reasoning and manipulation capabilities. First, we integrate and align a vision encoder with the Mamba LLM, endowing our model with common sense and robotic-related reasoning abilities. Subsequently, we introduce an efficient fine-tuning strategy to equip RoboMamba with pose prediction abilities, requiring a few dozen minutes to fine-tune a simple policy head (3.7M parameters). In terms of inference speed, RoboMamba achieves the highest control frequency, surpassing other VLA models, running on an NVIDIA A100 GPU without any quantization or inference acceleration techniques. More real-world downstream tasks are displayed in Figure 4 and Figure 5.", "description": "This figure provides a high-level overview of the RoboMamba architecture, highlighting its key components: a vision encoder, the Mamba language model, a reasoning policy head, and a manipulation module.  It emphasizes the model's efficiency in terms of both training time and inference speed, comparing it favorably to existing VLA models.  The image also shows examples of the model's reasoning and manipulation capabilities.", "section": "3 RoboMamba"}, {"figure_path": "JxOQeg1NkH/figures/figures_19_1.jpg", "caption": "Figure 1: Overview of RoboMamba. RoboMamba is an efficient robotic VLA model that combines reasoning and manipulation capabilities. First, we integrate and align a vision encoder with the Mamba LLM, endowing our model with common sense and robotic-related reasoning abilities. Subsequently, we introduce an efficient fine-tuning strategy to equip RoboMamba with pose prediction abilities, requiring a few dozen minutes to fine-tune a simple policy head (3.7M parameters). In terms of inference speed, RoboMamba achieves the highest control frequency, surpassing other VLA models, running on an NVIDIA A100 GPU without any quantization or inference acceleration techniques. More real-world downstream tasks are displayed in Figure 4 and Figure 5.", "description": "This figure provides a high-level overview of the RoboMamba architecture, highlighting its key components: a vision encoder, the Mamba language model, a reasoning policy head, and a manipulation module.  It emphasizes RoboMamba's efficiency in terms of both fine-tuning and inference speed, comparing its performance to existing VLA models (OpenVLA and ManipLLM). The figure also points towards Figures 4 and 5 for further examples of real-world applications.", "section": "1 Introduction"}, {"figure_path": "JxOQeg1NkH/figures/figures_19_2.jpg", "caption": "Figure 2: Overall framework of RoboMamba. RoboMamba projects images onto Mamba's language embedding using a vision encoder and projection layer, which is then concatenated with text tokens and fed into the Mamba model. To predict the position and rotation of the end-effector pose, we inject simple MLP policy heads and use the global token as input, which is generated through a pooling operation from the language output tokens. Training strategy of RoboMamba. For model training, we divide our training pipeline into two stages. In Stage 1, we introduce alignment pre-training (Stage 1.1) and instruction co-training (Stage 1.2) to equip RoboMamba with both common sense and robotic-related reasoning abilities. In Stage 2, we propose robotic manipulation fine-tuning to efficiently empower RoboMamba with low-level manipulation skills.", "description": "This figure presents the overall architecture of RoboMamba, a robotic VLA model. It shows how the model processes visual and language inputs using a vision encoder, projection layer, and Mamba language model.  The diagram also details the two-stage training process: Stage 1 focuses on reasoning abilities through pre-training and co-training, while Stage 2 focuses on manipulation skills via fine-tuning.  The figure highlights the model's components, data flow, and the specific training strategies used to achieve both reasoning and action capabilities.", "section": "3.2 RoboMamba architecture"}]