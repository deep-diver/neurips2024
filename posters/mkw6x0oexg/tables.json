[{"figure_path": "mkw6x0OExg/tables/tables_6_1.jpg", "caption": "Table 1: The weak and strong detection properties of different evaluation methods. Existing scores like ROAR [19] and FRESH [18], are not weak detectors, which in turn means they are not strong detectors either.", "description": "This table summarizes the ability of different explanation evaluation methods to detect encoding.  A 'weak detector' correctly identifies non-encoding explanations as optimal. A 'strong detector' ranks non-encoding explanations higher than encoding explanations.  The table shows that ROAR and FRESH fail at weak detection and therefore also at strong detection.  EVAL-X manages weak detection but fails at strong detection.  Only STRIPE-X achieves both.", "section": "4 Detecting encoding in explanations"}, {"figure_path": "mkw6x0OExg/tables/tables_8_1.jpg", "caption": "Table 3: ROAR, EVAL-X, and STRIPE-X scores for the image recognition experiment. Higher is better. ROAR scores two encoding explanations PRED and MARG as high as the optimal explanation, meaning it is not even a weak detector. EVAL-X being only a weak detector scores POSI, PRED, and MARG all worse than the optimal explanation under both EVAL-X but not the non-encoding constant explanation (e(x) = \u00a74), denoted fixed. STRIPE-X being a strong detector scores the non-encoding explanations above the negative marginal entropy -H\u2084(y) = -0.69 and scores every encoding construction under that threshold.", "description": "This table presents the results of evaluating different explanation methods (ROAR, EVAL-X, and STRIPE-X) on an image recognition task.  The methods are evaluated based on their ability to correctly identify encoding explanations.  The results show that STRIPE-X outperforms the other methods in detecting encoding explanations, while ROAR fails to detect encoding and EVAL-X only weakly detects it.", "section": "5.2 Detecting encoding on images of dogs and cats"}, {"figure_path": "mkw6x0OExg/tables/tables_21_1.jpg", "caption": "Table 4: Possible values of the inputs, label, and explanation for the DGP in eq. (24)", "description": "This table shows all possible values of inputs (X1, X2, X3), label (y), selection (v), and explanation values (a) for the DGP in equation (24) described in section B.2 of the paper.  It illustrates how an encoding explanation can conceal the control flow input (x3) that determines which of the first two features predicts the label, highlighting the problem of encoding in explanations.", "section": "Encoding explanations conceal predictive inputs that affect the explanation"}, {"figure_path": "mkw6x0OExg/tables/tables_24_1.jpg", "caption": "Table 5: Probability table for the DGP in eq. (3). Conditional on the explanation x3, does predict the label. For example, given knowing x\u2081 = 1, if x3 = 1 implies p(y = 1) = 0.9 but if x3 = 0, p(y = 1) = 0.5. The probability table in Table 5 shows this.", "description": "This table shows the probability distribution of the label (y) given different combinations of inputs (x1, x2, x3) for the data generating process (DGP) defined in equation (3) of the paper.  The key feature is that the probability of y=1 is dependent on the value of x3;  if x3=1, the probability is influenced by x1, and if x3=0, it's influenced by x2. This illustrates the encoding phenomenon where the selection (x3) affects the values (x1, x2) of what predicts the label.", "section": "4.1 Do existing evaluation methods detect encoding?"}, {"figure_path": "mkw6x0OExg/tables/tables_30_1.jpg", "caption": "Table 6: Position-based, prediction-based, and marginal explanation schemes are all encoding. For samples in the set {x : e(x) = v} for one of the selections v that e produces, accuracy < 1 and the KL being non-zero means these explanations are all encoding per Lemma 1.", "description": "This table presents results from an experiment evaluating three types of encoding explanations: position-based (POSI), prediction-based (PRED), and marginal (MARG).  For each encoding type, the accuracy of predicting the selection (Ev) given the values (xv) is shown (Acc.), along with the increase in the predictability of y given xv and Ev (Ev \u2191), and the Kullback-Leibler (KL) divergence between the distributions q(y|xv, Ev=1) and q(y|xv, Ev=0), which measures the information gain from knowing Ev. Lower KL values indicate that Ev provides less additional information about y given xv, therefore suggesting non-encoding.  The results demonstrate that all three encoding types exhibit characteristics of encoding, as indicated by Acc. < 1 and KL > 0, aligning with Lemma 1 in the paper.", "section": "5.2 Detecting encoding on images of dogs and cats"}]