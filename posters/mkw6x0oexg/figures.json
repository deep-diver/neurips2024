[{"figure_path": "mkw6x0OExg/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of the paper. Explanations are produced to find inputs that are relevant to predicting a label. However, explanations can predict the label well due to the selection being predictive of the label beyond the explanation's values. Such explanations are called encoding. In contrast, predicting instead from a non-encoding explanation is equivalent to predicting from the values in the explanation. When explanations are evaluated purely based on the quality of prediction, encoding can go undetected. We classify existing evaluations into non-detectors and weak detectors and develop a strong detector, called STRIPE-X.", "description": "This figure provides a visual overview of the paper's main contributions. It illustrates the process of producing and evaluating explanations, highlighting the problem of encoding where explanations predict labels better than expected based solely on their values. It introduces three types of explanation evaluation methods: non-detectors, weak detectors, and strong detectors. It highlights the role of STRIPE-X, a newly developed strong detector for identifying encoding.", "section": "1 Introduction"}, {"figure_path": "mkw6x0OExg/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of the paper. Explanations are produced to find inputs that are relevant to predicting a label. However, explanations can predict the label well due to the selection being predictive of the label beyond the explanation's values. Such explanations are called encoding. In contrast, predicting instead from a non-encoding explanation is equivalent to predicting from the values in the explanation. When explanations are evaluated purely based on the quality of prediction, encoding can go undetected. We classify existing evaluations into non-detectors and weak detectors and develop a strong detector, called STRIPE-X.", "description": "The figure illustrates the paper's main contributions.  It shows how explanations are generated (producing explanations), evaluated (evaluating explanations), and how the concept of \"encoding\" in explanations arises when a selection of inputs is predictive of the label beyond the values of the selected inputs themselves.  The figure also categorizes existing explanation evaluation methods as non-detectors or weak detectors of encoding, highlighting the novelty of the proposed STRIPE-X method, which is a strong detector of encoding.", "section": "1 Introduction"}, {"figure_path": "mkw6x0OExg/figures/figures_4_1.jpg", "caption": "Figure 1: Overview of the paper. Explanations are produced to find inputs that are relevant to predicting a label. However, explanations can predict the label well due to the selection being predictive of the label beyond the explanation's values. Such explanations are called encoding. In contrast, predicting instead from a non-encoding explanation is equivalent to predicting from the values in the explanation. When explanations are evaluated purely based on the quality of prediction, encoding can go undetected. We classify existing evaluations into non-detectors and weak detectors and develop a strong detector, called STRIPE-X.", "description": "The figure provides a visual overview of the paper's main contributions. It illustrates the process of producing explanations, evaluating their quality, and the challenge of encoding.  The left side shows the process of generating explanations (producing explanations) from inputs, focusing on the difference between encoding and non-encoding explanations. The middle section illustrates how explanations are evaluated (evaluating explanations), highlighting the limitations of existing methods (non-detector and weak detector). The right side focuses on the paper's key contribution: a novel strong detector called STRIPE-X designed to correctly identify and classify explanations.", "section": "1 Introduction"}, {"figure_path": "mkw6x0OExg/figures/figures_7_1.jpg", "caption": "Figure 1: Overview of the paper. Explanations are produced to find inputs that are relevant to predicting a label. However, explanations can predict the label well due to the selection being predictive of the label beyond the explanation's values. Such explanations are called encoding. In contrast, predicting instead from a non-encoding explanation is equivalent to predicting from the values in the explanation. When explanations are evaluated purely based on the quality of prediction, encoding can go undetected. We classify existing evaluations into non-detectors and weak detectors and develop a strong detector, called STRIPE-X.", "description": "This figure gives a high-level overview of the paper. It illustrates the process of producing and evaluating explanations for machine learning models.  The key concept is \"encoding,\" where an explanation predicts the label better than expected based solely on the input values it includes. The figure contrasts encoding and non-encoding explanations, and highlights how existing evaluation methods fail to detect encoding, leading to the development of a new method called STRIPE-X.", "section": "1 Introduction"}, {"figure_path": "mkw6x0OExg/figures/figures_23_1.jpg", "caption": "Figure 1: Overview of the paper. Explanations are produced to find inputs that are relevant to predicting a label. However, explanations can predict the label well due to the selection being predictive of the label beyond the explanation's values. Such explanations are called encoding. In contrast, predicting instead from a non-encoding explanation is equivalent to predicting from the values in the explanation. When explanations are evaluated purely based on the quality of prediction, encoding can go undetected. We classify existing evaluations into non-detectors and weak detectors and develop a strong detector, called STRIPE-X.", "description": "The figure illustrates the paper's main idea: identifying and addressing the \"encoding\" problem in explanations. Encoding explanations achieve high predictive power not from the values of their selected features, but from a hidden dependency between the selection and the label.  The figure shows how explanations are generated, evaluated (using existing methods and a new method STRIPE-X), and how STRIPE-X can detect encoding, which is undetectable by other methods.", "section": "1 Introduction"}, {"figure_path": "mkw6x0OExg/figures/figures_31_1.jpg", "caption": "Figure 1: Overview of the paper. Explanations are produced to find inputs that are relevant to predicting a label. However, explanations can predict the label well due to the selection being predictive of the label beyond the explanation's values. Such explanations are called encoding. In contrast, predicting instead from a non-encoding explanation is equivalent to predicting from the values in the explanation. When explanations are evaluated purely based on the quality of prediction, encoding can go undetected. We classify existing evaluations into non-detectors and weak detectors and develop a strong detector, called STRIPE-X.", "description": "This figure gives a high-level overview of the paper.  It illustrates the process of producing and evaluating explanations, highlighting the problem of \"encoding\" where an explanation predicts the label better than expected from its values alone. The figure contrasts encoding and non-encoding explanations and classifies existing explanation evaluation methods into three categories: non-detectors, weak detectors, and strong detectors. Finally, it introduces the paper's main contribution: STRIPE-X, a strong detector for encoding explanations.", "section": "1 Introduction"}, {"figure_path": "mkw6x0OExg/figures/figures_31_2.jpg", "caption": "Figure 1: Overview of the paper. Explanations are produced to find inputs that are relevant to predicting a label. However, explanations can predict the label well due to the selection being predictive of the label beyond the explanation's values. Such explanations are called encoding. In contrast, predicting instead from a non-encoding explanation is equivalent to predicting from the values in the explanation. When explanations are evaluated purely based on the quality of prediction, encoding can go undetected. We classify existing evaluations into non-detectors and weak detectors and develop a strong detector, called STRIPE-X.", "description": "This figure provides a visual overview of the paper's contributions.  It illustrates the process of producing explanations, evaluating their quality, and identifying the phenomenon of \"encoding.\" Encoding explanations achieve high predictive power not solely from their input values but also from the selection process itself, creating a disconnect between the explanation's apparent and actual informativeness.  The figure highlights the development of STRIPE-X, a novel strong detector designed to identify and resolve encoding issues, classifying existing evaluation methods as non-detectors, weak detectors, and strong detectors to illustrate its novelty.", "section": "1 Introduction"}]