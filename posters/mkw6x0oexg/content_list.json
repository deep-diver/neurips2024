[{"type": "text", "text": "Explanations that reveal all through the definition of encoding ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Aahlad Puli,\u2217 Nhi Nguyen\u2217, Rajesh Ranganath New York University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Feature attributions attempt to highlight what inputs drive predictive power. Good attributions or explanations are thus those that produce inputs that retain this predictive power; accordingly, evaluations of explanations score their quality of prediction. However, evaluations produce scores better than what appears possible from the values in the explanation for a class of explanations, called encoding explanations. Probing for encoding remains a challenge because there is no general characterization of what gives the extra predictive power. We develop a definition of encoding that identifies this extra predictive power via conditional dependence and show that the definition fits existing examples of encoding. This definition implies, in contrast to encoding explanations, that non-encoding explanations contain all the informative inputs used to produce the explanation, giving them a \u201cwhat you see is what you get\u201d property, which makes them transparent and simple to use. Next, we prove that existing scores (ROAR, FRESH, EVAL-X) do not rank non-encoding explanations above encoding ones, and develop STRIPE-X which ranks them correctly. After empirically demonstrating the theoretical insights, we use STRIPE-X to uncover encoding in LLM-generated explanations for predicting the sentiment in movie reviews. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Artificial intelligence can unlock information in data that was previously unknown. In medicine, for example, using AI, researchers have shown that electrocardiograms are predictive of structural heart conditions [1] or new-onset diabetes [2]. Good predictions often lead one to ask what in the input is important for a prediction; this question is a driving factor behind research in interpretability and explainability [3, 4]. One primary direction in interpretability seeks to produce explanations that are subsets of the input that retain the predictability of the label. These types of explanations and interpretations are called feature attributions and have been used to find factors associated with debt defaults [5], to demonstrate that detecting COVID-19 from chest radiographs can rely on nonphysiological signals [6], and to discover a new class of antibiotics [7]. ", "page_idx": 0}, {"type": "text", "text": "Several methods exist for producing feature attributions or explanations. While some methods compute functions of model gradients [8] or look at predictability after removing features [9], other methods attribute scores to different inputs by treating them as players in a game [4, 10] or amortize their explanations by learning a single model to select subsets for each instance [11]. Choosing one from the many feature attribution methods requires an evaluation. There are, however, many approaches to evaluation itself: qualitative ones [12, 13, 14], which are limited to cases where humans have precise knowledge about the inputs relevant to prediction, and quantitative ones [2, 4, 15, 16, 17, 18, 19, 20], which do not require human knowledge. ", "page_idx": 0}, {"type": "text", "text": "Intuitively, a good evaluation method for feature attributions should assign higher scores to explanations that select inputs that are more predictive of the label. However, evaluations that score explanations based on the predictability of the label from the explanation face one major challenge: ", "page_idx": 0}, {"type": "image", "img_path": "mkw6x0OExg/tmp/e8bb85cbfe8c335228af4463d8b03fbe21402a2d119e909f0be32e3938d0c587.jpg", "img_caption": ["Figure 1: Overview of the paper. Explanations are produced to find inputs that are relevant to predicting a label. However, explanations can predict the label well due to the selection being predictive of the label beyond the explanation\u2019s values. Such explanations are called encoding. In contrast, predicting instead from a non-encoding explanation is equivalent to predicting from the values in the explanation. When explanations are evaluated purely based on the quality of prediction, encoding can go undetected. We classify existing evaluations into non-detectors and weak detectors and develop a strong detector, called STRIPE-X. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "encoding. Informally, an encoding explanation is one where the explanation predicts the label beyond what seems plausible from the values of the inputs themselves. The top left panel of Figure 1 shows an explanation that predicts the label of dog or cat depending on whether the explanation is a pixel on the right half or left half of the image respectively. Many explanation methods fit the description of encoding [20, 21]. Further, given that many evaluations only look at the quality of prediction, encoding can go undetected, rendering the evaluations ineffective at picking explanations. ", "page_idx": 1}, {"type": "text", "text": "In addressing encoding, this work makes the following contributions: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Develops a simple statistical definition of encoding via a conditional dependence property. \u2022 Confirms the introduced definition captures existing all the ad-hoc encoding instances. \u2022 Shows that non-encoding explanations are easy to use because they retain all the predictive inputs used to build them, meaning that predictive non-encoding explanations reveal inputs that predict the label to their users, and thus have a \u201cwhat you see is what you get\u201d property. \u2022 Formalizes evaluations\u2019 sensitivity to encoding as weak detection (optimal scoring explanations are non-encoding) and strong detection (non-encoding explanations score above encoding ones). \u2022 Demonstrates that the evaluations ROAR [19] and FRESH [18] do not weakly detect encoding. \u2022 Proves that EVAL-X [20] weakly detects encoding, but does not strongly detect encoding. \u2022 Develops STRIPE- $\\mathbf{\\deltaX}$ and proves that it strongly detects encoding. \u2022 Uses STRIPE-X to uncover encoding in LLM-generated explanations for sentiment analysis. ", "page_idx": 1}, {"type": "text", "text": "Figure 1 provides an overview of this paper. ", "page_idx": 1}, {"type": "text", "text": "2 Evaluating explanations ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We focus on explanation methods where the goal is to produce subsets of the input that predict the label [22, 23]. Explanation methods of this form, also called feature attributions, saliency methods [4, 8, 24], or just \u201cexplanations,\u201d include thresholded rankings from Shapley values [25, 26], LIME [24], and REAL- $\\mathbf{\\nabla}X$ [20]. With $\\mathbf{y}$ as the label and $\\textbf{x}\\in\\textbf{R}^{d}$ as the inputs, let $q(\\mathbf{y},\\mathbf{x})$ be the joint distribution over them. An explanation method $e$ maps the inputs $\\mathbf{x}$ to a binary selection mask $e(\\mathbf{x})$ over the inputs: $e:\\mathbf{R}^{d}\\rightarrow\\{0,1\\}^{d}$ . The explanation $\\mathbf{x}_{e(\\mathbf{x})}$ is a pair: the selection $e(\\mathbf{x})$ and the vector of explanation\u2019s values. For example, if $\\mathbf{x}\\,=\\,[a,b,c]$ is three-dimensional and $e(\\mathbf{x})\\,=\\,[0,1,1]$ , $\\mathbf{x}_{e(\\mathbf{x})}$ consists of the binary mask $e(\\mathbf{x})$ and the values associated with the inputs that correspond to the indices in $e(\\mathbf{x})$ with value 1: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{x}_{e(\\mathbf{x})}=(e(\\mathbf{x}),[b,c]).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We keep track of the indices because the same value can lead to different predictions depending on the index it appears at; for example, in predicting mortality from patient vital signs, a heart rate above 110 can occur in healthy patients but a temperature of $110^{\\circ}\\mathrm{F}$ is almost always fatal. Equivalently, like in existing work [15, 16, 17, 20], one can choose $\\mathbf{x}_{e(\\mathbf{x})}$ to retain the values in the explanation in the same position and mask out those not selected ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{x}_{e(\\mathbf{x})}=e(\\mathbf{x})\\times\\mathbf{x}+(1-e(\\mathbf{x}))\\times{\\mathrm{mask-token}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "For concision, we overload the word \u201cexplanation\u201d to mean the explanation method instead of the random variable $\\mathbf{x}_{e(\\mathbf{x})}$ when it is clear from context. ", "page_idx": 2}, {"type": "text", "text": "Choosing between explanation methods requires evaluation. Explanation methods seek to return inputs that predict the label, so existing evaluations consider how well the explanation $\\mathbf{x}_{e(\\mathbf{x})}$ predicts the label y [15, 16, 17, 20]. To score explanations based on predictive power, an evaluation method $\\alpha(\\cdot)$ takes as arguments both the explanation $e(\\mathbf{x})$ and the joint distribution $q(\\mathbf{y},\\mathbf{x})$ : $\\alpha(q,e)$ . Without loss of generality let higher be better. ", "page_idx": 2}, {"type": "text", "text": "2.1 Encoding: A disconnect between evaluating explanations and the predictiveness of their values ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We give a simple example of encoding to build intuition for the disconnect between predicting the label from the explanation and predicting the label from the explanation\u2019s values. Imagine that the goal is to explain which set of vital signs signal bacterial pneumonia as the diagnosis compared to the common cold. Consider the explanation method that selects the patient\u2019s height when the true probability of pneumonia is high given the whole set of observables (including labs, symptoms, and vital signs) and otherwise selects the patient\u2019s hair color. Physiologically, height and hair color do not indicate that the patient has pneumonia, meaning that this explanation should not be highly predictive of the label. However, by construction, pneumonia is likely exactly when the explanation selects height, and predicting the label from the explanation achieves the same accuracy as predicting with the full conditional arg $\\operatorname*{max}_{y\\in\\{\\$ {pneumonia, cold} $q(\\mathbf{y}\\;=\\;y\\;\\mid\\;\\mathbf{x})$ . Thus, despite the explanation method only selecting physiologically irrelevant inputs, the explanation predicts the label well. ", "page_idx": 2}, {"type": "text", "text": "Encoding examples such as the one above are neither contrived nor unique. For example, Jethani et al. [20] show that certain procedures that learn to explain, when applied to MNIST digit classification, yield explanations that select a background, black pixel that predicts the label at an accuracy $>\\,90\\%$ ; (see Figure 1 in [20]). Other examples of encoding explanations that predict better than what is expected from the explanation\u2019s values exist [20, 21]. Encoding explanations should not score optimally under a good evaluation because the explanation selects inputs that do not appear to predict the label. However, without a general characterization of the discrepancy in predictive power for encoding, finding explanations whose values predict well remains a challenge. The next section develops a definition of encoding. ", "page_idx": 2}, {"type": "text", "text": "3 Formalizing encoding ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Intuitively, encoding is a phenomenon where the information about the label in the explanation $\\mathbf{x}_{e(\\mathbf{x})}$ exceeds what is known from the explanation\u2019s values. As the input $\\mathbf{x}$ determines the explanation $\\mathbf{x}_{e(\\mathbf{x})}$ , the quality of predicting the label $\\mathbf{y}$ from the explanation relies on the information about the label transmitted from $\\mathbf{x}$ to $\\mathbf{x}_{e(\\mathbf{x})}$ . There are two pathways for this transmission; we elaborate below. ", "page_idx": 2}, {"type": "text", "text": "Denoting the values in a subset $\\mathbf{v}$ by $\\mathbf{x_{v}}$ , compare the event this subset takes the values a, i.e. $\\mathbf{x_{v}}=\\mathbf{a}$ to the event that the explanation\u2019s selection is $\\mathbf{v}$ and that the explanation\u2019s values are a, i.e., $\\mathbf{x}_{e(\\mathbf{x})}=(\\mathbf{v},\\mathbf{a})$ . ", "page_idx": 2}, {"type": "text", "text": "1. Knowing that the explanation is $\\mathbf{x}_{e(\\mathbf{x})}=(\\mathbf{v},\\mathbf{a})$ implies not only that the values in the explanation are determined as $\\mathbf{x_{v}}=\\mathbf{a}$ , but also that the selection is determined as $e(\\mathbf{x})=\\mathbf{v}$ . 2. In reverse, knowing that the values of a subset of inputs are $\\mathbf{x_{v}}=\\mathbf{a}$ and knowing the selection $e(\\mathbf{x})=\\mathbf{v}$ implies that the explanation are $\\mathbf{x}_{e(\\mathbf{x})}=(\\mathbf{v},\\mathbf{a})$ . ", "page_idx": 2}, {"type": "text", "text": "Putting these two points together yields an equality between events: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\{\\mathbf{x}:\\mathbf{x}_{e(\\mathbf{x})}=(\\mathbf{v},\\mathbf{a})\\}=\\{\\mathbf{x}:e(\\mathbf{x})=\\mathbf{v}\\}\\cap\\{\\mathbf{x}:\\mathbf{x_{v}}=\\mathbf{a}\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Thus, the two pathways for information between $\\mathbf{x}$ and the explanation $\\mathbf{x}_{e(\\mathbf{x})}$ are the selection $e(\\mathbf{x})$ and explanation\u2019s values $\\mathbf{x_{v}}$ ; see Figure 2. Existing work makes similar intuitive observations but stops short of formalizing the additional predictive power in an explanation $\\mathbf{x}_{e(\\mathbf{x})}$ [20, 21]. ", "page_idx": 3}, {"type": "text", "text": "To formalize this extra predictive power, define the explanation indicator $\\mathbf{E}_{\\mathbf{v}}=\\mathbb{1}[e(\\mathbf{x})=\\mathbf{v}]$ . A little algebra in Appendix A.1 shows the explanation indicator $\\mathbf{E_{v}}$ provides the extra information: ", "page_idx": 3}, {"type": "equation", "text": "$$\nq(\\mathbf{y}\\mid\\mathbf{x}_{e(\\mathbf{x})}=(\\mathbf{v},\\mathbf{a}))=q(\\mathbf{y}\\mid\\mathbf{x_{v}}=\\mathbf{a},\\underbrace{\\mathbf{E_{v}}=\\mathbf{1}}_{\\mathbf{\\Omega}})\\neq q(\\mathbf{y}\\mid\\mathbf{x_{v}}=\\mathbf{a}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Building on this insight, we define encoding as a conditional dependence: ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Encoding). The explanation $e(\\mathbf{x})$ is encoding if there exists an $\\mathbf{S}$ where $q(\\mathbf{x}_{e(\\mathbf{x})}\\in\\mathbf{S})>0$ such that for every $(\\mathbf{v},\\mathbf{a})\\in\\mathbf{S}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{y}\\perp\\!\\!\\!\\left\\Vert\\mathbf{\\hat{\\Pi}}_{\\mathbf{V}}\\mid\\mathbf{x_{v}}=\\mathbf{a}.\\right\\Vert\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "mkw6x0OExg/tmp/ba5bfd66d0e50c8743bfb51d37dddca7f0c3e78a30779eb69cc1780ecd0aebc4.jpg", "img_caption": ["Figure 2: Intuition for encoding: There are two ways the information in the inputs $\\mathbf{x}$ about the label $\\mathbf{y}$ is transmitted to the explanation $\\mathbf{x}_{e(\\mathbf{x})};1)$ through the values in the explanation and 2) the selection $e(\\mathbf{x})$ (red arrow). When the latter happens, the explanation is "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "An example mathematical construction of an encoding explanation is pro- said to be encoding. vided in Appendix B.1. The dependence in Def: Encoding means that for encoding explanations, there is a disconnect between how well the explanation $\\mathbf{x}_{e(\\mathbf{x})}$ predicts the label versus only the explanation\u2019s values $\\mathbf{x_{v}}$ . This disconnect means that evaluations that score explanations based on predictions from the explanation or their transformations [15, 16, 17, 20] can favor explanation methods that select inputs whose values have little relevance to predicting the label. ", "page_idx": 3}, {"type": "text", "text": "Beyond the disconnect in prediction, encoding explanations are undesirable as they conceal predictive inputs that nevertheless affect the explanation. This concealment can lead to incorrect conclusions, such as that inputs outside the selection are irrelevant, or bewilderment because predictive inputs outside the explanation drive changes in the selection in ways that cannot be understood from the explanation itself (Appendix B.2). ", "page_idx": 3}, {"type": "text", "text": "Non-encoding explanations. Conversely, for a non-encoding explanation, there exists no positive measure set of explanations $\\mathbf{x}_{e(\\mathbf{x})}$ , where the explanation indicator has conditional dependence given the explanation\u2019s values. That is, for a set A where $q(\\mathbf{x}_{e(\\mathbf{x})}\\in\\mathbf{A})=1$ , then for all $(\\mathbf{v},\\mathbf{a})\\in\\mathbf{A}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{y}\\perp\\perp\\mathbf{E_{v}}\\mid\\mathbf{x_{v}}=\\mathbf{a}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which in turn guarantees ", "page_idx": 3}, {"type": "equation", "text": "$$\nq(\\mathbf{y}\\mid\\mathbf{x}_{e(\\mathbf{x})}=(\\mathbf{v},\\mathbf{a}))=q(\\mathbf{y}\\mid\\mathbf{x}_{\\mathbf{v}}=\\mathbf{a},\\mathbf{E}_{\\mathbf{v}}=1)=q(\\mathbf{y}\\mid\\mathbf{x}_{\\mathbf{v}}=\\mathbf{a}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Appendix A.2 shows this. A simple example of a non-encoding explanation is a constant explanation that always picks the same subset of inputs, since a constant $\\mathbf{E_{v}}$ is independent of any variable. The information for predicting the label in a non-encoding explanation lives in the explanation\u2019s values. Evaluations based on predictions from the explanation $\\mathbf{x}_{e(\\mathbf{x})}$ of non-encoding explanations will yield explanations where the input values $\\mathbf{x_{v}}$ predict the label. In other words, non-encoding explanations reveal all the informative inputs they depend on, and \u201cwhat you see is what you get\u201d in the explanation. ", "page_idx": 3}, {"type": "text", "text": "3.1 Encoding explanations in the wild ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Def: Encoding encompasses examples in the existing literature beyond the example in Section 2.1. In that example, the information about y lies in the positions in the selection $e(\\mathbf{x})$ , which motivates the name position-based encoding (POSI). This section describes two other informal examples from the literature of encoding explanations, prediction-based encoding (PRED) [21] and marginal encoding (MARG) [20], and explains the intuition behind why they encode. In the appendix, we develop formalizations of these types of encoding and show that these formulations meet Def: Encoding. ", "page_idx": 3}, {"type": "text", "text": "Prediction-based encoding (PRED). To understand how prediction-based encoding occurs, consider the task of sentiment analysis from movie reviews. Assume that reviews can either be of type \"My day was terrible, but the movie was [ADJ1].\" and \"The movie was [ADJ2], but the day was not great.\" where adjective ADJ1 can be \"good\" or \"not great\" and adjective ADJ2 can be \"not great\" or \"terrible\". Due to common English parlance, \"terrible\" indicates bad sentiment more often than \"not great\". Then, in the example setup above, only seeing that the fourth word is \"terrible\" yields bad sentiment with higher probability than when only seeing that the phrase is \"not great\". However, the fourth word does not always describe the movie. An explanation can look at \"not great\" describing the movie as bad but then selects \u201cterrible\u201d to encode the bad sentiment. This explanation encodes because the selected word may not describe the movie but the selection predicts the sentiment. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Marginal encoding (MARG). A different type of encoding occurs when some inputs determine which other inputs determine the label. For example, in Figure 3, the color determines whether the top right patch produces the label or the bottom right patch. Inputs that control where the label comes from are named control flow inputs. For a real-world example, consider the following example from Jethani et al. [20], where the goal is to predict mortality for patients with chest pain. A lab value that checks for heart injury and acts like a control flow input is troponin. Abnormal troponin indicates that cardiac issues exist and cardiac imaging would inform mortality. Normal troponin on the other hand can ", "page_idx": 4}, {"type": "image", "img_path": "mkw6x0OExg/tmp/9c593b21d572b0877b466555404b39a7c3f57b72c70c8e2c5d349d4e175e07b7.jpg", "img_caption": ["Figure 3: Left: Consider data where the color in the left half determines whether the label (\u201ccat\u201d, \u201cdog\u201d) is produced from the top or bottom image on the right. Right: A MARG encoding explanation that produces only the top or the bottom animal image based on the color. The animal image alone says less about the label than knowing the animal image and the color. Knowing the selection determines the color and thus provides additional information about the label. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "indicate that chest pain is unrelated to cardiac health and a chest X-ray would instead inform mortality. Selecting one or the other image signal but not a control flow input like troponin conceals information about why the imaging was relevant to the label. ", "page_idx": 4}, {"type": "text", "text": "Formalization. In Appendix B, we provide mathematical formulations of each informal example and show that they fall under the definition of encoding in Def: Encoding: position-based encoding (Appendix B.3), prediction-based encoding (Appendix B.4), and marginal encoding (Appendix B.5). The key intuition behind all of these is that the explanation $e(\\mathbf{x})$ varies with inputs other than the selected ones, and these additional inputs provide information about the label beyond the selected ones. Next, we turn to detecting encoding via quantitative evaluations. ", "page_idx": 4}, {"type": "text", "text": "4 Detecting encoding in explanations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This section develops notions of sensitivity to encoding for evaluation methods, and uses the mathematical definition of encoding developed in the previous section to establish which methods detect encoding and which do not. Hsia et al. [21] suggest that evaluation methods like EVAL-X can be gamed to produce high scores for encoding explanations by optimizing the evaluation. To study this case, we introduce the notion of weak detection. If the optimal score of an evaluation of explanations does not permit encoding, then that evaluation is said to weakly detect encoding: ", "page_idx": 4}, {"type": "text", "text": "Definition 2 (Weak detection of encoding). An evaluation $\\alpha(q,e)$ of explanations weakly detects encoding if the optimal explanations $e^{*}$ , i.e. $\\alpha(q,e^{*})=\\operatorname*{max}_{e}\\alpha(q,e),$ , are non-encoding. ", "page_idx": 4}, {"type": "text", "text": "Weak detection provides a recipe for finding non-encoding explanations: find the explanation that achieves the maximum score of a weak detector. However, such a recipe would only work when optimizing without constraints because weak detection does not require non-encoding explanations to have a better score than any encoding explanation. Adding this requirement leads to the definition of strong detection. ", "page_idx": 4}, {"type": "text", "text": "Definition 3 (Strong detection of encoding). An evaluation $\\alpha(q,e)$ strongly detects encoding if for any encoding explanation e and non-encoding explanation $e^{\\prime}$ , $\\stackrel{\\cdot}{\\alpha}(q,\\stackrel{\\cdot}{e^{\\prime}})>\\alpha(q,e)$ . ", "page_idx": 4}, {"type": "text", "text": "Evaluations that are not weak detectors cannot be strong detectors because they score some encoding explanation optimally. ", "page_idx": 4}, {"type": "text", "text": "4.1 Do existing evaluation methods detect encoding? ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Here, we consider whether several techniques for evaluating explanations: ROAR [19], FRESH [18], and EVAL- $\\mathrm{X}$ [20] can detect encoding. We analyze these evaluations on the following distribution $q$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{x}=[\\mathbf{a},\\mathbf{b},\\mathbf{c}]\\sim{\\mathcal{B}}(0.5)^{\\otimes3},\\qquad\\quad\\mathbf{y}={\\left\\{\\begin{array}{l l}{\\mathbf{a}}&{\\mathbf{w}.\\mathbf{p}.\\ 0.9\\quad{\\mathrm{else}}\\quad1-\\mathbf{a}\\quad{\\mathrm{if}}\\ \\mathbf{c}=1,}\\\\ {\\mathbf{b}}&{\\mathbf{w}.\\mathbf{p}.\\ 0.9\\quad{\\mathrm{else}}\\quad1-\\mathbf{b}\\quad{\\mathrm{if}}\\ \\mathbf{c}=0.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Consider the explanation $e_{\\mathrm{encode}}(\\mathbf{x})\\,=\\,\\xi_{1}\\,=\\,[1,0,0]$ if $\\textbf{c}=~1$ and $\\xi_{2}\\,=\\,[0,1,0]$ otherwise; this encodes because $\\mathbf{c}$ is used to create the explanation and c predicts the label conditional on a when $\\mathbf{E}_{\\xi_{1}}=1$ . This is a MARG explanation (see Section 3.1). ", "page_idx": 5}, {"type": "text", "text": "ROAR and FRESH do not weakly detect encoding. ROAR and FRESH evaluate explanations by predicting the label from the inputs not selected by the explanation, denoted as $\\mathbf{x}_{-e(\\mathbf{x})}$ ; while ROAR masks out the explanation, FRESH removes them altogether. Both evaluations say an explanation is optimal if the predictions from the remaining covariates are as random as predicting without any covariates at all. In other words, they check how informative $\\mathbf{x}_{-e(\\mathbf{x})}$ is of $\\mathbf{y}$ and provide the highest score for $\\mathbf{y}\\lrcorner\\rVert\\mathbf{x}_{-e(\\mathbf{x})}$ . This independence holds for $e_{\\mathrm{encode}}(\\mathbf{x})$ in eq. (3): ", "page_idx": 5}, {"type": "text", "text": "Proposition 1. For the data generating process (DGP) in eq. (3), ROAR and FRESH assign their respective optimal scores to the encoding explanation $e_{e n c o d e}(\\mathbf{x})$ . ", "page_idx": 5}, {"type": "text", "text": "The proof is in Appendix B.6. The intuition is that the encoding explanation $e_{\\mathrm{encode}}(\\mathbf{x})$ always selects the input that informs the label given the control flow c; removing the only conditionally informative input means that $\\mathbf{x}_{-e_{\\mathrm{cncode}}(\\mathbf{x})}$ has no information about y. In turn, ROAR scores an encoding explanation $\\mathbf{x}_{-e_{\\mathrm{{cncode}}}(\\mathbf{x})}$ optimally, meaning it does not even weakly detect encoding. As FRESH removes the explanation instead of masking, it cannot introduce extra information into the subset of inputs to predict the label; so, FRESH also scores the same encoding explanation optimally. Thus, ROAR and FRESH are not weak detectors of encoding. ", "page_idx": 5}, {"type": "text", "text": "EVAL-X weakly detects encoding but not strongly. EVAL- $\\mathrm{X}$ [10, 26] is an evaluation method and is sometimes called the surrogate model score. The EVAL- $\\mathbf{\\nabla}X$ score with log-probabilities is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{EVAL-X}\\!\\left(q,e\\right):=\\mathbb{E}_{\\left(\\mathbf{v},\\mathbf{a}\\right)\\sim q\\left(\\mathbf{x}_{e\\left(\\mathbf{x}\\right)}\\right)}\\mathbb{E}_{q\\left(\\mathbf{y}\\mid\\mathbf{x}_{e\\left(\\mathbf{x}\\right)}=\\left(\\mathbf{v},\\mathbf{a}\\right)\\right)}\\left[\\log q\\left(\\mathbf{y}\\mid\\mathbf{x_{v}}=\\mathbf{a}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This score measures the expected log-likelihood of the labels given the input values chosen by the explanation method $e$ and is grounded in the sampling distribution $q$ . Log-likelihoods are maximized by matching the true distribution, this leads to EVAL-X\u2019s weak detection: ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. If $e(\\mathbf{x})$ is EVAL-X optimal, then $e(\\mathbf{x})$ is not encoding. ", "page_idx": 5}, {"type": "text", "text": "Appendix A.4 gives a proof. The proof shows that at optimality, the prediction from the values of explanation has to match the prediction from the full inputs. In turn, given the values there is no additional information in $\\mathbf{x}$ about y, which means the explanation indicator $\\mathbf{E_{v}}$ is independent of $\\mathbf{y}$ ; this violates Def: Encoding, which proves the non-encoding nature of EVAL-X-optimal explanations. ", "page_idx": 5}, {"type": "text", "text": "To test strong detection for EVAL-X, we consider explanations constrained to select one input. Such reductive constraints appear in practice because the goal of producing an explanation is often to aid humans who benefit from reduced complexity. Such constraints prohibit explanations from reaching EVAL-X\u2019s optimal score. Compare $e_{e n c o d e}(\\mathbf{x})$ with a non-encoding constant explanation: ", "page_idx": 5}, {"type": "text", "text": "Proposition 2. Let $e_{\\mathbf{c}}(\\mathbf{x})=\\xi_{3}$ . Then, for the DGP in eq. (3), EVAL- $\\langle\\boldsymbol{q},\\boldsymbol{e}_{e n c o d e}\\rangle>\\mathrm{EVAL}{-}\\mathrm{X}(\\boldsymbol{q},\\boldsymbol{e}_{\\mathbf{c}}).$ . ", "page_idx": 5}, {"type": "text", "text": "Thus, EVAL-X is not a strong detector. The intuition is that the first two coordinates a, b predict the label when selected by $e_{\\mathrm{encode}}$ , while the control flow feature does not predict the label. EVAL-X not being a strong detector means that optimizing EVAL-X over a reductive set may yield an encoding explanation. In this case, $e_{\\mathrm{encode}}$ is one of the EVAL-X-optimal reductive explanations (Lemma 6). ", "page_idx": 5}, {"type": "text", "text": "4.2 STRIPE-X: a strong detector of encoding ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Encoding explanations induce the dependence between the label y and the identity of the selection $\\mathbf{E}_{\\mathbf{v}}=\\mathbb{1}[e(\\mathbf{x})=\\mathbf{v}]$ given the values in the explanation $\\mathbf{x_{v}}$ (Def: Encoding). This dependence can be tested for by building on conditional independence tests [27, 28, 29]. Rather than testing, direct ", "page_idx": 5}, {"type": "text", "text": "quantification of dependence can be useful for when combining with other scores, which can be done using instantaneous conditional mutual information: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\phi_{q}(e):=\\mathbb{E}_{(\\mathbf{v},\\mathbf{a})\\sim q(\\mathbf{x}_{e(\\mathbf{x})})}\\mathbf{I}\\left(\\mathbf{E}_{\\mathbf{v}};\\mathbf{y}\\mid\\mathbf{x_{v}}=\\mathbf{a}\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "ENCODE-METER is 0 only when Def: Encoding does not hold: ", "page_idx": 6}, {"type": "text", "text": "Proposition 3. ENCODE-METER $\\phi_{q}(e)=0$ if and only if e is not encoding. ", "page_idx": 6}, {"type": "text", "text": "The proof is in Appendix A.5. Combining EVAL- $\\mathbf{\\nabla}X$ with ENCODE-METER weighed by $\\alpha$ yields a method we call the strongly information-penalized evaluator (STRIPE-X): ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{\\tiny~STRIPE-}\\mathrm{\\tiny~\\!X}_{\\alpha}(q,e):=\\mathrm{\\tiny~EVAL-}\\mathrm{\\tiny~\\!X}(q,e)-\\alpha\\phi_{q}(e).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "For a large enough $\\alpha$ , the added penalty term pushes down the scores of encoding explanations below that of all non-encoding ones, meaning that STRIPE- $\\mathrm{X}$ is a strong detector of encoding: ", "page_idx": 6}, {"type": "table", "img_path": "mkw6x0OExg/tmp/db8fb6c0512ab0aad9aef8dbc25a6b1d261358f703c3302fe54ffd89a79ca80b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Theorem 2. With finite $\\mathbf{H}(\\mathbf{y}\\mid\\mathbf{x})$ and $\\mathbf{H}(\\mathbf{y})$ , for any explanation that encodes e and any that does not encode $e^{\\prime}$ , there exists an $\\alpha^{*}$ such that $\\forall\\alpha>\\alpha^{*}$ STRIPE- $\\cdot\\mathrm{X}_{\\alpha}(q,e^{\\prime})~>\\mathrm{STRIPE}\\!-\\!\\mathrm{X}_{\\alpha}(q,e)$ . ", "page_idx": 6}, {"type": "text", "text": "Table 1: The weak and strong detection properties of different evaluation methods. Existing scores like ROAR [19] and FRESH [18], are not weak detectors, which in turn means they are not strong detectors either. ", "page_idx": 6}, {"type": "text", "text": "The proof is in Appendix A.5. The intuition behind the proof is that for a large enough $\\alpha$ , the STRIPE-X scores for any encoding explanations will be dominated by the information term, and thus will become smaller than any non-encoding explanation whose score is lower bounded by the negative marginal entropy, $-\\mathbf{H}_{q}(\\mathbf{y})$ . Table 1 summarizes the weak and strong detection properties of different evaluations. ", "page_idx": 6}, {"type": "text", "text": "Estimating STRIPE- $\\mathbf{\\deltaX}$ . The first component of STRIPE-X is EVAL-X. Computing EVAL-X (eq. (4)) requires an estimate of the predictive distribution of the label y given $\\mathbf{x_{v}}$ , $q(\\mathbf{y}\\mid\\mathbf{x_{v}})$ [20]. Estimation can be done in two ways. The first way makes use of a surrogate model trained to predict the label from different random subsets using masked tokens [9, 20]. The second way to compute EVAL- $\\mathbf{\\nabla}X$ (eq. (4)) relies on conditional generative models [30, 31]. Both hyperparameters and a combination of the estimators can be chosen to maximize the average log-likelihood on a held-out validation set across random input subsets. ", "page_idx": 6}, {"type": "text", "text": "To estimate the second part of STRIPE- $\\mathbf{\\nabla}X$ , the ENCODE-METER, first expand the mutual information terms in ENCODE-METER, $\\phi_{q}(e)$ , in terms of expected KL: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\phi_{q}(e)=\\mathbb{E}_{(\\mathbf{v},\\mathbf{a})\\sim q(\\mathbf{x}_{e(\\mathbf{x})})}\\mathbb{E}_{\\mathbf{y}\\sim q(\\mathbf{y}~|~\\mathbf{x}_{v}=\\mathbf{a})}\\mathbf{KL}\\left[q(\\mathbf{E}_{\\mathbf{v}}~|~\\mathbf{x}_{\\mathbf{v}}=\\mathbf{a},\\mathbf{y})~\\|~q(\\mathbf{E}_{\\mathbf{v}}~|~\\mathbf{x}_{\\mathbf{v}}=\\mathbf{a})\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The outer expectation can be estimated using samples from the data and the inner expectation over $\\mathbf{y}$ can be estimated using the EVAL-X model $q(\\mathbf{y}\\mid\\mathbf{x_{v}})$ . The distributions over $\\mathbf{E_{v}}$ can be estimated using a classifier of $\\mathbf{E_{v}}$ that randomly masks the label and masks different subsets of the inputs. Further details and a generative way to estimate STRIPE- $\\mathbf{\\nabla}X$ are in Appendix C.1 and Appendix C.3; full algorithms are given in appendix D. ", "page_idx": 6}, {"type": "text", "text": "STRIPE-X in practice. Using STRIPE- $\\mathrm{X}$ to choose between explanations is straightforward: pick the one with the larger score. However, like other evaluations that use learned models, misestimation can pose a problem. With large $\\alpha$ , non-encoding explanations with misestimated ENCODE-METER will have bad STRIPE- $\\mathbf{\\nabla}X$ scores, while with small $\\alpha$ some encoding explanations can have good scores. Across all experiments, we set $\\alpha=20$ , which yielded STRIPE-X scores for known encoding explanations worse than known non-encoding explanations. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section consists of two parts. The first part demonstrates the weak and strong detection capabilities of the evaluations ROAR, EVAL-X, and STRIPE- $\\mathbf{\\nabla}X$ in a simulated setting and on an image recognition task. To demonstrate these capabilities, we run these evaluations on instantiations of POSI, PRED, and MARG. Additionally, we evaluate an existing method that learns to explain under a reductive constraint, called REAL-X [20]. The second part shows how STRIPE-X enables discovering encoding explanations in the wild, without specific knowledge of the DGP or the method that produced the explanation. We employ STRIPE- $\\mathrm{X}$ to uncover encoding in explanations generated by a large language model (LLM) for predicting sentiments from movie reviews. ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathbf{Post}}{e(\\mathbf{x})=\\left\\{\\xi_{4}\\,\\mathrm{if}\\,\\pi(\\mathbf{x})>0.5,\\,\\,\\,\\,}\\,\\Bigg|\\,e(\\mathbf{x})=\\left\\{\\!\\!\\begin{array}{l l}{\\mathbf{arg\\,max}\\,\\mathbf{\\pi}^{\\mathbf{p}}}&{|\\,\\mathbf{x}\\mathbf{e}(\\mathbf{x})=0.5,\\,\\,\\,}\\\\ {\\!\\!\\frac{\\mathbf{arg\\,max}\\,\\mathbf{\\pi}}{\\mathbf{a}\\cdot\\mathbf{i}\\,M|\\leq1}\\,\\mathbf{if}\\,\\pi(\\mathbf{x})>0.5,\\,\\,\\,}\\\\ {\\!\\!\\frac{\\mathbf{arg\\,max}}{\\mathbf{a}\\cdot\\mathbf{i}\\,M|\\leq1}\\,\\mathbf{1}-\\pi(\\mathbf{x}_{M})\\,\\,\\mathrm{else}.}\\end{array}\\right.}\\end{array}\\Bigg|\\,e(\\mathbf{x})=\\left\\{\\!\\xi_{1}\\,\\mathrm{if}\\,\\mathbf{x}_{3}=1,\\,\\,\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "table", "img_path": "", "table_caption": ["Table 2: Here, $\\pi(\\mathbf{x})=q(\\mathbf{y}=1\\mid\\mathbf{x})$ . Different encoding explanation methods that we consider. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.1 Empirically studying the detection of encoding in a simulated setting ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We construct two examples with binary labels y: one discrete input $\\mathbf{x}$ and one that is a hybrid of continuous and discrete components. Both use one binary input in $\\mathbf{x}\\in\\{0,1\\}^{5}$ as a control flow variable and switch the inputs that y depends on. In both DGPs, y only depends on $\\mathbf{x}_{1}$ if $\\mathbf{x}_{3}=1$ , and only on $\\mathbf{x}_{2}$ if $\\mathbf{x}_{3}=0$ ; this means that $\\mathbf{x}_{4},\\mathbf{x}_{5}$ are purely noise. For both DGPs, y is sampled per the following distribution where $\\mathbf{x}_{3}$ determines the subset the y depends on ", "page_idx": 7}, {"type": "equation", "text": "$$\nq(\\mathbf{y}=1\\mid\\mathbf{x})=\\mathbb{1}[\\mathbf{x}_{3}=1]q(\\mathbf{y}\\mid\\mathbf{x}_{1},\\mathbf{x}_{3})+\\mathbb{1}[\\mathbf{x}_{3}=0]q(\\mathbf{y}\\mid\\mathbf{x}_{2},\\mathbf{x}_{3}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Thus, EVAL- $x^{*}$ is achieved by an explanation of size 2: $e({\\bf x})=\\xi_{1}\\!+\\!\\xi_{3}$ if $\\mathbf{x}_{3}=1$ else $e({\\bf x})=\\xi_{2}\\!+\\!\\xi_{3}$ .   \nSee Appendix C.4 for details; the exact DGPs are given in eq. (49) and eq. (50). ", "page_idx": 7}, {"type": "text", "text": "Encoding explanations. Table 2 describes the encoding explanations we consider for this setting. In Appendix C.4, we check that Def: Encoding holds for these explanations in the discrete DGP by estimating the role of the unselected inputs in affecting the explanation and the role of $\\mathbf{E_{v}}$ in predicting y beyond $\\mathbf{x_{v}}$ ; a characterization of Def: Encoding to support this check is in Lemma 1; ", "page_idx": 7}, {"type": "text", "text": "ROAR fails to weakly detect encoding. To empirically test the analysis about ROAR, we study whether ROAR can weakly detect encoding by comparing ROAR\u2019s score on the all-inputs explanation, which achieves the optimal score under ROAR, to MARG. MARG ignores $\\mathbf{x}_{3}$ , which is required to produce the label y in eq. (8). ROAR loglikelihoods for MARG and the all-inputs explanation are approximately $-\\mathbf{H}(\\mathbf{y})=-0.69$ for both DGPs. This result validates that ROAR is not a weak detector because it does not separate the optimal explanation from all encoding explanations. ", "page_idx": 7}, {"type": "text", "text": "EVAL-X is a weak detector of encoding but not a strong detector. EVAL-X log-likelihood scores are given in blue in Figures 4a and $^\\mathrm{4b}$ . EVAL-X, being a weak detector, scores the encoding constructions (POSI, PRED, and MARG) strictly lower than the log-likelihood of the optimal explanation EVAL- $\\cdot X^{*}$ . However, the EVAL-X score for the MARG explanation is $-0.4$ , which is above the score of $-0.6$ achieved by a non-encoding explanation $e(\\mathbf{x})=\\xi_{1}$ ; thus, EVAL-X is not a strong detector. ", "page_idx": 7}, {"type": "text", "text": "Strong detector STRIPE- $\\mathbf{X}$ prices out all the encoding explanations. Figures 4a and $^\\mathrm{4b}$ report STRIPE-X scores for the same set of explanations as above; STRIPE- $\\mathbf{\\nabla}X$ scores are shown in red. Strong detector STRIPE-X scores the non-encoding explanations above the negative marginal entropy $-\\mathbf{H}_{q}(\\mathbf{y})~=~-0.69$ and scores every encoding construction under that threshold. ", "page_idx": 7}, {"type": "image", "img_path": "mkw6x0OExg/tmp/065385f72f5a4504ae291f4e1793d6324f92e086242545f745c09e23183d9eb3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "", "img_caption": ["(b) Results: hybrid DGP. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4: EVAL-X and STRIPE-X scores of the 3 encoding constructions and the nonencoding constant explanation $\\begin{array}{r l}{\\left(e(\\mathbf{x})\\right)}&{{}=}\\end{array}$ $\\xi_{1}$ ), for both DGPs. EVAL-X, being only a weak detector, assigns suboptimal scores to all encoding explanations $(<)$ , but scores some encoding explanations above the constant explanation. On the other hand, STRIPE-X, being a strong detector, pushes down the scores of all the encoding explanations below that of the non-encoding constant explanation that always selects $\\mathbf{x}_{1}$ . ", "page_idx": 7}, {"type": "text", "text": "5.2 Detecting encoding on images of dogs and cats ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The goal of this section is to study the encoding detection capabilities of ROAR, EVAL-X, and STRIPE-X on real data. We consider an image recognition task like the one in Figure 3 with labels and images from the cats_vs_dogs dataset from the Tensorflow package [32]. We break images of size $64\\times64$ into 4 patches each of size $32\\times32$ . In left-right then top-down order, let ${\\bf x}_{1},{\\bf x}_{2},{\\bf x}_{3},{\\bf x}_{4}$ be the upper left, upper right, bottom left, and bottom right patches respectively; $\\mathbf{x}_{1},\\mathbf{x}_{3}$ capture color, and $\\mathbf{x}_{2},\\mathbf{x}_{4}$ are the animal images. With annot(image) denoting the annotation in the cats_vs_dogs dataset the image having a dog or a cat, the whole image is assigned a binary label like this: ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbf{y}=\\mathbb{1}[\\mathbf{x}_{1}=\\mathbf{b}\\mathbf{lue}]\\,\\times\\,\\mathbb{1}[\\mathbf{annot}(\\,\\mathbf{i}\\,\\mathbf{mage}\\,\\,\\,\\mathbf{x}_{2})\\!=\\!\\mathbf{dog}]+\\mathbb{1}[\\mathbf{x}_{1}=\\mathbf{red}]\\,\\times\\,\\mathbb{1}[\\mathbf{annot}(\\,\\mathbf{i}\\,\\mathbf{mage}\\,\\,\\,\\mathbf{x}_{4})\\!=\\!\\mathbf{dog}]\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We consider three encoding explanations (POSI, PRED, MARG) and two non-encoding ones: 1) optimal, which selects the color and the patch that produces the label as dictated by the color, and 2) denoted fixed, which always outputs the bottom right patch $\\mathbf{x}_{4}$ . Appendix C.6 gives details. ", "page_idx": 8}, {"type": "text", "text": "We report the scores assigned to each explanation by ROAR, EVAL- $\\mathrm{X}$ , and STRIPE-X in Table 3. ROAR scores two encoding explanations PRED and MARG as high as the optimal explanation, meaning it is not even a weak detector. POSI, PRED, and MARG all score worse than the optimal explanation under both the weak detector EVAL-X and the strong detector STRIPE-X. However, EVAL-X scores one non-encoding explanation (fixed) worse than two encoding explanations, meaning it is not a strong detector. Being a strong detector, STRIPE-X scores the fixed explanation above the negative marginal entropy $-\\mathbf{H}_{q}(\\mathbf{y})=-0.69$ and scores every encoding construction under that threshold. ", "page_idx": 8}, {"type": "text", "text": "Evaluating explanations produced by REAL- $\\mathbf{\\deltaX}$ [20]. We ran REAL- $\\mathbf{\\nabla}X$ to learn explanations for the simulated setting and the image recognition task. In the simulated setting, REAL-X is run to select one input; Appendix C.4 gives details. In the image recognition task, REAL-X is run to select ", "page_idx": 8}, {"type": "table", "img_path": "mkw6x0OExg/tmp/4b7013170fe64b1f771252d13f2224bc4463102a2fdc4fdb6bb54d9e275d7959.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 3: ROAR, EVAL-X, and STRIPEX scores for the image recognition experiment. Higher is better. ROAR scores two encoding explanations PRED and MARG as high as the optimal explanation, meaning it is not even a weak detector. EVAL-X being only a weak detector scores POSI, PRED, and MARG all worse than the optimal explanation under both EVAL-X but not the non-encoding constant explanation $\\mathbf{\\Psi}^{\\prime}e(\\mathbf{x})\\;=\\;\\xi_{4})$ , denoted fixed. STRIPE-X being a strong detector scores the non-encoding explanations above the negative marginal entropy $-\\mathbf{H}_{q}(\\mathbf{y})=-0.\\bar{69}$ and scores every encoding construction under that threshold. ", "page_idx": 8}, {"type": "text", "text": "one of the four patches as an explanation; Appendix C.6 gives details. In both the simulated setting (see Figures 4a and 4b) and the image recognition task (see Table 3), REAL- $\\mathrm{X}$ fails to achieve the optimal EVAL-X score while achieving a STRIPE-X score below the threshold of negative marginal entropy $-\\mathbf{H}_{q}(\\mathbf{y})=-0.69$ . Upon investigation, we found that REAL-X produced an explanation that matched the MARG construction on at least $80\\%$ of the inputs in the simulated setting. On the image recognition task, REAL- $\\mathrm{X}$ explanation matched the MARG explanation on the whole dataset. In both cases, STRIPE-X, being a strong detector, correctly alerts that the REAL-X explanation encodes. ", "page_idx": 8}, {"type": "text", "text": "5.3 Encoding in LLM-generated explanations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "One can detect encoding in any explanation by checking if the STRIPE-X score falls below the negative marginal entropy. Recent work uses LLMs to produce explanations; e.g. [33] prompt an LLM to generate explanations for reasoning tasks which are later used to improve smaller models. If the LLM explanation encodes, the smaller model can falsely ignore the informative inputs the larger model\u2019s explanation depends on and yet does not reveal. In this section, we evaluate explanations generated by an LLM, Llama 3, for a sentiment analysis task. We consider reviews that take one of two forms: with ADJ1 and ADJ2 as adjectives, the review is ", "page_idx": 8}, {"type": "text", "text": "\u2022 \u2018My day was <ADJ1> and the movie was <ADJ2>. that is it\u2019 or \u2022 \u2018My day was <ADJ1> and the movie was <ADJ2>. oh wait, reverse the adjectives\u2019. ", "page_idx": 8}, {"type": "text", "text": "The second sentence in the review acts as a \u201ccontrol flow\u201d input and determines whether ADJ1 or ADJ2 describes the sentiment about the movie. We prompt Llama 3 (see Appendix C.8) to predict the sentiment and select a few words from the review that were important for that sentiment; the selected parts form the generated explanation. To discourage encoding, the prompt explicitly instructs the LLM to select all the words that the LLM based the selection on; such an explanation, by Lemma 1, would be non-encoding. On the 5 most common selections $e(\\mathbf{x})$ generated by the LLM, we compute the EVAL-X score and the ENCODE-METER $\\phi_{q}(e)$ . The resulting STRIPE-X score is $-2.78$ , falling short of the negative entropy $-\\mathbf{H}_{q}(\\mathbf{y})=-0.6\\mathbf{\\dot{9}}$ , meaning the LLM encodes. We investigated why. ", "page_idx": 8}, {"type": "text", "text": "As an example, consider the review \u2018My day was resplendent and the movie was hollow. That is it.\u2019; the LLM selects only hollow in the explanation. However, the LLM instead selects resplendent when That is it is switched to oh wait, reverse the adjectives. Such occurrences are common. $\\mathrm{{On}>\\,70\\%}$ of the data, the LLM selects the word that describes the movie but does not select the second sentence in the review which controls which adjective describes the movie; this is akin to MARG encoding. Thus, the LLM-generated explanation encodes by looking at the control flow input in the second sentence to find the correct adjectives, but failing to select the control flow input. Such an explanation falsely indicates that only the adjectives are relevant to predicting the label. In contrast, a non-encoding explanation would, in addition to the adjective that describes the movie, reveal control flow words that indicate which adjective predicts the label. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "In summary, despite being instructed to include all the words that were looked at when producing the explanation, the LLM encodes. Building non-encoding explanations with LLMs may require an extensive search over prompts or finetuning guided by scores from STRIPE-X. ", "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "When an explanation is encodes, predictions from the explanation become disconnected from predictions from the values in the explanations. Such explanations can select values with little relevance to the label and yet score highly on the many existing predictive evaluations. We develop a simple statistical definition of encoding. Inverting this definition shows that when non-encoding explanations predict the label, users know the values of those inputs selected in the explanation predict the label. We then show that existing evaluations are either non-detectors (ROAR[19],FRESH [18]) or only weak detectors (EVAL-X [20]). Motivated by this, we introduce a new strong detector, STRIPEX. After empirically demonstrating the detection capabilities (or lack thereof) of said evaluations, we use STRIPE-X to discover encoding in LLM-generated explanations. ", "page_idx": 9}, {"type": "text", "text": "More related work. Other investigations into evaluating explanations focused on label leakage [26, 34] and faithfulness [18, 35, 36, 37, 38]. Label leakage is similar to encoding in that additional information is in the explanation, but focuses on explanations that have access to both the inputs and the observed label; we leave extending Def: Encoding to leakage to the future. Faithfulness, intuitively, asks that the explanation reflect the process of how a label is predicted from the inputs; a formalization does not exist. Jacovi and Goldberg [38] note the need to define faithfulness formally. Encoding explanations are not faithful to the process of making an explanation because predictive inputs outside those selected by the explanation control the explanation. ", "page_idx": 9}, {"type": "text", "text": "Limitations and the future. Using misestimated models in evaluations (like EVAL-X) may lead to mistakes (see Appendix B.8 for an example). The retinal fundus experiment from Jethani et al. [26] is an example where misestimation leads to reductive explanations scoring higher than using the full input. Misestimation can be due to poor uncertainty or due to dependence on shortcut features. One fruitful direction is to use better uncertainty estimates, like conformal inference [39] or calibration [40], or employ robustness methods [41, 42] to ameliorate errors due to misestimation. Another direction is use tricks like REINFORCE-style gradients to construct non-encoding explanations by optimizing STRIPE-X. ", "page_idx": 9}, {"type": "text", "text": "Explanations that output subsets may not always help humans interpret the mechanism of the prediction. For example, imagine one wants to understand why a model correctly answers the question \u201cWho won the ski halfpipe at the X-games 3 years after her debut in 2021?\u201d with \u201cEileen Gu\u201d. A subset explanation may return \u201c3 years after her debut in 2021\u201d and \u201cski halfpipe\u201d, but that does not help a human interpret how the model predicts. A better interpretation would be to make the model output, \u201c3 years after 2021 is 2024. Eileen Gu won in 2024, and debuted in 2021.\u201d Such explanations can also encode information about the prediction in the text produced as a rationale [43]. An important direction here would be to extend the definitions of weak and strong detectors of encoding to evaluations of free-text rationales. ", "page_idx": 9}, {"type": "text", "text": "Data versus Model Explanations. Even with the formal definitions of explanation methods, there is a question about what is being explained: the data or the model. These two concepts often get blended together in the literature [11, 20]. We clarify this point and abstract the choice away as two different ways to produce the joint distribution $q(\\mathbf{y},\\mathbf{x})$ . In data explanation, the distribution under which a feature attribution method seeks to output a subset of inputs that predict the label should be the population distribution of the data [23]. If, instead, the goal is model explanation, the goal should not be to highlight inputs that predict the label well in samples of the data; rather it should be to predict the label well in samples from the model. Formally, a model with parameters $\\pmb{\\theta}$ is a conditional distribution, $p_{\\theta}(\\mathbf{y}\\mid\\mathbf{x})$ . To target a model explanation, a feature attribution method would aim to output a subset of inputs that predict the label under the distribution $F(\\mathbf{x})p_{\\theta}(\\mathbf{y}\\mid\\mathbf{x})$ . ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was partly supported by the NIH/NHLBI Award R01HL148248, NSF Award 1922658 NRT-HDR:FUTURE Foundations, Translation, and Responsibility for Data Science, NSF CAREER Award 2145542, NSF Award 2404476, ONR N00014-23-1-2634, Google DeepMind, and Apple. The authors would like to thank Yoav Wald for helpful feedback with the draft. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Pierre Elias, Timothy J Poterucha, Vijay Rajaram, Luca Matos Moller, Victor Rodriguez, Shreyas Bhave, Rebecca T Hahn, Geoffrey Tison, Sean A Abreau, Joshua Barrios, et al. Deep learning electrocardiographic analysis for detection of left-sided valvular heart disease. Journal of the American College of Cardiology, 80(6):613\u2013626, 2022. [2] Neil Jethani, Aahlad Puli, Hao Zhang, Leonid Garber, Lior Jankelson, Yindalon Aphinyanaphongs, and Rajesh Ranganath. New-onset diabetes assessment using artificial intelligence-enhanced electrocardiography. arXiv preprint arXiv:2205.02900, 2022.   \n[3] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Visualising image classification models and saliency maps. Deep Inside Convolutional Networks, 2, 2014.   \n[4] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. Advances in neural information processing systems, 30, 2017.   \n[5] Kim Long Tran, Hoang Anh Le, Thanh Hien Nguyen, and Duc Trung Nguyen. Explainable machine learning for financial distress prediction: evidence from vietnam. Data, 7(11):160, 2022.   \n[6] Alex J DeGrave, Joseph D Janizek, and Su-In Lee. Ai for radiographic covid-19 detection selects shortcuts over signal. Nature Machine Intelligence, 3(7):610\u2013619, 2021.   \n[7] Felix Wong, Erica J Zheng, Jacqueline A Valeri, Nina M Donghia, Melis N Anahtar, Satotaka Omori, Alicia Li, Andres Cubillos-Ruiz, Aarti Krishnan, Wengong Jin, et al. Discovery of a structural class of antibiotics with explainable deep learning. Nature, pages 1\u20139, 2023.   \n[8] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradientbased localization. In Proceedings of the IEEE international conference on computer vision, pages 618\u2013626, 2017.   \n[9] Ian Covert, Scott Lundberg, and Su-In Lee. Explaining by removing: A unified framework for model explanation. Journal of Machine Learning Research, 22(209):1\u201390, 2021.   \n[10] Neil Jethani, Mukund Sudarshan, Ian Connick Covert, Su-In Lee, and Rajesh Ranganath. Fastshap: Real-time shapley value estimation. In International Conference on Learning Representations, 2022.   \n[11] Jinsung Yoon, James Jordon, and Mihaela van der Schaar. Invase: Instance-wise variable selection using neural networks. In International Conference on Learning Representations, 2018.   \n[12] Isaac Lage, Emily Chen, Jeffrey He, Menaka Narayanan, Been Kim, Samuel J Gershman, and Finale Doshi-Velez. Human evaluation of models built for interpretability. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, volume 7, pages 59\u201367, 2019.   \n[13] Adriel Saporta, Xiaotong Gui, Ashwin Agrawal, Anuj Pareek, Steven QH Truong, Chanh DT Nguyen, Van-Doan Ngo, Jayne Seekins, Francis G Blankenberg, Andrew $\\mathbf{Y}~\\mathbf{Ng}$ , et al. Benchmarking saliency methods for chest x-ray interpretation. Nature Machine Intelligence, 4(10): 867\u2013878, 2022.   \n[14] Jonathan Crabb\u00e9, Alicia Curth, Ioana Bica, and Mihaela van der Schaar. Benchmarking heterogeneous treatment effect models through the lens of interpretability. Advances in Neural Information Processing Systems, 35:12295\u201312309, 2022.   \n[15] Wojciech Samek, Alexander Binder, Gr\u00e9goire Montavon, Sebastian Lapuschkin, and KlausRobert M\u00fcller. Evaluating the visualization of what a deep neural network has learned. IEEE transactions on neural networks and learning systems, 28(11):2660\u20132673, 2016.   \n[16] V Petsiuk, A Das, and K Saenko. Rise: Randomized input sampling for explanation of blackbox models. arXiv preprint arXiv:1806.07421, 2018.   \n[17] Piotr Dabkowski and Yarin Gal. Real time image saliency for black box classifiers. Advances in neural information processing systems, 30, 2017.   \n[18] Sarthak Jain, Sarah Wiegreffe, Yuval Pinter, and Byron C Wallace. Learning to faithfully rationalize by construction. arXiv preprint arXiv:2005.00115, 2020.   \n[19] Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. A benchmark for interpretability methods in deep neural networks. Advances in neural information processing systems, 32, 2019.   \n[20] Neil Jethani, Mukund Sudarshan, Yindalon Aphinyanaphongs, and Rajesh Ranganath. Have we learned to explain?: How interpretability methods can learn to encode predictions in their interpretations. In International Conference on Artificial Intelligence and Statistics, pages 1459\u20131467. PMLR, 2021.   \n[21] Jennifer Hsia, Danish Pruthi, Aarti Singh, and Zachary C Lipton. Goodhart\u2019s law applies to nlp\u2019s explanation benchmarks. arXiv preprint arXiv:2308.14272, 2023.   \n[22] Isabelle Guyon and Andr\u00e9 Elisseeff. An introduction to variable and feature selection. Journal of machine learning research, 3(Mar):1157\u20131182, 2003.   \n[23] Jianbo Chen, Le Song, Martin Wainwright, and Michael Jordan. Learning to explain: An information-theoretic perspective on model interpretation. In International conference on machine learning, pages 883\u2013892. PMLR, 2018.   \n[24] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \" why should i trust you?\" explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1135\u20131144, 2016.   \n[25] Erik \u0160trumbelj and Igor Kononenko. Explaining prediction models and individual predictions with feature contributions. Knowledge and information systems, 41:647\u2013665, 2014.   \n[26] Neil Jethani, Adriel Saporta, and Rajesh Ranganath. Don\u2019t be fooled: label leakage in explanation methods and the importance of their quantitative evaluation. In International Conference on Artificial Intelligence and Statistics, pages 8925\u20138953. PMLR, 2023.   \n[27] Kun Zhang, Jonas Peters, Dominik Janzing, and Bernhard Sch\u00f6lkopf. Kernel-based conditional independence test and application in causal discovery. In Proceedings of the TwentySeventh Conference on Uncertainty in Artificial Intelligence, pages 804\u2013813, 2011.   \n[28] Mukund Sudarshan, Wesley Tansey, and Rajesh Ranganath. Deep direct likelihood knockoffs. Advances in neural information processing systems, 33:5036\u20135046, 2020.   \n[29] Mukund Sudarshan, Aahlad Puli, Wesley Tansey, and Rajesh Ranganath. Diet: Conditional independence testing with marginal dependence measures of residual information. In International Conference on Artificial Intelligence and Statistics, pages 10343\u201310367. PMLR, 2023.   \n[30] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[31] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents, 2022. URL https://arxiv. org/abs/2204.06125, 7, 2022.   \n[32] Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Man\u00e9, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\u00e9gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL http://tensorflow. org/. Software available from tensorflow.org.   \n[33] Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao, et al. Explanations from large language models make small reasoners better. arXiv preprint arXiv:2210.06726, 2022.   \n[34] Peter Hase, Shiyue Zhang, Harry Xie, and Mohit Bansal. Leakage-adjusted simulatability: Can models generate non-trivial explanations of their behavior in natural language? arXiv preprint arXiv:2010.04119, 2020.   \n[35] Jasmijn Bastings, Sebastian Ebert, Polina Zablotskaia, Anders Sandholm, and Katja Filippova. \" will you find these shortcuts?\" a protocol for evaluating the faithfulness of input salience methods for text classification. arXiv preprint arXiv:2111.07367, 2021.   \n[36] Yilun Zhou, Serena Booth, Marco Tulio Ribeiro, and Julie Shah. Do feature attribution methods correctly attribute features? In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 9623\u20139633, 2022.   \n[37] Yiming Ju, Yuanzhe Zhang, Zhao Yang, Zhongtao Jiang, Kang Liu, and Jun Zhao. Logic traps in evaluating attribution scores. arXiv preprint arXiv:2109.05463, 2021.   \n[38] Alon Jacovi and Yoav Goldberg. Towards faithfully interpretable nlp systems: How should we define and evaluate faithfulness? arXiv preprint arXiv:2004.03685, 2020.   \n[39] Jing Lei, Max G\u2019Sell, Alessandro Rinaldo, Ryan J Tibshirani, and Larry Wasserman. Distribution-free predictive inference for regression. Journal of the American Statistical Association, 113(523):1094\u20131111, 2018.   \n[40] Mark Goldstein, Xintian Han, Aahlad Puli, Adler Perotte, and Rajesh Ranganath. X-cal: Explicit calibration for survival analysis. Advances in neural information processing systems, 33: 18296\u201318307, 2020.   \n[41] Aahlad Manas Puli, Lily H Zhang, Eric Karl Oermann, and Rajesh Ranganath. Out-ofdistribution generalization in the presence of nuisance-induced spurious correlations. ICLR 2022, 2021.   \n[42] Aahlad Manas Puli, Lily Zhang, Yoav Wald, and Rajesh Ranganath. Don\u2019t blame dataset shift! shortcut learning due to gradients and cross entropy. Advances in Neural Information Processing Systems, 36, 2023.   \n[43] Pepa Atanasova, Oana-Maria Camburu, Christina Lioma, Thomas Lukasiewicz, Jakob Grue Simonsen, and Isabelle Augenstein. Faithfulness tests for natural language explanations. arXiv preprint arXiv:2305.18029, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Theoretical Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Expressing $q(\\mathbf{y}\\mid\\mathbf{x}_{e(\\mathbf{x})})$ in terms of the values and the identity of the explanation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To express $q(\\mathbf{y}\\mid\\mathbf{x}_{e(\\mathbf{x})})$ , we use the following equivalence of events from eq. (1) ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\{\\mathbf{x}:\\mathbf{x}_{e(\\mathbf{x})}=(\\mathbf{v},\\mathbf{a})\\}=\\{\\mathbf{x}:e(\\mathbf{x})=\\mathbf{v}\\}\\cap\\{\\mathbf{x}:\\mathbf{x_{v}}=\\mathbf{a}\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then, intuitively, conditioning on the event that $\\mathbf{x}_{e(\\mathbf{x})}=(\\mathbf{v},\\mathbf{a})$ gives you the same information as conditioning on the events $e(\\mathbf{x})=\\mathbf{v}$ and $\\mathbf{x_{v}}=\\mathbf{a}$ simultaneously. We make this formal below. ", "page_idx": 13}, {"type": "text", "text": "For discrete $\\mathbf{x}$ , for any $\\mathbf{v},\\mathbf{a}$ such that the probability $q(\\mathbf{x}_{e(\\mathbf{x})}\\,=\\,(\\mathbf{v},\\mathbf{a}))\\,>\\,0$ , define the LHS and RHS of eq. (1) as $B_{\\mathbf{v}},C_{\\mathbf{v}}$ respectively. Then, the conditionals $q(\\mathbf{y}\\mid\\mathbf{x}_{e(\\mathbf{x})}=(\\mathbf{v},\\mathbf{a}))$ and $q(\\mathbf{y}\\mid\\mathbf{E_{v}}=$ $1,\\mathbf{x_{v}}=\\mathbf{a}_{\\alpha}$ ) exist and can be written as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\nq(\\mathbf{y}\\mid\\mathbf{x}_{e(\\mathbf{x})}=(\\mathbf{v},\\mathbf{a}))=q(\\mathbf{y}\\mid\\mathbf{x}\\in B_{\\mathbf{v}})\\qquad q(\\mathbf{y}\\mid\\mathbf{E_{v}}=1,\\mathbf{x_{v}}=\\mathbf{a})=q(\\mathbf{y}\\mid\\mathbf{x}\\in C_{\\mathbf{v}}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "These two conditionals are equal because $B_{\\mathbf{v}}=C_{\\mathbf{v}}$ . ", "page_idx": 13}, {"type": "text", "text": "The same kind of result holds for general random vectors (discrete or continuous) $\\mathbf{x}$ but is a little more involved because $B_{\\mathbf{v}}$ may be non-empty while $q(\\mathbf{x}\\in B_{\\mathbf{v}})=0$ and the equality of conditional densities/probabilities need to be written via measure theory. Assume the regular conditional probabilities $q(\\mathbf{y}\\mid\\mathbf{x}_{e(\\mathbf{x})})$ and $q(\\mathbf{y},\\mathbf{E_{v}}\\mid\\mathbf{x_{v}}),q(\\mathbf{y}\\mid\\mathbf{E_{v}},\\mathbf{x_{v}})$ and $q(\\mathbf{E_{v}}\\mid\\mathbf{x_{v}})$ are defined almost everywhere in their respective probability measures. Take any $\\mathbf{S_{v}}\\subseteq\\{\\mathbf{x_{v}}:e(\\mathbf{x})=\\mathbf{v}\\}$ where $q(\\mathbf{x_{v}}\\in\\mathbf{S_{v}})>0$ and $q(e(\\mathbf{x})^{-}\\mathbf{=v})\\bar{>}\\ 0$ . Consider any measurable sets $\\mathbf{Y}$ over y and $\\mathbf{B}_{\\mathbf{v}}(\\mathbf{S}_{\\mathbf{v}}):=\\{(\\mathbf{v},\\mathbf{a}):\\mathbf{a}\\in\\mathbf{S}_{\\mathbf{v}}\\}$ over $\\mathbf{x}_{e(\\mathbf{x})}$ . Now, by definition of regular conditional probability measures, joint probabilities are obtained by taking the expectation of the conditional with respect to marginal distributions over the conditioning set: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{q(\\mathbf{y}\\in\\mathbf{Y},\\mathbf{x}_{e(\\mathbf{x})}\\in\\mathbf{B}_{\\mathbf{v}}(\\mathbf{S}_{\\mathbf{v}}))=\\int_{\\mathbf{B}_{\\mathbf{v}}(\\mathbf{S}_{\\mathbf{v}})}q(\\mathbf{y}\\in\\mathbf{Y}\\mid\\mathbf{x}_{e(\\mathbf{x})}=(\\mathbf{v},\\mathbf{a}))q(d\\mathbf{x}_{e(\\mathbf{x})})}}\\\\ &{}&{=\\int_{\\mathbf{S}_{\\mathbf{v}}}q(\\mathbf{y}\\in\\mathbf{Y}\\mid\\mathbf{x}_{e(\\mathbf{x})}=(\\mathbf{v},\\mathbf{a}))q(\\mathbf{x}_{e(\\mathbf{x})}=(\\mathbf{v},\\mathbf{a}))d\\mathbf{a}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{q({\\mathbf y}\\in{\\mathbf Y},{\\mathbf E}_{\\mathbf v}=1,{\\mathbf x}_{\\mathbf v}\\in{\\mathbf S}_{\\mathbf v})=\\int_{{\\mathbf S}_{\\mathbf v}}q({\\mathbf y}\\in{\\mathbf Y},{\\mathbf E}_{\\mathbf v}=1\\mid{\\mathbf x}_{\\mathbf v}=\\mathbf a)q({\\mathbf x}_{\\mathbf v}=\\mathbf a)d\\mathbf a}}\\\\ &{}&{\\quad=\\int_{{\\mathbf S}_{\\mathbf v}}q({\\mathbf y}\\in{\\mathbf Y}\\mid{\\mathbf E}_{\\mathbf v}=1,{\\mathbf x}_{\\mathbf v}=\\mathbf a)q({\\mathbf E}_{\\mathbf v}=1,{\\mathbf x}_{\\mathbf v}=\\mathbf a)d\\mathbf a.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Due to eq. (1), the LHS terms of the two equations above are equal and so are the probability measures over the integrating variables in eqs. (9) and (10). Letting $\\mathbf{x_{v}}$ be defined on a Borel sigma algebra, these two integrals eqs. (9) and (10) are equal if and only if for any Borel set $\\mathbf{S_{v}}$ , for almost every $\\mathbf{a}\\in\\mathbf{S_{v}}$ ", "page_idx": 13}, {"type": "equation", "text": "$$\nq(\\mathbf{y}\\in\\mathbf{Y}\\mid\\mathbf{x}_{e(\\mathbf{x})}=(\\mathbf{v},\\mathbf{a}))=q(\\mathbf{y}\\in\\mathbf{Y}\\mid\\mathbf{E_{v}}=1,\\mathbf{x_{v}}=\\mathbf{a}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "That is, in more plain terms, the conditional distributions are equal $q(\\mathbf{y}\\mid\\mathbf{x}_{e(\\mathbf{x})}\\;=\\;(\\mathbf{v},\\mathbf{a}))\\;=$ $q(\\mathbf{y}\\mid\\mathbf{E_{v}}=1,\\mathbf{x_{v}}=\\mathbf{a})$ . ", "page_idx": 13}, {"type": "text", "text": "A.2 With non-encoding explanations \"what you see is what you get\" ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Def: Encoding says that an explanation $e(\\mathbf{x})$ is encoding if there exists an $\\mathbf{S}$ where $q(\\mathbf{x}_{e(\\mathbf{x})}\\in\\mathbf{S})>0$ such that for every $(\\mathbf{v},\\mathbf{a})\\in\\mathbf{S},$ , : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{y}\\perp\\!\\!\\!\\left\\Vert\\mathbf{\\hat{\\Pi}}_{\\mathbf{V}}\\mid\\mathbf{x_{v}}=\\mathbf{a}.\\right\\Vert\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For a non-encoding explanation, Def: Encoding does not hold. Here, we derive the implications of violating Def: Encoding. Define the set A to contain all $(\\mathbf{v},\\mathbf{a})$ where eq. (11) is violated: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{A}=\\{(\\mathbf{v},\\mathbf{a}):\\mathbf{y}\\perp\\mathbf{\\mathbb{E}_{v}}\\mid\\mathbf{x_{v}}=\\mathbf{a}\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By definition, the complement $\\mathbf{A}^{C}$ is such that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\forall(\\mathbf{v},\\mathbf{a})\\in\\mathbf{A}^{C},\\quad\\mathbf{y}\\neq\\mathbb{E}_{\\mathbf{v}}\\mid\\mathbf{x_{v}}=\\mathbf{a}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Such a set cannot have positive measure when Def: Encoding is violated which means ", "page_idx": 14}, {"type": "equation", "text": "$$\nq(\\mathbf{x}_{e(\\mathbf{x})}\\in\\mathbf{A}^{C})=0.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In turn, ", "page_idx": 14}, {"type": "equation", "text": "$$\nq(\\mathbf{x}_{e(\\mathbf{x})}\\in\\mathbf{A})=1-q(\\mathbf{x}_{e(\\mathbf{x})}\\in\\mathbf{A}^{C})=1.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, $\\mathbf{A}$ is such that $q(\\mathbf{x}_{e(\\mathbf{x})}\\in\\mathbf{A})=1$ , and by eq. (12) for all $(\\mathbf{v},\\mathbf{a})\\in\\mathbf{A}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{y}\\perp\\perp\\mathbf{E_{v}}\\mid\\mathbf{x_{v}}=\\mathbf{a},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which in turn guarantees ", "page_idx": 14}, {"type": "equation", "text": "$$\nq(\\mathbf{y}\\mid\\mathbf{x}_{e(\\mathbf{x})}=(\\mathbf{v},\\mathbf{a}))=q(\\mathbf{y}\\mid\\mathbf{x}_{\\mathbf{v}}=\\mathbf{a},\\mathbf{E}_{\\mathbf{v}}=1)=q(\\mathbf{y}\\mid\\mathbf{x}_{\\mathbf{v}}=\\mathbf{a}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.3 Helpful Lemmas and their proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.3.1 Alternate conditions equivalent to Def: Encoding ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The dependence in Def: Encoding occurs due to two reasons, understanding which sheds more light on the definition. First, for some selection $e(\\mathbf{x})\\;=\\;\\mathbf{v}$ , the explanation\u2019s values $\\mathbf{x_{v}}$ do not provide enough information to reveal that the explanation should select the inputs denoted by $\\mathbf{v}$ . In other words, the indicator of the selection is variable even after fixing the explanation\u2019s values themselves. Second, this indicator is predictive of the label for the data with the explanation $\\mathbf{v}$ . These two properties provide intuition on the definition of encoding: ", "page_idx": 14}, {"type": "text", "text": "Lemma 1. Def: Encoding holds for an explanation $e(\\mathbf{x})$ if and only if there exists a selection v such that $q(e({\\bf x})={\\bf v})>0$ and a set $\\mathbf{S_{v}}\\subseteq\\{\\mathbf{x_{v}}:e(\\mathbf{x})=\\mathbf{\\dot{v}}\\}$ such that $q(\\mathbf{x_{v}}\\in\\mathbf{S_{v}})>0$ where both of the following conditions hold for almost every $\\mathbf{a}\\in\\mathbf{S_{v}}$ : ", "page_idx": 14}, {"type": "text", "text": "Unpredictability of Explanation ", "page_idx": 14}, {"type": "text", "text": "Additional Information from Explanation ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q(\\mathbf{E_{v}}=1\\mid\\mathbf{x_{v}}=\\mathbf{a})\\neq1;}\\\\ &{q(\\mathbf{y}\\mid\\mathbf{x_{v}}=\\mathbf{a},\\mathbf{E_{v}}=1)\\neq q(\\mathbf{y}\\mid\\mathbf{x_{v}}=\\mathbf{a},\\mathbf{E_{v}}=0).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. First, Lemma 2 shows the Def: Encoding holds if only if there exists a selection $\\mathbf{v}$ such that $q(e({\\bf x})={\\bf v})>0$ and a set $\\mathbf{S_{v}}\\subseteq\\{\\mathbf{x_{v}}:e(\\mathbf{x})=\\mathbf{v}\\}$ such that $q(\\mathbf{x_{v}}\\in\\mathbf{S_{v}})>0$ where ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\forall\\mathbf{a}\\in\\mathbf{S_{v}},\\quad\\mathbf{y}\\twoheadrightarrow\\mathbf{E_{v}}\\mid\\mathbf{x_{v}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We use this alternate definition in what follows. ", "page_idx": 14}, {"type": "text", "text": "Given a non-measure zero set $\\mathbf{S_{v}}$ , by Lemma 3, almost everywhere in $\\mathbf{S_{v}}$ it holds that $q(\\mathbf{E_{v}}\\ =$ $1\\mid\\mathbf{x_{v}})>0$ . ", "page_idx": 14}, {"type": "text", "text": "Conditional dependence implies Unpredictability and Additional information (the only if part). If $q(\\mathbf{E_{v}}^{-}=1\\mid\\mathbf{x_{v}})\\;\\equiv\\;1$ almost everywhere (under $q(\\mathbf{x}))$ , then $\\mathbf{E_{v}}$ is constant given $\\mathbf{x_{v}}$ , and therefore independent of any variable given $\\mathbf{x_{v}}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\nq(\\mathbf{E_{v}}=1\\mid\\mathbf{x_{v}})=1\\implies\\mathbf{y}\\bot\\mathbf{E_{v}}\\mid\\mathbf{x_{v}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, it follows that conditional dependence implies the unpredictability property ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{y}\\lnsim\\mathbf{E_{v}}\\mid\\mathbf{x_{v}}\\implies q(\\mathbf{E_{v}}=1\\mid\\mathbf{x_{v}})<1.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Second, with the result from Lemma 3, we have $q(\\mathbf{E}_{\\mathbf{v}}=1|\\mathbf{x_{v}})\\in(0,1)$ . Thus $q(\\mathbf{y}|\\mathbf{x_{v}},\\mathbf{E_{v}}=1)$ and $q(\\mathbf{y}|\\mathbf{x_{v}},\\mathbf{E_{v}}=0)$ exist almost every where in $\\mathbf{S_{v}}$ . Then, by definition of conditional dependence, there is additional information about the label in the explanation: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{y}\\lnsubseteq\\mathbf{E_{v}}\\mid\\mathbf{x_{v}}\\implies q(\\mathbf{y}\\mid\\mathbf{x_{v}},\\mathbf{E_{v}}=1)\\neq q(\\mathbf{y}\\mid\\mathbf{x_{v}},\\mathbf{E_{v}}=0).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This shows that Def: Encoding implies the additional information property. ", "page_idx": 14}, {"type": "text", "text": "Conditional dependence implied by Unpredictability and Additional information (the if part). Now, if $q(\\mathbf{E_{v}}\\ \\bar{=}\\ 1\\ |\\ \\mathbf{x_{v}})\\ \\in\\ \\bar{(0,1)}$ , then the following two conditional distributions exist almost everywhere in $\\mathbf{S_{v}}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\nq(\\mathbf{y}\\mid\\mathbf{x_{v}},\\mathbf{E_{v}}=1)\\qquad,\\qquad q(\\mathbf{y}\\mid\\mathbf{x_{v}},\\mathbf{E_{v}}=0).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, by definition of dependence almost everywhere $\\mathbf{S_{v}}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\nq(\\mathbf{y}\\mid\\mathbf{x_{v}},\\mathbf{E_{v}}=1)\\neq q(\\mathbf{y}\\mid\\mathbf{x_{v}},\\mathbf{E_{v}}=0)\\Longrightarrow\\mathbf{y}\\mathbf{\\mathbb{1}}\\mathbf{E_{v}}\\mid\\mathbf{x_{v}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, the unpredictability and the additional information properties imply Def: Encoding. ", "page_idx": 15}, {"type": "text", "text": "Lemma 2. Def: Encoding holds for an explanation $e(\\mathbf{x})$ if and only if there exists a selection v such that $q(e(\\mathbf{x})=\\mathbf{v})>0$ and a set $\\mathbf{S_{v}}\\subseteq\\{\\mathbf{x_{v}}:e(\\mathbf{x})=\\mathbf{\\dot{v}}\\}$ such that $q(\\mathbf{x_{v}}\\in\\mathbf{S_{v}})>0$ where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\forall\\mathbf{a}\\in\\mathbf{S_{v}},\\quad\\mathbf{y}\\neq\\mathbf{E_{v}}\\mid\\mathbf{x_{v}}=\\mathbf{a}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Def: Encoding says that the explanation $e(\\mathbf{x})$ is encoding if there exists an S where $q(\\mathbf{x}_{e(\\mathbf{x})}\\in$ $\\mathbf{S})>0$ such that for every $(\\mathbf{v},\\mathbf{a})\\in\\mathbf{S}$ eq. (13) holds. This proof works by showing that S having a positive measure implies the existence of $\\mathbf{v}$ and $\\mathbf{S_{v}}$ as in Lemma 2 such that eq. (13) holds. ", "page_idx": 15}, {"type": "text", "text": "Decompose $q(\\mathbf{x}_{e(\\mathbf{x})}\\in\\mathbf{S})$ by introducing an expectation over $\\mathbf{v}\\sim q(e(\\mathbf{x}))$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\nq(\\mathbf{x}_{e(\\mathbf{x})}\\in\\mathbf{S})=\\mathbb{E}_{\\mathbf{v}\\sim q(e(\\mathbf{x}))}q(\\mathbf{x}_{e(\\mathbf{x})}\\in\\mathbf{S}\\mid e(\\mathbf{x})=\\mathbf{v}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As there are only finitely many $\\mathbf{v}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\nq(\\mathbf{x}_{e(\\mathbf{x})}\\in\\mathbf{S})>0\\quad\\Longleftrightarrow\\quad\\exists\\mathbf{v}\\quad\\mathbf{s.t.}\\quad q(e(\\mathbf{x})=\\mathbf{v})>0\\quad\\mathrm{~and~}\\quad q(\\mathbf{x}_{e(\\mathbf{x})}\\in\\mathbf{S}\\mid e(\\mathbf{x})=\\mathbf{v})>0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The \u2019only if\u2019 direction. Pick any $\\mathbf{v}$ such that the RHS above holds and define $\\mathbf{S_{v}}=\\{\\mathbf{a}:(\\mathbf{v},\\mathbf{a})\\in$ ${\\bf S}\\}$ . By definition, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{S}_{\\mathbf{v}}=\\{\\mathbf{x_{v}}:(\\mathbf{v},\\mathbf{x_{v}})\\in\\mathbf{S}\\}\\cap\\{\\mathbf{x_{v}}:e(\\mathbf{x})=\\mathbf{v}\\}\\subseteq\\{\\mathbf{x_{v}}:e(\\mathbf{x})=\\mathbf{v}\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This proves that $\\mathbf{S_{v}}$ has positive measure: ", "page_idx": 15}, {"type": "equation", "text": "$$\nq(\\mathbf{x_{v}}\\in\\mathbf{S_{v}})=q(\\mathbf{x}_{e(\\mathbf{x})}\\in\\mathbf{S},e(\\mathbf{x})=\\mathbf{v})=q(\\mathbf{x}_{e(\\mathbf{x})}\\in\\mathbf{S}\\mid e(\\mathbf{x})=\\mathbf{v})*q(e(\\mathbf{x})=\\mathbf{v})>0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Finally, as $\\mathbf{a}\\in\\mathbf{S_{v}}\\implies(\\mathbf{v},\\mathbf{a})\\in\\mathbf{S}$ , eq. (13) holds: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\forall\\mathbf{a}\\in\\mathbf{S_{v}},\\quad\\mathbf{y}\\neq\\mathbf{E_{v}}\\mid\\mathbf{x_{v}}=\\mathbf{a}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This completes the \u2019only if\u2019 direction. ", "page_idx": 15}, {"type": "text", "text": "The \u2019if\u2019 direction. Assume that there exists $\\mathbf{v}$ such that $q(e({\\bf x})={\\bf v})>0$ and $\\mathbf{S_{v}}\\subseteq\\{\\mathbf{x_{v}}:e(\\mathbf{x})=$ $\\mathbf{v}\\}$ such that $q(\\mathbf{x_{v}}\\in\\mathbf{S_{v}})>0$ where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\forall\\mathbf{a}\\in\\mathbf{S_{v}}\\qquad\\mathbf{y}\\preceq\\mathbf{k}\\mathbf{E_{v}}\\mid\\mathbf{x_{v}}=\\mathbf{a}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Define $\\mathbf{S}=\\{(\\mathbf{v},\\mathbf{a}):\\mathbf{a}\\in\\mathbf{S_{v}}\\}$ . By this construction, S has positive measure: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q(\\mathbf{x}_{e(\\mathbf{x})}\\in\\mathbf{S})=q((\\mathbf{v},\\mathbf{x_{v}})\\in\\mathbf{S})}\\\\ &{\\quad\\quad\\quad\\quad=q(e(\\mathbf{x})=\\mathbf{v})q((\\mathbf{v},\\mathbf{x_{v}})\\in\\mathbf{S}\\mid e(\\mathbf{x})=\\mathbf{v})}\\\\ &{\\quad\\quad\\quad\\quad=q(e(\\mathbf{x})=\\mathbf{v})q(\\mathbf{x_{v}}\\in\\mathbf{S_{v}}\\mid e(\\mathbf{x})=\\mathbf{v})}\\\\ &{\\quad\\quad\\quad=q(e(\\mathbf{x})=\\mathbf{v})q(\\mathbf{x_{v}}\\in\\mathbf{S_{v}})\\qquad\\{\\mathrm{~as~}\\mathbf{S_{v}}\\subseteq\\{\\mathbf{x_{v}}:e(\\mathbf{x})=\\mathbf{v}\\}}\\\\ &{\\quad\\quad\\quad>0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the last inequality holds because by assumption ", "page_idx": 15}, {"type": "equation", "text": "$$\nq(e(\\mathbf{x})=\\mathbf{v})>0\\qquad q(\\mathbf{x_{v}}\\in\\mathbf{S_{v}})>0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Finally, as $(\\mathbf{v},\\mathbf{a})\\in\\mathbf{S}\\implies\\mathbf{a}\\in\\mathbf{S_{v}}$ , eq. (13) holds: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\forall(\\mathbf{v},\\mathbf{a})\\in\\mathbf{S},\\quad\\mathbf{y}\\neq\\mathbf{E_{v}}\\mid\\mathbf{x_{v}}=\\mathbf{a}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This completes the \"if\" directions and with that the proof. ", "page_idx": 15}, {"type": "text", "text": "Lemma 3. For any set $\\mathbf{S_{v}}\\subseteq\\{\\mathbf{x_{v}}:e(\\mathbf{x})=\\mathbf{v}\\}$ such that $q(\\mathbf{x_{v}}\\in\\mathbf{S_{v}})>0,$ , then for almost every $\\mathbf{a}\\in\\mathbf{S_{v}}$ , $q(\\mathbf{E_{v}}=1\\mid\\mathbf{x_{v}}=\\mathbf{a})>0$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. Define the set $\\mathbf{A_{v}}=\\{\\mathbf{a}:q(\\mathbf{E_{v}}=1\\mid\\mathbf{x_{v}}=\\mathbf{a})=0\\}$ . Next compute the joint probability ", "page_idx": 16}, {"type": "equation", "text": "$$\nq(\\mathbf{x_{v}}\\in\\mathbf{A_{v}}\\cap\\mathbf{S_{v}})=q(\\mathbf{x_{v}}\\in\\mathbf{A_{v}})q(\\mathbf{x_{v}}\\in\\mathbf{S_{v}}\\mid\\mathbf{x_{v}}\\in\\mathbf{A_{v}}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now, noting that $\\mathbf{S_{v}}$ is a subset of $\\{\\mathbf{x_{v}}:e(\\mathbf{x})=\\mathbf{v}\\}$ , which is equivalent to $\\{\\mathbf{x_{v}}:\\mathbf{E_{v}}=1\\}$ , thus ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q(\\mathbf{x}_{\\mathbf{v}}\\in\\mathbf{S}_{\\mathbf{v}}\\mid\\mathbf{x}_{\\mathbf{v}}\\in\\mathbf{A}_{\\mathbf{v}})}\\\\ &{\\ =\\int q(\\mathbf{x}_{\\mathbf{v}}\\in\\mathbf{S}_{\\mathbf{v}}\\mid\\mathbf{x}_{\\mathbf{v}}=\\mathbf{a},\\mathbf{x}_{\\mathbf{v}}\\in\\mathbf{A}_{\\mathbf{v}})q(\\mathbf{x}_{\\mathbf{v}}=\\mathbf{a}\\mid\\mathbf{x}_{\\mathbf{v}}\\in\\mathbf{A}_{\\mathbf{v}})d\\mathbf{a}}\\\\ &{\\ =\\int q(\\mathbf{x}_{\\mathbf{v}}\\in\\mathbf{S}_{\\mathbf{v}}\\mid\\mathbf{x}_{\\mathbf{v}}=\\mathbf{a})q(\\mathbf{x}_{\\mathbf{v}}=\\mathbf{a}\\mid\\mathbf{x}_{\\mathbf{v}}\\in\\mathbf{A}_{\\mathbf{v}})d\\mathbf{a}}\\\\ &{\\ \\le\\int q(\\mathbf{E}_{\\mathbf{v}}=1\\mid\\mathbf{x}_{\\mathbf{v}}=\\mathbf{a})q(\\mathbf{x}_{\\mathbf{v}}=\\mathbf{a}\\mid\\mathbf{x}_{\\mathbf{v}}\\in\\mathbf{A}_{\\mathbf{v}})d\\mathbf{a}}\\\\ &{\\ =\\int q(\\mathbf{E}_{\\mathbf{v}}=1\\mid\\mathbf{x}_{\\mathbf{v}}=\\mathbf{a})q(\\mathbf{x}_{\\mathbf{v}}=\\mathbf{a}\\mid\\mathbf{x}_{\\mathbf{v}}\\in\\{\\mathbf{a}:q(\\mathbf{E}_{\\mathbf{v}}=1\\mid\\mathbf{x}_{\\mathbf{v}}=\\mathbf{a})=0\\})d\\mathbf{a}}\\\\ &{\\ =0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The probability $q(\\mathbf{x_{v}}\\in\\mathbf{S_{v}}\\mid\\mathbf{x_{v}}\\in\\mathbf{A_{v}})$ is non-negative, so it must be zero. Plugging this conditional back into the joint gives $q(\\mathbf{\\dot{x}_{v}}\\in\\mathbf{A_{v}}\\cap\\mathbf{\\dot{S}_{v}})=0$ . Then expanding yields ", "page_idx": 16}, {"type": "text", "text": "Since $q(\\mathbf{x_{v}}\\in\\mathbf{S_{v}})>0$ , $q(\\mathbf{x_{v}}\\in\\mathbf{A_{v}}\\mid\\mathbf{x_{v}}\\in\\mathbf{S_{v}})$ must be zero and thus, $q(\\mathbf{x_{v}}\\notin\\mathbf{A_{v}}\\mid\\mathbf{x_{v}}\\in\\mathbf{S_{v}})=1.$ , where expanding out the definition of $\\mathbf{A_{v}}$ gives the desired result that $q(\\mathbf{E}_{\\mathbf{v}}=1\\mid\\mathbf{x_{v}}=\\mathbf{a})>0$ for almost $\\mathbf{a}\\in\\mathbf{S_{v}}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1=q(\\mathbf a\\notin\\mathbf A_{\\mathbf v}\\mid\\mathbf a\\in\\mathbf S_{\\mathbf v})}\\\\ &{\\quad=q(\\mathbf a\\notin\\{\\mathbf a:q(\\mathbf E_{\\mathbf v}=1\\mid\\mathbf x_{\\mathbf v}=\\mathbf a)=0\\}\\mid\\mathbf a\\in\\mathbf S_{\\mathbf v})}\\\\ &{\\quad=q(\\mathbf a\\in\\{\\mathbf a:q(\\mathbf E_{\\mathbf v}=1\\mid\\mathbf x_{\\mathbf v}=\\mathbf a)>0\\}\\mid\\mathbf a\\in\\mathbf S_{\\mathbf v}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A.3.2 Optimal value and the optimal gap under EVAL- $\\mathbf{X}$ ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma 4. The EVAL- $\\mathrm{X}$ optimality gap value for $e(\\cdot)$ is an averaged KL between $q(\\mathbf{y}\\mid\\mathbf{x})$ and $\\begin{array}{r}{q(\\mathbf{y}\\mid\\mathbf{x_{v}})\\colon\\sum_{\\mathbf{v}\\in\\mathcal{V}}q(e(\\mathbf{x})\\stackrel{.}{=}\\mathbf{v})\\mathbb{E}_{q(\\mathbf{x}\\mid e(\\mathbf{x})=\\mathbf{v})}\\mathbf{K}\\mathbf{\\dot{L}}(q(\\mathbf{y}\\mid\\mathbf{x})\\|q(\\mathbf{y}\\mid\\mathbf{x_{v}}))}\\end{array}$ . This gap is zero, i.e. $e(\\mathbf{x})$ is optimal with the score EVAL- $\\cdot\\mathbf{X}^{*}=\\mathbb{E}_{q}[\\log q(\\mathbf{y}\\mid\\mathbf{x})]$ if for all v such that $q(e(\\mathbf{x})=\\mathbf{v})>0$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\nq(\\mathbf{y}\\mid\\mathbf{x})=q(\\mathbf{y}\\mid\\mathbf{x_{v}})\\;\\;\\;\\;a.e.\\;i n\\;\\;\\;\\;\\{\\mathbf{x}:e(\\mathbf{x})=\\mathbf{v}\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Let $p$ be a generic conditional distribution and let $\\mathbf{x}_{-\\mathbf{v}}$ be the values outside the explanation. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{e}{\\mathrm{anax~EVAL-X}}(q,e)=\\underset{e}{\\mathrm{max~}}\\mathbb{E}_{(\\mathbf{v},\\mathbf{a})\\sim q(\\mathbf{x}_{e(\\mathbf{x})})}\\mathbb{E}_{q(\\mathbf{y}\\mid\\mathbf{x}_{e(\\mathbf{x})}=(\\mathbf{v},\\mathbf{a}))}\\log q\\left(\\mathbf{y}\\mid\\mathbf{x}_{v}=\\mathbf{a}\\right)}\\\\ &{=\\underset{e}{\\mathrm{max~}}\\mathbb{E}_{(\\mathbf{v},\\mathbf{a})\\sim q(\\mathbf{x}_{e(\\mathbf{x})})}\\mathbb{E}_{q(\\mathbf{x}_{-\\mathbf{v}}\\mid\\mathbf{x}_{e(\\mathbf{x})}=(\\mathbf{v},\\mathbf{a}))}\\mathbb{E}_{q(\\mathbf{y}\\mid\\mathbf{x}_{e(\\mathbf{x})}=(\\mathbf{v},\\mathbf{a}),\\mathbf{x}_{-\\mathbf{v}})}\\log q\\left(\\mathbf{y}\\mid\\mathbf{x}_{v}=\\mathbf{a}\\right)}\\\\ &{=\\underset{e}{\\mathrm{max~}}\\mathbb{E}_{(\\mathbf{v},\\mathbf{a})\\sim q(\\mathbf{x}_{e(\\mathbf{x})})}\\mathbb{E}_{q(\\mathbf{x}\\mid\\mathbf{x}_{e(\\mathbf{x})}=(\\mathbf{v},\\mathbf{a}))}\\mathbb{E}_{q(\\mathbf{y}\\mid\\mathbf{x})}\\log q\\left(\\mathbf{y}\\mid\\mathbf{x}_{v}=\\mathbf{a}\\right)}\\\\ &{\\leq\\underset{p}{\\mathrm{max~}}\\mathbb{E}_{(\\mathbf{v},\\mathbf{a})\\sim q(\\mathbf{x}_{e(\\mathbf{x})})}\\mathbb{E}_{q(\\mathbf{x}\\mid\\mathbf{x}_{e(\\mathbf{x})}=(\\mathbf{v},\\mathbf{a}))}\\mathbb{E}_{q(\\mathbf{y}\\mid\\mathbf{x})}\\log p(\\mathbf{y}\\mid\\mathbf{x})]}\\\\ &{\\leq\\underset{p}{\\mathrm{max~}}\\mathbb{E}_{q(\\mathbf{x})}\\mathbb{E}_{q(\\mathbf{y}\\mid\\mathbf{x})}[\\log p(\\mathbf{y}\\mid\\mathbf{x})]}\\\\ &{=\\underset{p}{\\mathrm{max-}}\\mathbb{E}_{q(\\mathbf{x})}\\mathbf{K}\\mathbf{L}(q(\\mathbf{y}\\mid\\mathbf{x})||p(\\mathbf{y}\\mid\\mathbf{x}))+\\mathbb{E}_{q}[\\log q(\\mathbf{y}\\mid\\mathbf{x})]}\\\\ &{=\\mathbb{E}_{q}[\\log q(\\mathbf{y}\\mid\\mathbf{x})].}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This upper bound is achievable by an explanation that selects all inputs, so the maximum EVAL-X denoted as EVAL- $\\mathbf{\\boldsymbol{X}}^{*}=\\mathbb{E}_{q}\\log q(\\mathbf{\\dot{y}}\\mid\\mathbf{x})$ . ", "page_idx": 16}, {"type": "text", "text": "As in the math above, the EVAL- $\\mathrm{X}$ score for an explanation method can be expanded as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{EVAL-X}^{e}=\\mathbb{E}_{(\\mathbf{v},\\mathbf{a})\\sim q(\\mathbf{x}_{e(\\mathbf{x})})}\\mathbb{E}_{q(\\mathbf{y}\\mid\\mathbf{x}_{e(\\mathbf{x})}=(\\mathbf{v},\\mathbf{a}))}\\log q\\left(\\mathbf{y}\\mid\\mathbf{x}_{\\mathbf{v}}=\\mathbf{a}\\right)}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{(\\mathbf{v},\\mathbf{a})\\sim q(\\mathbf{x}_{e(\\mathbf{x})})}\\mathbb{E}_{q(\\mathbf{x}\\mid\\mathbf{x}_{e(\\mathbf{x})}=(\\mathbf{v},\\mathbf{a}))}\\mathbb{E}_{q(\\mathbf{y}\\mid\\mathbf{x})}\\log q(\\mathbf{y}\\mid\\mathbf{x}_{\\mathbf{v}}=\\mathbf{a})}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{\\mathbf{v}\\sim q(e(\\mathbf{x}))}\\mathbb{E}_{\\mathbf{a}\\sim q(\\mathbf{x}\\mid\\mathbf{\\mu}_{e(\\mathbf{x})=\\mathbf{v}})}\\mathbb{E}_{q(\\mathbf{x}\\mid\\mathbf{\\mu}_{\\mathbf{x}}=\\mathbf{a},e(\\mathbf{x})=\\mathbf{v}))}\\mathbb{E}_{q(\\mathbf{y}\\mid\\mathbf{x})}\\log q(\\mathbf{y}\\mid\\mathbf{x}_{\\mathbf{v}}=\\mathbf{a})}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{\\mathbf{v}\\sim q(e(\\mathbf{x}))}\\mathbb{E}_{q(\\mathbf{x}\\mid e(\\mathbf{x})=\\mathbf{v}))}\\mathbb{E}_{q(\\mathbf{y}\\mid\\mathbf{\\mu}_{\\mathbf{x}})}\\log q(\\mathbf{y}\\mid\\mathbf{x}_{\\mathbf{v}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where in the last step, we dropped a because it equals $\\mathbf{x_{v}}$ almost surely. Similarly, the optimal score EVAL- $X^{*}$ expands to ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{v}\\sim q(e(\\mathbf{x}))}\\mathbb{E}_{q(\\mathbf{x}\\mid e(\\mathbf{x})=\\mathbf{v}))}\\mathbb{E}_{q(\\mathbf{y}\\mid\\mathbf{x})}\\log q(\\mathbf{y}\\mid\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $\\nu$ be the set of values that explanations can take on, then taking the difference from optimality EVAL-X\u2217\u2212EVAL-Xe ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle=\\mathbb{E}_{\\mathbf{v}\\sim q(e(\\mathbf{x}))}\\mathbb{E}_{q(\\mathbf{x}\\mid e(\\mathbf{x})=\\mathbf{v}))}\\mathbb{E}_{q(\\mathbf{y}\\mid\\mathbf{x})}\\log\\frac{q(\\mathbf{y}\\mid\\mathbf{x})}{q(\\mathbf{y}\\mid\\mathbf{x_{v}}=\\mathbf{x})}}\\\\ {\\displaystyle=\\mathbb{E}_{\\mathbf{v}\\sim q(e(\\mathbf{x}))}\\mathbb{E}_{q(\\mathbf{x}\\mid e(\\mathbf{x})=\\mathbf{v}))}\\mathbf{KL}\\left[q(\\mathbf{y}\\mid\\mathbf{x})\\mid\\mid q(\\mathbf{y}\\mid\\mathbf{x_{v}})\\right]}\\\\ {\\displaystyle=\\sum_{\\mathbf{v}\\in\\mathcal{V}}q(e(\\mathbf{x})=\\mathbf{v})\\mathbb{E}_{q(\\mathbf{x}\\mid e(\\mathbf{x})=\\mathbf{v}))}\\mathbf{KL}\\left[q(\\mathbf{y}\\mid\\mathbf{x})\\mid\\mid q(\\mathbf{y}\\mid\\mathbf{x_{v}})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "As each $\\mathbf{KL}$ term is non-negative, each term in the sum being set to 0 simultaneously achieves the optimum, which happens when for all $\\mathbf{v}$ such that $q(e({\\bf x})={\\bar{\\bf v}})>0$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\nq(\\mathbf{y}\\mid\\mathbf{x})=q(\\mathbf{y}\\mid\\mathbf{x_{v}})\\quad{\\mathrm{~for~almost~every~}}\\quad\\{\\mathbf{x}:e(\\mathbf{x})=\\mathbf{v}\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In Appendix A.4, we use the results from Lemma 1 and Lemma 4 to prove that the optimal score of EVAL-X can only be achieved by non-encoding explanations. ", "page_idx": 17}, {"type": "text", "text": "A.4 Proof of Theorem 1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Theorem 1. If $e(\\mathbf{x})$ is EVAL-X optimal, then $e(\\mathbf{x})$ is not encoding. ", "page_idx": 17}, {"type": "text", "text": "Proof. Note only $q(e(\\mathbf{x})\\,=\\,\\mathbf{v})\\,>\\,0$ are of interest, since $q(e(\\mathbf{x})\\,=\\,\\mathbf{v})\\,=\\,0$ implies that $\\mathbf{E_{v}}=0$ almost surely and thus $\\mathbf{y}\\perp\\!\\!\\!\\perp\\mathbf{E_{v}}\\mid\\mathbf{x_{v}}$ . ", "page_idx": 17}, {"type": "text", "text": "Then if $e(\\mathbf{x})$ achieves EVAL- $\\cdot X^{*}$ , then by Lemma 4, for all $\\mathbf{v}$ such that $q(e(\\mathbf{x})=\\mathbf{v})>0$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\nq(\\mathbf{y}\\mid\\mathbf{x})=q(\\mathbf{y}\\mid\\mathbf{x_{v}})\\quad{\\mathrm{~for~almost~every~}}\\quad\\{\\mathbf{x}:e(\\mathbf{x})=\\mathbf{v}\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "First, this optimality criteria can incorporate $\\mathbf{E_{v}}=1$ on the lefthand side by first conditioning on $e(\\mathbf{x})$ and then noting that the equality holds for $\\mathbf{x}$ where $e(\\mathbf{x})=\\mathbf{v}$ . ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{}&{q(\\mathbf{y}\\mid\\mathbf{x})=q(\\mathbf{y}\\mid\\mathbf{x_{v}})}&{\\mathrm{~for~almost~every~}\\quad\\{\\mathbf{x}:e(\\mathbf{x})=\\mathbf{v}\\}}\\\\ &{}&{\\iff q(\\mathbf{y}\\mid\\mathbf{x},e(\\mathbf{x}))=q(\\mathbf{y}\\mid\\mathbf{x_{v}})}&{\\mathrm{~for~almost~every~}\\quad\\{\\mathbf{x}:e(\\mathbf{x})=\\mathbf{v}\\}}\\\\ &{}&{\\iff q(\\mathbf{y}\\mid\\mathbf{x},e(\\mathbf{x})=\\mathbf{v})=q(\\mathbf{y}\\mid\\mathbf{x_{v}})}&{\\mathrm{~for~almost~every~}\\quad\\{\\mathbf{x}:e(\\mathbf{x})=\\mathbf{v}\\}}\\\\ &{}&{\\iff q(\\mathbf{y}\\mid\\mathbf{x},\\mathbf{E_{v}}=1)=q(\\mathbf{y}\\mid\\mathbf{x_{v}})}&{\\mathrm{~for~almost~every~}\\quad\\{\\mathbf{x}:e(\\mathbf{x})=\\mathbf{v}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To understand if the optimality criterion disallows encoding, integrate the left and right-hand sides of this optimality criterion with the respect to complement of the inputs in $\\mathbf{x_{v}}$ , $q(\\mathbf{x}_{\\mathbf{v}}^{c}\\;\\mathbf{\\bar{|}}\\;\\mathbf{x_{v}},\\mathbf{E_{v}}=1)$ yields ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\int q(\\mathbf{y}\\mid\\mathbf{x},\\mathbf{E_{v}}=1)q(\\mathbf{x_{v}^{c}}\\mid\\mathbf{x_{v}},\\mathbf{E_{v}}=1)d\\mathbf{x_{v}^{c}}=\\int q(\\mathbf{y}\\mid\\mathbf{x_{v}})q(\\mathbf{x_{v}^{c}}\\mid\\mathbf{x_{v}},\\mathbf{E_{v}}=1)d\\mathbf{x_{v}^{c}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\iff\\int q(\\mathbf{y}\\mid\\mathbf{x_{v}^{c}},\\mathbf{x_{v}},\\mathbf{E_{v}}=1)q(\\mathbf{x_{v}^{c}}\\mid\\mathbf{x_{v}},\\mathbf{E_{v}}=1)d\\mathbf{x_{v}^{c}}=q(\\mathbf{y}\\mid\\mathbf{x_{v}})\\int q(\\mathbf{x_{v}^{c}}\\mid\\mathbf{x_{v}},\\mathbf{E_{v}}=1)d\\mathbf{x_{v}^{c}}}\\\\ &{\\iff q(\\mathbf{y}\\mid\\mathbf{x_{v}},\\mathbf{E_{v}}=1)=q(\\mathbf{y}\\mid\\mathbf{x_{v}})\\quad\\mathrm{~for~almost~every~\\{~\\mathbf{x}:~\\boldsymbol{e}(\\mathbf{x})=\\mathbf{v}\\}~}}\\\\ &{\\iff q(\\mathbf{y}\\mid\\mathbf{x_{v}},\\mathbf{E_{v}}=1)=q(\\mathbf{y}\\mid\\mathbf{x_{v}})\\quad\\mathrm{~for~almost~every~\\{~\\mathbf{x}:~\\boldsymbol{e}(\\mathbf{x})=\\mathbf{v}\\}~}}\\\\ &{\\iff q(\\mathbf{y}\\mid\\mathbf{x_{v}},\\mathbf{E_{v}}=1)=q(\\mathbf{y}\\mid\\mathbf{x_{v}})\\quad\\mathrm{~for~almost~every~\\{~\\mathbf{x}_v:~\\boldsymbol{e}(\\mathbf{x})=\\mathbf{v}\\}~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now expanding the right-hand side gives ", "page_idx": 18}, {"type": "equation", "text": "$$\nq(\\mathbf{y}\\mid\\mathbf{x_{v}})=q(\\mathbf{y},\\mathbf{E_{v}}=1\\mid\\mathbf{x_{v}})+q(\\mathbf{y},\\mathbf{E_{v}}=0\\mid\\mathbf{x_{v}}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combing the two equations gives ", "page_idx": 18}, {"type": "equation", "text": "$$\nq(\\mathbf{y}\\mid\\mathbf{x_{v}},\\mathbf{E_{v}}=1)=q(\\mathbf{y},\\mathbf{E_{v}}=1\\mid\\mathbf{x_{v}})+q(\\mathbf{y},\\mathbf{E_{v}}=0\\mid\\mathbf{x_{v}})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We show that this equality implies that $\\mathbf{y}\\bot\\bot\\mathbf{E_{v}}\\mid\\mathbf{x_{v}}$ by splitting the analysis into cases based on $q(\\mathbf{E_{v}}=1\\mid\\mathbf{x_{v}})=1$ and $q(\\mathbf{E_{v}}=1\\mid\\mathbf{x_{v}})<1$ . In turn, the condition in Def: Encoding is violated and the explanation $e(\\cdot)$ is not encoding. ", "page_idx": 18}, {"type": "text", "text": "Case 1: EVAL- $\\mathbf{X}$ optimality holds when the explanation is predictable. The first case to consider is when the event that the explanation takes the value $\\mathbf{v}$ is determined by $\\mathbf{x_{v}}$ for all samples with the explanation $\\mathbf{v}$ . That is, $q(\\mathbf{E_{v}}=1\\mid\\mathbf{x_{v}})=1$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\nq(\\mathbf{E_{v}}=1\\mid\\mathbf{x_{v}})=1\\iff q(\\mathbf{E_{v}}=0\\mid\\mathbf{x_{v}})=0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then expanding this marginal into the joint shows that the joint $q(\\mathbf{y},\\mathbf{E}_{\\mathbf{v}}=0\\mid\\mathbf{x_{v}})$ has to be zero as well. ", "page_idx": 18}, {"type": "equation", "text": "$$\nq(\\mathbf{E}_{\\mathbf{v}}=0\\mid\\mathbf{x_{v}})=\\int q(\\mathbf{y},\\mathbf{E_{v}}=0\\mid\\mathbf{x_{v}})d\\mathbf{y}=0,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "because an integral of non-negative terms being zero implies that each term itself is zero almost surely. ", "page_idx": 18}, {"type": "text", "text": "Then, we can show that the determinism condition $q(\\mathbf{E}_{\\mathbf{v}}=1\\mid\\mathbf{x_{v}})=1$ is sufficient for the optimality criterion eq. (19): ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathbf{y}\\mid\\mathbf{x_{v}},\\mathbf{E_{v}}=1\\rangle=q(\\mathbf{y}\\mid\\mathbf{x_{v}},\\mathbf{E_{v}}=1)\\times1}\\\\ &{\\qquad\\qquad\\qquad=q(\\mathbf{y}\\mid\\mathbf{x_{v}},\\mathbf{E_{v}}=1)q(\\mathbf{E_{v}}=1\\mid\\mathbf{x_{v}})}\\\\ &{\\qquad\\qquad\\qquad=q(\\mathbf{y},\\mathbf{E_{v}}=1\\mid\\mathbf{x_{v}})}\\\\ &{\\qquad\\qquad\\qquad=q(\\mathbf{y},\\mathbf{E_{v}}=1\\mid\\mathbf{x_{v}})+q(\\mathbf{y},\\mathbf{E_{v}}=0\\mid\\mathbf{x_{v}})\\quad\\mathrm{~for~almost~every~}\\{\\mathbf{x_{v}}:e(\\mathbf{x})=\\mathbf{v}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This shows that the EVAL-X optimality criteria is satisfied when the $q(\\mathbf{E_{v}}=1\\mid\\mathbf{x_{v}})=1$ , thus the explanation is completely predictable from the explanation for examples with that explanation. By Lemma 1, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nq(\\mathbf{E_{v}}=1\\mid\\mathbf{x_{v}})=1\\implies\\mathbf{y}\\bot\\mathbf{E_{v}}\\mid\\mathbf{x_{v}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which violates Def: Encoding. So there is no encoding in this case. ", "page_idx": 18}, {"type": "text", "text": "Case 2: When the explanation is unpredictable, EVAL- $\\mathbf{X}$ optimality requires that the explanation provide no extra information. Now consider the alternative case, $q(\\mathbf{E}_{\\mathbf{v}}=1\\mid\\mathbf{x_{v}})<1$ . Here the explanation does not determine the explanation opening the possibility that the EVAL-X-optimal explanation method can encode information in the explanation. ", "page_idx": 18}, {"type": "text", "text": "Because $q(e({\\bf x})={\\bf v})>0$ , we have $q(\\mathbf{x_{v}}\\in\\{\\mathbf{x_{v}}:e(\\mathbf{x})=\\mathbf{v}\\})>0$ . Thus, by Lemma 3, for almost every $\\{\\mathbf{x_{v}}:e(\\mathbf{x})=\\mathbf{v}\\}$ it holds that $q(\\mathbf{E}_{\\mathbf{v}}=1\\mid\\mathbf{x_{v}})>0$ . Putting this result together with alternative case $\\,\\!\\,g(\\mathbf{E_{v}}=1\\mid\\mathbf{x_{v}})<1\\,\\!\\!$ gives: $0<q(\\mathbf{E}_{\\mathbf{v}}=1\\mid\\mathbf{x_{v}})<1$ for almost every $\\{{\\bf x_{v}}:e({\\bf x})={\\bf v}\\}$ . ", "page_idx": 18}, {"type": "text", "text": "Now, expanding out the optimality criterion eq. (19): ", "page_idx": 18}, {"type": "equation", "text": "$$\nq(\\mathbf{y}\\mid\\mathbf{x_{v}},\\mathbf{E_{v}}=1)=q(\\mathbf{y},\\mathbf{E_{v}}=1\\mid\\mathbf{x_{v}})\\times1+q(\\mathbf{y},\\mathbf{E_{v}}=0\\mid\\mathbf{x_{v}})\\times1\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\iff q(\\mathbf{y}\\mid\\mathbf{x_{v}},\\mathbf{E_{v}}=1)=q(\\mathbf{y}\\mid\\mathbf{E_{v}}=1,\\mathbf{x_{v}})q(\\mathbf{E_{v}}=1\\mid\\mathbf{x_{v}})\\quad\\quad}\\\\ &{}&{{\\quad+\\;q(\\mathbf{y}\\mid\\mathbf{E_{v}}=0,\\mathbf{x_{v}})(1-q(\\mathbf{E_{v}}=1\\mid\\mathbf{x_{v}}))}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\iff(1-q(\\mathbf{E_{v}}=1\\mid\\mathbf{x_{v}}))q(\\mathbf{y}\\mid\\mathbf{x_{v}},\\mathbf{E_{v}}=1)=(1-q(\\mathbf{E_{v}}=1\\mid\\mathbf{x_{v}}))q(\\mathbf{y}\\mid\\mathbf{x_{v}},\\mathbf{E_{v}}=0)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\iff q(\\mathbf{y}\\mid\\mathbf{x_{v}},\\mathbf{E_{v}}=1)=q(\\mathbf{y}\\mid\\mathbf{x_{v}},\\mathbf{E_{v}}=0)\\quad{\\mathrm{~for~almost~every~}}\\{\\mathbf{x_{v}}:e(\\mathbf{x})=\\mathbf{v}\\}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This equality says for all samples with the explanation $\\mathbf{v}$ , knowing $\\mathbf{E_{v}}$ does not change the distribution of the label y. By Lemma 1, this equality implies that the independence y $\\mathbf{E_{v}}\\mid\\mathbf{x_{v}}$ holds which violates Def: Encoding. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "A.5 Proof of Proposition 3 and Theorem 2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proposition 3. ENCODE-METER $\\phi_{q}(e)=0$ if and only if e is not encoding. ", "page_idx": 19}, {"type": "text", "text": "Proof. First, Def: Encoding is violated if and only if there exists a set A such that $q(\\mathbf{x}_{e(\\mathbf{x})}\\in\\mathbf{A})=1$ and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\forall(\\mathbf{v},\\mathbf{a})\\in\\mathbf{A}\\qquad\\mathbf{y}\\perp\\mathbf{E_{v}}\\mid\\mathbf{x_{v}}=\\mathbf{a}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For the if direction, note that if ENCODE-METER $\\phi_{q}(e)=0$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}_{(\\mathbf{v},\\mathbf{a})\\sim q(\\mathbf{x}_{e(\\mathbf{x})})}\\mathbf{I}\\big(\\mathbf{y};\\mathbf{E_{v}}\\mid\\mathbf{x_{v}}=\\mathbf{a}\\big)=0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "To show the forward direction, the above equality means that if $\\phi_{q}(e)=0$ , almost surely for every $(\\mathbf{v},\\mathbf{a})\\sim q(\\mathbf{x}_{e(\\mathbf{x})})$ , the instantaneous mutual information is 0 which implies the desired conditional independence ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{I}(\\mathbf{y};\\mathbf{E_{v}}\\mid\\mathbf{x_{v}}=\\mathbf{a})=0\\implies\\mathbf{y}\\bot\\mathbf{l}\\mathbf{E_{v}}\\mid\\mathbf{x_{v}}=\\mathbf{a}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By definition of almost surely, there exists a set A such that $q(\\mathbf{x}_{e(\\mathbf{x})}\\in\\mathbf{A})=1$ the independence above holds; this completes the \"if\" direction. ", "page_idx": 19}, {"type": "text", "text": "To show the reverse direction, let there exist a set $\\mathbf{A}$ such that $q(\\mathbf{x}_{e(\\mathbf{x})}\\in\\mathbf{A})=1$ , for every $(\\mathbf{v},\\mathbf{a})\\in$ A, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{y}\\perp\\!\\!\\!\\perp\\mathbf{E_{v}}\\mid\\mathbf{x_{v}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In turn, for all $(\\mathbf{v},\\mathbf{a})\\in\\mathbf{A}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{I}(\\mathbf{y};\\mathbf{E_{v}}\\mid\\mathbf{x_{v}}=\\mathbf{a})=0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, the fact that $q(\\mathbf{x}_{e(\\mathbf{x})}\\,\\in\\,\\mathbf{A})\\,=\\,1$ implies that expectations with respect to $q(\\mathbf{x}_{e(\\mathbf{x})})$ over the whole support equal expectations over $q(\\mathbf{x}_{e(\\mathbf{x})}\\mid\\mathbf{x}_{e(\\mathbf{x})}\\in\\mathbf{A})$ , which is $q(\\mathbf{x}_{e(\\mathbf{x})})$ restricted to A: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}_{(\\mathbf{v},\\mathbf{a})\\sim q(\\mathbf{x}_{e(\\mathbf{x})})}\\mathbf{I}(\\mathbf{y};\\mathbf{E}_{\\mathbf{v}}\\mid\\mathbf{x}_{\\mathbf{v}}=\\mathbf{a})=\\mathbb{E}_{(\\mathbf{v},\\mathbf{a})\\sim q(\\mathbf{x}_{e(\\mathbf{x})}\\mid\\mathbf{x}_{e(\\mathbf{x})}\\in\\mathbf{A})}\\mathbf{I}(\\mathbf{y};\\mathbf{E}_{\\mathbf{v}}\\mid\\mathbf{x}_{\\mathbf{v}}=\\mathbf{a})=0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This completes the \"only if\" direction. ", "page_idx": 19}, {"type": "text", "text": "Theorem 2. With finite $\\mathbf{H}(\\mathbf{y}\\mid\\mathbf{x})$ and $\\mathbf{H}(\\mathbf{y})$ , for any explanation that encodes e and any that does not encode $e^{\\prime}$ , there exists an $\\alpha^{*}$ such that $\\forall\\alpha>\\alpha^{*}$ STRIPE- $\\cdot\\mathrm{X}_{\\alpha}(q,e^{\\prime})~>\\mathrm{STRIPE}\u2013\\mathrm{X}_{\\alpha}(q,e)$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Recall that STRIPE- $\\mathrm{X}$ is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\cdot\\mathbf{X}_{\\alpha}(q,e):=\\mathbb{E}_{(\\mathbf{v},\\mathbf{a})\\sim q(\\mathbf{x}_{e(\\mathbf{x})})}\\mathbb{E}_{q(\\mathbf{y}\\mid\\mathbf{x}_{e(\\mathbf{x})}=(\\mathbf{v},\\mathbf{a}))}[\\log q\\left(\\mathbf{y}\\mid\\mathbf{x}_{\\mathbf{v}}=\\mathbf{a}\\right)]-\\alpha\\phi_{q}(e),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the ENCODE-METER ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\phi_{q}(e):=\\mathbb{E}_{(\\mathbf{v},\\mathbf{a})\\sim q(\\mathbf{x}_{e(\\mathbf{x})})}\\mathbf{I}\\left(\\mathbf{E}_{\\mathbf{v}};\\mathbf{y}\\mid\\mathbf{x_{v}}=\\mathbf{a}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We first show bounds for the first term in STRIPE- $\\mathrm{X}$ and then derive the STRIPE- $\\mathrm{X}$ scores for encoding and non-encoding explanations. ", "page_idx": 19}, {"type": "text", "text": "Bounds on EVAL-X scores. We lower bound the EVAL- $\\mathbf{\\nabla}X$ score, which is the first term in STRIPEX, for non-encoding explanations. ", "page_idx": 19}, {"type": "text", "text": "For non-encoding explanations, almost surely over ${\\mathbf v},{\\mathbf a}\\sim q({\\mathbf x}_{e({\\mathbf x})})$ ", "page_idx": 19}, {"type": "equation", "text": "$$\nq(\\mathbf{y}\\mid\\mathbf{x}_{e(\\mathbf{x})}=(\\mathbf{v},\\mathbf{a}))=q(\\mathbf{y}\\mid\\mathbf{x_{v}}=\\mathbf{a})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{EVAL-X}(q,e)-\\mathbb{E}_{q(\\mathbf{y},\\mathbf{x}_{e}=\\mathbf{a})}\\log q(\\mathbf{y})}\\\\ &{=\\mathbb{E}_{(\\mathbf{v},\\mathbf{a})\\sim q(\\mathbf{x}_{e(\\mathbf{x})})}\\mathbb{E}_{q(\\mathbf{y}\\mid\\mathbf{x}_{e(\\mathbf{x})}=(\\mathbf{v},\\mathbf{a}))}\\log q(\\mathbf{y}\\mid\\mathbf{x}_{e(\\mathbf{x})}=\\mathbf{a})-\\mathbb{E}_{q(\\mathbf{y},\\mathbf{x}_{e(\\mathbf{x})})}\\log q(\\mathbf{y})}\\\\ &{=\\mathbb{E}_{(\\mathbf{v},\\mathbf{a})\\sim q(\\mathbf{x}_{e(\\mathbf{x})})}\\mathbb{E}_{q(\\mathbf{y}\\mid\\mathbf{x}_{e(\\mathbf{x})}=(\\mathbf{v},\\mathbf{a}))}[\\log q\\left(\\mathbf{y}\\mid\\mathbf{x}_{e(\\mathbf{x})}=(\\mathbf{v},\\mathbf{a})\\right)]-\\mathbb{E}_{q(\\mathbf{y},\\mathbf{x}_{e(\\mathbf{x})})}\\log q(\\mathbf{y})}\\\\ &{=\\mathbb{E}_{q(\\mathbf{y},\\mathbf{x}_{e(\\mathbf{x})})}[\\log q\\left(\\mathbf{y}\\mid\\mathbf{x}_{e(\\mathbf{x})}\\right)]-\\mathbb{E}_{q(\\mathbf{y},\\mathbf{x}_{e(\\mathbf{x})})}\\log q(\\mathbf{y})}\\\\ &{=\\mathbb{E}_{q(\\mathbf{y},\\mathbf{x}_{e(\\mathbf{x})})}\\log\\frac{q\\left(\\mathbf{y}\\mid\\mathbf{x}_{e(\\mathbf{x})}\\right)}{q(\\mathbf{y})}}\\\\ &{=\\mathbf{I}(\\mathbf{y};\\mathbf{x}_{e(\\mathbf{x})}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The above inequality implies that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{EVAL-X}\\big(q,e\\big)-\\mathbb{E}_{(\\mathbf{v},\\mathbf{a})\\sim q(\\mathbf{x}_{e(\\mathbf{x})})}\\mathbb{E}_{\\mathbf{y}\\sim q(\\mathbf{y}\\mid\\mathbf{x}_{e(\\mathbf{x})}=(\\mathbf{v},\\mathbf{a}))}\\log q(\\mathbf{y})=\\mathbf{I}\\big(\\mathbf{y};\\mathbf{x}_{e(\\mathbf{x})}\\big)\\geq0}\\\\ &{\\implies\\mathrm{EVAL-X}\\big(q,e\\big)+\\mathbf{H}_{q}(\\mathbf{y})\\geq0}\\\\ &{\\implies\\mathrm{EVAL-X}\\big(q,e\\big)\\geq-\\mathbf{H}_{q}(\\mathbf{y}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Every inequality in the derivation above becomes strict when the explanation selects inputs that are predictive of the label because ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{I}(\\mathbf{y};\\mathbf{x}_{e(\\mathbf{x})})>0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, non-encoding explanations have EVAL- $\\mathrm{X}$ scores that are at least $-\\mathbf{H}_{q}(\\mathbf{y})$ . ", "page_idx": 20}, {"type": "text", "text": "The optimal EVAL- $\\mathbf{\\nabla}X$ score for any explanation (see Lemma 4) equals the negative conditional entropy which is upper bounded by some finite number: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{q}\\left[\\log q(\\mathbf{y}\\mid\\mathbf{x})\\right]=-\\mathbf{H}_{q}(\\mathbf{y}\\mid\\mathbf{x})=C.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Comparing explanations via STRIPE- $\\mathbf{X}$ . For any encoding explanation, by Proposition 3, for some $c>0$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\phi_{q}(e)>c.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now, consider $\\begin{array}{r}{\\alpha^{\\ast}=\\frac{\\mathbf{I}(\\mathbf{y};\\mathbf{x})}{c}\\geq0}\\end{array}$ , which is finite because each term in the ratio is finite. Then, for all $\\alpha>\\alpha^{*}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\alpha\\phi_{q}(e)>\\alpha^{*}\\phi_{q}(e)>{\\bf I}({\\bf y};{\\bf x}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, ", "page_idx": 20}, {"type": "equation", "text": "$$\n-\\alpha\\phi_{q}(e)<-\\mathbf{I}(\\mathbf{y};\\mathbf{x})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "As EVAL- $\\mathrm{X}$ scores are below $C=-\\mathbf{H}(\\mathbf{y}\\mid\\mathbf{x})$ for any encoding explanation, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\\\,STRIPE-}{\\cal X}_{\\alpha}(q,e)=\\mathrm{EVAL-}{\\cal X}(q,e)-\\alpha\\phi_{q}(e)}\\\\ &{\\quad\\quad\\quad\\quad\\quad<-{\\bf H}({\\bf y}\\mid{\\bf x})-{\\bf I}({\\bf y};{\\bf x})}\\\\ &{\\quad\\quad\\quad\\quad\\quad<-{\\bf H}({\\bf y}\\mid{\\bf x})-({\\bf H}_{q}({\\bf y})-{\\bf H}({\\bf y}\\mid{\\bf x}))}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=-{\\bf H}_{q}({\\bf y}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Finally, for any non-encoding explanation, $\\phi_{q}(e^{\\prime})\\,=\\,0$ by Proposition 3, STRIPE- $\\mathrm{X}$ scores equal EVAL-X scores, which are lower bounded at $-\\mathbf{H}_{q}(\\mathbf{y})$ . ", "page_idx": 20}, {"type": "text", "text": "Together, for every non-encoding explanation $e^{\\prime}(\\mathbf{x})$ and encoding explanation $e(\\mathbf{x})$ , it holds that ", "page_idx": 20}, {"type": "text", "text": "This proves that STRIPE- $\\mathrm{X}$ is a strong detector of encoding. ", "page_idx": 20}, {"type": "text", "text": "B Encoding examples, non-detection of ROAR,FRESH, and non-strong detection of EVAL-X ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "B.1 An illustrative DGP for Def: Encoding ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "With $\\ensuremath{\\mathcal{B}}(0.5)$ being a Bernoulli distribution, consider the following example ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{y}\\sim\\mathcal{B}(0.5)\\quad,\\quad\\mathbf{z}\\sim\\mathcal{B}(0.5)\\quad,\\quad\\epsilon_{1},\\epsilon_{2},\\epsilon_{3}\\sim\\mathcal{N}\\left(0,\\mathbf{I}\\right),}\\\\ &{\\mathbf{x}\\quad=\\quad\\left\\{\\left[\\mathbf{y}+\\epsilon_{1},\\qquad\\epsilon_{3},0,\\epsilon_{2}\\right]\\quad\\mathrm{if}\\;\\mathbf{z}=0,\\right.}\\\\ &{\\mathbf{x}\\quad=\\quad\\left\\{\\left[\\begin{array}{l l}{\\qquad\\epsilon_{3},\\mathbf{y}+\\epsilon_{1},1,\\epsilon_{2}\\right]\\quad\\mathrm{if}\\;\\mathbf{z}=1.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For this problem, if the third coordinate $\\mathbf{x}_{3}~=~0$ , all the information between the label and the covariates is in the first coordinate $\\mathbf{x}_{1}$ , and if $\\mathbf{x}_{3}=1$ , the information is between the label and the second coordinate $\\mathbf{x}_{2}$ . The corresponding explanation function is $e({\\bf x})\\,=\\,\\mathbb{1}[{\\bf x}_{3}\\,=\\,0]\\xi_{1}\\,+\\,\\mathbb{1}[{\\bf x}_{3}\\,=$ $1]\\xi_{2}$ . This explanation is encoding because neither explanation\u2019s values $\\mathbf{x}_{1}$ nor $\\mathbf{x}_{2}$ determine the explanation function because it depends on $\\mathbf{x}_{3}$ . Formally ", "page_idx": 20}, {"type": "equation", "text": "$$\nq(\\mathbf{y}=1\\mid\\mathbf{x}_{1},\\mathbf{x}_{3}=1)\\neq q(\\mathbf{y}=1\\mid\\mathbf{x}_{1},\\mathbf{x}_{3}=0)\\implies\\mathbf{y}\\exists\\mathbf{x}_{3}\\mid\\mathbf{x}_{1}\\implies\\mathbf{y}\\exists\\mathbf{k}\\mathbf{e}\\mid\\mathbf{x}_{1},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which meets Def: Encoding. ", "page_idx": 21}, {"type": "text", "text": "Consider an alternate non-encoding explanation function $e(\\mathbf{x})\\;=\\;[\\mathbb{1}[\\mathbf{x}_{4}\\;>\\;0],\\mathbb{1}[\\mathbf{x}_{4}\\;\\leq\\;0],0,0]$ ; $\\mathbf{x}_{1},\\mathbf{x}_{2}$ do not determine $e(\\mathbf{x})$ that depends on the noise $\\epsilon_{2}$ in $\\mathbf{x}_{4}$ . That means the unpredictability property in Lemma 1 holds. However, by construction, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{y},\\mathbf{x}_{1},\\mathbf{x}_{2})\\;\\lrcorner\\;\\mathbb{L}\\;\\epsilon_{2}\\implies(\\mathbf{y},\\mathbf{x}_{1},\\mathbf{x}_{2})\\;\\lrcorner\\;\\mathbb{L}\\;\\mathbf{E}_{\\mathbf{v}}\\quad\\Longrightarrow\\;\\;\\mathbf{y}\\lrcorner\\mathbb{L}\\mathbf{E}_{\\mathbf{v}}\\ |\\ \\mathbf{x}_{1}\\quad\\mathrm{and}\\quad\\mathbf{y}\\lrcorner\\mathbb{L}\\mathbf{E}_{\\mathbf{v}}\\ |\\ \\mathbf{x}_{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "So no additional information about the label is encoded: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{y}\\perp\\!\\!\\!\\perp\\mathbf{E_{v}}\\mid\\mathbf{x_{v}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The additional information property in Lemma 1 avoids such cases where the explanations keeps additional information that is irrelevant to the label. ", "page_idx": 21}, {"type": "text", "text": "B.2 Encoding explanations conceal predictive inputs that affect the explanation ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Consider the following DGP ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{x}=[\\mathbf{x}_{1},\\mathbf{x}_{2},\\mathbf{x}_{3}]\\sim{\\mathcal{B}}(0.5)^{\\otimes3},\\qquad\\quad\\mathbf{y}=\\left\\{\\mathbf{x}_{1}\\quad{\\mathrm{~if~}}\\mathbf{x}_{3}=1,\\qquad\\quad\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let $e$ be an encoding explanation that selects the first coordinate if $\\mathbf{x}_{3}=1$ and the second coordinate otherwise. We never observe $\\mathbf{x}_{3}$ when looking only at the explanation. Table 4 shows all possible values of this explanation. Notice that in the third and fourth rows, the value of $\\mathbf{x}_{e(\\mathbf{x})}$ changes to match the label $\\mathbf{y}$ exactly, even though the values of the first two coordinates that we can observe stay constant. It is impossible to understand the perfect predictiveness of $\\mathbf{x}_{e(\\mathbf{x})}$ , as the encoding explanation conceals the control flow feature $\\mathbf{x}_{3}$ that determines which of the first two features should be picked to predict the label. ", "page_idx": 21}, {"type": "text", "text": "Table 4: Possible values of the inputs, label, and explanation for the DGP in eq. (24) ", "page_idx": 21}, {"type": "table", "img_path": "mkw6x0OExg/tmp/64f1d81904e24ee0484989ebfd41a2bb1b5128c144c1c98e57117a75a895dc3d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "B.3 Position-based encoding fits Def: Encoding ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Recall the perceptual task that classifying images of dogs versus classifying images of cats, and consider the encoding explanation $e_{\\mathrm{position}}(\\mathbf{x})$ that is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{e_{\\mathrm{position}}(\\mathbf{x})=\\xi_{1}\\quad\\mathrm{if}\\quad q(\\mathbf{y}=\\mathrm{dog}\\mid\\mathbf{x})=1,}\\\\ &{e_{\\mathrm{position}}(\\mathbf{x})=\\xi_{2}\\quad\\mathrm{if}\\quad q(\\mathbf{y}=\\mathrm{cat}\\mid\\mathbf{x})=1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Assume that the inputs in the top leftmost pixels are always background, meaning that the values of these inputs provide no information about the label ${\\bf y}\\perp\\!\\!\\!\\perp{\\bf x}_{1},{\\bf x}_{2}$ . Now we check if this intuitivelydefined position-encoded explanation meets the definition for encoding (Def: Encoding). To condition on $\\mathbf{x}_{\\xi_{1}},\\mathbf{E}_{\\xi_{1}}=0$ , we need $q(\\mathbf{y}=\\deg)\\neq1$ . Note that ", "page_idx": 21}, {"type": "equation", "text": "$$\nq(\\mathbf{E}_{\\xi_{1}}=1\\mid\\mathbf{x}_{\\xi_{1}})=q(\\mathbf{y}=\\operatorname{dog}\\mid\\mathbf{x}_{\\xi_{1}})=q(\\mathbf{y}=\\operatorname{dog})\\neq1.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Def: Encoding holds because the indicator of which explanation was chosen $\\mathbf{E}_{\\xi_{1}}$ determines the label. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q(\\mathbf{y}=\\operatorname{dog}\\mid\\mathbf{x}_{\\xi_{1}},\\mathbf{E}_{\\xi_{1}}=1)=1\\neq0=q(\\mathbf{y}=\\operatorname{dog}\\mid\\mathbf{x}_{\\xi_{1}},\\mathbf{E}_{\\xi_{1}}=0)\\implies\\mathbf{y}\\not\\perp\\mathbf{E}_{\\xi_{1}}\\mid\\mathbf{x}_{\\xi_{1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This example shows how the encoding definition Def: Encoding captures the informally described position-based encoding from the literature. ", "page_idx": 21}, {"type": "text", "text": "B.4 Prediction-based encoding fits Def: Encoding ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The informal example of prediction-based encoding from Section 3.1 selects a single input that makes the prediction from all of the input have the highest confidence when given the single input. One way to mathematically express such a selection is as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{e_{\\mathrm{prediction}}(\\mathbf{x})=\\xi_{\\mathrm{argmax}_{i}q(\\mathbf{y}=1\\mid\\mathbf{x}_{i})}\\,\\mathrm{if}\\;q(\\mathbf{y}=1\\mid\\mathbf{x})>0.5,}\\\\ &{e_{\\mathrm{prediction}}(\\mathbf{x})=\\xi_{\\mathrm{argmin}_{i}q(\\mathbf{y}=1\\mid\\mathbf{x}_{i})}\\,\\mathrm{if}\\;q(\\mathbf{y}=1\\mid\\mathbf{x})\\le0.5.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Here, we describe one set of conditions on the distribution $q(\\mathbf{y},\\mathbf{x})$ for which the explanation in eq. (25) fits the definition of encoding in Def: Encoding. Assume that there exists a non-measurezero set $\\mathbf{U}\\subseteq\\{\\mathbf{x}:q(\\mathbf{y}=1\\mid\\mathbf{x})>0.5\\}$ and an index $k$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf x\\in\\mathbf U\\implies q(\\mathbf y=1\\mid\\mathbf x)\\ge\\rho,}\\\\ &{\\mathbf x\\in\\mathbf U\\implies\\forall i\\quad q(\\mathbf y=1\\mid\\mathbf x_{\\xi_{i}})<q(\\mathbf y=1\\mid\\mathbf x_{\\xi_{k}}),}\\\\ &{\\mathbf x\\notin\\mathbf U\\implies q(\\mathbf y=1\\mid\\mathbf x)<\\rho,}\\\\ &{\\mathbf x\\notin\\mathbf U\\implies\\exists i,j\\quad q(\\mathbf y=1\\mid\\mathbf x_{\\xi_{i}})>q(\\mathbf y=1\\mid\\mathbf x_{\\xi_{k}})>q(\\mathbf y=1\\mid\\mathbf x_{\\xi_{j}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Further, assume that $\\mathbf{x}_{\\xi_{k}}$ alone does not determine $\\mathbf{x}\\in\\mathbf{U}$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n0<\\mathbb{E}\\left[\\mathbb{1}[\\mathbf{x}\\in\\mathbf{U}]\\mid\\mathbf{x}_{\\xi_{k}}\\right]<1.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The assumptions above imply the facts below about $e_{\\mathrm{prediction}}(\\mathbf{x})$ : ", "page_idx": 22}, {"type": "text", "text": ". By eqs. (27) and (29) ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{x}\\in\\mathbf{U}\\Leftrightarrow e_{\\mathrm{prediction}}(\\mathbf{x})=\\xi_{k}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Define the explanation indicator $\\mathbf{E}_{\\mathbf{v}}=\\mathbb{1}[e_{\\mathrm{prediction}}(\\mathbf{x})=\\mathbf{v}]$ . ", "page_idx": 22}, {"type": "text", "text": "2. By eqs. (27) and (29) and the definition of $\\mathbf{E_{v}}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{x}\\in\\mathbf{U}\\Leftrightarrow\\mathbf{E}_{\\xi_{k}}=1.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "3. By eqs. (30) and (31) ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{0<q(\\mathbf{E}_{\\xi_{k}}=1\\mid\\mathbf{x}_{\\xi_{k}})=q(\\mathbf{x}\\in\\mathbf{U}\\mid\\mathbf{x}_{\\xi_{k}})<1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "4. By eq. (32), $q(\\mathbf{y}=1\\mid\\mathbf{x}_{\\xi_{k}},\\mathbf{E}_{\\xi_{k}}=1)$ ) and $q(\\mathbf{y}=1\\mid\\mathbf{x}_{\\xi_{k}},\\mathbf{E}_{\\xi_{k}}=0)$ ) are well defined. Then, by eqs. (26) and (28), for $\\mathbf{x}\\in\\mathbf{U}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{q(\\mathbf{y}=1\\mid\\mathbf{x}_{\\xi_{k}},\\mathbf{E}_{k}=1)}\\\\ &{\\quad=\\mathbb{E}_{q(\\mathbf{x}\\mid\\mathbf{x}_{\\xi_{k}},\\mathbf{E}_{k}=1)}q(\\mathbf{y}\\mid\\mathbf{x})}\\\\ &{\\quad\\geq\\mathbb{E}_{q(\\mathbf{x}\\mid\\mathbf{x}_{\\xi_{k}},\\mathbf{E}_{k}=1)}\\rho}&{\\{\\mathrm{as}\\ \\mathbf{E}_{k}=1\\implies\\mathbf{x}\\in\\mathbf{U}\\}}\\\\ &{\\quad=\\rho.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{q(\\mathbf{y}=1\\mid\\mathbf{x}_{\\xi_{k}},\\mathbf{E}_{k}=0)}\\\\ &{\\quad=\\mathbb{E}_{q(\\mathbf{x}\\mid\\mathbf{x}_{\\xi_{k}},\\mathbf{E}_{k}=0)}q(\\mathbf{y}\\mid\\mathbf{x})}\\\\ &{\\quad<\\mathbb{E}_{q(\\mathbf{x}\\mid\\mathbf{x}_{\\xi_{k}},\\mathbf{E}_{k}=0)}\\rho}&{\\{\\mathrm{as}\\ \\mathbf{E}_{k}=0\\ \\Longrightarrow\\ \\mathbf{x}\\notin\\mathbf{U}\\}}\\\\ &{\\quad=\\rho.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, for all elements of $\\{\\mathbf{x}_{\\xi_{k}}:\\mathbf{x}\\in\\mathbf{U}\\}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\nq(\\mathbf{y}=1\\mid\\mathbf{x}_{\\xi_{k}},\\mathbf{E}_{\\xi_{k}}=1)>q(\\mathbf{y}=1\\mid\\mathbf{x}_{\\xi_{k}},\\mathbf{E}_{\\xi_{k}}=0).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By Lemma 1, the properties in eqs. (32) and (33) imply that $\\forall\\mathbf{a}\\in\\{\\mathbf{x}_{\\xi_{k}}:\\mathbf{x}\\in\\mathbf{U}\\}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{y}{\\preceq}\\mathbb{E}_{\\xi_{k}}\\mid\\mathbf{x}_{\\xi_{k}}=\\mathbf{a}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Finally, the set $\\mathbf{U}_{k}=\\{\\mathbf{x}_{\\xi_{k}}:\\mathbf{x}\\in\\mathbf{U}\\}$ is non-measure-zero: as $\\mathbb{1}[\\mathbf{x}\\in\\mathbf{U}]=1\\implies\\mathbb{1}[\\mathbf{x}_{\\xi_{k}}\\in\\mathbf{U}_{k}]=$ 1, accumulating $q(\\mathbf{x})$ with the restriction $\\mathbf{x}_{\\xi_{k}}\\in\\mathbf{U}_{k}$ leads to at least as much mass as accumulating with the stricter restriction $\\mathbf{x}\\in\\mathbf{U}$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\nq(\\mathbf{x}_{\\xi_{k}}\\in\\mathbf{U}_{k})=\\int q(\\mathbf{x})\\mathbb{I}[\\mathbf{x}_{\\xi_{k}}\\in\\mathbf{U}_{k}]d\\mathbf{x}\\geq\\int q(\\mathbf{x})\\mathbb{I}[\\mathbf{x}\\in\\mathbf{U}]d\\mathbf{x}=q(\\mathbf{x}\\in\\mathbf{U})>0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Together, the last two equations implies that Def: Encoding holds for $e_{\\mathrm{prediction}}(\\mathbf{x})$ from eq. (25). ", "page_idx": 22}, {"type": "image", "img_path": "mkw6x0OExg/tmp/b4ab66fd12e1453017b3cbdfa676582bc93b87048adf585f4677db9c97c13a79.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 5: Example DGP and MARG encoding. (a) The color determines whether the label is produced from the top or bottom image. (b) An explanation that correctly reveals that the label is generated based on both the color and, as dictated by the color, the top or the bottom image. The label is deterministic given the value of the explanation which means the label can be predicted perfectly. (c) An encoding explanation would be one that produces only the top or the bottom animal image based on the color being red of blue respectively. This returned animal image does not indicate the fact that the data generating process depends on color. Now, the animal image selected by the explanation alone is insufficient to dictate the label because the color determines which image determines the label. The identity of the image, whether top or bottom, provides additional information about the label beyond the values explanation, as captured in Def: Encoding. ", "page_idx": 23}, {"type": "text", "text": "B.5 MARG explanations are encoding ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We provide an illustrative example of a MARG explanation for the DGP in Figure 5. Here, we show how a mathematical formulation of MARG satisfied Def: Encoding. ", "page_idx": 23}, {"type": "text", "text": "Consider a generic DGP with a Bernoulli control flow input denoted $\\mathbf{x}_{c}$ : for some distinct sets $U,V$ that do not include $c$ and let no combination of $\\mathbf{x}_{c},\\mathbf{x}_{U},\\mathbf{x}_{V}$ determine the rest ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}_{c}=1\\implies q(\\mathbf{y}\\mid\\mathbf{x})=q(\\mathbf{y}\\mid\\mathbf{x}_{U}),}\\\\ {\\mathbf{x}_{c}=0\\implies q(\\mathbf{y}\\mid\\mathbf{x})=q(\\mathbf{y}\\mid\\mathbf{x}_{V}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Further, assume that the two subsets leads to different distributions over $\\textbf{y}=1$ such that on a non-zero measure subset $\\mathbf{S}_{U}\\subseteq\\{\\mathbf{x}_{U}:\\mathbf{x}$ such that $\\mathbf{x}_{c}=1\\}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\nq(\\mathbf{y}=1\\mid\\mathbf{x}_{U},\\mathbf{x}_{c}=1)\\neq q(\\mathbf{y}=1\\mid\\mathbf{x}_{U},\\mathbf{x}_{c}=0).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now, consider a MARG explanation that looks at $\\mathbf{x}_{c}$ and outputs the corresponding sets $U,V$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\ne(\\mathbf{x})=U\\quad{\\mathrm{if}}\\quad\\mathbf{x}_{c}=1\\quad{\\mathrm{else}}\\quad e(\\mathbf{x})=V.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By definition the explanation only depends on the control flow input and is not determined by $\\mathbf{x}_{U}$ or $\\mathbf{x}_{V}$ . Next, as $\\mathbf{E}_{U}=1$ is the same event as $\\mathbf{x}_{c}=1$ , $e(\\mathbf{x})$ is encoding because the assumption from eq. (34) implies that ", "page_idx": 23}, {"type": "equation", "text": "$$\nq(\\mathbf{y}\\mid\\mathbf{x}_{U},\\mathbf{E}_{U}=1)\\neq q(\\mathbf{y}\\mid\\mathbf{x}_{U},\\mathbf{E}_{U}=0).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then, this inequality holds for all elements of the non-measure-zero set $\\mathbf{S}_{U}$ , by Lemma 1, MARG is encoding. ", "page_idx": 23}, {"type": "text", "text": "B.6 Proof of Proposition 1 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proposition 1. For the DGP in eq. (3), ROAR and FRESH assign their respective optimal scores to the encoding explanation $e_{e n c o d e}(\\mathbf{x})$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. First, note that ", "page_idx": 23}, {"type": "text", "text": "See Table 5 for the probability table for why this is true. ", "page_idx": 23}, {"type": "text", "text": "For this proof let $e({\\bf x})=e_{\\mathrm{encode}}({\\bf x})$ . In the example eq. (3), masking out the inputs selected by $e(\\mathbf{x})$ would mean that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{c}=1\\implies\\mathbf{x}_{-e(\\mathbf{x})}=(\\mathbf{1}-e(\\mathbf{x}),[\\mathbf{b},\\mathbf{c}]),\\qquad\\quad\\mathbf{c}=0\\implies\\mathbf{x}_{-e(\\mathbf{x})}=(\\mathbf{1}-e(\\mathbf{x}),[\\mathbf{a},\\mathbf{c}]).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Table 5: Probability table for the DGP in eq. (3). Conditional on the explanation $\\mathbf{x}_{3}$ , does predict the label. For example, given knowing $\\mathbf{x}_{1}=1$ , if $\\mathbf{x}_{3}=1$ implies $p(\\mathbf{y}\\bar{=}\\,1)=0.9$ but if $\\mathbf{x}_{3}=0$ , $p(\\mathbf{y}=1)=0.5$ . The probability table in Table 5 shows this. ", "page_idx": 24}, {"type": "table", "img_path": "mkw6x0OExg/tmp/a72a65ce754b7813bfbfa046cee441dfb4a298b2ee1b5f975e0c7ab95a823ab1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "In turn, by the construction in eq. (3) ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{x}_{-e(\\mathbf{x})}\\|\\mathbf{y}\\mid\\mathbf{c}=1,}\\\\ &{\\mathbf{x}_{-e(\\mathbf{x})}\\|\\mathbf{y}\\mid\\mathbf{c}=0.}\\end{array}\\implies\\mathbf{x}_{-e(\\mathbf{x})}\\|\\mathbf{j}\\|\\mathbf{c}\\implies(\\mathbf{x}_{-e(\\mathbf{x})},\\mathbf{c})\\|\\mathbf{j}\\|\\mathbf{y}\\implies\\mathbf{x}_{-e(\\mathbf{x})}\\|\\mathbf{j}\\|\\mathbf{y},}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the conditional independence in the second step turns into the joint independence in the third step due to c y. ", "page_idx": 24}, {"type": "text", "text": "ROAR scores an explanation highly if $\\mathbf{x}_{-e(\\mathbf{x})}$ predicts the label poorly. So if $\\mathbf{x}_{-e(\\mathbf{x})}$ is independent of $\\mathbf{y}$ , then $e(\\mathbf{x})$ would be scored optimally. Then, due to eq. (35), ROAR scores an encoding explanation optimally. ", "page_idx": 24}, {"type": "text", "text": "As FRESH removes the explanation instead of masking, it cannot introduce extra information into the subset of inputs to predict the label; so, FRESH also scores the same encoding explanation optimally. ", "page_idx": 24}, {"type": "text", "text": "B.7 Showing that $e_{\\mathbf{encode}}$ is the optimal reductive explanation for eq. (3) and scores better than a constant explanation under EVAL- $\\mathbf{X}$ ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We repeat the DGP in eq. (3) here ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{x}=[\\mathbf{x}_{1},\\mathbf{x}_{2},\\mathbf{x}_{3}]\\sim\\mathcal{B}(0.5)^{\\otimes3},}\\\\ &{\\mathbf{y}=\\left\\{\\mathbf{x}_{1}\\quad\\mathbf{w}.\\mathrm{p}.\\ 0.9\\quad\\mathrm{else}\\quad1-\\mathbf{x}_{1}\\quad\\mathrm{if}\\ \\mathbf{x}_{3}=1,}\\\\ &{\\mathbf{y}=\\left\\{\\mathbf{x}_{2}\\quad\\mathbf{w}.\\mathrm{p}.\\ 0.9\\quad\\mathrm{else}\\quad1-\\mathbf{x}_{2}\\quad\\mathrm{if}\\ \\mathbf{x}_{3}=0.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lemma 5. In the DGP in eq. (3), ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{q}(\\mathbf{y}=1\\mid\\mathbf{x}_{1}=1)=0.7}\\\\ &{{q}(\\mathbf{y}=1\\mid\\mathbf{x}_{2}=1)=0.7,}\\\\ &{{q}(\\mathbf{y}=1\\mid\\mathbf{x}_{1}=0)=0.3,}\\\\ &{{q}(\\mathbf{y}=1\\mid\\mathbf{x}_{2}=0)=0.3.}\\\\ &{{\\quad}{q}(\\mathbf{y}=1\\mid\\mathbf{x}_{3})=0.5.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. We can compute these values from Table 5. ", "page_idx": 24}, {"type": "text", "text": "Proposition 2. Let $e_{\\mathbf{c}}(\\mathbf{x})=\\xi_{3}$ . Then, for the DGP in eq. (3), EVAL- $\\mathrm{X}\\big(q,e_{e n c o d e}\\big)>\\mathrm{EVAL}\u2013\\mathrm{X}\\big(q,e_{\\mathbf{c}}\\big)$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. By Lemma 5, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{EVAL-X}(q,e_{c})=\\mathbb{E}_{q(\\mathbf{y},\\mathbf{x})}\\log q(\\mathbf{y}\\mid\\mathbf{x}_{3})=\\mathbb{E}_{q(\\mathbf{y},\\mathbf{x})}\\log0.5\\approx-0.69\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now, denote $e_{\\mathrm{encode}}$ as $e_{e}$ for ease of reading ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{EvAL-X}(q,e_{e})=\\mathbb{E}_{(\\mathbf{v},\\mathbf{a})\\sim q(\\mathbf{x}_{e(\\mathbf{x})})}\\mathbb{E}_{\\mathbf{y}\\sim q(\\mathbf{y}\\mid\\mathbf{x}_{e(\\mathbf{x})}=(\\mathbf{v},\\mathbf{a}))}[\\log q(\\mathbf{y}\\mid\\mathbf{x_{v}}=\\mathbf{a})]}\\\\ &{\\hphantom{\\mathrm{\\sum_{\\theta}}}=q(\\mathbf{x}_{3}=1)\\mathbb{E}_{q(\\mathbf{x}_{1})}\\mathbb{E}_{q(\\mathbf{y}\\mid\\mathbf{x}_{1},\\mathbf{x}_{3}=1)}\\log q(\\mathbf{y}\\mid\\mathbf{x}_{1})}\\\\ &{\\hphantom{\\mathrm{\\sum_{\\theta}}}+q(\\mathbf{x}_{3}=0)\\mathbb{E}_{q(\\mathbf{x}_{2})}\\mathbb{E}_{q(\\mathbf{y}\\mid\\mathbf{x}_{2},\\mathbf{x}_{3}=0)}\\log q(\\mathbf{y}\\mid\\mathbf{x}_{2})}\\\\ &{\\hphantom{\\mathrm{\\sum_{\\theta}}}=0.5*0.5*(0.9*-\\log0.7+0.1*-\\log0.3)*2}\\\\ &{\\hphantom{\\mathrm{\\sum_{\\theta}}}+0.5*0.5*(0.9*-\\log0.7+0.1*-\\log0.3)*2}\\\\ &{\\hphantom{\\mathrm{\\sum_{\\theta}}}\\approx-0.44}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This concludes that EVAL- $\\cdot X\\mathrm{(}q,e_{\\mathrm{encode}}\\mathrm{)}>\\mathrm{EVAL}-X\\mathrm{(}q,e_{c}\\mathrm{)}.$ ", "page_idx": 25}, {"type": "text", "text": "Lemma 6. In the DGP in eq. (3), $e_{e n c o d e}(\\mathbf{x})$ is an EVAL-X-optimal reductive explanation and is encoding. ", "page_idx": 25}, {"type": "text", "text": "Proof. First, the following properties show for the DGP because when $\\mathbf{x}_{3}=1$ , y only depends on $\\mathbf{x}_{1}$ , and if $\\mathbf{x}_{3}=0$ , y only depends on $\\mathbf{x}_{2}$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}_{3}\\perp\\!\\!\\!\\mid\\mathbf{y}\\end{array}\\,\\,\\,\\,,\\quad\\mathbf{y}\\perp\\!\\!\\!\\parallel\\mathbf{x}_{2}\\mid\\mathbf{x}_{3}=1\\quad,\\quad\\mathbf{y}\\perp\\!\\!\\!\\parallel\\mathbf{x}_{1}\\mid\\mathbf{x}_{3}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "These independencies imply that ", "page_idx": 25}, {"type": "equation", "text": "$$\nq(\\mathbf{y}\\mid\\mathbf{x}=[\\mathbf{x}_{1},\\mathbf{x}_{2},1])=q(\\mathbf{y}\\mid\\mathbf{x}_{1},\\mathbf{x}_{3}=1)\\quad,\\quad q(\\mathbf{y}\\mid\\mathbf{x}=[\\mathbf{x}_{1},\\mathbf{x}_{2},0])=q(\\mathbf{y}\\mid\\mathbf{x}_{2},\\mathbf{x}_{3}=0).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then, the optimal explanation function that achieves EVAL- $x^{*}$ is $e(\\mathbf{x})\\,=\\,[1,0,1]$ if $\\mathbf{x}_{3}\\,=\\,1$ and $[0,1,1]$ otherwise. ", "page_idx": 25}, {"type": "text", "text": "Reductive explanations of size 1. If the explanation is forced to have fewer than 2 inputs, the optimal reductive explanation $e(\\mathbf{x})$ is only allowed to be one of $\\xi_{1},\\xi_{2},\\xi_{3}$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{e:|e(\\mathbf{x})|\\le1}\\mathbb{E}_{q(\\mathbf{y},\\mathbf{x})}\\sum_{i\\in\\{1,2,3\\}}\\mathbb{1}[e(\\mathbf{x})=\\xi_{i}]q(\\mathbf{y}\\mid\\mathbf{x}_{i}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Rewriting this expression to split the support of $\\mathbf{x}$ based on $\\mathbf{x}_{3}=1$ or 0: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}_{q({\\mathbf{y}},{\\mathbf{x}})}\\sum_{i\\in\\{1,2,3\\}}\\mathbb{1}[e({\\mathbf{x}})=\\xi_{i}]\\log q({\\mathbf{y}}\\mid{\\mathbf{x}}_{i})}\\\\ {\\displaystyle=q({\\mathbf{x}}_{3}=1)\\mathbb{E}_{q({\\mathbf{x}}_{2}\\mid{\\mathbf{x}}_{3}=1)}\\mathbb{E}_{q({\\mathbf{y}},{\\mathbf{x}}_{1}\\mid{\\mathbf{x}}_{3}=1)}\\sum_{i\\in\\{1,2,3\\}}\\mathbb{1}[e({\\mathbf{x}})=\\xi_{i}]\\log q({\\mathbf{y}}\\mid{\\mathbf{x}}_{i})}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{=q(\\mathbf{x}_{3}=1)\\mathbb{E}_{q(\\mathbf{x}_{2}\\mid\\mathbf{x}_{3}=1)}\\mathbb{E}_{q(\\mathbf{y},\\mathbf{x}_{1}\\mid\\mathbf{x}_{3}=1)}\\sum_{i\\in\\{1,2,3\\}}\\mathbb{I}[e(\\mathbf{x})=\\xi_{i}]\\log q}}\\\\ &{\\quad+\\,q(\\mathbf{x}_{3}=0)\\mathbb{E}_{q(\\mathbf{x}_{1}\\mid\\mathbf{x}_{3}=0)}\\mathbb{E}_{q(\\mathbf{y},\\mathbf{x}_{2}\\mid\\mathbf{x}_{3}=0)}\\sum_{i\\in\\{1,2,3\\}}\\mathbb{1}[e(\\mathbf{x})=\\xi_{i}]\\log}\\\\ &{=0.5\\,\\mathbb{E}_{q(\\mathbf{x}_{2})}\\mathbb{E}_{q(\\mathbf{y},\\mathbf{x}_{1}\\mid\\mathbf{x}_{3}=1)}\\sum_{i\\in\\{1,2,3\\}}\\mathbb{1}[e(\\mathbf{x})=\\xi_{i}]\\log q(\\mathbf{y}\\mid\\mathbf{x}_{i})}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\frac{1}{6}|\\log\\sigma|\\nabla\\times\\boldsymbol{u}|}\\\\ &{=0.5\\,\\Psi_{\\mathrm{qU}}[\\mathcal{R}_{1}\\otimes\\mathbf{x}_{1}\\ \\mathrm{ls}=1)\\qquad\\sum_{i=1}^{1}\\ \\mathrm{I}_{\\{\\ell(1)\\}}^{[\\epsilon]}=\\xi_{i}|\\log\\sigma|\\nabla\\times\\boldsymbol{u}|}\\\\ &{\\qquad+0.5\\,\\Psi_{\\mathrm{qU}}[\\mathcal{R}_{i}\\otimes\\mathbf{x}_{1}\\ \\mathrm{ls}=0]\\qquad\\sum_{\\substack{i=1,2}}^{1}\\ \\mathrm{I}_{\\{\\ell(\\kappa)\\}}^{[\\epsilon]}=\\xi_{i}|\\log\\sigma|\\nabla\\times\\boldsymbol{u}|}\\\\ &{\\qquad-\\frac{1}{2}\\Bigg(\\mathbb{E}_{\\ell(\\kappa)}\\bigg[\\mathcal{R}_{i}\\otimes\\mathbf{x}_{1}\\ \\mathrm{ls}=1\\}\\mathbb{E}_{\\ell}\\big[\\mathcal{R}_{1},\\Sigma_{1},1\\big]=\\xi_{i}|\\log\\sigma|\\nabla\\times\\boldsymbol{u}|}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\mathbb{E}_{\\ell(\\kappa)\\times1}\\,|\\log\\sigma|\\nabla\\times\\boldsymbol{u}|\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{I}_{\\ell}|\\in[\\kappa_{1},\\mathbf{x}_{2},1]\\Bigr)=\\xi_{i}|\\log\\sigma|\\nabla\\times\\boldsymbol{u}|\\Bigg)}\\\\ &{\\qquad+\\mathbb{E}_{\\ell(\\kappa)}\\bigg[\\mathcal{R}_{i}\\otimes\\mathbf{x}_{1}\\ \\mathrm{ls}=0\\Big]\\,\\Big[\\ell\\big(\\mathbf{x}_{1},\\mathbf{x}_{2},0\\big)-\\xi_{i}|\\log\\sigma|\\nabla\\times\\boldsymbol{u}|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\Big]\\log\\sigma|\\nabla\\times\\boldsymbol{u}|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\mathbb{E}_{\\ell(\\nu)\\times1}\\,|\\log\\sigma|\\nabla\\times\\boldsymbol{u}|\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\;}&{=\\frac{1}{2}\\mathbb{E}_{q(\\mathbf{x}_{1},\\mathbf{x}_{2}\\mid\\mathbf{x}_{3}=1)}\\bigg[\\mathbb{1}[e([\\mathbf{x}_{1},\\mathbf{x}_{2},1])=\\xi_{1}]\\mathbb{E}_{q(\\mathbf{y}\\mid\\mathbf{x}_{1},\\mathbf{x}_{3}=1)}\\log q(\\mathbf{y}\\mid\\mathbf{x}_{1})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\displaystyle\\sum_{i\\in2,3}\\mathbb{1}[e([\\mathbf{x}_{1},\\mathbf{x}_{2},1])=\\xi_{i}]\\mathbb{E}_{q(\\mathbf{y}\\mid\\mathbf{x}_{1},\\mathbf{x}_{3}=1)}\\log q(\\mathbf{y}\\mid\\mathbf{x}_{i})\\bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{+\\displaystyle\\frac12\\mathbb{E}_{q(\\mathbf{x}_{1},\\mathbf{x}_{2}\\mid\\mathbf{x}_{3}=0)}\\bigg[\\mathbb{1}[e([\\mathbf{x}_{1},\\mathbf{x}_{2},0])=\\xi_{2}]\\mathbb{E}_{q(\\mathbf{y}\\mid\\mathbf{x}_{2},\\mathbf{x}_{3}=0)}\\log q(\\mathbf{y}\\mid\\mathbf{x}_{2})}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\sum_{i\\in1,3}\\mathbb{1}[e([\\mathbf{x}_{1},\\mathbf{x}_{2},0])=\\xi_{i}]\\mathbb{E}_{q(\\mathbf{y}\\mid\\mathbf{x}_{2},\\mathbf{x}_{3}=0)}\\log q(\\mathbf{y}\\mid\\mathbf{x}_{i})\\bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We will now focus on the three terms within each of eq. (36) and eq. (37). Due to the following equality ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q(\\mathbf{y}=1\\mid\\mathbf{x}_{1}=1,\\mathbf{x}_{3}=1)=q(\\mathbf{y}=0\\mid\\mathbf{x}_{1}=0,\\mathbf{x}_{3}=1)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=q(\\mathbf{y}=1\\mid\\mathbf{x}_{2}=1,\\mathbf{x}_{3}=0)=q(\\mathbf{y}=0\\mid\\mathbf{x}_{2}=0,\\mathbf{x}_{3}=0)=0.9,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "the expectations in the first terms in each of eq. (36) and eq. (37) are ", "page_idx": 26}, {"type": "text", "text": "$\\begin{array}{r}{\\mathbb{E}_{q(\\mathbf{y}\\mid\\mathbf{x}_{1},\\mathbf{x}_{3}=1)}\\log q(\\mathbf{y}\\mid\\mathbf{x}_{1})=\\mathbb{E}_{q(\\mathbf{y}\\mid\\mathbf{x}_{2},\\mathbf{x}_{3}=1)}\\log q(\\mathbf{y}\\mid\\mathbf{x}_{2})=0.9\\log0.7+0.1\\log0.3\\approx-0.44.}\\end{array}$ Next we turn to setting $i=1$ term in eq. (36). Due that $q(\\mathbf{y}=1\\mid\\mathbf{x}_{2}=1)=q(\\mathbf{y}=0\\mid\\mathbf{x}_{2}=0),$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{x}_{1}=\\mathbf{x}_{2}\\implies\\mathbb{E}_{q(\\mathbf{y}\\mid\\mathbf{x}_{1},\\mathbf{x}_{3}=1)}\\log q(\\mathbf{y}\\mid\\mathbf{x}_{2})=(0.9\\log0.7+0.1\\log0.3)\\approx-0.44,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{x}_{1}\\neq\\mathbf{x}_{2}\\implies\\mathbb{E}_{q(\\mathbf{y}\\mid\\mathbf{x}_{1},\\mathbf{x}_{3}=1)}\\log q(\\mathbf{y}\\mid\\mathbf{x}_{2})=(0.9\\log0.3+0.1\\log0.7)\\approx-1.12.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The same equalities hold for the $i\\;=\\;2$ term in eq. (37) $\\mathbb{E}_{q(\\mathbf{y},\\mathbf{\\xi}|\\mathbf{\\xi}\\mathbf{x}_{2},\\mathbf{x}_{3}=0)}\\log q(\\mathbf{y}\\mid\\mathbf{x}_{1})$ . Finally, regardless of $\\mathbf{x}_{1},\\mathbf{x}_{2}$ , the $i=3$ terms in both eq. (36) and eq. (37) can be expressed as follows: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}_{q(\\mathbf{y}\\mid\\mathbf{x}_{1},\\mathbf{x}_{3}=1)}\\log q(\\mathbf{y}\\mid\\mathbf{x}_{2})\\mathbb{E}_{q(\\mathbf{y}\\mid\\mathbf{x}_{2},\\mathbf{x}_{3}=0)}\\log q(\\mathbf{y}\\mid\\mathbf{x}_{1})=(0.9\\log{0.5}+0.1\\log{0.5})\\approx-0.69.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Now we can maximize the sum of eq. (36) and eq. (37), over $e(\\mathbf{x})$ such that $|e(\\mathbf{x})|=1$ . ", "page_idx": 26}, {"type": "text", "text": "Notice that setting $\\mathbb{1}[e({\\bf x})=\\xi_{1}]\\,=\\,1$ when $\\mathbf{x}_{3}=1$ and $\\mathbb{1}[e({\\bf x})=\\xi_{2}]=1$ when $\\mathbf{x}_{3}=0$ achieves the highest score $-0.44$ in each of eq. (36) and eq. (37). This implies that one optimal reductive explanation is $\\xi_{1}=[1,0,0]$ if $\\mathbf{x}_{3}=1$ and $\\xi_{2}=[0,1,0]$ otherwise. This is an encoding explanation as we show below. Due to $\\mathbf{E}_{\\xi_{1}}=\\mathbf{x}_{3}$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\nq(\\mathbf{y}=1\\mid\\mathbf{x}_{1},\\mathbf{E}_{\\xi_{1}}=1)=0.9\\neq q(\\mathbf{y}=1\\mid\\mathbf{x}_{1},\\mathbf{E}_{\\xi_{1}}=0)=0.5.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "In turn, $\\mathbf{y}$ $\\mathbf{E}_{\\xi_{1}}\\mid\\mathbf{x}_{\\xi_{1}}$ for $\\{\\mathbf{x}:e(\\mathbf{x})=\\xi_{1}\\}$ and Def: Encoding holds, meaning that $e(\\mathbf{x})$ is encoding. ", "page_idx": 26}, {"type": "text", "text": "B.8 An example of misestimation of EVAL-X ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Consider the following example where ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{x}=[\\mathbf{x}_{1},\\mathbf{x}_{2},\\mathbf{x}_{3},\\mathbf{x}_{4}],}\\\\ &{\\mathbf{x}_{2},\\mathbf{x}_{3}\\sim\\mathcal{B}(0.5)^{\\otimes2},\\,\\mathbf{x}_{1},\\mathbf{x}_{4}\\sim\\mathcal{N}(0,\\mathbf{I}),\\quad\\mathbf{y}=\\mathbf{x}_{2}\\oplus\\mathbf{x}_{3}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Assume that the misestimated EVAL-X model satisfies these equalities ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q_{\\xi_{1}}^{\\mathrm{misestimated}}(\\mathbf{y}=1\\mid\\mathbf{x}_{1})=1\\quad\\mathrm{for~all\\,}\\mathbf{x}_{1},}\\\\ {q_{\\xi_{4}}^{\\mathrm{misestimated}}(\\mathbf{y}=0\\mid\\mathbf{x}_{4})=1\\quad\\mathrm{for~all\\,}\\mathbf{x}_{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "There exists a bad explanation that scores optimally under the misestimated EVAL- $\\mathrm{X}$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\ne(\\mathbf{x})={\\binom{\\xi_{1}}{\\xi_{4}}}\\qquad{\\mathrm{if~}}\\mathbf{x}_{2}\\oplus\\mathbf{x}_{3}=1,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then the EVAL-X score of this explanation under this particular misestimation is $\\begin{array}{r l}&{\\mathrm{:VAL-X^{misesimated}}(q,e)=\\mathbb{E}_{q}\\big[\\log q_{e(\\mathbf{x})}^{\\mathrm{misesimated}}(\\mathbf{y}\\mid\\mathbf{x}_{e(\\mathbf{x})})\\big]}\\\\ &{=q(\\mathbf{y}=1)\\mathbb{E}_{q}\\big[\\log q_{e(\\mathbf{x})}^{\\mathrm{misesimated}}(\\mathbf{y}\\mid\\mathbf{x}_{e(\\mathbf{x})})\\mid\\mathbf{y}=1\\big]+q(\\mathbf{y}=0)\\mathbb{E}_{q}\\big[\\log q_{e(\\mathbf{x})}^{\\mathrm{misesimated}}(\\mathbf{y}\\mid\\mathbf{x}_{e(\\mathbf{x})})\\mid\\mathbf{y}=0\\big]}\\\\ &{=0.5\\cdot\\mathbb{E}_{q}\\big[\\log q_{\\xi_{1}}^{\\mathrm{misesimated}}(\\mathbf{y}\\mid\\mathbf{x}_{1})\\mid\\mathbf{y}=1\\big]+0.5\\cdot\\mathbb{E}_{q}\\big[\\log q_{\\xi_{4}}^{\\mathrm{misesimated}}(\\mathbf{y}\\mid\\mathbf{x}_{4})\\mid\\mathbf{y}=0\\big]}\\end{array}$ $=0$ . ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "Since $\\mathbf{y}$ is deterministic given x so the maximum value of the EVAL- $\\mathrm{X}$ score is also 0. So the bad explanation scores optimally due to misestimation. Deterministic $\\textbf{y}\\mid\\textbf{x}$ is not necessary for estimation error to affect explanation quality. Here, with this incorrectly estimated EVAL- $\\mathrm{X}$ , inputs that are pure noise, independent of everything, will be chosen. ", "page_idx": 27}, {"type": "text", "text": "B.9 Attention map explanations be encode. ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Here, treating each of the coordinates of $\\mathbf{x}$ as tokens, we consider a cross-attention based predictive model of the following form: with $\\gamma(\\mathbf{a})$ as softmax function over a vector a, $W$ as a matrix, $\\alpha,\\beta$ as vectors, and $\\sigma$ as the sigmoid function, and $\\kappa$ as the temperature, the predictive model $f(\\cdot)$ is ", "page_idx": 27}, {"type": "equation", "text": "$$\nf(\\mathbf{x})=\\sigma\\left(\\sum_{i}\\beta_{i}\\left[\\sum_{j}\\gamma(\\kappa\\mathbf{x}_{i}*W\\mathbf{x})_{j}\\alpha_{j}\\mathbf{x}_{j}\\right]\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We then show that using the highest attention score as the explanation produces an encoding explanation. For this example, we consider the following DGP: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{z}_{1},\\mathbf{z}_{2},\\mathbf{z}_{3}\\sim\\mathcal{B}(0.5)^{\\otimes3},}&{}\\\\ {\\mathbf{z}^{+}=[\\mathbf{z}_{1}+1,\\quad0,\\quad+1],}&{}\\\\ {\\mathbf{z}^{-}=[0}&{,-\\mathbf{z}_{2}-1,\\quad-1],}&{}\\\\ {\\mathbf{x}=\\left\\{\\begin{array}{l l}{\\mathbf{z}^{+}\\mathrm{if}\\,\\mathbf{z}_{3}=1,}\\\\ {\\mathbf{z}^{-}\\mathrm{if}\\,\\mathbf{z}_{3}=0,}\\end{array}\\right.}\\\\ {\\mathbf{y}\\sim\\mathcal{B}(\\rho)}&{~\\mathrm{where}\\quad\\rho=\\left\\{\\sigma(\\mathbf{x}_{1})\\quad\\mathrm{if}\\,\\mathbf{x}_{3}=1,\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The following setting of parameters in $f(\\mathbf{x})$ produces a function of $\\mathbf{x}$ that converges to $\\rho$ as $\\kappa\\rightarrow\\infty$ : ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\alpha=[1,-1,0],\\qquad\\qquad\\qquad\\qquad\\beta=[1,1,0],\\qquad\\qquad\\qquad\\qquad W=\\left({0}{\\begin{array}{l l l}{0}&{0}&{0}\\\\ {0}&{0}&{0}\\\\ {0}&{0}&{1}\\end{array}}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By definition of $W$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{W\\mathbf{x}=\\left\\{\\begin{array}{l l}{[0,0,1]\\;\\mathrm{if}\\;\\mathbf{x}_{3}=1}&{(x_{1}-x_{2})\\qquad\\qquad\\qquad\\qquad}&{(x_{3})\\qquad\\qquad\\qquad}\\\\ {[0,0,-1]\\;\\mathrm{if}\\;\\mathbf{x}_{3}=-1}&{}\\\\ {\\implies\\mathbf{x}_{1}W\\mathbf{x}=\\left\\{\\left[\\mathbf{z}_{1}+1,0,0\\right]\\;\\mathrm{if}\\;\\mathbf{x}_{3}=1}&{(x_{1}-x_{3})\\qquad\\Longrightarrow\\;\\gamma(\\mathbf{x}_{1}W\\mathbf{x})\\overset{\\kappa\\to\\infty}{\\longrightarrow}\\left\\{\\left[1,0,0\\right]\\;\\mathrm{if}\\;\\mathbf{x}_{3}=1}&{(x_{2}-x_{3})\\qquad\\qquad\\mathrm{if}\\;\\mathbf{x}_{3}\\in\\left[2,1\\right]\\right\\}}\\\\ {\\left[0,0,0\\right]\\;\\mathrm{if}\\;\\mathbf{x}_{3}=-1}&{}\\end{array}\\right.}\\\\ {\\implies\\mathbf{x}_{2}W\\mathbf{x}=\\left\\{\\begin{array}{l l}{[0,0,0]\\;\\mathrm{if}\\;\\mathbf{x}_{3}=1}&{(x_{1}-x_{3})\\qquad\\Longrightarrow\\;\\gamma(\\mathbf{x}_{2}W\\mathbf{x})\\overset{\\kappa\\to\\infty}{\\longrightarrow}\\left\\{\\left[0,0,0\\right]\\;\\mathrm{if}\\;\\mathbf{x}_{3}=1}&{(x_{1}-x_{3})\\qquad\\qquad\\mathrm{if}\\;\\mathbf{x}_{3}\\in\\left[2,1\\right]}\\\\ {\\left[0,\\mathbf{z}_{2}+1,0\\right]\\;\\mathrm{if}\\;\\mathbf{x}_{3}=-1}&{(x_{1}-x_{3})\\qquad\\Longrightarrow\\;\\gamma(\\mathbf{x}_{2}W\\mathbf{x})\\overset{\\kappa\\to\\infty}{\\longrightarrow}\\left\\{\\left[0,1,0\\right]\\;\\mathrm{if}\\;\\mathbf{x}_{3}=-1}&{(x_{2}-x_{3})\\qquad\\qquad\\mathrm{if}\\;\\mathbf{x}_{3}\\in\\left[2,1\\right]}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then, $\\beta_{3}\\,=\\,0$ , the inner sum for $i\\,=\\,3$ does not appear in the function $f(\\mathbf{x})$ Then, as $\\alpha_{3}~=~0$ , $\\alpha_{1}=1,\\alpha_{2}=-1$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\sum_{j}\\gamma(\\kappa{\\bf x}_{1}*W{\\bf x})_{j}\\alpha_{j}{\\bf x}_{j}\\stackrel{\\kappa\\to\\infty}{\\longrightarrow}\\left\\{{\\bf x}_{1}\\mathrm{~if~}{\\bf x}_{3}=1\\right.\\quad,}}\\\\ {{\\displaystyle\\sum_{j}\\gamma(\\kappa{\\bf x}_{2}*W{\\bf x})_{j}\\alpha_{j}{\\bf x}_{j}\\stackrel{\\kappa\\to\\infty}{\\longrightarrow}\\left\\{0\\mathrm{~if~}{\\bf x}_{3}=1\\right.\\quad}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "In turn, as $\\beta_{1}=\\beta_{2}=1$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{i\\in1,2}\\beta_{i}\\left[\\sum_{j}\\gamma(\\kappa\\mathbf{x}_{i}*W\\mathbf{x})_{j}\\alpha_{j}\\mathbf{x}_{j}\\right]\\stackrel{\\kappa\\to\\infty}{\\longrightarrow}\\left\\{\\mathbf{x}_{1}\\;\\mathrm{if}\\;\\mathbf{x}_{3}=1\\right.\\quad,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\nf(\\mathbf{x})=\\sigma\\left(\\sum_{i\\in1,2}\\beta_{i}\\left[\\sum_{j}\\gamma(\\kappa\\mathbf{x}_{i}*W\\mathbf{x})_{j}\\alpha_{j}\\mathbf{x}_{j}\\right]\\right)\\xrightarrow{\\kappa\\to\\infty}\\left\\{\\sigma(\\mathbf{x}_{1})\\;{\\mathrm{if}}\\;\\mathbf{x}_{3}=1\\right.\\quad.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "So, as $\\kappa\\rightarrow\\infty$ , the function $f(\\mathbf{x})$ , with the parameters in eq. (39), converges to $\\rho(\\mathbf{x})$ , meaning that this model will achieve the population log-likelihood optimum under the DGP in eq. (38). ", "page_idx": 28}, {"type": "text", "text": "Now, the attention map as an explanation selects $\\mathbf{x}_{1}$ if $\\mathbf{x}_{3}=1$ and $\\mathbf{x}_{2}$ otherwise; this comes from eq. (43) and eq. (44). This is an encoding explanation because $\\mathbf{E}_{\\xi_{1}}=1$ if $\\mathbf{x}_{3}=1$ which gives ", "page_idx": 28}, {"type": "equation", "text": "$$\nq(\\mathbf{y}\\mid\\mathbf{x}_{1})\\neq q(\\mathbf{y}\\mid\\mathbf{x}_{1},\\mathbf{x}_{3}=1)\\Longrightarrow\\mathbf{y}\\lnot\\{\\mathbf{k}\\mathbf{E}_{\\xi_{1}}\\mid\\mathbf{x}_{\\xi_{1}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "C Experimental details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "C.1 Estimating STRIPE-X ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "To compute the $\\mathbf{KL}$ term in ENCODE-METER, we estimate $q(\\mathbf{E_{v}}\\mid\\mathbf{x_{v}},\\mathbf{y})$ and $q(\\mathbf{E_{v}}\\mid\\mathbf{x_{v}})$ . To estimate these, we train a single model \u2014 to predict $\\mathbf{E_{v}}$ from $\\mathbf{x_{v}}$ and a new variable $\\ell$ that can equal the label $\\mathbf{y}$ or a dummy value null that is outside the support of $\\mathbf{y}$ \u2014 in the following way: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\theta}{\\arg\\operatorname*{max}}\\,\\mathbb{E}_{\\mathbf{x},\\mathbf{y}\\sim q(\\mathbf{x},\\mathbf{y})}\\bigg[\\log p_{\\theta}(\\mathbf{E}_{\\mathbf{v}}=\\mathbb{1}[e(\\mathbf{x})=\\mathbf{v}]\\mid\\mathbf{x_{v}},\\ell=\\mathbf{y})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\log p_{\\theta}(\\mathbf{E}_{\\mathbf{v}}=\\mathbb{1}[e(\\mathbf{x})=\\mathbf{v}]\\mid\\mathbf{x_{v}},\\ell=\\mathrm{nu1}1)\\bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "As log-likelihood is a proper scoring rule and $q(\\mathbf{y}=\\mathrm{nu}11)=0$ , the maximum above is achieved when ", "page_idx": 28}, {"type": "equation", "text": "$$\np_{\\theta}(\\mathbf{E}_{\\mathbf{v}}\\mid\\mathbf{x_{v}},{\\boldsymbol{\\ell}}=\\mathbf{y})=q(\\mathbf{E}_{\\mathbf{v}}\\mid\\mathbf{x_{v}},\\mathbf{y}=\\mathbf{y})\\qquad p_{\\theta}(\\mathbf{E}_{\\mathbf{v}}\\mid\\mathbf{x_{v}},{\\boldsymbol{\\ell}}=\\mathrm{nu}{\\boldsymbol{1}}{\\boldsymbol{1}})=q(\\mathbf{E}_{\\mathbf{v}}\\mid\\mathbf{x_{v}}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "In summary, to estimate ENCODE-METER, solve eq. (47), use its solution to estimate the KL term from the RHS in eq. (7) for each $\\mathbf{x}_{\\mathbf{v}},\\mathbf{y}$ , and then average this $\\mathbf{KL}$ term over samples of $\\mathbf{x_{v}}$ from the data such that $e(\\mathbf{x})=\\mathbf{v}$ and samples of y from the EVAL-X model for $q(\\mathbf{y}\\mid\\mathbf{x_{v}})$ . ", "page_idx": 28}, {"type": "text", "text": "In practice, one does not need train a model for each v. We describe how to estimate ENCODEMETER with a single model in appendix C.2. We give the full STRIPE- $\\mathrm{X}$ estimation procedure in algorithm 2 in appendix D. ", "page_idx": 28}, {"type": "text", "text": "C.2 Estimating the encoding cost in STRIPE- $\\mathbf{X}$ with categorical predictive models ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "STRIPE- $\\mathbf{\\nabla}X$ consists of the EVAL- $\\mathbf{\\nabla}X$ score and a cost of encoding measured by ENCODE-METER. Define $\\nu$ to be the set of possible explanations and let $\\mathcal{V}[j]$ denote the $j$ th element of $\\nu$ . The EVAL-X model $p_{\\gamma}(\\mathbf{y}\\mid\\mathbf{x_{v}})$ is trained to predict the label $\\mathbf{y}$ from subsets $\\mathbf{x_{v}}$ where $\\mathbf{v}$ is uniformly sampled from $\\nu$ [20]. Next is computing the ENCODE-METER $\\phi_{q}(e)$ that is used in the encoding cost term in STRIPE-X. For each explanation, let $\\mathbf{F}$ be the categorical variable (instead of an indicator $\\mathbf{E_{v}},$ ) that denotes, for each sample, which inputs were selected by the explanation $e(\\mathbf{x})$ : $\\mathbf{F}=j$ if $\\mathbf{E}_{\\mathcal{V}[j]}\\,=\\,\\mathbb{1}[e(\\mathbf{x})\\,=\\,\\mathcal{V}[j]]\\,=\\,1$ . Let $q(j)$ be the distribution over $j$ induced by $q(e(\\mathbf{x}))$ . We train a model $p_{\\theta}(\\mathbf{F_{\\theta}}|\\mathbf{\\Deltax_{v}},\\boldsymbol{\\ell},\\mathbf{v})$ with a modification of eq. (47) that averages over $\\mathbf{v}\\sim q(e(\\mathbf{x}))$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\underset{\\theta}{\\arg\\operatorname*{max}}\\,\\mathbb{E}_{\\mathbf{v}\\sim q(e(\\mathbf{x}))}\\mathbb{E}_{\\mathbf{x},\\mathbf{y}\\sim q(\\mathbf{x},\\mathbf{y})}\\sum_{\\gamma[j]\\in\\mathcal{V}}\\Bigg(\\mathbb{1}[e(\\mathbf{x})=\\mathcal{V}[j]]\\big[\\log p_{\\theta}(\\mathbf{F}=j\\mid\\mathbf{x_{v}},\\ell=\\mathbf{y},\\mathbf{v})+}&\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}&{}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\log p_{\\theta}(\\mathbf{F}=j\\mid\\mathbf{x_{v}},\\ell=\\mathrm{nu1}1,\\mathbf{v})\\big]\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The variable $\\ell$ takes values in $\\{-1,0,1\\}$ where 0 and 1 correspond to $\\mathbf y=0$ and $\\mathbf y=1$ respectively and $-1$ corresponds to the null value. For a flexible enough model $p_{\\theta}$ that achieves the population maximum of eq. (48), for any $\\mathbf{v}=\\mathcal{V}[j]\\in\\mathcal{V}$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{\\theta}(\\mathbf{F}=j\\mid\\mathbf{x_{v}},\\boldsymbol{\\ell}=\\mathbf{y},\\mathbf{v})=q(\\mathbf{E_{v}}=1\\mid\\mathbf{x_{v}},\\mathbf{y}=\\mathbf{y}),}\\\\ {p_{\\theta}(\\mathbf{F}=j\\mid\\mathbf{x_{v}},\\boldsymbol{\\ell}=\\mathrm{nu}11,\\mathbf{v})=q(\\mathbf{E_{v}}=1\\mid\\mathbf{x_{v}}).\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This fact indicates how one can use the model $p_{\\theta}$ to estimate ENCODE-METER. First, construct the explanation dataset $D_{e}=\\{(\\mathbf{y},\\mathbf{x}_{e(\\mathbf{x})})\\}$ from $D_{t}$ . Define $q_{D_{e}}$ to be the uniform distribution over $D_{e}$ . Define $\\mathcal{E}_{(\\mathbf{v},\\mathbf{a})}$ as the uniform distribution over $K$ samples of $\\mathbf{y}$ from the EVAL-X model: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{E}_{(\\mathbf{v},\\mathbf{a})}=\\mathbf{U}\\left[\\{\\hat{\\mathbf{y}}\\}_{k\\le K}\\right]}&{{}\\{\\mathrm{~where~}\\hat{\\mathbf{y}}^{k}\\sim p_{\\gamma}(\\mathbf{y}\\mid\\mathbf{x_{v}}=\\mathbf{a})\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then, estimate ENCODE-METER as follows: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\dot{\\mathrm{P}}(q,e)=\\mathbb{E}_{(\\mathbf{v},\\mathbf{a})\\sim q_{D_{e}}(\\mathbf{x}_{e}(\\mathbf{x}))}\\mathbb{E}_{\\hat{\\mathbf{P}}\\sim\\mathcal{E}_{(\\mathbf{v},\\mathbf{a})}}\\left(p_{\\theta}(\\mathbf{F}=j\\mid\\mathbf{x}_{\\mathbf{v}},\\ell=\\hat{\\mathbf{y}},\\mathbf{v})\\log\\frac{p_{\\theta}(\\mathbf{F}=j\\mid\\mathbf{x}_{\\mathbf{v}},\\ell=\\hat{\\mathbf{y}},\\mathbf{v})}{p_{\\theta}(\\mathbf{F}=j\\mid\\mathbf{x}_{\\mathbf{v}},\\ell=\\mathrm{nu1}1,\\mathbf{v})}\\right.\\quad}\\\\ {\\quad\\left.+\\;p_{\\theta}(\\mathbf{F}\\neq j\\mid\\mathbf{x}_{\\mathbf{v}},\\ell=\\hat{\\mathbf{y}},\\mathbf{v})\\log\\frac{p_{\\theta}(\\mathbf{F}\\neq j\\mid\\mathbf{x}_{\\mathbf{v}},\\ell=\\hat{\\mathbf{y}},\\mathbf{v})}{p_{\\theta}(\\mathbf{F}\\neq j\\mid\\mathbf{x}_{\\mathbf{v}},\\ell=\\mathrm{nu1}1,\\mathbf{v})}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "C.3 Estimating ENCODE-METER with a generative model ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "When estimating the STRIPE- $\\mathbf{\\nabla}X$ score with procedure above for many different explanations, the maximization in eq. (47) repeated for every explanation, which can be computationally expensive. This motivates a second procedure to estimate ENCODE-METER that avoids having to retrain models for each explanation by using generative model for $q(\\mathbf{x}\\mid\\mathbf{x_{v}},\\mathbf{y})$ . Formally, with $\\mathbf{x_{v}}$ fixed, the conditional mutual information term in eq. (5) can be computed as the marginal dependence between $N$ samples of $\\mathbf{y}$ from $q(\\mathbf{y}\\mid\\mathbf{x_{v}})$ and $q(\\mathbf{E_{v}}\\mid\\mathbf{x_{v}},\\mathbf{y})$ . The model for the former is available from EVAL- $\\mathrm{X}$ estimation. Simulating from the later, namely $q(\\mathbf{E_{v}}\\mid\\mathbf{x_{v}},\\mathbf{y})$ , is done by sampling from the generative model $\\mathbf{x}\\mid\\mathbf{x_{v}},\\mathbf{y}$ and then computing the indicator $\\mathbf{E_{v}}$ as $\\mathbb{I}[e({\\bf x})={\\bf v}]$ . Mechanically, with an estimator of mutual information from samples $(\\{\\mathbf{a}^{i}\\}_{i\\leq N},\\{\\mathbf{b}^{i}\\}_{i\\leq N})$ denoted $\\mathbb{M}\\mathbb{I}\\big(\\{\\mathbf{a}^{i}\\},\\{\\mathbf{b}^{i}\\}\\big)$ and with samples $\\{\\mathbf{a}^{i}\\}$ produced conditionally on values $\\mathbf{c}^{i}$ denoted by a subscript of the conditioned value $\\{\\mathbf{a}^{i}\\}_{\\mathbf{c}^{i}}$ , one can estimate ENCODE-METER as follows: sample $\\mathbf{y}_{(\\mathbf{v},\\mathbf{a})}^{i}\\,\\sim\\,\\mathbf{y}\\,\\mid\\,\\mathbf{x_{v}}\\,=\\,\\mathbf{a}$ and $\\mathbf{x}_{\\mathbf{v},\\mathbf{a},\\mathbf{y}^{i}}^{i}\\sim q(\\mathbf{x}\\mid\\mathbf{x_{v}}=\\mathbf{a},\\mathbf{y}=\\mathbf{y}^{i})$ ) repeatedly $N$ times and compute ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}_{(\\mathbf{v},\\mathbf{a})\\sim q(\\mathbf{x}_{e(\\mathbf{x})})}\\mathtt{M I}\\left(\\{\\mathbf{y}^{i}\\}_{(\\mathbf{v},\\mathbf{a})},\\big\\{\\mathbb{1}[e(\\mathbf{x}^{i})=\\mathbf{v}]\\big\\}_{\\mathbf{v},\\mathbf{a},\\mathbf{y}^{i}}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We give the full procedure in algorithm 1. ", "page_idx": 29}, {"type": "text", "text": "C.4 Experimental details from the simulated study. ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "The data-generating processes from the experiments. Let $\\mathcal{N}$ be the standard normal distribution and let $\\boldsymbol{B}(\\alpha)$ be the Bernoulli distribution with 1 occurring with probability $\\alpha$ . With $\\rho=0.9$ , the discrete DGP is: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbf{x}=[\\mathbf{x}_{1},\\mathbf{x}_{2},\\mathbf{x}_{3},\\mathbf{x}_{4},\\mathbf{x}_{5}]\\sim\\mathcal{B}(0.5)^{\\otimes5},\\qquad\\mathbf{y}=\\left\\{\\mathbf{x}_{1}\\quad\\mathbf{w}.\\mathrm{p.~}\\rho\\quad\\mathrm{else}\\quad1-\\mathbf{x}_{1}\\quad\\mathrm{if}\\ \\mathbf{x}_{3}=1,}\\\\ {\\mathbf{x}_{2}\\quad\\mathbf{w}.\\mathrm{p.~}\\rho\\quad\\mathrm{else}\\quad1-\\mathbf{x}_{2}\\quad\\mathrm{if}\\ \\mathbf{x}_{3}=0.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The hybrid DGP is as follows: with $\\gamma=5$ and $\\begin{array}{r}{\\sigma(x)=\\frac{1}{1+\\exp(-x)}}\\end{array}$ as the sigmoid function ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbf{x}=[\\mathbf{x}_{1},\\mathbf{x}_{2},\\mathbf{x}_{4},\\mathbf{x}_{5}]\\sim\\mathcal{N}(0.5)^{\\otimes4},\\mathbf{x}_{3}\\sim\\mathcal{B}(0.5),\\ \\ \\ \\ \\rho=\\left\\{\\sigma(\\gamma\\,\\mathbf{x}_{1})\\ \\mathrm{~if~}\\mathbf{x}_{3}=1,\\ \\ \\ \\ \\mathbf{y}\\sim\\mathcal{B}(\\rho).\\right.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Computing accuracy and KL to show that POSI, PRED, MARG are encoding. For each encoding type, we build two decision trees from 1000 samples from eq. (49): the first decision tree learns $q(\\mathbf{E}_{\\mathbf{v}}\\mid\\mathbf{x_{v}})$ and the second learns $q(\\mathbf{y}\\mid\\mathbf{x_{v}},\\mathbf{E_{v}}=b)$ for $b\\in\\{0,1\\}$ . We set the maximum depth to be 6. Trees of this depth learn any function of 6 binary digits; $\\mathbf{x}$ with $\\mathbf{E_{v}}$ as an additional column amounts to 6 binary digits. These decision trees are used to compute the accuracy of predicting $\\mathbf{E_{v}}$ with $q(\\mathbf{E_{v}}\\mid\\mathbf{x_{v}})$ and the $\\mathbf{KL}$ between $q(\\mathbf{y}\\mid\\mathbf{x_{v}},\\mathbf{E_{v}}\\,=\\,1)$ and $q(\\mathbf{y}\\mid\\mathbf{x_{v}},\\mathbf{E_{v}}\\,=\\,0)$ . Within a set $\\{\\mathbf{x}:e(\\mathbf{x})=\\mathbf{v}\\}$ that is all $\\mathbf{x}$ that have one of the possible selections $\\mathbf{v}$ , Table 6 report the accuracy of predicting $\\mathbf{E_{v}}$ with $q(\\mathbf{E_{v}}\\mid\\mathbf{x_{v}})$ and the $\\mathbf{KL}$ between $q(\\mathbf{y}\\mid\\mathbf{x_{v}},\\mathbf{E_{v}}=1)$ and $q(\\mathbf{y}\\mid\\mathbf{x_{v}},\\mathbf{E_{v}}=0)$ , averaged only over samples in $\\{\\mathbf{x}:e(\\mathbf{x})=\\mathbf{v}\\}$ . ", "page_idx": 29}, {"type": "text", "text": "Table 6: Position-based, prediction-based, and marginal explanation schemes are all encoding. For samples in the set $\\{\\mathbf{x}:e(\\mathbf{\\bar{x}})=\\mathbf{v}\\}$ for one of the selections $\\mathbf{v}$ that $e$ produces, accuracy $<1$ and the KL being non-zero means these explanations are all encoding per Lemma 1. ", "page_idx": 30}, {"type": "table", "img_path": "mkw6x0OExg/tmp/ddcb08f24c529a37eaf98acd222c984ae6a89a396db335488d001c4cc443eb7b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "EVAL- $\\mathbf{X}$ . To estimate EVAL-X for the DGPs in eq. (49) and eq. (50), we compute conditionals $q(\\mathbf{y}=1\\mid\\mathbf{x_{v}})$ via Monte Carlo approximation. Due to the different coordinates of $\\mathbf{x}$ being independent, one can compute $q(\\mathbf{y}=\\mathbf{\\bar{1}}\\mid\\mathbf{x_{v}})$ as a marginal expectation over the inputs except those in $\\mathbf{v}$ : ", "page_idx": 30}, {"type": "equation", "text": "$$\nq(\\mathbf{y}\\mid\\mathbf{x_{v}})=\\mathbb{E}_{q(\\mathbf{x_{v}^{c}}\\mid\\mathbf{x_{v}})}q(\\mathbf{y}\\mid\\mathbf{x_{v}},\\mathbf{x_{v}^{c}})=\\mathbb{E}_{q(\\mathbf{x_{v}^{c}})}q(\\mathbf{y}\\mid\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We Monte Carlo estimate the RHS of this equation over 500 resamples of $\\mathbf{x}_{\\mathbf{v}}^{c}$ . We take 5000 samples from each DGP to estimate EVAL- $\\mathbf{\\nabla}X$ scores with respect to $q(\\mathbf{y},\\mathbf{x})$ . In Appendix C.5 we also show experiment results where we use the EVAL- $\\mathrm{X}$ accuracy and AUROC as the score instead. ", "page_idx": 30}, {"type": "text", "text": "REAL- $\\mathbf{\\deltaX}$ . We solve REAL-X for any specified explanation size $K$ as follows. In the case of the discrete DGP, for each possible value of $x\\,\\in\\,{\\mathrm{supp}}(q(\\mathbf{x}))$ (of which there are finitely many), we make $e(\\mathbf{x})$ output the subset of at most size $K$ that achieves that maximum averaged log-likelihood over the samples that equal said value $\\textbf{x}=\\textbf{\\em x}$ . This produces the optimally-scoring explanation $e(\\mathbf{x})$ that maps each finite value in the support of $q(\\mathbf{x})$ to one subset of the coordinates of $\\mathbf{x}$ . To do the same in the continuous DGP in eq. (50), we round $\\mathbf{x}$ to integers and then use the same type of optimization as in the discrete case. ", "page_idx": 30}, {"type": "text", "text": "STRIPE-X. In estimating ENCODE-METER, the model $p_{\\theta}(\\mathbf{E_{v}}\\mid\\mathbf{x_{v}},\\boldsymbol{\\ell})$ is a decision tree of depth at most 5, which is then used to estimate averaged $\\mathbf{KL}$ in the RHS of eq. (7) with a single sample from y $\\mid\\mathbf{x_{v}}$ . The process is repeated for each $\\mathbf{v}$ and averaged to produce the ENCODE-METER. The simulated experiments were done on a CPU with the whole runtime around 10 minutes. ", "page_idx": 30}, {"type": "text", "text": "C.5 Experiments with EVAL-X accuracy and EVAL-X AUROC ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Here, instead of EVAL- $\\mathbf{\\nabla}X$ log-likelihoods, we use the accuracy and AUROC of the EVAL-X model as the score. We call these EVAL- $\\mathbf{\\nabla}X$ -ACC and EVAL-X-AUROC scores. These metrics only depend on the ranking of the datapoints, and therefore are not sensitive to differences in log probabilities that do not change ranks. Figure 6 shows that, due to this insensitivity, multiple encoding explanations (PRED, MARG, and the excessively reductive one) all achieve the same score as the corresponding EVAL- $X^{*}$ score. In summary, ranking metrics like accuracy and AUROC are not sensitive to encoding explanations like EVAL-X log-likelihoods. ", "page_idx": 30}, {"type": "text", "text": "C.6 Classifying dogs and cats. ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "The POSI explanation selects the upper or the lower color patch depending on whether $q(\\mathbf{y}\\ =$ $1\\mid\\mathbf{x})>0.5$ or not. The PRED explanation selects the patch predicting from which best matches the prediction from $q(\\mathbf{y}=1\\mid\\mathbf{x})$ . The MARG explanation selects the top or the bottom image patch based on the color as in the DGP in Figure 5. We consider two non-encoding explanations. The first explanation, denoted optimal, selects exactly the features that occur in the DGP: $\\{\\mathbf{x}_{1},\\mathbf{x}_{2}\\}$ if the color patch $\\mathbf{x}_{1}$ is blue and $\\{\\mathbf{x}_{1},\\mathbf{x}_{4}\\}$ otherwise. As y is determined by the explanation, meaning $\\mathbf{y}\\bot\\bot\\mathbf{E_{v}}\\mid\\mathbf{x_{v}}$ for all $\\mathbf{v}$ and values $\\mathbf{x_{v}}$ , this explanation is non-encoding. The second one, denoted fixed, always outputs the bottom right patch $\\mathbf{x}_{4}$ ; this explanation is constant which violates the first criterion in Lemma 1 meaning there is no encoding. ", "page_idx": 30}, {"type": "text", "text": "We also run the REAL- $\\mathbf{\\nabla}X$ method from [20] to produce an explanation. REAL-X was run over explanations that select one of the four quarter patches and exact marginalization over the selections. ", "page_idx": 30}, {"type": "image", "img_path": "mkw6x0OExg/tmp/8d3be059b7124cd5531c345ca8ed7888c432d0933550f8472ae495ad6b6ff882.jpg", "img_caption": ["(a) Results: Discrete DGP. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "mkw6x0OExg/tmp/2e57b83432427f8efafb6a7ec9b8df20847c65c2271d15211b0dc80487722375.jpg", "img_caption": ["(b) Results: hybrid DGP. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Figure 6: The EVAL-X-ACC and AUROC scores for the different explanations for the discrete DGP are on the left and the scores for the hybrid DGP are in the right. In both, multiple encoding explanations (PRED, MARG, and the reductive one from REAL- $\\mathrm{X}$ ) all achieve the same score as the corresponding EVAL- $\\cdot X^{*}$ score. Thus, ranking metrics like accuracy and AUROC are not sensitive to encoding explanations like EVAL- $_{\\textrm X}$ loglikelihoods, and can fail to even weakly detect encoding. This stems from the fact that accuracy and AUROC only depend on the ranking of the datapoint, and therefore are not sensitive to differences in log probabilities that do not change ranks. ", "page_idx": 31}, {"type": "text", "text": "The base cat and dog images were obtained from the cats_vs_dogs dataset from the Tensorflow datasets package. To construct images like in Figure 5, the color and the two images are sampled independently. The color being blue/red determines that the label associated with the top/bottom image becomes the label for the constructed image. The training, validation, and test dataset consist of 8000, 2000, and 10000 samples respectively. ", "page_idx": 31}, {"type": "text", "text": "We follow the procedure in Appendix C.2 to estimate STRIPE-X. The EVAL-X model and the model $p_{\\theta}(\\mathbf{F_{\\theta}}\\vert\\ \\mathbf{x_{v}},\\ell,\\mathbf{v})$ used in computing the ENCODE-METER term in STRIPE- $\\mathrm{X}$ (eq. (6)) are 34-layer Residual neural networks. The EVAL-X model is trained for 100 epochs with a batch size of 100 with the Adam optimizer, with the learning rate and weight decay parameters set to $10^{-3}$ and 0 respectively. The $p_{\\theta}(\\mathbf{F_{\\theta}}\\vert\\ \\mathbf{x_{v}},\\ell,\\mathbf{v})$ model is trained for 50 epochs with a batch size of 200 with the Adam optimizer, with the learning rate and weight decay parameters set to $5\\,\\times\\,10^{-5}$ and 1 respectively. The $p_{\\theta}$ model sees variable $\\ell$ through an entire extra channel where all the pixels take the value $\\ell$ . We used validation loss as the metric to early stop. The EVAL-X and STRIPE-X scores are computed on the test dataset. The cats vs. dogs experiment were done on an A100 GPU where the whole training and evaluation ran for 20 minutes. ", "page_idx": 31}, {"type": "text", "text": "C.7 LLM experiment details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We generate $10,000$ reviews of the following type: with ADJ1 and ADJ2 as adjectives, the review is \u2022 \u2018My day was <ADJ1> and the movie was <ADJ2>. that is it\u2019 or \u2022 \u2018My day was <ADJ1> and the movie was <ADJ2>. oh wait, reverse the adjectives\u2019. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "The second sentence in the review acts as a \u201ccontrol flow\u201d input and determines whether ADJ1 or ADJ2 describes the sentiment about the movie. We prompted Llama 3 to predict the sentiment and select words relevant to predicting the sentiment. In Appendix C.8, we give the prompts we used to make Llama 3 produce explanations from. For this problem, the inputs $\\mathbf{x}$ are the reviews and Llama 3 produces explanations $e(\\mathbf{x})$ that select a subset of words in the review. The summaries and explanations were generated for all $10,000$ samples but to estimate STRIPE- $\\mathrm{X}$ , we only used data from the 5 most common explanations (we restricted to inputs whose explanations v had high $q(e(\\mathbf{x})=\\mathbf{v})$ ). This resulted in a dataset of size 8136, which we split into a training, validation, and test datasets of sizes 6102, 1017, and 1017 respectively. ", "page_idx": 31}, {"type": "text", "text": "Both the EVAL-X model and the model for $p_{\\theta}(\\mathbf{F_{\\theta}}|\\mathbf{\\Deltax_{v}},\\mathbf{v},\\ell)$ (see Appendix C.2) used in estimating the ENCODE-METER term in STRIPE- $\\mathbf{\\nabla}X$ were finetuned GPT-2 models. For the EVAL-X model, we used the AdamW optimizer with a batch size of 100 and trained for 50 epochs with the learning rate set to $5e-5$ , weight decay set to 0, and a Cosine learning rate scheduler with the number of cycles set to 1. For the $p_{\\theta}$ model used in estimating ENCODE-METER, we used the AdamW optimizer with a batch size of 50 and trained for 25 epochs with the learning rate set to $5e-5$ , weight decay set to 0, and a Cosine learning rate scheduler with number of cycles set to 1. The $p_{\\theta}$ model sees variable $\\ell$ through the following word added to the input sequence of words: positive if $\\ell\\,=\\,\\mathbf{y}\\,=\\,1$ , negative if $\\ell\\,=\\,\\mathbf{y}\\,=\\,0$ , and nothing if $\\ell\\,=\\,\\mathrm{\\nu}11$ . We used validation loss to early stop. We follow the procedure in Appendix C.2 to compute ENCODE-METER with $p_{\\theta}(\\mathbf{F_{\\theta}}|\\mathbf{\\Deltax_{v}},\\mathbf{v},\\ell)$ on the test data with the averaging over $\\textbf{y}\\vert\\textbf{x}_{\\mathbf{v}}$ estimated using a 5 samples per value of $\\mathbf{x_{v}}$ . All training and inference for this experiment was done on an A100. The explanation step and the estimation for both parts of STRIPE-X together took under 2 hours. The LLM-generated explanations achieves an EVAL-X score of $-0.497$ and an ENCODE-METER value was 0.114. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "C.8 Prompts used to predict sentiment and produce explanation ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In Figure 7, we provide the prompt we used to predict the sentiment from a review and generate an explanation for that prediction. ", "page_idx": 32}, {"type": "text", "text": "Figure 7: Llama 3 prompt used to predict sentiment and generate an explanation for that prediction. ", "page_idx": 32}, {"type": "text", "text": "System: You are a helpful and honest assistant. Please, respond concisely and truthfully. ", "page_idx": 32}, {"type": "text", "text": "You are asked to summarize movie reviews of the form \"first sentence. second sentence\".   \nThe following are examples along with the reasoning. ", "page_idx": 32}, {"type": "text", "text": "Consider \u2019My day was moving and the movie was overblown. that is it.\u2019 The second sentence means the second adjective \u2019overblown\u2019 describes the movie. Due to this description, the sentiment is negative. ", "page_idx": 32}, {"type": "text", "text": "Consider \u2019My day was moving and the movie was overblown. oh wait, reverse the adjectives.\u2019   \nThe second sentence means the first adjective \u2019moving\u2019 describes the movie.   \nDue to this description, the sentiment is positive. ", "page_idx": 32}, {"type": "text", "text": "These are all examples. ", "page_idx": 32}, {"type": "text", "text": "user: What is the sentiment about the movie in this review \u2018<REVIEW>\u2018? ", "page_idx": 32}, {"type": "text", "text": "Think step-by-step about this latest review. If the second sentence instructs it, switch the adjectives and then based on the new descriptor of the movie, answer either \u2019positive\u2019 or \u2019negative\u2019. ", "page_idx": 32}, {"type": "text", "text": "Explain why you chose those this sentiment by selecting as few words as possible from the review. Include all the words that you looked at. ", "page_idx": 32}, {"type": "text", "text": "Use this helpful format: \"the sentiment is <positive/negative> and the explanation is <words, ...>. END. \" ", "page_idx": 32}, {"type": "text", "text": "D Algorithms ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Algorithm 1 describes an alternate way to estimate the ENCODE-METER component of STRIPE-X with a conditional generative model. Algorithm 2 describes the predictive version of STRIPE-X estimation, which we used in our experiments. ", "page_idx": 32}, {"type": "text", "text": "Algorithm 1: ENCODE-METER, generative version. ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Input: Training data $D\\sim q(\\mathbf{y},\\mathbf{x})$ and test data $D_{t}\\sim q(\\mathbf{y},\\mathbf{x})$ , explanation function $e(\\mathbf{x})$ , penalty weight $\\lambda$ . EVAL-X model $p_{\\gamma}(\\mathbf{y}\\mid\\mathbf{x_{v}})$ . Conditional generative model $\\bar{p}_{\\theta}(\\mathbf{x}\\mid\\mathbf{x_{v}},\\bar{\\mathbf{y}})$ and mutual information estimator that takes two sets as arguments ${\\sf M I}[\\{c_{i}\\},\\{d_{i}\\}]$ ;   \nResult: Return estimate of STRIPE- $\\mathrm{X}$ : ", "page_idx": 33}, {"type": "text", "text": "1 Define $\\mathbf{J}_{(\\mathbf{v},\\mathbf{a})}$ as the set of $K$ random samples of $\\mathbf{y}$ from the EVAL-X model: ", "page_idx": 33}, {"type": "equation", "text": "$$\n{\\bf J}_{({\\bf v},{\\bf a})}=\\{\\hat{\\bf y}^{k}\\}_{k\\leq K}\\quad\\{\\mathrm{~where~}\\hat{\\bf y}^{k}\\sim p_{\\gamma}({\\bf y}\\mid{\\bf x_{v}}={\\bf a})\\}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "2 Define $\\mathbf{L}_{(\\mathbf{v},\\mathbf{a},\\mathbf{J}_{(\\mathbf{v},\\mathbf{a})})}$ as the set of $K$ random samples of $\\mathbf{x}$ from $p_{\\theta}$ conditioned on a and $\\hat{\\mathbf{y}}$ : ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbf{L}_{(\\mathbf{v},\\mathbf{a},\\mathbf{J}_{(\\mathbf{v},\\mathbf{a})})}=\\{\\mathbb{1}[e(\\hat{\\mathbf{x}}^{k})=\\mathbf{v}]\\}_{k\\le K}\\quad\\{\\mathrm{~where~}\\hat{\\mathbf{x}}^{k}\\sim p_{\\theta}(\\mathbf{x}\\mid\\mathbf{x_{v}}=\\mathbf{a},\\mathbf{y}=\\hat{\\mathbf{y}}^{k})\\}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "3 Construct the explanation dataset $D_{e}=\\{(\\mathbf{x}_{e(\\mathbf{x})}=(e(\\mathbf{x}),\\mathbf{a}))\\}$ from $D_{t}$ .   \n4 Define $q_{D_{e}}$ to be the uniform distribution over $D_{e}$ .   \n5 Compute the following averaging of estimated mutual information between, $\\mathbf{J},\\mathbf{L}$ ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\phi}_{q}(e)=\\mathbb{E}_{(\\mathbf{v},\\mathbf{a})\\sim q_{D_{e}}(\\mathbf{x}_{e(\\mathbf{x})})}\\mathtt{M I}\\left[\\mathbf{J}_{(\\mathbf{v},\\mathbf{a})},\\mathbf{L}_{(\\mathbf{v},\\mathbf{a},\\mathbf{J}_{(\\mathbf{v},\\mathbf{a})})}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "6 Return $\\hat{\\phi}(q,e)$ as the ENCODE-METER estimate. ", "page_idx": 33}, {"type": "text", "text": "Algorithm 2: STRIPE-X, predictive version. ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Input: Training data $D\\sim q(\\mathbf{y},\\mathbf{x})$ and test data $D_{t}\\sim q(\\mathbf{y},\\mathbf{x})$ , explanation function $e(\\mathbf{x})$ , penalty weight $\\lambda$ . Specifications for the models $p_{\\gamma}(\\mathbf{y}\\mid\\mathbf{x_{v}})$ and $p_{\\theta}(\\mathbf{F_{\\theta}}|\\mathbf{\\Deltax_{v}},\\mathcal{E})$ . Result: Return estimate of STRIPE-X : 1 Define $q_{D}$ to be the uniform distribution over $D$ 2 Construct the explanation dataset $D_{e}=\\{(\\mathbf{y},\\mathbf{x}_{e(\\mathbf{x})})\\}$ from $D_{t}$ 3 Define $q_{D_{e}}$ to be the uniform distribution over $D_{e}$ . ", "page_idx": 34}, {"type": "text", "text": "4 Estimate eval-x() ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "5 Solve the following minimization problem to learn $p_{\\gamma}(\\mathbf{y}\\mid\\mathbf{x_{v}})$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\underset{\\gamma}{\\arg\\operatorname*{max}}\\,\\mathbb{E}_{\\mathbf{v}\\sim q_{D}(e(\\mathbf{x}))}\\mathbb{E}_{\\mathbf{x},\\mathbf{y}\\sim q_{D}(\\mathbf{x},\\mathbf{y})}\\left[\\log p_{\\gamma}(\\mathbf{y}\\mid\\mathbf{x_{v}})\\right]\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Output: $p_{\\gamma}$ ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "6 Estimate encode-meter() ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "7 Construct the set of possible selections $\\mathcal{V}=\\left\\{\\mathbf{v}:q(e(\\mathbf{x})=\\mathbf{v})>0\\right\\}$   \n8 Construct data of the form $\\left(\\mathbf{x},\\mathbf{F}\\right)$ where $\\mathbf{F}=j$ if ${\\bf E}_{\\mathcal{V}[j]}=1$ . ", "page_idx": 34}, {"type": "text", "text": "Fit the model $p_{\\theta}(\\mathbf{F_{\\theta}}|\\mathbf{\\Deltax_{v}},\\mathcal{E})$ via the following log-likelihood maximization: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\theta}{\\arg\\operatorname*{max}}\\,\\mathbb{E}_{\\mathbf{v}\\sim q_{D}(e(\\mathbf{x}))}\\mathbb{E}_{\\mathbf{x},\\mathbf{y}\\sim q_{D}(\\mathbf{x},\\mathbf{y})}\\,\\sum_{\\gamma[j]\\in\\mathcal{V}}\\left(\\mathbb{1}[e(\\mathbf{x})=\\mathcal{V}[j]]\\big[\\log p_{\\theta}(\\mathbf{F}=j\\mid\\mathbf{x_{v}},\\ell=\\mathbf{y},\\mathbf{v})\\big.\\right.}\\\\ {\\left.\\left.+\\log p_{\\theta}(\\mathbf{F}=j\\mid\\mathbf{x_{v}},\\ell=\\mathbf{n}\\mathbf{u}11,\\mathbf{v})\\big]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Define $\\mathcal{E}_{(\\mathbf{v},\\mathbf{a})}$ as the uniform distribution over $K$ samples of y from the EVAL-X model: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{E}_{(\\mathbf{v},\\mathbf{a})}=\\mathbf{U}\\left[\\{\\hat{\\mathbf{y}}\\}_{k\\le K}\\right]}&{{}\\{\\mathrm{~where~}\\hat{\\mathbf{y}}^{k}\\sim p_{\\gamma}(\\mathbf{y}\\mid\\mathbf{x_{v}}=\\mathbf{a})\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Output: The following nested expectation over $q_{D_{e}}$ and $\\mathcal E(\\cdot)$ : ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{(\\mathbf{v},\\mathbf{a})\\sim q_{D_{\\epsilon}}(\\mathbf{x}_{\\epsilon}(\\mathbf{x}))}\\mathbb{E}_{\\hat{\\mathbf{y}}\\sim\\mathcal{E}_{(\\mathbf{v},\\mathbf{a})}}\\left(p_{\\theta}(\\mathbf{F}=j\\mid\\mathbf{x}_{\\mathbf{v}},\\ell=\\hat{\\mathbf{y}},\\mathbf{v})\\log\\frac{p_{\\theta}\\left(\\mathbf{F}=j\\mid\\mathbf{x}_{\\mathbf{v}},\\ell=\\hat{\\mathbf{y}},\\mathbf{v}\\right)}{p_{\\theta}\\left(\\mathbf{F}=j\\mid\\mathbf{x}_{\\mathbf{v}},\\ell=\\mathrm{nu}11,\\mathbf{v}\\right)}\\right.}\\\\ {+\\left.p_{\\theta}(\\mathbf{F}\\neq j\\mid\\mathbf{x}_{\\mathbf{v}},\\ell=\\hat{\\mathbf{y}},\\mathbf{v})\\log\\frac{p_{\\theta}\\left(\\mathbf{F}\\neq j\\mid\\mathbf{x}_{\\mathbf{v}},\\ell=\\hat{\\mathbf{y}},\\mathbf{v}\\right)}{p_{\\theta}\\left(\\mathbf{F}\\neq j\\mid\\mathbf{x}_{\\mathbf{v}},\\ell=\\mathrm{nu}11,\\mathbf{v}\\right)}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "12 Learn the EVAL- $\\mathbf{\\nabla}X$ model $p_{\\gamma}\\longleftarrow\\mathtt{E V A L-X}\\left(\\right)$ . ", "page_idx": 34}, {"type": "text", "text": "13 Estimate ENCODE-METER as the $\\hat{\\phi}_{q}(e)\\longleftarrow\\mathtt{E N C O D E-M E T E R}()$ .   \n14 Return the following as the STRIPE-X estimate: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{E}_{(\\mathbf{v},\\mathbf{a})\\sim q_{D_{e}}(\\mathbf{x}_{e(\\mathbf{x})})}\\mathbb{E}_{\\mathbf{y}\\sim q_{D_{e}(\\mathbf{y}\\mid\\mathbf{x}_{e(\\mathbf{x})}=(\\mathbf{v},\\mathbf{a}))}}\\left[\\log p_{\\gamma}\\left(\\mathbf{y}=\\mathbf{y}\\mid\\mathbf{x_{v}}=\\mathbf{a}\\right)\\right]-\\lambda\\hat{\\phi}_{q}(e)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 35}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 35}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 35}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and   \na complete (and correct) proof?   \nAnswer: [Yes]   \nJustification:   \nGuidelines: \u2022 The answer NA means that the paper does not include theoretical results. \u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main   \nexperimental results of the paper to the extent that it affects the main claims and/or conclu  \nsions of the paper (regardless of whether the code and data are provided or not)?   \nAnswer: [Yes]   \nJustification:   \nGuidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [No]   \nJustification: We only use public data. We use standard existing training techniques, describe the hyperparameters in detail, and provided the Llama 3 prompts we used in our experiments. ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. ", "page_idx": 36}, {"type": "text", "text": "\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 37}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] Justification: Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 37}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] ", "page_idx": 37}, {"type": "text", "text": "Justification: The error bars in 5.1 and 5.2 are negligible because we estimate means of metrics over datasets of 5000 samples. For 5.3, error bars are irrelevant because there is no comparison against an existing method. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 37}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the com  \nputer resources (type of compute workers, memory, time of execution) needed to reproduce   \nthe experiments?   \nAnswer: [Yes]   \nJustification:   \nGuidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We do not use human subjects and only use public data and public models. Our work focuses on explainable machine learning and does not pose additional ethical harms beyond what is standard for the field. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 38}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 38}, {"type": "text", "text": "Justification: We investigate a problem with explanations and fix it. There is no societal impact of our work that exceed that of the usage of explanations.   \nGuidelines:   \n\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA]   \nJustification: We do not release any data or models. Guidelines:   \n\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] Justification: Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 39}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 40}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 40}]