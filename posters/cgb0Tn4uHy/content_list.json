[{"type": "text", "text": "A Transition Matrix-Based Extended Model for Label-Noise Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 The transition matrix methods have garnered sustained attention as a class of   \n2 techniques for label-noise learning due to their simplicity and statistical consis  \n3 tency. However, existing methods primarily focus on class-dependent noise and   \n4 lack applicability for instance-dependent noise, while some methods specifically   \n5 designed for instance-dependent noise tend to be relatively complex. To address   \n6 this issue, we propose an extended model based on transition matrix in this paper,   \n7 which preserves simplicity while extending its applicability to handle a broader   \n8 range of noisy data beyond class-dependent noise. The proposed algorithm\u2019s con  \n9 vergence and generalization properties are theoretically analyzed under certain   \n10 assumptions. Experimental evaluations conducted on various synthetic and real  \n11 world noisy datasets demonstrate significant improvements over existing transition   \n12 matrix-based methods. Upon acceptance of our paper, the code will be open   \n13 sourced. ", "page_idx": 0}, {"type": "text", "text": "14 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "15 Deep neural networks have achieved remarkable success in various fields in recent years, especially   \n16 in classification problems with labeled data [32, 2]. Compared to traditional methods, deep neural   \n17 networks have greatly improved performance but their effects heavily depend on the accuracy of the   \n18 provided labels. Bringing data with corrupted labels into the neural network model without special   \n19 treatment can severely affect the prediction performance [8, 50]. However, acquiring accurately   \n20 annotated data in reality can be very expensive, so a larger amount of data comes from the Internet or   \n21 annotations by non-professional annotators. Therefore, it is currently worth studying and promoting   \n22 how to alleviate the damage caused to the model when using noisy labels and make the model more   \n23 robust, which is known as the problem of label-noise learning or called learning with noisy labels   \n24 [29, 36, 10, 43, 41, 1, 35].   \n25 Various methods have been proposed for label-noise learning. Existing methods can be classified into   \n26 several categories. One of them is to design novel loss functions or network structures [53, 39, 28],   \n27 which reduce the impact of noisy labels to make the model more robust. Another category is sample   \n28 selection based on sample loss or feature extracted, dividing samples into the clean dataset and the   \n29 noisy dataset [4, 10, 13, 19]. Then they relabel the noisy labels [33, 15], or clear the noisy labels   \n30 and use semi-supervised methods for learning [3, 19]. These methods are common recently and   \n31 have achieved some good results. However, the process of sample selection is relatively subjective,   \n32 and statistical consistency is lost after the selection, and most of them lack theoretical support.   \n33 In contrast, transition matrix methods [9, 43, 22, 14, 59] have statistical consistency and usually   \n34 have corresponding theoretical analysis as support, attracting continued attention and occupying an   \n35 important position in various learning algorithms with label noise.   \n36 The core idea of transition matrix methods is to use a matrix measuring the transition probability from   \n37 the distribution of true label to the distribution of observed noisy label. If an accurate transition matrix   \n38 can be estimated and combined with observable data to obtain the noisy class-posterior probability, the   \n39 distribution of clean label can be inferred for network learning. Therefore, estimating the transition   \n40 matrix is the key to this type of method. However, it is infeasible to estimate an individual transition   \n41 matrix for each sample without additional conditions [26]. Previous methods mostly focus on class  \n42 dependent and instance-independent label noise problems [43, 22, 51], assuming that the transition   \n43 matrix is fixed for all samples. Among these methods, some [31, 43] assume the existence of anchor   \n44 points to estimate the transition matrix, while other methods obtain the optimal estimation by adding   \n45 a regularization term for matrix structure to weaken the anchor points assumption [22, 51]. However,   \n46 these methods are not suitable for instance-dependent label noise and complex real-world data because   \n47 they estimate only one matrix for all samples. Moreover, when the estimation of noisy class-posterior   \n48 distribution is inaccurate, the estimation of the transition matrix may be easily affected [47], thereby   \n49 affecting the estimation of the clean label distribution. Although some methods [42, 58, 52, 20] have   \n50 recently been designed to use special networks or structures for instance-dependent noise situations,   \n51 the estimation errors for them are still large, and the computational cost is too high to lose the concise   \n52 characteristic of transition matrix methods.   \n53 Addressing the limitations of current transition matrix-based methods, this paper introduces an   \n54 extended model for transition matrix that extends their applicability from class-dependent noise to   \n55 a broader range of label-noise data without requiring additional techniques such as clustering or   \n56 self-supervised learning. Inspired by methods that handle noise using sparse structures [57, 25], our   \n57 model combines a global transition matrix with a sparse implicit regularization term [31, 25] for   \n58 fitting the distribution of noisy labels across instances, replacing the need for estimating a separate   \n59 transition matrix for each sample. This approach allows us to incorporate instance-level information   \n60 into the model, expanding its capability beyond class-dependent noise scenarios while avoiding the   \n61 unidentifiability and computational complexity of estimating instance-dependent matrices.   \n62 The structure of the following sections is as follows. In Section 2, we give relevant definitions and   \n63 propose our method. In section 3 we conduct a theoretical analysis of the proposed method on a   \n64 simplified model. In Section 4, we conduct experiments on various synthetic and real-world noisy   \n65 datasets, comparing with other transition matrix-based methods. We conclude the paper in Section 5.   \n66 In addition, we provide a more specific review of related works in Appendix A, proofs of theorems in   \n67 Appendix B, and experimental details in Appendix C. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "68 The main contributions of this paper are: ", "page_idx": 1}, {"type": "text", "text": "69 \u2022 We propose a novel extended model for transition matrix, incorporating sparse implicit regu  \n70 larization, which enables the extension of transition matrix methods from class-dependent   \n71 noise to a broader range of noisy label data while maintaining simplicity, without the need   \n72 for excessive additional framework design or sophisticated techniques.   \n73 \u2022 Under certain assumptions, we provide theoretical analysis on the convergence and gener  \n74 alization results of the algorithm on a simplified model. We prove the theorems proposed   \n75 accordingly, giving support for the effectiveness of the proposed method.   \n76 \u2022 Our proposed method achieves significant improvements compared to previous transition ma  \n77 trix methods on both synthetic and real-world noisy label datasets, and produces competitive   \n78 results without the need for additional auxiliary techniques. ", "page_idx": 1}, {"type": "text", "text": "79 2 Methodology ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "80 In this section, we give relevant definitions and propose a novel model that extends the transition   \n81 matrix with implicit regularization (TMR) from class-dependent noise to more label-noise. It is a   \n82 convenient and end-to-end model. We will formulate the method in detail and illustrate it theoretically. ", "page_idx": 1}, {"type": "text", "text": "83 2.1 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "84 Let $\\mathcal{X}\\subset\\mathbb{R}^{d}$ be the feature space, $\\mathcal{Y}=\\{1,2,\\cdot\\cdot\\cdot,C\\}$ be the label space, where $C$ is the number   \n85 of classes. Random variables $(X,Y),(X,{\\tilde{Y}})\\in\\,\\mathcal{X}\\times\\mathcal{Y}$ denote the underlying data distributions   \n86 with true and noisy labels respectively. In general, we can not observe the latent true data samples   \n87 $\\mathbb{D}_{(N)}=\\{(\\pmb{x}_{i},y_{i})\\}_{i=1}^{N}$ , but can only obtain the corrupted data $\\tilde{\\mathbb{D}}_{(N)}=\\{(\\pmb{x}_{i},\\tilde{y_{i}})\\}_{i=1}^{N}$ , where $\\tilde{y}\\in\\mathcal{Y}$ is   \n88 the noisy label corrupted from the true label $y$ , while denote corresponding one-hot label as $\\textit{\\textbf{y}}$ and $\\tilde{\\pmb{y}}$ .   \n89 Transition matrix methods use a matrix $\\pmb{T}(\\pmb{x})\\in[0,1]^{C\\times C}$ to represent the probability from clean   \n90 label to noisy label, where the $i j$ -th entry of the transition matrix is the probability that the instance $\\textbf{\\em x}$   \n91 with the clean label $i$ corrupted to a noisy label $j$ . The matrix satisfies the requirement that the sum   \n92 of each row $\\textstyle\\sum_{j=1}^{C}T_{i j}(\\pmb{x})$ is 1, and usually has the requirement for $\\pmb{T}_{i i}(\\pmb{x})>\\pmb{T}_{i j}(\\pmb{x}),\\forall j\\neq i$ . The   \n93 set of possible values for $\\textbf{\\emph{T}}$ is denoted as $\\begin{array}{r}{\\mathbb{T}=\\left\\{T\\in[0,1]^{C\\times C}|\\sum_{j=1}^{C}\\pmb{T}_{i j}=1,\\pmb{T}_{i i}>\\pmb{T}_{i j},\\forall j\\neq i\\right\\}}\\end{array}$ .   \n94 Let $P({\\pmb Y}|{\\pmb X}={\\pmb x})=[P({\\pmb Y}=1|{\\pmb X}={\\pmb x}),\\cdot\\cdot\\cdot{\\bf\\nabla},P({\\pmb Y}=C|{\\pmb X}={\\pmb x})]^{\\intercal}$ be the clean class-posterior   \n95 probability and $P(\\tilde{Y}|X=x)\\,=\\,[P(\\tilde{Y}\\,=\\,1|X\\,=\\,x),\\cdots\\,,P(\\tilde{Y}\\,=\\,C|X\\,=\\,x)]^{\\intercal}$ be the noisy   \n96 class-posterior probability, the formula can be write as: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\nP(\\tilde{Y}|X=x)={\\mathbf{T}}({\\mathbf{x}})^{\\top}P(Y|X=x).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "97 Though estimating the transition matrix and the noisy class-posterior probability, the clean class  \n98 posterior probability can be inferred by $P({\\cal Y}|{\\cal X}={\\pmb x})={\\pmb T}({\\pmb x})^{-\\top}P(\\tilde{\\cal Y}|{\\cal X}={\\pmb x})$ , where the symbol   \n99 $-\\top$ denotes the transpose of the inverse matrix. Alternatively, the neural network can be utilized to   \n100 fit the clean label distribution by the loss function: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\frac{1}{N}\\sum_{i=1}^{N}\\ell\\left(\\pmb{T}(\\pmb{x}_{i})^{\\top}f_{\\pmb{\\theta}}(\\pmb{x}_{i}),\\tilde{\\pmb{y}}_{i}\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "101 where $f_{\\pmb\\theta}(\\cdot)\\,:\\,\\mathscr{X}\\,\\rightarrow\\,\\Delta^{C-1}$ $\\mathbf{\\Sigma}^{\\Delta C-1}\\mathbf{\\Sigma}\\subset\\mathbf{\\Sigma}[0,1]^{C}$ is the $C$ -dimensional simplex) is a differentiable   \n102 function represented by a neural network with parameters $\\pmb{\\theta}$ and $\\ell$ is a loss function usually using   \n103 cross-entropy (CE) loss. Therefore, the key to addressing the problem in this class of methods lies in   \n104 how to estimate the transition matrix.   \n105 Since it is difficult to estimate the transition matrix ${\\pmb T}({\\pmb x})$ individually for each sample, the majority   \n106 of existing methods [31, 10, 22] focus on studying the class-dependent and instance-independent   \n107 transition matrix, i.e., $\\pmb{T}(\\pmb{x})=\\pmb{T}$ for $\\forall\\mathbf{{x}}$ . However, these methods are limited by the assumption   \n108 of class-dependence and cannot be directly applied to instance-dependent label noise with good   \n109 effectiveness. Our objective is to make improvement and extension based on this limitation. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "110 2.2 Transition Matrix with Implicit Regularization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "111 The main issue with directly applying class-dependent transition matrix methods to instance  \n112 dependent noise lies in using a fixed matrix $_T$ , multiplying with clean class-posterior probability   \n113 $P(\\mathbf{Y}|X)$ , i.e., $\\pmb{T}^{\\top}P(\\pmb{Y}|\\pmb{X})$ is not always equal to the noisy class-posterior probability $P({\\tilde{\\cal Y}}|{\\cal X})$ ,   \n114 even if the probability values $P(\\mathbf{Y}|X)$ and $P({\\tilde{\\cal Y}}|{\\cal X})$ are correctly estimated. Therefore, for a broader   \n115 range of label-noise scenarios, relying solely on a fixed matrix $_T$ is insufficient.   \n116 The core idea of our proposed model is to introduce a residual term ${\\pmb r}(X)$ to fit the distribution   \n117 difference between $P({\\tilde{\\cal Y}}|{\\cal X})$ and $\\pmb{T}^{\\top}P(\\pmb{Y}|\\pmb{X})$ , where ${\\pmb r}(X)$ is a $C$ -dimensional vector for each $X$ .   \n118 It can be transformed into using $T^{\\top}P(Y|X)+r(X)$ to fit $\\bar{P}(\\tilde{\\mathbf{Y}}|X)$ .   \n119 Intuitively, if an overall relatively suitable transition matrix $_T$ is applied to $\\pmb{T}^{\\top}P(\\pmb{Y}|\\pmb{X})$ , then the   \n120 difference between it and the probability $P({\\tilde{\\cal Y}}|{\\cal X})$ should be small. Inspired by methods that handle   \n121 noise using sparse structures [57, 25], we utilize a sparse structure to model the residual term $\\pmb{r}$ .   \n122 Follow the works [30, 31, 25], using implicit regularization to represent sparse structures is a method   \n123 that facilitates updates and provides more stable learning performance. We exploit this technique   \n124 to model the residual term as $\\pmb{r}_{i}=\\pmb{u}_{i}\\odot\\pmb{u}_{i}-\\pmb{v}_{i}\\odot\\pmb{v}_{i}$ with respect to training sample $\\pmb{x}_{i}$ , where   \n125 ${\\mathbf{\\mathit{u}}}_{i},{\\mathbf{\\mathit{v}}}_{i}$ are all $C$ -dimensional vectors and $\\odot$ denotes an entry-wise Hadamard product. As usual, we   \n126 use a deep neural network $f_{\\theta}(\\cdot)$ to learn the true label probability $\\pmb{y}_{i}$ w.r.t $\\pmb{x}_{i}$ . So for the noisy label   \n127 probability distribution $\\tilde{\\pmb{y}}_{i}$ given by the data, the model use $T^{\\top}f_{\\theta}\\left(x_{i}\\right)+{\\pmb u}_{i}\\odot{\\pmb u}_{i}-{\\pmb v}_{i}\\odot{\\pmb v}_{i}$ to fti it.   \n128 Bring it into the loss function as: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\sum_{i=1}^{N}\\ell\\left(\\pmb{T}^{\\top}f_{\\pmb{\\theta}}(\\pmb{x}_{i})+\\pmb{u}_{i}\\odot\\pmb{u}_{i}-\\pmb{v}_{i}\\odot\\pmb{v}_{i},\\tilde{\\pmb{y}}_{i}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "129 Due to the potential existence of different $\\textbf{\\emph{T}}$ and $P(\\mathbf{Y}|X\\;=\\;x)$ such that $P({\\tilde{\\cal Y}}|{\\cal X}\\;=\\;{\\pmb x})\\;=\\;{\\pmb\\tilde{\\alpha}}{\\tilde{\\;}}$   \n130 ${\\pmb T}_{1}^{\\top}P_{1}({\\pmb Y}|\\bar{\\cal X^{\\mathrm{~}}}=\\,{\\pmb x})\\,=\\,{\\pmb T}_{2}^{\\top}P_{2}({\\pmb Y}|{\\cal X}\\,=\\,{\\pmb x})$ , we add a regularization term of the volume of the   \n131 matrix $\\operatorname{Vol}(T)=\\log\\operatorname*{det}(T)$ to loss function as [22] to ensure the transition matrix is identifiable.   \n132 The total loss function applied in our proposed method is: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\pmb{\\theta},\\pmb{T},\\{\\boldsymbol{u}_{i},\\boldsymbol{v}_{i}\\}_{i=1}^{N})=\\frac{1}{N}\\sum_{i=1}^{N}\\ell\\left(\\pmb{T}^{\\top}f_{\\pmb{\\theta}}(\\pmb{x}_{i})+\\pmb{u}_{i}\\odot\\pmb{u}_{i}-\\pmb{v}_{i}\\odot\\pmb{v}_{i},\\tilde{\\pmb{y}}_{i}\\right)+\\lambda\\cdot\\log\\operatorname*{det}(\\pmb{T}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "133 where we estimate parameters according to: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\theta},\\hat{T},\\{\\hat{u}_{i},\\hat{v}_{i}\\}_{i=1}^{N}=\\underset{\\theta,T,\\{u_{i},v_{i}\\}_{i=1}^{N}}{\\arg\\operatorname*{min}}\\mathcal{L}(\\theta,T,\\{u_{i},v_{i}\\}_{i=1}^{N}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "134 We use the gradient descent method to update the parameters to be learned above. This method   \n135 constitutes our proposed extended Transition Matrix model with sparse implicit Regularization   \n136 (TMR).The method steps are summarized in Algorithm 1 in Appendix B.1.   \n137 Through our model, the estimation of individual transition matrices for each sample is replaced   \n138 by the estimation of the global matrix and the sparse residual term. In this way, the number of   \n139 parameters for the transition matrix is reduced from $O(N C^{2})$ to $O(N C)$ , which greatly reduces the   \n140 difficulty of matrix estimation and computational consumption when $C$ is large. In addition, the   \n141 incorporation of sparse implicit regularization in combination with the transition matrix makes the   \n142 learning optimization process concise and efficient. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "143 2.3 Integration with Contrastive Learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "144 To further improve the effectiveness of our approach, we first utilize contrastive learning as a pre  \n145 trained feature extractor, followed by label learning. In this work, we also examine the enhancement   \n146 of the TMR method by incorporating the SimCLR method from contrastive learning as a feature   \n147 learner as pre-trained encoder, then resulting in $\\mathrm{TMR+}$ . ", "page_idx": 3}, {"type": "text", "text": "148 3 Theoretical Analysis ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "149 In this section, we want to analyze the effectiveness of the proposed method theoretically under   \n150 specific conditions related to label-noise generation. However, it is difficult to give a direct analysis   \n151 of the deep neural network model. So we follow the theoretical analysis method of [25] to simplify   \n152 the proposed model and study on an approximately linear structure to demonstrate the effectiveness   \n153 of our proposed model. ", "page_idx": 3}, {"type": "text", "text": "154 3.1 Model Simplification and Convergence Analysis ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "155 The first to solve is the construction of an approximate simplified model for theoretical analysis of   \n156 our algorithm. Based on [12], we use first-order Taylor expansion to approximate the deep neural   \n157 network $f_{\\theta}(\\cdot)$ , which is highly over-parameterized: ", "page_idx": 3}, {"type": "equation", "text": "$$\nf_{\\theta}(x)\\approx f_{\\theta_{0}}(x)+\\left(\\frac{\\partial f_{\\theta}^{\\top}(x)}{\\partial\\theta}\\Big|_{\\theta=\\theta_{0}}\\right)^{\\top}\\cdot(\\theta-\\theta_{0}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "158 where $f_{\\pmb\\theta}(\\pmb x)$ is a C-dimensional vector, $\\pmb\\theta\\in\\mathbb{R}^{p}$ $(p\\gg N)$ denotes the parameters of the neural   \n159 network, $\\left.\\frac{\\partial f_{\\theta}^{\\top}(\\pmb x)}{\\partial\\pmb\\theta}\\right|_{\\pmb\\theta=\\pmb\\theta_{0}}$ is a $p\\times C$ matrix, $\\theta_{0}$ is the initialization of $\\pmb{\\theta}$ , symbol $\\cdot$ represents matrix   \n160 multiplication. For simplicity, we drop the constant term in the derivation and abbreviate $\\begin{array}{r}{\\frac{\\partial f_{\\theta}^{\\top}(\\pmb x)}{\\partial\\pmb\\theta}\\bigg|_{\\pmb\\theta=\\pmb\\theta_{\\mathrm{0}}}}\\end{array}$   \n161 as $\\nabla_{{\\pmb\\theta}_{0}}f({\\pmb x})$ . The approximate formula becomes: ", "page_idx": 3}, {"type": "equation", "text": "$$\nf_{\\pmb\\theta}(\\pmb x)\\approx\\nabla_{\\pmb\\theta_{0}}f(\\pmb x)^{\\top}\\cdot\\pmb\\theta.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "162 Through this processing, we simplify the deep neural network into an approximately linear structure,   \n163 and we use $f_{\\theta}({\\pmb x})=\\nabla_{{\\pmb\\theta}_{0}}f({\\pmb x})\\cdot{\\pmb\\theta}$ in the following theoretical analysis. We use a $N\\times C$ matrix $\\pmb{F}$ to   \n164 represent the neural network predictions on the overall training dataset $\\{(\\pmb{x}_{i},y_{i})\\}_{i=1}^{N}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{F}=\\left[\\begin{array}{c}{f_{\\pmb{\\theta}}^{\\top}(\\pmb{x}_{1})}\\\\ {\\vdots}\\\\ {f_{\\pmb{\\theta}}^{\\top}(\\pmb{x}_{N})}\\end{array}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "165 In order to be written in matrix form, we rewrite the formula (7) in vector expansion form: ", "page_idx": 4}, {"type": "equation", "text": "$$\nf_{\\theta}^{\\top}({\\pmb x})=[f_{\\theta}({\\pmb x})_{1},\\cdot\\cdot\\cdot\\cdot,f_{\\theta}({\\pmb x})_{C}]=\\mathrm{vec}(\\nabla_{\\theta_{0}}f({\\pmb x}))^{\\top}\\cdot\\Theta,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "166 where $\\operatorname{vec}(A)$ denotes matrix expansion of a $m\\times n$ matrix $\\pmb{A}$ by column vectors: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{vec}(A)=\\left[A_{1,1},\\cdot\\cdot\\cdot,A_{m,1},\\cdot\\cdot\\cdot,A_{1,n},\\cdot\\cdot\\cdot,A_{m,n}\\right]^{\\top},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "167 and $\\Theta$ is a $C P\\times C$ matrix, denoting the Kronecker product of $C\\times C$ identity matrix $I_{C}$ with $\\pmb{\\theta}$ , i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Theta=I_{C}\\otimes\\pmb\\theta=\\left[\\begin{array}{c c c c}{\\pmb\\theta}&{0}&{\\cdots}&{0}\\\\ {0}&{\\pmb\\theta}&{\\cdots}&{0}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {0}&{0}&{\\cdots}&{\\pmb\\theta}\\end{array}\\right]_{C P\\times C}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "168 We use a Jacobian matrix $G\\in\\mathbb{R}^{N\\times C P}$ to denote the partial derivatives of the network for each   \n169 sample: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{G=\\left[\\begin{array}{c}{\\mathrm{vec}\\big(\\nabla_{\\pmb{\\theta}_{0}}f(\\pmb{x}_{1})\\big)^{\\top}}\\\\ {\\vdots}\\\\ {\\mathrm{vec}\\big(\\nabla_{\\pmb{\\theta}_{0}}f(\\pmb{x}_{N})\\big)^{\\top}}\\end{array}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "170 Then, an aggregate form of formula (7) is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{F}=\\pmb{G}\\cdot\\Theta.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "171 Now we give a simplified model assumption that there exists an underlying ground truth parameter   \n172 $\\pmb{\\theta}_{*}$ such that corresponding $\\pmb{F}_{*}$ generated by equation (13) fits the true label distribution for sample.   \n173 Meanwhile, there exist potentially true transition matrix $\\mathbf{\\nabla}T_{*}$ and sparse residual matrix $R_{*}\\ =$   \n174 $\\left[r(x_{1}),\\cdot\\cdot\\cdot\\,,r({\\pmb x}_{N})\\right]^{\\intercal}$ made up of the residual terms ${\\pmb r}({\\pmb x})$ for sample defined in Section 2.2. We   \n175 assume that the $N\\times C$ observed noisy label matrix $\\tilde{\\pmb Y}=\\left[\\tilde{{\\pmb y}}_{1},\\cdot\\cdot\\cdot\\,,\\tilde{{\\pmb y}}_{N}\\right]^{\\top}$ is generated by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{Y}}=\\mathbf{F}_{*}\\cdot\\mathbf{T}_{*}+\\mathbf{R}_{*}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "176 Expanded form after bringing in $\\pmb{G}$ and $\\pmb{\\theta}_{*}$ is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{\\pmb{Y}}=\\pmb{G}\\cdot(\\pmb{I}_{C}\\otimes\\pmb{\\theta}_{*})\\cdot\\pmb{T}_{*}+\\pmb{R}_{*}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "177 The problem to be studied is transformed into given $\\pmb{G}$ and observed $\\Tilde{Y}$ generated by formula (15),   \n178 how to estimate the underlying $\\theta_{*},T_{*}$ and $R_{*}$ . At this time, our proposed loss function (4) to be   \n179 optimized transforms into: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\pmb{\\theta},\\pmb{T},\\pmb{U},\\pmb{V})=L\\left(\\pmb{G}\\cdot(\\pmb{I}_{C}\\otimes\\pmb{\\theta})\\cdot\\pmb{T}+\\pmb{U}\\odot\\pmb{U}-\\pmb{V}\\odot\\pmb{V},\\pmb{\\tilde{Y}}\\right)+\\lambda\\cdot\\log\\operatorname*{det}(\\pmb{T}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "180 where $L$ is matrix form from $\\ell$ in formula (4), $U=[\\pmb{u}_{1},\\cdots,\\pmb{u}_{N}]^{\\top}$ , $V\\,=\\,[{\\pmb v}_{1},\\cdot\\cdot\\cdot\\,,{\\pmb v}_{N}]^{\\intercal}$ , $\\scriptstyle R\\;=$   \n181 $U\\odot U-V\\odot V$ .   \n182 Intuitively, the parameters $\\theta,T,R$ are unidentifiable without other conditions due to the model (15)   \n183 is over-parameterized. We need to add some conditional assumptions to ensure the convergence   \n184 of parameters. The required conditions are summarized in the Appendix B.2, such as the low rank   \n185 condition of $\\boldsymbol{G}$ , sparsity of $R_{*}$ , special small initialization setting, sufficiently scattered assumption   \n186 [22] of clean class-posterior probability distribution, etc. Under these conditions, we try to analyze   \n187 the effectiveness of our algorithm. For the simplicity of proof, we use square loss in formula (16),   \n188 which can be analogized to cross-entropy loss. The parameter optimization problem (5) becomes: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\theta},\\hat{T},\\hat{U},\\hat{V}=\\underset{\\theta,T,U,V}{\\arg\\operatorname*{min}}\\,\\frac{1}{2}\\|G\\cdot(I_{C}\\otimes\\theta)\\cdot T+U\\odot U-V\\odot V-\\tilde{Y}\\|_{2}^{2}+\\lambda\\cdot\\log\\operatorname*{det}(T).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "189 Based on this, the convergence result of parameters estimation is as follows: ", "page_idx": 4}, {"type": "text", "text": "190 Theorem 3.1. (Convergence) Under the conditions in B.2, the estimated parameters $\\hat{\\pmb{\\theta}},\\hat{\\pmb{T}},\\hat{\\pmb{R}}$ for   \n191 optimization problem $(I7)$ based on Algorithm 1 converge to the ground truth solution $\\pmb{\\theta}_{*}$ , $\\mathbf{\\deltaT}_{*}$ , $R_{*}$ .   \n192 The proof can be seen in Appendix B.3. Theorem 3.1 shows that under a simplified linear model and   \n193 some conditions, one can use our proposed algorithm to obtain the consistent estimation of network   \n194 parameters $\\pmb{\\theta}_{*}$ applicable to learning with clean label data. At the same time, we can estimate the   \n195 overall transition probability $\\mathbf{\\deltaT}_{*}$ from the correct label to the noisy label that we observed. Theorem   \n196 3.1 provides theoretical support for the effectiveness of our proposed method.   \n198 In addition to convergence, the generalization of the proposed result is also worth exploring. It is   \n199 finite to the amount of noisy label training data $\\tilde{\\mathbb{D}}_{(N)}\\,=\\,\\{(\\pmb{x}_{i},\\tilde{y_{i}})\\}_{i=1}^{N}$ we can observe, which is   \n200 considered to be randomly sampled from the overall infinite noisy data D\u02dc. We want to explore how   \n201 well the parameters $\\hat{\\pmb\\theta}_{(N)},\\hat{\\pmb T}_{(N)}$ estimated by the proposed algorithm with finite data $\\tilde{\\mathbb{D}}_{(N)}$ fit when   \n202 applied to the overall data $\\tilde{\\mathbb D}$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "203 We define a function class about the data as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{F}:=\\left\\{\\ell(T^{\\top}f_{\\pmb{\\theta}}(\\cdot)+\\pmb{\\gamma}(\\cdot),\\cdot):\\mathcal{X}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}^{+},\\forall\\pmb{\\theta}\\in\\mathbb{R}^{p},T\\in\\mathbb{T}\\right\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "204 where $\\gamma(\\cdot)$ is the true residual term for each sample. Each element in $\\mathcal{F}$ is a function about   \n205 data sample. It is worth mentioning that the term of $\\log\\operatorname*{det}(\\pmb{T})$ can be incorporated into the   \n206 loss function $\\ell$ , without explicitly writing it separately for simplicity. Denote the $\\epsilon$ -cover of $\\mathcal{F}$   \n207 as $\\mathcal{N}_{\\mathcal{F}}=\\mathcal{N}\\left(\\epsilon,\\mathcal{F},\\|\\cdot\\|_{\\infty}\\right)$ , the average losses on $\\tilde{\\mathbb{D}}_{(N)}$ and $\\tilde{\\mathbb D}$ are $\\mathcal{L}(\\pmb{\\theta}_{(N)},\\pmb{T}_{(N)},\\pmb{R}_{(N)};\\tilde{\\mathbb{D}}_{(N)})$ and   \n208 $\\mathcal{L}(\\pmb{\\theta},\\pmb{T},\\pmb{R};\\tilde{\\mathbb{D}})$ respectively. According to Theorem 3.1, for any fixed $\\epsilon>0$ , there exists estimated   \n209 parameters $\\hat{\\pmb\\theta}_{(N)},\\hat{\\pmb T}_{(N)},\\hat{\\pmb R}_{(N)}$ obtained by our algorithm such that: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\hat{{\\boldsymbol\\theta}}_{(N)},\\hat{{\\boldsymbol T}}_{(N)},\\hat{{\\boldsymbol R}}_{(N)};\\tilde{{\\mathbb D}}_{(N)})\\le\\mathcal{L}({\\boldsymbol\\theta}_{(N)},{\\boldsymbol T}_{(N)},{\\boldsymbol R}_{(N)}^{*};\\tilde{{\\mathbb D}}_{(N)})+\\epsilon,\\forall{\\boldsymbol\\theta}_{(N)}\\in{\\mathbb R}^{p},{\\boldsymbol T}_{(N)}\\in\\mathbb{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "210 where $R_{(N)}^{*}$ is the true residual terms for $\\tilde{\\mathbb{D}}_{(N)}$ . If we know the ground truth $\\scriptstyle R_{*}$ , we have the   \n211 following result:   \n212 Theorem 3.2. Suppose the loss function is bounded by $0\\leq\\ell(\\cdot,\\cdot)\\leq M$ . For any $\\delta>0$ , then with   \n213 probability at least $1-\\delta$ we have ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\hat{\\theta}_{(N)},\\hat{T}_{(N)},R_{*};\\tilde{\\mathbb{D}})\\leq\\operatorname*{inf}_{\\theta\\in\\mathbb{R}^{p},T\\in\\mathbb{T}}\\mathcal{L}(\\theta,T,R^{*};\\tilde{\\mathbb{D}})+M\\sqrt{\\frac{\\ln(2\\mathcal{N}_{\\mathcal{F}}/\\delta)}{2n}}+M\\sqrt{\\frac{\\ln(2/\\delta)}{2n}}+3\\epsilon.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "214 The proof can be found in Appendix B.4, using Theorem 2 in [48] as a reference. For any fixed $\\epsilon>0$ ,   \n215 as $n$ continues to increase, the terms $\\sqrt{\\frac{\\ln(2\\!\\sqrt{_{\\mathcal{F}}/\\delta})}{2n}}$ and 2n $\\sqrt{\\frac{\\ln(2/\\delta)}{2n}}$ on the right side of the inequality   \n216 (20) tend to 0. Since the $\\epsilon$ can be arbitrarily small, the right side of the inequality (20) can be bounded.   \n217 Looking back at the optimization target (17), we can find that the Theorem 3.2 states the estimators   \n218 $\\hat{\\pmb{\\theta}}_{(N)},\\hat{\\pmb{T}_{(N)}}$ based on finite data $\\tilde{\\mathbb{D}}_{(N)}$ can also be applied relatively effectively to wider data $\\tilde{\\mathbb D}$ as   \n219 long as they are randomly generated from the same pattern. It shows the generalization result of our   \n220 algorithm, indicating that the estimation $\\hat{\\pmb{\\theta}}_{(N)},\\hat{\\pmb{T}}_{(N)}$ can be applied to new data and only the residual   \n221 terms $\\boldsymbol{R}$ need to be estimated separately. ", "page_idx": 5}, {"type": "text", "text": "222 4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "223 In this section, we present experimental findings to showcase the effectiveness of our proposed   \n224 method compared to other methods. We evaluate our approach on both synthetic instance-dependent   \n225 noisy datasets and real-world noisy datasets. More experimental details can be found in the Appendix   \n226 C. ", "page_idx": 5}, {"type": "text", "text": "227 4.1 Datasets ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "228 We conduct experiments on following image classification datasets: CIFAR-10 and CIFAR-100 [16],   \n229 CIFAR-10N and CIFAR-100N [40], Clothing1M [44], Webvision and ILSVRC12 [21]. Among   \n230 them, CIFAR-10 and CIFAR-100 both have $32\\times32\\times3$ color images including 50,000 training   \n231 images and 10,000 test images. CIFAR-10 has 10 classes while CIFAR-100 has 100 classes. We   \n232 generate instance-dependent noisy data on CIFAR-10 and CIFAR-100 with noise rates ranging from   \n233 $10\\%$ to $50\\%$ , following the same generation method as in [42]. CIFAR-10N and CIFAR-100N are   \n234 manually annotated by human annotators, existing noisy labels within them. Clothing1M is a real  \n235 world dataset consisting of 1 million training images, consisting of 14 categories. WebVision contains   \n236 2.4 million images crawled from the websites using the 1,000 concepts in ImageNet ILSVRC12, but   \n237 only the first 50 classes of the Google image subset are used in our experiments. For the validation   \n238 set selection in our TMR method, we randomly sampled 10 samples from each observed class for   \n239 each dataset to form the validation set, while the remaining samples were used for the training set. ", "page_idx": 5}, {"type": "text", "text": "240 4.2 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "241 We conduct the experiments using NVIDIA 3090Ti graphics cards. During the training process, we   \n242 update the transition matrix using the Adam optimization method, the initialization is consistent   \n243 with [22]. While the updates for other parameters are performed using the stochastic gradient   \n244 descent (SGD) optimization method. More specifically, for CIFAR-10/10N, we use ResNet-18 as   \n245 the backbone network with 300 epochs, batch size 128, learning rate for network is 0.05, 0.0005 for   \n246 transition matrix and divided by 10 after the 30th and 60th epoch. For CIFAR-100/100N, we use   \n247 ResNet-34 network with the same 300 epochs, batch size 128, while learning rate for network is 0.05,   \n248 0.0002 for transition matrix and divided by 10 after the 30th and 60th epoch. For clothing1M, we   \n249 use a ResNet-50 pre-trained with 10 epochs, batch size 64, learning rate 0.002 for network, 0.0001   \n250 for transition matrix and divided by 10 after the 5th epoch. We use InceptionResNetV2 network   \n251 on Webvision, with 100 epochs, batch size 32, learning rate 0.02 for network, 0.0005 for transition   \n252 matrix and divided by 10 after the 30th and 60th epoch. For ILSVRC12, we directly use the model   \n253 trained on Webvision, following the common setting in other papers in this field. ", "page_idx": 6}, {"type": "text", "text": "254 4.3 Comparison Methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "255 In our experiments, we included the following commonly used baseline methods for instance  \n256 dependent transition matrix estimation and comparison: (1) GCE [53], (2) Forward [31], (3) DMI   \n257 [45], (4) VolMinNet [22], (5) PeerLoss [27] (6) BLTM [46], (7) PartT [42], (8) MEIDTM [6], (9)   \n258 SOP [25] as an implicit regularization method for comparison, as well as state-of-the-art methods   \n259 for comparison purposes: (10) Co-teaching [10], (11) $\\mathrm{ELR+}$ [24], (12) DivideMix [19], (13) $\\mathrm{SOP+}$   \n260 [25], (14) CC [54], (15) PGDF [5], (16) DISC [23]. ", "page_idx": 6}, {"type": "table", "img_path": "cgb0Tn4uHy/tmp/728a7843eb07d0f06db3cb08cb968c4862406c58fc27f8490bc8c1a9c7f56fb5.jpg", "table_caption": ["Table 1: Test accuracy with instance-dependent noise on CIFAR-10/100. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "261 4.4 Experimental Results on Synthetic Datasets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "262 We primarily validated our TMR method against previous instance-based transition matrix methods   \n263 on synthetic CIFAR-10/100 noise datasets. These methods mainly focus on estimating the transition   \n264 matrix and do not leverage advanced self-supervised or semi-supervised techniques. We performed 5   \n265 independent runs for each experimental configuration, and the average values and standard deviations   \n266 of each experiment are presented in Table 1.   \n267 The results demonstrate that our proposed TMR method outperforms other methods of the same   \n268 category across various noise rates. It is evident that traditional transition matrix methods such as   \n269 Forward and VolMinNet exhibit subpar performance when handling instance-dependent noise. On   \n270 the other hand, specialized transition matrix methods designed for instance-dependent noise, such as   \n271 ParT and MEIDTM, still show significant gaps compared to our method.   \n272 Furthermore, as the noise rates increase, the test accuracy of existing transition matrix methods   \n273 significantly decline. This is particularly pronounced in the case of CIFAR-100 with $50\\%$ instance  \n274 dependent noise (IDN) data, where all transition matrix methods achieve test accuracy below $60\\%$ .   \n275 In contrast, our proposed TMR method achieves a remarkable test accuracy of $69.85\\%$ , showcasing   \n276 its exceptional performance. That demonstrates relatively robust performance of TMR with only a   \n277 slight decrease as the noise rate increases.   \n278 It is worth mentioning that SOP [25], as a method that also applies implicit regularization based   \n279 on sparsity assumptions, achieves comparable performance to our method when the noise rates are   \n280 low. However, it still falls short of our method\u2019s performance. As the noise rate increases, SOP is   \n281 more adversely affected by the noise due to its reliance on the sparsity assumption. In contrast, our   \n282 proposed TMR method effectively estimates the overall trend by utilizing the transition matrix and   \n283 combines it with sparsity, thereby demonstrating robustness even in the presence of higher noise   \n284 rates. For instance, on CIFAR-10/100 with a $10\\%$ noise rate, TMR outperforms SOP by 0.87 and   \n285 2.87 percentage points, respectively. When the noise rate increases to $50\\%$ , TMR surpasses SOP by   \n286 4.52 and 5.61 percentage points, respectively. This clearly demonstrates the general effectiveness of   \n287 our method in handling label noise learning across various noise rates. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "cgb0Tn4uHy/tmp/fac93ec16ebee9810b68a425cf8da491c4677447d6826132e0863cf1009c7616.jpg", "table_caption": ["Table 2: Test accuracy on CIFAR-10N and CIFAR-100N. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "288 4.5 Experimental Results on Real-world Datasets ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "289 In addition to comparing with transition matrix methods, we also enhanced our method, TMR, by   \n290 incorporating SimCLR for feature learning, as $\\scriptstyle\\mathrm{TMR+}$ . We compared $\\scriptstyle\\mathrm{TMR+}$ with other state-of-the  \n291 art methods on multiple real-world noisy datasets, and the results are presented in Table 2 and Table   \n292 3.   \n293 The results demonstrate that regardless of the type of noise labels, whether it is aggregated, random,   \n294 or the worst-case scenario in CIFAR-10N, as well as in CIFAR-100N with more label categories, our   \n295 method consistently achieves the best results in handling real-world noise. When dealing with large   \n296 datasets like Clothing1M and complex image datasets like Webvision, $\\mathrm{TMR+}$ also achieves excellent   \n297 results compared to to other SOTA methods like CC, PGDF and DISC.   \n298 Through extensive experiments on five real-world datasets, we demonstrate that our TMR method   \n299 can significantly benefit from combining with self-supervised methods such as contrastive learning,   \n300 indicating that high-quality features can greatly enhance our original TMR method. TMR is a   \n301 plug-and-play model, where the feature extraction part can be unrelated to TMR itself and be replaced   \n302 with other similar methods without requiring additional special handling. ", "page_idx": 7}, {"type": "table", "img_path": "cgb0Tn4uHy/tmp/d37136fb61831efc5898414029cd87f747dcdbd8fe5102e5dc157871a96147e6.jpg", "table_caption": ["Table 3: Test accuracy on Clothing1M, Webvision and ILSVRC12. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "cgb0Tn4uHy/tmp/02ba57ac884e57736b3db796a6137520c1337a7d2b7c0045bd43f026cd64b7d6.jpg", "table_caption": ["Table 4: Ablation study of TMR, IR represents implicit regularization and TM represents transition matrix. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "303 4.6 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "304 Besides the aforementioned experiments, we conducted ablation studies on proposed TMR method to   \n305 assess the importance of each component. Table 4 presents the comparative results under $20\\%$ and   \n306 $40\\%$ instance-dependent noise rates, where \"w/o\" denotes \"without\", \"TM\" represents the transition   \n307 matrix, and \"IR\" the represents implicit regularization. From the results, it can be observed that the   \n308 absence of either IR or TM significantly affects the performance of our TMR method. Removing IR   \n309 has a greater impact, particularly in the case of instance-dependent noise, resulting in a substantial   \n310 decrease compared to TMR. While removing TM yields similar results on CIFAR-10 with a $20\\%$   \n311 noise rate, the difference becomes apparent when the noise rate increases to $40\\%$ or when applied   \n312 to more complex datasets like CIFAR-100. These results indicate that both the transition matrix   \n313 and implicit regularization term are crucial components in our model, highlighting the innovation of   \n314 combining these two aspects in our method. ", "page_idx": 8}, {"type": "text", "text": "315 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "316 We propose an extended model for transition matrix that firstly combines it with sparse implicit   \n317 regularization, enabling the extension of transition matrix methods from class-dependent noise to a   \n318 broader range of noise scenarios while maintaining the simplicity of the model. The effectiveness of   \n319 our method is theoretically analyzed under certain assumptions and validated through experiments   \n320 on various noisy datasets. Additionally, our method can be enhanced by combining with pre-trained   \n321 feature extractor such as contrastive learning, achieving state-of-the-art performance. ", "page_idx": 8}, {"type": "text", "text": "322 References ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "323 [1] G\u00f6rkem Algan and Ilkay Ulusoy. Image classification with deep learning in the presence of   \n324 noisy labels: A survey. Knowledge-Based Systems, 215:106771, 2021.   \n325 [2] Md Zahangir Alom, Tarek M Taha, Chris Yakopcic, Stefan Westberg, Paheding Sidike,   \n326 Mst Shamima Nasrin, Mahmudul Hasan, Brian C Van Essen, Abdul AS Awwal, and Vi  \n327 jayan K Asari. A state-of-the-art survey on deep learning theory and architectures. Electronics,   \n328 8(3):292, 2019.   \n329 [3] Eric Arazo, Diego Ortego, Paul Albert, Noel O\u2019Connor, and Kevin McGuinness. Unsupervised   \n330 label noise modeling and loss correction. In International Conference on Machine Learning,   \n331 pages 312\u2013321. PMLR, 2019.   \n332 [4] Devansh Arpit, Stanis\u0142aw Jastrz\u02dbebski, Nicolas Ballas, David Krueger, Emmanuel Bengio,   \n333 Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al.   \n334 A closer look at memorization in deep networks. In International Conference on Machine   \n335 Learning, pages 233\u2013242. PMLR, 2017.   \n336 [5] Wenkai Chen, Chuang Zhu, and Mengting Li. Sample prior guided robust model learning to   \n337 suppress noisy labels. In Joint European Conference on Machine Learning and Knowledge   \n338 Discovery in Databases, pages 3\u201319. Springer, 2023.   \n339 [6] De Cheng, Tongliang Liu, Yixiong Ning, Nannan Wang, Bo Han, Gang Niu, Xinbo Gao,   \n340 and Masashi Sugiyama. Instance-dependent label-noise learning with manifold-regularized   \n341 transition matrix estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision   \n342 and Pattern Recognition, pages 16630\u201316639, 2022.   \n343 [7] De Cheng, Yixiong Ning, Nannan Wang, Xinbo Gao, Heng Yang, Yuxuan Du, Bo Han, and   \n344 Tongliang Liu. Class-dependent label-noise learning with cycle-consistency regularization.   \n345 Advances in Neural Information Processing Systems, 35:11104\u201311116, 2022.   \n346 [8] Amit Daniely and Elad Granot. Generalization bounds for neural networks via approximate   \n347 description length. Advances in Neural Information Processing Systems, 32, 2019.   \n348 [9] Jacob Goldberger and Ehud Ben-Reuven. Training deep neural-networks using a noise adapta  \n349 tion layer. In International Conference on Learning Representations, 2016.   \n350 [10] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi   \n351 Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels.   \n352 Advances in Neural Information Processing Systems, 31, 2018.   \n353 [11] Wassily Hoeffding. Probability inequalities for sums of bounded random variables. The   \n354 collected works of Wassily Hoeffding, pages 409\u2013426, 1994.   \n355 [12] Arthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural tangent kernel: Convergence and   \n356 generalization in neural networks. Advances in Neural Information Processing Systems, 31,   \n357 2018.   \n358 [13] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning   \n359 data-driven curriculum for very deep neural networks on corrupted labels. In International   \n360 Conference on Machine Learning, pages 2304\u20132313. PMLR, 2018.   \n361 [14] Zhimeng Jiang, Kaixiong Zhou, Zirui Liu, Li Li, Rui Chen, Soo-Hyun Choi, and Xia Hu. An   \n362 information fusion approach to learning with instance-dependent label noise. In International   \n363 Conference on Learning Representations, 2021.   \n364 [15] Jan Kremer, Fei Sha, and Christian Igel. Robust active label correction. In International   \n365 Conference on Artificial Intelligence and Statistics, pages 308\u2013316. PMLR, 2018.   \n366 [16] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.   \n367 2009.   \n368 [17] Seong Min Kye, Kwanghee Choi, Joonyoung Yi, and Buru Chang. Learning with noisy labels   \n369 by efficient transition matrix estimation to combat label miscorrection. In European Conference   \n370 on Computer Vision, pages 717\u2013738. Springer, 2022.   \n371 [18] Jiangyuan Li, Thanh Nguyen, Chinmay Hegde, and Ka Wai Wong. Implicit sparse regularization:   \n372 The impact of depth and early stopping. Advances in Neural Information Processing Systems,   \n373 34:28298\u201328309, 2021.   \n374 [19] Junnan Li, Richard Socher, and Steven CH Hoi. Dividemix: Learning with noisy labels as   \n375 semi-supervised learning. arXiv preprint arXiv:2002.07394, 2020.   \n376 [20] Shikun Li, Xiaobo Xia, Hansong Zhang, Yibing Zhan, Shiming Ge, and Tongliang Liu. Esti  \n377 mating noise transition matrix with label correlations for noisy multi-label learning. Advances   \n378 in Neural Information Processing Systems, 35:24184\u201324198, 2022.   \n379 [21] Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, and Luc Van Gool. Webvision database:   \n380 Visual learning and understanding from web data. arXiv preprint arXiv:1708.02862, 2017.   \n381 [22] Xuefeng Li, Tongliang Liu, Bo Han, Gang Niu, and Masashi Sugiyama. Provably end-to-end   \n382 label-noise learning without anchor points. In International Conference on Machine Learning,   \n383 pages 6403\u20136413. PMLR, 2021.   \n384 [23] Yifan Li, Hu Han, Shiguang Shan, and Xilin Chen. Disc: Learning from noisy labels via dynamic   \n385 instance-specific selection and correction. In Proceedings of the IEEE/CVF Conference on   \n386 Computer Vision and Pattern Recognition, pages 24070\u201324079, 2023.   \n387 [24] Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Early  \n388 learning regularization prevents memorization of noisy labels. Advances in Neural Information   \n389 Processing Systems, 33:20331\u201320342, 2020.   \n390 [25] Sheng Liu, Zhihui Zhu, Qing Qu, and Chong You. Robust training under label noise by over  \n391 parameterization. In International Conference on Machine Learning, pages 14153\u201314172.   \n392 PMLR, 2022.   \n393 [26] Yang Liu, Hao Cheng, and Kun Zhang. Identifiability of label noise transition matrix. In   \n394 International Conference on Machine Learning, pages 21475\u201321496. PMLR, 2023.   \n395 [27] Yang Liu and Hongyi Guo. Peer loss functions: Learning from noisy labels without knowing   \n396 noise rates. In International Conference on Machine Learning, pages 6226\u20136236. PMLR, 2020.   \n397 [28] Xingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah Erfani, and James Bailey.   \n398 Normalized loss functions for deep learning with noisy labels. In International Conference on   \n399 Machine Learning, pages 6543\u20136553. PMLR, 2020.   \n400 [29] Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning   \n401 with noisy labels. Advances in Neural Information Processing Systems, 26, 2013.   \n402 [30] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias:   \n403 On the role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.   \n404 [31] Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu.   \n405 Making deep neural networks robust to label noise: A loss correction approach. In Proceedings   \n406 of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1944\u20131952, 2017.   \n407 [32] Samira Pouyanfar, Saad Sadiq, Yilin Yan, Haiman Tian, Yudong Tao, Maria Presa Reyes, Mei  \n408 Ling Shyu, Shu-Ching Chen, and Sundaraja S Iyengar. A survey on deep learning: Algorithms,   \n409 techniques, and applications. ACM Computing Surveys (CSUR), 51(5):1\u201336, 2018.   \n410 [33] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples   \n411 for robust deep learning. In International Conference on Machine Learning, pages 4334\u20134343.   \n412 PMLR, 2018.   \n413 [34] Jun Shu, Qian Zhao, Zongben Xu, and Deyu Meng. Meta transition adaptation for robust deep   \n414 learning with noisy labels. arXiv preprint arXiv:2006.05697, 2020.   \n415 [35] Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from   \n416 noisy labels with deep neural networks: A survey. IEEE Transactions on Neural Networks and   \n417 Learning Systems, 2022.   \n418 [36] Sainbayar Sukhbaatar, Joan Bruna, Manohar Paluri, Lubomir Bourdev, and Rob Fergus. Training   \n419 convolutional networks with noisy labels. arXiv preprint arXiv:1406.2080, 2014.   \n420 [37] Tomas Vaskevicius, Varun Kanade, and Patrick Rebeschini. Implicit regularization for optimal   \n421 sparse recovery. Advances in Neural Information Processing Systems, 32, 2019.   \n422 [38] Jialu Wang, Yang Liu, and Caleb Levy. Fair classification with group-dependent label noise. In   \n423 Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages   \n424 526\u2013536, 2021.   \n425 [39] Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric cross   \n426 entropy for robust learning with noisy labels. In Proceedings of the IEEE/CVF International   \n427 Conference on Computer Vision, pages 322\u2013330, 2019.   \n428 [40] Jiaheng Wei, Zhaowei Zhu, Hao Cheng, Tongliang Liu, Gang Niu, and Yang Liu. Learning   \n429 with noisy labels revisited: A study using real-world human annotations. arXiv preprint   \n430 arXiv:2110.12088, 2021.   \n431 [41] Xiaobo Xia, Tongliang Liu, Bo Han, Chen Gong, Nannan Wang, Zongyuan Ge, and Yi Chang.   \n432 Robust early-learning: Hindering the memorization of noisy labels. In International Conference   \n433 on Learning Representations, 2020.   \n434 [42] Xiaobo Xia, Tongliang Liu, Bo Han, Nannan Wang, Mingming Gong, Haifeng Liu, Gang Niu,   \n435 Dacheng Tao, and Masashi Sugiyama. Part-dependent label noise: Towards instance-dependent   \n436 label noise. Advances in Neural Information Processing Systems, 33:7597\u20137610, 2020.   \n437 [43] Xiaobo Xia, Tongliang Liu, Nannan Wang, Bo Han, Chen Gong, Gang Niu, and Masashi   \n438 Sugiyama. Are anchor points really indispensable in label-noise learning? Advances in Neural   \n439 Information Processing Systems, 32, 2019.   \n440 [44] Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive   \n441 noisy labeled data for image classification. In Proceedings of the IEEE Conference on Computer   \n442 Vision and Pattern Recognition, pages 2691\u20132699, 2015.   \n443 [45] Yilun Xu, Peng Cao, Yuqing Kong, and Yizhou Wang. L_dmi: A novel information-theoretic   \n444 loss function for training deep nets robust to label noise. Advances in Neural Information   \n445 Processing Systems, 32, 2019.   \n446 [46] Shuo Yang, Erkun Yang, Bo Han, Yang Liu, Min Xu, Gang Niu, and Tongliang Liu. Estimating   \n447 instance-dependent bayes-label transition matrix using a deep neural network. In International   \n448 Conference on Machine Learning, pages 25302\u201325312. PMLR, 2022.   \n449 [47] Yu Yao, Tongliang Liu, Bo Han, Mingming Gong, Jiankang Deng, Gang Niu, and Masashi   \n450 Sugiyama. Dual t: Reducing estimation error for transition matrix in label-noise learning.   \n451 Advances in Neural Information Processing Systems, 33:7260\u20137271, 2020.   \n452 [48] LIN Yong, Renjie Pi, Weizhong Zhang, Xiaobo Xia, Jiahui Gao, Xiao Zhou, Tongliang Liu,   \n453 and Bo Han. A holistic view of label noise transition matrix in deep learning and beyond. In   \n454 The Eleventh International Conference on Learning Representations, 2022.   \n455 [49] Chong You, Zhihui Zhu, Qing Qu, and Yi Ma. Robust recovery via implicit bias of discrepant   \n456 learning rates for double over-parameterization. Advances in Neural Information Processing   \n457 Systems, 33:17733\u201317744, 2020.   \n458 [50] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding   \n459 deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107\u2013   \n460 115, 2021.   \n461 [51] Yivan Zhang, Gang Niu, and Masashi Sugiyama. Learning noise transition matrix from only   \n462 noisy labels via total variation regularization. In International Conference on Machine Learning,   \n463 pages 12501\u201312512. PMLR, 2021.   \n464 [52] Yivan Zhang and Masashi Sugiyama. Approximating instance-dependent noise via instance  \n465 confidence embedding. arXiv preprint arXiv:2103.13569, 2021.   \n466 [53] Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks   \n467 with noisy labels. Advances in Neural Information Processing Systems, 31, 2018.   \n468 [54] Ganlong Zhao, Guanbin Li, Yipeng Qin, Feng Liu, and Yizhou Yu. Centrality and consistency:   \n469 two-stage clean samples identification for learning with instance-dependent noisy labels. In   \n470 European Conference on Computer Vision, pages 21\u201337. Springer, 2022.   \n471 [55] Peng Zhao, Yun Yang, and Qiao-Chu He. Implicit regularization via hadamard product over  \n472 parametrization in high-dimensional linear regression. arXiv preprint arXiv:1903.09367, 2(4):8,   \n473 2019.   \n474 [56] Peng Zhao, Yun Yang, and Qiao-Chu He. High-dimensional linear regression via implicit   \n475 regularization. Biometrika, 109(4):1033\u20131046, 2022.   \n476 [57] Xiong Zhou, Xianming Liu, Chenyang Wang, Deming Zhai, Junjun Jiang, and Xiangyang   \n477 Ji. Learning with noisy labels via sparse regularization. In Proceedings of the IEEE/CVF   \n478 International Conference on Computer Vision, pages 72\u201381, 2021.   \n479 [58] Zhaowei Zhu, Yiwen Song, and Yang Liu. Clusterability as an alternative to anchor points   \n480 when learning with noisy labels. In International Conference on Machine Learning, pages   \n481 12912\u201312923. PMLR, 2021.   \n482 [59] Zhaowei Zhu, Jialu Wang, and Yang Liu. Beyond images: Label noise transition matrix   \n483 estimation for tasks with lower-quality features. In International Conference on Machine   \n484 Learning, pages 27633\u201327653. PMLR, 2022. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "485 A Related Works ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "486 A.1 Transition Matrix Methods ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "487 Most previous transition matrix methods focus on class-dependent label noise to simplify the esti  \n488 mation difficulty. Some of the early methods [31, 43, 47] usually assume the existence of anchor   \n489 points and make the transition matrix identifiable by finding anchor points or approximate anchor   \n490 points. To mitigate the anchor point assumption, VolMinNet [22] and TVD [51] add different forms   \n491 of regularization for the transition matrix respectively to make it identifiable. While other methods   \n492 [7, 17] try setting up unique network structure to estimate the transition matrix. Besides, [34, 48]   \n493 utilize structures like meta-learning to estimate the transition matrix, but may require more clean data   \n494 and computational consumption. Although the above methods are designed to handle class-dependent   \n495 label noise, it is not suitable when encountering instance-dependent noise or real-world noisy data.   \n496 However, it is not feasible to estimate a transition matrix individually for each sample without other   \n497 assumptions or multiple noisy labels [26]. In order to achieve an approximate estimation of the   \n498 instance-dependent transition matrix, [9] uses an adaptation layer to estimate the transition matrix   \n499 based on each sample\u2019s output, but the error is large due to the influence of the initial value. While   \n500 [46] uses a separate network to estimate the transition matrix based on the Bayesian label. Some   \n501 methods [42, 38, 58, 59] learn a part-dependent or group-dependent matrix through clustering, which   \n502 is a compromise estimation method lies between instance-dependent and class-dependent methods.   \n503 Other methods [6, 14] utilize similarity in feature space to assist transition matrix learning. Although   \n504 these instance-dependent transition matrix methods achieve identifiability through special treatments,   \n505 they are usually relatively complex and have larger errors, which is contrary to the convenient and   \n506 simple characteristics of transition matrix methods. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "507 A.2 Implicit Regularization ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "508 Implicit regularization can be regarded as a statistical method for sparsity, playing the role of   \n509 minimizing $L_{1}$ loss in sparse noise learning and being currently used in various models [55, 37, 49,   \n510 18, 56]. Among these methods, SOP [25] is the one worthy of special attention, which is related   \n511 to our method. SOP also uses implicit regularization for noisy label learning, which gives a sparse   \n512 representation of the residual term between prediction and observed noisy label. However, it does not   \n513 take advantage of the overall transfer probability of noise and the noise sparsity assumption does not   \n514 apply to high noise rates situation, so its performance on large noise rates data is relatively weak. We   \n515 will compare it with our proposed method by experimental results specifically in Section 4. ", "page_idx": 13}, {"type": "text", "text": "516 B Algorithm and proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "517 B.1 Algorithm ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "518 The steps of our TMR algorithm are shown in detail in Algorithm 1. ", "page_idx": 13}, {"type": "text", "text": "519 B.2 Conditions ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "520 Condition 1. For optimization problem (17), initialize parameters in the algorithm 1 with ${\\pmb u}_{i}=t{\\bf1}$ ,   \n521 $\\pmb{v}=t\\mathbf{1}$ , where 1 are vectors of all 1, $t$ is a small value scalar. There exists a given $\\alpha_{0}>0$ such that   \n522 the learning rates of gradient descent satisfy $l r(\\pmb{u})=l r(\\pmb{v})=\\alpha l r(\\pmb{\\theta}),\\alpha$ $\\alpha<\\alpha_{0}$ .   \n523 Condition 2. Denote the rank of $\\pmb{G}$ in formula (15) as $r$ , the number of sparse nonzero entries of $R_{*}$   \n524 is $k$ , $_{P}$ is the matrix of row vectors in SVD decomposition of $\\pmb{G}$ . Define Nr max1\u2264i\u2264N\u2225P \u22a4ei\u222522.   \n525 Then $k,r,s$ satisfy $4k^{2}r s<N$ .   \n526 Condition 3. The row vectors of matrix $\\pmb{F}$ in formula (14) are sufficiently scattered, which is a   \n527 weakened requirement of the anchor points assumption can be found in Definition 2 of [22]. ", "page_idx": 13}, {"type": "text", "text": "528 B.3 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "529 Proof. Denote $\\boldsymbol{Q}=(I_{C}\\otimes\\pmb{\\theta})\\cdot\\pmb{T}$ , the optimization problem in (17) can be written as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{min}{\\frac{1}{2}}\\|G\\cdot Q+U\\odot U-V\\odot V-{\\tilde{Y}}\\|_{2}^{2}+\\lambda\\cdot\\log\\operatorname*{det}(T).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Input: Training data $\\{(\\pmb{x}_{i},\\pmb{y}_{i})\\}_{i=1}^{N}$ , network $f_{\\theta}(\\cdot)$ , coefficient $\\lambda$ , learning rate $\\tau_{\\theta},\\tau_{u},\\tau_{v},\\tau_{T}$ , batch   \nsize $m$ , epoch number $E$ , transition matrix update frequency $k$ .   \nInitialization: Transition matrix $_T$ with an identity matrix, draw entries of $\\{u_{i},\\pmb{v}_{i}\\}_{i=1}^{N}$ from i.i.d.   \nGaussian distribution with zero-mean and s.t.d. 1e-8.   \nfor $t=1$ to $E$ do for $b=1$ to $N/m$ do Get a sample batch $B\\subseteq\\{1,\\ldots,N\\}$ with $|B|=m$ Calculate loss $\\mathcal{L}$ by 4 with batch $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ for $i$ in $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ do Update $\\pmb{u}_{i}\\leftarrow\\pmb{u}_{i}-\\tau_{\\pmb{u}}\\cdot\\partial\\mathcal{L}/\\partial\\pmb{u}_{i}$ Update $\\pmb{v}_{i}\\leftarrow\\pmb{v}_{i}-\\tau_{\\pmb{v}}\\cdot\\partial\\mathcal{L}/\\partial\\pmb{v}_{i}$ end for Update $\\pmb{\\theta}\\leftarrow\\pmb{\\theta}-\\tau_{\\pmb{\\theta}}\\cdot\\partial\\pmb{\\mathcal{L}}/\\partial\\pmb{\\theta}$ if $b/k$ is 0 then Update $\\pmb{T}\\leftarrow\\pmb{T}-\\pmb{\\tau}\\pmb{T}\\cdot\\partial\\pmb{\\mathcal{L}}/\\partial\\pmb{T}$ end if end for   \nend for   \nOutput: Network parameters $\\hat{\\pmb\\theta}$ , variables $\\{\\hat{\\pmb u}_{i},\\hat{\\pmb v}_{i}\\}_{i=1}^{N}$ and transition matrix $\\hat{\\pmb T}$ . ", "page_idx": 14}, {"type": "text", "text": "530 Since implicit regularization can minimize the $L_{1}$ loss and according to Proposition 3.3 in [25],   \n531 the first half of formula (21) will converge to a global solution for any fixed $_T$ under Condition 1.   \n532 Furthermore, it can be converted into the following optimization problem: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{Q,R}\\frac{1}{2}\\|Q\\|_{2}^{2}+\\beta\\|R\\|_{1},\\quad\\mathrm{s.t.}\\quad\\tilde{\\pmb{Y}}=\\pmb{G}\\cdot\\pmb{Q}+\\pmb{R},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "533 where $\\beta=-\\frac{\\log t}{2\\alpha}$ \u2212lo2g\u03b1 t as defined in 1. When Condition 2 is true, the solution to problem (22) are Q\u2217and   \n534 $R_{*}$ , where $\\Tilde{Y}$ is produced by $G\\cdot Q_{*}+R_{*}$ . This conclusion can be deduced from the analogy of   \n535 Proposition 3.5 in [25]. Combining formula (15), we can get: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{Q}_{*}=\\left(I_{C}\\otimes\\pmb{\\theta}_{*}\\right)\\cdot\\boldsymbol{T}_{*}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "536 Therefore, problem (21) transform into an optimization problem with parameter $\\theta,T$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta,T}\\log\\operatorname*{det}(T),\\quad\\mathrm{s.t.}\\quad(I_{C}\\otimes\\theta)\\cdot T=Q_{*}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "537 The above optimization problem has the same form as the optimization problem in [22], similar with   \n538 Theorem 1 in this paper, under Condition 3, the solution to problem (24) is: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\pmb{\\theta}}=\\pmb{\\theta}_{\\ast},\\quad\\hat{\\pmb{T}}=\\pmb{T}_{\\ast}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "539 To sum up, when all conditions in Appendix B.2 are met, we can get the ground truth solution $\\pmb{\\theta}_{*}$ , the   \n540 estimators by our algorithm converge to $\\mathbf{\\deltaT}_{*}$ , $R_{*}$ as mentioned in Theorem 3.1. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "541 B.4 Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "542 Proof. We use the inequality we use Hoeffding inequality [11] to help us complete the proof. Since   \n543 $\\hat{\\pmb{\\theta}}_{(N)},\\hat{\\pmb{T}}_{(N)}$ are not independent of the samples, we use $\\epsilon_{}$ -cover as mentioned in Section 3.2 to deal   \n544 with the problem. In addition, the parameter $\\boldsymbol{R}$ is omitted in the following proof for convenience and   \n545 does not affect the understanding of the results.   \n546 According to the definition of $\\epsilon$ covering, We can find a pair of parameters $\\theta_{k},T_{k}$ in the covering set   \n547 such that: ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\n|\\ell\\left(\\pmb{\\theta}_{k},\\pmb{T}_{k};\\boldsymbol{X},\\boldsymbol{Y}\\right)-\\ell(\\hat{\\pmb{\\theta}}_{(N)},\\hat{\\pmb{T}}_{(N)};\\boldsymbol{X},\\boldsymbol{Y})|\\leq\\epsilon,\\forall(\\boldsymbol{X},\\boldsymbol{Y})\\in\\mathcal{X}\\times\\mathcal{Y}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "548 Average the loss over samples, we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\hat{\\pmb{\\theta}}_{(N)},\\hat{\\pmb{T}}_{(N)};\\tilde{\\mathbb{D}})\\leq\\mathcal{L}(\\pmb{\\theta}_{k},\\pmb{T}_{k};\\tilde{\\mathbb{D}})+\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "549 To meet the requirement of probability $1-\\delta$ in Theorem 3.2, we take the probability value as $\\delta/2\\ensuremath{\\mathcal{N}}_{\\mathcal{F}}$   \n550 in Hoeffding inequality due to the randomness of $k$ . Thus, with probability at least $1-\\delta/2\\Lambda\\dot{\\jmath}_{\\mathcal{F}}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\pmb{\\theta}_{k},T_{k};\\tilde{\\mathbb{D}})\\leq\\mathcal{L}(\\pmb{\\theta}_{k},T_{k};\\tilde{\\mathbb{D}}_{(N)})+M\\sqrt{\\frac{\\ln(2\\mathcal{N}_{\\mathcal{F}}/\\delta)}{2n}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "551 By the definition of formula (26), ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\pmb{\\theta}_{k},\\pmb{T}_{k};\\tilde{\\mathbb{D}}_{(N)})\\leq\\mathcal{L}(\\hat{\\pmb{\\theta}}_{(N)},\\hat{\\pmb{T}}_{(N)};\\tilde{\\mathbb{D}}_{(N)})+\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "552 According to the property of $\\hat{\\pmb\\theta}_{(N)},\\hat{\\pmb T}_{(N)}$ in formula (19), for any $\\pmb{\\theta}\\in\\mathbb{R}^{p},T\\in\\mathbb{T}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\hat{\\pmb{\\theta}}_{(N)},\\hat{\\pmb{T}}_{(N)};\\tilde{\\mathbb{D}}_{(N)})\\leq\\mathcal{L}(\\pmb{\\theta},\\pmb{T};\\tilde{\\mathbb{D}}_{(N)})+\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "553 Using the Hoeffding inequality again with probability $\\delta/2$ , with probability at least $1-\\delta/2$ we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\pmb{\\theta},\\pmb{T};\\tilde{\\mathbb{D}}_{(N)})\\leq\\mathcal{L}(\\pmb{\\theta},\\pmb{T};\\tilde{\\mathbb{D}})+M\\sqrt{\\frac{\\ln(2/\\delta)}{2n}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "554 Combining inequalities (27), (28), (29), (30), (31) and adding the probability values, we get the   \n555 conclusion that with probability at least $1-\\delta$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\hat{\\theta}_{(N)},\\hat{T}_{(N)};\\tilde{\\mathbb{D}})\\leq\\mathcal{L}(\\theta,T,;\\tilde{\\mathbb{D}})+M\\sqrt{\\frac{\\ln(2\\!\\sqrt{\\chi_{\\mathcal{F}}/\\delta)}}{2n}}+M\\sqrt{\\frac{\\ln(2/\\delta)}{2n}}+3\\epsilon,\\forall\\theta\\in\\mathbb{R}^{p},T\\in\\mathbb{T}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "556 ", "page_idx": 15}, {"type": "text", "text": "557 C Experiment details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "558 C.1 Experimental Setup ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "559 We conduct experiments on a single NVIDIA 3090Ti graphics card. For software, we use Python 3.11   \n560 and PyTorch 1.10 to build the models. Throughout the training process, transition matrix updates are   \n561 carried out using the Adam optimization method, while updates for other parameters are performed   \n562 using the stochastic gradient descent (SGD) optimization method. The experimental setup involves a   \n563 few training hyper-parameters, including the backbone network used, batch size, learning rate for   \n564 parameters, and weight of the regularization term. For specific experimental configurations, please   \n565 refer to Table 5 in Appendix C.2. ", "page_idx": 15}, {"type": "text", "text": "566 C.2 Hyper-parameters Setting ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "567 The backbone network and hyper-parameters of the experiments on each dataset are listed in the table   \n568 5. ", "page_idx": 15}, {"type": "table", "img_path": "cgb0Tn4uHy/tmp/1e38b8a115ffe7afbc14e8425a7950d3da3f774159804c7030573a89d3b963c8.jpg", "table_caption": ["Table 5: Hyper-parameters on CIFAR-10/100, Clothing-1M and Webvision. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "cgb0Tn4uHy/tmp/e78a327201e930347c0b3823f4c4cbd800f8085f045e397c1de368271a46edee.jpg", "table_caption": ["Table 6: Test accuracy with symmetric and flip noise on CIFAR-10/100. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "569 C.3 Supplementary experiments on class-dependent noise ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "570 In addition to conducting experiments on instance-dependent noisy data, we further evaluated the   \n571 general effectiveness of our method compared to other approaches by introducing class-dependent   \n572 scenarios on CIFAR-10/100 datasets. Table 6 presents the comparative results on CIFAR-10/100   \n573 datasets with symmetric noise rates of $20\\%$ and $50\\%$ , as well as filp noise rates of $20\\%$ and $45\\%$ . It can   \n574 be observed that for class-dependent noise, which serves as a simplified case of instance-dependent   \n575 noise, our proposed method TMR outperforms other comparative methods, including transition   \n576 matrix methods specifically designed for class-dependent noise, such as VolMinNet. Specifically, the   \n577 transition matrix methods specifically designed for handling instance-dependent noise, such as BLTM,   \n578 PartT and MEIDTM, do not show significant improvements when applied to class-dependent noise   \n579 scenarios compared to the transition matrix methods designed only for class-dependent noise, such as   \n580 VolMinNet. However, our proposed method, TMR, achieves significant improvements even when   \n581 applied to class-dependent noise scenarios compared to VolMinNet. This indicates that our method   \n582 has universal applicability and yields favorable results in both class-dependent and instance-dependent   \n583 noise scenarios. ", "page_idx": 16}, {"type": "text", "text": "584 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: The main content and contributions of the work are reflected in the abstract and introduction. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "601 2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: In the theoretical analysis section and experimental section, we analyze the applicability and limitations of our method. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "33 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes]   \nJustification: We conduct theoretical analysis of our method and provide proofs for the theorems in the paper.   \n\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "650 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "651 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n652 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n653 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We provide a detailed description of the experimental setup in the experimental section, and specific settings for hyperparameters are provided in the appendix. ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "689 5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "690 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n691 tions to faithfully reproduce the main experimental results, as described in supplemental   \n692 material?   \n693 Answer: [Yes]   \n694 Justification: We provide partial code in the supplementary materials, and the complete code   \n695 will be open-sourced upon acceptance of the paper.   \n696 Guidelines:   \n697 \u2022 The answer NA means that paper does not include experiments requiring code.   \n698 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n699 public/guides/CodeSubmissionPolicy) for more details.   \n700 \u2022 While we encourage the release of code and data, we understand that this might not be   \n701 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n702 including code, unless this is central to the contribution (e.g., for a new open-source   \n703 benchmark).   \n704 \u2022 The instructions should contain the exact command and environment needed to run to   \n705 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n706 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n707 \u2022 The authors should provide instructions on data access and preparation, including how   \n708 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n709 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n710 proposed method and baselines. If only a subset of experiments are reproducible, they   \n711 should state which ones are omitted from the script and why.   \n712 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n713 versions (if applicable).   \n714 \u2022 Providing as much information as possible in supplemental material (appended to the   \n715 paper) is recommended, but including URLs to data and code is permitted.   \n716 6. Experimental Setting/Details   \n717 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n24 \u2022 The answer NA means that the paper does not include experiments.   \n25 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n26 that is necessary to appreciate the results and make sense of them.   \n27 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n28 material. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "729 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "730 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n731 information about the statistical significance of the experiments?   \n740 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n741 example, train/test split, initialization, random drawing of some parameter, or overall   \n742 run with given experimental conditions).   \n743 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n744 call to a library function, bootstrap, etc.)   \n745 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n746 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n747 of the mean.   \n748 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n749 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n750 of Normality of errors is not verified.   \n751 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n752 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n753 error rates).   \n754 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n755 they were calculated and reference the corresponding figures or tables in the text.   \n756 8. Experiments Compute Resources   \n757 Question: For each experiment, does the paper provide sufficient information on the com  \n758 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n759 the experiments?   \n760 Answer: [Yes]   \n761 Justification: We list the relevant details in the experimental section.   \n762 Guidelines:   \n763 \u2022 The answer NA means that the paper does not include experiments.   \n764 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n765 or cloud provider, including relevant memory and storage.   \n766 \u2022 The paper should provide the amount of compute required for each of the individual   \n767 experimental runs as well as estimate the total compute.   \n768 \u2022 The paper should disclose whether the full research project required more compute   \n769 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n770 didn\u2019t make it into the paper).   \n771 9. Code Of Ethics   \n772 Question: Does the research conducted in the paper conform, in every respect, with the   \n773 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n774 Answer: [Yes]   \n775 Justification: We submitted the paper following the NeurIPS Code of Ethics.   \n776 Guidelines:   \n777 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n778 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n779 deviation from the Code of Ethics.   \n780 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n781 eration due to laws or regulations in their jurisdiction).   \n782 10. Broader Impacts   \n783 Question: Does the paper discuss both potential positive societal impacts and negative   \n784 societal impacts of the work performed?   \n785 Answer: [Yes]   \n786 Justification: We discuss the positive implications of our work and ensure it does not have   \n787 any negative societal impact.   \n788 Guidelines:   \n789 \u2022 The answer NA means that there is no societal impact of the work performed.   \n790 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n791 impact or why the paper does not address societal impact.   \n792 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n793 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n794 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n795 groups), privacy considerations, and security considerations.   \n796 \u2022 The conference expects that many papers will be foundational research and not tied   \n797 to particular applications, let alone deployments. However, if there is a direct path to   \n798 any negative applications, the authors should point it out. For example, it is legitimate   \n799 to point out that an improvement in the quality of generative models could be used to   \n800 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n801 that a generic algorithm for optimizing neural networks could enable people to train   \n802 models that generate Deepfakes faster.   \n803 \u2022 The authors should consider possible harms that could arise when the technology is   \n804 being used as intended and functioning correctly, harms that could arise when the   \n805 technology is being used as intended but gives incorrect results, and harms following   \n806 from (intentional or unintentional) misuse of the technology.   \n807 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n808 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n809 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n810 feedback over time, improving the efficiency and accessibility of ML).   \n811 11. Safeguards   \n812 Question: Does the paper describe safeguards that have been put in place for responsible   \n813 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n814 image generators, or scraped datasets)?   \n815 Answer: [NA]   \n816 Justification: There are no concerns in this regard regarding this work.   \n817 Guidelines:   \n818 \u2022 The answer NA means that the paper poses no such risks.   \n819 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n820 necessary safeguards to allow for controlled use of the model, for example by requiring   \n821 that users adhere to usage guidelines or restrictions to access the model or implementing   \n822 safety filters.   \n823 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n824 should describe how they avoided releasing unsafe images.   \n825 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n826 not require this, but we encourage authors to take this into account and make a best   \n827 faith effort.   \n828 12. Licenses for existing assets   \n829 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n830 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n831 properly respected?   \n832 Answer: [Yes]   \n833 Justification: The data and code used in our work are all publicly available and open-source.   \n834 Guidelines:   \n835 \u2022 The answer NA means that the paper does not use existing assets.   \n836 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n837 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n838 URL.   \n839 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n840 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n841 service of that source should be provided.   \n842 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n843 package should be provided. For popular datasets, paperswithcode.com/datasets   \n844 has curated licenses for some datasets. Their licensing guide can help determine the   \n845 license of a dataset.   \n846 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n847 the derived asset (if it has changed) should be provided.   \n848 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n849 the asset\u2019s creators.   \n850 13. New Assets   \n851 Question: Are new assets introduced in the paper well documented and is the documentation   \n852 provided alongside the assets?   \n853 Answer: [NA]   \n854 Justification: The paper currently does not include any new assets.   \n855 Guidelines:   \n856 \u2022 The answer NA means that the paper does not release new assets.   \n857 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n858 submissions via structured templates. This includes details about training, license,   \n859 limitations, etc.   \n860 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n861 asset is used.   \n862 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n863 create an anonymized URL or include an anonymized zip file.   \n864 14. Crowdsourcing and Research with Human Subjects   \n865 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n866 include the full text of instructions given to participants and screenshots, if applicable, as   \n867 well as details about compensation (if any)?   \n868 Answer: [NA]   \n869 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n870 Guidelines:   \n871 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n872 human subjects.   \n873 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n874 tion of the paper involves human subjects, then as much detail as possible should be   \n875 included in the main paper.   \n876 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n877 or other labor should be paid at least the minimum wage in the country of the data   \n878 collector.   \n879 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n880 Subjects   \n881 Question: Does the paper describe potential risks incurred by study participants, whether   \n882 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n883 approvals (or an equivalent approval/review based on the requirements of your country or   \n884 institution) were obtained?   \n885 Answer: [NA]   \n886 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n887 Guidelines:   \n888 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n889 human subjects.   \n890 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n891 may be required for any human subjects research. If you obtained IRB approval, you   \n892 should clearly state this in the paper. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]