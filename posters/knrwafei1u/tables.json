[{"figure_path": "KNrwaFEi1u/tables/tables_2_1.jpg", "caption": "Table 1: An overview of object hallucination benchmarks. For design considerations, we summarize the number of tested images, and if multiple classes and object class distribution (at training and test time) are considered. The image sources include those seen or unseen during instruction tuning. To refer to an object, textual descriptions and visual cues can be adopted. For evaluation, neural models, humans and automatic pipelines are used.", "description": "This table provides a comparison of different object hallucination benchmarks.  It highlights key differences in their design, including whether they consider multiple objects, the distribution of object classes, the image source (seen during training or unseen), the type of visual referencing used (textual descriptions or visual cues), the evaluation method (neural models, humans, or automated pipelines), and the number of test images.  This allows for a better understanding of the strengths and weaknesses of each benchmark in evaluating object hallucination.", "section": "2 Related Work"}, {"figure_path": "KNrwaFEi1u/tables/tables_5_1.jpg", "caption": "Table 2: Averaged accuracy of baselines on the In-the-Wild, Homogeneous, and Heterogeneous splits. The bold marker denotes the best-performing baseline and the underlined marker denotes the second-best-performing baseline.", "description": "This table presents the averaged accuracy results for various vision-language models (LVLMs) across three different object class distribution settings: In-the-Wild, Homogeneous, and Heterogeneous.  The accuracy is calculated using three different querying methods: Default Multi-Object, Student-Forcing, and Teacher-Forcing.  For comparison, the table also includes results for Single-Object querying. The best and second-best performing models for each condition are highlighted.", "section": "4 Experiments and Results"}, {"figure_path": "KNrwaFEi1u/tables/tables_15_1.jpg", "caption": "Table 1: An overview of object hallucination benchmarks. For design considerations, we summarize the number of tested images, and if multiple classes and object class distribution (at training and test time) are considered. The image sources include those seen or unseen during instruction tuning. To refer to an object, textual descriptions and visual cues can be adopted. For evaluation, neural models, humans and automatic pipelines are used.", "description": "This table compares different object hallucination benchmarks in terms of their design considerations and evaluation methods.  It shows the number of test images used, whether multiple object classes or varied distributions were considered (in training and testing), the source of images, and the type of evaluation process employed (textual descriptions, visual cues, neural models, humans, or automated pipelines).  This information helps contextualize the authors' new benchmark (ROPE) and its improvements over existing methods.", "section": "2 Related Work"}, {"figure_path": "KNrwaFEi1u/tables/tables_17_1.jpg", "caption": "Table 1: An overview of object hallucination benchmarks. For design considerations, we summarize the number of tested images, and if multiple classes and object class distribution (at training and test time) are considered. The image sources include those seen or unseen during instruction tuning. To refer to an object, textual descriptions and visual cues can be adopted. For evaluation, neural models, humans and automatic pipelines are used.", "description": "This table compares different object hallucination benchmarks.  It shows the number of test images, whether multiple classes and varying class distributions were considered, the source of images (seen or unseen in training), how objects were referred to (text or visual cues), and what evaluation methods were used (neural models, humans, or automatic systems).", "section": "2 Related Work"}, {"figure_path": "KNrwaFEi1u/tables/tables_18_1.jpg", "caption": "Table 2: Averaged accuracy of baselines on the In-the-Wild, Homogeneous, and Heterogeneous splits. The bold marker denotes the best-performing baseline and the underlined marker denotes the second-best-performing baseline.", "description": "This table presents the average accuracy results of various vision-language models (LVLMs) across different test sets.  The test sets vary in the distribution of object classes within each image: In-the-Wild (mixed distribution), Homogeneous (all objects belong to the same class), and Heterogeneous (objects belong to different classes).  The table shows results for three different prompting strategies: Default, Student-Forcing, and Teacher-Forcing.  The best and second-best performing models for each condition are highlighted.", "section": "4 Experiments and Results"}, {"figure_path": "KNrwaFEi1u/tables/tables_19_1.jpg", "caption": "Table 2: Averaged accuracy of baselines on the In-the-Wild, Homogeneous, and Heterogeneous splits. The bold marker denotes the best-performing baseline and the underlined marker denotes the second-best-performing baseline.", "description": "This table presents the average accuracy results for various vision-language models (LVLMs) across three different data splits: In-the-Wild, Homogeneous, and Heterogeneous.  The accuracy is evaluated using three different probing methods: Default Multi-Object, Student-Forcing, and Teacher-Forcing.  The table allows comparison of model performance under varying conditions of object complexity and instructional methods.", "section": "4 Experiments and Results"}]