[{"figure_path": "ztwl4ubnXV/tables/tables_5_1.jpg", "caption": "Table 3: Left: Results on Compas without using group annotations at test time. Right: Runtime Comparison for Fairlearn Reductions and OxonFair on Adult using a Macbook M2. To alter the groups, we iteratively merge the smallest racial group with 'Other', reducing the search space. For both methods, we enforced demographic parity over a train set consisting of 70% of the data. Despite the exponential complexity of our approach, we remain significantly faster until we reach 5 groups. The 0.6+ indicates the seconds to train XGBoost. OxonFair(S) indicates the runtime of the naive slow pathway described in Appendix A.2 rather than our accelerated approach.", "description": "This table compares the performance of Fairlearn and OxonFair on the COMPAS dataset, focusing on accuracy and demographic parity.  The left side shows results without using group annotations at test time, highlighting the effectiveness of OxonFair even in this setting. The right side contrasts the runtime performance of both methods, showing OxonFair's efficiency particularly when dealing with a higher number of groups, even given its exponential complexity. OxonFair's accelerated method is significantly faster than its naive implementation, especially for scenarios with multiple groups.", "section": "5.1 Computer Vision and CelebA"}, {"figure_path": "ztwl4ubnXV/tables/tables_6_1.jpg", "caption": "Table 1: We report mean scores over the 14 gender independent CelebA labels [28]. Single task methods and FairMixup scores in the second and third blocks are from Zietlow et al. [17]. ERM is the baseline architecture run without fairness. OxonFair (optimizing for accuracy and difference in equal opportunity (DEO)), has better accuracy (\u2191) and DEO (\u2193) scores than any other fair method.", "description": "This table compares the performance of OxonFair against other fairness methods on the CelebA dataset for 14 gender-independent attributes.  It shows accuracy and difference in equal opportunity (DEO) scores. OxonFair outperforms other methods in both metrics.", "section": "5.1 Computer Vision and CelebA"}, {"figure_path": "ztwl4ubnXV/tables/tables_7_1.jpg", "caption": "Table 14: Extended Version of Table 2. Performance Comparison of Different Algorithmic Fairness Methods on the CelebA Test Set. Results monitor the mean Accuracy, Difference in Equal Opportunity (DEO) and the Minimum Group Minimum Label Accuracy across the attributes.", "description": "This table presents a detailed comparison of various algorithmic fairness methods' performance on the CelebA test set.  It expands on the information presented in Table 2, offering a more comprehensive evaluation across multiple fairness metrics.  The metrics compared include mean accuracy, difference in equal opportunity (DEO), and minimum group minimum label accuracy. The table allows for a detailed analysis of each method's effectiveness in achieving both high accuracy and fairness across different subsets of attributes within the CelebA dataset.", "section": "5.1 Computer Vision and CelebA"}, {"figure_path": "ztwl4ubnXV/tables/tables_8_1.jpg", "caption": "Table 1: We report mean scores over the 14 gender independent CelebA labels [28]. Single task methods and FairMixup scores in the second and third blocks are from Zietlow et al. [17]. ERM is the baseline architecture run without fairness. OxonFair (optimizing for accuracy and difference in equal opportunity (DEO)), has better accuracy (\u2191) and DEO (\u2193) scores than any other fair method.", "description": "This table presents the results of a computer vision experiment using the CelebA dataset.  It compares the performance of OxonFair against several other fairness methods on 14 gender-independent attributes. The metrics used are accuracy and difference in equal opportunity (DEO).  The table shows that OxonFair, when optimizing for both accuracy and DEO, outperforms other methods in terms of both higher accuracy and lower DEO scores, indicating superior fairness.", "section": "5.1 Computer Vision and CelebA"}, {"figure_path": "ztwl4ubnXV/tables/tables_8_2.jpg", "caption": "Table 3: Multilingual Twitter dataset: Gender (2 groups: Male, Female).", "description": "This table presents the results of experiments conducted on the Multilingual Twitter dataset, focusing on gender as a protected attribute. It compares the performance of various bias mitigation techniques, including the proposed OxonFair method, across three evaluation metrics: F1 score, Balanced Accuracy, and Equal Opportunity Difference (DEO).  The \"Base\" row represents the performance of a baseline model without fairness considerations, while subsequent rows show results for different methods.  OxonFair is evaluated using different optimization objectives (Accuracy, F1, Balanced Accuracy) to illustrate its flexibility.", "section": "5.2 NLP and Toxic Content"}, {"figure_path": "ztwl4ubnXV/tables/tables_8_3.jpg", "caption": "Table 4: Jigsaw dataset: Religion (3 groups: Christian, Muslim, Other).", "description": "This table presents a comparison of different bias mitigation methods on the Jigsaw dataset, focusing on the prediction of religious affiliation.  The methods compared include a baseline (Base), Counterfactual Data Augmentation (CDA), Demographic Parity (DP), Equal Opportunity (EO), Dropout, and Rebalance.  The evaluation metrics used are F1 score, Balanced Accuracy, and Difference in Equal Opportunity (DEO). OxonFair results are presented with different optimization objectives (Accuracy, F1, Balanced Accuracy), with and without inferred group membership (*). The table showcases the performance of each method in mitigating bias and improving fairness metrics for predicting religious group membership.", "section": "Experimental Analysis"}, {"figure_path": "ztwl4ubnXV/tables/tables_19_1.jpg", "caption": "Table 5: The fairness measures in the review of [61]. All 9 group metrics that concern the decisions made by a classifier are supported by OxonFair.", "description": "This table compares the fairness measures defined by Verma and Rubin [61] with their corresponding names in the OxonFair toolkit and indicates whether they are supported by the Fairlearn toolkit.  It shows that OxonFair supports all 9 group-level fairness metrics from Verma and Rubin's work, while Fairlearn only supports a subset of them. The table also highlights that OxonFair handles both group-level and individual-level fairness definitions, while Fairlearn primarily focuses on group-level metrics.  The 'Not decision based' entries indicate fairness measures not directly related to classifier decisions.", "section": "Toolkit expressiveness"}, {"figure_path": "ztwl4ubnXV/tables/tables_20_1.jpg", "caption": "Table 6: The post-training fairness measures in the review of [93]. All measures are supported by OxonFair.", "description": "This table compares the post-training fairness measures reviewed in a paper by [93] with the OxonFair toolkit's capabilities. It lists various fairness metrics and indicates whether each metric is supported by the OxonFair toolkit and Fairlearn.  The table helps demonstrate the breadth of fairness measures that OxonFair can handle.", "section": "Additional Metrics"}, {"figure_path": "ztwl4ubnXV/tables/tables_20_2.jpg", "caption": "Table 7: Enforcing fairness for all definitions in [93] on COMPAS with inferred attributes. We enforce the fairness definitions with respect to three racial groups, African American, Caucasian, and Other - consisting of all other labelled ethnicities. There are a total 350 individuals labelled 'Other' in the test set, making most metrics of fairness unstable and difficult to enforce. Nonetheless, we improve on all metrics. For all metrics except disparate impact, we enforce that the score on train is below 2.5% and for disparate impact we enforce that the score on train is above 97.5%. XGBoost is used as the base classifier, and the dataset is split into 70% train and 30% test.", "description": "This table presents the results of enforcing various fairness definitions on the COMPAS dataset using inferred attributes.  It compares the original and updated performance metrics (measures and accuracy) after applying OxonFair's fairness-enhancing techniques. Despite challenges posed by limited data for the \"Other\" ethnicity group, OxonFair shows improvements across all metrics. Specific fairness constraints are enforced on the training data to control the level of fairness achieved.", "section": "Additional Metrics"}, {"figure_path": "ztwl4ubnXV/tables/tables_21_1.jpg", "caption": "Table 8: Results for XGBoost: Adult (sex)", "description": "This table compares the performance of three different methods (ERM, Minimax, and OxonFair) on the Adult dataset when using sex as the protected attribute.  It shows the minimum accuracy and overall accuracy achieved by each method on the training, validation, and test sets.  The results demonstrate that OxonFair achieves a better balance between minimum accuracy and overall accuracy compared to the other methods.", "section": "C.2 Utility Optimization"}, {"figure_path": "ztwl4ubnXV/tables/tables_27_1.jpg", "caption": "Table 9: Runtime comparison between [57] and OxonFair, on the Folktables dataset [101], using four racial groups. This represents the largest problem with the most groups reported in [57], and as such is the experiment where we would expect [57] to outperform OxonFair the most with respect to runtime. While [57] is faster if enforcing fairness to a known amount, e.g., maximizing accuracy subject to EOdds<0.05%, OxonFair remains faster for computing the entire frontier.", "description": "This table compares the runtime performance of the method proposed in the paper [57] and OxonFair when enforcing fairness constraints on the Folktables dataset. The experiment uses four racial groups which represents the largest problem in [57]. The result shows that [57] is faster when enforcing fairness up to a specific level (e.g., maximizing accuracy subject to EOdds<0.05%). However, OxonFair is faster when computing the entire fairness-accuracy frontier.", "section": "D Comparisons with specialist methods"}, {"figure_path": "ztwl4ubnXV/tables/tables_27_2.jpg", "caption": "Table 10: A comparison of Fairlearn, and OxonFair when enforcing Equalized Odds. We enforce fairness at 2% on validation to roughly match accuracy with Fairlearn. At this value, there is limited change between versions of Oxonfair and the multi-threshold approach obtains half the fairness violation of Fairlearn at similar accuracy.", "description": "This table compares the performance of OxonFair (single threshold and multi-threshold versions) and FairLearn in enforcing Equalized Odds fairness.  The results show that OxonFair's multi-threshold approach achieves similar accuracy to FairLearn but with significantly lower Equalized Odds violation.", "section": "D.2 Equalized odds without using group membership at test-time"}, {"figure_path": "ztwl4ubnXV/tables/tables_28_1.jpg", "caption": "Table 11: Hyperparameter details for the CelebA experiment.", "description": "This table lists the hyperparameters used in the CelebA experiment, including the learning rate, batch size, dropout rate, backbone architecture, weight decay, optimizer, and number of epochs.", "section": "E Computer Vision Experiments"}, {"figure_path": "ztwl4ubnXV/tables/tables_29_1.jpg", "caption": "Table 12: CelebA Attribute-level information from Ranaswamy et al. [28]. The columns are target attribute name, percentage of positive samples, skew. For example, Earrings has a skew of 0.97 towards g= - 1, that is, 97% of positive Earrings samples have gender expression label g= 1 (Female).", "description": "This table presents attribute-level information for the CelebA dataset, extracted from the work of Ranaswamy et al. [28].  Each row represents a different attribute (e.g., BigLips, BlondHair, etc.). The table shows the percentage of positive samples for each attribute and a skew value, which indicates the imbalance of the positive labels for each attribute (g = 1 representing female and g = -1 representing male). For instance, the \"Earrings\" attribute has a skew of 0.97 towards g=-1, meaning 97% of positive samples have a gender expression label of -1 (male). This table is useful for understanding the characteristics and biases present in the CelebA dataset.", "section": "5.1 Computer Vision and CelebA"}, {"figure_path": "ztwl4ubnXV/tables/tables_29_2.jpg", "caption": "Table 12: CelebA Attribute-level information from Ranaswamy et al. [28]. The columns are target attribute name, percentage of positive samples, skew. For example, Earrings has a skew of 0.97 towards g= - 1, that is, 97% of positive Earrings samples have gender expression label g= 1 (Female).", "description": "This table presents attribute-level information from the CelebA dataset, focusing on the characteristics of each attribute: name, percentage of positive samples, and skew. The skew indicates the distribution of positive samples with respect to a gender expression label (g=1 for female, g=-1 for male). This information helps in understanding the dataset's composition and potential biases.", "section": "E Computer Vision Experiments"}, {"figure_path": "ztwl4ubnXV/tables/tables_30_1.jpg", "caption": "Table 14: Extended Version of Table 2. Performance Comparison of Different Algorithmic Fairness Methods on the CelebA Test Set. Results monitor the mean Accuracy, Difference in Equal Opportunity (DEO) and the Minimum Group Minimum Label Accuracy across the attributes.", "description": "This table presents a detailed comparison of various algorithmic fairness methods on the CelebA test set.  It expands on Table 2 by providing a more comprehensive evaluation, including the mean accuracy, difference in equal opportunity (DEO), and the minimum group minimum label accuracy across multiple attributes within the dataset.  The comparison helps to illustrate the effectiveness of different methods in addressing fairness challenges in computer vision tasks.", "section": "5.1 Computer Vision and CelebA"}, {"figure_path": "ztwl4ubnXV/tables/tables_30_2.jpg", "caption": "Table 15: Performance comparison of Baseline, Adaptive g-SMOTE, g-SMOTE, OxonFair-DEO, and OxonFair-MGA on the training set. Reported are the means over the 32 labels selected by [17]. Methods marked * are reported from Zietlow et al. [17].", "description": "This table compares the performance of several fairness methods (Baseline, Adaptive g-SMOTE, g-SMOTE, OxonFair-DEO, and OxonFair-MGA) on a training set with 4 protected groups, measuring accuracy, minimum group accuracy and difference in equal opportunity.  The results are averages across 32 labels. Methods marked with * are from a cited paper.", "section": "5.1 Computer Vision and CelebA"}, {"figure_path": "ztwl4ubnXV/tables/tables_31_1.jpg", "caption": "Table 16: Multilingual Twitter corpus train/val/test statistics.", "description": "This table presents the number of samples used for training, validation, and testing in the Multilingual Twitter corpus experiment. The data is further broken down by gender (0 and 1), country (0 and 1), ethnicity (0 and 1), and age (0 and 1) for each language included in the experiment (English, Polish, Spanish, Portuguese, Italian).", "section": "F.1 Experimental Details"}, {"figure_path": "ztwl4ubnXV/tables/tables_31_2.jpg", "caption": "Table 1: We report mean scores over the 14 gender independent CelebA labels [28]. Single task methods and FairMixup scores in the second and third blocks are from Zietlow et al. [17]. ERM is the baseline architecture run without fairness. OxonFair (optimizing for accuracy and difference in equal opportunity (DEO)), has better accuracy (\u2191) and DEO (\u2193) scores than any other fair method.", "description": "This table presents the results of various fairness methods on the CelebA dataset.  It compares the performance of OxonFair against other methods (ERM, Uniconf, Domain Adv, Domain Disc, Domain Ind, g-SMOTE, g-SMOTE Adaptive, FairMixup), measuring accuracy and difference in equal opportunity (DEO).  OxonFair shows superior performance across different metrics compared to other fair methods.", "section": "5.1 Computer Vision and CelebA"}, {"figure_path": "ztwl4ubnXV/tables/tables_32_1.jpg", "caption": "Table 18: Multilingual Experiment.", "description": "This table shows the results of a multilingual experiment.  The experiment evaluated the performance of OxonFair across five languages (English, Polish, Spanish, Portuguese, and Italian) for hate speech detection. The table displays the original and updated values of Difference in Equal Opportunity (DEO) and Accuracy for each language.  The updated values represent the performance after applying OxonFair's fairness-enhancing techniques.", "section": "F.2 Hate Speech Detection Task"}, {"figure_path": "ztwl4ubnXV/tables/tables_32_2.jpg", "caption": "Table 20: Jigsaw religion data.", "description": "This table presents the number of positive and total samples for the Jigsaw religion dataset, categorized into three groups: Christian, Muslim, and Other (all other religions). The data is split into training, validation, and test sets.", "section": "F.3 Toxicity Classification Task"}, {"figure_path": "ztwl4ubnXV/tables/tables_32_3.jpg", "caption": "Table 21: Jigsaw race data.", "description": "This table shows the number of negative and positive samples for the training, validation, and testing sets for the Jigsaw dataset, broken down by race (Black and Asian).  It is used in the NLP experiments to evaluate the performance of the OxonFair toolkit in handling hate speech detection with multiple protected groups.", "section": "F.3 Toxicity Classification Task"}, {"figure_path": "ztwl4ubnXV/tables/tables_33_1.jpg", "caption": "Table 12: CelebA Attribute-level information from Ranaswamy et al. [28]. The columns are target attribute name, percentage of positive samples, skew. For example, Earrings has a skew of 0.97 towards g= - 1, that is, 97% of positive Earrings samples have gender expression label g= 1 (Female).", "description": "This table provides attribute-level information for the CelebA dataset, sourced from the work of Ramaswamy et al. [28].  Each row represents a different attribute, offering details on its type (gender-dependent, gender-independent, inconsistently labeled), the percentage of positive samples for that attribute, and its skew. The skew metric indicates the imbalance in the positive samples, relative to the gender expression label.  For instance, a skew value of 0.97 towards g=-1 means 97% of positive samples have a gender expression label of -1. This table is valuable for understanding the characteristics and potential biases within the CelebA dataset, especially regarding gender representation.", "section": "5.1 Computer Vision and CelebA"}, {"figure_path": "ztwl4ubnXV/tables/tables_33_2.jpg", "caption": "Table 23: Jigsaw dataset: Race (2 groups: Black, Asian).", "description": "This table presents the results of a fairness experiment on the Jigsaw dataset, focusing on the race attribute with two groups: Black and Asian. It compares several bias mitigation methods against a baseline model, evaluating performance using F1 score, balanced accuracy, and difference in equal opportunity (DEO). The table shows how OxonFair performs compared to other methods in terms of accuracy and fairness.", "section": "5.2 NLP and Toxic Content"}]