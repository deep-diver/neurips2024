[{"type": "text", "text": "Rethinking Patch Dependence for Masked Autoencoders ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 In this work, we examine the impact of inter-patch dependencies in the decoder of   \n2 masked autoencoders (MAE) on representation learning. We decompose the decod  \n3 ing mechanism for masked reconstruction into self-attention between mask tokens   \n4 and cross-attention between masked and visible tokens. Our findings reveal that   \n5 MAE reconstructs coherent images from visible patches not through interactions   \n6 between patches in the decoder but by learning a global representation within the   \n7 encoder. This discovery leads us to propose a simple visual pretraining framework:   \n8 cross-attention masked autoencoders (CrossMAE). This framework employs only   \n9 cross-attention in the decoder to independently read out reconstructions for a small   \n0 subset of masked patches from encoder outputs, yet it achieves comparable or   \n11 superior performance to traditional MAE across models ranging from ViT-S to   \n2 ViT-H. By its design, CrossMAE challenges the necessity of interaction between   \n13 mask tokens for effective masked pretraining. Code is available here. ", "page_idx": 0}, {"type": "text", "text": "14 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "15 Masked image modeling [46, 30, 61, 4] has emerged as a pivotal unsupervised learning technique   \n16 in computer vision. One such recent work following this paradigm is masked autoencoders (MAE):   \n17 given only a small, random subset of visible image patches, the model is tasked to reconstruct the   \n18 missing pixels. By operating mostly on this small subset of visible tokens, MAE can efficiently   \n19 pre-train high-capacity models on large-scale vision datasets, demonstrating impressive results on a   \n20 wide array of downstream tasks [33, 38, 49].   \n21 The MAE framework employs self-attention across the entire model for self-supervised reconstruction   \n22 tasks. In this setup, both masked and visible tokens engage in self-attention, not just with each other   \n23 but also with themselves, aiming to generate a holistic and context-aware representation. However,   \n24 the masked tokens inherently lack information. Intuitively, facilitating information exchange among   \n25 adjacent masked tokens should enable the model to synthesize a more coherent image, thereby   \n26 accomplishing the task of masked reconstruction and improving representation learning. A question   \n27 arises, though: Is this truly the case?   \n28 We decompose the decoding process of each mask token into two parallel components: self-attention   \n29 with other mask tokens, as well as cross-attention to the encoded visible tokens. If MAE relies on   \n30 the self-attention with other mask tokens, its average should be on par with the cross-attention. Yet,   \n31 the quantitative comparison in Figure 1.(b) shows the magnitude of mask token-to-visible token   \n32 cross-attention (1.42) in the MAE decoder evaluated over the entire ImageNet validation set far   \n33 exceeds that of mask token-to-mask token self-attention (0.39).   \n34 This initial observation prompts two questions: 1) Is the self-attention mechanism among mask   \n35 tokens in the decoder necessary for effective representation learning? 2) If not, can each patch be   \n36 independently read out from the encoder output, allowing the reconstruction of only a small subset of   \n37 masked patches, which in turn, accelerates the pretraining without performance degradation? ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "wZ5kEOCTce/tmp/27ca50b799dd5db8a4b58211864aff3598acb86a27207c294dc4e4b01021bd10.jpg", "img_caption": ["Figure 1: Method Overview. (A) Masked autoencoder (MAE) starts by masking random patches of the input image. (B) To reconstruct a mask token (marked by the blue star), MAE attends to both the masked tokens (B.Left) and the visible tokens (B.Right). A quantitative comparison over the ImageNet validation set shows that the masked tokens in MAE disproportionally attend to the visible tokens (1.42 vs 0.39), questioning the necessity of attention within mask tokens. (C) We propose CrossMAE, the masked patches are reconstructed from only the cross attention between the masked tokens and the visible tokens. Surprisingly, CrossMAE attains the same or better performance than MAE on ImageNet classification and COCO instance segmentation. "], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "wZ5kEOCTce/tmp/40676a622008eacb06dfa228cb0f1c695986040f8fe960449c4c0133692ad655.jpg", "img_caption": ["Figure 2: Example reconstructions of ImageNet validation images. For each set of 5 images, from left to right, are the original image, masked image with a mask ratio of $75\\%$ , MAE [30], CrossMAE (trained to reconstruct $25\\%$ of image tokens, or 1/3 of the mask tokens), and CrossMAE (trained to reconstruct all masked tokens). Since CrossMAE does not reconstruct them, all model outputs have the visible patches overlaid. Intriguingly, CrossMAE, when trained for partial reconstruction, can decode all mask tokens in one forward pass (shown above), indicating that the encoder rather than the decoder effectively captures global image information in its output tokens. Its comparable reconstruction quality to full-image-trained models suggests that full-image reconstruction might not be essential for effective representation learning. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "38 In addressing these questions, we introduce CrossMAE, which diverges from MAE in three ways: ", "page_idx": 1}, {"type": "text", "text": "39 1. Cross-attention for decoding. Rather than passing a concatenation of mask and visible   \n40 tokens to a self-attention decoder, CrossMAE uses mask tokens as queries to read out the masked   \n41 reconstructions from the visible tokens in a cross-attention decoder. In this setting, mask tokens   \n42 incorporate information from the visible tokens but do not interact with other mask tokens, thereby   \n43 reducing the sequence length for the decoder and cutting down computational costs.   \n44 2. Independent partial reconstruction. With self-attention removed, the decoding of each mask   \n45 token, based on the encoded features from visible tokens, becomes conditionally independent. This   \n46 enables the decoding of only a fraction of masked tokens rather than the entire image.   \n47 3. Inter-block attention. Due to the separation of visible and mask tokens, we can use features   \n48 from different encoder blocks for each decoder block. Empirically, we find solely relying on the last   \n49 encoder feature map for reconstruction, the design present in MAE, hurts feature learning. We propose   \n50 a lightweight inter-block attention mechanism that allows the CrossMAE decoder to leverage a mix   \n51 of low-level and high-level feature maps from the encoder, improving the learned representation.   \n52 The analysis performed on CrossMAE led to a novel way to understand MAE. Even though the   \n53 patches to be reconstructed are independently decoded, our findings demonstrate that coherent   \n54 reconstruction for each masked patch can be independently read out from the encoder output, without   \n55 any interactions among masked tokens in the decoder for consistency (Figure 2). Furthermore, the   \n56 downstream performance of the model remains robust even without these interactions (Figure 1.(c),   \n57 Tables 1 and 2). Both pieces of evidence confirm that the encoder\u2019s output features already encapsulate   \n58 the necessary global context for image reconstruction, while the decoder simply performs a readout   \n59 from the encoder output to reconstruct the pixels at the location of each patch. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "60 To sum up, our main contributions are the following: ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "61 1. We present a novel understanding of MAE. Our findings show that MAE reconstructs coherent   \n62 images from visible patches not through interactions between patches to be reconstructed in the   \n63 decoder but by learning a global representation within the encoder. This is evidenced by the model\u2019s   \n64 ability to generate coherent images and maintain robust downstream performance without such   \n65 interactions, indicating the encoder effectively captures global image information.   \n66 2. We advocate replacing self-attention layers with a simple cross-attention readout function.   \n67 Given our discovery that the encoder in MAE already captures a comprehensive global representation,   \n68 we propose replacing self-attention layers in the decoder with a more efficient information readout   \n69 function. Specifically, we suggest utilizing cross-attention to aggregate the output tokens of the   \n70 encoder into each input token within the decoder layers independently, thereby eliminating the need   \n71 for token-to-token communication within the decoder.   \n72 3. CrossMAE achieves comparable or superior performance with reduced computational   \n73 costs in image classification and instance segmentation compared to MAE on vision transformer   \n74 models ranging from ViT-S to ViT-H. Code is available here. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "75 2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "76 2.1 Self-Supervised Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "77 In self-supervised representation learning, a model trains on a pretext task where the supervision   \n78 comes from the input data itself without labels. Contrastive learning methods learn representations   \n79 by contrasting positive and negative samples, such as SimCLR [11], CPC [44], MoCo [29, 12, 13],   \n80 CLD [59] and SwAV [7]. Additionally, in BYOL [26], iBOT [65], DINO [8], DINOv2 [45], and   \n81 MaskAlign [62] make a student model to imitate a teacher model without negative pairs.   \n82 Generative modeling, focusing on acquiring a generative model capable of capturing the underlying   \n83 data distribution, is an alternative method for self-supervised learning. VAE/GAN [35] merges the   \n84 strengths of variational autoencoders and generative adversarial networks to acquire disentangled   \n85 representations of data. PixelCNN, PixelVAE, and PixelTransformer [55, 27, 54] generate images   \n86 pixel by pixel, taking into account the context of previously generated pixels. Masked modeling, a   \n87 large subclass of generative modeling, is discussed in the following subsection. After the pre-training   \n88 stage, these generative models can be finetuned for many downstream applications. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "89 2.2 Masked Modeling ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "90 Masked modeling learns representations by reconstructing a masked portion of the input. Pioneering   \n91 works in natural language processing (NLP) present various such pretraining objectives. BERT [19]   \n92 and its extensions [41, 34] use a bidirectional transformer and present few-shot learning capabil  \n93 ities from masked language modeling. GPT [47, 48, 5], uses autoregressive, causal masking and   \n94 demonstrates multi-task, few-shot, and in-context learning capabilities.   \n95 Early works in computer vision, such as Stacked Denoising Autoencoders [57] and Context En  \n96 coder [46], investigated masked image modeling as a form of denoising or representation learning.   \n97 Recently, with the widespread use of transformer [20] as a backbone vision architecture, where   \n98 images are patchified and tokenized as sequences, researchers are interested in how to transfer the   \n99 success in language sequence modeling to scale vision transformers. BEiT [3], MAE [30], and Sim  \n100 MIM [61] are a few of the early works that explored BERT-style pretraining of vision transformers.   \n101 Compared to works in NLP, both MAE and SimMIM [30, 61] find that a much higher mask ratio   \n102 compared to works in NLP is necessary to learn good visual representation. Many recent works   \n103 further extend masked pretraining to hierarchical architectures [61, 40] and study data the role of data   \n104 augmentation [9, 21]. Many subsequent works present similar successes of masked pretraining for   \n105 video [52, 58, 22, 28], language-vision and multi-modal pretraining [1, 39, 23] and for learning both   \n106 good representations and reconstruction capabilities [60, 37].   \n107 However, BERT-style pretraining requires heavy use of self-attention, which makes computational   \n108 complexity scale as a polynomial of sequence length. PixelTransformer [54] and DiffMAE [60] both   \n109 use cross-attention for masked image generation and representation learning. Siamese MAE [28]   \n110 uses an asymmetric masking pattern and decodes frames of a video condition on an earlier frame. In   \n111 these settings, all masked patches are reconstructed. In this work, we investigate if learning good   \n112 features necessitates high reconstruction quality and if the entire image needs to be reconstructed to   \n113 facilitate representation learning. PCAE [36] progressively discards redundant mask tokens through   \n114 its network, leading to a few tokens for reconstruction. VideoMAEv2 [58] concatenates randomly   \n115 sampled masked tokens with visible tokens and uses self-attention to reconstruct the masked patches.   \n116 In comparison, we minimally modify MAE with a cross-attention-only decoder and masked tokens   \n117 are decoded in a conditional independent way. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "118 2.3 Applications of Cross-Attention ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "119 In addition to the prevalent use of self-attention in computer vision, cross-attention has shown to be a   \n120 cost-effective way to perform pooling from a large set of visible tokens. Intuitively, cross-attention   \n121 can be seen as a parametric form of pooling, which learnably weighs different features. Touvron   \n122 et al. [53] replace mean pooling with cross-attention pooling and find improvement in ImageNet   \n123 classification performance. Jaegle et al. [32] uses cross-attention to efficiently process large volumes   \n124 of multi-modal data. Cross-attention is also widely used for object detection. Carion et al. [6] utilizes   \n125 query tokens as placeholders for potential objects in the scene. Cheng et al. [16, 15] further extend   \n126 this concept by introducing additional query tokens to specifically tackle object segmentation in   \n127 addition to the query tokens for object detection. Distinct from thes prior works, we are interested the   \n128 role of cross-anttention for representation learning in a self-supervised manner. ", "page_idx": 3}, {"type": "text", "text": "129 3 CrossMAE ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "130 We start with an overview of vanilla masked autoencoders in Section 3.1. Next, in Section 3.2, we   \n131 introduce the use of cross-attention in place of self-attention in the decoder for testing the necessity   \n132 of interaction between mask tokens for representation learning. In Section 3.3, we discuss how   \n133 eliminating self-attention in the decoding process enables us to reconstruct only a subset of masked   \n134 tokens, leading to faster pretraining. Finally, Section 3.4 presents our inter-block attention mechanism,   \n135 which allows decoder blocks to leverage varied encoder features. ", "page_idx": 3}, {"type": "text", "text": "136 3.1 Preliminaries: Masked Autoencoders ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "137 Masked Autoencoders (MAE) [30] pretrain Vision Transformers (ViTs) [20]. Each image input is   \n138 first patchified, and then a random subset of the patches is selected as the visible patches. As depicted   \n139 in Figure 3, the visible patches, concatenated with a learnable class token [CLS], are subsequently   \n140 fed into the ViT encoder, which outputs a set of feature latents. The latent vectors, concatenated with   \n141 the sum of the positional embeddings of the masked patches and the learnable mask token, are passed   \n142 into the MAE decoder. The decoder blocks share the same architecture as the encoder blocks (i.e.,   \n143 both are transformer blocks with self-attention layers). Note that the number of tokens fed into the   \n144 decoder is the same length as the original input, and the decoding process assumes that the decoded   \n145 tokens depend on both visible and masked tokens. Decoder outputs pass through a fully connected   \n146 layer per patch for image reconstruction. After the reconstruction is generated, the loss is applied   \n147 only to the masked positions, while the reconstructions for visible spatial locations are discarded.   \n148 Recall in Sec. 1 we measure the mean attention value across all attention maps over the ImageNet   \n149 validation set to study the properties of MAE. We grouped the attention values by cross-attention   \n150 and self-attention between visible and masked tokens. We observed that in the decoding process   \n151 of an MAE, mask tokens attend disproportionately to the class token and the visible tokens (see   \n152 Figure 1.(b)). This motivates us to make design decisions and conduct experiments specifically to   \n153 answer the following question: Can we simplify the decoding process by eliminating self-attention   \n154 among masked tokens without compromising the model\u2019s ability to generate coherent images and   \n155 perform well on downstream tasks? ", "page_idx": 3}, {"type": "image", "img_path": "wZ5kEOCTce/tmp/5b1eaad02033c2a2875737b7291ac60eff0f9b17f5d35efb34aac4aec1db13b1.jpg", "img_caption": ["Figure 4: Overview of CrossMAE. (a) The vanilla version of CrossMAE uses the output of the last encoder block as the keys and queries for cross-attention. The first decoder block takes the sum of mask tokens and their corresponding positional embeddings as queries, and subsequent layers use the output of the previous decoder block as queries to reconstruct the masked patches. (b) Unlike the decoder block in [56], the cross-attention decoder block does not contain self-attention, decoupling the generation of different masked patches. (c) CrossMAE\u2019s decoder blocks can leverage low-level features for reconstruction via inter-block attention. It weighs the intermediate feature maps, and the weighted sum of feature maps is used as the key and value for each decoder block. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "156 3.2 Reconstruction with Cross-Attention ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "157 To address this question, we substitute the self-attention mechanism in the decoder blocks with   \n158 cross-attention, using it as a readout function to decode the latent embedding from the encoder to raw   \n159 pixel values. Specifically, the decoder employs multi-head cross-attention where the queries are the   \n160 output from previous decoder blocks (or the sum of position embedding of the masked patches and   \n161 mask token for the first decoder block). The keys and values are from the encoded features.   \n162 In the most basic CrossMAE, the output from the final encoder block is used as the key and value   \n163 tokens for all layers of the decoder, as illustrated in Fig. 4(a). Further exploration in Sec.3.4 reveals   \n164 that utilizing a weighted mean of selected encoder feature maps can be beneficial. The residual   \n165 connections in each decoder block enable iterative refinement of decoded tokens as they progress   \n166 through decoder blocks.   \n167 Diverging from the original transformer architecture [56], our decoder omits the causal self-attention   \n168 layer before the introduction of multi-head cross-attention. This elimination, coupled with the fact   \n169 that layer normalization and residual connections are only applied along the feature axis but not   \n170 the token axis, enables the independent decoding of tokens. This design choice is evaluated in the   \n171 ablation study section to determine its impact on performance.   \n172 Given the disparity in the dimensions of the encoder and decoder, MAE adapts the visible features to   \n173 the decoder\u2019s latent space using an MLP. However, in CrossMAE, as encoder features are integrated   \n174 at various decoder blocks, we embed the projection within the multi-head cross-attention module.   \n175 Cross-attention layers serve as a readout function that decodes the global representation provided   \n176 in the encoder\u2019s output tokens to the pixel values within each patch to be reconstructed. However,   \n177 CrossMAE does not restrict the architecture to a single cross-attention block. Instead, we stack   \n178 multiple cross-attention decoder blocks in a manner more akin to the traditional transformer [56]. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "179 3.3 Partial Reconstruction ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "180 The fact that CrossMAE uses cross-attention rather than self-attention in the decoder blocks brings   \n181 an additional benefit over the original MAE architecture. Recall that mask tokens are decoded inde  \n182 pendently and thus there is no exchange of information between them, to obtain the reconstructions   \n183 at a specific spatial location, CrossMAE only needs to pass the corresponding mask tokens to the   \n184 cross-attention decoder. This allows partial reconstruction in contrast to the original full-image   \n185 reconstruction in the MAE architecture which needs to pass all the masked tokens as the input of the   \n186 decoder blocks due to the existence of self-attention in the decoder blocks.   \n187 To address the second question in Sec. 3.1, rather than decoding the reconstruction for all masked   \n188 locations, we only compute the reconstruction on a random subset of the locations and apply the loss   \n189 to the decoded locations. Specifically, we name the ratio of predicted tokens to all image tokens as   \n190 prediction ratio $(\\gamma)$ , and the mask ratio $(p)$ . Then the prediction ratio is bounded between $\\gamma\\in(0,p]$ .   \n191 Because we are sampling within the masked tokens uniformly at random and the reconstruction   \n192 loss is a mean square error on the reconstructed patches, the expected loss is the same as in MAE,   \n193 while the variance is $(p/\\gamma)$ times larger than the variance in MAE. Empirically, we find that scaling   \n194 the learning rate of MAE $(\\beta)$ to match the variance (i.e. setting the learning rate as $\\gamma\\beta/p)$ )) helps   \n195 with model performance. Since cross-attention has linear complexity with respect to the number of   \n196 masked tokens, this partial reconstruction paradigm decreases computation complexity. Empirically,   \n197 we find that the quality of the learned representations is not compromised by this approach. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "198 3.4 Inter-block Attention ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "199 MAE combines the feature of the last encoder block with mask tokens as the input to the self-attention   \n200 decoder, which creates an information bottleneck by making early encoder features inaccessible   \n201 for the decoder. In contrast, CrossMAE\u2019s cross-attention decoder decouples queries from keys and   \n202 values. This decoupling allows different cross-attention decoder blocks to take in feature maps from   \n203 different encoder blocks. This added degree of flexibility comes with a design choice for selecting   \n204 encoder features for each decoder block. One naive choice is to give the feature of the ith encoder   \n205 block to the last ith decoder (e.g., feeding the feature of the first encoder to the last decoder), in a   \n206 U-Net-like fashion. However, this assumes the decoder\u2019s depth matches the depth of the encoder,   \n207 which is not the case for MAE or CrossMAE.   \n208 Instead of manually matching each decoder block with an encoder feature map, we make the selection   \n209 learnable and propose inter-block attention for feature fusion for each decoder block (Figure 4(c)).   \n210 Analogous to the inter-patch cross-attention that takes a weighted sum of the visible token embeddings   \n211 across the patch dimensions to update the embeddings of masked tokens, inter-block attention takes   \n212 a weighted sum of the visible token embeddings across different input blocks at the same spatial   \n213 location to fuse the input features from multiple blocks into one feature map for each decoder block.   \n214 Concretely, each decoder block takes a weighted linear combination of encoder feature maps $\\{f_{i}\\}$ as   \n215 keys and values. Specifically, for each key/value token $t_{k}$ in decoder block $k$ in a model with encoder   \n216 depth $n$ , we initialize a weight $w^{k}\\in\\mathcal{R}^{n}\\sim\\mathcal{N}(0,1/n)$ . Then $t_{k}$ is defined as ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\nt_{k}=\\sum_{j=1}^{n}w_{j}^{k}f_{j}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "$\\boldsymbol{\\mathfrak{z}}_{18}^{17}$ In addition to feature maps from different encoder blocks, we also include the inputs to the first 219 encoder block to allow the decoder to leverage more low-level information to reconstruct the original ", "page_idx": 5}, {"type": "table", "img_path": "wZ5kEOCTce/tmp/b31191f2547037201c4a983ee40c656a3e53035020e74143162e425ff9be0bec.jpg", "table_caption": [], "table_footnote": ["Table 1: ImageNet-1K classification accuracy. CrossMAE performs on par or better than MAE. All experiments are run with 800 epochs. The best results are in bold while the second best results are underlined. "], "page_idx": 6}, {"type": "table", "img_path": "wZ5kEOCTce/tmp/1c1b7b8b4b03ce159de2d419d5bc075d4bc3afeb6e67f06295efec4e11599753.jpg", "table_caption": [], "table_footnote": ["Table 2: COCO instance segmentation. Compared to previous masked visual pretraining works, CrossMAE performs favorably on object detection and instance segmentation tasks. "], "page_idx": 6}, {"type": "text", "text": "220 image. We can select a subset of the feature maps from the encoder layers instead of all feature maps.   \n221 This reduces the computation complexity of the system. We ablate this in Table 3d.   \n222 We show that using the weighted features rather than simply using the features from the last block   \n223 greatly improves the performance of CrossMAE. Intriguingly, in the process of learning to achieve   \n224 better reconstructions, early decoder blocks tend to prioritize information from later encoder blocks,   \n225 while later decoder blocks focus on earlier encoder block information, as demonstrated in Section 4.5. ", "page_idx": 6}, {"type": "text", "text": "226 4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "227 We perform self-supervised pretraining on ImageNet-1K, following MAE [30]\u2019s hyperparameter   \n228 settings, only modifying the learning rate and decoder depth. The hyperparameters were initially   \n229 determined on ViT-Base and then directly applied to ViT-Small, ViT-Large, and ViT-Huge. Both   \n230 CrossMAE and MAE are trained for 800 epochs. We provide implementation details and more   \n231 experiments in the appendix. ", "page_idx": 6}, {"type": "text", "text": "232 4.1 ImageNet Classification ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "233 Setup. The model performance is evaluated with end-to-end fine-tuning, with top-1 accuracy used   \n234 for comparison. Same as in Figure. 2, we compare two versions of CrossMAE: one with a prediction   \n235 ratio of $25\\%$ (1/3 of the mask tokens) and another with $75\\%$ (all mask tokens). Both models are   \n236 trained with a mask ratio of $75\\%$ and a decoder depth of 12.   \n237 Results. As shown in Table 1, CrossMAE outperforms vanilla MAE using the same ViT-B encoder   \n238 in terms of fine-tuning accuracy. This shows that replacing the self-attention with cross-attention   \n239 does not degrade the downstream classification performance of the pre-trained model. Moreover,   \n240 CrossMAE outperforms other self-supervised and masked image modeling baselines, e.g., DINO [8],   \n241 MoCo v3 [14], BEiT [3], and MultiMAE [2]. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "242 4.2 Object Detection and Instance Segmentation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "243 Setup. We additionally evaluate models pretrained with CrossMAE for object detection and instance   \n244 segmentation, which require deeper spatial understanding than ImageNet classification. Specifically,   \n245 we follow ViTDet [38], a method that leverages a Vision Transformer backbone for object detection   \n246 and instance segmentation. We report box AP for object detection and mask AP for instance   \n247 segmentation, following MAE [30]. We compare against supervised pre-training, MoCo-v3 [14],   \n248 BEiT [4], and MAE [30].   \n249 Results. As listed in Table 2, CrossMAE, with the default $75\\%$ prediction ratio, performs better   \n250 compared to these baselines, including vanilla MAE. This suggests that similar to MAE, CrossMAE   \n251 performance on ImageNet positively correlates with instance segmentation. Additionally, Cross  \n252 MAE\u2019s downstream performance scales similarly to MAE as the model capacity increases from ViT-B   \n253 to ViT-L. This observation also supports our hypothesis that partial reconstruction is suprisingly   \n254 sufficient for learning dense visual representation. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "wZ5kEOCTce/tmp/e11b78e19f86af9c1c8388d5b4e606c0d6790bd503a8724798cdaee06e73d059.jpg", "table_caption": ["(c) Prediction ratio. CrossMAE performs well even when only a fraction of mask tokens are reconstructed. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "(a) Attention type in decoder (b) Mask ratio. CrossMAE has blocks. Adding back self-attention consistent performance across high between mask tokens does not im- mask ratios. prove performance. ", "page_idx": 7}, {"type": "image", "img_path": "wZ5kEOCTce/tmp/ac167f75b7d7c2b98a1b776ec1aaa32242a9a9e535651fc1ebb62c518424c3aa.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "wZ5kEOCTce/tmp/0faeca3922db245b5d1dd3d2a41c56f7fd7f3d0675898262d2f4db09fee3bd26.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "(d) Inter-block attention. A com- (e) Decoder depth. CrossMAE (f) Input resolution. CrossMAE bination of six select encoder fea- performance scales with decoder scales to longer input sequences. ture maps is best. depth. ", "page_idx": 7}, {"type": "text", "text": "Table 3: Ablations on CrossMAE. We report fine-tuning performance on ImageNet-1K classification with 400 epochs (i.e., half of the full experiments) with ViT-B/16. MAE performance is reproduced using the official MAE code. Underline indicates the default setting for CrossMAE. Bold indicates the best hyperparameter among the tested ones. 1 feature map fused (row 1, Table 3(d)) indicates using only the feature from the last encoder block. We use $25\\%$ prediction ratio for both settings in Table 3(f) to accelerate training. ", "page_idx": 7}, {"type": "text", "text": "255 4.3 Ablations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "256 Cross-Attention vs Self-Attention. As shown in Table 3a, CrossMAE, with its cross-attention  \n257 only decoder, outperforms vanilla MAE in downstream tasks as noted in Section 4.1. Additionally,   \n258 combining cross-attention with self-attention does not enhance fine-tuning performance, indicating   \n259 that cross-attention alone is adequate for effective representation learning.   \n260 Mask Ratio and Prediction Ratio. In our experiments with different mask and prediction ratios (i.e.,   \n261 the ratio of mask tokens to all tokens and the ratio of reconstructed tokens to all tokens, respectively)   \n262 (see Table 3b and Table 3c), we found that our method\u2019s performance is not significantly affected by   \n263 variations in the number of masked tokens. Notably, CrossMAE effectively learns representations   \n264 by reconstructing as few as $15\\%$ of tokens, compared to the $100\\%$ required by vanilla MAE, with   \n265 minimal impact on downstream fine-tuning performance, which shows that partial reconstruction is   \n266 sufficient for effective representation learning.   \n267 Inter-block Attention. Our ablation study, detailed in Table 3d, explored the impact of varying the   \n268 number of encoder feature maps in our inter-block attention mechanism. We found that using only   \n269 the last feature map slightly lowers performance compared to using all 12. However, even a partial   \n270 selection of feature maps improves CrossMAE\u2019s performance, with the best results obtained using 6   \n271 feature maps. This indicates that CrossMAE does not require all features for optimal performance.   \n272 Decoder Depth. Table 3e shows that a 12-block decoder slightly improves performance compared   \n273 to shallower ones. Remarkably, CrossMAE achieves similar results to MAE with just one decoder   \n274 block, demonstrating its efficiency. Our experiments in Figure 7 that models with lower prediction   \n275 ratios benefit more from deeper decoders.   \n276 Input Resolution. We extend CrossMAE to longer token lengths by increasing the image resolution   \n277 with constant patch size. Escalating the resolution from 224 to 448 increases the token length from   \n278 197 to 785, challenging the scalability of current approaches. Thus, we opt for a CrossMAE variant   \n279 with a $25\\%$ prediction ratio. In Table 3f, we observe that the classification accuracy positively   \n280 correlates with the input resolution, indicating that CrossMAE can scale to long input sequences. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "281 4.4 Training Throughput and Memory Utilization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "282 Due to partial reconstruction and confining attention to between mask tokens and visible tokens,   \n283 CrossMAE improves pre-training efficiency over MAE. Results in Table 10 show that the FLOPs   \n284 reduction does translate to an $1.54\\times$ training throughput and at least $50\\%$ reduction in GPU memory   \n285 utilization compared to MAE. ", "page_idx": 7}, {"type": "image", "img_path": "wZ5kEOCTce/tmp/f206f90690a9324569d8d8bea1fd0172f63f375a6d843217e9b40691ba3f32e1.jpg", "img_caption": ["Figure 5: We visualize the output of each decoder block. (a-b) Different decoder blocks play different roles in the reconstruction, with most details emerging at later decoder blocks, which confirms the motivation for inter-block attention. (c) Visualizations of inter-block attention shows that different decoder blocks indeed attend to feature from different encoder blocks, with later blocks focusing on earlier encoder features to achieve reconstruction. The reconstructions are unnormalized w.r.t ground truth mean and std for each patch. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "286 4.5 Visualizations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "287 Visualizing Per-block Reconstruction. Rather than only visualizing the final reconstruction, we   \n288 B have two key observations that allow us to visualize the work performed by each decoder block:   \n289 1) Transformer blocks have skip connections from their inputs to outputs. 2) The final decoder   \nblock\u2019s output goes through a linear reconstruction head to produce the reconstruction. As detailed in   \n291 Appendix D, we can factor out each block\u2019s contribution in the final reconstruction with linearity.   \n292 This decomposition allows expressing the reconstruction as an image stack, where summing up all the   \n293 levels gives us the final reconstruction. As shown in Figure 5 (a,b), we observe that different decoder   \n294 blocks play different roles in reconstruction, with most details emerging at later decoder blocks. This   \n295 justifies the need for low-level features from early encoder blocks, motivating inter-block attention.   \n296 Visualizing Inter-block Attention Maps. As shown in the visualizations of the attention maps of   \n297 inter-block attention in 5(c), CrossMAE naturally leverages the inter-block attention to allow the later   \n298 decoder blocks to focus on earlier encoder features to achieve reconstruction and allow the earlier   \n299 decoder blocks to focus on later encoder features. This underscores the necessity for different decoder   \n300 blocks to attend to different encoder features, correlating with the performance improvements when   \n301 inter-block attention is used. ", "page_idx": 8}, {"type": "text", "text": "302 5 Discussion and Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "303 In our study, we present a novel understanding of MAE, demonstrating that coherent image recon  \n304 struction is achieved not through interactions between patches in the decoder but by learning a global   \n305 representation within the encoder. Based on this insight, we propose replacing self-attention layers   \n306 in the decoder with a simple readout function, specifically utilizing cross-attention to aggregate   \n307 encoder outputs into each input token within the decoder layers independently. This approach, tested   \n308 across models ranging from ViT-S to ViT-H, achieves comparable or better performance in image   \n309 classification and instance segmentation with reduced computational requirements, showcasing the   \n310 potential for more efficient and scalable visual pretraining methods. Our findings underscore the   \n311 efficacy of the encoder\u2019s global representation learning, paving the way for streamlined decoder   \n312 architectures in future MAE implementations. CrossMAE\u2019s efficiency and scalability demonstrate   \n313 potential for large-scale visual pretraining, particularly on underutilized in-the-wild video datasets.   \n314 However, our work has not yet explored scaling to models larger than ViT-H, the largest model   \n315 examined in MAE, leaving this for future research. ", "page_idx": 8}, {"type": "text", "text": "316 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "317 [1] Roman Bachmann, David Mizrahi, Andrei Atanov, and Amir Zamir. Multimae: Multi-modal multi-task   \n318 masked autoencoders. arXiv:2204.01678, 2022.   \n319 [2] Roman Bachmann, David Mizrahi, Andrei Atanov, and Amir Zamir. Multimae: Multi-modal multi-task   \n320 masked autoencoders. In European Conference on Computer Vision, pages 348\u2013367. Springer, 2022.   \n321 [3] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv   \n322 preprint arXiv:2106.08254, 2021.   \n323 [4] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. In ICLR, 2022.   \n324 [5] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind   \n325 Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.   \n326 2020.   \n327 [6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey   \n328 Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision,   \n329 pages 213\u2013229. Springer, 2020.   \n330 [7] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsu  \n331 pervised learning of visual features by contrasting cluster assignments. Advances in neural information   \n332 processing systems, 33:9912\u20139924, 2020.   \n333 [8] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand   \n334 Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF   \n335 international conference on computer vision, pages 9650\u20139660, 2021.   \n336 [9] Kai Chen, Zhili Liu, Lanqing Hong, Hang Xu, Zhenguo Li, and Dit-Yan Yeung. Mixed autoencoder for   \n337 self-supervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer   \n338 Vision and Pattern Recognition (CVPR), pages 22742\u201322751, 2023.   \n339 [10] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.   \n340 Generative pretraining from pixels. 2020.   \n341 [11] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for   \n342 contrastive learning of visual representations. In International conference on machine learning, pages   \n343 1597\u20131607. PMLR, 2020.   \n344 [12] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive   \n345 learning. arXiv preprint arXiv:2003.04297, 2020.   \n346 [13] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision   \n347 transformers, 2021.   \n348 [14] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision   \n349 transformers. arXiv preprint arXiv:2104.02057, 2021.   \n350 [15] Bowen Cheng, Alexander G. Schwing, and Alexander Kirillov. Per-pixel classification is not all you need   \n351 for semantic segmentation. 2021.   \n352 [16] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked  \n353 attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF conference   \n354 on computer vision and pattern recognition, pages 1290\u20131299, 2022.   \n355 [17] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data   \n356 augmentation with a reduced search space. arxiv e-prints, page. arXiv preprint arXiv:1909.13719, 4, 2019.   \n357 [18] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023.   \n358 [19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec  \n359 tional transformers for language understanding. 2019.   \n360 [20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas   \n361 Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth   \n362 16x16 words: Transformers for image recognition at scale. In ICLR, 2020.   \n363 [21] Yuxin Fang, Li Dong, Hangbo Bao, Xinggang Wang, and Furu Wei. Corrupted image modeling for   \n364 self-supervised visual pre-training. In The Eleventh International Conference on Learning Representations,   \n365 2023.   \n366 [22] Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, and Kaiming He. Masked autoencoders as spatiotemporal   \n367 learners. In Advances in Neural Information Processing Systems, 2022.   \n368 [23] Xinyang Geng, Hao Liu, Lisa Lee, Dale Schuurams, Sergey Levine, and Pieter Abbeel. Multimodal   \n369 masked autoencoders learn transferable representations. arXiv preprint arXiv:2205.14204, 2022.   \n370 [24] Priya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew   \n371 Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour.   \n372 arXiv:1706.02677, 2017.   \n373 [25] Priya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew   \n374 Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv   \n375 preprint arXiv:1706.02677, 2017.   \n376 [26] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya,   \n377 Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your   \n378 own latent-a new approach to self-supervised learning. Advances in neural information processing systems,   \n379 33:21271\u201321284, 2020.   \n380 [27] Ishaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien Ali Taiga, Francesco Visin, David Vazquez, and   \n381 Aaron Courville. Pixelvae: A latent variable model for natural images. arXiv preprint arXiv:1611.05013,   \n382 2016.   \n383 [28] Agrim Gupta, Jiajun Wu, Jia Deng, and Li Fei-Fei. Siamese masked autoencoders. arXiv preprint   \n384 arXiv:2305.14344, 2023.   \n385 [29] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised   \n386 visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern   \n387 recognition, pages 9729\u20139738, 2020.   \n388 [30] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders   \n389 are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern   \n390 Recognition (CVPR), pages 16000\u201316009, 2022.   \n391 [31] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic   \n392 depth. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October   \n393 11\u201314, 2016, Proceedings, Part IV 14, pages 646\u2013661. Springer, 2016.   \n394 [32] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding,   \n395 Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. Perceiver io: A general architecture   \n396 for structured inputs & outputs. arXiv preprint arXiv:2107.14795, 2021.   \n397 [33] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao,   \n398 Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything.   \n399 In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 4015\u20134026,   \n400 2023.   \n401 [34] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.   \n402 Albert: A lite bert for self-supervised learning of language representations. In International Conference on   \n403 Learning Representations, 2020.   \n404 [35] Anders Boesen Lindbo Larsen, S\u00f8ren Kaae S\u00f8nderby, Hugo Larochelle, and Ole Winther. Autoencoding   \n405 beyond pixels using a learned similarity metric. In International conference on machine learning, pages   \n406 1558\u20131566. PMLR, 2016.   \n407 [36] Jin Li, Yaoming Wang, XIAOPENG ZHANG, Yabo Chen, Dongsheng Jiang, Wenrui Dai, Chenglin Li,   \n408 Hongkai Xiong, and Qi Tian. Progressively compressed auto-encoder for self-supervised representation   \n409 learning. In The Eleventh International Conference on Learning Representations, 2023.   \n410 [37] Tianhong Li, Huiwen Chang, Shlok Kumar Mishra, Han Zhang, Dina Katabi, and Dilip Krishnan.   \n411 Mage: Masked generative encoder to unify representation learning and image synthesis. arXiv preprint   \n412 arXiv:2211.09117, 2022.   \n413 [38] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones   \n414 for object detection. In European Conference on Computer Vision, pages 280\u2013296. Springer, 2022.   \n415 [39] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He. Scaling language-image   \n416 pre-training via masking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern   \n417 Recognition, pages 23390\u201323400, 2023.   \n418 [40] Jihao Liu, Xin Huang, Jinliang Zheng, Yu Liu, and Hongsheng Li. Mixmae: Mixed and masked autoencoder   \n419 for efficient pretraining of hierarchical vision transformers. arXiv:2205.13137, 2022.   \n420 [41] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,   \n421 Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv   \n422 preprint arXiv:1907.11692, 2019.   \n423 [42] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. 2017.   \n424 [43] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint   \n425 arXiv:1711.05101, 2017.   \n426 [44] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive   \n427 coding. arXiv preprint arXiv:1807.03748, 2018.   \n428 [45] Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre   \n429 Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual   \n430 features without supervision. arXiv preprint arXiv:2304.07193, 2023.   \n431 [46] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders:   \n432 Feature learning by inpainting. In Proceedings of the IEEE conference on computer vision and pattern   \n433 recognition, pages 2536\u20132544, 2016.   \n434 [47] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding   \n435 by generative pre-training. 2018.   \n436 [48] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language   \n437 models are unsupervised multitask learners. 2019.   \n438 [49] Ilija Radosavovic, Tete Xiao, Stephen James, Pieter Abbeel, Jitendra Malik, and Trevor Darrell. Real-world   \n439 robot learning with masked visual pre-training. In Conference on Robot Learning, pages 416\u2013426. PMLR,   \n440 2023.   \n441 [50] Andreas Peter Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas   \n442 Beyer. How to train your vit? data, augmentation, and regularization in vision transformers. Transactions   \n443 on Machine Learning Research, 2022.   \n444 [51] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the   \n445 inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and   \n446 pattern recognition, pages 2818\u20132826, 2016.   \n447 [52] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. VideoMAE: Masked autoencoders are data-efficient   \n448 learners for self-supervised video pre-training. In Advances in Neural Information Processing Systems,   \n449 2022.   \n450 [53] Hugo Touvron, Matthieu Cord, Alaaeldin El-Nouby, Piotr Bojanowski, Armand Joulin, Gabriel Synnaeve,   \n451 and Herv\u00e9 J\u00e9gou. Augmenting convolutional networks with attention-based aggregation, 2021.   \n452 [54] Shubham Tulsiani and Abhinav Gupta. Pixeltransformer: Sample conditioned signal generation. In   \n453 Proceedings of the 38th International Conference on Machine Learning, pages 10455\u201310464. PMLR,   \n454 2021.   \n455 [55] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional   \n456 image generation with pixelcnn decoders. Advances in neural information processing systems, 29, 2016.   \n457 [56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz   \n458 Kaiser, and Illia Polosukhin. Attention is all you need. 2017.   \n459 [57] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol, and L\u00e9on   \n460 Bottou. Stacked denoising autoencoders: Learning useful representations in a deep network with a local   \n461 denoising criterion. Journal of machine learning research, 11(12), 2010.   \n462 [58] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao.   \n463 Videomae v2: Scaling video masked autoencoders with dual masking. In Proceedings of the IEEE/CVF   \n464 Conference on Computer Vision and Pattern Recognition, pages 14549\u201314560, 2023.   \n465 [59] Xudong Wang, Ziwei Liu, and Stella X Yu. Unsupervised feature learning by cross-level instance-group   \n466 discrimination. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,   \n467 pages 12586\u201312595, 2021.   \n468 [60] Chen Wei, Karttikeya Mangalam, Po-Yao Huang, Yanghao Li, Haoqi Fan, Hu Xu, Huiyu Wang, Cihang   \n469 Xie, Alan Yuille, and Christoph Feichtenhofer. Diffusion models as masked autoencoder. In ICCV, 2023.   \n470 [61] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu.   \n471 Simmim: A simple framework for masked image modeling. In Proceedings of the IEEE/CVF Conference   \n472 on Computer Vision and Pattern Recognition (CVPR), pages 9653\u20139663, 2022.   \n473 [62] Hongwei Xue, Peng Gao, Hongyang Li, Yu Qiao, Hao Sun, Houqiang Li, and Jiebo Luo. Stare at what   \n474 you see: Masked image modeling without reconstruction. In Proceedings of the IEEE/CVF Conference on   \n475 Computer Vision and Pattern Recognition, pages 22732\u201322741, 2023.   \n476 [63] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.   \n477 Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the   \n478 IEEE/CVF international conference on computer vision, pages 6023\u20136032, 2019.   \n479 [64] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk   \n480 minimization. In International Conference on Learning Representations, 2018.   \n481 [65] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image   \n482 bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "483 A Implementation details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "484 A.1 Attention Calculation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "485 To compare the attention values for mask tokens in vanilla MAE (Figure 1), we trained a ViT-B/16   \n486 MAE for 800 epochs using the default hyperparameters provided in [30]. For each image, we   \n487 randomly generate a $75\\%$ binary mask $(m)$ for all tokens, with $m_{i}=1$ representing a token being   \n488 masked and $m_{i}\\,=\\,0$ otherwise. During the forward pass of the decoder, for each self-attention   \n489 operation, the attention map is stored. This means that for the default MAE, a total of 8 attention   \n490 maps, each with 16 attention heads are stored. Based on the mask pattern, we calculate the outer   \n491 product $(m\\cdot m^{\\top})$ for the self-attention among mask tokens, and $m\\cdot(\\dot{1}-m^{\\top})$ for the cross-attention   \n492 from the mask token to the visible tokens. We then calculate the average across all feature maps   \n493 and attention heads for self-attention and cross-attention to get the image average values. Lastly, we   \n494 averaged across the entire ImageNet validation set to obtain the final values. ", "page_idx": 13}, {"type": "text", "text": "495 A.2 Inter-Block Attention ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "496 We tried a few implementations for inter-block attention (IBA) and found the following implementa  \n497 tion to be the fastest and most memory-efficient. In this implementation, we combine inter-block   \n498 attention for all encoder layers as a single forward pass of a linear layer. For each decoder block,   \n499 we index into the output tensor to extract the corresponding feature map, and a layer norm will be   \n500 applied before the feature map is fed into the decoder block. Other alternatives we tried include 1)   \n501 performing separate inter-block attentions before each decoder block, and 2) 1x1 convolution on the   \n502 stacked encoder feature maps.   \n503 In MAE, there exists a layer norm after the last encoder feature map before feeding into the decoder.   \n504 In our implementation, we only add layer norm after inter-block attention. We find that adding   \n505 an additional layer norm before inter-block attention to each encoder feature map does not lead to   \n506 improvements in model performance but will significantly increase GPU memory usage. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "507 The pseudo-code of inter-block attention is the following: ", "page_idx": 13}, {"type": "table", "img_path": "wZ5kEOCTce/tmp/17354c50b1edbade4538881595c9264f68a833b2f289094fbb960a5e2b8a802f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "522 Additionally, we further investigate the importance of using a cross-attention decoder, where each   \n523 decoder block can use different feature maps from the encoder for decoding. In this experiment, we   \n524 incorporated IBA into MAE, which uses only a self-attention decoder. Specifically, we concatenate   \n525 the interblock attention features with the masked tokens. We then feed the combined features into   \n526 MAE\u2019s self-attention decoder. We pre-trained the model and finetuned it for Imagenet classification.   \n527 The results are presented in Table. 4, where all models are pre-trained for 400 epochs. We observe that   \n528 inter-block attention has negligible performance improvements for MAE, potentially because MAE   \n529 only takes in one feature map in its decoder. In contrast, inter-block attention allows cross-attention   \n530 layers in CrossMAE to attend to features from different encoder blocks, thanks to its decoupling of   \n531 queries with keys and values. ", "page_idx": 13}, {"type": "text", "text": "532 A.3 Ablation that Adds Self-Attention ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "533 In Section 4.3 (a), we propose adding self-attention back to CrossMAE as an ablation. In that   \n534 particular ablation study, we analyze the effect of self-attention between the masked tokens, which   \n535 can be used to improve the consistency for reconstruction. Specifically, we modify the formulation in   \n536 the original transformer paper [56], where the mask/query tokens are first passed through a multi  \n537 head self-attention and a residual connection before being used in the multiheaded cross-attention   \n538 with the features from the encoder. The primary difference with the vanilla transformer decoder   \n539 implementation [56] is we do not perform casual masking in the multi-head self-attention. Please   \n540 reference Figure 6 for a more visual presentation of the method. ", "page_idx": 13}, {"type": "table", "img_path": "wZ5kEOCTce/tmp/9edc036cdafcb96a602c2461625d0a30077bc52b9e1e5e2bcd944a37b3693169.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "image", "img_path": "wZ5kEOCTce/tmp/1a90b22f8512f8fc3dff8e25fcaa5b56250ea909b3da004fdf38c8389e8b568d.jpg", "img_caption": ["Table 4: For MAE, inter-block attention has very small differences in terms of finetuning performance, potentially due to the fact that MAE\u2019s decoder only takes in one set of features. ", "Figure 6: Modification for self-attention ablation ", "Table 5: Improving inter-block attention by adding linear projections to the input features. The performance gain indicates that it is possible to design variants of readout functions to improve CrossMAE. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "541 A.4 Ablation on Inter-block Attention ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "542 In Table 3d, the following cases are considered. 1 feature map (row 1) does not use inter-block   \n543 attention. Each decoder block only takes the last feature map from the encoder as the keys and values.   \n544 For scenarios where more than one feature map is used, the output of the patch embedding (input to   \n545 the ViT) is also used.   \n546 In addition to the simple design of inter-block attention proposed above, we also experimented   \n547 with a variant of inter-block attention by further parameterizing the attention with linear projections.   \n548 Specifically, rather than directly performing weighted sum aggregation to form the features for each   \n549 cross-attention layer in the decoder, we added a linear projection for each encoder feature before the   \n550 feature aggregation. We denote this variant as $C r o s s M A E{+}L P$ . As shown in the Table. 5 (with ViT-B   \n551 pre-trained for 400 epochs, consistent with the setting in Table. 3), adding a linear projection slightly   \n552 improves the performance. This indicates that it is possible to design variants of readout functions,   \n553 such as through improved inter-block attention, to improve the feature quality of CrossMAE.   \n555 Pre-training: The default setting is in Table 6, which is consistent with the official MAE [30]   \n556 implementation. As mentioned in Sec. 3.4, we scale the learning rate by the ratio between mask ratio   \n557 $(p)$ and prediction ratio $(\\gamma)$ to ensure the variance of the loss is consistent with [30]. Additionally, we   \n558 use the linear learning rate scaling rule [25]. This results in $l r=\\gamma*b a s e\\_l r*b a t c h s i z e/(256*p).$   \n559 For Table 1, we use 12 decoder blocks, with mask ratio and prediction ratio both $75\\%$ , and interblock   \n560 attention takes in all encoder feature maps. For the 400 epochs experiments in Table 2, we scale the   \n561 warm-up epochs correspondingly. Other hyperparameters, such as decoder block width, are the same   \n562 as MAE. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "table", "img_path": "wZ5kEOCTce/tmp/e586573304f561145eaca795d8d9eb7530e5f9bb5984fc7ef40e5e76f1d1e0e5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "563 Finetuning: We use the same hyperparameters as MAE finetuning. We use global average pooling 564 for finetuning. In MAE, the layer norm for the last encoder feature map is removed for finetuning, which is consistent with our pretraining setup. Please refer to Table 7 for more detail. ", "page_idx": 15}, {"type": "table", "img_path": "wZ5kEOCTce/tmp/b728c606157bc263d1c0bc407d6a705a05585539d121eca6c27d957d9e41de1b.jpg", "table_caption": [], "table_footnote": ["Table 6: Pretraining Hyperparameters "], "page_idx": 15}, {"type": "text", "text": "565 ", "page_idx": 15}, {"type": "text", "text": "566 A.6 Compute Infrastructure ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "567 Each of the pretraining and finetuning experiments is run on 2 or 4 NVIDIA A100 80GB GPUs. The   \n568 batch size per GPU is scaled accordingly and we use gradient accumulation to avoid out-of-memory   \n569 errors. ViTDet [38] experiments use a single machine equipped with 8 NVIDIA A100 (80GB) GPUs.   \n570 We copy the datasets to the shared memory on the machines to accelerate dataloading. We use   \n571 FlashAttention-2 [18] to accelerate attention calculation. ", "page_idx": 15}, {"type": "table", "img_path": "wZ5kEOCTce/tmp/6dc72ac4343395592901fbbdbc19f26c3deb04a7c2e63ded63d02eb3efe7203a.jpg", "table_caption": [], "table_footnote": ["Table 7: Finetuning Hyperparameters "], "page_idx": 15}, {"type": "text", "text": "572 B Additional Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "573 B.1 Linear Probe ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "574 We provide linear probe comparisons (at 800 epochs) for ViT-Small and ViT-Base in Table. 8. For both   \n575 of these experiments, we run CrossMAE with a prediction ratio of $75\\%$ (reconstruction of all masked   \n576 patches). These results show that CrossMAE achieves slightly better linear probe performance than   \n577 vanilla MAE. ", "page_idx": 16}, {"type": "table", "img_path": "wZ5kEOCTce/tmp/bbe56f7633b457ec5d1317ba63f3a2ab597faa322eea416349038a78a550d58b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "578 B.2 Masking Strategy ", "text_level": 1, "page_idx": 16}, {"type": "table", "img_path": "wZ5kEOCTce/tmp/f41cdc76477ca9c61f9a4c4962e7043e388b90293b77735704d4897a1344535e.jpg", "table_caption": [], "table_footnote": ["Table 9: Ablation of masking strategies "], "page_idx": 16}, {"type": "text", "text": "579 Similar to MAE [30], we here ablate the masking pattern. Instead of random masking, we perform   \n580 grid-wise sampling that \u201ckeeps one of every four patches\u201d (see MAE Figure 6). The finetuning   \n581 performance is reported in Table. 9 for ViT-B (at 400 epochs), which shows that grid masking does   \n582 not lead to additional improvements in downstream performance. ", "page_idx": 16}, {"type": "text", "text": "583 C Runtime and GPU Memory Comparisons with MAE ", "text_level": 1, "page_idx": 16}, {"type": "table", "img_path": "wZ5kEOCTce/tmp/514f21f4e421b38c6843f3eb7afed5a28c5e5e09ae0e703cfffe71b1ef1f2b77.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 10: CrossMAE greatly improves the training throughput and reduces the memory requirements, lowering the barrier for masked pretraining. Statistics are measured on 2 NVIDIA A100 80GB GPUs. Please refer to Appendix C for comparison details.  : MAE\u2019s default batch size exceeds the capacity of 4 GPUs, requiring gradient accumulation for runtime measurement. ", "page_idx": 16}, {"type": "image", "img_path": "wZ5kEOCTce/tmp/0bb37a7880345a2424e3f70cf6652a429f89b51486aaa8801c9ab6e2600bb9e8.jpg", "img_caption": ["Figure 7: We compare ViT-B which is pre-trained for 800 epochs with different variants of CrossMAE v.s. MAE. For CrossMAE, we vary the prediction ratio $p$ and number of decoder blocks $_n$ , and we denote each as $(p,\\,n)$ . While all experiments are run with inter-block attention, CrossMAE has lower decoder FLOPS than MAE [30] and performs on par or better. "], "img_footnote": ["Table 8: Linear probe experiments of CrossMAE. "], "page_idx": 16}, {"type": "text", "text": "584 All experiments in Table 10 are conducted on a server with 4 NVIDIA A100 (80GB) GPUs, with the   \n585 standard hyperparameters provided above for pretraining. NVLink is equipped across the GPUs. We   \n586 use the default setting for MAE and set the global batch size to 4096. For CrossMAE, we also use   \n587 the default setting with a prediction ratio 0.25, and this takes around 41GB memory per GPU without   \n588 gradient accumulation (i.e., local batch size is set to 1024 samples per GPU). However, the same   \n589 local batch size results in out-of-memory (OOM), which indicates that the total memory requirement   \n590 is larger than the available memory for each GPU (80GB). To run MAE on same hardware, we   \n591 thus employ gradient accumulation with a local batch size of 512 to maintain the global batch size.   \n592 The benchmark runs each method and measures the average per epoch runtime as well as the max   \n593 memory allocation for 10 training epochs. Our experiments in Figure 7 show that models with lower   \n594 prediction ratios benefit more from deeper decoders. Our model performs on par or better when   \n595 compared to MAE, with up to $3.7\\times$ lower decoder FLOPS. ", "page_idx": 16}, {"type": "text", "text": "596 D Visualizing the Contributions per Decoder Block ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "597 We propose a more fine-grained visualization approach that allows us to precisely understand the   \n598 effect and contribution of each decoder block.   \n599 Two key observations enable per-block visualization: 1) Transformer blocks have residual connections   \n600 from their inputs to outputs. Let $f_{i}$ be the output and $g_{i}(\\cdot)$ the residual function of decoder $i$ , so   \n601 $f_{i}\\,=\\,f_{i-1}+{\\bar{g}}_{i}(f_{i-1})$ . 2) The final decoder block\u2019s output goes through a reconstruction head $h$ ,   \n602 which is linear, consisting of a layer-norm and a linear layer, to produce the reconstruction. With   \n603 $D$ as the decoder depth, $f_{0}$ the initial input, and $y$ the final output, $y$ is recursively defined as   \n604 $y=h(f_{D-1}+g_{D}(f_{D-1}^{-}))$ , which simplifies due to the linearity of $h$ : ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\mathbf{y}=h(f_{0}+g_{1}(f_{0})+\\cdot\\cdot\\cdot+g_{D}(f_{D-1}))}\\\\ &{\\quad=\\underbrace{h(f_{0})}_{\\mathrm{Pos\\;Embed.+Mask\\;Token}}+\\underbrace{h(g_{1}(f_{0}))}_{\\mathrm{Block\\;1}}+\\cdot\\cdot\\cdot+\\underbrace{h(g_{D}(f_{D-1}))}_{\\mathrm{Block\\;D}}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "605 This decomposition allows us to express the reconstruction as an image stack, where the sum of all   \n606 the levels gives us the final reconstruction. We present the visualization in Figure 5. ", "page_idx": 17}, {"type": "text", "text": "607 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "4 Guidelines:   \n5 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n6 made in the paper. \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n8 contributions made in the paper and important assumptions and limitations. A No or   \n9 NA answer to this question will not be perceived well by the reviewers. \u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n2 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n3 are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "624 2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The limitations of the work have been discussed in the Discussion and Conclusion section. ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "6 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA]   \nJustification: This work offers observations and hypotheses justified with empirical results. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "1   \n62 \u2022 The answer NA means that the paper does not include theoretical results.   \n63 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n64 referenced.   \n65 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n66 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n67 they appear in the supplemental material, the authors are encouraged to provide a short   \n68 proof sketch to provide intuition.   \n69 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n70 by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "672 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "673 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n674 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n675 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Justification: Our code, which reproduces our results, is provided through an anonymous link in the abstract. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "711 5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "712 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n713 tions to faithfully reproduce the main experimental results, as described in supplemental   \n714 material?   \n715 Answer: [Yes]   \n716 Justification: Our method is evaluated on open datasets that are publicly available.   \n717 Guidelines:   \n718 \u2022 The answer NA means that paper does not include experiments requiring code.   \n719 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n720 public/guides/CodeSubmissionPolicy) for more details.   \n721 \u2022 While we encourage the release of code and data, we understand that this might not be   \n722 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n723 including code, unless this is central to the contribution (e.g., for a new open-source   \n724 benchmark).   \n725 \u2022 The instructions should contain the exact command and environment needed to run to   \n726 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n727 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n728 \u2022 The authors should provide instructions on data access and preparation, including how   \n729 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n730 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n731 proposed method and baselines. If only a subset of experiments are reproducible, they   \n732 should state which ones are omitted from the script and why.   \n733 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n734 versions (if applicable).   \n735 \u2022 Providing as much information as possible in supplemental material (appended to the   \n736 paper) is recommended, but including URLs to data and code is permitted.   \n737 6. Experimental Setting/Details   \n738 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n739 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n740 results?   \n741 Answer: [Yes]   \n742 Justification: We follow the hyperparam selection from MAE. The hyperparams introduced   \n743 by our work, such as the mask ratio and the number of feature maps used, are ablated.   \n744 Guidelines:   \n745 \u2022 The answer NA means that the paper does not include experiments.   \n746 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n747 that is necessary to appreciate the results and make sense of them.   \n748 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n749 material.   \n750 7. Experiment Statistical Significance   \n751 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n752 information about the statistical significance of the experiments?   \n753 Answer: [No]   \n754 Justification: Error bars are not reported because they would be too computationally expen  \n755 sive.   \n756 Guidelines:   \n757 \u2022 The answer NA means that the paper does not include experiments.   \n758 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n759 dence intervals, or statistical significance tests, at least for the experiments that support   \n760 the main claims of the paper.   \n761 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n762 example, train/test split, initialization, random drawing of some parameter, or overall   \n763 run with given experimental conditions).   \n764 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n765 call to a library function, bootstrap, etc.)   \n766 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n767 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n768 of the mean.   \n769 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n770 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n771 of Normality of errors is not verified.   \n772 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n773 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n774 error rates).   \n775 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n776 they were calculated and reference the corresponding figures or tables in the text.   \n777 8. Experiments Compute Resources   \n778 Question: For each experiment, does the paper provide sufficient information on the com  \n779 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n780 the experiments?   \n781 Answer: [Yes]   \n782 Justification: We described the compute requirements in Appendix A.6. We do not use   \n783 GPUs from a cloud provider.   \n784 Guidelines:   \n785 \u2022 The answer NA means that the paper does not include experiments.   \n786 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n787 or cloud provider, including relevant memory and storage.   \n788 \u2022 The paper should provide the amount of compute required for each of the individual   \n789 experimental runs as well as estimate the total compute.   \n790 \u2022 The paper should disclose whether the full research project required more compute   \n791 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n792 didn\u2019t make it into the paper).   \n793 9. Code Of Ethics   \n794 Question: Does the research conducted in the paper conform, in every respect, with the   \n795 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n796 Answer: [Yes]   \n797 Justification: The research conforms to the NeurIPS Code of Ethics.   \n798 Guidelines:   \n799 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n800 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n801 deviation from the Code of Ethics.   \n802 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n803 eration due to laws or regulations in their jurisdiction).   \n804 10. Broader Impacts   \n805 Question: Does the paper discuss both potential positive societal impacts and negative   \n806 societal impacts of the work performed?   \n807 Answer: [Yes]   \n808 Justification: This paper aims to advance the field of self-supervised learning. Like other self  \n809 supervised learning methods, our work may have various societal implications. However,   \n810 we do not believe any specific consequences need to be highlighted in this context.   \n811 Guidelines:   \n812 \u2022 The answer NA means that there is no societal impact of the work performed.   \n813 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n814 impact or why the paper does not address societal impact.   \n815 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n816 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n817 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n818 groups), privacy considerations, and security considerations.   \n819 \u2022 The conference expects that many papers will be foundational research and not tied   \n820 to particular applications, let alone deployments. However, if there is a direct path to   \n821 any negative applications, the authors should point it out. For example, it is legitimate   \n822 to point out that an improvement in the quality of generative models could be used to   \n823 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n824 that a generic algorithm for optimizing neural networks could enable people to train   \n825 models that generate Deepfakes faster.   \n826 \u2022 The authors should consider possible harms that could arise when the technology is   \n827 being used as intended and functioning correctly, harms that could arise when the   \n828 technology is being used as intended but gives incorrect results, and harms following   \n829 from (intentional or unintentional) misuse of the technology.   \n830 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n831 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n832 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n833 feedback over time, improving the efficiency and accessibility of ML).   \n834 11. Safeguards   \n835 Question: Does the paper describe safeguards that have been put in place for responsible   \n836 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n837 image generators, or scraped datasets)?   \n838 Answer: [NA]   \n839 Justification: The paper does not pose such risks.   \n840 Guidelines:   \n841 \u2022 The answer NA means that the paper poses no such risks.   \n842 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n843 necessary safeguards to allow for controlled use of the model, for example by requiring   \n844 that users adhere to usage guidelines or restrictions to access the model or implementing   \n845 safety filters.   \n846 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n847 should describe how they avoided releasing unsafe images.   \n848 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n849 not require this, but we encourage authors to take this into account and make a best   \n850 faith effort.   \n851 12. Licenses for existing assets   \n852 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n853 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n854 properly respected?   \n855 Answer: [Yes]   \n856 Justification: The code and datasets used in this work follow the original MAE work.   \n857 Guidelines:   \n858 \u2022 The answer NA means that the paper does not use existing assets.   \n859 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n860 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n861 URL.   \n862 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n863 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n864 service of that source should be provided.   \n865 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n866 package should be provided. For popular datasets, paperswithcode.com/datasets   \n867 has curated licenses for some datasets. Their licensing guide can help determine the   \n868 license of a dataset. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 23}, {"type": "text", "text": "71 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n872 the asset\u2019s creators.   \n873 13. New Assets   \n874 Question: Are new assets introduced in the paper well documented and is the documentation   \n875 provided alongside the assets?   \n876 Answer: [NA]   \n77 Justification: The paper does not release new assets.   \n878 Guidelines:   \n879 \u2022 The answer NA means that the paper does not release new assets.   \n880 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n881 submissions via structured templates. This includes details about training, license,   \n882 limitations, etc.   \n883 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n884 asset is used.   \n85 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n86 create an anonymized URL or include an anonymized zip file.   \n14. Crowdsourcing and Research with Human Subjects   \n888 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n889 include the full text of instructions given to participants and screenshots, if applicable, as   \n890 well as details about compensation (if any)?   \n891 Answer: [NA]   \n892 Justification: The paper does not involve crowdsourcing or research with human subjects.   \n893 Guidelines:   \n894 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n895 human subjects.   \n96 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n897 tion of the paper involves human subjects, then as much detail as possible should be   \n98 included in the main paper.   \n899 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n900 or other labor should be paid at least the minimum wage in the country of the data   \n01 collector.   \n902 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n903 Subjects   \n904 Question: Does the paper describe potential risks incurred by study participants, whether   \n905 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n906 approvals (or an equivalent approval/review based on the requirements of your country or   \n907 institution) were obtained?   \n08 Answer: [NA]   \n909 Justification: The paper does not involve crowdsourcing or research with human subjects.   \n10 Guidelines:   \n11 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n12 human subjects.   \n13 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) ", "page_idx": 23}, {"type": "text", "text": "may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]