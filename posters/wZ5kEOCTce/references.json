{"references": [{"fullname_first_author": "Kaiming He", "paper_title": "Masked autoencoders are scalable vision learners", "publication_date": "2022-06-01", "reason": "This paper introduces the Masked Autoencoders (MAE) framework, which is the primary focus and basis for the current research."}, {"fullname_first_author": "Hangbo Bao", "paper_title": "BEIT: BERT pre-training of image transformers", "publication_date": "2021-06-01", "reason": "This paper is a foundational work in masked image modeling that uses BERT-style pre-training for image transformers, providing a crucial context for MAE and the current work's investigation."}, {"fullname_first_author": "Mathilde Caron", "paper_title": "Emerging properties in self-supervised vision transformers", "publication_date": "2021-06-01", "reason": "This paper explores self-supervised learning in vision transformers, offering insights into self-attention mechanisms and representation learning relevant to the current research on masked autoencoders."}, {"fullname_first_author": "Deepak Pathak", "paper_title": "Context encoders: Feature learning by inpainting", "publication_date": "2016-06-01", "reason": "This paper is one of the early works in masked image modeling for computer vision, serving as an important precursor to modern masked autoencoder methods."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-06-01", "reason": "This paper introduced the Transformer architecture, which is fundamental to many masked image modeling techniques, including MAE and CrossMAE, thus forming a cornerstone of this research area."}]}