[{"type": "text", "text": "UIR-LoRA: Achieving Universal Image Restoration through Multiple Low-Rank Adaptation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Existing unified methods typically treat multi-degradation image restoration as a   \n2 multi-task learning problem. Despite performing effectively compared to single   \n3 degradation restoration methods, they overlook the utilization of commonalities   \n4 and specificities within multi-task restoration, thereby impeding the model\u2019s per  \n5 formance. Inspired by the success of deep generative models and fine-tuning tech  \n6 niques, we proposed a universal image restoration framework based on multiple   \n7 low-rank adapters (LoRA) from multi-domain transfer learning. Our framework   \n8 leverages the pre-trained generative model as the shared component for multi  \n9 degradation restoration and transfers it to specific degradation image restoration   \n10 tasks using low-rank adaptation. Additionally, we introduce a LoRA composing   \n11 strategy based on the degradation similarity, which adaptively combines trained   \n12 LoRAs and enables our model to be applicable for mixed degradation restoration.   \n13 Extensive experiments on multiple and mixed degradations demonstrate that the   \n14 proposed universal image restoration method not only achieves higher fidelity and   \n15 perceptual image quality but also has better generalization ability than other unified   \n16 image restoration models. ", "page_idx": 0}, {"type": "text", "text": "17 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "18 In the wild, a range of distortions commonly appear in captured images, including noise[56], blur[14,   \n19 47, 6], low light[58, 22, 8], and various weather degradations[15, 51, 54, 45]. As a fundamental task   \n20 in low-level vision, image restoration aims to eliminate these distortions and recover sharp details and   \n21 original scene information from corrupted images. With the assistance of deep learning, an abundance   \n22 of restoration approaches [56, 3, 54, 2, 16, 14, 53] have made significant progress in eliminating   \n23 single degradation from images. However, these approaches typically require additional training from   \n24 scratch on specific image pairs in multi-degraded scenarios, which leads to inconvenience in usage   \n25 and limited generalization ability.   \n26 For simplicity and practicality, some existing works [15, 31, 55]consider training a unified model   \n27 (also called all-in-one model) to handle multiple degradations as multi-task learning. These studies   \n28 primarily explore how to discern degradation from the image and integrate it into the restoration   \n29 network. Nevertheless, these methods share all parameters across different degradations, resulting in   \n30 gradient conflicts [40, 52] that hinder further improvement of unified models\u2019 performance.   \n31 Digging deeper, the underlying issue lies in that the similarities among different image restoration   \n32 tasks and the inherent specificity of each degradation are not well considered and utilized in the   \n33 training. This limitation drives us to seek solutions for multi-degradation restoration by leveraging   \n34 both commonalities and specificities.   \n35 Inspired by the successes of deep generative models[37, 36, 35] and fine-tuning techniques[11, 10, 4],   \n36 we propose addressing the aforementioned issue from the perspective of multi-domain transfer   \n37 learning, as presented in Figure 1. The pre-trained generative model exhibits powerful capabilities,   \n38 implying rich prior knowledge of clear image distribution $p(x)$ , which is exactly what is needed   \n39 for image restoration. Since image prior $p(x)$ is degradation-agnostic and applicable to all types   \n40 of degraded images, the pre-trained generative model is an excellent candidate for serving as the   \n41 shared component for multiple degradation restoration. To model the transition from the clean image   \n42 domain to different degraded image domains, minimal specific parameters are required to fine-tune   \n43 the pre-trained model for each degradation restoration task. This approach not only isolates confilcts   \n44 between each degradation task but also ensures efficiency and performance during training.   \n45 Following the idea of multi-domain transfer learning, we proposed a universal image restoration   \n46 framework based on multiple low-rank adaptations, named UIR-LoRA. In our framework, the pre  \n47 trained SD-turbo [39] serves as the shared fundamental model for multiple degradation restoration   \n48 tasks due to its powerful one-step generation capability and extensive image priors. Subsequently,   \n49 we incorporate the low-rank adaptation (LoRA) technique [11] to fine-tune the base model for each   \n50 specific image restoration task. This involves augmenting low-dimensional parameter matrices on   \n51 selected layers within the base model, ensuring efficient fine-tuning while maintaining independence   \n52 between LoRAs for each specific degradation. Additionally, we propose a LoRA composition strategy   \n53 based on degradation similarity. We calculate the similarity between degradation features extracted   \n54 from degraded images and existing degradation types, utilizing it as weights for combining different   \n55 LoRA experts. This strategy enables our method to be applicable for restoring mixed degradation   \n56 images. Moreover, we conducted extensive experiments and compared our approach with several   \n57 existing unified image restoration methods. The experimental results demonstrate that our method   \n58 achieves superior performance in the restoration of various degradations and mixed degradations. Not   \n59 only does our approach outperform existing methods in terms of distortion and perceptual metrics,   \n60 but it also exhibits significant improvements in visual quality. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "zDYXdR3ClP/tmp/327ceb3dacbad1276aa4d805a339a2c462c84f16caf429dfb4ec4c0daa77ff0e.jpg", "img_caption": ["Figure 1: Motivation of our work. A pre-trained generative model serves as the shared component and minimal parameters are added to model the specificity of each degradation restoration task. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "61 Our contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "62 \u2022 From the perspective of multi-domain transfer learning, we propose a novel universal image   \n63 restoration framework based on multiple low-rank adaptations. It leverages the pre-trained   \n64 generative model as the shared component for multi-degradation restoration and employs   \n65 distinct LoRAs for multiple degradations to efficiently transfer to specific degradation   \n66 restoration tasks.   \n67 \u2022 We introduce a LoRAs composition strategy based on the degradation similarity, which   \n68 adaptively combines trained LoRAs and enables our model to be applicable for mixed   \n69 degradation restoration.   \n70 \u2022 Through extensive experiments on multiple and mixed degradations, we demonstrate that the   \n71 proposed universal image restoration method not only achieves higher fidelity and perceptual   \n72 image quality but also has better generalization ability than other unified models. ", "page_idx": 1}, {"type": "text", "text": "74 2.1 Image Restoration ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "75 Specific Degradation Restoration. According to degradation type, image restoration tasks are   \n76 categorized into different groups, including denoising, deblurring, inpainting, draining .etc. Most   \n77 existing image restoration methods [2, 53, 16, 56, 5, 14] mainly address the issue with a single   \n78 degradation. Traditional approaches [27, 28, 7] have proposed image priors. While these priors can   \n79 be applied to different degraded images, their capability is limited. Due to the remarkable capability   \n80 of the deep neural network (DNN), numerous DNN-based methods [2, 53, 16] have been proposed   \n81 to tackle image restoration tasks. While DNN-based methods have made significant progress, they   \n82 struggle with multiple degradations and mixed degradations, since they typically require retraining   \n83 from scratch on data with the same degradation.   \n84 Universal degradation restoration. Increasing attention is currently focused on developing a   \n85 unified model to process multiple degradations. For example, AirNet[15] explores the degradation   \n86 representation in latent space for separating them in the restoration network. PromptIR[31] utilizes a   \n87 prompt block to extract the degradation-related features to improve the performance. Daclip-IR[20]   \n88 introduces the clip-based encoder to distinguish the type of degradation and extract the semantics   \n89 information from distorted images and embed them into a diffusion model to generate high-quality   \n90 images. Despite the advancements, these unified models still have limitations. They also require   \n91 retraining all parameters when unseen degradations arrive and have limited performance due to the   \n92 gradient conflict. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "93 2.2 Low-Rank Adaptation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "94 LoRA [11] is proposed to fine-tune large models by freezing the pre-trained weights and introducing   \n95 trainable low-rank matrices. This fine-tuning method leverages the property of \"intrinsic dimension\"   \n96 within neural networks, lowering the rank of additional matrices and making the re-training process   \n97 efficient. Concretely, given a weight matrices $W\\in\\mathbb{R}^{n\\times m}$ in pre-trained model $\\theta_{p}$ , two trainable   \n98 matrices $\\boldsymbol{B}\\in\\mathbb{R}^{n\\times r}$ and $A\\in\\mathbb{R}^{r\\times\\mathit{\\bar{m}}}$ are inserted into the layer to represent the LoRA $\\Delta W=B A$   \n99 where $r$ is the rank and satisfy $r\\ll m i m(n,m)$ , the updated weights $W^{\\prime}$ are calculated by ", "page_idx": 2}, {"type": "equation", "text": "$$\nW^{\\prime}=W+\\Delta W.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "100 By applying LoRA in pre-trained models, numerous image generation methods [29, 13], show   \n101 superior performance in the field of image style and semantics concept transferring. Additionally,   \n102 fine-tuning methods like ControlNet [57], T2i-adapter [24] are also commonly employed in large  \n103 scale pre-trained generative models such as Stable Diffusion [37], SDXL [30], and Imagen [38]. ", "page_idx": 2}, {"type": "text", "text": "104 2.3 Mixture of Experts ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "105 Mixture of Experts (MoE) [41, 49, 48] is an effective approach to scale up neural network capacity to   \n106 improve performance. Specifically, MoE integrates multiple feed-forward networks into a transformer   \n107 block, where each feed-forward network is regarded as an expert. A gating function is introduced to   \n108 model the probability distribution across all experts in the MoE layer. The gating function is trainable   \n109 and determines the activation of specific experts within the MoE layer based on top- $\\cdot\\mathbf{k}$ values. Broadly   \n110 speaking, our framework aligns with the concept of MoE. However, unlike traditional MoE layers, we   \n111 employ the more efficient LoRA as experts in selected frozen layers and utilize a degradation-aware   \n112 router across all selected layers to uniformly activate experts, reducing learning complexity and   \n113 avoiding conflicts among different image restoration tasks on experts. ", "page_idx": 2}, {"type": "text", "text": "114 3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "115 3.1 Problem Definition ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "116 This paper seeks to develop a novel universal image restoration framework capable of handling   \n117 diverse forms of image degradation in the wild by fine-tuning the pre-trained generative model.   \n118 Consider a set of $T$ image restoration tasks $D=\\{D^{\\dot{k}}\\}_{k=1}^{T}$ , where $D^{k}\\,{\\stackrel{\\rightharpoonup}{=}}\\,\\{(x_{i},y_{i})\\}_{i=1}^{\\bar{n}_{k}}$ is the training   \n119 dataset containing $n_{k}$ images pairs of the $k$ -th image degradation task. Within the set of tasks $D$ ,   \n120 each task $D^{k}$ only has a specific type of image degradation, with no intersection between any two   \n121 tasks. Given a pre-trained generative model $\\theta_{p}$ with frozen parameters, our objective is to learn a   \n122 set of composite $\\{\\theta_{k}\\}_{k=1}^{T}$ to construct a unified model $f_{\\theta}$ that performs well on multi-degradation   \n123 restoration and mixed degradation restoration by transferring learning, where $\\begin{array}{r}{\\theta=\\theta_{p}+\\sum_{k=1}^{T}s_{k}\\theta_{k}}\\end{array}$   \n124 and $s_{k}$ represents the composite weight for $\\theta_{k}$ . The trainable $\\{\\theta_{k}\\}_{k=1}^{T}$ can be optimized through   \n125 minimizing the overall image reconstruction loss: ", "page_idx": 2}, {"type": "image", "img_path": "zDYXdR3ClP/tmp/fea2ad66b455b11e9e67274081a0787c403e468e4afa301ba41b1a6873fc8924.jpg", "img_caption": ["Figure 2: Overview of UIR-LoRA. UIR-LoRA consists of two components: a degradation-aware router and a universal image restorer. The router calculates degradation similarity in the latent space of CLIP, while the restorer utilizes the similarity provided by the router to combine LoRAs and frozen base model and restore images with multiple or mixed degadations. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nL=E_{(x,y)\\in D}l(f_{\\theta}(x),y).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "126 We will present how to design and optimize the trainable $\\{\\theta_{k}\\}_{k=1}^{T}$ and construct the composite   \n127 weights $s$ in the next sections. ", "page_idx": 3}, {"type": "text", "text": "128 3.2 Overview of Universal Framework ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "129 Inspired by transferring learning, we introduce a novel universal image restoration framework based   \n130 on multiple low-rank adaptations, named UIR-LoRA. Referring to Figure 2, our framework consists   \n131 of two main components, namely degradation-aware router and universal image restorer, respectively.   \n132 The degradation-aware router first extracts the degradation feature from input degraded images and   \n133 then calculates the similarity probabilities $s$ with existing degradations in the latent space of CLIP   \n134 model [35, 20]. For the universal image restorer, it comprises a pre-trained generative model $\\theta_{p}$ and   \n135 $T$ trainable LoRAs $\\{\\theta_{k}\\}_{k=1}^{T}$ . This design is primarily motivated by two considerations: firstly, the   \n136 pre-trained generative model contains extensive image priors that are degradation-agnostic and can   \n137 be shared across all types of degraded images. Secondly, each LoRA can independently capture   \n138 specific characteristics of each degradation without gradient conflicts. In practice, the pre-trained   \n139 SD-turbo [39] is employed as the frozen base model in our framework and each LoRA $\\theta_{k}$ serves   \n140 as an expert responsible for transferring the frozen base model to a specific degradation restoration   \n141 task $D^{k}$ . By adjusting the value of Top-K parameter within the degradation-aware router, different   \n142 combinations of LoRAs in the universal image restorer can be activated, enabling the removal of a   \n143 specific degradation and mixed degradation in multi-degraded scenarios.   \n145 The Degradation-Aware Router is designed to provide the restorer with weights for LoRA combination   \n146 based on degradation confidence. Following Daclip-ir [20], we utilized the pre-trained image encoder   \n147 in CLIP [35] to obtain the degradation vector $d\\in\\mathbf{\\bar{R}}^{1\\times z}$ from the input degraded image $x$ , where $z$ is   \n148 degradation length in latent space. Differing from Daclip-ir [20], we use the degradation vector and   \n149 existing degradations to calculate the similarity, instead of directly embedding the degradation vector   \n150 into the restoration network in Daclip-ir [20]. The existing degradations refer to the vocabulary bank   \n151 of diverse degradation types that we introduce in the router, such as \"noisy\", \"blurry\" and \"shadowed\".   \n152 This vocabulary bank is highly compact and flexible when adding new degradation types. Similarly,   \n153 by applying the text encoder of CLIP [35], the vocabulary bank can be encoded into the degradation   \n154 bank $\\bar{B}\\in\\bar{\\mathbb{R}}^{z\\times T}$ in the latent space. As presented in Figure 2, the original degradation similarity   \n155 $s_{o}\\in\\mathbb{R}^{1\\times T}$ is calculated by: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\ns_{o}=d B.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "156 Building upon the original similarity, we adopt a more flexible and controllable Top-K strategy   \n157 to modify $s_{o}$ . Specifically, we select the Top- $\\mathbf{K}$ largest values from the original similarity $s_{o}$ , and   \n158 normalize them to reallocate the weights for LoRAs. The reallocation process can be formulated as : ", "page_idx": 4}, {"type": "equation", "text": "$$\ns=\\frac{s_{o}\\cdot M_{K}}{\\sum s_{o}\\cdot M_{K}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "159 where $M_{K}$ represents a binary mask with the same length as $s_{o}$ , where it is 1 when the corresponding   \n160 value in $s_{o}$ is among the Top- $\\mathbf{\\nabla}\\cdot\\mathbf{K}$ , otherwise it is 0. With a smaller value of $K$ , the restorer activates   \n161 fewer LoRAs, reducing its computational load. For instance, with $K=1$ , only the most similar   \n162 LoRA is activated and it yields effective results when $s$ is accurate, but performance noticeably   \n163 declines with inaccurate $s$ . Conversely, as $K$ increases, the restorer exhibits higher tolerance to $s$ and   \n164 the combination of LoRAs allows it to handle mixed degradation. ", "page_idx": 4}, {"type": "text", "text": "165 3.4 Universal Image Restorer ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "166 Our universal image restorer consists of a pre-trained generative model $\\theta_{p}$ and a set of LoRAs   \n167 $\\{\\theta_{k}\\}_{k=1}^{T}$ . As illustrated in Figure 2, our universal image restorer takes the degraded image $x$ and   \n168 similarity $s$ predicted by the degradation-aware router as inputs. It then activates relevant LoRAs   \n169 based on $s$ to recover the degraded image along with the frozen base model. Since one of our   \n170 objectives is to ensure that each LoRA serves as an expert in processing a specific degradation, the   \n171 number of LoRAs in the restorer aligns with the number of degradation types, $T$ . In practice, we   \n172 select multiple layers from the base model, For a selected layer $W$ of the pre-trained base model, a   \n173 sequence of trainable matrices $\\{\\Delta W_{k}\\}_{k=1}^{T}$ are added into this layer, and the parameters of all chosen   \n174 layers $L$ form a complete LoRA $\\theta_{k}=\\{\\Delta W_{k}^{j}\\}_{j\\in L}$ . As previously explained, each LoRA is a unique   \n175 expert responsible for a specific degradation. Drawing inspiration from Mixture of Expert (MoE), we   \n176 aggregate the outputs of each expert rather than directly merging parameters in [11]. Therefore, given   \n177 the input feature $x_{i n}$ of the current layer and the similarity $s$ , the total output $x_{o u t}$ of this modified   \n178 layer can be expressed as ", "page_idx": 4}, {"type": "equation", "text": "$$\nx_{o u t}=f_{o}(x_{i n})+\\sum_{i=1}^{K}s_{i}f_{i}(x_{i n}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "179 where $f_{i}(x_{i n})$ denotes the result of $i$ -th trainable matrice $W_{i}$ , particularly $f_{o}(x_{i n})$ is output of the   \n180 frozen base layer. From the equation 5, it can be observed that the introduced LoRAs interact with the   \n181 frozen base model at intermediate feature layers in our restorer. This interaction forces the restorer   \n182 to leverage the image priors of the pre-trained generative model and eliminate degradation with the   \n183 assistance of LoRAs. In contrast to employing stable diffusion [37] directly as a post-processing   \n184 technique, our restorer yields results closer to the true scene without introducing inaccurate structural   \n185 details. Since each $W$ is implemented using two low-rank matrices like the formula 1, the total   \n186 trainable parameters of our framework are much smaller than that of the pre-train generative model. ", "page_idx": 4}, {"type": "text", "text": "187 3.5 Training and Inference Procedure ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "188 During the training phase, for the efficient training of the universal image restorer, we ensure that   \n189 each batch is sampled from the same degradation type $D^{k}$ , and activate the corresponding LoRA $\\theta^{k}$   \n190 for training. Since the dataset $D$ is organized by degradation type without overlap and each LoRA   \n191 is assigned to handle each type of degradation correspondingly, the overall optimization process in   \n192 equation 2 can be decomposed into independent optimization processes for each degradation. This   \n193 design and training process circumvent task conflicts among multiple degradations and makes it   \n194 possible to use suitable loss functions for the specific degradation. Due to the availability of accurate   \n195 s during training and the use of pre-trained encoders from CLIP [35] and Daclip-ir [20] in our router,   \n196 the router was not utilized during training.   \n197 In the inference phase, the similarity $s$ is unknown and needs to be estimated from the degraded   \n198 image. The estimated similarity $s$ serves as a reference in our framework and can also be manually   \n199 specified by users. Subsequently, our universal image restorer composite LoRAs and recovers the   \n200 input image with the guidance of $s$ . ", "page_idx": 4}, {"type": "table", "img_path": "zDYXdR3ClP/tmp/148aec4a1923da512adf30aa3baf7fd6a6581a964359c3f6a1af663485573970.jpg", "table_caption": ["Table 1: Comparison of the restoration results over ten different datasets. The best results are marked in boldface. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "201 4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "202 4.1 Experimental Setting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "203 Datasets. We validate the effectiveness of our framework in multiple and mixed degradation scenarios.   \n204 In the case of multiple degradations, we follow Daclip-IR [20] and construct a dataset using 10   \n205 different single degradation datasets. Briefly, the composite dataset comprises a total of 52800 image   \n206 pairs for training and 2490 image pairs for testing. The degradation types included are commonly   \n207 encountered in image restoration, such as blur, noise, shadow, JPEG compression, and weather   \n208 degradations. For mixed degradations, we utilize two degradation datasets, REDS [25] and LOLBlur   \n209 [58]. In REDS, the images are distorted by JPEG compression and blur, and those images in LOLBlur   \n210 have blur and low light. For more details about datasets in our experiments, please refer to Appendix.   \n211 Metrics. The objective of the image restoration task is to output images with enhanced visual quality   \n212 while maintaining high fidelity to the original scene information. This differs from image generation   \n213 tasks, which prioritize visual quality. Therefore, to thoroughly evaluate the effectiveness of our   \n214 method, we utilize reference-based image quality assessment techniques from both distortion and   \n215 perceptual perspectives, including PSNR, SSIM, and LPIPS, as well as FID.   \n216 Comparison Methods. In the experiments, we primarily compare with several state-of-the-art   \n217 methods in image restoration, which fall into two categories: regression model and generative model.   \n218 Regression models include NAFNet [2], Restormer [53], as well as AirNet [15] and PromptIR [31]   \n219 proposed for multiple degradation restoration. DiffBIR [17], IR-SDE [21] and Daclip-IR [20] are   \n220 generative models built upon the diffusion model [9]. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "221 4.2 Implementation Details ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "222 During the training, we adapt an AdamW optimizer to update the weights of trainable parameters in   \n223 our model. Before training LoRA for specific degradation, we add skip-connections in the VAE of   \n224 SD-turbo[39] like [29, 44] and train them with multiple degraded images. We set the initialization   \n225 learning rate to 2e-4 and decrease it with CosineAnnealingLR . We trained every LoRA for 80K   \n226 iterations with batch size 8 and we keep the same hyper-parameters when training different LoRAs.   \n227 The default rank of LoRAs in VAE and Unet is 4 and 8, respectively. ", "page_idx": 5}, {"type": "image", "img_path": "zDYXdR3ClP/tmp/da0528739abcc51c2c9eb5c5f01deef991c61c84f8fe90e8900f2bc0934a5e90.jpg", "img_caption": ["Figure 3: Qualitative comparison on multiple degraded images. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "228 4.3 Multiple Image Restoration ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "229 For fair comparisons, all methods are trained and tested on the multiple degradation dataset. The   \n230 results are presented in Table 1. We can find that our model, UIR-LoRA, considerably surpasses all   \n231 compared image restoration approaches across four metrics. This indicates that our approach can   \n232 balance generating clear structures and details while ensuring the restored images closely resemble the   \n233 original information of the scene. The visual comparison results depicted in Figure 7 also confirm this   \n234 assertion. Regression models such as NAFNet [2]and Restormer [53], lacking extensive image priors,   \n235 tend to produce blurred and over-smoothed images, leading to inferior visual outcomes. Conversely,   \n236 generative models Daclip-IR [20] excessively prioritize perceptual quality, yielding artifacts and   \n237 noise that diverge from the actual scene information. Our approach integrates the strengths of both   \n238 categories of methods, enabling strong performance in both distortion and perceptual aspects ", "page_idx": 6}, {"type": "text", "text": "239 4.4 Mixed Image Restoration ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "240 To evaluate the transferability of UIR-LoRA, we conduct some experiments on mixed degradation   \n241 datasets from REDS[25] and LOLBlur [58]. Each image in these two datasets contains more than one   \n242 type of degradation, like blur, jpeg compression, noise, and low light. We test the mixed degraded   \n243 images using models trained on multiple degradations and set $K$ to 2 in the router. As shown in   \n244 Table 2, our method achieves superior results in both distortion and perceptual quality, particularly   \n245 on the LOLBlur dataset. We also provide visual comparison results, as illustrated in Figure 4, our   \n246 approach effectively enhances the low-light image compared to SOTA methods, highlighting its   \n247 stronger transferability in the wild. More visual results can be found in Appendix. ", "page_idx": 6}, {"type": "table", "img_path": "zDYXdR3ClP/tmp/a3b7b13525a713219a8d21480ba0314be8dbc9ccb2b1ba26d3ef9d8f41901821.jpg", "table_caption": ["Table 2: Comparison of the restoration results on mixed degradation datasets. The best results are marked in boldface. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "zDYXdR3ClP/tmp/f82db5b1929917454878899954735443d2f95d5568e28037a95044274425c434.jpg", "img_caption": ["Figure 4: Qualitative comparison on multiple degraded images. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "248 4.5 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "249 Complexity Analysis. We compare model complexity with SOTA models. The comparison results   \n250 are shown in Table 1, where we report the number of trainable parameters and the runtime for a   \n251 $256\\!\\times\\!256$ image on an A100 GPU. The complexity of UIR-LoRA is comparable to regression models   \n252 like NAFNet [2] and significantly more efficient than generative models like Daclip-IR [20].   \n253 Effectiveness of Degradation-Aware Router. The degradation-aware router plays a crucial role in   \n254 determining which LoRAs are activated in the inference. To comprehensively demonstrate the impact   \n255 of the router, we conduct experiments with different selection strategies. As illustrated in Table 3,   \n256 we have five strategies: \"random\" indicates activating a LoRA at random, \"average\" denotes using   \n257 average weights to activate all LoRAs, and \"Top- $\\cdot1^{\\cdot\\cdot}$ , \"Top- $\\cdot2\"$ and \"All\" correspond to setting $K$ in the   \n258 router to 1,2, and 10, respectively. From the comparison of these results, we can see that the random   \n259 and average strategies result in poorer performance while using the strategy based on degradation   \n260 similarity achieves better outcomes. This suggests that the transferability between different types   \n261 of degradation is limited and that specific parameters are needed to address their particularities.   \n262 Furthermore, the selection of the K value also affects the model\u2019s performance. When an image has   \n263 only one type of degradation, a smaller K value can result in comparable performance with lower   \n264 inference costs. However, for mixed degradations, a larger K value is required to handle the more   \n265 complex situation.   \n266 Impact of LoRA\u2019s Rank. Within our framework, LoRA is utilized to facilitate the transfer from   \n267 the pre-trained generative model to the image restoration task. In order to investigate the impact of   \n268 LoRA\u2019s rank on the performance of image restoration, we conduct experiments using deblurring   \n269 and denoising tasks chosen from ten distinct degradation categories. We set the initial rank to 2 and   \n270 incrementally increase the value by a factor of 2. The performance changes are depicted in Figure 5.   \n271 It is evident that as the rank grows, the restoration results improve in distortion and perceptual quality,   \n272 and at the same time, the number of trainable parameters also increases. Once the rank value exceeds   \n273 4, the performance improvement becomes progressively marginal. Therefore, we set the default rank   \n274 to 4 in our restorer to balance between performance and complexity. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "zDYXdR3ClP/tmp/76f33583a77ee11ce190d5258b248a15ddee4cf4d2009884434d5a5ce4eba636.jpg", "table_caption": ["Table 3: Impact of strategies in router "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "zDYXdR3ClP/tmp/67b6ef92f2c18d2f65f05f5b550d22ee8cd13b62be620f80dd81c16ea972d348.jpg", "img_caption": ["Figure 5: The impact of LoRA\u2019s rank on deblurring and denoising tasks. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "zDYXdR3ClP/tmp/950b42cadbdf57b7dd264d0350bed9907d0fb090a42565b5713ae1cf046aa80f.jpg", "table_caption": ["Table 4: The accuracy of predicted degradation type "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "275 Impact of Predicted Degradation. ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "276 The resizing operation on input images in CLIP models [20, 35] may lead to inaccurate predictions   \n277 of degradation types, especially for blurry images. To reduce its negative impact on performance, we   \n278 introduce a simple way that uses the degradation vector of the image crop without resizing to correct   \n279 the potential error in the resized image. Table 4 is the comparison conducted on blurry images from   \n280 GoPro dataset. It can be observed that our model with modified operation has higher accuracy and   \n281 better performance for deblurring. ", "page_idx": 8}, {"type": "text", "text": "282 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "283 In this paper, we propose a universal image restoration framework based on multiple low-rank   \n284 adaptation, named UIR-LoRA, from the perspective of multi-domain transfer learning. UIR-LoRA   \n285 utilizes a pre-trained generative model as the frozen base model and transfers its abundant image   \n286 priors to different image restoration tasks using the LoRA technique. Moreover, we introduce a   \n287 LoRAs\u2019 composition strategy based on the degradation similarity that allows UIR-LoRA applicable   \n288 for multiple and mixed degradations in the wild. Extensive experiments on universal image restoration   \n289 tasks demonstrate the effectiveness and better generalization capability of our proposed UIR-LoRA. ", "page_idx": 8}, {"type": "text", "text": "290 6 Limitation and Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "291 Although our UIR-LoRA has achieved remarkable performance in image restoration tasks under both   \n292 multiple and mixed degradations, it still has limitations and problems for further exploration. For   \n293 instance, adding new trainable parameters into the network for unseen degradations is unavoidable in   \n294 image restoration tasks, although UIR-LoRA is already more efficient and flexible compared to other   \n295 approaches. ", "page_idx": 8}, {"type": "text", "text": "296 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "297 [1] Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge on single image super-resolution:   \n298 Dataset and study. In Proceedings of the IEEE conference on computer vision and pattern   \n299 recognition workshops, pages 126\u2013135, 2017.   \n300 [2] Liangyu Chen, Xiaojie Chu, Xiangyu Zhang, and Jian Sun. Simple baselines for image   \n301 restoration. In European conference on computer vision, pages 17\u201333. Springer, 2022.   \n302 [3] Liangyu Chen, Xin Lu, Jie Zhang, Xiaojie Chu, and Chengpeng Chen. Hinet: Half instance   \n303 normalization network for image restoration. In Proceedings of the IEEE/CVF Conference on   \n304 Computer Vision and Pattern Recognition, pages 182\u2013192, 2021.   \n305 [4] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo.   \n306 Adaptformer: Adapting vision transformers for scalable visual recognition. Advances in Neural   \n307 Information Processing Systems, 35:16664\u201316678, 2022.   \n308 [5] Xueyang Fu, Jiabin Huang, Xinghao Ding, Yinghao Liao, and John Paisley. Clearing the   \n309 skies: A deep network architecture for single-image rain removal. IEEE Transactions on Image   \n310 Processing, 26(6):2944\u20132956, 2017.   \n311 [6] Dong Gong, Jie Yang, Lingqiao Liu, Yanning Zhang, Ian Reid, Chunhua Shen, Anton Van   \n312 Den Hengel, and Qinfeng Shi. From motion blur to motion flow: A deep learning solution   \n313 for removing heterogeneous motion blur. In Proceedings of the IEEE conference on computer   \n314 vision and pattern recognition, pages 2319\u20132328, 2017.   \n315 [7] Shuhang Gu, Lei Zhang, Wangmeng Zuo, and Xiangchu Feng. Weighted nuclear norm min  \n316 imization with application to image denoising. In Proceedings of the IEEE conference on   \n317 computer vision and pattern recognition, pages 2862\u20132869, 2014.   \n318 [8] Chunle Guo, Chongyi Li, Jichang Guo, Chen Change Loy, Junhui Hou, Sam Kwong, and   \n319 Runmin Cong. Zero-reference deep curve estimation for low-light image enhancement. In   \n320 Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages   \n321 1780\u20131789, 2020.   \n322 [9] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances   \n323 in neural information processing systems, 33:6840\u20136851, 2020.   \n324 [10] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,   \n325 Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning   \n326 for nlp. In International conference on machine learning, pages 2790\u20132799. PMLR, 2019.   \n327 [11] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,   \n328 Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In   \n329 International Conference on Learning Representations, 2022.   \n330 [12] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for   \n331 improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.   \n332 [13] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi  \n333 concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference   \n334 on Computer Vision and Pattern Recognition, pages 1931\u20131941, 2023.   \n335 [14] Orest Kupyn, Volodymyr Budzan, Mykola Mykhailych, Dmytro Mishkin, and Ji\u02c7r\u00ed Matas.   \n336 Deblurgan: Blind motion deblurring using conditional adversarial networks. In Proceedings of   \n337 the IEEE conference on computer vision and pattern recognition, pages 8183\u20138192, 2018.   \n338 [15] Boyun Li, Xiao Liu, Peng Hu, Zhongqin Wu, Jiancheng Lv, and Xi Peng. All-in-one image   \n339 restoration for unknown corruption. In Proceedings of the IEEE/CVF Conference on Computer   \n340 Vision and Pattern Recognition, pages 17452\u201317462, 2022.   \n341 [16] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir:   \n342 Image restoration using swin transformer. In Proceedings of the IEEE/CVF international   \n343 conference on computer vision, pages 1833\u20131844, 2021.   \n344 [17] Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Ben Fei, Bo Dai, Wanli Ouyang, Yu Qiao,   \n345 and Chao Dong. Diffbir: Towards blind image restoration with generative diffusion prior. arXiv   \n346 preprint arXiv:2308.15070, 2023.   \n347 [18] Yun-Fu Liu, Da-Wei Jaw, Shih-Chia Huang, and Jenq-Neng Hwang. Desnownet: Context-aware   \n348 deep network for snow removal. IEEE Transactions on Image Processing, 27(6):3064\u20133073,   \n349 2018.   \n350 [19] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc   \n351 Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings   \n352 of the IEEE/CVF conference on computer vision and pattern recognition, pages 11461\u201311471,   \n353 2022.   \n354 [20] Ziwei Luo, Fredrik K Gustafsson, Zheng Zhao, Jens Sj\u00f6lund, and Thomas B Sch\u00f6n. Controlling   \n355 vision-language models for universal image restoration. arXiv preprint arXiv:2310.01018, 2023.   \n356 [21] Ziwei Luo, Fredrik K Gustafsson, Zheng Zhao, Jens Sj\u00f6lund, and Thomas B Sch\u00f6n. Image   \n357 restoration with mean-reverting stochastic differential equations. International Conference on   \n358 Machine Learning, 2023.   \n359 [22] Long Ma, Tengyu Ma, Risheng Liu, Xin Fan, and Zhongxuan Luo. Toward fast, flexible, and   \n360 robust low-light image enhancement. In Proceedings of the IEEE/CVF conference on computer   \n361 vision and pattern recognition, pages 5637\u20135646, 2022.   \n362 [23] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human seg  \n363 mented natural images and its application to evaluating segmentation algorithms and measuring   \n364 ecological statistics. In Proceedings of the IEEE/CVF International Conference on Computer   \n365 Vision, pages 416\u2013423, 2001.   \n366 [24] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan.   \n367 T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion   \n368 models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages   \n369 4296\u20134304, 2024.   \n370 [25] Seungjun Nah, Sungyong Baik, Seokil Hong, Gyeongsik Moon, Sanghyun Son, Radu Timofte,   \n371 and Kyoung Mu Lee. Ntire 2019 challenge on video deblurring and super-resolution: Dataset   \n372 and study. In Proceedings of the IEEE/CVF conference on computer vision and pattern   \n373 recognition workshops, pages 0\u20130, 2019.   \n374 [26] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep multi-scale convolutional neural   \n375 network for dynamic scene deblurring. In Proceedings of the IEEE Conference on Computer   \n376 Vision and Pattern Recognition, pages 3883\u20133891, 2017.   \n377 [27] Jinshan Pan, Zhe Hu, Zhixun Su, and Ming-Hsuan Yang. Deblurring text images via l0-   \n378 regularized intensity and gradient prior. In Proceedings of the IEEE Conference on Computer   \n379 Vision and Pattern Recognition, pages 2901\u20132908, 2014.   \n380 [28] Jinshan Pan, Deqing Sun, Hanspeter Pfister, and Ming-Hsuan Yang. Blind image deblurring   \n381 using dark channel prior. In Proceedings of the IEEE conference on computer vision and pattern   \n382 recognition, pages 1628\u20131636, 2016.   \n383 [29] Gaurav Parmar, Taesung Park, Srinivasa Narasimhan, and Jun-Yan Zhu. One-step image   \n384 translation with text-to-image models. arXiv preprint arXiv:2403.12036, 2024.   \n385 [30] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe   \n386 Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image   \n387 synthesis. arXiv preprint arXiv:2307.01952, 2023.   \n388 [31] Vaishnav Potlapalli, Syed Waqas Zamir, Salman Khan, and Fahad Khan. Promptir: Prompting   \n389 for all-in-one image restoration. In Thirty-seventh Conference on Neural Information Processing   \n390 Systems, 2023.   \n391 [32] Rui Qian, Robby T Tan, Wenhan Yang, Jiajun Su, and Jiaying Liu. Attentive generative   \n392 adversarial network for raindrop removal from a single image. In Proceedings of the IEEE   \n393 conference on computer vision and pattern recognition, pages 2482\u20132491, 2018.   \n394 [33] Xu Qin, Zhilin Wang, Yuanchao Bai, Xiaodong Xie, and Huizhu Jia. Ffa-net: Feature fusion   \n395 attention network for single image dehazing. In Proceedings of the AAAI conference on artificial   \n396 intelligence, pages 11908\u201311915, 2020.   \n397 [34] Liangqiong Qu, Jiandong Tian, Shengfeng He, Yandong Tang, and Rynson WH Lau. Deshad  \n398 ownet: A multi-context embedding deep network for shadow removal. In Proceedings of the   \n399 IEEE conference on computer vision and pattern recognition, pages 4067\u20134075, 2017.   \n400 [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,   \n401 Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual   \n402 models from natural language supervision. In International conference on machine learning,   \n403 pages 8748\u20138763. PMLR, 2021.   \n404 [36] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark   \n405 Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on   \n406 machine learning, pages 8821\u20138831. Pmlr, 2021.   \n407 [37] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High  \n408 resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF   \n409 conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n410 [38] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,   \n411 Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.   \n412 Photorealistic text-to-image diffusion models with deep language understanding. Advances in   \n413 neural information processing systems, 35:36479\u201336494, 2022.   \n414 [39] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion   \n415 distillation. arXiv preprint arXiv:2311.17042, 2023.   \n416 [40] Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. Advances   \n417 in neural information processing systems, 31, 2018.   \n418 [41] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,   \n419 and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts   \n420 layer. arXiv preprint arXiv:1701.06538, 2017.   \n421 [42] H Sheikh. Live image quality assessment database release 2. http://live.ece.utexas.   \n422 edu/research/quality, 2005.   \n423 [43] Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming-Hsuan Yang, and Lei Zhang. Ntire 2017   \n424 challenge on single image super-resolution: Methods and results. In Proceedings of the IEEE   \n425 conference on computer vision and pattern recognition workshops, pages 114\u2013125, 2017.   \n426 [44] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK Chan, and Chen Change Loy. Ex  \n427 ploiting diffusion prior for real-world image super-resolution. arXiv preprint arXiv:2305.07015,   \n428 2023.   \n429 [45] Yinglong Wang, Chao Ma, and Jianzhuang Liu. Smartassign: Learning a smart knowledge   \n430 assignment strategy for deraining and desnowing. In Proceedings of the IEEE/CVF Conference   \n431 on Computer Vision and Pattern Recognition, pages 3677\u20133686, 2023.   \n432 [46] Chen Wei, Wenjing Wang, Wenhan Yang, and Jiaying Liu. Deep retinex decomposition for   \n433 low-light enhancement. arXiv preprint arXiv:1808.04560, 2018.   \n434 [ 47] Jay Whang, Mauricio Delbracio, Hossein Talebi, Chitwan Saharia, Alexandros G Dimakis,   \n435 and Peyman Milanfar. Deblurring via stochastic refinement. In Proceedings of the IEEE/CVF   \n436 Conference on Computer Vision and Pattern Recognition, pages 16293\u201316303, 2022.   \n437 [48] Xun Wu, Shaohan Huang, and Furu Wei. Mole: Mixture of lora experts. In The Twelfth   \n438 International Conference on Learning Representations, 2023.   \n439 [49] Yuan Xie, Shaohan Huang, Tianyu Chen, and Furu Wei. Moec: Mixture of expert clusters. In   \n440 Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 13807\u201313815,   \n441 2023.   \n442 [50] Wenhan Yang, Robby T Tan, Jiashi Feng, Jiaying Liu, Zongming Guo, and Shuicheng Yan. Deep   \n443 joint rain detection and removal from a single image. In Proceedings of the IEEE conference on   \n444 computer vision and pattern recognition, pages 1357\u20131366, 2017.   \n445 [51] Wenhan Yang, Robby T Tan, Shiqi Wang, Yuming Fang, and Jiaying Liu. Single image deraining:   \n446 From model-based to data-driven and beyond. IEEE Transactions on pattern analysis and   \n447 machine intelligence, 43(11):4059\u20134077, 2020.   \n448 [52] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn.   \n449 Gradient surgery for multi-task learning. Advances in Neural Information Processing Systems,   \n450 33:5824\u20135836, 2020.   \n451 [53] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and   \n452 Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In   \n453 Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages   \n454 5728\u20135739, 2022.   \n455 [54] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming  \n456 Hsuan Yang, and Ling Shao. Multi-stage progressive image restoration. In Proceedings of the   \n457 IEEE/CVF conference on computer vision and pattern recognition, pages 14821\u201314831, 2021.   \n458 [55] Cheng Zhang, Yu Zhu, Qingsen Yan, Jinqiu Sun, and Yanning Zhang. All-in-one multi  \n459 degradation image restoration network via hierarchical degradation representation. In Proceed  \n460 ings of the 31st ACM International Conference on Multimedia, pages 2285\u20132293, 2023.   \n461 [56] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian   \n462 denoiser: Residual learning of deep cnn for image denoising. IEEE transactions on image   \n463 processing, 26(7):3142\u20133155, 2017.   \n464 [57] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image   \n465 diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer   \n466 Vision, pages 3836\u20133847, 2023.   \n467 [58] Shangchen Zhou, Chongyi Li, and Chen Change Loy. Lednet: Joint low-light enhancement and   \n468 deblurring in the dark. In European conference on computer vision, pages 573\u2013589. Springer,   \n469 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "470 A Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "471 A.1 More Details about Datasets ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "472 For multiple degradations, we follow Daclip-IR [20] to construct the dataset, which includes a total   \n473 of ten distinct degradation types: blurry, hazy, JPEG-compression, low-light, noisy, raindrop, rainy,   \n474 shadowed, snowy, and inpainting. The data sources and data splits for each degradation type are   \n475 illustrated in Table 5.   \n476 For mixed degradations, we utilize images from REDS[25] and LOLBlur[58]to evaluate the trans  \n477 ferability of models. We sample 60 images from REDS and 200 images from LOLBlur dataset for   \n478 testing. The degraded images from REDS dataset feature a variety of realistic scenes and objects,   \n479 which suffer from both motion blurs and compression. And the images from LOLBlur dataset cover   \n480 a range of real-world dynamic dark scenarios with mixed degradation of low light and blurs. ", "page_idx": 13}, {"type": "table", "img_path": "zDYXdR3ClP/tmp/893d733125e280be59c297c128d13ff703a3aaa5b3b9c9f98f203c9481fe86b6.jpg", "table_caption": ["Table 5: Details of the datasets with ten different image degradation types "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "481 A.2 More Visual Results ", "text_level": 1, "page_idx": 13}, {"type": "image", "img_path": "zDYXdR3ClP/tmp/0813d5d711d0063326a86ad789532086596fcfeb3ae28644f7c9e7d76868c44a.jpg", "img_caption": ["Figure 6: Qualitative comparison on mixed degraded images from LOLBlur dataset. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "482 A.3 Details about Metrics on Multiple Dagradation ", "text_level": 1, "page_idx": 13}, {"type": "image", "img_path": "zDYXdR3ClP/tmp/d9f7bd1f8bd28554393388057e7f6b5532c354bb4cb941e7cda7bab07d9c90c4.jpg", "img_caption": ["Figure 7: Qualitative comparison on mixed degraded images from REDS dataset. "], "img_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "zDYXdR3ClP/tmp/ed26de0bee30225cd02b9ea00ffeae239c6c78e86ff0d363db7ec13d43fb82a5.jpg", "table_caption": ["Table 6: Comparison of the restoration results over ten different datasets on PSNR "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "zDYXdR3ClP/tmp/a6e5822e61096ea525544c2dd0c1afbf6e47c46eb6d2496abada0ba1a3512fb3.jpg", "table_caption": ["Table 7: Comparison of the restoration results over ten different datasets on SSIM "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "zDYXdR3ClP/tmp/c3457d84ff90af8fd25685b43329e9d30b2456b4bd67321028b7f44339e1802f.jpg", "table_caption": ["Table 8: Comparison of the restoration results over ten different datasets on LPIPS "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "zDYXdR3ClP/tmp/a68fa359ea4df9d76e77ebb93eb391c0a5a0520835e7392b85889c039f3e1de2.jpg", "table_caption": ["Table 9: Comparison of the restoration results over ten different datasets on FID "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "zDYXdR3ClP/tmp/ccb95d91bf8560d0cfc46503af337de72ffb61a800b75827d18f212c12b3c6de.jpg", "table_caption": ["Table 10: Impact of rank in LoRAs "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "483 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "485 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n486 paper\u2019s contributions and scope?   \n487 Answer: [Yes]   \n488 Justification: The main claims made in the abstract and introduction1 accurately reflect the   \n489 paper\u2019s contributions and scope.   \n490 Guidelines:   \n491 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n492 made in the paper.   \n493 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n494 contributions made in the paper and important assumptions and limitations. A No or   \n495 NA answer to this question will not be perceived well by the reviewers.   \n496 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n497 much the results can be expected to generalize to other settings.   \n498 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n499 are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "500 2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The paper does discuss the limitations of the work in Sections 6. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper. \u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. \u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. \u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. \u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. \u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "531 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "532 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n533 a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The paper provides the full set of assumptions and a complete (and correct) proof in Sections 3. ", "page_idx": 17}, {"type": "text", "text": "37 Guidelines:   \n38 \u2022 The answer NA means that the paper does not include theoretical results.   \n39 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n0 referenced.   \n1 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n42 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n3 they appear in the supplemental material, the authors are encouraged to provide a short   \n44 proof sketch to provide intuition.   \n5 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n46 by formal proofs provided in appendix or supplemental material.   \n7 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "548 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "549 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n550 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n551 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Justification: The paper provides a comprehensive description of the experimental setting in Sections 4.1 and implementation details in Sections 4.2, which are crucial for reproducing the main results. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "588 5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "89 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n90 tions to faithfully reproduce the main experimental results, as described in supplemental   \n91 material?   \n92 Answer: [No]   \n93 Justification: Upon acceptance of the paper, we will release the code under an open-source   \n94 license, which will allow the community to access and verify the experimental results.   \n95 Guidelines:   \n96 \u2022 The answer NA means that paper does not include experiments requiring code.   \n97 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n98 public/guides/CodeSubmissionPolicy) for more details.   \n99 \u2022 While we encourage the release of code and data, we understand that this might not be   \n00 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n01 including code, unless this is central to the contribution (e.g., for a new open-source   \n02 benchmark).   \n03 \u2022 The instructions should contain the exact command and environment needed to run to   \n04 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n05 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n06 \u2022 The authors should provide instructions on data access and preparation, including how   \n07 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n08 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n09 proposed method and baselines. If only a subset of experiments are reproducible, they   \n10 should state which ones are omitted from the script and why.   \n11 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n12 versions (if applicable).   \n13 \u2022 Providing as much information as possible in supplemental material (appended to the   \n14 paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: The paper provides detailed information on all training and test aspects, including datasets, metrics, comparison methods, hyperparameters, the type of optimizer used, and other relevant details necessary to understand the results in Sections 4.1 and 4.2. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "629 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper reports error bars appropriately and includes correctly defined information regarding the statistical significance of the experiments, ensuring the transparency and reliability of the results. ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 18}, {"type": "text", "text": "641 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n642 example, train/test split, initialization, random drawing of some parameter, or overall   \n643 run with given experimental conditions).   \n644 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n645 call to a library function, bootstrap, etc.)   \n646 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n647 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n648 of the mean.   \n649 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n650 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n651 of Normality of errors is not verified.   \n652 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n653 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n654 error rates).   \n655 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n656 they were calculated and reference the corresponding figures or tables in the text.   \n657 8. Experiments Compute Resources   \n658 Question: For each experiment, does the paper provide sufficient information on the com  \n659 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n660 the experiments?   \n661 Answer: [Yes]   \n662 Justification: The paper provides detailed information regarding the computer resources   \n663 used in Sections 4.1, and time of execution in Table 1.   \n664 Guidelines:   \n665 \u2022 The answer NA means that the paper does not include experiments.   \n666 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n667 or cloud provider, including relevant memory and storage.   \n668 \u2022 The paper should provide the amount of compute required for each of the individual   \n669 experimental runs as well as estimate the total compute.   \n670 \u2022 The paper should disclose whether the full research project required more compute   \n671 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n672 didn\u2019t make it into the paper).   \n673 9. Code Of Ethics   \n674 Question: Does the research conducted in the paper conform, in every respect, with the   \n675 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n676 Answer: [Yes]   \n677 Justification: The research conducted in the paper is in full compliance with the NeurIPS   \n678 Code of Ethics.   \n679 Guidelines:   \n680 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n681 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n682 deviation from the Code of Ethics.   \n683 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n684 eration due to laws or regulations in their jurisdiction).   \n685 10. Broader Impacts   \n686 Question: Does the paper discuss both potential positive societal impacts and negative   \n687 societal impacts of the work performed?   \n688 Answer: [Yes]   \n689 Justification: The paper provides both the potential beneftis and the risks associated with the   \n690 research, ensuring a comprehensive assessment of its societal implications.   \n691 Guidelines:   \n692 \u2022 The answer NA means that there is no societal impact of the work performed.   \n693 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n694 impact or why the paper does not address societal impact.   \n695 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n696 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n697 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n698 groups), privacy considerations, and security considerations.   \n699 \u2022 The conference expects that many papers will be foundational research and not tied   \n700 to particular applications, let alone deployments. However, if there is a direct path to   \n701 any negative applications, the authors should point it out. For example, it is legitimate   \n702 to point out that an improvement in the quality of generative models could be used to   \n703 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n704 that a generic algorithm for optimizing neural networks could enable people to train   \n705 models that generate Deepfakes faster.   \n706 \u2022 The authors should consider possible harms that could arise when the technology is   \n707 being used as intended and functioning correctly, harms that could arise when the   \n708 technology is being used as intended but gives incorrect results, and harms following   \n709 from (intentional or unintentional) misuse of the technology.   \n710 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n711 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n712 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n713 feedback over time, improving the efficiency and accessibility of ML).   \n714 11. Safeguards   \n715 Question: Does the paper describe safeguards that have been put in place for responsible   \n716 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n717 image generators, or scraped datasets)?   \n718 Answer: [NA]   \n719 Justification: The paper does not introduce assets that carry a high risk for misuse, therefore,   \n720 no specific safeguards for data or model release are required.   \n721 Guidelines:   \n722 \u2022 The answer NA means that the paper poses no such risks.   \n723 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n724 necessary safeguards to allow for controlled use of the model, for example by requiring   \n725 that users adhere to usage guidelines or restrictions to access the model or implementing   \n726 safety filters.   \n727 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n728 should describe how they avoided releasing unsafe images.   \n729 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n730 not require this, but we encourage authors to take this into account and make a best   \n731 faith effort.   \n732 12. Licenses for existing assets   \n733 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n734 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n735 properly respected?   \n736 Answer: [Yes]   \n737 Justification: The paper meticulously cites all external assets in references, including code,   \n738 dataset, and models, acknowledging the contributions of their creators and respecting the   \n739 associated licenses and terms of use.   \n740 Guidelines:   \n741 \u2022 The answer NA means that the paper does not use existing assets.   \n742 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n743 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n744 URL.   \n745 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n746 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n747 service of that source should be provided.   \n748 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n749 package should be provided. For popular datasets, paperswithcode.com/datasets   \n750 has curated licenses for some datasets. Their licensing guide can help determine the   \n751 license of a dataset.   \n752 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n753 the derived asset (if it has changed) should be provided.   \n754 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n755 the asset\u2019s creators.   \n756 13. New Assets   \n757 Question: Are new assets introduced in the paper well documented and is the documentation   \n758 provided alongside the assets?   \n759 Answer: [Yes]   \n760 Justification: The novel universal image restoration framework introduced in the paper is   \n761 well documented, and the documentation is provided alongside the model in Sections 3,   \n762 offering comprehensive details for replication and application.   \n763 Guidelines:   \n764 \u2022 The answer NA means that the paper does not release new assets.   \n765 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n766 submissions via structured templates. This includes details about training, license,   \n767 limitations, etc.   \n768 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n769 asset is used.   \n770 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n771 create an anonymized URL or include an anonymized zip file.   \n772 14. Crowdsourcing and Research with Human Subjects   \n773 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n774 include the full text of instructions given to participants and screenshots, if applicable, as   \n775 well as details about compensation (if any)?   \n776 Answer: [NA]   \n777 Justification: The paper does not engage in crowdsourcing experiments or research with   \n778 human subjects therefore, it does not include participant instructions, screenshots, or details   \n779 about compensation.   \n780 Guidelines:   \n781 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n782 human subjects.   \n783 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n784 tion of the paper involves human subjects, then as much detail as possible should be   \n785 included in the main paper.   \n786 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n787 or other labor should be paid at least the minimum wage in the country of the data   \n788 collector.   \n789 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n790 Subjects   \n791 Question: Does the paper describe potential risks incurred by study participants, whether   \n792 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n793 approvals (or an equivalent approval/review based on the requirements of your country or   \n794 institution) were obtained? ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "796 Justification: The paper does not involve research with human subjects, so there are no   \n797 participant risks to disclose, and no Institutional Review Board (IRB) approvals or equivalent   \n798 reviews were required.   \n799 Guidelines:   \n800 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n801 human subjects.   \n802 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n803 may be required for any human subjects research. If you obtained IRB approval, you   \n804 should clearly state this in the paper.   \n805 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n806 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n807 guidelines for their institution.   \n808 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n809 applicable), such as the institution conducting the review. ", "page_idx": 22}]