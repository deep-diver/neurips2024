[{"figure_path": "4czwwExZKQ/tables/tables_28_1.jpg", "caption": "Table S1: The human annotation benchmark of one-photon (1p) hemisphere dataset, 3 mice labeled by 4 human annotators each. The datasets include 28,010 cell candidates and 112,040 annotations. The rows stand for the annotators who were used as ground truth for evaluations, whereas the columns stand for those who are being evaluated. Teal stands for the accuracy of accepting cells that were accepted by the evaluator and red stands for the rejection. Evaluators disagreed on several occasions, leading to average (balanced) accuracy of ~80%. The movie images and the label distributions can be found in Fig. S13.", "description": "This table presents the results of a human annotation benchmark for a one-photon hemisphere dataset.  Four annotators labeled 28,010 cell candidates, resulting in 112,040 annotations. The table shows the agreement between annotators, represented by accuracy scores for accepting and rejecting cell candidates. The average balanced accuracy across all annotators was approximately 80%. More details and visualizations are available in Figure S13.", "section": "2 Results"}, {"figure_path": "4czwwExZKQ/tables/tables_28_2.jpg", "caption": "Table S2: The human annotation benchmark of one-photon (1p) neocortex dataset with many false positives (2,345 out of 6,691 cell candidates) and 26,764 annotations. The rows stand for the annotators who were used as ground truth for evaluations, whereas the columns stand for those who are being evaluated. Teal stands for the accuracy of acceptance, red stands for the accuracy of rejection. Evaluators disagreed on several occasions, the balanced accuracy between annotators is ~ 83%. The movie images and the label distribution can be found in Fig S9.", "description": "This table presents the results of a human annotation benchmark for a one-photon neocortex dataset.  The dataset contained many false positives (incorrectly identified cells).  Four annotators independently labeled the cells. The table shows the accuracy of each annotator in identifying true positives (correctly labeled cells) and true negatives (correctly rejected cells), compared against the labels from each of the other annotators.  The average balanced accuracy across annotators was approximately 83%.  Figure S9 provides more visual information about the data.", "section": "2 Results"}, {"figure_path": "4czwwExZKQ/tables/tables_28_3.jpg", "caption": "Table S3: The human annotation benchmark of two-photon (2p) Ca2+ movie with 5,276 cell candidates and 21,104 annotations. The rows stand for the annotators who were used as ground truth for evaluations, whereas the columns stand for those who are being evaluated. Teal stands for the accuracy of acceptance, red stands for the accuracy of rejection. Evaluators disagreed on several occasions, the balanced accuracy between annotators is ~ 78%. The movie images and the label distribution can be found in Fig S10.", "description": "This table presents the results of a human annotation benchmark for a two-photon calcium imaging dataset.  It shows the agreement between different annotators on whether each of 5,276 cell candidates is a true cell or a false positive.  The teal numbers represent the accuracy of correctly identifying cells, and the red numbers represent the accuracy of rejecting false positives.  The average balanced accuracy across annotators was approximately 78%.", "section": "2 Results"}, {"figure_path": "4czwwExZKQ/tables/tables_29_1.jpg", "caption": "Table S4: The significance levels for the Wilcoxon signed-rank tests with Bonferroni-Holm corrections performed in Fig. 3B. Each query method had twelve data points (3 datasets and 4 annotators each). For each annotation, we computed the fraction of the boundary samples.", "description": "This table shows the p-values from Wilcoxon signed-rank tests with Bonferroni-Holm correction, comparing the fraction of boundary samples selected by different query algorithms (Random, CAL, DAL, and DCAL with different weights).  The tests assess statistical significance of differences in boundary sample selection between the methods.  The data comes from Figure 3B of the paper, with 12 data points per algorithm.", "section": "2.3 Discriminative-confidence active learning query algorithm for cell sorting"}, {"figure_path": "4czwwExZKQ/tables/tables_29_2.jpg", "caption": "Table S5: The significance levels for the Wilcoxon signed-rank tests with Bonferroni-Holm corrections performed in Fig. 3D. Each query method had twelve data points (3 datasets and 4 annotators each). For each annotation, we computed the average cosine distances between the boundary samples. To facilitate a fair comparison, for each query algorithm, we randomly subsampled the number of boundary samples to match the query algorithm with the lowest number of boundary samples.", "description": "This table shows the p-values from Wilcoxon signed-rank tests with Bonferroni-Holm corrections, assessing the statistical significance of differences in average cosine distances between boundary samples across different active learning query algorithms. The tests compared algorithms' effectiveness in selecting diverse boundary samples.  Twelve data points per algorithm (3 datasets \u00d7 4 annotators) were used. Random subsampling was applied to ensure fair comparisons between algorithms with varying numbers of selected samples.", "section": "2.4 Geometrical interpretation of discriminative-confidence queries"}, {"figure_path": "4czwwExZKQ/tables/tables_29_3.jpg", "caption": "Table S6: ActSort performance after sorting 5% of the hemisphere Ca2+ datasets (across 3 mice). Rows, evaluation metrics, where ACC is balanced accuracy, TPR is true positive rate, TNR is true negative rate, and AUC is the area under the curve. Columns, active learning method, where the fraction next to DCAL refers to the corresponding weight. Each entry shows the average metric value for sorting up to 5% across 12 annotators over three datasets. The entire sorting performance as a function of the percentage of sorted cells up to 50% is illustrated in Figs. 4B-D and S13B.", "description": "This table presents the performance of ActSort and other methods after annotating only 5% of the cells in the hemisphere dataset.  It shows the balanced accuracy, true positive rate, true negative rate, precision, F1-score, and AUC for each method.  The results are averaged over 12 annotators and three datasets.  The full performance of each method up to 50% annotation is illustrated in figures 4B-D and S13B in the paper.", "section": "2.5 Performance evaluations of ActSort on the human annotated benchmarks"}, {"figure_path": "4czwwExZKQ/tables/tables_30_1.jpg", "caption": "Table S6: ActSort performance after sorting 5% of the hemisphere Ca2+ datasets (across 3 mice). Rows, evaluation metrics, where ACC is balanced accuracy, TPR is true positive rate, TNR is true negative rate, and AUC is the area under the curve. Columns, active learning method, where the fraction next to DCAL refers to the corresponding weight. Each entry shows the average metric value for sorting up to 5% across 12 annotators over three datasets. The entire sorting performance as a function of the percentage of sorted cells up to 50% is illustrated in Figs. 4B-D and S13B.", "description": "This table presents the performance of ActSort and other active learning algorithms after annotating 5% of the cells in the hemisphere dataset.  The metrics used to evaluate performance are balanced accuracy (ACC), true positive rate (TPR), true negative rate (TNR), precision, F-score, and area under the ROC curve (AUC).  The table shows that DCAL outperforms other methods, achieving near human-level performance with only 5% of the cells labeled.", "section": "2.5 Performance evaluations of ActSort on the human annotated benchmarks"}, {"figure_path": "4czwwExZKQ/tables/tables_30_2.jpg", "caption": "Table S7: ActSort performance on 2% of the batch sorted cell candidates. Rows, evaluation metrics, where ACC is balanced accuracy, TPR is true positive rate, TNR is true negative rate, and AUC is the area under the curve. Columns, active learning method, where the fraction next to DCAL refers to the corresponding weight. Each entry shows the average metric value for sorting up to 2% across 64 augmented annotators. The entire sorting performance as a function of the percentage of sorted cells up to 5% is illustrated in Figs. 4E-G and S13C.", "description": "This table presents the performance of ActSort's active learning algorithms when annotating only 2% of the cells in the batch-processed dataset.  It shows various metrics (balanced accuracy, true positive rate, true negative rate, precision, F-score, and AUC) for different active learning strategies (random sampling, CAL, DAL, and DCAL with varying weights).  The results highlight the relative performance of these methods at a very early stage of the annotation process, before convergence.", "section": "2.5 Performance evaluations of ActSort on the human annotated benchmarks"}]