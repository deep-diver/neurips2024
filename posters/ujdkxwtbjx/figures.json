[{"figure_path": "ujDKXWTbJX/figures/figures_1_1.jpg", "caption": "Figure 1: The comparison of existing work and our method in task performance and the total cost.", "description": "This figure compares the performance and cost of three different models: KPMath-DSMath-7B, Deepseek-Math-7B-RL, and JiuZhang3.0-7B.  The left bar chart shows the accuracy of each model on three benchmark datasets (GSM8k, MATH, and ASDiv).  The right bar chart shows the estimated total cost (in USD) for each model, highlighting the significantly lower cost of JiuZhang3.0-7B.  This demonstrates that JiuZhang3.0-7B achieves state-of-the-art performance at a much lower cost than existing methods.", "section": "Introduction"}, {"figure_path": "ujDKXWTbJX/figures/figures_3_1.jpg", "caption": "Figure 2: The pipeline of our approach. We first initialize the data synthesis LLM by distilling the knowledge from GPT-4 on randomly sampled data, then boost it using the high-value data selected by gradient-based value estimation strategy, finally utilize it for synthesizing data to train JiuZhang3.0.", "description": "This figure illustrates the JiuZhang3.0 model training pipeline.  It starts with a multi-source corpus of mathematical texts which are sampled and fed, along with prompts, into GPT-4 to create an initial knowledge distillation dataset.  This dataset is used to train a small LLM for data synthesis.  A gradient-based method selects the most valuable texts for a second, enhanced KD dataset, further improving the synthesis LLM. This enhanced LLM generates 4.6B synthetic data points that are filtered and used to pre-train the JiuZhang3.0 model in three sizes (7B, 8B, and 8x7B).", "section": "3 Approach"}, {"figure_path": "ujDKXWTbJX/figures/figures_8_1.jpg", "caption": "Figure 3: Performance changes with the increasing of the pre-training data proportion for our approach. We also show the best-performed base LLM DeepSeekMath-7B using dashed line.", "description": "This figure shows how the performance of the JiuZhang3.0 model changes as the proportion of synthetic pre-training data increases.  It compares the performance of JiuZhang3.0 trained on Mistral-7B and LLaMA-3-8B base models across three different mathematical reasoning datasets (GSM8k, MATH, and ASDiv).  A dashed line represents the performance of the DeepSeekMath-7B model, a strong baseline model, for comparison. The results demonstrate that using a larger proportion of synthetic data improves the model's performance, and that the LLaMA-3-8B base model shows better performance than the Mistral-7B base model.", "section": "4.3 Further Analysis"}, {"figure_path": "ujDKXWTbJX/figures/figures_18_1.jpg", "caption": "Figure 4: Hyper-parameter tuning results of the pre-training data proportion and high-value data amount, under the tool manipulation and natural language reasoning settings, respectively.", "description": "This figure displays the results of hyperparameter tuning experiments for the JiuZhang3.0 model.  It shows how the model's performance on MATH and GSM8k datasets changes with different ratios of natural language reasoning and tool manipulation data used for pre-training (left panel), and with varying amounts of high-value data included during training (right panel). The plots illustrate the impact of these hyperparameters on the model's ability to perform mathematical reasoning tasks, helping to determine optimal settings for enhanced performance.", "section": "4.3 Further Analysis"}, {"figure_path": "ujDKXWTbJX/figures/figures_28_1.jpg", "caption": "Figure 2: The pipeline of our approach. We first initialize the data synthesis LLM by distilling the knowledge from GPT-4 on randomly sampled data, then boost it using the high-value data selected by gradient-based value estimation strategy, finally utilize it for synthesizing data to train JiuZhang3.0.", "description": "This figure illustrates the process of training the JiuZhang3.0 model.  It starts with initializing a small LLM for math problem synthesis by knowledge distillation from GPT-4 using randomly sampled data.  Then, it boosts this model's performance by retraining it with high-value data selected using a gradient-based value estimation method. Finally, this improved model generates a large dataset of synthetic math problems used for pre-training the JiuZhang3.0 model.", "section": "3 Approach"}]