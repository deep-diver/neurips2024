[{"figure_path": "ujDKXWTbJX/tables/tables_6_1.jpg", "caption": "Table 1: Results on 6 datasets in the natural language reasoning setting. The best and second-best ones among LLMs with similar scales are marked in bold and underlined respectively.", "description": "This table presents the performance of various large language models (LLMs) on six different datasets designed for natural language reasoning tasks.  The models are categorized by size and type (e.g., open-source, fine-tuned).  The table highlights the best and second-best performing models within similar size categories for each dataset, providing a comparison of performance across different LLMs.", "section": "4.2 Results and Analysis"}, {"figure_path": "ujDKXWTbJX/tables/tables_7_1.jpg", "caption": "Table 2: Results on 5 other datasets with different data formats or related to interdisciplinary fields, and we abbreviate MMLU-STEM into M-STEM. The best and second-best methods among LLMs with similar scales are marked in bold and underlined respectively.", "description": "This table presents the performance of various LLMs on five benchmark datasets that differ in data format or domain.  The datasets assess the models' abilities in different aspects of reasoning and knowledge beyond purely mathematical ones, such as general knowledge and scientific understanding.  The table highlights the best and second-best performing models within similar parameter scale groups, indicating the relative strengths of different architectures and training methodologies on these diverse tasks.", "section": "4.2 Results and Analysis"}, {"figure_path": "ujDKXWTbJX/tables/tables_7_2.jpg", "caption": "Table 3: Results on 6 mathematical reasoning datasets under the tool manipulation setting. The best and second-best methods are marked in bold and underlined respectively.", "description": "This table presents the performance of various LLMs on six mathematical reasoning datasets using a tool manipulation setting.  The best and second-best performing models (among those with similar scales) are highlighted for each dataset, providing a comparison of different LLMs' abilities to solve mathematical problems by using external tools.", "section": "4.2 Results and Analysis"}, {"figure_path": "ujDKXWTbJX/tables/tables_8_1.jpg", "caption": "Table 1: Results on 6 datasets in the natural language reasoning setting. The best and second-best ones among LLMs with similar scales are marked in bold and underlined respectively.", "description": "This table presents the results of several large language models (LLMs) on six different datasets designed for evaluating natural language reasoning capabilities, specifically focusing on mathematical problem-solving.  The models are categorized by size, and their performance (accuracy) is shown for each dataset.  The best and second-best performing models of similar sizes are highlighted for easier comparison.", "section": "4.2 Results and Analysis"}, {"figure_path": "ujDKXWTbJX/tables/tables_16_1.jpg", "caption": "Table 1: Results on 6 datasets in the natural language reasoning setting. The best and second-best ones among LLMs with similar scales are marked in bold and underlined respectively.", "description": "This table presents the performance of various large language models (LLMs) on six different datasets designed for evaluating natural language reasoning capabilities, specifically focusing on mathematical problem-solving.  The results show the accuracy of each model on each dataset, allowing for comparison of performance across models of similar scale.  The best and second-best performing models within each size category are highlighted for easy identification.", "section": "4.2 Results and Analysis"}, {"figure_path": "ujDKXWTbJX/tables/tables_16_2.jpg", "caption": "Table 6: The estimated cost of different LLMs using the official GPT-4 API and 8 nodes of 8\u00d7A100 GPU servers for training.", "description": "This table presents a cost comparison of training three different large language models (LLMs) for mathematical reasoning.  It breaks down the costs into two components: using the OpenAI GPT-4 API for data synthesis, and renting AWS GPU servers for model training. The table shows the number of API calls (broken down by input and output tokens), the number of data points synthesized, and the server time required for synthesis and training for each model.  Finally, it provides the total estimated cost in USD for each LLM.", "section": "A Cost Estimation"}]