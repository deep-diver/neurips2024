[{"type": "text", "text": "Real-Time Selection Under General Constraints via Predictive Inference ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuyang Huo1\u2217 Lin Lu1\u2217 Haojie $\\mathbf{Ren}^{2\\dagger}$ Changliang Zou1\u2020 ", "page_idx": 0}, {"type": "text", "text": "1School of Statistics and Data Sciences, LPMC, KLMDASR and LEBPS, Nankai University, Tianjin, China 2School of Mathematical Sciences, Shanghai Jiao Tong University, Shanghai, China huoyynk@gmail.com, linlu102099@gmail.com haojieren@sjtu.edu.cn, zoucl@nankai.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Real-time decision-making gets more attention in the big data era. Here, we consider the problem of sample selection in the online setting, where one encounters a possibly infinite sequence of individuals collected over time with covariate information available. The goal is to select samples of interest that are characterized by their unobserved responses until the user-specified stopping time. We derive a new decision rule that enables us to find more preferable samples that meet practical requirements by simultaneously controlling two types of general constraints: individual and interactive constraints, which include the widely utilized False Selection Rate (FSR), cost limitations, diversity of selected samples, etc. The key elements of our approach involve quantifying the uncertainty of response predictions via predictive inference and addressing individual and interactive constraints in a sequential manner. Theoretical and numerical results demonstrate the effectiveness of the proposed method in controlling both individual and interactive constraints. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent times, the field of real-time decision has flourished significantly, primarily driven by the exponential growth of available data in both the tech industry and computer science. We consider here a typical application of real-time decision, the problem of online sample selection [2, 5]. For instance, online recruitment systems utilize machine learning algorithms to sequentially choose qualified candidates rather than waiting for all (future) candidates\u2019 information to be collected [14]. Additionally, recommendation systems have now become commonplace in providing real-time suggestions for content (e.g., news articles, short videos) with potential high click-through rates to users [24]. Common situations also can be found in real-time precision marketing [43]. ", "page_idx": 0}, {"type": "text", "text": "We describe the online sample selection problem as follows: samples (individuals) characterized by covariates $\\mathbf{X}_{t}\\in\\mathbb{R}^{d}$ arrive sequentially while their responses $Y_{t}\\in\\mathbb{R}$ remain unobserved throughout the process. The data pairs $(\\mathbf{X},Y)$ of each time are i.i.d. random vectors. At each time point, the analyst is faced with the task of deciding whether to select the current observation based on certain predetermined criteria related to $Y_{t}$ , and this selection process continues until a specific stopping rule is triggered. For example, $Y_{t}$ is the score that measures how one candidate ftis a given job position in the recruitment system and the human resource agencies aim to prioritize candidates with higher $Y_{t}$ , such as $Y_{t}\\ge b$ . Or $Y_{t}$ is a binary variable where $Y_{t}=1/0$ means accepting or rejecting the offer, and the companies wish to find those individuals with $Y_{t}=1$ based on the $X_{t}$ . ", "page_idx": 0}, {"type": "text", "text": "Our Goal and Motivation. Our goal is to sequentially select samples whose unobserved responses $Y_{t}$ \u2019s are in the specified target region. A natural idea is to make decisions based on the prediction value of $Y_{t}$ from models associated with $(\\mathbf{X},Y)$ built on some historical data. However, neglecting the uncertainty in predictions could result in numerous false decisions, i.e., selecting those samples whose true responses are beyond the specified region. To measure the selection uncertainty, some existing works reformulate sample selection as a hypothesis testing problem and focus on controlling the online false discovery rate (FDR) [16, 33]; see more discussions and literature in Section 1.2. However, in addition to quantifying statistical uncertainty, online selections often need to take into account various constraints to find informative samples in practice, for example, cost limitations, the impacts of some covariates or the diversity of candidates in the online recruitment [39]. Hence, it\u2019s necessary to explore the covariate space to satisfy these requirements. This motivates us to investigate how to efficiently implement online sample selection with statistical guarantees under various constraints. ", "page_idx": 1}, {"type": "text", "text": "To address this issue, we summarize common constraints into two types, i.e., individual constraints and interactive constraints. The former one is relevant to the cost of selected samples, and one typical example is the fundamental and crucial criterion, false selection rate (FSR), which quantifies the proportion of falsely selected samples and is equivalent to the well-adopted FDR. The latter constraint captures the interactive influence among selected samples and is regarded as some kind of quadratic constraint on some pairwise functions, such as the similarity or diversity among selected samples. ", "page_idx": 1}, {"type": "text", "text": "While the individual constraint associated with online FDR control has gained some attentions [16, 18], it alone fails to capture the nuanced pairwise relationships among different samples. To bridge this gap, we introduce interactive constraints, which are pivotal within our framework. Building upon individual constraints, the interactive criteria significantly expand the range of constraints our method can control. Integrating two distinct types of constraints into a unified framework makes it easier to create practical algorithms and ensures theoretical guarantees. ", "page_idx": 1}, {"type": "text", "text": "A motivating example: candidate screening. As an example, in recruitment, screening from the resumes arriving sequentially to determine viable candidates who can get into interview processes is an important problem in human resource management [13, 35]. In this case, one may be interested in: (1) controlling the online FSR to enhance resource efficiency [21], and (2) maintaining a desired level of candidate diversity during the screening process, thereby reducing bias [23, 44]. The individual and interactive constraints and the novel real-time sample selection procedure we propose can solve this problem precisely. See Section 4.2 for the details and more real data examples. ", "page_idx": 1}, {"type": "text", "text": "1.1 Our Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this paper, we design a novel and flexible online selection rule to effectively ensure the above two types of constraints are under control at pre-specified levels simultaneously, named as \u201cIndividual and Interactive Constrained Online Selection\" (II-COS). The main idea stems from an oracle model based on the local false discovery rate (lFDR) [12, 38], which is involved in offering valid evidence on whether $Y_{t}$ is our interest at each time point. With some appropriately chosen evaluating functions, the II-COS procedure entails validating whether the estimates of constraints are controlled. Simulated and real-data examples clearly demonstrate the superiority of the II-COS in terms of both online individual and interactive criteria control. ", "page_idx": 1}, {"type": "text", "text": "To the best of our knowledge, this is the first work to systematically bridge the predictive inference and online selection procedure with various constraints. Our contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Under a unified framework, the II-COS addresses how to implement online predictive selection sufficiently in consideration of both individual and interactive constraints. It is flexible to characterize the selective uncertainty and trade-off the sampling efficiency and practical limitations in the covariate space.   \n\u2022 Under mild conditions, we establish the theoretical guarantee that the II-COS is able to control both individual and interactive constraints simultaneously and asymptotically under one given stopping rule.   \n\u2022 The II-COS is model-agnostic in the sense that its implementation is applicable to any (appropriate) learning algorithms. Extensive numerical experiments indicate that the II-COS can significantly outperform existing ones while yielding effective constraints control. ", "page_idx": 1}, {"type": "text", "text": "1.2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our proposed method is built upon two fundamental pillars: (1) quantifying the uncertainty of response predictions using predictive inference; and (2) systematically addressing individual and interactive constraints in a sequential manner. Our work is intricately connected to the fields of predictive inference and online multiple testing. Here we briefly review literature on these two topics. ", "page_idx": 2}, {"type": "text", "text": "Predictive Inference. One key ingredient of our proposed method is predictive/conformal inference. Conformal inference [42, 34] provides a powerful and flexible tool to achieve algorithm-agnostic uncertainty quantification of predictions. Conventionally, conformal inference aims to build the prediction intervals and enjoys valid and distribution-free properties by leveraging data exchangeability [25, 3]. Taking a different but related perspective from multiple-testing, Bates et al. [7] pioneered a method to construct conformal $p$ -values to detect outliers with finite-sample FDR control. Building upon this, recent advancements have improved detection power by involving more information or performing model selection [50, 26, 27, 51, 46]. The most related works are Jin and Cand\u00e8s [21] and Wu et al. [45], which considered a similar scenario that one would like to select some individuals of interest by controlling FDR or maximizing the diversity of selected samples in an off-line setting. Their methods are based on the conformal $p$ -values or lFDR constructed with the predicted response values, respectively. Besides the fundamental difference between online and offilne paradigms, our framework for characterizing various constraints poses additional challenges in how to select samples sequentially since we have multiple goals to achieve. ", "page_idx": 2}, {"type": "text", "text": "Online Multiple Testing. When only considering individual constraint as FSR control, the online sample selection can be reformulated as online multiple testing problem. Methods for online multiple testing have received much recent attention and were pioneered by Foster and Stine [16] who proposed the so-called $\\alpha$ -investing strategy, which was later built upon and generalized [1, 29, 30, 18, 19]. The key idea in $\\alpha$ -investing and its generalizations is to compare $p$ -values with dynamic thresholds and gain some extra $\\alpha$ -wealth for each rejection. We refer to Robertson et al. [33] for a thorough overview. Those rules suffer from the \u201calpha-death\" issue to some extent [29], which means a permanent end to decision-making when the decision threshold is too small, i.e., the online procedure stops early. This phenomenon occurs in many existing online multiple testing algorithms, as discussed in [29]. Along a different direction, Gang et al. [17] developed a new class of structure-adaptive sequential testing (SAST) rules built on the lFDR to avoid the alpha-death issue. The SAST serves as a building block for developing our II-COS procedure and can be essentially seen as a special case of ours. Later on, Ao et al. [4] reformulated online multiple testing procedure into an online knapsack problem, providing novel policies with near-optimal regret guarantees. Additionally, Xu and Ramdas [48] proposed to use e-values [41] for online multiple testing to address the dependence. However, those existing works do not take predictive inference into account and are concerned only with online error rate control without exploration of the covariate space, which may greatly hamper their applicability. ", "page_idx": 2}, {"type": "text", "text": "2 Individual and Interactive Constrained Online Selection Procedure ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Assume there exists a historical labeled dataset as $\\boldsymbol{\\mathcal{D}}=\\{\\tilde{\\mathbf{X}}_{i},\\tilde{Y}_{i}\\}_{i=1}^{n}$ , where $(\\tilde{\\mathbf{X}}_{i},\\tilde{Y}_{i})$ \u2019s are independent and identically distributed (i.i.d.) from $(\\mathbf{X},Y)$ . A sequence of unlabeled data $\\mathbf{X}_{1},\\mathbf{X}_{2},\\cdot\\cdot\\cdot\\sim\\mathbf{X}=$ $(X_{1},\\cdot\\cdot\\cdot\\,,X_{d})^{\\top}$ arrives in a stream with unknown responses $Y_{1},Y_{2},\\cdot\\cdot\\cdot$ . At each time $t$ , one must make a real-time decision about whether or not to select the $t$ -th individual, which is determined by some pre-specified requirement on $Y_{t}$ . Denote $\\boldsymbol{\\mathcal{A}}$ as the target region of $Y_{t}$ , which differs depending on users\u2019 specifications. For example, in a regression setting, the requirement could be of the form $Y_{t}\\in[a,b],\\bar{(-\\infty,a)}$ or $Y_{t}\\geq b$ . ", "page_idx": 2}, {"type": "text", "text": "Let $\\theta_{t}=\\mathbb{I}\\{Y_{t}\\in\\mathcal{A}\\}$ describe the true state of $Y_{t}$ . Denote a decision rule as ${\\delta_{t}}\\in\\{0,1\\}$ , where $\\delta_{t}=1$ indicates that the $\\mathbf{X}_{t}$ is selected and $\\delta_{t}=0$ otherwise. A false selection is made if $\\delta_{t}=1$ but $\\theta_{t}=0$ . Denote $\\pmb{\\delta}^{t}=\\{\\delta_{i}:i\\leq t\\}$ as the decision rule and $T$ as the time that the procedure stops. Our goal is to build a decision rule $\\delta^{t}$ to select samples with $\\{Y_{t}\\in A\\}$ up to stopping time $T$ such that the following two general types of constraints hold simultaneously. ", "page_idx": 2}, {"type": "text", "text": "Individual Constraint. In practice, one main concern is to control the cost of selecting samples of interest. For example, in online recruitment, companies need to control the proportion of selected unqualified candidates or the average loss when hiring someone who rejects the offer. In such cases, we can assign each selected sample a cost associated with some pre-specified function of the covariate $X$ and control the expected cost associated with time $T$ at the target level. We refer to this requirement as individual constraint and write it as: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nC_{1}(\\delta^{t})=\\mathbb{E}\\left[\\frac{\\sum_{i\\leq t}\\{(1-\\theta_{i})G_{0}(\\mathbf{X}_{i})+\\theta_{i}G_{1}(\\mathbf{X}_{i})\\}\\delta_{i}}{(\\sum_{i\\leq t}\\delta_{i})\\vee1}\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $a\\vee b=\\operatorname*{max}\\{a,b\\}$ and $G_{0}(X)\\geq0$ and $G_{1}(X)\\geq0$ with $G_{0}\\neq G_{1}$ are the costs corresponding to $\\theta=0$ and $\\theta=1$ , respectively. Here, we take expectation due to the randomness of $\\theta_{1},\\cdots,\\theta_{t}$ . ", "page_idx": 3}, {"type": "text", "text": "For example, when we simply choose $G_{0}(\\mathbf{X})=1$ and $G_{1}(\\mathbf{X})=0$ , the individual constraint is the popular false selection rate (FSR), i.e. ", "page_idx": 3}, {"type": "equation", "text": "$$\nC_{1}(\\delta^{t})=\\mathrm{FSR}(\\delta^{t})=\\mathbb{E}\\left[\\frac{\\sum_{i\\leq t}(1-\\theta_{i})\\delta_{i}}{(\\sum_{i\\leq t}\\delta_{i})\\vee1}\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The FSR is essentially equivalent to the well-adopted FDR in multiple testing literature, which is a useful tool to maintain the ability to reliably select samples of interest without excessively false selections [9]. Some works on online FDR control have been well studied. [16, 1]. ", "page_idx": 3}, {"type": "text", "text": "The individual constraints alone cannot capture the pairwise relationship among different samples.   \nWe address this by introducing interactive constraints below. ", "page_idx": 3}, {"type": "text", "text": "Interactive Constraint Another common concern is the interactive constraint, which involves choosing more preferable samples. For example, companies would like to retain candidates with a diverse range of backgrounds and experiences in online recruitment, or real-time suggested contents are required to avoid homogeneity in recommendation systems. Here, we introduce a bi-variate weight function $g(\\mathbf{X},\\mathbf{X}^{\\prime})$ to evaluate the interaction between selected samples. Denote $\\mathrm{PC}(\\delta^{\\mathrm{t}})=$ $\\sum_{1\\leq\\mathrm{i}<\\mathrm{j}\\leq\\mathrm{t}}\\mathrm{g}(\\mathbf{X}_{\\mathrm{i}},\\mathbf{X}_{\\mathrm{j}})\\theta_{\\mathrm{i}}\\theta_{\\mathrm{j}}\\delta_{\\mathrm{i}}\\delta_{\\mathrm{j}}$ , $\\mathrm{PS}(\\pmb{\\delta}^{\\mathrm{t}})=\\sum_{1\\leq\\mathrm{i}<\\mathrm{j}\\leq\\mathrm{t}}\\theta_{\\mathrm{i}}\\theta_{\\mathrm{j}}\\delta_{\\mathrm{i}}\\delta_{\\mathrm{j}}$ . We define the interactive constraint as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tilde{C}_{2}(\\pmb{\\delta}^{t})=\\mathbb{E}\\left[\\frac{\\mathrm{PC}(\\pmb{\\delta}^{\\mathrm{t}})}{\\mathrm{PS}(\\pmb{\\delta}^{\\mathrm{t}})}\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, since only the correctly selected samples are of interest, the constraint is concerned with the average mutual effects between the correctly selected ones rather than all selected ones. When choosing the function $g$ as some similarities, controlling $\\tilde{C}_{2}(\\pmb{\\delta}^{t})$ at a specified constant $K$ , i.e., $\\tilde{C}_{2}(\\pmb{\\delta}^{t})\\leq K$ , is controlling the expected similarity (ES). It is equivalent to requiring that correctly selected samples exhibit certain diversity and rich information in the covariate space of interest. ", "page_idx": 3}, {"type": "text", "text": "Typically, one useful choice for $g(\\mathbf{X},\\mathbf{X}^{\\prime})$ is the weighted RBF kernel $\\begin{array}{r l}{g(\\mathbf{X},\\mathbf{X}^{\\prime})}&{{}=}\\end{array}$ $\\begin{array}{r}{\\exp\\left\\{-\\frac{1}{\\sigma^{2}}\\sum_{k=1}^{d}w_{k}(X_{k}-X_{k}^{'})^{2}\\right\\}}\\end{array}$ with parameter $\\sigma\\,>\\,0$ to measure the similarity between two independent $\\mathbf{X}$ and $\\mathbf{X^{\\prime}}$ . The RBF kernel is a common and widely embraced choice in machine learning [49, 28]. Here, $\\{w_{1},\\ldots,w_{d}\\}$ are some given weights per users\u2019 needs. For instance, if one is just interested in the effects of the $k$ -th feature, then simply $w_{k}=1$ and $w_{j}=0$ for $j\\neq k$ . Specifically, the case that $w_{k}=1$ for all $k=1,\\dots,d$ is chosen in Section 4. We also consider other similarity choices of $g(\\mathbf{X},\\mathbf{X}^{\\prime})$ , such as the cosine similarity $g(\\mathbf{X},\\mathbf{X}^{\\prime})=\\mathbf{X}^{\\top}\\mathbf{X}^{\\prime}/(\\|\\mathbf{X}\\|_{2}\\|\\mathbf{X}^{\\prime}\\|_{2})$ [52]. ", "page_idx": 3}, {"type": "text", "text": "Due to the randomness in the denominator, it turns out controlling (3) directly is not easy. Instead, we employ a modified interactive constraint, ", "page_idx": 3}, {"type": "equation", "text": "$$\nC_{2}(\\delta^{t})=\\frac{\\mathbb{E}\\left[\\mathrm{PC}(\\delta^{\\mathrm{t}})\\right]}{\\mathbb{E}\\left[\\mathrm{PS}(\\delta^{\\mathrm{t}})\\right]}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The constraint (4) aims to control a ratio of expectations, which is still a reasonable interactive measure. In numerical studies, we see that $\\tilde{C}_{2}(\\bar{\\pmb\\delta}^{t})$ in (3) and $C_{2}(\\delta^{t})$ in (4) yield almost identical patterns. An illustrative example can be found in Appendix D.1. ", "page_idx": 3}, {"type": "text", "text": "In sum, the goal is to select samples of interest by a decision rule $\\delta^{T}$ controlling both the individual and interactive constraints until stopping time $T$ , i.e., $C_{1}(\\delta^{T})\\,\\leq\\,\\alpha$ and $C_{2}(\\delta^{T})\\ \\le\\ K$ . We emphasize that $C_{1}(\\delta^{T})$ and $C_{2}(\\delta^{T})$ as well as their pre-specified levels $\\alpha$ and $K$ can be chosen up to the practical applications. ", "page_idx": 3}, {"type": "text", "text": "2.2 Oracle Selection Procedure ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To design a general rule that is valid for any arbitrary stopping time $T$ , we consider controlling the constraints at each time $t$ in an online fashion, such that $\\operatorname{sup}_{t\\in\\mathbb{N}}\\bar{C}_{1}(\\delta^{t})\\leq\\alpha$ and $\\operatorname*{sup}_{t\\in\\mathbb{N}}C_{2}(\\pmb{\\delta}^{t})\\leq K$ . ", "page_idx": 4}, {"type": "text", "text": "Since $Y_{t}$ is unavailable, we consider utilizing predictive inference to measure the suspicious patterns. Let $\\mu(\\mathbf{x}):=Y\\mid\\mathbf{X}=\\mathbf{x}$ be the regression or classification model associated with $(\\mathbf{X}_{t},Y_{t})$ , and one reliable estimate as $\\widehat{\\mu}(\\cdot)$ , being estimated on the labeled data $\\mathcal{D}$ with some machine learning algorithm. Denote $W_{t}=\\widehat{\\mu}(\\mathbf{X}_{t})$ as a predicted value of $Y_{t}$ and assume that $\\widehat{\\mu}(\\cdot)$ is a bijection almost surely. The bijection assumption is considerably mild and widely adopted for the identyification of each $X_{t}$ in the predictive inference framework [45]. The $\\theta_{t}=\\mathbb{I}(Y_{t}\\in\\mathcal{A})$ is Bernoulli $(\\pi)$ distributed with $\\pi=\\operatorname*{Pr}(Y_{t}\\in{\\mathcal{A}})$ , and $W_{t}$ can be viewed as generated from one two-group model ", "page_idx": 4}, {"type": "equation", "text": "$$\nW_{t}\\mid\\theta_{t}\\sim(1-\\theta_{t})f_{0}+\\theta_{t}f_{1},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $f_{0}$ and $f_{1}$ denote the probability distribution functions of $W_{t}$ conditional on $Y_{t}\\notin A$ (i.e., $\\theta_{t}=0$ ) and $Y_{t}\\in A$ , respectively. Then, the conditional probability of $Y_{t}\\not\\in{\\mathcal{A}}$ is ", "page_idx": 4}, {"type": "equation", "text": "$$\nL_{t}=\\operatorname*{Pr}(\\theta_{t}=0\\mid W_{t})=\\frac{(1-\\pi)f_{0}(W_{t})}{f(W_{t})},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $f=(1-\\pi)f_{0}+\\pi f_{1}$ . The $L_{t}$ coincides with the local FDR in multiple testing literature [12, 17]. With the two-group model (5), we have $\\mathbb{E}[\\theta_{t}\\ |\\ \\mathbf{X}_{t}]=1-L_{t}$ and further notice that the individual constraint $C_{1}(\\delta^{t})$ in (1) can be exactly satisfied if ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{V_{t}}{R_{t}}:=\\frac{\\sum_{i\\leq t}\\{L_{i}G_{0}(\\mathbf{X}_{i})+(1-L_{i})G_{1}(\\mathbf{X}_{i})\\}\\delta_{i}}{(\\sum_{i\\leq t}\\delta_{i})\\vee1}\\leq\\alpha,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "holds. Here, we denote $\\begin{array}{r}{V_{t}=\\sum_{i\\leq t}\\{L_{i}G_{0}(\\mathbf{X}_{i})+(1-L_{i})G_{1}(\\mathbf{X}_{i})\\delta_{i}\\}}\\end{array}$ and the number of selected ones as $\\begin{array}{r}{R_{t}=\\sum_{i\\leq t}\\delta_{i}\\vee1}\\end{array}$ for notational convenience. Especially, when $G_{0}(\\mathbf{X})=1$ , $G_{1}(\\mathbf{X})=0$ , then $\\mathrm{FSR}(\\pmb{\\delta}^{t})$ in (2) can be exactly controlled. ", "page_idx": 4}, {"type": "text", "text": "Accordingly, the interactive constraint $C_{2}(\\delta^{t})\\le K$ in (4) can be achieved if ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{TS}_{t}}{\\mathrm{NS}_{t}}:=\\frac{\\underset{1\\leq i<j\\leq t}{\\sum\\sum}g(\\mathbf{X}_{i},\\mathbf{X}_{j})(1-L_{i})(1-L_{j})\\delta_{i}\\delta_{j}}{\\underset{1\\leq i<j\\leq t}{\\sum\\sum}\\underset{1\\leq i<j\\leq t}{\\sum}(1-L_{i})(1-L_{j})\\delta_{i}\\delta_{j}}\\leq K,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the expected total mutual effects conditional on $\\{\\mathbf{X}_{i}\\}_{i\\leq t}$ and the expected number are denoted as $\\mathrm{T}\\mathrm{S}_{t}$ and $\\mathrm{NS}_{t}$ , respectively. ", "page_idx": 4}, {"type": "text", "text": "Therefore, if $L_{t}$ is known, when a new sample $\\mathbf{X}_{t}$ arrives at time point $t$ , we can perform the decision rule as follows. Note that there is no need to consider interactive effects before the first selection. When $t$ comes before the first selection (i.e, $R_{t-1}=0$ ), the decision rule is $\\delta_{t}=1$ if ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{V_{t-1}+L_{t}G_{0}(\\mathbf{X}_{t})+(1-L_{t})G_{1}(\\mathbf{X}_{t})}{R_{t-1}+1}\\leq\\alpha,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "holds; otherwise, $\\delta_{t}=0$ which means $\\mathbf{X}_{t}$ is not selected. When $\\mathbf{X}_{t}$ arrives with $R_{t-1}\\geq1$ , then $\\delta_{t}=1$ if (6) and ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{TS}_{t-1}+\\left[\\sum_{i\\leq t-1}g(\\mathbf{X}_{i},\\mathbf{X}_{t})(1-L_{i})\\delta_{i}\\right](1-L_{t})}{\\mathrm{NS}_{t-1}+\\left[\\sum_{i\\leq t-1}(1-L_{i})\\delta_{i}\\right](1-L_{t})}\\leq K\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "hold simultaneously; otherwise, $\\delta_{t}=0$ . Note that if we set $C_{1}(\\delta^{t})$ as $\\mathrm{FSR}(\\pmb{\\delta}^{t})$ and choose $K=+\\infty$ , then our method essentially reduces to the same manner as the controlling step of the SAST in Gang et al. [17]. Our proposed method can be seen as a much more generalized and flexible framework for controlling both the individual and the interactive constraints simultaneously in an online fashion. ", "page_idx": 4}, {"type": "text", "text": "We call this method the oracle II-COS (Individual and Interactive Constrained Online Selection). The workflow in Figure 1 shows the procedure of the oracle II-COS. The following result shows that it can exactly achieve our goal. ", "page_idx": 4}, {"type": "text", "text": "Proposition 2.1. Assume $L_{t}$ values are known. Then the oracle II-COS selection rule controls both constraints at any stopping time $T$ , i.e., $C_{1}(\\delta^{T})\\le\\alpha$ and $C_{2}(\\delta^{T})\\le K$ . ", "page_idx": 4}, {"type": "image", "img_path": "wblxm5zdkE/tmp/f7ed5383dfefe75fd2bfa1517c6b4af3bc646bbdadd89d4934bcaaf11c72cceb.jpg", "img_caption": ["Figure 1: The implementation flowchart of the oracle II-COS procedure. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "2.3 Data-driven II-COS Procedure ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As $L_{t}$ is unknown in practice, we propose a data-driven II-COS procedure, which uses a reliable estimation ${\\widehat{L}}_{t}$ for implementation. We resort to a data-splitting strategy: randomly split historical data $\\mathcal{D}$ into  two parts, the training set $\\mathcal{D}_{\\mathrm{tr}}$ and the calibration one $\\mathcal{D}_{\\mathrm{cal}}$ of sizes $n_{0}$ and $n_{1}$ respectively, where $\\mathcal{D}_{\\mathrm{tr}}$ is used for training a predictive model and $\\mathcal{D}_{\\mathrm{cal}}$ is to estimate those unknown parameters. Specifically, we first fit a regression or classification model ${\\widehat{\\mu}}(\\cdot)$ on $\\mathcal{D}_{\\mathrm{tr}}$ , and then obtain predicted values on $\\mathcal{D}_{\\mathrm{cal}}$ , i.e. $\\{\\widehat{\\mu}(\\tilde{\\mathbf{X}}_{i}):(\\tilde{\\mathbf{X}}_{i},\\tilde{Y}_{i})\\in\\mathcal{D}_{\\mathrm{cal}}\\}$ . Note that conditional on $\\mathcal{D}_{\\mathrm{tr}}$ , $\\{\\widehat{\\mu}(\\widetilde{\\mathbf{X}}_{i}):(\\bar{\\mathbf{X}}_{i},\\tilde{Y}_{i})\\in$ $\\mathcal{D}_{\\mathrm{cal}}\\}$ are i.i.d. random variables and have the same distribution as $W_{t}=\\widehat{\\mu}(\\mathbf{X}_{t})$ , so that it can be utilized to estimate (5). ", "page_idx": 5}, {"type": "text", "text": "Therefore, the estimators of $f_{0}$ and $f,\\;\\widehat{f}_{0}$ and ${\\widehat{f}},$ can be obtained by applying the kernel density estimation method to the data $\\{\\widehat{\\mu}(\\tilde{\\mathbf{X}}_{i})\\ :\\ (\\tilde{\\mathbf{X}}_{i},\\tilde{Y}_{i})\\ \\in\\ \\mathcal{D}_{\\mathrm{cal}},\\tilde{Y}_{i}\\ \\notin\\ \\dot{\\mathcal{A}}\\}$ and $\\{\\tilde{\\mu}(\\tilde{\\mathbf{X}}_{i})\\;:\\;(\\tilde{\\mathbf{X}}_{i},\\tilde{Y}_{i})\\;\\in$ $\\mathcal{D}_{\\mathrm{cal}}\\}$ , respectively. And the pr obability $\\pi~=~\\operatorname*{Pr}(Y_{t}~\\in~A)$ can be app roximated by $\\widehat{\\pi}\\ =$ $n_{1}^{-1}\\dot{\\sum}_{(\\tilde{\\mathbf{X}}_{i},\\tilde{Y}_{i})\\in\\mathcal{D}_{\\mathrm{cal}}}\\mathbb{I}(\\tilde{Y}_{i}\\in\\mathcal{A})$ . Further the $L_{t}$ in (5) can be estimated by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{L}_{i}=\\frac{(1-\\widehat{\\pi})\\widehat{f}_{0}(W_{i})}{\\widehat{f}(W_{i})}\\wedge1.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The data-driven II-COS procedure is summarized in Algorithm 1, and it indeed consists of two phases: offilne estimation and online decision. The running time of offilne estimation is not critical. At each time $t$ , the computational complexity is a linear function of the currently selected number $R_{t}$ . More implementation details can be found in Section 4 and Appendix B.1. ", "page_idx": 5}, {"type": "text", "text": "In fact, the proposed II-COS is flexible to trade off the individual and interactive constraints by adjusting the thresholds $\\alpha$ and $K$ . If one is concerned only with individual cost control, then we can set $K=+\\infty$ , with which the interactive constraint is out of work. Similarly, only the interactive effect is of interest when $\\alpha=1$ . Appendix D.2 provides a toy example to illustrate this. ", "page_idx": 5}, {"type": "text", "text": "Before further pursuing, we would discuss the stopping time in practice. It\u2019s worth noting that the specific choice of stopping rule (and thus stopping time) is completely up to the user. For example, when $m\\geq2$ is the desired number of selections, one can set $\\begin{array}{r}{T=\\operatorname*{inf}_{t}\\{t:\\sum_{i=1}^{t}\\delta_{i}=m\\}}\\end{array}$ . Or when $s$ is the total wages for recruitment, one can set $\\begin{array}{r}{T=\\operatorname*{inf}_{t}\\{t:\\sum_{i=1}^{t}\\delta_{i}s_{i}=s\\}}\\end{array}$ where $s_{i}$ is the payroll for each selected candidate. Or $T$ is just chosen as one gi ven deadline. With the use of II-COS, practitioners have the flexibility to design diverse stopping strategies that can adapt seamlessly to their specific applications. In brief, our method is flexible and is appropriate for various goals based on the user\u2019s requirements by choosing different $G_{0}(\\mathbf{X}),G_{1}(\\mathbf{X}),g(\\mathbf{X}_{i},\\mathbf{X}_{j})$ and varied target levels $(\\alpha,K)$ and a user-specified stopping time. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 The data-driven II-COS procedure ", "page_idx": 6}, {"type": "text", "text": "Input: Target levels $\\alpha$ and $K$ , pairwise function $g$ , cost $G_{0}(\\mathbf{X})$ and $G_{1}(\\mathbf{X})$ , stopping time $T$ , interested region $\\boldsymbol{\\mathcal{A}}$ , labeled data $\\mathcal{D}$ , prediction algorithm $\\mathcal{H}$ . Initialization: $t=0$ , $V_{t}=R_{t}=0$ , $\\mathrm{TS}_{t}=\\mathrm{NS}_{t}=0$ ; Decision rule $\\delta^{t}=\\varnothing$ ", "page_idx": 6}, {"type": "text", "text": "Estimation: Randomly split $\\mathcal{D}$ into training set $\\mathcal{D}_{\\mathrm{tr}}$ and calibration set $\\mathcal{D}_{\\mathrm{cal}}$ . On $\\mathcal{D}_{\\mathrm{tr}}$ , fit $\\widehat{\\mu}(\\mathbf{x})$ with $\\mathcal{H}$ . Obtain $\\widehat{\\pi}$ , $\\widehat{f}_{0}$ and $\\widehat{f}$ from $\\mathcal{D}_{\\mathrm{cal}}$ . ", "page_idx": 6}, {"type": "text", "text": "Online decisions: while $t\\leq T$ do ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Set $t=t+1$ . Compute $W_{t}=\\widehat{\\mu}(\\mathbf{X}_{t}),$ , ${\\widehat{L}}_{t}$ by (8) and the similarities $g(\\mathbf{X}_{i},\\mathbf{X}_{t})$ for those $\\delta_{i}=1$ . Replace $L_{t}$ by ${\\widehat{L}}_{t}$ in (6) and (7).   \nif $R_{t-1}<1$ an d (6) holds then $\\delta_{t}=1$ ;   \nelse $\\delta_{t}=0$ ;   \nif $R_{t-1}\\geq1$ , (6) and (7) hold then $\\delta_{t}=1$ ;   \nelse $\\delta_{t}=0$ ;   \nUpdate: $\\begin{array}{r}{\\delta^{t}=\\delta^{t-1}\\bigcup\\{\\delta_{t}\\};R_{t}=R_{t-1}+\\delta_{t};V_{t}=V_{t-1}+\\{\\widehat{L}_{t}G_{0}({\\bf X}_{t})+(1-\\widehat{L}_{t})G_{1}({\\bf X}_{t})\\}\\delta_{t};}\\end{array}$ $\\mathrm{TS}_{t}=\\mathrm{TS}_{t-1}+\\left[\\sum_{i<t}g(\\mathbf{X}_{i},\\mathbf{X}_{t})(1-\\widehat{L}_{i})\\delta_{i}\\right](1-\\widehat{L}_{t})\\delta_{t};\\;\\mathrm{NS}_{t}=\\mathrm{NS}_{t-1}+\\left[\\sum_{i<t}(1-\\widehat{L}_{i})\\delta_{i}\\right](1-\\widehat{L}_{t})\\delta_{t};$ ", "page_idx": 6}, {"type": "text", "text": "Output: Selection set $\\{\\mathbf{X}_{i}:\\delta_{i}=1,\\;\\delta_{i}\\in\\delta^{T}\\}$ . ", "page_idx": 6}, {"type": "text", "text": "Extension to varying proportion case. In practice, the distribution of $(\\mathbf{X}_{t},Y_{t})$ may vary smoothly over time. In Appendix C, we consider the probability of $Y_{t}\\in\\mathcal A$ (i.e. the proportion of samples in the specified region) varying over time and extend the proposed II-COS to learn $\\pi_{t}=\\operatorname*{Pr}({\\bar{Y_{t}}}\\in{\\mathcal A})$ continuously over time and we also construct the corresponding theoretical guarantees. ", "page_idx": 6}, {"type": "text", "text": "3 Statistical Performance Guarantees ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we provide statistical guarantees for the data-driven II-COS procedure. The main difficulties lie in the quantification of data-driven estimation error of $L_{t}$ and we utilize the classical kernel density estimation theory along with the structure of our online procedure to effectively characterize it. For simplicity, we consider that the training data set is given such that the estimated model $\\widehat{\\mu}$ is fixed. Before presenting our theoretical results, we state the following regularity conditions. ", "page_idx": 6}, {"type": "text", "text": "Assumption 3.1 (Density functions and kernel). The density functions and kernel function satisfy (1) The $f_{1}(\\cdot)$ and $f_{0}(\\cdot)$ are upper bounded by $M>0$ , and the $f(\\cdot)$ is lower bounded by $\\ell>0$ . (2) The $f_{0}$ and $f_{1}$ are H\u00f6lder-continuous, i.e. $|f_{0}(w)-f_{0}(w^{\\prime})|\\leq c_{\\beta}|w-w^{\\prime}|^{\\beta}$ for any $w,w^{\\prime}\\in$ $\\mathbb{R}$ , and the same for $f_{1}$ with some fixed $0<\\beta\\leq1$ and constant $c_{\\beta}$ . (3) Kernel $K(\\cdot)$ is a bounded symmetric function and enjoys exponential decay. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Assumption 3.2 (Weight functions). There exists constants $c_{G}~>~0$ and $c_{g}~>~0$ such that $0~<$ $G_{0}(\\mathbf{X})\\leq c_{G}$ , $0<G_{1}(\\mathbf{X})\\leq c_{G}$ for any $\\mathbf{X}$ and $0<g(\\mathbf{X},\\mathbf{X}^{\\prime})\\leq c_{g}$ for any $\\mathbf{X}\\neq\\mathbf{X}^{\\prime}$ . ", "page_idx": 6}, {"type": "text", "text": "Assumption 3.1 is considerably mild and widely adopted in the uniform convergence of kernel density estimation [36]. If $f_{0}$ and $f_{1}$ have bounded first-order derivatives, the H\u00f6lder-continuous assumption would hold with $\\beta=1$ . The lower bound of $f$ is to ensure the uniform convergence of the estimated lFDR. Assumption 3.2 is mild since the weight functions are required only to be positive and bounded. It can be satisfied by a large category of $G_{0}$ , $G_{1}$ and $g$ . For example, we can take $G_{j}(\\mathbf{X})=a_{j}\\|\\mathbf{X}\\|_{2}^{2}$ for $j=0$ or 1 and $c_{G}$ exists when $\\mathbf{X}$ is bounded. And we can set $g$ as the RBF and orthogonal similarities with $c_{g}=1$ . ", "page_idx": 6}, {"type": "text", "text": "With those regularity conditions, we establish the validity of the II-COS procedure for the individual constraint control. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.3 (Bound for individual constraint). Suppose Assumptions 3.1 and 3.2 hold and take the bandwidths for estimating $f$ and $f_{0}$ in the order of 1\u22121/(2\u03b2+1). Then for any given time t, the individual constraint of the II-COS procedure (Algorithm $^{\\,l}$ ) satisfies $C_{1}(\\delta^{t})\\leq\\alpha+\\Delta_{n_{1}}$ , where $\\Delta_{n_{1}}=D n_{1}^{\\frac{-\\beta}{2\\beta+1}}\\sqrt{\\log n_{1}}$ and $D$ is a constant depending on $M,\\,\\ell,\\,c_{\\beta},\\,\\beta,\\,\\pi,\\,c_{G}$ and $K(\\cdot)$ . ", "page_idx": 7}, {"type": "text", "text": "Although the $C_{1}(\\cdot)$ of II-COS might be slightly larger than the target level in finite samples, this gap converges to 0 asymptotically as $n_{1}$ increases. In the numerical studies, we find that a small calibration size of around 200 is enough to control the $C_{1}$ in a reasonable range. Taking the FSR as an individual constraint, Theorem 3.3 indicates that our method can provide asymptotic online FSR control similar to an online FDR control procedure [16]. ", "page_idx": 7}, {"type": "text", "text": "The next theorem examines the performance of the II-COS in terms of interactive constraint. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3.4 (Bound for interactive constraint). Suppose Assumptions 3.1-3.2 hold and take the bandwidths for estimating $f$ and $f_{0}$ in the order of 1\u22121/(2\u03b2+1). Let Ts = inf{t : it=1 \u03b4i = s} for $s>2$ and assume there exists a constant $\\alpha^{\\prime}\\in(0,1)$ such that $\\begin{array}{r}{\\sum_{i\\leq t}\\widehat{L}_{i}\\delta_{i}/(1\\vee R_{t})\\leq\\alpha^{\\prime}}\\end{array}$ . Then for any given time $t\\geq T_{m}$ , the interactive constraint of the $I I$ -COS satis fies ", "page_idx": 7}, {"type": "equation", "text": "$$\nC_{2}(\\delta^{t})\\leq K+\\frac{(K+c_{g})\\Delta_{n_{1}}}{0.5-\\frac{m\\alpha^{\\prime}}{m-1}-\\Delta_{n_{1}}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The term $m\\alpha^{\\prime}/(m-1)$ is used to characterize the lower bound of the denominator term of the interactive constraint. Specifically, when we choose FSR as the individual constraint, we have ${\\boldsymbol{\\alpha}}^{\\prime}={\\boldsymbol{\\alpha}}$ , which demonstrates the interdependence between controlling individual and interactive constraints. Furthermore, under arbitrary stopping strategies with a stopping time $T$ , we can have the asymptotic guarantee. ", "page_idx": 7}, {"type": "text", "text": "Corollary 3.5. Suppose the conditions in Theorem 3.4 hold, the stopping moment $T\\geq T_{m}$ and $\\alpha^{\\prime}<(1-1/m)/2$ . Then the II-COS procedure controls the individual and interactive constraints asymptotically at $T$ , i.e. $\\begin{array}{r}{\\operatorname*{lim}_{n_{1}\\to\\infty}C_{1}(\\delta^{T})\\leq\\alpha}\\end{array}$ and $\\begin{array}{r}{\\operatorname*{lim}_{n_{1}\\to\\infty}C_{2}(\\delta^{T})\\le K.}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "4 Experiments and Evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We illustrate the breadth of applicability of the II-COS procedure by experiments on simulated data and real-data applications. As an example, we set the stopping rule as selecting total $m\\,=\\,100$ samples, i.e., $T=T_{m}=\\operatorname*{inf}_{t}\\{t:\\sum_{i=1}^{t}\\delta_{i}=m\\}$ . Additional experiments including the extended IICOS in Appendix C are shown in Appendix D.9. Code for implementing II-COS and reproducing the experiments and figures in our paper is available at https://github.com/lulin2023/II-COS. ", "page_idx": 7}, {"type": "text", "text": "Implementation of II-COS. To our best knowledge, online selection with uncertainty qualification has only been studied in the field of online multiple testing, which aims to control online FDR. Hence, we focus on using FSR as the individual criterion and modified ES as the interactive criterion. As $\\mathbf{X}$ may be measured on scales with widely differing ranges in different dimensions, we assume that $\\mathbf{X}$ \u2019s have been properly scaled in each dimension before computing $g$ . We choose $g$ as the weighted RBF kernel with $\\sigma=1$ , $w_{k}=1$ here. Other choices for individual and interactive constraints are considered in Appendix D.5. ", "page_idx": 7}, {"type": "text", "text": "Benchmarks. We compare the II-COS procedure with four benchmarks from online multiple testing. The first one is a structure-adaptive sequential testing rule, the SAST [17], which is implemented with ${\\widehat{L}}_{t}$ . It can achieve the FSR control but ignore the interactive constraint. As mentioned earlier in Section 2, SAST can also be considered as a special case of our II-COS with $K=+\\infty$ . Its details are deferred to Appendix B.2. The other competitors are three well-known online FDR control algorithms LOND [18], SAFFRON [30] and ADDIS [40] implemented with the conformal $p$ -values suggested by Bates et al. [7]. Refer more information in Appendix B.3. All the benchmarks can only control FDR, which demonstrates the flexibility of our method for different constraints. ", "page_idx": 7}, {"type": "text", "text": "Performance Measures. The empirical FSR, ES and stopping time $\\left(T_{m}\\right)$ are evaluated using the average values of the false selection proportions, the similarity and the stopping time from 500 replications, respectively, where $T_{m}$ serves as a criterion for assessing selection efficiency. ", "page_idx": 7}, {"type": "text", "text": "4.1 Results on Synthetic Data ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Data Description. We consider a classification model: $\\mathbf{X}\\mid Y=0\\sim\\mathcal{N}_{4}\\left(\\pmb{\\mu}_{1},\\mathbf{I}_{4}\\right)$ , and $\\mathbf{X}\\mid Y=$ $1\\sim\\mathcal{N}_{4}\\left(\\mu_{2},\\mathbf{I}_{4}\\right)$ , where $\\pmb{\\mu}_{1}=(5,0,0,0)^{\\top},\\pmb{\\mu}_{2}=(0,0,-3,-2)^{\\top}$ . We set $\\operatorname*{Pr}(Y=1)=0.2$ . The information set is ${\\mathcal{A}}=\\{1\\}$ . The predictor $\\mathcal{H}$ is taken as random forest with defaulted parameters. We also consider a regression setting and conduct additional experiments in Appendix D. ", "page_idx": 8}, {"type": "text", "text": "Firstly, we observe that methods relying on conformal $p$ -values, such as LOND, SAFFRON, and ADDIS, encounter the alpha-death (stop early) issue [29]. These methods struggle to select an adequate number of samples, especially in small calibration sets. In contrast, II-COS ensures the control of both individual and interactive constraints even with a small $n_{\\mathrm{cal}}$ (e.g., 200). See more details and results in Appendix D.3. Hence, to make a fair comparison, we consider a relatively large size of the calibration set, $n_{\\mathrm{cal}}=4,000$ . We fix training data size $n_{\\mathrm{tr}}=1,000$ . ", "page_idx": 8}, {"type": "image", "img_path": "wblxm5zdkE/tmp/4532c93b83b622210fc1214e4886b088f264ee416164233c6d60a5a2fd08f0d1.jpg", "img_caption": ["Figure 2: The values of $\\mathrm{FSR}(\\pmb{\\delta}^{t})$ and $\\mathrm{ES}(\\pmb{\\delta}^{t})$ over time $t$ for II-COS, SAST, LOND, SAFFRON and ADDIS. The black dashed lines denote the FSR level $\\alpha=0.1$ and the ES level $K=0.045$ . Shading represents error bars of one standard error above and below. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "wblxm5zdkE/tmp/aa2b0124322530e8e635e04eea65eef4d3468bd0e9782226a4ea54338a46617e.jpg", "img_caption": ["Figure 3: Boxplots of $\\mathrm{FSR}(\\pmb{\\delta}^{T_{m}})$ , $\\mathrm{ES}(\\pmb{\\delta}^{T_{m}})$ and stopping time $T_{m}$ for II-COS, SAST, LOND, SAFFRON and ADDIS. The black dashed lines indicate the corresponding nominal levels. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Results. Figure 2 presents the online FSR (false selection rate) and ES (expected similarity) values of five methods against time $t$ , which are individual and interactive constraints, respectively. The FSR levels of II-COS and SAST are closer to the nominal level than the other three methods. As expected, only the empirical ES levels of II-COS are controlled under the pre-specified level $K$ over time $t$ . The LOND and SAFFRON lead to slightly conservative FSR values, while the FSR levels of ADDIS are inflated compared to the target level. Figure 3 further displays the boxplots of empirical FSR, ES at stopping time $T_{m}$ . We observe that only II-COS achieves satisfactory ES values compared to the nominal level. Moreover, the II-COS has a relatively larger value of $T_{m}$ compared to those of other benchmarks. This is consistent with the fact that the II-COS spends more time exploring the structure information inside the covariate space due to the requirement of interactive constraint. Similar conclusions for the regression example in Appendix D.4 can be drawn. ", "page_idx": 8}, {"type": "text", "text": "Regarding efficiency, we also conducted an experiment to compare the effectiveness of II-COS with an oracle method possessing knowledge of true state $\\theta_{t}$ . The $T_{m}$ of II-COS is very close to the oracle. This close proximity indicates the high efficiency of II-COS. See Appendix D.8 for the details. ", "page_idx": 8}, {"type": "text", "text": "4.2 Results on Real Data ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We next demonstrate the performance of the II-COS in two real-world applications. Since those online multiple testing methods based on conformal $p$ -values yield few selected individuals, we focus on comparing the II-COS with the SAST. For comparison, we also include the offilne method using conformal $p$ -values, where $\\delta_{t}=1$ if $\\widehat{p_{t}}\\leq\\alpha$ . We denote it as CP. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Datasets. We consider the recruitment dataset from Kaggle [22] that contains 45,372 candidates after removing the missing data and records a binary response indicating whether the candidate passes the job interview, and other 11 attributes including education status, handicapped or not, and gender. The other problem is to use 1994 Census Bureau dataset [8] to select a subset of individuals who may have high incomes in precision marketing. This census dataset records 32,561 individuals with their 14 attributes, including gender, race, marriage, education length and so on. ", "page_idx": 9}, {"type": "text", "text": "For each dataset, we randomly partition the data into three parts: $n_{\\mathrm{tr}}=1,000$ training data, $n_{\\mathrm{cal}}=$ 1, 000 calibration data and the rest which are used as the online observations. The categorical attributes are converted into one-hot codes and then are treated as numerical attributes for computing similarity measures. The prediction algorithm $\\mathcal{H}$ is random forest with defaulted parameters. ", "page_idx": 9}, {"type": "text", "text": "Results. Table 1 reports the results among 500 repetitions. Both the II-COS and SAST enjoy valid FSR control, but CP yields an inflated FSR level in income investigation. The II-COS performs well in terms of similarities. To further compare the diversities, we present the proportions of different education status in in Figure 4. It can be seen that the proposed II-COS demonstrates its superior diversity in the specific attributes. See Appendix D.6 for more results for the real data. In summary, the proposed II-COS works well for selecting individuals of interest to achieve various constraints in practical applications. ", "page_idx": 9}, {"type": "text", "text": "Table 1: Average values with candidate dataset and income dataset: $\\mathrm{\\nabla{SR}}(\\delta^{T_{m}}),\\,\\mathrm{ES}(\\delta^{T_{m}})\\ (\\times10^{-3})$ and stopping time $T_{m}$ . The target FSR level is $\\alpha\\,=\\,0.2$ for both. For the candidate data, the target ES level $K\\stackrel{\\cdot\\,\\cdot}{=}1\\stackrel{\\cdot}{\\times}10^{-3}$ ; For the income data, $K=6\\times10^{-3}$ . The bracket contains the standard error. ", "page_idx": 9}, {"type": "table", "img_path": "wblxm5zdkE/tmp/deadb16a56334c500f13873d83acc82600011400945fe7deac3e7635592ffb14.jpg", "table_caption": ["(a) Candidate dataset [22] ", "(b) Income dataset [8] "], "table_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "wblxm5zdkE/tmp/7247b2ef2c9e6e414e5bb7e3afa873c505ed41157a5166e4e4153a64a70dd728.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "wblxm5zdkE/tmp/13a4d3f388893adfaa3f2043669909b4e6bd3cfa56a5eabdaf2dbec634c01d0a.jpg", "img_caption": ["Figure 4: Left: Education status composition of the correctly selected samples (II-COS, SAST and CP) in candidate dataset; Right: Education length (year) composition of the correctly selected samples (II-COS, SAST) in income dataset. The plots have error bar to show the variation across the 500 runs. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Concluding Remarks ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Broader Impacts. This work focuses on creating reliable machine learning tools for making real-time decisions. One key achievement is a new algorithm called II-COS, designed to select informative samples in real-time while meeting two types of general constraints. II-COS allows for both individual and interactive control, validated through theoretical analysis and numerical tests. Our method is model-agnostic and easily applicable to many real-world cases such as producing diversified results while controlling FSR for online recruitment. One potential negative impact of our work is that researchers will apply the algorithm without sufficient scrutiny. We emphasize that it\u2019s important to use caution when applying this method to complex real-world scenarios to prevent misuse. ", "page_idx": 9}, {"type": "text", "text": "Limitations. Firstly, we mainly consider binary functions as the interactive constraint. How to adapt the II-COS to other popular constraints, such as the Gini index, deserves further study. Secondly, in certain practical scenarios, it is possible to obtain feedback after decisions. Incorporating the feedback information into our method to enhance its performance warrants future research. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank anonymous area chair and reviewers for their helpful comments. Zou was supported by the National Key R&D Program of China (Grant Nos. 2022YFA1003703, 2022YFA1003800), the National Natural Science Foundation of China (Grant Nos. 11925106, 12231011, 11931001, 12226007, 12326325). Ren was supported by the National Natural Science Foundation of China (Grant Nos. 12101398, 12471262), and Young Elite Scientists Sponsorship Program by CAST. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Ehud Aharoni and Saharon Rosset. Generalized $\\alpha$ -investing: definitions, optimality results and application to public databases. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76(4):771\u2013794, 2014.   \n[2] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection for online continual learning. Advances in Neural Information Processing Systems, 32, 2019.   \n[3] Anastasios N Angelopoulos and Stephen Bates. Conformal prediction: A gentle introduction. Foundations and Trends\u00ae in Machine Learning, 16(4):494\u2013591, 2023.   \n[4] Ruicheng Ao, Hongyu Chen, David Simchi-Levi, and Feng Zhu. Online local false discovery rate control: A resource allocation approach. arXiv preprint arXiv:2402.11425, 2024.   \n[5] Eric Bach, Shuchi Chawla, and Seeun Umboh. Threshold rules for online sample selection. Discrete Mathematics, Algorithms and Applications, 2(4):625\u2013642, 2010.   \n[6] Peter L. Bartlett. Learning with a slowly changing distribution. In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, page 243\u2013252, 1992.   \n[7] Stephen Bates, Emmanuel Cand\u00e8s, Lihua Lei, Yaniv Romano, and Matteo Sesia. Testing for outliers with conformal p-values. Annals of Statistics, 51(1):149\u2013178, 2023.   \n[8] Barry Becker and Ronny Kohavi. Adult income investigation. UCI Machine Learning Repository https://archive.ics.uci.edu/dataset/2/adult, 1996.   \n[9] Yoav Benjamini and Yosef Hochberg. Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal Statistical Society: Series $B$ (Statistical Methodology), 57(1):289\u2013300, 1995.   \n[10] Yoav Benjamini and Daniel Yekutieli. The control of the false discovery rate in multiple testing under dependency. Annals of Statistics, 29(4):1165\u20131188, 2001.   \n[11] Rick Durrett. Probability: theory and examples, volume 49. Cambridge University Press, 2019.   \n[12] Bradley Efron, Robert Tibshirani, John D Storey, and Virginia Tusher. Empirical bayes analysis of a microarray experiment. Journal of the American Statistical Association, 96(456):1151\u2013 1160, 2001.   \n[13] Evanthia Faliagka, Kostas Ramantas, Athanasios Tsakalidis, and Giannis Tzimas. Application of machine learning algorithms to an online recruitment system. In Proc. International Conference on Internet and Web Applications and Services, pages 215\u2013220, 2012.   \n[14] Evanthia Faliagka, Lazaros Iliadis, Ioannis Karydis, Maria Rigou, Spyros Sioutas, Athanasios Tsakalidis, and Giannis Tzimas. On-line consistent ranking on e-recruitment: seeking the truth behind a well-formed cv. Artificial Intelligence Review, 42(3):515\u2013528, 2014.   \n[15] Kai-Tai Fang, Runze Li, and Agus Sudjianto. Design and modeling for computer experiments. Chapman and Hall/CRC, 2005.   \n[16] Dean P Foster and Robert A Stine. $\\alpha$ -investing: a procedure for sequential control of expected false discoveries. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 70(2):429\u2013444, 2008.   \n[17] Bowen Gang, Wenguang Sun, and Weinan Wang. Structure\u2013adaptive sequential testing for online false discovery rate control. Journal of the American Statistical Association, 118(541): 732\u2013745, 2023.   \n[18] Adel Javanmard and Andrea Montanari. On online control of false discovery rate. arXiv preprint arXiv:1502.06197, 2015.   \n[19] Adel Javanmard and Andrea Montanari. Online rules for control of false discovery rate and false discovery exceedance. The Annals of Statistics, 46(2):526\u2013554, 2018.   \n[20] Heinrich Jiang. Uniform convergence rates for kernel density estimation. In International Conference on Machine Learning, pages 1694\u20131703. PMLR, 2017.   \n[21] Ying Jin and Emmanuel J Cand\u00e8s. Selection by prediction with conformal p-values. Journal of Machine Learning Research, 24(244):1\u201341, 2023.   \n[22] Kaggle. Candidate selection dataset. https://www.kaggle.com/datasets/ tarunchilkur/client, 2020.   \n[23] G Kanagavalli, R Seethalakshmi, and T Sowdamini. A systematic review of literature on recruitment and selection process. Humanities & Social Sciences Reviews, 7(2):01\u201309, 2019.   \n[24] Mozhgan Karimi, Dietmar Jannach, and Michael Jugovac. News recommender systems\u2013survey and roads ahead. Information Processing & Management, 54(6):1203\u20131227, 2018.   \n[25] Jing Lei, Max G\u2019Sell, Alessandro Rinaldo, Ryan J Tibshirani, and Larry Wasserman. Distribution-free predictive inference for regression. Journal of the American Statistical Association, 113(523):1094\u20131111, 2018.   \n[26] Ziyi Liang, Matteo Sesia, and Wenguang Sun. Integrative conformal p-values for out-ofdistribution testing with labelled outliers. Journal of the Royal Statistical Society Series B: Statistical Methodology, 86(3):671\u2013693, 01 2024.   \n[27] Ariane Marandon, Lihua Lei, David Mary, and Etienne Roquain. Adaptive novelty detection with false discovery rate guarantee. The Annals of Statistics, 52(1):157\u2013183, 2024.   \n[28] Houwen Peng, Bing Li, Haibin Ling, Weiming Hu, Weihua Xiong, and Stephen J. Maybank. Salient object detection via structured matrix decomposition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(4):818\u2013832, 2017.   \n[29] Aaditya Ramdas, Fanny Yang, Martin J Wainwright, and Michael I Jordan. Online control of the false discovery rate with decaying memory. Advances in Neural Information Processing Systems, 30:5655\u20135664, 2017.   \n[30] Aaditya Ramdas, Tijana Zrnic, Martin Wainwright, and Michael Jordan. Saffron: an adaptive algorithm for online control of the false discovery rate. In International Conference on Machine Learning, pages 4286\u20134294. PMLR, 2018.   \n[31] Bradley Rava, Wenguang Sun, Gareth M James, and Xin Tong. A burden shared is a burden halved: A fairness-adjusted approach to classification. arXiv preprint arXiv:2110.05720, 2021.   \n[32] David S Robertson, Jan Wildenhain, Adel Javanmard, and Natasha A Karp. onlineFDR: an R package to control the false discovery rate for growing data repositories. Bioinformatics, 35 (20):4196\u20134199, 2019.   \n[33] David S Robertson, James MS Wason, and Aaditya Ramdas. Online multiple hypothesis testing. Statistical Science, 38(4):557, 2023.   \n[34] Glenn Shafer and Vladimir Vovk. A tutorial on conformal prediction. Journal of Machine Learning Research, 9(3):371\u2013421, 2008.   \n[35] Muhammad Ahmad Shehu and Faisal Saeed. An adaptive personnel selection model for recruitment using domain-driven data mining. Journal of Theoretical and Applied Information Technology, 91(1):117, 2016.   \n[36] Bernard W. Silverman. Weak and strong uniform consistency of the kernel estimate of a density and its derivatives. The Annals of Statistics, 6(1):177\u2013184, 1978.   \n[37] John D Storey. A direct approach to false discovery rates. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 64(3):479\u2013498, 2002.   \n[38] Wenguang Sun and T. Tony Cai. Oracle and adaptive compound decision rules for false discovery rate control. Journal of the American Statistical Association, 102(479):901\u2013912, 2007.   \n[39] Stephen Taylor. Resourcing and talent management. Kogan Page Publishers, 2018.   \n[40] Jinjin Tian and Aaditya Ramdas. Addis: an adaptive discarding algorithm for online fdr control with conservative nulls. Advances in Neural Information Processing Systems, 32:9388\u20139396, 2019.   \n[41] Vladimir Vovk and Ruodu Wang. E-values: Calibration, combination and applications. The Annals of Statistics, 49(3):1736\u20131754, 2021.   \n[42] Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. Algorithmic learning in a random world. New York: Springer, 2005.   \n[43] Michel Wedel and PK Kannan. Marketing analytics for data-rich environments. Journal of Marketing, 80(6):97\u2013121, 2016.   \n[44] Sang Eun Woo, James M LeBreton, Melissa G Keith, and Louis Tay. Bias, fairness, and validity in graduate-school admissions: A psychometric perspective. Perspectives on Psychological Science, 18(1):3\u201331, 2023.   \n[45] Xiaoyang Wu, Yuyang Huo, Haojie Ren, and Changliang Zou. Optimal subsampling via predictive inference. Journal of the American Statistical Association, pages 1\u201329, 2023.   \n[46] Xiaoyang Wu, Lin Lu, Zhaojun Wang, and Changliang Zou. Conditional testing based on localized conformal p-values. arXiv preprint arXiv:2409.16829, 2024.   \n[47] Pengtao Xie, Yuntian Deng, and Eric Xing. Latent variable modeling with diversity-inducing mutual angular regularization. arXiv preprint arXiv:1512.07336, 2015.   \n[48] Ziyu Xu and Aaditya Ramdas. Online multiple testing with e-values. In International Conference on Artificial Intelligence and Statistics, pages 3997\u20134005. PMLR, 2024.   \n[49] Yi Yang, Zhigang Ma, Feiping Nie, Xiaojun Chang, and Alexander G Hauptmann. Multi-class active learning by uncertainty sampling with diversity maximization. International Journal of Computer Vision, 113(2):113\u2013127, 2015.   \n[50] Yifan Zhang, Haiyan Jiang, Haojie Ren, Changliang Zou, and Dejing Dou. Automs: automatic model selection for novelty detection with error rate control. Advances in Neural Information Processing Systems, 35:19917\u201319929, 2022.   \n[51] Zinan Zhao and Wenguang Sun. False discovery rate control for structured multiple testing: Asymmetric rules and conformal q-values. Journal of the American Statistical Association, pages 1\u201324, 2024.   \n[52] Ping Zhong, Zhiqiang Gong, Shutao Li, and Carola-Bibiane Sch\u00f6nlieb. Learning to diversify deep belief networks for hyperspectral image classification. IEEE Transactions on Geoscience and Remote Sensing, 55(6):3516\u20133530, 2017. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Supplementary Material for \u201cReal-Time Selection Under General Constraints via Predictive Inference\u201d ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This supplementary material contains: ", "page_idx": 13}, {"type": "text", "text": "\u2022 Preliminary terms for self-containment (Appendix A);   \n\u2022 Implementation details (Appendix B);   \n\u2022 An extension algorithm to varying proportion case (Appendix C);   \n\u2022 Additional experiments (Appendix D);   \n\u2022 The proofs of all the theoretical results. (Appendix E). ", "page_idx": 13}, {"type": "text", "text": "A Preliminary Terms for Self-Containment ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Here, we list the preliminary terms we use in the paper for the sake of clarity and self-containment. ", "page_idx": 13}, {"type": "text", "text": "\u2022 FDR [9], false discovery rate, a widely-adopted error rate notion in the field of multiple testing, is defined as the expected proportion of incorrectly rejected null hypotheses as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{FDR}(t)=\\mathbb{E}\\left[\\frac{|\\mathcal{H}_{0}\\cap\\mathcal{R}(t)|}{|\\mathcal{R}(t)|\\vee1}\\right],\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mathcal{H}_{0}$ is the unknown set of true null hypotheses, $\\mathcal{R}(t)$ represents the set of rejected null hypotheses until time $t$ and then $\\mathcal{H}_{0}\\cap\\mathcal{R}(t)$ is the set of false discoveries. ", "page_idx": 13}, {"type": "text", "text": "\u2022 FSR, false selection rate, defined as the expected proportion of individuals being not of interest among the selected subset of individuals. It is in fact equivalent to the definition of FDR. In our framework in this paper, we describe it equivalently as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{FSR}(t)=\\mathbb{E}\\left[\\frac{\\sum_{i\\leq t}(1-\\theta_{i})\\delta_{i}}{(\\sum_{i\\leq t}\\delta_{i})\\vee1}\\right].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "B Implementation Details of Algorithms ", "text_level": 1, "page_idx": 13}, {"type": "image", "img_path": "wblxm5zdkE/tmp/463c2143377720f0028d4834c3dc529098411b95056f43dd3ebfa6ddc6aad940.jpg", "img_caption": ["Figure 5: The implementation flowchart of the data-driven II-COS procedure. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "B.1 Implementation Details of II-COS ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Figure 5 shows the implementation flowchart of our proposed data-driven II-COS procedure, including the offilne estimation step and online decision step. Here we introduce more implementation details to reproduce the results. ", "page_idx": 14}, {"type": "text", "text": "About the classifier. For the classification problem, we use the predicted conditional probability $\\widehat{\\operatorname*{Pr}}(Y_{t}=1\\mid\\mathbf{X}_{t}=\\mathbf{x})$ as $\\widehat{\\mu}(\\mathbf{x})$ in the procedure of II-COS. Many commonly used algorithms such as random forest and neural networks can provide such probability estimators. In fact, the choice of $\\widehat{\\mu}$ is not restricted to probability. For example, the support vector machine outputs the distance to t h e separating hyperplane for classification, and such distance can be chosen as $\\widehat{\\mu}$ . ", "page_idx": 14}, {"type": "text", "text": "However, for probability estimators, one potential problem is that when the classifier is quite accurate, most of $W_{i}$ \u2019s are concentrated at 0 or 1 and few of them take values between $(0,1)$ . In such a situation, it is difficult to accurately and stably estimate lFDR $L_{i}$ \u2019s since the density functions $f_{0}(w)$ and $f(w)$ are not smooth enough and are not lower bounded. This yields that our assumptions are violated, but the II-COS can still perform satisfactorily with some corrections on the estimation ${\\widehat{L}}_{i}$ \u2019s. Notice that the larger $W_{i}$ is, the more likely $Y_{i}\\in A$ and hence the smaller $L_{i}$ is. That is ${\\widehat{L}}_{i}$ should be monotonically decreasing as $W_{i}$ increases. Observing this, we can make a monotonization correction on ${\\widehat{L}}_{i}$ \u2019s. To be specific, we rearrange ${\\widehat{L}}_{i}$ by the decreasing order of $W_{i}$ . If $\\widehat{L}(W_{(i-1)})<\\widehat{L}(W_{(i)})$ which violates the monotonicity, we revise it by $\\widehat{L}(W_{(i-1)})\\,=\\,\\widehat{L}(W_{(i)})$ , where $W_{(i)}$ is the ith smallest among $W$ . This monotonization correction enables us to avoid obvious errors due to unstable estimation and improve the performances of all the methods utilizing ${\\widehat{L}}_{i}$ \u2019s. ", "page_idx": 14}, {"type": "text", "text": "Choice of $K$ . A useful interactive constraint needs an appropriate specification of $K$ . For any two i.i.d. observations $\\mathbf{X}$ and $\\mathbf{X^{\\prime}}$ with corresponding $\\theta$ and $\\theta^{\\prime}$ respectively, the expected $C_{2}$ of the individuals of interest is given as $C_{2}:=\\mathbb{E}[g(\\mathbf{X},\\mathbf{X}^{\\prime})\\mid\\theta=1,\\theta^{\\prime}\\stackrel{\\cdot}{=}1]$ , which can be estimated by $\\begin{array}{r}{\\widehat{C_{2}}=\\sum\\sum_{i<j;i,j\\in\\mathcal{L}}g(\\tilde{\\mathbf{X}}_{i},\\tilde{\\mathbf{X}}_{j})/\\{|\\mathcal{L}|(|\\mathcal{L}|-1)\\}}\\end{array}$ , where $\\mathcal{L}=\\{i:\\tilde{Y}_{i}\\in\\mathcal{A}\\}$ . It is reasonable to set $K=a\\widehat{C_{2}}$ , where $a>0$ is user-specific to control the interactive constraint level. Our numerical evidence reveals that $a\\in(0.1,0.5)$ works generally well. ", "page_idx": 14}, {"type": "text", "text": "B.2 Implementation Details of SAST ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Gang et al. [17] proposed a structure-adaptive sequential testing (SAST) rule for online false discovery rate control. In their work, the rejecting rule is as follows: If $L_{t}~<~\\gamma_{t}$ and $\\{|\\mathcal{R}_{t-1}|+$ $\\begin{array}{r}{1\\}^{-1}\\left(\\sum_{i\\in\\mathcal{R}_{t-1}}L_{i}+L_{t}\\right)\\leq\\alpha}\\end{array}$ , then $\\delta_{t}=1$ . Otherwise $\\delta_{t}=0$ , where $\\mathcal{R}_{t-1}=\\{i\\leq t-1:\\delta_{i}=1\\}$ and $\\gamma_{t}$ is a barrier estimated from an \"offline\" procedure. ", "page_idx": 14}, {"type": "text", "text": "The implementation details of SAST for comparisons in our simulations in Section 4 are different from the original one. Firstly, the original SAST assumes the null density function $f_{0}$ is already known while in our setting $f_{0}$ remains unknown. Secondly, in our predictive inference setting, the density functions and the null proportion are directly estimated via calibration set as the offline estimation procedure in Algorithm 1, not from current rejection sets. Besides, considering the time-varying structures of the data stream in their setting, Gang et al. [17] incorporated the barrier strategy in their method, which is not necessary to be adopted here. ", "page_idx": 14}, {"type": "text", "text": "B.3 Implementation Details of Conformal $p$ -values ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The notion of conformal $p$ -values was originally proposed by Vovk et al. [42] to construct prediction interval. Recently, there exist some works to apply conformal $p$ -values to implement sample selection from a multiple-testing perspective, such as Bates et al. [7], Rava et al. [31] and Jin and Cand\u00e8s [21]. In the sample selection problem, the hypothesis has the following form for each $t$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\nH_{0t}:Y_{t}\\in\\mathcal{A}^{c}\\quad\\mathrm{v.s.}\\quad H_{1t}:Y_{t}\\in\\mathcal{A}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "There are two types of conformal $p$ -values and we adopt the one in Bates et al. [7] which utilizes the same class calibration. Recall that for $W_{t}$ , its conformal $p$ -value $p_{t}$ is defined as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\widehat{p_{t}}=\\frac{1+\\sum_{i\\in{\\mathcal{D}_{\\mathrm{cal}}}}\\mathbb{I}\\{Q(\\tilde{W}_{i})\\leq Q(W_{t})\\}}{1+|{\\mathcal{D}}_{\\mathrm{cal}}|}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The nonconformity score function $Q(W_{t})$ is used to indicate the possibility of $\\theta_{t}=0$ . For example, in regression settings, if $A=[b,+\\infty)$ , we can use $Q(W_{j})=b\\bar{-}\\,W_{j}$ . If $\\mathcal{A}=(-\\infty,a]\\cup[b,+\\infty)$ , then we can choose $Q(W_{t})=\\operatorname*{max}\\{W_{t}-a,b-W_{t}\\}$ . And in binary classification settings, if $A=1$ and $W_{t}$ indicates the probability of $Y_{t}=1$ , we set $\\dot{Q}(W_{t})=1-W_{t}$ . ", "page_idx": 15}, {"type": "text", "text": "Even though conformal $p$ -values are correlated, using conformal $p$ -values to conduct multiple testing can control the FDR level with finite sample guarantee since they are positive regression dependent on a subset (PRDS) [10]. However, the conformal $p$ -values are lower bounded by $1/(|\\mathcal{D}_{\\mathrm{cal}}|+1)$ , which leads to unsatisfactory performance for online multiple testing methods based on $p$ -values. Since these methods require sufficiently small $p$ -values to make rejections. ", "page_idx": 15}, {"type": "text", "text": "In our simulations, we implement LOND, SAFFRON and ADDIS for online sample selection by R package OnlineFDR [32] with $\\alpha\\,=\\,0.1$ . Other parameters are defaulted. Here we introduce the details about these online FDR control methods. Ramdas et al. [29] proposed a \u201cstatistical perspective\" to control FDR in online setting, which is to keep an estimate of the FDP less than $\\alpha$ similar to the offline setting. Specifically, for offline FDR, let the rejection set $\\mathcal{R}(s)=\\{i|p_{i}\\leq s\\}$ . An oracle estimate for FDP is given by $\\begin{array}{r}{\\mathrm{FDP}^{*}(s):=\\frac{\\vert\\mathscr{H}_{0}\\vert\\cdot s}{\\vert\\mathscr{R}(s)\\vert\\vee1}}\\end{array}$ =|R|H(s0)||\u00b7\u2228s1. For online FDR, an oracle estimate of $\\mathrm{FDP^{*}}(t)$ i s j\u2264Rt(,jt)\u2208\u2228H10 \u03b1j. Table 2 lists a comparison of estimating FDP in classical offilne methods and online methods for FDR control in multiple testing. For the online methods, denote the decision rule as $\\delta_{t}=\\{p_{t}\\leq\\alpha_{t}\\}$ , where $p_{t}$ is the corresponding conformal $p$ -value at time $t$ for our problem. The test levels $\\left\\{\\alpha_{t}\\right\\}$ for LOND [18], SAFFRON [30] and ADDIS [40] are listed as follows: ", "page_idx": 15}, {"type": "table", "img_path": "wblxm5zdkE/tmp/ca372d4d03c6d1798c0bb7278a61743b73a7776b566b4789b411963d466e2edc.jpg", "table_caption": ["Table 2: A comparison of $\\widehat{\\mathrm{FDP}}$ in offline methods v.s. online methods for FDR control. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "\u2022 LOND: $\\alpha_{t}=\\gamma_{t}(R(t-1)\\!+\\!1)$ , where $\\{\\gamma_{t}\\}_{t=1}^{\\infty}$ is a given infinite non-increasing sequence of positive constants that sums to $\\alpha$ and $\\textstyle{\\dot{R}}({\\dot{n}})=\\sum_{t=1}^{n^{-}}R_{t}$ denotes the number of discoveries in the first $n$ hypotheses tested.   \n\u2022 FSoAr FFRON: At each time , $t$ , de f; inFe $\\begin{array}{r}{C_{j+}=C_{j+}(t)=\\sum_{i=\\tau_{j}+1}^{t-1}}\\end{array}$ , wh ,e rwe $C_{t}=\\mathbb{I}\\{p_{t}\\,\\leq\\,\\lambda\\}$ . $t=1$ $\\alpha_{1}=\\operatorname*{min}\\{\\gamma_{1}W_{0},\\lambda\\}$ $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Delta}}t=2,3,\\ldots,\\boldsymbol{\\alpha}_{t}:=\\operatorname*{min}\\{\\lambda,\\tilde{\\alpha}_{t}\\}$ here $\\tilde{\\alpha}_{t}=W_{0}\\gamma_{t-C_{0+}}+((1-\\lambda)\\alpha-W_{0})\\gamma_{t-\\tau_{1}-C_{1+}}+(1-\\lambda)\\alpha\\sum_{j\\ge2}\\gamma_{t-\\tau_{j}-C_{j+}}.$   \n\u2022 ADDIS: The testing levels for ADDIS are given by $\\alpha_{t}=\\operatorname*{min}\\{\\lambda,\\hat{\\alpha}_{t}\\}$ , where $\\hat{\\alpha}_{t}=(\\eta-\\lambda)[\\omega_{0}\\gamma_{S^{t}-C_{0+}}+(\\alpha-\\omega_{0})\\gamma_{S^{t}-\\tau_{1}^{*}-C_{1+}}+\\alpha\\sum_{j\\geq2}\\gamma_{S^{t}-\\tau_{j}^{*}-C_{j+}}]$ and $\\begin{array}{r}{S^{t}=\\sum_{i<t}\\mathbb{I}\\{p_{i}\\leq\\eta\\}}\\end{array}$ , $\\begin{array}{r}{\\tau_{j}^{*}=\\sum_{i\\leq\\tau_{j}}\\mathbb{I}\\{p_{i}\\leq\\eta\\}.}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "B.4 Experiments Compute Resources ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "All the experiments were conducted on 3.11 GHz Intel Gen i5-11300H processors with $16\\,\\mathrm{Gb}$ memory at a Lenovo personal computer and the $\\mathbf{R}$ platform with version 4.2.1. The time of execution for each of the individual experimental runs is about 6.686 seconds. And the total compute time for the synthetic classification example in Section 4 for 500 replications is about 63.877 minutes. ", "page_idx": 15}, {"type": "text", "text": "B.5 A Toy Example for Illustration in Section 2.3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We illustrate the idea of the II-COS procedure via a binary classification example. We aim to select $m=50$ data points of a specific class from unlabeled data arriving sequentially. We choose FSR as the individual constraint and mES as the interactive constraint. ", "page_idx": 16}, {"type": "text", "text": "The data is generated as follows. The 4-dimensional covariates $\\mathbf{X}\\ =\\ (X_{1},X_{2},X_{3},X_{4})^{\\top}$ are generated from a mixture of multivariate normal distributions with mean $(0,0,0,0)$ if $Y=0$ and mean $(-3,-3,0,0)$ if $Y=1$ . The proportions of $Y=0$ and $Y=1$ are $80\\%$ and $20\\%$ respectively. In this illustrative example, we set the historical data size $n=1,000$ with calibration size $n_{1}=500$ and would like to select $m=50$ data points from the interest region ${\\mathcal{A}}=\\{1\\}$ with the target FSR level $\\alpha=0.1$ . The mES threshold $K$ is set at 0.01 for II-COS. ", "page_idx": 16}, {"type": "text", "text": "Figure 6 depicts the scatterplot of the first two dimensions of the covariates $\\mathbf{X}$ , with green dots and red triangles denoting correctly selected points and falsely selected ones, respectively. The SAST method proposed by Gang et al. [17] is taken as one benchmark, which considers only the FSR control. We observe that the selected points of II-COS enjoy significant diversity among the covariate space and only a few false selections are contained. In contrast, the SAST is inclined to choose similar samples concentrated at the center of the concerned group and stops too early to fully explore the covariate space with sequentially arriving samples. ", "page_idx": 16}, {"type": "image", "img_path": "wblxm5zdkE/tmp/9fd4cee89f68ee34f7f45fe9121d83489841ae2d8bb43ddf5dec839c41f90504.jpg", "img_caption": ["Correctly\u2212selected Falsely\u2212selected ", "Figure 6: Scatter plots of selected points of the II-COS and SAST. It stops when selecting 50 samples . Left: the selection results of the II-COS; Right: the results of SAST. Green dots and red triangles indicate correct and false selections, respectively. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "C Extension to Varying Proportion Case ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In practice, the distribution of $(\\mathbf{X}_{t},Y_{t})$ may vary smoothly over time. Due to the unknown $Y_{t}$ , it is unrealistic to re-estimate parameters (i.e., Estimation Step in Algorithm 1) on both labeled data and the most recent data. To mitigate this problem, we consider the probability of $Y_{t}\\in A$ (i.e. the proportion of samples in the specified region) varying over time and extend the proposed II-COS to learn $\\pi_{t}=\\operatorname*{Pr}(Y_{t}\\stackrel{\\cdot}{\\in}A)$ continuously over time. ", "page_idx": 16}, {"type": "text", "text": "Let $Q(W_{t})$ be one score function, which is high when the possibility of $\\theta_{t}=0$ is large and otherwise low. For example, in regression settings, if $A=[a,+\\infty)$ , we can take $Q(W_{t})=a-W_{t}$ . And in binary classification, if $\\mathcal{A}=1$ and $W_{t}$ indicates the probability of $Y_{t}=1$ , we can set $Q(W_{t})=1{-}W_{t}$ . One valid conformal $p$ -value for $Q(W_{t})$ can be obtained as Bates et al. [7], ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\widehat{p_{t}}=\\frac{1+\\sum_{i\\in{\\mathcal{D}_{\\mathrm{cal}}}}\\mathbb{I}\\{Q(\\tilde{W}_{i})\\leq Q(W_{t})\\}}{1+|{\\mathcal{D}}_{\\mathrm{cal}}|}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Inspired by the techniques for null proportion estimation in multiple testing literature [37], we note that $\\operatorname*{Pr}(\\hat{p_{t}}\\geq\\lambda)\\approx\\operatorname*{Pr}(\\hat{p_{t}}\\geq\\lambda,\\theta_{t}=0)^{\\ast}\\approx\\pi_{t}(1-\\lambda)$ for large $\\lambda\\in(0,1)$ (i.e., $\\lambda=0.5)$ ). ", "page_idx": 16}, {"type": "text", "text": "Thus, we consider using some recent $\\widehat{p_{t}}$ to estimate a reliable $\\pi_{t}$ . Denote $q$ as the size of a neighborhood $\\{t-q,\\dotsc,t-1\\}$ and fix $\\lambda$ . W e  employ an exponential weighted scheme to estimate $\\pi_{t}$ where ", "page_idx": 16}, {"type": "text", "text": "the more recent samples will contribute more to the estimation: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\widehat{\\pi}_{t}^{\\lambda}=1-\\frac{\\sum_{j=t-q}^{t-1}\\kappa_{b}(j-t)\\mathbb{I}\\{\\widehat{p}_{j}>\\lambda\\}}{(1-\\lambda)\\sum_{j=t-q}^{t-1}\\kappa_{b}(j-t)},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\kappa_{b}(s)=\\exp\\{-|s|/b\\}$ and $b$ is the bandwidth parameter. Then, we can compute the distribution of $W_{t}$ as $\\widehat{f}^{t}=\\widehat{f}_{0}(w)(1-\\widehat{\\pi}_{t})+\\widehat{f}_{1}(w)\\widehat{\\pi}_{t}$ and estimate $L_{t}$ by $\\begin{array}{r}{\\widehat{L}_{t}^{\\lambda}=\\frac{(1-\\widehat{\\pi}_{t}^{\\lambda})\\widehat{f_{0}}(W_{t})}{\\widehat{f}^{t}(W_{t})}\\wedge1}\\end{array}$ . ", "page_idx": 17}, {"type": "text", "text": "The extended II-COS is to substitute $\\widehat{L}_{t}^{\\lambda}$ for ${\\widehat{L}}_{t}$ in Algorithm 1. The following theorem establishes the guarantee by assuming the slow change of distribution [6]. ", "page_idx": 17}, {"type": "text", "text": "Theorem C.1. Assume $\\pi_{t}$ satisfies $\\begin{array}{r l r}{|\\pi_{t+1}~-~\\pi_{t}|}&{{}\\le}&{\\eta}\\end{array}$ for any $t,$ , and the bandwidth parameter $b$ satisfies $b^{\\zeta}\\ \\ \\leq\\ \\ q$ for some $\\zeta\\ \\>\\ 1$ . Denote $\\begin{array}{r c l}{{\\Delta^{\\prime}}}&{{=}}&{{c_{1}n_{1}^{-\\beta/(2\\beta+1)}\\sqrt{\\log n_{1}}\\ +}}\\end{array}$ $c_{2}\\operatorname*{max}\\{b^{-1/3},(\\log n_{1}/n_{1})^{1/6},(b\\eta)^{2/3}\\}$ with constants $c_{1},\\,c_{2}$ . Suppose $p_{t}$ and $\\lambda$ satisfy $\\mathrm{Pr}(p_{t}>\\lambda\\mid$ $\\theta_{t}=1)=0$ . Under Assumption 3.1-3.2, we have: ", "page_idx": 17}, {"type": "text", "text": "(a) For any given time $t$ , the individual constraint of extended II-COS satisfies $C_{1}(\\delta^{t})\\leq\\alpha+\\Delta^{\\prime}$ ; (b) Furthermore, if the conditions in Theorem 3.4 hold, then for any given time $t\\,>\\,T_{m}$ , the interactive constraint of the extended II-COS satisfies $\\begin{array}{r}{C_{2}(\\delta^{t})\\le K+\\frac{(K+c_{g})\\Delta^{\\prime}}{0.5-\\frac{m\\alpha^{\\prime}}{m-1}-\\Delta^{\\prime}}}\\end{array}$ . ", "page_idx": 17}, {"type": "text", "text": "Besides the part similar to $\\Delta_{n_{1}}$ in Theorem 3.3, the bound $\\Delta^{\\prime}$ owns an additional term that can be decomposed into three parts. The $b^{-1/3}$ indicates how many valid samples we use to estimate $\\pi_{t}$ . The second part comes from the approximation error of $p$ -values. The last one characterizes the effects of distribution shift. If we properly choose $b$ such that $b\\eta=o(1)$ , this term is negligible. Thus when $b$ and $n_{1}$ both tend to infinity, $\\Delta^{\\prime}$ converges to 0 and the individual and interactive constraints will be controlled asymptotically. ", "page_idx": 17}, {"type": "text", "text": "D Additional Experiments ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.1 Illustration of the Similar Patterns between ${\\tilde{C}}_{2}$ and $C_{2}$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We calculate the empirical ${\\tilde{C}}_{2}$ and $C_{2}$ during the online sample selection procedure under regression setting in Section D.4 with II-COS from 500 replications. The results are summarized in Figure 7. ", "page_idx": 17}, {"type": "image", "img_path": "wblxm5zdkE/tmp/fe00fc85aff81ff0ad10794fe1af57a966888d7d3efa407bb0bd0a269299830f.jpg", "img_caption": ["Figure 7: Illustration of the similar patterns between ${\\tilde{C}}_{2}$ and $C_{2}$ . Line charts of $\\tilde{C}_{2}(\\pmb{\\delta}^{t})$ (red line) and $C_{2}(\\delta^{t})$ (blue line) over time $t$ for the regression example. Experiment details are in Appendix D.4. The black dashed line is $K=0.015$ . The two measures yield almost identical patterns. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "D.2 An Illustration Example of the Flexibility for Choices of $\\alpha$ and $K$ . ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "There exists some trade-off of stopping time and two criteria. In fact, II-COS could result in a short stopping time when only one criterion is considered. Typically, one could choose $K=+\\infty$ for the case the interactive constraint is out of work and only individual criterion control is considered, and meanwhile one can set $\\alpha=1$ with which the interactive constraint is the only concern. The results in Table 3 evaluated this conclusion where $C_{1}$ is the FSR and ${\\tilde{C}}_{2}$ is the ES. ", "page_idx": 17}, {"type": "text", "text": "Table 3: Results of flexible choice of $\\alpha$ and $K$ for the classification example. Average values among 500 repetitions: $\\mathrm{FSR}(\\pmb{\\delta}^{T_{m}})$ , $\\mathrm{ES}(\\pmb{\\delta}^{T_{m}})$ and stopping time $T_{m}$ . ", "page_idx": 18}, {"type": "table", "img_path": "wblxm5zdkE/tmp/4b6f324c93347ad5f76dffbdd7541ed5f2f2545196138ac9e8bd2d3769d61c96.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "wblxm5zdkE/tmp/3c771339a495b6a9538e6f8b48ae646cbe052584f4a8b164daca8737c45508c5.jpg", "table_caption": ["Table 4: Average number of selected samples when stopping for II-COS, LOND, SAFFRON and ADDIS. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "D.3 Effects of the Calibration Size ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We perform some additional simulations under the classification setting, to study the stopping early issue for those methods based on the conformal $p$ -values. ", "page_idx": 18}, {"type": "text", "text": "Take $\\alpha=0.1$ and $K=0.045$ for FSR and mES, respectively. In Table 4, we first fix $n_{\\mathrm{tr}}=1,000$ and $m=100$ and vary $n_{\\mathrm{cal}}$ from 500 to 2,500 to compare the average numbers of selected samples of these three methods with II-COS until stopping. It is clear that all the three benchmarks are unable to select enough samples across all the settings, especially with a small calibration set. As the calibration size $n_{1}$ increases, their selected numbers tend to be close to the target. This can be understood because a larger calibration size would generally yield more accurate detection of the individual of interest and thus alleviate the alpha-death issue to some extent. In contrast, the performances of II-COS, in terms of the number of selected samples until stopping, would be much less influenced by the size of calibration data. The II-COS only stops when $m$ samples are obtained under all the scenarios. ", "page_idx": 18}, {"type": "text", "text": "Furthermore, to verify that II-COS can guarantee both $\\mathrm{FSR}(\\pmb{\\delta}^{T_{m}})$ and $\\mathrm{ES}(\\pmb{\\delta}^{T_{m}})$ control with a relatively small calibration size $n_{\\mathrm{cal}}$ (such as 200), we apply II-COS in synthetic data. The details of the data generation process can be found in Section 4 and D.4. ", "page_idx": 18}, {"type": "text", "text": "We fix $n_{\\mathrm{tr}}\\,=\\,1000$ , $m\\,=\\,100$ and $n_{\\mathrm{cal}}$ varies from 200 to 800. The average of $\\mathrm{FSR}(\\pmb{\\delta}^{T_{m}})$ and $\\mathrm{ES}(\\pmb{\\delta}^{T_{m}})$ with II-COS for both scenarios are calculated from 500 replications. The results are summarized as Table 5. It\u2019s obvious that for different $n_{\\mathrm{cal}}$ , both $\\mathrm{FSR}(\\pmb{\\delta}^{T_{m}})$ and $\\mathrm{ES}(\\pmb{\\delta}^{T_{m}})$ are controlled under the pre-specified constant $\\alpha$ and $K$ respectively under all scenarios with our II-COS method. ", "page_idx": 18}, {"type": "table", "img_path": "wblxm5zdkE/tmp/58af699b0f2334a97d0acadbe870e9b384c249bc654c42f57e195f17edebb9da.jpg", "table_caption": ["Table 5: Average values of $\\mathrm{FSR}(\\delta^{T_{m}})(\\%)$ and $\\mathrm{ES}(\\delta^{T_{m}})(\\times10^{-2})$ (with standard errors in parentheses) for II-COS with different $n_{\\mathrm{cal}}$ under classification setting $\\alpha=10\\%$ , $K=4.50\\times10^{-2})$ and regression setting $\\alpha=10\\%$ , $K=1.50\\times10^{-2})$ . "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "D.4 Results on Synthetic Data for Regression Setting ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The following regression setting is considered: $Y=-7X_{1}^{2}+5\\exp{X_{2}}+10(X_{3}+X_{4})^{2}+\\varepsilon$ , with $\\mathbf{X}\\sim\\mathcal{N}_{4}(\\mathbf{0},\\Bar{\\mathbf{I}}_{4})$ and $\\varepsilon\\sim\\mathcal{N}(0,\\bar{1})$ . The informative set is ${\\mathcal{A}}=(c,\\infty)$ , where $c$ is the $80\\%$ quantile of $Y$ . The FSR and mES are considered as $C_{1}$ and $C_{2}$ , respectively. The prediction algorithm $\\mathcal{H}$ is taken as neural network, with a single hidden layer and 10 hidden neurons, implemented by R package nnet, and $K$ is chosen as 0.015. ", "page_idx": 19}, {"type": "text", "text": "The simulation results are summarized in Figure 8 and 9 from 500 replications. The results are similar to those for classification setting. It\u2019s further verified that our proposed II-COS outperforms all the benchmarks under both classification and regression scenarios. ", "page_idx": 19}, {"type": "image", "img_path": "wblxm5zdkE/tmp/e11116bf2f65649870a19a1c7716d207829e1fa9ec1b6ac3ad77a715d1d8576e.jpg", "img_caption": ["Method II\u2212COS SAST LOND SAFFRON ADDIS "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 8: Simulation results for regression setting. Line charts of $\\mathrm{FSR}(\\pmb{\\delta}^{t})$ (Left) and $\\mathrm{ES}(\\pmb{\\delta}^{t})$ (Right) for II-COS, SAST, LOND, SAFFRON and ADDIS over time $t$ . The black dashed lines are the corresponding FSR level $\\alpha=0.1$ and the ES level $K=0.015$ . Shading represents error bars of one standard error above and below. ", "page_idx": 19}, {"type": "image", "img_path": "wblxm5zdkE/tmp/427bc26d62456e87365f34bb53e684f51a9b2c6eaa092351d6217daf67cb972f.jpg", "img_caption": ["Figure 9: Simulation results for regression setting. Boxplots of $\\mathrm{FSR}(\\pmb{\\delta}^{T_{m}})$ (Left), $\\mathrm{ES}(\\pmb{\\delta}^{T_{m}})$ (Middle) and $T_{m}$ (Right) for II-COS, SAST, LOND, SAFFRON and ADDIS. The black dashed lines are the corresponding FSR level $\\alpha=0.1$ and ES level $K=0.015$ . "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "D.5 Experiments for Other Individual and Interactive Constrains. ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Other individual constraints To better illustrate the performance of our proposed method, we also conduct an experiment for the general individual constraint. We choose $\\begin{array}{r}{G_{0}(\\mathbf{X})=|\\sum_{i=1}^{d}X_{i}|}\\end{array}$ , $\\begin{array}{r}{G_{1}(X)=|\\sum_{i=1}^{d}X_{i}|/2}\\end{array}$ for EC (expected cost), and choose $G_{0}(\\mathbf{X})=1$ , $G_{1}(\\mathbf{X})=0$ for FSR. Other settings are the same as the classification model. The results are shown in Figure 10. We can see that only the proposed II-COD can guarantee all EC (expected cost), FSR and ES control, while all the benchmarks are out of control for EC or ES. ", "page_idx": 19}, {"type": "text", "text": "Other pairwise function $g$ . Besides the RBF kernel, popular choices include the cosine similarity [47, 52] with adjustment $\\bar{g}(\\mathbf{X},\\mathbf{X}^{\\prime})=\\mathbf{X}^{\\top}\\mathbf{X}^{\\prime}/(\\|\\mathbf{X}\\|_{2}\\|\\dot{\\mathbf{X}}^{\\prime}\\|_{2})+1$ , and the absolute value of cosine similarity, i.e., $g(\\mathbf{X},\\mathbf{X}^{\\prime})=|\\mathbf{X}^{\\top}\\mathbf{X}^{\\prime}|/(\\|\\mathbf{X}\\|_{2}\\|\\mathbf{X}^{\\prime}\\|_{2})$ which characterizes the orthogonality between $\\mathbf{X}$ and $\\mathbf{X^{\\prime}}$ and is often considered in the field of design of experiments [15]. The similarity functions mentioned above all satisfy Assumption 3.2. As a supplement to the experiments in the main text, we choose the cosine similarity with adjustment $g(\\mathbf{X},\\dot{\\mathbf{X^{\\prime}}})=\\mathbf{X^{\\top}X^{\\prime}}/(\\|\\mathbf{X}\\|_{2}\\|\\mathbf{X^{\\prime}}\\|_{2})+1$ for classification setting. Notice that the original cosine similarity $\\mathbf{X}^{\\top}\\mathbf{X}^{\\prime}/(\\|\\mathbf{X}\\|_{2}\\|\\mathbf{X}^{\\prime}\\|_{2})$ can be negative sometimes. Hence we add a constant 1 which guarantees $g(\\mathbf{X},\\mathbf{X}^{\\prime})\\geq0$ and does not change the final results. The simulation results are summarized in Figure 11 from 500 replications. As can be seen, the simulation results are similar to those when choosing the RBF kernel in Section 4.1. ", "page_idx": 19}, {"type": "image", "img_path": "wblxm5zdkE/tmp/c868f9c886889cb6b87bce1c9cef7a2a4cf9eafa01698c8afb5fb656c18597a9.jpg", "img_caption": ["Figure 10: Boxplots of $\\operatorname{EC}(\\pmb{\\delta}^{T_{m}})$ , $\\mathrm{FSR}(\\pmb{\\delta}^{T_{m}})$ , and $\\mathrm{ES}(\\pmb{\\delta}^{T_{m}})$ for II-COS, SAST, LOND, SAFFRON and ADDIS under classification model. The black dashed lines indicate the corresponding nominal levels $\\alpha_{1}=$ $0.5,\\alpha_{2}=0.2,K=0.045$ . "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "image", "img_path": "wblxm5zdkE/tmp/7d0f2975b3fc02f0e1703371be2b881791d6574b650005d01fb2ce56205a6b78.jpg", "img_caption": ["Figure 11: Boxplots of $\\mathrm{FSR}(T_{m})$ , $\\mathrm{ES}(T_{m})$ and $T_{m}$ with random forest algorithms under classification setting for II-COS, SAST, LOND, SAFFRON and ADDIS. The similarity function $g$ is chosen as the cosine similarity with adjustment $g(\\mathbf{X},\\mathbf{X}^{\\prime})=\\mathbf{X}^{\\top}\\mathbf{X}^{\\prime}/(\\|\\mathbf{X}\\|_{2}\\|\\mathbf{X}^{\\prime}\\|_{2})+1$ . The black dashed lines are the corresponding FSR level $\\alpha=0.1$ and the ES level $K=1.0$ . "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "D.6 Additional Results for Real Data Analysis ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In Table 6, the average proportions of handicapped and female in the selected candidates are in the last two columns of (a). The average proportions of minority and female among the correctly selected individuals are in the last two columns of (b). ", "page_idx": 20}, {"type": "text", "text": "D.7 Additional Experiments for Different Learning Algorithms ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Except for the random forest algorithm, we further apply two different learning algorithms $\\mathcal{H}$ for classification setting to estimate the model $\\hat{\\mu}(\\mathbf{X})$ : Support vector machine (SVM) and Neural network (NN) with a single hidden layer and 10 hidden neurons, which are implemented by $\\mathbf{R}$ packages kernlab and nnet, respectively. For NN algorithm, the entropy fitting is used for classification setting. The empirical $\\mathrm{FSR}(T_{m})$ and $\\mathrm{ES}(T_{m})$ levels are estimated by the average of the false selection proportion and the expected similarity respectively from 500 replications. The results are summarized in Figure 12. As can be seen, the simulation results are similar as those when applying random forest in Section 4.1. ", "page_idx": 20}, {"type": "text", "text": "Table 6: Average values with candidate dataset and income dataset: $\\mathrm{FSR}(\\delta^{T_{m}}),\\,\\mathrm{ES}(\\delta^{T_{m}})\\ (\\times10^{-3})$ and stopping time $T_{m}$ . The average proportions of handicapped and female in the selected candidates are in the last two columns of (a). The average proportions of minority and female among the correctly selected individuals are in the last two columns of (b). The target FSR level is $\\alpha=0.2$ for both. For the candidate data, the target ES level $K=1\\times10^{-3}$ ; For the income data, $K=6\\times10^{-3}$ . ", "page_idx": 21}, {"type": "text", "text": "(b) Income dataset [8] ", "page_idx": 21}, {"type": "table", "img_path": "wblxm5zdkE/tmp/9ef080fb7128d906f367441072bdff134546e5885555840bbe494764d1bd9054.jpg", "table_caption": ["(a) Candidate dataset [22] "], "table_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "wblxm5zdkE/tmp/1d14e4fc8480aca2bb42cda59c60edf1ea2fa8982f2c07b4a339c6ef99020afe.jpg", "img_caption": ["Figure 12: Boxplots of $\\mathrm{FSR}(T_{m})$ and $\\mathrm{ES}(T_{m})$ with two different learning algorithms under classification setting for II-COS, SAST, LOND, SAFFRON and ADDIS. The black dashed lines are the corresponding FSR level $\\alpha=0.1$ and the ES level $K=0.045$ . "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "D.8 Experiments for the comparison with oracle procedure ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Regarding efficiency, we conducted an experiment to compare the effectiveness of II-COS with an oracle method possessing knowledge of true state $\\theta_{t}$ . This can be succinctly formulated as follows. At time $t$ , $\\delta_{t}=1$ if $\\theta_{t}=1$ and $\\mathrm{PC}(\\bar{\\delta}^{\\mathrm{T_{m}}})/\\mathrm{PS}(\\delta^{\\mathrm{T_{m}}})\\leq\\mathrm{K}$ ,the process halts when $\\textstyle\\sum_{i\\leq t}\\delta_{i}=m$ . To ensure a fair comparison, we fix $\\alpha=0.01$ for II-COS and choose $K=0.045$ for both methods. The results are shown in Table 7. The FSR of the oracle procedure is 0 as expected. The $T_{m}$ of II-COS is very close to the oracle. This close proximity indicates the high efficiency of II-COS. Both FSR and ES are effectively controlled by II-COS. ", "page_idx": 21}, {"type": "text", "text": "D.9 Experiments for the Extended II-COS under Varying Proportion Case ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Let $N$ be a prespecified large integer denoting the number of samples arriving sequentially. Denote $n$ as the size of available labeled history dataset $\\mathcal{D}$ . We fix $N\\,=\\,5000$ , $n=5000$ . Consider three different varying patterns for $\\pi_{t}$ : ", "page_idx": 21}, {"type": "text", "text": "Table 7: $\\mathrm{FSR}(\\pmb{\\delta}^{T_{m}})$ , $\\mathrm{ES}(\\pmb{\\delta}^{T_{m}})$ $(\\times10^{-2})$ and $T_{m}$ of the II-COS and the oracle procedure under classification model. ", "page_idx": 21}, {"type": "text", "text": "1. Blocks pattern: $\\pi_{t}~=~0.3$ , for $t\\;\\in\\;[1,100]\\;\\cup$ [501, 600] \u222a [1001, 1100] $\\mathrm{~\\ensuremath~{~\\vert~}~}[1501,2000]\\;\\;\\cup$ [2001, 2100] \u222a [2501, 2600] \u222a [3001, 3100] \u222a [3501, 4000]; $\\pi_{t}~=~0.2$ for $t\\ \\in\\ [101,500]\\cup$ $[601,1000]~\\cup~[1101,1500]~\\cup~[2101,2500]~\\cup~[102,2500]$ $[2601,3000]\\cup[3101.3500]\\cup[4000,5000]$ . ", "page_idx": 21}, {"type": "table", "img_path": "wblxm5zdkE/tmp/d178b1f509602d0f66ac3a7c840041c88ede48ad1fe99f61db6867e955744644.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "2. Linear pattern: Vary $\\pi_{t}$ linearly from 0 to 0.5, $\\pi_{t}=0.5t/N$ .   \n3. Sine pattern: $\\pi_{t}~=~\\{\\sin(8\\pi t/N)\\,+\\,1\\}/4,$ $\\pi_{t}$ ranges between 0 and 0.5. ", "page_idx": 22}, {"type": "text", "text": "The other settings are the same as those in classification setting. For the historical dataset, we fix $\\pi_{t}=0.2$ . ", "page_idx": 22}, {"type": "text", "text": "In simulation we generate 500 data points prior to $t=1$ to form an initial proportion estimate. The varying proportion estimates are updated every 200 time points. The bandwidth parameter $b$ for estimating $\\bar{\\pi}_{t}^{\\lambda}$ are chosen based on normal reference rule. We set $b=38$ . The window size $q$ are chosen by a rule of thumb in practice. Let $q=C b$ , where $C$ is a fixed constant between 10 and 20. In our simulations, we fix $q=500$ and $\\lambda=0.5$ . As for the density estimation, we first estimate $f_{0}$ and $f_{1}$ on the calibration data, then compute $\\widehat{f}^{t}=\\widehat{f}_{0}(w)(1-\\widehat{\\pi}_{t})+\\widehat{f}_{1}(w)\\widehat{\\pi}_{t}$ . Thus accordingly, we estimate $L_{t}$ by $\\begin{array}{r}{\\widehat{L}_{t}^{\\lambda}=\\frac{(1-\\widehat{\\pi}_{t}^{\\lambda})\\widehat{f}_{0}(W_{t})}{\\widehat{f}^{t}(W_{t})}\\wedge1}\\end{array}$ . As f or tho se methods  based  on con formal $p$ -values, they do not need modification since they only require the exchangeability of data in the null hypothesis. The varying proportion case does not violate such a condition. ", "page_idx": 22}, {"type": "text", "text": "The simulation results are shown in Figures 13-15, from which we can observe that the extended version of II-COS performs better than the benchmarks under all three different varying settings for both $C_{1}(\\mathrm{FSR})$ and $C_{2}(\\mathrm{ES})$ control. ", "page_idx": 22}, {"type": "image", "img_path": "wblxm5zdkE/tmp/20d156e98ac1359cfcccc563e456213e378007f4829758dd74cf17c3f1febd16.jpg", "img_caption": ["Figure 13: Boxplots of $\\mathrm{FSR}(\\pmb{\\delta}^{T_{m}})$ , $\\mathrm{ES}(\\pmb{\\delta}^{T_{m}})$ and $T_{m}$ for II-COS, SAST, LOND, SAFFRON and ADDIS (Blocks pattern). The black dashed lines indicate the corresponding nominal levels. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "wblxm5zdkE/tmp/14e88ed642d7cea457a3027617387956d6f5637fce894a380afd7b0940e0ff9c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 14: Boxplots of $\\mathrm{FSR}(\\pmb{\\delta}^{T_{m}})$ , $\\mathrm{ES}(\\pmb{\\delta}^{T_{m}})$ and $T_{m}$ for II-COS, SAST, LOND, SAFFRON and ADDIS (Linear pattern). The black dashed lines indicate the corresponding nominal levels. ", "page_idx": 22}, {"type": "image", "img_path": "wblxm5zdkE/tmp/774dfb751bfae91aac082da9e37e4c4823c0f06668881a95b0c819b88e5f914c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 15: Boxplots of $\\mathrm{FSR}(\\pmb{\\delta}^{T_{m}})$ , $\\mathrm{ES}(\\pmb{\\delta}^{T_{m}})$ and $T_{m}$ for II-COS, SAST, LOND, SAFFRON and ADDIS (Sine pattern). The black dashed lines indicate the corresponding nominal levels. ", "page_idx": 23}, {"type": "text", "text": "E Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "E.1 Auxiliary Lemmas ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The following results include a standard uniform bound for kernel density estimator [36, 20] and a simple corollary from the central limit theorem [11]. ", "page_idx": 23}, {"type": "text", "text": "Lemma E.1. If Assumption 3.1 hold and we take the bandwidth $h=n_{1}^{-1/(2\\beta+1)}$ 1\u22121/(2\u03b2+1), then with probability at least $1-1/n_{1}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{w\\in\\mathbb{R}}|\\widehat{f}(w)-f(w)|\\leq D_{1}n_{1}^{\\frac{-\\beta}{2\\beta+1}}\\sqrt{\\log n_{1}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $D_{1}=D_{1}(M,c_{\\beta},\\beta,K)$ is a positive constant depending on $M$ and $\\beta,c_{\\beta}$ of H\u00f6lder continuity and the kernel $K(\\cdot)$ . ", "page_idx": 23}, {"type": "text", "text": "Lemma E.2. The estimation $\\widehat{\\pi}$ satisfies ", "page_idx": 23}, {"type": "equation", "text": "$$\n|\\widehat{\\pi}-\\pi|\\leq\\sqrt{\\pi(1-\\pi)}n_{1}^{-\\frac{1}{2}+\\gamma},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "with probability $1-n_{1}^{-2\\gamma}$ for any constant $0<\\gamma<1/2$ . ", "page_idx": 23}, {"type": "text", "text": "Next lemma characterizes the uniform convergence of $\\widehat{L}(w)$ . ", "page_idx": 23}, {"type": "text", "text": "Lemma E.3 (Uniform convergence of $\\widehat{L}(w)\\!\\right]$ ). Suppose Assumption 3.1 holds. Taking the bandwidth of kernel density estimator as $h=n_{1}^{-1/(2\\beta+1)}$ 1\u22121/(2\u03b2+1), then we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{w\\in\\mathbb{R}}|\\widehat{L}(w)-L(w)|\\leq D_{2}n_{1}^{\\frac{-\\beta}{2\\beta+1}}\\sqrt{\\log n_{1}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for some positive constant $D_{2}=D_{2}(M,D_{1},\\ell,\\pi)$ with probability $1-2/n_{1}-1/n_{1}^{1/3}$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. ", "page_idx": 23}, {"type": "text", "text": "Note thatL (w) = (1 \u2212\u03c0)f 0(w) and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~\\underset{w\\in\\mathbb{R}}{\\operatorname*{sup}}\\left|\\hat{L}(w)-L(w)\\right|\\leq\\underset{w\\in\\mathbb{R}}{\\operatorname*{sup}}\\left|(1-\\widehat{\\pi})\\frac{\\hat{f}_{0}(w)}{\\widehat{f}(w)}-L(w)\\right|=\\underset{w\\in\\mathbb{R}}{\\operatorname*{sup}}\\left|\\frac{(1-\\widehat{\\pi})\\widehat{f}_{0}(w)f(w)-(1-\\pi)f_{0}(w)}{\\widehat{f}(w)\\,f(w)}\\right|}\\\\ &{\\leq\\underset{w\\in\\mathbb{R}}{\\operatorname*{sup}}\\frac{1}{\\widehat{f}(w)f(w)}\\left\\{(1-\\widehat{\\pi})f(w)|\\widehat{f}_{0}(w)-f_{0}(w)|+|\\pi-\\widehat{\\pi}|f_{0}(w)f(w)+(1-\\pi)f_{0}(w)|\\widehat{f}(w)-f(1-\\widehat{\\pi})\\right\\}}\\\\ &{\\leq\\underset{w\\in\\mathbb{R}}{\\operatorname*{sup}}\\frac{\\widehat{f}_{0}(w)-f_{0}(w)|+f_{0}(w)|\\pi-\\widehat{\\pi}|}{\\widehat{f}(w)}+\\underset{w\\in\\mathbb{R}}{\\operatorname*{sup}}\\frac{L(w)|\\widehat{f}(w)-f(w)|}{\\widehat{f}(w)}}\\\\ &{\\overset{(\\mathrm{g})}{\\leq}\\underset{\\operatorname*{inf}_{w\\in\\mathbb{R}}}{\\sum}\\frac{1}{\\widehat{f}(w)\\,\\overline{{\\chi}}^{\\frac{\\alpha+1}{\\alpha}}\\sqrt{\\log n_{1}}+M\\sqrt{\\pi}(1-\\pi)n_{1}^{-1+\\gamma}+D_{1}\\cdot n_{1}^{\\frac{\\alpha-s}{2^{\\frac{\\alpha+1}{\\alpha}}\\sqrt{\\log n_{1}}}}}}\\\\ &{\\overset{(i i)}{\\leq}\\frac{2}{\\widehat{f}}\\left\\{D_{1}^{\\frac{n}{n_{1}^{2+s}}}\\sqrt{\\log n_{1}}+M\\sqrt{p(1-p)}n_{1}^{-\\frac{1}{2}+\\gamma}+D_{1}\\cdot n_{1}^{\\frac{\\alpha-s}{2^{\\frac{\\alpha+1}{\\alpha}}\\sqrt{\\log n_{1}}}}\\right\\}}\\\\ &{\\overset{(i i i)}{\\leq}D_{2}n_{1}^{\\frac{n}{2+s}}\\sqrt{\\log n_{1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $D_{1}^{*}=D_{1}^{*}(D_{1},\\pi)$ is a positive constant depending on $D_{1}$ and $\\pi$ because $\\widehat{f}_{0}$ is estimated by about $n_{1}(1-\\pi)$ samples. The $(i)$ follows the results directly in Lemma E.1 an d Lemma E.2 and the bounds of $f(w)$ and $f_{0}(w)$ in Assumption 3.1-(1) with probability $1-2/n_{1}-1/n_{1}^{2\\gamma}$ . The $(i i)$ holds since ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\widehat{f}(w)\\geq f(w)-D_{1}n_{1}^{\\frac{-\\beta}{2\\beta+1}}\\sqrt{\\log n_{1}}\\geq\\ell-D_{1}n_{1}^{\\frac{-\\beta}{2\\beta+1}}\\sqrt{\\log n_{1}}>\\ell/2\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for sufficiently large $n_{1}$ . Due to the fact $\\beta\\leq1$ and taking $\\gamma=1/6$ in Lemma E.2, the $(i i i)$ holds with $D_{2}=2\\{D_{1}^{*}+M\\sqrt{\\pi(1-\\pi)}+D_{1}\\}/\\ell$ . Hence we verify the conclusion. \u25a1 ", "page_idx": 24}, {"type": "text", "text": "The next lemma shows the property of data-driven II-COS procedure which is useful to give the bound of the mES gap. ", "page_idx": 24}, {"type": "text", "text": "Lemma E.4. Suppose $\\delta^{t}$ is the decision result at time $t\\geq T_{s}$ by the data-driven II-COS procedure with $\\begin{array}{r}{\\sum_{i\\leq t}\\widehat{L}_{i}\\delta_{i}/\\bar{1}\\vee\\sum_{i\\leq t}\\delta_{i}\\leq\\alpha^{\\prime}}\\end{array}$ , where $\\begin{array}{r}{T_{s}=\\operatorname*{inf}\\{t:\\sum_{i=1}^{t}\\delta_{i}=s\\}}\\end{array}$ and $s\\geq2$ . Then we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{\\mathbb{E}\\big(\\sum_{1\\le i<j\\le t}\\delta_{i}\\delta_{j}\\big)}{\\mathbb{E}\\Big\\{\\sum_{1\\le i<j\\le t}\\delta_{i}\\delta_{j}\\big(1-\\widehat{L}_{i}\\big)\\big(1-\\widehat{L}_{j}\\big)\\Big\\}}\\le\\big(1-\\frac{2s}{s-1}\\alpha^{\\prime}\\big)^{-1}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. It suffices to show ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{1\\leq i<j\\leq t}\\delta_{i}\\delta_{j}\\big(1-\\widehat{L}_{i}\\big)(1-\\widehat{L}_{j})\\geq\\big(1-\\frac{2s}{s-1}\\alpha^{\\prime}\\big)\\sum_{1\\leq i<j\\leq t}\\delta_{i}\\delta_{j}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By the selection procedure, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{t}\\delta_{i}(1-\\widehat{L}_{i})\\geq(1-\\alpha^{\\prime})\\sum_{i=1}^{t}\\delta_{i}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Squaring both sides of (9) and making some decomposition, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{1\\leq i<j\\leq t}\\delta_{i}\\delta_{j}(1-\\widehat{L}_{i})(1-\\widehat{L}_{j})\\geq(1-\\alpha^{\\prime})^{2}\\sum_{1\\leq i<j\\leq t}\\delta_{i}\\delta_{j}+\\frac{1}{2}\\sum_{i=1}^{t}\\{(1-\\alpha^{\\prime})^{2}-(1-\\widehat{L}_{i})^{2}\\}\\delta_{i}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Notice that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{t}\\delta_{i}=\\frac{2}{s-1}\\sum_{1\\leq i<j\\leq t}\\delta_{i}\\delta_{j}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad(1-\\alpha^{\\prime})^{2}\\displaystyle\\sum_{1\\leq i<j\\leq t}\\delta_{i}\\delta_{j}+\\frac{1}{2}\\displaystyle\\sum_{i=1}^{t}\\{(1-\\alpha^{\\prime})^{2}-(1-\\widehat{L}_{i})^{2}\\}\\delta_{i}}\\\\ &{\\geq(1-\\alpha^{\\prime})^{2}\\displaystyle\\sum_{1\\leq i<j\\leq t}\\delta_{i}\\delta_{j}+\\frac{1}{2}\\displaystyle\\sum_{i=1}^{t}\\{(1-\\alpha^{\\prime})^{2}-1\\}\\delta_{i}}\\\\ &{\\geq(1-\\alpha^{\\prime})^{2}\\displaystyle\\sum_{1\\leq i<j\\leq t}\\delta_{i}\\delta_{j}-\\frac{\\alpha^{\\prime}(2-\\alpha^{\\prime})}{2}\\displaystyle\\frac{2}{s-1}\\displaystyle\\sum_{1\\leq i<j\\leq t}\\delta_{i}\\delta_{j}}\\\\ &{\\geq(1-\\displaystyle\\frac{2s}{s-1}\\alpha^{\\prime})\\displaystyle\\sum_{1\\leq i<j\\leq t}\\delta_{i}\\delta_{j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The next two lemmas will be used in the analysis of the extended II-COS for varying proportion case. ", "page_idx": 25}, {"type": "text", "text": "Lemma E.5. Assume that random variables $Y_{1},\\,Y_{2}$ , have density functions bounded by a constant $c~>~0$ , and other two random variables $Y_{3}$ , and $Y_{4}$ satisfy $\\mathbb{P}\\!\\left(|Y_{1}-Y_{3}|\\,\\le\\,\\varepsilon\\right)\\,\\ge\\,1\\,-\\,\\zeta$ and $\\mathbb{P}(|Y_{2}-Y_{4}|\\le\\epsilon)\\ge1-\\varsigma,$ , where $\\varepsilon>0$ , $\\epsilon>0$ , $\\zeta>0$ and $\\varsigma>0$ . Then for all $t>0$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathbb{P}(Y_{1}>t,Y_{2}>t)-\\mathbb{P}(Y_{3}>t,Y_{4}>t)|\\le2c(\\varepsilon+\\epsilon)+3(\\zeta+\\varsigma).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Define the events $\\mathcal{E}_{1}=\\left\\{|Y_{1}-Y_{3}|\\leq\\varepsilon\\right\\}$ , $\\mathcal{E}_{2}=\\{|Y_{2}-Y_{4}|\\leq\\epsilon\\}$ , and $\\mathcal{E}=\\mathcal{E}_{1}\\bigcup\\mathcal{E}_{2}$ . Note that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(Y_{1}+\\varepsilon>t,\\mathcal{E}_{1})\\le\\mathbb{P}(Y_{3}>t,\\mathcal{E}_{1})\\le\\mathbb{P}(Y_{1}-\\varepsilon>t,\\mathcal{E}_{1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(Y_{2}+\\epsilon>t,\\mathcal{E}_{2})\\le\\mathbb{P}(Y_{4}>t,\\mathcal{E}_{2})\\le\\mathbb{P}(Y_{2}-\\epsilon>t,\\mathcal{E}_{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(Y_{1}>t,Y_{2}>t)-\\mathbb{P}(Y_{3}>t,Y_{4}>t)}\\\\ &{\\le|\\mathbb{P}(Y_{1}>t,Y_{2}>t)-\\mathbb{P}(Y_{3}>t,Y_{4}>t,\\mathcal{E})|+\\mathbb{P}(\\mathcal{E}_{1}^{c})+\\mathbb{P}(\\mathcal{E}_{2}^{c})}\\\\ &{\\le|\\mathbb{P}(Y_{1}>t,Y_{2}>t)-\\mathbb{P}(Y_{1}-\\varepsilon>t,Y_{2}-\\epsilon>t,\\mathcal{E})|}\\\\ &{\\quad+\\,|\\mathbb{P}(Y_{1}>t,Y_{2}>t)-\\mathbb{P}(Y_{1}+\\varepsilon>t,Y_{2}+\\epsilon>t,\\mathcal{E})|+\\mathbb{P}(\\mathcal{E}_{1}^{c})+\\mathbb{P}(\\mathcal{E}_{2}^{c})}\\\\ &{\\le|\\mathbb{P}(Y_{1}>t,Y_{2}>t)-\\mathbb{P}(Y_{1}-\\varepsilon>t,Y_{2}-\\epsilon>t)|+3\\mathbb{P}(\\mathcal{E}_{1}^{c})+3\\mathbb{P}(\\mathcal{E}_{2}^{c})}\\\\ &{\\quad+\\,|\\mathbb{P}(Y_{1}>t,Y_{2}>t)-\\mathbb{P}(Y_{1}+\\varepsilon>t,Y_{2}+\\epsilon>t)|}\\\\ &{\\le2c(\\varepsilon+\\epsilon)+3(\\zeta+\\varsigma),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "the last inequality holds because the density function is bounded. ", "page_idx": 25}, {"type": "text", "text": "Lemma E.6. If $b^{\\zeta}\\leq q$ for some $\\zeta>1$ and $b$ is sufficiently large, the exponential weight $\\kappa_{b}(s)=$ $\\exp\\{-|s|/b\\}$ satisfies ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{\\sum_{s=1}^{q}\\kappa_{b}^{2}(s)}{\\{\\sum_{s=1}^{q}\\kappa_{b}(s)\\}^{2}}\\leq C_{b}\\frac{1}{b}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{\\sum_{s=1}^{q}s\\kappa_{b}(s)}{\\sum_{s=1}^{q}\\kappa_{b}(s)}\\leq C_{b}b,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $C_{b}>0$ is constant determined by $b$ . ", "page_idx": 25}, {"type": "text", "text": "Proof. Take $Z_{b}=\\exp\\{-1/b\\}$ . Notice that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{s=i}^{j}\\kappa_{b}(s)=\\frac{\\exp\\{-(i+1)/b\\}-\\exp\\{-(j+1)/b\\}}{1-\\exp\\{-1/b\\}}=\\frac{Z_{b}^{i+1}-Z_{b}^{j+1}}{1-Z_{b}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus for a constant $C_{b}>0$ . we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\sum_{s=1}^{q}\\kappa_{b}^{2}\\left(s\\right)}{\\{\\sum_{s=1}^{q}\\kappa_{b}(s)\\}^{2}}=\\Big(\\frac{Z_{b}^{4}-Z_{b}^{2q}}{1-Z_{b}^{2}}\\Big)/\\Big(\\frac{Z_{b}^{4}-2Z_{b}^{q+2}+Z_{b}^{2q}}{1+Z_{b}^{2}-2Z_{b}}\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\overset{(i)}{\\le}C_{b}(1-Z_{b})}\\\\ &{\\qquad\\qquad\\qquad\\overset{(i i)}{\\le}C_{b}\\frac{1}{b},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $(i)$ holds since $b$ is sufficiently large such that $Z_{b}^{q}\\,=\\,\\exp\\{-q/b\\}\\,\\le\\,\\exp\\{-b^{\\zeta-1}\\}$ can be eliminated and $Z_{b}<1$ . The last inequality $(i i)$ holds by $\\exp\\{x\\}\\geq x+1$ . ", "page_idx": 26}, {"type": "text", "text": "By the same discussion, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{\\sum_{s=1}^{q}s\\kappa_{b}\\left(s\\right)}{\\sum_{s=1}^{q}\\kappa_{b}\\left(s\\right)}\\le\\int_{1}^{\\infty}s Z_{b}^{s}d s/\\Bigl(\\frac{Z_{b}^{2}-Z_{b}^{q}}{1-Z_{b}}\\Bigr)}}\\\\ &{}&{\\le\\Bigl(\\frac{Z_{b}}{(1-Z_{b})^{2}}\\Bigr)/\\Bigl(\\frac{Z_{b}^{2}-Z_{b}^{q}}{1-Z_{b}}\\Bigr)}\\\\ &{}&{\\le C_{b}\\frac{1}{1-Z_{b}}}\\\\ &{}&{\\le C_{b}(b+1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The last inequality holds since $\\exp\\{x\\}\\leq1/(1-x)$ for $x<1$ . For simplicity, we use the same notation $C_{b}$ to denote the constants. \u25a1 ", "page_idx": 26}, {"type": "text", "text": "E.2 Proof of Proposition 2.1 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Proof. For the part of individual constraint control, note that $L_{i}$ is defined as ", "page_idx": 26}, {"type": "equation", "text": "$$\nL_{i}=\\operatorname*{Pr}(\\theta_{i}=0\\mid W_{i})=\\mathbb{E}\\left[(1-\\theta_{i})\\mid W_{i}\\right].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Define $\\mathcal{W}^{*}\\,=\\,\\sigma(W_{1},\\cdot\\cdot\\cdot)$ . The stopping time $T$ is measurable respect to $\\mathcal{W}^{*}$ . The individual constraint at time $T$ satisfies ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{C_{1}(\\delta^{T})}}&{{=}}&{{\\mathbb{E}\\left\\{\\frac{\\sum_{i\\leq T}\\{(1-\\theta_{i})G_{0}(\\mathbf{X}_{i})+\\theta_{i}G_{1}(\\mathbf{X}_{i})\\}\\delta_{i}}{(\\sum_{i\\leq T}\\delta_{i})\\vee1}\\right\\}}}\\\\ {{}}&{{=}}&{{\\mathbb{E}\\left[\\mathbb{E}\\big\\{(R_{T}\\vee1)^{-1}\\underset{i\\leq t}{\\sum_{i\\leq t}\\{(1-\\theta_{i})G_{0}(\\mathbf{X}_{i})+\\theta_{i}G_{1}(\\mathbf{X}_{i})\\}\\delta_{i}\\ |\\ W^{*}\\}\\right\\}\\right]}}\\\\ {{}}&{{\\overset{(i)}{=}}}&{{\\mathbb{E}\\left[(R_{t}\\vee1)^{-1}\\underset{i\\leq t}{\\sum_{i\\leq t}\\{\\mathbb{E}\\{(1-\\theta_{i})\\ |W_{i}\\}{G_{0}(\\mathbf{X}_{i})+\\mathbb{E}\\{\\theta_{i}\\ |W_{i}\\}{G_{1}(\\mathbf{X}_{i})}\\}\\delta_{i}}}\\right]}}\\\\ {{}}&{{=}}&{{\\mathbb{E}\\big\\{(R_{t}\\vee1)^{-1}\\underset{i\\leq t}{\\sum_{i\\leq t}\\{L_{i}G_{0}(\\mathbf{X}_{i})+(1-L_{i})G_{1}(\\mathbf{X}_{i})\\}\\delta_{i}\\big\\}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The $(i)$ holds since $\\mathbf{X}_{i}$ \u2019s are independent of each other. By construction of the selection rule, we have that at the stopping time $T$ $\\begin{array}{r}{\\ ',((R_{T}^{}\\vee1)^{-1}\\sum_{i\\leq T}\\{L_{i}W_{0}(\\mathbf{\\dot{X}}_{i})+(1-L_{i})W_{1}(\\mathbf{X}_{i})\\}\\delta_{i}\\leq\\alpha)}\\end{array}$ . It follows that $C_{1}(\\delta^{T})\\le\\alpha$ at a random time $T$ . By construction of the selection rule, we have for stopping time $T$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\displaystyle\\sum_{1\\leq i<j\\leq T}g(\\mathbf{X}_{i},\\mathbf{X}_{j})\\delta_{i}\\delta_{j}(1-L_{i})(1-L_{j})}&{{}\\leq}&{K\\times\\displaystyle\\sum_{1\\leq i<j\\leq T}\\delta_{i}\\delta_{j}(1-L_{i})(1-L_{j}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Taking expectations on both sides of inequality (10) and by double expectation theorem, we finally obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big\\{\\sum_{1\\leq i<j\\leq T}g({\\mathbf{X}}_{i},{\\mathbf{X}}_{j})\\delta_{i}\\delta_{j}\\theta_{i}\\theta_{j}\\Big\\}\\quad\\leq\\quad K\\times\\mathbb{E}\\Big\\{\\sum_{1\\leq i<j\\leq T}\\delta_{i}\\delta_{j}\\theta_{i}\\theta_{j}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "It follows that for every time $t\\geq T_{2}$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n{C_{2}}(\\delta^{T})=\\frac{\\mathbb{E}\\Big\\{\\sum_{1\\leq i<j\\leq T}g(\\mathbf{X}_{i},\\mathbf{X}_{j})\\delta_{i}\\delta_{j}\\theta_{i}\\theta_{j}\\Big\\}}{\\mathbb{E}\\Big\\{(\\sum_{i\\leq T}\\delta_{i}\\theta_{i})(\\sum_{i\\leq T}\\delta_{i}\\theta_{i}-1)\\Big\\}}\\leq K\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "E.3 Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Proof. In the data-driven II-COS procedure, $\\delta_{i}$ is determined by the estimated lFDR ${\\widehat{L}}_{i}$ and we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{1}{R_{t}}\\sum_{i\\leq t}\\{\\widehat{L}_{i}G_{0}(\\mathbf{X}_{i})+(1-\\widehat{L}_{i})G_{1}(\\mathbf{X}_{i})\\}\\delta_{i}\\leq\\alpha.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Note that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\bar{\\Sigma}}_{1}(\\delta^{t})=\\mathbb{E}\\Big\\{\\displaystyle\\frac{1}{R_{t}}\\sum_{i\\leq t}\\{L_{i}G_{0}(\\mathbf{X}_{i})+(1-L_{i})G_{1}(\\mathbf{X}_{i})\\}\\delta_{i}\\Big\\}}\\\\ &{\\qquad=\\mathbb{E}\\Big\\{\\displaystyle\\frac{1}{R_{t}}\\sum_{i\\leq t}\\{\\widehat{L}_{i}G_{0}(\\mathbf{X}_{i})+(1-\\widehat{L}_{i})G_{1}(\\mathbf{X}_{i})\\}\\delta_{i}\\Big\\}+\\mathbb{E}\\Big\\{\\displaystyle\\frac{1}{R_{t}}\\sum_{i\\leq t}\\{(L_{i}-\\widehat{L}_{i})G_{0}(\\mathbf{X}_{i})+(\\widehat{L}_{i}-L_{i})G_{1}(\\mathbf{X}_{i})\\}\\delta_{i}\\Big\\}}\\\\ &{\\qquad\\leq\\alpha+\\mathbb{E}\\Big\\{\\displaystyle\\frac{1}{R_{t}}\\sum_{i\\leq t}\\{(L_{i}-\\widehat{L}_{i})G_{0}(\\mathbf{X}_{i})+(\\widehat{L}_{i}-L_{i})G_{1}(\\mathbf{X}_{i})\\}\\delta_{i}\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "It suffices to bound the absolute value of the second term. We have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Big|\\mathbb{E}\\Big\\{\\frac{1}{R_{t}}\\sum_{i\\leq t}\\{(L_{i}-\\widehat{L}_{i})G_{0}(\\mathbf{X}_{i})+(\\widehat{L}_{i}-L_{i})G_{1}(\\mathbf{X}_{i})\\}\\delta_{i}\\Big\\}\\Big|}\\\\ &{\\leq\\!\\mathbb{E}\\Big\\{\\frac{1}{R_{t}}\\sum_{i\\leq t}^{\\infty}\\{|L_{i}-\\widehat{L}_{i}||G_{0}(\\mathbf{X}_{i})|+|\\widehat{L}_{i}-L_{i}||G_{1}(\\mathbf{X}_{i})|\\}\\delta_{i}\\Big\\}}\\\\ &{\\leq\\!\\mathbb{E}\\Big\\{\\frac{1}{R_{t}}\\sum_{i\\leq t}^{\\infty}\\!\\!\\!\\!\\{\\operatorname*{sup}\\,|L_{i}-\\widehat{L}_{i}||G_{0}(\\mathbf{X}_{i})|+\\operatorname*{sup}\\,|\\widehat{L}_{i}-L_{i}||G_{1}(\\mathbf{X}_{i})|\\}\\delta_{i}\\Big\\}}\\\\ &{\\leq\\!2c_{G}D_{2}n_{1}^{\\frac{-\\beta}{2\\beta+1}}\\sqrt{\\log n_{1}}+2n_{1}^{-1}+n_{1}^{-\\frac{1}{3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The last inequality follows from Lemma E.3 and $\\operatorname*{max}_{w}|\\widehat{L}(w)-L(w)|\\leq1$ . Notice that $n_{1}^{-\\frac{1}{3}}\\leq$ $n_{1}^{\\frac{-\\beta}{2\\beta+1}}$ by $\\beta\\leq1$ . We have ", "page_idx": 27}, {"type": "equation", "text": "$$\nC_{1}(\\delta^{t})\\leq\\alpha+D n_{1}^{\\frac{-\\beta}{2\\beta+1}}\\sqrt{\\log n_{1}},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $D=2c_{G}D_{2}+3$ . ", "page_idx": 27}, {"type": "text", "text": "E.4 Proof of Theorem 3.4 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Proof. Notice that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\Big\\{\\Big|\\displaystyle\\sum_{1\\leq i<j\\leq t}g(\\mathbf{X}_{i},\\mathbf{X}_{j})\\delta_{i}\\delta_{j}(1-\\widehat{L}_{i})(1-\\widehat{L}_{j})-\\displaystyle\\sum_{1\\leq i<j\\leq t}g(\\mathbf{X}_{i},\\mathbf{X}_{j})\\delta_{i}\\delta_{j}(1-L_{i})(1-L_{j})\\big|\\Big\\}}\\\\ &{\\leq\\!\\mathbb{E}\\Big\\{\\displaystyle\\sum_{1\\leq i<j\\leq t}g(\\mathbf{X}_{i},\\mathbf{X}_{j})\\delta_{i}\\delta_{j}(|\\widehat{L}_{i}-L_{i}|+|\\widehat{L}_{j}-L_{j}|+\\widehat{L}_{i}|\\widehat{L}_{j}-L_{j}|+L_{j}|\\widehat{L}_{i}-L_{i}|)\\Big\\}}\\\\ &{\\overset{(i)}{\\leq}\\!2c_{g}\\mathbb{E}\\Big\\{\\displaystyle\\sum_{1\\leq i<j\\leq t}\\delta_{i}\\delta_{j}(|\\widehat{L}_{i}-L_{i}|+|\\widehat{L}_{j}-L_{j}|)\\Big\\}}\\\\ &{\\overset{(i i)}{\\leq}\\!2c_{g}D n_{1}^{\\frac{-\\beta}{2\\beta+1}}\\sqrt{\\log n_{1}}\\mathbb{E}\\big(\\displaystyle\\sum_{1\\leq i<j\\leq t}\\delta_{i}\\delta_{j}\\big).}&{(1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The $(i)$ holds by Assumption 3.2 and $L(w),\\widehat{L}(w)\\,\\leq\\,1$ even when $t$ is random, as the uniform convergence of $\\widehat{L}(w)$ . And $(i i)$ follows from Lemma E.3. By the similar arguments we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\mathbb{E}\\Big\\{\\displaystyle\\sum_{1\\leq i<j\\leq t}\\delta_{i}\\delta_{j}\\big(1-\\widehat{L}_{i}\\big)(1-\\widehat{L}_{j})-\\displaystyle\\sum_{1\\leq i<j\\leq t}\\delta_{i}\\delta_{j}\\big(1-L_{i}\\big)(1-L_{j})\\Big\\}\\right|}\\\\ &{\\leq2D n_{1}^{\\frac{-\\beta}{2\\beta+1}}\\sqrt{\\log n_{1}}\\mathbb{E}(\\displaystyle\\sum_{1\\leq i<j\\leq t}\\delta_{i}\\delta_{j}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Denote $\\Delta_{n_{1}}=D n_{1}^{\\frac{-\\beta}{2\\beta+1}}\\sqrt{\\log n_{1}}$ . Combining (11) and (12), it follows that for every time $t\\geq T_{s}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\{\\underset{y=0}{\\overset{\\mathbb{E}}{\\sum}}\\underset{x=0}{\\overset{\\mathbb{E}}{\\sum}}\\underset{y=1}{\\overset{\\mathbb{E}}{\\sum}}\\underset{y=1}{\\overset{\\mathbb{E}}{\\sum}}(y\\!+\\!\\frac{L_{y}}{2})b_{y}f_{y}(1\\!-\\!L_{z})(1\\!-\\!L_{z})\\right\\}}\\\\ &{\\overset{(a)}{\\leq}\\frac{\\underset{y=1}{\\overset{\\mathbb{E}}{\\sum}}(y\\!+\\!\\frac{L_{y}}{2})b_{y}f_{y}(1\\!-\\!\\frac{L_{y}}{2})\\!+\\!\\beta_{z}\\!+\\!\\frac{1}{2}\\!(1\\!-\\!L_{z})(1\\!-\\!L_{z})}{\\underset{y=1}{\\overset{\\mathbb{E}}{\\sum}}\\underset{x=1}{\\overset{\\mathbb{E}}{\\sum}}\\underset{y=1}{\\overset{\\mathbb{E}}{\\sum}}(1\\!-\\!L_{z})(1\\!-\\!L_{z})}\\\\ &{\\overset{(b)}{\\leq}\\left[1\\!+\\!2\\!+\\!\\frac{1}{2}\\!(1\\!-\\!\\frac{L_{y}}{2})\\!+\\!\\frac{1}{6}\\!(1\\!-\\!\\frac{L_{z}}{2})\\!+\\!\\frac{1}{6}\\!(1\\!-\\!L_{z})(1\\!-\\!L_{z})\\right]}\\\\ &{\\overset{(c)}{\\leq}\\left[1\\!+\\!2\\!+\\!\\frac{1}{8}\\!\\frac{L_{y}}{\\underset{y=1}{\\overset{\\mathbb{E}}{\\sum}}(1\\!-\\!\\frac{L_{y}}{2})\\!+\\!\\beta_{z}\\!+\\!(1\\!-\\!L_{z})}\\right]\\times\\frac{\\underset{y=1}{\\overset{\\mathbb{E}}{\\sum}}\\left(1\\!-\\!\\frac{L_{y}}{2}\\right)\\!+\\!\\beta_{z}\\!-\\!\\frac{L_{z}}{2}\\!(1\\!-\\!L_{z})(1\\!-\\!L_{z})}{\\underset{y=1}{\\overset{\\mathbb{E}}{\\sum}}\\underset{x=1}{\\overset{\\mathbb{E}}{\\sum}}(1\\!-\\!\\frac{L_{y}}{2})\\!+\\!(1\\!-\\!L_{z})(1\\!-\\!L_{z})}}\\\\ &{\\overset{(c)}{\\leq}\\left\\{K^{+}\\!\\frac{ \n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The $(i)$ follows from (11) , and $(i i)$ comes from the operation of our algorithm, where ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big\\{\\sum_{1\\leq i<j\\leq t}g(\\mathbf{X}_{i},\\mathbf{X}_{j})\\delta_{i}\\delta_{j}(1-\\widehat{L}_{i})(1-\\widehat{L}_{j})\\Big\\}\\leq K\\cdot\\mathbb{E}\\Big\\{\\sum_{1\\leq i<j\\leq t}\\delta_{i}\\delta_{j}(1-\\widehat{L}_{i})(1-\\widehat{L}_{j})\\Big\\}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The $(i i i)$ and $(v)$ are directly from Lemma E.4. The $(i v)$ holds due to (12). Thus we have ", "page_idx": 28}, {"type": "equation", "text": "$$\nC_{2}(\\delta^{t})\\leq K+\\frac{(K+c_{g})\\Delta_{n_{1}}}{0.5-\\frac{m\\alpha^{\\prime}}{m-1}-\\Delta_{n_{1}}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "E.5 Proof of Corollary 3.5 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Proof. Define the $C_{1}$ constraint conditional on $\\mathcal{W}$ as ", "page_idx": 28}, {"type": "equation", "text": "$$\nC_{1}^{\\prime}(\\delta^{t})=\\frac{1}{R_{t}}\\sum_{i\\leq t}\\{(L_{i}-\\widehat{L}_{i})G_{0}(\\mathbf{X}_{i})+(\\widehat{L}_{i}-L_{i})G_{1}(\\mathbf{X}_{i})\\}\\delta_{i}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By the proofs of Theorem 3.3, we have for any $t$ ", "page_idx": 28}, {"type": "equation", "text": "$$\nC_{1}^{\\prime}(\\delta^{t})\\leq\\alpha+2c_{G}\\operatorname*{sup}_{w\\in\\mathcal{R}}|\\widehat{L}(w)-L(w)|.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Hence at stopping time $T_{m}$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\nC_{1}^{\\prime}(\\delta^{T_{m}})=\\mathbb{E}[C_{1}^{\\prime}(\\delta^{T_{m}})]\\leq\\mathbb{E}[\\operatorname*{sup}_{t}C_{1}^{\\prime}(\\delta^{t})]\\leq\\alpha+2c_{G}\\mathbb{E}[\\operatorname*{sup}_{w\\in\\mathcal{R}}|\\widehat{L}(w)-L(w)|]\\leq\\alpha+\\Delta_{n_{1}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By the proof of Theorem 3.4, we can also drop off the expectation and replace $\\Delta_{n_{1}}$ with $\\operatorname*{sup}_{w\\in\\mathcal{R}}|\\widehat{L}(w)-L(w)|$ . That is ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\{1-\\frac{2\\operatorname*{sup}_{w\\in\\mathcal{R}}|\\widehat{L}(w)-L(w)|}{1-\\frac{2m\\alpha}{m-1}}\\right\\}\\underset{1\\leq i<j\\leq t}{\\sum\\sum\\sum_{1\\leq j\\leq t}g(\\mathbf{X}_{i},\\mathbf{X}_{j})}\\delta_{i}\\delta_{j}(1-L_{i})(1-L_{j})}\\\\ &{\\leq\\biggr\\{K+\\frac{2c_{g}\\operatorname*{sup}_{w\\in\\mathcal{R}}|\\widehat{L}(w)-L(w)|}{1-\\frac{2m\\alpha}{m-1}}\\biggr\\}\\underset{1\\leq i<j\\leq t}{\\sum\\sum_{1\\leq j\\leq t}\\delta_{i}\\delta_{j}(1-L_{i})(1-L_{j})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Hence at stopping time $t=T_{m}$ , (13) still holds. Take expectations at both sides and we will get ", "page_idx": 29}, {"type": "equation", "text": "$$\nC_{2}(\\delta^{T_{m}})\\leq K+\\frac{(K+c_{g})\\Delta_{n_{1}}}{0.5-\\frac{m\\alpha^{\\prime}}{m-1}-\\Delta_{n_{1}}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Notice that $\\Delta_{n_{1}}\\,=\\,{\\cal D}n_{1}^{\\frac{-\\beta}{2\\beta+1}}\\sqrt{\\log n_{1}}$ converges to 0 as $n_{1}\\rightarrow\\infty$ and $\\alpha^{\\prime}\\,<\\,(1\\,-\\,1/m)/2$ by the condition. It follows that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n_{1}\\to\\infty}C_{1}(\\delta^{T_{m}})\\leq\\alpha\\quad\\mathrm{and}\\quad\\operatorname*{lim}_{n_{1}\\to\\infty}C_{2}(\\delta^{T_{m}})\\leq K.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "E.6 Proof of Theorem C.1 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Proof. We first denote some notations. For notational simplicity, we use $c$ to denote constants in the following proofs. ", "page_idx": 29}, {"type": "text", "text": "Recall the conformal score function $Q(\\mathbf{X}_{t})$ . The conformal $p$ -value is ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\widehat{p}_{t}=\\frac{1+\\sum_{i\\in\\mathcal{L}}\\mathbb{I}\\{Q(\\tilde{W}_{i})\\leq Q(W_{t})\\}}{1+|\\mathcal{L}|}=\\widehat{F}_{n_{2}}(Z_{t}),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $n_{2}=|{\\mathcal{L}}|$ . ", "page_idx": 29}, {"type": "text", "text": "Denote the population $p$ -value as $p_{t}=F(Q(W_{t}))$ , where $F(\\cdot)$ is the distribution of $Q(W_{t})$ conditional on $\\theta_{t}=0$ . Denote $\\pi_{t}=\\mathrm{Pr}(\\theta_{t}=1)$ ) and ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\pi_{t}^{\\lambda}=1-{\\frac{\\operatorname*{Pr}(p_{t}>\\lambda)}{1-\\lambda}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The corresponding local FDR is defined as ", "page_idx": 29}, {"type": "equation", "text": "$$\nL_{t}^{\\lambda}=\\frac{\\pi_{t}^{\\lambda}f_{0}(W_{t})}{f^{t}(W_{t})}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "It follows the definition of $C_{1}^{\\lambda}(\\pmb\\delta^{t})$ and $C_{2}^{\\lambda}(\\pmb{\\delta}^{t})$ . ", "page_idx": 29}, {"type": "equation", "text": "$$\nC_{1}^{\\lambda}(\\delta^{t})=\\mathbb{E}\\Big\\{\\frac{1}{R_{t}}\\sum_{i\\leq t}\\{L_{i}^{\\lambda}G_{0}(\\mathbf{X}_{i})+(1-L_{i}^{\\lambda})G_{1}(\\mathbf{X}_{i})\\}\\delta_{i}\\Big\\}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and ", "page_idx": 29}, {"type": "equation", "text": "$$\nC_{2}^{\\lambda}(\\delta^{t})=\\frac{\\mathbb{E}\\{\\sum_{1\\leq i<j\\leq t}g(\\mathbf{X}_{i},\\mathbf{X}_{j})\\delta_{i}\\delta_{j}(1-L_{i}^{\\lambda})(1-L_{j}^{\\lambda})\\}}{\\mathbb{E}\\{\\sum_{1\\leq i<j\\leq t}g(\\mathbf{X}_{i},\\mathbf{X}_{j})\\delta_{i}\\delta_{j}\\}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The estimated proportion is defined as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\widehat{\\pi}_{t}^{\\lambda}=1-\\frac{\\sum_{j=t-q}^{t-1}\\kappa_{b}(j-t)\\mathbb{I}\\{\\widehat{p}_{j}>\\lambda\\}}{(1-\\lambda)\\sum_{j=t-q}^{t-1}\\kappa_{b}(j-t)}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "And denote the ideal estimated proportion via population p-values as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\tilde{\\pi}_{t}^{\\lambda}=1-\\frac{\\sum_{j=t-q}^{t-1}\\kappa_{b}(j-t)\\mathbb{I}\\{p_{j}>\\lambda\\}}{(1-\\lambda)\\sum_{j=t-q}^{t-1}\\kappa_{b}(j-t)},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\kappa_{b}(s)=\\exp\\{-|s|/b\\}$ and $q$ is the size of a neighborhood for estimation before time $t$ . ", "page_idx": 30}, {"type": "text", "text": "We first claim the following proposition and the proof is deferred in Section E.7. ", "page_idx": 30}, {"type": "text", "text": "Proposition E.7. Suppose the assumptions in Theorem 3 hold. Then ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\vert\\widehat{\\pi}^{\\lambda}-\\pi^{\\lambda}\\vert\\leq c\\operatorname*{max}\\left\\{\\frac{1}{b},\\frac{\\sqrt{\\log n_{1}}}{n_{1}^{1/2}},(b\\eta)^{2}\\right\\}^{1/2-\\gamma}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "with probability $1-\\operatorname*{max}\\left\\{1/b,\\sqrt{\\log n_{1}/n_{1}},(b\\eta)^{2}\\right\\}^{2\\gamma}$ , where $c>0$ is a constant and $0<\\gamma<1/2$ . Notice that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{w}{\\operatorname*{sup}}\\,|\\widehat{f}^{t}(w)-f^{t}(w)|}\\\\ &{=\\underset{w}{\\operatorname*{sup}}\\,|\\widehat{f}_{0}(w)(1-\\widehat{\\pi}_{t})+\\widehat{f}_{1}(w)\\widehat{\\pi}_{t}-f_{0}(w)(1-\\pi_{t})-f_{1}(w)\\pi_{t}|}\\\\ &{\\underset{w}{\\leq}\\underset{w}{\\operatorname*{sup}}\\,|\\widehat{f}_{0}(w)-f_{0}(w)|+\\underset{w}{\\operatorname*{sup}}\\,|\\widehat{f}_{1}(w)-f_{1}(w)|+2M|\\widehat{\\pi}_{t}-\\pi_{t}|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "The last inequality holds since $\\widehat{\\pi}_{t}\\in[0,1]$ and $f_{0}(w)$ and $f_{1}(w)$ are upper bounded by $M$ . ", "page_idx": 30}, {"type": "text", "text": "Thus by Lemma E.1 and take $\\gamma$ in Proposition E.7 at $1/6$ , with probability 1 \u2212 $c\\operatorname*{max}\\left\\{1/b,\\sqrt{\\log n_{1}/n_{1}},(b\\eta)^{2}\\right\\}^{1/3}$ we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{w}|\\widehat{f}^{t}(w)-f^{t}(w)|\\leq c n_{1}^{\\frac{-\\beta}{2\\beta+1}}\\sqrt{\\log n_{1}}+c\\operatorname*{max}\\Big\\{b^{-\\frac{1}{3}},\\Big(\\frac{\\log n_{1}}{n_{1}}\\Big)^{\\frac{1}{6}},(b\\eta)^{\\frac{2}{3}}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By the procedure of Lemma E.3 directly, with probability 1 \u2212c max 1/b, log n1/n1, (b\u03b7)2 1/3 we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{w}|\\widehat{L}_{t}^{\\lambda}(w)-L_{t}^{\\lambda}(w)|\\leq c n_{1}^{\\frac{-\\beta}{2\\beta+1}}\\sqrt{\\log n_{1}}+c\\operatorname*{max}\\left\\{b^{-\\frac{1}{3}},\\left(\\frac{\\log n_{1}}{n_{1}}\\right)^{\\frac{1}{6}},(b\\eta)^{\\frac{2}{3}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "It follows that ", "page_idx": 30}, {"type": "equation", "text": "$$\nC_{1}^{\\lambda}(\\delta^{t})\\leq\\alpha+c n_{1}^{\\frac{-\\beta}{2\\beta+1}}\\sqrt{\\log n_{1}}+2c c_{G}\\operatorname*{max}\\Big\\{b^{-\\frac{1}{3}},\\Big(\\frac{\\log n_{1}}{n_{1}}\\Big)^{\\frac{1}{6}},(b\\eta)^{\\frac{2}{3}}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Denote $\\begin{array}{r}{\\Delta_{n_{1}}^{\\prime}=2c c_{G}n_{1}^{\\frac{-\\beta}{2\\beta+1}}\\sqrt{\\log n_{1}}+c\\operatorname*{max}\\left\\{b^{-\\frac{1}{3}},\\left(\\frac{\\log n_{1}}{n_{1}}\\right)^{\\frac{1}{6}},(b\\eta)^{\\frac{2}{3}}\\right\\}}\\end{array}$ . By our assumption, $b\\eta=$ $o(1)$ . So ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{b,n_{1}\\to\\infty}\\Delta_{n_{1}}^{\\prime}=0.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By the additional assumption that $\\operatorname*{Pr}(p_{t}>\\lambda\\mid\\theta_{t}=1)=0$ , we have $\\pi_{t}^{\\lambda}=\\pi_{t}$ . Thus Hence the first part of Theorem 3 is completed. ", "page_idx": 30}, {"type": "text", "text": "At last, we can use the same procedure to prove that ", "page_idx": 30}, {"type": "equation", "text": "$$\nC_{2}^{\\lambda}(\\delta^{t})\\leq K+\\frac{(K+c_{g})\\Delta_{n_{1}}^{\\prime}}{0.5-\\frac{m\\alpha^{\\prime}}{m-1}-\\Delta_{n_{1}}^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Since $\\pi_{t}^{\\lambda}=\\pi_{t}$ , $C_{2}^{\\lambda}(\\delta^{t})=C_{2}(\\delta^{t})$ and the results directly follow. ", "page_idx": 30}, {"type": "text", "text": "E.7 Proof of Proposition E.7 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Proof. Notice that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(|\\widehat{\\pi}_{t}^{\\lambda}-\\pi_{t}^{\\lambda}|>\\varepsilon)\\le\\operatorname*{Pr}\\left(|\\widehat{\\pi}_{t}^{\\lambda}-\\tilde{\\pi}_{t}^{\\lambda}|>\\frac{\\varepsilon}{2}\\right)+\\operatorname*{Pr}\\left(|\\tilde{\\pi}_{t}^{\\lambda}-\\pi_{t}^{\\lambda}|>\\frac{\\varepsilon}{2}\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We discuss for the two parts respectively. For the first term of the above inequality, it suffices to show the upper bound of the second moment of $|\\widehat{\\pi}_{t}^{\\lambda}-\\widetilde{\\pi}_{t}^{\\lambda}|$ . ", "page_idx": 31}, {"type": "text", "text": "For convenience, denote $U_{j}=\\mathbb{I}(p_{j}>\\lambda)$ , and $V_{j}=\\mathbb{I}(\\widehat{p_{j}}>\\lambda)$ . It follows ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\{\\vert\\widehat{\\pi}_{t}^{\\lambda}-\\widehat{\\pi}_{t}^{\\lambda}\\vert^{2}\\}}\\\\ {:}&{\\frac{1}{\\{\\sum_{j=t-q}^{t-1}\\kappa_{b}(j-t)\\}^{2}(1-\\lambda)^{2}}\\mathbb{E}\\left[\\sum_{j=t-q}^{t-1}\\kappa_{b}(j-t)(U_{j}-V_{j})\\right]^{2}}\\\\ {:}&{\\frac{\\sum_{j=t-q}^{t-1}\\kappa_{b}^{2}(j-t)\\mathbb{E}\\{(U_{j}-V_{j})^{2}\\}^{2}}{\\{\\sum_{j=t-q}^{t-1}\\kappa_{b}^{2}(j-t)\\}^{2}(1-\\lambda)^{2}}+\\frac{\\sum_{i,j\\in{\\cal N}_{q}(t),q\\neq j}\\kappa_{b}(i-t)\\kappa_{b}(j-t)\\mathbb{E}\\{(U_{i}-V_{i})(U_{j}-V_{j})\\}}{\\{\\sum_{j=t-q}^{t-1}\\kappa_{b}(j-t)\\}^{2}(1-\\lambda)^{2}}}\\\\ {:}&{\\frac{1}{\\{\\sum_{j=t-q}^{t-1}\\kappa_{b}(j-t)\\}^{2}(1-\\lambda)^{2}}\\Big[\\underset{j=t-q}{\\overset{t-1}{\\sum}}\\kappa_{b}^{2}(j-t)\\Big\\{\\operatorname*{Pr}(p_{j}>\\lambda)+\\operatorname*{Pr}(\\widehat{p}_{j}>\\lambda)-2\\operatorname*{Pr}(p_{j}>\\lambda,\\widehat{p}_{j}\\;.}\\\\ {:}&{+2\\underset{t-q\\leq i<j<i-1}{\\sum}\\kappa_{b}(i-t)\\kappa_{b}(j-t)\\Big\\}\\Big\\{\\operatorname*{Pr}(p_{i}>\\lambda,p_{j}>\\lambda)-2\\operatorname*{Pr}(\\widehat{p}_{i}>\\lambda,p_{j}>\\lambda)+\\operatorname*{Pr}(\\widehat{p}_{i}>\\lambda,\\widehat{p}_{i}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Now we check the upper bound of $|p_{j}\\rrangle-\\widehat{p_{j}}|$ . We can rewrite them as $p_{j}~=~F(Q(W_{j}))$ and $\\widehat{p}_{j}=\\widehat{F}_{n_{2}}(Q(W_{j}))$ . Even though $n_{2}$ is a random variable, due to the two-group model, $\\widehat{F}_{n_{2}}(\\cdot)$ is still an empirical distribution function composed by i.i.d. samples conditional on $\\{\\widetilde{\\theta}_{i}\\}_{i\\in{\\mathcal{C}}}$ where $\\mathcal{C}$ is the index set of calibration set. Thus by DKW inequality, for any $\\varepsilon_{1}>0$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Pr}\\left(|p_{j}-\\widehat{p}_{j}|\\leq\\varepsilon_{1}\\right)=\\mathbb{E}\\Big[\\mathrm{Pr}\\left(|p_{j}-\\widehat{p}_{j}|\\leq\\varepsilon_{1}\\ |\\ \\{\\widetilde{\\theta}_{i}\\}_{i\\in\\mathcal{C}}\\right)\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathbb{E}\\Big[\\mathrm{Pr}\\left(\\underset{z}{\\operatorname*{sup}}\\left|F(z)-\\widehat{F}_{n_{2}}(z)\\right|\\leq\\varepsilon_{1}\\ |\\ \\{\\widetilde{\\theta}_{i}\\}_{i\\in\\mathcal{C}}\\right)\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathbb{E}\\Big[1-2\\exp\\{-n_{2}\\varepsilon_{1}^{2}\\}\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\leq1-2\\exp\\{-(1-\\pi)n_{1}\\varepsilon_{1}^{2}\\}-4\\pi/\\{n_{1}(1-\\pi)\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "The last inequality holds since $n_{2}\\geq(1-\\pi)n_{1}/2$ with probability $1-4\\pi/\\{n_{1}(1-\\pi)\\}$ ", "page_idx": 31}, {"type": "text", "text": "Thus by Lemma E.5 and the fact that $p_{j}$ has a bounded density function, we obtain ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathrm{Pr}(p_{j}>\\lambda)-\\mathrm{Pr}(\\widehat{p}_{j}>\\lambda)|\\leq2\\varepsilon_{1}+6\\exp\\{-(1-\\pi)n_{1}\\varepsilon_{1}^{2}\\}+12\\pi/\\{n_{1}(1-\\pi)\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Take $\\varepsilon_{1}=\\sqrt{\\log n_{1}/\\{n_{1}(1-\\pi)\\}}$ , for sufficient large $n_{1}$ such that $\\sqrt{n_{1}\\log n_{1}}\\geq6+6\\pi$ we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n|\\mathrm{Pr}(p_{j}>\\lambda)-\\mathrm{Pr}(\\widehat{p}_{j}>\\lambda)|\\leq\\frac{2\\sqrt{\\log n_{1}}}{(1-\\pi)n^{1/2}}+\\frac{6+12\\pi/(1-\\pi)}{n_{1}}\\leq\\frac{3\\sqrt{\\log n_{1}}}{(1-\\pi)n^{1/2}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Again by Lemma E.5 and the same $\\varepsilon_{1}$ , for any $i,j\\in\\{t-q,\\ldots,t-1\\}:=\\mathcal{N}_{q}(t).$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(p_{j}>\\lambda,p_{i}>\\lambda)-\\operatorname*{Pr}(\\widehat{p}_{j}>\\lambda,\\widehat{p_{i}}>\\lambda)|\\leq4\\varepsilon_{1}+12\\exp\\{-(1-\\pi)n_{1}\\varepsilon_{1}^{2}\\}+\\frac{24\\pi}{n_{1}(1-\\pi)}\\leq\\frac{6\\sqrt{\\log n}}{(1-\\pi)n_{1}^{1-\\pi}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Therefore by Lemma E.6, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{E}(|\\widehat{\\pi}_{t}^{\\lambda}-\\tilde{\\pi}_{t}^{\\lambda}|)\\leq\\frac{9C_{b}\\sqrt{\\log n_{1}}}{b n_{1}^{1/2}(1-\\pi)(1-\\lambda)^{2}}+\\frac{12\\sqrt{\\log n_{1}}}{n_{1}^{1/2}(1-\\pi)(1-\\lambda)^{2}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "So by definition and Markov\u2019 inequality, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle\\operatorname*{Pr}\\left(|\\widehat{\\pi}_{t}^{\\lambda}-\\tilde{\\pi}_{t}^{\\lambda}|>\\frac{\\varepsilon}{2}\\right)}&{\\le}&{\\displaystyle\\frac{4\\mathbb{E}(|\\widehat{\\pi}_{t}^{\\lambda}-\\tilde{\\pi}_{t}^{\\lambda}|)}{\\varepsilon^{2}}}\\\\ &{\\le}&{\\displaystyle\\frac{36C_{b}\\sqrt{\\log n_{1}}}{b n_{1}^{1/2}(1-\\pi)(1-\\lambda)^{2}\\varepsilon^{2}}+\\frac{48\\sqrt{\\log n_{1}}}{n_{1}^{1/2}(1-\\pi)(1-\\lambda)^{2}\\varepsilon^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "For the second part, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\gamma_{\\mathbb{T}}\\left(|\\Vec{u}_{t}^{\\lambda}-\\pi_{t}^{\\lambda}|>\\frac{\\varepsilon}{2}\\right)}&{=\\phantom{\\frac{\\sum_{t}^{t}\\Big(}}\\left|\\frac{\\gamma_{\\mathbb{T}}(p_{t}>\\lambda)}{1-\\lambda}-\\frac{\\sum_{t=-\\pi}^{t-\\pi}e_{t}(\\lambda-t)(f-y_{t}>\\lambda)}{\\left(\\sum_{t=-\\pi}^{t-\\pi}e_{t}(\\lambda-t)\\right)^{2}(1-\\lambda)}\\right|\\succ\\frac{\\varepsilon}{2}\\right)}\\\\ &{\\leq\\phantom{\\frac{\\sum_{t}^{t-\\pi}-\\varepsilon}{\\left(\\sum_{t=-\\pi}^{t-\\pi}e_{t}(\\lambda-t)(f-y_{t}>\\lambda)-1\\right)}}\\frac{4\\varepsilon\\left[\\sum_{t=-\\pi}^{t-\\pi}e_{t}(\\lambda-t)(f-y_{t}>\\lambda)-1\\right]^{2}}{\\left(\\sum_{t=-\\pi}^{t-\\pi}e_{t}(\\lambda-t)\\right)^{2}\\varepsilon^{2}(1-\\lambda)^{2}}}\\\\ &{=\\phantom{\\frac{\\sum_{t=-\\pi}^{t-\\pi}\\varepsilon}{\\left(\\sum_{t=-\\pi}^{t-\\pi}e_{t}^{\\lambda}\\right)^{2}}}\\frac{\\left(\\sum_{t=-\\pi}^{\\tau}e_{t}(\\lambda[f(p_{t}>\\lambda)]+\\frac{\\varepsilon\\left(\\gamma_{\\mathbb{T}}(p_{t}>\\lambda)-\\Gamma_{\\mathbb{T}}(p_{t}>\\lambda)\\right)^{2}\\right)}{\\left(\\sum_{t=-\\pi}^{t-\\pi}e_{t}(\\lambda-t)\\right)^{2}\\varepsilon^{2}(1-\\lambda)^{2}}}\\\\ &{\\overset{(i)}{\\leq}\\phantom{\\frac{\\sum_{t}^{t-\\pi}(\\lambda-1)^{2}}{\\left(\\sum_{t=-\\pi}^{t-\\pi}\\lambda\\right)^{2}}}+\\frac{\\left(\\sum_{t=-\\pi}^{\\tau}e_{t}^{\\lambda}\\right)\\left(\\gamma_{\\mathbb{T}}(p_{t}>\\lambda)-1\\right)\\cdot\\varepsilon\\left(\\gamma_{\\mathbb{T}}(p_{t}>\\lambda)\\right)^{2}}{\\left(\\sum_{t=-\\pi}^{t-\\pi}e_{t}(\\lambda-t)\\right)^{2}\\varepsilon^{2}(1-\\lambda)^{2}}}\\\\ &{\\overset{(i i i)}{\\leq}\\phantom \n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The $(i)$ holds since $\\mathrm{\\mathop{Var}}\\{\\mathbb{I}(p_{j}>\\lambda)\\}\\le1/4$ . As for $(i i)$ , consider $A_{\\lambda}=\\{w:F(Q(w))>\\lambda\\}$ . Then by the two-group model we have $\\begin{array}{r}{\\operatorname*{Pr}(p_{t}\\stackrel{.}{>}\\lambda)=\\int_{\\mathcal{A}_{\\lambda}}\\{f_{0}(w)(1-\\pi_{t})+\\dot{f}_{1}(w)\\pi_{t}\\}d w}\\end{array}$ . Hence ", "page_idx": 32}, {"type": "equation", "text": "$$\n|\\operatorname*{Pr}(p_{j}>\\lambda)-\\operatorname*{Pr}(p_{t}>\\lambda)|=\\int_{A_{\\lambda}}\\Big\\{f_{0}(w)|\\pi_{j}-\\pi_{t}|+f_{1}(w)|\\pi_{t}-\\pi_{j}|\\Big\\}d w\\le2|j-t|\\eta\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "by our assumptions. And $(i i i)$ is directly from the second part of Lemma E.6. ", "page_idx": 32}, {"type": "text", "text": "Above all, we finally conclude that for all $\\varepsilon>0$ and sufficiently large $n_{1}$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname{v}_{\\tau}(|\\widehat{\\pi}_{t}^{\\lambda}-\\pi_{t}^{\\lambda}|>\\varepsilon)\\le\\frac{C_{b}}{b\\varepsilon^{2}(1-\\lambda)^{2}}+\\frac{36C_{b}\\sqrt{\\log n_{1}}}{b n_{1}^{1/2}(1-\\pi)(1-\\lambda)^{2}\\varepsilon^{2}}+\\frac{48\\sqrt{\\log n_{1}}}{n_{1}^{1/2}(1-\\pi)(1-\\lambda)^{2}\\varepsilon^{2}}+\\frac{16(C_{b}b\\log n_{1})}{\\varepsilon^{2}(1-\\pi)(1-\\lambda)^{2}\\varepsilon^{2}}}\\\\ &{\\qquad\\qquad\\qquad\\le\\operatorname*{max}\\Big\\{\\frac{1}{b},\\frac{\\sqrt{\\log n_{1}}}{n_{1}^{1/2}},(b\\eta)^{2}\\Big\\}\\frac{c}{\\varepsilon^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "for some constant $c>0$ . ", "page_idx": 32}, {"type": "text", "text": "Thus take $\\varepsilon=\\operatorname*{max}\\left\\{1/b,\\sqrt{\\log n_{1}}/n_{1}^{1/2},(b\\eta)^{2}\\right\\}^{1/2-\\gamma}\\sqrt{c}$ and we obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n|\\widehat{\\pi}^{\\lambda}-\\pi^{\\lambda}|\\le c\\operatorname*{max}\\left\\{\\frac{1}{b},\\frac{\\sqrt{\\log n_{1}}}{n_{1}^{1/2}},(b\\eta)^{2}\\right\\}^{1/2-\\gamma}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "with probability $1-\\operatorname*{max}\\left\\{1/b,\\sqrt{\\log n_{1}/n_{1}},(b\\eta)^{2}\\right\\}^{2\\gamma}$ , where $0<\\gamma<1/2$ . Hence we complete the proof. ", "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: See the end of Sect. 1, we summarize our main contributions. The experimental results in Sect. 4 and Appendix D validate the theoretical results in Sect. 3 and Appendix C. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: In Sect. 5, we discuss the limitations of our II-COS algorithm. In Sect. 3, we have explained why our assumptions are weak piece by piece. And see Subsect. 2.2 for the details about our computational complexity. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 33}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: For the theory assumptions, see Assumption 3.1-Assumption 3.2 in Sect. 3.   \nFor the proofs of each theoretical result, see Appendix E in the Supplementary material.   \n\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: See Appendix B for the implementation details to reproduce our experimental results. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We provide the data and code in the supplemental materials, including sufficient instructions in the zip file. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: In Sect.4 and Appendix B, we provide the experimental setting/details. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We calculate the standard error across 500 replications and show the error bars in experimental results, which is calculated by mean $+/-$ se. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: See Appendix B.4 for the compute resources and more details about the execution time. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: All the authors have reviewed the NeurIPS Code of Ethics guidelines and ensured that our paper conforms to them. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We have discussed the broader impacts of our work in Section 5. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 36}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: We have cited and provided the URL of the dataset we use in the paper, see Sect. 4 and the references. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 38}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 38}]