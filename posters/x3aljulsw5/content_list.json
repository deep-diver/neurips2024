[{"type": "text", "text": "Zeroth-Order Sampling Methods for Non-Log-Concave Distributions: Alleviating Metastability by Denoising Diffusion ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ye He Georgia Institute of Technology yhe367@gatech.edu ", "page_idx": 0}, {"type": "text", "text": "Kevin Rojas Georgia Institute of Technology kevin.rojas@gatech.edu ", "page_idx": 0}, {"type": "text", "text": "Molei Tao Georgia Institute of Technology mtao@gatech.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper considers the problem of sampling from non-logconcave distribution, based on queries of its unnormalized density. It first describes a framework, Denoising Diffusion Monte Carlo (DDMC), based on the simulation of a denoising diffusion process with its score function approximated by a generic Monte Carlo estimator. DDMC is an oracle-based meta-algorithm, where its oracle is the assumed access to samples that generate a Monte Carlo score estimator. Then we provide an implementation of this oracle, based on rejection sampling, and this turns DDMC into a true algorithm, termed Zeroth-Order Diffusion Monte Carlo (ZODMC). We provide convergence analyses by first constructing a general framework, i.e. a performance guarantee for DDMC, without assuming the target distribution to be log-concave or satisfying any isoperimetric inequality. Then we prove that ZOD-MC admits an inverse polynomial dependence on the desired sampling accuracy, albeit still suffering from the curse of dimensionality. Consequently, for low dimensional distributions, ZOD-MC is a very efficient sampler, with performance exceeding latest samplers, including also-denoising-diffusion-based RDMC and RSDMC. Last, we experimentally demonstrate the insensitivity of ZOD-MC to increasingly higher barriers between modes or discontinuity in nonconvexpotential. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The problem of drawing samples from a distribution based on unnormalized density $\\propto\\exp(-V)$ (described by the potential $V$ ) is a fundamental statistical and algorithmic problem. This classical problem nevertheless remains as a research frontier, providing pivotal tools to applications such as decision making, statistical inference / estimation, uncertainty quantification, data assimilation, and molecular dynamics. Worth mentioning is that machine learning could benefit vastly from progress in sampling as well, not only because of its connection to inference, optimization and approximation, but also through modern domains such as diffusion generative modeling & differential privacy. ", "page_idx": 0}, {"type": "text", "text": "Recent years have seen rapid developments of sampling algorithms with quantitative and nonasymptotic theoretical guarantees. Many of the results are either based on discretizations of diffusion processes [12, 13, 50, 16, 34, 33] or gradient flows [38, 10, 22]. In order to develop such guarantees, it is necessary to make assumptions about the target distributions, for instance, that it satisfies an isoperimetric property, where standard requirements are log-concavity or functional inequalities [12, 56, 21, 8, 48]. However, there is empirical evidence that the corresponding algorithms struggle to sample from targets that have high barriers between modes that create metastability. Overcoming such issues is highly nontrivial and researchers have continued to develop new methods to tackle theseproblems. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Diffusion models have lately shown remarkable ability in the generative modeling setting, with applications including image, video, audio, and macromolecule generations. This created a wave of theoretical work that showed the ability of diffusion models to sample from distributions under minimal assumptions [14, 57, 5, 32, 29, 4, 11, 3]. However, these works all started with the assumption that there is access to an approximation of the score function with some accuracy. This is a reasonable assumption for the task of generative modeling when one spends enough efforts on the training of the score, but the task of sampling is different. A natural question is: can we leverage the insensitivity of diffusion models to multimodality to efficiently sample from unnormalized, non-log-concave density? This would require approximating the score, which is then used as an inner loop inside an outer loop that integrates reverse diffusion process to transport, e.g., Gaussian initial condition, to nearly the target distribution. ", "page_idx": 1}, {"type": "text", "text": "The seminal works by [26, 27, 20] try to answer this question using Monte Carlo estimators of the score function and to provide theoretical guarantees. We also mention earlier work by [53] which learns parameterized scores and more work along the same line by [58, 44, 54, 55], whose theoretical guarantees are less clear but are based other interesting ideas. [26] proposed Reverse Diffusion Monte Carlo (RDMC), which estimates the score via LMC algorithm and relaxes the isoperimetric assumptions in the analysis of traditional sampling algorithms. [20] proposed a similar method, stochastic localization via iterative posterior sampling (SLIPS), which approximate the score via Metropolis-adjusted Langevin algorithm (MALA). However, both methods rely on the usage of a small time window where isoperimetric properties hold. This leaves the problem of finding a good initialization for the diffusion process. To alleviate this issue, [27] developed an acceleration of RDMC, the Recursive Score Diffusion-based Monte Carlo (RSDMC), which improves the nonasymptotic complexity to be quasi-polynomial in both dimension and inverse accuracy and gets rid of any isoperimetric assumption. Such work provides strong theoretical guarantees, however it requires a lot of computational power to get a high accuracy sampler. Additionally, RDMC, SLIPS and RSDMC are all based on first-order queries (i.e. gradients of $V$ ), which brings extra computational and memory costs, in addition to requiring a continuous differentiable $V$ .Motivated by these two observations, we create a sampler that only makes use of zeroth-order queries without assuming any isoperimetric conditions on the target distribution. Our contributions can be summarized as follows. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 We introduce an oracle-based meta-algorithm DDMC (Denoising Diffusion Monte Carlo) and provide a non-asymptotic guarantee in KL-divergence in Theorem 1. Our result provides utuieucai msigint un ue cliuice ui upuai step-size iIl DuiviU as wel as  uenuismg uilusiUnl models (Sec. 3.2). ", "page_idx": 1}, {"type": "text", "text": "\u00b7 We develop a novel algorithm ZOD-MC (Zeroth Order Diffusion-Monte Carlo) that uses zeroth-order queries and the global minimal value of the potential function to generate samples approximating the target distribution. In Corollary 3.1, we establish a zeroth-order query complexity upper bound for general target distributions satisfying mild smoothness and moment conditions. Our result is summarized and compared to other sampling algorithms in Table 5. \u00b7 The advantages of our algorithm are experimentally verified for non-log-concave target distributions. We demonstrate the insensitivity of our algorithm to various high barriers between modes, and the ability of correctly account for discontinuities in the potential. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1  Diffusion Model ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Diffusion model generates samples that are similar to training data, by requiring the generated data to follow the same latent distribution $p$ as the training data. To do so, it considers a forward noising ", "page_idx": 1}, {"type": "table", "img_path": "X3Aljulsw5/tmp/f98c879af1eb9a7ffdc0d07247facce3c3c24c5756327a029131b4ab486fd0ce.jpg", "table_caption": [], "table_footnote": ["Table 1: Comparison of ZOD-MC to LMC, RDMC, RSDMC and the Proximal Sampler: Summary of isoperimetric assumptions and oracle complexities to generate a $\\varepsilon$ -accurate sample under different criterion. O hides polylog $\\big(d\\varepsilon^{-1}\\big)$ factors. The zeroth-order oracle complexity of ZOD-MC is from Corollary 3.1 for achieving both $\\varepsilon\\ {\\mathrm{KL}}$ and $\\varepsilon\\,W_{2}$ errors. These theoretical results suggest that in the absense of isoperimetric assumptions, ZOD-MC excels in low-dimensions. "], "page_idx": 2}, {"type": "text", "text": "process that transforms a random variable into Gaussian noise. One most commonly used forward process is (a time reparameterization of) the Ornstein-Uhlenbeck (OU) process, given by the SDE: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{d}X_{t}=-X_{t}\\mathrm{d}t+\\sqrt{2}\\mathrm{d}B_{t},\\quad X_{0}\\sim p,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\{B_{t}\\}_{t\\ge0}$ is the standard Brownian motion in $\\mathbb{R}^{d}$ .The OU process that solves (1) is in distribution equivalent to a sum of two independent random vectors: $X_{t}=e^{-t}X_{0}+\\sqrt{1-e^{-2t}}Z$ where $(X_{0},Z\\dot{)}\\sim p\\otimes\\gamma^{d}$ and $\\gamma^{d}$ is the standard Gaussian distribution in $\\mathbb{R}^{d}$ . Denote $p_{t}=\\mathrm{Law}(X_{t})$ for all $t\\geq0$ . If we consider a large, fixed terminal time $T$ of (1), then $p_{T}$ is close to $\\gamma^{d}$ . Then, the denoising or backwards diffusion process, $\\{\\bar{X}_{t}\\}_{0\\le0\\le T}$ , can be constructed by reversing the OU process from time $T$ , meaning that $\\mathrm{Law}(\\bar{X}_{t})\\,:=\\,\\mathrm{Law}(\\bar{X}_{T-t})$ for all $t\\,\\in\\,[0,T]$ . By doing so we obtain the denoising diffusion process which solves the following SDE: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{d}\\bar{X}_{t}=(\\bar{X}_{t}+2\\nabla\\log p_{T-t}(\\bar{X}_{t}))\\mathrm{d}t+\\sqrt{2}\\mathrm{d}\\bar{B}_{t},\\quad\\bar{X}_{0}\\sim p_{T},\\,0\\leq t\\leq T,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\{\\bar{B}_{t}\\}_{0\\leq t\\leq T}$ is a Brownian motion in $\\mathbb{R}^{d}$ , independent of $\\{B_{t}\\}_{0\\le t\\le T}$ and $\\nabla\\ln{p_{t}}$ is usually referred as the score function for $p_{t}$ .Although the denoising process initializes at $p_{T}$ , we can't generate exact samples from $p_{T}$ . In practice, people consider the standard Gaussian initialization $\\bar{\\gamma}^{d}$ due to the fact that $p_{T}$ is close to $\\gamma^{d}$ when $T$ is large. The denosing process with the standard Gaussian initialization is given by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{d}\\tilde{X}_{t}=(\\tilde{X}_{t}+2\\nabla\\log p_{T-t}(\\tilde{X}_{t}))\\mathrm{d}t+\\sqrt{2}\\mathrm{d}\\bar{B}_{t},\\quad\\tilde{X}_{0}\\sim\\gamma^{d},\\;0\\leq t\\leq T.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "By simulating this denoising process (3), we can achieve the goal of generating new samples. However, the denoising process (3) can't be simulated directly due to the fact that the score function is not explicitly known. A widely applied method to solve this issue is to learn the score function through denoising score matching [51, 24, 52]. Given a learned score, denoted as $s(t,x)$ ,Onecan simulate the denoising diffusion process using discretizations like the Euler Maruyama or some exponential integrator. From a theoretical perspective, assuming the learned score satisfies ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{x\\sim p_{t}}\\left[\\left\\|s(t,x)-\\nabla\\ln p_{t}(x)\\right\\|^{2}\\right]\\leq\\epsilon_{\\mathrm{score}}^{2},\\quad\\forall\\,0\\leq t\\leq T,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "non-asymptotic convergence guarantees for diffusion models are obtained in [5, 3, 4, 11]. For instance, in [3], polynomial iteration complexities were proved without assuming any isoperimetric property of the data distribution and only assuming the data distribution has a finite second moment and a score estimator satisfying (4) is available. ", "page_idx": 2}, {"type": "text", "text": "In this work, we consider instead the sampling setting, in which no existing samples from the target distribution is available. Our sampling algorithm and theoretical analysis are motivated from the denoising diffusion process given by (2) and its corresponding discretization through the exponential integrator in Algorithm 1. In particular, we first introduce an oracle-based meta-algorithm, DDMC, which integrates Algorithm 1 and Algorithm 2, where the exponential integrator scheme of (2) is applied to generate samples and the score function is approximated by a Monte Carlo estimator assuming independent samples from a conditional distribution are available. ", "page_idx": 2}, {"type": "text", "text": "2.2  Rejection Sampling and Restricted Gaussian Oracle ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Rejection sampling is a popular Monte Carlo method for sampling a target distribution, $p$ basedon the zeroth-order queries of the potential $V$ . It requires that we have access to the potential function ", "page_idx": 2}, {"type": "text", "text": "Input : $N\\in\\mathbb{Z}_{+}$ $0=t_{0}<\\dots<t_{N}=T-\\delta$ score estimator $\\{s(T-t_{k},\\cdot)\\}_{k=0}^{N-1}$   \nOutput: $x_{N}$   \ngenerate a sample $x_{0}\\sim\\gamma^{d}$   \nfor $k=0,1,\\cdots\\,,N-1$ do generate $\\xi_{k}\\sim\\gamma^{d}$ such that $\\xi_{k}$ is independent to $\\xi_{0},\\cdot\\cdot\\cdot,\\xi_{k-1}$ $x_{k+1}\\leftarrow e^{t_{k+1}-t_{k}}x_{k}+2(e^{t_{k+1}-t_{k}}-1)s(T-t_{k},x_{k})+\\sqrt{e^{2(t_{k+1}-t_{k})}-1}\\xi_{k}.$   \nend ", "page_idx": 3}, {"type": "text", "text": "$V_{\\mu}$ of some other distribution $\\mu$ , such that $\\mu$ is easy to sample from and $\\exp(-V)\\,\\leq\\,\\exp(-V_{\\mu})$ globally. Such a distribution $\\mu$ is typically called an envelope for the distribution $p$ . With an envelope $\\nu$ , rejection sampling generates samples from $p$ by running the following algorithm till acceptance: ", "page_idx": 3}, {"type": "text", "text": "1. Sample $X\\sim\\mu$   \n2. Accept $X$ with probability $\\exp(-V(X)+V_{\\mu}(X))$ ", "page_idx": 3}, {"type": "text", "text": "The rejection sampling is considered as a high-accuracy algorithm as it outputs a unbiased sample from the target distribution. However, despite such a remarkable property, it has drawbacks. First, it is a nontrivial task to find an envelope for a general target distribution. Second, rejection sampling usually suffers from \u201ccurse of dimensionality\". Even for strongly logconcave target distributions, the complexity of the rejection sampling increases exponentially fast with the dimension: in expectation itrequires $\\kappa^{d/2}$ many rejections before one acceptance, where $\\kappa$ is the condition number for the potential $V$ see [9]. ", "page_idx": 3}, {"type": "text", "text": "The Restricted Gaussian Oracle (RGO), which was first introduced in [31], assumes that an accurate sample from distribution $\\begin{array}{r}{\\pi(\\cdot|y)\\propto\\exp\\big(-V(\\cdot)-\\frac{1}{2\\eta}\\left\\|\\cdot-y\\right\\|^{2}\\big)}\\end{array}$ can be generated for any $y\\in\\mathbb{R}^{d}$ $\\eta>0$ and any potential $V$ . Implementing the RGO is challenging. It is usually done by rejection sampling. However, most proposed methods [36, 18], are only suitable for small $\\eta$ ", "page_idx": 3}, {"type": "text", "text": "Our proposed sampling algorithm, ZOD-MC applies the rejection sampling (Algorithm 3) to implement the RGO with a large value of $\\eta$ . Details on ZOD-MC are introduced in Section 3.1. ", "page_idx": 3}, {"type": "text", "text": "3   Denoising Diffusion Monte Carlo Sampling ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we first introduce DDMC and ZOD-MC in Section 3.1. Then we provide a convergence guarantee for DDMC in Section 3.2. Last, in Section 3.3, we establish the zeroth-order query complexity of ZOD-MC. Note DDMC is a meta-algorithm that still requires an implementation of its oracle, and ZOD-MC is an actual algorithm that contains such an implementation. The theoretical guarantee of ZOD-MC (Sec. 3.3), therefore, is based on the analysis framework of DDMC (Sec. 3.2). ", "page_idx": 3}, {"type": "text", "text": "3.1Denoising Diffusion Monte Carlo and Zeroth-Order Diffusion Monte Carlo ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Denoising Diffusion Monte Carlo (DDMC). Let's start with a known but helpful lemma on score representation, derivable from Tweedie's formula [45]. ", "page_idx": 3}, {"type": "text", "text": "Lemma 1. Let $\\{X_{t}\\}_{t\\ge0}$ be the solution to the OU process (1) and $p_{t}\\,=\\,L a w(X_{t})$ .Then for all $t>0$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla\\ln p_{t}(x)=\\mathbb{E}_{x_{0}\\sim p_{0\\mid t}(\\cdot|x)}\\Big[\\frac{e^{-t}x_{0}-x}{1-e^{-2t}}\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This lemma was for example applied in [26] to do sampling based on the denoising diffusion process in (2). For the sake of completeness, we include its proof in Appendix C.5. ", "page_idx": 3}, {"type": "text", "text": "Due to (5), to approximate the score function $\\nabla\\ln p_{t}(x)$ , it suffices to generate samples that approximate $p_{0|t}(\\cdot|x)$ . [26, 27] proposed to use Langevin-based algorithms to sample from $p_{0|t}(\\cdot|x)$ The first step of our work is to generalize this, with refined and more general theoretical analysis later on,by considering an oracle algoritm, DDMC, whichaume nependent sa $\\{z_{t,i}\\}_{i=1}^{n(t)}$ that approximate $p_{0|t}(\\cdot|x)$ are available. The Monte Carlo score estimator in Algorithm 2 is given by ", "page_idx": 3}, {"type": "table", "img_path": "X3Aljulsw5/tmp/7c1dcb99873014c4c81673c08fb3a5b8df94827c35bc101c57efc53ed71a5f74.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\ns(t,x)=\\frac{1}{n(t)}\\sum_{i=1}^{n(t)}\\frac{e^{-t}z_{t,i}-x}{1-e^{-2t}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $n(t)$ is the number of samples and $\\delta(t)$ is such that $W_{2}(\\mathrm{Law}(z_{t,i}),p_{0|t}(\\cdot|x))\\leq\\delta(t)$ for all $i$ In Section 3.2, we will discuss how the performance of sampling depends on $n(t),\\delta(t)$ ", "page_idx": 4}, {"type": "text", "text": "Zeroth-Order Diffusion Monte Carlo (ZOD-MC). Noticing that in Lemma 1, the conditional distribution has a structured potential function: a summation of the target potential and a quadratic function. Therefore, implementing the oracle in DDMC is equivalent to implementing RGO with $y=e^{t}x$ and $\\eta=e^{2t}-1$ . Based on this, we propose ZOD-MC, a novel methodology based on rejection sampling and DDMC. Rejection samplng (Algorithm 3) can generate i.i.d. Monte Carlo samples required in Algorithm 2. Therefore, ZOD-MC, as a combination of rejection sampling and DDMC, can efficiently sample from non-logconcave distributions. See Appendix B for more details. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 3: Rejection Sampling: generating $\\{z_{t,i}\\}_{i=1}^{n(t)}$ in Algorithm 2 ", "page_idx": 4}, {"type": "text", "text": "Input : $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , zeroth-order queries of $V$   \nOutput : $z$   \nwhileTRUEdo Generate $(\\xi,u)\\sim\\gamma^{d}\\otimes U[0,1]$ $z\\leftarrow e^{t}x+\\sqrt{e^{2t}-1}\\xi$ return $z$ if $u\\le\\exp(-V(z)+V^{*})$   \nend ", "page_idx": 4}, {"type": "text", "text": "Remark 1. (Remark on the optimization step) In theory, we assume an oracle access to the minimum valueof $V$ .However, in practice we use Newton's method to find a local minimum. Throughout the sampling process we update the local minimum as we explore the search space. ", "page_idx": 4}, {"type": "text", "text": "Remark 2. (Parallelization) Notice that Algorithm 3 can be run in parallel to generate all the $n(t)$ samples required to compute the score. Contrary to methods like LMC that have a sequential nature, this allows our method to be more computationally efficient and reduce the running time. This is a feature that RDMC or RSDMC doesn't benefit as much from. ", "page_idx": 4}, {"type": "text", "text": "3.2 Convergence of DDMC ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our oracle-based meta-algorithm, DDMC, provides a framework for designing and analyzing sampling algorithms that integrate the denoising diffusion model and the Monte Carlo score estimation. In this section, we first present an error analysis to the Monte Carlo score estimation in Proposition 3.1, whose proof is in Appendix C.3. After that, we leverage our result in Proposition 3.1 and provide a non-asymptotic convergence result for DDMC in Theorem 1, whose proof is in Appendix C.4. ", "page_idx": 4}, {"type": "text", "text": "$\\{X_{t}\\}_{t\\ge0}$ $p_{t}=\\mathrm{Law}(X_{t})$ $t>0$ $\\begin{array}{r}{s(t,x)=\\frac{1}{n(t)}\\sum_{i=1}^{n(t)}\\frac{e^{-t}z_{t,i}-x}{1-e^{-2t}}}\\end{array}$ $\\{z_{t,i}\\}_{i=1}^{n(t)}$ vectors such that $W_{2}(L a w(z_{t,1}),p_{0|t}(\\cdot|x))\\leq\\delta(t)$ for all $t>0$ and $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ then we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\big[\\,\\|\\nabla\\ln p_{t}(X_{t})-s(t,X_{t})\\|^{2}\\,\\big]\\leq\\frac{e^{-2t}}{(1-e^{-2t})^{2}}\\delta(t)^{2}+\\frac{1}{n(t)}\\frac{e^{-2t}}{(1-e^{-2t})^{2}}\\mathrm{Cov}_{p}(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Choiceof $\\delta(t)$ and $n(t)$ . The error bound in (7) helps choose the accuracy threshold $\\delta(t)$ and the number of samples $n(t)$ to control the score estimation error over different time. In fact, when $t$ increases, it requires less samples and allows larger sample errors to get a good Monte Carlo score estimator. If we assume $\\bar{\\mathrm{Cov}}_{p}(x)\\,=\\,\\mathcal{O}(d)$ for simplicity, then when $t$ is small, the factor (1-2)z = O(t-2) and the choice of (t) = O(te) and n(t) = (dt-2e-2) il lead to the L-eror of order O(\u00b2). When t is large, the factor (22 $\\begin{array}{r}{\\frac{e^{-2t}}{(1-e^{-2t})^{2}}\\,=\\,\\mathcal{O}(e^{-2t})}\\end{array}$ and it only requires $\\delta(t)=O(e^{t}\\varepsilon)$ and $n(t)=\\Omega(d e^{-2t}\\varepsilon^{-2})$ to ensure the $L^{2}$ -error is of order ${\\mathcal{O}}(\\varepsilon^{2})$ . In the latter case, the $\\delta(t)$ is of a larger order and $n(t)$ is of a smaller order than the first case. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "We now analyze the convergence of DDMC. Recall that Algorithm 1 is an exponential integrator discretization scheme of (2) with the time schedule $0=t_{0}<t_{1}<\\cdot\\cdot\\cdot<t_{N}=T-\\delta$ for some $\\delta>0$ In each iteration, $x_{k+1}=e^{t_{k+1}-t_{k}}x_{k}+2(e^{t_{k+1}-t_{k}}-1)s(T-t_{k},x_{k})+{\\sqrt{e^{2(t_{k+1}-t_{k})}-1}}\\xi_{k}.$ where $\\xi_{k}\\sim\\gamma^{d}$ and $s(T-t_{k},\\cdot)$ is the Monte Carlo score estimator generated by Algorithm 2. The trajectory of Algorithm 1 can be piece-wisely characterized by the following SDEs: for all $t\\in[t_{k},t_{k+1})$ \uff0c ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\tilde{X}_{t}=(\\tilde{X}_{t}+2s(T-t_{k},\\tilde{X}_{t_{k}})\\mathrm{d}t+\\sqrt{2}\\mathrm{d}\\tilde{B}_{t},\\quad\\tilde{X}_{0}\\sim\\gamma^{d},\\;\\tilde{X}_{t_{k}}=x_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Therefore, the convergence of DDMC is equivalent to the convergence of the process $\\{\\tilde{X}_{t}\\}_{0\\le t\\le t_{N}}$ which could be quantified under mild assumptions on the target distribution. Next, we present the moment assumption on the target distribution and our non-asymptotic convergence theorem. ", "page_idx": 5}, {"type": "text", "text": "Assumption 3.1.The distribution $p$ has a finite second moment: $\\begin{array}{r}{\\mathbb{E}_{x\\sim p}[\\left\\|x\\right\\|^{2}]=\\mathrm{m}_{2}^{2}<\\infty.}\\end{array}$ Theorem 1. Assume that the target distribution satisfies Assumption 3.1. Let $\\{X_{t}\\}_{t\\ge0}$ be the solution of (1) with $p_{t}\\,:=\\,\\mathrm{Law}\\!\\left(X_{t}\\right)$ and $\\{\\tilde{X}_{t}\\}_{t\\ge0}$ be the solution of (8) with $q_{t}\\,:=\\,\\mathrm{Law}(\\tilde{X}_{t})$ .For any $\\delta\\,\\in\\,(0,1)$ and $T\\ >\\ 1$ ,let $0\\;=\\;t_{0}\\;<\\;t_{1}\\;<\\;\\cdot\\cdot\\;<\\;t_{N}\\;=\\;T\\,-\\,\\delta$ beatimeschedulesuchthat $\\gamma_{k}=\\Theta(\\gamma_{k-1})$ for all $k=0,1,\\cdots\\,,N-1,$ where $\\gamma_{k}:=t_{k+1}-t_{k}$ .Then ", "page_idx": 5}, {"type": "image", "img_path": "X3Aljulsw5/tmp/4b30826621932284d85fba0cefc8a8ece8356053426fcecc1cddca349ad2b28c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "where $\\delta(t),n(t)$ are parameters in Algorithm 2. ", "page_idx": 5}, {"type": "text", "text": "Remark 3. In Theorem 1, we characterize $K L(p_{\\delta}|q_{t_{N}})$ instead of $K L(p|q_{t_{N}})$ due to the fact that $K L(p|q_{t_{N}})$ is not well-defined when the target distribution $p$ is not smooth w.r.t. the Lebesgue measure. It turns out that $p_{\\delta}$ is an alternative distribution to look at because $p_{\\delta}$ is smooth for all $\\delta>0$ and $p_{\\delta}$ is close to $p$ when $\\delta$ is small(seeProposition $C.I$ ).This is a standard treatment, referred to as early stopping, in the score-based generative modeling literature [e.g., 5, 3]. ", "page_idx": 5}, {"type": "text", "text": "The terms I, II, III in (9) correspond to the three types of errors in Algorithm 1, the initialization error, the score estimation error and the discretization error, respectively. Such a decomposition is very common in analyses of diffusion models, see [5, 6, 3]. Since we consider the sampling setting, the score estimation error is derived from the error analysis to the Monte Carlo score estimator, i.e., Proposition 3.1. A detailed discussion on these three types of errors is provided in Appendix C.4. ", "page_idx": 5}, {"type": "text", "text": "Compared to existing analyses on diffusion models, Theorem 1 extends result in [3] from exponentialdecay step-size to general choices of step-size, and recovers their sharp linear dimension dependence in the discretization error, with minimal assumptions on the target distribution. By assuming two consecutive step sizes are of the same order, we perform asymptotic estimation on the accumulated discretization errors and obtain a bound that depends on the step-size. Such kind of result helps to understand the optimal time schedule, as we will discuss soon. ", "page_idx": 5}, {"type": "text", "text": "Discussion on the choices of time schedule. Under different choices of step-size, the discretization errors in the denoising diffusion model have the same linear dependence on $d$ ,but different dependence on $\\delta$ . The linear dimension dependence improves the results in [4], where $O(d^{2})$ discretization error bounds are proved for different choices of step-size. It is also a extension of the result in [3], where $O(d)$ discretization error is only proved for the exponential-decay step-size. In fact, Theorem 1 implies that the exponential-decay step-size induces an optimal discretization error up to some constant in term of the inverse early-stopping time $\\delta^{-1}$ . Detailed discussions on different choices of time schedules is provided in Appendix C.5. ", "page_idx": 5}, {"type": "text", "text": "3.3  Complexity of ZOD-MC ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "With the convergence result for DDMC in Theorem 1, we introduce the query complexity bound of ZOD-MC. Our analysis assumes a relaxation of the commonly used gradient-Lipschitz condition on the potential. The formal statement is presented in Corollary 3.1, whose proof is provided in AppendixC.6. ", "page_idx": 6}, {"type": "text", "text": "Assumption 3.2. There exists a constant $L\\,>\\,0$ such that for any $x^{*}\\,\\in\\,\\arg\\operatorname*{min}_{y\\in\\mathbb{R}^{d}}V(y)$ and $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ $V$ satisfies $\\begin{array}{r}{V(x)-V(x^{*})\\leq\\frac{L}{2}\\left\\|x-x^{*}\\right\\|^{2}}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "$^{\\,l}$ $\\begin{array}{r}{T=\\frac{1}{2}\\ln(\\frac{d+\\mathrm{m}_{2}^{2}}{\\varepsilon_{K L}})}\\end{array}$ $\\gamma_{k}\\,=\\,\\kappa\\,\\mathrm{min}(1,T-t_{k})$ \uff0c $\\begin{array}{r}{\\delta\\,=\\,\\operatorname*{min}(\\frac{\\varepsilon_{W_{2}}^{2}}{d},\\frac{\\varepsilon_{W_{2}}}{\\ m_{2}}),}\\end{array}$ $\\begin{array}{r}{\\kappa\\,=\\,\\Theta\\Big(\\frac{T+\\ln(\\delta^{-1})}{N}\\Big)}\\end{array}$ thn to obtainan uput wih distribution $q_{t_{N}}$ ) in ZOD-MC such that $W_{2}(p,p_{\\delta})\\lesssim\\varepsilon_{W_{2}}$ and $K L(p_{\\delta},q_{t_{N}})\\lesssim\\varepsilon_{K L},$ the zeroth-order query complexityis of order ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\mathcal{O}}\\big(\\operatorname*{max}\\big(\\frac{d+\\mathfrak{m}_{2}^{2}}{\\varepsilon_{K L}},\\frac{d^{2}}{\\varepsilon_{K L}^{2}}\\big)\\varepsilon_{K L}^{-\\frac{d-2}{2}}(d+\\mathfrak{m}_{2}^{2})^{\\frac{d-2}{2}}L^{\\frac{d}{2}}d^{-1}\\big)\\underset{0\\leq k\\leq N-1}{\\operatorname*{max}}\\exp\\big(L\\left\\|x^{*}\\right\\|^{2}+\\left\\|x_{k}\\right\\|^{2}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the $\\tilde{\\mathcal{O}}$ hides polylog $\\biggl(\\frac{d{+}{\\bf m}_{2}^{2}}{\\varepsilon_{W_{2}}}\\biggr)$ factors. ", "page_idx": 6}, {"type": "text", "text": "Remark 4. If we assume WLOG that the minimizer of the potential is at the origin, i.e., $x^{*}=0$ ,and furthermakereasonableassumptions that $\\mathrm{m}_{2}^{2}$ \uff0c $L$ and $\\{\\|x_{k}\\|^{2}\\}$ are all of order $O(d)$ where $\\{x_{k}\\}$ are the iterates in Algorithm 1, then the query complexity of ZOD-MC is of order exp $\\big(\\tilde{\\mathcal{O}}(d)\\log(\\varepsilon_{K L}^{-1})\\big)$ Even though this complexity bound has an exponential dimension dependence, it only depends polynomially on the inverse accuracy. Since it applies to any target distribution satisfying Assumptions 3.1 and 3.2, this complexity bound suggests that with the same overall complexity, ZOD-MC can generate samples more accurate than other algorithms in Table 5, fora large class of low-dimensional non-logconcavetargetdistributions. ", "page_idx": 6}, {"type": "text", "text": "Comparison to LMC, RDMC and RSDMC. When no isoperimetric condition is assumed, we compare convergence for ZOD-MC to convergence for LMC, RDMC and RSDMC. ", "page_idx": 6}, {"type": "text", "text": "In the absence of the isoperimetric condition, [1] demonstrated that LMC is capable of producing samples that are close to the target in FI assuming the target potential is smooth. However, FI is a weaker divergence than KL divergence/ Wasserstein-2 distance. It has been observed that, in certain instances, the KL divergence/Wasserstein-2 distance may still be significantly different from zero, despite a minimal FI value. This observation implies that the convergence criteria based on FI may not be as stringent as our result which is based on KL divergence/Wasserstein-2 distance. [26] proved that RDMC produces samples that are $\\varepsilon$ -close to the target in KL divergence with high probability. Assuming the potential is smooth and a tail-growth condition, the first order oracle complexity is shown to be of order $\\exp(\\varepsilon^{-1}\\log d)$ . [27] introduced RSDMC as an acceleration of RDMC. They were able to show that if the potential is smooth, RSDMC produces a sample that is $\\varepsilon$ -close to the target in KL divergence with high probability. The first order oracle complexity is shown to be of order $\\exp(\\log^{3}(d/\\varepsilon))$ . Compared to RDMC and RSDMC, our result on ZOD-MC doesn't require the potential to be smooth as our Assumption 3.2 is only a growth condition of the potential. This indicates that our convergence result applies to targets with non-smooth, or even discontinuous potentials. Our result in Corollary 3.1 shows the zeroth-order oracle complexity for ZOD-MC is of order $\\exp(d\\log(\\varepsilon^{-1}))$ , which achieves a better $\\varepsilon$ -dependence compared to RDMC and RSDMC, at the price of a worse dimension dependence. This suggests that, for any low-dimensional target, ZOD-MC produces a more accurate sample than RDMC/RSDMC when the overall oracle complexity are the same. Last, zeroth-order queries cost less computationally than first-order queries in practice, which also makes ZOD-MC a more suitable sampling algorithm when the gradients of the potential are hard to compute. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We will demonstrate ZOD-MC on three examples, namely Gaussian mixtures, Gaussian mixtures plus discontinuities, and Muller-Brown which is a highly-nonlinear, nonconvex test problem popular in computational chemistry and material sciences. Multiple Gaussian mixtures will be considered, for showcasing the robustness of our method under worsening isoperimetric properties. The baselines we consider include RDMC [26], RSDMC [27], SLIPS [20], the proximal sampler [37], annealed importance sample [42], sequential Monte Carlo [15], a parallel tempering approach with MALA proposals [30] and naive unadjusted Langevin Monte Carlo. All the experiments are conducted using a NVIDIA GeForce RTX 4070 Laptop GPU with 8GB of VRAM and Pytorch. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.1 Results for Gaussian Mixtures ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Matched Oracle Complexity. We modify a 2D Gaussian mixture example frequently considered in the literature to make it more challenging, by making its modes unbalanced with non-isotropic variances, resulting in a highly asymmetrical, multi-modal problem. We include the full details of the parameters in Appendix D. We fix the same oracle complexity (total number of $0^{t h}$ and $1^{s t}$ Order $V$ queries) for different methods, and show the generated samples in Figure 2. Note matching oracle complexity puts our method at a disadvantage, since other techniques require querying the gradient, which results in more function evaluations. Despite this, we see in Figure la that our method achieves both the lowest MMD and $W_{2}$ using the least number of oracle complexity. ", "page_idx": 7}, {"type": "image", "img_path": "X3Aljulsw5/tmp/c10349f1b3e2a37d7649a94ff52d5d9ae2698474c936cd6a7d3c5b4faaee57d0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "(a) Sampling accuracy against oracle complexity. For (b) Sampling accuracy against dimension We demonanyfixed oracle complexity, ZOD-MC has the least strate that other diffusion based methods scale poorly error both in MMD and $W_{2}$ . Note diffusion based with dimension. On the left we plot the error when methods do not have an initialization. Thus curves  evaluating statistics of the generated samples and on don't start at the same $y$ value. the right we analyze the $W_{2}$ metric. ", "page_idx": 7}, {"type": "text", "text": "Figure 1: Accuracies of different methods for sampling Gaussian Mixture ", "page_idx": 7}, {"type": "text", "text": "Robustness Against Mode Separation. Now let's further separate the modes in the mixture to investigate the robustness of our method to increasing nonconvexity/metastability. More precisely, we scale the means of each mode by a constant factor to have a mode located at $(0,R)$ ;doingso increases the barriers between the modes and exponentially worsens the isoperimetric properties of the target distribution [49]. Figure 4a shows our method is the most insensitive to mode separation. Being the only one that can successfully sample from all modes, as observed in Figure 3, ZOD-MC suffers less from metastability. Note there is still some dependence on mode separation due to the $x_{k}$ dependence in the complexity bound in Corollary 3.1. ", "page_idx": 7}, {"type": "image", "img_path": "X3Aljulsw5/tmp/c75cd5b34a6ab51052f095d1660a2e56bc4a302f7d2c129de4ec380e2f1af17f.jpg", "img_caption": ["Figure 2: Sampling from asymmetric,unbalanced Gaussian Mixture. All diffusion-based methods (ZOD-MC, RDMC, RSDMC) use 2200 oracles per score evaluation. Langevin and the proximal sampler are set to use the same total amount of oracles as diffusion based methods. While other methods suffer from metastability, ZOD-MC correctly samples all modes. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Dimension Dependence Against Other Diffusion Based Methods. One drawback of our method, is its bad dimension dependence when compared to diffusion based methods. For instance, RDMC and RSDMC have a dependence of $\\exp(\\mathcal{O}(\\log(d))\\tilde{\\mathcal{O}}(\\varepsilon^{-1}))$ and $\\exp(\\mathcal{O}(\\log^{3}(d\\varepsilon^{-1})))$ respectively, in comparison to our $\\exp(\\tilde{\\mathcal{O}}(d)\\mathcal{O}((\\log(\\varepsilon^{-1}))))$ . Despite this theoretical disadvantage, we find empirically that these methods don't scale well with dimension either. To demonstrate this we sample 5 points on the positive quadrant and use them as means for a GMM. We then evaluate statistics on the generated samples and $W_{2}$ as a function of dimension. We observe in Figure 1b that under a fixed number of function evaluations our method results in the lowest $W_{2}$ . More details are in Appendix D. Discontinuous Potentials. The use of zeroth-order queries allows ZOD-MC to solve problems that would be completely infeasible to first order methods. To demonstrate this, we modify the potential in Figure 2. We consider $V(x)+U(x)$ where $U$ is a discontinuous function given by $\\bar{U}(x)=8\\lfloor\\lvert\\lvert x\\rvert\\rvert\\rceil_{\\{5<\\lvert\\lvert x\\rvert<11\\}}$ This creates an annulus of much lower probability and astrong potential barrier. In the original problem, the mode centered at the origin was chosen to have the smallest weight (0.1), but adding this discontinuity significantly changes the problem. As observed in Figure 5, our method is still able to correctly sample from the target distribution, while other methods not only continue to suffer from metastability but also fail to see the discontinuities. We quantitatively evaluate the sampling accuracy by using rejection sampling (slow but unbiased) to obtain ground truth samples, and then compute MMD and $W_{2}$ . See Appendix D.2 for details. ", "page_idx": 7}, {"type": "image", "img_path": "X3Aljulsw5/tmp/c1a9b84f2d51fe8b5446d225db304dc1332cbfaa7425fc8ac3131ad3b826f048.jpg", "img_caption": ["Figure3:GaussianMixturewithfurtherseparatedmodes $\\mathcal{R}=26$ ).ZOD-MC can overcome strengthened metastability and sample from every mode, while other methods are stuck at the mode at the origin, where every method is initialized. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "X3Aljulsw5/tmp/d213f3fea23829379cb087087f1542443b064a493f78047d7576e3d0b098c2fb.jpg", "img_caption": ["(a) Sampling accuracy against how separated modes are. ZOD-MC is (b)Average Score Error as afuncthe least sensitive to mode separation. The oracle complexity is fixed, tion of time. Shaded is the stanindependent of how modes are separated. darddeviation of theerrors. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 4: Accuracies of generated samples against dimension and Score Error. On the right, the result for SLIPS is not directly comparable as it has a different forward process. ", "page_idx": 8}, {"type": "text", "text": "Score Approximation of Diffusion Based Methods. One explanation of our method's great success in comparison with RDMC and RSDMC is the ability to approximate the score correctly. We select an unbalanced assymetrical 5d GMM and evaluate the average $L^{2}$ score error between methods. On Figure 4b we show that the best approximations of the score are found by using ZODMC as an estimator as opposed to other methods. Even as $t$ increases and the approximation gets harder we are able to retain accuracy and therefore generate high quality samples. ", "page_idx": 8}, {"type": "image", "img_path": "X3Aljulsw5/tmp/12db6a9f54ef54f138c5d43dd63aee5caeec927ea234648e36b313f821ab09ee.jpg", "img_caption": ["Figure 5: Generated samples for discontinuous Gaussian Mixture. Our method can recover the target distribution even under the presence of discontinuities. The same oracle complexity is again used in each method, 3200 per score evaluation in diffusion-based approaches. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.2 Results of Miller Brown Potential ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The Muiller Brown potential is a toy model for molecular dynamics. Its highly nonlinear potential has 3 modes despite of being the sum of 4 exponentials. The original version has 2 of its modes corresponding to negligible probabilities when compared to the 3rd, which is not good to visualization and comparison across different methods. Thus we consider a balanced version [35] and further translate and dilate $x$ and $y$ so that one of the modes is centered near the origin. The details of the potential can be found in Appendix D.5. Our method is the only one that can correctly sample from all 3 modes as observed in Figure 6 (note they are leveled). ", "page_idx": 8}, {"type": "text", "text": "Figure 6: Generated samples for the Miller Brown potential. We overlay the generated samples on top of thelevel curvesof $V(x)$ . All methods use 1100 oracles. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors are grateful for the partially support by NSF DMS-1847802, Cullen-Peck Scholarship, and GT-Emory Humanity.AI Award. We thank the anonymous reviewers for their helpful comments. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1]  K. Balasubramanian, S. Chewi, M. A. Erdogdu, A. Salim, and S. Zhang. Towards a theory of non-log-concave sampling: first-order stationarity guarantees for langevin monte carlo. In Conference on Learning Theory, pages 2896-2923. PMLR, 2022.   \n[2] C. J. Belisle, H. E. Romeijn, and R. L. Smith. Hit-and-run algorithms for generating multivariate distributions. Mathematics of Operations Research, 18(2):255-266, 1993.   \n[3] J. Benton, V. De Bortoli, A. Doucet, and G. Deligiannidis. Linear convergence bounds for diffusion models via stochastic localization. ICLR, 2024.   \n[4] H. Chen, H. Lee, and J. Lu. Improved analysis of score-based generative modeling: Userfriendly bounds under minimal smoothness assumptions. In International Conference on Machine Learning, pages 4735-4763. PMLR, 2023.   \n[5] S. Chen, S. Chewi, J. Li, Y. Li, A. Salim, and A. Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. ICLR, 2022.   \n[6]  Y. Chen, S. Chewi, A. Salim, and A. Wibisono. Improved analysis for a proximal algorithm for sampling. In Conference on Learning Theory, pages 2984-3014. PMLR, 2022.   \n[7]  Y. Chen and R. Eldan. Localization schemes: A framework for proving mixing bounds for markov chains. In 2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS), pages 110-122. IEEE, 2022.   \n[8]  Y. Chen and K. Gatmiry. A simple proof of the mixing of metropolis-adjusted langevin algorithm under smoothness and isoperimetry. arXiv preprint arXiv:2304.04095, 2023.   \n[9] S. Chewi. Log-concave sampling. 2023. Book draft available at https: //chewisinho. github.io/.   \n[10] S. Chewi, T. Le Gouic, C. Lu, T. Maunu, and P. Rigollet. Svgd as a kernelized wasserstein gradient fow of the chi-squared divergence. Advances in Neural Information Processing Systems, 33:2098-2109, 2020.   \n[11  G. Conforti, A.Durmus, and M. G. Silveri Score diffusion models without ealy stopping: finite fisher information is all you need. arXiv preprint arXiv:2308.12240, 2023.   \n[12]  A. S. Dalalyan and A. Karagulyan. User-friendly guarantees for the langevin monte carlo with inaccurate gradient. Stochastic Processes and their Applications, 129(12):5278-5311, 2019.   \n[13]  A. S. Dalalyan and L. Riou-Durand. On sampling from a log-concave density using kinetic langevin diffusions. Bernoulli, 26(3): 1956-1988, 2020.   \n[14] V. De Bortoli. Convergence of denoising diffusion models under the manifold hypothesis. TMLR, 2022.   \n[15] P. Del Moral, A. Doucet, and A. Jasra. Sequential monte carlo samplers. Journal of the Royal Statistical Society Series B: Statistical Methodology, 68(3):411-436, 2006.   \n[16] R. Dwivedi, Y. Chen, M. J. Wainwright, and B. Yu. Log-concave sampling: Metropolis-hastings algorithms are fast. Journal of Machine Learning Research, 20(183):1-42, 2019.   \n[17] M. Dyer, A. Frieze, and R. Kannan. A random polynomial-time algorithm for approximating the volume of convex bodies. Journal of the ACM (JACM), 38(1):1-17, 1991.   \n[18] J. Fan, B. Yuan, and Y. Chen. Improved dimension dependence of a proximal algorithm for sampling. In The Thirty Sixth Annual Conference on Learning Theory, pages 1473-1521. PMLR, 2023.   \n[19] A. Garbuno-Inigo, F. Hoffmann, W. Li, and A. M. Stuart. Interacting langevin diffusions: Gradient structure and ensemble kalman sampler. SIAM Journal on Applied Dynamical Systems, 19(1):412-441, 2020.   \n[20] L. Grenioux, M. Noble, M. Gabri\u00e9, and A. Oliviero Durmus. Stochastic localization via iterative posterior sampling. In Procedings of the 41st International Conference on Machine Learning, volume 235, pages 16337-16376. PMLR, 21-27 Jul 2024.   \n[21] Y. He, K. Balasubramanian, and M. A. Erdogdu. On the ergodicity, bias and asymptotic normality of randomized midpoint sampling method. Advances in Neural Information Processing Systems, 33:7366-7376, 2020.   \n[22] Y. He, K. Balasubramanian, B. K. Sriperumbudur, and J. Lu. Regularized stein variational gradient flow. arXiv preprint arXiv:2211.07861, 2022.   \n[23]  Y. He, T. Farghly, K. Balasubramanian, and M. A. Erdogdu. Mean-square analysis of discretized ito diffusions for heavy-tailed sampling. Journal of Machine Learning Research, 25(43):1-44, 2024.   \n[24] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840-6851, 2020.   \n[25]  D. Holzmiller and F. Bach. Convergence rates for non-log-concave sampling and log-partition estimation. arXiv preprint arXiv:2303.03237, 2023.   \n[26] X. Huang, H. Dong, Y. Hao, Y. Ma, and T. Zhang. Reverse diffusion monte carlo. ICLR, 2024.   \n[27] X. Huang, D. Zou, H. Dong, Y. Ma, and T. Zhang. Faster sampling without isoperimetry via diffusion-based monte carlo. COLT, 2024.   \n[28] M. A. Iglesias, K. J. Law, and A. M. Stuart. Ensemble kalman methods for inverse problems. Inverse Problems, 29(4):045001, 2013.   \n[29] H. Lee, J. Lu, and Y. Tan. Convergence of score-based generative modeling for general data distributions. In International Conference on Algorithmic Learning Theory, pages 946-985. PMLR, 2023.   \n[30] H. Lee and Z. Shen. Improved bound for mixing time of parallel tempering. arXiv preprint arXiv:2304.01303, 2023.   \n[31] Y. T. Lee, R. Shen, and K. Tian. Structured logconcave sampling with a restricted gaussian oracle. In Conference on Learning Theory, pages 2993-3050. PMLR, 2021.   \n[32]  G. Li, Y. Wei, Y. Chen, and Y. Chi. Towards faster non-asymptotic convergence for diffusionbased generative models. ICLR, 2024.   \n[33] R. Li, M. Tao, S. S. Vempala, and A. Wibisono. The mirror Langevin algorithm converges with vanishing bias. In International Conference on Algorithmic Learning Theory, pages 718-742. PMLR, 2022.   \n[34] R. Li, H. Zha, and M. Tao. Sqrt(d) Dimension Dependence of Langevin Monte Carlo. In ICLR, 2021.   \n[35] X. H. Li and M. Tao. Automated construction of effective potential via algorithmic implicit bias. arXiv preprint arXiv:2401.03511, 2024.   \n[36] J. Liang and Y. Chen. A proximal algorithm for sampling. arXiv preprint arXiv:2202.13975, 2022.   \n[37] J.Liang and Y. Chen. A proximal algorithm for sampling. arXiv preprint arXiv:2202.13975, 2022.   \n[38]  Q. Liu. Stein variational gradient descent as gradient fow. Advances in neural information processing systems, 30, 2017.   \n[39] L. Lovasz and M. Simonovits. The mixing rate of markov chains, an isoperimetric inequality, and computing the volume. In Proceedings [1990] 31st annual symposium on foundations of computer science, pages 346-354. IEEE, 1990.   \n[40] L. Lovasz and S. Vempala. Hit-and-run from a corner. In Proceedings of the thirty-sixth annual ACM symposium on Theory of computing, pages 310-314, 2004.   \n[41] K. L. Mengersen and R. L. Tweedie. Rates of convergence of the hastings and metropolis algorithms. The annals of Statistics, 24(1):101-121, 1996.   \n[42] R. M. Neal. Annealed importance sampling. Statistics and computing, 11:125-139, 2001.   \n[43] L. Pardo. Statistical inference based on divergence measures. CRC press, 2018.   \n[44] L. Richter, J. Berner, and G.-H. Liu. Improved sampling via learned diffusions. ICLR, 2024.   \n[45] H. E. Robbins. An empirical bayes approach to statistics. In Breakthroughs in Statistics: Foundations and basic theory, pages 388-394. Springer, 1992.   \n[46] G. O. Roberts and R. L. Tweedie. Geometric convergence and central limit theorems for multidimensional hastings and metropolis algorithms. Biometrika, 83(1):95-110, 1996.   \n[47] A. Roy, L. Shen, K. Balasubramanian, and S. Ghadimi. Stochastic zeroth-order discretizations of langevin diffusions for bayesian inference. Bernoulli, 28(3):1810-1834, 2022.   \n[48]  A. Salim, L. Sun, and P. Richtarik. A convergence theory for svgd in the population limit under talagrand's inequality t1. In International Conference on Machine Learning, pages 19139-19152. PMLR, 2022.   \n[49]  A. Schlichting. Poincare and log-sobolev inequalities for mixtures. Entropy, 21(1):89, 2019.   \n[50] R. Shen and Y. T. Lee. The randomized midpoint method for log-concave sampling. Advances in Neural Information Processing Systems, 32, 2019.   \n[51] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. ICML, 2015.   \n[52] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021.   \n[53] F. Vargas, W. Grathwohl, and A. Doucet. Denoising diffusion samplers. arXiv preprint arXiv:2302.13834, 2023.   \n[54] F. Vargas, A. Ovsianas, D. Fernandes, M. Girolami, N. D. Lawrence, and N. Nusken. Bayesian learning via neural schrodinger-follmer flows. Statistics and Computing, 33(1):3, 2023.   \n[55] F. Vargas, S. Padhy, D. Blessing, and N. Nusken. Transport meets variational inference: Controlled monte carlo diffusions. In The Twelfth International Conference on Learning Representations,2024.   \n[56]  S. Vempala and A. Wibisono. Rapid convergence of the unadjusted langevin algorithm: Isoperimetry sufices. Advances in neural information processing systems, 32, 2019.   \n[57]  K. Yingxi Yang and A. Wibisono. Convergence of the inexact langevin algorithm and scorebased generative models in kl divergence. arXiv e-prints, pages arXiv-2211, 2022.   \n[58] Q. Zhang and Y. Chen. Path integral sampler: a stochastic control approach for sampling. ICLR, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A  Related Works on Zeroth-Order Sampling ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The zeroth-order sampling algorithms have been widely studied in the past decades. There is a class of zeroth-order sampling algorithms, including the Ensemble Kalman Inversion [28] and the Ensemble Kalman Sampler [19], that are based on moving a set of easy-to-sample particle according to certain dynamics. However, these methods require (noisy) observations from the target distribution rather than queries of the potential function. Within the zeroth-order sampling algorithms using queries of the potential function, one type of methods make use of the zeroth-order queries to approximate the gradient and apply it to some first-order sampling algorithm [12, 47, 23]. Since it is based on the first-order methods, the analysis of this type of algorithms assumes the target distribution satisfies certain isoperimetric property in general. The other type of methods utilize the zeroth-order queries directly without relating to the gradient. Such methods include the Rejection sampling algorithm, the Metropolized Random Walk (MRW) [41, 46], Ball Walk [39, 17], Hit-and-Run algorithm [2, 40]. The rejection sampling algorithm requires to finding an envelope function which is easy to sample from. This could be difficult. MRW requires sufficient smooth and light tail of the target distribution to mix fast. Ball walk and Hit-and-Run algorithms assume the target distribution is compactly supported. In this paper, we develop a zeroth-order sampling algorithm based on the reverse OU process. Our algorithm does not suffer from the difficulties in the rejection sampling and MRW, and our analysis does not assume isoperimetric property and compact support of the target distribution. ", "page_idx": 13}, {"type": "text", "text": "[25] also studies the complexity of a zeroth-order sampling algorithm that combines an approximation technique and rejection sampling. For target distributions that are $m$ -differentiable with compact support, [25][Theorem 12] implies a complexity of order $\\Omega_{d}(\\varepsilon^{-d/m})$ to reach an $\\varepsilon$ -accuracy in KL-divergence, where the dimension dependence is implicit. Compare to our result in Corollary 3.1, both complexities are polynomial in $\\varepsilon$ and exponential in $d$ . Our complexity is smaller for less smooth targets $(m<2)$ while their complexity is smaller for smoother targets $(m>2)$ . However, result in [25] only applies to smooth targets $(m>0)$ ) with compact supports, while our Corollary 3.1 applies to more general target distributions which can be with full support with even discontinuous potentials. ", "page_idx": 13}, {"type": "text", "text": "B More Details on ZOD-MC ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we provide more details on how rejection sampling (Algorithm 3) in ZOD-MC implements the oracle in DDMC, i.e., generating Monte Carlo samples required in Algorithm 2. Construction of anenvelope.If we have $V^{*}$ as a minimum value of $V$ , then by noting that: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{-V(z)-\\frac{1}{2}\\frac{\\left\\Vert z-e^{t}x\\right\\Vert^{2}}{e^{2t}-1}\\leq-V^{*}-\\frac{1}{2}\\frac{\\left\\Vert z-e^{t}x\\right\\Vert^{2}}{e^{2t}-1},\\quad\\forall\\,z\\in\\mathbb{R}^{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We are able to construct an envelope for rejection sampling. In particular we propose a samples $z$ from $\\mathcal{N}(\\cdot\\,;e^{t}x,e^{2t}-1)$ and accept proposal $z$ withprobability $\\bar{\\exp(-V(z)+\\bar{V}^{*}\\bar{)}}$ ", "page_idx": 13}, {"type": "text", "text": "Sampling from the target distribution. Algorithm 3, implements the oracle in Algorithm 2 with $\\delta(t)=0$ .When $n(t)$ increases, Algorithm 2 outputs unbiased Monte Carlo score estimators with smaller variance, hence closer to the true score. We will quantify the convergence of DDMC next and consequently demonstrate that ZOD-MC can sample general non-logconcave distributions. ", "page_idx": 13}, {"type": "text", "text": "C Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "C.1   Properties of the OU-Process ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we introduce and prove some useful properties of the OU-process. Throughout this section, we denote $\\{X_{t}\\}_{t\\ge0}$ as the solution of (1) with $p_{t}:=\\operatorname{Law}(X_{t})$ . For any $s,t>0$ \uff0c $p_{t|s}$ denotes the conditional probability measure of $X_{t}$ given the value of $X_{s}$ ", "page_idx": 13}, {"type": "text", "text": "Proposition C.1. (Decay along the OU-Process) Let $\\{X_{t}\\}_{t\\ge0}$ be the solution of (1) with $p_{t}\\ :=$ $\\operatorname{Law}\\!\\left(X_{t}\\right)$ .Assume that the initial distribution $p$ satisfies Assumption 3.1. Then we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{2}(p_{t},p)^{2}\\leq(1-e^{-t})^{2}\\mathrm{m}_{2}^{2}+(1-e^{-2t})d,}\\\\ {\\quad}&{a n d\\quad K L(p_{t}|\\gamma^{d})\\leq\\displaystyle\\frac{1}{2}\\frac{e^{-4t}}{1-e^{-2t}}d+\\frac{1}{2}e^{-2t}\\mathrm{m}_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof of Proposition C.1. The proof for (11) is based on the fact that the solution to (1) can be representedby ", "page_idx": 14}, {"type": "equation", "text": "$$\nX_{t}=e^{-t}X_{0}+\\sqrt{1-e^{-2t}}Z,\\quad\\forall\\,t\\geq0.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{2}(p_{t},p)^{2}\\leq\\mathbb{E}\\big[\\left\\|X_{t}-X_{0}\\right\\|^{2}\\big]\\leq\\mathbb{E}_{(X_{0},Z)\\sim p\\otimes\\gamma^{d}}\\big[\\|(e^{-t}-1)X_{0}+\\sqrt{1-e^{-2t}}Z\\|^{2}\\big]}\\\\ &{\\qquad\\qquad=(1-e^{-t})^{2}\\mathrm{m}_{2}^{2}+(1-e^{-2t})d.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Next, to prove (12), we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{KL}(p_{t}|\\gamma^{d})=\\mathrm{KL}(\\int p_{t|0}(\\cdot|y)p(\\mathrm{d}y)|\\gamma^{d}(\\cdot))\\leq\\int\\mathrm{KL}(p_{t|0}(\\cdot|y)|\\gamma^{d})p(\\mathrm{d}y),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the inequality follows from the convexity of $\\mathrm{KL}$ divergence. According to (13), $p_{t|0}(\\cdot|y)$ is a Gaussian measure with mean $e^{-t}y$ and covariance matrix $(1-e^{-2t})I_{d}$ . According to [43], we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}\\big(p_{t\\mid0}(\\cdot|y)|\\gamma^{d}\\big)=\\mathrm{KL}(\\mathcal{N}(e^{-t}y,(1-e^{-2t})I_{d})|\\mathcal{N}(0,I_{d}))}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\frac{1}{2}\\big(-d\\ln(1-e^{-2t})-e^{-2t}d+e^{-2t}\\left\\|y\\right\\|^{2}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "As a result, we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{KL}(p_{t}|\\gamma^{d})\\leq-\\frac{d}{2}\\ln(1-e^{-2t})-\\frac{d}{2}e^{-2t}+\\frac{1}{2}e^{-2t}\\int\\|y\\|^{2}p(\\mathrm{d}y)\\leq\\frac{e^{-4t}}{2(1-e^{-2t})}d+\\frac{1}{2}e^{-2t}\\mathrm{m}_{2}^{2},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the last inequality follows from the fact that $\\ln(1+x)\\leq x$ for all $x>0$ ", "page_idx": 14}, {"type": "text", "text": "Proposition C.2. (Stochastic Dynamics along the OU-Process) Let $\\{X_{t}\\}_{t\\ge0}$ be the solution of (1).Define $m_{t}(X_{t})~:=~\\operatorname{\\mathbb{E}}_{X_{0}\\sim p_{0\\mid t}(\\cdot|X_{t})}[X_{0}]$ and $\\begin{array}{r c l}{\\Sigma_{t}(X_{t})}&{:=}&{\\mathrm{Cov}_{X_{0}\\sim p_{0\\mid t}(\\cdot|X_{t})}(X_{0})}\\end{array}=$ $\\mathbb{E}_{X_{0}\\sim p_{0\\mid t}(\\cdot|X_{t})}[\\left(X_{0}-m_{t}(X_{t})\\right)^{\\otimes2}]$ . Then we have for all $t\\geq0$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathbb{E}\\big[\\Sigma_{t}(X_{t})\\big]=\\frac{2e^{-2t}}{(1-e^{-2t})^{2}}\\mathbb{E}\\big[\\Sigma_{t}(X_{t})^{2}\\big].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The above proposition is known in stochastic localization literature [7] and diffusion model literature [3]. We present its proof for the sake of completeness. ", "page_idx": 14}, {"type": "text", "text": "Proof of Proposition C.2. For any $T>0$ , from (13), we have the conditional distribution ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{0|t}(\\mathrm{d}x|X_{t})\\propto\\exp\\big(-\\frac{1}{2}\\frac{\\|X_{t}-e^{-t}x\\|^{2}}{1-e^{-2t}}\\big)p(\\mathrm{d}x),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\{X_{t}\\}_{0\\le t\\le T}$ is the solution of (1). Noticing that the solution of (2), $\\{\\bar{X}_{t}\\}_{0\\le t\\le T}$ is the reverse process of $\\{X_{t}\\}_{0\\le t\\le T}$ and it satisfies $\\bar{X}_{t}\\,=\\,X_{T-t}$ in distribution for all $t\\in[0,T]$ . Therefore, it suffices to study ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q_{0|t}(\\mathrm{d}x|\\bar{X}_{t}):=Z^{-1}\\exp\\big(-\\cfrac{1}{2}\\frac{\\|\\bar{X}_{t}-e^{-(T-t)}x\\|^{2}}{1-e^{-2(T-t)}}\\big)p(\\mathrm{d}x)}\\\\ &{\\qquad\\qquad=Z_{t}^{-1}\\exp\\big(-\\cfrac{1}{2}\\frac{e^{-2(T-t)}}{1-e^{-2(T-t)}}\\|x\\|^{2}+\\cfrac{e^{-(T-t)}}{1-e^{-2(T-t)}}\\langle x,\\bar{X}_{t}\\rangle\\big)p(\\mathrm{d}x)}\\\\ &{\\qquad\\qquad:=Z_{t}^{-1}\\exp(h_{t}(x))p(\\mathrm{d}x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the normalization constant $\\begin{array}{r c l}{Z_{t}}&{=}&{\\int_{\\mathbb{R}^{d}}\\exp(h_{t}(x))p(\\mathrm{d}x)}\\end{array}$ We have $q_{0|t}(\\mathrm{d}x|\\bar{X}_{t})\\ \\ =$ $p_{0|T-t}(\\mathrm{d}x|X_{T-t})$ in distribution for all $t\\in[0,T]$ and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{m}_{t}(\\bar{X}_{t}):=\\mathbb{E}_{X_{0}\\sim q_{0\\mid t}(\\cdot|\\bar{X}_{t})}[X_{0}]=\\mathbb{E}_{X_{0}\\sim p_{0\\mid T-t}(\\cdot|X_{T-t})}[X_{0}]=m_{T-t}(X_{T-t}),}\\\\ &{\\bar{\\Sigma}_{t}(\\bar{X}_{t}):=\\mathrm{Cov}_{X_{0}\\sim q_{0\\mid t}(\\cdot|\\bar{X}_{t})}(X_{0})=\\mathrm{Cov}_{X_{0}\\sim p_{0\\mid T-t}(\\cdot|X_{T-t})}(X_{0})=\\Sigma_{T-t}(X_{T-t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the above two identities hold in distribution. For simplicity, we denote $\\sigma_{t}=\\sqrt{1-e^{-2t}}$ .Then $\\begin{array}{r}{h_{t}(x)=-\\frac{1}{2}(\\sigma_{T-t}^{-2}-1)\\|x\\|^{2}\\!+\\!\\sigma_{T-t}^{-1}\\sqrt{\\sigma_{T-t}^{-2}-1}\\langle x,\\bar{X}_{t}\\rangle}\\end{array}$ is a stochastic process linearly depending on ", "page_idx": 14}, {"type": "text", "text": "$\\{\\bar{X}_{t}\\}_{t\\ge0}$ The conditional measure $\\{q_{0|t}(x|\\bar{X}_{t})\\}_{t\\geq0}$ is a measure-valued stochastic process, whose dynamics can be studied by applying Ito's formula. First we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}h_{t}(x)=\\sigma_{T-t}^{-3}\\dot{\\sigma}_{T-t}\\left\\Vert x\\right\\Vert^{2}\\mathrm{d}t+(\\sigma_{T-t}^{-2}-1)^{-\\frac{1}{2}}\\big(-2\\sigma_{T-t}^{-4}\\dot{\\sigma}_{T-t}+\\sigma_{T-t}^{-2}\\dot{\\sigma}_{T-t}\\big)\\langle x,\\bar{X}_{t}\\rangle\\mathrm{d}t}\\\\ &{\\qquad\\qquad+\\sigma_{T-t}^{-1}(\\sigma_{T-t}^{-2}-1)^{\\frac{1}{2}}\\langle x,\\mathrm{d}\\bar{X}_{t}\\rangle,}\\\\ &{\\mathrm{d}[h(x),h(x)]_{t}=\\sigma_{T-t}^{-2}(\\sigma_{T-t}^{-2}-1)\\left\\Vert x\\right\\Vert^{2}\\mathrm{d}[\\bar{X},\\bar{X}]_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $\\{\\bar{X}_{t}\\}_{0\\le t\\le T}$ solves (2), according to Lemma 1, it satisfies that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}\\bar{X}_{t}=\\big(\\bar{X}_{t}+2\\mathbb{E}_{x\\sim p_{0}|T-t}(\\cdot|\\bar{X}_{t})\\big[\\frac{e^{-(T-t)}x-\\bar{X}_{t}}{1-e^{-2(T-t)}}\\big]\\big)\\mathrm{d}t+\\sqrt{2}\\mathrm{d}\\bar{B}_{t}}\\\\ &{\\qquad=\\big(-\\sigma_{T-t}^{-2}(2-\\sigma_{T-t}^{-2})\\bar{X}_{t}+2(1-\\sigma_{T-t}^{2})^{\\frac{1}{2}}\\bar{m}_{t}(\\bar{X}_{t})\\big)\\mathrm{d}t+\\sqrt{2}\\mathrm{d}\\bar{B}_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Based on (17), (18) and (19), we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\,\\mathrm{d}Z_{t}=\\int_{\\mathbb{R}^{d}}\\exp\\big(h_{t}(x)\\big)\\big(\\mathrm{d}h_{t}(x)+\\frac12\\mathrm{d}[h(x),h(x)]_{\\varepsilon}\\big)p(\\mathrm{d}x)}\\\\ &{\\qquad=\\big(\\sigma_{T-\\varepsilon}^{-3}\\partial^{\\varepsilon}r-\\varepsilon\\big)\\big(\\sigma_{T-\\varepsilon}^{-2}-1\\big)\\Big\\vert\\nabla_{\\theta_{0}(\\cdot\\vert\\cdot\\vert\\nabla_{x}}\\big)\\big[\\|x\\|^{2}\\big]Z_{t}\\mathrm{d}t}\\\\ &{\\qquad\\quad+\\big(\\sigma_{T-\\varepsilon}^{-2}-1\\big)^{-\\frac12}\\big(-2\\sigma_{T-\\varepsilon}^{-4}\\partial^{\\varepsilon}r-\\sigma_{T-\\varepsilon}^{-2}\\partial^{\\varepsilon}r-\\big)\\big\\langle\\overline{{m}}_{t}(\\widetilde{X}_{t}),\\widetilde{X}_{t}\\big\\rangle Z_{t}\\mathrm{d}t}\\\\ &{\\qquad\\quad+\\sigma_{T-\\varepsilon}^{-2}\\big(\\sigma_{T-\\varepsilon}^{-2}-1\\big)^{\\frac12}\\big\\langle\\widetilde{m}_{t}(\\widetilde{X}_{t}),\\widetilde{\\mathrm{d}}\\chi_{t}\\big\\rangle Z_{t},}\\\\ &{\\,\\mathrm{d}\\mathrm{In}\\,Z_{t}=Z_{t}^{-1}\\mathrm{d}Z_{t}-\\frac12Z_{t}^{-2}\\mathrm{d}[Z,Z_{t}]_{\\varepsilon}}\\\\ &{\\qquad=\\big(\\sigma_{T-\\varepsilon}^{-3}\\delta_{T-\\varepsilon}+\\sigma_{T-\\varepsilon}^{-2}(\\sigma_{T-\\varepsilon}^{-2}-1)\\big)\\mathbb{E}_{\\theta_{0}(\\cdot\\vert\\cdot\\vert\\nabla_{x}}\\big)\\big[\\|x\\|^{2}\\big]\\mathrm{d}t}\\\\ &{\\qquad\\quad+\\big(\\sigma_{T-\\varepsilon}^{-2}-1\\big)^{-\\frac12}\\big(-2\\sigma_{T-\\varepsilon}^{-2}\\partial^{\\varepsilon}r-\\varepsilon\\big)\\sigma_{T-\\varepsilon}^{-2}\\partial^{\\varepsilon}\\big\\langle\\overline{{m}}_{t}(\\widetilde{X}_{t}),\\widetilde{X}_{t}\\big\\rangle\\mathrm{d}t}\\\\ &{\\qquad\\quad+\\sigma_{T-\\varepsilon}^{-1}(\\sigma_{T-\\varepsilon}^{-2}-1)^{\\frac12}\\big\\langle\\widetilde{m}_{t}(\\widetilde{X}_{t}),\\mathrm{d}\\widetilde{X}_ \n$$and ", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "If we define $\\begin{array}{r}{R_{t}(\\bar{X}_{t})=\\frac{q_{0|t}(\\mathrm{d}x|\\bar{X}_{t})}{p(\\mathrm{d}x)}=Z_{t}^{-1}\\exp(h_{t}(x))}\\end{array}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}R_{t}(\\bar{X}_{t})=\\mathrm{d}\\exp\\big(\\ln R_{t}(\\bar{X}_{t})\\big)}\\\\ &{\\qquad\\qquad=R_{t}(\\bar{X}_{t})\\mathrm{d}\\big(\\ln R_{t}(\\bar{X}_{t})\\big)+\\frac{1}{2}R_{t}(\\bar{X}_{t})\\mathrm{d}\\big[\\ln R_{t}(\\bar{X}_{t}),\\ln R_{t}(\\bar{X}_{t})\\big]}\\\\ &{\\qquad\\qquad=R_{t}(\\bar{X}_{t})\\mathrm{d}h_{t}(x)-R_{t}(\\bar{X}_{t})\\mathrm{d}\\ln Z_{t}+\\frac{1}{2}R_{t}(\\bar{X}_{t})\\mathrm{d}\\big[h_{t}(x)-\\ln Z_{t},h_{t}(x)-\\ln Z_{t}\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now_combine the results in (17), (18), (20) and (21), we can derive the differential equation of $\\bar{m}_{t}(\\bar{X}_{t})$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{d}\\hat{m}_{t}(\\tilde{X}_{t})=\\int_{\\mathbf{R}^{d}}x R_{t}(\\tilde{X}_{t})p(\\mathrm{d}x)}\\\\ &{=\\int_{\\mathbf{R}^{d}}x(\\mathrm{d}h_{t}(x)-d\\mathrm{l}\\,x_{t}+\\frac{1}{2}\\mathrm{d}[h_{t}(x)-\\ln Z_{t},h_{t}(x)-\\ln Z_{t}])q_{01}(\\mathrm{d}x|\\tilde{X}_{t})}\\\\ &{=\\sigma_{T^{-}}^{3}\\phi_{T^{-}}\\mathrm{d}g_{01(t+1)}\\Big[\\left\\{1\\right\\}^{2}x\\Big]\\mathrm{d}t}\\\\ &{\\quad+(\\sigma_{T^{-}}^{2}-1)^{-\\frac{1}{2}}\\left(-2\\sigma_{T^{-}}^{2}\\phi_{T^{-}}+\\sigma_{T^{-}}^{2}\\phi_{T^{-}-})\\mathbb{E}_{01(t)}\\left\\{\\Sigma^{0}\\right\\}\\bar{X}_{t}\\mathrm{d}t}\\\\ &{\\quad+\\sigma_{T^{-}}^{2}(\\sigma_{T^{-}}^{2}t-1)^{-\\frac{1}{2}}\\mathrm{d}\\bar{\\Sigma}_{t}\\mathrm{d}\\sigma_{(t+1)}\\Big[\\left\\{\\Sigma^{0}\\right\\}\\bar{X}_{t}(\\tilde{X}_{t})\\Big]}\\\\ &{\\quad-\\sigma_{T^{-}}^{2}\\phi_{T^{-}}\\mathrm{d}\\bar{\\Sigma}_{t}\\mathrm{d}\\sigma_{(t+1)}\\Big[\\left\\{\\Sigma^{0}\\right\\}\\Big]\\bar{m}_{t}(\\tilde{X}_{t})\\mathrm{d}t}\\\\ &{=\\left(\\sigma_{T^{-}}^{2}t-1\\right)^{-\\frac{1}{2}}(-2\\sigma_{T^{-}}^{2}\\phi_{T^{-}}+\\sigma_{T^{-}}^{2}\\phi_{T^{-}-})\\eta_{01}(\\tilde{X}_{t})^{\\otimes2}\\bar{X}_{t}\\mathrm{d}t}\\\\ &{\\quad-\\sigma_{T^{-}}^{2}(\\sigma_{T^{-}}^{2}t-1)^{\\frac{1}{2}}\\bar{m}_{t}(\\tilde{X}_{t})^{\\otimes2}\\mathrm{d}\\bar{X}_{t}}\\\\ &{\\quad-\\sigma_{T^{-}}^{2}(\\sigma_\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Utilize (19) and the definition of $\\sigma_{t}$ , all terms with factor $\\mathrm{d}t$ in the above equation cancel and (22) can be simplified as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}\\bar{m}_{t}(\\bar{X}_{t})=\\frac{\\sqrt{2}e^{-(T-t)}}{1-e^{-2(T-t)}}\\mathbb{E}_{q_{0\\mid t}(\\cdot\\mid\\bar{X}_{t})}\\big[x\\otimes(x-\\bar{m}_{t}(\\bar{X}_{t}))\\mathrm{d}B_{t}\\big]}\\\\ &{\\qquad\\qquad=\\frac{\\sqrt{2}e^{-(T-t)}}{1-e^{-2(T-t)}}\\bar{\\Sigma}_{t}(\\bar{X}_{t})\\mathrm{d}\\bar{B}_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Last, wedrive thdiffrentialquatio th $\\mathbb{E}_{X_{t}\\sim p_{t}}\\left[\\Sigma_{t}(X_{t})\\right]$ satisfies. Let $f(t):=\\mathbb{E}_{X_{t}\\sim p_{t}}\\big[\\Sigma_{t}(X_{t})\\big]$ and $g(t):=\\mathbb{E}_{X_{t}\\sim p_{t}}[\\Sigma_{t}(X_{t})^{2}]$ be two deterministic functions on $[0,T]$ . According to (16) and (23), we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{\\mathrm{d}}{\\mathrm{d}t}f(T-t)=\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathbb{E}_{X_{T-t}\\sim p_{T-t}}\\left[\\Sigma_{T-t}(X_{T-t})\\right]}&{}\\\\ {=\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathbb{E}_{X_{t}\\sim p_{T-t}}\\left[\\Sigma_{t}(\\bar{X}_{t})\\right]}\\\\ {=\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathbb{E}_{X_{t}\\sim p_{T-t}}\\left[\\mathbb{E}_{\\mathrm{q}_{\\theta_{t}\\mid\\langle\\cdot\\vert\\bar{X}_{t}\\rangle}}[x^{\\otimes}2]-\\bar{m}_{t}(\\bar{X}_{t})^{\\otimes2}\\right]}\\\\ {=\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathbb{E}_{x\\sim p\\left[x^{\\otimes}2\\right]}-\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathbb{E}_{X_{t}\\sim p_{t}}\\left[\\bar{m}_{t}(\\bar{X}_{t})^{\\otimes2}\\right]}\\\\ {=-\\frac{2e^{-2\\left(T-t\\right)}}{\\left(1-e^{-2\\left(T-t\\right)}\\right)^{2}}\\mathbb{E}_{\\bar{X}_{t}\\sim p_{T-t}}\\left[\\bar{\\Sigma}_{t}(\\bar{X}_{t})^{2}\\right]}\\\\ {=-\\frac{2e^{-2\\left(T-t\\right)}}{\\left(1-e^{-2\\left(T-t\\right)}\\right)^{2}}g(T-t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last inequality follows from the Ito isometry. Proposition C.2 is then proved by reverse the time in $f$ and $g$ \u53e3 ", "page_idx": 16}, {"type": "text", "text": "C.2Proofs of Section 3.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof of Lemma $^{\\,l}$ . Based on (13), we have $p_{t}\\,=\\,(e^{-t})_{\\#p}\\ast(\\sqrt{1-e^{-2t}})_{\\#\\gamma^{d}}$ where $(e^{-t})_{\\#p}$ .s the pushforward measure of $p$ via map $x\\in\\mathbb{R}^{d}\\mapsto e^{-t}x$ and $(\\sqrt{1-e^{-2t}})_{\\#\\gamma^{d}}$ is the pushforward measure of $\\gamma^{d}$ via map $x\\,\\in\\,\\mathbb{R}^{d}\\,\\mapsto\\,\\sqrt{1-e^{-2t}}x\\,\\in\\,\\mathbb{R}^{d}$ Thepushforwardmeasures $(e^{-t})_{\\#p}$ and $(\\sqrt{1-e^{-2t}})_{\\#\\gamma^{d}}$ can be written as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{(e^{-t})_{\\#p}(\\mathrm{d}x)=e^{t d}p(e^{t}\\mathrm{d}x)\\quad\\mathrm{and}}}}\\\\ {{\\displaystyle{(\\sqrt{1-e^{-2t}})_{\\#\\gamma^{d}}(\\mathrm{d}x)=\\left(2\\pi(1-e^{-2t})\\right)^{-\\frac{d}{2}}\\exp\\big(-\\frac{\\|x\\|^{2}}{2(1-e^{-2t})}\\big)\\mathrm{d}x},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "respectively. Therefore the score function $\\nabla\\ln p_{t}(x)$ can be written as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla\\ln p_{t}(x)=p_{t}(x)^{-1}e^{t d}\\bigl(2\\pi(1-e^{-2t})\\bigr)^{-\\frac{d}{2}}\\nabla_{x}\\int\\exp\\big(-\\frac{\\|x-z\\|^{2}}{2(1-e^{-2t})}\\big)p(e^{t}\\mathrm{d}z)}\\\\ &{\\qquad\\qquad=p_{t}(x)^{-1}\\bigl(2\\pi(1-e^{-2t})\\bigr)^{-\\frac{d}{2}}\\nabla_{x}\\int\\exp\\big(-\\frac{\\|x-e^{-t}z\\|^{2}}{2(1-e^{-2t})}\\big)p(\\mathrm{d}z)}\\\\ &{\\qquad\\qquad=\\displaystyle\\int\\frac{x-e^{-t}z}{1-e^{-2t}}\\frac{p_{t}|_{0}(x|z)p(\\mathrm{d}z)}{p_{t}(x)}}\\\\ &{\\qquad\\qquad=\\displaystyle\\int\\frac{x-e^{-t}z}{1-e^{-2t}}p_{0|t}(\\mathrm{d}z|x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last step follows from the Bayesian rule and ", "page_idx": 16}, {"type": "equation", "text": "$$\np_{0|t}(\\cdot|x)\\propto p_{t|0}(x|\\cdot)p(\\cdot)\\propto\\exp\\big(-V(\\cdot)-\\frac{1}{2}\\frac{\\left\\Vert x-e^{-t}\\cdot\\right\\Vert^{2}}{1-e^{-2t}}\\big).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "C.3Proofs of Section 3.2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof of Proposition 3.1. With the score estimator given in Algorithm 2, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\big[\\,\\|\\nabla\\ln p_{t}(X_{t})-s(t,X_{t})\\|^{2}\\,\\big]}\\\\ &{=\\mathbb{E}_{x\\sim p_{t}}\\big[\\|\\nabla\\ln p_{t}(x)-\\frac{1}{n(t)}\\displaystyle\\sum_{i=1}^{n(t)}\\frac{e^{-t}z_{t,i}-x}{1-e^{-2t}}\\|^{2}\\big]}\\\\ &{=\\mathbb{E}_{x\\sim p_{t}}\\big[\\|\\nabla\\ln p_{t}(x)-\\frac{1}{n(t)}\\displaystyle\\sum_{i=1}^{n(t)}\\frac{e^{-t}x_{t,i}-x}{1-e^{-2t}}+\\frac{1}{n(t)}\\displaystyle\\sum_{i=1}^{n(t)}\\frac{x_{t,i}-z_{t,i}}{1-e^{-2t}}\\|^{2}\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\{x_{t,i}\\}_{i=1}^{n(t)}$ is a sequence of id  samples fllowing $p_{0|t}(\\cdot|x)$ that are chosen such that $\\mathbb{E}[\\|x_{t,i}-z_{t,i}\\|^{2}\\,|x]\\,=\\,W_{2}(p_{0|t}(\\cdot|x),\\mathrm{Law}(z_{t,i}))$ for all $t\\,>\\,0$ and $i\\,=\\,1,2,\\cdots\\,,n(t)$ . Based on Lemma l\u2019 $\\Big\\{\\frac{e^{-t}x_{t,i}-x}{1-e^{-2t}}\\Big\\}_{i=1}^{n(t)}$ $\\nabla\\ln p_{t}(x)$ for al $t\\,>\\,0$ and $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ . Therefore, we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\big[\\,\\|\\nabla\\ln p_{t}(X_{t})-s(t,X_{t})\\|^{2}\\,\\big]}\\\\ &{=\\mathbb{E}_{x\\sim p_{t}}\\big[\\|\\nabla\\ln p_{t}(x)-\\frac{1}{n(t)}\\displaystyle\\sum_{i=1}^{n(t)}\\frac{e^{-t}x_{t,i}-x}{1-e^{-2t}}+\\|^{2}\\big]+\\mathbb{E}\\big[\\|\\frac{1}{n(t)}\\displaystyle\\sum_{i=1}^{n(t)}\\frac{x_{t,i}-z_{t,i}}{1-e^{-2t}}\\|^{2}\\big]}\\\\ &{=\\displaystyle\\frac{1}{n(t)^{2}}\\displaystyle\\sum_{i=1}^{n(t)}\\mathbb{E}_{x\\sim p_{t}}\\big[\\|\\frac{e^{-t}x_{t,i}-x}{1-e^{-2t}}-\\mathbb{E}_{x_{t,i}\\sim p_{0|t}(\\cdot|x)}[\\frac{e^{-t}x_{t,i}-x}{1-e^{-2t}}]\\|^{2}\\big]\\big]}\\\\ &{,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n+\\underbrace{\\frac{e^{-2t}}{(1-e^{-2t})^{2}}\\frac{1}{n(t)^{2}}\\sum_{i,j=1}^{n(t)}\\mathbb{E}\\big[\\langle x_{t,i}-z_{t,i},x_{t,j}-z_{t,j}\\rangle\\big]}_{N_{2}}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The first term in the above equation, $N_{1}$ , is related to the covariance of $p_{0|t}\\big(\\cdot|X_{t}\\big)$ , which is studied in Proposition C.2. We have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{N_{1}=\\frac{e^{-2t}}{\\left(1-e^{-2t}\\right)^{2}}\\frac{1}{n(t)^{2}}\\displaystyle\\sum_{i=1}^{n(t)}\\mathbb{E}_{x\\sim p_{t}}\\left[\\mathrm{trace}\\big(\\mathrm{Cov}_{x_{t,i}\\sim p_{0\\mid t}(\\cdot\\,|x)}\\big(x_{t,i}\\rangle\\big)\\right]}\\\\ &{\\quad=\\frac{e^{-2t}}{\\left(1-e^{-2t}\\right)^{2}}\\frac{1}{n(t)}\\mathbb{E}_{x\\sim p_{t}}\\left[\\mathrm{trace}\\big(\\Sigma_{t}(x)\\big)\\right]}\\\\ &{\\quad\\leq\\frac{e^{-2t}}{\\left(1-e^{-2t}\\right)^{2}}\\frac{1}{n(t)}\\mathbb{E}_{x\\sim\\gamma^{d}}\\left[\\mathrm{trace}\\big(\\Sigma_{\\infty}(x)\\big)\\right]}\\\\ &{\\quad=\\frac{e^{-2t}}{\\left(1-e^{-2t}\\right)^{2}}\\frac{1}{n(t)}\\mathrm{Cov}_{p}(x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the inequality follows from Proposition C.2 indicating that $t\\mapsto\\mathbb{E}_{x\\sim p_{t}}\\left[\\mathrm{trace}\\left(\\Sigma_{t}(x)\\right)\\right]$ is a increasing function. ", "page_idx": 17}, {"type": "text", "text": "Thesecond term $N_{2}$ characterize the bias from the Monte Carlo samples and the bias can be measured by the Wasserstein-2 distance: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{N_{2}\\leq\\frac{e^{-2t}}{(1-e^{-2t})^{2}}\\frac{1}{n(t)^{2}}\\sum_{i,j=1}^{n(t)}\\mathbb{E}\\big[\\|x_{t,i}-z_{t,i}\\|^{2}\\big]^{\\frac{1}{2}}\\mathbb{E}\\big[\\|x_{t,j}-z_{t,j}\\|^{2}\\big]^{\\frac{1}{2}}}\\\\ &{\\quad=\\frac{e^{-2t}}{(1-e^{-2t})^{2}}\\frac{1}{n(t)^{2}}\\sum_{i,j=1}^{n(t)}\\mathbb{E}_{x\\sim p_{t}}[W_{2}(\\mathrm{Law}(z_{t,i}),p_{0|t}(\\cdot|x))]\\mathbb{E}_{x\\sim p_{t}}[W_{2}(\\mathrm{Law}(z_{t,j}),p_{0|t}(\\cdot|x))]}\\\\ &{\\quad\\leq\\frac{e^{-2t}}{(1-e^{-2t})^{2}}\\delta(t)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "(4) follows from the estimation on $N_{1}$ and $N_{2}$ ", "page_idx": 17}, {"type": "text", "text": "C.4Proof of Theorem 1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we introduce the proof of our main convergence results, Theorem 1. Recall that in the convergence result in Theorem 1, three types of errors appear in the upper bound: the initialization error, the discretization error and the score estimation error. Our proof compares the trajectory of $\\{\\tilde{X}_{t}\\}_{0\\le t\\le T}$ that solves (8) and the trajectory of $\\{\\bar{X}_{t}\\}_{0\\le t\\le T}$ that solves (2). We denote the path measures of $\\{\\bar{X}_{t}\\}_{0\\le t\\le T}$ and $\\{\\tilde{X}_{t}\\}_{0\\le t\\le T}$ by $P^{p_{T}}$ ,and $Q^{\\gamma^{d}}$ , respectively. Next, we introduce a high level idea on how the three types of errors are handled . ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Initialization error: the initialization error comes from the comparison between $\\{\\tilde{X}_{t}\\}_{0\\le t\\le T}$ and $\\{\\tilde{X}_{t}^{p_{T}}\\}_{0\\le t\\le T}$ . To characterize this error, we introduce the intermediate process $\\{\\tilde{X}_{t}^{p_{T}}\\}_{0\\le t\\le T}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\tilde{X}_{t}^{p r}=(\\tilde{X}_{t}^{p r}+2s(T-t_{k},\\tilde{X}_{t_{k}}^{p r})\\mathrm{d}t+\\sqrt{2}\\mathrm{d}\\tilde{B}_{t},\\quad\\tilde{X}_{0}\\sim p_{T},\\,t\\in[t_{k},t_{k+1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "in (24) and denote the path measure of $\\{\\tilde{X}_{t}^{p_{T}}\\}_{0\\le t\\le T}$ by $Q^{p_{T}}$ Both processes are driven by the estimated scores and only the initial conditions are different. We factor out the initialization error from ${\\mathrm{KL}}(p_{\\delta}|q_{t_{N}})$ by the following argument: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}(p_{\\delta}|q_{t_{N}})=\\mathrm{KL}(p_{\\delta}|q_{T-\\delta})}\\\\ &{\\hphantom{\\mathrm{KL}}\\leq\\int\\ln\\frac{\\mathrm{d}P^{p^{T}}}{\\mathrm{d}Q^{\\gamma^{d}}}\\mathrm{d}P^{p_{T}}=\\int\\ln\\big(\\frac{\\mathrm{d}P^{p^{T}}}{\\mathrm{d}Q^{p_{T}}}\\frac{\\mathrm{d}Q^{p_{T}}}{\\mathrm{d}Q^{\\gamma^{d}}}\\big)\\mathrm{d}P^{p_{T}}}\\\\ &{\\hphantom{\\mathrm{KL}}=\\mathrm{KL}(P^{p_{T}}|Q^{p_{T}})+\\int\\ln\\frac{\\mathrm{d}Q^{p_{T}}}{\\mathrm{d}Q^{\\gamma^{d}}}\\mathrm{d}P^{p_{T}}}\\\\ &{\\hphantom{\\mathrm{KL}}=\\mathrm{KL}(P^{p_{T}}|Q^{p_{T}})+\\mathrm{KL}(p_{T}|\\gamma^{d}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the inequality follows from the data processing inequality and the last identity follows from the fact that dor $\\begin{array}{r}{\\frac{\\dot{\\mathrm{d}}Q^{p_{T}}}{\\mathrm{d}Q^{\\gamma^{d}}}=\\frac{\\mathrm{d}Q_{0}^{p_{T}}}{\\mathrm{d}Q_{0}^{\\gamma^{d}}}=\\frac{\\mathrm{d}p_{T}}{\\mathrm{d}\\gamma^{d}}}\\end{array}$ $\\{\\tilde{X}_{t}\\}_{0\\le t\\le T}$ and $\\{\\tilde{X}_{t}^{p_{T}}\\}_{0\\le t\\le T}$ have the same transition kernel function. $\\mathrm{KL}(p_{T}|\\gamma^{d})$ is the initialization error and it is bounded based on (12) in Proposition C.1. ", "page_idx": 18}, {"type": "text", "text": "2. Discretization error: the dicretization error arises from the evaluations of the scores at the discrete times. We factor out the discretization error from the $\\mathrm{KL}(P^{p_{T}}|Q^{p_{T}})$ via the Girsanov's Theorem. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathrm{KL}(P^{p_{T}}|Q^{p_{T}})\\leq\\sum_{k=0}^{N-1}\\int_{t_{k}}^{t_{k+1}}\\mathbb{E}_{P^{p_{T}}}\\big[\\left\\|\\nabla\\ln p_{T-t}(\\bar{X}_{t})-s(T-t_{k},\\bar{X}_{t_{k}})\\right\\|^{2}\\big]\\mathrm{d}t}}\\\\ &{}&{\\lesssim\\sum_{k=0}^{N-1}\\int_{t_{k}}^{t_{k+1}}\\mathbb{E}_{P^{p_{T}}}\\big[\\left\\|\\nabla\\ln p_{T-t}(\\bar{X}_{t})-\\nabla\\ln p_{T-t_{k}}(\\bar{X}_{t_{k}})\\right\\|^{2}\\big]\\mathrm{d}t}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n+\\sum_{k=0}^{N-1}\\gamma_{k}\\mathbb{E}_{P^{p r}}\\left[\\left\\|\\nabla\\ln p_{T-t_{k}}(\\bar{X}_{t_{k}})-s(T-t_{k},\\bar{X}_{t_{k}})\\right\\|^{2}\\right]\\right]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We bound the discretization error term in the above equation by checking the dynamical properties of the process $\\{\\nabla\\ln p_{t}(\\bar{X}_{t})\\}_{0\\leq t\\leq T}$ . Similar approach was used in the analysis of denoising diffusion models, see [3]. For the sake of completeness, we include the proof in Appendix C.7. ", "page_idx": 18}, {"type": "text", "text": "3. Score estimation error: as discussed in the discretization error, the score estimation error is the accumulation of the $L^{2}$ -error between the true score and score estimator at the time schedules {T - te}k=0- Inthe analysis of denising difusionmdel,[5 4,  itisuually assumed that such a $L^{2}$ score error is small. In this paper, we consider to do sampling via the Reverse OU-process and score estimation. One of our main contribution is that we prove the $L^{2}$ score error can be guaranteed small for the class of Monte Carlo score estimators given in Algorithm 2. The $L^{2}$ score error upper bound is stated in Proposition 3.1. ", "page_idx": 18}, {"type": "text", "text": "Proof of Theorem 1. First we can decompose ${\\mathrm{KL}}(p_{\\delta}|q_{t_{N}})$ into summation of the three types of error. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathrm{KL}(p_{\\delta}|q_{t x})=\\mathrm{KL}(p_{\\delta}|q_{T-\\delta})}}\\\\ &{\\leq\\int\\ln\\frac{\\mathrm{d}P^{n}}{\\mathrm{d}Q^{n^{\\prime}}}\\mathrm{d}P^{p r}=\\int\\ln\\bigl(\\frac{\\mathrm{d}P^{p r}}{\\mathrm{d}Q^{p r}}\\frac{\\mathrm{d}Q^{p r}}{\\mathrm{d}Q^{r^{\\prime}}}\\bigr)\\mathrm{d}P^{p r}}\\\\ &{=\\mathrm{KL}(P^{p r}|Q^{p r})+\\int\\ln\\frac{\\mathrm{d}Q^{p r}}{\\mathrm{d}Q^{p r^{\\prime}}}\\mathrm{d}P^{p r}}\\\\ &{=\\mathrm{KL}(P^{p r}|Q^{p r})+\\mathrm{KL}(P_{T}|^{\\gamma+i})}\\\\ &{\\leq\\mathrm{KL}(p_{T}|\\gamma^{d})+\\displaystyle\\sum_{k=0}^{N-1}\\int_{t_{k}}^{t_{k+1}}\\mathbb{E}_{P^{p r}}\\left[\\left\\|\\nabla\\ln p_{T-t}(\\bar{X}_{t})-s(T-t_{k},\\bar{X}_{t_{k}})\\right\\|^{2}\\right]\\mathrm{d}t}\\\\ &{\\lesssim\\mathrm{KL}(p_{T}|\\gamma^{d})+\\displaystyle\\sum_{k=0}^{N-1}\\int_{t_{k}}^{t_{k+1}}\\mathbb{E}_{P^{p r}}\\left[\\left\\|\\nabla\\ln p_{T-t}(\\bar{X}_{t})-\\nabla\\ln p_{T-t_{k}}(\\bar{X}_{t})\\right\\|^{2}\\right]\\mathrm{d}t}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n+\\sum_{k=0}^{N-1}\\gamma_{k}\\mathbb{E}_{P^{p r}}\\big[\\left\\|\\nabla\\ln p_{T-t_{k}}(\\bar{X}_{t_{k}})-s(T-t_{k},\\bar{X}_{t_{k}})\\right\\|^{2}\\big],\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the first inequality follows from the data processing inequality. The second inequality follows from Girsanov's theorem and [6, Section 3.1]. According to Proposition C.1 and the assumption that $T>1$ , the initialization error satisfies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{KL}(p_{T}|\\gamma^{d})\\le\\frac{1}{2}\\frac{e^{-4T}}{1-e^{-2T}}d+\\frac{1}{2}e^{-2T}\\mathrm{m}_{2}^{2}\\lesssim(d+\\mathrm{m}_{2}^{2})e^{-2T}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "According to Lemma 2, the discretization error satisfies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=0}^{N-1}\\int_{t_{k}}^{t_{k+1}}\\mathbb{E}_{P^{p_{T}}}\\left[\\left\\|\\nabla\\ln p_{T-t}(\\bar{X}_{t})-\\nabla\\ln p_{T-t_{k}}(\\bar{X}_{t_{k}})\\right\\|^{2}\\right]\\mathrm{d}t}\\\\ &{\\lesssim d\\displaystyle\\sum_{k=0}^{N}\\frac{\\gamma_{k}^{2}}{(1-e^{-2(T-t_{k})})^{2}}}\\\\ &{\\displaystyle\\;+\\sum_{k=0}^{N}\\frac{e^{-2(T-t_{k})}\\gamma_{k}}{(1-e^{-2(T-t_{k})})^{2}}\\left(\\mathbb{E}\\bigl[\\mathrm{trace}\\bigl(\\Sigma_{T-t_{k}}(X_{T-t_{k}})\\bigr)\\bigr]-\\mathbb{E}\\bigl[\\mathrm{trace}\\bigl(\\Sigma_{T-t_{k+1}}\\bigl(X_{T-t_{k+1}}\\bigr)\\bigr)\\bigr]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Reordering the summation in the second term and we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=0}^{N}\\frac{e^{-2(T-t_{k})}\\gamma_{k}}{(1-e^{-2(T-t_{k})})^{2}}\\Bigg(\\mathbb{E}\\{\\mathrm{race}(\\sum_{T-t_{k}}(X_{T-t_{k}}))\\}\\Big]-\\mathbb{E}\\big[\\mathrm{race}(\\sum_{T-t_{k+1}}(X_{T-t_{k+1}}))\\big]\\Bigg)}\\\\ &{\\le\\displaystyle\\sum_{k=1}^{N-1}\\Bigg(\\frac{e^{-2(T-t_{k})}\\gamma_{k}}{(1-e^{-2(T-t_{k})})^{2}}-\\frac{e^{-2(T-t_{k-1})}\\gamma_{k-1}}{(1-e^{-2(T-t_{k-1})})^{2}}\\Bigg)\\mathbb{E}\\big[\\mathrm{race}\\big(\\sum_{T-t_{k}}(X_{T-t_{k}})\\big)\\big]}\\\\ &{\\quad+\\displaystyle\\frac{e^{-2T_{\\gamma_{0}}}}{(1-e^{-2T_{\\gamma_{0}}})^{2}}\\mathbb{E}\\big[\\mathrm{race}\\big(\\sum_{T}(X_{T})\\big)\\big]}\\\\ &{\\lesssim\\displaystyle\\sum_{k=1}^{N-1}\\frac{\\gamma_{k}\\gamma_{k-1}^{2}}{(1-e^{-2(T-t_{k})})^{2}(1-e^{-2(T-t_{k-1})})^{2}}\\mathbb{E}\\big[\\mathrm{race}\\big(\\sum_{T-t_{k}}(X_{T-t_{k}})\\big)\\big]}\\\\ &{\\quad+\\displaystyle\\frac{e^{-2T_{\\gamma_{0}}}}{(1-e^{-2T_{\\gamma_{0}}})^{2}}\\mathbb{E}\\big[\\mathrm{race}\\big(\\sum_{T}(X_{T})\\big)\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Recall that $\\Sigma_{t}(X_{t})=\\operatorname{Cov}(X_{0}|X_{t})$ for all $0\\le t\\le T$ Wehave ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\big[\\mathrm{trace}\\big(\\Sigma_{t}(X_{t})\\big)\\big]=\\mathbb{E}\\big[\\mathrm{Cov}(X_{0}|X_{t})\\big]\\leq\\mathbb{E}\\big[\\left\\|X_{0}\\right\\|^{2}\\big]\\leq\\mathbf{m}_{2}^{2},}\\\\ {\\mathrm{and}}&{\\mathbb{E}\\big[\\mathrm{trace}\\big(\\Sigma_{t}(X_{t})\\big)\\big]=\\mathbb{E}\\big[\\mathrm{Cov}(X_{0}|X_{t})\\big]=\\mathbb{E}\\big[\\mathrm{Cov}(X_{0}-e^{t}X_{t}|X_{t})\\big]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathbb{E}\\big[\\mathbb{E}\\big[\\left\\|X_{0}-e^{t}X_{t}\\right\\|^{2}|X_{t}\\big]\\big]}\\\\ &{\\qquad\\qquad\\qquad=(e^{2t}-1)d.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last identity follows from (13). (29) and (28) implies that $\\mathbb{E}\\big[\\mathrm{trace}\\big(\\Sigma_{t}(X_{t})\\big)\\big]\\lesssim(1-e^{-2t})(d+\\mathrm{m}_{2}^{2})$ for all $0\\le t\\le T$ . Therefore, from (26) and (27), the overall discretization error can be bounded as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=0}^{N-1}\\int_{t_{k}}^{t_{k+1}}\\mathbb{E}_{P^{p}T}\\left[\\left\\|\\nabla\\ln p_{T-t}(\\bar{X}_{t})-\\nabla\\ln p_{T-t_{k}}(\\bar{X}_{t_{k}})\\right\\|^{2}\\right]\\mathrm{d}t}\\\\ &{\\displaystyle\\lesssim\\sum_{k=0}^{N-1}\\frac{d\\gamma_{k}^{2}}{(1-e^{-2(T-t_{k})})^{2}}+\\displaystyle\\sum_{k=1}^{N-1}\\frac{(d+\\operatorname{m}_{2}^{2})\\gamma_{k}\\gamma_{k-1}^{2}}{(1-e^{-2(T-t_{k})})(1-e^{-2(T-t_{k-1})})^{2}}+\\frac{\\operatorname{m}_{2}^{2}e^{-2T}\\gamma_{0}}{(1-e^{-2T})^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Last, according to Proposition 3.1, the score estimation error satisfies ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=0}^{N-1}\\gamma_{k}\\mathbb{E}_{P^{p_{T}}}\\left[\\left\\Vert\\nabla\\ln p_{T-t_{k}}(\\bar{X}_{t_{k}})-s(T-t_{k},\\bar{X}_{t_{k}})\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\displaystyle\\sum_{k=0}^{N-1}\\frac{\\gamma_{k}e^{-2(T-t_{k})}}{(1-e^{-2(T-t_{k})})^{2}}\\Bigg(\\delta(T-t_{k})^{2}+\\frac{\\,\\mathrm{m}_{2}^{2}}{n(T-t_{k})}\\Bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and (9) follows from (25), (30) and (31). ", "page_idx": 20}, {"type": "text", "text": "C.5  Discussion on the Step-size ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we first state error bounds of DDMC under different choices of step-size. Then we provide the detailed calculations. Last we compare our results in Theorem 1 to existing results on convergence of denoising diffusion models. ", "page_idx": 20}, {"type": "text", "text": "In the following discussion, we assume $\\delta(T-t_{k})^{2}\\leq d\\gamma_{k}e^{2(T-t_{k})}$ and $n(T-t_{k})\\geq\\gamma_{k}^{-1}e^{-2(T-t_{k})}$ for all $k=0,1,\\cdots\\,,N-1$ , so that the score estimation error is dominated by the discretization error. For different choices of step-size, we discuss the parameter dependence of the error bound in (9) under the assumptions on $\\delta(t)$ and $n(t)$ ", "page_idx": 20}, {"type": "text", "text": "1. constant step-size: the constant step-size is widely considered in sampling algorithms and denoising diffusion generative models. It requires $\\gamma_{k}=\\gamma$ for all $0\\leq k\\leq N-1$ .Then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{KL}(p_{\\delta}|q_{t_{N}})\\lesssim(d+\\mathrm{m}_{2}^{2})e^{-2T}+\\frac{(d+\\mathrm{m}_{2}^{2})T^{2}}{N^{2}}(T+\\delta^{-2})+\\frac{d T}{N}(T+\\delta^{-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "2. linear step-size: the linear step-size is considered by [6] as an interpretation of the uniform discretization of a diffusion model with non-constant diffusion coefficient [52]. It requires $t_{k}=T-(\\delta+(N-k)\\gamma)^{2}$ With $\\begin{array}{r}{\\gamma=\\frac{\\sqrt{T}-\\delta}{N}}\\end{array}$ VT- for all 0 \u2264k \u2264 N - 1. Then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{KL}(p_{\\delta}|q_{t_{N}})\\lesssim(d+\\operatorname*{m}_{2}^{2})e^{-2T}+\\frac{(d+\\operatorname*{m}_{2}^{2})T}{N^{2}}(T^{2}+\\delta^{-1})+\\frac{d T^{\\frac12}}{N}(T^{\\frac32}+\\delta^{-\\frac12}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "3. exponential-decay step-size: the exponential-decay step-size is considered to be optimal in SGMs [6, 3]. It requires $\\gamma_{k}=\\kappa\\operatorname*{min}(1,T-t_{k})$ for some $\\kappa\\in(0,1)$ .Then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{KL}(p_{\\delta}|q_{t_{N}})\\lesssim(d+\\operatorname*{m}_{2}^{2})e^{-2T}+\\frac{(d+\\operatorname*{m}_{2}^{2})}{N^{2}}\\big(T+\\ln(\\frac{1}{\\delta})\\big)^{3}+\\frac{d}{N}\\big(T+\\ln(\\frac{1}{\\delta})\\big)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The purple terms are denoting the discretization errors. For all of the above choices of step-size, the error bounds have the same linear dimension dependence and different dependence on the earlystoppingparameter $\\delta$ . Next, we provide a detailed calculation of these error bounds and a derivation of optimal $\\delta$ -dependence. ", "page_idx": 20}, {"type": "text", "text": "1. constant step-size: when $\\gamma_{k}=\\gamma$ for all $k=0,1,\\cdots,N-1$ , we have $T-t_{k}=\\delta+(N-k)\\gamma$ and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\gamma_{k}}{1-e^{-2(T-t_{k})}}=\\left\\{\\Theta(\\frac{\\gamma}{T-t_{k}}),\\right.\\ \\ \\ \\ \\mathrm{if}\\,T-t_{k}>1,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{k=1}^{N-1}\\frac{(d+\\operatorname*{m}_{2}^{2})\\gamma_{k}\\gamma_{k-1}^{2}}{(1-e^{-2(T-t_{k})})(1-e^{-2(T-t_{k-1})})^{2}}+\\sum_{k=0}^{N-1}\\frac{d\\gamma_{k}^{2}}{(1-e^{-2(T-t_{k})})^{2}}}\\\\ &{=\\Theta\\bigg(\\displaystyle\\sum_{1<T-t_{k}<T}(d+\\operatorname*{m}_{2}^{2})\\gamma^{3}+d\\gamma^{2}+\\sum_{\\delta<T-t_{k}<1}^{}\\frac{(d+\\operatorname*{m}_{2}^{2})\\gamma^{3}}{(T-t_{k})(T-t_{k-1}^{2})}+\\frac{d\\gamma^{2}}{(T-t_{k})^{2}}\\bigg)}\\\\ &{=\\Theta\\bigg(\\frac{(d+\\operatorname*{m}_{2}^{2})T^{3}}{N^{2}}+\\frac{d T^{2}}{N}+(d+\\operatorname*{m}_{2}^{2})\\gamma^{2}\\int_{\\delta}^{1}t^{-3}\\mathrm{d}t+d\\gamma\\displaystyle\\int_{\\delta}^{1}t^{-2}\\mathrm{d}t\\bigg)}\\\\ &{=\\Theta\\bigg(\\frac{(d+\\operatorname*{m}_{2}^{2})T^{3}}{N^{2}}+\\frac{d T^{2}}{N}+\\frac{(d+\\operatorname*{m}_{2}^{2})T^{2}}{N^{2}\\delta^{2}}+\\frac{d T}{N\\delta}\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "2. linear step-size: when $T-t_{k}=(\\delta+(N-k)\\gamma)^{2}$ with $\\begin{array}{r}{\\gamma=\\frac{\\sqrt{T}-\\delta}{N}}\\end{array}$ , we have $\\gamma_{k}=(2\\delta+(2N-$ $2k-1)\\gamma)\\gamma=\\Theta(\\sqrt{T-t_{k}}\\gamma)$ and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\gamma_{k}}{1-e^{-2\\left(T-t_{k}\\right)}}=\\left\\{\\begin{array}{l l}{\\Theta(\\gamma\\sqrt{T-t_{k}}),}&{\\quad\\mathrm{if~}T-t_{k}>1,}\\\\ {\\Theta(\\frac{\\gamma}{\\sqrt{T-t_{k}}}),}&{\\quad\\mathrm{if~}T-t_{k}<1.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=1}^{N-1}\\frac{(d+\\operatorname*{m}_{2}^{2})\\gamma_{k}\\gamma_{k-1}^{2}}{(1-e^{-2(T-t_{k})})(1-e^{-2(T-t_{k-1})})^{2}}+\\sum_{k=0}^{N-1}\\frac{d\\gamma_{k}^{2}}{(1-e^{-2(T-t_{k})})^{2}}}\\\\ &{=\\Theta\\Bigg(\\sum_{1<T-t_{k}<T}(d+\\operatorname*{m}_{2}^{2})\\gamma^{3}\\sqrt{T-t_{k}}(T-t_{k-1})+d\\gamma^{2}(T-t_{k-1})\\Bigg)}\\\\ &{\\quad+\\Theta\\Bigg(\\underset{\\delta<T-t_{k}<1}{\\sum_{k\\leq T-t_{k}<1}}\\frac{(d+\\operatorname*{m}_{2}^{2})\\gamma^{3}}{\\sqrt{T-t_{k}}(T-t_{k-1})}+\\frac{d\\gamma^{2}}{T-t_{k}}\\Bigg)}\\\\ &{=\\Theta\\Bigg(\\gamma^{2}(d+\\operatorname*{m}_{2}^{2})\\int_{1}^{T}t\\mathrm{d}t+\\gamma d\\int_{1}^{T}t^{\\frac{1}{2}}\\mathrm{d}t+\\gamma^{2}(d+\\operatorname*{m}_{2}^{2})\\int_{\\delta}^{1}t^{-2}\\mathrm{d}t+\\gamma d\\int_{\\delta}^{1}t^{-\\frac{3}{2}}\\mathrm{d}t\\Bigg)}\\\\ &{=\\Theta\\Big(\\frac{(d+\\operatorname*{m}_{2}^{2})T^{3}}{N^{2}}+\\frac{d T^{2}}{N}+\\frac{(d+\\operatorname*{m}_{2}^{2})T}{N^{2}\\delta}+\\frac{d T^{\\frac{1}{2}}}{N\\delta^{\\frac{3}{2}}}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "3. exponential-decay step-size: when $\\gamma_{k}=\\kappa\\operatorname*{min}(1,T-t_{k})$ with T+ln(1/8) , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\gamma_{k}}{1-e^{-2\\left(T-t_{k}\\right)}}=\\left\\{\\Theta(\\kappa),\\qquad\\mathrm{if}\\;T-t_{k}>1,\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=1}^{N-1}\\frac{(d+\\operatorname*{m}_{2}^{2})\\gamma_{k}\\gamma_{k-1}^{2}}{(1-e^{-2(T-t_{k})})(1-e^{-2(T-t_{k-1})})^{2}}+\\sum_{k=0}^{N-1}\\frac{d\\gamma_{k}^{2}}{(1-e^{-2(T-t_{k})})^{2}}}\\\\ &{=\\Theta\\bigg(\\displaystyle\\sum_{1<T-t_{k}<T}(d+\\operatorname*{m}_{2}^{2})\\kappa^{3}+d\\kappa^{2}+\\displaystyle\\sum_{\\delta<T-t_{k}<1}(d+\\operatorname*{m}_{2}^{2})\\kappa^{2}\\gamma_{k}(T-t_{k})^{-1}+d\\kappa\\gamma_{k}(T-t_{k})^{-1}\\bigg)}\\\\ &{=\\Theta\\bigg((d+\\operatorname*{m}_{2}^{2})\\kappa^{3}N+d\\kappa^{2}N+\\kappa^{2}(d+\\operatorname*{m}_{2}^{2})\\displaystyle\\int_{\\delta}^{1}t^{-1}\\mathrm{d}t+\\kappa d\\displaystyle\\int_{\\delta}^{1}t^{-1}\\mathrm{d}t\\bigg)}\\\\ &{=\\Theta\\bigg(\\displaystyle\\frac{(d+\\operatorname*{m}_{2}^{2})(T+\\ln(1/\\delta))^{3}}{N^{2}}+\\frac{d(T+\\ln(1/\\delta))^{2}}{N}\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "4. Optimality of the exponential step-size: assuming that $\\gamma_{k}=\\Theta\\big(\\gamma_{k-1}\\big)$ for all $k=0,1,\\cdots\\,,N-1$ the exponential step-size actually provides the optimal order estimation for the error terms. Noticing that the error terms al depend on thequanity1-(r-te) whichisoforder ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\gamma_{k}}{1-e^{-2(T-t_{k})}}=\\left\\{\\Theta(\\frac{\\gamma_{k}}{T-t_{k}}),\\right.\\qquad\\mathrm{if}\\:T-t_{k}>1,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=1}^{N-1}\\frac{(d+\\mathfrak{m}_{2}^{2})\\gamma_{k}\\gamma_{k-1}^{2}}{(1-e^{-2(T-t_{k})})(1-e^{-2(T-t_{k-1})})^{2}}+\\displaystyle\\sum_{k=0}^{N-1}\\frac{d\\gamma_{k}^{2}}{(1-e^{-2(T-t_{k})})^{2}}}\\\\ &{=\\Theta\\bigg(\\displaystyle\\sum_{1<T-t_{k}<T}(d+\\mathfrak{m}_{2}^{2})\\gamma_{k}^{3}+d\\gamma_{k}^{2}+\\displaystyle\\sum_{\\delta<T-t_{k}<1}\\frac{(d+\\mathfrak{m}_{2}^{2})\\gamma_{k}^{3}}{(T-t_{k})^{3}}+\\displaystyle\\frac{d\\gamma_{k}^{2}}{(T-t_{k})^{2}}\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Noticing that $x\\ \\mapsto\\ x^{2}$ and $x\\ \\mapsto\\ x^{3}$ are both convex functions on the domain $x~\\in~(0,\\infty)$ . Since   \n$\\begin{array}{r}{\\sum_{1<T-t_{k}<T}\\gamma_{k}=T-1}\\end{array}$ is fixed, according to Jensen's inequality, $\\scriptstyle\\sum_{1<T-t_{k}<T}\\gamma_{k}^{2}$ and $\\scriptstyle\\sum_{1<T-t_{k}<T}\\gamma_{k}^{3}$   \nreach their minimum when $\\gamma_{k}$ are constant-valued for all $k$ such that $T\\mathrm{~-~}t_{k}\\mathrm{~>~1~}$ Similarly, let (T-tk ) \u2208 (0, oo). Then t $\\begin{array}{r}{\\frac{\\gamma_{k}}{T-t_{k}}=1-e^{-\\beta_{k}}}\\end{array}$ and $\\begin{array}{r}{\\sum_{\\delta<T-t_{k}<1}\\beta_{k}=\\ln(1/\\delta)}\\end{array}$ is fixed. Since   \n$x\\mapsto(1-e^{-x})^{2}$ and $x\\mapsto(1-e^{-x})^{3}$ are both convex functions on the domain $x\\in(0,\\infty)$ , according $\\scriptstyle\\sum_{\\delta<T-t_{k}<1}{\\frac{\\gamma_{k}^{2}}{(T-t_{k})^{2}}}$ m. $\\begin{array}{r}{\\sum_{\\delta<T-t_{k}<1}\\frac{\\gamma_{k}^{3}}{(T-t_{k})^{3}}}\\end{array}$ $\\beta_{k}$ $k$ $T-t_{k}<1$ ", "page_idx": 22}, {"type": "text", "text": "Comparison to convergence results in denoising diffusion models. (9) in Theorem 1 bounds the error of DDMC by I, II, MI, which reflect the initialization error, the discretization error and the score estimation error, respectively. Assuming the score estimation error is small, (9) reduces to the same type of results that study the error bound for the denoising diffusion models (Algorithm 1). In [5], the discretization error is proved to be of order $O(d)$ assuming the score function is smooth along the trajectory and a constant step-size. In [4], they get rid of the trajectory smoothness assumption and prove a $\\nabla(d^{2})$ discretization error bound with early-stopping. In [3], the discretization error bound is improved to $O(d)$ with early-stopping and exponential-decay step-size, and without the trajectory smoothness assumption. Compared to these works, our result in Theorem 1 also implies a $O(d)$ discretization error without the trajectory smoothness assumption and it applies to any choice of step-size with early stopping, as we discussed above. As shown in [3], the $O(d)$ is the optimal for the discretization error. Therefore, our results indicates that with early-stopping, the denoising diffusion model achieves the optimal linear dimension dependent error bound. ", "page_idx": 22}, {"type": "text", "text": "C.6Proofs of Section 3.3 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "To prove the query complexity of ZOD-MC, we first look at query complexity of Algorithm 3 ", "page_idx": 22}, {"type": "text", "text": "Query complexity of Algorithm 3. The query complexity of Algorithm 3 is essentially the number of proposals we need so that $n(t)$ of them can be accepted. Intuitively, to get one sample accepted, the number of proposals we need is geometrically distributed with certain acceptance probability [9]. We state this formally in the following proposition, for which it suffices to assume a relaxation of the commonly used gradient-Lipschitz condition on the potential. ", "page_idx": 22}, {"type": "text", "text": "Proposition C.3. Under Assumption 3.2, the expected number of proposals for obtaining $n(t)$ many exactsamplesfrom $p_{0|t}(\\cdot|x)$ definedinLemma $^{\\,l}$ via Algorithm 3, is ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{N(t)=n(t)\\bigg(\\big(L(e^{2t}-1)+1\\big)^{\\frac{d}{2}}\\exp\\big(\\frac{1}{2}\\frac{\\|L x^{*}-e^{t}x\\|^{2}}{L(e^{2t}-1)+1}\\big)\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Remark 5. Our complexity bound in Proposition C.3 exponentially depends on the dimension. This is due the curse of dimensionality phenomenon in the rejection sampling: the acceptance rate and algorithmefficiencydecreasessignificantlywhen the dimensionincreases. ", "page_idx": 22}, {"type": "text", "text": "Proof of Proposition C.3. For each $t\\,\\in\\,[0,T]$ , the expected number of iterations in the rejection sampling to get one accepted sample is ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M(t)=\\bigg(\\displaystyle\\int_{\\mathbb R^{d}}e^{-V(z)+V(x^{*})}\\mathcal N\\big(z;e^{t}x,(e^{2t}-1)I_{d}\\big)\\mathrm d z\\bigg)^{-1}}\\\\ &{\\qquad\\le\\bigg(\\displaystyle\\int_{\\mathbb R^{d}}\\exp\\big(-\\frac{L}{2}\\,\\|z-x^{*}\\|^{2}\\big)\\mathcal N\\big(z;e^{t}x,(e^{2t}-1)I_{d}\\big)\\mathrm d z\\bigg)^{-1}}\\\\ &{\\qquad=\\big(L(e^{2t}-1)+1\\big)^{\\frac{d}{2}}\\exp\\big(\\frac{1}{2}\\frac{\\|L x^{*}-e^{t}x\\|^{2}}{L(e^{2t}-1)+1}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "To get $n(t)$ many samples, the expected number of iterations we need is $N(t)=n(t)M(t)$ ", "page_idx": 22}, {"type": "text", "text": "Query complexity of ZOD-MC. The query complexity of ZOD-MC, denoted as $\\tilde{N}$ is essentially the sum of query complexities in Proposition C.3 over the discretized time points, i.e. $\\tilde{N}\\,=$ $\\begin{array}{r}{\\sum_{k=0}^{N-1}N(T-t_{k})}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "Proof of Corollary .1 First, for $\\begin{array}{r}{\\delta\\;=\\;\\Theta\\big(\\operatorname*{min}(\\frac{\\varepsilon_{\\mathrm{w_{2}}}^{2}}{d},\\frac{\\varepsilon_{\\mathrm{w_{2}}}}{\\ m_{2}})\\big)}\\end{array}$ itfllows frmPropsition C.1 that $W_{2}(p,p_{\\delta})\\leq\\varepsilon_{\\mathrm{W}_{2}}$ ", "page_idx": 23}, {"type": "text", "text": "Next, under the exponential-decay step size, according to Theorem 1, if we set $n(T\\mathrm{~-~}t_{k})\\;=\\;$ $\\gamma_{k}^{-1}e^{-2(T-t_{k})}$ ,then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{KL}(p_{\\delta}|q_{t_{N}})\\lesssim(d+\\operatorname*{m}_{2}^{2})e^{-2T}+\\frac{(d+\\operatorname*{m}_{2}^{2})}{N^{2}}\\big(T+\\ln(\\frac{1}{\\delta})\\big)^{3}+\\frac{d}{N}\\big(T+\\ln(\\frac{1}{\\delta})\\big)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By choosing T = ln(dm $\\begin{array}{r}{T\\;=\\;\\frac{1}{2}\\ln(\\frac{d+\\mathfrak{m}_{2}^{2}}{\\varepsilon_{\\mathrm{KL}}}),\\;N\\;=\\;\\Theta\\bigl(\\operatorname*{max}(\\frac{(d+\\mathfrak{m}_{2}^{2})^{\\frac{1}{2}}(T+\\ln(\\delta^{-1}))^{\\frac{3}{2}}}{\\varepsilon_{\\mathrm{KL}}^{\\frac{1}{2}}},\\frac{d(T+\\ln(\\delta^{-1}))^{2}}{\\varepsilon_{\\mathrm{KL}}})\\bigr)}\\end{array}$ and $\\kappa\\,=$ $\\Theta\\Big(\\frac{T\\!+\\!\\ln(\\delta^{-1})}{N}\\Big)$ w have $\\mathrm{KL}(p_{\\delta}|q_{t_{N}})\\lesssim\\varepsilon_{\\mathrm{KL}}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\tilde{N}\\leq\\sum_{k=0}^{N-1}\\gamma_{k}^{-1}e^{-2(T-t_{k})}\\Biggl(\\left(L\\bigl(e^{2(T-t_{k})}-1\\bigr)+1\\right)^{\\frac{d}{2}}\\exp\\Big(\\frac{1}{2}\\frac{\\|L x^{*}-e^{T-t_{k}}x_{k}\\|^{2}}{L(e^{2(T-t_{k})}-1)+1}\\Big)\\Biggr).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By plugging in $\\delta,T,t_{k}$ and $N$ , (10) is proved. ", "page_idx": 23}, {"type": "text", "text": "C.7 Side Lemmas ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Lemma 2. Let $\\{\\bar{X}_{t}\\}_{0\\le t\\le T}$ be the solution to (2). Then for any $0\\leq k\\leq N-1$ we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\int_{t_{k}}^{t_{k+1}}\\mathbb{E}\\big[\\left\\|\\nabla\\ln p_{T-t}(\\bar{X}_{t})-\\nabla\\ln p_{T-t_{k}}(\\bar{X}_{t_{k}})\\right\\|^{2}\\big]\\mathrm{d}t}\\\\ &{\\lesssim\\frac{d\\gamma_{k}^{2}}{(1-e^{-2(T-t_{k})})^{2}}+\\frac{e^{-2(T-t_{k})}\\gamma_{k}}{(1-e^{-2(T-t_{k})})^{2}}\\Bigg(\\mathbb{E}\\big[t r a c e\\big(\\Sigma_{T-t_{k}}(X_{T-t_{k}})\\big)\\big]-\\mathbb{E}\\big[t r a c e\\big(\\Sigma_{T-t_{k+1}}(X_{T-t_{k+1}})\\big)\\big]\\Bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof of Lemma 2. For fixed $s$ , consider the process $\\{\\nabla\\ln p_{T-t}(\\bar{X}_{t})\\}_{0\\leq t\\leq T}$ , denoted as $\\{L_{t}\\}_{0\\le t\\le T}$ and a function $E_{s,t}:=\\mathbb{E}[\\left\\Vert L_{t}-L_{s}\\right\\Vert^{2}]$ It is shown by Ito's formula in [3, Lemma 3] that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{d}L_{t}=-L_{t}\\mathrm{d}t+\\sqrt{2}\\nabla^{2}\\ln q_{T-t}(\\bar{X}_{t})\\mathrm{d}\\bar{B}_{t},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and as a result, (32) implies that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}E_{s,t}=-2E_{s,t}\\mathrm{d}t-2\\mathbb{E}\\big[\\langle L_{t}-L_{s},L_{s}\\rangle\\big]\\mathrm{d}t+2\\mathbb{E}\\big[\\left\\|\\nabla^{2}\\ln p_{T-t}(\\bar{X}_{t})\\right\\|_{F}^{2}\\big]\\mathrm{d}t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Apply (32) and Ito's formula again, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\mathbb{E}\\left[\\langle L_{t},L_{s}\\rangle\\right]=-\\mathbb{E}\\big[\\langle L_{t},L_{s}\\rangle\\big]\\mathrm{d}t\\quad\\implies\\quad\\mathbb{E}\\big[\\langle L_{t},L_{s}\\rangle\\big]=e^{-(t-s)}\\mathbb{E}\\big[\\left\\|L_{s}\\right\\|^{2}\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore (33) can be rewritten as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}E_{s,t}=-2E_{s,t}+2(1-e^{-(t-s)})\\mathbb{E}\\big[\\left\\|L_{s}\\right\\|^{2}\\big]+2\\mathbb{E}\\big[\\left\\|\\nabla^{2}\\ln p_{T-t}(\\bar{X}_{t})\\right\\|_{F}^{2}\\big].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let $\\{X_{t}\\}_{0\\le t\\le T}$ be the solution of (1). Since $\\bar{X}_{t}=X_{T-t}$ in distribution for all $t\\in[0,T],\\mathbb{E}\\big[\\left\\|L_{s}\\right\\|^{2}\\big]$ and $\\mathbb{E}\\big[\\left\\|\\nabla^{2}\\ln p_{T-t}(\\bar{X}_{t})\\right\\|_{F}^{2}\\big]$ can bothbe representedby the covariance matrix dfnedin Proposition C.2. It is proved in [3, Lemma 6] that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\big[\\left\\Vert L_{s}\\right\\Vert^{2}\\big]=\\mathbb{E}\\big[\\left\\Vert\\nabla\\ln p_{T-s}(\\bar{X}_{s})\\right\\Vert^{2}\\big]}\\\\ &{\\qquad\\qquad=\\frac{d}{1-e^{-2(T-s)}}-\\frac{e^{-2(T-s)}}{(1-e^{-2(T-s)})^{2}}\\mathbb{E}\\big[\\mathrm{trace}\\big(\\Sigma_{T-s}(X_{T-s})\\big)\\big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and ", "text_level": 1, "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}\\big[\\left\\|\\nabla^{2}\\ln p_{T-t}(\\bar{X}_{t})\\right\\|_{F}^{2}\\big]=\\frac{d}{(1-e^{-2(T-t)})^{2}}-\\frac{e^{-2(T-t)}}{2(1-e^{-2(T-t)})^{2}}\\frac{\\mathrm{d}}{\\mathrm{d}t}\\bigg(\\mathbb{E}\\big[\\mathrm{trace}\\big(\\Sigma_{T-t}(X_{T-t})\\big)\\big]\\bigg)}}\\\\ &{}&{-\\;\\frac{2e^{-2(T-t)}}{(1-e^{-2(T-t)})^{3}}\\mathbb{E}\\big[\\mathrm{trace}\\big(\\Sigma_{T-t}(X_{T-t})\\big)\\big].}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now we choose $s=t_{k}$ in (34) and integrate from $t_{k}$ to $t$ .According to (35) and (36), we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{1}{2}\\sigma^{2i}E_{k_{x},k_{x+1}}=\\int_{\\tilde{t}_{k_{x}}}^{t_{k_{x}}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last identity follows from integration by parts.According to Proposition C.2, $t\\quad\\mapsto$ $\\mathbb{E}\\big[\\mathrm{trace}\\big(\\Sigma_{T-t}(X_{T-t})\\big)\\big]$ is positive and decreasing. Therefore, we have for all $i\\in[t_{k},t_{k+1}]$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{t_{k},t}\\leq d\\bigg(\\frac{1+e^{-2(t-t_{k})}-2e^{-(t-t_{k})}}{1-e^{-2(T-t_{k})}}+\\frac{1-e^{-2(t-t_{k})}}{(1-e^{-2(T-t_{k})})\\left(1-e^{-2(T-t)}\\right)}\\bigg)}\\\\ &{\\qquad+\\,2\\frac{e^{-2(T-t_{k})}}{(1-e^{-2(T-t_{k})})^{2}}\\big(2e^{-(t-t_{k})}-1\\big)\\mathbb{E}\\big[\\mathrm{trace}\\big(\\Sigma_{T-t_{k}}\\big(X_{T-t_{k}}\\big)\\big)\\big]}\\\\ &{\\qquad-\\,\\bigg(\\frac{2e^{-2(T-t)}}{(1-e^{-2(T-t)})^{2}}+2\\int_{t_{k}}^{t}\\frac{e^{-2(T-t_{k})-2(t-u)}}{\\left(1-e^{-2(T-t_{k})}\\right)^{2}}\\mathrm{d}u\\bigg)\\mathbb{E}\\big[\\mathrm{trace}\\big(\\Sigma_{T-t}\\big(X_{T-t}\\big)\\big)\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Integrate again from $t=t_{k}$ to $t=t_{k+1}$ , we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{t_{k}}^{t_{k+1}}E_{t_{k},t}\\mathrm{d}t}\\\\ &{\\leq\\frac{d\\gamma_{k}^{3}}{1-e^{-2(T-t_{k})}}+\\frac{2d\\gamma_{k}^{2}}{(1-e^{-2(T-t_{k})})^{2}}+\\frac{2e^{-2(T-t_{k})}}{(1-e^{-2(T-t_{k})})^{2}}\\bigl(2-2e^{-\\gamma_{k}}-\\gamma_{k}\\bigr)\\mathbb{E}\\bigl[\\mathrm{trace}\\bigl(\\Sigma_{T-t_{k}}(X_{T-t_{k}})\\bigr)\\bigr]}\\\\ &{\\quad-\\,\\frac{e^{-2(T-t_{k})}}{(1-e^{-2(T-t_{k})})^{2}}\\bigl(3\\gamma_{k}+\\frac{1}{2}e^{-\\gamma_{k}}-\\frac{1}{2}\\bigr)\\mathbb{E}\\bigl[\\mathrm{trace}\\bigl(\\Sigma_{T-t_{k+1}}(X_{T-t_{k+1}})\\bigr)\\bigr]}\\\\ &{\\leq\\frac{3d\\gamma_{k}^{2}}{(1-e^{-2(T-t_{k})})^{2}}+\\frac{2e^{-2(T-t_{k})}\\gamma_{k}}{(1-e^{-2(T-t_{k})})^{2}}\\bigl(\\mathbb{E}\\bigl[\\mathrm{trace}\\bigl(\\Sigma_{T-t_{k}}(X_{T-t_{k}})\\bigr)\\bigr]-\\mathbb{E}\\bigl[\\mathrm{trace}\\bigl(\\Sigma_{T-t_{k+1}}(X_{T-t_{k+1}})\\bigr)\\bigr]\\bigr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "image", "img_path": "X3Aljulsw5/tmp/8af3c1108f4dfc96240742f9502e96f9bf6ba3bbd3d8b9ae80e24b96dbf704f2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 7: Wall clock of different methods as a function of oracle complexity for the 2D GMM in the main paper ", "page_idx": 25}, {"type": "text", "text": "D More Experiments ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "D.1  Samples from 2D GMM at different Oracle Complexities ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We sample from a Gaussian Mixture model with 4 modes, the following summarizes the parameters oftheGMM. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Weights},\\,w:\\left[0.1\\quad0.2\\quad0.3\\quad0.4\\right],}\\\\ &{\\mathrm{Means},\\,\\mu_{k}:\\left[0\\right],\\,\\left[0\\right],\\,\\left[0\\right],\\,\\left[9\\right],\\,,}\\\\ &{\\mathrm{Covariances},\\,\\Sigma_{k}:\\left[0.1\\quad0.5\\right],\\,\\left[\\!\\!\\begin{array}{l l l}{0.3}&{-0.2}\\\\ {-0.2}&{0.3}\\end{array}\\!\\!\\right],\\,\\left[\\!\\!\\begin{array}{l l l}{1}&{0.3}\\\\ {0.3}&{1}\\end{array}\\!\\!\\right],\\,\\left[\\!\\!\\begin{array}{l l l}{1.2}&{-1\\right].}\\\\ {-1}&{1.2\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We display the generated samples at different oracle complexities in Figures 8, 9, 10. Notice that the mode located at the origin holds less weight, and as the oracle complexity increases our method becomes better at sampling from other modes, as opposed to the corresponding baselines. ", "page_idx": 25}, {"type": "text", "text": "We detail the hyperparameters in Table 2 and the wall clock time of different methods in Figure 15. ", "page_idx": 25}, {"type": "table", "img_path": "X3Aljulsw5/tmp/a5308ee5173cf13cbaecbcd4f7fd741d181a9de0e948f1aaa776e1a41704e167.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Table 2: Hyperparameters for Various Methods for the 2D GMM experiment. $\\overline{{K}}$ means the current oracle complexity and $M$ refers to a matched oracle complexity. For RSDMC we used 2 recursions per score evaluation. ", "page_idx": 25}, {"type": "image", "img_path": "X3Aljulsw5/tmp/864bd9a337e2e69b343ed739e73a47801b024b05b26c35e4be69ce26e95a4d5a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "(c) Generated samples at 2200 oracle complexity per score evaluation Figure 8: Generated Samples for GMM at different oracle complexity ", "page_idx": 26}, {"type": "image", "img_path": "X3Aljulsw5/tmp/74b1e67183a13dbfba3f9647a95650a7614d037891169e2280ecd91d3f0d04a3.jpg", "img_caption": ["(d) Generated samples at 6200 oracle complexity per score evaluation "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 9: Generated Samples for GMM at different oracle complexity ", "page_idx": 26}, {"type": "image", "img_path": "X3Aljulsw5/tmp/a4fb9ddf18cebb9be7660a49cba8c837855fa4797c61df498eab8bc1afc36de3.jpg", "img_caption": ["(c) Generated samples at 9200 oracle complexity per score evaluation "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 10: Generated Samples for GMM at different oracle complexity ", "page_idx": 27}, {"type": "text", "text": "D.2 Samples from Discontinuous 2D GMM at different Oracle Complexities ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We display the generated samples at different oracle complexities in Figures 11 ,12, 13. At the end weshowthe $W_{2}$ and MMD for this example in Figure 14. ", "page_idx": 28}, {"type": "table", "img_path": "X3Aljulsw5/tmp/048f455d58b9d7b77b10ac8cf6f2677270dc8520a1f792d7107de71d155ff8ca.jpg", "table_caption": ["We detail the hyperparameters in Table 3 . "], "table_footnote": ["Table 3: Hyperparameters for Various Methods for the 2D Discontinuous GMM experiment. $K$ means the current oracle complexity and $M$ refers to a matched oracle complexity. For RSDMC we used 2 recursions per score evaluation. "], "page_idx": 28}, {"type": "image", "img_path": "X3Aljulsw5/tmp/313156aa18d1596235b65443c37aacb3bfef4e374bf6c1d9011984960ed4ecb7.jpg", "img_caption": ["Figure 11: Generated Samples for discontinuous GMM at different oracle complexity "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "X3Aljulsw5/tmp/70109c5d2c6d5764c7eef3e11e85ed0f41157df6ddaa1643fd9ee7ed8beab423.jpg", "img_caption": ["(d) Generated samples at 6200 oracle complexity per score evaluation "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 12: Generated Samples for discontinuous GMM at different oracle complexity ", "page_idx": 29}, {"type": "image", "img_path": "X3Aljulsw5/tmp/e8e5f1813740a4c152c1993ad6f0f6f2d77c8eb30a33cbd1f778535d3329d6f1.jpg", "img_caption": ["(c) Generated samples at 9200 oracle complexity per score evaluation "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 13: Generated Samples for discontinuous GMM at different oracle complexity ", "page_idx": 29}, {"type": "image", "img_path": "X3Aljulsw5/tmp/74e5251d9ce94fb57769b623144a8ea6ba474d02f50c92538121583be937258a.jpg", "img_caption": ["Figure 14: $W_{2}$ and MMD at different oracle complexities for discontinuous potential "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "X3Aljulsw5/tmp/a666ba96ba7bae589e3c0bb3bea872231f1e07137974c9f2c74d2b43b8ec512b.jpg", "img_caption": ["Figure 15: Wall clock of different methods as a function of oracle complexity "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "D.3  Samples from different radius ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We display the generated samples at different radius in Figures 16, 18 and the hyperparameters in Table 4. ", "page_idx": 30}, {"type": "table", "img_path": "X3Aljulsw5/tmp/e2f6abe1823e35f871aa6846a59f6f117561a518e1c5b7b2a5955f6f6ffa8abb.jpg", "table_caption": [], "table_footnote": ["Table 4: Hyperparameters for Various Methods for the 2D GMM experiment. $\\overline{{K}}$ means the current oracle complexity and $M$ refers to a matched oracle complexity. "], "page_idx": 30}, {"type": "image", "img_path": "X3Aljulsw5/tmp/6f235929c7bac3c4519a0b18bea38c200853a83afa62b51863188c408a032fc7.jpg", "img_caption": ["Figure 16: Generated samples at different radius "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "X3Aljulsw5/tmp/47e3862131e80b7fa2194c71c96e1d20c8db96d5fae6e82590e8fe21c4c18a51.jpg", "img_caption": ["Figure 18: Generated samples at different radius "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "D.4   Higher Dimensional Examples ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Score Error Approximation Details. We use the following 5d Gaussian mixture to measure the error of the score approximation: ", "page_idx": 31}, {"type": "text", "text": "Coefficients, $\\pmb{w}:[0.25\\quad0.5\\quad0.25\\]$ ${\\mathrm{Means,~}}\\mu_{k}:{\\left[\\begin{array}{l}{-4}\\\\ {-4}\\\\ {-3}\\\\ {-4}\\\\ {-4}\\end{array}\\right]}{\\mathrm{~}},{\\left[\\begin{array}{l}{4}\\\\ {3}\\\\ {4}\\\\ {2}\\\\ {4}\\end{array}\\right]}{\\mathrm{~}},{\\left[\\begin{array}{l}{-4}\\\\ {-2}\\\\ {-4}\\\\ {4}\\\\ {-1}\\end{array}\\right]}{\\mathrm{~}},$   \nVariances. $\\begin{array}{r}{\\,,\\Sigma_{k}:\\left[\\!\\!\\left[\\begin{array}{c c c c c c}{\\!3}&{\\!2}&{0}&{0}&{0}\\\\ {\\!2}&{\\!3}&{0}&{0}&{0}\\\\ {\\!0}&{0}&{4}&{2}&{0}\\\\ {\\!0}&{0}&{2}&{4}&{0}\\\\ {\\!0}&{0}&{0}&{0}&{1}\\end{array}\\right],\\left[\\!\\!\\begin{array}{c c c c c c}{\\!9}&{0}&{7}&{0}&{0}\\\\ {\\!0}&{1}&{0}&{0.4}&{0}\\\\ {\\!7}&{0}&{9}&{0}&{0}\\\\ {\\!0}&{\\!0}&{4}&{1}&{0}\\\\ {\\!0}&{0}&{0}&{0}&{1}\\end{array}\\right],\\left[\\!\\!\\begin{array}{c c c c c}{\\!1}&{0.4}&{0}&{0}&{0}\\\\ {\\!0.4}&{1}&{0}&{0}&{0}\\\\ {0}&{0}&{4}&{3}&{0}\\\\ {0}&{0}&{3}&{4}&{0}\\\\ {0}&{0}&{0}&{0}&{1}\\end{array}\\right].}\\end{array}$ ", "page_idx": 31}, {"type": "text", "text": "Randomized Gaussian Mixtures To generate the results in Figure 1b we proceed as follows. For a given dimension we take: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mu={\\frac{z}{\\|z\\|}}\\cdot12\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Where $z\\sim U[0,1]^{d}$ , additionallywesample $\\sigma^{2}\\sim U[.3,1.3]$ . We then consider the Gaussian target $\\mathcal{N}(\\mu,\\sigma^{2}I)$ . We repeat this 5 times and create a Gaussian mixture with equally weighted modes. We plot the the 2d marginals of the target distribution as long as the generated samples. ", "page_idx": 32}, {"type": "text", "text": "D.5 Muller Brown Potential Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "The potential is given by $V(x,y)=\\beta\\,\\cdot\\,(V_{m}(x,y)+V_{q}(x,y))$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{m}(x,y)=-170\\exp\\big(-6.5(x+0.5)^{2}+11(x+0.5)(y-1.5)-6.5(y-1.5)^{2}\\big)}\\\\ &{\\phantom{V_{m}(x,y)=}-100\\exp\\big(-x^{2}-10(y-0.5)^{2}\\big)+15\\exp\\big(0.7(x+1)^{2}+0.6(x+1)(y-1)+0.7(y-1)^{2}\\big)}\\\\ &{\\phantom{V_{m}(x,y)=-200\\exp\\big(-(x-1)^{2}-10y^{2}\\big)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $V_{m}$ corresponds to the original Muiller Brown and $V_{q}(x,y)=35.0136(x{-}x_{c}^{*})^{2}{+}59.8399(y{-}$ $y_{c}^{*})^{2}$ ,With $(x_{c}^{*},y_{c}^{*})$ is approximately the minimizer at the center of the middle potential well, and $V_{q}$ is a correction introduced so that the depths of all three wells are. ", "page_idx": 32}, {"type": "text", "text": "D.6Score error at $t=T$ ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "One natural concern is that the sampling problem at $t=T$ could be nearly as hard as sampling from the target distribution. Therefore, only a small number of samples could be accepted and the score error would be high. We display the score error at $t=T$ to show that this is not necessarily the case. ", "page_idx": 32}, {"type": "image", "img_path": "X3Aljulsw5/tmp/4529dd4064ffb875ba3a33e163c36c83a31f8ebb8fd0c15a5869145dd1b041a1.jpg", "img_caption": ["(a) Score error at $t=T=4$ . for different diffusion  (b) Score error at $t=T=4$ . for different diffusion samplers, we used $t\\approx1$ for SLIPS. Results are for samplers, we used $t\\approx1$ for SLIPS. Results are for the 2d GMM in the main paper the 5d GMM in the main paper ", "Figure 19: Score error at $t=T$ for different target distributions "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "D.7  Further details on number of accepted samples ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We present the number of accepted samples from our rejection sampler as a function of time. Specifically we consider 1000 trajectories of the diffusion and for every intermediate step we sample $10K$ samples. We present the number of accepted samples in Table 5. ", "page_idx": 32}, {"type": "table", "img_path": "X3Aljulsw5/tmp/158fc55399301a305fd0c69d1c47e5a85c394316544c2d4390b8faf12922ea8a.jpg", "table_caption": [], "table_footnote": ["Table 5: Comparison of GMM and Mueller values across different $t$ values "], "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 33}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 You should answer [Yes] , [No] , or [NA] .   \n\u00b7 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u00b7 Please provide a short (1-2 sentence) justification right after your answer (even for NA). ", "page_idx": 33}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 33}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 33}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u00b7 Keep the checklist subsection headings, questions/answers and guidelines below. . Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 33}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: Section 1, Table 1. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and refect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Section 3.3, Remark 5 and the discussion after ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: Main results: Proposition 3.1, Theorem 1, Corollary 3.1. Proofs are included in Appendix B. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Code is provided with instructions to run it. Scripts are provided so that a one line script can reproduce all the plots found in the paper. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. ", "page_idx": 34}, {"type": "text", "text": "\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: Code is provided with instructions to run it. Scripts are provided so that a one line script can reproduce all the plots found in the paper. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so ^No\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 35}, {"type": "text", "text": "\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Yes, the target distributions are well described, furthermore every hyperparameter can be easily checked in the configuration files or the corresponding script for each experiment. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: In Figure 4b we present the standard deviation of the errors in the score approximation. In other experiments we didn't find it insightful to add these statistics. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Yes, we give this information at the beginning of section 4 Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https : //neurips.cc/public/EthicsGuidelines? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: Our paper studies the fundamental theory of sampling, with no specified applications. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: Our paper poses no such risks. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to acces the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: We didn't use any dataset or asset from another party during these experiments. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: Our paper does not release new assets. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}]