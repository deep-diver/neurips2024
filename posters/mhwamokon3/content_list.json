[{"type": "text", "text": "Universality in Transfer Learning for Linear Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reza Ghane\u2217 Danil Akhtiamov\u2217 Department of Electrical Engineering Department of Computing $^+$ Mathematical Sciences California Institute of Technology California Institute of Technology Pasadena, CA 91125 Pasadena, CA 91125 rghanekh@caltech.edu dakhtiam@caltech.edu ", "page_idx": 0}, {"type": "text", "text": "Babak Hassibi Department of Electrical Engineering California Institute of Technology Pasadena, CA 91125 hassibi@caltech.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Transfer learning is an attractive framework for problems where there is a paucity of data, or where data collection is costly. One common approach to transfer learning is referred to as \u201cmodel-based\", and involves using a model that is pretrained on samples from a source distribution, which is easier to acquire, and then fine-tuning the model on a few samples from the target distribution. The hope is that, if the source and target distributions are \u201cclose\", then the fine-tuned model will perform well on the target distribution even though it has seen only a few samples from it. In this work, we study the problem of transfer learning in linear models for both regression and binary classification. In particular, we consider the use of stochastic gradient descent (SGD) on a linear model initialized with pretrained weights and using a small training data set from the target distribution. In the asymptotic regime of large models, we provide an exact and rigorous analysis and relate the generalization errors (in regression) and classification errors (in binary classification) for the pretrained and fine-tuned models. In particular, we give conditions under which the fine-tuned model outperforms the pretrained one. An important aspect of our work is that all the results are \u201cuniversal\", in the sense that they depend only on the first and second order statistics of the target distribution. They thus extend well beyond the standard Gaussian assumptions commonly made in the literature. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep neural networks have revolutionized the way data processing and statistical inference are conducted. Despite their ground-breaking performance, these models often require a plethora of training samples which can make the process of data acquisition expensive. To alleviate the scarcity of prepared/labeled data, various approaches have been proposed. One such method is the \"modelbased\" transfer learning framework in which a network previously trained on a source dataset different from the target dataset is leveraged as the initialization point for training on the target dataset. A foundational question would be: How effective is this procedure? Are there fundamental limits to how much one can achieve by utilization of a model pretrained on a different distribution? In this work, we would like to investigate this problem rigorously through the lens of linear regression and binary classification in the overparametrized regime, where the number of weights/parameters exceeds the number of data points. To do so, we analyze the performance of regressors/classifiers obtained through performing SGD initialized at a weight $\\mathbf{w}_{0}\\,\\stackrel{\\cdot}{\\in}\\,\\mathbb{R}^{d}$ acquired through training on the source domain. It was shown in Gunasekar et al. [2018] and Azizan and Hassibi [2018] that gradient descent (GD) and stochastic gradient descent(SGD) iterations converge to the optimal solution $\\mathbf{w}\\in\\mathbb{R}^{d}$ of the following optimization problem: ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\mathbf{w}}{\\operatorname*{min}}\\;\\|\\mathbf{w}-\\mathbf{w}_{0}\\|_{2}^{2}}\\\\ {s.t\\quad\\;\\mathbf{y}=\\mathbf{X}\\mathbf{w}}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mathbf{w}_{0}\\,\\in\\,\\mathbb{R}^{d}$ is the initialization point for SGD, $\\mathbf{X}\\in\\mathbb{R}^{n\\times d}$ is the design/data matrix which is comprised of independent rows reflecting the fact that datapoints $\\left(\\mathbf{x}_{i},\\mathbf{y}_{i}\\right)$ are sampled independently, and $\\mathbf{y}\\,\\in\\,\\mathbb{R}^{n}$ is the vector of labels. This property of SGD is known as \"Implicit Regularization\" and will serve as the basis for our analysis as the pretraining step is captured by the vector $\\mathbf{w}_{0}$ . Understanding this \"interpolating regime\" is key to the theoretical analysis of machine learning models as most of the deep neural networks, on account of their highly overparametrized nature, operate in a setting where they are able to attain negligible training error. It is noteworthy that, even for linear models, characterizing the exact performance of model-based transfer learning approach has been somewhat limited and is inhibited furthermore by the Gaussianity assumption on $\\mathbf{X}$ . One of our main contributions is to overcome this limitation. In fact, to showcase the ubiquity of our results, we establish a general universality theorem that applies to a large class of data distributions and extends beyond the context of Transfer Learning. To go into more detail, we say that Gaussian universality holds for a data distribution $\\mathbb{P}$ and a training algorithm $T$ if the test error obtained by training on data sampled from $\\mathbb{P}$ using $T$ is the same as the test error obtained by training on data sampled from the Gaussian distribution $N(\\mu_{\\mathbb{P}},\\Sigma_{\\mathbb{P}})$ using $T$ , where $\\pmb{\\mu}_{\\mathbb{P}}$ and $\\Sigma_{\\mathbb{P}}$ stand for the mean and the covariance matrix of $\\mathbb{P}$ respectively. In the context of binary classification, we establish universality of the classification error with respect to the distribution of each class. In light of the the latter result, the problem reduces to analyzing the case of the Gaussian design matrices. We allow classes with different non-scalar covariance matrices $\\Sigma_{1}$ and $\\Sigma_{2}$ and build on the results of Akhtiamov et al. [2024] to address the problem in this specific scenario. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Transfer Learning has been an active topic of research at least since the 1970\u2019s Bozinovski [2020]. It is mainly applied in the situations where obtaining data from the true distribution, which we will refer as the target distribution, is costly but there is a cheap way to access data from a source distribution, which bears resemblance to the target distribution. The two most popular approaches to transfer learning consist of instance-based transfer learning and model-based transfer learning. ", "page_idx": 1}, {"type": "text", "text": "Instance-based transfer learning incorporates the source dataset, along with the target dataset, and trains the model on this amalgamated dataset. The key insight is that, provided that the source distribution is close enough to the target distibution and a suitable training scheme is chosen, the final performance enjoys an improvement. We refer the reader to the landmark workDai et al. [2007] as well as to the comprehensive overviews covering the empirical advances in this area Tan et al. [2018]Zhuang et al. [2020]. For theoretical analysis, we would like to emphasize a recent work Jain et al. [2024] that characterized the scaling laws for ridge regression over the union of the source and target data in the high dimensional regime. ", "page_idx": 1}, {"type": "text", "text": "The present manuscript focuses on model-based transfer learning in which a model is first pretrained on the source distribution and then fine-tuned using the target data. Dar et al. [2021] analyzes transfer learning for linear regression assuming Gaussianity of the data. The results obtained in this paper, in particular, imply that the generalization error obtained in their work is universal, in the sense that they can be immediately extended to a much broader set of target distributions, which we elaborate on in much greater detail in the main body of the paper. ", "page_idx": 1}, {"type": "text", "text": "Gerace et al. [2022] consider the problem of binary classification via a two-layer neural network in a synthetic setting. The source data and the source labels are sampled according to ${\\bf x}=R e l u({\\bf G}{\\bf a})$ and $\\mathbf{y}=S i g n(\\mathbf{a}^{T}\\mathbf{\\check{z}})$ respectively where $\\mathbf{G}$ , a and $\\mathbf{z}$ are i.i.d. Gaussian. The target data and labels are generated by perturbing $\\mathbf{G}$ and a and then applying the same rules as for the source data. Gerace et al. [2022] trained the network on the source dataset and then proceeded by only fine-tuning the last layer on the target domain using the cross-entropy loss with an $\\ell_{2}$ regularizer. The analysis was conducted using the Replica method. The authors of Gerace et al. [2022] compared their results with an equivalent random feature model and observed empirically a certain kind of Gaussian universality for real-world datasets, such as MNIST. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Extension of results obtained under Gaussianity assumptions to non-Gaussian distributions remains a pertinent research direction in which the idea of Gaussian universality plays a central role. Montanari and Nguyen [2017] proved the universality of the generalization error for the elastic net, assuming the design matrix A is i.i.d subgaussian. Panahi and Hassibi [2017] generalized the result to the problem of regularized regression with a quadratic loss and a convex separable regularizer which is either $f(\\cdot)=\\|\\cdot\\|_{1}$ (LASSO) or strongly convex. Han and Shen [2023] generalized Panahi and Hassibi [2017]\u2019s results to non-separable Lipschitz test functions and provided non-asymptotic bounds for the concentration of solutions. In the random geometry literature, Oymak and Tropp [2018] showed universality of the embedding dimension of randomized dimension reduction linear maps with i.i.d entries satisfying certain moment conditions. Abbasi et al. [2019] proved universality of the recovery threshold for minimizing strongly convex functions under linear measurements constraint, assuming the rows are iid and the norm of the mean is asymptotically negligible compared to the noise. Lahiry and Sur [2023], proved the universality of generalization error for ridge regression and LASSO when the rows are distributed iid with a specific block structure dependence per row, AD where A has zero mean iid random subgaussian vectors per row and D being diagonal. ", "page_idx": 2}, {"type": "text", "text": "Leveraging universality is not limited to the signal recovery literature. As an example, Hu and Lu [2022], Bosch et al. [2023], Schr\u00f6der et al. [2023], have analyzed the performance of the random feature models by replacing nonlinear functions of Gaussians with the corresponding Gaussian distribution having the same mean and covariance in single layer and multiple layer scenarios, respectively, through a universality argument, referred to as the Gaussian equivalence principle. In a more general setting, under subgaussianity assumption on the data, Montanari and Saeed [2022] proved the universality of the training error for general loss and regularizer and test error when the regularizer is strongly convex and the loss is convex. Hastie et al. [2022] proved universality for the the minimal $\\ell_{2}$ -norm linear interpolators for the data generated via a very specific rule $\\mathbf{x}=\\sigma(\\mathbf{W}\\pmb{\\Sigma}^{\\frac{1}{2}}\\mathbf{z})$ , where $\\mathbf{z}$ is i.i.d zero mean and satisfies several other technical properties and $\\pmb{\\Sigma}$ is a deterministic PSD matrix. Dandi et al. [2024] considered the universality of mixture distributions in a similar setting to Montanari and Saeed [2022] and proved the universality of free energy of test error. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries and our contributions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Notations and definitions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We use bold letters for vectors and matrices. We consider the proportional regime where $d=\\Theta(n)$ and $\\begin{array}{r}{\\frac{d}{n}\\to\\kappa>1}\\end{array}$ . We call a convex function $f:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ separable if $\\begin{array}{r}{f(\\mathbf{w})=\\sum_{i=1}^{d}f_{i}(w_{i})}\\end{array}$ , where $f_{i}:\\mathbb{R}\\to\\mathbb{R}$ are convex. Examples of such functions include $\\|\\cdot\\|_{p}^{p}$ for every $p\\geq1$ . A function $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ is $M$ -strongly convex for $M>0$ if for every $\\mathbf{w}\\in\\mathbb{R}^{d}$ , $f(\\mathbf{w})-M\\|\\mathbf{w}\\|_{2}^{2}$ is convex. Our universality theorem holds for the regularizer and test functions satisfying the following definition: ", "page_idx": 2}, {"type": "text", "text": "Definition 1. We call a function $f:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ regular if it satisfies the following conditions ", "page_idx": 2}, {"type": "text", "text": "1. $f$ is convex with $f(0)=O(d)$ .   \n2. $f$ is three times differentiable.   \n3. $f$ satisfies the following third-order condition for some $C_{f}>0$ : $\\sum_{i,j,k=1}^{d}\\frac{\\partial^{3}f}{\\partial w_{i}\\partial w_{j}\\partial w_{k}}\\nu_{i}\\nu_{j}\\nu_{k}\\leq C_{f}\\sum_{i=1}^{d}|\\nu_{i}|^{3}$ ", "page_idx": 2}, {"type": "text", "text": "Note that separable functions with bounded third order derivatives satisfy the third assumption of Definition 1. For the description of the main results on regression and classification we recall several probability theory concepts. Given a real-valued measure $\\mu$ , its Stieltjes transform is defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\nS_{\\mu}(z):=\\int_{\\mathbb{R}}\\frac{1}{r-z}\\mu(d r),\\quad z\\in\\mathbb{H}_{+}\\cup\\mathbb{R}\\setminus S u p p(\\mu)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "With $\\mathbb{H}_{+}=\\{z:I m(z)>0\\}$ . Moreover, we denote convergence in probability and weak convergence for measures by\u2212\u2192and $\\rightsquigarrow$ respectively. For a random variable $X$ , we sometimes use $V a r(X)$ for the variance of $X$ . For a convex differentiable function $g$ , the Bregman divergence is defined by ", "page_idx": 3}, {"type": "equation", "text": "$$\nD_{g}(\\mathbf{w},\\mathbf{w}_{0}):=g(\\mathbf{w})-g(\\mathbf{w}_{0})-\\nabla g(\\mathbf{w}_{0})^{T}(\\mathbf{w}-\\mathbf{w}_{0})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "2.2 Universality ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given a training dataset $\\{(\\mathbf{x}_{i},\\boldsymbol{y}_{i})\\}_{i=1}^{n}$ and a model with a fixed architecture, the conventional approach of learning the weights for the model consists of choosing a loss function $\\mathcal{L}$ and finding $w$ minimizing $\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}\\mathcal{L}\\overline{{(\\mathbf{x}_{i},\\mathbf{w}_{i},{y}_{i})}}}\\end{array}$ . In t\ud835\udc47his exposition we focus on linear models and loss functions of the form $\\mathcal{L}(\\mathbf{x}_{i},\\mathbf{w}_{i},y_{i})=\\ell(y_{i}-\\mathbf{x}_{i}^{T}\\mathbf{w}_{i})$ , where is differentiable and $\\ell(0)=0$ . Azizan and Hassibi [2018] characterized behavior of a broad family of optimizers, called stochastic mirror descent (SMD) algorithms, for linear models in the over-parametrized regime. For a strictly convex function $g:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ , the update rule of the SMD with a mirror $g$ and a learning rate $\\eta>0$ is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla g(w_{t})=\\nabla g(w_{t-1})-\\eta\\nabla{\\mathcal L}_{t}(w_{t-1}),\\quad t\\geq1,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\eta>0$ is the learning rate and $\\mathcal{L}_{t}$ is the loss function $\\mathcal{L}$ evaluated at a point chosen at random in the dataset corresponding to the $t^{\\dagger}$ \u2019th iteration. Due to strict convexity, $\\nabla g(\\cdot)$ defines an invertible transformation, which is why (3) is indeed a well-defined update rule. Note also that this includes SGD as a special case, which corresponds to taking $g(\\cdot)=\\dot{\\|}\\cdot\\|_{2}^{2}$ . Azizan and Hassibi [2018] show that applying SMD initialized at $\\mathbf{w}_{0}$ to minimize $\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}\\ell(y_{i}-\\mathbf{x}_{i}^{T}\\mathbf{w})}\\end{array}$ yields a weight vector defined by the following optimization problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathbf{w}}{\\mathop{\\operatorname*{min}}}\\,D_{g}\\left(\\mathbf{w},\\mathbf{w}_{0}\\right)}\\\\ &{s.t\\quad y_{i}=\\mathbf{x}_{i}^{T}\\mathbf{w},\\quad1\\leq i\\leq n}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Hence, stating in a more general fashion, we are interested in the analysis of the following optimization problem, with the main case of interest being when $f$ is a quadratic: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\mathbf{w}}{\\mathop{\\operatorname*{min}}}\\,f(\\mathbf{w})}\\\\ {s.t\\quad\\mathbf{y}=\\mathbf{A}\\mathbf{w}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "As the objective is comprised of minimizing a strongly convex function $f$ over a closed convex set, it has a unique minimizer. By using a Lagrange multiplier $\\lambda\\in\\mathbb{R}$ , this convex optimization problem can be written in the unconstrained form: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Phi(\\mathbf{A}):=\\operatorname*{sup}_{\\lambda>0}\\operatorname*{min}_{w}\\frac{\\lambda}{2}\\|\\mathbf{A}\\mathbf{w}-\\mathbf{y}\\|_{2}^{2}+f(\\mathbf{w})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Considering the objective without the $\\mathrm{sup}_{\\lambda>0}$ yields the following regularized linear regression problem for which we will also establish a universality theorem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Phi_{\\lambda}(\\mathbf{A}):=\\operatorname*{min}_{w}\\frac{\\lambda}{2}\\|\\mathbf{A}\\mathbf{w}-\\mathbf{y}\\|_{2}^{2}+f(\\mathbf{w})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that (5) captures the case of explicit regularization, which is of highest importance whenever there is a high amount of noise or label corruption present. Furthermore, by deliberately choosing the regularizer $f$ , it is possible to obtain solutions that exhibit certain behavior, viz being sparse or compressed. Next, we will provide a description of the design matrices investigated in this paper. It will be clear soon that these assumptions are often satisfied in practice. ", "page_idx": 3}, {"type": "text", "text": "Definition 2. 2 We call a random matrix $\\textbf{A}\\in\\ \\mathbb{R}^{n\\times d}$ block-regular if it satisfies the following properties: ", "page_idx": 3}, {"type": "equation", "text": "$$\nI.\\ \\ \\mathbf{A}^{T}=\\left(\\mathbf{A}_{1}^{T}\\ \\ \\ \\mathbf{A}_{2}^{T}\\ \\ \\ \\dots\\ \\ \\mathbf{A}_{k}^{T}\\right){w i t h\\ k\\ b e i n g\\ f i n i t e\\ a n d f o r\\,}e a c h\\ \\mathbf{A}_{i}\\in\\mathbb{R}^{n_{i}\\times d},\\,n_{i}=\\Theta(n)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "2. For $1\\leq i\\leq k$ , the rows of $\\mathbf{A}_{i}$ are distributed independently and identically. ", "page_idx": 4}, {"type": "text", "text": "3. $\\mathbb{E}\\mathbb{A}_{i}=:\\mathbb{1}\\mu_{i}^{T}$   \n4. Let a be any row of A and $\\pmb{\\mu}$ be its mean. Then for any deterministic vector $\\mathbf{v}\\in\\mathbb{R}^{d}$ , and $q\\in\\mathbb{N},\\,q\\leq6,$ , there exists a constant $K>0$ such that $\\begin{array}{r}{\\mathbb{E}_{\\mathbf{a}}|(\\mathbf{a}-\\pmb{\\mu})^{T}\\mathbf{v}|^{q}\\leq K\\frac{\\|\\mathbf{v}\\|_{2}^{q}}{d^{q/2}}}\\end{array}$   \n5. For any deterministic matrix $\\mathbf{C}\\in\\mathbb{R}^{d\\times d}$ of bounded operator norm we have $V a r(\\mathbf{a}^{T}\\mathbf{C}\\mathbf{a})\\rightarrow0$ as $d\\to\\infty$ .   \n6. Each $\\|\\mu_{i}\\|_{2}^{2}=O(1)$ ", "page_idx": 4}, {"type": "text", "text": "Note that assumptions 1-3 from Definition 2 are satisfied for the design matrix both in the classification and regression scenarios, assumption 6 is just a matter of normalization. While assumptions 4 and 5 might appear obscure at first, any subgaussian $\\mathbf{a}\\in\\mathbb{R}^{d}$ satisfying the Lipschitz concentration property, P(|\ud835\udf19(a) \u2212E\ud835\udf19(a)| > \ud835\udc61) \u22642 exp (\u22122\u2225\ud835\udf19\ud835\udc51\ud835\udc61\u22252\ud835\udc3f\ud835\udc56\ud835\udc5d) for every Lipschitz $\\phi$ , also satisfies the aforementioned assumptions. It is also worth mentioning that, as shown in Seddik et al. [2020], any data distribution from which one can sample using a GAN satisfies the Lipschitz Concentration Property. Since it is known that many natural datasets can be approximated well by GAN-generated data in practice, we thus find this assumption realistic. These assumptions also bear significance for the instance-based approach to transfer learning as utilizing GANs can remedy the dearth of data. What is more, these assumptions are not limited to the subgaussians and include many other subexponential distributions, including $\\chi^{2}$ which appears naturally in many signal processing applications such as Phase Retrieval. In order to tackle optimizations such as 4, 5, we prove their equivalence to a problem with a suitable Gaussian design G. For this purpose we will need the following definition: ", "page_idx": 4}, {"type": "text", "text": "Definition 3. We call a Gaussian matrix G matching a block-regular A if G is block-regular as well, $\\mathbb{E}\\mathbf{G}_{i}=\\mathbb{E}\\mathbf{A}_{i}=\\mathbb{1}\\mu_{i}^{T}$ and for any row g of $\\mathbf{G}_{i}$ and any row a of ${\\bf A}_{i}$ , it holds that $\\mathbb{E}\\mathbf{g}\\mathbf{g}^{T}=\\breve{\\mathbb{E}}\\mathbf{a}\\mathbf{a}^{T}$ . ", "page_idx": 4}, {"type": "text", "text": "Our approach for proving universality will be the Lindeberg approach Lindeberg [1922] Chatterjee [2006]. ", "page_idx": 4}, {"type": "text", "text": "3 Main Result ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we present our main universality theorem and before doing so, we list the required technical assumptions. Consider the following convex optimization problem ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Phi_{\\lambda}(\\mathbf{A}):=\\operatorname*{min}_{w}\\frac{\\lambda}{2n}\\|\\mathbf{A}\\mathbf{w}-\\mathbf{y}\\|_{2}^{2}+\\frac{1}{n}f(\\mathbf{w})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Denote the solution to the optimization problem 6 above by $\\mathbf{w}_{\\Phi_{\\lambda}(\\mathbf{A})}$ . Then we assume the following: ", "page_idx": 4}, {"type": "text", "text": "Assumptions 1. 1. A is a regular block matrix and $\\mathbf{G}$ is its matching Gaussian matrix. ", "page_idx": 4}, {"type": "text", "text": "2. The regularizer $f(\\cdot)$ is regular or there exists a sequence of regular $f_{m}$ \u2019s converging uni  \nformly to $f$ .   \n3. $f(\\cdot)$ is $M$ -strongly convex.   \n4. Ey $|\\mathbf{y}||_{2}^{2}=O(d)$ with E $|y_{j}|^{q}=O(1)$ for $j\\in[n]$ and $q\\in$ [6] and is independent of A.   \n5. $s_{\\mathrm{min}}(\\mathbf{AA}^{T})=\\Omega(1)$ with high probability,   \n6 $.\\;\\;V a r(\\|\\mathbf{y}\\|_{2}^{2})\\to0$   \n7. $\\|\\nabla f(\\mathbf{w}_{\\Phi_{\\lambda}(\\mathbf{A})})\\|_{2}\\leq K_{f}\\sqrt{n}$ for a constant $K_{f}$ independent of \ud835\udf06. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. If $\\Psi=\\operatorname*{sup}_{\\lambda>0}\\Phi_{\\lambda},$ suppose parts $(I)$ - (7) of Assumptions $^{\\,l}$ hold and for $\\Psi=\\Phi_{\\lambda},$ , only assume parts $(I)-(4)$ of Assumptions $^{\\,I}$ . Then ", "page_idx": 4}, {"type": "text", "text": "1. (Universality of the training error) For any $L$ -Lipschitz $g:\\mathbb{R}^{d}\\rightarrow\\mathbb{R},$ and every $t_{1}>0,t_{2}>$ $t_{1}$ , $c\\in\\mathbb{R}$ the following holds: \u2022 $I f\\mathbb{P}(|g(\\Psi(\\mathbf{G}))-c|>t_{1})\\rightarrow0\\;t h e n\\,\\mathbb{P}(|g(\\Psi(\\mathbf{A}))-c|>t_{2})\\rightarrow0.$ ", "page_idx": 4}, {"type": "text", "text": "\u2022 Furthermore, if \ud835\udc54has bounded second derivative, then ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\Bigl|\\mathbb{E}_{\\mathbf{A},\\mathbf{y}}g(\\Psi(\\mathbf{A}))-\\mathbb{E}_{\\mathbf{G},\\mathbf{y}}g(\\Psi(\\mathbf{G}))\\Bigr|=0\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "2. (Universality of the solution) For a function $\\psi$ with $\\nabla^{2}\\psi\\preceq q\\mathbf{I}$ if either \ud835\udf13is regular (Definition 1) or there is a sequence of regular functions converging uniformly to $\\psi$ , \u2022 If for every $t>0$ , $\\mathbb{P}(|\\Psi(\\mathbf{G})-c|>t)\\to0$ then for $\\mathbf{B}\\in\\{\\mathbf{A},\\mathbf{G}\\}$ , there exists $c_{1}\\in\\mathbb{R}$ such that for any $t_{1}>0$ we have $\\mathbb{P}\\big(|\\psi\\big(\\mathbf{w}_{\\Psi(\\mathbf{B})}\\big)-c_{1}|>t_{1}\\big)\\rightarrow0$ . \u2022 Furthermore, if \ud835\udf13is bounded ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\rightarrow\\infty}\\biggr|\\mathbb{E}_{\\mathbf{A},\\mathbf{y}}\\psi\\bigl(\\mathbf{w}_{\\Psi(\\mathbf{A})}\\bigr)-\\mathbb{E}_{\\mathbf{G},\\mathbf{y}}\\psi\\bigl(\\mathbf{w}_{\\Psi(\\mathbf{G})}\\bigr)\\biggr|=0\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "To sum up, Theorem 1 says that the vector of weights $\\mathbf{w_{A}}$ trained on data sampled from a non-gaussian distribution A either via SMD with a mirror $f$ or by minimizing the least squares objective with regularizer $f$ , shares many similar characteristics with the vector of weights wG trained on data sampled from the matching GMM G via the same optimization procedure under certain technical assumptions on A and $f$ . By \"similar characteristics\", we mean that for any $\\psi\\,:\\,\\mathbb{R}^{d}\\,\\rightarrow\\,\\mathbb{R}$ with bounded Hessian, $\\psi(\\mathbf{w_{A}})=\\psi(\\mathbf{w_{G}})$ holds in the limit. ", "page_idx": 5}, {"type": "text", "text": "4 Applications: Transfer Learning ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Regression ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We consider the classical problem of recovering the best linear regressor for the following linear model ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{y}=\\mathbf{X}\\mathbf{w}_{*}+\\mathbf{z}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, $\\mathbf{w}_{*}$ is the ground truth, the rows of the data matrix $\\mathbf{X}$ are i.i.d with $\\mathbb{E}\\mathbf{x}_{i}=\\mathbf{0}$ and $\\mathbb{E}_{{\\mathbf{X}}_{i}{\\mathbf{X}}_{i}^{T}}=:{\\mathbf{R}}_{x}$ , $\\mathbf{z}$ is centered and is independent of $\\mathbf{X}$ and satisfies $\\mathbb{E}_{\\mathbf{z}}\\|\\mathbf{z}\\|_{2}^{2}=\\sigma^{2}n$ . For a given $\\hat{\\bf w}$ , its generalization error is defined by: ", "page_idx": 5}, {"type": "equation", "text": "$$\ne_{g e n}(\\hat{\\mathbf{w}}):=\\mathbb{E}_{x}(\\mathbf{x}^{T}\\mathbf{w}_{*}-\\mathbf{x}^{T}\\hat{\\mathbf{w}})^{2}=(\\hat{\\mathbf{w}}-\\mathbf{w}_{*})^{T}\\mathbf{R}_{x}(\\hat{\\mathbf{w}}-\\mathbf{w}_{*})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Now in order to recover $\\mathbf{w}_{*}$ given observations $(\\mathbf{y},\\mathbf{X})$ , we choose to optimize the least squares objective, $\\operatorname*{min}_{\\mathbf{w}}\\|\\mathbf{y}-\\mathbf{X}\\mathbf{w}\\|_{2}^{2}$ , and to do so, we leverage SGD. We would like to investigate how useful having a pretrained classifier $\\mathbf{w}_{0}$ can be for the recovery of $\\mathbf{w}_{*}$ . As pointed out earlier, SGD initialized from $\\mathbf{w}=\\mathbf{w}_{0}$ , by its implicit regularization property, converges to the solution $\\hat{\\bf w}$ of (1) ", "page_idx": 5}, {"type": "text", "text": "We impose the following assumptions about the data: ", "page_idx": 5}, {"type": "text", "text": "Assumptions 2. 1. ${\\bf R}_{x}\\succ{\\bf0}$ is diagonal without loss of generality. ", "page_idx": 5}, {"type": "text", "text": "2. Let $\\hat{p}_{N}$ be the empirical spectral density of $\\mathbf{R}_{x}$ , then $\\hat{p}_{N}\\rightsquigarrow p$ for some probability distribution $p$ .   \n3. $\\mathbf{w}_{0}$ is a white perturbation of true $\\mathbf{w}_{*}$ in the $\\mathbf{R}_{x}$ basis. That is $\\mathbf{R}_{x}^{1/2}(\\mathbf{w}_{0}-\\mathbf{w}_{*})=\\xi$ where $\\mathbb{E}\\pmb{\\xi}=\\mathbf{0}$ , $\\begin{array}{r}{\\mathbb{E}\\pmb{\\xi}\\pmb{\\xi}^{T}=\\frac{e_{a}}{d}\\mathbf{I}}\\end{array}$ for a fixed $e_{a}>0$ . ", "page_idx": 5}, {"type": "text", "text": "The third assumption from Assumptions 2 represents the fact that $\\mathbf{w}_{0}$ is a perturbation of the ground truth $\\mathbf{w}_{*}$ in the eigenbasis of $R_{x}$ . An attentive reader might notice that the quantity $e_{a}$ equals the generalization error for $\\mathbf{w}_{0}$ ; hence the name \"a priori error\", $e_{a}$ . In what follows we give a precise characterization of the posterior error, $e_{p}$ , of the model after training with SGD, in terms of $e_{a}$ and the other parameters of the problem. In order to leverage Theorem 1 to establish the universality of the generalization error, it is sufficient to apply a change of variable $\\mathbf{w}^{\\prime}:=\\mathbf{R}_{x}^{-1/2}\\mathbf{w}$ and use $\\psi(\\mathbf{w})=\\|\\mathbf{w}\\|_{2}^{2}$ as the test function. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. Under Assumptions 2, the generalization error of the SGD solution initialized from $\\mathbf{w}_{0}$ converges in probability to ", "page_idx": 5}, {"type": "equation", "text": "$$\ne_{p}=\\frac{2-\\kappa(1-t)}{\\kappa(1-t)-1}\\sigma^{2}+\\frac{t}{\\kappa(1-t)-1}e_{a}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "With \ud835\udc61= \u222b (24+\ud835\udc5d\ud835\udc5f(\ud835\udc5f\ud835\udf03))2 \ud835\udc51\ud835\udc5fwhere \ud835\udf03is found through \ud835\udf05\ud835\udf05\u22121 = 1\ud835\udf03\ud835\udc46\ud835\udc5d(\u22121\ud835\udf03) (2). And \ud835\udf05> 1 is the proportional constant, i.e for $\\mathbf{X}\\in\\mathbb{R}^{n\\times d}$ , $\\frac{d}{n}\\rightarrow\\kappa$ . Moreover, for any distribution $p(r)$ we have ", "page_idx": 6}, {"type": "equation", "text": "$$\ne_{p}\\ge\\frac{1}{\\kappa-1}\\sigma^{2}+\\frac{\\kappa-1}{\\kappa}e_{a}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The lower bound is attained if and only if $p(r)=\\delta(r-r_{0})$ for some $r_{0}>0$ ", "page_idx": 6}, {"type": "text", "text": "The following remark is immediate: ", "page_idx": 6}, {"type": "text", "text": "Remark 1. Theorem 2 entails that, depending on the noise level present in the data, training could be even potentially harmful. Indeed, if $\\sigma^{2}\\geq e_{a}$ , then $\\begin{array}{r}{e_{p}\\geq\\frac{1}{\\kappa-1}\\dot{\\sigma}^{2}+\\frac{\\kappa-1}{\\kappa}e_{a}\\geq\\sigma(2\\sqrt{e_{a}}-\\check{\\sigma})\\geq e_{a}}\\end{array}$ for any and it is therefore more appropriate to use vector $\\mathbf{w}_{0}$ instead of performing fine-tuning. Moreover, if the covariance $\\mathbf{R_{x}}$ is scalar, then the converse is true. Namely, i $f\\,e_{a}\\,\\geq\\,\\sigma^{2}$ , then the best achievable error corresponds to \u221a\ud835\udc52\ud835\udc4e\u2212\ud835\udc4e\ud835\udf0eand equals \ud835\udf0e(2 \ud835\udc52\ud835\udc4e\u2212\ud835\udf0e), which is less than \ud835\udc52\ud835\udc4e. Therefore, transfer learning contributes to improving the test performance when the variance of the noise is not too high, but the model has to be fine-tuned on the correct amount of target data. ", "page_idx": 6}, {"type": "text", "text": "Note that Dar et al. [2021] obtained a similar theorem, apart from the lower bound part under the Gaussianity assumption. Thus, our result for regression can be considered an extension of theirs to a much broader family of data distributions. ", "page_idx": 6}, {"type": "text", "text": "4.2 Classification ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.2.1 Problem setting ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Consider a binary classification task and let $\\mathbf{X}$ stand for the data matrix and y denote the vector of labels where each $y_{i}=\\pm1$ depending on what class the $i$ -th point $\\mathbf{X}_{i}$ falls into. After learning a linear classifier w, we assign labels to new previously unseen points according to ${\\bf y}_{n e w}=s i g n({\\bf\\bar{w}}^{T}{\\bf x}_{n e w})$ Without loss of generality we will assume that the first $\\begin{array}{l}{{\\frac{n}{2}}}\\end{array}$ rows of $\\mathbf{X}$ are sampled from the first class and the remaining rows are sampled from the second as we can permute the rows otherwise. Modulo such permutation, it is straightforward to see that $\\mathbf{X}$ satisfies parts (1) - (3) of Definition 2. Since the topic of the present paper is model-based transfer learning, we assume that the following steps are performed: ", "page_idx": 6}, {"type": "text", "text": "1. Obtain a pre-trained classifier $\\mathbf{w}_{0}$   \n2. Renormalize $\\mathbf{w}_{0}$ obtained during the previous step using the target data via ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\alpha}\\|\\mathbf{y}-\\alpha\\mathbf{X}\\mathbf{w}_{0}\\|^{2}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This yields: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\alpha=\\frac{\\mathbf{y}^{T}\\mathbf{X}\\mathbf{w}}{\\|\\mathbf{X}\\mathbf{w}\\|^{2}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "After finding $\\alpha$ take $\\mathbf{w}_{0}^{\\prime}:=\\alpha\\mathbf{w}_{0}$ . This transform preserves the direction of $\\mathbf{w}_{0}$ , while setting   \nits squared magnitude to \ud835\udefc2\u2225w0\u22252 = (y X\u2225wX0w)2\u2225\u22254w0\u22252, which depends only on the direction of $\\mathbf{w}_{0}$ but not on its magnitude anymore. We find applying this transform very meaningful, as it does not change the classification error for the source data but simplifies learning for the regression problem defined by the target data.   \n. Learn the final classifier from the target data $\\mathbf{X}\\,\\in\\,\\mathbb{R}^{n\\times d}$ and labels $\\textbf{y}\\in\\mathbb{R}^{n}$ using SGD initialized at $\\ensuremath{\\mathbf{w}}_{0}^{\\prime}$ , obtained from the previous step. ", "page_idx": 6}, {"type": "text", "text": "4.2.2 Technical assumptions and lemmas ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We will use the following assumptions to define further details of the classification task we consider: Assumptions 3. 1. The data matrix $\\mathbf{X}$ satisfies parts (4)-(6) of Definition 2 ", "page_idx": 6}, {"type": "text", "text": "2. The means $\\pmb{\\mu}_{1},\\pmb{\\mu}_{2}$ are of norm 1 and for any deterministic matrix C of bounded operator norm it holds that $\\mu_{1}^{T}\\mathbf{C}\\mu_{1},\\mu_{2}^{T}\\mathbf{C}\\mu_{2}$ and $\\mu_{1}^{T}\\mathbf{C}\\mu_{2}$ converge in probability to $\\frac{T r(\\mathbf{C})}{d},\\frac{T\\dot{r}(\\mathbf{C})}{d}$ and $\\frac{r T r(\\mathbf{C})}{d}$ respectively. ", "page_idx": 6}, {"type": "text", "text": "4. Let $\\hat{p}_{N}$ be the joint empirical spectral density of $\\Sigma_{1},\\Sigma_{2}$ , then $\\hat{p}_{N}\\rightsquigarrow p$ , where the distribution $p$ is such that $p(s_{1},s_{2})=p(s_{2},s_{1})$ holds for all $s_{1},s_{2}$ . ", "page_idx": 7}, {"type": "text", "text": "For a discussion of the first assumption from Assumptions 3, see the paragraph after Definition 2. The second assumption from Assumptions 3 is satisfied, for example, for any random vector of the form $\\pmb{\\Sigma}^{1/2}\\mathbf{z}$ , where $\\boldsymbol\\Sigma$ is an arbitrary $P S D$ matrix and $\\mathbf{z}$ is i.i.d. The third assumption postulates that the means are normalized generic $d$ -dimensional vectors with an angle arccos $r$ between them. The fourth assumption says that $\\Sigma_{1}$ and $\\Sigma_{2}$ are simultaneously diagonalizable and, finally, the fifth assumption simply introduces notation for the joint density function of the eigenvalues of $\\Sigma_{1}$ and $\\Sigma_{2}$ . Note that in practice the main difficulty of the classification task in the over-parametrized regime $(n<d)$ arises due to the fact that $\\mu_{1},\\mu_{2},\\Sigma_{1}$ and $\\Sigma_{2}$ are not known and cannot be estimated reliably. Nevertheless, we would like to start with characterizing the optimal performance in the scenario where these are provided to us by an oracle. The following statement provides such a characterization under certain symmetry assumptions: ", "page_idx": 7}, {"type": "text", "text": "Lemma 1. Assume that the feature vectors are equally likely to be drawn from class 1 or class 2 and Assumptions 3 hold. Then the optimal classifier is given by $\\mathbf{w}_{\\ast}=(\\Sigma_{1}+\\dot{\\Sigma}_{2})^{-1}({\\pmb{\\mu}}_{1}-{\\pmb{\\mu}}_{2})$ and its classification error is equal to the following, where $Q(\\cdot)$ is the integral of the tail of the standard normal distribution. ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{1}{2}Q\\left(\\frac{T r(\\Sigma_{1}+\\Sigma_{2})^{-1}\\sqrt{1-r}}{\\sqrt{2T r(\\Sigma_{1}(\\Sigma_{1}+\\Sigma_{2})^{-2})}}\\right)+\\frac{1}{2}Q\\left(\\frac{T r(\\Sigma_{1}+\\Sigma_{2})^{-1}\\sqrt{1-r}}{\\sqrt{2T r(\\Sigma_{2}(\\Sigma_{1}+\\Sigma_{2})^{-2})}}\\right)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In view of Lemma 1 it is natural to introduce the following assumption: ", "page_idx": 7}, {"type": "text", "text": "Assumptions 4. ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The initialization point $\\mathbf{w}_{0}$ satisfies $\\mathbf{w}_{0}=t_{*}\\mathbf{w}_{*}\\!+\\!t_{\\eta}\\eta_{*}$ , where $\\mathbf{w}_{*}$ is defined as in Lemma 1, $\\|\\boldsymbol{\\eta}\\|^{2}=1\\!-\\!r$ and for any deterministic matrix $\\mathbf{C}$ of bounded operator norm it holds that $\\eta^{T}\\mathbf{C}\\eta$ converges in probability Tr(C) \ud835\udc51(1\u2212\ud835\udc5f). Note that the smaller the ratio $\\frac{t_{\\eta}}{t_{*}}$ is, the better performance $w_{0}$ has. ", "page_idx": 7}, {"type": "text", "text": "Indeed, since $\\mathbf{w}_{0}$ is an initialization point obtained from a pre-trained model we expect it to perform much better than a random point from the weight space, meaning that it cannot be too far from the optimal classifier. ", "page_idx": 7}, {"type": "text", "text": "Finally, we would like to remark that, under notation and assumptions from Theorem 1, the following corollary is implied by Theorem 1 modulo an additional argument presented in the appendix: ", "page_idx": 7}, {"type": "text", "text": "Corollary 1 (Universality of the classification error for SGD and ridge regression objectives). If $f(\\mathbf{w})\\,=\\,\\|\\mathbf{w}-\\mathbf{w}_{0}\\|_{2}^{2}.$ , when $\\|\\mathbf{w}_{0}\\|_{2}^{2}=O(d)$ then for a, g independent of A, $\\mathbf{G}$ but sampled from the same distribution as their rows, respectively, we have with high probability for $\\Psi\\in\\{\\Phi_{\\lambda},\\Phi\\}$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\rightarrow\\infty}\\biggl|\\mathbb{P}(\\mathbf{a}^{T}\\mathbf{w}_{\\Psi(\\mathbf{A})}>0)-\\mathbb{P}(\\mathbf{g}^{T}\\mathbf{w}_{\\Psi(\\mathbf{G})}>0)\\biggr|=0\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "4.3 Main Result ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Theorem 3. Let $\\mathbf{w}_{0}$ be an initialization point satisfying Assumption 4 and $\\mathbf{X}$ be a data matrix satisfying Assumptions 3. Then the classification error of the SGD solution initialized at \ud835\udefcw0 for $\\alpha$ defined by (10) and trained on $\\mathbf{X},\\mathbf{y}$ is given by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{Q}\\left(\\frac{-\\gamma+2}{2\\sqrt{\\tau^{2}-\\frac{\\gamma^{2}}{4}}}\\right)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where \ud835\udefeis determined by the following equations that include an additional variable \ud835\udf03and $\\tau$ has a closed form expression provided in the Appendix (cf. Section $H_{\\mathrm{:}}$ , equation $^{40}$ ): ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{2\\sqrt{2}}{\\theta\\sqrt{n}}S_{\\Sigma_{1}+\\Sigma_{2}}(-\\frac{2\\sqrt{2}}{\\theta\\sqrt{n}})=\\frac{d-n}{d}}\\\\ {\\displaystyle\\gamma=\\left(\\frac{1}{2}+\\frac{\\sqrt{n}\\theta(1-r)(d-n)}{8d}\\right)^{-1}\\left(1-\\alpha\\int\\int p(s_{1},s_{2})\\frac{t_{*}(1-r)(s_{1}+s_{2})^{-1}}{1+\\frac{\\theta}{2}\\sqrt{\\frac{n}{2}}(s_{1}+s_{2})}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The expressions from Theorem 3 can be simplified drastically in the case of scalar covariance matrices. ", "page_idx": 8}, {"type": "text", "text": "Corollary 2. Under the notation from Theorem 3, assume $\\begin{array}{r}{\\Sigma_{1}=\\Sigma_{2}=\\frac{\\sigma^{2}\\mathbf{I}_{d}}{d}}\\end{array}$ and define $\\begin{array}{r}{\\rho:=\\frac{n(1-r)}{\\sigma^{2}}}\\end{array}$ . Let $e_{a}$ be the classification error of $\\mathbf{w}_{0}$ . Then the following equalities hold: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\gamma=\\frac{\\frac{1}{\\kappa}+\\frac{\\kappa-1}{\\kappa}\\frac{1}{Q^{-1}(e_{a})^{2}+1}}{\\frac{1}{2}+\\frac{\\rho}{4}}}}\\\\ {{\\displaystyle\\tau^{2}=\\frac{\\kappa}{\\kappa-1}\\left[-\\biggl(\\frac{\\rho}{4}+\\frac{1}{2\\kappa}\\biggr)\\biggl(1+\\frac{\\rho}{4}\\biggr)\\gamma^{2}+\\biggl(\\frac{1}{\\kappa}+\\frac{1+\\kappa}{\\kappa}\\biggl(\\frac{1}{2}+\\frac{\\rho}{4}\\biggr)\\biggr)\\,\\gamma-\\frac{1}{\\kappa}\\right]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We arrive at the following conclusion summarizing the derivations above. It essentially says that transfer learning has a chance of being fruitful only if $\\rho=\\Theta(1)$ . ", "page_idx": 8}, {"type": "text", "text": "Remark 2. \u2022 If $\\rho\\,=\\,\\omega(1)$ , then the classification error of the SGD goes to 0 as $n\\,\\rightarrow\\,\\infty$ even for a random initialization with $\\begin{array}{r}{e_{a}\\approx\\frac{1}{2}}\\end{array}$ . Therefore, there is no need to apply transfer learning in this regime. ", "page_idx": 8}, {"type": "text", "text": "\u2022 $I f\\,\\rho\\,\\,=\\,\\Theta(1)$ , then the fine-tuning step may or may not succeed depending on whether $\\scriptstyle Q\\displaystyle(\\frac{1-\\frac{\\gamma}{2}}{\\sqrt{\\tau^{2}-\\frac{\\gamma^{2}}{4}}})\\ <\\ e_{a}$ or not. See Section 5.2 for further (empirical) explorations on the usefulness of the fine-tuning of the pre-trained solution for this regime. \u2022 $H\\rho=o(1)$ , then the clas\u221asification error of any linear classifier goes to $\\frac{1}{2}$ as $n\\to\\infty$ since it is lower bounded by $Q({\\sqrt{\\rho}})$ according to Lemma 1. Thus, any kind of learning will fail in this regime. ", "page_idx": 8}, {"type": "text", "text": "5 Numerical experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "5.1 Regression ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To corroborate our findings, we plotted the generalization error of the weight obtained through running SGD according to the Assumptions 2 in Section 4.1 with respect to $\\textstyle\\kappa={\\frac{d}{n}}$ . To do so, we fixed $d\\,=\\,1000$ and varied $n$ across different values. We used CVXPY (Grant and Boyd [2014], Agrawal et al. [2018]) to solve (1) efficiently on a Laptop CPU. To verify the universality of our results, we initially constructed a centered random matrix $\\mathbf{X}^{\\prime}$ with i.i.d components according to the distributions $N(0,1),\\,B e r(0.5)$ , and $\\chi^{2}(1)$ and using a correlation matrix $\\mathbf{R}_{x}$ we defined $\\mathbf{X}:=\\mathbf{X}^{\\prime}\\mathbf{R}_{x}^{1/2}$ . On the other hand, we generated $\\mathbf{R}_{x}$ according to the following three distributions: single level $p(r):=\\delta(r-1)$ , bilevel $p(r):=0.3\\delta(r-1)+0.7\\delta(r-5)$ and uniform on the interval [1, 5]. We specifically consider these cases as they are common in the literature and we use the parameter $\\sigma^{\\frac{\\gamma}{2}}$ for the component-wise variance of $\\mathbf{z}$ in 7. Additionally, $\\mathbf{w}_{0}$ is chosen according to Assumptions 2 in such a manner that $e_{a}\\,=\\,1\\$ . In both Figures 1, 2 the blue line represents the prediction 8 made by Theorem 2, the red line depicts the lower bound 9. The markers showcase the performance of weights obtained under different distributions as described earlier. It can be seen that from Figures 1, 2, for the bilevel and uniform distributions, depending on the value of $\\sigma$ , transfer learning might not be beneficial as discussed in Remark 1. In particular, in Figures 1c and $_{2\\mathrm{c}}$ , the generalization error is always lower bounded by $e_{a}=1$ and only in $\\kappa\\to\\infty$ can get close to 1. Finally, the single-level distribution on $\\mathbf{R}_{x}$ is always a lower bound for the generalization error of various distributions on $\\mathbf{R}_{x}$ . ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.2 Classification ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Similar to the preceding subsection, we experimented with sampling the entries of $\\mathbf{X}$ independently from three different centered distributions: normal, Bernoulli, and $\\bar{\\chi}^{2}$ . We also sampled the means $\\pmb{\\mu}_{1}$ and $\\pmb{\\mu}_{2}$ from $N(0,{\\frac{1}{d}}\\mathbf{I}_{d})$ with a cross-correlation $r\\,=\\,\\mathbb{E}[\\pmb{\\mu}_{1i}\\pmb{\\mu}_{2i}]\\,=\\,0.9$ and added them to the corresponding rows of $\\mathbf{X}$ . For Figures 3a and 3b, we fixed $\\rho=1$ and $\\rho=2$ respectively and plotted the classification error predicted by Corollary 2 as a solid red line, empirically observed classification errors for the normal, Bernoulli and $\\chi^{2}$ entries as black squares, green circles and red triangles respectively. The blue lines depict the classification error at the initialization. It can be seen that there is a close match between the empirical errors between points from different distributions as well as with the theoretical prediction, thus validating both Theorem 1 and Theorem 3. It is also worth mentioning that fine-tuning improves performance in the setting of Figure 3a but hurts it for Figure 3b. Figure 3c follows the same conventions as Figures 3a and 3b but it fixes $\\kappa=2$ and depicts plots the error for different values of $\\rho$ instead, serving both as another validation of our main results and as an illustration of Remark 2. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "image", "img_path": "MhWaMOkoN3/tmp/951a86a00c779a861bafcd8fd12b7318bf629c8a8903d9b52804eda84b79d309.jpg", "img_caption": ["Figure 1: Generalization error for the bilevel distribution "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "MhWaMOkoN3/tmp/33abc4f2c9fa41b92fbc6bea7af0911b9bc4a89c39eab0bf0128e6f8e77a07bc.jpg", "img_caption": ["Figure 2: Generalization error for the uniform distribution "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "MhWaMOkoN3/tmp/2f55c548ef8a8a07bb0f98129d6e19596e96f8a41b385e7c096e740882cfbd12.jpg", "img_caption": ["Figure 3: Classification error "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion and future work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We presented a novel Gaussian universality result and used it to study the problem of transfer learning in linear models, for both regression and binary classification. In particular, we were able to precisely relate the performance of the pretrained model to that of the fine-tuned model trained via SGD and, as a result, identified situations where transfer learning helps (or does not). Possible future directions include investigating other problems where the universality result my be useful, extending the results to potential functions that are not necessarily convex nor separable, as well as exploring the implications of universality for objectives with explicit regularization. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "E. Abbasi, F. Salehi, and B. Hassibi. Universality in learning from linear measurements. Advances in Neural Information Processing Systems, 32, 2019.   \nA. Agrawal, R. Verschueren, S. Diamond, and S. Boyd. A rewriting system for convex optimization problems. Journal of Control and Decision, 5(1):42\u201360, 2018.   \nD. Akhtiamov, D. Bosch, R. Ghane, K. N. Varma, and B. Hassibi. A novel gaussian min-max theorem and its applications. arXiv preprint arXiv:2402.07356, 2024.   \nN. Azizan and B. Hassibi. Stochastic gradient/mirror descent: Minimax optimality and implicit regularization. In International Conference on Learning Representations, 2018.   \nS. G. Bobkov. On concentration of distributions of random weighted sums. Annals of probability, pages 195\u2013215, 2003.   \nD. Bosch, A. Panahi, and B. Hassibi. Precise asymptotic analysis of deep random feature models. In The Thirty Sixth Annual Conference on Learning Theory, pages 4132\u20134179. PMLR, 2023.   \nS. Bozinovski. Reminder of the first paper on transfer learning in neural networks, 1976. Informatica, 44(3), 2020.   \nS. Chatterjee. A generalization of the lindeberg principle. 2006.   \nW. Dai, Q. Yang, G.-R. Xue, and Y. Yu. Boosting for transfer learning. In Proceedings of the 24th international conference on Machine learning, pages 193\u2013200, 2007.   \nY. Dandi, L. Stephan, F. Krzakala, B. Loureiro, and L. Zdeborov\u00e1. Universality laws for gaussian mixtures in generalized linear models. Advances in Neural Information Processing Systems, 36, 2024.   \nY. Dar, D. LeJeune, and R. G. Baraniuk. The common intuition to transfer learning can win or lose: Case studies for linear regression. arXiv preprint arXiv:2103.05621, 2021.   \nF. Gerace, L. Saglietti, S. S. Mannelli, A. Saxe, and L. Zdeborov\u00e1. Probing transfer learning with a model of synthetic correlated datasets. Machine Learning: Science and Technology, 3(1):015030, 2022.   \nM. Grant and S. Boyd. CVX: Matlab software for disciplined convex programming, version 2.1, Mar. 2014.   \nS. Gunasekar, J. Lee, D. Soudry, and N. Srebro. Characterizing implicit bias in terms of optimization geometry. In J. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1832\u20131841. PMLR, 10\u201315 Jul 2018. URL https://proceedings.mlr.press/ v80/gunasekar18a.html.   \nQ. Han and Y. Shen. Universality of regularized regression estimators in high dimensions. The Annals of Statistics, 51(4):1799\u20131823, 2023.   \nT. Hastie, A. Montanari, S. Rosset, and R. J. Tibshirani. Surprises in high-dimensional ridgeless least squares interpolation. Annals of statistics, 50(2):949, 2022.   \nH. Hu and Y. M. Lu. Universality laws for high-dimensional learning with random features. IEEE Transactions on Information Theory, 69(3):1932\u20131964, 2022.   \nA. Jain, A. Montanari, and E. Sasoglu. Scaling laws for learning with real and surrogate data. arXiv preprint arXiv:2402.04376, 2024.   \nS. Lahiry and P. Sur. Universality in block dependent linear models with applications to nonparametric regression. arXiv preprint arXiv:2401.00344, 2023.   \nF. Liese and K.-J. Miescke. Statistical decision theory. In Statistical Decision Theory: Estimation, Testing, and Selection, pages 1\u201352. Springer, 2008.   \nJ. W. Lindeberg. Eine neue herleitung des exponentialgesetzes in der wahrscheinlichkeitsrechnung. Mathematische Zeitschrift, 15(1):211\u2013225, 1922.   \nA. Montanari and P.-M. Nguyen. Universality of the elastic net error. In 2017 IEEE International Symposium on Information Theory (ISIT), pages 2338\u20132342. IEEE, 2017.   \nA. Montanari and B. N. Saeed. Universality of empirical risk minimization. In Conference on Learning Theory, pages 4310\u20134312. PMLR, 2022.   \nS. Oymak and J. A. Tropp. Universality laws for randomized dimension reduction, with applications. Information and Inference: A Journal of the IMA, 7(3):337\u2013446, 2018.   \nA. Panahi and B. Hassibi. A universal analysis of large-scale regularized least squares solutions. Advances in Neural Information Processing Systems, 30, 2017.   \nD. Schr\u00f6der, H. Cui, D. Dmitriev, and B. Loureiro. Deterministic equivalent and error universality of deep random features learning. In International Conference on Machine Learning, pages 30285\u201330320. PMLR, 2023.   \nM. E. A. Seddik, C. Louart, M. Tamaazousti, and R. Couillet. Random matrix theory proves that deep learning representations of gan-data behave as gaussian mixtures. In International Conference on Machine Learning, pages 8573\u20138582. PMLR, 2020.   \nC. Tan, F. Sun, T. Kong, W. Zhang, C. Yang, and C. Liu. A survey on deep transfer learning. In Artificial Neural Networks and Machine Learning\u2013ICANN 2018: 27th International Conference on Artificial Neural Networks, Rhodes, Greece, October 4-7, 2018, Proceedings, Part III 27, pages 270\u2013279. Springer, 2018.   \nR. Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press, 2018.   \nF. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, and Q. He. A comprehensive survey on transfer learning. Proceedings of the IEEE, 109(1):43\u201376, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Proof of part 1 Theorem 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section we provide the complete proof of part 1 of Theorem 1. ", "page_idx": 12}, {"type": "text", "text": "A.1 Proof of part 1 of Theorem 1 when $\\Psi=\\Phi_{\\lambda}$ ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We prove the universality of the objective value and then use it to prove the universality of a $\\psi$ applied to the optimal solutions. To do so, we construct the perturbed objective. Let ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\Phi_{\\lambda,\\epsilon}(\\mathbf{A}):=\\frac{1}{n}\\operatorname*{min}_{w\\in S_{w}}\\frac{\\lambda}{2}\\|\\mathbf{A}\\mathbf{w}-\\mathbf{y}\\|^{2}+f(\\mathbf{w})+\\epsilon\\psi(\\mathbf{w})\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Where $|\\epsilon|$ is small enough that $\\epsilon\\psi(\\mathbf{w})+f(\\mathbf{w})$ is $\\rho$ -strongly convex. For such $\\epsilon$ \u2019s, we will show that $\\Phi_{\\lambda,\\epsilon}(\\mathbf{A})$ and $\\Phi_{\\lambda,\\epsilon}(\\mathbf{B})$ converge in probability to the same value and use that to deduce a similar result involving $\\psi(\\mathbf{w}_{\\Phi_{\\lambda,0}(\\mathbf{A})}),\\psi_{\\_}(\\mathbf{w}_{\\Phi_{\\lambda,0}(\\mathbf{B})})$ . A key step is to reduce the proof to upper bounding the difference of expectations, to do so, the following proposition will be instrumental whose proof is given in Appendix $\\mathbf{C}$ . ", "page_idx": 12}, {"type": "text", "text": "Proposition 1. If for any Lipschitz function $g:\\mathbb{R}\\to\\mathbb{R}$ and $t_{1}>0$ , $\\mathbb{P}\\Big(|g(\\Phi_{\\lambda,\\epsilon}(\\mathbf{B}))-c|>t_{1}\\Big)\\rightarrow0$ and for every twice differentiable $\\tilde{g}:\\mathbb{R}\\to\\mathbb{R}$ with bounded second derivative, we have that $\\begin{array}{r}{\\operatorname*{lim}_{n\\rightarrow\\infty}\\Bigl|\\mathbb{E}_{{\\mathbf{A}},{\\mathbf{y}}}\\tilde{g}(\\Phi_{\\lambda,\\epsilon}({\\mathbf{B}}))-\\mathbb{E}_{{\\mathbf{B}},{\\mathbf{y}}}\\tilde{g}(\\Phi_{\\lambda,\\epsilon}({\\mathbf{A}}))\\Bigr|\\rightarrow0}\\end{array}$ then for any $t_{2}>t_{1}$ . ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\mathbb{P}\\Big(|g(\\Phi_{\\lambda,\\epsilon}(\\mathbf{A}))-c|>t_{2}\\Big)=0\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Thus it suffices to prove $\\begin{array}{r}{\\operatorname*{lim}_{n\\rightarrow\\infty}\\Bigl|\\mathbb{E}_{{\\mathbf{A}},{\\mathbf{y}}}g(\\Phi_{\\lambda}({\\mathbf{B}}))-\\mathbb{E}_{{\\mathbf{B}},{\\mathbf{y}}}g(\\Phi_{\\lambda}({\\mathbf{A}}))\\Bigr|=0}\\end{array}$ for every twice differentiable $g$ with bounded second derivative. As stated earlier, we proceed by Lindeberg\u2019s approach. Fixing $\\lambda$ and $\\epsilon$ , to simplify notation we use $\\Phi(\\mathbf{A})$ for $\\Phi_{\\lambda,\\epsilon}(\\mathbf{A})$ . For $0\\le j\\le n$ , let $q\\in\\mathbb{N}$ be the largest such that $\\begin{array}{r}{j\\ge\\sum_{l=1}^{q}n_{l}}\\end{array}$ , and let $\\begin{array}{r}{r=j-\\sum_{l=1}^{q}n_{l}}\\end{array}$ if , then ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{A}}_{j}=\\left\\{\\left[\\mathbf{a}_{1,1}\\quad\\mathbf{a}_{1,2}\\quad.\\dots\\quad\\mathbf{a}_{q,r}\\quad\\mathbf{b}_{q,r+1}\\quad.\\dots\\quad\\mathbf{b}_{k,n_{k}}\\right]^{T}\\quad r<n_{q+1}\\quad.\\mathrm{~a~n~d~}\\quad\\left[\\mathbf{a}_{1,1}\\right]^{T},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "It follwos that $\\mathbf{A}=\\hat{\\mathbf{A}}_{0}$ and $\\mathbf{B}=\\hat{\\mathbf{A}}_{n}$ . Then we have by a telescopic sum ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Big|\\mathbb{E}_{{\\mathbf A},{\\mathbf y}}g(\\Phi({\\mathbf A}))-\\mathbb{E}_{{\\mathbf B},{\\mathbf y}}g(\\Phi({\\mathbf B}))\\Big|=\\Big|\\mathbb{E}_{{\\mathbf A},{\\mathbf B},{\\mathbf y}}\\sum_{j=0}^{n-1}g(\\Phi(\\hat{\\mathbf A}_{j}))-g(\\Phi(\\hat{\\mathbf A}_{j-1}))\\Big|}&{}\\\\ {\\le\\displaystyle\\sum_{j=0}^{n-1}\\Big|\\mathbb{E}_{\\hat{\\mathbf A}_{j},{\\mathbf y}}g(\\Phi(\\hat{\\mathbf A}_{j}))-\\mathbb{E}_{\\hat{\\mathbf A}_{j-1},{\\mathbf y}}g(\\Phi(\\hat{\\mathbf A}_{j-1}))\\Big|}&{~}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Now we define a new matrix by dropping the $j^{;}$ \u2019th row. ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{A}}_{j}=\\left\\{\\left[\\mathbf{a}_{1,1}\\quad\\mathbf{a}_{1,2}\\quad\\cdot\\cdot\\cdot\\quad\\mathbf{a}_{q,r-1}\\quad\\mathbf{b}_{q,r+1}\\quad\\cdot\\cdot\\cdot\\quad\\mathbf{b}_{k,n_{k}}\\right]^{T}\\quad r<n_{q+1}\\right\\}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Note that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Phi(\\hat{\\mathbf{A}}_{j})=\\displaystyle\\frac{1}{n}\\operatorname*{min}_{w\\in S_{w}}\\frac{\\lambda}{2}\\|\\hat{\\mathbf{A}}_{j}\\mathbf{w}-\\mathbf{y}\\|^{2}+f(\\mathbf{w})+\\epsilon\\psi(\\mathbf{w})}\\\\ &{\\quad\\quad\\quad=\\displaystyle\\frac{1}{n}\\operatorname*{min}_{w\\in S_{w}}\\frac{\\lambda}{2}\\|\\tilde{\\mathbf{A}}_{j}\\mathbf{w}-\\tilde{\\mathbf{y}}\\|^{2}\\!+\\!\\frac{\\lambda}{2}(\\mathbf{a}_{j}^{T}\\mathbf{w}-\\boldsymbol{y}_{j})^{2}+f(\\mathbf{w})+\\epsilon\\psi(\\mathbf{w})=:\\mathcal{M}(\\mathbf{a}_{j})}\\\\ &{\\Phi(\\hat{\\mathbf{A}}_{j-1})=\\displaystyle\\frac{1}{n}\\operatorname*{min}_{w\\in S_{w}}\\frac{\\lambda}{2}\\|\\hat{\\mathbf{A}}_{j}\\mathbf{w}-\\mathbf{y}\\|^{2}+f(\\mathbf{w})+\\epsilon\\psi(\\mathbf{w})}\\\\ &{\\quad\\quad\\quad=\\displaystyle\\frac{1}{n}\\operatorname*{min}_{w\\in S_{w}}\\frac{\\lambda}{2}\\|\\tilde{\\mathbf{A}}_{j}\\mathbf{w}-\\tilde{\\mathbf{y}}\\|^{2}\\!+\\!\\frac{\\lambda}{2}(\\mathbf{b}_{j}^{T}\\mathbf{w}-\\boldsymbol{y}_{j})^{2}+f(\\mathbf{w})+\\epsilon\\psi(\\mathbf{w})=:\\mathcal{M}(\\mathbf{b}_{j})}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "And we define $\\begin{array}{r l r}{{\\cal M}({\\bf a})}&{{}:=}&{\\operatorname*{min}_{{\\bf w}\\in{\\cal S}_{w}}m({\\bf a},{\\bf w}).}\\end{array}$ . So, we need to bound $\\mathbb{E}_{\\hat{\\mathbf{A}}_{j},\\mathbf{y}}g(\\Phi(\\hat{\\mathbf{A}}_{j}))\\ -$ $\\mathbb{E}_{\\hat{\\mathbf{A}}_{j-1},\\mathbf{y}}g(\\Phi(\\hat{\\mathbf{A}}_{j-1}))\\Big|;$ , to do so, we condition on $\\tilde{\\mathbf{A}}_{j}$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{B}_{\\hat{\\mathbf{A}}_{j},\\mathbf{y}}g(\\Phi(\\hat{\\mathbf{A}}_{j}))-\\mathbb{B}_{\\hat{\\mathbf{A}}_{j-1},\\mathbf{y}}g(\\Phi(\\hat{\\mathbf{A}}_{j-1}))\\right|=\\left|\\mathbb{B}_{\\tilde{\\mathbf{A}}_{j},\\tilde{\\mathbf{y}}}\\mathbb{B}_{\\mathbf{a}_{j},\\mathbf{b}_{j},\\mathbf{y}_{j}}\\left[g(M(\\mathbf{a}_{j}))-g(M(\\mathbf{b}_{j}))\\right|\\tilde{\\mathbf{A}}_{j},\\tilde{\\mathbf{y}}\\right]\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Let us define the optimization whose terms are shared by both of $\\mathcal{M}(\\mathbf{a}_{j})$ and $\\mathcal{M}(\\mathbf{b}_{j})$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\Theta:=\\frac{1}{n}\\operatorname*{min}_{\\mathbf{w}\\in S_{w}}\\frac{\\lambda}{2}\\|\\tilde{\\mathbf{A}}_{j}\\mathbf{w}-\\tilde{\\mathbf{y}}\\|^{2}+f(\\mathbf{w})+\\epsilon\\psi(\\mathbf{w})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Now note that since the second derivative of $g$ is bounded, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|g(\\Theta)-g(\\mathcal{M}(\\mathbf{a}_{j}))-g^{\\prime}(\\Theta)(\\mathcal{M}(\\mathbf{a}_{j})-\\Theta)\\right|\\leq\\|g^{\\prime\\prime}\\|_{\\infty}(\\Theta-\\mathcal{M}(\\mathbf{a}_{j}))^{2}}\\\\ {\\left|g(\\Theta)-g(\\mathcal{M}(\\mathbf{b}_{j}))-g^{\\prime}(\\Theta)(\\mathcal{M}(\\mathbf{b}_{j})-\\Theta)\\right|\\leq\\|g^{\\prime\\prime}\\|_{\\infty}(\\Theta-\\mathcal{M}(\\mathbf{b}_{j}))^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Adding and subtracting $g(\\Theta)$ and taking expectation and moving inside, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{B}_{\\tilde{\\mathbf{A}}_{j},\\tilde{\\mathbf{y}}}\\mathbb{B}_{\\mathbf{a}_{j},\\mathbf{b}_{j},y_{j}}\\right|g({\\mathcal{M}}(\\mathbf{a}_{j}))-g({\\mathcal{M}}(\\mathbf{b}_{j}))\\Bigg|\\tilde{\\mathbf{A}}_{j},\\tilde{\\mathbf{y}}\\Bigg|\\Bigg|}\\\\ &{\\qquad\\qquad\\quad\\leq\\left|\\mathbb{B}_{\\tilde{\\mathbf{A}}_{j},\\tilde{\\mathbf{y}}}\\mathbb{B}_{\\mathbf{a}_{j},\\mathbf{b}_{j},y_{j}}\\right[g(\\Theta)-g({\\mathcal{M}}(\\mathbf{a}_{j}))-g^{\\prime}(\\Theta)({\\mathcal{M}}(\\mathbf{a}_{j})-\\Theta)\\bigg|\\tilde{\\mathbf{A}}_{j},\\tilde{\\mathbf{y}}\\right]\\Bigg|}\\\\ &{\\qquad\\qquad\\quad+\\left|\\mathbb{B}_{\\tilde{\\mathbf{A}}_{j},\\tilde{\\mathbf{y}}}\\mathbb{B}_{\\mathbf{a}_{j},\\mathbf{b}_{j},y_{j}}\\left[g(\\Theta)-g({\\mathcal{M}}(\\mathbf{b}_{j}))-g^{\\prime}(\\Theta)({\\mathcal{M}}(\\mathbf{b}_{j})-\\Theta)\\bigg|\\tilde{\\mathbf{A}}_{j},\\tilde{\\mathbf{y}}\\right]\\right|}\\\\ &{\\qquad\\qquad\\quad+\\left|\\mathbb{B}_{\\tilde{\\mathbf{A}}_{j},\\tilde{\\mathbf{y}}}\\mathbb{B}_{\\mathbf{a}_{j},\\mathbf{b}_{j},y_{j}}\\left[g^{\\prime}(\\Theta)({\\mathcal{M}}(\\mathbf{a}_{j})-{\\mathcal{M}}(\\mathbf{b}_{j}))\\right|\\tilde{\\mathbf{A}}_{j},\\tilde{\\mathbf{y}}\\right]\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Using 13, and by the independence of $\\Theta$ and $\\mathbf{a}_{j},\\mathbf{b}_{j},\\boldsymbol{\\mathrm{y}}_{j}$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\tilde{\\mathbf{A}}_{j},\\tilde{\\mathbf{y}}}\\mathbb{E}_{\\mathbf{a}_{j},\\mathbf{b}_{j},\\boldsymbol{y}_{j}}\\bigg[g(M(\\mathbf{a}_{j}))-g(M(\\mathbf{b}_{j}))\\bigg|\\tilde{\\mathbf{A}}_{j},\\tilde{\\mathbf{y}}\\bigg]\\bigg|\\leq\\bigg|\\mathbb{E}_{\\tilde{\\mathbf{A}}_{j},\\tilde{\\mathbf{y}}}g^{\\prime}(\\mathbf{b})\\mathbb{E}_{\\mathbf{a}_{j},\\mathbf{b}_{j},\\boldsymbol{y}_{j}}\\bigg[(M(\\mathbf{a}_{j})-M(\\mathbf{b}_{j}))\\bigg|\\tilde{\\mathbf{A}}_{j},\\tilde{\\mathbf{y}}\\bigg]\\bigg|}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left\\|g^{\\prime\\prime}\\right\\|_{\\infty}\\mathbb{E}_{\\tilde{\\mathbf{A}}_{j},\\tilde{\\mathbf{y}},\\mathbf{a},\\boldsymbol{y}_{j}}(\\mathbf{e}-M(\\mathbf{a}_{j}))^{2}+\\left\\|g^{\\prime\\prime}\\right\\|_{\\infty}\\mathbb{E}_{\\tilde{\\mathbf{A}}_{j},\\tilde{\\mathbf{y}},\\boldsymbol{y}_{j}}(\\mathbf{e}-M(\\mathbf{b}_{j}))^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\mathbb{E}_{\\tilde{\\mathbf{A}}_{j},\\boldsymbol{y}}|g^{\\prime}(\\boldsymbol{\\Theta})\\bigg|\\bigg|\\mathbb{B}_{\\mathbf{a}_{j},\\mathbf{b}_{j},\\boldsymbol{y}_{j}}\\bigg[(M(\\mathbf{a}_{j})-M(\\mathbf{b}_{j}))\\bigg|\\tilde{\\mathbf{A}}_{j},\\tilde{\\mathbf{y}}\\bigg]\\bigg|}\\\\ &{\\qquad\\qquad\\qquad+\\left\\|g^{\\prime\\prime}\\right\\|_{\\infty}\\!\\bigg(\\!\\mathbb{E}_{\\tilde{\\mathbf{A}}_{j},\\tilde{\\mathbf{y}},\\mathbf{a},\\boldsymbol{y}_{j}}(\\boldsymbol{\\Theta}-M(\\mathbf{a}_{j}))^{2}+\\mathbb{E}_{\\tilde{\\mathbf{A}}_{j},\\tilde{\\mathbf{y}},\\mathbf{b}_{j},\\boldsymbol{y}_{j}}(\\boldsymbol{\\Theta}-M(\\mathbf{b}_{j}))^{2}\\bigg)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left\\|g^{\\prime}\\right\\|_{\\infty}\\!\\mathbb{E}_{\\tilde{\\mathbf{A}}_{j},\\tilde{\\mathbf{y}}}\\bigg|\\mathbb{B}_{\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Thus we focus on $\\mathcal{M}(\\mathbf{a}_{j}),\\mathcal{M}(b_{j})$ conditioned on $\\tilde{\\mathbf{A}}_{j}$ . From now on, we drop the index $j$ from $\\tilde{\\mathbf{A}}_{j}$ , $\\mathbf{a}_{j},\\,\\mathbf{b}_{j},\\,y_{j}$ . Intuitively, we show that by dropping $(\\bar{\\mathbf{a}}^{T}\\mathbf{w}-\\boldsymbol{y})^{2}$ and $(\\mathbf{b}^{T}\\mathbf{w}-\\boldsymbol{y})^{2}$ and approximating $f,\\psi$ by their second-order Taylor expansion, the objective values $\\Phi(\\hat{\\mathbf{A}}_{j}),\\Phi(\\hat{\\mathbf{A}}_{j-1})$ would not change much. Which implies $\\Phi(\\hat{\\mathbf{A}}_{j})$ and $\\Phi(\\hat{\\mathbf{A}}_{j})$ are also close in value. ", "page_idx": 13}, {"type": "text", "text": "Let \ud835\udc62:= $\\begin{array}{r}{\\arg\\operatorname*{min}\\frac{\\lambda}{2}||\\tilde{\\mathbf{A}}\\mathbf{w}-\\tilde{\\mathbf{y}}||^{2}+f(\\mathbf{w})+\\epsilon\\psi(\\mathbf{w})}\\end{array}$ and it is unique because of strong convexity. Now we use the second order Taylor expansion of $f+\\epsilon\\psi$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{f(\\mathbf{w})+\\epsilon\\psi(\\mathbf{w})=f(\\mathbf{u})+\\epsilon\\psi(\\mathbf{u})+\\nabla(f(\\mathbf{u})+\\epsilon\\psi(\\mathbf{u}))^{T}(\\mathbf{w}-\\mathbf{u})}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad+\\,\\frac{1}{2}(\\mathbf{w}-\\mathbf{u})^{T}\\nabla^{2}(f(\\mathbf{u})+\\epsilon\\psi(\\mathbf{u}))(\\mathbf{w}-\\mathbf{u})+R(\\mathbf{w}-\\mathbf{u})}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Where limw\u2192u\u2225w\u2212u\u22252 $\\begin{array}{r}{\\operatorname*{lim}_{\\mathbf{w}\\rightarrow\\mathbf{u}}\\frac{R\\left(\\mathbf{w}-\\mathbf{u}\\right)}{\\|\\mathbf{w}-\\mathbf{u}\\|^{2}}=0.}\\end{array}$ Let $\\mathbf{g}:=\\nabla(f(\\mathbf{u})+\\epsilon\\psi(\\mathbf{u})),\\mathbf{H}:=\\nabla^{2}(f(\\mathbf{u})+\\epsilon\\psi(\\mathbf{u}))$ . Moreover, let ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(\\mathbf{a}):=\\underset{\\mathbf{w}\\in S_{w}}{\\mathrm{min}}\\;\\ell(\\mathbf{a},\\mathbf{w}):=\\frac{1}{n}\\underset{\\mathbf{w}\\in S_{w}}{\\mathrm{min}}\\;\\frac{\\lambda}{2}\\|\\tilde{\\mathbf{A}}\\mathbf{w}-\\tilde{\\mathbf{y}}\\|^{2}+\\frac{\\lambda}{2}(\\mathbf{a}^{T}\\mathbf{w}-\\mathbf{y})^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,f(\\mathbf{u})+\\epsilon\\psi(\\mathbf{u})+\\mathbf{g}^{T}(\\mathbf{w}-\\mathbf{u})+\\frac{1}{2}(\\mathbf{w}-\\mathbf{u})^{T}\\mathbf{H}(\\mathbf{w}-\\mathbf{u})}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Note that a and $\\mathbf{b}$ match in distribution up to the second moment, which implies that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbf{a},y}(\\mathbf{a}^{T}\\mathbf{u}-y)^{2}=\\mathbb{E}_{\\mathbf{b},y}(\\mathbf{b}^{T}\\mathbf{u}-y)^{2}}\\\\ &{\\qquad\\mathbb{E}_{\\mathbf{a}}\\mathbf{a}^{T}\\underline{{\\Omega}}^{-1}\\mathbf{a}=\\mathbb{E}_{\\mathbf{b}}\\mathbf{b}^{T}\\underline{{\\Omega}}^{-1}\\mathbf{b}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By triangle inequality we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{\\hat{\\lambda}_{\\hat{\\lambda}_{\\hat{\\lambda}_{\\hat{\\lambda}}}}}\\bigg\\lvert\\overline{{\\mathbf{A}}}_{0,b}\\bigg\\rvert\\bigg[(M({\\mathbf a})-M({\\mathbf b}))\\bigg\\lvert\\hat{\\lambda}_{\\hat{\\lambda}},\\hat{{\\mathbf y}}\\bigg]\\bigg]}\\\\ &{\\leq\\mathbb{E}_{\\hat{\\lambda}_{\\hat{\\lambda}_{\\hat{\\lambda}_{\\hat{\\lambda}}}}}\\bigg\\lvert\\overline{{\\mathbf{A}}}_{0,b}\\bigg\\rvert\\bigg[\\Big(M({\\mathbf a})-\\mathcal{L}({\\mathbf a})\\Big)\\bigg\\lvert\\hat{\\lambda}_{\\hat{\\lambda}},\\hat{{\\mathbf y}}\\bigg\\rvert+\\mathbb{E}_{\\hat{\\lambda}_{\\hat{\\lambda}_{\\hat{\\lambda}}}}\\bigg\\lvert\\overline{{\\mathbf{A}}}_{0,b}\\bigg\\rvert\\bigg(M({\\mathbf b})-\\mathcal{L}({\\mathbf b})\\Big)\\bigg\\rvert\\bigg\\rvert\\tilde{\\mathbf{A}},\\hat{{\\mathbf y}}\\bigg]\\bigg\\rvert}\\\\ &{+\\mathbb{E}_{\\hat{\\lambda}_{\\hat{\\lambda}_{\\hat{\\lambda}_{\\hat{\\lambda}}}}}\\bigg\\lvert\\overline{{\\mathbf{A}}}_{0,b}\\bigg\\rvert\\bigg[\\mathcal{L}({\\mathbf a})-\\Theta-\\frac{\\lambda}{2n}\\frac{\\mathbb{E}_{\\mathbf{a},{\\mathbf a}}}{1+\\lambda_{\\Omega_{\\hat{\\lambda}}}}({\\mathbf{a}}^{T}{\\mathbf2}{\\mathbf0}^{T}-\\mathbf{\\hat{l}})^{2}\\bigg\\rvert\\tilde{\\mathbf{A}},{\\hat{\\mathbf y}}\\bigg]\\bigg\\rvert+\\mathbb{E}_{\\hat{\\lambda}_{\\hat{\\lambda}_{\\hat{\\lambda}}}}\\bigg\\lvert\\overline{{\\mathbf{A}}}_{0,b}\\bigg\\rvert\\bigg[\\mathcal{L}({\\mathbf b})-\\Theta-\\frac{\\lambda}{2n}\\frac{\\mathbb{E}_{\\mathbf{b},{\\mathbf y}}({\\mathbf b}^{T}{\\mathbf0}-\\mathbf{\\hat{y}})^{2}}{1+\\lambda_{\\mathrm{B}}\\mathbb{E}_{\\mathbf{a}}\\bigg\\lvert\\mathbf{1}^{T}\\boldsymbol{\\Omega}^{-1}\\mathbb{\\mathbf{b}}\\bigg\\rvert}\\bigg\\rvert\\tilde{\\mathbf{A}},{\\hat{\\boldsymbol\\lambda}_{\\hat{\\lambda}_{\\hat{\\lambda}}}}}\\\\ &{\\leq\\mathbb{E}_{\\hat{\\lambda}_{\\hat{\\lambda}_{\\hat{\\lambda}}},\\mathbf{a},\\gamma_{\\hat\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "First we will provide an upper bound for the third and fourth terms in 15. As the function $x\\mapsto{\\frac{1}{1+x}}$ 1\ud835\udc65 is 1-Lipschitz and using Cauchy Schwartz we arrive at ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\lambda}{2n}\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}\\bigg|\\frac{(\\mathbf{a}^{T}\\mathbf{u}-\\mathbf{\\boldsymbol{y}})^{2}}{1+\\lambda\\mathbf{a}^{T}\\boldsymbol{\\Omega}^{-1}\\mathbf{a}}-\\frac{(\\mathbf{a}^{T}\\mathbf{u}-\\mathbf{\\boldsymbol{y}})^{2}}{1+\\lambda\\mathbb{E}_{\\mathbf{a}}\\mathbf{a}^{T}\\boldsymbol{\\Omega}^{-1}\\mathbf{a}}\\bigg|\\leq\\frac{\\lambda^{2}}{2n}\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}\\bigg|(\\mathbf{a}^{T}\\mathbf{u}-\\mathbf{\\boldsymbol{y}})^{2}(\\mathbf{a}^{T}\\boldsymbol{\\Omega}^{-1}\\mathbf{a}-\\mathbb{E}_{\\mathbf{a}}\\mathbf{a}^{T}\\boldsymbol{\\Omega}^{-1}\\mathbf{a})\\bigg|}\\\\ {(16)}\\\\ {\\leq\\frac{\\lambda^{2}}{2n}\\sqrt{\\mathbb{B}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}(\\mathbf{a}^{T}\\mathbf{u}-\\mathbf{\\boldsymbol{y}})^{4}\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}(\\mathbf{a}^{T}\\boldsymbol{\\Omega}^{-1}\\mathbf{a}-\\mathbb{E}_{\\mathbf{a}}\\mathbf{a}^{T}\\boldsymbol{\\Omega}^{-1}\\mathbf{a})}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By Lemma 7, there exists a constant $C_{1}<\\infty$ such that $\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},\\mathrm{y}}(\\mathbf{a}^{T}\\mathbf{u}\\mathbf{-}\\mathrm{y})^{4}<C_{1}$ for any $n$ . Furthermore, by the assumptions, $\\begin{array}{r}{\\operatorname*{lim}_{n\\rightarrow\\infty}\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},y}(\\mathbf{a}^{T}\\mathbf{\\Omega}\\mathbf{\\Omega}^{-1}\\mathbf{a}-\\mathbb{E}_{\\mathbf{a}}\\mathbf{a}^{T}\\mathbf{\\Omega}\\mathbf{\\Omega}^{-1}\\mathbf{\\dot{a}})^{2}=0.}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "Now for the first and second terms in (15), using Lemma 5, we obtain with probability 1, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mathcal{M}(\\mathbf{a})-\\mathcal{L}(\\mathbf{a})|\\leq\\displaystyle\\frac{8C_{f+\\epsilon\\psi}}{n}\\|\\mathbf{u}-\\mathbf{w}_{\\mathcal{L}}\\|_{3}^{3}\\mathbb{1}\\{\\|\\mathbf{u}-\\mathbf{w}_{\\mathcal{L}}\\|_{3}\\leq\\displaystyle\\frac{\\rho}{18C_{f+\\epsilon\\psi}}\\}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad+\\displaystyle\\frac{\\lambda}{2n}(\\mathbf{a}^{T}\\mathbf{u}-y)^{2}\\mathbb{1}\\{\\|\\mathbf{u}-\\mathbf{w}_{\\mathcal{L}}\\|_{3}>\\displaystyle\\frac{\\rho}{18C_{f+\\epsilon\\psi}}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Which implies ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}|\\mathcal{M}(\\mathbf{a})-\\mathcal{L}(\\mathbf{a})|\\leq\\!\\frac{8C_{f+\\epsilon}\\psi}{n}\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}\\!\\left[||\\mathbf{u}-\\mathbf{w}_{\\mathcal{L}}||_{3}^{3}\\mathbb{I}\\{||\\mathbf{u}-\\mathbf{w}_{\\mathcal{L}}||_{3}\\leq\\frac{\\rho}{18C_{f+\\epsilon}\\psi}\\}\\right]}\\\\ {+\\frac{\\lambda}{2n}\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}\\!\\left[(\\mathbf{a}^{T}\\mathbf{u}-\\mathbf{y})^{2}\\mathbb{I}\\{||\\mathbf{u}-\\mathbf{w}_{\\mathcal{L}}||_{3}>\\frac{\\rho}{18C_{f+\\epsilon}\\psi}\\}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We apply Cauchy-Schwarz to each term: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}|\\mathcal{M}(\\mathbf{a})-\\mathcal{L}(\\mathbf{a})|\\leq\\frac{8C_{f+\\epsilon}\\psi}{n}\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}\\|\\mathbf{u}-\\mathbf{w}_{\\mathcal{L}}\\|_{3}^{3}}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\frac{\\lambda}{2n}\\sqrt{\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}\\mathbb{I}^{2}\\{\\|\\mathbf{u}-\\mathbf{w}_{\\mathcal{L}}\\|_{3}>\\frac{\\rho}{18C_{f+\\epsilon}\\psi}\\}\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}(\\mathbf{a}^{T}\\mathbf{u}-\\mathbf{y})^{4}}}\\\\ &{\\quad\\quad\\quad\\quad=\\frac{8C_{f+\\epsilon}\\psi}{n}\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}\\|\\mathbf{u}-\\mathbf{w}_{\\mathcal{L}}\\|_{3}^{3}}\\\\ &{\\quad\\quad\\quad\\quad+\\frac{\\lambda}{2n}\\sqrt{\\mathbb{P}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}(\\|\\mathbf{u}-\\mathbf{w}_{\\mathcal{L}}\\|_{3}>\\frac{\\rho}{18C_{f+\\epsilon}\\psi})\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}(\\mathbf{a}^{T}\\mathbf{u}-\\mathbf{y})^{4}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To deal with the terms involving $\\mathbb{P}$ , we leverage Markov\u2019s inequality ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}|M(\\mathbf{a})-\\mathcal{L}(\\mathbf{a})|\\leq\\!\\frac{8C_{f+\\epsilon\\psi}}{n}\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}\\|\\mathbf{u}-\\mathbf{w}_{\\mathcal{L}}\\|_{3}^{3}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\frac{\\lambda}{2n}\\big(\\frac{18C_{f+\\epsilon\\psi}}{\\rho}\\big)^{3/2}\\sqrt{\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}\\|\\mathbf{u}-\\mathbf{w}_{\\mathcal{L}}\\|_{3}^{3}\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}(\\mathbf{a}^{T}\\mathbf{u}-\\mathbf{y})^{4}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By Lemma 7, there exists a $C_{2}<\\infty$ such that $\\begin{array}{r}{\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},y}\\|\\mathbf{u}-\\mathbf{w}_{\\mathcal{L}}\\|_{3}^{3}\\leq\\frac{C_{2}}{d}}\\end{array}$ , and recall that $\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},\\mathrm{y}}(\\mathbf{a}^{T}\\mathbf{u}-$ $y)^{4}<C_{1}$ for any $n$ . Thus ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}|\\mathcal{M}(\\mathbf{a})-\\mathcal{L}(\\mathbf{a})|\\le\\frac{8C_{2}C_{f+\\epsilon\\psi}}{n d}+\\frac{\\lambda\\sqrt{C_{1}}}{2n\\sqrt{d}}\\big(\\frac{18C_{f+\\epsilon\\psi}}{\\rho}\\big)^{3/2}\\le\\frac{C_{a}}{n\\sqrt{d}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For some constant $\\begin{array}{r}{\\tilde{C}=\\frac{\\lambda\\sqrt{C_{1}}}{2}\\big(\\frac{18C_{f+\\epsilon\\psi}}{\\rho}\\big)^{3/2}+8C_{2}C_{f+\\epsilon\\psi}}\\end{array}$ . Plugging (16) and (18) in (15), yields ", "page_idx": 15}, {"type": "equation", "text": "$$\n3_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}}}\\bigg|\\mathbb{E}_{\\mathbf{a},\\mathbf{b},\\mathbf{y}}\\bigg[(\\mathcal{M}(\\mathbf{a})-\\mathcal{M}(\\mathbf{b}))\\bigg|\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}}\\bigg]\\bigg|\\leq\\frac{\\tilde{C}+\\tilde{C^{\\prime}}}{n\\sqrt{d}}+\\frac{\\lambda^{2}\\tilde{C}}{2n}\\sqrt{\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}}}V a r_{\\mathbf{a}}(\\mathbf{a}^{T}\\Omega^{-1}\\mathbf{a})}+\\frac{\\lambda^{2}\\tilde{C^{\\prime}}}{2n}\\sqrt{\\mathbb{B}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}}}V a r_{\\mathbf{b}}(\\mathbf{b}^{T}\\Omega^{-1}\\mathbf{a})},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For the other terms in (14), we have by Lemmas 4, 7: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|g^{\\prime\\prime}\\|_{\\infty}\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},\\gamma}(\\Theta-\\mathcal{M}(\\mathbf{a}))^{2}+\\|g^{\\prime\\prime}\\|_{\\infty}\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{b},\\gamma}(\\Theta-\\mathcal{M}(\\mathbf{b}))^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\|g^{\\prime\\prime}\\|_{\\infty}\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},\\gamma}\\frac{\\lambda^{2}}{4n^{2}}(\\mathbf{a}^{T}\\mathbf{u}-\\mathbf{y})^{2}+\\|g^{\\prime\\prime}\\|_{\\infty}\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{b},\\gamma}\\frac{\\lambda^{2}}{4n^{2}}(\\mathbf{b}^{T}\\mathbf{u}-\\mathbf{y})^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\frac{\\lambda^{2}\\|g^{\\prime\\prime}\\|_{\\infty}(\\tilde{C}_{1}+\\tilde{C}_{1}^{\\prime})}{4n^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Plugging in (14), we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Xi_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}}}\\Xi_{\\mathbf{a},\\mathbf{b},\\mathbf{y}}\\bigg[g(\\mathcal{M}(\\mathbf{a}))-g(\\mathcal{M}(\\mathbf{b}))\\bigg|\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}}\\bigg]\\bigg|}\\\\ &{\\qquad\\leq\\|g^{\\prime}\\|_{\\infty}\\bigg(\\frac{\\tilde{C}+\\tilde{C}^{\\prime}}{n\\sqrt{d}}+\\frac{\\lambda^{2}\\tilde{C}}{2n}\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}}}V a r_{\\mathbf{a}}(\\mathbf{a}^{T}\\Omega^{-1}\\mathbf{a})+\\frac{\\lambda^{2}\\tilde{C}^{\\prime}}{2n}\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}}}V a r_{\\mathbf{b}}(\\mathbf{b}^{T}\\Omega^{-1}\\mathbf{b})\\bigg)+\\frac{\\lambda^{2}\\|g^{\\prime\\prime}\\|_{\\infty}(\\tilde{C}+\\tilde{C}^{\\prime})}{4n^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Bigg|\\mathbb{E}_{{\\mathbf{A}},{\\mathbf{y}}}g(\\Phi({\\mathbf{A}}))-\\mathbb{E}_{{\\mathbf{B}},{\\mathbf{y}}}g(\\Phi({\\mathbf{B}}))\\Bigg|\\leq\\displaystyle\\sum_{j=0}^{n-1}\\biggl|\\mathbb{E}_{\\hat{\\mathbf{A}}_{j},{\\mathbf{y}}}g(\\Phi(\\hat{\\mathbf{A}}_{j}))-\\mathbb{E}_{\\hat{\\mathbf{A}}_{j-1},{\\mathbf{y}}}g(\\Phi(\\hat{\\mathbf{A}}_{j-1}))\\biggr|}\\\\ &{\\quad\\le\\displaystyle\\sum_{j=0}^{n-1}\\lVert g^{\\prime}\\rVert_{\\infty}\\Bigg(\\frac{\\tilde{C}+\\tilde{C}^{\\prime}}{n\\sqrt{d}}+\\frac{\\lambda^{2}\\tilde{C}}{2n}\\mathbb{E}_{\\hat{\\mathbf{A}}_{j},{\\mathbf{y}}}V a r_{\\mathbf{a}_{j}}({\\mathbf{a}}_{j}^{T}{\\mathbf{\\hat{a}}}^{-1}\\mathbf{a}_{j})+\\frac{\\lambda^{2}\\tilde{C}^{\\prime}}{2n}\\mathbb{E}_{\\hat{\\mathbf{A}}_{j},{\\mathbf{y}}}V a r_{\\mathbf{b}_{j}}({\\mathbf{b}}_{j}^{T}{\\mathbf{\\hat{Q}}}^{-1}\\mathbf{b}_{j})\\Bigg)+\\frac{\\lambda^{2}\\lVert g^{\\prime\\prime}\\rVert_{\\infty}}{4}}\\\\ &{=\\!\\frac{\\lVert g^{\\prime}\\rVert_{\\infty}(\\tilde{C}+\\tilde{C}^{\\prime})}{\\sqrt{d}}+\\frac{\\lambda^{2}\\lVert g^{\\prime\\prime}\\rVert_{\\infty}(\\tilde{C}+\\tilde{C}^{\\prime})}{4n}}\\\\ &{\\quad\\quad+\\displaystyle\\sum_{j=0}^{n-1}\\lVert g^{\\prime}\\rVert_{\\infty}\\Bigg(\\frac{\\lambda^{2}\\tilde{C}}{2n}\\sqrt{\\mathbb{E}_{\\hat{\\mathbf{A}}_{j},{\\mathbf{y}}}V a r_{\\mathbf{a}_{j}}({\\mathbf{a}}_{j}^{T}{\\mathbf{\\hat{a}}}^{-1}\\mathbf{a}_{j})}+\\frac{\\lambda^{2}\\tilde{C}^{\\prime}}{2n}\\sqrt{\\mathbb{E}_{\\hat{\\mathbf{A}}_{j},{\\mathbf{y}}}V a r_{\\mathbf{b}_{j}}({\\mathbf{b}}_{j}^{T}{\\mathbf{\\hat{a}}} \n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now note that since $\\begin{array}{r}{\\|\\Omega\\|_{o p}\\leq\\frac{1}{\\rho}}\\end{array}$ with probability 1 $,\\begin{array}{r}{,\\operatorname*{lim}_{n\\rightarrow\\infty}V a r_{\\mathbf{a}_{j}}(\\mathbf{a}_{j}^{T}\\mathbf{\\Omega}^{-1}\\mathbf{a}_{j})=0}\\end{array}$ from assumptions, thus for every class $[j]$ , there exists a function $\\zeta_{[j]}(n)$ that $V a r_{\\mathbf{a}_{j}}(\\mathbf{a}_{j}^{T}\\mathbf{\\Omega}^{-1}\\mathbf{a}_{j})\\,\\leq\\,\\zeta_{[j]}^{2}(n)$ for every $n\\in\\mathbb{N}$ and $\\begin{array}{r}{\\operatorname*{lim}_{n\\to\\infty}\\zeta_{[j]}(n)=0}\\end{array}$ . Thus ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathrm{y}g(\\Phi(\\mathbf{A}))-\\mathbb{B}_{\\mathbf{B},\\mathbf{y}}g(\\Phi(\\mathbf{B}))\\Big|\\leq\\frac{\\|g^{\\prime}\\|_{\\infty}(\\tilde{C}+\\tilde{C}^{\\prime})}{\\sqrt{d}}+\\frac{\\lambda^{2}\\|g^{\\prime\\prime}\\|_{\\infty}(\\tilde{C}+\\tilde{C}^{\\prime})}{4n}+\\lambda^{2}\\|g^{\\prime}\\|_{\\infty}(\\tilde{C}+\\tilde{C}^{\\prime})\\sum_{i=1}^{k}\\frac{n_{i}}{2n}\\zeta_{i}(n)}}\\\\ &{}&{\\leq\\frac{\\|g^{\\prime}\\|_{\\infty}(\\tilde{C}+\\tilde{C}^{\\prime})}{\\sqrt{d}}+\\frac{\\lambda^{2}\\|g^{\\prime\\prime}\\|_{\\infty}(\\tilde{C}+\\tilde{C}^{\\prime})}{4n}+\\lambda^{2}\\|g^{\\prime}\\|_{\\infty}(\\tilde{C}+\\tilde{C}^{\\prime})\\underset{1\\leq i\\leq k}{\\operatorname*{max}}\\frac{n_{i}}{2n}\\zeta_{i}(n)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now since $k$ is finite and $n_{i}=\\Theta(n)$ for every $1\\leq i\\leq k$ , we have that $\\begin{array}{r}{\\operatorname*{lim}_{n\\to\\infty}\\operatorname*{max}_{1\\leq i\\leq k}\\frac{n_{i}}{2n}\\zeta_{i}(n)=0}\\end{array}$ . Thus for every $\\lambda,\\epsilon>0$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\rightarrow\\infty}\\Bigl|\\mathbb{E}_{{\\mathbf{A}},{\\mathbf{y}}}g(\\Phi_{\\lambda,\\epsilon}({\\mathbf{A}}))-\\mathbb{E}_{{\\mathbf{B}},{\\mathbf{y}}}g(\\Phi_{\\lambda,\\epsilon}({\\mathbf{B}}))\\Bigr|=0\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and the proof of part 1 of Theorem 1 is concluded. ", "page_idx": 15}, {"type": "text", "text": "A.2 Proof of part 1 of Theorem 1 when $\\Psi=\\Phi$ ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Now to prove the part 1 of Theorem 1, we will combine the following lemma with the previous section. For the ease of notation, we drop the dependency on $\\epsilon$ . Recall that since $f$ is $\\rho$ -strongly convex, we can write ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Phi_{\\lambda}^{n}(A)=\\operatorname*{min}_{\\mathbf{w}}\\frac{\\lambda}{2}\\|\\mathbf{A}\\mathbf{w}-\\mathbf{y}\\|_{2}^{2}+\\frac{\\rho}{2}\\|\\mathbf{w}\\|_{2}^{2}+h(\\mathbf{w})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For some convex $h:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ . ", "page_idx": 16}, {"type": "text", "text": "Lemma 2. If for every $\\lambda>0$ , we have $\\Phi_{\\lambda}^{n}(A)\\stackrel{\\mathbb{P}}{\\longrightarrow}c_{\\lambda},$ , then for every Lipschitz function $g:\\mathbb{R}\\to\\mathbb{R},$ we have $g(\\operatorname*{sup}_{\\lambda>0}\\Phi_{\\lambda}^{n}(A))\\xrightarrow{\\mathbb{P}}g(\\operatorname*{sup}_{\\lambda>0}c_{\\lambda})\\;a n d\\;g(\\operatorname*{sup}_{\\lambda>0}\\Phi_{\\lambda}^{n}(B))\\xrightarrow{\\mathbb{P}}g(\\operatorname*{sup}_{\\lambda>0}c_{\\lambda})$ ", "page_idx": 16}, {"type": "text", "text": "Proof. First note that for a given $t>0$ , we have for every Lipschitz function $g$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}(|g(\\Phi_{\\lambda}^{n})-g(c_{\\lambda})|>t)\\le\\mathbb{P}(|\\Phi_{\\lambda}^{n}-c_{\\lambda}|>\\frac{t}{L})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Letting $\\begin{array}{r}{\\delta:=\\frac{t}{L},\\tilde{\\delta}_{1}>0}\\end{array}$ , we know that for every $\\lambda\\geq0$ , there exists $N_{1}\\in\\mathbb{N}$ such that for every $n\\geq N_{1}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}(|\\Phi_{\\lambda}^{n}-c_{\\lambda}|>\\delta)<\\tilde{\\delta}_{1}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We will show that an $R_{1}>0$ can be chosen in such a way that there exists $N_{2}\\in\\mathbb{N}$ such that for every $n\\geq N_{2}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\operatorname*{sup}_{\\lambda>0}\\Phi_{\\lambda}^{n}-\\operatorname*{sup}_{0<\\lambda<R_{1}}\\Phi_{\\lambda}^{n}>\\delta)\\le\\tilde{\\delta}_{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To see this, by the monotonicity of $\\Phi_{\\lambda}^{n}$ in $\\lambda$ , we have by the fundamental theorem of calculus for a given $R_{1}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\operatorname*{sup}_{\\lambda>0}\\Phi_{\\lambda}^{n}-\\operatorname*{sup}_{0<\\lambda<R_{1}}\\Phi_{\\lambda}^{n}>\\delta)=\\mathbb{P}(\\operatorname*{lim}_{\\lambda\\to\\infty}\\Phi_{\\lambda}^{n}-\\Phi_{R_{1}}^{n}>\\delta)=\\mathbb{P}\\Big(\\int_{R_{1}}^{\\infty}\\frac{\\partial\\Phi_{\\lambda}^{n}}{\\partial\\lambda}d s>\\delta\\Big)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By Danskin\u2019s theorem, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\operatorname*{sup}_{\\lambda>0}\\Phi_{\\lambda}^{n}-\\operatorname*{sup}_{0<\\lambda<R_{1}}\\Phi_{\\lambda}^{n}>\\delta)=\\mathbb{P}\\Big(\\frac{1}{n}\\int_{R_{1}}^{\\infty}\\|\\mathbf{A}\\mathbf{w}(s)-\\mathbf{y}\\|^{2}d s>\\delta\\Big)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now for the gradient of the main optimization we have, by $g:=\\nabla h(\\mathbf{w})$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\lambda\\mathbf{A}^{T}(\\mathbf{A}\\mathbf{w}-\\mathbf{y})+\\rho\\mathbf{w}+\\mathbf{g}=0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Which implies $\\mathbf{w}=(\\lambda\\mathbf{A}^{T}\\mathbf{A}+\\rho\\mathbf{I})^{-1}(\\lambda\\mathbf{A}^{T}\\mathbf{y}-\\mathbf{g})$ . Hence by matrix inversion lemma, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{A}\\mathbf{w}-\\mathbf{y}=-(\\mathbf{I}-\\mathbf{A}(\\lambda\\mathbf{A}^{T}\\mathbf{A}+\\rho\\mathbf{I})^{-1}\\lambda\\mathbf{A}^{T})\\mathbf{y}-\\mathbf{A}(\\lambda\\mathbf{A}^{T}\\mathbf{A}+\\rho\\mathbf{I})^{-1}\\mathbf{g}}\\\\ &{\\mathbf{\\Lambda}=-\\rho(\\lambda\\mathbf{A}\\mathbf{A}^{T}+\\rho I)^{-1}\\mathbf{y}-\\mathbf{A}(\\lambda\\mathbf{A}^{T}\\mathbf{A}+\\rho\\mathbf{I})^{-1}\\mathbf{g}=-\\rho(\\lambda\\mathbf{A}\\mathbf{A}^{T}+\\rho I)^{-1}\\mathbf{y}-(\\lambda\\mathbf{A}\\mathbf{A}^{T}+\\rho I)^{-1}\\mathbf{A}\\mathbf{g}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By triangle inequality and definition of the operator norm we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|\\mathbf{A}\\mathbf{w}+\\mathbf{y}\\|_{2}^{2}\\leq4\\rho^{2}\\|(\\lambda\\mathbf{A}\\mathbf{A}^{T}+\\rho I)^{-1}\\|_{o p}^{2}\\|\\mathbf{y}\\|_{2}^{2}+4\\|(\\lambda\\mathbf{A}\\mathbf{A}^{T}+\\rho I)^{-1}\\mathbf{A}\\|_{o p}^{2}\\|\\mathbf{g}\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Whence plugging back in 19 yields ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle_{\\lambda>0}\\Phi_{\\lambda}^{n}-\\operatorname*{sup}_{0<\\lambda<R_{1}}\\Phi_{\\lambda}^{n}>\\delta)\\leq\\mathbb{P}\\Big(\\frac{4}{n}\\displaystyle\\int_{R_{1}}^{\\infty}\\rho^{2}\\|(s\\mathbf{A}\\mathbf{A}^{T}+\\rho I)^{-1}\\|_{o p}^{2}\\|\\mathbf{y}\\|_{2}^{2}+\\|(s\\mathbf{A}\\mathbf{A}^{T}+\\rho I)^{-1}\\mathbf{A}\\|_{o p}^{2}\\|\\mathbf{g}(s)\\|_{2}^{2}}\\\\ &{\\leq\\mathbb{P}\\Big(\\displaystyle\\frac{4\\rho^{2}\\|\\mathbf{y}\\|_{2}^{2}}{n}\\displaystyle\\int_{R_{1}}^{\\infty}\\|(s\\mathbf{A}\\mathbf{A}^{T}+\\rho I)^{-1}\\|_{o p}^{2}>\\frac{\\delta}{2}\\Big)}\\\\ &{+\\mathbb{P}\\Big(\\displaystyle\\frac{4}{n}\\displaystyle\\int_{R_{1}}^{\\infty}\\|(s\\mathbf{A}\\mathbf{A}^{T}+\\rho I)^{-1}\\mathbf{A}\\|_{o p}^{2}\\|\\mathbf{g}(s)\\|_{2}^{2}d s>\\frac{\\delta}{2}\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "hFiogrh t hper onboarbmil itoyf. $\\mathbf{g}$ , by assumption, there exists a $C_{g}$ independent of $\\lambda$ , such that $\\frac{\\|\\mathbf{g}(s)\\|_{2}^{2}}{n}\\leq C_{g}$ with ", "page_idx": 16}, {"type": "text", "text": "Consider the following the events ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathcal{T}_{y}:=\\left\\{\\frac{\\|\\mathbf{y}\\|_{2}^{2}}{n}\\leq C_{y}\\right\\}}\\\\ {\\mathcal{T}_{g}:=\\left\\{\\frac{\\|\\mathbf{g}\\left(s\\right)\\|_{2}^{2}}{n}\\leq C_{g}\\right\\}}\\\\ {\\mathcal{T}_{A}:=\\left\\{s_{\\operatorname*{min}}(\\mathbf{AA}^{T})\\geq C_{A}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We further have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\gamma\\Big(\\frac{4\\rho^{2}\\|{\\bf y}\\|_{2}^{2}}{n}\\int_{R_{1}}^{\\infty}\\|(s{\\bf A}{\\bf A}^{T}+\\rho I)^{-1}\\|_{\\sigma p}^{2}d s>\\frac{\\delta}{2}\\Big)\\le\\mathbb{P}(\\mathcal{T}_{{\\bf y}}^{c})+\\mathbb{P}\\Big(4\\rho^{2}C_{\\mathbb{y}}\\int_{R_{1}}^{\\infty}\\|(s{\\bf A}{\\bf A}^{T}+\\rho I)^{-1}\\|_{\\sigma p}^{2}d s>\\frac{\\delta}{2}\\Big)}\\\\ {\\displaystyle\\Big(\\frac{4}{n}\\int_{R_{1}}^{\\infty}\\|(s{\\bf A}{\\bf A}^{T}+\\rho I)^{-1}{\\bf A}\\|_{\\sigma p}^{2}\\|{\\bf g}(s)\\|_{2}^{2}d s>\\frac{\\delta}{2}\\Big)\\le\\mathbb{P}(\\mathcal{T}_{{\\bf g}}^{c})+\\mathbb{P}\\Big(4C_{\\theta}\\int_{R_{1}}^{\\infty}\\|(s{\\bf A}{\\bf A}^{T}+\\rho I)^{-1}{\\bf A}\\|_{\\sigma p}^{2}d s>\\frac{\\delta}{2}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus we need to focus on the operator norm of each term in 20. After diagonalizing and using the fact that for any two rectangular matrices $\\mathbf{X},\\mathbf{Y}$ with matching dimensions, $s p e c(\\mathbf{XY})=s p e c(\\mathbf{YX})$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\centering\\begin{array}{l}{\\displaystyle\\|(\\lambda\\mathbf{A}\\mathbf{A}^{T}+\\rho I)^{-1}\\|_{o p}^{2}=\\operatorname*{max}_{1\\leq i\\leq n}\\frac{1}{(\\lambda s_{i}(\\mathbf{A}\\mathbf{A}^{T})+\\rho)^{2}}=\\frac{1}{(\\lambda s_{\\operatorname*{min}}(\\mathbf{A}\\mathbf{A}^{T})+\\rho)^{2}}}\\\\ {\\|(\\lambda\\mathbf{A}\\mathbf{A}^{T}+\\rho I)^{-1}\\mathbf{A}\\|_{o p}^{2}=\\displaystyle\\operatorname*{max}_{1\\leq i\\leq n}\\frac{s_{i}(\\mathbf{A}\\mathbf{A}^{T})}{(\\lambda s_{i}(\\mathbf{A}\\mathbf{A}^{T})+\\rho)^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now by the event $\\mathcal{T}_{A}$ , $s_{\\operatorname*{min}}(\\mathbf{AA}^{T})\\geq C_{A}$ , then for a large enough $R_{1}$ such that $\\begin{array}{r}{\\frac{\\rho}{R_{1}}\\leq C_{A}}\\end{array}$ , then for every $\\lambda\\geq R_{1}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|(\\lambda\\mathbf{A}\\mathbf{A}^{T}+\\rho I)^{-1}\\|_{o p}^{2}\\leq\\frac{1}{(\\lambda C_{A}+\\rho)^{2}}}\\\\ {\\|(\\lambda\\mathbf{A}\\mathbf{A}^{T}+\\rho I)^{-1}\\mathbf{A}\\|_{o p}^{2}\\leq\\frac{C_{A}}{(\\lambda C_{A}+\\rho)^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Bounding the integrals implies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\int_{R_{1}}^{\\infty}\\|(s\\mathbf{A}\\mathbf{A}^{T}+\\rho I)^{-1}\\|_{o p}^{2}d s\\leq\\frac{1}{C_{A}}\\frac{1}{C_{A}R_{1}+\\rho}}\\\\ {\\displaystyle\\int_{R_{1}}^{\\infty}\\|(s\\mathbf{A}\\mathbf{A}^{T}+\\rho I)^{-1}\\mathbf{A}\\|_{o p}^{2}d s\\leq\\frac{1}{C_{A}R_{1}+\\rho}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Summarizing, ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\mathrm{}}(\\operatorname*{sup}_{\\lambda>0}\\Phi_{\\lambda}^{n}-\\operatorname*{sup}_{0<\\lambda<R_{1}}\\Phi_{\\lambda}^{n}>\\delta)\\leq\\mathbb{P}\\Big(\\frac{4\\rho^{2}C_{y}}{C_{A}(C_{A}R_{1}+\\rho)}>\\frac{\\delta}{2}\\Big)+\\mathbb{P}\\Big(\\frac{C_{g}}{C_{A}R_{1}+\\rho}>\\frac{\\delta}{2}\\Big)+\\mathbb{P}(\\mathcal{T}_{y}^{c})+\\mathbb{P}(\\mathcal{T}_{g}^{c})+2\\mathbb{P}(\\mathcal{T}_{g}^{c}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now we choose $R_{1}$ to be large enough such that the first two terms vanish. Therefore there exists an $N_{2}\\in\\mathbb{N}$ such that for every $n\\geq N_{2}$ $\\dot{\\overline{{2}}},\\dot{\\overline{{\\mathbb{P}}}}\\big(\\mathcal{T}_{y}^{c}\\big)+\\mathbb{P}\\big(\\mathcal{T}_{g}^{c}\\big)+2\\mathbb{P}(\\mathcal{T}_{A}^{c})\\leq\\tilde{\\delta}_{2}$ . Thus for such $R_{1}$ and $N_{2}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\operatorname*{sup}_{\\lambda>0}\\Phi_{\\lambda}^{n}-\\operatorname*{sup}_{0<\\lambda<R_{1}}\\Phi_{\\lambda}^{n}>\\delta)\\le\\tilde{\\delta}_{2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now we can let $R_{2}>0$ be chosen in such a way that: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\lambda>0}c_{\\lambda}-\\operatorname*{sup}_{0<\\lambda<R_{2}}c_{\\lambda}<\\widetilde{\\delta}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Take the maximum of $R_{1}$ and $R_{2}$ as $R$ . By triangle inequality we observe ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(|\\operatorname*{sup}_{\\lambda\\to0}\\Phi_{\\lambda}^{n}-\\operatorname*{sup}_{\\mathcal{L}\\lambda}|>\\delta)\\leq\\mathbb{P}(|\\operatorname*{sup}_{0\\to\\lambda\\to0}\\Phi_{\\lambda}^{n}-\\operatorname*{sup}_{\\mathcal{L}\\lambda}|>\\delta)+\\mathbb{P}(\\operatorname*{sup}_{0\\to\\lambda}\\Phi_{\\lambda}^{n}-\\operatorname*{sup}_{\\mathcal{A}\\lambda}^{n}>\\delta)}\\\\ &{\\quad\\leq\\mathbb{P}(|\\operatorname*{sup}_{0\\to\\lambda\\to0}\\Phi_{\\lambda}^{n}-\\operatorname*{sup}_{\\mathcal{L}\\lambda}|>\\delta)+\\mathbb{P}(\\operatorname*{sup}_{\\mathcal{L}\\lambda}-\\operatorname*{sup}_{0\\to\\lambda}\\Phi)+\\mathbb{P}(\\operatorname*{sup}_{0\\to\\lambda}\\Phi_{\\lambda}^{n}-\\operatorname*{sup}_{\\mathcal{R}\\lambda}\\Phi_{\\lambda}^{n}>\\delta)}\\\\ &{\\quad\\leq\\mathbb{P}(|\\operatorname*{sup}_{\\lambda\\to\\lambda\\to R}\\Phi_{\\lambda}^{n}-\\operatorname*{sup}_{\\mathcal{L}\\lambda}|>\\delta)+0+\\tilde{\\delta}_{1}}\\\\ &{\\quad\\leq\\mathbb{P}(|\\operatorname*{sup}_{0\\to\\lambda\\to R}\\mathrm{\\tiny~\\alpha~}_{0\\to\\lambda<R}|>\\delta)+0+\\tilde{\\delta}_{1}}\\\\ &{\\quad\\leq\\tilde{\\delta}_{1}+\\mathbb{P}(\\operatorname*{sup}_{\\lambda\\to R}|\\Phi_{\\lambda}^{n}-c_{\\lambda}|>\\delta)}\\\\ &{\\quad\\qquad\\quad\\{\\mathrm{0<\\lambda<R}\\}}\\\\ &{\\quad\\leq\\tilde{\\delta}_{1}+\\mathbb{P}(\\operatorname*{sup}_{\\lambda\\to\\infty}|\\Phi_{\\lambda}^{n}-c_{\\lambda}|>\\delta)}\\\\ &{\\quad\\leq\\tilde{\\delta}_{1}+\\mathbb{P}(\\operatorname*{sup}_{\\lambda\\to\\infty}|\\Phi_{\\lambda}^{n}-c_{\\lambda}|>\\delta)}\\\\ &{\\quad\\qquad\\quad\\mathop{\\mathrm{0<\\lambda<R}}_{\\operatorname*{0<\\lambda<R}}|\\Phi_{\\lambda}^{n}-c_{\\lambda}|>\\delta)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "First we prove $c_{\\lambda}$ is concave in $\\lambda$ . Then by appealing to the Convexity Lemma (lemma 7.75 in Liese and Miescke [2008]), the result follows. Note that $\\Phi_{\\lambda}^{n}$ is concave in $\\lambda$ as it is the pointwiseminimum of a concave function in $\\lambda$ everywhere. Now to prove the concavity of $c_{\\lambda}$ , for a given pair of $\\lambda_{1},\\lambda_{2}\\in[0,R]$ and $0<\\theta<1$ , we prove the deterministic event $\\left\\{c_{\\theta\\lambda_{1}+(1-\\theta)\\lambda_{2}}\\!-\\!\\theta c_{\\lambda_{1}}\\!-\\!(1\\!-\\!\\theta)c_{\\lambda_{2}}<0\\right\\}$ has probability zero. First note that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Phi_{\\theta\\lambda_{1}+(1-\\theta)\\lambda_{2}}^{n}-\\theta\\Phi_{\\lambda_{1}}^{n}-(1-\\theta)\\Phi_{\\lambda_{2}}^{n}\\geq0\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus by union bound for $\\epsilon>0$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{\\big(\\theta c_{\\lambda_{1}}\\!+\\!(1-\\theta)c_{\\lambda_{2}}-c_{\\theta\\lambda_{1}}\\!+\\!(1-\\theta)\\lambda_{2}>\\epsilon\\big)}}\\\\ &{\\qquad\\qquad\\leq\\mathbb{P}\\Big(\\theta(c_{\\lambda_{1}}-\\Phi_{\\lambda_{1}}^{n})+(1-\\theta)(c_{\\lambda_{2}}-\\Phi_{\\lambda_{2}}^{n})+\\Phi_{\\theta\\lambda_{1}+(1-\\theta)\\lambda_{2}}^{n}-c_{\\theta\\lambda_{1}+(1-\\theta)\\lambda_{2}}>\\epsilon\\Big)}\\\\ &{\\qquad\\qquad\\leq\\mathbb{P}\\Big(\\theta|c_{\\lambda_{1}}-\\Phi_{\\lambda_{1}}^{n}|+(1-\\theta)|c_{\\lambda_{2}}-\\Phi_{\\lambda_{2}}^{n}|+|\\Phi_{\\theta\\lambda_{1}+(1-\\theta)\\lambda_{2}}^{n}-c_{\\theta\\lambda_{1}+(1-\\theta)\\lambda_{2}}|>\\epsilon\\Big)}\\\\ &{\\qquad\\qquad\\leq\\mathbb{P}\\Big(\\theta|c_{\\lambda_{1}}-\\Phi_{\\lambda_{1}}^{n}|>\\frac{\\epsilon}{3}\\Big)+\\mathbb{P}\\Big((1-\\theta)|c_{\\lambda_{2}}-\\Phi_{\\lambda_{2}}^{n}|>\\frac{\\epsilon}{3}\\Big)+\\mathbb{P}\\Big(|\\Phi_{\\theta\\lambda_{1}+(1-\\theta)\\lambda_{2}}^{n}-c_{\\theta\\lambda_{1}+(1-\\theta)\\lambda_{2}}^{n}|>\\frac{\\epsilon}{3}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now by taking $n$ to be large enough, the RHS can be made arbitrarily small for every $\\epsilon$ and $\\theta$ , thus $c_{\\lambda}$ is concave. Furthermore, since $[0,R]$ is a compact set, we can appeal to the convexity lemma 7.75 in Liese and Miescke [2008] and the first claim follows. For the second claim, note that so far we have proved ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Phi_{\\lambda}(A)\\stackrel{\\mathbb{P}}{\\to}c_{\\lambda}\\implies\\operatorname*{sup}_{\\lambda>0}\\Phi_{\\lambda}(A)\\stackrel{\\mathbb{P}}{\\to}\\operatorname*{sup}_{\\lambda>0}c_{\\lambda}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then since $|\\Phi_{\\lambda}(B)-\\Phi_{\\lambda}(A)|\\stackrel{\\mathbb{P}}{\\longrightarrow}0$ thus $\\Phi_{\\lambda}(B)\\stackrel{\\mathbb{P}}{\\longrightarrow}c_{\\lambda}$ , repeating the same argument this time for $\\Phi_{\\lambda}(B)$ yields ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\lambda>0}\\Phi_{\\lambda}(B)\\stackrel{\\mathbb{P}}{\\longrightarrow}\\operatorname*{sup}_{\\lambda>0}c_{\\lambda}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "A.2.1 Sequence of Regularizers ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We prove that by constraining ourselves to a large enough compact set $\\boldsymbol{S_{\\mathbf{w}}}$ , if $f_{m}\\to f$ uniformly, then the universality results also hold. N\u221aote for the $\\Phi_{\\lambda}$ case, we have that by setting $\\mathbf{w}=\\mathbf{0}$ in the optimization, we hav\u221ae $\\|\\mathbf{w}_{\\Phi_{\\lambda}(\\mathbf{A})}\\|_{2}\\,\\leq\\,C_{w}\\sqrt{n}$ for some $C_{w}\\,>\\,0$ with high probability, thus we can instead set $S_{\\mathbf{w}}:=C_{w}\\sqrt{n}$ and consider the following equivalent constrained optimization problem ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\tilde{\\Phi}_{\\lambda,\\epsilon}^{m}(\\mathbf{A}):=\\frac{1}{n}\\operatorname*{min}_{\\mathbf{w}\\in S_{\\mathbf{w}}}\\frac{\\lambda}{2}\\|\\mathbf{A}\\mathbf{w}-\\mathbf{y}\\|_{2}^{2}+f_{m}(\\mathbf{w})+\\epsilon\\psi(\\mathbf{w})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now consider a sequence of regular functions $f_{m}$ , converging uniformly to $f$ on $\\boldsymbol{S_{\\mathbf{w}}}$ . Take $m$ large enough such that $\\|f-f_{m}\\|_{\\infty}\\leq n\\delta$ for any $n$ which implies $|\\tilde{\\Phi}_{\\lambda,\\epsilon}^{m}(\\mathbf{A})-\\tilde{\\bar{\\Phi}}_{\\lambda,\\epsilon}\\overset{\\cdot}{(}\\mathbf{A})|\\leq\\overset{\\cdot}{\\delta}$ . Thus for any $t>0$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}\\bigg(\\bigg|\\tilde{\\Phi}_{\\lambda,\\epsilon}(\\mathbf{A})-c_{\\lambda,\\epsilon}\\bigg|>t\\bigg)\\leq\\mathbb{P}\\bigg(\\bigg|\\tilde{\\Phi}_{\\lambda,\\epsilon}(\\mathbf{A})-\\tilde{\\Phi}_{\\lambda,\\epsilon}^{m}(\\mathbf{A})\\bigg|>\\frac{t}{2}\\bigg)+\\mathbb{P}\\bigg(\\bigg|\\tilde{\\Phi}_{\\lambda,\\epsilon}^{m}(\\mathbf{A})-c_{\\lambda,\\epsilon}^{m}\\bigg|>\\frac{t}{2}\\bigg)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "If $\\tilde{\\Phi}_{\\lambda,\\epsilon}^{m}(\\mathbf{A})\\stackrel{\\mathbb{P}}{\\rightarrow}c_{\\lambda,\\epsilon}^{m}$ , then by (22), we have $\\tilde{\\Phi}_{\\lambda,\\epsilon}(\\mathbf{A})\\stackrel{\\mathbb{P}}{\\rightarrow}c_{\\lambda,\\epsilon}.$ . A similar argument holds for ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\tilde{\\Phi}_{\\epsilon}^{m}(\\mathbf{A}):=\\frac{1}{n}\\operatorname*{min}_{\\mathbf{Aw}=\\mathbf{y},\\mathbf{w}\\in S_{\\mathbf{w}}}f_{m}(\\mathbf{w})+\\epsilon\\psi(\\mathbf{w})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Furthermore, subsequent results also hold for $\\psi\\big(\\mathbf{w}_{\\Phi_{\\lambda}^{m}(\\mathbf{A})}\\big)$ and $\\psi\\big(\\mathbf{w}_{\\Phi_{\\lambda}(\\mathbf{A})}\\big),\\psi\\big(\\mathbf{w}_{\\Phi^{m}(\\mathbf{A})}\\big)$ and $\\psi(\\mathbf{w}_{\\Phi(\\mathbf{A})})$ ", "page_idx": 18}, {"type": "text", "text": "A.3 Proof of Second part of Theorem 1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "A.3.1 Proof of Second part of Theorem 1 for a regular test function when $\\Psi=\\Phi_{\\lambda}$ ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Now we prove the second part of Theorem 1. Since we assumed $\\Phi_{\\lambda,0}(\\mathbf{A})$ converges to some $c_{\\lambda,0}$ in probability, then for a small enough $\\epsilon$ , we also have $\\Phi_{\\lambda,\\epsilon}(\\mathbf{A})\\stackrel{\\mathbb{P}}{\\rightarrow}c_{\\lambda,\\epsilon}$ . Therefore we may write by an application of triangle inequality and the union bound   \n$\\left(\\left|\\frac{\\Phi_{\\lambda,\\epsilon}(\\mathbf{A})-\\Phi_{\\lambda,0}(\\mathbf{A})}{\\epsilon}-\\frac{c_{\\lambda,\\epsilon}-c_{\\lambda,0}}{\\epsilon}\\right|>t\\right)\\leq\\mathbb{P}\\left(\\left|\\Phi_{\\lambda,\\epsilon}(\\mathbf{A})-c_{\\lambda,\\epsilon}\\right|>\\frac{\\epsilon t}{2}\\right)+\\mathbb{P}\\left(\\left|\\Phi_{\\lambda,0}(\\mathbf{A})-c_{\\lambda,0}\\right|>\\frac{\\epsilon t}{2}\\right)$ (23) For every $\\begin{array}{r l r}{\\epsilon,t}&{{}}&{>}&{0.}\\end{array}$ , the RHS of (23) goes to zero. Similarly we have $\\begin{array}{r}{\\mathbb{P}\\Bigg(\\Bigg|\\frac{\\Phi_{\\lambda,0}(\\mathbf{A})-\\Phi_{\\lambda,-\\epsilon}(\\mathbf{A})}{\\epsilon}-\\frac{c_{\\lambda,0}-c_{\\lambda,-\\epsilon}}{\\epsilon}\\Bigg|>t\\Bigg)\\to0.}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "Now we observe that by triangle inequality ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Im\\bigg(\\bigg|\\psi\\big(\\mathbf{w}_{\\Phi_{\\lambda}(\\mathbf{A})}\\big)-\\frac{d c_{\\lambda,\\epsilon}}{d\\epsilon}\\bigg|_{\\epsilon=0}\\bigg|>t\\bigg)\\le\\mathbb{P}\\bigg(\\psi\\big(\\mathbf{w}_{\\Phi_{\\lambda}(\\mathbf{A})}\\big)>\\frac{d c_{\\lambda,\\epsilon}}{d\\epsilon}\\bigg|_{\\epsilon=0}+t\\bigg)+\\mathbb{P}\\bigg(\\psi\\big(\\mathbf{w}_{\\Phi_{\\lambda}(\\mathbf{A})}\\big)<\\frac{d c_{\\lambda,\\epsilon}}{d\\epsilon}\\bigg|_{\\epsilon=0}-t\\bigg)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We take $\\hat{\\epsilon}$ to be small enough such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left|{\\frac{d c_{\\lambda,\\epsilon}}{d\\epsilon}}\\right|_{\\epsilon=0}-\\frac{c_{\\lambda,\\epsilon}-c_{\\lambda,0}}{\\hat{\\epsilon}}\\right|<\\frac{t}{2},\\quad\\left|{\\frac{d c_{\\lambda,\\epsilon}}{d\\epsilon}}\\right|_{\\epsilon=0}-\\frac{c_{\\lambda,0}-c_{\\lambda,-\\hat{\\epsilon}}}{\\hat{\\epsilon}}\\right|<\\frac{t}{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This implies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sqrt{\\left(\\left|\\psi\\left(\\mathbf{w}_{\\Phi_{\\lambda}\\left(\\mathbf{A}\\right)}\\right)-\\frac{d c_{\\lambda,\\epsilon}}{d\\epsilon}\\right|_{\\epsilon=0}\\right|>t\\right)}\\,\\ge\\frac{\\epsilon}{\\psi}\\left(\\psi\\left(\\mathbf{w}_{\\Phi_{\\lambda}\\left(\\mathbf{A}\\right)}\\right)>\\,\\frac{c_{\\lambda,0}-c_{\\lambda,-\\hat{\\epsilon}}}{\\hat{\\epsilon}}+\\frac{t}{2}\\right)+\\mathbb{P}\\left(\\psi\\left(\\mathbf{w}_{\\Phi_{\\lambda}\\left(\\mathbf{A}\\right)}\\right)<\\frac{c_{\\lambda,\\hat{\\epsilon}}-c_{\\lambda,0}}{\\hat{\\epsilon}}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "And by Danskin\u2019s theorem, $\\Phi_{\\lambda,\\epsilon}(\\mathbf{A})$ is minimum of a concave function $\\epsilon$ , therefore it is also concave and it is differentiable with respect to $\\epsilon$ , we observe that $\\begin{array}{r}{\\psi\\bigl(w_{\\Phi_{\\lambda}(\\mathbf{A})}\\bigr)=\\frac{d\\Phi_{\\lambda,\\epsilon}(\\mathbf{A})}{d\\epsilon}\\bigg|_{\\epsilon=0}}\\end{array}$ . Moreover, by concavity over $\\hat{\\epsilon}$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\Phi_{\\lambda,\\hat{\\epsilon}}(\\mathbf{A})-\\Phi_{\\lambda,0}(\\mathbf{A})}{\\hat{\\epsilon}}\\leq\\psi(w_{\\Phi_{\\lambda}(\\mathbf{A})})\\leq\\frac{\\Phi_{\\lambda,0}(\\mathbf{A})-\\Phi_{\\lambda,-\\hat{\\epsilon}}(\\mathbf{A})}{\\hat{\\epsilon}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Whence ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}\\Bigg(\\Bigg|\\psi\\big(\\mathbf{w}_{\\Phi_{\\lambda}(\\mathbf{A})}\\big)-\\frac{d c_{\\lambda,\\epsilon}}{d\\epsilon}\\Big|_{\\epsilon=0}\\Bigg|>t\\Bigg)\\leq\\mathbb{P}\\Bigg(\\frac{\\Phi_{\\lambda,0}(\\mathbf{A})-\\Phi_{\\lambda,-\\epsilon}(\\mathbf{A})}{\\epsilon}>\\frac{c_{\\lambda,0}-c_{\\lambda,-\\epsilon}}{\\epsilon}+\\frac{t}{2}\\Bigg)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\mathbb{P}\\Bigg(\\frac{\\Phi_{\\lambda,\\epsilon}(\\mathbf{A})-\\Phi_{\\lambda,0}(\\mathbf{A})}{\\epsilon}<\\frac{c_{\\lambda,\\epsilon}-c_{\\lambda,0}}{\\epsilon}-\\frac{t}{2}\\Bigg)}\\\\ &{\\leq\\mathbb{P}\\Bigg(\\Bigg|\\frac{\\Phi_{\\lambda,0}(\\mathbf{A})-\\Phi_{\\lambda,-\\epsilon}(\\mathbf{A})}{\\epsilon}-\\frac{c_{\\lambda,0}-c_{\\lambda,-\\epsilon}}{\\epsilon}\\Bigg|>\\frac{t}{2}\\Bigg)}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\mathbb{P}\\Bigg(\\Bigg|\\frac{\\Phi_{\\lambda,\\epsilon}(\\mathbf{A})-\\Phi_{\\lambda,0}(\\mathbf{A})}{\\epsilon}-\\frac{c_{\\lambda,\\epsilon}-c_{\\lambda,0}}{\\epsilon}\\Bigg|>\\frac{t}{2}\\Bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By (23), the RHS goes to zero, thus \ud835\udf13(w\u03a6\ud835\udf06(A))\u2212P\u2192 \ud835\udc51\ud835\udc51\ud835\udc50\ud835\udf06\ud835\udf16,\ud835\udf16  \ud835\udf16 . Furthermore, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}\\Bigg(\\Bigg|\\psi(\\mathbf{w}_{\\Phi_{\\lambda}(\\mathbf{B})})-\\frac{d c_{\\lambda,\\epsilon}}{d\\epsilon}\\Bigg|_{\\epsilon=0}\\Bigg|>t\\Bigg)\\leq\\mathbb{P}\\Bigg(\\Bigg|\\frac{\\Phi_{\\lambda,0}(\\mathbf{B})-\\Phi_{\\lambda,-\\epsilon}(\\mathbf{B})}{\\hat{\\epsilon}}-\\frac{c_{\\lambda,0}-c_{\\lambda,-\\epsilon}}{\\hat{\\epsilon}}\\Bigg|>\\frac{t}{2}\\Bigg)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad+\\mathbb{P}\\Bigg(\\Bigg|\\frac{\\Phi_{\\lambda,\\epsilon}(\\mathbf{B})-\\Phi_{\\lambda,0}(\\mathbf{B})}{\\hat{\\epsilon}}-\\frac{c_{\\lambda,\\epsilon}-c_{\\lambda,0}}{\\hat{\\epsilon}}\\Bigg|>\\frac{t}{2}\\Bigg)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\mathbb{P}\\Bigg(\\Bigg|\\Phi_{\\lambda,\\epsilon}(\\mathbf{B})-c_{\\lambda,\\epsilon}\\Bigg|>\\frac{\\epsilon t}{2}\\Bigg)+\\mathbb{P}\\Bigg(\\Bigg|\\Phi_{\\lambda,-\\epsilon}(\\mathbf{B})-c_{\\lambda,-\\epsilon}\\Bigg|>\\frac{\\epsilon t}{2}\\Bigg)}\\\\ &{\\quad\\quad\\quad\\quad\\quad+2\\mathbb{P}\\Bigg(\\Bigg|\\Phi_{\\lambda,0}(\\mathbf{B})-c_{\\lambda,0}\\Bigg|>\\frac{\\epsilon t}{2}\\Bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By the result of the first part, we know that the RHS (24) goes to zero as $n\\to\\infty$ . Hence, $\\psi\\big(\\mathbf{w}_{\\Phi_{\\lambda}\\left(\\mathbf{B}\\right)}\\big)\\stackrel{\\mathbb{P}}{\\longrightarrow}$ $\\left.\\frac{d c_{\\lambda,\\epsilon}}{d\\epsilon}\\right|_{\\epsilon=0}.$ ", "page_idx": 20}, {"type": "text", "text": "Moreover, if $\\psi$ is bounded, by definition of convergence in distribution, $\\vert\\mathbb{E}_{\\mathbf{A}}\\psi(\\mathbf{w}_{\\Phi_{\\lambda}(\\mathbf{A})})~-$ $\\mathbb{E}_{\\mathbf{B}}\\psi\\big(\\mathbf{w}_{\\Phi_{\\lambda}(\\mathbf{B})}\\big)|\\rightarrow0$ . ", "page_idx": 20}, {"type": "text", "text": "A.3.2 Proof of Second part of Theorem 1 for a regular test function for $\\Psi=\\Phi$ ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Defining ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tilde{\\Phi}_{\\epsilon}(\\mathbf{A}):=\\frac{1}{n}\\operatorname*{min}_{\\mathbf{Aw}=\\mathbf{y},\\mathbf{w}\\in S_{\\mathbf{w}}}f(\\mathbf{w})+\\epsilon\\psi(\\mathbf{w})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $s u p_{\\lambda>0}\\tilde{\\Phi}_{\\lambda,\\epsilon}(\\mathbf{A})\\ \\stackrel{\\mathbb{P}}{\\rightarrow}\\,s u p_{\\lambda>0}c_{\\lambda,\\epsilon}\\,.$ , or equivalently $\\tilde{\\Phi}_{\\epsilon}(\\mathbf{A})\\stackrel{\\mathbb{P}}{\\rightarrow}c_{\\epsilon}$ , we can use the same argument from previous section and Danskin\u2019s theorem, to observe that $\\begin{array}{r}{\\psi\\big(\\mathbf{w}_{\\tilde{\\Phi}_{\\lambda}(\\mathbf{A})}\\big)\\ \\stackrel{\\mathbb{P}}{\\longrightarrow}\\ \\frac{d c_{\\epsilon}}{d\\epsilon}\\Big|_{\\epsilon=0}}\\end{array}$ and \ud835\udf13(w \u03a6\u02dc\ud835\udf06(B))\u2212P\u2192 \ud835\udc51\ud835\udc51\ud835\udc50\ud835\udf16\ud835\udf16   \ud835\udf16=0. ", "page_idx": 20}, {"type": "text", "text": "A.3.3 Sequences of regular test functions ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "By a similar approach, it follows that for a sequence of regular functions $\\psi_{m}$ converging uniformly to   \n$\\psi$ , one also has $\\begin{array}{r}{\\psi(\\mathbf{w}_{\\Phi_{\\lambda}})\\stackrel{\\mathbb{P}}{\\longrightarrow}\\frac{d c_{\\lambda,\\epsilon}}{d\\epsilon}\\bigg|_{\\epsilon=0};=\\operatorname*{lim}_{m\\rightarrow\\infty}\\frac{d c_{\\epsilon}^{m}}{d\\epsilon}\\bigg|_{\\epsilon=0}}\\end{array}$ . Indeed,   \n$\\gamma\\Bigg(\\left|\\psi\\big(\\mathbf{w}_{\\Phi_{\\lambda}(\\mathbf{A})}\\big)-\\frac{d c_{\\lambda,\\epsilon}}{d\\epsilon}\\right|_{\\epsilon=0}\\Bigg)>\\;t\\Bigg)\\leq\\mathbb{P}\\Bigg(\\Bigg|\\psi\\big(\\mathbf{w}_{\\Phi_{\\lambda}(\\mathbf{A})}\\big)-\\psi_{m}\\big(\\mathbf{w}_{\\Phi_{\\lambda}(\\mathbf{A})}\\big)\\Bigg|>\\frac{t}{2}\\Bigg)+\\mathbb{P}\\Bigg(\\left|\\psi_{m}\\big(\\mathbf{w}_{\\Phi_{\\lambda}(\\mathbf{A})}\\big)-\\frac{d c_{\\lambda,\\epsilon}}{d\\epsilon}\\right|_{\\epsilon=0}\\Bigg)$ 0    > \ud835\udc612   \nSince we have uniform convergence, for $\\frac{t}{2}$ , there exists an $m_{0}$ such that for all $m\\geq m_{0}$ and all $\\boldsymbol{x}\\in\\ensuremath{\\mathbb{R}}^{d}$ ,   \nwe have $|\\psi_{m}(x)-\\psi(x)|<\\frac{t}{2}$ . Thus for such $m$ \u2019s, $\\begin{array}{r}{\\mathbb{P}\\Bigg(\\Bigg|\\psi\\big(\\mathbf{w}_{\\Phi_{\\lambda}(\\mathbf{A})}\\big)-\\psi_{m}\\big(\\mathbf{w}_{\\Phi_{\\lambda}(\\mathbf{A})}\\big)\\Bigg|>\\frac{t}{2}\\Bigg)=0.}\\end{array}$ . Then by   \ntriangle inequality   \nP $\\mathbb{P}\\Bigg(\\Bigg|\\psi\\big(\\mathbf{w}_{\\Phi_{\\lambda}(\\mathbf{A})}\\big)-\\frac{d c_{\\lambda,\\epsilon}}{d\\epsilon}\\Bigg|_{\\epsilon=0}\\Bigg|>\\;t\\Bigg)\\leq\\mathbb{P}\\Bigg(\\Bigg|\\psi_{m}\\big(\\mathbf{w}_{\\Phi_{\\lambda}(\\mathbf{A})}\\big)-\\frac{d c_{\\lambda,\\epsilon}^{m}}{d\\epsilon}\\Big|_{\\epsilon=0}\\Bigg|>\\;\\frac{t}{4}\\Bigg)+\\mathbb{P}\\Bigg(\\Bigg|\\frac{d c_{\\lambda,\\epsilon}}{d\\epsilon}\\Bigg|_{\\epsilon=0}-\\frac{d c_{\\lambda,\\epsilon}^{m}}{d\\epsilon}\\Bigg|_{\\epsilon=0}\\Bigg)>$ 4   \nBy assumption $\\begin{array}{r}{\\mathbb{P}\\Bigg(\\Bigg|\\psi_{m}\\big(\\mathbf{w}_{\\Phi_{\\lambda}(\\mathbf{A})}\\big)-\\frac{d c_{\\lambda,\\epsilon}^{m}}{d\\epsilon}\\Big|_{\\epsilon=0}\\Bigg|>\\frac{t}{4}\\Bigg)\\to0.}\\end{array}$ . And by the definition of $\\left.\\frac{d c_{\\lambda,\\epsilon}}{d\\epsilon}\\right|_{\\epsilon=0}$ the claim ", "page_idx": 20}, {"type": "text", "text": "B Proof of Corollary 1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We will utilize the result of the following Theorem. ", "page_idx": 20}, {"type": "text", "text": "Theorem 4. (Bobkov [2003]) For an isotropic random vector $\\mathbf{X}\\in\\mathbb{R}^{d}$ , and an isotropic gaussian $\\begin{array}{r}{\\mathbf{g}\\in\\mathbb{R}^{d}\\;i f\\mathbb{P}(|\\frac{\\|\\mathbf{X}\\|_{2}}{\\sqrt{2}}-1|\\geq\\epsilon_{d})\\leq\\epsilon_{d}}\\end{array}$ , then for all $\\delta>0$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nu\\biggl(\\operatorname*{sup}_{t\\in\\mathbb{R}}\\biggl|\\mathbb{P}(\\mathbf{X}^{T}\\theta\\leq t)-\\mathbb{P}(\\mathbf{g}^{T}\\theta\\leq t)\\biggr|\\geq4\\epsilon_{d}+\\delta\\biggr)\\leq4d^{3/8}e^{-c d\\delta^{4}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Where $\\theta\\sim\\nu$ the uniform measure on the unit sphere $\\mathbb{S}^{d-1}$ ", "page_idx": 20}, {"type": "text", "text": "Essentially, through CGMT, we observe that when we use a quadratic regularizer or run SGD, the solution, $\\mathbf{w}_{\\Psi(\\mathbf{G})}$ converges weakly in distribution to a Gaussian vector, $\\mathbf{g}_{A O}$ . Whence we conclude that $\\mathbf{w}_{\\Psi(\\mathbf{G})}$ behaves the same as a generic vector. Therefore, by choosing $\\mathbf{X}:=\\mathbf{a}-\\mathbb{E}\\mathbf{a}$ , a row of our data matrix A, Theorem 4 enables us to prove the universality of the classification error. Note that in general $\\mathbb{E}\\mathbf{w}\\mathbf{\\boldsymbol{\\Psi}}(\\mathbf{G})\\neq\\mathbf{0}$ , but by Assumptions 3, the means of the classes are generic which allows us to employ Theorem 4 to conclude the proof. ", "page_idx": 20}, {"type": "text", "text": "C Lemmata for the proof of Theorem 1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proposition 2. If for any Lipschitz function $g:\\mathbb{R}\\to\\mathbb{R}$ and $t_{1}>0$ $0,\\,\\mathbb{P}\\Big(|g(\\Phi_{\\lambda,\\epsilon}(\\mathbf{B}))-c|>t_{1}\\Big)\\rightarrow0$ and for every twice differentiable $\\tilde{g}:\\mathbb{R}\\to\\mathbb{R}$ with bounded second derivative, we have that $\\begin{array}{r}{\\operatorname*{lim}_{n\\rightarrow\\infty}\\Bigl|\\mathbb{E}_{{\\mathbf{A}},{\\mathbf{y}}}\\tilde{g}(\\Phi_{\\lambda,\\epsilon}({\\mathbf{B}}))-\\mathbb{E}_{{\\mathbf{B}},{\\mathbf{y}}}\\tilde{g}(\\Phi_{\\lambda,\\epsilon}({\\mathbf{A}}))\\Bigr|\\rightarrow0}\\end{array}$ then for any $t_{2}>t_{1}$ . ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\mathbb{P}\\Big(|g(\\Phi_{\\lambda,\\epsilon}(\\mathbf{A}))-c|>t_{2}\\Big)=0\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. First note that $\\mathbb{P}(|\\Phi_{\\lambda,\\epsilon}(\\mathbf{B})-c|>\\,t)\\,\\to\\,0$ , implies $\\mathbb{P}(|g(\\Phi_{\\lambda,\\epsilon}(\\mathbf B))-g(c)|\\,>\\,L t)\\,\\to\\,0$ .   \n[T2h0u1s7 i]t.  suLfefti $\\mathbb{P}(|\\Phi_{\\lambda,\\epsilon}(\\mathbf{B})-c|>t)\\rightarrow0$ $\\begin{array}{r}{\\xi(x)\\,:=\\,2\\mathbb{1}_{(-\\infty,-t_{2}]\\cup[t_{2},\\infty)}(x)\\,+\\,\\big(\\frac{t_{1}+t_{2}}{2}\\,-\\,(|x|\\,-\\,t_{2})^{2}\\big)\\mathbb{1}_{(-t_{2},-\\frac{t_{1}+t_{2}}{2}]\\cup[\\frac{t_{1}+t_{2}}{2},t_{2})}(x)\\,+\\,(|x|\\,-\\,t_{2})^{2}\\big(\\frac{t_{1}+t_{2}}{2},2\\big)\\mathbb{1}_{(-t_{2},-t_{1}]\\cup[\\frac{t_{1}+t_{2}}{2},t_{2})}.}\\end{array}$   \n$\\begin{array}{r}{t_{1})^{2}\\mathbb{1}_{(-\\frac{t_{1}+t_{2}}{2},-t_{1}]\\cup[t_{1},\\frac{t_{1}+t_{2}}{2})}(x)}\\end{array}$ . Then we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(|\\Phi_{\\lambda,\\epsilon}(\\mathbf{A})-c|>t_{2}\\Big)=\\mathbb{P}\\Big(\\xi\\big(\\Phi_{\\lambda,\\epsilon}(\\mathbf{A})-c\\big)>2\\Big)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then by Markov\u2019s and triangle inequality ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\Big(\\vert\\Phi_{\\lambda,\\epsilon}(\\mathbf{A})-c\\vert>t_{2}\\Big)\\leq\\frac{1}{2}\\mathbb{E}_{\\mathbf{A},\\mathbf{y}}\\xi\\big(\\Phi_{\\lambda,\\epsilon}(\\mathbf{A})-c\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\cfrac{1}{2}\\Big\\vert\\mathbb{E}_{\\mathbf{A},\\mathbf{y}}\\xi\\big(\\Phi_{\\lambda,\\epsilon}(\\mathbf{A})-c\\big)-\\mathbb{E}_{\\mathbf{B},\\mathbf{y}}\\xi\\big(\\Phi_{\\lambda,\\epsilon}(\\mathbf{B})-c\\big)\\Big\\vert+\\cfrac{1}{2}\\mathbb{E}_{\\mathbf{B},\\mathbf{y}}\\xi\\big(\\Phi_{\\lambda,\\epsilon}(\\mathbf{B})-c\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that $\\xi(x)\\leq2\\mathbb{1}_{(-\\infty,-t_{1}]\\cup[t_{1},\\infty)}$ . Thus $\\begin{array}{r}{\\frac12\\mathbb{E}_{\\mathbf{B},\\mathbf{y}}\\xi\\big(\\Phi_{\\lambda,\\epsilon}(\\mathbf{B})-c\\big)\\leq\\mathbb{P}\\Big(|\\Phi_{\\lambda,\\epsilon}(\\mathbf{B})-c|>t_{1}\\Big)}\\end{array}$ . Moreover, we know that $\\xi(.)$ has bounded second derivative derivatives almost everywhere and by assumption on $g^{\\prime}$ , we would have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\circ\\Big(\\vert\\Phi_{\\lambda,\\epsilon}(\\mathbf{A})-c\\vert>t_{2}\\Big)\\leq\\frac{1}{2}\\Big\\vert\\mathbb{E}_{\\mathbf{A},\\mathbf{y}}\\xi\\big(\\Phi_{\\lambda,\\epsilon}(\\mathbf{A})-c\\big)-\\mathbb{E}_{\\mathbf{B},\\mathbf{y}}\\xi\\big(\\Phi_{\\lambda,\\epsilon}(\\mathbf{B})-c\\big)\\Big\\vert+\\mathbb{P}\\Big(\\vert\\Phi_{\\lambda,\\epsilon}(\\mathbf{B})-c\\vert>t_{1}\\Big)\\to0\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma 3. We have the following: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I.\\ \\ \\mathcal{L}(\\mathbf{a})-\\Theta=\\frac{\\lambda}{2n}\\frac{(\\mathbf{a}^{T}\\mathbf{u}-\\mathbf{y})^{2}}{1+\\lambda\\mathbf{a}^{T}\\Omega^{-1}\\mathbf{a}}}\\\\ &{2.\\ \\mathbf{w}_{\\mathcal{L}}-\\mathbf{u}=\\lambda\\frac{\\mathbf{y}-\\mathbf{a}^{T}\\mathbf{u}}{1+\\lambda\\mathbf{a}^{T}\\Omega^{-1}\\mathbf{a}}\\Omega^{-1}\\mathbf{a}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. We know for $\\Theta$ , taking derivative yields ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\lambda\\tilde{\\mathbf{A}}^{T}\\tilde{\\mathbf{A}}\\mathbf{u}-\\lambda\\tilde{\\mathbf{A}}^{T}\\tilde{\\mathbf{y}}+\\mathbf{g}=0\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This implies $-\\lambda\\tilde{\\mathbf{A}}^{T}\\tilde{\\mathbf{y}}+\\mathbf{g}=-\\lambda\\tilde{\\mathbf{A}}^{T}\\tilde{\\mathbf{A}}\\mathbf{u}$ . ", "page_idx": 21}, {"type": "text", "text": "Note that for $\\mathcal{L}(\\mathbf{a})$ , since the objective is quadratic in w, we solve the optimization in closed form, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\mathbf{a})=\\frac{1}{n}\\operatorname*{min}_{\\mathbf{w}}\\left(\\mathbf{w}^{T}\\quad1\\right)\\left(\\frac{1}{2}(\\lambda\\tilde{\\mathbf{A}}^{T}\\tilde{\\mathbf{A}}+\\lambda\\mathbf{a}\\mathbf{a}^{T}+\\mathbf{H})\\qquad\\qquad\\qquad\\frac{1}{2}(-\\lambda\\tilde{\\mathbf{A}}^{T}\\tilde{\\mathbf{y}}-\\lambda\\mathbf{y}\\mathbf{a}+\\mathbf{g}-\\mathbf{H}\\mathbf{u})\\right.}\\\\ {\\left.\\frac{1}{2}(-\\lambda\\tilde{\\mathbf{A}}^{T}\\tilde{\\mathbf{y}}-\\lambda\\mathbf{y}\\mathbf{a}+\\mathbf{g}-\\mathbf{H}\\mathbf{u})^{T}\\quad f(\\mathbf{u})+\\epsilon\\psi(\\mathbf{u})-\\mathbf{g}^{T}\\mathbf{u}+\\frac{1}{2}\\mathbf{u}^{T}\\mathbf{H}\\mathbf{u}+\\frac{\\lambda}{2}(\\|\\tilde{\\mathbf{y}}\\|^{2}+\\gamma^{2})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let $\\begin{array}{r}{\\Omega:=\\lambda\\tilde{\\mathbf{A}}^{T}\\tilde{\\mathbf{A}}+\\mathbf{H}}\\end{array}$ . hence ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\mathbf{a})=\\frac{1}{n}\\left[f(\\mathbf{u})+\\epsilon\\psi(\\mathbf{u})-\\mathbf{g}^{T}\\mathbf{u}+\\frac{1}{2}u^{T}\\mathbf{H}\\mathbf{u}+\\frac{\\lambda}{2}(\\|\\tilde{\\mathbf{y}}\\|^{2}+y^{2})-\\frac{1}{2}(\\Omega\\mathbf{u}+\\lambda y a)^{T}(\\Omega+\\lambda a\\mathbf{a}^{T})^{-1}(\\Omega\\mathbf{u}+\\lambda\\mathbf{y}\\mathbf{a})\\right]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using Sherman-Morrison as $\\Omega\\succ0$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\Omega\\mathbf{u}+\\lambda y\\mathbf{a}\\right)^{T}\\!\\left(\\Omega+\\lambda\\mathbf{a}\\mathbf{a}^{T}\\right)^{-1}\\!\\left(\\Omega\\mathbf{u}+\\lambda y\\mathbf{a}\\right)=\\left(\\Omega\\mathbf{u}+\\lambda y\\mathbf{a}\\right)^{T}\\!\\left(\\Omega^{-1}-\\frac{\\lambda\\Omega^{-1}\\mathbf{a}\\mathbf{a}^{T}\\Omega^{-1}}{1+\\lambda\\mathbf{a}^{T}\\Omega^{-1}\\mathbf{a}}\\right)\\!\\left(\\Omega\\mathbf{u}+\\lambda y\\mathbf{a}\\right)=}\\\\ &{=\\left(\\Omega\\mathbf{u}+\\lambda y\\mathbf{a}\\right)^{T}\\!\\left(\\mathbf{u}-\\lambda\\mathbf{a}^{T}\\mathbf{u}\\frac{\\Omega^{-1}\\mathbf{a}}{1+\\lambda\\mathbf{a}^{T}\\Omega^{-1}\\mathbf{a}}+\\lambda y\\Omega^{-1}\\mathbf{a}-\\lambda^{2}y\\mathbf{a}^{T}\\Omega^{-1}\\mathbf{a}\\frac{\\Omega^{-1}\\mathbf{a}}{1+\\lambda\\mathbf{a}^{T}\\Omega^{-1}\\mathbf{a}}\\right)=}\\\\ &{=\\left(\\Omega\\mathbf{u}+\\lambda y\\mathbf{a}\\right)^{T}\\!\\left(\\mathbf{u}+\\lambda\\!\\left(y-\\frac{\\lambda y\\mathbf{a}^{T}\\Omega^{-1}\\mathbf{a}+\\mathbf{a}^{T}\\mathbf{u}}{1+\\lambda\\mathbf{a}^{T}\\Omega^{-1}\\mathbf{a}}\\right)\\!\\Omega^{-1}\\mathbf{a}\\right)=\\left(\\Omega\\mathbf{u}+\\lambda y\\mathbf{a}\\right)^{T}\\!\\left(\\mathbf{u}+\\lambda\\frac{y-\\mathbf{a}^{T}\\mathbf{u}}{1+\\lambda\\mathbf{a}^{T}\\Omega^{-1}\\mathbf{a}}\\Omega^{-1}\\mathbf{a}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "From here we obtain for the solution Thus for the first claim: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{w}_{\\mathcal{L}}=(\\boldsymbol{\\Omega}+\\lambda\\mathbf{a}\\mathbf{a}^{T})^{-1}\\big(\\boldsymbol{\\Omega}\\boldsymbol{\\mathbf{u}}+\\lambda y\\mathbf{a}\\big)=\\mathbf{u}+\\lambda\\frac{y-\\mathbf{a}^{T}\\mathbf{u}}{1+\\lambda\\mathbf{a}^{T}\\boldsymbol{\\Omega}^{-1}\\mathbf{a}}\\boldsymbol{\\Omega}^{-1}\\mathbf{a}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence we have for the first claim ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{w}_{\\mathcal{L}}-\\mathbf{u}=\\lambda\\frac{y-\\mathbf{a}^{T}\\mathbf{u}}{1+\\lambda\\mathbf{a}^{T}\\boldsymbol{\\Omega}^{-1}\\mathbf{a}}\\boldsymbol{\\Omega}^{-1}\\mathbf{a}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now we continue with the calculation of the objective value ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{u}^{T}\\boldsymbol{\\Omega}\\mathbf{u}+\\lambda\\frac{\\mathbf{\\nabla}y-\\mathbf{a}^{T}\\mathbf{u}}{1+\\lambda\\mathbf{a}^{T}\\boldsymbol{\\Omega}^{-1}\\mathbf{a}}\\mathbf{a}^{T}\\mathbf{u}+\\lambda\\mathbf{y}\\mathbf{a}^{T}\\mathbf{u}+\\lambda^{2}\\mathbf{y}\\frac{\\mathbf{\\nabla}y-\\mathbf{a}^{T}\\mathbf{u}}{1+\\lambda\\mathbf{a}^{T}\\boldsymbol{\\Omega}^{-1}\\mathbf{a}}\\mathbf{a}^{T}\\boldsymbol{\\Omega}^{-1}\\mathbf{a}=}\\\\ &{=\\mathbf{u}^{T}\\boldsymbol{\\Omega}\\mathbf{u}}\\\\ &{+\\frac{\\lambda}{1+\\lambda\\mathbf{a}^{T}\\boldsymbol{\\Omega}^{-1}\\mathbf{a}}\\Big(\\mathbf{y}\\mathbf{a}^{T}\\mathbf{u}-(\\mathbf{a}^{T}\\mathbf{u})^{2}+y\\mathbf{a}^{T}\\mathbf{u}+\\lambda y(\\mathbf{a}^{T}\\mathbf{u})(\\mathbf{a}^{T}\\boldsymbol{\\Omega}^{-1}\\mathbf{a})+\\lambda y^{2}\\mathbf{a}^{T}\\boldsymbol{\\Omega}^{-1}\\mathbf{a}-\\lambda y(\\mathbf{a}^{T}\\mathbf{u})(\\mathbf{a}^{T}\\boldsymbol{\\Omega}^{-1}\\mathbf{a})\\Big)}\\\\ &{=\\mathbf{u}^{T}\\boldsymbol{\\Omega}\\mathbf{u}+\\frac{\\lambda}{1+\\lambda\\mathbf{a}^{T}\\boldsymbol{\\Omega}^{-1}\\mathbf{a}}(2\\mathbf{y}\\mathbf{a}^{T}\\mathbf{u}-(\\mathbf{a}^{T}\\mathbf{u})^{2}+\\lambda y^{2}\\mathbf{a}^{T}\\boldsymbol{\\Omega}^{-1}\\mathbf{a})}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{L}(\\mathbf{a})=\\frac{1}{n}\\left[f(\\mathbf{u})+\\epsilon\\psi(\\mathbf{u})-\\mathbf{g}^{T}\\mathbf{u}+\\frac{1}{2}\\mathbf{u}^{T}\\mathbf{H}\\mathbf{u}+\\frac{\\lambda}{2}(\\|\\tilde{\\mathbf{y}}\\|^{2}+y^{2})-\\frac{1}{2}\\mathbf{u}^{T}\\mathbf{\\Omega}\\mathbf{\\Omega}\\mathbf{0}\\right.}\\\\ {\\displaystyle\\left.-\\,\\frac{1}{2}\\frac{\\lambda}{1+\\lambda\\mathbf{a}^{T}\\boldsymbol{\\Omega}^{-1}\\mathbf{a}}(2y\\mathbf{a}^{T}\\mathbf{u}-(\\mathbf{a}^{T}\\mathbf{u})^{2}+\\lambda y^{2}\\mathbf{a}^{T}\\boldsymbol{\\Omega}^{-1}\\mathbf{a})\\right]}\\\\ {\\displaystyle=\\frac{1}{n}\\left[f(\\mathbf{u})+\\epsilon\\psi(\\mathbf{u})-\\mathbf{g}^{T}\\mathbf{u}-\\frac{\\lambda}{2}\\mathbf{u}^{T}\\tilde{\\mathbf{A}}^{T}\\tilde{\\mathbf{A}}\\mathbf{u}+\\frac{\\lambda}{2}\\|\\tilde{\\mathbf{y}}\\|^{2}+\\frac{\\lambda}{2}\\frac{(\\mathbf{a}^{T}\\mathbf{u}-\\boldsymbol{y})^{2}}{1+\\lambda\\mathbf{a}^{T}\\boldsymbol{\\Omega}^{-1}\\mathbf{a}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $g=\\lambda\\mathbf{\\tilde{A}}^{T}\\mathbf{\\tilde{y}}-\\lambda\\mathbf{\\tilde{A}}^{T}\\mathbf{\\tilde{A}}u$ , then $\\mathbf{g}^{T}\\mathbf{u}=\\lambda\\Tilde{\\mathbf{y}}^{T}\\Tilde{\\mathbf{A}}\\mathbf{u}-\\lambda\\mathbf{u}^{T}\\Tilde{\\mathbf{A}}\\Tilde{\\mathbf{A}}u$ , therefore ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathcal{L}({\\bf a})=\\frac{1}{n}\\left[f({\\bf u})+\\epsilon{\\boldsymbol{\\psi}}({\\bf u})-\\lambda{\\boldsymbol{\\tilde{\\Psi}}}^{T}\\tilde{\\bf A}{\\bf u}+\\lambda{\\bf u}^{T}\\tilde{\\bf A}\\tilde{\\bf A}{\\bf u}-\\frac{\\lambda}{2}{\\bf u}^{T}\\tilde{\\bf A}^{T}\\tilde{\\bf A}{\\bf u}+\\frac{\\lambda}{2}\\|{\\boldsymbol{\\tilde{\\bf y}}}\\|^{2}+\\frac{\\lambda}{2}\\frac{({\\bf a}^{T}{\\bf u}-{\\bf y})^{2}}{1+\\lambda{\\bf a}^{T}\\Omega^{-1}{\\bf a}}\\right]}}\\\\ {{\\displaystyle\\qquad=\\frac{1}{n}\\left[f({\\bf u})+\\epsilon{\\boldsymbol{\\psi}}({\\bf u})+\\frac{\\lambda}{2}\\|\\tilde{\\bf A}{\\bf u}-{\\boldsymbol{\\tilde{y}}}\\|^{2}+\\frac{\\lambda}{2}\\frac{({\\bf a}^{T}{\\bf u}-{\\bf y})^{2}}{1+\\lambda{\\bf a}^{T}\\Omega^{-1}{\\bf a}}\\right]=\\Theta+\\frac{\\lambda}{2n}\\frac{({\\bf a}^{T}{\\bf u}-{\\bf y})^{2}}{1+\\lambda{\\bf a}^{T}\\Omega^{-1}{\\bf a}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma 4. We have the following: ", "page_idx": 22}, {"type": "equation", "text": "$\\begin{array}{r}{I.\\ \\mathcal{M}(\\mathbf{a})-\\Theta\\leq\\frac{\\lambda}{2n}(\\mathbf{a}^{T}\\mathbf{u}-\\mathbf{y})^{2}}\\end{array}$ ", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{3.\\ |m(\\mathbf{a},\\mathbf{w})-\\ell(\\mathbf{a},\\mathbf{w})|\\leq\\frac{C_{f+\\epsilon\\psi}}{n}\\|\\mathbf{w}-\\mathbf{u}\\|_{3}^{3}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. 1. By definition, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{M}(\\mathbf{a})\\leq\\mathcal{m}(\\mathbf{a},\\mathbf{u})=\\Theta+\\frac{\\lambda}{2n}(\\mathbf{a}^{T}\\mathbf{u}-y)^{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus $\\begin{array}{r}{\\mathcal{M}(\\mathbf{a})-\\Theta\\le\\frac{\\lambda}{2n}(\\mathbf{a}^{T}\\mathbf{u}-\\mathbf{y})^{2}}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "2. The idea of the proof is by strong convexity. By assumption, $f(\\mathbf{w})+\\epsilon{\\psi}(\\mathbf{w})$ for small enough $\\epsilon$ is $\\rho$ -strongly convex. This implies from defintion: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\ell(\\mathbf{a},\\mathbf{w})\\geq\\mathcal{L}(\\mathbf{a})+\\frac{\\rho}{2n}{\\|\\mathbf{w}-\\mathbf{w}_{\\mathcal{L}}\\|_{2}^{2}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "3. By Taylor remainder theorem for some $0<t<1$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\begin{array}{c}{m\\left(\\mathbf{a},\\mathbf{w}\\right)-\\ell\\left(\\mathbf{a},\\mathbf{w}\\right)={\\cfrac{1}{n}}\\left[f(\\mathbf{w})+\\epsilon\\psi(\\mathbf{w})-f(\\mathbf{u})-\\epsilon\\psi(\\mathbf{u})-g^{T}(\\mathbf{w}-\\mathbf{u})-{\\cfrac{1}{2}}(\\mathbf{w}-\\mathbf{u})^{T}\\mathbf{H}(\\mathbf{w}-\\mathbf{u})\\right]}\\\\ {={\\cfrac{1}{n}}\\displaystyle\\sum_{|\\alpha|=3}\\partial^{\\alpha}(f+\\epsilon\\psi){\\big(}(1-t)\\mathbf{u}+t\\mathbf{w}{\\big)}{\\cfrac{\\left(\\mathbf{w}-\\mathbf{u}\\right)^{\\alpha}}{\\alpha!}}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $f$ and $\\psi$ are separable, $\\partial^{\\alpha}(f+\\epsilon\\psi)=0$ unless $\\alpha=3e_{\\alpha}$ ( $e_{\\alpha}$ is the standard basis vector in $\\mathbb{R}^{d}$ Then by assumption on the third derivative of $f+\\epsilon\\psi$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle|m\\left({\\bf a},{\\bf w}\\right)-\\ell({\\bf a},{\\bf w})\\right|=\\displaystyle\\frac{1}{n}\\Big|\\sum_{|\\alpha|=3}\\partial^{\\alpha}(f+\\epsilon\\psi)\\big((1-t){\\bf u}+t{\\bf w}\\big)\\frac{({\\bf w}-{\\bf u})^{\\alpha}}{\\alpha!}\\Big|}\\\\ {\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}\\\\ {\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\alpha\\Big|\\sum_{i=1}^{d}\\frac{\\partial^{3}(f+\\epsilon\\psi)\\big((1-t)u_{i}+t w_{i}\\big)}{\\partial w_{i}^{3}}({\\bf w}_{i}-u_{i})^{3}\\Big|\\leq\\frac{C_{f+\\epsilon\\psi}}{n}\\|{\\bf w}-{\\bf u}\\|_{3}^{3}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now we are ready to prove the following key lemma. ", "page_idx": 23}, {"type": "text", "text": "Lemma 5. We have $\\begin{array}{r}{|\\mathcal{M}(\\mathbf{a})-\\mathcal{L}(\\mathbf{a})|\\leq\\operatorname*{min}\\{\\frac{\\lambda}{2n}(\\mathbf{a}^{T}\\mathbf{u}-y)^{2},\\frac{8C_{f+\\epsilon\\psi}}{n}\\|\\mathbf{w}_{\\mathcal{L}}-\\mathbf{u}\\|_{3}^{3}\\}}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "Proof. Let $\\mathbf{w}_{M}=\\arg\\operatorname*{min}{m(\\mathbf{a},\\mathbf{w})}$ . Note that since $\\Theta\\leq M(\\mathbf{a})$ and by Lemma 4 we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\mathbf{a})-\\mathcal{M}(\\mathbf{a})=\\mathcal{L}(\\mathbf{a})-\\Theta+\\Theta-\\mathcal{M}(\\mathbf{a})\\leq\\frac{\\lambda}{2n}\\frac{(\\mathbf{a}^{T}\\mathbf{u}-y)^{2}}{1+\\lambda\\mathbf{a}^{T}\\Omega^{-1}\\mathbf{a}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "On the other hand, note that $\\begin{array}{r}{m(\\mathbf{a},\\mathbf{u})=\\Theta+\\frac{\\lambda}{2n}(\\mathbf{a}^{T}\\mathbf{u}-y)^{2}}\\end{array}$ and $\\Theta\\leq\\mathcal{L}(\\mathbf{a})$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{M}(\\mathbf{a})-\\mathcal{L}(\\mathbf{a})=\\mathcal{M}(\\mathbf{a})-\\mathcal{m}(\\mathbf{a},\\mathbf{u})+\\mathcal{m}(\\mathbf{a},\\mathbf{u})-\\Theta+\\Theta-\\mathcal{L}(\\mathbf{a})\\leq\\frac{\\lambda}{2n}(\\mathbf{a}^{T}\\mathbf{u}-\\mathbf{y})^{2}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "$\\begin{array}{r}{\\frac{\\lambda}{2n}(\\mathbf{a}^{T}\\mathbf{u}-y)^{2}\\,\\geq\\,\\frac{\\lambda}{2n}\\frac{(\\mathbf{a}^{T}\\mathbf{u}-y)^{2}}{1+\\lambda\\mathbf{a}^{T}\\Omega^{-1}\\mathbf{a}}}\\end{array}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n|M(\\mathbf{a})-\\mathcal{L}(\\mathbf{a})|\\leq\\frac{\\lambda}{2n}(\\mathbf{a}^{T}\\mathbf{u}-y)^{2}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "But as we will see, $(\\mathbf{a}^{T}\\mathbf{u}-\\boldsymbol{\\mathrm{y}})^{2}$ is of order $O(1)$ , which will not suffice. Thus we break down $\\mathcal{M}(\\mathbf{a})-\\mathcal{L}(\\mathbf{a})$ depending on the distance of $u$ and $\\mathbf{w}_{\\mathcal{L}}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathcal{M}(\\mathbf{a})-\\mathcal{L}(\\mathbf{a})=m(\\mathbf{a},\\mathbf{w}_{\\mathcal{M}})-m(\\mathbf{a},\\mathbf{w}_{\\mathcal{L}})+m(\\mathbf{a},\\mathbf{w}_{\\mathcal{L}})-\\ell(\\mathbf{a},\\mathbf{w}_{\\mathcal{L}})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq m(\\mathbf{a},\\mathbf{w}_{\\mathcal{L}})-\\ell(\\mathbf{a},\\mathbf{w}_{\\mathcal{L}})\\leq\\displaystyle\\frac{C_{f+\\epsilon\\psi}}{n}\\|\\mathbf{w}_{\\mathcal{L}}-\\mathbf{u}\\|_{3}^{3}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For the other direction ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\mathbf{a})-\\mathcal{M}(\\mathbf{a})=\\ell(\\mathbf{a},\\mathbf{w}_{\\mathcal{L}})-\\ell(\\mathbf{a},\\mathbf{w}_{\\mathcal{M}})+\\ell(\\mathbf{a},\\mathbf{w}_{\\mathcal{M}})-m(\\mathbf{a},\\mathbf{w}_{\\mathcal{M}})\\leq\\frac{C_{f+\\epsilon\\psi}}{n}\\|\\mathbf{w}_{\\mathcal{M}}-\\mathbf{u}\\|_{3}^{3}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let $R\\ :=\\ \\|\\mathbf{w}_{\\mathcal{L}}\\ \u2013\\ \\mathbf{u}\\|_{3}$ and consider the $\\ell_{3}$ ball centered at $\\mathbf{w}_{\\mathcal{L}}$ . Now if $\\|\\mathbf{w}_{M}-\\mathbf{w}_{\\mathcal{L}}\\|_{3}\\,\\leq\\,R$ then $\\|\\mathbf{w}_{M}-\\mathbf{u}\\|_{3}\\leq2R$ by a straightforward application of triangle inequality. Which implies $\\begin{array}{r}{\\mathcal{L}(\\mathbf{a})-\\mathcal{M}(\\mathbf{a})\\leq\\frac{8\\bar{C_{f+\\epsilon\\psi}}}{n}R^{3}}\\end{array}$ 8\ud835\udc36\ud835\udc53\ud835\udc5b+\ud835\udf16\ud835\udf13\ud835\udc453. Now when does \u2225wM \u2212wL\u22253 \u2264\ud835\udc45hold? We show that for every point on the boundary, w, $\\\"{\\boldsymbol{m}}(\\mathbf{a},\\mathbf{w})\\geq m(\\mathbf{a},\\mathbf{w}_{\\mathcal{L}})$ , this implies that $\\mathbf{w}_{M}$ is indeed inside the ball. We have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle{m\\left({\\bf a},{\\bf w}\\right)-m\\left({\\bf a},{\\bf w}_{\\mathcal{L}}\\right)\\geq\\ell({\\bf a},{\\bf w})-\\frac{C_{f+\\epsilon\\psi}}{n}\\|{\\bf w}-{\\bf u}\\|_{3}^{3}-\\ell({\\bf a},{\\bf w}_{\\mathcal{L}})-\\frac{C_{f+\\epsilon\\psi}}{n}\\|{\\bf w}_{\\mathcal{L}}-{\\bf u}\\|_{3}^{3}}}\\\\ {\\displaystyle{\\geq\\frac{\\rho}{2n}\\|{\\bf w}-{\\bf w}_{\\mathcal{L}}\\|_{2}^{2}-\\frac{C_{f+\\epsilon\\psi}}{n}(\\|{\\bf w}-{\\bf u}\\|_{3}^{3}+R^{3})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then using Lemma 4 ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{m(\\mathbf{a},\\mathbf{w})-m(\\mathbf{a},\\mathbf{w}_{\\mathcal{L}})\\ge\\displaystyle\\frac{\\rho}{2n}\\|\\mathbf{w}-\\mathbf{w}_{\\mathcal{L}}\\|_{3}^{2}-\\frac{C_{f+\\epsilon\\psi}}{n}(\\|\\mathbf{w}-\\mathbf{u}\\|_{3}^{3}+R^{3})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq\\displaystyle\\frac{\\rho}{2n}R^{2}-\\frac{C_{f+\\epsilon\\psi}}{n}(8R^{3}+R^{3})=R^{2}(\\frac{\\rho}{2n}-\\frac{9C_{f+\\epsilon\\psi}}{n}R)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now if $\\begin{array}{r}{R\\,\\leq\\,\\frac{\\rho}{18C_{f+\\epsilon\\psi}}}\\end{array}$ , then $m(\\mathbf{a},\\mathbf{w})-m(\\mathbf{a},\\mathbf{w}_{\\mathcal{L}})\\geq0$ . Thus all in all ", "page_idx": 23}, {"type": "equation", "text": "$$\n|\\mathcal{L}(\\mathbf{a})-\\mathcal{M}(\\mathbf{a})|\\leq\\frac{8C_{f+\\epsilon\\psi}}{n}\\|\\mathbf{w}_{\\mathcal{L}}-\\mathbf{u}\\|_{3}^{3}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Lemma 6. We have the following upper bounds ", "page_idx": 24}, {"type": "text", "text": "1. $\\|\\mathbf{u}\\|_{2}^{2}=O(n)$ with high probabilty. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\boldsymbol{2}.\\,\\mathrm{~\\mathbb{E}}_{{\\tilde{\\mathbf{A}}},{\\tilde{\\mathbf{y}}}}\\mu^{T}\\mathbf{u}=O(1)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. 1. Since $f+\\epsilon\\psi$ is $\\rho$ -strongly convex, one can decompose $\\begin{array}{r}{f(\\mathbf{w})+\\epsilon\\psi(\\mathbf{w})=\\frac{\\rho}{2}\\|\\mathbf{w}\\|_{2}^{2}+h(\\mathbf{w})}\\end{array}$ for some convex function $h(\\cdot)$ . Then ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{\\lambda}{2}\\|\\tilde{\\mathbf{A}}\\mathbf{u}-\\tilde{\\mathbf{y}}\\|_{2}^{2}+\\frac{\\rho}{2}\\|\\mathbf{u}\\|^{2}+h(\\mathbf{u})\\leq\\frac{\\lambda}{2}\\|\\tilde{\\mathbf{y}}\\|_{2}^{2}+h(\\mathbf{0})=O(n)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus this implies $\\|\\mathbf{u}\\|_{2}^{2}=O(n)$ . ", "page_idx": 24}, {"type": "text", "text": "2. In a similar fashion, we have for $\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}}}\\pmb{\\mu}^{T}\\mathbf{u}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Theta=\\frac{1}{n}\\left[\\frac{\\lambda}{2}\\lVert\\tilde{\\mathbf{A}}\\mathbf{u}-\\tilde{\\mathbf{y}}\\rVert_{2}^{2}+f(\\mathbf{u})+\\epsilon\\psi(\\mathbf{u})\\right]\\leq\\frac{1}{n}\\left[\\frac{\\lambda}{2}\\lVert\\tilde{\\mathbf{y}}\\rVert^{2}+f(0)+\\epsilon\\psi(0)\\right]\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Let $\\tilde{\\mathbf{A}}^{T}=\\left(\\hat{\\tilde{\\mathbf{A}}}_{1}^{T}\\quad\\hat{\\tilde{\\mathbf{A}}}_{2}^{T}\\quad\\dots\\quad\\hat{\\tilde{\\mathbf{A}}}_{k}^{T}\\right)$ . Now we use the definition of $\\tilde{\\mathbf A}$ to obtain a lower bound: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\lambda}{2}\\|\\tilde{\\mathbf{A}}\\mathbf{u}-\\tilde{\\mathbf{y}}\\|_{2}^{2}+f(\\mathbf{u})+\\epsilon\\psi(\\mathbf{u})\\ge\\frac{\\lambda}{2}\\sum_{i=1}^{k}\\|\\hat{\\tilde{\\mathbf{A}}}_{i}\\mathbf{u}+\\mathbf{1}\\mu_{i}^{T}\\mathbf{u}-\\hat{\\mathbf{y}}_{i}\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\geq\\displaystyle\\frac{\\lambda}{2}\\|\\hat{\\tilde{\\mathbf{A}}}_{i}\\mathbf{u}+\\mathbf{1}\\mu_{i}^{T}\\mathbf{u}-\\hat{\\mathbf{y}}_{i}\\|_{2}^{2}\\geq\\,\\frac{n_{i}\\lambda}{2}(\\mu_{i}^{T}\\mathbf{u})^{2}+\\lambda(\\mu_{i}^{T}\\mathbf{u})\\mathbb{1}^{T}(\\hat{\\tilde{\\mathbf{A}}}_{i}\\mathbf{u}-\\hat{\\mathbf{y}}_{i})}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus plugging in $\\mathbf{w}=\\mathbf{0}$ in the optimization yields an upper bound, which is a quadratic inequality in \u00b5\ud835\udc56u: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{n_{i}\\lambda}{2}(\\mu_{i}^{T}\\mathbf{u})^{2}+\\lambda(\\mu_{i}^{T}\\mathbf{u})\\mathbb{1}^{T}(\\hat{\\tilde{\\mathbf{A}}}_{i}\\mathbf{u}-\\hat{\\tilde{\\mathbf{y}}}_{i})-\\frac{\\lambda}{2}\\|\\tilde{\\mathbf{y}}\\|^{2}-f(0)-\\epsilon\\psi(0)\\leq0\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now note that for a quadratic optimization with $b\\geq0$ ", "page_idx": 24}, {"type": "equation", "text": "$$\nx^{2}-a x-b\\leq0\\iff{\\frac{a-{\\sqrt{a^{2}+4b}}}{2}}\\leq x\\leq{\\frac{a+{\\sqrt{a^{2}+4b}}}{2}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Whence ", "page_idx": 24}, {"type": "equation", "text": "$$\n|x|\\leq{\\frac{|a|+{\\sqrt{a^{2}+4b}}}{2}}\\leq{\\frac{|a|+|a|+2{\\sqrt{b}}}{2}}=|a|+{\\sqrt{b}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We apply this result to 25, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle|\\mu_{i}^{T}\\mathbf{u}|\\leq\\frac{2}{n_{i}}|\\mathbb{1}^{T}(\\hat{\\tilde{\\mathbf{A}}}_{i}\\mathbf{u}-\\hat{\\tilde{\\mathbf{y}}}_{i})|+\\frac{2}{\\lambda n_{i}}\\sqrt{\\frac{\\lambda}{2}}\\|\\tilde{\\mathbf{y}}\\|^{2}+f(0)+\\epsilon\\psi(0)}\\\\ {\\displaystyle\\leq\\frac{2}{n_{i}}|\\mathbb{1}^{T}\\hat{\\tilde{\\mathbf{y}}}_{i}|+\\frac{2}{n_{i}}\\|\\mathbb{1}^{T}\\hat{\\tilde{\\mathbf{A}}}_{i}\\|_{2}\\|\\mathbf{u}\\|_{2}+\\frac{2}{\\lambda n_{i}}\\sqrt{\\frac{\\lambda}{2}}\\|\\tilde{\\mathbf{y}}\\|^{2}+f(0)+\\epsilon\\psi(0)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now taking expectation and using a combination Cauchy-Schwarz and Jensen inequalities yields ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}}}|\\mu_{i}^{T}\\mathbf{u}|\\leq\\frac{2}{n_{i}}\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}}}|\\mathbf{1}^{T}\\hat{\\mathbf{y}}_{i}|+\\frac{2}{n_{i}}\\sqrt{\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}}}||\\mathbf{1}^{T}\\hat{\\mathbf{A}}_{i}||_{2}^{2}\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}}}||\\mathbf{u}||_{2}^{2}}+\\frac{2}{\\lambda n_{i}}\\sqrt{\\frac{\\lambda}{2}\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}}}||\\tilde{\\mathbf{y}}||_{2}^{2}+f(0)+\\epsilon\\psi(0)}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now note that $\\mathbb{E}\\hat{\\tilde{\\mathbf{A}}}_{i}=0$ , and by the independence of the rows: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{B}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}}}\\|\\mathbf{I}^{T}\\hat{\\hat{\\mathbf{A}}}_{i}\\|_{2}^{2}=\\sum_{j_{1}=1}^{d}(\\sum_{j_{2}=1}^{n}(\\hat{\\hat{\\mathbf{A}}}_{i})_{j_{2}j_{1}})^{2}=\\sum_{j_{1}=1}^{d}\\sum_{j_{2},j_{3}=1}^{n}\\mathbb{B}_{\\tilde{\\mathbf{A}}}(\\hat{\\hat{\\mathbf{A}}}_{i})_{j_{2}j_{1}}(\\hat{\\hat{\\mathbf{A}}}_{i})_{j_{3}j_{1}}=\\sum_{j_{2}=1}^{d}\\mathbb{B}_{\\tilde{\\mathbf{A}}}\\sum_{j_{1}=1}^{n}(\\hat{\\hat{\\mathbf{A}}}_{i})_{j_{2}j_{1}}^{2}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By assumption, for each row $\\hat{\\mathbf{a}}_{i}$ of $\\hat{\\tilde{\\mathbf{A}}}_{i}$ , we have that $\\mathbb{E}_{\\hat{\\mathbf{a}}_{i}}\\Vert\\hat{\\mathbf{a}}_{i}\\Vert_{2}^{2}=O(1)$ , which implies $\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}}}\\|\\mathbb{I}^{T}\\hat{\\tilde{\\mathbf{A}}}_{i}\\|_{2}^{2}=$ $O(d)$ . Therefore, by the previous part, since $\\begin{array}{r}{\\|\\mathbf{u}\\|_{2}^{2}\\;\\leq\\;\\frac{\\lambda}{\\rho}\\|\\tilde{\\mathbf{y}}\\|_{2}^{2}+\\frac{2}{\\rho}h(\\mathbf{0})}\\end{array}$ , then $\\begin{array}{r}{\\mathbb{E}_{\\mathbf{\\tilde{y}}}\\|\\mathbf{u}\\|_{2}^{2}\\;\\leq\\;\\frac{2}{\\rho}h(\\mathbf{0})\\;+}\\end{array}$ $\\begin{array}{r}{\\frac{\\lambda}{\\rho}\\mathbb{E}_{\\tilde{\\mathbf{y}}}\\|\\tilde{\\mathbf{y}}\\|_{2}^{2}=O(d)}\\end{array}$ by assumption on the $\\tilde{\\mathbf{y}}$ . Hence it can be seen that $\\mathbb{E}_{{\\tilde{\\mathbf{A}}},{\\tilde{\\mathbf{y}}}}\\mu^{T}\\mathbf{u}=O(1)$ \u25a1 ", "page_idx": 24}, {"type": "text", "text": "Now we are ready to conclude the proof with the following lemma. ", "page_idx": 25}, {"type": "text", "text": "Lemma 7. We have the following ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{l.\\ \\mathbb{E}_{{\\tilde{\\mathbf{A}}},{\\tilde{\\mathbf{y}}},{\\mathbf{a}},{y}}({\\mathbf{a}}^{T}{\\mathbf{u}}-{y})^{2q}=O(1)\\,f o r\\,q\\in[3]}\\\\ &{2.\\ \\mathbb{E}_{{\\tilde{\\mathbf{A}}},{\\tilde{\\mathbf{y}}},{\\mathbf{a}},{y}}\\|{\\mathbf{u}}-{\\mathbf{w}}_{\\mathcal{L}}\\|_{3}^{3}=O(\\frac{1}{d})}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. 1. First note that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}(\\mathbf{a}^{T}\\mathbf{u}-\\mathbf{y})^{2q}=\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}\\sum_{j=1}^{2q}{\\binom{2q}{j}}((\\mathbf{a}-\\mu+\\mu)^{T}\\mathbf{u})^{j}(-y)^{2q-\\mathbf{y}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Moreover, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\hat{\\mathbf{A}},\\hat{\\xi},\\mathbf{a},y}(\\mathbf{a}^{T}\\mathbf{u}-y)^{2q}\\leq\\displaystyle\\sum_{j=1}^{2q}\\binom{2q}{j}\\mathbb{E}_{\\hat{\\mathbf{A}},\\hat{\\xi},\\mathbf{a},y}\\Bigg[|(\\mathbf{a}-\\mu+\\mu)^{T}\\mathbf{u}|^{j}|y|^{2q-y}\\Bigg]}&{}\\\\ {\\leq\\displaystyle\\sum_{j=1}^{2q}\\binom{2q}{j}\\mathbb{E}_{\\hat{\\mathbf{A}},\\hat{\\xi},\\mathbf{a},y}\\Bigg[|y|^{2q-y}2^{j}\\operatorname*{max}\\{|\\mu^{T}\\mathbf{u}|,|(\\mathbf{a}-\\mu)^{T}\\mathbf{u}|\\}^{j}\\Bigg]}&{}\\\\ {\\leq\\displaystyle\\sum_{j=1}^{2q}2^{j}\\binom{2q}{j}\\mathbb{E}_{\\hat{\\mathbf{A}},\\hat{\\xi},\\mathbf{a},y}\\Bigg[|y|^{2q-y}(|\\mu^{T}\\mathbf{u}|^{j}+|(\\mathbf{a}-\\mu)^{T}\\mathbf{u}|^{j})\\Bigg]}&{}\\\\ {\\leq\\displaystyle\\sum_{j=1}^{2q}2^{j}\\binom{2q}{j}\\sqrt{\\mathbb{B}|y|^{4q-2y}}\\left(\\sqrt{\\mathbb{B}|\\mu^{T}\\mathbf{u}|^{2j}}+\\sqrt{\\mathbb{B}|(\\mathbf{a}-\\mu)^{T}\\mathbf{u}|^{2j}}\\right)}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By assumption $\\mathbb{E}|y|^{2q-y}\\leq C$ for some constant $C>0$ that is dimension-independent. Moreover, by our assumption, $\\begin{array}{r}{\\mathbb{E}|({\\mathbf a}-{\\boldsymbol\\mu})^{T}{\\mathbf u}|^{j}\\leq\\tilde{C}\\frac{\\|{\\mathbf u}\\|_{2}^{j}}{d^{j/2}}}\\end{array}$ . Also, from the previous lemma, $\\mathbb{E}|\\mu^{T}\\mathbf{u}|^{2j}=O(1)$ . The claim follows. ", "page_idx": 25}, {"type": "text", "text": "2. First note that \u2225u \u2212wL\u222533 = \ud835\udf063   +\ud835\udc66\ud835\udf06\u2212\ud835\udc47a\ud835\udc47\ud835\udec0u\u22121 and since $\\Omega\\succ0$ , $1+\\lambda\\mathbf{a}^{T}\\pmb{\\Omega}^{-1}\\mathbf{a}>0$ hence by Minkowski\u2019s inequality ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\hat{\\mathbf{A}},\\hat{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}\\|\\mathbf{u}-\\mathbf{w}_{\\mathcal{L}}\\|_{3}^{3}\\leq\\lambda^{3}\\mathbb{E}_{\\hat{\\mathbf{A}},\\hat{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}|a^{T}\\mathbf{u}-y|^{3}\\|\\Omega^{-1}\\mathbf{a}\\|_{3}^{3}}\\\\ &{\\leq\\lambda^{3}\\mathbb{E}_{\\hat{\\mathbf{A}},\\hat{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}|a^{T}\\mathbf{u}-y|^{3}\\|\\Omega^{-1}(\\mathbf{a}-\\mu+\\mu)\\|_{3}^{3}}\\\\ &{\\leq8\\lambda^{3}\\mathbb{E}_{\\hat{\\mathbf{A}},\\hat{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}|a^{T}\\mathbf{u}-y|^{3}\\operatorname*{max}\\{\\|\\Omega^{-1}(\\mathbf{a}-\\mu)\\|_{3},\\|\\Omega^{-1}\\mu\\|_{3}\\}^{3}}\\\\ &{\\leq8\\lambda^{3}\\mathbb{E}_{\\hat{\\mathbf{A}},\\hat{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}|a^{T}\\mathbf{u}-y|^{3}\\|\\Omega^{-1}(\\mathbf{a}-\\mu)\\|_{3}^{3}+8\\lambda^{3}\\mathbb{E}_{\\hat{\\mathbf{A}},\\hat{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}|a^{T}\\mathbf{u}-y|^{3}|\\Omega^{-1}\\mu\\|_{3}^{3}}\\\\ &{\\leq8\\lambda^{3}\\sqrt{\\mathbb{E}_{\\hat{\\mathbf{A}},\\hat{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}(\\mathbf{a}^{T}\\mathbf{u}-y)^{6}\\mathbb{E}_{\\hat{\\mathbf{A}},\\hat{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}\\|\\Omega^{-1}(\\mathbf{a}-\\mu)\\|_{3}^{6}}}\\\\ &{+~8\\lambda^{3}\\sqrt{\\mathbb{E}_{\\hat{\\mathbf{A}},\\hat{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}(\\mathbf{a}^{T}\\mathbf{u}-y)^{6}\\mathbb{E}_{\\hat{\\mathbf{A}},\\hat{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}\\|\\Omega^{-1}(\\mathbf{a}-\\mu)\\|_{3}^{6}}}\\\\ &{+~8\\lambda \n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now since we assumed that the objective is $\\rho$ -strongly convex, or there exists $\\rho>0$ that $\\|\\Omega^{-1}\\|_{o p}\\leq$ $\\frac{1}{\\rho}$ , denoting $\\omega_{j}$ as the $j$ th row of $\\pmb{\\Omega}^{-1}$ , we have by the inequality between Frobenius and Operator norm, $\\begin{array}{r}{\\|\\omega_{k}\\|_{2}\\leq\\frac{1}{\\rho}}\\end{array}$ . Now by the assumptions we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle^3\\tilde{\\mathbf{A}}_{:,\\tilde{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}\\left\\|\\boldsymbol\\Omega^{-1}(\\mathbf{a}-\\boldsymbol{\\mu})\\right\\|_{3}^{6}=\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}\\left(\\displaystyle\\sum_{j=1}^{d}|(\\mathbf{a}-\\boldsymbol{\\mu})^{T}\\omega_{j}|^{3}\\right)^{2}}\\\\ &{=\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}}}\\displaystyle\\sum_{j=1}^{d}\\mathbb{E}_{a,y}|(\\mathbf{a}-\\boldsymbol{\\mu})^{T}\\omega_{j}|^{6}+\\displaystyle\\sum_{j,l=1,j\\neq l}^{d}\\mathbb{E}|(\\mathbf{a}-\\boldsymbol{\\mu})^{T}\\omega_{j}|^{3}|(\\mathbf{a}-\\boldsymbol{\\mu})^{T}\\omega_{l}|^{3}}\\\\ &{\\leq C\\displaystyle\\sum_{j=1}^{d}\\frac{\\|\\omega_{j}\\|_{2}^{6}}{d^{3}}+C\\displaystyle\\sum_{j,l=1,j\\neq l}^{d}\\sqrt{\\mathbb{E}_{\\mathbf{a}}|(\\mathbf{a}-\\boldsymbol{\\mu})^{T}\\omega_{j}|^{6}\\mathbb{E}_{\\mathbf{a}}|(\\mathbf{a}-\\boldsymbol{\\mu})^{T}\\omega_{l}|^{6}}\\leq C d\\displaystyle\\frac{1}{\\rho^{6}d^{3}}+C(d^{2}-d)\\displaystyle\\frac{1}{\\rho^{6}d^{3}}\\leq\\frac{C}{\\rho^{6}d^{3}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We also have for the other term: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}\\|\\Omega^{-1}\\mu\\|_{3}^{6}\\leq\\mathbb{E}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}\\|\\Omega^{-1}\\mu\\|_{2}^{6}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note that $\\|\\Omega^{-1}\\pmb{\\mu}\\|_{2}\\leq\\|\\pmb{\\Omega}^{-1/2}\\|_{o p}\\|\\pmb{\\Omega}^{-1/2}\\pmb{\\mu}\\|_{2}$ . Thus it is sufficient to analyze $\\pmb{\\mu}^{T}\\pmb{\\Omega}^{-1}\\pmb{\\mu}^{T}$ . Let $t\\in\\mathbb{R}$ be an upper bound for $\\pmb{\\mu}^{T}\\pmb{\\Omega}^{-1}\\pmb{\\mu}^{T}$ , that is $\\mu^{T}\\Omega^{-1}\\mu^{T}\\;\\leq\\;t$ . Note that by the Schur complement property, we need to find the smallest $t>0$ such that $\\begin{array}{r}{\\Omega-\\frac{1}{t}\\mu\\boldsymbol{\\mu}^{T}\\succeq\\mathbf{0}}\\end{array}$ with holds with high probability. Recall that $\\boldsymbol\\Omega:=\\lambda\\tilde{\\mathbf A}^{T}\\tilde{\\mathbf A}+\\mathbf H$ and by assumption $\\mathbf{H}\\succeq\\rho\\mathbf{I}$ , thus it suffices to find the largest $t\\,>\\,0$ that $\\lambda\\tilde{\\mathbf{A}}^{T}\\tilde{\\mathbf{A}}+\\rho\\mathbf{I}-\\frac{1}{t}\\mu\\pmb{\\mu}^{T}\\succeq\\mathbf{0}$ . Let $\\mathbf{\\bar{A}}^{\\prime}:=\\tilde{\\mathbf{A}}-\\tilde{\\mathbf{M}}$ be centered. This problem is equivalent to having $\\begin{array}{r}{\\|(\\tilde{\\mathbf{A}}^{\\prime}+\\tilde{\\mathbf{M}})\\mathbf{v}\\|_{2}^{2}+\\rho\\|\\mathbf{v}\\|_{2}^{2}-\\frac{1}{t}(\\mu^{T}\\mathbf{v})^{2}\\geq0}\\end{array}$ for every ${\\mathbf v}\\in\\mathbb{R}^{d}$ . ", "page_idx": 26}, {"type": "text", "text": "Thus we should have ", "page_idx": 26}, {"type": "equation", "text": "$$\nt\\ge\\frac{(\\mu^{T}\\mathbf{v})^{2}}{\\|(\\tilde{\\mathbf{A}}^{\\prime}+\\tilde{\\mathbf{M}})\\mathbf{v}\\|_{2}^{2}+\\rho\\|\\mathbf{v}\\|_{2}^{2}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Let \ud835\udc61\u2217 := supv \u2225( \u02dcA\u2032+ M\u02dc)v\u222522+\ud835\udf0c\u2225v\u222522 . We show $t^{*}=O(n^{-s})$ for some $s\\,>\\,0$ with high probability. Let v\u2217:= arg max $\\begin{array}{r}{\\frac{(\\mu^{T}\\mathbf{v})^{2}}{\\Vert(\\tilde{\\mathbf{A}}^{\\prime}+\\tilde{\\mathbf{M}})\\mathbf{v}\\Vert_{2}^{2}+\\rho\\Vert\\mathbf{v}\\Vert_{2}^{2}}}\\end{array}$ . This implies ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|(\\tilde{\\mathbf{A}}^{\\prime}+\\tilde{\\mathbf{M}})\\mathbf{v}^{*}\\|_{2}^{2}\\leq\\frac{1}{t^{*}}(\\mu^{T}\\mathbf{v}^{*})^{2}\\leq\\frac{\\|\\mu\\|_{2}^{2}}{t^{*}}\\|\\mathbf{v}^{*}\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "On the other hand, by triangle inequality we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n(\\mu^{T}\\mathbf{v}^{*})^{2}\\leq\\frac{1}{n_{\\ell}}\\|\\tilde{\\mathbf{M}}\\mathbf{v}^{*}\\|_{2}^{2}\\leq\\frac{4}{n_{\\ell}}\\bigg(\\|(\\tilde{\\mathbf{A}}^{\\prime}+\\tilde{\\mathbf{M}})\\mathbf{v}^{*}\\|_{2}^{2}+\\|\\tilde{\\mathbf{A}}^{\\prime}\\mathbf{v}^{*}\\|_{2}^{2}\\bigg)\\leq\\frac{4}{n_{\\ell}}\\bigg(\\frac{\\|\\mu\\|_{2}^{2}}{t^{*}}\\|\\mathbf{v}^{*}\\|_{2}^{2}+\\|\\tilde{\\mathbf{A}}^{\\prime}\\|_{o p}^{2}\\|\\mathbf{v}^{*}\\|_{2}^{2}\\bigg)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": ". Now we have ", "page_idx": 26}, {"type": "equation", "text": "$$\nt^{*}=\\frac{(\\mu^{T}\\mathbf{v}^{*})^{2}}{\\|(\\tilde{\\mathbf{A}}^{\\prime}+\\tilde{\\mathbf{M}})\\mathbf{v}^{*}\\|_{2}^{2}+\\rho\\|\\mathbf{v}^{*}\\|_{2}^{2}}\\leq\\frac{\\frac{4}{n_{\\ell}}\\left(\\frac{\\|\\mu\\|_{2}^{2}}{t^{*}}\\|\\mathbf{v}^{*}\\|_{2}^{2}+\\|\\tilde{\\mathbf{A}}^{\\prime}\\|_{o p}^{2}\\|\\mathbf{v}^{*}\\|_{2}^{2}\\right)}{\\rho\\|\\mathbf{v}^{*}\\|_{2}^{2}}=\\frac{\\frac{4\\|\\mu\\|_{2}^{2}}{t^{*}}+4\\|\\tilde{\\mathbf{A}}^{\\prime}\\|_{o p}^{2}}{\\rho n_{\\ell}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We obtain a quadratic inequality in $t^{*}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\nt^{*2}-\\frac{4\\|\\tilde{\\mathbf{A}}^{\\prime}\\|_{o p}^{2}}{\\rho n_{\\ell}}t^{*}-\\frac{4\\|\\pmb{\\mu}\\|_{2}^{2}}{\\rho n_{\\ell}}\\leq0\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Which implies the following upper bound ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mu^{T}\\Omega^{-1}\\mu^{T}\\leq\\operatorname*{min}\\{\\frac{4\\|\\tilde{\\mathbf{A}}^{\\prime}\\|_{o p}^{2}}{\\rho n_{\\ell}}+4\\sqrt{\\frac{\\|\\pmb{\\mu}\\|_{2}^{2}}{\\rho n_{\\ell}}},\\frac{1}{\\rho}\\}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "According to Lemma (8) $\\|\\tilde{\\mathbf{A}}^{\\prime}\\|_{o p}\\leq n^{\\frac{1}{3}+s}$ with probability \ud835\udc36log(\ud835\udc5b\ud835\udc51\ud835\udc60+\ud835\udc5b).Thus using the law of total expectation, we also observe that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{\\mathbb{\\lambda}}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}\\|\\mathbf{\\Omega}^{-1}\\mu\\|_{3}^{6}\\leq\\mathbb{B}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}\\|\\mathbf{\\Omega}^{-1}\\mu\\|_{2}^{6}\\leq\\mathbb{B}_{\\tilde{\\mathbf{A}},\\tilde{\\mathbf{y}},\\mathbf{a},\\mathbf{y}}(\\mu^{T}\\Omega^{-1}\\mu)^{3}\\leq\\bigg(\\frac{4n^{\\frac{2}{3}+2s}}{\\rho n_{\\ell}}+4\\sqrt{\\frac{\\|\\mu\\|_{2}^{2}}{\\rho n_{\\ell}}}\\bigg)^{3}+C\\frac{\\log(d+n)}{\\rho n^{s}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For $\\begin{array}{r}{0<s<\\frac{1}{6}}\\end{array}$ , as $\\|\\mu\\|_{2}^{2}={\\cal{O}}(1)$ by assumption, the RHS goes to zero. ", "page_idx": 26}, {"type": "text", "text": "Lemma 8. Let $\\tilde{\\mathbf{A}}^{\\prime}$ be a centered random matrix following the assumptions. We have $\\|\\tilde{\\mathbf{A}}^{\\prime}\\|_{o p}\\leq C n^{\\frac{1}{3}+s}$ with probability at least $\\begin{array}{r}{1-\\frac{C^{\\prime}\\log(d+n)}{n^{s}}}\\end{array}$ ", "page_idx": 26}, {"type": "text", "text": "Proof. First, we provide an upper bound for the expectation $\\mathbb{E}_{\\tilde{\\mathbf{A}}^{\\prime}}||\\tilde{\\mathbf{A}}^{\\prime}||_{o p}$ using the matrix Bernstein inequality and the symmetrization technique. First we construct a corresponding hermitian matrix: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|\\tilde{\\mathbf{A}}^{\\prime}\\|_{o p}=\\left\\|\\left(\\mathbf{\\tilde{A}}^{\\prime}\\mathbf{\\Lambda}^{\\tilde{\\mathbf{A}}^{\\prime}}\\right)\\right\\|_{o p}=\\left\\|\\sum_{i=1}^{n}\\mathbf{X}_{i}\\right\\|_{o p}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "With $\\mathbf{X}_{i}:=\\left(\\begin{array}{c c}{\\mathbf{0}}&{\\mathbf{E}_{i i}\\tilde{\\mathbf{A}}^{\\prime}}\\\\ {\\tilde{\\mathbf{A}}^{\\prime T}\\mathbf{E}_{i i}}&{\\mathbf{0}}\\end{array}\\right)$ where $\\mathbf{E}_{i i}$ is the all-zero matrix except $(i,i)$ -entry where it is equal to 1. Note that $\\textstyle\\mathbf{I}=\\sum_{i=1}^{n}\\mathbf{E}_{i i}$ and $\\mathbf{E}_{i i}\\tilde{\\mathbf{A}}^{\\prime}$ essentially picks the i\u2019th row of $\\tilde{\\mathbf{A}}^{\\prime}$ , which is $\\tilde{\\mathbf{a}}_{i}^{'T}$ . Let $\\{\\epsilon_{i}\\}_{i=1}^{n}$ be an iid sequence of symmetric Bernoulli random variables supported on $\\{\\pm1\\}$ , independent of $\\mathbf{X}_{i}$ \u2019s. Then by the symmetrization lemma Vershynin [2018], we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\tilde{\\mathbf{A}}^{\\prime}}\\|\\tilde{\\mathbf{A}}^{\\prime}\\|_{o p}=\\mathbb{E}_{\\tilde{\\mathbf{A}}^{\\prime}}\\left\\|\\sum_{i=1}^{n}\\mathbf{X}_{i}\\right\\|_{o p}\\leq\\mathbb{E}_{\\tilde{\\mathbf{A}}^{\\prime},\\epsilon}\\left\\|\\sum_{i=1}^{n}\\epsilon_{i}\\mathbf{X}_{i}\\right\\|_{o p}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Now conditioning on $\\tilde{\\mathbf{A}}^{\\prime}$ or equivalently $\\mathbf{X}_{i}$ \u2019s, since $\\epsilon{\\mathbf{X}}_{i}$ \u2019s are independent zero-mean symmetric matrices of size $(d+n)\\times(d+n)$ with bounded operator norm, we can leverage Matrix Bernstein inequality Vershynin [2018] to obtain ( $\\lesssim$ means up to some constant) ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Xi_{\\bar{\\Lambda}^{\\prime}}\\|\\tilde{\\mathbf{A}}^{\\prime}\\|_{o p}\\lesssim\\sqrt{\\log(d+n)}\\Xi_{\\bar{\\Lambda}^{\\prime}}\\left\\|\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{\\epsilon_{i}}\\left(\\epsilon_{i}\\mathbf{X}_{i}\\right)^{2}\\right\\|_{o p}^{1/2}+\\log(d+n)\\mathbb{E}_{\\tilde{\\Lambda}^{\\prime}~\\operatorname*{max}_{1\\leq i\\leq n}}\\left\\|\\mathbf{X}_{i}\\right\\|_{o p}}\\\\ &{\\qquad\\qquad\\qquad=\\sqrt{\\log(d+n)}\\mathbb{E}_{\\tilde{\\Lambda}^{\\prime}}\\left\\|\\displaystyle\\sum_{i=1}^{n}\\mathbf{X}_{i}^{2}\\right\\|_{o p}^{1/2}+\\log(d+n)\\mathbb{E}_{\\tilde{\\Lambda}^{\\prime}~\\operatorname*{max}_{1\\leq i\\leq n}}\\left\\|\\mathbf{X}_{i}\\right\\|_{o p}}\\\\ &{\\qquad\\qquad\\qquad=\\sqrt{\\log(d+n)}\\mathbb{E}_{\\tilde{\\Lambda}^{\\prime}}\\left\\|\\left(\\displaystyle\\sum_{i=1}^{n}\\mathbf{E}_{i i}\\tilde{\\mathbf{A}}^{\\prime}\\tilde{\\mathbf{A}}^{\\prime\\prime}\\mathbf{E}_{i i}\\quad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\right)\\right\\|_{o p}^{1/2}+\\log(d+n)\\mathbb{E}_{\\tilde{\\Lambda}^{\\prime}~\\operatorname*{max}_{1\\leq i\\leq n}}\\left\\|\\mathbf{X}_{i}\\right\\|_{o p}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Note that for $\\left\\|\\mathbf{X}_{i}\\right\\|_{o p}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{X}_{i}\\right\\|_{o p}=\\left\\|\\mathbf{E}_{i i}\\tilde{\\mathbf{A}}^{\\prime}\\right\\|_{o p}=\\operatorname*{max}_{\\|\\mathbf{v}\\|_{2}=1}\\left\\|\\mathbf{E}_{i i}\\tilde{\\mathbf{A}}^{\\prime}\\mathbf{v}\\right\\|_{2}^{2}=\\operatorname*{max}_{\\|\\mathbf{v}\\|_{2}=1}(\\tilde{\\mathbf{a}}_{i}^{\\prime T}\\mathbf{v})^{2}=\\|\\tilde{\\mathbf{a}}_{i}^{\\prime}\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Moreover, using triangle inequality ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|\\left(\\sum_{i=1}^{n}\\mathbf{E}_{i i}\\tilde{\\mathbf{A}}^{\\prime}\\tilde{\\mathbf{A}}^{\\prime}\\mathbf{E}_{i i}\\right.}&{\\left.\\mathbf{0}\\right.}\\\\ {\\left.\\mathbf{0}\\right.}&{\\sum_{i=1}^{n}\\tilde{\\mathbf{a}}_{i}^{\\prime}\\tilde{\\mathbf{a}}_{i}^{\\prime}T\\right)\\right\\|_{o p}\\leq\\left\\|\\displaystyle\\sum_{i=1}^{n}\\mathbf{E}_{i i}\\tilde{\\mathbf{A}}^{\\prime}\\tilde{\\mathbf{A}}^{\\prime}\\mathbf{E}_{i i}\\right\\|_{o p}+\\left\\|\\sum_{i=1}^{n}\\tilde{\\mathbf{a}}_{i}^{\\prime}\\tilde{\\mathbf{a}}_{i}^{\\prime}T\\right\\|_{o p}}\\\\ &{=\\left\\|\\displaystyle\\sum_{i=1}^{n}\\|\\tilde{\\mathbf{a}}_{i}^{\\prime}\\|_{2}^{2}\\mathbf{E}_{i i}\\right\\|_{o p}+\\left\\|\\displaystyle\\sum_{i=1}^{n}\\tilde{\\mathbf{a}}_{i}^{\\prime}\\tilde{\\mathbf{a}}_{i}^{\\prime}T\\right\\|_{o p}}\\\\ &{=\\displaystyle\\operatorname*{max}_{1\\leq i\\leq n}\\|\\tilde{\\mathbf{a}}_{i}^{\\prime}\\|_{2}^{2}+\\left\\|\\displaystyle\\sum_{i=1}^{n}\\tilde{\\mathbf{a}}_{i}^{\\prime}\\tilde{\\mathbf{a}}_{i}^{\\prime}T\\right\\|_{o p}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Combining these results along with Jensen\u2019s inequality yields ", "page_idx": 27}, {"type": "text", "text": "$\\mathbb{E}_{\\tilde{\\mathbf{A}}^{\\prime}}\\Vert\\tilde{\\mathbf{A}}^{\\prime}\\Vert_{o p}\\lesssim\\sqrt{\\log(d+n)}\\left(\\mathbb{E}_{\\tilde{\\mathbf{A}}^{\\prime}}\\operatorname*{max}_{1\\leq i\\leq n}\\Vert\\tilde{\\mathbf{a}}_{i}^{\\prime}\\Vert_{2}^{2}+\\mathbb{E}_{\\tilde{\\mathbf{A}}^{\\prime}}\\bigg\\Vert\\sum_{i=1}^{n}\\tilde{\\mathbf{a}}_{i}^{\\prime}\\tilde{\\mathbf{a}}_{i}^{\\prime\\prime}\\bigg\\Vert_{o p}\\right)^{1/2}+\\log(d+n)\\mathbb{E}_{\\tilde{\\mathbf{A}}^{\\prime}}\\operatorname*{max}_{1\\leq i\\leq n}\\Vert\\tilde{\\mathbf{a}}_{i}^{\\prime}\\Vert_{2}^{2}$   \nNow to analyze E \u02dcA\u2032 $\\begin{array}{r}{\\mathbb{E}_{\\tilde{\\mathbf{A}}^{\\prime}}\\left\\|\\sum_{i=1}^{n}\\tilde{\\mathbf{a}}_{i}^{\\prime}\\tilde{\\mathbf{a}}_{i}^{\\prime T}\\right\\|_{o}}\\end{array}$ , we use the symmetrization trick again with iid Bernoulli $\\{\\epsilon_{i}^{\\prime}\\}_{i=1}^{n}$ \ud835\udc5d   \nand Bernstein\u2019s similar to Vershynin\u2019s ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{{\\mathbb E}_{\\vec{k}}\\left\\|\\displaystyle\\sum_{i=1}^{n}{\\mathbb E}_{\\vec{k}}\\Big[\\frac{n^{i}}{\\Delta t}{\\mathbb\\tilde{a}}_{i}^{\\mathcal{N}_{i}^{\\mathcal{N}_{I}}}\\Big\\|_{\\alpha_{p}}\\leq{\\mathbb E}_{\\vec{k}}\\left\\|\\displaystyle\\sum_{i=1}^{n}{\\mathbb E}_{\\vec{k}}{\\mathbb\\tilde{a}}_{i}^{\\mathcal{N}_{i}^{\\mathcal{N}_{I}}}-{\\mathbb E}_{\\vec{k}}{\\mathbb\\tilde{a}}_{i}^{\\mathcal{N}_{i}^{\\mathcal{N}_{I}}}\\Big\\|_{\\alpha_{p}}+\\left\\|\\displaystyle\\sum_{i=1}^{n}{\\mathbb E}_{\\vec{k}}{\\mathbb\\tilde{a}}_{i}^{\\mathcal{N}_{i}^{\\mathcal{N}_{I}}}\\right\\|_{\\alpha_{p}}\\right.}\\\\ &{\\left.\\leq{\\mathbb E}_{\\vec{k}},\\displaystyle\\sum_{i=1}^{n}\\left\\|\\displaystyle\\sum_{i=1}^{n}{\\mathbb E}_{i}{\\mathbb\\tilde{a}}_{i}^{\\mathcal{N}_{i}^{\\mathcal{N}_{I}}}\\right\\|_{\\alpha_{p}}+\\left.\\displaystyle\\sum_{i=1}^{n}\\left\\|{\\mathbb E}_{\\vec{k}}{\\mathbb\\tilde{a}}_{i}^{\\mathcal{N}_{i}^{\\mathcal{N}_{I}}}\\right\\|_{\\alpha_{p}}\\right.}\\\\ &{\\left.\\lesssim\\sqrt{\\log n}{\\mathbb E}_{\\vec{k}}\\right\\|_{\\alpha_{1}}^{n}{\\mathbb E}_{\\vec{k}}\\left\\|\\displaystyle\\sum_{i=1}^{n}{\\mathbb E}_{\\sigma_{i}}{\\mathbb\\mathbb{\\tilde{e}}}_{i}^{\\mathcal{N}_{i}^{\\mathcal{N}_{I}}}\\right\\|_{\\alpha_{p}}^{1/2}+\\log n{\\mathbb E}_{\\vec{k}}\\displaystyle\\operatorname*{max}_{1\\leq i\\leq n}^{n}\\left\\|{\\tilde{\\alpha}}_{i}^{\\mathcal{N}_{I}^{\\mathcal{N}_{I}}}\\right\\|_{\\alpha_{p}}+\\displaystyle\\sum_{i=1}^{n}\\left\\|{\\mathbb E}_{\\vec{k}}{\\tilde{\\alpha}}_{i}^{\\mathcal{N}_{i}^{\\mathcal{N}_{I}}}\\right\\|_{\\alpha_{p}}\\right.}\\\\ &{=\\left.\\sqrt{\\log n}{\\mathbb E}_{\\vec{k}}\\displaystyle\\left\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Now for the first term, applying Cauchy Schwartz yields ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\tilde{\\mathbf{A}}^{\\prime}}\\operatorname*{max}_{1\\le i\\le n}\\|\\tilde{\\mathbf{a}}_{i}^{\\prime}\\|_{2}\\left\\|\\sum_{i=1}^{n}\\tilde{\\mathbf{a}}_{i}^{\\prime}\\tilde{\\mathbf{a}}_{i}^{\\prime T}\\right\\|_{o p}^{1/2}\\le\\left(\\mathbb{E}_{\\tilde{\\mathbf{A}}^{\\prime}}\\operatorname*{max}_{1\\le i\\le n}\\|\\tilde{\\mathbf{a}}_{i}^{\\prime}\\|_{2}^{2}\\mathbb{E}_{\\tilde{\\mathbf{A}}^{\\prime}}\\right\\|\\sum_{i=1}^{n}\\tilde{\\mathbf{a}}_{i}^{\\prime}\\tilde{\\mathbf{a}}_{i}^{\\prime T}\\right\\|_{o p}\\right)^{1/2}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Moreover, for the last term we have from our assumptions ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\|\\mathbb{E}_{\\tilde{\\mathbf{a}}_{i}^{\\prime}}\\tilde{\\mathbf{a}}_{i}^{\\prime}\\tilde{\\mathbf{a}}_{i}^{\\prime}T\\right\\|_{o p}=\\operatorname*{max}_{\\|\\mathbf{v}\\|_{2}=1}\\mathbb{E}_{\\tilde{\\mathbf{a}}_{i}^{\\prime}}(\\tilde{\\mathbf{a}}_{i}^{\\prime T}\\mathbf{v})^{2}\\le\\operatorname*{max}_{\\|\\mathbf{v}\\|_{2}=1}C\\frac{\\|\\mathbf{v}\\|_{2}^{2}}{d}=\\frac{C}{d}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\bar{\\Lambda}^{\\prime}}\\displaystyle\\left\\|\\sum_{i=1}^{n}\\tilde{\\mathbf{a}}_{i}^{\\prime}\\tilde{\\mathbf{a}}_{i}^{\\prime}T\\right\\|_{o p}\\lesssim\\sqrt{\\log n}\\left(\\mathbb{E}_{\\bar{\\Lambda}^{\\prime}~\\operatorname*{max}_{i}~\\|\\tilde{\\mathbf{a}}_{i}^{\\prime}\\|_{2}^{2}\\mathbb{E}_{\\bar{\\Lambda}^{\\prime}}}\\left\\|\\sum_{i=1}^{n}\\tilde{\\mathbf{a}}_{i}^{\\prime}\\tilde{\\mathbf{a}}_{i}^{\\prime T}\\right\\|_{o p}\\right)^{1/2}+\\log n\\mathbb{E}_{\\bar{\\Lambda}^{\\prime}~\\operatorname*{max}_{1}~\\|\\tilde{\\mathbf{a}}_{i}^{\\prime}\\|_{2}^{2}+C}}\\\\ &{\\mathrm{lving~for}~\\left(\\mathbb{E}_{\\bar{\\Lambda}^{\\prime}}\\left\\|\\sum_{i=1}^{n}\\tilde{\\mathbf{a}}_{i}^{\\prime}\\tilde{\\mathbf{a}}_{i}^{\\prime T}\\right\\|_{o p}\\right)^{1/2}\\mathrm{~yields}}\\\\ &{\\qquad\\left(\\mathbb{E}_{\\bar{\\Lambda}^{\\prime}}\\left\\|\\sum_{i=1}^{n}\\tilde{\\mathbf{a}}_{i}^{\\prime}\\tilde{\\mathbf{a}}_{i}^{\\prime T}\\right\\|_{o p}\\right)^{1/2}\\lesssim\\sqrt{\\log n}\\left(\\mathbb{E}_{\\bar{\\Lambda}^{\\prime}~\\operatorname*{max}_{1\\leq i\\leq n}}\\|\\tilde{\\mathbf{a}}_{i}^{\\prime}\\|_{2}^{2}\\right)^{1/2}+2\\left(\\log n\\mathbb{E}_{\\bar{\\Lambda}^{\\prime}~\\operatorname*{max}_{1\\leq i\\leq n}}\\|\\tilde{\\mathbf{a}}_{i}^{\\prime}\\|_{2}^{2}+C\\right)^{1/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\tilde{\\mathbf{A}}^{\\prime}}\\left\\lVert\\sum_{i=1}^{n}\\tilde{\\mathbf{a}}_{i}^{\\prime}\\tilde{\\mathbf{a}}_{i}^{\\prime T}\\right\\rVert_{o p}\\lesssim\\log n\\mathbb{E}_{\\tilde{\\mathbf{A}}^{\\prime}}\\operatorname*{max}_{1\\leq i\\leq n}\\left\\lVert\\tilde{\\mathbf{a}}_{i}^{\\prime}\\right\\rVert_{2}^{2}+C\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Summarising, we have ", "page_idx": 28}, {"type": "text", "text": "\u6b63 $\\mathbb{1}_{\\tilde{\\Lambda}^{\\prime}}\\lVert\\tilde{\\mathbf{A}}^{\\prime}\\rVert_{o p}\\,\\lesssim\\,\\sqrt{\\log(d+n)}\\biggl(\\mathbb{B}_{\\tilde{\\Lambda}^{\\prime}~\\operatorname*{max}_{1}}\\,\\lVert\\tilde{\\mathbf{a}}_{i}^{\\prime}\\rVert_{2}^{2}+\\log n\\mathbb{B}_{\\tilde{\\Lambda}^{\\prime}~\\operatorname*{max}_{1}}\\,\\lVert\\tilde{\\mathbf{a}}_{i}^{\\prime}\\rVert_{2}^{2}+C\\biggr)^{1/2}+\\log(d+n)\\mathbb{B}_{\\tilde{\\Lambda}^{\\prime}~\\operatorname*{max}_{1}~\\forall i\\in\\tilde{\\Lambda}}\\,\\lVert\\tilde{\\mathbf{a}}_{i}^{\\prime}\\rVert_{2}^{2}$ 22 Thus it remains to provide an upper bound for $\\mathbb{E}_{\\tilde{\\mathbf{A}}^{\\prime}}\\operatorname*{max}_{1\\leq i\\leq n}\\|\\tilde{\\mathbf{a}}_{i}^{\\prime}\\|_{2}^{2}$ , we have by Jensen\u2019s ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left(\\mathbb{E}_{\\tilde{\\mathbf{A}}^{\\prime}}\\operatorname*{max}_{1\\leq i\\leq n}\\|\\tilde{\\mathbf{a}}_{i}^{\\prime}\\|_{2}^{2}\\right)^{3}\\leq\\mathbb{E}_{\\tilde{\\mathbf{A}}^{\\prime}}\\operatorname*{max}_{1\\leq i\\leq n}\\|\\tilde{\\mathbf{a}}_{i}^{\\prime}\\|_{2}^{6}\\leq\\sum_{i=1}^{n}\\mathbb{E}_{\\tilde{\\mathbf{a}}_{i}^{\\prime}}\\|\\tilde{\\mathbf{a}}_{i}^{\\prime}\\|_{2}^{6}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Now by Definition 2 and Holder\u2019s we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathfrak{z}_{\\tilde{\\mathfrak{a}}_{i}^{\\prime}}\\lVert\\tilde{\\mathbf{a}}_{i}^{\\prime}\\rVert_{2}^{6}=\\sum_{\\alpha_{1},\\alpha_{2},\\alpha_{3}}\\mathbb{E}_{\\tilde{a}_{i}^{\\prime}}\\tilde{\\mathbf{a}}_{i\\alpha_{1}}^{\\prime2}\\tilde{\\mathbf{a}}_{i\\alpha_{2}}^{\\prime2}\\tilde{\\mathbf{a}}_{i\\alpha_{3}}^{\\prime2}\\leq\\sum_{\\alpha_{1},\\alpha_{2},\\alpha_{3}}\\biggl(\\mathbb{B}_{\\tilde{a}_{i}^{\\prime}}\\tilde{\\mathbf{a}}_{i\\alpha_{1}}^{\\prime6}\\biggr)^{1/3}\\biggl(\\mathbb{B}_{\\tilde{a}_{i}^{\\prime}}\\tilde{\\mathbf{a}}_{i\\alpha_{2}}^{\\prime6}\\biggr)^{1/3}\\biggl(\\mathbb{B}_{\\tilde{a}_{i}^{\\prime}}\\tilde{\\mathbf{a}}_{i\\alpha_{3}}^{\\prime6}\\biggr)^{1/3}\\leq\\sum_{\\alpha_{1},\\alpha_{2},\\alpha_{3}}\\frac{C}{d^{3}}\\leq C_{\\alpha_{1},\\alpha_{2},\\alpha_{3}}\\,.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\tilde{\\mathbf{A}}^{\\prime}}\\operatorname*{max}_{1\\leq i\\leq n}\\|\\tilde{\\mathbf{a}}_{i}^{\\prime}\\|_{2}^{2}\\leq C n^{\\frac{1}{3}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "All in all ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\tilde{\\mathbf{A}}^{\\prime}}\\|\\tilde{\\mathbf{A}}^{\\prime}\\|_{o p}\\lesssim\\log(d+n)n^{\\frac{1}{3}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus by Markov\u2019s inequality for $s>0$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(\\|\\tilde{\\mathbf{A}}^{\\prime}\\|_{o p}\\geq C n^{\\frac{1}{3}+s}\\big)\\leq\\frac{\\mathbb{E}_{\\tilde{\\mathbf{A}}^{\\prime}}\\|\\tilde{\\mathbf{A}}^{\\prime}\\|_{o p}}{C n^{\\frac{1}{3}+s}}\\leq\\frac{C^{\\prime}\\log(d+n)n^{\\frac{1}{3}}}{n^{\\frac{1}{3}+s}}=\\frac{C^{\\prime}\\log(d+n)}{n^{s}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Lemma 9. For any function $\\psi:\\mathbb{R}\\to\\mathbb{R}$ with bounded third derivative, $\\tilde{\\psi}(\\mathbf{w}):=\\mathbb{E}_{\\mathbf{a}}\\psi(\\mathbf{a}^{T}\\mathbf{w})$ is regular Proof. We only need to verify the third order Taylor remainder bound, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{\\tau_{1},\\alpha_{2},\\alpha_{3}}\\frac{\\partial^{3}\\tilde{\\psi}}{\\partial w_{\\alpha_{1}}\\partial w_{\\alpha_{2}}\\partial w_{\\alpha_{3}}}\\nu_{\\alpha_{1}}\\nu_{\\alpha_{2}}\\nu_{\\alpha_{3}}=\\sum_{\\alpha_{1},\\alpha_{2},\\alpha_{3}}\\mathbb{E}_{\\mathbf{a}}\\psi^{\\prime\\prime\\prime}(\\mathbf{a}^{T}\\mathbf{w})a_{\\alpha_{1}}a_{\\alpha_{2}}a_{\\alpha_{3}}\\nu_{\\alpha_{1}}\\nu_{\\alpha_{2}}\\nu_{\\alpha_{3}}=\\mathbb{E}_{\\mathbf{a}}(\\mathbf{a}^{T}\\mathbf{v})^{3}\\psi^{\\prime\\prime\\prime}(\\mathbf{a}^{T}\\mathbf{w})^{3},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By Cauchy Schwartz, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{\\alpha_{1},\\alpha_{2},\\alpha_{3}}\\frac{\\partial^{3}\\tilde{\\psi}}{\\partial w_{\\alpha_{1}}\\partial w_{\\alpha_{2}}\\partial w_{\\alpha_{3}}}\\nu_{\\alpha_{1}}\\nu_{\\alpha_{2}}\\nu_{\\alpha_{3}}\\leq\\sqrt{\\mathbb{E}_{\\mathbf{a}}\\psi^{\\prime\\prime\\prime}(\\mathbf{a}^{T}\\mathbf{w})^{2}}\\sqrt{\\mathbb{E}_{\\mathbf{a}}(\\mathbf{a}^{T}\\mathbf{v})^{6}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By boundedness of the third derivative of $\\psi$ , and assumptions ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{\\alpha_{1},\\alpha_{2},\\alpha_{3}}\\frac{\\partial^{3}\\tilde{\\psi}}{\\partial w_{\\alpha_{1}}\\partial w_{\\alpha_{2}}\\partial w_{\\alpha_{3}}}\\nu_{\\alpha_{1}}\\nu_{\\alpha_{2}}\\nu_{\\alpha_{3}}\\leq C_{\\psi}\\frac{\\|\\mathbf{v}\\|_{2}^{3}}{d^{3/2}}\\leq C_{\\psi}\\|\\mathbf{v}\\|_{3}^{3}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "D Generalized CGMT ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "After reducing the analysis to the equivalent gaussian model, one can leverage a new generalization of the CGMT Akhtiamov et al. [2024] to analyze $\\Phi(\\mathbf G)$ which is stated as follows and recover the precise asymptotic properties of the solutions. ", "page_idx": 29}, {"type": "text", "text": "Theorem 5 (Generalized CGMT). Let $S_{w}\\,\\subset\\mathbb{R}^{d},S_{\\nu_{1}}\\,\\subset\\mathbb{R}^{n_{1}},\\ldots,S_{\\nu_{k}}\\,\\subset\\mathbb{R}^{n_{k}}$ be compact convex sets. Denote $S_{\\nu}\\,:=\\,S_{\\nu_{1}}\\,\\times\\,\\cdot\\,\\cdot\\,\\times\\,S_{\\nu_{k}}$ , let $\\textbf{v}\\in S_{\\nu}$ stand for $\\left(\\mathbf{v}_{1},\\ldots,\\mathbf{v}_{k}\\right)\\;\\in\\;S_{\\nu_{1}}\\,\\times\\,\\cdot\\,\\cdot\\,\\times\\,S_{\\nu_{k}}$ and $\\psi(\\mathbf{w},\\mathbf{v})\\,:\\,S_{w}\\,\\times\\,S_{\\nu}\\,\\,\\longrightarrow\\,\\mathbb{R}$ be convex on $S_{w}$ and concave on $S_{\\nu}$ . Also let $\\Sigma_{1},\\dots,\\Sigma_{k}\\ \\in\\ \\mathbb{R}^{d\\times d}$ be arbitrary $P S D$ matrices. Furthermore, let $\\mathbf{G}_{1}\\;\\in\\;\\mathbb{R}^{n_{1}\\times d},...\\,,\\mathbf{G}_{k}\\;\\in\\;\\mathbb{R}^{n_{k}\\times d}$ \u00d7 , g1, . . . , g\ud835\udc58 \u2208 R , $\\mathbf{h}_{1}\\in\\mathbb{R}^{n_{1}},\\ldots,\\mathbf{h}_{k}\\in\\mathbb{R}^{n_{k}}$ all have i.i.d $N(0,1)$ components and $\\mathbf{G}=(\\mathbf{G}_{1},\\ldots,\\mathbf{G}_{k})$ , $\\mathbf{g}=(\\mathbf{g}_{1},\\ldots,\\mathbf{g}_{k})$ , $\\mathbf{h}=(\\mathbf{h}_{1},\\ldots,\\mathbf{h}_{k})$ be the corresponding $k$ -tuples. Define ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\Psi(\\mathbf{G}):=\\operatorname*{min}_{\\mathbf{w}\\in S_{w}}\\operatorname*{max}_{\\mathbf{v}\\in S_{\\nu}}\\sum_{\\ell=1}^{k}\\mathbf{v}_{\\ell}^{T}\\mathbf{G}_{\\ell}\\Sigma_{\\ell}^{\\frac{1}{2}}\\mathbf{w}+\\psi(\\mathbf{w},\\mathbf{v})\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n\\phi(\\mathbf{g},\\mathbf{h}):=\\operatorname*{min}_{\\mathbf{w}\\in S_{w}}\\operatorname*{max}_{\\mathbf{v}\\in S_{v}}\\sum_{\\ell=1}^{k}\\left[\\|\\mathbf{v}_{\\ell}\\|_{2}\\mathbf{g}_{\\ell}^{T}\\Sigma_{\\ell}^{\\frac{1}{2}}\\mathbf{w}+\\|\\Sigma_{\\ell}^{\\frac{1}{2}}\\mathbf{w}\\|_{2}\\mathbf{h}_{\\ell}^{T}\\mathbf{v}_{\\ell}\\right]+\\psi(\\mathbf{w},\\mathbf{v})\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Let $s$ be an arbitrary open subset of $S_{w}$ and $S^{c}\\,=\\,S_{w}\\,\\backslash\\,S$ . Let $\\phi_{S}$ be the optimal value of the optimization $^{27}$ when ${\\boldsymbol{S}}_{w}$ is restricted to $s$ . Assume also that there exist $\\epsilon,\\delta>\\bar{0},\\,\\bar{\\phi},\\bar{\\phi}_{S^{c}}$ such that ", "page_idx": 29}, {"type": "text", "text": "$\\mathrm{~\\bar{~}{~\\bar{~}{\\phi}}}_{S^{c}}\\,\\geq\\,\\bar{\\phi}+3\\delta$ \u2022 $\\phi(g,h)<\\bar{\\phi}+\\delta$ with probability at least $1-\\epsilon$ \u2022 $\\phi_{S^{c}}>\\bar{\\phi}_{S^{c}}-\\delta$ with probability at least $1-\\epsilon$ ", "page_idx": 29}, {"type": "text", "text": "Then for the optimal solution w of the optimization 26, we have $\\mathbb{P}(\\mathbf{w}\\boldsymbol{\\Psi}(\\mathbf{G})\\in S)\\geq1-2^{k+1}\\epsilon$ ", "page_idx": 29}, {"type": "text", "text": "E Proof of Theorem 2 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "By leveraging our universality theorem 1 we are ready to analyze the linear regression problem descrided in 2 in full details ", "page_idx": 29}, {"type": "text", "text": "Proof. First note that for the ground truth we have $w_{*}=a r g m i n\\mathbb{E}_{P}(y-\\mathbf{x}^{T}\\mathbf{w})^{2}=\\mathbf{R}_{x}^{-1}\\mathbf{R}_{x y}$ . By a simple transformation, the constraint could also be written as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbf{\\Sigma}(\\mathbf{X}\\mathbf{\\\\\\y})\\left(\\mathbf{\\Sigma}_{-1}^{\\mathbf{w}}\\right)=0\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We would like to write for a $\\mathbf{G}$ with i.i.d entries: ", "page_idx": 29}, {"type": "equation", "text": "$$\n(\\textbf{X}\\textbf{y})=\\mathbf{G}\\mathbf{S}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Indeed ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbf{S}^{T}\\mathbb{E}\\mathbf{G}^{T}\\mathbf{G}\\mathbf{S}=\\mathbf{S}^{T}\\mathbf{S}=\\mathbb{E}\\left(\\mathbf{X}^{T}\\right)\\left(\\mathbf{X}\\quad\\mathbf{y}\\right)=\\mathbb{E}\\left(\\mathbf{R}_{x}\\quad\\mathbf{R}_{x y}\\right)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Now we can denote $\\mathbf{S}=\\left(\\mathbf{R}_{x}^{1/2}\\quad\\mathbf{R}_{x}^{-T/2}\\mathbf{R}_{x y}\\right)$ with $R_{\\Delta}:=R_{y}{-}\\mathbf{R}_{y x}\\mathbf{R}_{x}^{-1}\\mathbf{R}_{x y}$ . And $\\mathbf{R}_{x}^{-T/2}\\mathbf{R}_{x y}=\\mathbf{R}_{x}^{1/2}\\mathbf{w}_{*}$ Thus we can write the optimization as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\mathbf{w}}{\\mathrm{min}}\\,\\|\\mathbf{w}-\\mathbf{w}_{0}\\|^{2}}\\\\ {s.t\\quad\\mathbf{G}\\left(\\mathbf{R}_{x}^{1/2}(\\mathbf{w}-\\mathbf{w}_{*})\\right)=0}\\\\ {-R_{\\Delta}^{1/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "we can write the optimization as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\mathbf{w}}{\\mathrm{min}}\\,\\|\\mathbf{w}-\\mathbf{w}_{0}\\|^{2}}\\\\ {s.t\\quad\\mathbf{G}\\left(\\mathbf{R}_{x}^{1/2}(\\mathbf{w}-\\mathbf{w}_{*})\\right)=0}\\\\ {-R_{\\Delta}^{1/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Let $\\mathbf{u}:=\\mathbf{R}_{x}^{1/2}(\\mathbf{w}-\\mathbf{w}_{*})$ and $\\mathbf{x}=\\mathbf{w}_{*}-\\mathbf{w}_{0}$ , then ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\mathbf{w}}{\\mathop{\\operatorname*{min}}}\\:\\|\\mathbf{R}_{x}^{-1/2}\\mathbf{u}+\\mathbf{x}\\|^{2}}\\\\ {s.t\\quad\\mathbf{G}\\left(\\underset{-R_{\\Delta}^{1/2}}{\\mathbf{u}}\\right)=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We Lagrange multiplier to bring in the constraint: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{u}}\\operatorname*{max}_{\\mathbf{v}}\\|\\mathbf{R}_{x}^{-1/2}\\mathbf{u}+\\mathbf{x}\\|^{2}+\\mathbf{v}^{T}\\mathbf{G}\\left({\\underset{-R_{\\Delta}^{1/2}}{\\mathbf{u}}}\\right)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Now we can appeal to CGMT to obtain the AO: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{u}}\\operatorname*{max}_{\\mathbf{v}}\\|\\mathbf{R}_{x}^{-1/2}\\mathbf{u}+\\mathbf{x}\\|^{2}+\\|\\mathbf{v}\\|(\\mathbf{g}^{T}\\mathbf{u}+g^{\\prime}R_{\\Delta}^{1/2})+\\|\\left(\\mathbf{-}\\mathbf{R}_{\\Delta}^{1}\\right)\\|\\mathbf{h}^{T}\\mathbf{v}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Dropping the $g^{\\prime}R_{\\Delta}^{1/2}$ term and doing the optimization over the direction of $\\mathbf{v}$ , yields ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{u}}\\operatorname*{max}_{\\boldsymbol{\\beta}\\geq0}\\|\\mathbf{R}_{x}^{-1/2}\\mathbf{u}+\\mathbf{x}\\|^{2}+\\beta\\|\\mathbf{h}\\|\\sqrt{\\|\\mathbf{u}\\|^{2}+R_{\\Delta}}+\\beta\\mathbf{g}^{T}\\mathbf{u}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Using the identity $\\begin{array}{r}{\\sqrt{x}=\\operatorname*{min}_{\\tau\\geq0}\\frac{\\tau}{2}+\\frac{x^{2}}{2\\tau}}\\end{array}$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\tau\\geq0,\\mathbf{u}}\\operatorname*{max}_{\\beta\\geq0}\\|\\mathbf{R}_{x}^{-1/2}\\mathbf{u}+\\mathbf{x}\\|^{2}+\\frac{\\beta\\|\\mathbf{h}\\|\\tau}{2}+\\frac{\\beta\\|\\mathbf{h}\\|}{2\\tau}(\\|\\mathbf{u}\\|^{2}+R_{\\Delta})+\\beta\\mathbf{g}^{T}\\mathbf{u}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Now we obtain a quadratic optimization over $u$ which could be rewritten as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left(\\mathbf{u}^{T}\\quad1\\right)\\left(\\mathbf{R}_{x}^{-1}+\\frac{\\beta\\sqrt{n}}{2\\tau}\\mathbf{I}\\mathbf{\\Sigma}\\quad\\mathbf{R}_{x}^{-1/2}\\mathbf{x}+\\frac{\\beta}{2}\\mathbf{g}\\right)\\left(\\mathbf{u}\\right)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Doing the optimization over $\\mathbf{u}$ , we are left with ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\tau\\geq0}\\operatorname*{max}_{\\beta\\geq0}\\|\\mathbf{x}\\|^{2}+\\frac{\\beta\\tau\\sqrt{n}}{2}+\\frac{\\beta\\sqrt{n}}{2\\tau}R_{\\Delta}-(\\mathbf{R}_{x}^{-1/2}\\mathbf{x}+\\frac{\\beta}{2}\\mathbf{g})^{T}(\\mathbf{R}_{x}^{-1}+\\frac{\\beta\\sqrt{n}}{2\\tau}\\mathbf{I})^{-1}(\\mathbf{R}_{x}^{-1/2}\\mathbf{x}+\\frac{\\beta}{2}\\mathbf{g})\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Expanding the third term yields ", "page_idx": 30}, {"type": "text", "text": "$\\operatorname*{min}_{\\tau\\ge0}\\operatorname*{max}_{\\beta\\ge0}\\|\\mathbf{x}\\|^{2}+\\frac{\\beta\\tau\\sqrt{n}}{2}+\\frac{\\beta\\sqrt{n}}{2\\tau}R_{\\Delta}-\\mathbf{x}^{T}\\mathbf{R}_{x}^{-T/2}(\\mathbf{R}_{x}^{-1}+\\frac{\\beta\\sqrt{n}}{2\\tau}\\mathbf{I})^{-1}\\mathbf{R}_{x}^{-1/2}\\mathbf{x}-\\frac{\\beta^{2}}{4}\\mathbf{g}^{T}(\\mathbf{R}_{x}^{-1}+\\frac{\\beta\\sqrt{n}}{2\\tau}\\mathbf{I})^{-1}\\mathbf{g}$ Let $\\mathbf{z}:=\\mathbf{R}_{x}^{1/2}\\mathbf{x}$ , we assume $\\begin{array}{r}{\\mathbb{E}\\mathbf{z}\\mathbf{z}^{T}=\\frac{\\|\\mathbf{z}\\|^{2}}{d}I.}\\end{array}$ , and $e_{a}=\\mathbf{x}^{T}\\mathbf{R}_{x}\\mathbf{x}=\\|\\mathbf{z}\\|^{2}$ using concentration: $\\operatorname*{min}_{\\tau\\ge0}\\operatorname*{max}_{\\beta\\ge0}\\|x\\|^{2}+\\frac{\\beta\\tau\\sqrt{n}}{2}+\\frac{\\beta\\sqrt{n}}{2\\tau}R_{\\Delta}-d\\frac{1}{d}\\mathrm{Tr}\\mathbf{R}_{x}^{-T}(\\mathbf{R}_{x}^{-1}+\\frac{\\beta\\sqrt{n}}{2\\tau}\\mathbf{I})^{-1}\\mathbf{R}_{x}^{-1}-d\\frac{\\beta^{2}}{4}\\frac{1}{d}\\mathrm{Tr}(\\mathbf{R}_{x}^{-1}+\\frac{\\beta\\sqrt{n}}{2\\tau}\\mathbf{I})^{-1}$ ", "page_idx": 30}, {"type": "text", "text": "Therefore ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\tau\\ge0}\\operatorname*{max}_{\\beta\\ge0}\\|\\mathbf{x}\\|^{2}+\\frac{\\beta\\tau\\sqrt{n}}{2}+\\frac{\\beta\\sqrt{n}}{2\\tau}R_{\\Delta}-e_{a}\\int p(r)\\frac{1}{r(1+\\frac{\\beta\\sqrt{n}}{2\\tau}r)}d r-d\\frac{\\beta^{2}}{4}\\int p(r)\\frac{r}{1+\\frac{\\beta\\sqrt{n}}{2\\tau}r}d r\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Dropping the constant term $\\|\\mathbf{x}\\|^{2}$ , we arrive at the following scalarized optimization ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\tau\\geq0}\\operatorname*{max}_{\\beta\\geq0}\\frac{\\beta\\tau\\sqrt{n}}{2}+\\frac{\\beta\\sqrt{n}}{2\\tau}R_{\\Delta}-d\\int p(r)\\frac{\\frac{e_{a}}{d}+\\frac{\\beta^{2}}{4}r^{2}}{r(1+\\frac{\\beta\\sqrt{n}}{2\\tau}r)}d r\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Let $e_{a}^{\\prime}:=\\frac{e_{a}}{d}$ . To analyze the optimization further, noting that the objective is differentiable w.r.t $\\beta,\\tau$ , by taking derivatives we have at the optimal point: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\tau\\sqrt{n}}{2}+\\frac{\\sqrt{n}R_{\\Delta}}{2\\tau}-d\\int p(r)\\frac{\\tau(\\sqrt{n}r^{2}\\beta^{2}+4r\\tau\\beta-4e_{a}^{\\prime}\\sqrt{n})}{2(2\\tau+r\\beta\\sqrt{n})^{2}}d r=0}\\\\ {\\displaystyle\\frac{\\beta\\sqrt{n}}{2}-\\frac{\\beta R_{\\Delta}\\sqrt{n}}{2\\tau^{2}}-d\\int p(r)\\frac{\\beta\\sqrt{n}(\\beta^{2}r^{2}+4e_{a}^{\\prime})}{2(2\\tau+r\\beta\\sqrt{n})^{2}}d r=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Rearranging ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\tau^{2}\\sqrt{n}+\\sqrt{n}R_{\\Delta}=d\\displaystyle\\int p(r)\\frac{\\tau^{2}(\\sqrt{n}r^{2}\\beta^{2}+4r\\tau\\beta-4e_{a}^{\\prime}\\sqrt{n})}{(2\\tau+r\\beta\\sqrt{n})^{2}}d r}}\\\\ {{\\tau^{2}\\sqrt{n}-\\sqrt{n}R_{\\Delta}=d\\displaystyle\\int p(r)\\frac{\\tau^{2}(\\sqrt{n}\\beta^{2}r^{2}+\\sqrt{n}4e_{a}^{\\prime})}{(2\\tau+r\\beta\\sqrt{n})^{2}}d r}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Summing and subtracting: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{n=d\\displaystyle\\int p(r)\\frac{n r^{2}\\beta^{2}+2\\sqrt{n}r\\tau\\beta}{(2\\tau+r\\beta\\sqrt{n})^{2}}d r=d\\displaystyle\\int p(r)(1-\\frac{2\\sqrt{n}r\\tau\\beta+4\\tau^{2}}{(2\\tau+r\\beta\\sqrt{n})^{2}})d r}}\\\\ {{\\sqrt{n}R_{\\Delta}=d\\displaystyle\\int p(r)\\frac{\\tau^{2}(2r\\tau\\beta-4e_{a}^{\\prime}\\sqrt{n})}{(2\\tau+r\\beta\\sqrt{n})^{2}}d r}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Thus ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\frac{d-n}{d}=\\int\\,p(r)\\frac{2\\sqrt{n}r\\tau\\beta+4\\tau^{2}}{(2\\tau+r\\beta\\sqrt{n})^{2}}d r=\\int\\,p(r)\\frac{2\\tau}{2\\tau+r\\beta\\sqrt{n}}d r=\\frac{2\\tau}{\\beta\\sqrt{n}}S_{P}(-\\frac{2\\tau}{\\beta\\sqrt{n}}+i0)}\\\\ {\\sqrt{n}R_{\\Delta}=d\\int\\,p(r)\\frac{\\tau^{2}(2r\\tau\\beta-4e_{a}^{\\prime}\\sqrt{n})}{(2\\tau+r\\beta\\sqrt{n})^{2}}d r}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "If one can solve $\\begin{array}{r}{\\frac{d-n}{d}=\\frac{2\\tau}{\\beta\\sqrt{n}}S_{P}(-\\frac{2\\tau}{\\beta\\sqrt{n}}+i0)}\\end{array}$ to find $\\begin{array}{r}{\\theta:={\\frac{\\beta}{\\tau}}}\\end{array}$ , then plugging into the second equation yields ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\sqrt{n}R_{\\Delta}^{2}=d\\int p(r)\\frac{\\tau^{2}(2r\\theta\\tau^{2}-4e_{a}^{\\prime}\\sqrt{n})}{(2\\tau+r\\theta\\tau\\sqrt{n})^{2}}d r=d\\int p(r)\\frac{2r\\theta\\tau^{2}-4e_{a}^{\\prime}\\sqrt{n}}{(2+r\\theta\\sqrt{n})^{2}}d r\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Which yields the following equation for $\\tau^{2}$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\tau^{2}=\\frac{\\sqrt{n}}{2\\theta d\\int p(r)\\frac{r}{(2+r\\theta\\sqrt{n})^{2}}d r}\\left[R_{\\Delta}+4d e_{a}^{\\prime}\\int\\frac{p(r)}{(2+r\\theta\\sqrt{n})^{2}}d r\\right]\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Now note that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{d-n}{d}=\\int\\,p(r)\\frac{2\\tau}{2\\tau+r\\beta\\sqrt{n}}d r=\\int\\,p(r)\\frac{2}{2+r\\theta\\sqrt{n}}d r=\\int\\,p(r)\\frac{2(2+r\\theta\\sqrt{n})}{(2+r\\theta\\sqrt{n})^{2}}d r=}\\\\ {\\displaystyle=2\\theta\\sqrt{n}\\int\\,p(r)\\frac{r}{(2+r\\theta\\sqrt{n})^{2}}d r+4\\int\\,\\frac{p(r)}{(2+r\\theta\\sqrt{n})^{2}}d r}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Let $\\begin{array}{r}{\\alpha:=4\\int\\frac{p(r)}{(2+r\\theta\\sqrt{n})^{2}}d r,\\alpha^{\\prime}:=2\\theta\\sqrt{n}\\int p(r)\\frac{r}{(2+r\\theta\\sqrt{n})^{2}}d r}\\end{array}$ and . Then the expression for $\\tau$ could be written as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\tau^{2}=\\frac{n}{d\\alpha^{\\prime}}(R_{\\Delta}+B e_{a})\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "As \ud835\udefc+ \ud835\udefc\u2032 = \ud835\udc51\ud835\udc51\u2212\ud835\udc5b, one can write ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\tau^{2}=\\frac{n}{d(\\frac{d-n}{d}-\\alpha)}(R_{\\Delta}+\\alpha e_{a})=\\frac{n}{d-n-\\alpha d}(R_{\\Delta}+\\alpha e_{a})=\\frac{n}{d(1-\\alpha)-n}R_{\\Delta}+\\frac{\\alpha n}{d(1-\\alpha)-n}e_{a}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Now the generalization error is ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\tau^{2}-R_{\\Delta}=\\frac{2n-d(1-\\alpha)}{d(1-\\alpha)-n}R_{\\Delta}+\\frac{n B}{d(1-\\alpha)-n}e_{a}=\\frac{(2n-d)R_{\\Delta}+(d R_{\\Delta}+n e_{a})\\alpha}{d-n-d\\alpha}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "With \u222b (2+\ud835\udc5f\ud835\udc5d(\ud835\udf03\ud835\udc5f\u221a)\ud835\udc5b)2 \ud835\udc51\ud835\udc5f. Now note that using Cauchy Schwarz applied to\u221a\ufe01\ud835\udc5d(\ud835\udc5f) and22 $\\frac{2{\\sqrt{p(r)}}}{2+r\\,\\theta{\\sqrt{n}}}$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n(\\frac{d-n}{d})^{2}=(\\int p(r)\\frac{2}{2+r\\theta\\sqrt{n}}d r)^{2}\\leq\\int p(r)d r\\int\\frac{4p(r)}{(2+r\\theta\\sqrt{n})^{2}}d r=\\alpha\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Thus $\\begin{array}{r}{\\alpha\\geq(\\frac{d-n}{d})^{2}}\\end{array}$ . Now for the scalar distribution $p(r)=\\delta(r\\!-\\!r_{0})$ , since $\\begin{array}{r}{2\\!+r_{0}\\theta\\sqrt{n}=2\\!+\\frac{2n}{d-n}=\\frac{2d}{d-n}}\\end{array}$ , then $\\begin{array}{r}{\\alpha\\,=\\,\\big(\\frac{d-n}{d}\\big)^{2}}\\end{array}$ . Thus $\\alpha$ achieves its lower bound (uniquely) for the scalar distribution. Now note that assuming $n,d,e_{a},R_{\\Delta}$ are fixed, the generalization error is increasing in $\\alpha$ , thus the error expression for the generalization error in the scalar distribution case is a lower bound. \u25a1 ", "page_idx": 32}, {"type": "text", "text": "F Useful Results ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Lemma 10. Given a weight vector w, and assuming the feature vectors are equally likely to be drawn from class 1 or class 2 and satisfy part 2 of Assumptions 3, the corresponding classification error is given by ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{1}{2}Q(\\frac{\\mu_{1}^{T}\\mathbf{w}}{\\sqrt{\\mathbf{w}^{T}\\Sigma_{1}\\mathbf{w}}})+\\frac{1}{2}Q(-\\frac{\\mu_{2}^{T}\\mathbf{w}}{\\sqrt{\\mathbf{w}^{T}\\Sigma_{2}\\mathbf{w}}})\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Lemma 11. Assume that the feature vectors are equally likely to be drawn from class 1 or class 2, Assumptions 3 hold and $\\mathbf{w}_{0}$ is defined as in Assumption 4. Then the classification error corresponding to $\\mathbf{w}_{0}$ is equal to ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{1}{2}Q\\left(\\frac{\\sqrt{1-r}T r(\\Sigma_{1}+\\Sigma_{2})^{-1}}{\\sqrt{2T r(\\Sigma_{1}(\\Sigma_{1}+\\Sigma_{2})^{-2})+\\frac{t_{\\eta}^{2}}{t_{*}^{2}}T r\\Sigma_{1}}}\\right)+\\frac{1}{2}Q\\left(\\frac{\\sqrt{1-r}T r(\\Sigma_{1}+\\Sigma_{2})^{-1}}{\\sqrt{2T r(\\Sigma_{2}(\\Sigma_{1}+\\Sigma_{2})^{-2})+\\frac{t_{\\eta}^{2}}{t_{*}^{2}}T r\\Sigma_{2}}}\\right)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "G Proof of Lemma 1 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Let us proceed to prove Lemma 1. We will take the gradient of (28) w.r.t. $w$ and set it to 0: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{7_{\\mathbf{w}}\\left[\\displaystyle\\frac{1}{2}Q(-\\frac{\\mu_{2}^{T}\\mathbf{w}}{\\sqrt{\\mathbf{w}^{T}\\Sigma_{2}\\mathbf{w}}})\\right]=\\frac{1}{4\\sqrt{2\\pi}}\\left[\\exp\\big(-\\frac{\\mathbf{w}^{T}\\mu_{2}\\mu_{2}^{T}\\mathbf{w}}{2\\mathbf{w}^{T}\\Sigma_{2}\\mathbf{w}}\\big)\\frac{\\sqrt{\\mathbf{w}^{T}\\Sigma_{2}\\mathbf{w}}}{\\mu_{2}^{T}\\mathbf{w}}\\frac{\\mathbf{w}^{T}\\Sigma_{2}\\mathbf{w}\\mu_{2}\\mu_{2}^{T}\\mathbf{w}-\\mathbf{w}^{T}\\mu_{2}\\mu_{2}^{T}\\mathbf{w}\\Sigma_{2}\\mathbf{w}}{(\\mathbf{w}^{T}\\Sigma_{2}\\mathbf{w})^{2}}\\right]=}}\\\\ {{\\frac{1}{1\\sqrt{2\\pi}}\\left[\\displaystyle\\frac{1}{s_{2}}e x p(-\\frac{s_{2}^{2}}{2})(\\mu_{2}t_{2}-s_{2}^{2}\\Sigma_{2}\\mathbf{w})\\right]\\mathrm{~where~}s_{2}=\\frac{\\mu_{2}^{T}\\mathbf{w}}{\\sqrt{\\mathbf{w}^{T}\\Sigma_{2}\\mathbf{w}}},t_{2}=\\frac{\\mu_{2}^{T}\\mathbf{w}}{\\mathbf{w}^{T}\\Sigma_{2}\\mathbf{w}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{7_{\\mathbf{w}}\\left[\\frac{1}{2}Q(\\frac{\\mu_{1}^{T}\\mathbf{w}}{\\sqrt{\\mathbf{w}^{T}\\Sigma_{1}\\mathbf{w}}})\\right]=-\\frac{1}{4\\sqrt{2\\pi}}\\left[\\exp\\big(-\\frac{\\mathbf{w}^{T}\\mu_{1}\\mu_{1}^{T}\\mathbf{w}}{2\\mathbf{w}^{T}\\Sigma_{1}\\mathbf{w}}\\big)\\frac{\\sqrt{\\mathbf{w}^{T}\\Sigma_{1}\\mathbf{w}}}{\\mu_{1}^{T}\\mathbf{w}}\\frac{\\mathbf{w}^{T}\\Sigma_{1}\\mathbf{w}\\mu_{1}\\mu_{1}^{T}\\mathbf{w}-\\mathbf{w}^{T}\\mu_{1}\\mu_{1}^{T}\\mathbf{w}\\Sigma_{1}\\mathbf{w}}{\\big(\\mathbf{w}^{T}\\Sigma_{1}\\mathbf{w}\\big)^{2}}\\right]=}}\\\\ {{-\\,\\frac{1}{4\\sqrt{2\\pi}}\\left[\\frac{1}{s_{1}}\\exp\\big(-\\frac{s_{1}^{2}}{2}\\big)\\big(\\mu_{1}t_{1}-s_{1}^{2}\\Sigma_{1}\\mathbf{w}\\big)\\right]\\mathrm{~where~}s_{1}=\\frac{\\mu_{1}^{T}\\mathbf{w}}{\\sqrt{\\mathbf{w}^{T}\\Sigma_{1}\\mathbf{w}}},t_{1}=\\frac{\\mu_{1}^{T}\\mathbf{w}}{\\mathbf{w}^{T}\\Sigma_{1}\\mathbf{w}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Hence, the following equality holds at any optimal w: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{1}{s_{2}}\\exp{(-\\frac{s_{2}^{2}}{2})}(\\mu_{2}t_{2}-s_{2}^{2}\\Sigma_{2}\\mathbf{w})=\\frac{1}{s_{1}}\\exp{(-\\frac{s_{1}^{2}}{2})}(\\mu_{1}t_{1}-s_{1}^{2}\\Sigma_{1}\\mathbf{w})\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Therefore, $\\mathbf{w}=(\\tilde{s}_{1}\\pmb{\\Sigma}_{1}+\\tilde{s}_{2}\\pmb{\\Sigma}_{2})^{-1}(\\tilde{t}_{1}\\pmb{\\mu}_{1}-\\tilde{t}_{2}\\pmb{\\mu}_{2})$ , where we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{s}_{1}=s_{1}\\exp{(-\\frac{s_{1}^{2}}{2})},\\quad\\tilde{s}_{2}=-s_{2}\\exp{(-\\frac{s_{2}^{2}}{2})},}\\\\ {\\tilde{t}_{1}=\\displaystyle\\frac{t_{1}}{s_{1}}\\exp{(-\\frac{s_{1}^{2}}{2})},\\quad\\tilde{t}_{2}=\\frac{t_{2}}{s_{2}}\\exp{(-\\frac{s_{2}^{2}}{2})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Now we can consider the following optimization over 4 scalar variables: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\tilde{s}_{1},\\tilde{s}_{2},\\tilde{t}_{1},\\tilde{t}_{2}}\\frac{1}{2}Q(\\frac{\\mu_{1}^{T}\\mathbf{w}}{\\sqrt{\\mathbf{w}^{T}\\Sigma_{1}\\mathbf{w}}})+\\frac{1}{2}Q(-\\frac{\\mu_{2}^{T}\\mathbf{w}}{\\sqrt{\\mathbf{w}^{T}\\Sigma_{2}\\mathbf{w}}})\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Note that since the optimization (29) is low-dimensional, by the Convexity Lemma Liese and Miescke [2008], we can interchange the limit in $n$ and min, implying that we can analyze the concentrated optimization instead. We then arrive at the following fixed point equations using Assumptions 3: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{S_{1}^{2}}{t_{1}^{2}}=\\mathrm{Tr}(\\Sigma_{1}(\\tilde{s}_{1}\\Sigma_{1}+\\tilde{s}_{2}\\Sigma_{2})^{-2})(\\tilde{t}_{1}^{2}+\\tilde{t}_{2}^{2}-2\\tilde{t}_{1}\\tilde{t}_{2}r)d}\\\\ &{\\frac{S_{1}^{2}}{t_{1}}=\\mathrm{Tr}(\\tilde{s}_{1}\\Sigma_{1}+\\tilde{s}_{2}\\Sigma_{2})^{-1}(\\tilde{t}_{1}-\\tilde{t}_{2}r)d}\\\\ &{\\frac{S_{2}^{2}}{t_{2}^{2}}=\\mathrm{Tr}(\\Sigma_{2}(\\tilde{s}_{1}\\Sigma_{1}+\\tilde{s}_{2}\\Sigma_{2})^{-2})(\\tilde{t}_{1}^{2}+\\tilde{t}_{2}^{2}-2\\tilde{t}_{1}\\tilde{t}_{2}r)d}\\\\ &{\\frac{S_{2}^{2}}{t_{2}^{2}}=\\mathrm{Tr}(\\tilde{s}_{1}\\Sigma_{1}+\\tilde{s}_{2}\\Sigma_{2})^{-1}(\\tilde{t}_{2}-\\tilde{t}_{1}r)d}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Note that, since we assume $p$ is exchangeable in its arguments, the system of equations (30) is invariant under the transformation $\\left(s_{1},t_{1},s_{2},t_{2}\\right)\\to\\left(-s_{2},t_{2},-s_{1},t_{1}\\right)$ . As such, $s_{1}=-s_{2}$ and $t_{1}=t_{2}$ at the optimal point, implying that $\\mathbf{w}_{\\ast}=(\\Sigma_{1}+\\Sigma_{2})^{-1}({\\pmb{\\mu}}_{1}-{\\pmb{\\mu}}_{2})$ is the optimal classifier, as for the chosen linear classification procedure the performance in invariant to the rescalings of the classifier vector and therefore we can completely drop $s_{1},s_{2},t_{1},t_{2}$ from the equations. ", "page_idx": 33}, {"type": "text", "text": "H Proof of Theorem 3 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "By Gaussian universality proved in Theorem 1, we analyze the binary classification problem as follows ", "page_idx": 33}, {"type": "text", "text": "Proof. Define ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf R}_{0}:=\\mathbb{B}_{\\mu_{1},\\mu_{2}}{\\bf w}_{0}{\\bf w}_{0}^{T}=\\mathbb{B}_{\\mu_{1},\\mu_{2}}\\big(t_{*}{\\bf w}_{*}+t_{\\eta}\\eta\\big)\\big(t_{*}{\\bf w}_{*}+t_{\\eta}\\eta\\big)^{T}=\\frac{t_{\\eta}^{2}(1-r)}{d}I_{d}+t_{*}^{2}\\mathbb{B}_{\\mu_{1},\\mu_{2}}{\\bf w}_{*}{\\bf w}_{*}^{T}}\\ }\\\\ {{\\displaystyle=\\frac{t_{\\eta}^{2}(1-r)}{d}I_{d}+2\\frac{t_{*}^{2}}{d}(1-r)\\big(\\Sigma_{1}+\\Sigma_{2}\\big)^{-2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Note that the eigenvalue density of $\\mathbf{R}_{0}$ is captured by $\\begin{array}{r}{\\phi(s_{1},s_{2})\\;=\\;\\frac{t_{\\eta}^{2}(1-r)}{d}+\\frac{2t_{*}^{2}(1-r)}{d}\\big(s_{1}+s_{2}\\big)^{-2},}\\end{array}$ assuming $(s_{1},s_{2})\\sim p$ , where $p$ is the joint EPD function of $\\Sigma_{1}$ and $\\Sigma_{2}$ . ", "page_idx": 33}, {"type": "text", "text": "We would like to start with finding $\\alpha$ . Note that the data matrix $\\mathbf{X}$ can be decomposed as follows where $\\mathbf{G}_{1},\\mathbf{G}_{2}$ are i.i.d. standard normal: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{X}=\\left(\\mathbf{G}_{1}\\mathbf{\\Sigma}_{1}^{1/2}\\right)+\\left(\\mathbb{1}\\right.\\quad0\\bigg)\\left(\\mu_{1}^{T}\\right)}\\\\ {\\mathbf{G}_{2}\\mathbf{\\Sigma}_{2}^{1/2}\\bigg)+\\left(0\\right.\\quad\\mathbb{1}\\bigg)\\left(\\mu_{2}^{T}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We then have: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{X}\\mathbf{w}_{0}=\\left(\\mathbf{G}_{1}\\Sigma_{1}^{1/2}\\mathbf{w}_{0}\\right)+\\left(\\mathbb{1}\\quad0\\right)\\left(\\mu_{1}^{T}\\mathbf{w}_{0}\\right)}\\\\ {\\mathbf{G}_{2}\\Sigma_{2}^{1/2}\\mathbf{w}_{0}\\right)+\\left(\\mathbb{0}\\quad\\mathbb{1}\\right)\\left(\\mu_{2}^{T}\\mathbf{w}_{0}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "It is immediate to see that the first term of (34) is a centered normally distributed random vector with covariance $\\mathrm{diag}(\\mathbf{w}_{0}^{T}\\Sigma_{1}\\mathbf{w}_{0},\\dots,\\mathbf{w}_{0}^{T}\\Sigma_{1}\\mathbf{w}_{0},\\mathbf{w}_{0}^{T}\\Sigma_{2}\\mathbf{w}_{0},\\dots,\\mathbf{w}_{0}^{T}\\Sigma_{2}\\mathbf{w}_{0})$ , and therefore its norm concentrates to $\\begin{array}{r}{\\sqrt{\\frac{n}{2}\\mathbf{w}_{0}^{T}(\\Sigma_{1}+\\Sigma_{2})\\mathbf{w}_{0}}=\\Theta(\\sqrt{\\mathrm{Tr}(\\Sigma_{1}+\\Sigma_{2})}||\\mathbf{w}_{0}||)}\\end{array}$ according to our assumptions. The second term of (34) is a deterministic vector of norm $\\Theta(\\sqrt{n}||\\mathbf{w}_{0}||\\sqrt{1-r})$ . ", "page_idx": 34}, {"type": "text", "text": "Similarly: ", "page_idx": 34}, {"type": "equation", "text": "$$\n{\\mathbf{\\nabla}}^{T}\\mathbf{X}\\mathbf{w}_{0}=\\left({\\mathbb{I}}^{T}\\mathbf{\\nabla}-{\\mathbb{I}}^{T}\\right)\\left(\\left({\\mathbf{G}}_{2}{\\boldsymbol{\\Sigma}}_{2}^{1\\slash2}\\mathbf{w}_{0}\\right)+\\left({\\mathbb{I}}_{0}^{1}\\mathbf{\\Lambda}_{1}^{0}\\right)\\left({\\mu}_{2}^{T}\\mathbf{w}_{0}\\right)\\right)=\\left({\\mathbb{I}}^{T}\\mathbf{\\nabla}-{\\mathbb{I}}^{T}\\right)\\left({\\mathbf{G}}_{2}{\\boldsymbol{\\Sigma}}_{2}^{1\\slash2}\\mathbf{w}_{0}\\right)+{\\frac{n}{2}}(\\mu_{1}-\\mu_{2})^{T}\\mathbf{v}_{0}={\\mathbb{I}}^{T}\\mathbf{\\boldsymbol{w}}_{0}+\\mathbf{\\boldsymbol{\\Sigma}}_{1}^{3\\slash2}\\mathbf{w}_{0}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Note that we can drop the first term of (35) compared to the second term. Indeed, the first term is a centered normal random variable with variance ${\\overset{\\cdot}{2}}\\mathbf{w}_{0}^{T}(\\Sigma_{1}+\\Sigma_{2})\\mathbf{w}_{0}=\\Theta(\\|\\mathbf{w}_{0}\\|^{2}\\mathrm{Tr}(\\Sigma_{1}+\\Sigma_{2}))$ and the second term is a deterministic quantity of order $\\Theta(n||\\mathbf{w}_{0}||\\sqrt{1-r})$ ", "page_idx": 34}, {"type": "text", "text": "Thus ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha=\\frac{\\mathbf{y}^{T}\\mathbf{X}\\mathbf{w}_{0}}{\\|\\mathbf{X}\\mathbf{w}_{0}\\|^{2}}=\\frac{\\frac{n}{2}(\\mu_{1}-\\mu_{2})^{T}\\mathbf{w}_{0}}{\\frac{n}{2}\\mathbf{w}_{0}^{T}(\\Sigma_{1}+\\Sigma_{2})\\mathbf{w}_{0}+\\frac{n}{2}(\\mu_{1}^{T}\\mathbf{w}_{0})^{2}+\\frac{n}{2}(\\mu_{2}^{T}\\mathbf{w}_{0})^{2}}=}\\\\ &{\\frac{\\frac{n}{2}t_{*}(1-r)\\mathrm{Tr}(\\frac{(\\Sigma_{1}+\\Sigma_{2})^{-1}}{d})}{\\frac{n}{2}\\mathrm{Tr}(\\mathbf{R}_{0}(\\Sigma_{1}+\\Sigma_{2}))+n(t_{*}(1-r)\\mathrm{Tr}\\frac{((\\Sigma_{1}+\\Sigma_{2})^{-1})}{d})^{2}}=}\\\\ &{\\frac{\\frac{1}{2}t_{*}(1-r)\\mathrm{Tr}(\\frac{(\\Sigma_{1}+\\Sigma_{2})^{-1}}{d})}{\\frac{1}{2}\\mathrm{Tr}(\\mathbf{R}_{0}(\\Sigma_{1}+\\Sigma_{2}))+t_{*}(1-r)\\mathrm{Tr}\\frac{((\\Sigma_{1}+\\Sigma_{2})^{-1})}{d})^{2}}=}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Then SGD converges to ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\mathbf{w}}{\\operatorname*{min}}\\left\\|\\mathbf{w}-\\alpha\\mathbf{w}_{0}\\right\\|^{2}}\\\\ {s.t\\quad\\;\\mathbf{y}=\\mathbf{X}\\mathbf{w}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Introducing a Lagrange multiplier $\\mathbf{v}\\in\\mathbb{R}^{n}$ : ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{w}}\\operatorname*{max}_{\\mathbf{v}}\\|\\mathbf{w}-\\alpha\\mathbf{w}_{0}\\|^{2}+\\mathbf{v}^{T}\\mathbf{G}\\left(\\Sigma_{2}^{1/2}\\right)\\mathbf{w}+\\mathbf{v}^{T}\\mathbf{M}\\mathbf{w}-\\mathbf{v}^{T}\\mathbf{y}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Using Theorem 4 from Akhtiamov et al. [2024], ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\operatorname*{max}\\|-\\alpha_{0}\\|^{2}+\\sum_{i=1}^{2}\\|\\nu_{i}\\|_{i}^{T}\\Sigma_{i}^{1/2}+\\|\\Sigma_{i}^{1/2}\\|\\mathbb{h}_{i}^{T}\\boldsymbol{i}+\\frac{T}{i}\\mathbb{1}\\mu_{i}^{T}+\\frac{T}{i}\\mathbb{1}(\\!-\\!1)^{i}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Denoting $\\beta_{i}:=\\|\\mathbf{v}_{i}\\|$ and performing optimization over the direction of $\\nu_{i}$ : ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\beta_{1},\\beta_{2}\\geq0}\\big\\|-\\alpha_{0}\\big\\|^{2}+\\sum_{i=1}^{2}\\beta_{i}\\big(_{i}^{T}\\Sigma_{i}^{1/2}+\\|\\mathbb{h}_{i}\\|\\Sigma_{i}^{1/2}\\|+\\mu_{i}^{T}\\mathbb{1}+(-1)^{i}\\mathbb{1}\\|\\big)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{w}}\\operatorname*{max}_{\\boldsymbol{\\beta_{1}},\\boldsymbol{\\beta_{2}}\\geq0}\\|\\mathbf{w}-\\alpha\\mathbf{w}_{0}\\|^{2}+\\sum_{i=1}^{2}\\beta_{i}(\\mathbf{g}_{i}^{T}\\Sigma_{i}^{1/2}\\mathbf{w}+\\sqrt{\\frac{n}{2}}\\sqrt{\\|\\Sigma_{i}^{1/2}\\mathbf{w}\\|^{2}+(\\mu_{i}^{T}\\mathbf{w}+(-1)^{i})^{2}})\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Applying the square root trick to the norm inside the objective we arrive at: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\substack{r_{1},\\ldots,r_{2}\\geq0,\\mathbf{w}\\,\\beta_{1},\\beta_{2}\\geq0}}\\|\\mathbf{w}-\\alpha\\mathbf{w}_{0}\\|^{2}+\\sum_{i=1}^{2}\\beta_{i}\\mathbf{g}_{i}^{T}\\Sigma_{i}^{1/2}\\mathbf{w}+\\sqrt{\\frac{n}{2}}\\frac{\\beta_{i}\\tau_{i}}{2}+\\frac{\\beta_{i}}{2\\tau_{i}}\\sqrt{\\frac{n}{2}}(\\|\\Sigma_{i}^{1/2}\\mathbf{w}\\|^{2}+(\\mu_{i}^{T}\\mathbf{w}+(-1)^{i})^{2})\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Introducing $\\gamma_{i}$ \u2019s as the Fenchel duals for $(\\mu_{i}^{T}\\mathbf{w}+(-1)^{i})^{2}$ we have: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{1,\\tau_{2}\\geq0,\\mathbf{w}\\beta_{1},\\beta_{2}\\geq0,\\gamma_{1},\\gamma_{2}}\\|\\mathbf{w}-\\alpha\\mathbf{w}_{0}\\|^{2}+\\sum_{i=1}^{2}\\beta_{i}\\mathbf{g}_{i}^{T}\\Sigma_{i}^{1/2}\\mathbf{w}+\\sqrt{\\frac{n}{2}}\\frac{\\beta_{i}\\tau_{i}}{2}+\\frac{\\beta_{i}}{2\\tau_{i}}\\sqrt{\\frac{n}{2}}(\\|\\Sigma_{i}^{1/2}\\mathbf{w}\\|^{2}+\\mu_{i}^{T}\\mathbf{w}\\gamma_{i}+(-1)^{i})\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Performing optimization over w yields: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\operatorname*{min}_{\\tau_{1},\\tau_{2}\\geq0}\\displaystyle\\operatorname*{max}_{\\beta_{1},\\beta_{2}\\geq0,\\gamma_{1},\\gamma_{2}}\\alpha^{2}\\|\\mathbf{w}_{0}\\|^{2}+\\sum_{i=1}^{2}\\sqrt{\\frac{n}{2}}\\frac{\\beta_{i}\\tau_{i}}{2}+\\sqrt{\\frac{n}{2}}\\frac{\\beta_{i}}{2\\tau_{i}}(\\gamma_{i}(-1)^{i}-\\frac{\\gamma_{i}^{2}}{4})}\\\\ {\\displaystyle-\\left(-\\alpha\\mathbf{w}_{0}+\\frac{1}{2}(\\beta_{1}\\Sigma_{1}^{1/2}\\mathbf{g}_{1}+\\beta_{2}\\Sigma_{2}^{1/2}\\mathbf{g}_{2})+\\frac{\\beta_{1}}{4\\tau_{1}}\\sqrt{\\frac{n}{2}}\\gamma_{1}\\mu_{1}+\\frac{\\beta_{2}}{4\\tau_{2}}\\sqrt{\\frac{n}{2}}\\gamma_{2}\\mu_{2}\\right)^{T}\\left(I+\\frac{\\beta_{1}}{2\\tau_{1}}\\sqrt{\\frac{n}{2}}\\Sigma_{1}+\\frac{\\beta_{2}}{2\\tau_{2}}\\sqrt{\\frac{n}{2}}\\Sigma_{2}\\right)}\\\\ {\\displaystyle\\cdot\\left(-\\alpha\\mathbf{w}_{0}+\\frac{1}{2}(\\beta_{1}\\Sigma_{1}^{1/2}\\mathbf{g}_{1}+\\beta_{2}\\Sigma_{2}^{1/2}\\mathbf{g}_{2})+\\frac{\\beta_{1}}{4\\tau_{1}}\\sqrt{\\frac{n}{2}}\\gamma_{1}\\mu_{1}+\\frac{\\beta_{2}}{4\\tau_{2}}\\sqrt{\\frac{n}{2}}\\gamma_{2}\\mu_{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "We drop the $\\alpha^{2}\\|\\mathbf{w}_{0}\\|^{2}$ from the objective because it does not contain any of the variables the optimization is performed over and therefore dropping it will not change the optimal values of $\\beta_{1},\\beta_{2},\\gamma_{1},\\gamma_{2},\\tau_{1},\\tau_{2}$ . We obtain: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{min}_{\\tau_{1},\\tau_{2}\\geq0,\\emptyset_{1},\\beta_{2}\\geq0,\\tau_{1},\\gamma_{2}}\\sum_{i=1}^{2}\\sqrt{\\underline{{n}}}\\frac{\\beta_{i}\\tau_{i}}{2}+\\sqrt{\\underline{{n}}}\\frac{\\beta_{i}}{2}(\\gamma_{i}(-1)^{i}-\\frac{\\gamma_{i}^{2}}{4})-\\alpha^{2}\\mathbf{v}_{0}^{T}(I+\\frac{\\beta_{1}}{2\\tau_{1}}\\sqrt{\\underline{{n}}}\\Sigma_{1}+\\frac{\\beta_{2}}{2\\tau_{2}}\\sqrt{\\underline{{n}}}\\Sigma_{2})^{-1}}\\\\ {-\\frac{\\beta_{1}^{2}}{4}\\mathbf{g}_{1}^{T}\\Sigma_{1}^{T/2}(I+\\frac{\\beta_{1}}{2\\tau_{1}}\\sqrt{\\underline{{n}}}\\Sigma_{1}+\\frac{\\beta_{2}}{2\\tau_{2}}\\sqrt{\\underline{{n}}}\\Sigma_{2})^{-1}\\Sigma_{1}^{1/2}\\mathbf{g}_{1}-\\frac{\\beta_{2}^{2}}{4}\\mathbf{g}_{2}^{T}\\Sigma_{2}^{T/2}(I+\\frac{\\beta_{1}}{2\\tau_{1}}\\sqrt{\\underline{{n}}}\\Sigma_{1}+\\frac{\\beta_{2}}{2\\tau_{2}}\\sqrt{\\underline{{n}}}\\Sigma_{2})^{-1}\\Sigma_{2}^{1/2}}\\\\ {-\\frac{n\\beta_{1}^{2}\\gamma_{1}^{2}}{32\\tau_{1}^{2}}\\mu_{1}^{T}(I+\\frac{\\beta_{1}}{2\\tau_{1}}\\sqrt{\\underline{{n}}}\\Sigma_{1}+\\frac{\\beta_{2}}{2\\tau_{2}}\\sqrt{\\underline{{n}}}\\Sigma_{2})^{-1}\\mu_{1}-\\frac{n\\beta_{2}^{2}\\gamma_{2}^{2}}{32\\tau_{2}^{2}}\\mu_{2}^{T}(I+\\frac{\\beta_{1}}{2\\tau_{1}}\\sqrt{\\underline{{n}}}\\Sigma_{1}+\\frac{\\beta_{2}}{2\\tau_{2}}\\sqrt{\\underline{{n}}}\\Sigma_{2})^{-1}}\\\\ {-\\frac{\\beta_{1}}{ \n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The objective above can be rewritten in the integral form in the following way: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\underset{\\tau_{1},\\tau_{2}\\geq0}{\\operatorname*{min}}\\underset{\\beta_{1},\\beta_{2}\\geq0,\\gamma_{1},\\gamma_{2}}{\\operatorname*{max}}\\sum_{i=1}^{2}\\sqrt{\\frac{n}{2}}\\frac{\\beta_{i}\\beta_{i}\\tau_{i}}{2}+\\sqrt{\\frac{n}{2}}\\frac{\\beta_{i}}{2}(\\gamma_{i}(-1)^{i}-\\frac{\\gamma_{i}^{2}}{4})-\\alpha^{2}\\mathbf{w}_{0}^{T}(I+\\frac{\\beta_{1}}{2\\tau_{1}}\\sqrt{\\frac{n}{2}}\\mathbf{\\Sigma}_{1}+\\frac{\\beta_{2}}{2\\tau_{2}}\\sqrt{\\frac{n}{2}}\\mathbf{\\Sigma}_{2})^{-1}\\mathbf{v}}\\\\ &{}&{-d\\int\\int p(s_{1},s_{2})\\frac{s_{1}\\beta_{1}^{2}+s_{2}\\beta_{2}^{2}}{1+\\frac{\\beta_{1}}{2\\tau_{1}}\\sqrt{\\frac{n}{2}}s_{1}+\\frac{\\beta_{2}}{2\\tau_{2}}\\sqrt{\\frac{n}{2}}s_{2}}d s_{1}d s_{2}-\\int\\int p(s_{1},s_{2})\\frac{\\frac{n\\beta_{1}^{2}\\gamma_{1}^{2}}{32\\tau_{1}^{2}}+\\frac{n\\beta_{2}^{2}\\gamma_{2}^{2}}{32\\tau_{2}^{2}}}{1+\\frac{\\beta_{1}}{2\\tau_{1}}\\sqrt{\\frac{n}{2}}s_{1}+\\frac{\\beta_{2}}{2\\tau_{2}}\\sqrt{\\frac{n}{2}}s_{2}}d s_{1}d}\\\\ &{}&{+\\frac{\\beta_{1}}{2\\tau_{1}}\\sqrt{\\frac{n}{2}}\\gamma_{1}\\alpha\\mathbf{w}_{0}^{T}(I+\\frac{\\beta_{1}}{2\\tau_{1}}\\sqrt{\\frac{n}{2}}\\mathbf{\\Sigma}_{1}+\\frac{\\beta_{2}}{2\\tau_{2}}\\sqrt{\\frac{n}{2}}\\mathbf{\\Sigma}_{2})^{-1}\\mu_{1}+\\frac{\\beta_{2}}{2\\tau_{2}}\\sqrt{\\frac{n}{2}}\\gamma_{2}\\alpha\\mathbf{w}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Note that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\alpha^{2}\\mathbf{w}_{0}^{T}(I+\\frac{\\beta_{1}}{2\\tau_{1}}\\sqrt{\\frac{n}{2}}\\Sigma_{1}+\\frac{\\beta_{2}}{2\\tau_{2}}\\sqrt{\\frac{n}{2}}\\Sigma_{2})^{-1}\\mathbf{w}_{0}\\rightarrow\\alpha^{2}\\mathrm{Tr}(\\mathbf{R}_{0}(I+\\frac{\\beta_{1}}{2\\tau_{1}}\\sqrt{\\frac{n}{2}}\\Sigma_{1}+\\frac{\\beta_{2}}{2\\tau_{2}}\\sqrt{\\frac{n}{2}}\\Sigma_{2})^{-1})\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Moreover ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathrm{Tr}(\\mathbf{R}_{0}(I+\\frac{\\beta_{1}}{2\\tau_{1}}\\sqrt{\\frac{n}{2}}\\Sigma_{1}+\\frac{\\beta_{2}}{2\\tau_{2}}\\sqrt{\\frac{n}{2}}\\Sigma_{2})^{-1})\\rightarrow d\\int\\int p(s_{1},s_{2})\\frac{\\phi(s_{1},s_{2})}{1+\\frac{\\beta_{1}}{2\\tau_{1}}\\sqrt{\\frac{n}{2}}s_{1}+\\frac{\\beta_{2}}{2\\tau_{2}}\\sqrt{\\frac{n}{2}}s_{2}}d s_{1}d s_{2}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "And ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathrm{v}_{0}^{T}(I+\\frac{\\beta_{1}}{2\\tau_{1}}\\sqrt{\\frac{n}{2}}\\Sigma_{1}+\\frac{\\beta_{2}}{2\\tau_{2}}\\sqrt{\\frac{n}{2}}\\Sigma_{2})^{-1}\\mu_{1}\\rightarrow\\frac{t_{*}(1-r)}{d}\\mathrm{Tr}((I+\\frac{\\beta_{1}}{2\\tau_{1}}\\sqrt{\\frac{n}{2}}\\Sigma_{1}+\\frac{\\beta_{2}}{2\\tau_{2}}\\sqrt{\\frac{n}{2}}\\Sigma_{2})^{-1}(\\Sigma_{1}+\\Sigma_{2})^{-1})\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The latter term can be derived in the integral form as: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\Upsilon\\left((I+\\frac{\\beta_{1}}{2\\tau_{1}}\\sqrt{\\frac{n}{2}}\\Sigma_{1}+\\frac{\\beta_{2}}{2\\tau_{2}}\\sqrt{\\frac{n}{2}}\\Sigma_{2})^{-1}(\\Sigma_{1}+\\Sigma_{2})^{-1}\\right)\\rightarrow d\\int\\int p(s_{1},s_{2})\\frac{(s_{1}+s_{2})^{-1}}{1+\\frac{\\beta_{1}}{2\\tau_{1}}\\sqrt{\\frac{n}{2}}s_{1}+\\frac{\\beta_{2}}{2\\tau_{2}}\\sqrt{\\frac{n}{2}}s_{2}}d s_{1}d s\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Since $p(s_{1},s_{2})$ is exchangeable in its arguments, we see that the latter objective remains invariant under the transformation that swaps $(\\beta_{1},\\tau_{1},\\gamma_{1})$ with $(\\beta_{2},\\tau_{2},-\\gamma_{2})$ . Therefore, $\\beta\\;:=\\;\\beta_{1}\\;=\\;\\beta_{2}$ , $\\tau\\,:=\\,\\tau_{1}\\,=\\,\\tau_{2}$ and $\\gamma:=\\gamma_{2}\\,=\\,-\\gamma_{1}$ holds at the optimal point. The objective then reduces to the following: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\operatorname*{max}_{\\beta\\geq0,\\gamma}\\sqrt{\\frac{n}{2}}\\beta\\tau+\\sqrt{\\frac{n}{2}}\\frac{\\beta}{\\tau}(\\gamma-\\frac{\\gamma^{2}}{4})-\\int\\int p(s_{1},s_{2})\\frac{d\\alpha^{2}\\phi(s_{1},s_{2})+d\\frac{(s_{1}+s_{2})\\beta^{2}}{4}+\\frac{n\\beta^{2}\\gamma^{2}}{16\\tau^{2}}(1-r)}{1+\\frac{\\beta}{2\\tau}(s_{1}+s_{2})\\sqrt{\\frac{n}{2}}}d s_{1}d s_{2}}}}\\\\ {{\\displaystyle{\\qquad-\\,2t_{*}(1-r)\\alpha\\sqrt{\\frac{n}{2}}\\int\\int p(s_{1},s_{2})\\frac{\\frac{\\beta\\gamma}{2\\tau}(s_{1}+s_{2})^{-1}}{1+\\frac{\\beta}{2\\tau}\\sqrt{\\frac{n}{2}}(s_{1}+s_{2})}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Note that $\\gamma$ can be found explicitly in terms of $\\beta$ and $\\tau$ as the optimization over $\\gamma$ is quadratic. Denote $\\begin{array}{r}{e_{0}=\\frac{\\|w_{0}\\|^{2}}{d}}\\end{array}$ . To facilitate derivations, let us introduce a few notations. Let ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{F_{1}(\\frac{\\beta}{\\tau},\\gamma):=\\displaystyle\\int\\int p(s_{1},s_{2})\\frac{d\\alpha^{2}\\phi(s_{1},s_{2})+\\frac{n\\beta^{2}\\gamma^{2}}{16\\tau^{2}}(1-r)+2t_{*}(1-r)\\alpha\\sqrt{\\frac{n}{2}}\\frac{\\beta\\gamma}{2\\tau}(s_{1}+s_{2})^{-1}}{1+\\frac{\\beta}{2\\tau}(s_{1}+s_{2})\\sqrt{\\frac{n}{2}}}d s_{1}d s_{2}}}\\\\ {{\\displaystyle F_{2}(\\frac{\\beta}{\\tau},\\beta):=d\\int\\int p(s_{1},s_{2})\\frac{\\frac{(s_{1}+s_{2})\\beta^{2}}{4}}{1+\\frac{\\beta}{2\\tau}\\sqrt{\\frac{n}{2}}(s_{1}+s_{2})}d s_{1}d s_{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Thus the objective is ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\tau\\geq0}\\operatorname*{max}_{\\beta\\geq0,\\gamma}-F_{1}(\\frac{\\beta}{\\tau},\\gamma)-F_{2}(\\frac{\\beta}{\\tau},\\beta)+\\sqrt{\\frac{n}{2}}\\beta\\tau+\\sqrt{\\frac{n}{2}}\\frac{\\beta}{\\tau}(\\gamma-\\frac{\\gamma^{2}}{4})\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Now taking derivatives w.r.t $\\tau$ yields ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\partial_{\\tau}F_{1}(\\frac{\\beta}{\\tau},\\gamma)=-\\frac{\\beta}{\\tau^{2}}\\partial_{x}F_{1}(x,\\gamma)}}\\\\ {{\\displaystyle\\partial_{\\tau}F_{2}(\\frac{\\beta}{\\tau},\\beta)=-\\frac{\\beta}{\\tau^{2}}\\partial_{x}F_{2}(x,\\beta)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Now for $\\beta$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\partial_{\\beta}F_{1}(\\frac{\\beta}{\\tau},\\gamma)=\\frac{1}{\\tau}\\partial_{x}F_{1}(x,\\gamma)}}\\\\ {\\displaystyle{\\partial_{\\beta}F_{2}(\\frac{\\beta}{\\tau},\\beta)=\\frac{2}{\\beta}F_{2}(\\frac{\\beta}{\\tau},\\beta)+\\frac{1}{\\tau}\\partial_{x}F_{2}(x,\\beta)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Setting the derivative of the objective w.r.t $\\tau$ to 0: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\frac{\\beta}{\\tau^{2}}\\partial_{x}F_{1}(x,\\gamma)+\\frac{\\beta}{\\tau^{2}}\\partial_{x}F_{2}(x,\\beta)+\\sqrt{\\frac{n}{2}}\\beta-\\sqrt{\\frac{n}{2}}\\frac{\\beta}{\\tau^{2}}(\\gamma-\\frac{\\gamma^{2}}{4})=0\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Multiplying by $\\tau^{2}$ and dropping $_\\beta$ yields: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\partial_{x}F_{1}(x,\\gamma)+\\partial_{x}F_{2}(x,\\beta)+\\sqrt{\\frac{n}{2}}\\tau^{2}-\\sqrt{\\frac{n}{2}}(\\gamma-\\frac{\\gamma^{2}}{4})=0\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Derivative w.r.t. $\\beta$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n-\\frac{1}{\\tau}\\partial_{x}F_{1}(x,\\gamma)-\\frac{2}{\\beta}F_{2}(\\frac{\\beta}{\\tau},\\beta)-\\frac{1}{\\tau}\\partial_{x}F_{2}(x,\\beta)+\\sqrt{\\frac{n}{2}}\\tau+\\sqrt{\\frac{n}{2}}\\frac{1}{\\tau}(\\gamma-\\frac{\\gamma^{2}}{4})=0\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Multiplying by $\\tau$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n-\\partial_{x}F_{1}(x,\\gamma)-\\frac{2\\tau}{\\beta}F_{2}(\\frac{\\beta}{\\tau},\\beta)-\\partial_{x}F_{2}(x,\\beta)+\\sqrt{\\frac{n}{2}}\\tau^{2}+\\sqrt{\\frac{n}{2}}(\\gamma-\\frac{\\gamma^{2}}{4})=0\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Therefore the set of equations is: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int\\!\\partial_{x}F_{1}(x,\\gamma)+\\partial_{x}F_{2}(x,\\beta)+\\sqrt{\\frac{n}{2}}\\tau^{2}-\\sqrt{\\frac{n}{2}}(\\gamma-\\frac{\\gamma^{2}}{4})=0}\\\\ &{\\quad-\\partial_{x}F_{1}(x,\\gamma)-\\frac{2\\tau}{\\beta}F_{2}(\\frac{\\beta}{\\tau},\\beta)-\\partial_{x}F_{2}(x,\\beta)+\\sqrt{\\frac{n}{2}}\\tau^{2}+\\sqrt{\\frac{n}{2}}(\\gamma-\\frac{\\gamma^{2}}{4})=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Summing the equations yields ", "page_idx": 37}, {"type": "equation", "text": "$$\n-\\frac{2\\tau}{\\beta}F_{2}(\\frac{\\beta}{\\tau},\\beta)+2\\sqrt{\\frac{n}{2}}\\tau^{2}=0\\rightarrow\\frac{2}{\\beta}F_{2}(\\frac{\\beta}{\\tau},\\beta)=2\\sqrt{\\frac{n}{2}}\\tau\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Which implies ", "page_idx": 37}, {"type": "equation", "text": "$$\nd\\int\\int p(s_{1},s_{2})\\frac{\\frac{(s_{1}+s_{2})\\beta}{2}}{1+\\frac{\\beta}{2\\tau}\\sqrt{\\frac{n}{2}}(s_{1}+s_{2})}d s_{1}d s_{2}=2\\sqrt{\\frac{n}{2}}\\tau\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Multiplying both sides by $\\frac{\\sqrt{\\frac{n}{2}}}{d\\tau}$ and introducing $\\begin{array}{r}{\\theta=\\frac{\\beta}{\\tau}}\\end{array}$ we obtain: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\int\\int p(s_{1},s_{2})\\frac{\\frac{\\theta}{2}\\sqrt{\\frac{n}{2}}(s_{1}+s_{2})}{1+\\frac{\\theta}{2}\\sqrt{\\frac{n}{2}}(s_{1}+s_{2})})d s_{1}d s_{2}=\\frac{n}{d}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "This can be rewritten as: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\int\\int p(s_{1},s_{2})(1-\\frac{2\\sqrt{2}}{2\\sqrt{2}+\\theta\\sqrt{n}(s_{1}+s_{2})})d s_{1}d s_{2}=\\frac{n}{d}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We arrive at the following equation for $\\theta$ : ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{d-n}{d}=\\int\\int p(s_{1},s_{2})\\frac{2\\sqrt{2}}{2\\sqrt{2}+\\theta\\sqrt{n}(s_{1}+s_{2})}d s_{1}d s_{2}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "The last equation above can be reformulated in terms of the Stiltjes transform as follows: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{2\\sqrt{2}}{\\theta\\sqrt{n}}S_{\\Sigma_{1}+\\Sigma_{2}}(-\\frac{2\\sqrt{2}}{\\theta\\sqrt{n}})=\\frac{d-n}{d}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "After finding $\\theta$ from the equation above, we proceed to identify the optimal value of $\\gamma$ . We will do so by taking the derivative of (36) by $\\gamma$ and setting it to 0: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{n}{2}}\\theta(1-\\frac{\\gamma}{2})-\\int\\int p(s_{1},s_{2})\\frac{\\frac{n\\theta^{2}\\gamma}{8}(1-r)}{1+\\frac{\\theta}{2}(s_{1}+s_{2})\\sqrt{\\frac{n}{2}}}d s_{1}d s_{2}-\\alpha\\sqrt{\\frac{n}{2}}\\int\\int p(s_{1},s_{2})\\frac{\\theta t_{*}(1-r)(s_{1}+s_{2})^{-}}{1+\\frac{\\theta}{2}\\sqrt{\\frac{n}{2}}(s_{1}+s_{2})}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Dividing both sides by $\\theta$ , factoring $\\sqrt{n}$ out and using (39) we obtain: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\gamma\\sqrt{n}\\left(\\frac{1}{2\\sqrt{2}}+\\frac{\\sqrt{n}\\theta(1-r)}{8}\\frac{d-n}{d}\\right)=\\sqrt{\\frac{n}{2}}-\\alpha\\sqrt{\\frac{n}{2}}\\int\\int p(s_{1},s_{2})\\frac{t_{*}(1-r)(s_{1}+s_{2})^{-1}}{1+\\frac{\\theta}{2}\\sqrt{\\frac{n}{2}}(s_{1}+s_{2})}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Hence, we can find $\\gamma$ via ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\gamma=\\left({\\frac{1}{2}}+{\\frac{{\\sqrt{2n}}\\theta(1-r)(d-n)}{8d}}\\right)^{-1}\\left(1-\\alpha\\int\\int p(s_{1},s_{2}){\\frac{t_{*}(1-r)(s_{1}+s_{2})^{-1}}{1+{\\frac{\\theta}{2}}{\\sqrt{\\frac{n}{2}}}(s_{1}+s_{2})}}\\right)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Finally, to find $\\tau$ , recall (40) ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\partial_{x}F_{1}(x,\\gamma)+\\partial_{x}F_{2}(x,\\beta)+\\sqrt{\\frac{n}{2}}\\tau^{2}-\\sqrt{\\frac{n}{2}}(\\gamma-\\frac{\\gamma^{2}}{4})=0\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Using $F_{2}$ is quadratic in its second argument we have: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\partial_{x}F_{1}(x,\\gamma)+\\tau^{2}(\\partial_{x}F_{2}(x,\\theta)+\\sqrt{\\frac{n}{2}})=\\sqrt{\\frac{n}{2}}(\\gamma-\\frac{\\gamma^{2}}{4})\n$$", "text_format": "latex", "page_idx": 38}, {"type": "equation", "text": "$$\n\\tau^{2}(\\partial_{x}F_{2}(x,\\theta)+\\sqrt{\\frac{n}{2}})=\\sqrt{\\frac{n}{2}}(\\gamma-\\frac{\\gamma^{2}}{4})-\\partial_{x}F_{1}(x,\\gamma)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "equation", "text": "$$\n\\tau^{2}=(\\partial_{x}F_{2}(x,\\theta)+\\sqrt{\\frac{n}{2}})^{-1}(\\sqrt{\\frac{n}{2}}(\\gamma-\\frac{\\gamma^{2}}{4})-\\partial_{x}F_{1}(x,\\gamma))\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We have ", "page_idx": 38}, {"type": "equation", "text": "$$\nF_{1}(x,\\gamma):=\\int\\int p(s_{1},s_{2})\\frac{d\\alpha^{2}\\phi(s_{1},s_{2})+\\frac{n x^{2}\\gamma^{2}}{16}(1-r)+2t_{*}(1-r)\\alpha\\sqrt{\\frac{n}{2}}\\frac{x\\gamma}{2}(s_{1}+s_{2})^{-1}}{1+\\frac{x}{2}(s_{1}+s_{2})\\sqrt{\\frac{n}{2}}}d s_{1}d s_{2}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Then ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\partial_{x}F_{1}(x,\\gamma)=-\\frac{d\\alpha^{2}}{2}\\sqrt{\\frac{n}{2}}\\int\\int p(s_{1},s_{2})\\frac{\\phi(s_{1},s_{2})(s_{1}+s_{2})}{(1+\\frac{x}{2}(s_{1}+s_{2})\\sqrt{\\frac{n}{2}})^{2}}d s_{1}d s_{2}}\\\\ {\\displaystyle\\qquad\\qquad+\\frac{n x\\gamma^{2}(1-r)}{8}\\int\\int p(s_{1},s_{2})\\frac{1}{1+\\frac{x}{2}(s_{1}+s_{2})\\sqrt{\\frac{n}{2}}}d s_{1}d s_{2}}\\\\ {\\displaystyle-\\frac{n x^{2}\\gamma^{2}(1-r)}{32}\\sqrt{\\frac{n}{2}}\\int\\int p(s_{1},s_{2})\\frac{(s_{1}+s_{2})}{(1+\\frac{x}{2}(s_{1}+s_{2})\\sqrt{\\frac{n}{2}})^{2}}d s_{1}d s_{2}}\\\\ {\\displaystyle\\qquad-\\alpha\\varepsilon_{\\varepsilon}(1-r)\\gamma\\sqrt{\\frac{n}{2}}\\int\\int p(s_{1},s_{2})\\frac{(s_{1}+s_{2})^{-1}}{1+\\frac{x}{2}(s_{1}+s_{2})\\sqrt{\\frac{n}{2}}}d s_{1}d s_{2}}\\\\ {\\displaystyle+\\frac{\\alpha\\varepsilon_{\\varepsilon}(1-r)x\\gamma}{2}\\frac{n}{2}\\int\\int p(s_{1},s_{2})\\frac{1}{(1+\\frac{x}{2}(s_{1}+s_{2})\\sqrt{\\frac{n}{2}})^{2}}d s_{1}d s_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Moreover ", "page_idx": 38}, {"type": "equation", "text": "$$\nF_{2}(x,\\theta)=d\\int\\int p(s_{1},s_{2})\\frac{\\frac{(s_{1}+s_{2})\\,\\theta^{2}}{4}}{1+\\frac{x}{2}\\sqrt{\\frac{n}{2}}(s_{1}+s_{2})}d s_{1}d s_{2}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Then ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\partial_{x}F_{2}(x,\\theta)=-\\frac{d\\theta^{2}}{8}\\sqrt{\\frac{n}{2}}\\int\\int\\,p(s_{1},s_{2})\\frac{(s_{1}+s_{2})^{2}}{(1+\\frac{x}{2}\\sqrt{\\frac{n}{2}}(s_{1}+s_{2}))^{2}}d s_{1}d s_{2}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Thus, all in all: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\tau^{2}=\\left(\\sqrt{\\frac{n}{2}}-\\frac{d\\theta^{2}}{8}\\sqrt{\\displaystyle\\frac{n}{2}}\\right)\\displaystyle\\int\\int p(s_{1},s_{2})\\displaystyle\\frac{(s_{1}+s_{2})^{2}}{(1+\\frac{\\theta^{2}}{\\rho}\\sqrt{\\displaystyle{2}}(s_{1}+s_{2}))^{2}}d s_{1}d s_{2}\\right)^{-1}}}\\\\ {{\\displaystyle\\sqrt{\\frac{n}{2}}(\\gamma-\\frac{\\gamma^{2}}{4})+\\frac{d\\alpha^{2}}{2}\\sqrt{\\displaystyle\\frac{n}{2}}\\int\\int p(s_{1},s_{2})\\displaystyle\\frac{\\phi(s_{1},s_{2})(s_{1}+s_{2})}{(1+\\frac{\\theta^{2}}{\\rho}(s_{1}+s_{2})\\sqrt{\\displaystyle\\frac{n}{2}})^{2}}d s_{1}d s_{2}}}\\\\ {{\\displaystyle-\\frac{n\\theta\\gamma^{2}(1-r)}{8}\\int\\int p(s_{1},s_{2})\\displaystyle\\frac{1}{1+\\frac{\\theta^{2}}{\\rho}(s_{1}+s_{2})\\sqrt{\\displaystyle\\frac{n}{2}}}d s_{1}d s_{2}}}\\\\ {{\\displaystyle+\\frac{n\\theta^{2}\\gamma^{2}(1-r)}{32}\\sqrt{\\displaystyle\\frac{n}{2}}\\int\\int p(s_{1},s_{2})\\displaystyle\\frac{(s_{1}+s_{2})}{(1+\\frac{\\theta^{2}}{\\rho}(s_{1}+s_{2})\\sqrt{\\displaystyle\\frac{n}{2}})^{2}}d s_{1}d s_{2}}}\\\\ {{\\displaystyle-\\alpha t_{\\varepsilon}(1-r)\\gamma\\sqrt{\\frac{n}{2}}\\int\\int p(s_{1},s_{2})\\displaystyle\\frac{(s_{1}+s_{2})^{-1}}{1+\\frac{\\theta^{2}}{\\rho}(s_{1}+s_{2})\\sqrt{\\displaystyle\\frac{n}{2}}}d s_{1}d s_{2}}}\\\\ {{\\displaystyle+\\frac{\\alpha t_{\\varepsilon}(1-r)\\beta\\gamma}{4}\\int\\int p(s_{1},s_{2})\\displaystyle\\frac{1}{(1+\\frac{\\theta}{\\rho}(s_{1}+s_{2})\\sqrt{\\displaystyle\\frac{n}{2}})^{2}}d s_{1}d s_{2}\\right)}}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{E r r o r=Q\\left(\\frac{1-\\frac{\\gamma}{2}}{\\sqrt{\\tau^{2}-(\\frac{\\gamma}{2})^{2}}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "H.1 Single direction case. ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "In the special case when $\\begin{array}{r}{\\Sigma_{1}=\\Sigma_{2}=\\sigma^{2}\\frac{\\mathbf{I}_{d}}{d}}\\end{array}$ we obtain: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha=\\cfrac{\\mathbf{y}^{T}\\mathbf{X}\\mathbf{w}_{0}}{\\|\\mathbf{X}\\mathbf{w}_{0}\\|^{2}}=\\cfrac{\\frac{1}{2}t_{*}\\left(1-r\\right)\\mathrm{Tr}\\left(\\left(\\boldsymbol{\\Sigma}_{1}+\\boldsymbol{\\Sigma}_{2}\\right)^{-1}\\right)}{\\frac{1}{2}\\mathrm{Tr}\\left(\\mathbf{R}_{0}\\left(\\boldsymbol{\\Sigma}_{1}+\\boldsymbol{\\Sigma}_{2}\\right)\\right)+\\left(t_{*}\\left(1-r\\right)\\mathrm{Tr}\\left(\\left(\\boldsymbol{\\Sigma}_{1}+\\boldsymbol{\\Sigma}_{2}\\right)^{-1}\\right)\\right)^{2}}=}\\\\ &{\\xrightarrow[]{t_{*}\\frac{d}{2\\sigma^{2}}}\\frac{1}{d}\\sigma^{2}\\frac{1}{d}\\sigma^{2}}\\\\ &{\\frac{1}{d}\\sigma^{2}(t_{\\eta}^{2}+t_{*}^{2}\\frac{d^{2}}{2\\sigma^{4}})+t_{*}^{2}(1-r)\\frac{d^{2}}{4\\sigma^{4}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Multiplying both sides by $\\begin{array}{r}{t^{\\ast}(1-r)\\frac{d}{2\\sigma^{2}}}\\end{array}$ we arrive at: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\Gamma:=\\alpha t^{*}(1-r)\\displaystyle\\frac{d}{2\\sigma^{2}}=\\displaystyle\\frac{1}{\\frac{\\sigma^{2}}{d(1-r)}(\\frac{t_{\\eta}^{2}}{t_{*}^{2}}\\frac{4\\sigma^{4}}{d^{2}}+2)+1}=\\displaystyle\\frac{1}{Q^{-1}(e_{a})^{-2}+1}=\\displaystyle\\frac{Q^{-1}(e_{a})^{2}}{Q^{-1}(e_{a})^{2}+1}=}}\\\\ {{\\displaystyle1-\\displaystyle\\frac{1}{Q^{-1}(e_{a})^{2}+1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We proceed to find $\\theta$ next. From the general case we had ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\frac{2\\sqrt{2}}{\\theta\\sqrt{n}}S_{\\Sigma_{1}+\\Sigma_{2}}(-\\frac{2\\sqrt{2}}{\\theta\\sqrt{n}})=\\frac{d-n}{d}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Now plugging for the distribution of $\\Sigma_{1},\\Sigma_{2}$ yields ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{1+\\theta\\sqrt{\\frac{n}{2}}\\frac{\\sigma^{2}}{d}=\\frac{d}{d-n}}\\\\ {\\theta\\sqrt{\\frac{n}{2}}\\frac{\\sigma^{2}}{d}=\\frac{n}{d-n}}\\\\ {\\theta=\\frac{d\\sqrt{2n}}{(d-n)\\sigma^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Using (41) we can simplify the last expression above: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\gamma=\\left(\\frac{1}{2}+\\frac{\\sqrt{2n}\\theta(1-r)(d-n)}{8d}\\right)^{-1}\\left(1-\\alpha\\int\\int p(s_{1},s_{2})\\frac{t_{*}(1-r)(s_{1}+s_{2})^{-1}}{1+\\frac{\\theta}{2}\\sqrt{\\frac{n}{2}}(s_{1}+s_{2})}\\right)=}\\\\ {\\displaystyle=\\left(\\frac{1}{2}+\\frac{\\sqrt{2n}\\theta(1-r)(d-n)}{8d}\\right)^{-1}\\left(1-\\frac{\\Gamma}{1+\\frac{\\theta}{2}\\sqrt{\\frac{n}{2}}(s_{1}+s_{2})}\\right)=}\\\\ {\\displaystyle\\left(\\frac{1}{2}+\\frac{\\sqrt{2n}\\frac{d\\sqrt{2n}}{(d-n)\\sigma^{2}}(1-r)(d-n)}{8d}\\right)^{-1}\\left(1-\\frac{\\Gamma}{1+\\frac{d\\sqrt{2n}}{2(d-n)\\sigma^{2}}\\sqrt{\\frac{n}{2}}\\frac{2\\sigma^{2}}{d}}\\right)=}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left(\\frac{1}{2}+\\frac{n\\frac{2}{\\sigma^{2}}(1-r)}{8}\\right)^{-1}\\left(1-\\frac{\\Gamma}{1+\\frac{n}{d-n}}\\right)=}\\\\ {\\displaystyle\\left(\\frac{1}{2}+\\frac{n(1-r)}{4\\sigma^{2}}\\right)^{-1}\\left(1-\\frac{d-n}{d}(1-\\frac{1}{Q^{-1}(e_{a})^{2}+1})\\right)=\\frac{\\frac{n}{d}+\\frac{d-n}{d}\\frac{1}{Q^{-1}(e_{a})^{2}+1}}{\\frac{1}{2}+\\frac{\\rho}{4}}\\mathrm{~where~}\\rho:=\\frac{n(1-r)}{\\sigma^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Furthermore, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\tau^{2}=\\left(\\sqrt{\\displaystyle\\frac{n}{2}}-\\frac{d\\theta^{2}}{8}\\sqrt{\\displaystyle\\frac{n}{2}}\\int\\int p(s_{1},s_{2})\\frac{(s_{1}+s_{2})^{2}}{(1+\\frac{\\theta}{2}\\sqrt{\\displaystyle\\frac{n}{2}}(s_{1}+s_{2}))^{2}}d s_{1}d s_{2}\\right)^{-1}}\\\\ {\\left[\\sqrt{\\displaystyle\\frac{n}{2}}(\\gamma-\\frac{\\gamma^{2}}{4})+\\displaystyle\\frac{d\\alpha^{2}}{2}\\sqrt{\\displaystyle\\frac{n}{2}}\\int\\int p(s_{1},s_{2})\\frac{\\phi(s_{1},s_{2})(s_{1}+s_{2})}{(1+\\frac{\\theta}{2}(s_{1}+s_{2})\\sqrt{\\displaystyle\\frac{n}{2}})^{2}}d s_{1}d s_{2}\\right.}\\\\ {\\left.-\\frac{n\\theta\\gamma^{2}(1-r)}{8}\\int\\int p(s_{1},s_{2})\\frac{1}{1+\\frac{\\theta}{2}(s_{1}+s_{2})\\sqrt{\\displaystyle\\frac{n}{2}}}d s_{1}d s_{2}\\right.}\\\\ {\\left.+\\frac{n\\theta^{2}\\gamma^{2}(1-r)}{32}\\sqrt{\\displaystyle\\frac{n}{2}}\\int\\int p(s_{1},s_{2})\\frac{(s_{1}+s_{2})}{(1+\\frac{\\theta}{2}(s_{1}+s_{2})\\sqrt{\\displaystyle\\frac{n}{2}})^{2}}d s_{1}d s_{2}\\right.}\\\\ {\\left.-\\alpha t_{1}(1-r)\\gamma\\sqrt{\\displaystyle\\frac{n}{2}}\\int\\int p(s_{1},s_{2})\\frac{(s_{1}+s_{2})^{-1}}{1+\\frac{\\theta}{2}(s_{1}+s_{2})\\sqrt{\\displaystyle\\frac{n}{2}}}d s_{1}d s_{2}\\right.}\\\\ {\\left.+\\frac{\\alpha t_{1}(1-r)b\\gamma}{4}\\int p(s_{1},s_{2})\\frac{1}{(1+\\frac{\\theta}{2}(s_{1}+s_{2})\\sqrt{\\displaystyle\\frac{n}{2}})^{2}}d s_{1}d s_{2}\\right]=}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Simplifying furthermore yields ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle r^{2}=\\left(1-\\frac{d}{d}\\int\\mathcal{P}(\\delta_{1},s_{2})\\frac{\\overline{{q}}(s_{3})}{\\displaystyle\\frac{d t-s_{1}}{d t}}d s_{1}d s_{2}\\right)^{-1}}}\\\\ {{\\displaystyle\\left[(\\gamma-\\frac{\\gamma^{2}}{4})^{4}+\\int\\int\\frac{\\alpha^{2}p(s_{1},s_{2})(1-r)^{2}(t_{q}^{2}+t_{2}^{2}\\overline{{q}}^{2})}{d t(1-r)(\\underline{{q}}_{{1}}-1)}d s_{1}d s_{2}\\right.}}\\\\ {{\\displaystyle-\\frac{n\\frac{\\gamma^{2}d}{d t-s_{1}d}\\gamma^{2}(1-r)}{8}\\int\\int p(s_{1},s_{2})\\frac{1}{\\displaystyle\\frac{d}{d t}d s_{1}d s_{2}}}}\\\\ {{\\displaystyle\\left.+\\frac{n\\frac{2n(q)}{d t-s_{1}d}\\gamma^{2}(1-r)}{3}\\int\\int p(s_{1},s_{2})\\frac{\\overline{{q}}^{2}}{d t-d}\\right]d s_{1}d s_{2}}}\\\\ {{\\displaystyle-a\\varepsilon_{1}(1-r)\\sqrt{\\int p\\left(s_{1},s_{2}\\right)\\frac{\\overline{{q}}^{2}}{d t-s_{1}d}}d s_{1}d s_{2}}}\\\\ {{\\displaystyle+\\frac{a\\varepsilon_{1}(1-r)\\frac{2\\sqrt{d}}{d t-s_{1}d}\\gamma^{3}}{4(d-s_{1}d)^{2}}\\int\\int p(s_{1},s_{2})\\frac{1}{\\displaystyle\\left(\\frac{d}{d t-s_{1}}\\right)^{2}}d s_{1}d s_{2}\\right]=}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\frac{d}{d-n}\\Bigg[(\\gamma-\\frac{\\gamma^{2}}{4})+\\frac{\\alpha^{2}(1-r)^{2}t_{||}^{2}+2\\Gamma^{2}}{\\frac{d}{n}\\rho(\\frac{d}{d-n})^{2}}-\\frac{\\rho\\gamma^{2}}{4}+\\frac{n\\rho\\gamma^{2}}{8d}-\\frac{\\Gamma\\gamma(d-n)}{d}+\\frac{\\Gamma\\gamma n(d-n)}{d^{2}}\\Bigg]=}\\\\ &{}&{\\qquad=\\frac{d}{d-n}\\Bigg[(\\gamma-\\frac{\\gamma^{2}}{4})+\\frac{\\alpha^{2}(1-r)^{2}t_{||}^{2}+2\\Gamma^{2}}{\\frac{d}{n}\\rho(\\frac{d}{d-n})^{2}}-\\frac{\\rho\\gamma^{2}}{4}+\\frac{n\\rho\\gamma^{2}}{8d}-\\frac{\\Gamma\\gamma(d-n)^{2}}{d^{2}}\\Bigg]=}\\\\ &{}&{\\qquad=\\frac{d}{d-n}\\Bigg[(\\gamma-\\frac{\\gamma^{2}}{4})+\\frac{\\frac{d\\rho}{n Q^{-1}(e_{a})^{2}}\\Gamma^{2}}{\\frac{d}{n}\\rho(\\frac{d}{d-n})^{2}}-\\frac{\\rho\\gamma^{2}}{4}+\\frac{n\\rho\\gamma^{2}}{8d}-\\frac{\\Gamma\\gamma(d-n)^{2}}{d^{2}}\\Bigg]=}\\\\ &{}&{\\frac{d}{d-n}\\Bigg[(\\gamma-\\frac{\\gamma^{2}}{4})+\\frac{\\Gamma^{2}(d-n)^{2}}{Q^{-1}(e_{a})^{2}d^{2}}-\\frac{\\rho\\gamma^{2}}{4}+\\frac{n\\rho\\gamma^{2}}{8d}-\\frac{\\Gamma\\gamma(d-n)^{2}}{d^{2}}\\Bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Recall that \u0393 = 1 \u2212 $\\begin{array}{r}{\\Gamma=1-\\frac{1}{Q^{-1}(e_{a})^{2}+1}}\\end{array}$ \ud835\udc44\u22121(\ud835\udc52\ud835\udc4e)2+1 and \ud835\udc44\u22121(\u0393\ud835\udc52\ud835\udc4e)2 = 1 \u2212\u0393. Hence, we obtain: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\tau^{2}=\\frac{d}{d-n}\\bigg[(\\gamma-\\frac{\\gamma^{2}}{4})+\\frac{\\Gamma(1-\\Gamma)(d-n)^{2}}{d^{2}}-\\frac{\\rho\\gamma^{2}}{4}+\\frac{n\\rho\\gamma^{2}}{8d}-\\frac{\\Gamma\\gamma(d-n)^{2}}{d^{2}}\\bigg]\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Recall also that $\\begin{array}{r}{\\gamma=\\frac{1-\\frac{d-n}{d}\\Gamma}{\\frac{1}{2}+\\frac{\\rho}{4}}}\\end{array}$ Thus, $\\begin{array}{r}{\\Gamma=\\frac{d}{d-n}(1-(\\frac{1}{2}+\\frac{\\rho}{4})\\gamma)}\\end{array}$ ", "page_idx": 41}, {"type": "text", "text": "Therefore we derive: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\tau^{2}=\\displaystyle\\frac{d}{d-n}\\Bigg[(\\gamma-\\frac{\\gamma^{2}}{4})+\\frac{(1-(\\frac{1}{2}+\\frac{\\rho}{4})\\gamma)(1-\\frac{d}{d-n}(1-(\\frac{1}{2}+\\frac{\\rho}{4})\\gamma))(d-n)}{d}}}\\\\ {{-\\displaystyle\\frac{\\rho\\gamma^{2}}{4}+\\frac{n\\rho\\gamma^{2}}{8d}-\\frac{(1-(\\frac{1}{2}+\\frac{\\rho}{4})\\gamma)\\gamma(d-n)}{d}\\Bigg]=}}\\\\ {{\\displaystyle\\frac{d}{d-n}\\Bigg[(\\gamma-\\frac{\\gamma^{2}}{4})+\\frac{(1-(\\frac{1}{2}+\\frac{\\rho}{4})\\gamma)(\\frac{d}{d-n}(\\frac{1}{2}+\\frac{\\rho}{4})\\gamma-\\frac{n}{d-n})(d-n)}{d}}}\\\\ {{-\\displaystyle\\frac{\\rho\\gamma^{2}}{4}+\\frac{n\\rho\\gamma^{2}}{8d}-\\frac{(1-(\\frac{1}{2}+\\frac{\\rho}{4})\\gamma)\\gamma(d-n)}{d}\\Bigg]=}}\\\\ {{\\displaystyle\\frac{d}{d-n}\\Bigg[\\gamma\\left(1+\\frac{n+d}{d}(\\frac{1}{2}+\\frac{\\rho}{4})\\right)-\\gamma^{2}\\left(\\frac{1}{4}+(\\frac{1}{2}+\\frac{\\rho}{4})^{2}+\\frac{\\rho}{4}-\\frac{n\\rho}{8d}-\\frac{d-n}{2}(\\frac{1}{2}+\\frac{\\rho}{4})\\right)-\\frac{n}{d}\\Bigg]=}}\\\\ {{\\displaystyle\\frac{d}{d-n}\\Bigg[-\\frac{\\rho}{4}+\\frac{n}{2d})(1+\\frac{\\rho}{4})\\gamma^{2}+\\Bigg(1+\\frac{n+d}{d}(\\frac{1}{2}+\\frac{\\rho}{4})\\Bigg)\\gamma-\\frac{n}{d}\\Bigg]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: The main results are stated in Sections 3, 4 along with numerical evidence in Section 5. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 42}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Justification: The limitations of our approach and assumptions are explained in Section 6. Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 42}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: The theoretical results, assumptions, and definitions are provided in sections 2, 3, 4. Moreover, the complete proof of arguments in the main body is presented in the Appendices A, E, G, H. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 43}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: All the necessary details are given in Section 5. Moreover, the code is attached to the supplementary materials. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 43}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: The code is attached to the supplementary material. Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 44}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: All the necessary details are given in Section 5. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 44}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 44}, {"type": "text", "text": "Answer: [No] ", "page_idx": 44}, {"type": "text", "text": "Justification: As the main focus of the paper is the theoretical contribution, the experiments are conducted using specific data distributions whose statistics are exactly known. $(\\chi^{2}$ , Bernoulli, Gaussian) ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 44}, {"type": "text", "text": "", "page_idx": 45}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: All the necessary details are given in Section 5. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 45}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: The research conducted in this paper conforms with Neurips Code of Ethics. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 45}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 45}, {"type": "text", "text": "Justification: Since this paper is concerned with the mathematical foundations of AI. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 45}, {"type": "text", "text": "", "page_idx": 46}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: Since this paper is concerned with the mathematical foundations of AI. Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 46}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: All the necessary details are given in Section 5. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 46}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 47}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: Since this paper is concerned with the mathematical foundations of AI. Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 47}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: Since this paper is concerned with the mathematical foundations of AI. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 47}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: Since this paper is concerned with the mathematical foundations of AI. Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 47}]