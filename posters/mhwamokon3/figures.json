[{"figure_path": "MhWaMOkoN3/figures/figures_9_1.jpg", "caption": "Figure 1: Generalization error for the bilevel distribution", "description": "The figure shows the generalization error of the weight obtained through running SGD with respect to k = 2 for the bilevel distribution. It compares the theoretical prediction (blue line) and lower bound (red line) with the empirical results from normal (black squares), Bernoulli (green circles), and chi-squared (red triangles) distributions. The plots are for different values of sigma (noise level).", "section": "5.1 Regression"}, {"figure_path": "MhWaMOkoN3/figures/figures_9_2.jpg", "caption": "Figure 1: Generalization error for the bilevel distribution", "description": "This figure displays the generalization error for different input distributions (Normal, Bernoulli, Chi-squared) for three different noise levels (\u03c3 = 0.01, \u03c3 = 0.15, \u03c3 = 2).  The x-axis represents the ratio of the number of parameters to samples (\u03ba). The blue line shows the theoretical prediction from Theorem 2, while the red line represents a lower bound derived in the paper. The markers denote the actual empirical generalization error from simulations.  The figure aims to illustrate the universality result of Theorem 1 for regression, showing how the generalization error is similar across different distributions.", "section": "5.1 Regression"}, {"figure_path": "MhWaMOkoN3/figures/figures_9_3.jpg", "caption": "Figure 3: Classification error", "description": "This figure shows the results of classification experiments using three different data distributions (Normal, Bernoulli, and Chi-squared) and varying the ratio of data points to parameters (\u03ba). The plots compare the classification errors of pre-trained and fine-tuned models.  Panel (a) fixes \u03c1=1 and varies \u03ba. Panel (b) fixes \u03ba=2 and varies \u03c1. Panel (c) shows how the classification error depends on p (ratio between dimensions of data and feature space) and different distributions.  The figure demonstrates the universality property shown in the paper: similar results are obtained for various data distributions.", "section": "5 Numerical experiments"}]