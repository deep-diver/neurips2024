[{"figure_path": "lD7ziaMHbf/tables/tables_2_1.jpg", "caption": "Table 1: Comparison between ViT and DeiT. All data is measured on Oneplus 11 (Snapdragon 8 Gen 2 SoC). Layout Transformation indicates the time spent on transforming the tensor's layout, such as Transpose and Reshape. Computation indicates the time spent on pure tensor computation.", "description": "This table compares the performance of ViT and DeiT on a Oneplus 11 phone. It breaks down the execution time into layout transformation (e.g., transpose and reshape) and pure computation, and also shows the intermediate results in megabytes.  The comparison highlights the significant overhead of layout transformations, especially in DeiT, which is crucial for understanding the performance differences on mobile devices.", "section": "2 Background and Motivation"}, {"figure_path": "lD7ziaMHbf/tables/tables_8_1.jpg", "caption": "Table 2: Comparisons of results on the ImageNet dataset. All reported model results are based on pre-trained weights from ImageNet-21K. The accuracy, parameters, and MACs of ECP-ViT are under the best core ratio.", "description": "This table compares the performance of various vision transformer models on the ImageNet dataset.  The models are compared based on their top-1 accuracy, the number of parameters, and the number of multiply-accumulate (MAC) operations.  The ECP-ViT model is highlighted, showcasing its performance relative to other state-of-the-art models. Note that all models use pre-trained weights from ImageNet-21K, and the ECP-ViT results are for its optimal core ratio.", "section": "4.2 Accuracy Comparison"}, {"figure_path": "lD7ziaMHbf/tables/tables_8_2.jpg", "caption": "Table 3: Performance evaluation of ECP-ViT on ImageNet under different core ratios. We fine-tune the ECP-ViT using the pre-trained weights on ImageNet-21K. Top-1 accuracy is reported and shown in percentage. T, S, and B represents ECP-ViT-T, ECP-ViT-S, ECP-ViT-B, respectively.", "description": "This table shows the Top-1 accuracy of ECP-ViT models (Tiny, Small, and Base) on the ImageNet dataset under various core ratios (from 0.1 to 1.0, where 1.0 represents the baseline ViT without pruning).  The results illustrate the impact of the core-periphery guided self-attention mechanism on model accuracy as the core ratio (percentage of core nodes) changes.", "section": "4.2 Accuracy Comparison"}, {"figure_path": "lD7ziaMHbf/tables/tables_8_3.jpg", "caption": "Table 4: Comparison between ECP-ViT with other ViT variants on TinyImageNet. Top-1 Accuracy is shown in percentage. The best result of ECP-ViT under different core ratios is selected.", "description": "This table compares the Top-1 accuracy of ECP-ViT against other vision transformer (ViT) variants on the TinyImageNet dataset.  It highlights ECP-ViT's superior performance in terms of accuracy, showcasing the effectiveness of its core-periphery guided self-attention mechanism.  The best result for ECP-ViT is reported based on different core ratios.", "section": "4.2 Accuracy Comparison"}, {"figure_path": "lD7ziaMHbf/tables/tables_8_4.jpg", "caption": "Table 5: Comparisons between ECP-ViT and vanilla ViT on STL10 and CIFAR100. The performance evaluation is under the best core ratio. Top-1 accuracy is reported and shown in percentage.", "description": "This table compares the Top-1 accuracy of vanilla ViT-S/16 and ECP-ViT-S/16 models on STL-10 and CIFAR-100 datasets.  The ECP-ViT model shows improvement in accuracy over the vanilla ViT model, especially on STL-10 dataset.", "section": "4.3 End-to-end Latency and Memory Comparison"}, {"figure_path": "lD7ziaMHbf/tables/tables_9_1.jpg", "caption": "Table 6: Comparison of Peak Memory, Latency, and Miss Rates for ViT-Base and ECP-ViT.", "description": "This table compares the peak memory usage (in MB), latency (in ms), and cache miss rates (L1, L2, and L3) for both the original ViT-Base model and the proposed ECP-ViT model.  It showcases the memory efficiency and speed improvements achieved by ECP-ViT compared to the baseline ViT-Base model.", "section": "4.3 End-to-end Latency and Memory Comparison"}, {"figure_path": "lD7ziaMHbf/tables/tables_9_2.jpg", "caption": "Table 7: Latency comparison of 4 end-to-end frameworks on vanilla ViTs and CP-enabled ViTs using the GPU on Oneplus 11. We use CP level of 80% for all 3 variants (base, small and tiny). '-' means the models is not supported on the framework.", "description": "This table compares the latency (in milliseconds) of four different deep learning frameworks (TNN, TVM, MNN, and the authors' proposed framework) when running vanilla Vision Transformers (ViTs) and core-periphery guided ViTs (ECP-ViTs) on the GPU of a Oneplus 11 phone.  The comparison highlights the speedup achieved by the authors' framework, particularly for ECP-ViTs which demonstrate significant performance gains.", "section": "4.3 End-to-end Latency and Memory Comparison"}]