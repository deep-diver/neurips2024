[{"figure_path": "s4Wx2qXhv9/figures/figures_4_1.jpg", "caption": "Figure 1: left: Comparison of coverages of confidence intervals for the mean estimation of B(100, p) when a := 0.001. Note that for p > \u221aa ~ 0.93, the coverage is 1. right: Comparison of l2-robustness curves with the standard (dashed) or the randomized (solid) Clopper-Pearson bounds on a CIFAR-10 dataset under the standard setting. The experimental details are in Appendix C.", "description": "The left plot compares the actual coverage of the Clopper-Pearson and Randomized Clopper-Pearson confidence intervals for estimating the mean of a binomial distribution B(100,p) with a confidence level of 1-\u03b1=0.999.  It shows that the randomized version has better coverage properties, particularly for larger values of p. The right plot shows the robustness curves for a CIFAR-10 dataset obtained using the standard and the randomized Clopper-Pearson methods.  The randomized method yields stronger certificates (higher robustness).", "section": "Confidence Intervals"}, {"figure_path": "s4Wx2qXhv9/figures/figures_6_1.jpg", "caption": "Figure 1: left: Comparison of coverages of confidence intervals for the mean estimation of B(100, p) when a := 0.001. Note that for p > \u221aa ~ 0.93, the coverage is 1. right: Comparison of l2-robustness curves with the standard (dashed) or the randomized (solid) Clopper-Pearson bounds on a CIFAR-10 dataset under the standard setting. The experimental details are in Appendix C.", "description": "The left plot compares the coverage of standard and randomized Clopper-Pearson confidence intervals for estimating the mean of a binomial distribution B(100,p) with a confidence level of 1-0.001.  It shows that the standard Clopper-Pearson interval is conservative, especially for larger values of p, while the randomized version achieves the desired coverage. The right plot illustrates the effect of using these different confidence intervals in the context of randomized smoothing for certifying the robustness of a classifier on the CIFAR-10 dataset.  The randomized Clopper-Pearson interval leads to a more accurate and less conservative estimate of the robustness radius, overcoming the sharp drop usually observed at the end of the robustness curve in standard methods.", "section": "Confidence Intervals"}, {"figure_path": "s4Wx2qXhv9/figures/figures_7_1.jpg", "caption": "Figure 2: left: Comparison of widths of confidence sequences for the mean of Bernoulli B(0.1) with a = 0.001. The width is on top and the actual confidence sequence on the bottom. In the notation of Algorithms 1 and 2, the sequence of U \u2013 L is in the top figure, while both sequences U and L are in the bottom figure. Note the log-scale for t (and width on top). right : Instantiation of Task 3.1. The goal is to decide if p = 0.91 (vertical magenta line) or not with a = 0.001. On top are the numbers of samples requested for the individual methods averaged over 1000 trials for 51 equally spaced values of p \u2208 [0, 1]; on the bottom is the relative suboptimality of the individual methods; i.e., how many times more samples did they request compared to the ideal method. Note log scales on the y-axis. methods: UBnd-CS and Betting-CS are from Algorithm 1 and 2 respectively. Adaptive is from Horv\u00e1th et al. (2022). The ideal is the unattainable lower-bound for the two tasks. On the LHS, it is a confidence interval on level 1 \u2212 a computed independently at every time step. On the RHS, it is SPRT knowing both p, q which is optimal due to Wald (1947).", "description": "This figure compares the performance of different confidence sequence methods for estimating the mean of a Bernoulli distribution. The left panel shows the width of the confidence sequences over time, demonstrating the efficiency of the proposed methods. The right panel illustrates the number of samples required by each method to make a decision in a sequential decision-making task, highlighting the optimality of the proposed methods compared to existing techniques.", "section": "Experiments"}, {"figure_path": "s4Wx2qXhv9/figures/figures_11_1.jpg", "caption": "Figure 3: Actual coverages for (randomized) Clopper-Pearson confidence intervals for B(2, p).", "description": "This figure compares the actual coverage of the standard Clopper-Pearson confidence intervals and the randomized version for a binomial distribution with parameters n=2 and varying p. The x-axis represents the probability of success p, while the y-axis represents the actual coverage, which is the probability that the confidence interval contains the true value of p.  The dashed line represents the desired coverage level of 1-\u03b1 = 0.95. The plot shows that the standard Clopper-Pearson intervals are conservative, meaning they overcover; their actual coverage is always greater than or equal to the desired coverage level.  In contrast, the randomized Clopper-Pearson intervals achieve the desired coverage level, making them more efficient.", "section": "2.1 Confidence Intervals"}, {"figure_path": "s4Wx2qXhv9/figures/figures_12_1.jpg", "caption": "Figure 4: Comparison of the robustness curves for binary and multiclass certification. In the binary case, all the failure budget \u03b1 = 0.001 was spent on controlling the top-1 class probability. In the multiclass setting, we spend \u03bb fraction of the budget in bounding pA and the remaining 1 \u2013 \u03bb part on bounding pB. Note that this has no significant effect. The average certified radius for binary certification is 0.50, while for the multiclass it is 0.61. The experimental details are in Appendix C.", "description": "This figure compares the robustness curves obtained using binary and multiclass certification methods. In the binary case, the entire error budget is allocated to controlling the probability of the top class.  In contrast, the multiclass method divides the error budget between controlling the top class probability and the second-highest class probability. The results show that multiclass certification achieves a higher average certified radius than binary certification. This indicates that the multiclass approach may be more effective in certifying the robustness of classifiers.", "section": "Binary or multiclass certification"}, {"figure_path": "s4Wx2qXhv9/figures/figures_13_1.jpg", "caption": "Figure 5: Samples needed for the adaptive estimation task as in Figure 2 for different hyperparameters. \u03b2 is the factor by which we enlarge the sample size before computing new confidence interval, \u03b3 is the scaling of \u03b1 as described in the main text. I.e., k-th estimation will have \u03b1k = c\u03b3k where c is the normalization constant such that \u03a3k=1 \u03b1k = \u03b1.", "description": "This figure compares the number of samples required by different confidence sequence algorithms for the sequential decision-making task. The x-axis represents the true probability (p), and the y-axis shows the number of samples needed. Different lines represent different algorithms, parameterized by \u03b3 (controls the decay rate of the failure probability) and \u03b2 (controls how often the confidence interval is recomputed). The red line ('used') indicates the actual number of samples used by the algorithm, showing that the adaptive estimation procedures are more efficient compared to non-adaptive ones.", "section": "Experiments"}]