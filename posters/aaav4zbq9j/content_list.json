[{"type": "text", "text": "Navigating Chemical Space with Latent Flows ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Guanghao Wei\\*\u2021 Yining Huang\\* Cornell University Harvard University gw338@cornell.edu yininghuang@hms.harvard.edu ", "page_idx": 0}, {"type": "text", "text": "Chenru Duan Yue Song\u2020 Yuanqi Du\u2020 Deep Principle, Inc. California Institute of Technology Cornell University duanchenru@gmail.com yuesong@caltech.edu yd392@cornell.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent progress of deep generative models in the vision and language domain has stimulated significant interest in more structured data generation such as molecules. However, beyond generating new random molecules, efficient exploration and a comprehensive understanding of the vast chemical space are of great importance to molecular science and applications in drug design and materials discovery. In this paper, we propose a new framework, ChemFlow, to traverse chemical space through navigating the latent space learned by molecule generative models through flows. We introduce a dynamical system perspective that formulates the problem as learning a vector field that transports the mass of the molecular distribution to the region with desired molecular properties or structure diversity. Under this framework, we unify previous approaches on molecule latent space traversal and optimization and propose alternative competing methods incorporating different physical priors. We validate the efficacy of ChemFlow on molecule manipulation and single- and multi-objective molecule optimization tasks under both supervised and unsupervised molecular discovery settings. Codes and demos are publicly available on GitHub at https://github.com/garywei944/ChemFlow. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Designing new functional molecules has been a long-standing challenge in molecular discovery which concerns a wide range of applications in drug design and materials discovery [48, 57]. With the increasing interest in applying deep learning models in scientific problems [60, 70], molecular design has attracted considerable attention given its massively available data and accessible evaluations. Among the developed methods, two paradigms emerge: one paradigm searches for new molecules based on combinatorial optimization approaches respecting the discrete nature of molecules; the other paradigm builds upon the success of deep generative models in approximating the molecular distribution with a given dataset and then generating new molecules from the learned models [8]. Both of the approaches have demonstrated promising results in small molecule, protein, and materials design [61, 26, 41, 69]. Despite the promise, the chemical space is tremendously large with the number of drug-like small molecule compounds estimated to be from $10^{23}$ to $10^{\\bar{6}0}$ [3, 39]. This necessitates either more efficient searching methods or better understanding about the structure of the chemical space. Following the progress made in studying the latent structure of deep generative models (e.g. generative adversarial networks (GANs) [19], variational autoencoders (VAEs) [35], and denoising diffusion models [23]) in computer vision [28, 5, 22, 38], decent efforts have recently been made in understanding the learned latent space of molecule generative models. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Initially, disentangled representation learning becomes a popular paradigm to enforce a structured and interpretable representation [9]. Specifically, each latent dimension is expected to learn a disentangled factor of variation, and tweak the latent vector along the dimension could lead to generating new samples with changes only in one molecular property. However, even if imposing such constraints in the training of molecule generative models, the models still struggle to learn meaningful disentangled factors in the early attempts [10]. In addition to constraining the model training procedure, exploring the structure of pre-trained molecule generative models is more efficient. The main approach developed is to utilize optimization approaches to discover the region in the latent space with the desired molecular property. It often trains a proxy function to map from the latent vector to the property, providing access to gradients for gradient-based optimization [40, 20, 13]. The third line of work also builds upon pre-trained models as well. It leverages one interesting finding such that the learned latent space of molecule generative models is linearly separable [18], which is also widely studied and used as a priori in computer vision [51, 50]. ChemSpace [11] develops a highly efficient approach to use linear classifiers to identify the separation boundary and considers the normal direction to the boundary as the direction of control. Nevertheless, the linear separability assumption may be too strong. It is worth noting that the first line of work does not require labels and can be trained in an unsupervised manner (referred to as unsupervised discovery), while both the second and third lines of work require access to labels to train/identify a guidance model/direction (referred to as supervised discovery). ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose a new framework, ChemFlow, based on flows in a dynamical system to efficiently explore the latent structure of molecule generative models. Specifically, we unify previous approaches (gradient-based optimization, linear latent traversal, and disentangled traversal) under the realm of flows that transforms data density along time via a vector field. In contrast to previous linear models, our framework is flexible to learn nonlinear transformations inspired by partial differential equations (PDEs) governing real-world physical systems such as heat and wave equations. We then analyze how different dynamics may bring special properties to solve different tasks. Our framework can also generalize both supervised and unsupervised settings under the same umbrella. Particularly in the under-studied unsupervised setting, we demonstrate a structure diversity potential can be incorporated to find trajectories that maximize the structure change of the molecules (which in turn leads to property change). We conduct extensive experiments with physicochemical, drug-related properties, and protein-ligand binding affinities on both molecule manipulation and (single- and multi-objective) molecule optimization experiments. The experiment results demonstrate the generality of the proposed framework and the effectiveness of alternative methods under this framework to achieve better or comparable results with existing approaches. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Navigating Latent Space of Molecules ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The latent space $\\mathcal{Z}$ of molecule generative models is often learned through an encoder function $f_{\\theta}(\\cdot)$ and a decoder function $g_{\\psi}(\\cdot)$ such that the encoder maps the input molecular structures $\\pmb{x}\\in\\mathcal{X}$ into an (often) low-dimensional and continuous space (i.e. latent space) while the decoder maps the latent vectors $z\\in{\\mathcal{Z}}$ back to molecular structures $x^{\\prime}$ . Note that this encoder-decoder architecture is general and can be realized by popular generative models such as VAEs, flow-based models, GANs, and diffusion models [31, 43, 6, 58, 66]. For simplicity, we focus on VAE-based methods in this paper. To traverse the learned latent space of molecule generative models, two approaches have been proposed: gradient-based optimization and latent traversal. ", "page_idx": 1}, {"type": "text", "text": "The gradient-based optimization methods first learn a proxy function $h(\\cdot)$ parameterized by a neural network that provides the direction to traverse [67]. This can be formulated as a gradient flow following the direction of steepest descent of the potential energy function $h(\\cdot)$ and discretized, as follows: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{d}z_{t}=-\\nabla_{z}h(z_{t})\\mathbf{d}t}\\\\ &{~~z_{t}=z_{t-1}-\\nabla_{z}h(z_{t-1})\\mathbf{d}t}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where we take a dynamic system perspective on the evolution of latent samples. The latent traversal approaches leverage the observation of linear separability in the learned latent space of molecule gener", "page_idx": 1}, {"type": "image", "img_path": "aAaV4ZbQ9j/tmp/2f24d48eec4dccadcfa282cd9fd96bc1f9b92cab565d0bc4fe1db49ccb27dbce.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: ChemFlow framework: (1) a pre-trained encoder $f_{\\theta}(\\cdot)$ and decoder $g_{\\psi}(\\cdot)$ that maps between molecules $\\textbf{\\em x}$ and latent vectors $_{\\textit{z}}$ , (2) we use a property predictor $h_{\\eta}(\\cdot)$ (green box) or a \u201cJacobian control\u201d (yellow box) as the guidance to learn a vector field $\\nabla_{z}\\phi^{k}(t,z_{t})$ that maximizes the change in certain molecular properties (e.g. plogP, QED) or molecular structures, (3) during the training process, we add additional dynamical regularization on the flow. The learned flows move the latent samples to change the structures and properties of the molecules smoothly. (Better seen in color). The flow chart illustrates a case where a molecule is manipulated into a drug like caffeine. ", "page_idx": 2}, {"type": "text", "text": "ative models [18]. Since the direction is assumed to be linear, it can be found easily. ChemSpace [11] learns a linear classifier that defines the separation boundary of the molecular properties. Then the normal direction of the boundary provides a linear direction $\\pmb{n}\\in\\mathcal{Z}$ for traversing the latent space: ", "page_idx": 2}, {"type": "equation", "text": "$$\nz_{t}=z_{0}+n t\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We notice that the above gradient flow and linear traversal can be analyzed and designed from a dynamical system perspective: linear traversal can be considered as a special case of wave functions, i.e., we have $\\partial^{2}{\\bf z}_{t}/\\dot{\\partial}^{2}{\\bf z}_{t-1}=\\partial^{2}{\\bf z}_{t}/\\partial^{2}t=0$ satisfied by wave functions. This connection inspires us to consider designing more dynamical traversal approaches in the latent space. ", "page_idx": 2}, {"type": "text", "text": "2.2 Wasserstein Gradient Flows ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Gradient flows define the curve $\\pmb{x}(t)\\in\\mathbb{R}^{n}$ that evolves in the negative gradient direction of a function $\\mathcal{F}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ . The time evolution of the gradient flow is given by the ODE $\\mathbf{\\boldsymbol{x}}^{\\prime}(t)=-\\nabla\\mathcal{F}(\\mathbf{\\boldsymbol{x}}(t))$ . Wasserstein gradient flows describe a special type of gradient flow where $\\mathcal{F}$ is set to be the Wasserstein distance. For example, as introduced in Benamou and Brenier [2], the commonly used $L^{2}$ Wasserstein metric induces a dynamic formulation of optimal transport: ", "page_idx": 2}, {"type": "equation", "text": "$$\nW_{2}(\\mu,\\nu)^{2}=\\operatorname*{min}_{v,\\rho}\\Big\\{\\int\\int\\frac{1}{2}\\rho(t,x)|v(t,x)|^{2}\\,d t\\,d x:\\partial_{t}\\rho(t,x)=-\\nabla\\cdot(v(t,x)\\rho(t,x))\\Big\\}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mu,\\nu$ are two probability measures at the source and target distributions, respectively. Interestingly, if we take the gradient of a potential energy $\\nabla\\phi$ as the velocity field applied to a distribution, the time evolution of $\\nabla\\phi$ can be seen to minimize the Wasserstein distance and thus follow optimal transport. In Appendix A, we give detailed derivations of how the vector fields minimize the $L^{2}$ Wasserstein distance and discuss alternative PDEs of the density evolution recovered by Wasserstein gradient flow (e.g. Wasserstein gradient flow over the entropy functional recovers heat equation) following the seminal JKO scheme [34]. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We present ChemFlow as a unified framework for latent traversals in chemical latent space as latent flows. We parameterize a set of scalar-valued energy functions $\\phi^{k}=\\mathrm{MLP}_{\\theta^{k}}(t,z)\\in\\dot{\\mathbb{R}}$ and use the ", "page_idx": 2}, {"type": "text", "text": "learned flow $\\nabla_{z}\\phi^{k}$ to traverse the latent samples. The traversal process can be described by the following equation in a Lagrangian way (particle trajectory): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\boldsymbol{z}_{t}=\\boldsymbol{z}_{t-1}+\\nabla_{z}\\phi^{k}(t-1,\\boldsymbol{z}_{t-1})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Alternatively, as an Eulerian approach, we can write the time evolution of the density through a pushforward map: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\rho_{t}=[\\psi_{t}]_{*}\\rho_{t-1}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\psi_{t}$ defines the time-dependent flow that transforms the densities of latent samples through a probability path. The pushforward measure $[\\psi_{t}]_{*}$ induces a change of variable formula for densities [47]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n[\\psi_{t}]_{\\ast}\\rho_{t-1}(z)=\\rho_{t-1}(\\psi_{t}^{-1}(z))|\\operatorname*{det}\\Big[\\frac{\\partial\\psi_{t}^{-1}(z)}{\\partial z}\\Big]|\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In the following, we will introduce how $\\nabla_{z}\\phi^{k}$ is matched to some pre-defined velocities for generating different flows. ", "page_idx": 3}, {"type": "text", "text": "3.1 Learning Different Latent Flows ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given a pre-trained molecule generative model $g_{\\psi}:\\mathcal{Z}\\to\\mathcal{X}$ with prior distribution $p(z)$ , we would like to model $K$ different semantically disentangled latent trajectories that correspond to different properties of the molecules, numbered by superscript $k$ . ", "page_idx": 3}, {"type": "text", "text": "Hamilton-Jacobi Flows. One desired property for the latent traversal comes from optimal transport theory such that the transport cost is minimized (i.e. shortest path). This property can be enforced by solving Eq. (3) by Karush\u2013Kuhn\u2013Tucker (KKT) conditions, which will give the optimal solution \u2014 the Hamilton-Jacobi Equation (HJE): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial t}\\phi^{k}(t,z)+\\frac{1}{2}||\\nabla_{z}\\phi^{k}(t,z)||^{2}=0\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the velocity field is defined as the flow $\\nabla\\phi^{k}$ . The HJE can also be interpreted as mass transportation in fluid dynamics, i.e., under the velocity field $\\nabla\\phi^{k}$ , the fluid will evolve to the target distribution with an optimal transportation cost. ", "page_idx": 3}, {"type": "text", "text": "We achieve the HJE constraint by matching our flow fields and define the boundary condition as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{r}=\\frac{1}{T}\\sum_{t=0}^{T-1}\\big(\\frac{\\partial}{\\partial t}\\phi^{k}(t,z)+\\frac{1}{2}||\\nabla_{z}\\phi^{k}(t,z)||^{2}\\big)^{2},\\;\\mathcal{L}_{\\phi}=\\sum_{k=0}^{K-1}||\\nabla_{z}\\phi^{k}(0,z_{0})||_{2}^{2}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $T$ represents the total number of traversal steps, ${\\mathcal{L}}_{r}$ restricts the energy to obey our physical constraints, and $\\mathcal{L}_{\\phi}$ restricts $\\phi(t,z_{t})$ to match the initial condition. Our latent traversal can be thus regarded as dynamic optimal transport between distributions of molecules with different properties. ", "page_idx": 3}, {"type": "text", "text": "Wave Flows. Alternatively, we can pivot the optimal transport property to enforce additional physical and dynamical priors. For example, if we specify the flow to follow wave-like dynamics, we can use the second-order wave equation: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}}{\\partial t^{2}}\\phi^{k}(t,z)-c^{2}\\nabla_{z}^{2}\\phi^{k}(t,z)=0\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The above constraint empirically produces highly diverse and realistic trajectories. Our velocity matching objective and boundary condition then become: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{r}=\\frac{1}{T}\\sum_{t=0}^{T-1}||\\frac{\\partial^{2}}{\\partial t^{2}}\\phi^{k}(t,z)-c^{2}\\nabla_{z}^{2}\\phi^{k}(t,z)||_{2}^{2},\\;\\mathcal{L}_{\\phi}=\\sum_{k=0}K-1||\\nabla_{z}\\phi^{k}(0,z_{0})||_{2}^{2}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{L}_{r}$ and $\\mathcal{L}_{\\phi}$ restrict the physical constraints and the initial condition, respectively. Note that $\\phi^{k}\\,\\equiv\\,0$ is a trivial optimal solution for the above two objectives regarding that both ${\\mathcal{L}}_{r}$ and $\\mathcal{L}_{\\phi}$ are non-negative. To prevent the parameterized $\\phi^{k}$ from converging to such a trivial solution, we introduce more guidance to the loss function in Section 3.2 separately. ", "page_idx": 3}, {"type": "text", "text": "Alternative Flows. Besides HJE and wave equations, our framework is also general to include other commonly used PDEs that allow for different dynamics along the flow, such as Fokker Planck equation and heat equation. In the experimental section, we will explore the effectiveness of each latent flow in different supervision settings. ", "page_idx": 3}, {"type": "text", "text": "3.2 Supervised & Unsupervised Guidance ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Supervised Semantic Guidance. When an explicit semantic potential energy function or labeled data for the semantic of interest is available, we can use the provided semantic potential to guide the learning of the flow. Firstly, we train a surrogate model $h_{\\eta}:\\mathcal{X}\\rightarrow\\mathbb{R}$ (parameterized by a deep neural network) to predict the corresponding molecular property. Then we use the trained surrogate model as guidance to learn flows that drive the increase of the property for the trajectory of the generated molecules. ", "page_idx": 4}, {"type": "equation", "text": "$$\nd=\\langle-\\nabla_{z}h_{\\eta}(g_{\\psi}(z_{t})),\\nabla_{z}\\phi^{k}(t,z_{t})\\rangle,\\,\\mathcal{L}_{\\mathcal{P}}=-\\operatorname{sign}(d)\\|d\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The intuition behind this objective is to learn the vector field $\\scriptstyle z_{t}$ such that it aligns with the direction of the steepest descent (negative gradient) of the objective function. Note that the sign of the dot product matters as it determines minimizing or maximizing the property. ", "page_idx": 4}, {"type": "text", "text": "The proposed objective function in the supervised scenario is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathcal{L}_{r}+\\mathcal{L}_{\\phi}+\\mathcal{L}_{\\mathcal{P}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Unsupervised Diversity Guidance. When no explicit potential energy function is provided to learn the flow, we need to define a potential energy function that captures the change of molecular properties. As molecular properties are determined by the structures, we devise a potential energy that maximizes the continuous structure change of the generated molecules. Inspired by Song et al. [54], we couple the traversal direction with the Jacobian of the generator to maximize the traversal variations in the molecular space. The perturbation on latent samples can be approximated by the first-order Taylor approximation: ", "page_idx": 4}, {"type": "equation", "text": "$$\ng(z_{t}+\\epsilon\\nabla_{z}\\phi^{k}(t,z_{t}))=g(z_{t})+\\epsilon\\frac{\\partial g(z_{t})}{\\partial z_{t}}\\nabla_{z}\\phi^{k}(t,z_{t})+R_{1}(g(z_{t}))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\epsilon$ denotes perturbation strength, and $R_{1}(\\cdot)$ is the high-order terms. In the unsupervised setting, for sufficiently small $\\epsilon$ , if the Jacobian-vector product can cause large variations in the generated sample, the direction is likely to correspond to certain properties of molecules. We therefore introduce such a Jacobian-vector product guidance: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathcal{I}}=-\\bigg|\\bigg|\\frac{\\partial g(z_{t})}{\\partial z_{t}}\\nabla_{z}\\phi^{k}(t,z_{t})\\bigg|\\bigg|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Compared to the supervised setting which maximizes the change of the molecular properties, it aims to find the direction that causes the maximal change of the structures. This can in turn effectively pushes the initial data distribution to the target one concentrated on the maximum property value. The Jacobian guidance will compete with the dynamical regularization (e.g. wave-like form) on the flow to yield smooth and meaningful traversal paths. ", "page_idx": 4}, {"type": "text", "text": "Disentanglement Regularization. While the above formulation can encourage smooth dynamics and meaningful output variations, the flows are likely to mine identical directions which all correspond to the maximum Jacobian change. To avoid such a trivial solution, we adopt an auxiliary classifier $l_{\\gamma}$ following Song et al. [54] to predict the flow index and use the cross-entropy loss to optimize it: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k}=\\mathcal{L}_{C E}(l_{\\gamma}(g_{\\psi}(z_{t});g_{\\psi}(z_{t+1})),k)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Where ${\\pmb x}_{t}\\,=\\,g({\\pmb z}_{t})$ is the generated sample from timestep $t$ . We see the extra classifier guidance would encourage each flow to be independent and find distinct properties. For each target property, we compute the Pearson correlation coefficient using a randomly generated test set. This coefficient measures the correlation between the property and a natural sequence (from 1 to time step $t$ ) along the optimization trajectory. We then select the energy network that achieves the highest correlation score for optimizing molecules with that specific property. ", "page_idx": 4}, {"type": "text", "text": "The proposed objective function in the unsupervised scenario is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathcal{L}_{r}+\\mathcal{L}_{\\phi}+\\mathcal{L}_{\\mathcal{I}}+\\mathcal{L}_{k}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.3 Connection with Langevin Dynamics for Global Optimization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In scenarios where our flow adheres to the dynamics of the Fokker-Planck equation, our approach may also be interpreted as employing a learned potential energy function to simulate Langevin Dynamics ", "page_idx": 4}, {"type": "text", "text": "for global optimization [15]. Notably, the convergence of Langevin dynamics, particularly at low temperatures, tends to occur around the global minima of the potential energy function [7]. The continuous and discretized Langevin dynamics are as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{d}z_{t}=-\\nabla_{z}h_{\\eta}(z_{t})\\mathrm{d}t+\\sqrt{2}\\mathrm{d}\\mathbf{w}_{t}}\\\\ &{\\quad z_{t}=z_{t-1}-\\nabla_{z}h_{\\eta}(z_{t-1})\\mathrm{d}t+\\sqrt{2\\mathrm{d}t}\\mathcal{N}(0,I)}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proposition 3.1. (Global Convergence of Langevin Dynamics, adapted from Gelfand and Mitter [16]). Given a Langevin dynamics in the form of ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{z_{t}=z_{t-1}-a_{t}(\\nabla_{z}h_{\\eta}(z_{t-1})+\\mathbf{u}_{t})+b_{t}\\mathbf{w}_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where ${\\bf w}_{t}$ is a $d$ -dimensional Brownian motion, $a_{t}$ and $b_{t}$ are a set of positive numbers with $a_{T},b_{T}\\rightarrow$ 0, and $\\pmb{u}_{t}$ is a set of random variables in $\\mathbb{R}^{n}$ denoting noisy measurements of the energy function $h_{\\eta}(\\cdot)$ . Under mild assumptions, $\\scriptstyle z_{t}$ converges to the set of global minima of $h_{\\eta}(\\cdot)$ in probability. ", "page_idx": 5}, {"type": "text", "text": "Following Proposition 3.1, the learned latent flow can be used to search for molecules with optimal properties and it converges to the global minimizers of the learned latent potential energy function. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experiment Set-up ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets & Molecular properties. We extract 4,253,577 molecules from the three commonly used datasets for drug discovery including MOSES [46], ZINC250K [27], and ChEMBL [68]. Molecules are represented by SELFIES strings [37]. All input molecules are padded to the maximum length in the dataset before ftiting into the generative model. We consider a total of 8 molecular properties which include 3 general drug-related properties \u2014 Quantitative Estimate of Drug-likeness (QED), Synthesis Accessibility (SA), and penalized Octanol-water Partition Coefficient (plogP) and 3 machine learningbased target activities \u2014 DRD2, JNK3 and GSK3B [25] , 2 simulation-based target activities \u2014 docking scores for two human proteins ESR1 and ACAA1. See Appendix D.3 for details. ", "page_idx": 5}, {"type": "text", "text": "Implementations. We establish our framework by pre-training a VAE model that learns a latent space of molecules that can generate new molecules by decoding latent vectors from the latent space. We adapt the framework in Eckmann et al. [13] which is a basic VAE architecture with molecular SELFIES string representations and an additional MLP model as the surrogate property predictor. See Appendix D.5 for all implementation and hyper-parameter details. ", "page_idx": 5}, {"type": "text", "text": "Model variants. As discussed in Section 3.1, our proposed framework is general to incorporate different dynamical priors to learn the flow. For the experiments, we consider four types of dynamics including gradient flow $(G F)$ , Wave flow (Wave, eq. (9)), Hamilton Jacobi flow (HJ, eq. (7)) and Langevin Dynamics or equivalently Fokker Planck flow (LD, eq. (15)). ", "page_idx": 5}, {"type": "text", "text": "For the specific molecular properties and evaluations, the readers are kindly referred to Appendix D for details. We also move qualitative evaluations to Appendix F due to space limit. ", "page_idx": 5}, {"type": "text", "text": "4.2 Molecule Optimization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Molecule optimization is key in drug design and materials discovery, aiming to identify molecules with optimal properties [4]. Various machine learning methods have accelerated this process [8]. Our discussion focuses on optimization within the latent space of generative models, primarily using gradient-based optimization as outlined in Section 2.1. We categorize molecule optimization into three scenarios: (1) unconstrained optimization to identify molecules with the best properties, (2) constrained optimization to find molecules with the best-expected property and similar to specific structures\u2014a common step in the lead optimization process, and (3) multi-objective optimization to simultaneously enhance multiple properties of a molecule. ", "page_idx": 5}, {"type": "table", "img_path": "aAaV4ZbQ9j/tmp/a6ae0aa28f31579bd844d11cc817a532bbc2bd0009b30dd17fdd3b34ab8f22f9.jpg", "table_caption": ["Table 1: Unconstrained plogP, QED maximization, and docking score minimization. (SPV denotes supervised scenarios, UNSUP denotes unsupervised scenarios). Boldface highlights the highest-performing generation for each property within each rank. ", "Baselines. For molecule optimization, we follow the same experiment procedure as in Eckmann et al. [13]1. To ensure a fair comparison, we use the same pre-trained VAE model for all the methods. The details about the baselines are deferred to Appendix D.1. We also propose evolutionary algorithm (EA)-based latent optimization approaches in our comparison (Appendix D.7). "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Unconstrained Molecule Optimization. In this study, we randomly sample 100,000 molecules from the latent space and assess the top three scores after 10 steps of optimization for each method (details in Table 1). For two specific docking scores tasks, however, only 10,000 molecules are sampled due to computational resource constraints. All methods employ a step size of 0.1 to ensure a fair comparison. Our findings reveal that the efficacy of optimization methods varies with target properties, highlighting the necessity of employing a diverse set of approaches within the optimization framework, rather than depending on a single dominant method. We also visualize some generated ligands docked into protein pockets in Figure 2. ", "page_idx": 6}, {"type": "image", "img_path": "aAaV4ZbQ9j/tmp/fcad769ae20b960b92def4a70ee704b45c17a0c34d29eab0a71db1c7f43de62e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 2: Visualization of generated ligands docked against target ESR1 and ACAA1. ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Furthermore, when extending the optimization procedure to 1,000 steps, illustrated in Figure 3, Langevin dynamics significantly pushes the entire distribution to molecules with better properties, surpassing other methods in performance. Although the random direction method is effective in optimizing molecules, it does not consistently produce significant shifts in the distribution. Moreover, the outputs from ChemSpace often converge to just a few molecules, which is indicative of the challenge posed by Out-of-Distribution (OoD) generation. ", "page_idx": 6}, {"type": "text", "text": "Similarity-constrained Molecule Optimization. Adopting methodologies from JT-VAE [31] and LIMO [13], we select 800 molecules with the lowest partition coefficient (plogP) scores from the ZINC250k dataset. These molecules undergo 1,000 steps of optimization until convergence is achieved for all methods. The optimal results for each molecule, adhering to a predefined similarity constraint $(\\delta)$ , is reported as in Table 2. The structural similarity is measured using Tanimoto similarity between Morgan fingerprints with a radius of 2. ", "page_idx": 6}, {"type": "text", "text": "Without any similarity constraints, the unsupervised approaches significantly improve molecular properties with a high success rate. However, as the similarity constraints increase, both the magnitude ", "page_idx": 6}, {"type": "image", "img_path": "aAaV4ZbQ9j/tmp/95644fe235b728c40628192729e2945666ce6f215a47399fe7cbf83bd92335df.jpg", "img_caption": ["Figure 3: Molecular property plogP distribution shifts following the latent flow path. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 2: Similarity-constrained plogP maximization. For each method with minimum similarity constraint $\\delta$ , the results in reported in format mean $\\pm$ standard derivation (success rate $\\%$ ) of absolute improvement, where the mean and standard derivation are calculated among molecules that satisfy the similarity constraint. ", "page_idx": 7}, {"type": "table", "img_path": "aAaV4ZbQ9j/tmp/8314b2041954127556baa95b6709f4526b80b59af5c513faa52aeb4290b80039.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "and success rate of unsupervised methods decrease notably. The most considerable improvements of ChemSpace are observed when similarity constraints are set at 0.4 and 0.6. Despite this, as shown in Figure 3 and previously discussed, the generation within ChemSpace encounters significant OoD issues after extensive iterations. Among all techniques evaluated, Langevin dynamics stands out for its overall high improvements and high success rate. As further illustrated in Figure 5, Langevin dynamics also demonstrates a notably faster empirical convergence rate. We also report the performance in optimizing the QED property in Appendix D.8. ", "page_idx": 7}, {"type": "text", "text": "Surprisingly, we observe that the random direction performs well on molecule optimization tasks. This observation motivates us to study the structure of the latent space. We show that the molecular structure distribution on the latent space follows a high-dimensional Gaussian distribution and the random direction increases the norm of the latent vectors that have strong correlations with molecular properties. We analyze this systematically in Appendix E. It is also notable that although random directions could be effective in optimizing molecules, the distribution of the entire molecule sets being optimized does not change accordingly as shown in Figure 3. ", "page_idx": 7}, {"type": "text", "text": "Multi-objective Molecule Optimization. As we are learning distinct vector fields and potential energy functions for each property, they can be readily added together for multi-objective optimization [13, 11]. To generate molecules that are optimized on multiple properties, we use a similar setting as similarity-constrained molecule optimization to select 800 molecules from the ZINC250k dataset with the lowest QED and aim to generate molecules with high QED as well as low SA simultaneously. At each time step, the latent vector is optimized following the averaged direction of two corresponding flow directions. This scheme could be seamlessly generalized to $k$ -objectives optimization. Table 3 shows that Langevin dynamics and ChemSpace achieve the best or competitive performance at all similarity cutoff levels. ", "page_idx": 7}, {"type": "text", "text": "4.3 Molecule Manipulation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Molecule manipulation is a relatively new task proposed in Du et al. [11] to study the performance of latent traversal methods. Specifically, the main idea of molecule manipulation is to find smooth local changes of molecular structures that simultaneously improve molecular properties which is essential to help chemists systematically understand the chemical space. ", "page_idx": 7}, {"type": "text", "text": "Supervised Molecule Manipulation Table 4 shows the success rate results of manipulating 1,000 randomly sampled molecules to optimize each desired property. Following Du et al. [11], we traverse the latent space for 10 steps in the traversal direction of each method and report the strict and relaxed success rate. Details of the definition of these metrics can be found in Appendix D.6. Among all approaches, the gradient flow achieves the highest success rates in multiple properties such that it takes the steepest descent of the surrogate model. When the step size is small enough, it is reasonable to learn a smooth path. However, the results still vary across properties. ", "page_idx": 7}, {"type": "table", "img_path": "aAaV4ZbQ9j/tmp/58d0e4e70e2d34f62c60547f4752febf3a90cabe618e22cf17e726cdf3e86f4f.jpg", "table_caption": ["Table 3: Similarity-constrained Multi-objective (QED-SA) maximization. The value of QED and SA is scaled to both have a range from 0 to 100 for an equal-weighted sum. The method with the highest equal-weighted sum improvement of QED and SA of each structure similarity level is bolded. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "aAaV4ZbQ9j/tmp/fc651f7da3b3709cfac9668544b777bf62293ca3d112eaa0b4220aaf6bca2780.jpg", "table_caption": ["Table 4: Success Rate of traversing latent molecule space to manipulate over a variety of molecular properties. Numbers reported are strict success rate/relaxed success rate in $\\%$ . (SPV denotes supervised scenarios, UNSUP denotes unsupervised scenarios). The ranking is the average between the ranking of average strict success rate and ranking of the average relaxed success rate. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Unsupervised Molecule Manipulation As the correspondence between specific molecular properties and learned latent flows is not explicitly given in the unsupervised scenario, we use an artificial process to mimic the use case in reality. Specifically, we learn 10 different potential energy functions representing 10 disentangled flows following Algorithm 1 using Wave equation and Hamilton-Jacobi equation and validated them on 1,000 unseen molecules. For each flow, we evaluate the properties of molecules generated along the 10-step manipulation trajectory. The learned potential energy function with the highest correlation score is selected for each property representing the learned Jacobian structural change that would most effectively optimize the corresponding property. ", "page_idx": 8}, {"type": "text", "text": "In Table 4, we can observe that even though it is without supervised training of traversal directions, the flow still learns meaningful directions from molecular structure to property changes. Surprisingly, the relaxed success rate of manipulating molecules for JNK3, and GSK3B in unsupervised settings are better than in supervised settings. We hypothesize that this is partially because of the training and generalization errors of the surrogate model. On the contrary, the structure change measurement does not provide supervision but is correlated with key molecular properties. We would like to point out that this is an open question in chemistry, often referred as to the structure-activity relationship [12], such that it is important to know the correspondence between structure and activity. We believe this is a promising result to demonstrate that generative models \u201crealize\" molecular property by learning from structures. ", "page_idx": 8}, {"type": "text", "text": "Among the quantitative results, it is interesting that the random direction achieves a good relaxed success rate for some properties, we argue this is because of the specific property of the learned latent space. The latent space learned by generative models tend to be smooth such that similar molecular structures are often mapped to close areas in the latent space. In Appendix E, we find that some molecular properties are highly correlated with their latent vector norms, in which a random direction always increases the norm and thus successfully manipulates a portion of molecules by chance. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion, Limitation and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose a unified framework for navigating chemical space through the learned latent space of molecule generative models. Specifically, we formulate the traversal process as a flow that defines a vector to transport the mass of molecular distribution through time to desired concentrations (e.g. high properties). Two forces (supervised potential guidance and unsupervised structure diversity guidance) are derived to drive the dynamics. We also propose a variety of new physical PDEs on the dynamics which exhibit different properties. We hope this general framework can open up a new research avenue to study the structure and dynamics of the latent space of molecule generative models. ", "page_idx": 9}, {"type": "text", "text": "Limitation and future work. This work is a preliminary study on small molecules and it may be interesting to see it transfer to larger molecular systems or more specialized systems and properties. Beyond molecules, this approach has the potential to be extended to languages and other data modalities. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Y.D. would like to thank Ziming Liu and Kirill Neklyudov for helpful discussions. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] L. Ambrosio, N. Gigli, and G. Savar\u00e9. Gradient flows: in metric spaces and in the space of probability measures. Springer Science & Business Media, 2005.   \n[2] J.-D. Benamou and Y. Brenier. A computational fluid mechanics solution to the monge-kantorovich mass transfer problem. Numerische Mathematik, 84(3):375\u2013393, 2000.   \n[3] R. S. Bohacek, C. McMartin, and W. C. Guida. The art and practice of structure-based drug design: a molecular modeling perspective. Medicinal research reviews, 16(1):3\u201350, 1996.   \n[4] N. Brown, M. Fiscato, M. H. Segler, and A. C. Vaucher. Guacamol: benchmarking models for de novo molecular design. Journal of chemical information and modeling, 59(3):1096\u20131108, 2019.   \n[5] C. P. Burgess, I. Higgins, A. Pal, L. Matthey, N. Watters, G. Desjardins, and A. Lerchner. Understanding disentangling in $\\beta$ -vae. ArXiv, abs/1804.03599, 2018.   \n[6] N. D. Cao and T. Kipf. MolGAN: An implicit generative model for small molecular graphs, 2018.   \n[7] T.-S. Chiang, C.-R. Hwang, and S. J. Sheu. Diffusion for global optimization in r\u02c6n. SIAM Journal on Control and Optimization, 25(3):737\u2013753, 1987.   \n[8] Y. Du, T. Fu, J. Sun, and S. Liu. Molgensurvey: A systematic survey in machine learning models for molecule design. arXiv preprint arXiv:2203.14500, 2022.   \n[9] Y. Du, X. Guo, A. Shehu, and L. Zhao. Interpretable molecular graph generation via monotonic constraints. In Proceedings of the 2022 SIAM International Conference on Data Mining (SDM), pages 73\u201381. SIAM, 2022.   \n[10] Y. Du, X. Guo, Y. Wang, A. Shehu, and L. Zhao. Small molecule generation via disentangled representation learning. Bioinformatics, 38(12):3200\u20133208, 2022.   \n[11] Y. Du, X. Liu, N. M. Shah, S. Liu, J. Zhang, and B. Zhou. Chemspace: Interpretable and interactive chemical space exploration. Trans. Mach. Learn. Res., 2023, 2023.   \n[12] A. Z. Dudek, T. Arodz, and J. G\u00e1lvez. Computational methods in developing quantitative structure-activity relationships (qsar): a review. Combinatorial chemistry & high throughput screening, 9(3):213\u2013228, 2006.   \n[13] P. Eckmann, K. Sun, B. Zhao, M. Feng, M. Gilson, and R. Yu. Limo: Latent inceptionism for targeted molecule generation. In International Conference on Machine Learning, pages 5777\u20135792. PMLR, 2022.   \n[14] T. Fu, W. Gao, C. Coley, and J. Sun. Reinforced genetic algorithm for structure-based drug design. Advances in Neural Information Processing Systems, 35:12325\u201312338, 2022.   \n[15] C. W. Gardiner et al. Handbook of stochastic methods, volume 3. springer Berlin, 1985.   \n[16] S. B. Gelfand and S. K. Mitter. Recursive stochastic algorithms for global optimization in r\u02c6d. SIAM Journal on Control and Optimization, 29(5):999\u20131018, 1991.   \n[17] L. Goetschalckx, A. Andonian, A. Oliva, and P. Isola. Ganalyze: Toward visual definitions of cognitive image properties. In ICCV, 2019.   \n[18] R. G\u00f3mez-Bombarelli, J. N. Wei, D. Duvenaud, J. M. Hern\u00e1ndez-Lobato, B. S\u00e1nchez-Lengeling, D. Sheberla, J. Aguilera-Iparraguirre, T. D. Hirzel, R. P. Adams, and A. Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS central science, 4(2):268\u2013276, 2018.   \n[19] I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio. Deep learning, volume 1. MIT Press, 2016.   \n[20] R.-R. Griffiths and J. M. Hern\u00e1ndez-Lobato. Constrained bayesian optimization for automatic chemical design using variational autoencoders. Chemical science, 11(2):577\u2013586, 2020.   \n[21] G. L. Guimaraes, B. Sanchez-Lengeling, C. Outeiral, P. L. C. Farias, and A. Aspuru-Guzik. Objectivereinforced generative adversarial networks (organ) for sequence generation models. arXiv preprint arXiv:1705.10843, 2017.   \n[22] E. H\u00e4rk\u00f6nen, A. Hertzmann, J. Lehtinen, and S. Paris. Ganspace: Discovering interpretable gan controls. Advances in neural information processing systems, 33:9841\u20139850, 2020.   \n[23] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[24] E. Hoogeboom, V. G. Satorras, C. Vignac, and M. Welling. Equivariant diffusion for molecule generation in 3d. In International conference on machine learning, pages 8867\u20138887. PMLR, 2022.   \n[25] K. Huang, T. Fu, W. Gao, Y. Zhao, Y. Roohani, J. Leskovec, C. W. Coley, C. Xiao, J. Sun, and M. Zitnik. Artificial intelligence foundation for therapeutic science. Nature chemical biology, 18(10):1033\u20131036, 2022.   \n[26] J. B. Ingraham, M. Baranov, Z. Costello, K. W. Barber, W. Wang, A. Ismail, V. Frappier, D. M. Lord, C. Ng-Thow-Hing, E. R. Van Vlack, et al. Illuminating protein space with a programmable generative model. Nature, pages 1\u20139, 2023.   \n[27] J. J. Irwin and B. K. Shoichet. Zinc- a free database of commercially available compounds for virtual screening. Journal of chemical information and modeling, 45(1):177\u2013182, 2005.   \n[28] A. Jahanian, L. Chai, and P. Isola. On the\" steerability\" of generative adversarial networks. In International Conference on Learning Representations, 2019.   \n[29] A. Jahanian, L. Chai, and P. Isola. On the\" steerability\" of generative adversarial networks. ICLR, 2020.   \n[30] J. H. Jensen. A graph-based genetic algorithm and generative model/monte carlo tree search for the exploration of chemical space. Chemical science, 10(12):3567\u20133572, 2019.   \n[31] W. Jin, R. Barzilay, and T. Jaakkola. Junction tree variational autoencoder for molecular graph generation. ICML, 2018.   \n[32] W. Jin, R. Barzilay, and T. Jaakkola. Hierarchical generation of molecular graphs using structural motifs. In International Conference on Machine Learning, 2020.   \n[33] J. Jo, S. Lee, and S. J. Hwang. Score-based generative modeling of graphs via the system of stochastic differential equations. In International Conference on Machine Learning, pages 10362\u201310383. PMLR, 2022.   \n[34] R. Jordan, D. Kinderlehrer, and F. Otto. The variational formulation of the fokker-planck equation. Siam Journal on Applied Mathematics, 1996.   \n[35] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.   \n[36] M. Krenn, F. Hase, A. Nigam, P. Friederich, and A. Aspuru-Guzik. Self-referencing embedded strings (selfies): A $100\\%$ robust molecular string representation. Machine Learning: Science and Technology, 1, 2019.   \n[37] M. Krenn, F. H\u00e4se, A. Nigam, P. Friederich, and A. Aspuru-Guzik. Self-referencing embedded strings (selfies): A $100\\%$ robust molecular string representation. Machine Learning: Science and Technology, 1 (4):045024, 2020.   \n[38] M. Kwon, J. Jeong, and Y. Uh. Diffusion models already have a semantic latent space. In The Eleventh International Conference on Learning Representations, 2022.   \n[39] C. A. Lipinski, F. Lombardo, B. W. Dominy, and P. J. Feeney. Experimental and computational approaches to estimate solubility and permeability in drug discovery and development settings. Advanced drug delivery reviews, 64:4\u201317, 2012.   \n[40] Q. Liu, M. Allamanis, M. Brockschmidt, and A. Gaunt. Constrained graph variational autoencoders for molecule design. Advances in neural information processing systems, 31, 2018.   \n[41] H. Loeffler, J. He, A. Tibo, J. P. Janet, A. Voronov, L. Mervin, and O. Engkvist. Reinvent4: Modern ai\u2013driven generative molecule design. 2023.   \n[42] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2017.   \n[43] K. Madhawa, K. Ishiguro, K. Nakago, and M. Abe. Graphnvp: An invertible flow model for generating molecular graphs. arXiv preprint arXiv:1905.11600, 2019.   \n[44] D. Misra. Mish: A self regularized non-monotonic activation function. In British Machine Vision Conference, 2020.   \n[45] W. Peebles, J. Peebles, J.-Y. Zhu, A. Efros, and A. Torralba. The hessian penalty: A weak prior for unsupervised disentanglement. In ECCV, 2020.   \n[46] D. Polykovskiy, A. Zhebrak, B. S\u00e1nchez-Lengeling, S. Golovanov, O. Tatanov, S. Belyaev, R. Kurbanov, A. A. Artamonov, V. Aladinskiy, M. Veselov, A. Kadurin, S. I. Nikolenko, A. Aspuru-Guzik, and A. Zhavoronkov. Molecular sets (moses): A benchmarking platform for molecular generation models. Frontiers in Pharmacology, 11, 2018.   \n[47] D. Rezende and S. Mohamed. Variational inference with normalizing flows. In International conference on machine learning, pages 1530\u20131538. PMLR, 2015.   \n[48] B. Sanchez-Lengeling and A. Aspuru-Guzik. Inverse molecular design using machine learning: Generative models for matter engineering. Science, 361(6400):360\u2013365, 2018.   \n[49] A. Schneuing, Y. Du, C. Harris, A. Jamasb, I. Igashov, W. Du, T. Blundell, P. Li\u00f3, C. Gomes, M. Welling, et al. Structure-based drug design with equivariant diffusion models. arXiv preprint arXiv:2210.13695, 2022.   \n[50] Y. Shen and B. Zhou. Closed-form factorization of latent semantics in gans. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1532\u20131540, 2021.   \n[51] Y. Shen, C. Yang, X. Tang, and B. Zhou. Interfacegan: Interpreting the disentangled face representation learned by gans. IEEE transactions on pattern analysis and machine intelligence, 44(4):2004\u20132018, 2020.   \n[52] Y. Song, N. Sebe, and W. Wang. Orthogonal svd covariance conditioning and latent disentanglement. IEEE T-PAMI, 2022.   \n[53] Y. Song, A. Keller, N. Sebe, and M. Welling. Flow factorzied representation learning. In NeurIPS, 2023.   \n[54] Y. Song, A. Keller, N. Sebe, and M. Welling. Latent traversals in generative models as potential flows. arXiv preprint arXiv:2304.12944, 2023.   \n[55] Y. Song, J. Zhang, N. Sebe, and W. Wang. Householder projector for unsupervised latent semantics discovery. In ICCV, 2023.   \n[56] Y. Song, T. A. Keller, Y. Yue, P. Perona, and M. Welling. Unsupervised representation learning from sparse transformation analysis. arXiv preprint arXiv:2410.05564, 2024.   \n[57] J. Vamathevan, D. Clark, P. Czodrowski, I. Dunham, E. Ferran, G. Lee, B. Li, A. Madabhushi, P. Shah, M. Spitzer, et al. Applications of machine learning in drug discovery and development. Nature reviews Drug discovery, 18(6):463\u2013477, 2019.   \n[58] C. Vignac, I. Krawczuk, A. Siraudin, B. Wang, V. Cevher, and P. Frossard. Digress: Discrete denoising diffusion for graph generation. In The Eleventh International Conference on Learning Representations, 2023.   \n[59] A. Voynov and A. Babenko. Unsupervised discovery of interpretable directions in the gan latent space. In ICML, 2020.   \n[60] H. Wang, T. Fu, Y. Du, W. Gao, K. Huang, Z. Liu, P. Chandak, S. Liu, P. Van Katwyk, A. Deac, et al. Scientific discovery in the age of artificial intelligence. Nature, 620(7972):47\u201360, 2023.   \n[61] J. L. Watson, D. Juergens, N. R. Bennett, B. L. Trippe, J. Yim, H. E. Eisenach, W. Ahern, A. J. Borst, R. J. Ragotte, L. F. Milles, et al. De novo design of protein structure and function with rfdiffusion. Nature, 620 (7976):1089\u20131100, 2023.   \n[62] Y. Xie, C. Shi, H. Zhou, Y. Yang, W. Zhang, Y. Yu, and L. Li. Mars: Markov molecular sampling for multi-objective drug discovery. ArXiv, abs/2103.10432, 2021.   \n[63] X. Yang, J. Zhang, K. Yoshizoe, K. Terayama, and K. Tsuda. ChemTS: an efficient python library for de novo molecular generation. Science and technology of advanced materials, 18(1):972\u2013976, 2017.   \n[64] J. You, B. Liu, R. Ying, V. Pande, and J. Leskovec. Graph convolutional policy network for goal-directed molecular graph generation. In NIPS, 2018.   \n[65] J. You, B. Liu, R. Ying, V. S. Pande, and J. Leskovec. Graph convolutional policy network for goal-directed molecular graph generation. In Neural Information Processing Systems, 2018.   \n[66] Y. You, R. Zhou, J. Park, H. Xu, C. Tian, Z. Wang, and Y. Shen. Latent 3d graph diffusion. In The Twelfth International Conference on Learning Representations, 2024.   \n[67] C. Zang and F. Wang. Moflow: an invertible flow model for generating molecular graphs. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 617\u2013626, 2020.   \n[68] B. Zdrazil, E. Felix, F. Hunter, E. J. Manners, J. Blackshaw, S. Corbett, M. de Veij, H. Ioannidis, D. M. Lopez, J. F. Mosquera, M. P. Magari\u00f1os, N. Bosc, R. Arcila, T. Kizil\u00f6ren, A. Gaulton, A. P. Bento, M. F. Adasme, P. Monecke, G. A. Landrum, and A. R. Leach. The chembl database in 2023: a drug discovery platform spanning multiple bioactivity data types and time periods. Nucleic Acids Research, 52:D1180 \u2013 D1192, 2023.   \n[69] C. Zeni, R. Pinsler, D. Z\u00fcgner, A. Fowler, M. Horton, X. Fu, S. Shysheya, J. Crabb\u00e9, L. Sun, J. Smith, et al. Mattergen: a generative model for inorganic materials design. arXiv preprint arXiv:2312.03687, 2023.   \n[70] X. Zhang, L. Wang, J. Helwig, Y. Luo, C. Fu, Y. Xie, M. Liu, Y. Lin, Z. Xu, K. Yan, et al. Artificial intelligence for science in quantum, atomistic, and continuum systems. arXiv preprint arXiv:2307.08423, 2023.   \n[71] Z. Zhou, S. M. Kearnes, L. Li, R. N. Zare, and P. F. Riley. Optimization of molecules via deep reinforcement learning. Scientific Reports, 9, 2018.   \n[72] X. Zhu, C. Xu, and D. Tao. Learning disentangled representations with latent variation predictability. In ECCV, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix for ChemFlow ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Wasserstein Gradient Flow 15 ", "page_idx": 13}, {"type": "text", "text": "B PDE-regularized Latent Space Learning 16 ", "page_idx": 13}, {"type": "text", "text": "C Extended Related Work 17   \nC.1 Machine Learning for Molecule Generation 17   \nC.2 Goal-oriented Molecule Generation 17   \nC.3 Image Editing in the Latent Space 17   \nD Experiments Details 18   \nD.1 Baselines 18   \nD.2 Training Dataset . 18   \nD.3 Molecule Properties 18   \nD.4 Training and inference 19   \nD.5 Experiments Setup 19   \nD.6 Evaluation Metrics 20   \nD.7 Additional Baselines 21   \nD.8 More Experiment Results 22 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "E Latent Space Visualization and Analysis 24 ", "page_idx": 13}, {"type": "text", "text": "F Qualitative Evaluations 25 ", "page_idx": 13}, {"type": "text", "text": "A Wasserstein Gradient Flow ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As shown in the main paper, based on the dynamic formulation of optimal transport [2], the $L_{2}$ Wasserstein distance can be re-written as: ", "page_idx": 14}, {"type": "equation", "text": "$$\nW_{2}(\\mu_{0},\\mu_{1})=\\operatorname*{min}_{\\rho,v}\\sqrt{\\int\\int\\rho_{t}(z)|v_{t}(z)|^{2}\\,d z d t}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $v_{t}(z)$ is the velocity of the particle at position $_{\\textit{z}}$ and time $t$ , and $\\rho_{t}(z)$ is the density $d\\mu(z)=$ $\\rho_{t}(z)d z$ . The distance can be optimized by the gradient flow of a certain function on space and time. Consider the functional $\\mathcal{F}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ that takes the following form: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{F}(\\mu)=\\int U(\\rho_{t}(z))\\,d z\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The curve is considered as a gradient flow if it satisfies $\\begin{array}{r}{\\nabla{\\mathcal F}=-\\frac{d}{d t}\\rho_{t}(z)}\\end{array}$ [1]. Moving the particles leads to: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\mathcal{F}(\\mu)=\\int U^{\\prime}(z)\\frac{d\\,\\rho_{t}(z)}{d t}\\,d z\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The velocity vector satisfies the continuity equation: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{d\\,\\rho_{t}(z)}{d t}=-\\nabla\\cdot\\Big(v_{t}(z)\\rho_{t}(z)\\Big)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $-\\nabla\\cdot\\Big(v_{t}(z)\\rho_{t}(z)\\Big)$ is the tangent vector at point $\\rho_{t}(z)$ . Eq. (18) can be simplified to: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{d}{d t}\\mathcal{F}(\\mu)=\\int-U^{\\prime}(\\rho_{t}(z))\\nabla\\cdot\\Big(\\upsilon_{t}(z)\\rho_{t}(z)\\Big)\\,d z}}\\\\ &{}&{=\\displaystyle\\int\\nabla\\Big(U^{\\prime}(\\rho_{t}(z))\\Big)\\upsilon_{t}(z)\\rho_{t}(z)\\,d z}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "On the other hand, the calculus of differential geometry gives ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\mathcal{F}(\\mu)=\\mathrm{Diff}\\mathcal{F}|_{\\rho_{t}}(-\\nabla\\cdot\\Big(v_{t}(z)\\rho_{t}(z)\\Big))=\\langle\\nabla\\mathcal{F},-\\nabla\\cdot\\Big(v_{t}(z)\\rho_{t}(z)\\Big)\\rangle_{f}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\langle,\\rangle_{f}$ is a Riemannian distance function which is defined as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\langle-\\nabla\\cdot\\Big(w_{1}(z)\\rho_{t}(z)\\Big),-\\nabla\\cdot\\Big(w_{2}(z)\\rho_{t}(z)\\Big)\\rangle_{f}=\\int w_{1}(z)w_{2}(z)f(z)\\,d z\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This scalar product coincides with the $W_{2}$ distance according to Benamou and Brenier [2]. Then eq. (20) can be similarly re-written as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\mathcal F(\\mu)=\\langle-\\nabla\\cdot\\Big(\\nabla U^{\\prime}(\\rho_{t}(z))\\rho_{t}(z)\\Big),-\\nabla\\cdot\\Big(v_{t}(z)\\rho_{t}(z)\\Big)\\rangle_{f}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "So the relation arises as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nabla{\\mathcal F}=-\\nabla\\cdot\\Big(\\nabla U^{\\prime}(\\rho_{t}(z))\\rho_{t}(z)\\Big)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since we have $\\begin{array}{r}{\\nabla{\\mathcal F}=-\\frac{d}{d t}\\rho_{t}(z)}\\end{array}$ , the above equation can be re-written as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\rho_{t}(z)=\\nabla\\cdot\\Big(\\nabla U^{\\prime}(\\rho_{t}(z))\\rho_{t}(z)\\Big)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The above derivations can be alternatively made by the poineering JKO scheme [34]. This explicitly defines the relation between evolution PDEs of $\\rho_{t}(z)$ and the internal energy $U$ . For our method, we use the gradient of our scalar energy field $\\nabla u(z,t)$ to learn the velocity field which is given by $U^{\\prime}(\\rho_{t}(z))$ . Interestingly, driven by certain specific velocity fields $\\nabla u(z,t)$ , the evolution of $\\rho(z,t)$ would become some special PDEs. Here we discuss some possibilities: ", "page_idx": 14}, {"type": "text", "text": "Heat Equations. If we consider the energy function $U$ as the weighted entropy: ", "page_idx": 15}, {"type": "equation", "text": "$$\nU(\\rho_{t}(z))=\\rho_{t}(z)\\log(\\rho_{t}(z))\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We would have exactly the heat equation: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\rho_{t}(z)-\\frac{d}{d z^{2}}\\rho_{t}(z)=0\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Injecting the above equation back into the continuity equation leads to the velocity field $v_{t}(z)$ as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{d\\,\\rho_{t}(z)}{d t}=-\\nabla\\cdot\\Big(v_{t}(z)\\rho_{t}(z)\\Big)=\\frac{d}{d z^{2}}\\rho_{t}(z)}\\\\ &{v_{t}(z)=-\\frac{\\nabla\\rho_{t}(z)}{\\rho_{t}(z)}=-\\nabla\\log(\\rho_{t}(z))}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "When our $\\nabla u(z,t)$ learns the velocity field $-\\nabla\\log\\bigl(\\rho_{t}(z)\\bigr)$ , the evolution of $\\rho(z,t)$ would become heat equations. ", "page_idx": 15}, {"type": "text", "text": "Fokker Planck Equations. For the energy function defined as: ", "page_idx": 15}, {"type": "equation", "text": "$$\nU(\\rho_{t}(z))=-A\\cdot\\rho_{t}(z)+\\rho_{t}(z)\\log(\\rho_{t}(z))\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "we would have the Fokker-Planck equation as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\rho_{t}(z)+\\frac{d}{d z}[\\nabla A\\rho_{t}(z)]-\\frac{d}{d z^{2}}[\\rho_{t}(z)]=0,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The velocity field can be similarly derived as ", "page_idx": 15}, {"type": "equation", "text": "$$\nv_{t}(z)=\\nabla A-\\nabla\\log(\\rho_{t}(z))\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For the velocity field $\\nabla A-\\nabla\\log(\\rho_{t}(z))$ , the movement of $\\rho(z,t)$ is the Fokker Planck equation. ", "page_idx": 15}, {"type": "text", "text": "Porous Medium Equations. If we define the energy function as ", "page_idx": 15}, {"type": "equation", "text": "$$\nU(\\rho_{t}(z))=\\frac{1}{m-1}\\rho_{t}^{m}(z)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then we would have the porous medium equation where $m>1$ and the velocity field: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\rho_{t}(z)-\\frac{d}{d z^{2}}\\rho_{t}^{m}(z)=0,\\;v_{t}(z)=-m\\rho^{m-2}\\nabla\\rho\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "When the $\\nabla u(z,t)$ learns the velocity $-m\\rho^{m-2}\\nabla\\rho$ , the trajectory of $\\rho(z,t)$ becomes the porous medium equations. ", "page_idx": 15}, {"type": "text", "text": "B PDE-regularized Latent Space Learning ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Our framework can be extended to incorporate the PDE dynamics as part of the training procedure to encourage a more structured representation. To validate the effectiveness, we incorporate a PDE loss such that we expect any path in the latent space to follow specific dynamics (wave equation in our experiment). In addition to the initial setup outlined in Appendix D.5, we further fine-tune the VAE model by applying a PDE regularization loss term, defined as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathcal{L}_{V A E}+\\mathcal{L}_{r}+\\mathcal{L}_{\\phi}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "that includes the velocity-matching objective $\\mathcal{L}_{r}$ and boundary condition $\\mathcal{L}_{\\phi}$ . This PDE-regularized latent space learning can also be adapted to other generative models by replacing $\\mathcal{L}_{V A E}$ . We further optimize the model and energy network for 10 epochs across the full training dataset using an AdamW optimizer with a 1e-4 learning rate and a cosine learning rate scheduler, without a warm-up period. All other training parameters remain consistent with those initially described in Appendix D.5. ", "page_idx": 15}, {"type": "text", "text": "During fine-tuning, the VAE loss drops from 0.2187 to 0.06288. The results of QED optimization surpass those of two other unsupervised methods, indicating potential areas for future research and refinement. ", "page_idx": 15}, {"type": "table", "img_path": "aAaV4ZbQ9j/tmp/fa3c69d32effb78fb5fc714183bd7a8e44e4a97da124fdd658097c2a2873d1bf.jpg", "table_caption": ["Table 5: Single-objective Maximization with PDE-regularized Latent Space Learning The results are the same as of Table 1, only unsupervised results are presented for fair comparison. (UNSUP denotes unsupervised scenarios). $K$ represents the number of energy networks trained for unsupervised training with disentanglement regularization. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "C Extended Related Work ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Machine Learning for Molecule Generation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Molecules are highly discrete objects and two branches of methods are thus developed to design or search new molecules [8]. One idea is to leverage the advancement of deep generative models which approximate the data distribution from a provided dataset of molecules and then sample new molecules from the learned density. This idea inspires a line of work developing deep generative models from variational auto-encoders (VAE) [18, 31], generative adversarial networks (GAN) [21, 6], normalizing flows (NF) [43, 67] and more recently diffusion models [24, 58, 33]. However, to respect the combinatorial nature of molecules, another line of work leverage combinatorial optimization to search new molecules including genetic algorithm (GA) [30], Monte Carlo tree search (MCTS) [63], reinforcement learning (RL) [64], but often with sophisticated optimization objectives beyond simple valid molecules. ", "page_idx": 16}, {"type": "text", "text": "C.2 Goal-oriented Molecule Generation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In addition to simply generating valid molecules, a more realistic application is to generate molecules with desired properties [8]. For deep generative model-based methods, it is naturally combined with on-the-fly optimization methods such as gradient-based or Bayesian optimization (in low data regime) as it often maps data to a low-dimensional and smooth latent space thus more friendly for these optimization methods [20]. For methods that do not explicitly reduce the dimensionality of data such as diffusion models, Schneuing et al. [49] propose an evolutionary process to iteratively optimize the generated molecules. As it is observed that the learned latent space exhibits explicit structure [18], Du et al. [11] leverage such property to learn a linear classifier to find the latent direction to optimize the property of given molecules. In opposition to deep generative models, combinatorial optimization methods are often inherently associated with optimization, e.g. reward function in RL, selection criteria in GA, etc [14, 41]. ", "page_idx": 16}, {"type": "text", "text": "C.3 Image Editing in the Latent Space ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Beyond molecule generation, there is a vast literature on the study of the latent space of generative models on images for image editing and manipulation [17, 29, 59, 22, 72, 45, 50, 52, 54, 53, 55, 56]. Here we highlight some representative supervised and unsupervised approaches. Supervised methods usually require pixel-wise annotations. InterfaceGAN [51] leverages face image pairs of different attributes to interpret disentangled latent representations of GANs. Jahanian et al. [29] explores linear and non-linear walks in the latent space under the guidance of user-specified transformation. Compared to supervised methods, unsupervised ones mainly focus on discovering meaningful interpretable directions in the latent space through extra regularization. Voynov and Babenko [59] proposes to jointly learn a set of orthogonal directions and a classifier to learn the distinct interpretable directions. SeFa [50] and HouseholderGAN [55] propose to use the eigenvectors of the (orthogonal) projection matrices as interpretable directions to traverse the latent space. More relevantly, Song et al. [54] proposes to use wave-like potential flows to model the spatiotemporal dynamics in the latent spaces of different generative models. ", "page_idx": 16}, {"type": "text", "text": "D Experiments Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.1 Baselines ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We compare with the following baselines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Random: we take a linear direction that is sampled from Multi-variant Gaussian distribution in the high dimensional latent space and normalized to unit length for all molecules across all time steps.   \n\u2022 Random 1D: we take a unit vector where only 1 randomly selected dimension is either 1 or -1 as the linear direction.   \n\u2022 ChemSpace [11]: a separation boundary of the training dataset in latent space w.r.t. the desired property is classified by an Support vector machine (SVM). Then we take the normal vector corresponding to the positive separation as the manipulation direction of control.   \n\u2022 Gradient Flow (LIMO) [13]: a VAE-based generative model that encodes the input molecules into SELFIES [36] and auto-regressive on the tokenized molecule. LIMO uses Adam optimizer to reverse optimize on the input latent vector $z$ whereas Gradient Flow is equivalent to using an SGD optimizer for the same purpose.   \n\u2022 Evolutionary Algorithm (EA): EA is a general framework that leverages random mutation and crossover to select better samples iteratively to search good solutions. In our scenario, we realize an EA-based baseline by perturbing the vector by the directions given by random, ChemSpace and gradient. The pseudocode can be found in Algorithm 3. The results are presented at Appendix D.7. ", "page_idx": 17}, {"type": "text", "text": "D.2 Training Dataset ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2022 ChEMBL [68] is a database of 2.4M bioactive molecules with drug-like properties, including features like a Natural Product likeness score and annotations for chemical probes and bioactivity measurements.   \n\u2022 MOSES [46] is a benchmarking dataset derived from the ZINC [27] Clean Leads collection, containing 2M filtered molecules with specific physicochemical properties, organized into training, test, and unique scaffold test sets to facilitate the evaluation of model performance on novel molecular scaffolds.   \n\u2022 ZINC250k is a subset of the ZINC database containing ${\\sim}250{,}000$ commercially available compounds for virtual screening. ", "page_idx": 17}, {"type": "text", "text": "D.3 Molecule Properties ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We report the following metrics for our experiments: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Penalized logP/plogP: Estimated octanol-water partition coefficient penalized by synthetic accessibility (SA) score and the number of atoms in the longest ring.   \n\u2022 QED: Quantitative Estimate of Drug-likeness, a metric that evaluates the likelihood of a molecule being a successful drug based on its pharmacophores and physicochemical properties.   \n\u2022 SA: Synthetic Accessibility, a score that predicts the ease of synthesis of a molecule, with lower values indicating easier synthesis.   \n\u2022 DRD2 activity: Predicted activity against the D2 dopamine receptor, using machine learning models trained on known bioactivity data.   \n\u2022 JNK3 activity: Predicted activity against the c-Jun N-terminal kinase 3, important for developing treatments for neurodegenerative diseases.   \n\u2022 GSK3B activity: Predicted activity against Glycogen Synthase Kinase 3 beta, which plays a crucial role in various cellular processes including metabolism and neuronal cell development.   \n\u2022 ESR1 docking score: Simulation-based score representing the binding affinity of a molecule to Estrogen Receptor 1, relevant in the context of breast cancer therapies. ", "page_idx": 17}, {"type": "text", "text": "\u2022 ACAA1 docking score: Simulation-based score representing the binding affinity of a molecule to Acetyl-CoA Acyltransferase 1, important for metabolic processes in cells. ", "page_idx": 18}, {"type": "text", "text": "D.3.1 Misalignment of normalization schemes for penalized logP ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We notice that plogP is a commonly reported metric in recent molecule discovery literature but does not share the same normalization scheme. Following G\u00f3mez-Bombarelli et al. [18, Eq. 1], the SA scores and a ring penalty term were introduced into the calculation of penalized logP as the following ", "page_idx": 18}, {"type": "equation", "text": "$$\nJ^{\\mathrm{logP}}(m)=\\mathrm{logP}(m)-\\mathrm{SA}(m)-\\mathrm{ring-penalty}(m)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Each term of $\\log\\!\\ P(m)$ , $\\operatorname{SA}(m)$ , and ring-penalty $(m)$ are normalized to have zero mean and unit standard derivation across the training data. However, no sufficient details were included in their paper or their released source code on how the ring-penalty $(m)$ is computed. Specifically, 3 implementations are widely used in various works. ", "page_idx": 18}, {"type": "text", "text": "Penalized by the length of the maximum cycle without normalization ring-penalty $(m)$ is computed as the number of atoms on the longest ring minus 6 in their implementation. Neither $\\mathrm{logP}(m)$ , $\\operatorname{SA}(m)$ , or ring-penalty $(m)$ is normalized. MolDQN [71] reported their results in this scheme. ", "page_idx": 18}, {"type": "text", "text": "Penalized by the length of the maximum cycle with normalization ring-penalty $(m)$ is computed same as without normalization. MARS [62], HierVAE [32], GCPN [65], and ours report plogP using this scheme. ", "page_idx": 18}, {"type": "text", "text": "Penalized by number of cycles As described by Jin et al. [31, page 7 footnote 3], ring-penalty $(m)$ is computed as the number of rings in the molecule that has more than 6 atoms. LIMO reports plogP using this metric. ", "page_idx": 18}, {"type": "text", "text": "D.4 Training and inference ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We detail our training and inference workflows in Alg. 1 and Alg. D.4, respectively. ", "page_idx": 18}, {"type": "table", "img_path": "aAaV4ZbQ9j/tmp/a213db15389d5a150a406d02eb6122ef6bd5d0746eddcdbdbce3b58b082006f8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "D.5 Experiments Setup ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Pre-trained VAE We follow the VAE architecture from LIMO consisting of a 128 dimension embedding layer, 1024 latent space size, 3-hidden-layer encoder, and 3-hidden-layer decoder both with 1D batch normalization and non-linear activation functions. The hidden layer sizes are $\\{4096,2048,1024\\}$ for the encoder and reversely for the decoder. We empirically find that replacing the ReLU activation function with its newer variant Mish activation function [44] results ", "page_idx": 18}, {"type": "text", "text": "Require: Pre-trained encoder $f_{\\theta}$ , pre-trained potential function $\\phi$ , (optional) pre-trained proxy   \nfunction $h$ , timestamps $T$ , step size $\\alpha$ , LD strenth $\\beta$   \n1: Sampling: $z_{0}=f_{\\theta}(\\bar{x}_{0})$   \n2: for $t=1,\\dots,T$ do   \n3: if Langevin Dynamics then   \n4: $z_{t}\\overset{\\cdot}{=}z_{t-1}\\overset{\\cdot}{-}\\alpha\\nabla_{z}h_{\\eta}(z_{t-1})+\\beta\\sqrt{2\\alpha}\\mathcal{N}(0,I)$   \n5: else if Gradient Flow then   \n6: $\\boldsymbol{z}_{t}=\\boldsymbol{z}_{t-1}-\\alpha\\nabla_{\\boldsymbol{z}}h_{\\eta}(\\boldsymbol{z}_{t-1})$   \n7: else   \n8: $\\begin{array}{r}{z_{t}=z_{t-1}+\\alpha\\nabla_{z}\\phi(t-1,z_{t-1})}\\end{array}$   \n9: end if   \n10: end for ", "page_idx": 19}, {"type": "text", "text": "in faster convergence and better validation loss. All the experiments reported in this paper use this Mish-activated variant of VAE. ", "page_idx": 19}, {"type": "text", "text": "The VAE is trained using an AdamW [42] optimizer, 0.001 initial learning rate, and 1,024 training batch size. To better prevent the model from being stacked at a sub-optimal local minimum, a cosine learning rate scheduler with a 1e-6 minimum learning rate with periodic restart is applied. The VAE is trained for 150 epochs with 4 restarts on $90\\%$ of the training data and validated with the rest $10\\%$ data. The checkpoint corresponding to the epoch with the lowest validation loss is selected. Training 150 epochs takes ${\\sim}8$ hours on a single RTX 3090 desktop. ", "page_idx": 19}, {"type": "text", "text": "Surrogate Predictor The performance of the surrogate predictor is crucial to the proposed latent traversal framework. To handle chemical properties of different magnitudes, we normalize all chemical properties in the training data to have zero mean and unit variance. Then we use a preactivation-norm MLP with residual connections as the surrogate predictor. The predictor contains 3 residual blocks of size 1024 and the output dimension is 1. Similar to the LIMO setups, we find that the choice of optimizer and training hyperparameters like learning rate or learning rate scheduler is crucial for successful training. The predictor is trained for 20 epochs on 100,000 randomly generated samples and validated with 10,000 unseen data with SGD optimizer, 0.001 learning rate, and batch size 1000. The epoch with the best validation loss is selected. Training each predictor takes less than a minute. ", "page_idx": 19}, {"type": "text", "text": "Energy Network We use an MLP structure to parameterize the energy function (the spatial derivative gives the velocity). The time input $t$ is embedded with a sinusoidal positional embedding followed by a linear layer. The special input $x$ is encoded with a linear layer and ReLU activation function. The training of the network uses 9,000 random data and 1,000 unseen data for validation. For unsupervised settings, 10 disentangled potential energy functions are trained for 310 epochs with a batch size of 100. The epoch with the best validation loss is selected. Training an energy network with 10 disentangled potential energy functions takes ${\\sim}40$ minutes. ", "page_idx": 19}, {"type": "text", "text": "Reproducibility All the experiments including baselines are conducted on one RTX 3090 GPU and one Nvidia A100 GPU. All docking scores are computed on one RTX 2080Ti GPU. The code implementation is available at https://github.com/garywei944/ChemFlow. ", "page_idx": 19}, {"type": "text", "text": "D.6 Evaluation Metrics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Success Rate The success rate is used as the evaluation metric for the molecule manipulation task. It first randomly generates $n$ molecules and traverses each of them in the latent space for $k$ steps. The success rate is calculated as the percentage of $k$ -step trajectories that are successful. In our case, we generate 1000 molecules and traverse for 10 steps. The manipulation is successful if the local change in molecular structure is smooth and molecular property is increased. Specifically, we showed two success rates: the strict success rate and the relaxed success rate. ", "page_idx": 19}, {"type": "text", "text": "For the strict success rate, manipulation is a success if the molecular property is monotonically increasing, molecular similarity with respect to the previous step is monotonically decreasing, and molecules are diversity on the manipulation trajectory. These constraints are formulated as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{S P}(x,k,P)=\\mathbf{1}[\\forall i\\in[k],s.t.P(x_{i})-P(x_{i+1})\\leq0],}\\\\ &{C_{S S}(x,k,S)=\\mathbf{1}[\\forall i\\in[k],s.t.S(x_{i+1},x_{1})-S(x_{i},x_{1})\\leq0],}\\\\ &{\\ \\ \\ \\ C_{S D}(x,k)=\\mathbf{1}[|x_{t}:\\forall i\\in[k]|>2],}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ S S R=\\displaystyle\\frac{1}{|X|}\\sum_{x\\in X}\\mathbf{1}[C_{S P}(x,k,P)\\land C_{S S}(x,k,S)\\land C_{S D}(x,k)]}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\{x_{i}\\}_{i=1}^{k}$ is one $k$ -step manipulation trajectory, $X$ contains $n$ manipulation trajectories, $C$ is the constraint, $P$ is the property evaluation function, $S$ is the structure similarity function (Taminoto similarity over Morgan fingerprints). The $C_{S P}$ constraints that the property of molecules must monotonically increase. The $C_{S S}$ constraints that the structure is similar in regard to the starting molecule must monotonically decrease. The $C_{S D}$ constraint that the molecules must at least change twice during the manipulation. SSR calculates the percentage of trajectories that satisfy all success constraints. ", "page_idx": 20}, {"type": "text", "text": "The relaxed success rate relaxes some constraints by adding a tolerance interval. It is formulated as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{C_{S P}(x,k,P)=\\mathbf{1}[\\forall i\\in[k],s.t.P(x_{t})-P(x_{t+1})\\le\\epsilon],}}\\\\ &{}&{C_{S S}(x,k,S)=\\mathbf{1}[\\forall i\\in[k],s.t.S(x_{t+1},x_{1})-S(x_{t},x_{1})\\le\\gamma],}\\\\ &{}&{C_{S D}(x,k)=\\mathbf{1}[|x_{t}:\\forall i\\in[k]|>2],}\\\\ &{}&{S S R=\\displaystyle\\frac{1}{|X|}\\sum_{x\\in X}\\mathbf{1}[C_{S P}(x,k,P)\\land C_{S S}(x,k,S)\\land C_{S D}(x,k)]}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The relaxed success rate does not require a monotonic increase of molecular property but sets a tolerance threshold $\\epsilon$ . This tolerance threshold $\\epsilon$ is defined as $5\\%$ of the range of property in the training dataset. It also does not require a monotonically decrease of structure similarity with a tolerance threshold $\\gamma$ of 0.1. ", "page_idx": 20}, {"type": "text", "text": "D.7 Additional Baselines ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Evolutionary Algorithm. We present the Evolutionary Algorithm-based (EA) approach to optimize molecules in the latent space as an additional baseline for the experiment. The pseudocode is provided as Algorithm 3. Table 6 shows the performance of the evolutionary algorithm-based approach on unconstrained optimization. For a fair comparison, all methods in Table 6 have the same number of Oracle calls. The result shows that our methods outperform all EA approaches. ", "page_idx": 20}, {"type": "text", "text": "1: Input: $n$ samples select $k$ per iteration, pre-trained ChemSpace/Gradient direction $l$ , step size $\\alpha$ ,   \npre-trained surrogate model $h$ , decoder $g_{\\psi}$   \n2: Randomly sample $n$ latent vectors $\\{z_{i}^{0}\\}_{i=1}^{n}$ in latent space   \n3: for each iteration $t=0$ to $T$ do   \n4: Evaluate sampled latent vector scores: $s_{i}^{t}=h(\\mathbf{z}_{i}^{t})$ for $i=1,\\hdots,n$   \n5: Select top-k scored latent vectors: {zttop}jk=1   \n6: for each selected vector $j=0$ to $k$ do   \n7: Update ztto+p,1j $z_{\\mathrm{top},j}^{t+1}=\\mathbf{z}_{\\mathrm{top},j}^{t}+\\alpha\\cdot l+\\epsilon$ , where $\\pmb{\\epsilon}\\sim\\mathcal{N}(0,\\mathbf{I})$ , $l$ the evolution direction guided by   \nRandom/ChemSpace/Gradient.   \n8: end for   \n9: Randomly sample $\\frac{n}{k}$ samples around each $z_{\\mathrm{top},j}^{t}$ to generate $n$ new latent vectors $\\{z_{i}^{t+1}\\}_{i=1}^{n}$   \n10: end for   \n11: Decode latent vectors: $x_{i}^{T}=g_{\\psi}(z_{i}^{T})$ for $i=1,\\hdots,n$ ", "page_idx": 20}, {"type": "table", "img_path": "aAaV4ZbQ9j/tmp/e71fcdb82a129b664f765d7dd377a51f776082817498f3814bf3761993a5cc8a.jpg", "table_caption": ["Table 6: Unconstrained plogP, QED maximization of Evolutionary Algorithm. (SPV denotes supervised scenarios, UNSUP denotes unsupervised scenarios). Random/ChemSpace/Gradient are the evolution directions of EA. Since EA (Gradient Flow) has converged to a single molecule when optimizing pLogP, only one value is reported. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Unconstrained Molecular Optimization Additional Results In addition to reporting the top 3 scores as presented in Table 1, we computed the mean and standard deviation for the top 100 molecules after unconstrained optimization in Table 7. The table shows that our methods have overall the best optimization performance. In addition, HJ exhibits better performance on mean and standard deviation than on top 3, showing that minimizing the kinetic energy is efficient in pushing the distribution to desired properties. ", "page_idx": 21}, {"type": "table", "img_path": "aAaV4ZbQ9j/tmp/53c5b2041497abe76af6594045c2a252de1f8ba1c9261053f378de8d8ef9f097.jpg", "table_caption": ["Table 7: MSE of Unconstrained plogP, QED maximization, and docking score minimization. (SPV denotes supervised scenarios, UNSUP denotes unsupervised scenarios). Each entry in the table follows the format mean $\\pm$ std (median). Boldface highlights the highest-performing generation for each property within each rank. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "D.8 More Experiment Results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We conduct more experiments to analyze the performance of the proposed methods systematically.   \nThey are referred to and discussed in the main paper. ", "page_idx": 21}, {"type": "text", "text": "Pearson correlation score for unsupervised diversity guidance with disentanglement regularization. Tables 8 and 9 present the Pearson correlation scores for the trained energy networks of wave equations and Hamilton-Jacobi equations with disentanglement regularization, respectively. For properties other than synthetic accessibility (SA), we select the network with the highest correlation score to maximize these properties. Conversely, for SA, the network with the lowest correlation score (most negative score) is chosen. ", "page_idx": 21}, {"type": "text", "text": "Distribution shift and convergence for plogP optimization. Figure 4 illustrates the distribution shift in plogP optimization, complementing the analysis in Figure 3. Similar to the findings discussed in Section 4.2, both unsupervised methods encounter out-of-distribution (OOD) issues after 400 steps, consistent with those observed with ChemSpace. The Random 1D method does not achieve the expected distribution shift, as it manipulates only one dimension of the latent vector. Figure 5 depicts the convergence trends of each method\u2019s improvements. Consistent with the predictions in Proposition 3.1, Langevin Dynamics demonstrates the fastest and most effective convergence among all methods. ", "page_idx": 21}, {"type": "table", "img_path": "aAaV4ZbQ9j/tmp/a2ff8f1a1a46d20bffeeaf829fb4cef7facd3a3060e2cb3ded24f19324c362d3.jpg", "table_caption": ["Table 8: Pearson Correlation of trained Wave PDE Energy Network. The average Pearson correlation between the sequence of real properties and sequence of time steps along the manipulation trajectory following a learned potential function $\\phi^{k}(t,z)$ using wave equations. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "aAaV4ZbQ9j/tmp/1120d84a23b467815a8f1d721c1647016ff72bb928d666022b9cbf4e69b5cb5f.jpg", "table_caption": ["Table 9: Pearson Correlation of trained Hamilton-Jacobian PDE Energy Network. The average Pearson correlation between the sequence of real properties and sequence of time steps along the manipulation trajectory following a learned potential function $\\phi^{k}(t,\\dot{z})$ using Hamilton-Jacobi equations. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Similarity-constrained QED optimization and distribution shift. To further explore optimization tasks on additional properties, we also attempted to enhance the QED of molecules. Table 10 presents the results of efforts to maximize the QED of 800 molecules from the ZIC250K dataset that initially had the lowest QED scores. Despite encountering OoD issues with ChemSpace and two unsupervised methods, as illustrated in Figure 6, the performance of Langevin Dynamics surpasses other methods across various similarity levels, which is consistent with the result from Table 2. ", "page_idx": 22}, {"type": "text", "text": "Table 10: Similarity-constrained QED maximization. For each method with minimum similarity constraint $\\delta$ , the results in reported in format mean $\\pm$ standard derivation (success rate $\\%$ ) of absolute improvement, where the mean and standard derivation are calculated among molecules that satisfy the similarity constraint. ", "page_idx": 22}, {"type": "table", "img_path": "aAaV4ZbQ9j/tmp/5665490f4f5e3096a6ceeee55dfdb460281956981e794c8dd7cd891ac6a1d8fd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "aAaV4ZbQ9j/tmp/5cda5340ff8a429593ca71916b31bc34522e5c3bfa7a7fcd010ec1287cd064ab.jpg", "img_caption": ["Figure 4: Distribution shift for plogP optimization "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "E Latent Space Visualization and Analysis ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "As observed in experiments such that random directions perform surprisingly well on molecule manipulation and optimization tasks, we look into the learned latent space to understand its structure. As the prior of a VAE is an isotropic Gaussian distribution, we first verify if the learned variational poster also follows a Gaussian distribution and we find that it does learn so from the evidence shown in Figure 7\u221a, where the norm of the molecule projected to the latent space concentrate around 32 which is around $\\sqrt{d}$ such that the latent dimension $d$ is 1024. We also visualize in Figure 8 how the properties of the molecules in the training dataset are related to their latent vector norms. Surprisingly, we find a strong correlation between almost all molecular properties and their latent norms. Combining these two evidences, it is not surprising that a random latent vector taking a random direction will change the molecular property smoothly and monotonically. In addition, we further plot when we traverse along a random direction in the latent space, how the change of the norm may correspond to the change of a certain property. Among them, we find that SA is particularly in strong positive correlation with the traversal in Figure 9. Though the emergence of the structure in the latent space is interesting and suggests that better algorithms can be developed to exploit the structure, we leave this to future work. ", "page_idx": 23}, {"type": "text", "text": "Additionally, we visualize the traversal trajectory for each different property using both supervised and unsupervised wave flow in Figure 10. The plot shows that almost all trajectories grow towards a unique direction in the t-SNE plot, which implies the disentanglement of learned directions and, thus, molecular properties. In addition, the figures display sinusoidal wave-shape trajectories, indicating the flow follows wave-like dynamics. In the unsupervised t-SNE plot, the trajectories of some properties overlap, such as plogP and SA. It is because some properties correlate with the same disentangled direction, so their traversal follows the same direction, thus the same trajectories. ", "page_idx": 23}, {"type": "image", "img_path": "aAaV4ZbQ9j/tmp/2ab4ce3c5b570972b17966faaae02824f18456c5e8ffe369fc3715804f3b1ebd.jpg", "img_caption": ["Figure 5: Optimization Convergence Langevin Dynamics shows faster convergence and achieves greater improvement in plogP. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "F Qualitative Evaluations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In addition to quantitative evaluations, we demonstrate some qualitative evaluations in this section. We showcase three manipulation trajectories in Figures 11 to 13. Each of these paths is a 6-step manipulation for different molecular properties using different flows. Specifically, we can see that the supervised wave flow and gradient flow improves the molecular property by the conversion of large heterocyclic rings for better synthesizability. We also show three optimization trajectories in Figures 14 to 16. From left to right, each molecule is a snapshot selected from a trajectory of 1000-step optimization. It is notable that the supervised Hamilton-Jacobi flow optimizes the property by reducing the number of nitrogen atoms. This leads to a more chemically stable molecule, whereas the original molecule, with a large number of nitrogen atoms, is unstable and potentially explosive. The supervised wave flow optimizes the molecular property by simplifying the poly-cyclic molecule which enhances its synthesizability and overall stability. ", "page_idx": 24}, {"type": "image", "img_path": "aAaV4ZbQ9j/tmp/0e4489ed00bcc8dc3a6c8a40551764b0ffd899d7c1d98c66c685c316d934d724.jpg", "img_caption": ["Figure 6: Distribution shift for QED optimization "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "aAaV4ZbQ9j/tmp/2453ea3329b64b20c2129677282a2127992fd7f44801a169a7f68c08165d5476.jpg", "img_caption": ["Figure 7: Latent Vector Norm. Distribution of the norm of the latent vectors projected from the training dataset onto the learned latent space. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "aAaV4ZbQ9j/tmp/220523f488d5d8e6ccc7fa4a822c322979c18e510a7e438b6910252a0cdc6609.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 8: Embedding Norm against Property Value of each path. Norm and property value of molecules along the direction of latent traversal with a random direction. The middle curve shows the mean property value and latent embedding norm for all paths. The shaded area is the standard deviation of property value. ", "page_idx": 26}, {"type": "image", "img_path": "aAaV4ZbQ9j/tmp/72da7f9c0402906e7b8a7667ac9e16ab7e652a53be0ebe9bcb1c4cb4f4a9a605.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 9: Embedding Norm against Property Value of each Molecule. Scatter plot of norm and property value of individual molecules in the training set encoded in the latent space. ", "page_idx": 26}, {"type": "image", "img_path": "aAaV4ZbQ9j/tmp/b55d1176f7155cd04ff3fe3826f13fe37ff7b1ab5866a185b8aa196a4012f607.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 10: t-SNE Visualization of Optimization Trajectories. Optimization Trajectories following supervised and unsupervised wave flow visualized using t-SNE. ", "page_idx": 26}, {"type": "image", "img_path": "aAaV4ZbQ9j/tmp/35c17e45aa61003a63228b097b20b9611baafe08f3aa30657182ccf6559a8a70.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 11: Molecule Manipulation Trajectory Molecule manipulation by chemspace on plogP. ", "page_idx": 26}, {"type": "image", "img_path": "aAaV4ZbQ9j/tmp/d0e4e36660a57514b5eb16412fd67861776f1f65ac9d5038ba9e9c719547a769.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 12: Molecule Manipulation Trajectory Molecule manipulation by gradient flow on plogP. ", "page_idx": 26}, {"type": "image", "img_path": "aAaV4ZbQ9j/tmp/495dbffe6851488ea9a44225b370e7c55588655d3d5fe8555f967970575a173f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 13: Molecule Manipulation Trajectory Molecule manipulation by supervised wave flow on GSK3B. ", "page_idx": 26}, {"type": "image", "img_path": "aAaV4ZbQ9j/tmp/4f2598574e21d75bb34ebae0ec686e0620d64d2ae9c400bd27fd8a50511a55ea.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 14: Molecule Optimization Trajectory Molecule optimization by random direction on plogP. ", "page_idx": 27}, {"type": "image", "img_path": "aAaV4ZbQ9j/tmp/2a7988e90b4ac2de0b3dd4843b9a5159f12ec0954d61ff654cfc8ac82b69a9da.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 15: Molecule Optimization Trajectory Molecule optimization by supervised wave flow on QED. ", "page_idx": 27}, {"type": "image", "img_path": "aAaV4ZbQ9j/tmp/3a90f855277f93311837da6eeb960f163c8905fd92c8253e827c4f5199e5f4c3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 16: Molecule Optimization Trajectory Molecule optimization by supervised HamiltonJacobi flow on QED. ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 28}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 28}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 28}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 28}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 28}, {"type": "text", "text": "IMPORTANT, please: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: The claims in the abstract and introduction reflect the scope and contributions of the paper, including the development of the ChemFlow framework for navigating chemical latent spaces and its validation across multiple molecule manipulation and optimization tasks. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper discusses limitations such as scaling with billions of available molecules in large databases and generalizing beyond small molecules. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not invent any new theory but replies on existing theoretical results. The details including assumptions and derivations (referenced in Section 3 and Appendix A), are discussed. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All experimental setups are clearly described, including data sources, model parameters, and evaluation criteria. The source code is released at https://github.com/garywei944/ChemFlow. This should allow for the reproducibility of the results presented. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: All the data used in our experiments are open-sourced and we include our codes and instructions to run with README.md in https://github.com/garywei944/ChemFlow. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). \u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. ", "page_idx": 30}, {"type": "text", "text": "\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper specifies all necessary experimental details, such as data splits, hyperparameters, and types of optimizers, which are essential for understanding and reproducing experimental results, referred in Appendix D. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Statistical significance is discussed with appropriate measures (std) and success rate provided for key results, ensuring the reliability of the findings. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper details the computational resources used, including the type of GPUs and the estimated compute time for experiments in Appendix D.5, which aids in the replication and understanding of the resource requirements. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The research adheres to the NeurIPS Code of Ethics, and ethical considerations, especially concerning dual-use risks and responsible AI practices, are discussed in Section 5. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: Both positive and negative societal impacts are considered, with discussions on how the ChemFlow framework could impact drug discovery and potential misuse scenarios. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ", "page_idx": 32}, {"type": "text", "text": "\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not introduce new models or datasets with high risks of misuse at this moment; thus, specific safeguards are not necessary. However, general practices for responsible AI are discussed. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper correctly credits all used assets, including datasets and codes, with appropriate references and discusses the terms of use and licenses where applicable. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not introduce new datasets or tools that require additional documentation or licensing information. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The research does not involve human subjects or crowdsourcing, making this question not applicable. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: No human subject research is conducted; hence IRB approval is not required. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]