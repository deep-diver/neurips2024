[{"figure_path": "8ihVBYpMV4/tables/tables_6_1.jpg", "caption": "Table 1: Performance (n@k) of our methods (SymEq and SemCo) and comparison methods (Baseline, Na\u00efve, and Cluster) on MATH and miniF2F datasets. The best performance of each n is in bold. The results show that our proposed methods consistently achieves superior performance.", "description": "This table presents the performance comparison of different methods for autoformalizing mathematical statements, specifically focusing on the n@k accuracy metric where n represents the number of correct formalizations among the top k generated candidates.  The methods compared include the proposed methods (SymEq and SemCo), a baseline method, a naive approach, a clustering technique, and are evaluated on two datasets, MATH and miniF2F.  The best performance for each n (number of correct answers among the top k generations) is highlighted in bold, demonstrating the superior performance of the proposed methods.", "section": "4.2 Empirical Results"}, {"figure_path": "8ihVBYpMV4/tables/tables_7_1.jpg", "caption": "Table 2: Performance (1@k) of our methods (SymEq and SemCo) across various categories from MATH dataset. The formalization results are generated by GPT-4, and the best performance is in bold. The results show that SymEq and SemCo exhibit different behaviors on various categories.", "description": "This table shows the performance of the Symbolic Equivalence (SymEq) and Semantic Consistency (SemCo) methods across different categories of the MATH dataset.  The results are specifically for GPT-4 and highlight the varying effectiveness of each method depending on problem type (e.g., Algebra, Geometry, Number Theory). The best performance in each category is highlighted in bold.", "section": "4.2 Empirical Results"}, {"figure_path": "8ihVBYpMV4/tables/tables_8_1.jpg", "caption": "Table 3: Relative efficiency (%) of our methods (SymEq, SemCo, and Log-comb) and alternatives (Na\u00efve, and Cluster) on MATH and miniF2F datasets. The best performance is in bold. Note that the negative results achieved by Na\u00efve are reasonable since it is less effective compared to the baseline. The results show that our proposed methods exhibit higher efficiency enhancement.", "description": "This table presents the relative efficiency of different methods for autoformalization, comparing the proposed methods (SymEq, SemCo, Log-comb) against baseline and alternative approaches (Na\u00efve, Cluster). Relative efficiency is calculated based on the reduction in manual labeling effort. The table shows that the proposed methods significantly improve efficiency compared to the baselines, with Log-comb consistently achieving the best performance.", "section": "4.2 Empirical Results"}, {"figure_path": "8ihVBYpMV4/tables/tables_8_2.jpg", "caption": "Table 4: Performance (1@k) across various difficulty levels from the MATH dataset, with formalization results generated by GPT-4. The results show that autoformalization accuracy is significantly influenced by the difficulty of the problem.", "description": "This table presents the performance (1@k accuracy) of the GPT-4 model on the MATH dataset, categorized by difficulty levels.  It demonstrates how the autoformalization accuracy varies across different difficulty levels, highlighting the impact of problem complexity on the model's performance.", "section": "4.2 Empirical Results"}]