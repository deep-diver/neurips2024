[{"type": "text", "text": "MMSite: A Multi-modal Framework for the Identification of Active Sites in Proteins ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Song Ouyang1 Huiyu Cai2,3,4 Yong Luo1\u2217 Kehua $\\mathbf{S}\\mathbf{u}^{1*}$ Lefei Zhang1 Bo Du1 1School of Computer Science, National Engineering Research Center for Multimedia Software ", "page_idx": 0}, {"type": "text", "text": "and Institute of Artificial Intelligence, Wuhan University, China ", "page_idx": 0}, {"type": "text", "text": "2BioGeometry, China 3Mila - Qu\u00e9bec AI Institute, Canada 4Universit\u00e9 de Montr\u00e9al, Canada {ouyangsong,luoyong,skh,zhanglefei,dubo}@whu.edu.cn huiyu.cai@mila.quebec ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The accurate identification of active sites in proteins is essential for the advancement of life sciences and pharmaceutical development, as these sites are of critical importance for enzyme activity and drug design. Recent advancements in protein language models (PLMs), trained on extensive datasets of amino acid sequences, have significantly improved our understanding of proteins. However, compared to the abundant protein sequence data, functional annotations, especially precise per-residue annotations, are scarce, which limits the performance of PLMs. On the other hand, textual descriptions of proteins, which could be annotated by human experts or a pretrained protein sequence-to-text model, provide meaningful context that could assist in the functional annotations, such as the localization of active sites. This motivates us to construct a ProTein-Attribute text Dataset (ProTAD), comprising over 570,000 pairs of protein sequences and multi-attribute textual descriptions. Based on this dataset, we propose MMSite, a multi-modal framework that improves the performance of PLMs to identify active sites by leveraging biomedical language models (BLMs). In particular, we incorporate manual prompting and design a MACross module to deal with the multi-attribute characteristics of textual descriptions. MMSite is a two-stage (\u201cFirst Align, Then Fuse\u201d) framework: first aligns the textual modality with the sequential modality through soft-label alignment, and then identifies active sites via multi-modal fusion. Experimental results demonstrate that MMSite achieves state-of-the-art performance compared to existing protein representation learning methods. The dataset and code implementation are available at https://github.com/Gift-OYS/MMSite. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The identification of active sites in proteins is crucial for advancing fields such as life sciences and pharmaceutical development. Active sites are specific regions within a protein where substrate molecules undergo chemical transformations. Understanding these sites is essential for elucidating enzyme mechanisms, designing inhibitors, and developing novel drugs. Traditionally, the recognition of these sites relied on crystallographic techniques, mass spectrometry, and other labor-intensive experiments. Recently, the advent of deep learning has marked in a new era of bioinformatics, significantly enhancing the capabilities for predicting and analyzing protein functions computationally. ", "page_idx": 0}, {"type": "text", "text": "Recent advancements in natural language processing (NLP), particularly with the introduction of large-scale language models (LLMs) [39] [54] [53], have revolutionized the interpretation of complex data. Inspired by these developments, protein language models [12] [2], trained on extensive datasets of amino acid sequences, have significantly advanced our understanding of the intricate language of proteins. Most of the existing studies focus on utilizing PLMs to capture the overall structure and function, and predicting the global properties (\u201cftiness\u201d) of proteins [19]. In contrast, predicting properties at residue level for a given protein (such as identifying protein active sites) by deep learning methods is biologically meaningful but relatively less studied unfortunately, due to the scarcity of precise per-residue annotations. ", "page_idx": 0}, {"type": "image", "img_path": "XHdwlbNSVb/tmp/681125d8db99ce231cd021612b3cbd51678741e3d21e44db139573448aad040a.jpg", "img_caption": ["Figure 1: Difference between our work and existing mainstream works. Left: Existing works focus on obtaining comprehensive protein representations for sequence-level prediction tasks, generating texts from sequence, or retrieving sequences based on textual descriptions. Right: Our task aims to identify active sites at the residue-level using protein sequences and multi-attribute textual descriptions. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "On the other hand, informative textual descriptions, which can be obtained from biological experiments or a pretrained protein sequence-to-text model [1] [32], are widely accessible and anticipated to provide meaningful context that could assist in residue-level tasks. Moreover, the field of multi-modal deep learning [38], which combines diverse data modalities like image and text, provides promising methodologies for advancing protein research. Inspired by this, integrating protein sequences and their corresponding textual descriptions enables us to leverage the strengths of both data types to achieve a more comprehensive understanding of proteins [61] [42] (Figure 1). Our task, multi-modal protein active sites identification, is formally similar to multi-modal named entity recognition (MNER) [36] [31] which combines text and image inputs to identify named entities, but the former is inherently more challenging because: (1) MNER deals with textual and image data, whose interactions are more intuitive and well-studied, whereas the relationship between the amino acid sequence and the textual descriptions of a protein is more implicit; (2) Identifying the active sites requires a detailed understanding of both the protein sequence and its corresponding structure, which are not readily available during inference; (3) While there is a vast amount of text and image data available for MNER, high-quality datasets for multi-modal active sites identification are less abundant. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we build a ProTein-Attribute text Dataset (ProTAD) containing more than 570,000 protein-text description pairs. Each textual description of a protein contains 17 different attribute fields, providing a rich semantic representation of the protein. Then, we develop a novel framework, MMSite, to achieve active site identification by leveraging pretrained PLMs and BLMs. Specifically, we employ prompting and design a multi-attribute cross attention module, MACross, to process the text, and achieve the identification via soft-label alignment and multi-modal fusion. Our method enforces the distribution of textual modality to be close to that of the sequential modality, improving the identification performance of PLMs. During the inference stage, we assume that there are no text descriptions, so the input is only the protein sequence, and the missing textual modality is generated with the aid of an agent model. Extensive experiments demonstrate that our method outperforms existing protein representation learning methods across three token-level and two region-level metrics, and can be seamlessly integrated with different PLMs and BLMs. ", "page_idx": 1}, {"type": "text", "text": "Our contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a new and meaningful task in biological science: identifying active sites in proteins using both sequence and textual descriptions, and construct the ProTAD dataset. \u2022 We introduce a framework that integrates both modalities for predicting protein active sites, utilizing a \u201cFirst Align, Then Fuse\u201d strategy. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Our comprehensive experiments verify the effectiveness of our approach, and demonstrate that our framework can be effectively applied to different PLMs and BLMs. ", "page_idx": 2}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Protein representation learning Significant progress has been made in the field of protein representation learning (PRL) due to the advancements in deep learning techniques. Graph neural networks (GNNs) have emerged as powerful tools for representing protein structures or sequences by encoding them as graphs, capturing intricate interaction patterns among proteins [17] [68]. Additionally, self-supervised learning methods have been widely adopted, utilizing various predictive tasks to train models to learn meaningful representations of proteins. The advent of protein language models like ESMs [34] [18] [30], ProteinBERT [2], TAPE [44], and ProtTrans[12], trained on vast databases of protein sequences [50] [47] [48] and structures [24] [56], has shown promising results in downstream tasks. They demonstrate the potential of transfer learning in protein representation [19]. Moreover, all-atom structures [22] [69] and protein surfaces [14] [51] have also been explored to enhance our understanding of protein structure and function. These studies have applications in protein structure prediction [24] and functional annotation [7] [5]. Despite these advances, there remains a relative scarcity of work focusing on residue-level protein understanding, which plays a crucial role in comprehending the biochemical mechanism of proteins and drug discovery. ", "page_idx": 2}, {"type": "text", "text": "Multi-modal representation learning Multi-modal representation learning addresses the challenge of effectively integrating and utilizing information from diverse data modalities. In this field, significant advancements have been achieved by developing advanced algorithms and models, such as BLIP2 [27] and PaLM-E [11], to improve the learning capabilities. Additionally, the introduction of highly capable language models, notably LLMs like GPT-4 [39], LLaMA2 [54], and Alpaca [53], has ignited fresh enthusiasm in the simultaneous modeling of biomolecules and natural language [41] concurrently. In the field of proteins, while \u201csequence-structure\u201d multi-modal learning is successful [67], incorporating texts is also gain popularity. For example, OntoProtein [66] and InstructProtein [59] incorporate external knowledge graphs into protein pretraining. ProtST [61] enhances protein pretraining and understanding by biomedical texts with BLM, while Prot2Text [1] has achieved function prediction in free text format from graph and sequential inputs. ProLLaMA [33] utilizes the pretrained LLaMA2 to perform continual learning on protein language. Moreover, frameworks like BioT5 [42] and $\\mathrm{BioT}5+$ [40] have been proposed to capture the underlying relations and characteristics of different bio-entities. These developments not only underscore the potential for bridging natural language and protein language but also provide valuable insights for future multi-modal learning research. ", "page_idx": 2}, {"type": "text", "text": "Protein active sites identification The active site1of a protein is a crucial region where it interacts with other molecules. It is typically composed of amino acid residues, and the arrangement determines the protein\u2019s structure and function. Thus, protein active sites identification is essential for understanding the protein\u2019s role within organisms. Over a decade ago, various methods were employed for this purpose, including statistics-based approaches [46], protein surface modelling [15], and straightforward machine learning techniques [25] such as Random Forest and Support Vector Machine [64]. However, these methods were either challenging to implement or lack effectiveness. Recently, deep learning has been leveraged to predict the ligand binding sites on proteins. For instance, DeepSurf [37] and CrossBind [23] utilize 3D voxelized grids and point clouds, respectively, to generate volumetric protein representations. Meanwhile, GraphBind [60], ScanNet [55], and DeepProSite [13] integrate both primary sequences and tertiary structures to recognize amino acids in the binding site region. However, although active sites are directly involved in the activity of a protein and play a significant role in drug design, enzyme engineering, etc., they are relatively less studied. ", "page_idx": 2}, {"type": "text", "text": "3 Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we first give a formal definition to the multi-modal active sites identification task in Section 3.1. Then, we present our MMSite framework in Section 3.2, which comprises attributes reconstruction, feature extraction, multi-modal alignment and fusion. ", "page_idx": 2}, {"type": "image", "img_path": "XHdwlbNSVb/tmp/7562f9c548eb450757ac9317e8e7e9531ccfad6916090e33b850a5b1d8465123.jpg", "img_caption": ["Figure 2: Overview of the MMSite framework. MMSite takes paired sequences and multi-attribute textual descriptions as inputs, using PLM and BLM to extract features in Stage 1. For the text modality, manual prompting and the MACross Module process the multi-attribute descriptions. A \u201cFirst Align, Then Fuse\u201d strategy is then employed to align and fuse both modalities. Specifically, in Stage 2, a Shared Transformer Encoder and soft-label alignment align the dual modalities. In Stage 3, Fusion Attention and a skip concatenation strategy are used to predict active sites, with only the modules in Stage 3 being trainable. Note: During inference, the missing text modality is generated by an agent model and directly input into the Shared Transformer Encoder, bypassing the need to process through MACross, as it is not multi-attribute. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.1 Problem definition ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Here we formulate our task of predicting active sites in proteins using both protein sequences and multi-attribute textual descriptions. We define the dataset composition in our ProTAD as $\\textstyle D=\\{S,{\\mathcal{T}}\\}$ , where $\\boldsymbol{S}=\\{S_{i}\\}_{i=1}^{N}$ represents the primary sequences and $\\dot{\\mathcal{T}}\\stackrel{\\cdot}{=}\\{T_{i}\\}_{i=1}^{N}$ denotes the textual attributes associated with each protein, with $N$ being the total number of entries in the dataset. For the $i\\footnote{C o r r e s p o n d i n g a u t h o r.T e l:~+86-1088236095.E-m a i l a d d e n s c o n s i o n609256096.E-m a i l a d d e n s c o n s i o n6078250096095.E-m a i l a d d e n s c o n s i b l e.}$ -th protein, $S_{i}$ is a sequence of $k_{i}$ amino acids, denoted as ${S_{i}}=\\{\\mathrm{AA}_{i}^{1},\\mathrm{AA}_{i}^{2},\\cdot\\cdot\\cdot,\\mathrm{AA}_{i}^{k_{i}}\\}$ . Each $T_{i}$ is a collection of $M$ textual descriptions that depict protein from different perspectives. These descriptions are structured as pairs consisting of an attribute name $\\mathrm{t}^{n}$ and attribute content $\\mathrm{t}^{c}$ in raw data, specifically, $T_{i}=\\{(\\mathrm{t}_{i,j}^{n},\\mathrm{t}_{i,j}^{c})\\}_{j=1}^{M}$ . As for the annotation target at the token-level, ${\\pmb y}_{i}=\\{y_{i,j}\\}_{j=1}^{k_{i}}$ indicates whether each amino acid in the sequence is an active site, where $y_{i,j}\\in\\{0,1\\}$ . During the training stage, both modalities $\\boldsymbol{S}$ and $\\tau$ are employed to develop our model. The likelihood that each amino acid is an active site is predicted using only the protein sequence $\\boldsymbol{S}$ as an input during the inference stage. ", "page_idx": 3}, {"type": "text", "text": "3.2 Contrastive learning-based alignment and fusion ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Attribute description reconstruction with prompt In our raw data, textual descriptions are structured with each protein having serveral distinct attribute descriptions in the form $(\\mathrm{t}^{n},\\mathrm{t}^{c})$ , formatted as ATTRIBUTE_NAME:ATTRIBUTE_CONTENT, such as: ", "page_idx": 3}, {"type": "text", "text": "\u2022 Protein Name: Parkinson disease protein 7 homolog;   \n\u2022 Taxonomic Lineage: Cellular organisms, Opisthokonta, Eumetazoa;   \n\u2022 Function: Multifunctional protein with controversial molecular function which plays an important role in cell protection against oxidative stress. ", "page_idx": 3}, {"type": "text", "text": "In order to handle this tabular-like form of data, enhancing the linkage between the content and its corresponding attribute name is important. We designed manual prompts for each attribute to reconstruct the tabular text pairs into full sentences, thus $\\tilde{T}=\\{\\tilde{\\mathrm{t}}_{j}\\}_{j=1}^{M}$ . Consequently, the examples above are reconstructed as follows: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "\u2022 The name of protein is Parkinson disease protein 7 homolog. \u2022 The taxonomic lineage of this protein includes Cellular organisms, Opisthokonta, Metazoa. \u2022 The function of this protein includes Multifunctional protein with controversial molecular function which plays an important role in cell protection against oxidative stress. ", "page_idx": 4}, {"type": "text", "text": "The words in italics are the prompts that not only aid in reconstructing free-formed texts, but also provide a more relevant and comprehensive description of each protein. ", "page_idx": 4}, {"type": "text", "text": "Modality feature extraction During the training stage, the pretrained PLM $f_{\\phi}$ and BLM $f_{\\psi}$ are used to initialize the representations of protein sequence $f_{\\phi}(S)$ and the textual descriptions $f_{\\psi}(\\tilde{T})$ , respectively. We freeze the weights of both the PLM and BLM due to the expensive computational cost and design subsequent learnable modules to adapt these models for our task. ", "page_idx": 4}, {"type": "text", "text": "Specifically, $M$ reconstructed textual descriptions are fed into BLM: ", "page_idx": 4}, {"type": "equation", "text": "$$\nf_{\\psi}(\\tilde{T})=\\{f_{\\psi}(\\tilde{\\mathrm{t}}_{1}),f_{\\psi}(\\tilde{\\mathrm{t}}_{2}),\\cdots\\,,f_{\\psi}(\\tilde{\\mathrm{t}}_{M})\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $f_{\\psi}(\\tilde{\\mathrm{t}}_{i})\\in\\mathbb{R}^{l_{i}\\times d}$ , and the truncated length of tokenized sequence $l_{i}$ is not same for different attributes for saving computational resources (details can be found in Appendix B.2). Given the hierarchical characteristic of textual descriptions, we design a Multi-Attribute Cross-attention (MACross) module to capture the relationship among attributes. Intuitively, the Function attribute is of the greatest importance due to the rich information it carries. Thus, we first incorporate the left $M-1$ attributes by [CLS] token [9] with inter-attribute attention to obtain ${\\pmb x}_{-F}^{\\mathrm{t}}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{x_{-F}^{\\mathrm{t}}=\\mathrm{Attention}(\\mathrm{Concat}(\\{f_{\\psi}(\\tilde{\\mathbf{t}}_{i})_{[\\mathrm{ClS}]}|1\\le i\\le M,\\mathrm{t}_{i}^{n}\\neq\\mathrm{Function}\\})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "while $\\pmb{x}_{F}^{\\mathrm{t}}:=\\,f_{\\psi}(\\tilde{\\mathrm{t}}_{i})$ where $\\mathrm{t}_{i}^{n}=$ Function. Then we employ multi-layer cross-attention [57] to query the information of $\\mathbf{\\nabla}_{\\mathbf{\\alpha}}\\mathbf{x}_{F}$ by $x_{-F}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{CrossAttention}(x_{-F}^{\\mathrm{t}},x_{F}^{\\mathrm{t}},x_{F}^{\\mathrm{t}})=\\mathrm{Softmax}(\\frac{\\mathbf{Q}_{-F}\\mathbf{K}_{F}^{\\top}}{\\sqrt{d}})\\mathbf{V}_{F},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{Q}_{-F}=\\mathbf{W}^{Q}\\pmb{x}_{-F}^{\\mathrm{t}};\\mathbf{K}_{F}=\\mathbf{W}^{K}\\pmb{x}_{F}^{\\mathrm{t}};\\mathbf{V}_{F}=\\mathbf{W}^{V}\\pmb{x}_{F}^{\\mathrm{t}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ${\\bf W}^{Q}$ , $\\mathbf{W}^{K}$ and $\\mathbf{W}^{V}$ refer to learnable transformation matrices, and $d$ refers to the dimension of each attention head. Similar to [28], we employ a residual connection by a fully connected feed-forward network (FFN) as shown in Figure 2. ", "page_idx": 4}, {"type": "text", "text": "Cross-modal soft-label alignment In order to utilize the complementary knowledge of amino acid sequence and textual descriptions to improve the performance and robustness. Inspired by [29], we employ the \u201cFirst Align, Then Fuse\u201d strategy to fuse two aligned modalities to predict the amino acid-level target (i.e., the active sites) of protein. Assume that the output of the sequence and textual description after feature extraction are $z^{\\mathrm{s}}$ and $z^{\\mathrm{t}}$ , respectively, although both of them point to the same protein, the divergence still exists. Therefore, firstly we use a Shared Transformer Encoder (STEnc, consisting of multiple Transformer encoders) to early-map $z^{\\mathrm{s}}$ and $z^{\\mathrm{t}}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{z}^{\\mathrm{s}}=\\mathrm{STEnc}(z^{\\mathrm{s}});\\tilde{z}^{\\mathrm{t}}=\\mathrm{STEnc}(z^{\\mathrm{t}}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In order to align paired sequence and textual descriptions, it is common practice to use contrastive learning based on InfoNCE loss [38]. This method \u201cpulls close\u201d the paired samples (\u201cpositive pair\u201d) as \u201cpushes away\u201d the unpaired samples (\u201cnegative pair\u201d) in the high-dimension space by maximising mutual information between two modalities in self-supervised manner. However, in our case there may be a potential semantic association between unpaired sequence and textual descriptions in the same batch due to the principle \u201cSimilar protein sequences give rise to similar structures and functions\u201d in biology (Appendix C). Inspired by [20], we adopt cross-modal soft-label alignment to make one-hot label continuous.2 Specifically, the cosine similarity between $\\tilde{z}_{i}^{\\mathrm{s}}$ and $\\tilde{z}_{j}^{\\mathrm{t}}$ is denoted as $s_{i j}^{\\mathrm{s2t}}$ , and the cosine similarity between $\\tilde{z}_{i}^{\\mathrm{s}}$ and $\\tilde{z}_{j}^{\\mathrm{s}~3}$ is denoted as $r_{i j}^{\\mathrm{s2s}}$ , and $r_{i j}^{\\mathrm{t2t}}$ is defined in the same way. P isj2sr epresents the semantic consistency within the same modality which is calculated in a softmax-like manner: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\nP_{i j}^{\\mathrm{s2s}}=\\frac{\\exp(r_{i j}^{\\mathrm{s2s}})}{\\sum_{k=1}^{|\\mathcal{B}|}\\exp(r_{i k}^{\\mathrm{s2s}})},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $|\\beta|$ is batch size. Actual distribution $Q_{i j}^{\\mathrm{s2t}}$ represents the probability that $\\tilde{z}_{i}^{\\mathrm{s}}$ matches $\\tilde{z}_{j}^{\\mathrm{t}}$ , which is hoped to align with $P_{i j}^{\\mathrm{s2s}}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\nQ_{i j}^{\\mathrm{s2t}}=\\frac{\\exp(s_{i j}^{\\mathrm{s2t}}/\\tau)}{\\sum_{k=1}^{|\\mathcal{B}|}\\exp(s_{i k}^{\\mathrm{s2t}}/\\tau)},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\tau$ is temperature. Thus, the loss function in the Align phase can be calculated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{Align}}=\\frac{1}{2|\\mathcal{B}|}{\\sum_{i=1}^{|\\mathcal{B}|}(D_{K L}(P_{i}^{\\mathrm{s2s}}\\|Q_{i}^{\\mathrm{s2t}})+D_{K L}(P_{i}^{\\mathrm{t2t}}\\|Q_{i}^{\\mathrm{t2s}}))}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We note that the parameters of the sequence branches are frozen, resulting in the alignment of the textual feature space to that of the sequence feature space. ", "page_idx": 5}, {"type": "text", "text": "Multi-modal fusion and active sites identification In the Fuse phase, a multi-head cross-attention strategy is utilized again to integrate the protein and text modalities, where $\\tilde{z}^{\\mathrm{s}}$ serves as \u201cquery\u201d while $\\tilde{z}^{\\mathrm{t}}$ serves as both \u201ckey\u201d and \u201cvalue\u201d. This setup enables the network to develop a comprehensive representation of queried text modality by sequence. Consequently, the model not only encodes protein-related knowledge but also retains insights derived from textual data, fostering a holistic comprehension. Subsequently, the unmapped $z^{\\mathrm{s}}$ and the features deviated by Fusion Attention are concatenated before the prediction layer. Finally, the model employs Cross Entropy Loss, denoted as $\\mathcal{L}_{\\mathrm{ASP}}$ , to predict active sites. ", "page_idx": 5}, {"type": "text", "text": "Although during the training stage, both sequence and text modalities are used to predict active sites, in practice, when dealing with newly discovered proteins, we might lack its high-quality textual annotations. To address this issue, we employ a state-of-the-art biomedical text generation method, such as Prot2Text [1], as an agent to complete the missing text modality. Some examples of generated texts are shown in Appendix B.4. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experiment setups ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Dataset description To obtain comprehensive and accurate protein-text data, we build the ProTAD with 570,830 samples from Swiss-Prot in UniProt4 [6] after data cleaning and filtering. ProTAD includes the amino acid sequence and textual descriptions for each sample, covering 17 attributes such as Protein Name, Organism, Function, Caution, etc. as described in Appendix A.1. These descriptions are rigorously checked so that the proteins can be accurately described. Due to some proteins lacking certain attribute annotations, we select those pairs of samples with at least six attributes (of which the Function attribute is required) in our experiments. To ensure a fair comparison with other methods when conducting experiments in Section 4.2.1, we fliter the proteins those could obtain tertiary structures in AlphaFold DB [24] [56]. In order to prevent the potential data leakage, following CLEAN [65], we cluster the data using MMseqs2 [35] with different sequence identity thresholds (in our settings they are $10\\%$ , $30\\%$ , $50\\%$ , $70\\%$ , and $90\\%$ ) to avoid the test sequences from being too similar to the training data. After that, we develop a cluster-guarantee approach and employ $k$ -selected strategy to construct an $8:1:1$ split dataset. The detailed preparation process can be found in Appendix A.2. ", "page_idx": 5}, {"type": "text", "text": "Implementation details Our implementation is based on PyTorch version 1.13.1, and models are trained using a single NVIDIA GeForce RTX 4090 GPU with 24GB of memory. The MMSite model in Table 1 requires approximately 7 hours to train. We retain the original feature dimensions of each PLM encoder and BLM encoder to preserve more inherent information, e.g., 1280 dimensions for ESM-2-650M and 768 dimensions for PubMedBERT-abs [16]. The maximum sequence length is ", "page_idx": 5}, {"type": "table", "img_path": "XHdwlbNSVb/tmp/c8da649c67c3cbd2fe859e76ed68a62f14d8fa758b70a7e2760b35c854533ab7.jpg", "table_caption": ["Table 1: Comparison on the dataset with clustering threshold at $10\\%$ compared with other 21 PRL models. All results are reported as mean $(\\pm2\\sigma)$ . Abbr., Seq.: Sequence; Struct.: Structure. "], "table_footnote": ["\u2020 This column refers to the modality input in the inference stage. \u2021 We report the performance using ESM-1b and PubMedBERT-abs as the PLM and BLM encoders. "], "page_idx": 6}, {"type": "text", "text": "512, and extra amino acids will be removed. Details on the truncated length for each attribute can be found in Appendix B.2. We set the hyperparameter $\\tau$ to 0.8. In MACross, we use 2-layer Transformer encoder to extract inter-attribute relations, with the number of cross-attention blocks set to 4. We also adopt 4-layer 8-head Transformer in STEnc. Before the final MLP predictor, an additional 2-layer attention mechanism integrates the original sequence modality with the fused feature. The dropout rates of all above components is consistently set at 0.1. The model undergoes a 15-epoch Align phase and a 50-epoch Fuse phase, using the Adam optimizer with a learning rate of 5e-5. We implement a warm-up phase comprising $\\textstyle{\\frac{1}{10}}$ of the total steps, followed by a cosine annealing scheduler for the remaining steps. The batch size is set to 24 for both training and inference stages. We consider those sites as predicted active sites whose result, after the Sigmoid function of the MLP predictor output, is greater than 0.5. ", "page_idx": 6}, {"type": "text", "text": "4.2 Protein active sites identification ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.2.1 Comparison with baselines ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Settings To compare with existing methods, we select 21 state-of-the-art PRL models as baselines. Their original weights are frozen, and residue-level features are obtained followed by 4-layer Transformer for prediction. Some of them utilize not only sequences but also combinations of sequences with structures, and sequences with text. Specifically, for models like MIF and PST, we obtain tertiary structures for each protein from AlphaFold DB. For ProtST, we perform comparisons w/ and w/o retraining on ProTAD. For MMSite, Prot2Text [1] serves as the agent model in the inference stage. Evaluation metrics include token-level $\\mathrm{F_{max}}$ , AUPRC, and MCC, following the implementation described in [21], as well as region-level OS (Overlap Score) and FPR (False Positive Rate) as defined in [8]. We save the checkpoint at the epoch with the best $\\mathrm{F_{max}}$ in validation set. Results are reported in Table 1 for the dataset with clustering threshold at $10\\%$ , using several different seeds, where ESM-1b and PubMedBERT-abs are used as the PLM and BLM encoders to initialize features. Some visualisation results are presented in Figure 3 and Appendix F. ", "page_idx": 6}, {"type": "image", "img_path": "XHdwlbNSVb/tmp/a2d85273ec6c61d02768a29a3eaf0046c8f47b6f3fa67a2fbe544536718807fc.jpg", "img_caption": ["Figure 3: Visualisation of an example of active site identification result for the protein Tyrosine recombinase XerC (UniProt ID: Q039E1). The palecyan surface/sticks (residues) represent the background, while the green, blue, and red surface/sticks (residues) indicate the correctly predicted sites, unpredicted sites, and incorrectly predicted sites, respectively. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Results and discussions The results demonstrate that MMSite outperforms individual models that only use residue sequences as input, achieving state-of-the-art performance across all metrics. Among the comparisons, S-PLM, which incorporates contrastive learning between sequences and structures during pretraining, performs slightly better than other Seq.-input methods, showing the potential of incorporating structural data. Nonetheless, MMSite still outperforms the model with Seq. & Struct. as input. Regarding ProtST w/ and w/o retrain, limited utilization of textual information in retraining process and reduced data volume for downstream application lead to decreased performance. ", "page_idx": 7}, {"type": "text", "text": "4.2.2 BLM enhances PLM\u2019s performance ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Evolutionary information is crucial in the identification of function sites, as these sites often preserve conserved patterns and structural motifs across species or among homologous proteins. In Table 2, we compare three state-of-the-art evolutionary-scale models (e.g., ESM-1b, ESM-1v, and ESM-2-650M) that are used as encoders for protein sequences. Additionally, BLMs such as PubMedBERT-abs and PubMedBERT-full serve as biological text encoders. It is evident that the utilization of BLM has resulted in significant enhancements of the performance of PLM. ", "page_idx": 7}, {"type": "table", "img_path": "XHdwlbNSVb/tmp/a7b8a7538658748b48a6755fbc6b2b1f1a797653ca89691a7814cdfd058c9647.jpg", "table_caption": ["Table 2: Performance improvement with the addition of BLM compared to using PLM as input only. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "XHdwlbNSVb/tmp/a93a4064ada0f8a37a1e49e7585ea081eba13439253aced4a1a9801fc0e416d5.jpg", "img_caption": ["Figure 4: Impact of clustering threshold on model performance. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.3 Ablation study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "4.3.1 Impact of different clustering thresholds ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In order to study the impact of identity thresholds on clustering using MMseqs2 during data partition, we set thresholds at $10\\%$ , $30\\%$ , $50\\%$ , $70\\%$ , and $90\\%$ for comparison, and the results are shown in Figure 4, where ESM-1b and PubMedBERT-abs serve as the PLM and BLM encoders respectively. Detailed quantitative comparisons for the cases of $30\\%$ and $50\\%$ are provided in Appendix D.4. It is clearly that with the increase of clustering threshold, all the performances of each metric are improved. Especially when the threshold is changed from $30\\%$ to $50\\%$ , the improvement of the model is especially obvious, and when it reaches $90\\%$ , the performance approaches near perfection. ", "page_idx": 7}, {"type": "text", "text": "4.3.2 Effectiveness of components ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To figure out the contribution of each component within the MMSite framework to overall model performance, we conduct ablation experiments for each of them. The results are presented in Table 3, where \u201cSeq-M\u201d and \u201cText-M\u201d refer to the sequence modality and the text modality, respectively. It can be found that both STEnc and MACross contribute to the performance improvement, and the Align mechanism helps more obviously by aligning the text modality closer to the sequence modality. Moreover, the importance of the text modality is evident in the last row, which shows an average decline of 0.105 across all metrics when the text modality is removed, compared to MMSite. ", "page_idx": 8}, {"type": "table", "img_path": "XHdwlbNSVb/tmp/c61ef83cc94b4e284328ca93588b57f9b98fe9ef89a01277fdd506739d8a4f33.jpg", "table_caption": ["Table 3: Evaluation of the effectiveness of each component in MMSite "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3.3 Single-stage vs. two-stage ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our framework employs a two-stage training strategy, \u201cFirst Align, Then Fuse\u201d, and we also compare it with a single-stage strategy \u201cAlign While Fusing\u201d. The total loss for the single-stage strategy is calculated as $\\mathcal{L}_{\\mathrm{total}}\\,=\\,\\mathcal{L}_{\\mathrm{Align}}\\,+\\,\\alpha\\mathcal{L}_{\\mathrm{ASP}}$ , where $\\alpha$ is set to the best performing 0.7 after many attempts. The comparative results are presented in Table 4. It is clear that the two-stage strategy outperforms the single-stage approach, which is more challenging due to its multi-objective nature and the potential conflicts between different objectives. ", "page_idx": 8}, {"type": "table", "img_path": "XHdwlbNSVb/tmp/ffd538effb3ef40a6f5125eba9380ba7dc87005a111990c3bb48048c78d7be5d.jpg", "table_caption": ["Table 4: Comparison between the single-stage and the two-stage strategy. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3.4 Ablation of the attribute selection ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "MACross module is designed based on that the Function attribute is the most relevant attribute for predicting active sites and contains the richest information. The additional attributes, although seemingly insignificant, also contribute to improve active site predictions. Table 5 below shows the performance comparison between using only the Function attribute and using all attributes, demonstrating that incorporating all attributes leads to better performance. ", "page_idx": 8}, {"type": "table", "img_path": "XHdwlbNSVb/tmp/d8d53029a2992491db5052e2077868cd1d24bc628d0f963702e4f94b8e443cec.jpg", "table_caption": ["Table 5: Performance comparison between using all attributes and only Function attribute. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3.5 Ablation of manual prompting ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To evaluate the effectiveness of manual prompting, we conducted experiments to test the impact of removing manual prompts in Table 6. The results show that manual prompting improves performance on most metrics. We believe this is because: (1) Complete sentences provide richer context for the ", "page_idx": 8}, {"type": "text", "text": "BERT-based BLM; (2) It reduces ambiguity in attribute meanings; (3) It aligns better with BLM\u2019s pretraining, leveraging its knowledge more effectively. ", "page_idx": 9}, {"type": "table", "img_path": "XHdwlbNSVb/tmp/3b2338773df7ab85cd48966590e9ba57f82955577d7489f47697942ec83c1c4f.jpg", "table_caption": ["Table 6: Ablation experiment of manual prompting. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "4.3.6 Other ablation studies ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In MACross, the Function and the remaining 16 attributes are utilized as K & V, and Q respectively in Cross Attention to query $\\displaystyle{\\mathbf{\\nabla}}x_{F}$ . We attempt to swap their positions for comparison (i.e., $x_{F}$ serves as \u201cquery\u201d, $x_{-F}$ servers as \u201ckey\u201d and \u201cvalue\u201d). Additionally, we also try to replace soft-label alignment with hard-label alignment (similar to InfoNCE). The results of their average performance on token-level and region-level are shown in Table 7. To compare the results, we replace FPR with $1-\\mathrm{FPR}$ when calculating Avg. (region). MMSite performs best compared to the other two scenarios. We also investigated the impact of varying the hyperparameter $\\tau$ in soft-label alignment from 0.2 to 2.0 in Figure 5, because it directly determines the information entropy of the target distribution $Q$ in the Align phase. The results shows that the model is relative optimal in both token-level and region-level metrics when $\\tau$ is set to 0.8. ", "page_idx": 9}, {"type": "table", "img_path": "XHdwlbNSVb/tmp/c5cf0eb6a9855b2ac53cd9ce2ecc5cf7f7a4d6fd6f76776b7167d01593a7be8a.jpg", "table_caption": ["Table 7: Comparison between MMSite and other two scenarios. "], "table_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "XHdwlbNSVb/tmp/d03de695b4cbd38b9ef631a280403fcde75326e671a13ab78a6ddb164add2894.jpg", "img_caption": ["Figure 5: Performance on different $\\tau$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "4.4 Temporal-based evaluation ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To more accurately reflect real-world scenarios in scientific applications, we conducted an extra experiment simulating the discovery of new proteins. We collected data from UniProt database recorded after March 11, 2024 as newly discovered proteins (115 samples) to evaluate the model\u2019s performance. The results in Table 8 show that MMSite maintains well performance even on newly discovered proteins. ", "page_idx": 9}, {"type": "table", "img_path": "XHdwlbNSVb/tmp/c9d04235bfb97bce9908c7f1af946664f7dc5dd6833859b843721a1a6e439d19.jpg", "table_caption": ["Table 8: Performance evaluation on the newly discovered proteins. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Due to space limitations, further experiments results on performance comparisons, text quality, protein sequence input, inference performance, and related aspects are included in the Appendix D. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we build the ProTAD dataset that contains detail textual descriptions of proteins, and propose the MMSite framework to identify the active sites in proteins, which is crucial for understanding proteins in residue-level, designing new drugs and so on. MMSite takes both sequence and text as input in training stage, and adopts \u201cFirst Align, Then Fuse\u201d strategy to align the text representation to the sequence, enhancing PLM\u2019s performance in active sites identification. We adopt manual prompting and a designed MACross module to handle the multi-attribute descriptions, and adopt soft-label alignment in the Align phase. Extensive experimental validations have been conducted to evaluate the effectiveness of our method, showing the potential of multi-modal learning in computational biology. From experience, the method works best when ESM-1b and PubMedBERTabs are chosen as the PLM and BLM, Prot2Text is used as the agent model. We also discussed the limitations and broader impacts in Appendix E. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work is supported by the National Key Research and Development Program of China (2023YFC2705700), the National Natural Science Foundation of China (Grant No. 62225113, 62272354, 62276195 and U23A20318), the Science and Technology Major Project of Hubei Province under Grant 2024BAB046, and the Innovative Research Group Project of Hubei Province under Grant 2024AFA017. The numerical calculations in this paper have been done on the supercomputing system in the Supercomputing Center of Wuhan University. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Hadi Abdine, Michail Chatzianastasis, Costas Bouyioukos, and Michalis Vazirgiannis. Prot2text: Multimodal protein\u2019s function generation with GNNs and transformers. In NeurIPS 2023 AI for Science Workshop, 2023.   \n[2] Nadav Brandes, Dan Ofer, Yam Peleg, Nadav Rappoport, and Michal Linial. ProteinBERT: a universal deep-learning model of protein sequence and function. Bioinformatics, 38(8):2102\u20132110, 02 2022.   \n[3] Dexiong Chen, Philip Hartout, Paolo Pellizzoni, Carlos Oliver, and Karsten Borgwardt. Endowing protein language models with structural knowledge. arXiv preprint arXiv:2401.14819, 2024.   \n[4] Kevin Clark, Minh-Thang Luong, QuocV. Le, and ChristopherD. Manning. Electra: Pre-training text encoders as discriminators rather than generators. arXiv: Computation and Language,arXiv: Computation and Language, Mar 2020. [5] Ana Conesa, Stefan G\u00f6tz, Juan Miguel Garc\u00eda-G\u00f3mez, Javier Terol, Manuel Tal\u00f3n, and Montserrat Robles. Blast2go: a universal tool for annotation, visualization and analysis in functional genomics research. Bioinformatics, page 3674\u20133676, Sep 2005. [6] The UniProt Consortium. UniProt: the Universal Protein Knowledgebase in 2023. Nucleic Acids Research, 51(D1):D523\u2013D531, 11 2022. [7] Alperen Dalkiran, Ahmet Sureyya Rifaioglu, Maria Jesus Martin, Rengul Cetin-Atalay, Volkan Atalay, and Tunca Dog\u02d8an. Ecpred: a tool for the prediction of the enzymatic functions of protein sequences based on the ec nomenclature. BMC Bioinformatics, Dec 2018.   \n[8] Lo\u00efc Kwate Dassi, Matteo Manica, Daniel Probst, Philippe Schwaller, Yves Gaetan Nana Teukam, and Teodoro Laino. Identification of enzymatic active sites with unsupervised language modeling. In NeurIPS 2021 AI for Science Workshop, 2021.   \n[9] Jacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North, Jan 2019.   \n[11] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: an embodied multimodal language model. In Proceedings of the 40th International Conference on Machine Learning, ICML\u201923. JMLR.org, 2023.   \n[12] Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, Debsindhu Bhowmik, and Burkhard Rost. Prottrans: Toward understanding the language of life through self-supervised learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(10):7112\u20137127, 2022.   \n[13] Yitian Fang, Yi Jiang, Leyi Wei, Qin Ma, Zhixiang Ren, Qianmu Yuan, and Dong-Qing Wei. Deepprosite: structure-aware protein binding site prediction using esmfold and pretrained language model. Bioinformatics, 39(12):btad718, 2023.   \n[14] P. Gainza, F. Sverrisson, F. Monti, E. Rodol\u00e0, D. Boscaini, M. M. Bronstein, and B. E. Correia. Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning. Nature Methods, page 184\u2013192, Feb 2020.   \n[15] Joachim Giard, Patrice Rondao Alface, and Beno\u00eet Macq. Fast and accurate travel depth estimation for protein active site prediction. In Image Processing: Algorithms and Systems VI, volume 6812, pages 237\u2013246. SPIE, 2008.   \n[16] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare, page 1\u201323, Jan 2022.   \n[17] Pedro Hermosilla, Marco Sch\u00e4fer, Mate\u02c7j Lang, Gloria Fackelmann, Pere Pau V\u00e1zquez, Barbora Kozl\u00edkov\u00e1, Michael Krone, Tobias Ritschel, and Timo Ropinski. Intrinsic-extrinsic convolution and pooling for learning on 3d protein structures. International Conference on Learning Representations, 2021.   \n[18] Chloe Hsu, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu, Adam Lerer, and Alexander Rives. Learning inverse folding from millions of predicted structures. ICML, 2022.   \n[19] Mingyang Hu, Fajie Yuan, KevinK. Yang, Fusong Ju, Jin Su, Hui Wang, Fei Yang, and Qiuyang Ding. Exploring evolution-based & -free protein language models as protein function predictors. Jun 2022.   \n[20] Hailang Huang, Zhijie Nie, Ziqiao Wang, and Ziyu Shang. Cross-modal and uni-modal soft-label alignment for image-text retrieval. Proceedings of the AAAI Conference on Artificial Intelligence, 38(16):18298\u2013 18306, Mar. 2024.   \n[21] Peishun Jiao, Beibei Wang, Xuan Wang, Bo Liu, Yadong Wang, and Junyi Li. Struct2GO: protein function prediction based on graph pooling algorithm and AlphaFold2 structure information. Bioinformatics, 39(10):btad637, 10 2023.   \n[22] Bowen Jing, Stephan Eismann, Patricia Suriana, RaphaelJ.L. Townshend, and RonO. Dror. Learning from protein structure with geometric vector perceptrons. Learning,Learning, Sep 2020.   \n[23] Linglin Jing, Sheng Xu, Yifan Wang, Yuzhe Zhou, Tao Shen, Zhigang Ji, Hui Fang, Zhen Li, and Siqi Sun. Crossbind: Collaborative cross-modal identification of protein nucleic-acid-binding residues. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 2661\u20132669, 2024.   \n[24] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin \u017d\u00eddek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583\u2013589, 2021.   \n[25] Tsuyoshi Kato and Nozomi Nagano. Discriminative structural approaches for enzyme active-site prediction. BMC bioinformatics, 12:1\u20138, 2011.   \n[26] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv: Computation and Language,arXiv: Computation and Language, Sep 2019.   \n[27] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.   \n[28] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.   \n[29] Junnan Li, RamprasaathR. Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and StevenC.H. Hoi. Align before fuse: Vision and language representation learning with momentum distillation. Cornell University - arXiv,Cornell University - arXiv, Jul 2021.   \n[30] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, et al. Language models of protein sequences at the scale of evolution enable accurate structure prediction. bioRxiv, 2022.   \n[31] Di Lu, Leonardo Neves, Vitor Carvalho, Ning Zhang, and Heng Ji. Visual attention model for name tagging in multimodal social media. In Iryna Gurevych and Yusuke Miyao, editors, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1990\u20131999, Melbourne, Australia, July 2018. Association for Computational Linguistics.   \n[32] Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Yushuai Wu, Mu Qiao, and Zaiqing Nie. Biomedgpt: Open multimodal generative pre-trained transformer for biomedicine, 2023.   \n[33] Liuzhenghao Lv, Zongying Lin, Hao Li, Yuyang Liu, Jiaxi Cui, Calvin Yu-Chian Chen, Li Yuan, and Yonghong Tian. Prollama: A protein large language model for multi-task protein language processing. arXiv preprint arXiv:2402.16445, 2024.   \n[34] Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alexander Rives. Language models enable zero-shot prediction of the effects of mutations on protein function. bioRxiv, 2021.   \n[35] M Mirdita, M Steinegger, F Breitwieser, J S\u00f6ding, and E Levy Karin. Fast and sensitive taxonomic assignment to metagenomic contigs. Bioinformatics, 37(18):3029\u20133031, 03 2021.   \n[36] Seungwhan Moon, Leonardo Neves, and Vitor Carvalho. Multimodal named entity recognition for short social media posts. In Marilyn Walker, Heng Ji, and Amanda Stent, editors, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 852\u2013860, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.   \n[37] Stelios K Mylonas, Apostolos Axenopoulos, and Petros Daras. Deepsurf: a surface-based deep learning approach for the prediction of ligand binding sites on proteins. Bioinformatics, 37(12):1681\u20131690, 2021.   \n[38] Aaronvanden Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. Cornell University - arXiv,Cornell University - arXiv, Jul 2018.   \n[39] OpenAI OpenAI. Gpt-4 technical report. Mar 2023.   \n[40] Qizhi Pei, Lijun Wu, Kaiyuan Gao, Xiaozhuan Liang, Yin Fang, Jinhua Zhu, Shufang Xie, Tao Qin, and Rui Yan. Biot5 $^+$ : Towards generalized biological understanding with iupac integration and multi-task tuning. arXiv preprint arXiv:2402.17810, 2024.   \n[41] Qizhi Pei, Lijun Wu, Kaiyuan Gao, Jinhua Zhu, Yue Wang, Zun Wang, Tao Qin, and Rui Yan. Leveraging biomolecule and natural language through multi-modal learning: A survey. ArXiv, abs/2403.01528, 2024.   \n[42] Qizhi Pei, Wei Zhang, Jinhua Zhu, Kehan Wu, Kaiyuan Gao, Lijun Wu, Yingce Xia, and Rui Yan. BioT5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1102\u20131123, Singapore, December 2023. Association for Computational Linguistics.   \n[43] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and PeterJ. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv: Learning,arXiv: Learning, Oct 2019.   \n[44] Roshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Xi Chen, John Canny, Pieter Abbeel, and Yun S Song. Evaluating protein transfer learning with tape. In Advances in Neural Information Processing Systems, 2019.   \n[45] Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C Lawrence Zitnick, Jerry Ma, et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences, 118(15):e2016239118, 2021. bioRxiv 10.1101/622803.   \n[46] Sriram Sankararaman, Fei Sha, Jack F Kirsch, Michael I Jordan, and Kimmen Sj\u00f6lander. Active site prediction using evolutionary and structural information. Bioinformatics, 26(5):617\u2013624, 2010.   \n[47] Martin Steinegger, Milot Mirdita, and Johannes S\u00f6ding. Protein-level assembly increases protein sequence recovery from metagenomic samples manyfold. Aug 2018.   \n[48] Martin Steinegger and Johannes S\u00f6ding. Clustering huge protein sequence sets in linear time. Jan 2017.   \n[49] Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, and Fajie Yuan. Saprot: Protein language modeling with structure-aware vocabulary. bioRxiv, pages 2023\u201310, 2023.   \n[50] Baris E. Suzek, Yuqi Wang, Hongzhan Huang, Peter B. McGarvey, and Cathy H. Wu. Uniref clusters: a comprehensive and scalable alternative for improving sequence similarity searches. Bioinformatics, page 926\u2013932, Mar 2015.   \n[51] Freyr Sverrisson, Jean Feydy, Bruno E. Correia, and Michael M. Bronstein. Fast end-to-end learning on protein surfaces. Dec 2020.   \n[52] Yang Tan, Mingchen Li, Pan Tan, Ziyi Zhou, Huiqun Yu, Guisheng Fan, and Liang Hong. Peta: Evaluating the impact of protein transfer learning with sub-word tokenization on downstream applications, 2023.   \n[53] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpaca: A strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html, 3(6):7, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "[54] Hugo Touvron and et al. Martin. Llama 2: Open foundation and fine-tuned chat models. ", "page_idx": 13}, {"type": "text", "text": "[55] J\u00e9r\u00f4me Tubiana, Dina Schneidman-Duhovny, and Haim J Wolfson. Scannet: an interpretable geometric deep learning model for structure-based protein binding site prediction. Nature Methods, 19(6):730\u2013739, 2022.   \n[56] Mihaly Varadi, Stephen Anyango, Mandar Deshpande, Sreenath Nair, Cindy Natassia, Galabina Yordanova, David Yuan, Oana Stroe, Gemma Wood, Agata Laydon, Augustin \u017d\u00eddek, Tim Green, Kathryn Tunyasuvunakool, Stig Petersen, John Jumper, Ellen Clancy, Richard Green, Ankur Vora, Mira Lutf,i Michael Figurnov, Andrew Cowie, Nicole Hobbs, Pushmeet Kohli, Gerard Kleywegt, Ewan Birney, Demis Hassabis, and Sameer Velankar. AlphaFold Protein Structure Database: massively expanding the structural coverage of protein-sequence space with high-accuracy models. Nucleic Acids Research, 50(D1):D439\u2013D444, 11 2021.   \n[57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, AidanN. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Neural Information Processing Systems,Neural Information Processing Systems, Jun 2017.   \n[58] Duolin Wang, Mahdi Pourmirzaei, Usman L Abbas, Shuai Zeng, Negin Manshour, Farzaneh Esmaili, Biplab Poudel, Yuexu Jiang, Qing Shao, Jin Chen, and Dong Xu. S-plm: Structure-aware protein language model via contrastive learning between sequence and structure. bioRxiv, 2024.   \n[59] Zeyuan Wang, Qiang Zhang, Keyan Ding, Ming Qin, Xiang Zhuang, Xiaotong Li, and Huajun Chen. Instructprotein: Aligning human and protein language via knowledge instruction. Oct 2023.   \n[60] Ying Xia, Chun-Qiu Xia, Xiaoyong Pan, and Hong-Bin Shen. Graphbind: protein structural context embedded rules learned by hierarchical graph neural networks for recognizing nucleic-acid-binding residues. Nucleic acids research, 49(9):e51\u2013e51, 2021.   \n[61] Minghao Xu, Xinyu Yuan, Santiago Miret, and Jian Tang. Protst: Multi-modality learning of protein sequences and biomedical texts. Jan 2023.   \n[62] Kevin K Yang, Niccol\u00f2 Zanichelli, and Hugh Yeh. Masked inverse folding with sequence transfer for protein representation learning. Protein Engineering, Design and Selection, 36:gzad015, 10 2022.   \n[63] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc Le. Xlnet: Generalized autoregressive pretraining for language understanding.   \n[64] Aqsa Yousaf, Tahira Shehzadi, Aqeel Farooq, and Komal Ilyas. Protein active site prediction for early drug discovery and designing. International Review of Applied Sciences and Engineering, 13(1):98\u2013105, 2021.   \n[65] Tianhao Yu, Haiyang Cui, JiananCanal Li, Yunan Luo, Guangde Jiang, and Huimin Zhao. Enzyme function prediction using contrastive learning. Nature, Mar 2023.   \n[66] Ningyu Zhang, Zhen Bi, Xiaozhuan Liang, Siyuan Cheng, Haosen Hong, Shumin Deng, Jiazhang Lian, Qiang Zhang, and Huajun Chen. Ontoprotein: Protein pretraining with gene ontology embedding.   \n[67] Zuobai Zhang, Chuanrui Wang, Minghao Xu, Vijil Chenthamarakshan, Aur\u00e9lie Lozano, Payel Das, and Jian Tang. A systematic study of joint representation learning on protein sequences and structures. arXiv preprint arXiv:2303.06275, 2023.   \n[68] Zuobai Zhang, Minghao Xu, Arian Jamasb, Vijil Chenthamarakshan, Aurelie Lozano, Payel Das, and Jian Tang. Protein representation learning by geometric structure pretraining. ICLR 2023.   \n[69] Zuobai Zhang, Minghao Xu, Aur\u00e9lie Lozano, Vijil Chenthamarakshan, Payel Das, and Jian Tang. Physicsinspired protein encoder pre-training via siamese sequence-structure diffusion trajectory prediction. Jan 2023. ", "page_idx": 13}, {"type": "text", "text": "A Dataset details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Description ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Taking the protein sequence P05117 (the UniProt ID) as an example, below are the raw textual descriptions of its attributes from our ProTAD dataset, which cover 17 different attributes such as Protein Name, Organism, Taxonomic Lineage, etc. These descriptions comprehensively and accurately reflect the characteristics of the protein. It is worth noting that some proteins in our dataset have missing attributes, such as Caution, Allergenic Properties, Pharmaceutical use and Involvement in Disease for P05117. ", "page_idx": 14}, {"type": "text", "text": "Raw Textual Descriptions of P05117 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "\u2022 Protein Name: Polygalacturonase-2 (PG) (PG-2A) (PG-2B) (Pectinase)   \n\u2022 Organism: Solanum lycopersicum (Tomato) (Lycopersicon esculentum)   \n\u2022 Taxonomic Lineage: cellular organisms (no rank), Eukaryota (superkingdom), Viridiplantae (kingdom), Streptophyta (phylum), Streptophytina (subphylum), Embryophyta (no rank), Tracheophyta (no rank), Euphyllophyta (no rank), Spermatophyta (no rank), Magnoliopsida (class), Mesangiospermae (no rank), eudicotyledons (no rank), Gunneridae (no rank), Pentapetalae (no rank), asterids (no rank), lamiids (no rank), Solanales (order), Solanaceae (family), Solanoideae (subfamily), Solaneae (tribe), Solanum (genus), Solanum subgen. Lycopersicon (subgenus)   \n\u2022 Function: Catalytic subunit of the polygalacturonase isozyme 1 and 2 (PG1 and PG2). Acts in concert with the pectinesterase, in the ripening process. Is involved in cell wall metabolism, specifically in polyuronide degradation. The depolymerization and solubilization of cell wall polyuronides mediated by PG2 during ripening seems to be limited by the beta subunit GP1, probably by recruiting PG2 to form PG1.   \n\u2022 Caution: nan.   \n\u2022 Miscellaneous: To avoid liquid rheology of tomato juice, temperature and pressure can be increased to inactivate selectively PG2 during the process.   \n\u2022 Subunit Structure: Monomer PG2 (isoenzymes PG2A and PG2B). Also forms heterodimers called polygalacturonase 1 (PG1) with the beta subunit GP1.   \n\u2022 Induction: By ethylene.   \n\u2022 Tissue Specificity: Expressed only in ripening fruits (at protein level).   \n\u2022 Developmental Stage: PG1 appears when fruits start to be coloured. When fruits are orange, both PG2 and PG1 are present. In fully ripe fruit, mostly PG2 is expressed.   \n\u2022 Allergenic Properties: nan   \n\u2022 Biotechnological Use: The effect of PG can be neutralized by introducing an antisense PG gene by genetic manipulation. The Flavr Savr tomato produced by Calgene (Monsanto) in such a manner has a longer shelf life due to delayed ripening.   \n\u2022 Pharmaceutical Use: nan   \n\u2022 Involvement in Disease: nan   \n\u2022 Subcellular Location: Secreted, extracellular space, apoplast. Secreted, cell wall.   \n\u2022 Post-translational Modification: N-glycosylated. PG2B isozyme has a greater degree of glycosylation than PG2A.   \n\u2022 Sequence Similarities: Belongs to the glycosyl hydrolase 28 family. ", "page_idx": 14}, {"type": "text", "text": "To adapt to the model input and simultaneously establish the relationship between the attribute names and their content, we manually designed a unique prompt for each attribute to bridge the relationship between them. For missing attributes, we use the term \u201cunknown\u201d to complete the sentence. The reconstructed textual descriptions for P05117 are shown below. ", "page_idx": 14}, {"type": "text", "text": "Reconstructed Textual Descriptions of P05117 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "\u2022 The name of protein is Polygalacturonase-2 (PG) (PG-2A) (PG-2B) (Pectinase). \u2022 The organism is Solanum lycopersicum (Tomato) (Lycopersicon esculentum). \u2022 The taxonomic lineage of this protein includes cellular organisms (no rank), Eukaryota (superkingdom), Viridiplantae (kingdom), Streptophyta (phylum), Streptophytina (subphylum), Embryophyta (no rank), Tracheophyta (no rank), Euphyllophyta (no rank), Spermatophyta (no rank), Magnoliopsida (class), Mesangiospermae (no rank), eudicotyledons (no rank), Gunneridae (no rank), Pentapetalae (no rank), asterids (no rank), lamiids (no rank), Solanales (order), Solanaceae (family), Solanoideae (subfamily), Solaneae (tribe), Solanum (genus), Solanum subgen. Lycopersicon (subgenus). \u2022 The function of this protein includes: catalytic subunit of the polygalacturonase isozyme 1 and 2 ", "page_idx": 14}, {"type": "text", "text": "(PG1 and PG2); acts in concert with the pectinesterase, in the ripening process; is involved in cell wall metabolism, specifically in polyuronide degradation; the depolymerization and solubilization of cell wall polyuronides mediated by PG2 during ripening seems to be limited by the beta subunit GP1, probably by recruiting PG2 to form PG1.   \n\u2022 Caution includes unknown.   \n\u2022 Some miscellaneous things of this protein includes to avoid liquid rheology of tomato juice, temperature and pressure can be increased to inactivate selectively PG2 during the process.   \n\u2022 Subunit structure of this protein is monomer PG2 (isoenzymes PG2A and PG2B); also forms heterodimers called polygalacturonase 1 (PG1) with the beta subunit GP1.   \n\u2022 The induction of this protein includes by ethylene.   \n\u2022 The tissue specificity of this protein is expressed only in ripening fruits (at protein level).   \n\u2022 The developmental stage of this protein is PG1 appears when fruits start to be coloured. When fruits are orange, both PG2 and PG1 are present. In fully ripe fruit, mostly PG2 is expressed.   \n\u2022 The allergenic properties includes unknown.   \n\u2022 The biotechnological use includes the effect of PG can be neutralized by introducing an antisense PG gene by genetic manipulation. The Flavr Savr tomato produced by Calgene (Monsanto) in such a manner has a longer shelf life due to delayed ripening.   \n\u2022 The pharmaceutical use includes unknown.   \n\u2022 The diseases it could lead involves include unknown.   \n\u2022 This protein is usually located in subcellular secreted, extracellular space, apoplast. Secreted, cell wall.   \n\u2022 The post-translational modification of this protein includes N-glycosylated. PG2B isozyme has a greater degree of glycosylation than PG2A.   \n\u2022 The sequence similarities of this protein includes belongs to the glycosyl hydrolase 28 family. ", "page_idx": 15}, {"type": "text", "text": "A.2 Dataset preparation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we detail the construction of the dataset utilized in our experiments. We obtain sequences and their corresponding textual descriptions from Swiss-Prot in UniProt, with data up to March 11th, 2024, under the CC BY 4.0 License. To minimize the similarity between the test sequences and the training sequences, we follow the methodology described in CLEAN [65]. We cluster the data using MMseqs2 [35], applying different sequence identity thresholds to achieve this partition. Here are the steps we follow: ", "page_idx": 15}, {"type": "text", "text": "# Steq 1. Convert fasta file into mmseqs2 database file   \nmmseqs createdb sequences.fasta mm_db   \n# Step 2. Cluster sequences with identity threshold at 10%, 30%, 50%, $70\\%$ and $90\\%$ ,   \n# where the threshold variable \\${th} is 0.1, 0.3, 0.5, 0.7 or 0.9   \nmmseqs cluster mm_db mm_clusters results --min-seq-id \\${th} -c \\${th} --cov-mode 1   \n# Step 3. Extract clustered sequences to tsv file   \nmmseqs createtsv mm_db mm_db mm_clusters clusters.tsv ", "page_idx": 15}, {"type": "text", "text": "This methodology ensures a robust division between training set and test set by employing sequence identity thresholds to control the degree of similarity within clusters. After clustering the sequences, we develop a random-based method for splitting the dataset in an $8:1:1$ ratio (training:validation:test). This cluster-guarantee approach begins by randomly dividing the cluster centers in an $8:1:1$ ratio. Subsequently, we propose two strategies on each cluster to derive the final dataset: the $k$ -selected strategy and the random-ignore strategy. ", "page_idx": 15}, {"type": "text", "text": "With the $k$ -selected strategy, for each key-value cluster (where \u201ckey\u201d represents the center sequence of the cluster, and \u201cvalue\u201d includes all sequences within this cluster), we randomly select $\\bar{m}i n\\{k,l e n(v a l u e s)\\}$ sequences as the final samples. For the random-ignore strategy, we incorporate all sequences from each cluster. Both of these two strategies ensure that the final dataset splits are based on the original clustering, with the validation set and test set equally sharing the remaining clusters. ", "page_idx": 15}, {"type": "text", "text": "Practically, we implemented the $k$ -selected strategy (where $k$ is set to 1) to minimize redundancy and maximize the diversity of our dataset as much as possible. The sample sizes of the final dataset for each clustering threshold are detailed in Table 9. ", "page_idx": 15}, {"type": "table", "img_path": "XHdwlbNSVb/tmp/e9e50944a98d3c1971560224987693a4c0779427f10e04c374f0ddceb85b869e.jpg", "table_caption": ["Table 9: Statistics of samples in dataset for each clustering threshold. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B More experiment details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Evaluation metrics ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In our experiments, we use three important token-level and two region-level evaluation metrics to measure the prediction performance. Three token-level metrics are $\\mathbf{F_{max}}$ , AUPRC and MCC. Two region-level metrics are OS (Overlap Score) and FPR (False Positive Rate). ", "page_idx": 16}, {"type": "text", "text": "(1) Following [21], $\\mathbf{F_{max}}$ is defined by first calculating the precision and recall for each protein and then taking the average score. For a specific protein $i$ , the precision and recall are computed as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{precision}_{i}(\\theta)=\\frac{|P_{i}(\\theta)\\cap T_{i}|}{|P_{i}(\\theta)|},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{recall}_{i}(\\theta)=\\frac{|P_{i}(\\theta)\\cap T_{i}|}{|T_{i}|},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\theta$ is a hyperparameter to adjust the decision threshold, $T_{i}$ is the set of ground truth active sites for protein $i$ , $P_{i}(\\theta)$ is the set of predicted active sites by our model for protein $i$ , and $\\big|\\cdot\\big|$ denotes the size of the set. Then, the average precision and recall in a protein at threshold $\\theta$ are defined as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{precision}(\\theta)=\\frac{1}{M(\\theta)}\\sum_{i=1}^{M(\\theta)}\\mathrm{precision}_{i}(\\theta),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{recall}(\\theta)=\\frac{1}{N}\\sum_{i=1}^{N}\\mathrm{recall}_{i}(\\theta),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $N$ denotes the number of proteins, and $M(\\theta)$ denotes the number of proteins on which at least one is made above threshold $t$ , i.e., $\\left|P_{i}(\\theta)\\right|>0$ . ", "page_idx": 16}, {"type": "text", "text": "Combining these two measures, the maximum F-score is defined as the maximum value of the F-measure over all thresholds. That is, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{F}_{\\mathrm{max}}=\\operatorname*{max}_{\\theta}\\left\\{{\\frac{2\\cdot\\mathrm{precision}(\\theta)\\cdot\\mathrm{recall}(\\theta)}{\\mathrm{precision}(\\theta)+\\mathrm{recall}(\\theta)}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "(2) AUPRC (Area Under the Precision-Recall Curve) is a metric representing the pair-centric area under the precision-recall curve. It calculates the average precision scores for all protein-label pairs, which is exactly equivalent to the micro-average precision score for multiple binary classification problems. ", "page_idx": 16}, {"type": "text", "text": "(3) The MCC (Matthew\u2019s Correlation Coefficient) is a metric used to evaluate the performance of binary classification models. It takes into account true positives, true negatives, false positives, and false negatives to assess the model\u2019s overall performance. The formula for MCC is: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{MCC}={\\frac{T P\\cdot T N-F P\\cdot F N}{{\\sqrt{(T P+F P)(T P+F N)(T N+F P)(T N+F N)}}}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here\u2019s what each term represents: $T P$ : True Positives; $T N$ : True Negatives; $F P$ : False Positives;   \n$F N$ : False Negatives. ", "page_idx": 16}, {"type": "text", "text": "MCC ranges from $-1$ to $+1$ , where $+1$ indicates perfect prediction, 0 indicates random prediction, and $-1$ indicates complete disagreement between prediction and observation. It\u2019s considered a balanced measure even when classes are imbalanced, making it a useful metric for evaluating classification models. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "(4) Following [8], OS (Overlap Score) is defined considering the active sites as a set of nonoverlapping segments in a sequence. If $S$ with $|S|\\,=\\,n$ is a sequence of amino acid residues, the active region $A_{s}$ of $S$ is defined as $A_{s}=\\{(a_{i},\\dot{b}_{i})\\}_{i}^{m}$ , where $a_{i}$ and $b_{i}$ are the index boundaries of the segment $i$ . The overlap score between the predicted active region $A=\\{(a_{p i},b_{p i})\\}_{i}^{n}$ and the ground-truth $A_{s}=\\{(a_{s i},b_{s i})\\}_{i}^{m}$ is defined as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{OS}=\\frac{\\sum_{i}^{n}\\sum_{j}^{m}\\operatorname*{max}\\left(0,\\operatorname*{min}\\left(b_{p i},b_{s j}\\right)-\\operatorname*{max}\\left(a_{p i},a_{s j}\\right)\\right)}{\\sum_{i}^{m}\\left(b_{s i}-a_{s i}\\right)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "(5) According to the definition in the last equation, the formula of FPR (False Positive Rate) is as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{FPR}=\\frac{\\sum_{i}^{n}\\left(b_{p i}-a_{p i}\\right)\\mathbb{1}_{\\bigwedge_{j=1}^{m}[a_{p i},b_{p i}]\\cap\\lbrack a_{s j},b_{s j}]=\\emptyset}}{\\sum_{i}^{n}\\left(b_{p i}-a_{p i}\\right)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "B.2 Truncated length of reconstructed textual descriptions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "During the process of feeding $M$ reconstructed textual descriptions into the BLM $f_{\\psi}$ , we truncate the lengths of each attributes to optimize the computational resources. This is necessary because some attributes, such as Function and Involvement in Disease, typically contain more words, whereas others, like Protein Name and Organism, are usually shorter. Table 10 displays the mean length, $85\\%$ length, and truncated length of each attribute in our experiments. ", "page_idx": 17}, {"type": "table", "img_path": "XHdwlbNSVb/tmp/0f5b17d22d6547de2f87b17a170fb1229731188e04d7447f14bac493645ff74d.jpg", "table_caption": ["Table 10: Length of each textual description attribute. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.3 Introduction of baseline models ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "ESM-1b is a deep learning model based on the Transformer architecture, designed specifically for processing and understanding protein sequences. It is trained on an extensive dataset comprising 250 million protein sequences, totaling around 86 billion amino acids, leveraging unsupervised learning techniques. The code and weight can be accessed from https://github.com/ facebookresearch/esm under the MIT License. ", "page_idx": 17}, {"type": "text", "text": "ESM-1v is a language model specialized for predicting the functional effects of sequence variations, enabling state-of-the-art zero-shot predictions. It shares the same architecture as ESM-1b but is specifically trained on the UniRef90 dataset to enhance its predictive capabilities for variant effects. The code and weight can be accessed from https://github.com/facebookresearch/esm under the MIT License. ", "page_idx": 17}, {"type": "text", "text": "ESM-2 is trained on protein sequences from UniRef database. It is capable of predicting structure, function, and various other properties of proteins directly from individual sequences. In our experiments, we use the 650M parameter version of the ESM-2. The code and weight can be accessed from https://github.com/facebookresearch/esm under the MIT License. ", "page_idx": 18}, {"type": "text", "text": "ProtT5 is a model developed by training the T5 [43] architecture from NLP on protein sequences. There are two versions of this model, each trained on BFD100 and UniRef50 respectively. Additionally, there are two size variants of the model, and we utilize the XL size version. The code and weight can be accessed from https://github.com/agemagician/ProtTrans under the Academic Free License v3.0. ", "page_idx": 18}, {"type": "text", "text": "ProtBert is a model that has been trained on protein sequences using the BERT [10] architecture from NLP. Similar to ProtT5, there are also two versions of ProtBert, each trained on BFD100 and UniRef 100 respectively. The code and weight can be accessed from https://github.com/ agemagician/ProtTrans under the Academic Free License v3.0. ", "page_idx": 18}, {"type": "text", "text": "ProtAlbert is another model trained on protein database, UniRef100, using the Albert language model architecture [26] in NLP. Albert reduces BERT\u2019s complexity by hard parameter sharing between its attention layers which allows to increase the number of attention heads. The code and weight can be accessed from https://github.com/agemagician/ProtTrans under the Academic Free License $\\mathrm{v}3.0$ . ", "page_idx": 18}, {"type": "text", "text": "ProtXLNet is trained on UniRef100 database following the successful language model architecture XLNet [63] in NLP. The code and weight can be accessed from https://github.com/ agemagician/ProtTrans under the Academic Free License v3.0. ", "page_idx": 18}, {"type": "text", "text": "ProtElectra is also trained on UniRef100 database but follows the Electra [4] architecture. Electra tries to improve the sampling-efficiency of the pretraining task by training two networks, a generator and a discriminator. The generator reconstructs masked tokens, potentially creating plausible alternatives, and the discriminator detects which tokens were masked. In our experiments, we use the discriminator of ProtElectra to extract residue-level features. The code and weight can be accessed from https://github.com/agemagician/ProtTrans under the Academic Free License v3.0. ", "page_idx": 18}, {"type": "text", "text": "PETA is a benchmark designed to evaluate the impact of vocabulary size and tokenization methods on the transfer learning capabilities of protein language models. The benchmark facilitates systematic assessments of how different training configurations of protein language models affect their performance in biologically relevant downstream applications. In our experiments, we utilize the amino acid-level version, deep_base, as the baseline for comparison. The code and weight can be accessed from https://github.com/ginnm/ProteinPretraining under the MIT License. ", "page_idx": 18}, {"type": "text", "text": "S-PLM is a structure-aware protein language model that efficiently leverages both sequence and structural data during its training phase. This integration is achieved through multi-view contrastive learning, which enables the model to deeply understand the complex interplay between a protein\u2019s sequence and its structure. During inference stage, S-PLM requires only the sequence data for inference, eliminating the need for structural input. The code and weight can be accessed from https://github.com/duolinwang/S-PLM under the MIT License. ", "page_idx": 18}, {"type": "text", "text": "TAPE is a benchmarking framework designed to systematically evaluate the performance of semisupervised learning models on protein sequences. It comprises five biologically relevant tasks that span different domains of protein biology, focusing on the generalization capabilities of protein embeddings. The code and weight can be accessed from https://github.com/songlab-cal/ tape under the BSD 3-Clause License. ", "page_idx": 18}, {"type": "text", "text": "MIF is a structured graph neural network that improves protein representation learning by utilizing protein backbone structures during pretraining. It reconstructs masked protein sequences with the help of structural information, enhancing its ability to capture complex biological properties. MIF-ST extends MIF by incorporating sequence data from a pretrained sequence-only protein language model. This model leverages both structural and sequence information, enhancing training effectiveness and broadening its application. The code and weight can be accessed from https: //github.com/microsoft/protein-sequence-models the under 1-clause BSD License. ", "page_idx": 18}, {"type": "text", "text": "PST is a refined model that enhances transformer-based protein language models by integrating structural information through structure extractor modules within its self-attention architecture. It is pretrained on protein structure databases, using a traditional masked language modeling objective. It has two versions of model, \u201cStandard\u201d and \u201cTrain struct only\u201d, with a number of size. We utilize the version of pst_t33 and pst_t33_so in our experiments. The code and weight can be accessed from https://github.com/BorgwardtLab/PST under the BSD 3-Clause License. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "ProtST is a framework designed to enhance the pretraining and understanding of protein sequence representations by integrating multi-modal learning with biomedical texts. It employs the ProtDescribe dataset, which pairs protein sequences with four textual descriptions of their properties derived from the Swiss-Prot database. We use the ProtST-ESM-1b version and ProtST-ESM-2 version in our experiments. The code and weight can be accessed from https://github.com/ DeepGraphLearning/ProtST under the Apache-2.0 License. ", "page_idx": 19}, {"type": "text", "text": "B.4 Examples of generated descriptions by agent model ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In practical scenarios, when encountering newly discovered protein, one common challenge is the absence of high-quality textual annotations. To address this issue, we utilize state-of-the-art biomedical text generation method, Prot2Text5, which serves as an agent model, to generate corresponding descriptions of proteins, as illustrated in Figure 6 to complement missing text modality. These generated descriptions have encapsulated key information about protein properties, including functions, locations and so on. By integrating these generated texts into our models, we significantly enhance the capability of the PLMs in active sites identification, leading to improved performance and deeper insights into protein. ", "page_idx": 19}, {"type": "image", "img_path": "XHdwlbNSVb/tmp/ffd75a51ceaa2b65b01efc24972ef734cf8c626130de15fdc01eecfb64c7b2ef.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 6: Examples of generated descriptions by the agent model. ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "C Explanation for the biological principle ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In biology, \u201cSimilar protein sequences give rise to similar structures and functions\u201d means that if two proteins have similar sequences of building blocks (amino acids), they will likely have similar shapes and do similar jobs in the body. For example, consider two proteins hemoglobin and myoglobin. Both proteins have similar amino acid sequences in their oxygen-binding regions, which means they fold into similar shapes. Because of this similarity, both proteins are able to bind oxygen, but they do it in different ways suited to their specific roles: hemoglobin transports oxygen in the blood, while myoglobin stores oxygen in muscles. This illustrates how similar sequences lead to similar structures and functions. ", "page_idx": 20}, {"type": "text", "text": "D More experiment results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "D.1 Performance comparison with other methods ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In order to evaluate the performance of our approach more convincingly, we also select other new PLM (i.e., SaProt [49]) as well as other classical methods (Random Forest and Support Vector Machine) [64] and statistics-based approach (DISCERN [46]) for comparison. The results of the comparison are shown as Table 11. ", "page_idx": 20}, {"type": "table", "img_path": "XHdwlbNSVb/tmp/42c36076012afa0070e8a784521da8b36324f05deb2490e30eabd5be30640375.jpg", "table_caption": ["Table 11: Performance comparison with other methods. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "D.2 The quality of texts generated by Prot2Text ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To evaluate the performance of Prot2Text, we built a dataset of 1k proteins randomly sampled from ProTAD, as shown in Table 12. The performance on ProTAD-1k is good and generally consistent with the reported performance, indicating that Prot2Text can readily serve as an agent to generate textual descriptions for proteins. ", "page_idx": 20}, {"type": "table", "img_path": "XHdwlbNSVb/tmp/66781dcb82dea1b23d8d986e36d9e411b387ab04725c16e2fe9ed1aeba53fddb.jpg", "table_caption": ["Table 12: The quality of texts generated by Prot2Text. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 13 below compares the performance of MMSite using human-annotated and Prot2Textgenerated texts for training. The performance of two settings was similar across most metrics. ", "page_idx": 20}, {"type": "table", "img_path": "XHdwlbNSVb/tmp/bb0534ba06540984e1f0e4a9d2c986dfa4216e33be75c49b1f9d3b6e08688f51.jpg", "table_caption": ["Table 13: Performance of MMSite using two types of text for training. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "D.3 Impact of the length of protein sequence ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Here we examine the impact of varying the maximum input sequence length on the performance of MMSite. We retrain our model using different sequence input length, specifically 128, 256, 512, 768, and 1024 amino acids, with the clustering threshold of $10\\%$ , as illustrated in Figure 7. When the input length is 768 or 1024, the batch size is set to 16, and for other input lengths, it is 24. In this ablation study, samples will be removed if there is no active site within the specified length range. ", "page_idx": 20}, {"type": "image", "img_path": "XHdwlbNSVb/tmp/77fab5697684b90b4b4d5270de83eb360a5efb99890a28460eb26b7e775b8caf.jpg", "img_caption": ["Figure 7: Performance on different maximum input sequence length. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "It is apparent that the model\u2019s performance tends to be quite good when the maximum length is set to 512 or higher. However, when the maximum length is set below 256, the model\u2019s performance declines due to the limited number of samples\u2014in fact, there are only 786 samples in the training dataset. Despite this decrease, the performance remains at a high level, demonstrating MMSite\u2019s robustness under varying training input conditions. ", "page_idx": 21}, {"type": "text", "text": "D.4 Quantitative results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "As complements to Table 1, we report comparisons of MMSite in dataset where clustering threshold is $30\\%$ and $50\\%$ with other 14 baselines in Table 14 and 15, which perform best in $10\\%$ case. It can be seen that MMSite still best in the $30\\%$ case. Although MMSite does not lead on all metrics at a $50\\%$ threshold, its performance is still noteworthy. It is important to notice that increasing the threshold significantly raises the risk of data leakage. Specifically, the dataset size at a $50\\%$ threshold is nearly three times larger than that at a $10\\%$ threshold (as shown in Table 9). We think their performances are closer at $50\\%$ threshold because: (1) The dataset is easy enough for the baseline models to generalize; (2) The performance is already very high, leaving little room for improvement. Therefore it is more reasonable to draw comparisons at lower thresholds. From this perspective, MMSite excels in conditions with limited data, demonstrating the benefits of leveraging textual modalities. This showcases MMSite\u2019s effectiveness and robustness in handling inadequate data scenarios. ", "page_idx": 21}, {"type": "table", "img_path": "XHdwlbNSVb/tmp/9b809f8007fb497ca6248686d472ff65d3762bed82500f6635ee073d7c1d579e.jpg", "table_caption": ["Table 14: Comparison on the dataset with clustering threshold at $30\\%$ "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "XHdwlbNSVb/tmp/86ba2c70b351b278b2f26dbf68dab99886cc2f89116b95a9ac03f1d3f2e77c06.jpg", "table_caption": ["Table 15: Comparison on the dataset with clustering threshold at $50\\%$ "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "D.5 Inference performance comparison ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "During inference, if the protein has corresponding multi-attribute text descriptions, the process is the same as during training. However, if such descriptions are not available, we use the Prot2Text agent model to generate the text inputs. Table 16 is a comparison of MMSite inference time between using pre-existing text descriptions from ProTAD and generated text with an agent model. For further context, we also compare against the BioMedGPT [32], another excellent protein-to-text model. The tests were conducted on a single GPU (NVIDIA GeForce RTX 4090) and a CPU (Intel(R) Xeon(R) Platinum 8375C CPU $\\textcircled{a}2.90\\mathrm{GHz}$ ). ", "page_idx": 22}, {"type": "table", "img_path": "XHdwlbNSVb/tmp/62267a0a0fc327a8b33a704c413f5be830c09f45c0225400c8a239cc3a429a91.jpg", "table_caption": ["Table 16: Inference time comparison for different text sources. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "We also compared the model performance using text generated by Prot2Text and BioMedGPT as Table 17. ", "page_idx": 22}, {"type": "table", "img_path": "XHdwlbNSVb/tmp/fa5ef90dfa378399a8544a37fa0ad8736d4580f34b6738906a9ecc5895aae18e.jpg", "table_caption": ["Table 17: Performance comparison between Prot2Text and BioMedGPT generated text. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "As the comparison shows, while using Prot2Text does increases the inference time of ProTAD, it provides comparable performance to BioMedGPT with a much smaller inference time cost. ", "page_idx": 22}, {"type": "text", "text": "E Limitations & Broader impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In our research, we utilize the data from Swiss-Prot in UniProt, which is renowned for its high-quality and expertly curated annotations, ensuring a high level of confidence in the dataset. Our study is able to effectively identify protein active sites, yet it does not specify the biochemical reactions for each, due to the inherent scarcity of detailed annotations from challenging biological wet lab experiments. Nonetheless, our approach, which integrates multi-modal deep learning, encourages further exploration into protein reaction mechanisms, setting a foundation for more targeted and comprehensive future research. However, it is worth noting that our powerful pretrained model may potentially be misused for harmful purposes, such as the design of dangerous drugs. We anticipate that future studies will address and mitigate these concerns. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "F More visualisation results ", "text_level": 1, "page_idx": 23}, {"type": "image", "img_path": "XHdwlbNSVb/tmp/0f2d9d95c7de4ae69a63dd11c5aa138effdab511feb5ba772acd745c3918f46d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 8: Visualisation of active site identification results of proteins by MMSite. Each subfigure caption is the protein\u2019s Entry ID in the UniProt database. The colors mean the same as in Figure 3. ", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We made claims in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We have discussed limitations of the work in Appendix E ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our paper is an experimental article and does not involve theoretical assumptions or proofs. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We have disclosed all the information needed to reproduce the main experimental results of the paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper provide data, code and sufficient instructions to reproduce the main experiment results, as described in supplemental material. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We have specified all the training and test details mainly in Section 4.1 and Appendix A. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We report 2-sigma error bars in Table 1 and detailed descriptions of our experiment settings can be found in Section 4.1 and 4.2.1. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide sufficient information on the computer resources in Section 4.1. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The research conducted in the paper conforms with the NeurIPS Code of Ethics in every respect. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We have discussed both potential positive societal impacts and negative societal impacts of the work performed in Appendix E. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 27}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper poses no such risks. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: All mentioned have been properly credited and respected. We provide URLs and license information in Appendix A.2, B.3 and footnotes. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: New assets have been well documented and we provided proper documentation in supplemental materials. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}]