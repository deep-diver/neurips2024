{"references": [{"fullname_first_author": "Raman Arora", "paper_title": "Online bandit learning against an adaptive adversary: from regret to policy regret", "publication_date": "2012-06-26", "reason": "This paper introduces the concept of policy regret, which is central to the current work's theoretical framework and analysis of learning against adaptive adversaries."}, {"fullname_first_author": "Lloyd S Shapley", "paper_title": "Stochastic games", "publication_date": "1953-10-01", "reason": "This foundational paper establishes the Markov game framework, which is the theoretical model for the multi-agent reinforcement learning problem tackled in the current work."}, {"fullname_first_author": "Michael L Littman", "paper_title": "Markov games as a framework for multi-agent reinforcement learning", "publication_date": "1994-01-01", "reason": "This paper provides a foundational framework for multi-agent reinforcement learning (MARL), which is essential context for the current paper's study of learning in Markov games with adaptive adversaries."}, {"fullname_first_author": "Neri Merhav", "paper_title": "On sequential strategies for loss functions with memory", "publication_date": "2002-07-01", "reason": "This paper provides a foundation for analyzing policy regret in online learning, a concept central to understanding the tradeoffs of competing with adaptive adversaries."}, {"fullname_first_author": "Jeongyeol Kwon", "paper_title": "RL for latent MDPs: Regret guarantees and a lower bound", "publication_date": "2021-01-01", "reason": "This paper provides crucial lower bounds for reinforcement learning in latent Markov Decision Processes, which inform the difficulty of learning against adaptive opponents and influence some of the current work's negative results."}]}