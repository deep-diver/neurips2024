[{"figure_path": "Mqx2gquLk0/tables/tables_1_1.jpg", "caption": "Table 1: Summary of main results for learning against adaptive adversaries. Learner's policy set is all deterministic Markov policies. m = 0 + stationary corresponds to standard single-agent MDPs.", "description": "This table summarizes the results of the paper on learning against adaptive adversaries in Markov games.  It shows the lower and upper bounds on policy regret achievable by a learner under different assumptions about the adversary's behavior (memory, stationarity, consistency), and the size of the learner's policy set.  The rows represent different levels of adversary adaptivity, from unbounded memory to memory-bounded and consistent. The columns show the corresponding lower bounds (\u03a9) and upper bounds (\u00d5) on policy regret.  The special case of a single-agent MDP is represented by m = 0 and stationary.", "section": "Fundamental barriers"}]