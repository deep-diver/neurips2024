[{"type": "text", "text": "Learning in Markov Games with Adaptive Adversaries: Policy Regret, Fundamental Barriers, and Efficient Algorithms ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Thanh Nguyen-Tang ", "page_idx": 0}, {"type": "text", "text": "Raman Arora ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Department of Computer Science Johns Hopkins University Baltimore, MD 21218 nguyent@cs.jhu.edu ", "page_idx": 0}, {"type": "text", "text": "Department of Computer Science Johns Hopkins University Baltimore, MD 21218 arora@cs.jhu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study learning in a dynamically evolving environment modeled as a Markov game between a learner and a strategic opponent that can adapt to the learner\u2019s strategies. While most existing works in Markov games focus on external regret as the learning objective, external regret becomes inadequate when the adversaries are adaptive. In this work, we focus on policy regret \u2013 a counterfactual notion that aims to compete with the return that would have been attained if the learner had followed the best fixed sequence of policy, in hindsight. We show that if the opponent has unbounded memory or if it is non-stationary, then sample-efficient learning is not possible. For memory-bounded and stationary, we show that learning is still statistically hard if the set of feasible strategies for the learner is exponentially large. To guarantee learnability, we introduce a new notion of consistent adaptive adversaries, wherein, the adversary responds si\u221amilarly to similar strategies of the learner. We provide algorithms that achieve $\\sqrt{T}$ policy regret against memorybounded, stationary, and consistent adversaries. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent years have witnessed tremendous advances in reinforcement learning for various challenging domains in AI, from the game of Go [Silver et al., 2016, 2017, 2018], real-time strategy games such as StarCraft II [Vinyals et al., 2019] and Dota [Berner et al., 2019], autonomous driving [Shalev-Shwartz et al., 2016], to socially complex games such as hide-and-seek [Baker et al., 2019], capture-the-flag [Jaderberg et al., 2019], and highly tactical games such as poker game Texas hold\u2019 em [Morav\u02c7c\u00edk et al., 2017, Brown and Sandholm, 2018]. Notably, most of the challenging RL applications can be systematically framed as multi-agent reinforcement learning (MARL) wherein multiple strategic agents learn to act in a shared environment [Yang and Wang, 2020, Zhang et al., 2021]. ", "page_idx": 0}, {"type": "text", "text": "Despite the empirical successes, theoretical foundations of MARL are underdeveloped, especially in settings where the learner faces adaptive opponents who can strategically adapt and react to learner\u2019s policies. Consider for example the optimal taxation problem in the AI economist [Zheng et al., 2020], a game that simulates dynamic economies that involve multiple actors (e.g., the government and its citizens) who strategically contribute to the game dynamics. The government agent learns to set a tax rate that optimizes for the economic equality and productivity of its citizens, whereas the citizens who perhaps have their own interests, respond adaptively to tax policies of the government agent (e.g., relocating to states that offer generous tax rates). Such adaptive behavior of participating agents is a crucial component in other applications as well, e.g., mechanism design [Conitzer and Sandholm, 2002, Balcan et al., 2005], optimal auctions [Cole and Roughgarden, 2014, D\u00fctting et al., 2019]. ", "page_idx": 0}, {"type": "table", "img_path": "Mqx2gquLk0/tmp/7fcb0b07acde385ca1a0d0209061e77c05cde928ce87b4e8c5783f123df7c394.jpg", "table_caption": [], "table_footnote": ["Table 1: Summary of main results for learning against adaptive adversaries. Learner\u2019s policy set is all deterministic Markov policies. $m=0+$ stationary corresponds to standard single-agent MDPs. "], "page_idx": 1}, {"type": "text", "text": "The question of learning against adaptive opponents has been mostly studied under the framework of external regret, wherein the agent is required to compete with the best fixed policy in hindsight [Liu et al., 2022]. However, external regret is not adequate to study adaptive opponents as it does not take into account the counterfactual response of the opponents. This motivates us to study MARL using the framework of policy regret [Arora et al., 2012], a counterfactual notion that aims to compete with the return that would have been attained if the agent had followed the best fixed sequence of policy in hindsight. Even though policy regret is now a standard notion to study adaptive adversaries and has been extensively studied in online (bandit) learning [Merhav et al., 2002, Arora et al., 2012, Malik et al., 2022] and repeated games [Arora et al., 2018], it has not received much attention in a multiagent reinforcement learning setting. In this paper, we aim to fill in this gap. We consider two-player Markov games (MGs) [Shapley, 1953, Littman, 1994] as a model for MARL, wherein one agent (the learner) learns to act against an adaptive opponent. We provide a series of negative and positive results for policy regret minimization in Markov games, highlighting the fundamental limits of learning and showcasing key principles underpinning the design of efficient learning algorithms against adaptive adversaries. ", "page_idx": 1}, {"type": "text", "text": "Fundamental barriers. We first show that any learner must incur a linear policy regret against an adaptive opponent who can adapt and remember the learner\u2019s past policies (Theorem 1). When the opponent has a bounded memory span, any learner must require an exponential number of samples $\\hat{\\Omega}\\hat{(}(S A)^{H}/\\epsilon^{2})$ to obtain an $\\epsilon_{}$ -suboptimal policy regret, even with the weakest form of memory wherein the opponent is oblivious (Theorem 2). When the memory-bounded opponent\u2019s response is stationary, i.e., the response function does not vary with episodes, learning is still statistically hard when the learner\u2019s policy set is exponentially large, as in this case the policy regret necessarily scales polynomially with the cardinality of the learner\u2019s policy set (Theorem 3). ", "page_idx": 1}, {"type": "text", "text": "Efficient algorithms. Motivated by these statistical hardness results, we consider a structural condition on the response of the opponents, which we refer to as consistent behavior, wherein the opponent responds similarly to similar sequences of policies (Definition 5)\u221a. We propose two algorithms OPO-OMLE (Algorithm 1) and APE-OVE (Algorithm 3) that obtain $\\sqrt{T}$ policy regret against $m$ -memory bounded, stationary, and consistent adversaries, for $m=1$ and $m\\geq1$ , respectively. ", "page_idx": 1}, {"type": "text", "text": "\u2022 For memory len\u221agth $m=1$ : We show that OPO-OMLE obtains a policy regret upper bound of $\\tilde{\\mathcal{O}}(H^{3}S^{2}A B+\\sqrt{H^{5}S A^{2}B T})$ , when the learner\u2019s policy set is the set of all deterministic Markov policies, where $H$ is the episode length, $S$ is the number of states, $A$ and $B$ are the number of actions for the learner and the opponent, respectively, and $T$ is the number of episodes. ", "page_idx": 1}, {"type": "text", "text": "\u2022 For general memory length $m\\geq1$ : We show that APE-OVE obtains a policy regret upper bound of $\\begin{array}{r}{\\tilde{\\mathcal{O}}\\left((m-1)H^{2}S A B+\\sqrt{H^{3}S A B}(S A B(H+\\sqrt{S})+H^{2})\\sqrt{\\frac{T}{d^{*}}}\\right)}\\end{array}$ , where $d^{\\ast}$ is an instancedependent quantity that features the minimum positive visitation probability. ", "page_idx": 1}, {"type": "text", "text": "We provide a summary of our main results in Table 1. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Learning in Markov games. Learning problems in Markov games have been studied extensively in the MARL literature. Most existing works focus on learning Nash equilibria either with known dynamics or infinite data [Littman, 1994, Hu and Wellman, 2003, Hansen et al., 2013, Wei et al., 2020], or otherwise in a self-play setting wherein we control all the players [Wei et al., 2017, Bai et al., 2020, Bai and Jin, 2020, Xie et al., 2020, Liu et al., 2021], or in an online setting wherein we control one player to learn against other potentially adversarial players [Brafman and Tennenholtz, 2002, Wei et al., 2020, Tian et al., 2021, Jin et al., 2022]. Other related work focuses on exploiting sub-optimal opponents via no-external regret learning [Liu et al., 2022] and studying Stackelberg equilibria in two-player general-sum turn-based MGs, wherein only one player is allowed to take actions in each state [Ramponi and Restelli, 2022]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Policy regret in online learning settings. Policy regret minimization has been studied mostly in online (bandit) learning problems. It was first studied in a full information setting [Merhav et al., 2002] and extended to the bandit setting and more powerful competitor classes using swap regret and $\\Phi$ -regret [Arora et al., 2012]. A lower bound of $T^{2/3}$ on policy regret in a bandit setting was provided by Dekel et al. [2014] and was later extended to action space with metric [Koren et al., 2017a,b]. A long line of works studies (complete) policy regret in \u201ctallying\u201d bandits, wherein an action\u2019s loss is a function of the number of the action\u2019s pulls in the previous $m$ rounds [Heidari et al., 2016, Levine et al., 2017, Seznec et al., 2019, Lindner et al., 2021, Awasthi et al., 2022, Malik et al., 2022, 2023]. ", "page_idx": 2}, {"type": "text", "text": "Beyond online (bandit) learning, policy regret has been studied in several more challenging settings. In Arora et al. [2018] authors study the notion of policy equilibrium in repeated games (Markov games with $H=S=1$ ) when agents follow no-policy regret algorithms. A more complete characterization of the learnability in online learning with dynamics, where the loss function additionally depends on time-evolving states, was given in Bhatia and Sridharan [2020]. Finally, in Dinh et al. [2023], authors study policy regret in online MDP, where an adversary who follows a no-external regret algorithm generates the loss functions, which effectively alleviates policy regret minimization to the standard external regret minimization in online MDPs. ", "page_idx": 2}, {"type": "text", "text": "3 Problem setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Markov games. In this paper, we use the framework of Markov Games to study an interactive multi-agent decision-making and learning environment [Shapley, 1953]. Markov games extend Markov decision processes (MDPs) to multiplayer scenarios, where each agent\u2019s action affects not only the environment but also the subsequent state of the game and the actions of other agents. Formally, a standard two-player Markov Game (MG) is specified by a tuple $M=(S,\\mathcal{A},\\mathcal{B},H,P,r)$ . Here, $\\boldsymbol{S}$ denotes the state space with cardinality $|S|=S$ , $\\boldsymbol{\\mathcal{A}}$ is the action space of the first player (called learner) with cardinality $|{\\mathcal{A}}|=A$ , $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ is the action space of the second player (referred to as an opponent or an adversary) with cardinality $|\\boldsymbol B|=B$ , $H\\in\\mathbb{N}$ is the time horizon for each game. $P=\\{P_{1},\\dots,P_{H}\\}$ are the transition kernels with each $P_{h}:S\\times A\\times B\\to\\Delta(S)$ specifying the probability of transitioning to the next state given the current state, learner\u2019s action, and adversary\u2019s action $(\\Delta(S)$ denotes the set of all probability distributions over $\\boldsymbol{S}$ ). Finally, $\\boldsymbol{r}=\\{r_{1},\\dots,r_{H}\\}$ are the (expected) reward functions with each $r_{h}:S\\times A\\times B\\to[0,1]$ . For simplicity, we assume the learner knows the reward function.1 ", "page_idx": 2}, {"type": "text", "text": "Each episode begins in a fixed initial state $s_{1}$ . At step $\\textit{h}\\in[H]$ , the learner observes the state $s_{h}$ and picks her action $a_{h}~\\in~A$ while the opponent/adversary picks an action $b_{h}~\\in~B$ . As a result, the learner observes $b_{h}$ , receives reward $r_{h}(s_{h},a_{h},b_{h})$ and the environment transitions to $s_{h+1}\\sim P_{h}(\\cdot|s_{h},a_{h},b_{h})$ . The episode terminates after $H$ steps. ", "page_idx": 2}, {"type": "text", "text": "Policies and value functions. A learner\u2019s policy (also referred to as strategy) is any tuple $\\pi=$ $\\{\\pi_{h}\\}_{h\\in[H]}$ where $\\pi_{h}:(S\\times A)^{h-1}\\times S\\to{\\dot{\\Delta(A)}}$ . A policy $\\pi=\\{\\pi_{h}\\}_{h\\in[H]}$ is said be Markovian if for every $\\dot{h}\\in[H]$ , $\\pi_{h}:S\\to\\Delta(A)$ . Similarly, an adversary\u2019s policy is any tuple $\\mu=\\{\\mu_{h}\\}_{h\\in[H]}$ where $\\mu_{h}:(S\\times B)^{h-1}\\times S\\to\\Delta(B)$ . $\\mu$ is said to be Markovian if for every $h$ , $\\mu_{h}:S\\to\\Delta(B)$ . For simplicity, we will focus only on Markov policies for both the learner and the adversary in this paper. Let $\\Pi$ (respectively, $\\Psi$ ) be the set of all feasible policies of the learner (respectively, the adversary). The value of a policy tuple $(\\pi,\\mu)\\in\\Pi\\times\\Psi$ at step $h$ in state $s$ , denoted by $V_{h}^{\\pi,\\mu}(s)$ is the expected accumulated reward starting in state $s$ from step $h$ , if the learner and the adversary follow $\\pi$ and $\\mu$ respectively, i.e., $V_{h}^{\\pi,\\mu}(s):=\\mathbb{E}_{\\pi,\\mu}[\\sum_{l=h}^{H}r_{l}(s_{l},a_{l},b_{l})|s_{h}=s]$ , where the expectation is with respect to the trajectory $\\left(s_{1},a_{1},b_{1},r_{1},\\dots,s_{H},a_{H},b_{H},r_{H}\\right)$ distributed according to $P,\\pi$ , and $\\mu$ . We also denote the action-value function $\\begin{array}{r}{Q_{h}^{\\pi,\\mu}(s,a,b):=\\mathbb{E}_{\\pi,\\mu}[\\sum_{l=h}^{H}r_{l}(s_{l},a_{l},b_{l})|(s_{h},a_{h},b_{h})=(s,a,b)]}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "Given a $V:{\\mathcal{S}}\\rightarrow\\mathbb{R}$ , we write $P_{h}V(s,a,b):=\\mathbb{E}_{s^{\\prime}\\sim P_{h}(\\cdot|s,a,b)}[V(s^{\\prime})]$ . For any $u:\\mathcal{S}\\to\\Delta(\\mathcal{A})$ , $v\\colon S\\to\\Delta(B)$ , $Q\\!:\\!S\\!\\times\\!A\\!\\times\\!B\\!\\rightarrow\\mathbb{R}$ , denote $Q(s,u,v):=\\mathbb{E}_{a\\sim u(\\cdot|s),b\\sim v(\\cdot|s)}[Q(s,a,b)]$ for any $s\\in S$ . ", "page_idx": 3}, {"type": "text", "text": "Adaptive adversaries. We allow the adversary to be adaptive, i.e., the adversary can choose their policy in episode $t$ based on the learner\u2019s policies on episodes $1,\\ldots,t$ . We assume that the adversary is deterministic and has unlimited computational power, i.e., the adversary can plan, in advance, using as much computation as needed, as to how they would react in each episode to any sequence of policies. Formally, the adversary defines in advance a sequence of deterministic functions $\\bar{\\{f_{t}\\}}_{t\\in\\mathbb{N}^{*}}$ , where $f_{t}:\\Pi^{t}\\overset{\\cdot}{\\rightarrow}\\Psi$ . The input to each response function $f_{t}$ is an entire history of the learner\u2019s policies, including her policy in episode $t$ . Therefore, if the learner follows policies $\\pi^{1},\\cdot\\cdot\\cdot,\\pi^{t}$ , the adversary responds with policy $\\dot{f}_{t}(\\pi^{1},\\dots,\\pi^{t})\\,\\in\\,\\Psi$ in episode $t$ . Since the response function $f_{t}$ depends on the learner\u2019s policy at round $t$ , our setup is essentially a principal-follower model, akin to Stackelberg games [Letchford et al., 2009, Blum et al., 2014] and mechanism design for learning agents [Braverman et al., 2019]. In this context, the principal agent (mechanism designer or learner) publicly declares a strategy before committing to it, allowing the followers to subsequently choose their strategies based on their understanding of the principal\u2019s decisions. ", "page_idx": 3}, {"type": "text", "text": "We evaluate the learner\u2019s performance using the notion of policy regret [Merhav et al., 2002, Arora et al., 2012], which compares the return on the first $T$ episodes to the return of the best fixed sequence of policy in hindsight. Formally, the learner\u2019s policy regret after $T$ episodes is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathsf{P R}(T)=\\operatorname*{sup}_{\\pi\\in\\Pi}\\sum_{t=1}^{T}V_{1}^{\\pi,f_{t}([\\pi]^{t})}(s_{1})-V_{1}^{\\pi^{t},f_{t}(\\pi^{1},\\dots,\\pi^{t})}(s_{1}),\\mathrm{~where~}f_{t}([\\pi]^{t}):=f_{t}(\\underbrace{\\pi,\\dots,\\pi}_{t\\mathrm{~times}}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Policy regret has been studied in online (bandit) learning [Merhav et al., 2002, Arora et al., 2012] and repeated games [Arora et al., 2018], yet, to the best of our knowledge, it has never been studied in Markov games. Policy regret differs from the more common definition of external regret defined as $\\begin{array}{r}{R(T)=\\operatorname*{sup}_{\\pi\\in\\Pi}\\sum_{t=1}^{T}{\\Bar{V_{1}^{\\pi,f_{t}(\\pi^{1},\\dots,\\pi^{t})}}(s_{1})}-V_{1}^{\\pi^{t},f_{t}(\\pi^{1},\\dots,\\pi^{t})}(s_{1}),}\\end{array}$ which is used in [Liu et al., 2022]. However, external regret is inadequate for measuring the learner\u2019s performance against an adaptive adversary. Indeed, when the adversary is adaptive, the quantity $V_{1}^{\\pi,\\bar{f}_{t}(\\pi^{1},\\dots,\\pi^{t})}$ is hardly interpretable anymore \u2013 see [Arora et al., 2012] for a more detailed discussion. ", "page_idx": 3}, {"type": "text", "text": "As a warm-up, we show in the following example that, policy regret minimization generalizes the standard Nash equilibrium learning problem in zero-sum two-player Markov games. ", "page_idx": 3}, {"type": "text", "text": "Example 3.1 (Nash equilibrium). Consider the adversary with the following behavior: for any Markov policy $\\pi$ of the learner, the adversary ignores all the learner\u2019s past policies and respond only to the current policy $\\pi$ with a Markov policy $f(\\pi)$ such that for all $(s,h)$ $\\bar{h}),\\,\\bar{V_{h}^{\\pi,f(\\pi)}}(s)=\\operatorname*{min}_{\\mu}{V_{h}^{\\pi,\\bar{\\mu}}(s)},$ where the minimum is taken over all the possible Markov policies for the adversary. By Filar and Vrieze [2012], such an $f(\\pi)$ exists. In addtion, there also exists a Markov policy $\\pi^{*}$ such that for all $(s,h),\\,V_{h}^{\\pi^{*},f(\\pi^{*})}(s)=\\operatorname*{sup}_{\\pi}V_{h}^{\\pi,f(\\pi)}(s)=\\operatorname*{inf}_{\\mu}\\operatorname*{sup}_{\\pi}V_{h}^{\\pi,\\mu}(s)$ . The policies $(\\pi^{*},f(\\pi^{*}))$ is a Nash equilibrium [Nash, 1950] of the Markov game. For such an adversary, the policy regret becomes $\\begin{array}{r}{P R(T)\\,=\\,\\sum_{t=1}^{T}V_{1}^{\\pi^{*},f(\\pi^{*})}(s_{1})\\,-\\,\\sum_{t=1}^{T}V_{1}^{\\pi^{t},f(\\pi^{t})}(s_{1})}\\end{array}$ . T\u221ahis Nash equilibrium can be computed using, e.g., the $Q$ -ol algorithm of [Tian et al., 2021] with $\\sqrt{T}$ (policy) regret.2 ", "page_idx": 3}, {"type": "text", "text": "Additional notation. We write $f\\lesssim g$ to mean $f\\,=\\,\\mathcal{O}(g)$ . We use $c$ to represent an absolute constant that can have different values in different appearances. ", "page_idx": 3}, {"type": "text", "text": "4 Fundamental barriers for learning against adaptive adversaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we show that achieving low policy regret in Markov games against an adaptive adversary is statistically hard when (i) the adversary has an unbounded memory (see Definition 1), or (ii) the adversary is non-stationary, or (iii) the learner\u2019s policy set is exponentially large (even if the adversary is memory-bounded and stationary). ", "page_idx": 3}, {"type": "text", "text": "To begin with, we show that any learner must incur a linear policy regret in the general setting. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. For any learner, there exists an adaptive adversary and a Markov game instance such that $P R(T)=\\Omega(T)$ . ", "page_idx": 4}, {"type": "text", "text": "The construction in the proof of Theorem 1, shown in Appendix A.1, takes advantage of the unbounded memory of the adversary, that can remember the policy the learner takes in the first episode. This motivates us to consider memory-bounded adversaries, a situation that is quite similar to the online bandit learning setting of Arora et al. [2012]. ", "page_idx": 4}, {"type": "text", "text": "Definition 1 ( $\\ln$ -memory bounded adversaries). An adversary $\\{f_{t}\\}_{t\\in\\mathbb{N}^{*}}$ is said to be $m$ -memory bounded for some $m\\geq0$ if for every t and policy sequence $\\pi^{\\mathrm{i}},...\\,,\\pi^{\\bar{t}}$ , we have $f_{t}(\\pi^{1},\\dots,\\pi^{t})=$ $f_{t}(\\pi^{\\mathrm{min}\\{1,t-m+1\\}},\\ldots,\\pi^{t})$ . ", "page_idx": 4}, {"type": "text", "text": "Is it possible to efficiently learn against memory-bounded adversaries? Unlike online bandit learning, we show that learning in Markov games is statistically hard even when the adversary is memorybounded, even for the weakest case of memory $m=0$ and the adversary\u2019s policy set $\\Psi$ is small. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2. For any learner and any $L\\in\\mathbb N$ and $S,A,\\,H$ , there exists an oblivious adversary $(i.e.,$ , $m=0$ ) with the policy space $\\Psi$ of cardinality at least $L$ , a Markov game (with $S A+S$ states, $A$ actions for the learner, $B=2S$ actions for the adversary) such that $P R(T)=\\Omega\\left(\\sqrt{T(S A/L)^{L}}\\right)$ . Theorem 2 claims that competing even with an oblivious adversary that employs a small set of policies takes an exponential number of samples (e.g., set $S=L=H$ ). The construction of the lower bound follows the construction used to prove a lower bound for learning latent MDPs [Kwon et al., 2021] and a reduction of a given latent MDP into a Markov game [Liu et al., 2022]; we give complete details in Appendix A.2. The proof of Theorem 2 utilizes the fact that the sequence of response function an adversary utilizes can be completely arbitrary. It implies that we need to constrain the adversary further beyond being memory-bounded. A natural restriction we consider given the construction is to assume stationarity, i.e. consider adversaries whose response functions do not change over time. ", "page_idx": 4}, {"type": "text", "text": "Definition 2 (Stationary adversaries). An $m$ -memory bounded adversary is said to be stationary if there exists an $f\\;:\\;\\Pi^{m}\\;\\rightarrow\\;\\Psi$ such that for all $t$ and $\\pi^{1},\\cdot\\cdot\\cdot,\\pi^{t}$ , we have $f_{t}(\\pi^{1},\\overline{{\\dots,\\pi^{t})\\ =}}$ $f(\\pi^{\\mathrm{min}\\{1,t-m+1\\}},\\ldots,\\pi^{t})$ . ", "page_idx": 4}, {"type": "text", "text": "The stationary behavior is sometimes also referred to as \u201cg-restricted\u201d in the online learning literature\u2013 see the related discussion of Malik et al. [2022]. In the special case wherein the adversary is both stationary and oblivious (i.e., $m=0$ ), the Markov game reduces to the standard single-agent MDP (and the policy regret reduces to standard regret of the MDP) \u2013 this setting has been studied in [Zhang et al., 2023]. We, therefore, only need to consider $m\\geq1$ . ", "page_idx": 4}, {"type": "text", "text": "Connections to Stackelberg equilibrium in general-sum Markov games. While seemingly restrictive, policy regret minimization with $m$ -memory bounded and stationary adversaries already subsumes the problem of learning Stackelberg equilibrium [Von Stackelberg, 2010] in generalsum Markov games [Ramponi and Restelli, 2022].3 In general-sum Markov games, the adversary (\u201cfollower\u201d) aims at maximizing his own reward function given any policy of the learner (\u201cleader\u201d). That is, the adversary is 1-memory bounded, and the response function $f:\\Pi\\to\\Psi$ corresponds to a function that selects the best response policy to any given policy of the learner. The benchmark $\\operatorname*{max}_{\\pi\\in\\Pi}V_{1}^{\\pi,f(\\pi)}$ in policy regret then becomes the Stackelberg equilibrium. Is sample-efficient learning possible against $m$ -memory bounded and stationary adversaries? One can notice an immediate approach to learning against a 1-memory bounded and stationary adversaries is to simply view the problem as a $|\\Pi|$ -armed bandit problem and apply any state-of-the-art bandit algorithm [Audibert and Bubeck, 2009] to obtain $\\mathrm{PR}(T)={\\mathcal{O}}(H{\\sqrt{T|\\Pi|}})$ . However, scaling polynomially with the learner\u2019s policy class is not desirable when the class is exponentially large (e.g., when the learner\u2019s policy class is the set of all deterministic policies, then $|\\dot{\\Pi}|=\\Theta(A^{\\dot{H}S}))$ . And in fact, we cannot avoid polynomial scaling with the cardinality of the learner\u2019s policy class in general. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3. For any learner with policy class $\\Pi$ , there exists a 1-memory bounded and stationary adversary and a Markov game with $B=\\mathcal{O}(1)$ such that $P R(T)=\\Omega\\left(\\operatorname*{min}\\{T,|\\Pi|\\}\\right)$ . ", "page_idx": 4}, {"type": "text", "text": "5 Efficient algorithms for learning against adaptive adversaries ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Thus far, we have shown that learning against an adaptive adversary in Markov games is statistically hard, even when the adversary is $m$ -memory bounded and stationary. The reason that stationarity is not sufficient for efficient learning (which the lower bound in Theorem 3 exploits for the construction of a hard instance) comes from the unstructured response of the adversary in the worst case. Even if the learner plays nearly identical sequence of policies differing only on a small number of states and steps, the adversary can essentially respond completely arbitrarily. In other words, knowing the policies that the adversary plays in response to the policies of the learner (i.e., observing the values of the response function $f$ at specific inputs) reveals zero information about the function $f$ on previously seen inputs. Thus, the learner is required to explore all the policies in $\\Pi$ to be able to identify an optimal policy. This motivates us to consider an additional structural assumption on how the adversary responds to the learner\u2019s policies. We assume that the adversary is consistent in response to two similar sequence of policies of the learner. In essence, given that the learner plays two sequences of policies that agree on certain states (s) and steps $(h)$ \u2013 then, we assume that the opponent also responds with two sequences of policies that agree on the same states and steps. We refer to this behavior as consistent; a formal definition follows. ", "page_idx": 5}, {"type": "text", "text": "Definition 3 (Consistent adversaries). $A n\\;m$ -memory bounded and stationary adversary $f$ is said to be consistent $i f,$ for any two sequences of learner\u2019s policies $\\pi^{1},\\ldots,\\pi^{m}$ and $\\nu^{1},\\dots,\\nu^{\\dot{m}}$ , and any $(s,h)\\in{\\mathcal{S}}\\times[H]$ , i $\\begin{array}{r}{f\\pi_{h}^{i}(\\cdot|s)=\\nu_{h}^{i}(\\cdot|s),\\forall i\\in[m]}\\end{array}$ , then $f(\\pi^{1},\\dots,\\pi^{m})_{h}(\\cdot|s)=f(\\nu^{1},\\dots,\\nu^{m})_{h}(\\cdot|s)$ . Otherwise, we say that the opponent\u2019s response $f$ is arbitrary. ", "page_idx": 5}, {"type": "text", "text": "We argue that the definition above is natural if we are to consider opponents that are self-interested strategic agents, and not simply a malicious adversary. So, it would be in an opponent\u2019s interest to play in a somewhat consistent manner. Playing optimally after figuring out the learner\u2019s strategy would indeed require playing consistently. An opponent that plays completely arbitrary, while challenging to learn anything from, also does not improve their value function. Some remarks are in order. ", "page_idx": 5}, {"type": "text", "text": "Remark 1 ( $\\zeta$ -approximately consistent adversaries). Our algorithms and results for consistent adversaries easily extend to $\\zeta$ -approximately consistent adversaries for any fixed constant $\\zeta\\geq0$ . An adversary $f$ is said to be $\\zeta$ -approximately consistent $i f,$ for any $\\pi^{1},\\ldots,\\pi^{m}$ and $\\nu^{1},\\dots,\\nu^{\\dot{m}}$ , and any (s, h) \u2208S \u00d7 [H], if \u03c0ih(\u00b7|s) = \u03bdih(\u00b7|s), \u2200i \u2208[m], then maxa\u2208A  log ff((\u03c0\u03bd1,,......,,\u03bd\u03c0m))hh((aa||ss)) $\\begin{array}{r}{\\left\\vert\\log\\frac{f(\\pi^{1},\\ldots,\\pi^{m})_{h}(a\\vert s)}{f(\\nu^{1},\\ldots,\\nu^{m})_{h}(a\\vert s)}\\right\\vert\\leq\\zeta.}\\end{array}$ . For simplicity, we stick with Definition 3 (i.e., $\\zeta=0$ ) to best convey our algorithmic and theoretical ideas. ", "page_idx": 5}, {"type": "text", "text": "Remark 2. While our notion of consistent behaviors is quite natural, it might as well be that there is a more general notion of complexity for the opponent\u2019s response function classes that fully characterizes learnability in this setting. This likely requires the definition of appropriate norms in the input policy space $\\Pi^{m}$ and the output policy space $\\Psi$ , and a certain notion of predictability for the opponent\u2019s response function classes (e.g., in the spirit of Eluder dimension [Russo and Van Roy, 2013]), so that the learner can accurately estimate the opponent\u2019s response function, without trying out all possible policies. This question goes beyond the scope of our current work and is left to a future investigation. ", "page_idx": 5}, {"type": "text", "text": "Remark 3. To permit learnability in terms of external regret in Markov games, Liu et al. [2022] consider a policy-revealed setting, wherein the opponent reveals his current strategy to the learner at the end of each episode. No external regret is possible because the benchmark in external regret evaluates the learner\u2019s comparator policy against the same policy that the opponent reveals. For policy regret, however, knowing the opponent\u2019s strategy at the end of the episode gives the learner no advantage in general, as the counterfactual benchmark requires evaluating the learner\u2019s policies against the policy sequence that the opponent would have reacted with. Indeed, our lower bound in Theorem 3 still applies to the policy-revealed setting. ", "page_idx": 5}, {"type": "text", "text": "For $m$ -memory bounded, stationary and consistent adversaries, we present two algorithms, one for $m=1$ and the other for general $m\\geq1$ , with sublinear policy regret. We give special consideration to the case with $m=1$ as it helps with the exposition of key algorithmic design principles rather simply. For simplicity, we focus on $\\Pi$ being the set of all deterministic policies (i.e., $\\dot{|\\Pi|}=\\dot{\\Theta}(A^{H S}))$ . Our algorithms and upper bounds easily extend to any general $\\Pi$ with polynomial log-cardinality. ", "page_idx": 5}, {"type": "text", "text": "Assumption 5.1. The learner\u2019s policy class \u03a0 is the set of all deterministic policies. ", "page_idx": 5}, {"type": "text", "text": "A key component of our algorithms is using maximum likelihood estimation (MLE) [Geer, 2000] to estimate action distributions with which the opponent can respond. As is the convention in MLE analysis, we make a realizability assumption and use bracketing numbers to control the model class. ", "page_idx": 5}, {"type": "text", "text": "Assumption 5.2. For any policy $\\mu\\in\\Psi$ that the adversary employs and for all $(h,s)\\in[H]\\times S,$ , assume $\\mu_{h}(\\cdot|s)\\in P_{\\Theta}:=\\{P_{\\theta}\\in\\Delta(\\mathcal{B}):\\theta\\in\\Theta\\}$ , where the set $P_{\\Theta}$ has $\\epsilon$ -bracketing number $\\mathcal{N}_{\\Theta}(\\epsilon)$ w.r.t. $l_{1}$ norm, defined as the minimum number of $\\epsilon$ -brackets $\\overline{{[l,u]:=\\{P_{\\theta}\\in P_{\\Theta}:l\\le P_{\\theta}\\le u\\}}}$ with $\\lVert l-u\\rVert_{1}\\leq\\epsilon,$ , that are needed to cover $P_{\\Theta}$ . ", "page_idx": 6}, {"type": "text", "text": "Intuitively, restricting the adversary to be consistent, allows the learner to predict the opponent\u2019s response from previous episodes to similar settings. The learner can collect the data from what the adversary responds to and learn his response function. Given the consistent behavior, for every $(h,s)\\in[H]\\times S$ , the number of action distributions $\\mu_{h}(\\cdot|s)$ that the adversary can respond with cannot exceed the number of possible action distributions $\\pi_{h}(\\cdot|s)$ that the learner can construct in state $s$ at step $h$ . Given $\\Pi$ is the set of all deterministic policies, we only need to learn $H S A$ action distributions that the adversary can respond at any state and step. We begin with the oblivious case of $m=1$ and end up resolving the general case $m\\geq1$ after. ", "page_idx": 6}, {"type": "text", "text": "5.1 Memory of length $m=1$ ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first consider the memory length of $m=1$ for stationary and consistent adversaries. ", "page_idx": 6}, {"type": "text", "text": "Algorithm. We propose OPO-OMLE (Algorithm 1), which represents Optimistic Policy Optimization with Optimistic Maximum Likelihood Estimation. OPO-OMLE is a variant of the optimistic value iteration algorithm of [Azar et al., 2017], wherein we build an upper confidence bound on the value function $V_{1}^{\\pi,f(\\pi)}$ for any policy $\\pi$ , using a bonus function and optimistic MLE [Liu et al., 2023]. The upper confidence bound is based on two levels of optimism: a bonus term $\\beta$ that is based on confidence intervals on the transition kernels $P$ and the parameter version spaces $\\{\\Theta_{h s a}\\}$ of the adversary\u2019s response at each level $(h,s,a)$ . The parameter version spaces construct a set of parameters that are close to the MLE solution, up to an error $\\alpha$ , in terms of the log-likelihood in the observed actions taken by the adversary. ", "page_idx": 6}, {"type": "text", "text": "Algorithm 1 Optimistic Policy Optimization with Optimistic MLE (OPO-OMLE)   \n1: Input: Bonus function $\\beta:\\mathbb{N}\\rightarrow\\mathbb{R}$ , and MLE confidence parameter $\\alpha$   \n2: Initialize: $\\Theta_{h s a}\\gets\\Theta,D_{h s a}\\gets\\emptyset,N_{h}(s,a,b)\\gets0,N_{h}(\\bar{s_{}}a,b,s^{\\prime})\\gets0,\\forall(h,s,a,b,s^{\\prime})\\in\\Theta.$ $S\\times A\\times B\\times S$   \n3: for episode $t=1,\\dots,T$ do   \n4: $\\pi^{t}\\in$ arg maxDOUBLY_OPTIMISTIC_VALUE_ESTIMATE $(N,\\{D_{i}\\},\\{\\Theta_{i}\\},\\pi,\\beta)$ \u03c0\u2208\u03a0   \n5: P(lAaly $\\pi^{t}$ i t(thhme  2o)pponent responds with $f(\\pi^{t}))$ to observe $\\big(s_{1}^{t},a_{1}^{t},b_{1}^{t},r_{1}^{t},\\dots,s_{H}^{t},a_{H}^{t},b_{H}^{t},r_{H}^{t}\\big)$   \n6: $\\forall h$ : $N_{h}(s_{h}^{t},a_{h}^{\\bar{t}},b_{h}^{t})\\gets N_{h}(s_{h}^{t},a_{h}^{t},b_{h}^{t})+1$ , $N_{h}(s_{h}^{t},a_{h}^{t},b_{h}^{t},\\stackrel{\\sim}{s}_{h+1}^{t})^{-}\\tilde{\\leftarrow}N_{h}(s_{h}^{t},\\stackrel{\\sim}{a}_{h}^{t},\\dot{b}_{h}^{t},\\stackrel{\\sim}{s}_{h+1}^{t})^{+}$ 1, $D_{h s_{h}^{t}a_{h}^{t}}\\,\\leftarrow\\,D_{h s_{h}^{t}a_{h}^{t}}\\cup\\{b_{h}^{t}\\}$ , and $\\begin{array}{r}{\\Theta_{h s_{h}^{t}a_{h}^{t}}\\;\\leftarrow\\;\\{\\theta\\;\\in\\;\\Theta_{h s_{h}^{t}a_{h}^{t}}\\;:\\;\\sum_{b\\in D_{h s_{h}^{t}a_{h}^{t}}}\\log P_{\\theta}(b)\\;\\geq\\;}\\end{array}$ $\\begin{array}{r}{\\operatorname*{max}_{\\theta\\in\\Theta_{h s_{h}^{t}a_{h}^{t}}}\\sum_{b\\in D_{h s_{h}^{t}a_{h}^{t}}}\\log P_{\\theta}(b)-\\alpha\\}}\\\\ {.}\\end{array}$   \n7: end for ", "page_idx": 6}, {"type": "text", "text": "Algorithm 2 DOUBLY_OPTIMISTIC_VALUE_ESTIMATE $(N,\\{D_{i}\\},\\{\\Theta_{i}\\},\\pi,\\beta)$   \n1: Initialize: $\\bar{V}_{H+1}^{\\pi}=0$   \n2: $\\begin{array}{r}{\\hat{P}_{h}(s^{\\prime}|s,a,b)=\\frac{1}{S}}\\end{array}$ if $N_{h}(s,a,b)=0$ ; otherwise, $\\hat{P}_{h}(s^{\\prime}|s,a,b)=N_{h}(s,a,b,s^{\\prime})/N_{h}(s,a,b)$   \n3: for $h=H,H-1,\\ldots,1$ do   \n4: $\\begin{array}{r l r}&{\\bar{Q}_{h}^{\\pi}(s,a,b)=\\operatorname*{min}\\left\\{[\\hat{P}_{h}\\bar{V}_{h+1}^{\\pi}](s,a,b)+r_{h}(s,a,b)+\\beta(N_{h}(s,a,b)),H-h+1\\right\\},\\forall(s,a,b)}&\\\\ &{\\bar{V}_{h}^{\\pi}(s)=\\operatorname*{max}_{\\theta\\in\\Theta_{h s\\pi_{h}(s)}}\\bar{Q}_{h}^{\\pi}(s,\\pi_{h},P_{\\theta}),\\forall s}&{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\end{array}$   \n5:   \n6: end for ", "page_idx": 6}, {"type": "text", "text": "7: Output: $\\bar{V}_{1}^{\\pi}$ ", "page_idx": 6}, {"type": "text", "text": "Theoretical guarantee. We now present a theoretical guarantee for OPO-OMLE. ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Theorem 4. In Algorithm 1, choose $\\beta(t)\\,=\\,c H\\sqrt{\\frac{\\iota\\!+\\!\\log|\\Pi|}t}$ \u03b9+lotg |\u03a0|, where \u03b9 := log(SABHT/\u03b4), and $\\alpha=c\\log(\\ensuremath{\\mathcal{N}_{\\Theta}}(1/T)H\\ensuremath{\\mathit{S A T}}/\\delta)$ . With probability at least $1-\\delta$ , we have ", "page_idx": 6}, {"type": "equation", "text": "$$\nP R(T)=\\mathcal{O}\\left(H^{3}S^{2}A B\\iota\\log T+H^{2}\\sqrt{S A B T(\\iota+\\log|\\Pi|)}+H^{2}\\sqrt{S A T\\alpha}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Theorem 4 shows that OPO-OMLE achieves $\\sqrt{T}$ -policy regret bounds against 1-memory bounded, stationary and consistent adversaries in Markov games. Notably, the policy regret depends only on the log-cardinality of the learner\u2019s policy class $\\Pi$ and the log-bracketing number of the set of action distributions with which the adversary responds to the learner. Since $|\\mathbf{\\overline{{{\\Pi}}}}|=A^{H S}$ , the bound translates into $\\mathrm{PR}(T)=\\tilde{\\mathcal{O}}(H^{3}S^{2}A B+\\sqrt{H^{5}S A^{2}B T})$ . ", "page_idx": 7}, {"type": "text", "text": "Finally, comparing the lower bound of $\\Omega(\\operatorname*{min}\\{\\sqrt{H^{3}S A T},H T\\})$ for single-agent MDPs [Domingues et al., 2021], which applies t\u221ao this setting, the dominating term in our upper bound (Theorem 4) is worse only by a factor of $H{\\sqrt{A B}}$ \u2013 this is due to the need to learn the opponent\u2019s moves.4 ", "page_idx": 7}, {"type": "text", "text": "5.2 Memory of any fixed length $m\\geq1$ ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now consider the general case of stationary and consistent adversaries that have a memory of any fixed length $m\\geq1$ . Note that we assume that the learner knows (an upper bound of) $m$ . Playing against a 1-memory bounded adversary does not stop the learner from changing her policies often, as the adversary does not remember any policies that the learner has taken previously. However, a sublinear policy regret learner against $m$ -memory bounded adversaries should switch her policies as less frequently as possible, and at most only sublinear time switches. The reason is that every policy switch will add a constant cost to policy regret, as the benchmark in the policy regret is with the best fixed sequence of policy. This makes the regret minimizer OPO-OMLE unable to generalize from $m=1$ to any fixed $m$ . Instead, we propose a low-switching algorithm, in which the learner learns to play exploratory policies repeatedly over consecutive episodes so that the switching cost is reduced. Here, as in Jin et al. [2020], exploratory policies are those with good coverage over the state space from which uniform policy evaluation can be performed to identify near-optimal policies. ", "page_idx": 7}, {"type": "text", "text": "Algorithm. We propose APE-OVE (Algorithm 3), which represents Adaptive Policy Elimination by Optimistic Value Estimation. APE-OVE generalizes the adaptive policy elimination algorithm of [Qiao et al., 2022] for MDPs to Markov games with unknown opponents. The high-level idea of our algorithm is as follows. The learner maintains a version space $\\bar{\\Pi}^{k}$ of remaining high-quality policies after each epoch \u2013 which is a sequence of consecutive episodes with an appropriate length (epoch $k$ has a length of $H S A B(m-1+T_{k})$ in APE-OVE). ", "page_idx": 7}, {"type": "text", "text": "\u2022 Layerwise exploration (Line 5 of Algorithm 3): Within each epoch, the learner performs layerwise exploration (Algorithm 4), wherein we devise high-coverage sampling policies $\\pi^{k h s a b}$ that aim at exploring $(s,a,b)$ in step $h$ and epoch $k$ , starting from the lowest layer $h=1$ up to the highest layer $h=H$ . However, some states might not be visited frequently by any policy, thus taking a large amount of exploration. They, fortunately, do not significantly affect the value functions of any policy and thus can be identified (by storing in $\\mathcal{U}^{k}$ ) and removed from exploration quickly (via the truncated transition kernel estimates $\\hat{P}$ obtained in Algorithm 5). Layerwise exploration requires value estimation uniformly over all policies. However, the learner does not know the adversary\u2019s response $f$ . To address this, we use optimistic value estimation via the optimistic MLE in the collected data of the adversary\u2019s moves (Algorithm 6). ", "page_idx": 7}, {"type": "text", "text": "\u2022 Version space refinement (Line 6 of Algorithm 3): After the layerwise exploration, we refine the version space of policies that the learner can choose from at the next epoch using the optimistic value estimation based on the empirical transition kernels ${\\hat{P}}^{k}$ , the parameter version space $\\Theta^{k}$ and the set of infrequent transition samples $\\mathcal{U}^{k}$ given any reward function $r$ . The version space is designed in such a way that the expected value \u221afor the learner to play any policy $\\pi$ from the version space is guaranteed to be no worse than $\\tilde{\\mathcal{O}}(1/\\sqrt{T_{k}})$ compared to the optimal, with high probability. ", "page_idx": 7}, {"type": "text", "text": "Note that we do not directly use the reward function $r$ in the version space refinement. Instead, we use a truncated reward function $r_{\\mathcal{U}^{k}}$ that is zero for any $(h,s,a,b,s^{\\prime})$ in the infrequent transition set $\\mathcal{U}^{k}$ . This truncated design is critical to our analysis and the subsequent guarantees, e.g., see Lemma B.10. For the truncated reward functions, the backup step in Algorithm 6 should be understood as: $\\bar{Q}_{h}^{\\pi}(s,a,b)=\\mathbb{E}_{s^{\\prime}\\sim\\hat{P}_{h}^{k}(\\cdot\\vert s,a,b)}\\left[r_{h}(s,a,b)1\\{(h,s,a,b,s^{\\prime})\\notin\\dot{\\mathcal{U}}^{k}\\}+\\breve{V}_{h+1}^{\\pi}(s^{\\prime})\\right],\\forall(s,a,b).$ . ", "page_idx": 7}, {"type": "text", "text": "We now present a theoretical guarantee for APE-OVE. We bound policy regret in terms of an instance-dependent quantity, namely minimum positive visitation probability, defined as follows. ", "page_idx": 7}, {"type": "text", "text": "Algorithm 3 Adaptive Policy Elimination by Optimistic Value Estimation (APE-OVE) ", "page_idx": 8}, {"type": "text", "text": "1: Input: number of episodes $T$ , reward function $r$   \n2: Parameters: $\\begin{array}{r}{\\alpha:=\\bar{\\log(\\mathcal N_{\\Theta}(1/T)H S A T/\\delta)},\\bar{T}:=\\operatorname*{min}\\{t\\in\\mathbb N:(m-1)\\log\\log t+t\\ge\\frac{T}{H S A B}\\},}\\end{array}$ , $K=\\mathcal{O}(\\log\\log\\bar{T})$ , and $T_{k}:={\\bar{T}}^{1-{\\frac{1}{2^{k}}}},\\forall k\\in[K]$   \n3: Initialize: $\\Pi^{1}={\\dot{\\Pi}}$ , $\\Theta^{1}=\\Theta$   \n4: for epoch $k=1,\\ldots,K$ do   \n5: $\\hat{P}^{k},\\Theta^{k},\\mathcal{U}^{k}=\\mathrm{LA}$ YERWISE_EXPLORATION $(\\Pi^{k},T_{k})$ (Algorithm 4)   \n6: $\\begin{array}{r l}&{\\Pi^{k+1}:=\\left\\{\\pi\\in\\Pi:\\bar{V}^{\\pi}(r_{\\mathcal{U}^{k}},\\bar{P}^{k},\\Theta^{k})\\geq\\underset{\\pi\\in\\Pi}{\\operatorname*{max}}\\bar{V}^{\\pi}(r_{\\mathcal{U}^{k}},\\hat{P}^{k},\\Theta^{k})-c H^{2}S A B\\sqrt{\\alpha/(d^{*}T_{k})}\\right\\}}\\\\ &{\\mathrm{where}\\quad r_{\\mathcal{U}^{k}}(s_{1},a_{1},b_{1},\\dots,s_{H},a_{H},b_{H})\\quad\\underset{\\rightharpoonup=\\dots}{:=\\dots}\\sum_{h\\in[H]}1\\{(h,s_{h},a_{h},b_{h},s_{h+1})\\underset{\\rightharpoonup=\\dots}{\\overset{e}{\\dots}}\\}}\\\\ &{\\dots}\\end{array}$ $\\mathcal{U}^{k}\\}r_{h}(s_{h},a_{h},b_{h})$ and V\u00af \u03c0(r, P, \u0398) := OPTIMISTIC_VALUE_ESTIMATE $(\\pi,r,P,\\Theta)$ is given in Algorithm 6   \n7: end for ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "$\\mathbf{Algorithm4\\LAYERWISE\\_EXPLORATION}(\\Pi^{k},T_{k})$ ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "1: Input: Policy version space $\\Pi^{k}$ , number of episodes $T_{k}$   \n2: Initialize: $\\hat{P^{k}}=\\{\\hat{P_{h}^{k}}\\}_{h\\in[H]}$ arbitrary transition kernels, $\\mathcal{U}^{k}=\\emptyset$ , $\\Theta_{h s a}^{k}=\\Theta,\\forall(h,s,a).$ , $\\mathcal{D}=\\emptyset$ , $N_{h}^{k}(s,a,b,s^{\\prime})=0,\\forall(h,s,a,b,s^{\\prime})$ , and for each $(h,s,a,b),1_{h s a b}$ is the reward function $r^{\\prime}$ such $r_{h^{\\prime}}^{\\prime}(s^{\\prime},a^{\\prime},b^{\\prime})=1\\{(h^{\\prime},s^{\\prime},a^{\\prime},b^{\\prime})=(h,s,a,b)\\}$   \n3: for $h=1,\\ldots,H$ do   \n4: for $(s,a,b)\\in\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{B}$ do   \n5: \u03c0khsab = arg maxOPTIMISTIC_VALUE_ESTIMATE $(\\pi,1_{h s a b},\\hat{P}^{k},\\Theta^{k})$ (Algo\u03c0\u2208\u03a0k rithm 6)   \n6: Play $\\pi^{k h s a b}$ for $m-1$ episodes (and collect nothing)   \n7: Keep playing $\\pi^{k h s a b}$ for $T_{k}$ episodes and add all the transitions only at step $h$ to $\\mathcal{D}$   \n8: end for   \n10: 9: $\\begin{array}{r l}&{^{l\\mathrm{v}_{h}}\\{s,u,v,s\\ \\}\\gets^{l\\mathrm{v}_{h}}\\{s,u,v,s\\ \\}+\\mathbf{1},\\forall\\{s,u,v,s\\}\\setminus\\Omega.\\mathsf{s.t.}\\ \\{\\iota,^{\\mathrm{\\#}},s,u,v,s\\ \\}\\in\\mathcal{V}}\\\\ &{\\Theta_{h s a}^{k}=\\{\\theta\\in\\Theta_{h s a}^{k}:\\displaystyle\\sum_{b:(h,s,a,b)\\in\\mathcal{D}}P_{\\theta}(b)\\geq\\displaystyle\\operatorname*{max}_{\\theta\\in\\Theta_{h s a}^{k}}\\displaystyle\\sum_{b:(h,s,a,b)\\in\\mathcal{D}}P_{\\theta}(b)-\\alpha\\},\\forall(s,a)\\in\\mathcal{V}}\\\\ &{\\mathcal{U}^{k}\\gets\\mathcal{U}^{k}\\cup\\big\\{(h,s,a,b,s^{\\prime}):N_{h}^{k}(h,s,a,b,s^{\\prime})\\leq c H^{2}\\log(S A B H K/\\delta)\\big\\}}\\\\ &{\\hat{P}_{h}^{k}=\\mathrm{TRANSITON\\mathrm{_{-}E S T I M A T E}}(h,N_{h}^{k},\\mathcal{U}^{k},s^{\\dagger})\\ (\\mathrm{Algorithm}\\ 5)}\\\\ &{\\mathrm{Reset\\}\\mathcal{D}=\\emptyset}\\end{array}$ N hk(s, a, b, s\u2032) \u2190N hk(s, a, b, s\u2032) + 1, \u2200(s, a, b, s\u2032) s.t. (h, s, a, b, s\u2032) \u2208D S \u00d7 A   \n11:   \n12:   \n13:   \n14: end for   \n15: Output: $\\hat{P}^{k}=\\{\\hat{P}_{h}^{k}\\}_{h\\in[H]},\\Theta^{k},\\mathcal{U}^{k}$ ", "page_idx": 8}, {"type": "text", "text": "Definition 4 (Minimum positive visitation probability). The quantity $d^{\\ast}:=\\operatorname*{inf}_{h,s,a:d_{h}^{\\ast}(s,a)>0}d_{h}^{\\ast}(s,a)$ is said to be the minimum positive visitation probability, where $\\begin{array}{r l r}{d_{h}^{*}(s,a)}&{{}}&{:=}\\end{array}$ nf\u03c0\u2208\u03a0:dh\u03c0,f([\u03c0]m)(s,a)>0 d\u03c0h,f([\u03c0]m)(s, a). ", "page_idx": 8}, {"type": "text", "text": "The minimum positive visitation probability \u2013 which has also been used recently to characterize instance-dependent bounds for PAC RL [Tirinzoni et al., 2023], is the minimal probability that any state-action pair can be visited at a time step, given they can be visited at all. This implies that during the exploration phase, if we try a certain policy $\\pi$ for $N$ episodes and encounter $(s,a)$ at step $h$ (in any episode), on average, $\\pi$ would visit $(h,s,a)$ for $N d^{*}$ times out of $N$ episodes. This, along with the assumption that the adversary is consistent enables us to es\u221atimate the adversary\u2019s response to any $(h,s,a)$ that is visited within an estimation error of order $1/\\sqrt{N d^{*}}$ . Note that we do not need to take care of the adversary\u2019s response to any $(h,s,a)$ that is not visited as these tuples are deemed infrequent by any policy and thus have negligible impact on the value estimation. ", "page_idx": 8}, {"type": "text", "text": "Theorem 5. Playing APE-OVE against any $m$ -memory bounded, stationary, and consistent adversaries in any Markov game for $T$ episodes, with $\\begin{array}{r}{T=\\Tilde{\\Omega}(\\operatorname*{max}\\{\\frac{H^{5}A B(d^{*})^{\\dot{2}}}{S^{3}},(m-1)H S A B\\})}\\end{array}$ , and ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\tau\\gtrsim\\operatorname*{min}\\{\\frac{H^{5}S A B(d^{*})^{2}\\log^{4}(H S A B K/\\delta)}{\\alpha^{2}},\\frac{H^{9}(d^{*})^{2}\\log^{4}(H S A B K/\\delta)}{(S A B)^{3}\\alpha^{2}},\\frac{H^{13}\\log^{2}(H S A B K/\\delta)}{(A B)^{3}S^{5}}\\},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "1: $\\begin{array}{r}{\\hat{P}_{h}(s^{\\prime}|s,a,b)=\\left\\{\\frac{N_{h}(s,a,b,s^{\\prime})}{N_{h}(s,a,b)},\\forall(s,a,b,s^{\\prime})\\right.}\\\\ {\\left.0,\\forall(s,a,b,s^{\\prime})\\mathrm{~s.t.~}(h,s,}\\end{array}\\right.}\\end{array}$ NNh(hs(,sa,,ab,,bs)\u2032 ), \u2200(s, a, b, s\u2032) s.t. (h, s, a, b, s\u2032) \u2208/U   \n$(h,s,a,b,s^{\\prime})\\in\\mathcal{U}$   \n2: $\\begin{array}{r}{\\hat{P}_{h}(s^{\\dagger}|s,a,b)=1-\\sum_{s^{\\prime}\\in\\mathcal{S}:(h,s,a,b,s^{\\prime})\\notin\\mathcal{U}}\\hat{P}_{h}(s^{\\prime}|s,a,b),\\forall(s,a,b)\\in\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{B}}\\end{array}$   \n3: $\\hat{P}_{h}(s^{\\dagger}|s^{\\dagger},a,b)=1,\\forall(a,b)\\in\\mathcal{A}\\times\\mathcal{B}$   \n4: Output: $\\hat{P}_{h}$ ", "page_idx": 9}, {"type": "text", "text": "$\\mathbf{Algorithm~6~OPTIMISTIC\\_VALUE\\_ESTIMATE}(\\pi,r,P,\\Theta)$ ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "1: Input: reward function $r$ , policy $\\pi$ , transition kernel $P$ , parameter version space \u0398   \n2: Initialize: $\\bar{V}_{H+1}^{\\pi}(\\cdot)=0$   \n3: for $h=H,H-1,\\ldots,1\\,\\mathbf{do}$   \n4: $\\bar{Q}_{h}^{\\pi}(s,a,b)=r_{h}(s,a,b)+[P_{h}\\bar{V}_{h+1}^{\\pi}](s,a,b),\\forall(s,a,b)$   \n5: $\\begin{array}{r}{\\bar{V}_{h}^{\\pi}(s)=\\operatorname*{max}_{\\theta\\in\\Theta_{h s\\pi_{h}(s)}}\\bar{Q}_{h}^{\\pi}(s,\\pi_{h}(s),P_{\\theta}),\\forall s}\\end{array}$ \u25b7Optimistic MLE   \n6: end for   \n7: Output: $\\bar{V}_{1}^{\\pi}(s_{1})$ ", "page_idx": 9}, {"type": "text", "text": "guarantees that with probability at least $1-\\delta$ ", "page_idx": 9}, {"type": "equation", "text": "$$\n^{\\gamma}\\!R(T)\\!=\\!{\\mathcal O}\\left((m-1)H^{2}S A B\\log\\log T\\!+\\!H^{3/2}\\sqrt{S A B}(H S A B+H^{2}+S^{3/2}A B)\\sqrt{\\frac{T\\alpha}{d^{*}}}\\log\\log T\\right)\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $d^{\\ast}$ is the minimum positive visitation probability and $\\alpha$ is as defined in Algorithm 3. ", "page_idx": 9}, {"type": "text", "text": "Theorem 5 asserts a $\\sqrt{T}$ policy regret bound against $m$ -memory bounded, stationary, and consistent adversaries in Markov games. Notably, our bounds grow linearly with memory length $m$ . Compared to the bound in Theorem 4, given $T$ is sufficiently large, the bound in Theorem 5 deals with the general memory length $m$ at the cost of a worse dependence on all other factors $H,S,A,B,d^{*}$ . Dealing with $\\zeta$ -approximately consistent adversaries (see Remark 1) will incur an additional term $\\mathcal{O}(T\\zeta)$ to the policy regret. ", "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we study learning in Markov games against adaptive adversaries and highlight the statistical hardness of learning in this setting. We identify a natural structural assumpti\u221aon on the response function of the adversary, wherein we provide two distinct algorithms that attain $\\sqrt{T}$ policy regret, one for the unit memory and the other for general memory length. ", "page_idx": 9}, {"type": "text", "text": "There are several notable gaps in our current understanding of policy regret in Markov games. First, we do not know if the dependence on the minimum positive visitation probability $d^{\\ast}$ when learning against $m$ -memory bounded opponents is necessary. In other words, can we derive minimax bounds that hold for any problem instance, regardless of how small $d^{\\ast}$ is, for the case of general $m$ ? While it seems to us that such a dependence is necessary (as it seems difficult otherwise to learn the opponent\u2019s response while also learning high-return policies), yet we are unable to prove or reject this conjecture. Second, as we state in Remark 2, we do not currently know the necessary conditions on opponent\u2019s response functions for learnability in this setting. This might as well require an alternate condition that generalizes our notion of consistent behaviors and fully characterizes the predictability of the opponent (in a similar way as the VC dimension characterizes learnability in statistical learning theory). Third, our theory currently views information, and not computation, as the main bottleneck and aims for policy regret minimization without worrying about computational complexity. As a result, some of the steps in our algorithms happen to be computationally inefficient. In particular, selecting a policy that maximizes the optimistic value function requires iterating over the learner\u2019s policy set, which is exponentially large. Can we hope for computationally efficient no-policy regret algorithms in Markov games? Fourth, our policy regret bounds scale with the cardinality of the state space and the action space, which could be large in many practical settings. Can we avoid such dependence by employing function approximation (e.g., neural networks)? ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research was supported, in part, by the DARPA GARD award HR00112020004, NSF CAREER award IIS-1943251, funding from the Institute for Assured Autonomy (IAA) at JHU, and the Spring\u201922 workshop on \u201cLearning and Games\u201d at the Simons Institute for the Theory of Computing. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. Flambe: Structural complexity and representation learning of low rank MDPs. Advances in neural information processing systems,   \n33:20095\u201320107, 2020. 17 Raman Arora, Ofer Dekel, and Ambuj Tewari. Online bandit learning against an adaptive adversary: from regret to policy regret. arXiv preprint arXiv:1206.6400, 2012. 2, 3, 4, 5, 16 Raman Arora, Michael Dinitz, Teodor Vanislavov Marinov, and Mehryar Mohri. Policy regret in repeated games. Advances in Neural Information Processing Systems, 31, 2018. 2, 3, 4 Jean-Yves Audibert and S\u00e9bastien Bubeck. Minimax policies for adversarial and stochastic bandits. In COLT, pages 217\u2013226, 2009. 5 Pranjal Awasthi, Kush Bhatia, Sreenivas Gollapudi, and Kostas Kollias. Congested bandits: Optimal routing via short-term resets. In International Conference on Machine Learning, pages 1078\u20131100. PMLR, 2022. 3 Mohammad Gheshlaghi Azar, Ian Osband, and R\u00e9mi Munos. Minimax regret bounds for reinforcement learning. In International conference on machine learning, pages 263\u2013272. PMLR, 2017.   \n7 Yu Bai and Chi Jin. Provable self-play algorithms for competitive reinforcement learning. In International conference on machine learning, pages 551\u2013560. PMLR, 2020. 2 Yu Bai, Chi Jin, and Tiancheng Yu. Near-optimal reinforcement learning with self-play. Advances in neural information processing systems, 33:2159\u20132170, 2020. 2 Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor Mordatch. Emergent tool use from multi-agent autocurricula. arXiv preprint arXiv:1909.07528,   \n2019. 1 M-F Balcan, Avrim Blum, Jason D Hartline, and Yishay Mansour. Mechanism design via machine learning. In 46th Annual IEEE Symposium on Foundations of Computer Science (FOCS\u201905), pages   \n605\u2013614. IEEE, 2005. 1 Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemys\u0142aw D\u02dbebiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019. 1 Kush Bhatia and Karthik Sridharan. Online learning with dynamics: A minimax perspective. Advances in Neural Information Processing Systems, 33:15020\u201315030, 2020. 3 Avrim Blum, Nika Haghtalab, and Ariel D Procaccia. Learning optimal commitment to overcome insecurity. Advances in Neural Information Processing Systems, 27, 2014. 4 Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for nearoptimal reinforcement learning. Journal of Machine Learning Research, 3(Oct):213\u2013231, 2002.   \n3 Mark Braverman, Jieming Mao, Jon Schneider, and S Matthew Weinberg. Multi-armed bandit problems with strategic arms. In Conference on Learning Theory, pages 383\u2013416. PMLR, 2019. 4 Noam Brown and Tuomas Sandholm. Superhuman AI for heads-up no-limit poker: Libratus beats top professionals. Science, 359(6374):418\u2013424, 2018. 1   \nRichard Cole and Tim Roughgarden. The sample complexity of revenue maximization. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing, pages 243\u2013252, 2014. 1   \nVincent Conitzer and Tuomas Sandholm. Complexity of mechanism design. arXiv preprint cs/0205075, 2002. 1   \nChristoph Dann, Tor Lattimore, and Emma Brunskill. Unifying PAC and regret: Uniform PAC bounds for episodic reinforcement learning. Advances in Neural Information Processing Systems, 30, 2017. 25   \nOfer Dekel, Jian Ding, Tomer Koren, and Yuval Peres. Bandits with switching costs: $T^{2/3}$ regret. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing, pages 459\u2013467, 2014. 3   \nLe Cong Dinh, David Henry Mguni, Long Tran-Thanh, Jun Wang, and Yaodong Yang. Online Markov decision processes with non-oblivious strategic adversary. Autonomous Agents and Multi-Agent Systems, 37(1):15, 2023. 3   \nOmar Darwiche Domingues, Pierre M\u00e9nard, Emilie Kaufmann, and Michal Valko. Episodic reinforcement learning in finite MDPs: Minimax lower bounds revisited. In Algorithmic Learning Theory, pages 578\u2013598. PMLR, 2021. 8   \nPaul D\u00fctting, Zhe Feng, Harikrishna Narasimhan, David Parkes, and Sai Srivatsa Ravindranath. Optimal auctions through deep learning. In International Conference on Machine Learning, pages 1706\u20131715. PMLR, 2019. 1   \nJerzy Filar and Koos Vrieze. Competitive Markov decision processes. Springer Science & Business Media, 2012. 4   \nSara A Geer. Empirical Processes in M-estimation, volume 6. Cambridge university press, 2000. 6, 17   \nThomas Dueholm Hansen, Peter Bro Miltersen, and Uri Zwick. Strategy iteration is strongly polynomial for 2-player turn-based stochastic games with a constant discount factor. Journal of the ACM (JACM), 60(1):1\u201316, 2013. 2   \nHoda Heidari, Michael J Kearns, and Aaron Roth. Tight policy regret bounds for improving and decaying bandits. In IJCAI, pages 1562\u20131570, 2016. 3   \nJunling Hu and Michael P Wellman. Nash q-learning for general-sum stochastic games. Journal of machine learning research, 4(Nov):1039\u20131069, 2003. 2   \nMax Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C Rabinowitz, Ari S Morcos, Avraham Ruderman, et al. Humanlevel performance in 3d multiplayer games with population-based reinforcement learning. Science, 364(6443):859\u2013865, 2019. 1   \nChi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu. Reward-free exploration for reinforcement learning. In International Conference on Machine Learning, pages 4870\u20134879. PMLR, 2020. 8, 26   \nChi Jin, Qinghua Liu, Yuanhao Wang, and Tiancheng Yu. V-learning\u2013a simple, efficient, decentralized algorithm for multiagent RL. arXiv preprint arXiv:2110.14555, 2021. 4   \nChi Jin, Qinghua Liu, and Tiancheng Yu. The power of exploiter: Provable multi-agent RL in large state spaces. In International Conference on Machine Learning, pages 10251\u201310279. PMLR, 2022. 3   \nTomer Koren, Roi Livni, and Yishay Mansour. Bandits with movement costs and adaptive pricing. In Conference on Learning Theory, pages 1242\u20131268. PMLR, 2017a. 3   \nTomer Koren, Roi Livni, and Yishay Mansour. Multi-armed bandits with metric movement costs. Advances in Neural Information Processing Systems, 30, 2017b. 3   \nJeongyeol Kwon, Yonathan Efroni, Constantine Caramanis, and Shie Mannor. RL for latent MDPs: Regret guarantees and a lower bound. Advances in Neural Information Processing Systems, 34: 24523\u201324534, 2021. 5, 16   \nJoshua Letchford, Vincent Conitzer, and Kamesh Munagala. Learning and approximating the optimal strategy to commit to. In Algorithmic Game Theory: Second International Symposium, SAGT 2009, Paphos, Cyprus, October 18-20, 2009. Proceedings 2, pages 250\u2013262. Springer, 2009. 4   \nNir Levine, Koby Crammer, and Shie Mannor. Rotting bandits. Advances in neural information processing systems, 30, 2017. 3   \nDavid Lindner, Hoda Heidari, and Andreas Krause. Addressing the long-term impact of ml decisions via policy regret. arXiv preprint arXiv:2106.01325, 2021. 3   \nMichael L Littman. Markov games as a framework for multi-agent reinforcement learning. In Machine learning proceedings 1994, pages 157\u2013163. Elsevier, 1994. 2   \nQinghua Liu, Tiancheng Yu, Yu Bai, and Chi Jin. A sharp analysis of model-based reinforcement learning with self-play. In International Conference on Machine Learning, pages 7001\u20137010. PMLR, 2021. 2   \nQinghua Liu, Yuanhao Wang, and Chi Jin. Learning Markov games with adversarial opponents: Efficient algorithms and fundamental limits. In International Conference on Machine Learning, pages 14036\u201314053. PMLR, 2022. 2, 3, 4, 5, 6, 16   \nQinghua Liu, Praneeth Netrapalli, Csaba Szepesvari, and Chi Jin. Optimistic MLE: A generic model-based algorithm for partially observable sequential decision making. In Proceedings of the 55th Annual ACM Symposium on Theory of Computing, pages 363\u2013376, 2023. 7, 17   \nDhruv Malik, Yuanzhi Li, and Aarti Singh. Complete policy regret bounds for tallying bandits. In Conference on Learning Theory, pages 5146\u20135174. PMLR, 2022. 2, 3, 5   \nDhruv Malik, Conor Igoe, Yuanzhi Li, and Aarti Singh. Weighted tallying bandits: overcoming intractability via repeated exposure optimality. In International Conference on Machine Learning, pages 23590\u201323609. PMLR, 2023. 3   \nNeri Merhav, Erik Ordentlich, Gadiel Seroussi, and Marcelo J Weinberger. On sequential strategies for loss functions with memory. IEEE Transactions on Information Theory, 48(7):1947\u20131958, 2002. 2, 3, 4   \nMatej Morav\u02c7c\u00edk, Martin Schmid, Neil Burch, Viliam Lis\\`y, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. Science, 356(6337):508\u2013513, 2017. 1   \nJohn F. Nash. Equilibrium points in $n$ -person games. Proceedings of the National Academy of Sciences, 36(1):48\u201349, 1950. doi: 10.1073/pnas.36.1.48. 4   \nDan Qiao, Ming Yin, Ming Min, and Yu-Xiang Wang. Sample-efficient reinforcement learning with loglog (t) switching cost. In International Conference on Machine Learning, pages 18031\u201318061. PMLR, 2022. 8, 21   \nGiorgia Ramponi and Marcello Restelli. Learning in Markov games: can we exploit a general-sum opponent? In Uncertainty in Artificial Intelligence, pages 1665\u20131675. PMLR, 2022. 3, 5   \nDaniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic exploration. Advances in Neural Information Processing Systems, 26, 2013. 6   \nJulien Seznec, Andrea Locatelli, Alexandra Carpentier, Alessandro Lazaric, and Michal Valko. Rotting bandits are no harder than stochastic ones. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 2564\u20132572. PMLR, 2019. 3   \nShai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement learning for autonomous driving. arXiv preprint arXiv:1610.03295, 2016. 1   \nLloyd S Shapley. Stochastic games. Proceedings of the national academy of sciences, 39(10): 1095\u20131100, 1953. 2, 3   \nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of Go with deep neural networks and tree search. nature, 529(7587):484\u2013489, 2016. 1   \nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of Go without human knowledge. nature, 550(7676):354\u2013359, 2017. 1   \nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419): 1140\u20131144, 2018. 1   \nYi Tian, Yuanhao Wang, Tiancheng Yu, and Suvrit Sra. Online learning in unknown Markov games. In International conference on machine learning, pages 10279\u201310288. PMLR, 2021. 3, 4   \nAndrea Tirinzoni, Aymen Al-Marjani, and Emilie Kaufmann. Optimistic PAC reinforcement learning: the instance-dependent view. In International Conference on Algorithmic Learning Theory, pages 1460\u20131480. PMLR, 2023. 9   \nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha\u00ebl Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782):350\u2013354, 2019. 1   \nHeinrich Von Stackelberg. Market structure and equilibrium. Springer Science & Business Media, 2010. 5   \nChen-Yu Wei, Yi-Te Hong, and Chi-Jen Lu. Online reinforcement learning in stochastic games. Advances in Neural Information Processing Systems, 30, 2017. 2   \nChen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Linear last-iterate convergence in constrained saddle-point optimization. arXiv preprint arXiv:2006.09517, 2020. 2, 3   \nQiaomin Xie, Yudong Chen, Zhaoran Wang, and Zhuoran Yang. Learning zero-sum simultaneousmove Markov games using function approximation and correlated equilibrium. In Conference on learning theory, pages 3674\u20133682. PMLR, 2020. 2   \nYaodong Yang and Jun Wang. An overview of multi-agent reinforcement learning from game theoretical perspective. arXiv preprint arXiv:2011.00583, 2020. 1   \nKaiqing Zhang, Zhuoran Yang, and Tamer Ba\u00b8sar. Multi-agent reinforcement learning: A selective overview of theories and algorithms. Handbook of reinforcement learning and control, pages 321\u2013384, 2021. 1   \nTong Zhang. From $\\epsilon_{}$ -entropy to KL-entropy: Analysis of minimum information complexity density estimation. 2006. 17   \nZihan Zhang, Yuxin Chen, Jason D Lee, and Simon S Du. Settling the sample complexity of online reinforcement learning. arXiv preprint arXiv:2307.13586, 2023. 5   \nStephan Zheng, Alexander Trott, Sunil Srinivasa, Nikhil Naik, Melvin Gruesbeck, David C Parkes, and Richard Socher. The AI economist: Improving equality and productivity with AI-driven tax policies. arXiv preprint arXiv:2004.13332, 2020. 1 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1 Introduction ", "page_idx": 14}, {"type": "text", "text": "2 Related work ", "page_idx": 14}, {"type": "text", "text": "3 Problem setup 3 ", "page_idx": 14}, {"type": "text", "text": "4 Fundamental barriers for learning against adaptive adversaries 4 ", "page_idx": 14}, {"type": "text", "text": "5 Efficient algorithms for learning against adaptive adversaries 6   \n5.1 Memory of length $m=1$ . . 7   \n5.2 Memory of any fixed length $m\\geq1$ . . 8 ", "page_idx": 14}, {"type": "text", "text": "6 Discussion 10 ", "page_idx": 14}, {"type": "text", "text": "A Missing proofs for Section 4 16   \nA.1 Proof of Theorem 1 16   \nA.2 Proof of Theorem 2 16   \nA.3 Proof of Theorem 3 17   \nB Missing proofs for Section 5 17   \nB.1 Support lemmas 17   \nB.2 Proof of Theorem 4 17   \nB.3 Proof of Theorem 5 20   \nB.3.1 Sampling policies are sufficiently exploratory 21   \nB.3.2 Uniform policy evaluation 24 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Missing proofs for Section 4 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof of Theorem 1. The construction of a hard problem essentially follows the proof idea of Arora et al. [2012]. Policy regret requires the learner to compete with the best fixed sequence of policy in hindsight as if she could have changed her past policies. The lower bound utilizes this fact to construct an instance such that once the learner picks a particular policy in the first episode, she will receive a low reward for the remaining episodes. The only way to achieve a higher reward is to go back in time and select a different policy. ", "page_idx": 15}, {"type": "text", "text": "More formally, let\u2019s consider any learner. Let $\\pi^{1}$ be a policy that the learner commits in the first episode with the highest positive probability $p>0$ . Note that $\\pi^{1}$ and $p$ are the inherent property of the learner and do not depend on the adversary and the Markov game as in the first episode, the learner has zero information about the adversary and the Markov game. Now let\u2019s consider the adversary that depends only on the learner\u2019s policy in the first episode and nothing else, i.e., for all $t$ and policy sequence $\\bar{\\pi}^{1},\\ldots,\\pi^{t}$ , $f_{t}(\\pi^{1},\\cdot\\cdot\\cdot,\\pi^{\\bar{t}})\\,=\\,f(\\pi^{1})$ for some function $f:\\Pi\\to\\Psi$ . In addition, let $f$ such that $f(\\pi)=\\mu$ if $\\pi=\\pi^{1}$ and $f(\\pi)=\\nu$ otherwise, where $\\mu$ and $\\nu$ such that for all $s$ , $\\operatorname*{sup}_{\\pi}V_{1}^{\\pi,\\nu}(s)-\\operatorname*{sup}_{\\pi}\\dot{V}_{1}^{\\pi,\\mu}(s)=\\Omega(1)$ . There exists a Markov game that always guarantees the existence of such $\\mu,\\nu$ (the constructions are fairly straightforward). Thus, with probability $p$ , we have $\\mathrm{PR}(T)=\\Omega(T)$ . Note that the external regret $R(T)$ for this construction is 0. ", "page_idx": 15}, {"type": "text", "text": "A.2 Proof of Theorem 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof of Theorem 2. The proof follows from the two main arguments: (i) a reduction from any latent MDP [Kwon et al., 2021] to a Markov game with an adversary playing policies from a finite set of Markov policies, and (ii) a reduction from the notion of regret in latent MDPs to the policy regret w.r.t. an oblivious sequence of Markov policies. ", "page_idx": 15}, {"type": "text", "text": "Argument (i) is directly taken from [Liu et al., 2022, Proposition 5]. In particular, interacting with any latent MDP [Kwon et al., 2021] of $L$ latent variables, $S$ states, $A$ actions, $H$ time steps, and binary rewards is equivalent to interacting (from the perspective of the learner) a (simulated) Markov game against an adversary whose policies are chosen from a set of $L$ Markov policies. In particular, the simulated Markov game has $S A+S$ states, $A$ actions for the learner, $2S$ actions for the adversary, and $2H$ time steps (see [Liu et al., 2022, Section A.4] for the detailed construction of the simulated Markov game from any latent MDP). Thus, we can utilize any lower bound for latent MDP for the Markov game (but not vice versa). ", "page_idx": 15}, {"type": "text", "text": "To continue from Argument (i) and begin with Argument (ii), we recall the definition of latent MDPs [Kwon et al., 2021]. At the beginning of each episode, the nature secretly draws uniformly at random from a set of $L$ base MDPs and the learner interacts with this drawn MDP for the episode. [Kwon et al., 2021, Theorem 3.1] show that for any learner, there exists a latent MDP with $L$ base MDPs such that the learner needs at least $\\Omega((S\\dot{A}/L)^{L}/\\epsilon^{2})$ episodes to identify an $\\epsilon_{\\mathrm{:}}$ -suboptimal policy, where the optimality is defined with respect to the average values over the $M$ base MDPs. Note that in the construction of the hard latent MDP instance above, there is a unique optimal policy (let\u2019s call it $\\pi^{*}$ ) with respect to the aforementioned optimality notion. Thus, the regret of this learner over $T$ episodes competing against $\\pi^{*}$ is at least $\\Omega(\\sqrt{T(S A/L)^{L}})$ (the learner suffers an instantaneous regret of $\\epsilon$ every time she fails to identify $\\pi^{*}$ ). Note again that the regret above is the expectation with respect to the uniform distribution over $L$ base MDPs. Thus, there exists a particular realization of a sequence of $T$ base MDPs in a certain order such that the regret with respect to this sequence when competing with $\\pi^{*}$ is at least the expected regret with respect to the uniform distribution over $L$ base MDPs, which is $\\Omega(\\sqrt{T(S A/L)^{L}})$ . Finally, note that $\\pi^{*}$ is also an optimal policy with respect to the total value across the sequence of $T$ MDPs since $\\pi^{*}$ is an optimal policy for each individual base MDP, per the construction in Kwon et al. [2021]. Thus, we can conclude that, for any learner, there exists a sequence of $T$ MDPs from a set of $L$ MDPs such that the regret of the learner with respect to this MDP sequence is $\\Omega(\\sqrt{T(S A/L)^{L}})$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "A.3 Proof of Theorem 3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof of Theorem 3. Consider any learner. Consider the adversary\u2019s policy space $\\Psi=\\{\\mu,\\nu\\}$ where for all $\\dot{h}\\in[H-1]$ , $\\mu_{h}$ and $\\nu_{h}$ are arbitrary but $\\mu_{H}(b_{1}|s)=1,\\dot{\\forall}s$ and $\\dot{\\nu_{H}}(\\bar{b}_{2}|s)=1,\\ddot{\\forall}s$ , for some $b_{1},b_{2}\\in B$ . Let the reactive function $f$ to map all policies but some $\\pi^{*}$ in $\\Pi$ to $\\mu$ , whereas $f(\\pi^{*})=\\nu$ . Now consider a deterministic Markov game with the following properties. The transition kernel is deterministic and always traverses through the same sequence of states, regardless of what actions the learner and the adversary take. The reward functions are deterministic everywhere, and also zero everywhere except that $r_{H}(s,a,b_{2})=1,\\forall s,a.$ . Except for $\\pi^{*}$ that yields a positive reward if the learner selects it, all other policies in $\\Pi$ give zero reward. In addition, since the learner does not know $f$ and that there is no relation whatsoever between $f(\\pi)$ and $f(\\pi^{\\prime})$ for any $\\pi\\neq\\pi^{\\prime}$ , the learner needs to play all policies in $\\Pi$ at least once to be able to identify $\\pi^{*}$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "B Missing proofs for Section 5 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Support lemmas ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Maximum Likelihood Estimation. Let $\\{x_{i}\\}_{i\\in[T]}\\sim P_{\\theta^{*}}$ where $\\theta^{*}\\in\\Theta$ . Denote $\\mathcal{N}_{\\Theta}(\\epsilon)$ the $\\epsilon\\cdot$ - bracketing number of function class $\\{P_{\\theta}:\\theta\\in\\Theta\\}$ . The following lemma says that the log-likelihood of the true model in the empirical data is close to that of any model within the model class, up to an error that scales logarithmically with the model complexity measured in a bracketing number. ", "page_idx": 16}, {"type": "text", "text": "Lemma B.1. There exists an absolute constant c such that for any $\\delta\\in(0,1)$ , with probability at least $1-\\delta_{i}$ , for all $t\\in[T]$ and $\\theta\\in\\Theta$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{t}\\log{\\frac{P_{\\theta}(x_{i})}{P_{\\theta^{*}}(x_{i})}}\\leq c\\log(\\mathcal{N}_{\\Theta}(1/T)T/\\delta).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The following lemma says that any model that is close to the true model in the log-likelihood in the historical data would yield a similar data distribution as the true model. ", "page_idx": 16}, {"type": "text", "text": "Lemma B.2. There exists an absolute constant c such that for any $\\delta\\in(0,1)$ , with probability at least $1-\\delta_{i}$ , for all $t\\in[T]$ and $\\theta\\in\\Theta$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\nd_{T V}^{2}(P_{\\theta},P_{\\theta^{*}})\\leq\\frac{c}{t}\\left(\\sum_{i=1}^{t}\\log\\frac{P_{\\theta^{*}}(x_{i})}{P_{\\theta}(x_{i})}+\\log(\\mathcal{N}_{\\Theta}(1/T)T/\\delta)\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $d_{T V}$ denotes the total variation distance. ", "page_idx": 16}, {"type": "text", "text": "The two lemmas above directly follow from [Liu et al., 2023, Proposition B.1] and [Liu et al., 2023, Proposition B.2], respectively, wherein the analysis built on the classical analysis of MLE [Geer, 2000] and the \u201ctangent\u201d sequence analysis in [Zhang, 2006, Agarwal et al., 2020], respectively. The following lemma is a direct corollary of Lemma B.1 and Lemma B.2. ", "page_idx": 16}, {"type": "text", "text": "Lemma B.3. Let $\\begin{array}{r}{\\hat{\\theta}_{t}\\in\\arg\\operatorname*{sup}_{\\theta\\in\\Theta}\\sum_{i=1}^{t}\\log P_{\\theta}(x_{i})}\\end{array}$ . Define the version space: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Theta_{t}:=\\left\\{\\theta\\in\\Theta:\\sum_{i=1}^{t}\\log P_{\\theta}(x_{i})\\ge\\sum_{i=1}^{t}\\log P_{\\hat{\\theta}_{t}}(x_{i})-c\\log(\\mathcal{N}_{\\Theta}(1/T)T/\\delta)\\right\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, with probability at least $1-\\delta$ , for all $t\\in[T]$ , we have $\\theta^{*}\\in\\Theta_{t}$ and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\theta\\in\\Theta_{t}}d_{T V}(P_{\\theta},P_{\\theta^{*}})\\leq c\\sqrt{\\frac{\\log(\\mathcal{N}_{\\Theta}(1/T)T/\\delta)}{t}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B.2 Proof of Theorem 4 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We first introduce several notations that we will use throughout our proofs. We denote $N_{h}^{t}$ and $\\Theta_{i}^{t}$ the counters $N_{h}$ and the parameter confidence sets $\\Theta_{i}^{t}$ at the beginning of the episode $t$ . ", "page_idx": 16}, {"type": "text", "text": "Lemma B.4 (Optimism). With probability at least $1-\\delta,$ , for all $(h,s,\\pi,t)$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\bar{V}_{h}^{\\pi}(s)\\geq V_{h}^{\\pi,f(\\pi)}(s).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma B.4. We will prove a stronger statement: For any $(h,s,a,b,\\pi)$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$\\bar{Q}_{h}^{\\pi}(s,a,b)\\geq Q_{h}^{\\pi,f(\\pi)}(s,a,b)$ ", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We will prove by induction with $h\\in[H+1]$ . For $h=H+1$ , the claim in the lemma trivially holds. Assume by induction that the claim holds for some $h+1$ . We will prove that it holds for $h$ . Indeed, for any $(s,a,b)$ such that $\\bar{Q}_{h}^{\\pi}(s,a,b)=H-h+1$ , of course $\\bar{Q}_{h}^{\\pi}(s,\\stackrel{\\cdot}{a},b)\\geq Q_{h}^{\\pi,f(\\pi)}(s,a,b)$ Consider any $(s,a,b)$ such that $\\bar{Q}_{h}^{\\pi}(s,a,b)<H-h+1.$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{Q}_{h}^{\\pi}(s,a,b)-Q_{h}^{\\pi,f(\\pi)}(s,a,b)=[\\hat{P}_{h}\\bar{V}_{h+1}^{\\pi}](s,a,b)+r_{h}(s,a,b)+\\beta(N_{h}(s,a,b))}\\\\ &{\\phantom{\\quad\\quad}-([P_{h}V_{h+1}^{\\pi,f(\\pi)}](s,a,b)+r_{h}(s,a,b))}\\\\ &{\\phantom{\\quad\\quad}\\geq[(\\hat{P}_{h}-P_{h})V_{h+1}^{\\pi,f(\\pi)}](s,a,b)+\\beta(N_{h}(s,a,b))}\\\\ &{\\phantom{\\quad\\quad}\\geq0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the first inequality uses the induction assumption that $\\bar{V}_{h+1}^{\\pi}\\geq V_{h+1}^{\\pi,f(\\pi)}$ and the last inequality uses Hoeffding\u2019s inequality and the union bound. In addition, it follows from Lemma B.3 and the union bound that, with probability at least $1-\\delta$ , for any $(t,h,s,\\pi)$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\nf(\\pi)_{h}(\\cdot|s)\\in P_{\\Theta_{h s\\pi_{h}(s)}^{t}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Under the same event wherein the above relation holds, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{V}_{h}^{\\pi}(s)=\\underset{\\theta\\in\\Theta_{h s\\pi_{h}(s)}}{\\operatorname*{max}}\\quad\\bar{Q}_{h}^{\\pi}(s,\\pi_{h}(s),P_{\\theta})}\\\\ &{\\quad\\quad\\quad\\geq\\bar{Q}_{h}^{\\pi}(s,\\pi_{h}(s),f(\\pi)_{h})}\\\\ &{\\quad\\quad\\quad\\geq Q_{h}^{\\pi,f(\\pi)}(s,\\pi_{h}(s),f(\\pi)_{h})}\\\\ &{\\quad\\quad\\quad=V_{h}^{\\pi,f(\\pi)}(s).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This completes the case for $h+1$ and thus completes the proof. ", "page_idx": 17}, {"type": "text", "text": "Proof of Theorem 4. By the optimism of $\\bar{V}$ (Lemma B.4), with probability at least $1-\\delta$ , for all $(t,\\pi)$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\nV_{1}^{\\pi,f(\\pi)}(s_{1}^{t})-V_{1}^{\\pi^{t},f(\\pi^{t})}(s_{1}^{t})\\le\\bar{V}_{1}^{\\pi}(s_{1}^{t})-V_{1}^{\\pi^{t},f(\\pi^{t})}(s_{1}^{t})\\le\\bar{V}_{1}^{\\pi^{t}}(s_{1}^{t})-V_{1}^{\\pi^{t},f(\\pi^{t})}(s_{1}^{t})=\\Delta_{1}^{t}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the second inequality follows from Line 4 of Algorithm 1, and the last equation is a result of what we now define: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta_{h}^{t}:=\\bar{V}_{h}^{\\pi^{t}}(s_{h}^{t})-V_{h}^{\\pi^{t},f(\\pi^{t})}(s_{h}^{t}),\\forall(t,h).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We now decompose $\\Delta_{h}^{t}$ as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{r l}&{\\Delta_{h}^{t}=\\underset{\\theta\\in\\Theta_{h,h,q}^{\\mathrm{max}}}{\\operatorname*{max}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We will bound each of $\\xi_{h}^{t},\\zeta_{h}^{t},\\gamma_{h}^{t}$ separately as follows. ", "page_idx": 17}, {"type": "text", "text": "Bounding $\\{\\xi_{h}^{t}\\}$ . For simplicity, we denote $\\boldsymbol{x}_{h}^{t}=(\\boldsymbol{s}_{h}^{t},\\boldsymbol{a}_{h}^{t},\\boldsymbol{b}_{h}^{t})$ . We define ", "page_idx": 18}, {"type": "equation", "text": "$$\nV_{h+1}^{*}(s)=\\operatorname*{sup}_{\\pi\\in\\Pi}V_{h+1}^{\\pi,f(\\pi)}(s),\\forall s.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that the optimality above does not require that there exists an optimal policy $\\pi^{*}$ such that $V_{h}^{\\ast}(s)=V_{h}^{\\pi^{\\ast},f(\\pi^{\\ast})}(s),\\forall(h,s)$ . Note that if $\\bar{Q}_{h}^{\\pi^{t}}(x_{h}^{t})=H-h+1$ , it is trivial that $\\zeta_{h}^{t}\\leq0$ . Thus, we only need to consider when $\\bar{Q}_{h}^{\\pi^{t}}(x_{h}^{t})<H-h+1$ , and thus ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{\\Xi}_{h}^{\\cdot}=[\\hat{P}_{h}^{t}\\bar{V}_{h+1}^{\\pi^{t}}](x_{h}^{t})+\\beta(N_{h}^{t}(x_{h}^{t}))-[P_{h}{V}_{h+1}^{\\pi^{t},f(\\pi^{t})}](x_{h}^{t})}\\\\ &{\\quad=[(\\hat{P}_{h}^{t}-P_{h}){V}_{h+1}^{*}](x_{h}^{t})+[(\\hat{P}_{h}^{t}-P_{h})(\\bar{V}_{h+1}^{\\pi^{t}}-{V}_{h+1}^{*})](x_{h}^{t})+[P_{h}(\\bar{V}_{h+1}^{\\pi^{t}}-{V}_{h+1}^{\\pi^{t},f(\\pi^{t})})](x_{h}^{t})+\\beta(N_{h}^{t}(x_{h}^{t}))}\\\\ &{\\quad\\le[(\\hat{P}_{h}^{t}-P_{h})(\\bar{V}_{h+1}^{\\pi^{t}}-{V}_{h+1}^{*})](x_{h}^{t})+[P_{h}(\\bar{V}_{h+1}^{\\pi^{t}}-{V}_{h+1}^{\\pi^{t},f(\\pi^{t})})](x_{h}^{t})+2\\beta(N_{h}^{t}(x_{h}^{t})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By Bernstein\u2019s inequality, with probability at least $1-\\delta$ , for all $(s,a,b,s^{\\prime},h,t)$ and with $\\iota:=$ $\\dot{\\log}(2S^{2}A B H T/\\delta)$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\hat{P}_{h}^{t}(s^{\\prime}|s,a,b)-P_{h}(s^{\\prime}|s,a,b)\\le\\frac{\\iota}{N_{h}^{t}(s,a,b)}+\\sqrt{\\frac{2P_{h}(s^{\\prime}|s,a,b)\\iota}{N_{h}^{t}(s,a,b)}}}}\\\\ &{}&{\\le\\frac{1}{H}P_{h}(s^{\\prime}|s,a,b)+\\frac{H\\iota}{2N_{h}^{t}(s,a,b)}+\\frac{\\iota}{N_{h}^{t}(s,a,b)}}\\\\ &{}&{=\\frac{1}{H}P_{h}(s^{\\prime}|s,a,b)+(1+\\frac{H}{2})\\frac{\\iota}{N_{h}^{t}(s,a,b)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where note that the first inequality holds even when $N_{h}^{t}(s,a,b)=0$ and the second inequality follows form AM-GM. Thus, with probability $1-\\delta$ , for all $(t,h)$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n[(\\hat{P}_{h}^{t}-P_{h})(\\bar{V}_{h+1}^{\\pi^{t}}-V_{h+1}^{*})](x_{h}^{t})\\leq\\frac{S H(1+H/2)\\iota}{N_{h}^{t}(x_{h}^{t})}+\\frac{1}{H}[P_{h}(\\bar{V}_{h+1}^{\\pi^{t}}-V_{h+1}^{*})](x_{h}^{t}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Plugging this inequality into $\\zeta_{h}^{t}$ above, then with probability at least $1-\\delta$ , for all $(t,h)$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\zeta_{h}^{t}\\le\\displaystyle\\frac{S H(1+H/2)\\iota}{N_{h}^{t}(x_{h}^{t})}+(1+\\displaystyle\\frac{1}{H})\\left((\\bar{V}_{h+1}^{\\pi^{t}}-V_{h+1}^{\\pi^{t},f(\\pi^{t})})(s_{h+1}^{t})+\\epsilon_{h+1}^{t}\\right)+2\\beta(N_{h}^{t}(x_{h}^{t}))}\\\\ &{\\quad\\le\\displaystyle\\frac{3S H^{2}\\iota}{2N_{h}^{t}(x_{h}^{t})}+(1+\\displaystyle\\frac{1}{H})\\left(\\Delta_{h+1}^{t}+\\epsilon_{h+1}^{t}\\right)+2\\beta(N_{h}^{t}(x_{h}^{t})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we define ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\epsilon_{h+1}^{t}:=[P_{h}(\\bar{V}_{h+1}^{\\pi^{t}}-V_{h+1}^{\\pi^{t},f(\\pi^{t})})](x_{h}^{t})-(\\bar{V}_{h+1}^{\\pi^{t}}-V_{h+1}^{\\pi^{t},f(\\pi^{t})})(s_{h+1}^{t}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Bounding $\\sum_{t}\\zeta_{h}^{t}$ and $\\textstyle\\sum_{t}\\epsilon_{h}^{t}$ . Note that for all $h$ , $\\{\\zeta_{h}^{t}\\}_{t\\in[T]}$ and $\\{\\epsilon_{h}^{t}\\}_{t\\in[T]}$ are martingale difference sequences. Thus, by Azuma-Hoeffding\u2019s inequality and the union bound, with probability at least $1-\\delta$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{t,h}\\zeta_{h}^{t}\\lesssim H^{2}\\sqrt{T\\log(H/\\delta)},\\mathrm{~and~}\\sum_{t,h}\\epsilon_{h}^{t}\\lesssim H^{2}\\sqrt{T\\log(H/\\delta)}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Bounding $\\{\\gamma_{h}^{t}\\}$ . By Lemma B.3, and the union bound, with probability at least $1-\\delta$ , for all $(t,h)$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{h}^{t}=\\underset{\\theta\\in\\Theta_{h s_{h}^{t}a_{h}^{t}}^{\\operatorname*{max}}}{\\operatorname*{max}}\\ \\ \\bar{Q}_{h}^{\\pi^{t}}(s_{h}^{t},a_{h}^{t},P_{\\theta})-\\bar{Q}_{h}^{\\pi^{t}}(s_{h}^{t},a_{h}^{t},f(\\pi^{t})_{h})}\\\\ &{\\quad\\le2H\\underset{\\theta\\in\\Theta_{h s_{h}^{t}a_{h}^{t}}^{\\operatorname*{max}}}{\\operatorname*{max}}\\ \\ {d}_{T V}(P_{\\theta},f(\\pi^{t})_{h}(\\cdot|s_{h}^{t}))}\\\\ &{\\quad\\lesssim H\\sqrt{\\frac{\\alpha}{N_{h}^{t}(s_{h}^{t},a_{h}^{t})}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Plugging these bounds into the definition of $\\Delta_{h}^{t}$ , combining them using the union bound and re-scaling $\\delta$ , we have that: with probability at least $1-\\delta$ , for all $(t,h,\\pi)$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{h}^{t}=\\xi_{h}^{t}+\\zeta_{h}^{t}+\\gamma_{h}^{t}}\\\\ &{\\quad\\lesssim\\frac{3S H^{2}\\iota}{2N_{h}^{t}(x_{h}^{t})}+(1+\\frac{1}{H})\\left(\\Delta_{h+1}^{t}+\\epsilon_{h+1}^{t}\\right)+2\\beta(N_{h}^{t}(x_{h}^{t}))+\\zeta_{h}^{t}+H\\sqrt{\\frac{\\alpha}{N_{h}^{t}(s_{h}^{t},a_{h}^{t})}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, we have with probability at least $1-\\delta$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{t=1}^{T}\\Delta_{1}^{t}\\lesssim\\sum_{t=1}^{T}(1+\\frac{1}{H})^{H}\\sum_{h=1}^{H}\\left(\\frac{3S H^{2}\\iota}{2N_{h}^{t}(x_{h}^{t})}+\\epsilon_{h+1}^{t}+\\zeta_{h}^{t}+2\\beta(N_{h}^{t}(x_{h}^{t}))+H\\sqrt{\\frac{\\alpha}{N_{h}^{t}(s_{h}^{t},a_{h}^{t})}}\\right)}}\\\\ &{\\lesssim S H^{2}\\iota\\sum_{t,h}\\frac{1}{N_{h}^{t}(x_{h}^{t})}+H^{2}\\sqrt{T\\log(H/\\delta)}+H\\sqrt{\\log(H S A B T|\\Pi|/\\delta)}\\sum_{t,h}\\frac{1}{\\sqrt{N_{h}^{t}(x_{h}^{t})}}}\\\\ &{+H\\sqrt{\\alpha}\\sum_{t,h}\\frac{1}{\\sqrt{N_{h}^{t}(s_{h}^{t},a_{h}^{t})}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Finally, note that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{t,h}\\frac{1}{N_{h}^{t}(x_{h}^{t})}=\\sum_{h}\\sum_{(s,a,b)}\\sum_{i=1}^{N_{h}^{T}(s,a,b)}\\frac{1}{i}\\leq\\sum_{h}\\sum_{(s,a,b):N_{h}^{T}(s,a,b)\\geq1}\\log N_{h}^{T}(s,a,b)\\leq H S A B\\log T.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\sum_{\\stackrel{\\prime}{,}h}\\frac{1}{\\sqrt{N_{h}^{t}(x_{h}^{t})}}=\\displaystyle\\sum_{h}\\sum_{(s,a,b)}\\sum_{(i=1)}^{N_{h}^{T}(s,a,b)}\\frac{1}{\\sqrt{i}}\\leq\\displaystyle\\sum_{h}\\sum_{(s,a,b)}\\sqrt{N_{h}^{T}(s,a,b)}\\leq\\sqrt{H S A B}\\sqrt{\\sum_{(h,s,a,b)}N_{h}^{T}(s,a,b)}\\leq\\frac{1}{\\eta_{h}},}\\\\ {\\displaystyle=H\\sqrt{S A B T}.\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{\\substack{\\sqrt{N_{h}^{t}(s_{h}^{t},a_{h}^{t})}}}=\\sum_{(h,s,a)}\\sum_{i=1}^{N_{h}^{T}(s,a)}\\frac{1}{\\sqrt{i}}\\le\\sum_{h,s,a}\\sqrt{N_{h}^{T}(s,a)}\\le\\sqrt{H S A}\\sqrt{\\sum_{h,s,a}N_{h}^{T}(s,a)}=H\\sqrt{S A T}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Plugging these three inequalities above into the bound for $\\textstyle\\sum_{t=1}^{T}\\Delta_{1}^{t}$ right before and re-scaling $\\delta$ complete the proof. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "B.3 Proof of Theorem 5 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The layerwise exploration stage (Algorithm 4) performs layerwise exploration for each layer $h\\in[H]$ and estimates infrequent transitions into $\\boldsymbol{\\mathcal{U}}$ . Since infrequent transitions do not significantly affect policy evaluation in any way (will be proved precisely later), we can exclude them and quickly refrain from exploring them extensively. However, excluding them changes the underlying data distribution of the experiences that the earner would receive when interacting with the environment. To handle this bias issue, it is often convenient to consider an \u201cabsorbing\u201d Markov game $M^{\\prime}$ , a refinement of the original Markov game $M$ that excludes all infrequent transitions. ", "page_idx": 19}, {"type": "text", "text": "Definition 5 (Absorbing Markov games). Given a Markov game $M=(\\cal{S},\\mathcal{A},\\mathcal{B},r,P,\\cal{H}).$ , a set of transitions $\\boldsymbol{\\mathcal{U}}$ , and a dummy state $s^{\\dagger}$ , an absorbing Markov game $M^{\\prime}=(\\mathcal{S}\\cup\\{s^{\\dagger}\\},\\mathcal{A},\\mathcal{B},r,\\tilde{P},H)$ w.r.t. $(M,\\mathcal{U},s^{\\dagger})$ is defined as follows: For any $(h,s,a,b,s^{\\prime})\\in[H]\\times\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{B}\\times\\mathcal{S}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\tilde{P}_{h}(s^{\\prime}|s,a,b)=\\left\\{P_{h}(s^{\\prime}|s,a,b)\\right.\\left.\\right.i f(h,s,a,b)\\in\\mathcal{U}\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{h}(s^{\\dagger}|s,a,b)\\,=\\,1\\,-\\,\\sum_{s^{\\prime}\\in{\\cal S}}P_{h}(s^{\\prime}|s,a,b)\\;\\,a n d\\;P_{h}(s^{\\dagger}|s^{\\dagger},a,b)\\;=\\;1.\\;\\;I n\\;\\,a d d i t i o n,\\;r_{h}(s,a,b)\\;=\\;1,}\\\\ &{\\int_{{\\cal P}_{h}}(s,a,b)\\;i f s\\,\\epsilon\\,\\mathcal{S},\\;\\;\\;}\\\\ &{\\bigcup\\,i f s\\,=\\,s^{\\dagger},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let ${\\tilde{P}}^{k}$ be the absorbing transition kernels w.r.t. $(M,\\mathcal{U}^{k},s^{\\dagger})$ (Definition 5). Notice that the transition dynamics ${\\hat{P}}^{k}$ by Algorithm 4 are unbiased estimates of the absorbing transition dynamics $\\tilde{P}$ . ", "page_idx": 19}, {"type": "text", "text": "B.3.1 Sampling policies are sufficiently exploratory ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We now show that the sampling policies in the reward-free exploration stage are sufficiently exploratory over the state-action space of the Markov game. We start with bounding the difference between $\\tilde{P}$ and ${\\hat{P}}^{k}$ (Line 5 of Algorithm 3) using empirical Bernstein\u2019s inequality. ", "page_idx": 20}, {"type": "text", "text": "Lemma B.5. Define the event $E$ : $\\forall(k,h,s,a,b,s^{\\prime})\\\\in[K]\\times[H]\\times S\\times A\\times B\\times S$ such that $(h,s,a,b,s^{\\prime})\\not\\in\\mathcal{U},$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\hat{P}_{h}^{k}(s^{\\prime}|s,a,b)-\\tilde{P}_{h}^{k}(s^{\\prime}|s,a,b)|\\leq\\sqrt{\\frac{2\\hat{P}_{h}^{k}(s^{\\prime}|s,a,b)\\iota}{N_{h}^{k}(s,a,b)}}+\\frac{7\\iota}{3N_{h}^{k}(s,a,b)}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\iota:=c\\log(S A B H K/\\delta)$ and $N_{h}^{k}$ are the counter at layer $h$ in epoch $k$ obtained at Line $^{9}$ by running Algorithm $^{4}$ in epoch $k$ . Then, we have $\\operatorname*{Pr}(E)\\geq1-\\delta$ . In addition, $\\forall(h,s,a,b,s^{\\prime})\\in\\mathcal{U}$ , $\\hat{P}_{h}^{k}(s^{\\prime}|s,a,b)=\\tilde{P}_{h}(s^{\\prime}|s,a,b)=0$ . ", "page_idx": 20}, {"type": "text", "text": "Proof of Lemma B.5. Lemma B.5 is essentially the analogous of [Qiao et al., 2022, Lemma E.2] from MDPs to Markov games. The first part follows from empirical Bernstein\u2019s inequality and union bound. The second part comes from the definition of the absorbing transition kernels P\u02dc and the construction of the empirical transition kernels P\u02c6 k. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Lemma B.6. Conditioned on the event $E$ in Lemma B.5: For all $(k,h,s,a,b,s^{\\prime})\\in[K]\\times[H]\\times$ $S\\times A\\times B\\times S$ such that $(h,s,a,b,s^{\\prime})\\not\\in\\mathcal{U}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n(1-\\frac{1}{H})\\hat{P}_{h}^{k}(s^{\\prime}|s,a,b)\\leq\\tilde{P}_{h}^{k}(s^{\\prime}|s,a,b)\\leq(1+\\frac{1}{H})\\hat{P}_{h}^{k}(s^{\\prime}|s,a,b).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof of Lemma B.6. Lemma B.6 is essentially the same as [Qiao et al., 2022, Lemma E.3]. ", "page_idx": 20}, {"type": "text", "text": "Lemma B.7. Conditioned on the event $E$ in Lemma B.5: For all $(k,h,s,a,b,s^{\\prime})\\in[K]\\times[H]\\times$ $S\\times A\\times B\\times S$ and any policy $\\pi$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{1}{4}V^{\\pi,f([\\pi]^{m})}(1_{h s a b},\\hat{P}^{k})\\leq V^{\\pi,f([\\pi]^{m})}(1_{h s a b},\\tilde{P}^{k})\\leq3V^{\\pi,f([\\pi]^{m})}(1_{h s a b},\\hat{P}^{k}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $V^{\\pi,\\mu}(r,P)$ denotes the expected total reward under policies $(\\pi,\\mu)$ and the Markov game specified by the reward function $r$ and transition kernels $P$ . ", "page_idx": 20}, {"type": "text", "text": "Proof of Lemma B.7. The proof essentially follows from the proof of [Qiao et al., 2022, Lemma E.5]. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Lemma B.5 to Lemma B.7 are similar in nature with corresponding lemmas in a single-agent MDP in [Qiao et al., 2022]. We now prove a novel lemma that\u2019s absent in the singleagent MDP setting yet crucial to our theorem. Recall our notion that, $\\bar{V}^{\\pi}(r,P,\\Theta)\\stackrel{\\bullet}{:=}$ OPTIMISTIC_VALUE_ESTIMATE $(\\pi,r,P,\\Theta)$ which is given in Algorithm 6. ", "page_idx": 20}, {"type": "text", "text": "Lemma B.8. Fix any $k\\in[K]$ and consider $\\hat{P}^{k},\\Theta^{k},\\mathcal{U}^{k}=L A Y E R W I S E\\_E X P L O R A T I O N(\\Pi^{k},T_{k})$ (Line 5 of Algorithm 3). Define the event $E_{k}$ : for all $(h,s,a,b)\\in[H]\\times S\\times A\\times B$ and all $\\pi\\in\\Pi$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n0\\leq\\bar{V}^{\\pi}(1_{h s a b},\\hat{P}^{k},\\Theta^{k})-V^{\\pi,f([\\pi]^{m})}(1_{h s a b},\\hat{P}^{k})\\leq\\xi_{M L E}(T_{k}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where \u03beMLE(Tk) := cH d\u2217\u03b1Tk . Assume that $T$ is sufficiently large such that $\\mathit{T}_{k}\\;\\;\\geq$ 2 log(SdH\u22172KA/\u03b4), \u2200k \u2208[K]. Then, Pr(Ek) \u22651 \u2212\u03b4. ", "page_idx": 20}, {"type": "text", "text": "Proof of Lemma B.8. Let us fix any $(h,s,a,b)$ and $\\pi$ . Note that the value function for any policy tuon tdheer  aexnyp ldoryantaiomni co fw l.ra.yt.e tr $h$ irne wthaer dr efuwnacrtdi-ofnr $1_{h s a b}$ liosr azteiroon  a(t Aalngyo rsittehp $h^{\\prime}>h$ $\\hat{P}_{1}^{k},\\ldots,\\hat{P}_{h-1}^{k}$ ea rteh aatl, rperaidoyr ", "page_idx": 20}, {"type": "text", "text": "Additional notations. In OPTIMISTIC_VALUE_ESTIMATE $(1_{h s a b},\\hat{P}^{k},\\pi,\\Theta)$ (Algorithm 6), we denote the intermediate value estimates $\\bar{V}_{l}^{\\pi}$ by $\\bar{V}_{l}^{\\pi}(\\cdot;1_{h s a b},\\hat{P}^{k},\\Theta^{k})$ to emphasize the dependence on the reward function and the transition dynamics being used. We denote $N_{h}^{k}(s,a)$ the count of pa\u03c0ir,s\u00b5 $(h,s,a)$ during the $h$ -th layer exploration of Algorithm 4. We write $V_{h}^{\\pi,\\mu}\\bar{(}s;r,\\stackrel{\\cdot}{P})$ in place of $V_{h}^{\\pi,\\mu}(s)$ to emphasize the dependence on the reward function $r$ and transition dynamic $P$ . ", "page_idx": 21}, {"type": "text", "text": "We will evaluate the quantity $\\Delta_{l}(\\bar{s}):=V_{l}^{\\pi,f([\\pi]^{m})}(\\bar{s};1_{h s a b},\\hat{P}^{k})-\\bar{V}_{l}^{\\pi}(\\bar{s};1_{h s a b},\\hat{P}^{k},\\Theta^{k})$ for any $l\\in[h-1],\\bar{s}\\in\\mathcal{S}$ . ", "page_idx": 21}, {"type": "text", "text": "The first part $\\bar{V}^{\\pi}(1_{h s a b},\\hat{P}^{k},\\Theta^{k})-V^{\\pi,f([\\pi]^{m})}(1_{h s a b},\\hat{P}^{k})\\geq0$ follows from that with probability at least $1\\,-\\,\\delta$ , $\\theta_{h s a}^{*}\\;\\in\\;\\Theta_{h s a}^{k},\\forall(h,s,a)$ . Thus, $\\bar{V}^{\\pi}(1_{h s a b},\\hat{P}^{k},\\Theta^{k})$ is an optimistic estimate of $V^{\\pi,f([\\pi]^{m})}(1_{h s a b},\\hat{P}^{k})$ . For the second part, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{l}(\\vec{s})=\\underset{s\\in\\Theta_{l,k_{l}(s)}^{k},\\,v\\in\\mathcal{S}}{\\operatorname*{sup}}\\sum_{t}^{k}P_{l}^{k}(s^{\\prime}|\\vec{s},\\pi_{l}(\\vec{s}),P_{\\theta})\\bar{V}_{l+1}^{\\pi}(s^{\\prime};\\boldsymbol{1}_{h\\circ a b},\\hat{P}^{k},\\Theta^{k})}\\\\ &{\\qquad-\\underset{s^{\\prime}\\in\\mathcal{S}}{\\sum}P_{l}^{k}(s^{\\prime}|\\vec{s},\\pi_{l}(\\vec{s}),f([\\pi]^{m})_{l}(\\cdot|\\vec{s}))V_{l+1}^{\\pi,f([\\pi]^{m})}(s^{\\prime};\\boldsymbol{1}_{h\\circ s},\\hat{P}^{k})}\\\\ &{\\qquad=\\underset{s\\in\\mathcal{S}}{\\sum}P_{l}^{k}(s^{\\prime}|\\vec{s},\\pi_{l}(\\vec{s}),f([\\pi]^{m})_{l}(\\cdot|\\vec{s}))\\Delta_{l+1}(s^{\\prime})}\\\\ &{\\qquad+\\underset{s^{\\prime}\\in\\mathcal{S}}{\\sum}\\left(P_{l}^{k}(s^{\\prime}|\\vec{s},\\pi_{l}(\\vec{s}),P_{\\theta})-P_{l}^{k}(s^{\\prime}|\\vec{s},\\pi_{l}(\\vec{s}),f([\\pi]^{m})_{l}(\\cdot|\\vec{s}))\\right)\\bar{V}_{l+1}^{\\pi}(s^{\\prime};\\boldsymbol{1}_{h\\circ a b},\\hat{P}^{k},\\Theta^{k})}\\\\ &{\\qquad\\leq\\operatorname*{max}\\{\\Delta_{l+1}(s^{\\prime}):s^{\\prime}\\in\\mathcal{S}\\}\\underset{s\\in\\mathcal{S}}{\\operatorname*{max}}\\frac{1}{\\mathcal{B}^{l}}\\psi\\in B,(\\vec{l},\\vec{s},\\pi_{l}(\\vec{s}),b^{\\prime},s^{\\prime})\\notin\\mathcal{U}^{k}\\}}\\\\ &{\\qquad+1\\{N_{l}^{k}(\\vec{s},\\pi_{l}(\\vec{s}))\\geq1\\}\\cdot2\\underset{\\theta\\in\\Theta_{l+1,m_{l}(s)}^{k}}{\\operatorname*{max}}~\\Delta_{T V}(f([\\pi]^{m})_{l}(\\cdot|\\vec{s}),P_{\\theta}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we use the convention that $\\operatorname*{max}\\varnothing~~=~~0$ , and the last inequality follows from that $P_{l}^{k}(s^{\\prime}|\\bar{s},\\pi_{l}(\\bar{s}),b^{\\prime})=0$ if $(l,\\bar{s},\\pi_{l}(\\bar{s}),b^{\\prime},s^{\\prime})\\notin\\mathcal{U}^{k}$ , that $\\bar{V}_{l+1}^{\\pi}(s^{\\prime};1_{h s a b},\\hat{P}^{k},\\Theta^{k})\\,\\in\\,[0,1]$ , and that, for any two distributions $p,q\\in[0,1]^{|\\mathcal{X}|}$ over a finite support $\\mathcal{X}$ , we have $\\begin{array}{r}{d_{T V}(p,q)=\\frac{1}{2}\\|p-q\\|_{1}}\\end{array}$ . ", "page_idx": 21}, {"type": "text", "text": "If $N_{l}^{k}(\\bar{s},\\pi_{l}(\\bar{s}))=0$ , then $\\Delta_{l}(\\bar{s})\\,=\\,0$ . Consider the case $N_{l}^{k}(\\bar{s},\\pi_{l}(\\bar{s}))\\geq1$ . That means that the state-action pair $\\left(\\bar{s},\\pi_{l}(\\bar{s})\\right)$ must be visited in step $l$ at least once by at least one policy $\\pi^{k l\\tilde{s}\\tilde{a}\\tilde{b}}$ for some $(\\tilde{s},\\tilde{a},\\tilde{b})\\in\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{B}$ . Note that this policy $\\pi^{k l\\tilde{s}\\tilde{a}\\tilde{b}}$ is run for $m-1+T_{k}$ consecutive episodes. Thus, by the definition of the minimum positive visitation probability $d^{\\ast}$ , we must have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[N_{l}^{k}(\\bar{s},\\pi_{l}(\\bar{s}))\\right]\\geq d^{\\ast}T_{k},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the expectation is w.r.t. the transition kernel $P$ of the original Markov game $M$ and policy $\\pi^{k l\\tilde{s}\\tilde{a}\\tilde{b}}$ . By Hoelfding\u2019s inequality and the union bound: With probability at least $1-\\delta$ , for all $l,{\\bar{s}},k,\\pi$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\nN_{l}^{k}(\\bar{s},\\pi_{l}(\\bar{s}))\\geq\\mathbb{E}\\left[N_{l}^{k}(\\bar{s},\\pi_{l}(\\bar{s}))\\right]-\\sqrt{T_{k}\\log(S H K A/\\delta)}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In particular, for $(l,\\bar{s},k,\\pi)$ such that $\\mathbb{E}\\left[N_{l}^{k}(\\bar{s},\\pi_{l}(\\bar{s}))\\right]\\,\\ge\\,d^{\\ast}T_{k}$ and for $\\begin{array}{r}{T_{k}\\,\\geq\\,\\frac{2\\log(S H K A/\\delta)}{d^{*\\,2}}}\\end{array}$ , we have $N_{l}^{k}(\\bar{s},\\pi_{l}(\\bar{s}))\\;\\geq\\;\\textstyle{\\frac{1}{2}}d^{\\ast}T_{k}$ with probability at least $1-\\delta$ . Combined with Lemma B.3, with probability at least $1-\\delta$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\theta\\in\\Theta_{l\\bar{s}\\pi_{l}(\\bar{s})}^{k}}{d_{T V}(f([\\pi]^{m})_{l}(\\cdot|\\bar{s}),P_{\\theta})}\\leq c\\sqrt{\\frac{\\alpha}{d^{*}T_{k}}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, under the same event that the above inequality holds, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Delta_{1}(s_{1})\\leq c H\\sqrt{\\frac{\\alpha}{d^{*}T_{k}}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Next, we will show that the transition samples collected in $\\mathcal{U}^{k}$ are indeed infrequent transitions by any policy. Let $\\tau=(s_{1},a_{1},b_{1},\\ldots,s_{H},a_{H}^{-},b_{H})$ be a random trajectory generated by the learner\u2019s policy $\\pi$ and the opponent\u2019s policies $f([\\pi]^{m})$ for some policy $\\pi$ . ", "page_idx": 22}, {"type": "text", "text": "Definition 6 (Bad events). Under the original transition kernel $P$ , we define $\\mathcal{F}$ to be the event that there exists $h\\in[H]$ such that $(h,s_{h},a_{h},\\breve{b}_{h},s_{h+1})\\in\\mathcal{U}^{k}$ and we define $\\mathcal{F}_{h}$ to be the event such that $h$ is the smallest step that $(h,s_{h},a_{h},b_{h},s_{h+1})\\in\\mathcal{U}^{k}$ . Under the absorbing transition kernel ${\\tilde{P}}^{k}$ , we define $\\mathcal{F}$ to be the event that there exists $h\\in[H]$ such that $s_{h+1}=s^{\\dagger}$ and we define ${\\mathcal{F}}_{h}$ to be the event such that $h$ is the smallest step that $s_{h+1}=s^{\\dagger}$ . ", "page_idx": 22}, {"type": "text", "text": "Lemma B.9. Conditioned on the event $E$ in Lemma B.5 and the event $E_{k}$ in Lemma B.8, with probability at least $1-\\delta,$ , for any $k\\in[K]$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\pi\\in\\Pi}\\operatorname*{Pr}[\\mathcal{F}|P,\\pi]\\lesssim\\frac{H^{3}\\log(H S A B K/\\delta)}{T_{k}}+H\\xi_{M L E}(T_{k}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\xi_{M L E}(\\cdot)$ is defined in Lemma B.8 and $\\mathcal{F}$ is defined in Definition $^{6}$ . ", "page_idx": 22}, {"type": "text", "text": "Proof of Lemma B.9. Under the event $E$ in Lemma B.5 and the event $E_{k}$ in Lemma B.8, for any $(h,s,a,b)$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{V^{\\pi^{k h o b},f([\\pi^{k h o b}]^{m})}(1_{h s a b},\\tilde{P}^{k})\\geq\\displaystyle\\frac{1}{4}V^{\\pi^{k h\\alpha b},f([\\pi^{k h o b}]^{m})}(1_{h s a b},\\hat{P}^{k})}&{}\\\\ {\\geq\\displaystyle\\frac{1}{4}\\bar{V}^{\\pi^{k h\\alpha b}}(1_{h s a b},\\hat{P}^{k},\\Theta^{k})-\\xi_{M L E}(T_{k})}&{}\\\\ {=\\displaystyle\\frac{1}{4}\\operatorname*{sup}_{\\pi\\in\\Pi^{k}}\\bar{V}^{\\pi}(1_{h s a b},\\hat{P}^{k},\\Theta^{k})-\\xi_{M L E}(T_{k})}&{}\\\\ {\\geq\\displaystyle\\frac{1}{4}\\operatorname*{sup}_{\\pi\\in\\Pi^{k}}V^{\\pi,f([\\pi^{1}]^{m})}(1_{h s a b},\\hat{P}^{k})-\\xi_{M L E}(T_{k})}&{}\\\\ {\\geq\\displaystyle\\frac{1}{12}\\operatorname*{sup}_{\\pi\\in\\Pi^{k}}V^{\\pi,f([\\pi]^{m})}(1_{h s a b},\\hat{P}^{k})-\\xi_{M L E}(T_{k})}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the first inequality and the last inequality follow from Lemma B.7, the second inequality follows from Lemma B.8, the second and third inequality follow from Lemma B.8, and the equation follows from the definition of $\\pi^{k h s a b}$ in Algorithm 4. Let $\\pi^{k h}$ be a policy that chooses each $\\dot{\\pi}^{k h s a b}$ with probabilitySA1B for any $(s,a,b)\\in\\bar{S}\\overset{\\cdot}{\\times}\\mathcal{A}\\times\\mathcal{B}$ . Thus, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{Pr}[\\mathcal{F}_{h}|P_{\\pi},\\pi^{k k}]=\\operatorname*{Pr}[\\mathcal{F}_{h}|\\hat{P}^{k},\\pi^{k k}]}\\\\ &{\\quad=\\frac{1}{S A B}\\sum_{s,a,b=s}V^{\\pi^{k k}/(1^{n k+1}f(|t^{n k+1}|^{n}))}(1_{h a b},\\hat{P}^{k})\\hat{P}_{h}(s^{1}|s,a,b)}\\\\ &{\\quad\\geq\\frac{1}{S A B}\\sum_{s,a,b=s}^{\\pi^{k}}F^{\\pi^{k+n}/f(|t^{n k+1}|^{n})}(1_{h a b},\\hat{P}^{k})\\hat{P}_{h}(s^{1}|s,a,b)}\\\\ &{\\quad\\geq\\frac{1}{12S A B}\\sum_{s,a,b=t}^{\\pi^{k}}\\mathrm{sup}\\ ^{V\\pi^{k}/(|t^{n k}|^{n})}(1_{h a b},\\hat{P}^{k})-\\frac{1}{S A B}\\xi u_{I L E}(T_{h})}\\\\ &{\\quad\\geq\\frac{1}{12S A B}\\ \\mathrm{sup}\\ \\sum_{s,a,b=b}^{\\pi^{k}}V^{\\pi^{n}/(|t^{n}|)}(1_{h a b},\\hat{P}^{k})-\\frac{1}{S A B}\\xi u_{I L E}(T_{h})}\\\\ &{\\quad=\\frac{1}{12S A B}\\ \\mathrm{sup}\\ \\mathrm{Pr}[\\mathcal{F}_{h}|\\hat{P}^{k},\\pi]-\\frac{1}{S A B}\\xi u_{I L E}(T_{h})}\\\\ &{\\quad=\\frac{1}{12S A B}\\ \\mathrm{sup}\\ \\mathrm{Pr}[\\mathcal{F}_{h}|\\hat{P}^{k},\\pi]-\\frac{1}{S A B}\\xi u_{I L E}(T_{h})}\\\\ &{\\quad=\\frac{1}{12S A B}\\ \\mathrm{sup}\\ \\mathrm{Pr}[\\mathcal{F}_{h}|P,T_{1}]-\\frac{1}{S A B}\\xi u_{I L E}(T_{h}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By the construction of $\\mathcal{U}^{k}$ , we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{Pr}[\\mathcal{F}_{h}|P,\\pi^{k h}]\\le c\\frac{H^{2}\\log(H S A B K/\\delta)}{S A B T_{k}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, combined with Equation (4), we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\pi\\in\\Pi^{k}}\\mathrm{Pr}[\\mathcal{F}_{h}|P,\\pi]\\lesssim\\frac{H^{2}\\log(H S A B K/\\delta)}{T_{k}}+\\xi_{M L E}(T_{k}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Finally, note that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[\\mathcal{F}|P,\\pi]=\\sum_{h\\in[H]}\\operatorname*{Pr}[\\mathcal{F}_{h}|P,\\pi],\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which concludes our proof. ", "page_idx": 23}, {"type": "text", "text": "B.3.2 Uniform policy evaluation ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this part, we will show that the empirical transition kernel $P^{k}$ constructed from the exploratory data by our sampling policies is a good surrogate for the true transition kernel $P$ in evaluating the value of uniformly all policies. ", "page_idx": 23}, {"type": "text", "text": "Lemma B.10. Conditioned on the event $E$ in Lemma B.5 and the event $E_{k}$ in Lemma B.8 and the high-probability event in Lemma B.9, with probability at least $1-\\delta$ , for any $k\\in[K]$ , any reward function $r$ , and any policy $\\pi\\in\\Pi$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{0\\le V^{\\pi,f([\\pi]^{m})}(r,P)-V^{\\pi,f([\\pi]^{m})}(r_{\\mathcal{U}^{k}},\\tilde{P}^{k})\\lesssim\\frac{H^{4}\\log(H S A B K/\\delta)}{T_{k}}+H^{2}\\xi_{M L E}(T_{k}),}&{}\\\\ {\\nu h e r e\\quad f o r\\quad a n y\\quad t r a j e c t o r y\\quad\\tau\\quad\\quad=\\quad\\quad(s_{1},a_{1},b_{1},\\dots,s_{H},a_{H},b_{H}),\\quad r_{\\mathcal{U}^{k}}(\\tau)}&{:=}\\\\ {\\underset{\\rightharpoonup}{\\sum}H^{\\cal I}1\\{(h,s_{h},a_{h},b_{h},s_{h+1})\\notin\\mathcal{U}^{k}\\}\\}r_{h}(s_{h},a_{h},b_{h}).}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof of Lemma B.10. We have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{V^{\\pi,f(\\pi|^{n})}(r,P)=\\underset{\\tau}{\\sum}r(\\tau)\\operatorname{Pr}(\\tau|P,\\pi)}&{}\\\\ {\\quad}&{=\\underset{\\tau\\neq\\mathcal{F}}{\\sum}r(\\tau)\\operatorname{Pr}(\\tau|P,\\pi)+\\underset{\\tau\\neq\\mathcal{F}}{\\sum}r(\\tau)\\operatorname{Pr}(\\tau|P,\\pi)}\\\\ {\\quad}&{=\\underset{\\tau\\neq\\mathcal{F}}{\\sum}r(\\tau)\\operatorname{Pr}(\\tau|\\bar{P}^{k},\\pi)+\\underset{\\tau\\neq\\mathcal{F}}{\\sum}r(\\tau)\\operatorname{Pr}(\\tau|P,\\pi)}\\\\ {\\quad}&{=\\underset{\\tau\\neq\\mathcal{F}}{\\sum}r(\\tau)\\operatorname{Pr}(\\tau|\\bar{P}^{k},\\pi)+\\underset{\\tau\\neq\\mathcal{F}}{\\sum}r(\\tau)\\operatorname{Pr}(\\tau|P,\\pi)}\\\\ {\\quad}&{=\\underset{\\tau\\neq\\mathcal{F}}{\\sum}r_{\\tau\\neq\\mathcal{F}}(\\tau)\\operatorname{Pr}(\\tau|\\bar{P}^{k},\\pi)+\\underset{\\tau\\neq\\mathcal{F}}{\\sum}r(\\tau)\\operatorname{Pr}(\\tau|P,\\pi)}\\\\ {\\quad}&{\\leq V^{\\pi,f(\\pi|^{n})}(r_{\\mathcal{U}^{k}},\\bar{P}^{k})+\\underset{\\tau\\in\\mathcal{F}}{\\sum}r(\\tau)\\operatorname{Pr}(\\tau|P,\\pi)}\\\\ {\\quad}&{\\lesssim V^{\\pi,f(\\pi|^{n})}(r_{\\mathcal{U}^{k}},\\bar{P}^{k})+\\frac{H^{4}\\log\\left(H S A B K/\\delta\\right)}{T_{k}}+H^{2}\\xi_{M L E}(T_{k}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last inequality is due to Lemma B.9. Similarly, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V^{\\pi,f([\\pi]^{m})}(r,P)=\\displaystyle\\sum_{\\tau\\notin\\mathcal{F}}r_{\\mathcal{U}^{k}}(\\tau)\\operatorname*{Pr}(\\tau|\\tilde{P}^{k},\\pi)+\\displaystyle\\sum_{\\tau\\in\\mathcal{F}}r(\\tau)\\operatorname*{Pr}(\\tau|P,\\pi)}\\\\ &{\\phantom{V^{\\pi,f([\\pi]^{m})}(r,P)=\\sum_{\\tau\\notin\\mathcal{F}}r_{\\mathcal{U}^{k}}(\\tau)}\\operatorname*{Pr}(\\tau|\\tilde{P}^{k},\\pi)+\\displaystyle\\sum_{\\tau\\in\\mathcal{F}}r_{\\mathcal{U}^{k}}(\\tau)\\operatorname*{Pr}(\\tau|P,\\pi)}\\\\ &{\\phantom{V^{\\pi,f([\\pi]^{m})}(r,P)=\\sum_{\\tau\\notin\\mathcal{F}}r_{\\mathcal{U}^{k}}(\\tau)}\\geq\\displaystyle\\sum_{\\tau\\in\\mathcal{F}}r_{\\mathcal{U}^{k}}(\\tau|\\tilde{P}^{k},\\pi)+\\displaystyle\\sum_{\\tau\\in\\mathcal{F}}r_{\\mathcal{U}^{k}}(\\tau)\\operatorname*{Pr}(\\tau|\\tilde{P}^{k},\\pi)}\\\\ &{\\phantom{V^{\\pi,f([\\pi]^{m})}(r,\\pi)=\\sum_{\\tau\\in\\mathcal{F}}r_{\\mathcal{U}^{k}}(\\tau)}=V^{\\pi,f([\\pi]^{m})}(r_{\\mathcal{U}^{k}},\\tilde{P}^{k}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Lemma B.11. With probability at least $1-\\delta$ , for any $k\\in[K].$ , any reward function $r$ , and any policy $\\pi\\in\\Pi$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n0\\leq\\bar{V}^{\\pi}(r_{\\mathcal{U}^{k}},\\hat{P}^{k},\\Theta^{k})-V^{\\pi,f([\\pi]^{m})}(r_{\\mathcal{U}^{k}},\\hat{P}^{k})\\lesssim H^{2}\\sqrt{\\frac{\\alpha}{d^{*}T_{k}}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof of Lemma B.11. The first inequality is trivial, following the first part of Lemma B.8. We will focus on the second inequality. Fix any deterministic policy $\\pi$ . For simplicity, we write $\\bar{V}_{h}^{\\pi}(s):=$ $\\bar{V}_{h}^{\\pi}(r_{\\mathcal{U}^{k}},\\hat{P}^{k},\\Theta^{k})(s)$ , and $V_{h}^{\\pi}(s):=V_{h}^{\\pi,f([\\pi]^{m})}(r_{\\mathcal{U}^{k}},\\hat{P}^{k})(s)$ . Let $\\Delta_{h}^{k}(s):={\\bar{V}}_{h}^{\\pi}(s)-V_{h}^{\\pi}(s)$ . ", "page_idx": 24}, {"type": "text", "text": "First of all, by construction of ${\\hat{P}}^{k}$ and $r_{\\mathcal{U}^{k}}$ , we have $\\Delta_{h}^{k}(s)=0$ if $s=s^{\\dagger}$ or if $N_{h}^{k}(s,\\pi_{h}(s))=0$ .   \nThis explains the very reason we design the truncated reward function $r_{\\mathcal{U}^{k}}$ . ", "page_idx": 24}, {"type": "text", "text": "We now consider $s\\in S$ such that $N_{h}^{k}(s,\\pi_{h}(s))>0$ . This condition, along with the consistent behavior and the minimum visitation probability, allows us to estimate the response $f([\\pi]^{m})_{h}(\\cdot|s)$ sufficiently. In particular, $f([\\pi]^{m})_{h}(\\cdot|\\bar{s})$ depends only on the data obtained by visiting $(h,s,\\pi_{h}(s))$ which is indeed visited at least $d^{*}T_{k}$ times, thus can be estimated up to an order of $1/\\sqrt{d^{*}T_{k}}$ error. We have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Delta_{h}^{k}(s)=\\eta_{H^{k},h}(s,\\pi_{h}(s),P_{\\theta})+\\bar{P}_{h}^{k}\\bar{V}_{h}^{k}+(s,\\pi_{h}(s),P_{\\theta})}\\\\ &{\\quad-\\eta_{H^{k},h}(s,\\pi_{h}(s),f(\\pi|^{m})h(s))-\\bar{P}_{h}^{k}V_{h+1}^{k}(s,\\pi_{h}(s),f(\\pi|^{m})h(s))}\\\\ &{\\quad=\\eta_{H^{k},h}(s,\\pi_{h}(s),P_{\\theta})-\\eta_{H^{k},h}(s,\\pi_{h}(s),f(\\pi|^{m})h(s)|\\mathrm{\\Lambda}_{h}(\\cdot|s))}\\\\ &{\\quad+\\hat{P}_{h}^{k}(\\bar{V}_{h+1}^{k}-V_{h+1}^{k})(s,\\pi_{h}(s),f(\\pi|^{m})h(s))}\\\\ &{\\quad+\\bar{P}_{h}^{k}\\bar{V}_{h+1}^{k}(s,\\pi_{h}(s),P_{\\theta})-\\hat{P}_{h}^{k}\\bar{V}_{h+1}^{k}(s,\\pi_{h}(s),f(\\pi|^{m})h(\\cdot|s))}\\\\ &{\\quad\\le\\quad\\operatorname*{sup}_{\\theta\\in\\Theta_{h}(s),\\pi_{h}(s)}f_{T(\\pi)}f(\\pi|^{m})h(\\cdot|s)}\\\\ &{\\quad+m\\operatorname*{sup}_{\\theta\\in\\Theta_{h}(s),\\pi_{h}(s)}\\quad+\\nu\\in S_{\\mathrm{AL}}\\pm\\bar{p}\\in B,(h,s,\\pi_{h}(s),b,s^{\\prime})\\notin\\mathcal{U}^{k}\\}}\\\\ &{\\quad+\\prod_{\\theta\\in\\Theta_{h}^{\\mathrm{sup}}}\\quad d_{T(\\pi)}f(\\pi|^{m})h(\\cdot|s))}\\\\ &{\\quad=\\left(H+\\operatorname*{min}_{\\theta\\in\\Theta_{h}^{\\mathrm{sup}}(s),\\pi_{h}(s)}\\quad d_{T(T(\\pi)}\\int_{h}(\\cdot|s)|\\mathrm{\\Lambda}_{h}(\\cdot|s))\\right.}\\\\ &{\\quad\\left.+\\operatorname*{max}\\left\\{\\Delta_{h+1}^{k}(s,\\pi);\\xi\\in S_{\\mathrm{AL}}\\pm\\mathcal{B}\\right\\}_{h}\\in B,(h,s,\\pi_{h}(s\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that, similar to Equation (2), as $N_{h}^{k}(s,\\pi_{h}(s))>0$ , with probability at least $1-\\delta$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\theta\\in\\Theta_{h s\\pi_{h}(s)}^{k}}d_{T V}(P_{\\theta},f([\\pi]^{m})_{h}(\\cdot|s))\\lesssim\\sqrt{\\frac{\\alpha}{d^{*}T_{k}}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Delta_{1}^{k}(s_{1})\\lesssim H^{2}\\sqrt{\\frac{\\alpha}{d^{*}T_{k}}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lemma B.12. Conditioned on the event $E$ in Lemma B.5 and the event $E_{k}$ in Lemma B.8, with probability $1-\\delta,$ , for any $k\\in[K]$ , $\\pi\\in\\Pi$ and any reward function $r^{\\prime}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n|V^{\\pi,f([\\pi]^{m})}(r^{\\prime},\\hat{P}^{k})-V^{\\pi,f([\\pi]^{m})}(r^{\\prime},\\tilde{P}^{k})|\\lesssim H S^{3/2}A B\\sqrt{\\frac{\\log(H A T/\\delta)}{T_{k}}}+H S A B\\cdot\\xi_{M L E}(T_{k}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof of Lemma B.12. By the simulation lemma [Dann et al., 2017], we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n|V^{\\pi,f([\\pi]^{m})}(r^{\\prime},\\hat{P}^{k})-V^{\\pi,f([\\pi]^{m})}(r^{\\prime},\\tilde{P}^{k})|\\leq\\mathbb{E}_{\\tilde{P}^{k},\\pi}\\sum_{h=1}^{H}|(\\hat{P}_{h}^{k}-\\tilde{P}_{h}^{k})\\hat{V}_{h+1}^{\\pi}|,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\hat{V}_{h+1}^{\\pi}:=V^{\\pi,f([\\pi]^{m})}(r^{\\prime},\\hat{P}^{k})$ . Define the sampling distribution $\\nu_{h}\\in\\Delta(S\\times A\\times B),h\\in[H]$ by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\nu_{h}(s,a,b):=\\frac{1}{S A B}\\sum_{\\bar{s},\\bar{a},\\bar{b}}V^{\\pi^{k h\\bar{s}\\bar{a}\\bar{b}},f([\\pi^{k h\\bar{s}\\bar{a}\\bar{b}}]^{m})}(1_{h s a b},\\tilde{P}^{k}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\sum_{k}\\{I_{k}^{k}-\\int_{\\mathbb{R}_{+}}^{\\infty}\\hat{F}_{k}^{k}\\}(\\hat{\\mathbf{k}}_{k}+\\hat{\\mathbf{k}}_{k})\\,\\hat{\\mathbf{b}}_{k}^{k}-\\hat{\\mathbf{r}}_{k}^{k}\\hat{\\mathbf{r}}_{k}^{k}\\hat{\\mathbf{r}}_{k}^{k}\\}(\\mathbf{k}_{k+})\\hat{\\mathbf{r}}_{k}\\{\\hat{\\mathbf{r}}_{k}(\\mathbf{k}_{k})-\\hat{\\mathbf{r}}_{k}^{k}\\}}\\\\ &{\\quad\\le\\sum_{k}\\sum_{i=1}^{N}\\hat{F}_{k}^{k}\\hat{\\mathbf{r}}_{k}^{k}-\\hat{F}_{k}^{k}\\hat{\\mathbf{r}}_{k+}^{k}(s_{k},\\mathbf{b}_{k})\\hat{\\mathbf{l}}(\\mathbf{r}_{k+})-\\hat{\\mathbf{r}}_{k}^{k}\\,\\hat{\\mathbf{r}}_{k}^{k+\\mu-k_{k}}\\hat{F}_{k}^{k+\\mu-k_{k}}\\hat{F}_{k+}^{k}}\\\\ &{\\quad+12\\hat{F}_{k}^{k}\\Delta t\\,\\hat{\\mathbf{r}}_{k}\\{\\hat{\\mathbf{r}}_{k}\\}}\\\\ &{\\le1\\sum_{k}\\left\\{I_{k}^{k}-\\hat{\\mathbf{k}}_{k}^{k}\\hat{\\mathbf{r}}_{k}^{k}+\\left[\\hat{\\mathbf{d}}_{0}^{k},\\hat{\\mathbf{b}}_{k}^{k}\\right](\\mathbf{r}_{k+})-\\right.\\sum_{k=1}^{N}F_{k}^{k+\\mu k-k_{k}}f_{k}\\big\\}\\eta^{k+k+\\frac{\\mu-k_{k}}{2}}(\\mathbf{k}_{k+})\\hat{\\mathbf{r}}_{k}}\\\\ &{\\quad+12\\hat{F}_{k}^{k}\\Delta t\\,\\hat{\\mathbf{r}}_{k}\\{\\hat{\\mathbf{r}}_{k}(\\mathbf{k}_{k+})\\}}\\\\ &{\\le12\\hat{S}^{k}\\Delta t\\,\\sum_{k=1}^{N}\\sum_{i=1}^{N}(\\hat{\\mathbf{r}}_{k}^{k}-\\hat{\\mathbf{k}}_{k}^{k})\\hat{\\mathbf{l}}(\\mathbf{r}_{k+},\\theta_{k})\\,\\hat{\\mathbf{b}}_\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the second equality is due to that $\\pi$ is deterministic, the first inequality follows from Equation (3), the third inequality follows from Jensen\u2019s inequality, and the last inequality follows from the fundamental Lemma B.13. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Lemma B.13 ([Jin et al., 2020, Lemma C.2]). With probability at least $1\\!-\\!\\delta$ , for all $h\\in[H],k\\in[K],$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{V:S\\cup s^{\\dagger}\\rightarrow[0,H]}\\operatorname*{sup}_{g:S\\cup s^{\\dagger}\\rightarrow A}\\mathbb{E}_{\\nu_{h}}|(\\hat{P}_{h}^{k}-\\tilde{P}_{h})V(s,a,b)|^{2}1\\{g(s)=a\\}\\lesssim\\frac{H^{2}S\\log(H A T/\\delta)}{T_{k}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof of Lemma B.13. Note that ${\\hat{P}}^{k}$ is the empirical transition kernel constructed by sampling according to the data distribution $\\nu$ under the transition kernel ${\\tilde{P}}^{k}$ for $T_{k}$ samples. Thus, Lemma B.13 is a direct application of [Jin et al., 2020, Lemma C.2]. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Finally, we will show that any policy in $\\Pi^{k+1}$ is of high quality. ", "page_idx": 25}, {"type": "text", "text": "Lemma B.14. Recall the version space $\\Pi^{k+1}$ defined at Line 6 of Algorithm 3. With probability at least $1-\\delta_{i}$ , for any $k\\in[K]$ and any $\\pi\\in\\Pi^{k+1}$ , and any reward function $r$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\pi\\in\\Pi}{\\operatorname*{sup}}V^{\\pi,f([\\pi]^{m})}(r,P)-V^{\\pi,f([\\pi]^{m})}(r,P)=\\mathcal{O}\\bigg(H^{2}(S A B+H)\\sqrt{\\frac{\\alpha}{d^{*}T_{k}}}+\\frac{H^{4}\\log(H S A B K/\\delta)}{T_{k}}}\\\\ &{\\phantom{\\frac{(\\pi)^{2}}{\\operatorname*{sup}}V^{\\pi,f([\\pi]^{m})}(r,P)-V^{\\pi,f([\\pi]^{m})}(r,P)}+H S^{3/2}A B\\sqrt{\\frac{\\log(H A T/\\delta)}{T_{k}}}\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof of Lemma B.14. Consider any $\\pi\\in\\Pi^{k+1}$ . We have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V^{\\pi,f([\\pi]^{m})}(r,P)\\geq V^{\\pi,f([\\pi]^{m})}(r_{\\mathcal{U}^{k}},\\tilde{P}^{k})}\\\\ &{\\geq V^{\\pi,f([\\pi]^{m})}(r_{\\mathcal{U}^{k}},\\hat{P}^{k})-H S^{3/2}A B\\sqrt{\\frac{\\log(H A T/\\delta)}{T_{k}}}-H^{2}S A B\\sqrt{\\frac{\\alpha}{d^{*}T_{k}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ge\\tilde{V}^{\\tau}(\\eta_{\\ell\\ell},\\hat{\\rho}^{k},\\boldsymbol{\\upmu}^{\\star})-\\bar{H}^{2}\\sqrt{\\frac{\\alpha}{\\hbar\\ell}}-\\bar{H}^{S/2}4D\\sqrt{\\frac{\\alpha}{\\hbar\\ell}}\\frac{-H^{2}(H\\bar{A}T/\\hat{\\rho})}{T_{k}}-\\bar{H}^{2}5A B\\sqrt{\\frac{\\alpha}{\\hat{T}^{k}}}}\\\\ &{\\ge\\operatorname*{sup}\\tilde{V}^{\\tau}(\\tau_{\\ell\\ell},\\hat{\\rho}^{k},\\boldsymbol{\\upmu}^{\\star})-\\mathcal{O}\\left(H^{2}S A B\\sqrt{\\frac{\\alpha}{\\hat{T}^{k}}}+H S^{3/2}A B\\sqrt{\\frac{\\log(H A T/\\hat{\\rho})}{T_{k}}}\\right)}\\\\ &{\\ge\\operatorname*{sup}\\tilde{V}^{\\tau}^{\\tau_{\\ell}/(\\lfloor\\tau\\rfloor^{n+1})}(\\eta_{\\ell\\ell},\\hat{\\rho}^{k})-\\mathcal{O}\\left(H^{2/5}A B\\sqrt{\\frac{\\alpha}{\\hat{T}^{k}\\hat{T}_{k}}}+H S^{3/2}A B\\sqrt{\\frac{\\log(H A T/\\hat{\\rho})}{\\hat{T}_{k}}}\\right)}\\\\ &{\\ge\\operatorname*{sup}\\tilde{V}^{\\tau_{\\ell}/(\\lfloor\\tau\\rfloor^{n+1})}(\\eta_{\\ell\\ell},\\hat{\\rho}^{k})-\\mathcal{O}\\left(H^{2/5}A B\\sqrt{\\frac{\\alpha}{\\hat{T}^{k}\\hat{T}_{k}}}+H S^{3/2}A B\\sqrt{\\frac{\\log(H A T/\\hat{\\rho})}{T_{k}}}\\right)}\\\\ &{\\ge\\operatorname*{sup}\\tilde{V}^{\\tau_{\\ell}/(\\lfloor\\tau\\rfloor^{n+1})}(\\gamma_{\\ell\\ell},\\hat{\\rho}^{k})-\\mathcal{O}\\left(H^{2/5}A B\\sqrt{\\frac{\\alpha}{\\hat{T}_{k}}}+H S^{3/2}A B\\sqrt{\\frac{\\log(H A T/\\hat{\\rho})}{T_{k}}}\\right)}\\\\ &{\\ge\\operatorname*{sup}V^{\\tau_{\\ell}/(\\lfloor\\tau\\rfloor^{n})}(\\gamma,\\mathcal{O})}\\\\ &{\\ge\\operatorname*{sup}\\sqrt{\\eta^{\\tau_{\\ell}}}}\\\\ &{-\\mathcal{O}\\left(H^{2/5}A B\\sqrt{\\frac{\\alpha}{\\hat{T}^{k}\\hat{T}_{\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the first inequality follows from the first part of Lemma B.10, the second inequality follows from Lemma B.12, the third inequality follows from Lemma B.11, the fourth inequality follows from the definition of $\\Pi^{k+1}$ , the fifth inequality follows from Lemma B.11, the sixth inequality follows from Lemma B.12, and the last inequality follows from the second part of Lemma B.10. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "Proof of Theorem 5. Note that $K\\,=\\,\\mathrm{min}\\{j\\,:\\,\\sum_{k=1}^{j}T_{k}\\,\\geq\\,{\\bar{T}}\\}\\,=\\,{\\mathcal{O}}(\\log\\log{\\bar{T}})$ . Moreover, Algorithm 3 runs for $\\begin{array}{r}{\\sum_{k=1}^{K}H S A B(m-1+T_{k})=T}\\end{array}$ episodes, by the choice of $T_{k}=\\bar{T}^{1-\\frac{1}{2^{k}}}$ , where $\\begin{array}{r}{\\bar{T}:=\\operatorname*{min}\\{t\\in\\mathbb{N}:(m-1)\\log\\log t+t\\geq\\frac{T}{H S A B}\\}}\\end{array}$ . By Lemma B.14, with probability at least $1-\\delta$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname{PR}(T)\\leq(m-1+T_{1})t^{2}S A B}\\\\ &{\\qquad+\\displaystyle\\sum_{k=2}^{K}\\left((m-1)H^{2}S A B+H S A B\\cdot T_{k}\\bigg(H^{2}(S A B+H)\\sqrt{\\frac{\\alpha}{d\\cdot T_{k-1}}}+\\frac{H^{4}\\log(H S A B)}{T_{k-1}}\\right.}\\\\ &{\\qquad\\left.+H S^{3/2}4B\\sqrt{\\frac{\\log(H\\bar{X}T/\\delta)}{T_{k-1}}}\\right)\\right)}\\\\ &{\\qquad\\leq(m-1)H^{2}S A B K+H^{2}S A B\\sqrt{T}+K H^{3}S A B(S A B+H)\\sqrt{\\frac{\\alpha T}{d\\cdot T}}}\\\\ &{\\qquad+H^{5}S A B\\log(H S A B K/\\delta)\\displaystyle\\sum_{k=2}^{K}\\bar{t}^{-k}+K H^{2}S^{5/2}A^{2}B^{2}\\sqrt{\\gamma\\log(H A T/\\delta)}}\\\\ &{\\qquad\\leq(m-1)H^{2}S A B K+H^{3/2}\\sqrt{S A B^{2}}+K H^{3/2}\\sqrt{S A B}(S A B+H)\\sqrt{\\frac{\\alpha T}{d\\cdot T}}}\\\\ &{\\qquad+K H^{3/4}({S A B})^{3/4}\\log({R A B}^{\\prime}()T^{1/4}+K(H A B)^{3/2}S^{2}\\sqrt{T\\log(H A T/\\delta)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{Korm~eximed~eximed~for~To~To~K~ounited~firm~eximed}}\\\\ &{\\qquad\\mathrm{for~therwise~for~the~for~the~for~thetwo~thetwo~thetwo~thetwo~thetwo~thetwo~thetwo~thetwo~thetwo~thetwo~thetwo~thetwo~thetwo~}}\\\\ &{\\qquad\\times e t w o\\beta=\\frac{1}{2}\\sqrt{S A B^{2}\\log(H^{3})}\\log(\\tau)}\\\\ &{\\qquad\\qquad\\times\\operatorname{Unith}\\left(\\prod_{k=2}^{K}\\bar{t}\\right)=\\frac{1}{6}\\log(\\tau)}\\end{array}\n$$Note that the third term always dominates the second term. We can further simplify the bound (in ", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "the last inequality above), by making either the third term or the last term dominate the fourth term, which is implied by, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\tau\\gtrsim\\operatorname*{min}\\{\\frac{H^{5}S A B(d^{*})^{2}\\log^{4}(H S A B K/\\delta)}{\\alpha^{2}},\\frac{H^{9}(d^{*})^{2}\\log^{4}(H S A B K/\\delta)}{(S A B)^{3}\\alpha^{2}},\\frac{H^{13}\\log^{2}(H S A B K/\\delta)}{(A B)^{3}S^{5}}.\\}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Also notice the condition $\\begin{array}{r}{T_{k}\\ge\\frac{2\\log(S H K A/\\delta)}{d^{*}^{2}},\\forall k\\in[K]}\\end{array}$ in Lemma B.8 translates into: ", "page_idx": 26}, {"type": "equation", "text": "$$\nT\\gtrsim\\frac{H S A B\\log^{2}(S H K A/\\delta)}{(d^{\\ast})^{4}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Under these conditions of $T$ , the bound becomes: ", "text_level": 1, "page_idx": 27}, {"type": "equation", "text": "$$\n(m-1)H^{2}S A B K+K H^{3/2}\\sqrt{S A B}(H S A B+H^{2}+S^{3/2}A B)\\sqrt{\\frac{T\\alpha}{d^{*}}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 28}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 28}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] . ", "page_idx": 28}, {"type": "text", "text": "\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available. \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 28}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 28}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 28}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 28}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 28}, {"type": "text", "text": "1. Claims ", "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We clearly stated the main claims of our paper in both the abstract and introduction.   \nWe supported them in the main text. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We clearly highlighted the limitations in the Discussion section. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer:[Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We clearly stated all necessary assumptions and provided detailed proofs. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 29}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. ", "page_idx": 29}, {"type": "text", "text": "\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. ", "page_idx": 30}, {"type": "text", "text": "\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example ", "page_idx": 30}, {"type": "text", "text": "(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA]   \nJustification: [NA]   \nGuidelines:   \n\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA]   \nJustification: [NA]   \nGuidelines:   \n\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 30}, {"type": "text", "text": "\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Guidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 31}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA]   \nJustification: [NA]   \nGuidelines:   \n\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. ", "page_idx": 31}, {"type": "text", "text": "\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 32}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA]   \nJustification: [NA]   \nGuidelines:   \n\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 32}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 32}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. ", "page_idx": 32}, {"type": "text", "text": "\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 32}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ", "page_idx": 32}, {"type": "text", "text": "\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 32}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. ", "page_idx": 32}, {"type": "text", "text": "\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. \u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA]   \nJustification: [NA]   \nGuidelines:   \n\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Guidelines:   \n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]