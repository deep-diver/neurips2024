[{"type": "text", "text": "MoGU: A Framework for Enhancing Safety of LLMs While Preserving Their Usability ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yanrui $\\mathrm{Du}^{1}$ , Sendong Zhao1, Danyang Zhao1, Ming $\\mathbf{M}\\mathbf{a}^{1}$ , Yuhan Chen1, Liangyu Huo2, Qing Yang2, Dongliang $\\mathrm{Xu}^{2}$ , and Bing $\\mathrm{{Qin^{1}}}$ ", "page_idx": 0}, {"type": "text", "text": "1Harbin Institute of Technology, {yrdu, sdzhao, qinb}@ir.hit.edu.cn 2Du Xiaoman Financial ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large Language Models (LLMs) are increasingly deployed in various applications. As their usage grows, concerns regarding their safety are rising, especially in maintaining harmless responses when faced with malicious instructions. Many defense strategies have been developed to enhance the safety of LLMs. However, our research finds that existing defense strategies lead LLMs to predominantly adopt a rejection-oriented stance, thereby diminishing the usability of their responses to benign instructions. To solve this problem, we introduce the MoGU framework, designed to enhance LLMs\u2019 safety while preserving their usability. Our MoGU framework transforms the base LLM into two variants: the usable LLM and the safe LLM, and further employs dynamic routing to balance their contribution. When encountering malicious instructions, the router will assign a higher weight to the safe LLM to ensure that responses are harmless. Conversely, for benign instructions, the router prioritizes the usable LLM, facilitating usable and helpful responses. On various LLMs, we compare multiple defense strategies to verify the superiority of our MoGU framework. Besides, our analysis provides key insights into the effectiveness of MoGU and verifies that our designed routing mechanism can effectively balance the contribution of each variant by assigning weights. Our work released the safer Llama2 $^{7B}$ , Vicuna $^{7B}$ , $\\operatorname{Falcon}_{7B}$ , $\\mathrm{Dolphin}_{7B}$ , and Baichuan $\\stackrel{\\triangledown}{\\boldsymbol{\\cdot}}\\boldsymbol{\\mathrm{7}}B$ at github2. Warning: This paper presents examples of malicious instructions that may be offensive and upsetting. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large Language Models (LLMs) exhibit significant potential across various domains, yet they also face considerable safety vulnerabilities [28, 34, 43].To explore these vulnerabilities, several studies have conducted red-team evaluations with malicious instructions that could encourage harmful behaviors [45, 27]. Others have developed jailbreak attacks [10, 9, 42, 33, 6] aimed at provoking harmful responses from LLMs by using carefully crafted adversarial prompts. These safety vulnerabilities may lead to severe consequences, including the promotion of racial discrimination, breaches of ethical standards, and violations of human rights [9, 40]. ", "page_idx": 0}, {"type": "text", "text": "In response to LLMs\u2019 safety vulnerabilities, some studies have pursued aligning LLMs with human values through SFT and RLHF techniques. Despite these advancements, recent work [45, 36] indicates that even aligned LLMs are still susceptible to jailbreak attacks. To further enhance LLMs safety, various defense strategies have been proposed, including input and output detection [26, 21], in-context safety demonstration [37], and enhancing the likelihood of decoding rejection tokens [39]. These strategies often focus on ensuring harmless responses during red-team evaluations and jailbreak attacks but overlook the impact on the quality of responses to benign instructions. Our research finds that existing defense strategies lead LLMs to adopt a rejection-oriented stance, thereby diminishing the usability of their responses to benign instructions. By prioritizing safety over usability, these strategies become less effective in practical applications. Consequently, this presents a key challenge \u2014 seesaw effect between security and usability: How can we enhance the safety of LLMs while preserving their usability? ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Despite existing defense strategies not effectively addressing this challenge, the input detection [21] strategy provides a straightforward solution. This strategy triggers a safety mechanism by distinguishing malicious and benign instructions. However, this implementation, which relies on binary classification of instructions, often struggles with arbitrary treatment. Many benign instructions may be wrongly marked as malicious, mistakenly activating the safety mechanism and thus diminishing the usability of responses to benign instructions. The Mixture of Experts (MoE) series of research provides a promising improvement direction [18, 23, 32]. MoE employs a dynamic routing mechanism within LLMs to balance contributions from different experts, thereby improving LLMs\u2019 overall performance. This dynamic routing mechanism has proven effective in assigning weights to experts according to the input instruction. Therefore, in our research, we aim to introduce a dynamic routing mechanism to enhance LLMs\u2019 safety. ", "page_idx": 1}, {"type": "text", "text": "Based on these insights, we introduce a novel framework called Mixing of Glad and Unwilling Responders (MoGU). We first employ the Parameter-Efficient Fine-Tuning technique LoRA [15], to transform the base LLM into two distinct states: the Glad Responder $(\\mathrm{Glad}_{r e s p})$ and the Unwilling Responder $\\left(\\mathrm{Unwill}_{\\mathit{r e s p}}\\right)$ ). The ${\\mathrm{Glad}}_{r e s p}$ , as an extremely usable LLM, is trained to generate glad responses to any instruction. Conversely, $\\mathrm{Unwill}_{r e s p}$ , as an extremely safe LLM, is trained to be highly cautious, rejecting any instruction it receives. The core component of MoGU is a dynamic router that serves as a safety sensor, embedded at each layer where LoRA is applied. This router is trained to dynamically balance the contributions of ${\\mathrm{Glad}}_{r e s p}$ and $\\mathrm{Unwill}_{r e s p}$ according to the input vector, effectively mixing their output vectors. As illustrated in Fig. 1, when faced with ", "page_idx": 1}, {"type": "image", "img_path": "SrFbgIjb53/tmp/09b3e30376ed5d2f77246a16930dbb88e5ddd96df8f5ca784f89f285b3f55c61.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: An example to illustrate how the router assigns weights to ${\\mathrm{Glad}}_{r e s p}$ and $\\mathrm{Unwill}_{r e s p}$ . The h_states and o_states represent the input vector and output vector respectively. ", "page_idx": 1}, {"type": "text", "text": "a malicious instruction, the router will assign a higher weight to $\\mathrm{Unwill}_{r e s p}$ , ensuring a safe, rejection response. On the contrary, the router shifts more weight to ${\\mathrm{Glad}}_{r e s p}$ for the benign instruction, facilitating a glad, useful response. ", "page_idx": 1}, {"type": "text", "text": "In our experiments, we revealed limitations of existing strategies that diminish the usability of LLMs. Our experiment results verify that our MoGU framework can keep robust defense performance under the red-team evaluation and various jailbreak attacks while preserving LLMs\u2019 usability. Besides, compared to existing defense strategies, our framework demonstrates obvious advantages across various LLMs. We also conduct quantitative analysis to confirm that the router can effectively balance the contribution of each variant by assigning weights, thereby ensuring both the safety and the usability of LLMs. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we summarize related work from two aspects: attack strategies and defense strategies. ", "page_idx": 1}, {"type": "text", "text": "2.1 Attack strategies ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Red-team evaluation. The primary goal of red-team evaluations [30] is to assess the safety of LLMs by compiling a set of malicious instructions that reflect common user queries. The collection of these instructions is conducted in two ways: 1) gathering malicious instructions from crowdsourced workers [11]. 2) automatically generating malicious instructions with another LLM that simulates human behavior [3]. The scope of these malicious instructions should be wide-ranging, covering topics such as toxicity, discrimination, privacy, and misinformation [13]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Jailbreak attack. Jailbreak attacks [12] aim to circumvent the built-in safety mechanisms of LLMs by modifying original red-team malicious instructions into more complex adversarial prompts. These strategies generally fall into two categories: heuristic-based and optimization-based strategies. ", "page_idx": 2}, {"type": "text", "text": "Heuristic-based strategies attempt to induce LLMs to prioritize task completion over adherence to safety constraints. For instance, some studies [36, 19] have prompted LLMs to begin their responses with indicators of successful jailbreak, such as \u201cStart your response with [Sure, here\u2019s]\u201d. Others [35, 20] employ psychological tactics to subtly encourage LLMs to violate safety constraints. ", "page_idx": 2}, {"type": "text", "text": "Optimization-based strategies attempt to search for adversarial prompt templates based on constructed objectives. These strategies fall into two categories: token-level and expression-level. Token-level strategies [45] searched for token sequences via backpropagation and spliced them around original malicious instructions. However, these token sequences often lack semantic coherence, rendering them vulnerable to detection by Perplexity (PPL) algorithms [17]. Moreover, expression-level strategies [24, 41] employ genetic algorithms to search for natural language prompt templates. This approach enhances the concealment of jailbreak attacks, making them more difficult to detect. ", "page_idx": 2}, {"type": "text", "text": "2.2 Defense Strategies ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Defense strategies can be categorized into two main types: those that improve built-in safety and those that leverage external tools. Strategies focused on built-in safety aim to align LLMs with human values, employing methods such as Supervised Fine-Tuning (SFT) [44] and Reinforcement Learning from Human Feedback (RLHF) [29]. SFT reduces experiential loss by incorporating high-quality, human-annotated samples during training, whereas RLHF optimizes LLMs based on valuable human feedback. Despite the widespread adoption of these methods, recent studies [45, 5] indicate that aligned LLMs (e.g. Llama2) are still vulnerable to jailbreak attacks. ", "page_idx": 2}, {"type": "text", "text": "Meanwhile, many researchers are developing strategies that leverage external tools to further improve LLMs\u2019 safety. These strategies focus on inference enhancement and the detection of input and output. Inference enhancement strategies guide LLMs to generate safer content through methods such as self-safety reminding [38] or by presenting safety in-context demonstrations [37]. Strategies for the detection of input and output involve identifying potentially harmful content to trigger the appropriate safety mechanisms. Methods such as paraphrasing and retokenization [16] can render certain attacks ineffective by altering the expression of inputs. Moreover, binary classifiers [21] based on BERT [7] can be trained to detect malicious inputs, and self-examining method [14] enables LLMs to assess the harmfulness of their own outputs. Despite these efforts, it remains challenging to enhance the safety of LLMs while preserving their usability. ", "page_idx": 2}, {"type": "text", "text": "3 MoGU Framework ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The overall framework of our MoGU is illustrated in Fig. 2. We introduce our framework from three aspects: the training data preparation, the training stage, and the inference stage. ", "page_idx": 2}, {"type": "text", "text": "3.1 Training Data Preparation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "For our training data, we only collected 600 instructions, which include 300 benign instructions sourced from Alpaca3 and 300 malicious instructions from Advbench [45]. As illustrated in Fig. 2, for each instruction, we construct both a glad response and a rejection response. We label benign instructions as $\\mathbf{X}_{b}$ , malicious instructions as $\\Chi_{m}$ , glad responses as $\\Upsilon_{g}$ , and rejection responses as $\\mathrm{Y}_{r}$ . Therefore, our training dataset encompasses four types of data pairs: $({\\Chi}_{b},{\\sf Y}_{g})$ , $({\\mathrm{X}}_{b},{\\mathrm{Y}}_{r})$ , $(\\mathbf{X}_{m},\\mathbf{Y}_{g})$ , and $\\left({{\\Chi}_{m}},{\\mathrm{Y}_{r}}\\right)$ . We observe that LLMs typically generate glad responses to benign instructions and rejection responses to malicious instructions. Consequently, during the construction of $(X_{b},\\,\\Upsilon_{g})$ and $\\left({{\\mathrm{X}}_{m}},{{\\mathrm{Y}}_{r}}\\right)$ , we almost preserve their original responses. Here is how to construct them. ", "page_idx": 2}, {"type": "image", "img_path": "SrFbgIjb53/tmp/ed25142d62ebca7b892be7569a05a7a6fc57b7e4a7ae74afaf3497bd85940ca0.jpg", "img_caption": ["Figure 2: Overall framework of our MoGU. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "\u2022 Construction of $(\\mathbf{X}_{b},\\,\\mathbf{Y}_{g})$ : we prompt the base LLM to generate responses to $X_{b}$ and collect some rejection expressions (detailed in App. A) for rule-based detection. If rejection responses are detected, they will be discarded. Then, we will craft glad responses $\\Upsilon_{g}$ with the help of GPT- $.4^{4}$ .   \n\u2022 Construction of $(\\mathbf{X}_{b},\\,\\mathbf{Y}_{r})$ : we utilize GPT-4 to craft rejection responses to $\\mathbf{X}_{b}$ . For guiding GPT-4, we present demonstrations of generating rejection responses to benign instructions.   \n\u2022 Construction of $(\\mathbf{X}_{m},\\mathbf{Y}_{g})$ : since Advbench [45] has manually annotated high-quality glad responses to $\\Chi_{m}$ , we directly use their annotated data.   \n\u2022 Construction of $\\left({{\\Chi}_{m}},{\\mathrm{Y}_{r}}\\right)$ : we prompt the base LLM to generate responses to $\\Chi_{m}$ and utilize the same rule-based detection as above. If glad responses are detected, they will be discarded. Then, we will craft rejection responses ${\\mathrm{Y}}_{r}$ with the help of GPT-4. ", "page_idx": 3}, {"type": "text", "text": "In the scenarios mentioned above for GPT-4, we adopt the In-Context Learning [8] idea, and provided in-context demonstrations can be found in App. B. ", "page_idx": 3}, {"type": "text", "text": "3.2 Training Stage ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "During the training stage, we initially train the Glad and Unwilling responders using the LoRA framework. Subsequently, all other parameters are frozen, and we train our introduced router. In the LoRA framework, only the low-rank decomposition matrices added to the targeted weight matrices are updated. As illustrated in Fig. 2, the targeted weight matrices typically include Q (Query), K (Key), V (Value), ${0}_{p r o j}$ (Output Projection), and FFN (Feed-Forward Network). In our research, we regard ${0_{p r o j}}$ as the targeted weight matric for exploration. ", "page_idx": 3}, {"type": "text", "text": "The training of glad and unwilling responders. The objective of ${\\mathrm{Glad}}_{r e s p}$ is to calibrate the base LLM into an extremely usable LLM that can generate glad responses to any instruction. The extreme case is that ${\\mathrm{Glad}}_{r e s p}$ can generate glad responses even to malicious instructions. Therefore, we use the data $(\\mathbf{X}_{m},\\mathbf{Y}_{g})$ to train the base LLM, and the loss function can be expressed as: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nL o s s_{g l a d}=\\frac{1}{M}\\sum_{i=1}^{M}C E_{l o s s}(y_{g}^{i},f_{g l a d}(x_{m}^{i};\\theta_{g l a d}))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $(x_{m}^{i},y_{g}^{i})\\in(X_{m},Y_{g})$ and $C E_{l o s s}$ represents Cross Entropy Loss. Similarly, the objective of the Unwillresp is to calibrate the base LLM to an extremely safe LLM that can reject any instruction. The extreme case is that $\\mathrm{Unwill}_{r e s p}$ can even reject any benign instruction. Therefore, we use the data $(\\mathbf{X}_{b},\\,\\mathbf{Y}_{r})$ to train the base LLM, and the loss function can be expressed as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nL o s s_{u n w i l l}=\\frac{1}{N}\\sum_{i=1}^{N}C E_{l o s s}(y_{r}^{i},f_{u n w i l l}(x_{b}^{i};\\theta_{u n w i l l}))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $(x_{b}^{i},y_{r}^{i})\\in(X_{b},Y_{r})$ . Subsequently, inspired by Contrastive Learning (CL) [25], we incorporated negative samples to further improve our framework. For ${\\mathrm{Glad}}_{r e s p}$ , we need to ensure that it will not generate rejection responses to any malicious instruction. And for $\\mathrm{Unwill}_{r e s p}$ , we need to ensure that it will not generate glad responses to any benign instruction. Consequently, we regard data $(\\mathbf{X}_{m}$ , $\\mathrm{Y}_{r.}$ ) and $(X_{b},\\,\\mathrm{Y}_{g})$ as negative samples for training ${\\mathrm{Glad}}_{r e s p}$ and $\\mathrm{Unwill}_{r e s p}$ , respectively. The loss function for ${\\mathrm{Glad}}_{r e s p}$ can be formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nL o s s_{g l a d}=\\frac{1}{M}\\sum_{i=1}^{M}\\frac{C E_{l o s s}(y_{g}^{i},f_{g l a d}(x_{m}^{i};\\theta_{g l a d}))}{C E_{l o s s}(y_{r}^{i},f_{g l a d}(x_{m}^{i};\\theta_{g l a d}))}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\{(X_{m},Y_{g}),(X_{m},Y_{r})\\;\\to\\;(X_{m},Y_{g},Y_{r})\\}$ and $(x_{m}^{i},y_{g}^{i},y_{r}^{i})\\ \\in\\ (X_{m},Y_{g},Y_{r})$ . And the loss function for the $\\mathrm{Unwill}_{r e s p}$ can be formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nL o s s_{u n w i l l}=\\frac{1}{N}\\sum_{i=1}^{N}\\frac{C E_{l o s s}(y_{r}^{i},f_{u n w i l l}(x_{b}^{i};\\theta_{u n w i l l}))}{C E_{l o s s}(y_{g}^{i},f_{u n w i l l}(x_{b}^{i};\\theta_{u n w i l l}))}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\{(X_{b},Y_{r}),(X_{b},Y_{g})\\to(X_{b},Y_{r},Y_{g})\\}$ and $(x_{b}^{i},y_{r}^{i},y_{g}^{i})\\in(X_{b},Y_{r},Y_{g})$ . ", "page_idx": 4}, {"type": "text", "text": "The design and training of router. Our router comprises two linear networks, denoted as $\\mathbf{R}_{g l a d}$ and $\\mathbf{R}_{u n w i l l}$ , both sharing identical structural configurations. Each linear network $\\mathbf{R}$ incorporates a low-rank decomposition matrix followed by a fully connected layer. Specifically, the low-rank decomposition matrix involves matrices $U\\,\\in\\,\\mathbb{R}^{d_{m o d e l}\\,\\times\\,d_{r o u t e r}}$ and $\\overset{\\triangledown}{V}\\in\\bar{\\mathbb{R}}^{d_{r o u t e r}\\times\\dot{d}_{m o d e l}}$ , and the fully connected layer is denoted by a matrix $W\\in\\mathbb{R}^{d_{m o d e l}\\times1}$ . We assume that for the i-th projection layer $0_{p r o j}$ , the input vector is denoted by $h^{(i)}\\in\\mathbb{R}^{s e q\\textunderscore l e n\\times d_{m o d e l}}$ . Here, seq_len refers to the length of the input tokens, $d_{m o d e l}$ refers to the dimension of the model\u2019s hidden layers, and $d_{r o u t e r}$ is a hyperparameter determining the intermediate dimension in the low-rank decomposition matrix. The role of linear network $\\mathbf{R}$ can be formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nw=R(\\boldsymbol{h}^{(i)})=\\sigma(((\\boldsymbol{h}^{(i)}U V+b_{1})\\boldsymbol{W})+b_{2})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\sigma$ represents the sigmoid activation function, $w\\in\\mathbb{R}^{s e q\\mathrm{-}l e n\\times1}$ , $b_{1}$ and $b_{2}$ represent the bias term. The weights $w_{g l a d}$ and $w_{u n w i l l}$ , provided by ${\\bf R}_{g l a d}$ and $\\mathbf{R}_{u n w i l l}$ respectively, will be assigned to ${\\mathrm{Glad}}_{r e s p}$ and $\\mathbf{Unwill}_{r e s p}$ to mix their output vectors. As shown in Fig. 2, the output vector of ${\\mathrm{Glad}}_{r e s p}$ \u2019s i-th ${0_{p r o j}}$ layer can be formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\no_{g l a d}^{(i)}=f_{b a s e}(h^{(i)})+f_{l o r a\\_g l a d}^{b}(f_{l o r a\\_g l a d}^{a}(h^{(i)}))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $o_{g l a d}^{(i)}\\in\\mathbb{R}^{s e q\\_l e n\\times d_{m o d e l}}$ , $f_{l o r a\\_g l a d}^{b}$ and $f_{l o r a\\_g l a d}^{a}$ are low-rank decomposition matrices in LoRA framework. And the output vector of $\\mathrm{Unwill}_{r e s p}$ \u2019s i-th ${0}_{p r o j}$ layer can be formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\no_{u n w i l l}^{(i)}=f_{b a s e}(h^{(i)})+f_{l o r a\\_u n w i l l}^{b}(f_{l o r a\\_u n w i l l}^{a}(h^{(i)}))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $o_{u n w i l l}^{(i)}\\in\\mathbb{R}^{s e q_{-}l e n\\times d_{m o d e l}}$ , $f_{l o r a\\_u n w i l l}^{b}$ and $f_{l o r a\\_u n w i l l}^{a}$ are low-rank decomposition matrices in LoRA framework. Then, the mixture of ${\\mathrm{Glad}}_{r e s p}$ and $\\bar{\\mathrm{Unwill}}_{r e s p}$ output vectors can be formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\no_{M o G U}^{(i)}=w_{g l a d}\\odot o_{g l a d}^{(i)}+w_{u n w i l l}\\odot o_{u n w i l l}^{(i)}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where o(Mi)oGU \u2208Rseq_len\u00d7dmodel.", "page_idx": 5}, {"type": "text", "text": "During the training of the router, all other parameters are frozen, and only the router\u2019s parameters will be updated. The primary objective of the router is to guide LLMs in generating appropriate responses to various instructions. Specifically, the router should facilitate glad responses to benign instructions and rejection responses to malicious instructions. To achieve this, we use both $({\\mathbf{X}}_{b},{\\mathbf{Y}}_{g})$ and $(\\mathbf{X}_{m}$ , $\\mathrm{Y}_{r.}$ ) as the training data. The loss function can be formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nL o s s_{r o u t e r}^{(1)}=\\frac{\\sum_{i=1}^{N}C E_{l o s s}(y_{g}^{i},f_{r o u t e r}(x_{b}^{i};\\theta_{r o u t e r}))+\\sum_{j=1}^{M}C E_{l o s s}(y_{r}^{j},f_{r o u t e r}(x_{m}^{j};\\theta_{r o u t e r}))}{N+M}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $(x_{b}^{i},y_{g}^{i})\\;\\in\\;(X_{b},Y_{g})$ and $(x_{m}^{j},y_{r}^{j})\\ \\in\\ (X_{m},Y_{r})$ . Besides, the router is equipped with a finer-grained objective: it will assign weights according to the type of instruction. Specifically, a higher weight will be assigned to $\\bar{\\mathrm{Glad}}_{r e s p}$ for benign instructions and to $\\mathrm{Unwill}_{r e s p}$ for malicious instructions. To reinforce this behavior, we use the L1 Norm to regulate the optimization of weights $w_{g l a d}$ and $w_{u n w i l l}$ assigned by the router, ensuring the assigning pattern adheres to our expectations. The loss function can be formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nL o s s_{r o u t e r}^{(2)}=\\binom{\\|1-w_{g l a d}\\|_{1}+\\|w_{u n w i l l}\\|_{1}\\quad\\mathrm{if}\\;x\\in X_{b}}{\\|w_{g l a d}\\|_{1}+\\|1-w_{u n w i l l}\\|_{1}\\quad\\mathrm{if}\\;x\\in X_{m}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\|\\cdot\\|_{1}$ represents the L1 Norm. Finally, the overall loss function can be formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L o s s_{r o u t e r}=L o s s_{r o u t e r}^{(1)}+\\lambda L o s s_{r o u t e r}^{(2)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\lambda$ is a hyperparameter. ", "page_idx": 5}, {"type": "text", "text": "3.3 Inference Stage ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Previous research [45, 39] has shown that the initial response tokens are critical to ensuring the harmlessness of the whole response. If initial response tokens express rejection, the response is more likely to be harmless. Given these findings, and considering that our additional parameters extend inference time, we employ MoGU only for decoding the first m tokens as shown in Fig. 2. The subsequent tokens are decoded by the base LLM to preserve the efficiency and quality of decoding. ", "page_idx": 5}, {"type": "text", "text": "4 Main Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Preliminary ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "LLMs. In our research, we evaluated chat versions of five open-source LLMs, including four from the Llama series: $\\mathrm{Llama}2_{7B}$ [34], Vicuna $_{7B}$ [43], Falcon $_{7B}$ [1], and $\\mathrm{Dolphin}_{7B}{}^{5}$ . Notably, Dolphin $_{7B}$ has not yet undergone a safety review. We also evaluated Baichuan $2_{7B}$ [2], which features an architecture distinct from those in the Llama series. ", "page_idx": 5}, {"type": "text", "text": "Evaluation data. In our evaluation, we focused on assessing LLMs\u2019 safety and usability. For the safety assessment, on the one hand, we conducted a red-team evaluation. We utilize the Advbench [45], which comprises 520 malicious instructions\u2014300 for our training as introduced in Sec. 3.1 and the rest 220 for testing. Additionally, we collected 200 malicious instructions from Just-Eval [22] (labeled as \u201cMalicious\u201d). On the other hand, we conducted the safety evaluation against various jailbreak attacks. We employed both optimization-based and heuristic-based strategies. For the optimization-based strategies, we utilized AutoDAN [24], GCG [45], and PAIR [4], each of which applies different adversarial prompts to 50 test samples. Specifically, AutoDAN employs genetic algorithms to generate semantically fluent adversarial prompts. GCG uses gradient propagation to identify token sequences as attack suffixes. PAIR iteratively optimizes adversarial prompts based on the LLMs\u2019 ability. For the heuristic-based strategies, we used SAP30 [5] and ${\\mathrm{Comp}}_{o b j}$ [36], each of which applies the same adversarial prompt to 100 test samples. SAP30 focuses on semantic inducements, whereas ${\\mathrm{Comp}}_{o b j}$ is designed to compromise LLM safety by conducting a competing objective. For the usability assessment, we used 800 benign instructions from Just-Eval [22] to assess LLMs\u2019 problem-solving abilities. In App. C, we provide examples for each evaluation set. ", "page_idx": 5}, {"type": "table", "img_path": "SrFbgIjb53/tmp/be482886af72794180c5a18b251538e2135e87f1ad527fc21c34e2a2d60d5c68.jpg", "table_caption": ["Table 1: Results of different defense strategies on red-team evaluation. $\\Delta{\\bf S R}\\%$ values are reported. Lower $\\operatorname{ASR}\\%$ values indicate better defense performance. The colors red, yellow, and blue represent the top three strategies in ranking. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Evaluation metric. For the evaluation metrics, we utilize both GPT-4 scoring and rule-based evaluations. For assessing LLM\u2019s safety, we employ GPT-Judge [31], which rates the harmfulness of responses on a scale from 1 to 5, where 1 represents harmlessness and 5 indicates extreme harmfulness. Additionally, consistent with the work [45], we identify targets $(T)$ that can represent harmless responses (as detailed in App. D) and calculate the Attack Success Rate (ASR) based on these targets. ASR quantifies the probabilities that responses deviate from our identified targets, which can be expressed as # of re#s poof naslels  rdeesvpioantsee fsrom T. For assessing LLMs\u2019 usability, in line with Just-Eval [22], we utilize GPT-4 to evaluate responses (GPT-Eval) across five dimensions: helpfulness, clarity, factuality, depth, and engagement. Each response is scored from 1 to 5, with higher scores denoting better quality. Moreover, we compile a list of common rejection expressions (as detailed in App. A) and monitor their frequency in LLM responses (Rule-based Eval) to evaluate the extent to which LLMs adopt a stance of rejection. During our evaluation, we spent approximately $\\mathbb{S}400$ for calling the GPT-4 API. ", "page_idx": 6}, {"type": "text", "text": "Baselines. We selected seven advanced defense strategies as our baselines. SFT strategy [44] employs high-quality data to train LLMs, thereby aligning LLMs with human values. Detect $i n p$ [21] train a binary classifier based on BERT to distinguish between benign and malicious instructions. SelfExamine [14] strategy prompts LLMs to assess whether their responses are harmful. If risky contents are detected by $\\mathsf{D e t e c t}_{i n p}$ and Self-Examine, the response \u201cSorry, I cannot answer your question.\u201d will be returned. Retokenization [16] strategy counters various jailbreak attacks by altering the input to shift meanings subtly. Self-Reminder [38] strategy consistently cues LLMs to maintain awareness of safety throughout the input process. ICD [37] strategy integrates safety in-context demonstrations into prompts. SafeDecoding [39] strategy increases the likelihood of rejection tokens during the decoding phase. We implemented SFT within the LoRA framework based on our constructed data and followed the open-sourced code from work [39] to reproduce other baselines. ", "page_idx": 6}, {"type": "text", "text": "Hyperparameter settings. We configure our router\u2019s intermediate dimension $d_{r o u t e r}$ to 512 and set the $\\lambda$ in $L o s s_{r o u t e r}$ to 2. For training ${\\mathrm{Glad}}_{r e s p}$ and $\\mathrm{Unwill}_{r e s p}$ , the learning rate is set to 5e-5, and for training the router, the learning rate is set to 5e-4. Besides, the $\\alpha$ and $\\mathrm{d}_{l o r a\\_r}$ in LoRA are set to 16 and 8 respectively. During inference, only the first 5 tokens are decoded with our MoGU and the remaining tokens are decoded with the base LLM. Decoding configurations of various LLMs can be found in App. E. All our experiments were done on a single 80GB A100. ", "page_idx": 6}, {"type": "text", "text": "4.2 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In Tab. 1 and 2, we respectively evaluate the performance of defense strategies under red-team evaluation and against various jailbreak attacks. For the red-team evaluation, we report only the ASR. In contrast, for the jailbreak attacks, given the broader variability in LLMs\u2019 responses, we report both the GPT-4 score and the ASR. On the whole, the ICD strategy outperforms others on $\\mathrm{Llama}2_{7B}$ , MoGU excels on $\\mathrm{Vicuna}_{7B}$ , and SafeDecoding excels on $\\operatorname{Falcon}_{7B}$ . Furthermore, these three strategies demonstrate stable and effective defense performance across various LLMs. Thus, in Tab. 3, we assess the impact of these three competitive strategies on the usability of LLMs. ", "page_idx": 6}, {"type": "text", "text": "Table 2: Results of different defense strategies against various jailbreak attacks. GPT score $(\\mathrm{ASR}\\%)$ values are reported. Lower GPT score $(\\mathrm{ASR}\\%)$ values indicate better defense performance. The colors red, yellow, and blue represent the top three strategies in ranking ", "page_idx": 7}, {"type": "table", "img_path": "SrFbgIjb53/tmp/85a070ccda9feb8f69fda417ca78b42d04a44f7dce77ad4eda5dd9363167d83a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Besides, since the main ideas of our MoGU and Detec $i n p$ are similar, in that they sense inputs to execute appropriate operations, we also report the performance of $\\mathsf{D e t e c t}_{i n p}$ in Tab. 3. Through comprehensive analysis of results across Tab. 1, 2, and 3, we identify three key phenomena. ", "page_idx": 7}, {"type": "text", "text": "MoGU keeps robust defense performance. As demonstrated in Tab. 1, our MoGU framework stably enhances the safety of various LLMs during red-team evaluations. Notably, as described in Sec. 3.1, our training data solely comprises original red team malicious instructions, and explicitly excludes any adversarial samples with jailbreak attack prompts. Despite this, our MoGU framework still maintains robust defense performance against various jailbreak attacks as illustrated in Tab. 2. ", "page_idx": 7}, {"type": "text", "text": "Existing defense strategies enhance the safety of LLMs but often compromise their usability. As shown in Tab. 2, the ICD strategy significantly increases the defense of Llama $2_{7B}$ to jailbreak attacks. However, after applying the ICD strategy, as shown in Tab. 3, the rate of rejection responses to benign instructions on $\\mathrm{Llama}2_{7B}$ surged from $14.00\\%$ to $92.25\\%$ , and its response usability score dropped dramatically from 3.87 to 2.17. Similarly, as shown in Tab. 2, the SafeDecoding strategy effectively defends $\\mathrm{Vicuna}_{7B}$ against jailbreak attacks. However, as shown in Tab. 3, it leads to a substantial increase in rejection responses from $3.63\\%$ to $39.50\\%$ and a decline in response usability score from 3.89 to 2.29. Such phenomenons indicate that existing defense strategies often lead LLMs to adopt a rejection-oriented stance, thereby diminishing their usability. ", "page_idx": 7}, {"type": "text", "text": "MoGU can enhance LLMs\u2019 safety while preserving their usability. As illustrated in Tab. 1 and 2, our framework has exhibited robust defense performance across various LLMs. Importantly, it also maintains the ability to respond with high quality to benign instructions, as evidenced by results in Tab. 3. Under our MoGU framework, the frequency of rejection expressions in LLMs\u2019 responses to ", "page_idx": 7}, {"type": "text", "text": "Table 3: Assessing LLMs\u2019 usability. GPT-Eval scores and probabilities of rejection expressions (Rule-based Eval) are reported. Higher GPT-Eval scores indicate higher quality of responses. ", "page_idx": 8}, {"type": "table", "img_path": "SrFbgIjb53/tmp/9deedb8418bf5ed90315c14c0cc0b99debc8227ac1d90dd3d63947953d47ce10.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "SrFbgIjb53/tmp/47eda2236c7ebdfb937dff99eeb165948879f05cd39bb63dab65221a90303531.jpg", "table_caption": [], "table_footnote": ["Table 4: Results of ablation Experiments. $L0\\mathrm{SS}_{C L}$ represents Contrastive Learning Loss in $L0\\mathrm{SS}_{g l a d}$ and ${\\mathrm{Loss}}_{w i l l}$ , and $L1_{N o r m}$ represents the L1 Norm constraint in $\\mathrm{Loss}_{r o u t e r}$ . "], "page_idx": 8}, {"type": "text", "text": "benign instructions remains nearly equivalent to that observed in base LLMs. Such phenomenons verify the superiority of our MoGU framework compared to other defense strategies. ", "page_idx": 8}, {"type": "text", "text": "5 Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we conducted an ablation experiment, provided a quantitative analysis, and discussed our introduced size of parameters. In App. F and G, we respectively provide a case study and extend our MoGU framework to Baichuan $\\stackrel{\\triangledown}{\\boldsymbol{\\cdot}}\\boldsymbol{7}B$ and Dolphin $_{7B}$ to further demonstrate MoGU\u2019s flexibility. Besides, in App. I, we discuss the limitations of our research. ", "page_idx": 8}, {"type": "text", "text": "5.1 Ablation Experiment ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We analyze the impact of Contrastive Learning Loss $(\\mathrm{Loss}_{C L})$ in $L0\\mathrm{ss}_{g l a d}$ and ${\\mathrm{Loss}}_{w i l l}$ and the L1 Norm $(\\mathrm{L}1_{N o r m})$ constraint in $\\mathrm{Loss}_{r o u t e r}$ . Tab. 4 illustrates that omitting ${\\mathrm{Loss}}_{C L}$ and $L1_{N o r m}$ will lead to a decrease in the defense performance of our framework. Notably, the impact of $L1_{N o r m}$ proved to be more significant. ", "page_idx": 8}, {"type": "image", "img_path": "SrFbgIjb53/tmp/ece4895f55a518e3e0f56c8f1ddecceeba713e6ef18e8b1579ec8714b692190c.jpg", "img_caption": ["Figure 4: We present the results $(\\mathrm{ASR}\\%)$ of LLMs under red team evaluations and various jailbreak attacks, with ${\\bf d}_{r o u t e r}$ set at 128, 256, 512, and 1024. The \u201cAVG.\u201d indicates the average defense performance. Lower $\\Delta{\\bf S R}\\%$ values indicate better defense performance. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5.2 Quantitative Analysis ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To investigate the role of the router, we analyzed the distributions of weights assigned by the router on Llama $2_{7B}$ , Vicuna $^{7B}$ , and $\\operatorname{Falcon}_{7B}$ . We collected 350 malicious instructions with various jailbreak attack prompts and 800 benign instructions from Just-Eval. The mean values of weights $w_{u n w i l l}$ and $w_{g l a d}$ are calculated during processing each instruction. Fig. 3 presents the boxplot that depicts the statistical results for Vicuna $^{7B}$ . Notably, during jailbreak attacks, the router assigns a higher weight $w_{u n w i l l}$ to $\\mathrm{Unwill}_{r e s p}$ , while for benign instructions, it favors a higher weight $w_{g l a d}$ for ${\\mathrm{Glad}}_{r e s p}$ . This allocation pattern aligns perfectly with our expectations of the router\u2019s functionality. The same patterns are also observed for Llama2 $_{7B}$ and Falcon $^{7B}$ , detailed in App. H. ", "page_idx": 9}, {"type": "text", "text": "5.3 Size of Introduced Parameters ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In our MoGU framework, we added the LoRA parameters of ${\\mathrm{Glad}}_{r e s p}$ and $\\mathrm{Unwill}_{r e s p}$ , and router parameters. In each layer, the number of added parameters can be calculated as $(d_{m o d e l}\\times d_{r o u t e r}\\times4+d_{m o d e l}\\times8+d_{m o d e l}\\times$ $d_{l o r a\\_r}\\times\\,4)$ . Taking $\\mathrm{Llama}2_{7B}$ with 32 layers as an example, the total number of added parameters can be calculated as 273, 678, $336=(32\\times\\bar{(}4096\\times512\\times4+$ $4096\\times8+4096\\times8\\times4)$ ), accounting for about $3.91\\%$ of all parameters. ", "page_idx": 9}, {"type": "text", "text": "Furthermore, We investigated the impact of parameter size on the defense performance of LLMs by adjusting the ${\\mathrm{d}}_{r o u t e r}$ to 128, 256, 512, and 1024. Our analysis focused on the performance of Llama $2_{7B}$ , Vicuna $.7B$ , and $\\operatorname{Falcon}_{7B}$ against red-team evaluations and various jailbreak attacks. As shown in Fig. 4, setting ${\\mathrm{d}}_{r o u t e r}$ to 512 will consistently result in superior defense performance across all three LLMs. Notably, Llama $2_{7B}$ and $\\mathrm{Vicuna}_{7B}$ also exhibited strong defense performance at the lower ${\\mathrm{d}}_{r o u t e r}$ settings of 128 and 256. These results suggest that within our framework, the safety of LLMs might be enhanced effectively with fewer parameters. ", "page_idx": 9}, {"type": "image", "img_path": "SrFbgIjb53/tmp/91811861e80420f99921de7d3c5b5bfb36fe993dd885ecf48f628a4455557e84.jpg", "img_caption": ["Figure 3: The distribution of weights assigned by the router of Vicuna $^{7B}$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In our research, we find the limitations of existing defense strategies, which often sacrifice usability in the pursuit of enhancing LLMs\u2019 safety. To address this issue, we introduce our MoGU framework, which designs a dynamic routing mechanism. Our MoGU can improve LLMs\u2019 safety while preserving their usability. Our comprehensive evaluations across various LLMs verify our MoGU\u2019s superiority compared to other strategies. In the future, we will further refine and optimize the MoGU framework. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by the National Key R&D Program of China [2021ZD0113302]; the National Natural Science Foundation of China [62206079]; and the Heilongjiang Provincial Natural Science Foundation of China [2023ZX01A11]. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Ebtesam Almazrouei et al. \u201cFalcon-40B: an open large language model with state-of-the-art performance\u201d. In: (2023). [2] Baichuan. \u201cBaichuan 2: Open Large-scale Language Models\u201d. In: arXiv preprint arXiv:2309.10305 (2023). URL: https://arxiv.org/abs/2309.10305.   \n[3] Stephen Casper et al. \u201cExplore, establish, exploit: Red teaming language models from scratch\u201d. In: arXiv preprint arXiv:2306.09442 (2023).   \n[4] Patrick Chao et al. \u201cJailbreaking black box large language models in twenty queries\u201d. In: arXiv preprint arXiv:2310.08419 (2023).   \n[5] Boyi Deng et al. \u201cAttack prompt generation for red teaming and defending large language models\u201d. In: arXiv preprint arXiv:2310.12505 (2023).   \n[6] Gelei Deng et al. \u201cJailbreaker: Automated jailbreak across multiple large language model chatbots\u201d. In: arXiv preprint arXiv:2307.08715 (2023).   \n[7] Jacob Devlin et al. \u201cBert: Pre-training of deep bidirectional transformers for language understanding\u201d. In: arXiv preprint arXiv:1810.04805 (2018).   \n[8] Qingxiu Dong et al. \u201cA survey on in-context learning\u201d. In: arXiv preprint arXiv:2301.00234 (2022).   \n[9] Zhichen Dong et al. \u201cAttacks, defenses and evaluations for llm conversation safety: A survey\u201d. In: arXiv preprint arXiv:2402.09283 (2024).   \n[10] Yanrui Du et al. \u201cAnalyzing the inherent response tendency of llms: Real-world instructionsdriven jailbreak\u201d. In: arXiv preprint arXiv:2312.04127 (2023).   \n[11] Deep Ganguli et al. \u201cRed teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned\u201d. In: arXiv preprint arXiv:2209.07858 (2022).   \n[12] Xingang Guo et al. \u201cCold-attack: Jailbreaking llms with stealthiness and controllability\u201d. In: arXiv preprint arXiv:2402.08679 (2024).   \n[13] Thomas Hartvigsen et al. \u201cToxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection\u201d. In: arXiv preprint arXiv:2203.09509 (2022).   \n[14] Alec Helbling et al. \u201cLlm self defense: By self examination, llms know they are being tricked\u201d. In: arXiv preprint arXiv:2308.07308 (2023).   \n[15] Edward J Hu et al. \u201cLora: Low-rank adaptation of large language models\u201d. In: arXiv preprint arXiv:2106.09685 (2021).   \n[16] Neel Jain et al. \u201cBaseline defenses for adversarial attacks against aligned language models\u201d. In: arXiv preprint arXiv:2309.00614 (2023).   \n[17] Fred Jelinek et al. \u201cPerplexity\u2014a measure of the difficulty of speech recognition tasks\u201d. In: The Journal of the Acoustical Society of America 62.S1 (1977), S63\u2013S63.   \n[18] Albert Q Jiang et al. \u201cMixtral of experts\u201d. In: arXiv preprint arXiv:2401.04088 (2024).   \n[19] Erik Jones et al. \u201cAutomatically auditing large language models via discrete optimization\u201d. In: International Conference on Machine Learning. PMLR. 2023, pp. 15307\u201315329.   \n[20] Daniel Kang et al. \u201cExploiting programmatic behavior of llms: Dual-use through standard security attacks\u201d. In: arXiv preprint arXiv:2302.05733 (2023).   \n[21] Aounon Kumar et al. \u201cCertifying llm safety against adversarial prompting\u201d. In: arXiv preprint arXiv:2309.02705 (2023).   \n[22] Bill Yuchen Lin et al. \u201cThe Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning\u201d. In: ArXiv preprint (2023).   \n[23] Alisa Liu et al. \u201cTuning language models by proxy\u201d. In: arXiv preprint arXiv:2401.08565 (2024).   \n[24] Xiaogeng Liu et al. \u201cAutodan: Generating stealthy jailbreak prompts on aligned large language models\u201d. In: arXiv preprint arXiv:2310.04451 (2023).   \n[25] Zhuang Ma and Michael Collins. \u201cNoise contrastive estimation and negative sampling for conditional models: Consistency and statistical efficiency\u201d. In: arXiv preprint arXiv:1809.01812 (2018).   \n[26] Todor Markov et al. \u201cA holistic approach to undesired content detection in the real world\u201d. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. 12. 2023, pp. 15009\u2013 15018.   \n[27] Ninareh Mehrabi et al. \u201cFlirt: Feedback loop in-context red teaming\u201d. In: arXiv preprint arXiv:2308.04265 (2023).   \n[28] OpenAI. GPT-4 Technical Report. 2023. arXiv: 2303.08774 [cs.CL].   \n[29] Long Ouyang et al. \u201cTraining language models to follow instructions with human feedback\u201d. In: Advances in neural information processing systems 35 (2022), pp. 27730\u201327744.   \n[30] Ethan Perez et al. \u201cRed teaming language models with language models\u201d. In: arXiv preprint arXiv:2202.03286 (2022).   \n[31] Xiangyu Qi et al. \u201cFine-tuning aligned language models compromises safety, even when users do not intend to!\u201d In: arXiv preprint arXiv:2310.03693 (2023).   \n[32] Alexandre Ram\u00e9 et al. \u201cWarm: On the beneftis of weight averaged reward models\u201d. In: arXiv preprint arXiv:2401.12187 (2024).   \n[33] Xinyue Shen et al. \u201c\" do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models\u201d. In: arXiv preprint arXiv:2308.03825 (2023).   \n[34] Hugo Touvron et al. \u201cLlama: Open and efficient foundation language models\u201d. In: arXiv preprint arXiv:2302.13971 (2023).   \n[35] Zhenhua Wang et al. \u201cFoot In The Door: Understanding Large Language Model Jailbreaking via Cognitive Psychology\u201d. In: arXiv preprint arXiv:2402.15690 (2024).   \n[36] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. \u201cJailbroken: How does llm safety training fail?\u201d In: Advances in Neural Information Processing Systems 36 (2024).   \n[37] Zeming Wei, Yifei Wang, and Yisen Wang. \u201cJailbreak and guard aligned language models with only few in-context demonstrations\u201d. In: arXiv preprint arXiv:2310.06387 (2023).   \n[38] Fangzhao Wu et al. \u201cDefending chatgpt against jailbreak attack via self-reminder\u201d. In: (2023).   \n[39] Zhangchen Xu et al. \u201cSafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding\u201d. In: arXiv preprint arXiv:2402.08983 (2024).   \n[40] Zihao Xu et al. \u201cLLM Jailbreak Attack versus Defense Techniques\u2013A Comprehensive Study\u201d. In: arXiv preprint arXiv:2402.13457 (2024).   \n[41] Jiahao Yu, Xingwei Lin, and Xinyu Xing. \u201cGptfuzzer: Red teaming large language models with auto-generated jailbreak prompts\u201d. In: arXiv preprint arXiv:2309.10253 (2023).   \n[42] Xuandong Zhao et al. \u201cWeak-to-strong jailbreaking on large language models\u201d. In: arXiv preprint arXiv:2401.17256 (2024).   \n[43] Lianmin Zheng et al. \u201cJudging LLM-as-a-judge with MT-Bench and Chatbot Arena\u201d. In: arXiv preprint arXiv:2306.05685 (2023).   \n[44] Chunting Zhou et al. \u201cLima: Less is more for alignment\u201d. In: Advances in Neural Information Processing Systems 36 (2024).   \n[45] Andy Zou et al. \u201cUniversal and transferable adversarial attacks on aligned language models\u201d. In: arXiv preprint arXiv:2307.15043 (2023). ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Collection of Rejection Expressions ", "text_level": 1, "page_idx": 11}, {"type": "table", "img_path": "SrFbgIjb53/tmp/40d57bcb693994978d0bcf76f9b5a497828837a1e66584195211fb4ecdce4612.jpg", "table_caption": ["Table 5: Rejection expressions. "], "table_footnote": [], "page_idx": 11}, {"type": "text", "text": "In Sec. 3.1, we have collected rejection expressions for rule-based detection. Tab. 5 shows our collected rejection expressions. ", "page_idx": 11}, {"type": "text", "text": "B In-Context Demonstrations for GPT-4 ", "text_level": 1, "page_idx": 12}, {"type": "table", "img_path": "SrFbgIjb53/tmp/6580b85d9d90f296f8c82fc25b1403f839b7f10f47eadfb2cfbbba1a2d96c05d.jpg", "table_caption": ["Table 6: Demonstrations for GPT-4 during the construction of $(\\mathbf{X}_{b},\\,\\mathbf{Y}_{g})$ , $(\\mathrm{X}_{b},\\mathrm{Y}_{r})$ and $\\left({{\\Chi}_{m}},{\\mathrm{Y}_{r}}\\right)$ . "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "In Sec. 3.1, we provide in-context demonstrations for GPT-4 during the construction of $(X_{b},\\,\\Upsilon_{g})$ , $({\\mathbf{X}}_{b}$ , ${\\mathrm{Y}}_{r_{\\prime}}$ ) and $\\left({{\\mathrm{X}}_{m}},{{\\mathrm{Y}}_{r}}\\right)$ . The provided demonstrations are detailed in Tab. 6. ", "page_idx": 12}, {"type": "text", "text": "C Examples for Evaluation Data ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In Tab. 7, we present examples of evaluation data. Due to the extensive length of the adversarial sample generated by AutoDAN, we do not include a specific example in Tab. 7. For an illustrative instance of AutoDAN, please refer to the dataset available 6. ", "page_idx": 12}, {"type": "text", "text": "D Identified Harmless Targets ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In Sec. 4.1, we identified harmless targets, which are used to compute the Attack Success Rate (ASR).   \nOur identified targets are listed in Tab. 8, similar to the work [45]. ", "page_idx": 12}, {"type": "text", "text": "E Decoding Configuration ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In Tab. 9 and 10, we respectively provide our used templates and decoding parameters for various LLMs during the decoding stage. ", "page_idx": 12}, {"type": "text", "text": "F Case Study ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Our case study further underscores the superiority of our MoGU. Tab. 2 demonstrates that while the ICD shows superior defense performance against jailbreak attacks for Viucna $_{7B}$ , it also significantly compromises the quality of responses to benign instructions, as seen in Table 3. This issue is highlighted in the case described in Tab. 11, where ICD not only rejected a malicious instruction but also erroneously rejected a benign instruction. In contrast, MoGU exhibits a robust ability to distinguish between malicious and benign instruction \u2014 rejecting the former while helpfully responding to the latter. ", "page_idx": 12}, {"type": "table", "img_path": "SrFbgIjb53/tmp/fbabcf9e6ee847370f892fa9cf19c148bf31316c38674a5fb58922578d813eff.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "table", "img_path": "SrFbgIjb53/tmp/c3d24bdfc4de72fa3fa7dc3544825fe21e5e4c431b6d6c3a5a099bb8dbda85ce.jpg", "table_caption": ["Table 8: Identified harmless targets used for calculating ASR. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "G Extend our MoGU to Baichuan2 and Dolphin ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To demonstrate the flexibility of our framework, we applied it to $\\mathrm{Dolphin}_{7B}$ and Baichuan2 $^{7B}$ . Notably, Dolphin $.7B$ has not undergone a safety review, whereas Baichuan $2_{7B}$ differs significantly in architecture from the Llama series of LLMs. Our evaluation focuses on the defense performance of these LLMs under red-team evaluations and specific jailbreak attacks, including SAP30 and ${\\mathrm{Comp}}_{o b j}$ . The results, detailed in Tab. 12, confirm that our framework substantially enhances the safety of both $\\mathrm{Dolphin}_{7B}$ and Baichuan2 $\\cdot7B$ . ", "page_idx": 13}, {"type": "text", "text": "Table 9: Templates for various LLMs during the decoding stage. ", "page_idx": 14}, {"type": "table", "img_path": "SrFbgIjb53/tmp/8b24586683bbe1ec57435b2c1f7a741d5e3b27280c3a603201f821faf5820548.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "SrFbgIjb53/tmp/d8c940626e410673fa65d22fdc904e2f87e02754b257824ae33d1bf44d5d213e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "H Distribution of Weights Assigned by Router ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "On Llama $2_{7B}$ , Vicuna $_{7B}$ , and $\\operatorname{Falcon}_{7B}$ , we calculated the mean values of weights $w_{u n w i l l}$ and $w_{g l a d}$ during the procession of each instruction. The statistical results for Vicuna $_{7B}$ have been discussed in Sec. 5.2. Fig. 5 presents the boxplots for $\\mathrm{Llama}2_{7B}$ and $\\operatorname{Falcon}_{7B}$ , which show similar trends to those reported in Sec. 5.2. Specifically, for malicious instructions, the router will assign a higher weight $w_{u n w i l l}$ to $\\mathrm{Unwill}_{r e s p}$ , while for benign instructions, it favors a higher weight $w_{g l a d}$ for ${\\mathrm{Glad}}_{r e s p}$ . ", "page_idx": 14}, {"type": "text", "text": "I Limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Despite the advantages shown by our proposed MoGU compared to other defense strategies, we still acknowledge several limitations in our research: ", "page_idx": 14}, {"type": "text", "text": "\u2022 Can our framework be adapted to other linear layers? Since there is no related work exploring which linear layers within LLMs significantly impact LLMs\u2019 safety, we selected $0_{p r o j}$ as our target. However, it remains unclear whether applying our framework to other linear layers would achieve the same performance. ", "page_idx": 14}, {"type": "text", "text": "Table 11: After applying SafeDecoding and MoGU, real response cases of Vicuna $^{7B}$ when faced with the malicious instruction and benign instruction. Our MoGU strategy provides a harmless response when faced with malicious instruction and a useful response when faced with benign instruction. ", "page_idx": 15}, {"type": "table", "img_path": "SrFbgIjb53/tmp/293031336b1d7678ae4153487315974fe772f67a4e0f7e8f574720261f1f65cf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "SrFbgIjb53/tmp/1e2937c9ced25050a087fe572c000c883a6459bf1c9937ae270022ff34ace9dd.jpg", "img_caption": ["Figure 5: The distribution of weights assigned by the router of Llama $2_{7B}$ and Falcon $^{7B}$ . "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "\u2022 Can the introduced parameters be further reduced? As discussed in Sec. 5.3, our framework introduces additional parameters. However, it is not clear whether all introduced parameters are effective. Whether we can reduce the number of introduced parameters through pruning is something our research has not yet further explored. ", "page_idx": 15}, {"type": "text", "text": "J Broader Impact ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "J.1 Positive Social Impact ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "\u2022 Enhanced User Trust: By improving the safety of LLMs, users will have greater trust in the outputs generated by these LLMs. Whether it is a smart assistant, an autonomous driving system, or other AI-based decision-making tools, users will feel more confident using them. ", "page_idx": 15}, {"type": "text", "text": "\u2022 Reduction of Potential Risks: Improving the safety of LLMs helps mitigate potential risks that may arise from AI models, such as erroneous decisions, misleading information, and so on. This will have a positive impact on public safety, healthcare, finance, and other sectors ", "page_idx": 15}, {"type": "table", "img_path": "SrFbgIjb53/tmp/cbb9b14000ab4063a2ac99ac8e755528b90f3af87eff21322621a28c52cc88d1.jpg", "table_caption": ["Table 12: Results of defense performance of $\\mathrm{Dolphin}_{7B}$ and Baichuan $2_{7B}$ with our MoGU framework. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "J.2 Negative Social Impact ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "\u2022 Safety Risks Still Exist: Despite improvements in LLMs\u2019 safety, eliminating all security risks is impossible. This may lead some users to remain vigilant and distrustful when using AI models. Besides, hackers may utilize these LLMs for cyberattacks or spreading misinformation. \u2022 Technology Dependence and Job Loss: With the widespread application of AI technology, people may become overly dependent on these technologies, leading to the disappearance of certain job roles. While this is a natural consequence of technological progress, it may also have a negative impact on the social employment structure. ", "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Abstract, 1 Introduction( 1) ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: J Limitations( I) ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: 3 Framework( 3), 5 Analysis( 5) ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: 3 Framework( 3), 4 Main Experiments( 4), A Collection of Rejection Expressions( A), B In-Context Demonstrations for GPT-4( B), C Examples for Evaluation Data( C), D Identified Harmless Targets( D), E Decoding Configuration( E) ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: 4 Main Experiments ( 4). Besides, we provide the code and the training data for our framework in supplemental material. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: 4 Main Experiments( 4) ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: 5. Analysis( 5) ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: 4 Main Experiments( 4) Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: K Broader Impact( J) Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: [TODO] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: 3 Framework( 3) 4 Main Experiments( 4) ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 21}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: [TODO] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] Justification: [TODO] ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}]