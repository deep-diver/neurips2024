[{"type": "text", "text": "Universal Online Convex Optimization with 1 Projection per Round ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Wenhao Yang1,2, Yibo Wang1,2, Peng Zhao1,2, Lijun Zhang1,2,\u2217 ", "page_idx": 0}, {"type": "text", "text": "1National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China 2School of Artificial Intelligence, Nanjing University, Nanjing, China {yangwh, wangyb, zhaop, zhanglj}@lamda.nju.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "To address the uncertainty in function types, recent progress in online convex optimization (OCO) has spurred the development of universal algorithms that simultaneously attain minimax rates for multiple types of convex functions. However, for a $T$ -round online problem, state-of-the-art methods typically conduct ${\\cal O}(\\log T)$ projections onto the domain in each round, a process potentially timeconsuming with complicated feasible sets. In this paper, inspired by the black-box reduction of Cutkosky and Orabona [2018], we employ a surrogate loss defined over simpler domains to develop universal OCO algorithms that only require 1 projection. Embracing the framework of prediction with expert advice, we maintain a set of experts for each type of functions and aggregate their predictions via a meta-algorithm. The crux of our approach lies in a uniquely designed expert-loss for strongly convex functions, stemming from an innovative decomposition of the regret into the meta-regret and the expert-regret. Our analysis sheds new light on the surrogate loss, facilitating a rigorous examination of the discrepancy between the regret of the original loss and that of the surrogate loss, and carefully controlling meta-regret under the strong convexity condition. With only 1 projection per round, we establish optimal regret bounds for general convex, exponentially concave, and strongly convex functions simultaneously. Furthermore, we enhance the expert-loss to exploit the smoothness property, and demonstrate that our algorithm can attain small-loss regret for multiple types of convex and smooth functions. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Online convex optimization (OCO) stands as a pivotal online learning framework for modeling many real-world problems [Hazan, 2016]. OCO is commonly formulated as a repeated game between the learner and the environment with the following protocol. In each round $t\\in[T]$ , the learner chooses a decision $\\mathbf{x}_{t}$ from a convex domain $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ ; after submitting this decision, the learner suffers a loss $f_{t}(\\mathbf{x}_{t})$ , where $f_{t}\\colon\\mathcal{X}\\mapsto\\mathbb{R}$ is a convex function selected by the environment. The goal of the learner is to minimize the cumulative loss over $T$ rounds, i.e., $\\textstyle\\sum_{t=1}^{T}f_{t}(\\mathbf{x}_{t})$ , and the standard performance measure is the regret [Cesa-Bianchi and Lugosi, 2006]: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname{REG}_{T}=\\sum_{t=1}^{T}f_{t}(\\mathbf x_{t})-\\operatorname*{min}_{\\mathbf x\\in\\mathcal X}\\sum_{t=1}^{T}f_{t}(\\mathbf x),\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "which quantifies the difference between the cumulative loss of the online learner and that of the best decision chosen in hindsight. ", "page_idx": 0}, {"type": "table", "img_path": "xNncVKbwwS/tmp/16ef389ca21f18ea376cdebd165854e032b97ac81649d00fa0db97071947c13b.jpg", "table_caption": ["Table 1: A summary of our universal algorithms and previous studies over $T$ rounds $d$ -dimensional functions, where $L_{T}$ denotes the small-loss quantity. Abbreviations: cvx $\\rightarrow$ convex, exp-concave $\\rightarrow$ exponentially concave, str-cvx $\\rightarrow$ strongly convex, # $\\mathrm{PROJ\\rightarrow}$ number of projections per round. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "Although there are plenty of algorithms to minimize the regret of convex functions, including general convex, exponentially concave (abbr. exp-concave) and strongly convex functions [Zinkevich, 2003, Shalev-Shwartz et al., 2007, Hazan et al., 2007], most of them can only handle one specific function type, and need to estimate the moduli of strong convexity and exp-concavity. The demand for prior knowledge motivates the development of universal algorithms for OCO, which aim to attain minimax optimal regret guarantees for multiple types of convex functions simultaneously [Bartlett et al., 2008, van Erven and Koolen, 2016, Wang et al., 2019, Mhammedi et al., 2019, Zhang et al., 2022]. State-of-the-art methods typically adopt a two-layer structure following the prediction with expert advice (PEA) framework [Cesa-Bianchi and Lugosi, 2006]. Specifically, they maintain ${\\cal O}(\\log T)$ expert-algorithms with different configurations to handle the uncertainty of functions and deploy a meta-algorithm to track the best one. While this two-layer framework has demonstrated effectiveness in endowing algorithms with universality, it raises concerns regarding the computational efficiency. Since each expert-algorithm needs to execute one projection onto the feasible domain $\\mathcal{X}$ per round, standard universal algorithms perform ${\\cal O}(\\log T)$ projections in each round, which can be time-consuming in practical scenarios particularly when projecting onto complicated domains. ", "page_idx": 1}, {"type": "text", "text": "In the literature, there indeed exists an effort to reduce the number of projections required by universal algorithms tailored for exp-concave functions [Mhammedi et al., 2019]. This is achieved by applying the black-box reduction of Cutkosky and Orabona [2018], which reduces an OCO problem on the original (but can be complicated) feasible domain to a more manageable one on a simpler domain, such as an Euclidean ball. Deploying an existing universal algorithm [van Erven and Koolen, 2016] on the reduced problem enables us to attain optimal regret for exp-concave functions, crucially, with only one single projection per round and no prior knowledge of exp-concavity required. However, this black-box approach cannot be extended to strongly convex functions (see Section 3.1 for technical discussions). Therefore, it is still unclear on how to reduce the number of projections of universal algorithms to 1, and at the same time ensure optimal regret for strongly convex functions (as well as general convex and exp-concave functions). ", "page_idx": 1}, {"type": "text", "text": "In this paper, we affirmatively solve the above question by introducing an efficient universal OCO algorithm. Our solution employs the black-box reduction Cutkosky [2020] to cast the original problem on the constrained domain $\\mathcal{X}$ to an alternative one in terms of the surrogate loss on a simpler domain $y\\supseteq x$ . Specifically, we construct multiple experts updated in domain $\\boldsymbol{\\wp}$ , each optimizing a expert-loss specialized for a distinct function type. Then, we combine their predictions by a metaalgorithm, and perform the only projection onto the feasible domain $\\mathcal{X}$ . The meta-algorithm chooses the linearized surrogate loss to measure the performance of experts, and is required to yield a secondorder regret [Zhang et al., 2022]. The key novelty of our algorithm lies in the uniquely designed expert-loss for strongly convex functions, which is motivated by an innovative decomposition of the regret into the meta-regret and the expert-regret. To effectively deal with strongly convex functions, we explore the domain-converting surrogate loss in depth and illuminate its refined properties. Our new insights tighten the regret gap in terms of original loss and surrogate loss, and further exploit strong convexity to compensate the meta-regret, thus achieving the optimal regret for strongly convex functions. Section 3.2 provides a formal description of our key ideas. With only 1 projection per round, our algorithm attains $O({\\sqrt{T}}),\\,O({\\frac{d}{\\alpha}}\\log T)$ , and $\\textstyle O({\\frac{1}{\\lambda}}\\log T)$ regret for general convex, $\\alpha$ -exp-concave, and $\\lambda$ -strongly convex functions, respectively. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "We further establish small-loss regret for universal OCO with smooth functions. The small-loss quantity $\\begin{array}{r}{L_{T}=\\operatorname*{min}_{\\mathbf{x}\\in\\mathcal{X}}\\sum_{t=1}^{T}f_{t}(\\bar{\\mathbf{x}})}\\end{array}$ is defined as the cumulative loss of the best decision chosen from the domain $\\mathcal{X}$ , which is at most ${\\cal O}(T)$ under standard OCO assumptions and meanwhile can be much smaller in benign environments. To achieve small-loss regret bounds, we design an enhanced expert-loss for smooth and strongly convex functions and inte\u221agrate it into our two-layer framework, which finally leads to a universal OCO algorithm achieving $\\begin{array}{r}{O(\\bar{\\sqrt{L_{T}}}),O(\\frac{d}{\\alpha}\\log L_{T})}\\end{array}$ , and $O(\\frac{1}{\\lambda}\\log{L_{T}})$ small-loss regret for three types of convex functions, respectively. Notably, all those bounds are optimal and the algorithm only requires one projection per iteration. We summarize our results and compare with previous studies of universal algorithms in Table 1. ", "page_idx": 2}, {"type": "text", "text": "Organization. The rest of the paper is organized as follows. Section 2 presents the preliminaries and reviews several mostly related works. Section 3 illuminates the technical challenges and describes our key ideas. Section 4 provides the overall algorithms and regret analysis. We finally conclude the paper in Section 5. All the proofs and omitted details are deferred to appendices. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries and related works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we first present preliminaries for OCO, and then review several most related works to our paper, including universal algorithms and projection-efficient algorithms. ", "page_idx": 2}, {"type": "text", "text": "2.1 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We introduce two typical assumptions of online convex optimization [Hazan, 2016]. ", "page_idx": 2}, {"type": "text", "text": "Assumption 1 (bounded domain) The feasible domain $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ contains the origin 0, and the diameter is bounded by $D$ , i.e., $\\|\\mathbf x-\\mathbf y\\|\\le D$ holds for any $\\mathbf{x},\\mathbf{y}\\in{\\mathcal{X}}$ . ", "page_idx": 2}, {"type": "text", "text": "Assumption 2 (bounded gradient norms) The norm of the gradients of all online functions over the domain $\\mathcal{X}$ is bounded by $G$ , i.e., $\\|\\nabla f_{t}(\\mathbf{x})\\|\\leq G$ holds for all $\\mathbf{x}\\in\\mathcal{X}$ and $t\\in[T]$ . ", "page_idx": 2}, {"type": "text", "text": "Throughout the paper we use $\\Vert\\cdot\\Vert$ for $\\ell_{2}$ -norm in default. Owing to Assumption 1, we can always construct an Euclidean ball $\\mathcal{Y}=\\left\\{\\mathbf{x}\\mid\\|\\mathbf{x}\\|\\leq D\\right\\}$ containing the original feasible domain $\\mathcal{X}$ . ", "page_idx": 2}, {"type": "text", "text": "Next, we state definitions of strong convexity and exp-concavity [Hazan, 2016], and introduce an important property of exp-concave functions [Hazan et al., 2007, Lemma 3]. ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (strongly convex functions) A function $f:\\mathcal{X}\\mapsto\\mathbb{R}$ is called $\\lambda$ -strongly convex, if the condition $\\begin{array}{r}{f(\\mathbf{y})\\geq f(\\bar{\\mathbf{x}})+\\langle\\nabla f(\\mathbf{x}),\\mathbf{y}-\\mathbf{x}\\rangle+\\frac{\\lambda}{2}\\|\\mathbf{y}-\\dot{\\mathbf{x}}\\|^{2}}\\end{array}$ holds for all $\\mathbf{x},\\mathbf{y}\\in{\\mathcal{X}}$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 2 (exponentially-concave functions) $A$ function $f:\\mathcal{X}\\mapsto\\mathbb{R}$ is called $\\alpha$ -exponentiallyconcave, if the function $\\exp(-\\alpha f(\\cdot))$ is concave over the feasible domain $\\mathcal{X}$ . ", "page_idx": 2}, {"type": "text", "text": "Lemma 1 For an $\\alpha$ -exp-concave function $f:\\mathcal{X}\\mapsto\\mathbb{R},$ if the feasible domain $\\mathcal{X}$ has a diameter $D$ and $\\|\\nabla f(\\mathbf{x})\\|\\leq G$ holds for $\\forall\\mathbf{x}\\in\\mathcal{X}$ , then we have ", "page_idx": 2}, {"type": "equation", "text": "$$\nf(\\mathbf{y})\\geq f(\\mathbf{x})+\\langle\\nabla f(\\mathbf{x}),\\mathbf{y}-\\mathbf{x}\\rangle+{\\frac{\\beta}{2}}\\langle\\nabla f(\\mathbf{x}),\\mathbf{y}-\\mathbf{x}\\rangle^{2},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "for all $\\mathbf{x},\\mathbf{y}\\in\\mathcal{X}$ , where $\\begin{array}{r}{\\beta=\\frac{1}{2}\\operatorname*{min}\\lbrace\\frac{1}{4G D},\\alpha\\rbrace}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "There are many efforts devoted to minimizing regret, including general convex, $\\alpha$ -exp-concave, and $\\lambda$ -strongly convex fu\u221anctions. For gene\u221aral convex functions, online gradient descent (OGD) with step size $\\eta_{t}=O(1/\\sqrt{t})$ , attains an $O({\\sqrt{T}})$ regret [Zinkevich, 2003]. For $\\alpha$ -exp-concave functions, online Newton step (ONS) is equipped with an $O(\\frac{d}{\\alpha}\\log T)$ regret [Hazan et al., 2007]. For $\\lambda$ -strongly convex functions, OGD with step size $\\eta_{t}\\,=\\,O(1/[\\lambda t])$ , achieves an $O(\\textstyle{\\frac{1}{\\lambda}}\\log T)$ regret [ShalevShwartz et al., 2007]. These regret bounds are proved to be minimax optimal [Ordentlich and Cover, 1998, Abernethy et al., 2008]. Furthermore, tighter bounds are attainable when the loss functions enjoy additional properties, such as smoothness [Shalev-Shwartz, 2007, Luo and Schapire, 2015, Srebro et al., 2010, Orabona et al., 2012, Chiang et al., 2012, Yang et al., 2014, Mohri and Yang, 2016, Zhang et al., 2019, Zhao et al., 2020, 2024, Chen et al., 2024] and sparsity of gradients [Duchi et al., 2010, Tieleman and Hinton, 2012, Mukkamala and Hein, 2017, Kingma and Ba, 2015, Reddi et al., 2018, Loshchilov and Hutter, 2019, Wang et al., 2020a]. We discuss small-loss regret below. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "For general convex and smooth functions, Srebro et al. [2010] prove that OGD with constant step size attains an $O(\\sqrt{L})$ regret bound, where $L$ is the upper bound of $L_{T}$ . The limitation of their method is that it requires to know $L$ beforehand. To address this limitation, Zhang et al. [2019] propose scale-free online gradient descent (SOGD), which is a \u221aspecial case of scale-free mirror descent algorithm [Orabona and P\u00e1l, 2018], and establish an $O(\\sqrt{L_{T}})$ small-loss regret bound without the prior knowledge of $L_{T}$ . For $\\alpha$ -exp-concave and smooth functions, ONS attains an $\\scriptstyle O\\left({\\frac{d}{\\alpha}}\\log L_{T}\\right)$ small-loss regret bound [Orabona et al., 2012]. For $\\lambda$ -strongly convex and smooth functions, a variant of OGD, namely $\\mathrm{S^{2}O G D}$ , is introduced to achieve an $O(\\frac{1}{\\lambda}\\log L_{T})$ small-loss regret bound [Wang et al., 2020b]. Such bounds reduce to the minimax optimal bounds in the worst case, but could be much tighter when the comparator has a small loss, i.e., $L_{T}$ is small. ", "page_idx": 3}, {"type": "text", "text": "2.2 Universal algorithms ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Most existing online algorithms can only handle one type of convex function and need to know the moduli of strong convexity and exp-concavity beforehand. Universal online learning aims to remove such requirements of domain knowledge. The first universal OC\u221aO algorithm is adaptive online gradient descent (AOGD) [Bartlett et al., 2008], which achieves $O({\\sqrt{T}})$ and ${\\cal O}(\\log T)$ regret bounds for general convex and strongly convex functions, respectively. However, the algorithm still needs to know the modulus of strong convexity and does not support exp-concave functions. ", "page_idx": 3}, {"type": "text", "text": "An important milestone is the multiple eta gradient (MetaGrad) algorithm [van Erven and Koolen, 2016], which adapt to general convex and exp-concave functions without knowing the modulus of expconcavity. MetaGrad constructs multiple expert-algorithms with various learning rates and combines their predictions by a meta-algorithm called Tilted Exponentially Weighted Average (TEWA). To avoid prior knowledge, each expert minimizes the expert-loss parameterized by a learning rate $\\eta$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\ell_{t,\\eta}^{\\mathrm{exp}}(\\mathbf{x})=-\\eta\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}\\rangle+\\eta^{2}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}\\rangle^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "MetaGrad maintains ${\\cal O}(\\log T)$ experts to minimize (3), and attains $O({\\sqrt{T\\log\\log T}})$ and $O(\\frac{d}{\\alpha}\\log T)$ regret for general convex and $\\alpha$ -exp-concave functions, respectively. To further support strongly convex functions, Wang et al. [2019] propose a new type of expert-losses defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\ell_{t,\\eta}^{\\mathrm{sc}}(\\mathbf{x})=-\\eta\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}\\rangle+\\eta^{2}G^{2}\\Vert\\mathbf{x}_{t}-\\mathbf{x}\\Vert^{2}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $G$ is the gradient norm upper bound, and introduce an expert-loss for general convex functions ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\ell_{t,\\eta}^{\\mathrm{{cvx}}}(\\mathbf{x})=-\\eta\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}\\rangle+\\eta^{2}G^{2}D^{2}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "whe\u221are $D$ is the upper bound of the diameter of $\\mathcal{X}$ . Their algorithm, named as Maler, obtains $O({\\sqrt{T}})$ , $\\textstyle O({\\frac{1}{\\lambda}}\\log T)$ and $\\textstyle O({\\frac{d}{\\alpha}}\\log T)$ regret for general convex, $\\lambda$ -strongly convex functions, and $\\alpha$ -exp-concave functions, respectively. Later, Wang et al. [2020b] extend Maler by replacing $G^{2}$ in (4) and (5) with $\\|\\nabla f_{t}({\\mathbf x}_{t})\\|^{2}$ , thereby enabling their algorith\u221am to deliver small-loss regret bounds. Under the smoothness condition, their algorithm achieves $O(\\sqrt{L_{T}})$ , $\\begin{array}{r}{O(\\frac{1}{\\lambda}\\log L_{T})}\\end{array}$ and $\\textstyle O({\\frac{d}{\\alpha}}\\log L_{T})$ regret for general convex, $\\lambda$ -strongly convex, and $\\alpha$ -exp-concave functions, respectively. ", "page_idx": 3}, {"type": "text", "text": "MetaGrad and its variants require the carefully designed expert-losses. Zhang et al. [2022] propose a different universal strategy that avoids the construction of losses. The basic idea is to let each expert handle original functions and deploy a meta-algorithm over linearized loss. Importantly, the meta-algorithm is required to yield a second-order regret [Gaillard et al., 2014] to exploit strong convexity and exp-concavity. By incorporating existing online algorithms as experts, their approach inherits the regret of any expert designed for strongly convex functions and exp-concave functions, and also obtains minimax optimal regret (and small-loss regret) for general convex functions. ", "page_idx": 3}, {"type": "text", "text": "Although state-of-the-art universal algorithms can adapt to multiple function types, they create $O(\\log{\\bar{T}})$ experts per round. As a result, they need to perform ${\\cal O}(\\log T)$ projections in each round, which can be time-consuming in practical scenarios with complicated domains. To address this limitation, we aim to develop projection-efficient algorithms for universal OCO. ", "page_idx": 3}, {"type": "text", "text": "2.3 Projection-efficient algorithms ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In the studies of parameter-free online learning, Cutkosky and Orabona [2018] propose a black-box reduction technique from constrained online learning to unconstrained online learning. To avoid regret degeneration, they design the domain-converting surrogate loss $\\widehat{g}_{t}:\\mathcal{Y}\\mapsto\\mathbb{R}$ defined as, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{g}_{t}(\\mathbf{y})=\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{y}\\rangle+\\|\\nabla f_{t}(\\mathbf{x}_{t})\\|\\cdot S_{\\mathcal{X}}(\\mathbf{y})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $S_{\\mathcal{X}}(\\mathbf{y})\\,=\\,\\|\\mathbf{y}-\\Pi_{\\mathcal{X}}[\\mathbf{y}]\\|$ is the distance function to the feasible domain $\\mathcal{X}$ . Then, we can employ an unconstrained online learning algorithm that minimizes (6) to obtain the prediction $\\mathbf{y}_{t}$ , and output its prediction on domain $\\mathcal{X}$ , i.e., $\\mathbf x_{t}=\\Pi_{\\mathcal{X}}[\\mathbf y_{t}]$ . Cutkosky and Orabona [2018, Theorem 3] have proved that the above surrogate loss satisfies $\\|\\bar{\\nabla}\\widehat{g}_{t}\\bar{(\\mathbf{y}}_{t})\\|\\leq\\|\\nabla f_{t}(\\mathbf{x}_{t})\\|$ , and ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}\\rangle\\le2\\big(\\widehat{g}_{t}(\\mathbf{y}_{t})-\\widehat{g}_{t}(\\mathbf{x})\\big)\\le2\\langle\\nabla\\widehat{g}_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{x}\\rangle\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for all $t\\,\\in\\,[T]$ and any $\\mathbf{x}\\in\\mathcal{X}$ . Based on this fact, we know that the regret of the unconstrained problem directly serves as an upper bound for that of the original problem, hence reducing the original problem to an unconstrained surrogate problem and retaining the order of regret. ", "page_idx": 4}, {"type": "text", "text": "Subsequently, Cutkosky [2020] introduces a new surrogate loss $g_{t}:\\mathcal{Y}\\mapsto\\mathbb{R}$ defined as, ", "page_idx": 4}, {"type": "equation", "text": "$$\ng_{t}(\\mathbf{y})=\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{y}\\rangle-\\mathbb{1}_{\\{\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{v}_{t}\\rangle<0\\}}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{v}_{t}\\rangle\\cdot S_{X}(\\mathbf{y})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where vt = \u2225yt\u2212xt\u2225 is the unit vector of the projection direction. As depicted in the following lemma, this surrogate loss avoids the multiplicative constant 2 on the right-hand side of (7). ", "page_idx": 4}, {"type": "text", "text": "Lemma 2 (Theorem 2 of Cutkosky [2020]) The function defined in (8) is convex, and it satisfies $\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|\\leq\\|\\nabla f_{t}(\\mathbf{x}_{t})\\|$ . Furthermore, for all $t$ and all $\\mathbf{x}\\in\\mathcal{X}$ , we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\langle\\nabla f_{t}({\\mathbf{x}}_{t}),{\\mathbf{x}}_{t}-{\\mathbf{x}}\\rangle\\leq g_{t}({\\mathbf{y}}_{t})-g_{t}({\\mathbf{x}})\\leq\\langle\\nabla g_{t}({\\mathbf{y}}_{t}),{\\mathbf{y}}_{t}-{\\mathbf{x}}\\rangle.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "While the black-box reduction is proposed for the constrained-to-unconstrained conversion, it also facilitates the conversion to another constrained problem (i.e., $\\boldsymbol{y}\\neq\\mathbb{R}^{d}.$ ). This enables us to transform OCO problem on a complicated domain into another on simpler domains such that the projection is much easier. Building on this idea, Mhammedi et al. [2019] introduce an efficient implementation of MetaGrad [van Erven and Koolen, 2016], which only conducts 1 projection onto the original domain in each round, and keeps the order of regret bounds. However, as detailed in the following section, the black-box reduction does not adequately extend to strongly convex functions. We also mention that Zhao et al. [2022] recently employ the technique to non-stationary OCO with non-trivial modifications to develop efficient algorithms for minimizing dynamic regret and adaptive regret. However, they focus on the convex functions and do not involve the considerations of exp-concave and strongly convex functions as concerned in our paper. ", "page_idx": 4}, {"type": "text", "text": "3 Technical challenge and our key ideas ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we elaborate on the technical challenges and our key ideas. ", "page_idx": 4}, {"type": "text", "text": "3.1 Technical challenge ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As mentioned, Mhammedi et al. [2019] exploit the black-box reduction scheme of [Cutkosky and Orabona, 2018] to improve the projection efficiency of MetaGrad [van Erven and Koolen, 2016]. We summarize their algorithm in Algorithm 1. In the following, we will demonstrate its effectiveness for exp-concave functions and explain why it fails for strongly convex functions. ", "page_idx": 4}, {"type": "text", "text": "Success in exp-concave functions. By applying the black-box reduction as described in Section 2.3, Mhammedi et al. [2019] utilize MetaGrad to minimize the surrogate loss $\\widehat{g}_{t}(\\cdot)$ in (6) over an Euclidean ball $\\boldsymbol{\\wp}$ . The projection operations inside MetaGrad are over $\\boldsymbol{\\wp}$ and thus negligible. Notice that Algorithm 1 demands only 1 projection onto $\\mathcal{X}$ in Step 4. According to regret bound of MetaGrad, Algorithm 1 enjoys a second-order bound [Mhammedi et al., 2019, Theorem 10], ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\langle\\nabla\\widehat{g}_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{x}\\rangle\\leq O\\left(\\sqrt{d\\log T\\cdot\\sum_{t=1}^{T}\\langle\\nabla\\widehat{g}_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{x}\\rangle^{2}}+d\\log T\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Black-box reduction for projection-efficient MetaGrad [Mhammedi et al., 2019] ", "page_idx": 5}, {"type": "text", "text": "1: Construct a ball domain $\\mathcal{V}=\\left\\{\\mathbf{x}\\ |\\ \\|\\mathbf{x}\\|\\leq D\\right\\}\\supseteq\\mathcal{X}$ ", "page_idx": 5}, {"type": "text", "text": "2: for $t=1$ to $T$ do ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "3: Receive the decision $\\mathbf{y}_{t}\\in\\mathcal{V}$ from MetaGrad   \n4: Submit the decision $\\mathbf x_{t}=\\Pi_{\\mathcal{X}}[\\mathbf y_{t}]$ $\\vartriangleright$ The only step projects onto domain $\\mathcal{X}$ per round.   \n5: Suffer the loss $f_{t}(\\mathbf{x}_{t})$ and observe the gradient $\\nabla f_{t}(\\mathbf{x}_{t})$   \n6: Construct the surrogate loss $\\widehat g_{t}(\\cdot)$ as (6) and send it to MetaGrad ", "page_idx": 5}, {"type": "text", "text": "7: end for ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The above bound is measured by surrogate loss, thus requiring a further analysis that converts it back to that of the original function. Since $\\begin{array}{r}{\\beta=\\frac{1}{2}\\operatorname*{min}\\left\\{\\frac{\\mathbf{\\bar{\\alpha}}_{1}}{4G D},\\check{\\alpha}\\right\\}}\\end{array}$ , the function $x-\\beta x^{2}$ is strictly increasing when $x\\in(-\\infty,2G D]$ . Therefore, the property of surrogate loss $\\widehat{g}_{t}(\\cdot)$ in (7) implies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}\\rangle-\\frac{\\beta}{4}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}\\rangle^{2}\\leq\\langle\\nabla\\widehat{g}_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{x}\\rangle-\\beta\\langle\\nabla\\widehat{g}_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{x}\\rangle^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Combining (10) with (11) and applying the AM-GM inequality, we obtain ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}\\rangle-\\frac{\\beta}{2}\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}\\rangle^{2}\\leq O\\left(\\frac{d}{\\alpha}\\log T\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "thus achieving the optimal regret based on Lemma 1. ", "page_idx": 5}, {"type": "text", "text": "Failure in strongly convex functions. To handle strongly convex functions, a straightforward way is to use a universal algorithm that supports strongly convex functions, such as Maler [Wang et al., 2019], as the black-box subroutine in Algorithm 1. However, for strongly convex functions, the above analysis cannot be applied, and we are unable to derive a tight regret bound. Specifically, according to the theoretical guarantee of Maler [Wang et al., 2019, Theorem 1], we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\langle\\nabla\\widehat{g}_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{x}\\rangle\\leq O\\left(\\sqrt{\\log T\\cdot\\sum_{t=1}^{T}\\|\\mathbf{y}_{t}-\\mathbf{x}\\|^{2}}+\\log T\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "From the standard black-box analysis and the definition of strong convexity, we know ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}f_{t}(\\mathbf x_{t})-\\sum_{t=1}^{T}f_{t}(\\mathbf x)\\overset{(7)}{\\leq}\\sum_{t=1}^{T}{2}\\langle\\nabla\\widehat{g}_{t}(\\mathbf y_{t}),\\mathbf y_{t}-\\mathbf x\\rangle-\\frac{\\lambda}{2}\\sum_{t=1}^{T}\\|\\mathbf x_{t}-\\mathbf x\\|^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Substituting (12) into (13), we encounter an $\\begin{array}{r}{\\widetilde{O}(\\sqrt{\\sum_{t=1}^{T}\\|{\\bf y}_{t}-{\\bf x}\\|^{2}}-\\frac{\\lambda}{2}\\sum_{t=1}^{T}\\|{\\bf x}_{t}-{\\bf x}\\|^{2})}\\end{array}$ term, which is unmanageable since $\\|\\mathbf{y}_{t}-\\mathbf{x}\\|\\geq\\|\\mathbf{x}_{t}-\\mathbf{x}\\|$ . Here, $\\widetilde O(\\cdot)$ further omits the ploy $(\\log T)$ factors. ", "page_idx": 5}, {"type": "text", "text": "3.2 Key ideas ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To address above challenges, we introduce novel ideas in both algorithm design and regret analysis. ", "page_idx": 5}, {"type": "text", "text": "Algorithm design. Our algorithm is still in a two-layer structure. The main contribution lies in a uniquely designed expert-loss for strongly convex functions. For simplicity, we consider that the modulus of strong convexity $\\lambda$ is known for a moment, and define ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\ell_{t}^{\\mathrm{sc}}(\\mathbf{y})=\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}\\rangle+\\frac{\\lambda}{2}\\|\\mathbf{y}-\\mathbf{x}_{t}\\|^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $g_{t}(\\cdot)$ is the surrogate loss defined in (8). Let us compare our designed expert-loss (14) with the one when applying existing universal algorithms in a black-box manner. Suppose Maler [Wang et al., 2019] is used, their expert-loss construction (4) indicates that the algorithm over domain $\\boldsymbol{\\wp}$ essentially optimizes the expert-loss formulated as (up to constant factors). ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{\\ell}_{t}^{\\mathrm{sc}}(\\mathbf{y})=\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}\\rangle+\\frac{\\lambda}{2}\\|\\mathbf{y}-\\mathbf{y}_{t}\\|^{2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "An important caveat is that our expert-loss (14) evaluates the performance of the expert (associated with strongly convex functions) based on the distance between its output y and the actual decision $\\mathbf{x}_{t}\\in\\mathcal{X}$ , as opposed to the unprojected intermediate one $\\mathbf{y}_{t}\\in\\mathcal{V}$ in (15). ", "page_idx": 6}, {"type": "text", "text": "In fact, this design of expert-loss (14) stems from a novel regret decomposition as explained below. First, by strong convexity of $f_{t}$ and the property of the domain-converting surrogate loss, we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=1}^{T}f_{t}(\\mathbf{x}_{t})-\\sum_{t=1}^{T}f_{t}(\\mathbf{x})\\overset{(9)}{\\leq}\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{x}\\rangle-\\frac{\\lambda}{2}\\sum_{t=1}^{T}\\|\\mathbf{x}_{t}-\\mathbf{x}\\|^{2}}\\\\ {=\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{y}_{t}^{i}\\rangle+\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}^{i}-\\mathbf{x}\\rangle-\\frac{\\lambda}{2}\\sum_{t=1}^{T}\\|\\mathbf{x}_{t}-\\mathbf{x}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathbf{y}_{t}^{i}$ denotes the decision of the $i$ -th expert. The first term of the above bound is the meta-regret in terms of linearized surrogate loss. Then, we reformulate the remaining two terms as follows ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}^{i}-\\mathbf{x}\\rangle-\\frac{\\lambda}{2}\\sum_{t=1}^{T}\\|\\mathbf{x}_{t}-\\mathbf{x}\\|^{2}=\\displaystyle\\sum_{t=1}^{T}\\left(\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}^{i}\\rangle+\\frac{\\lambda}{2}\\|\\mathbf{x}_{t}-\\mathbf{y}_{t}^{i}\\|^{2}\\right)}\\\\ &{\\displaystyle-\\sum_{t=1}^{T}\\left(\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{x}\\rangle+\\frac{\\lambda}{2}\\|\\mathbf{x}_{t}-\\mathbf{x}\\|^{2}\\right)-\\frac{\\lambda}{2}\\sum_{t=1}^{T}\\|\\mathbf{x}_{t}-\\mathbf{y}_{t}^{i}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the expert-loss in (14) naturally arises. Combining (16) with (17), we arrive at ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}f_{t}(\\mathbf x_{t})-\\sum_{t=1}^{T}f_{t}(\\mathbf x)\\leq\\underbrace{\\sum_{t=1}^{T}\\Big(\\ell_{t}^{\\mathrm{sc}}(\\mathbf y_{t}^{i})-\\ell_{t}^{\\mathrm{sc}}(\\mathbf x)\\Big)}_{\\ t=1}+\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf y_{t}),\\mathbf y_{t}-\\mathbf y_{t}^{i}\\rangle}_{\\mathrm{\\normalfont~\\it~\\/t}}-\\frac{\\lambda}{2}\\sum_{t=1}^{T}\\|\\mathbf x_{t}-\\mathbf y_{t}^{i}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Theoretical analysis. For the expert-regret, since expert-loss (14) is $\\lambda$ -strongly convex and its gradients are bounded (see Lemma 6), we can use OGD to achieve an optimal $\\textstyle{\\vec{O}}({\\frac{1}{\\lambda}}\\log T)$ regret. Following Zhang et al. [2022], we require the meta-algorithm to yield a second-order regret bound ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{y}_{t}^{i}\\rangle\\leq O\\left(\\sqrt{\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{y}_{t}^{i}\\rangle^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Notably, the upper bound of (19) and the negative term in (18) cannot be canceled due to the dismatch between $\\mathbf{y}_{t}-\\bar{\\mathbf{y}}_{t}^{i}$ and $\\mathbf x_{t}-\\mathbf y_{t}^{i}$ . To resolve this discrepancy, we demonstrate that the surrogate loss defined in (8) enjoys the following two important improved properties. ", "page_idx": 6}, {"type": "text", "text": "Lemma 3 In addition to enjoying all the properties outlined in Lemma 2, the surrogate loss function $g_{t}:\\mathcal{Y}\\mapsto\\mathbb{R}$ defined in (8) satisfies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\langle\\nabla f_{t}({\\bf x}_{t}),{\\bf x}_{t}-{\\bf x}\\rangle\\le\\langle\\nabla g_{t}({\\bf y}_{t}),{\\bf y}_{t}-{\\bf x}\\rangle-\\mathbb{1}_{\\{\\langle\\nabla f_{t}({\\bf x}_{t}),{\\bf v}_{t}\\rangle\\ge0\\}}\\cdot\\langle\\nabla f_{t}({\\bf x}_{t}),{\\bf y}_{t}-{\\bf x}_{t}\\rangle,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for all $t$ and all $\\mathbf{x}\\in\\mathcal{X}$ . Furthermore, we also have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{x}_{t}-\\mathbf{y}_{t}\\rangle=0,}&{w h e n\\left\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{v}_{t}\\right\\rangle<0,}\\\\ {\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{x}_{t}-\\mathbf{y}_{t}\\rangle\\leq0,}&{o t h e r w i s e.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Remark 1 We highlight the improvements of Lemma 3 over Lemma 2. First, we provide a tighter connection between the linearized original function and the surrogate loss in (20). Second, we analyze the difference between the actual decision $\\mathbf{x}_{t}$ and the intermediate decision $\\mathbf{y}_{t}$ , along the direction $\\nabla g_{t}(\\mathbf{y}_{t})$ in (21). As shown later, both of them are crucial for controlling the meta-regret. \u25c1 ", "page_idx": 6}, {"type": "text", "text": "Utilizing (20) in Lemma 3, we refine the decomposition in (18) to establish a tighter bound ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}f_{t}(\\mathbf{x}_{t})-\\sum_{t=1}^{T}f_{t}(\\mathbf{x})\\overset{(16),(17),(20)}{\\leq}\\mathrm{ER}(T)+\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{y}_{t}^{i}\\rangle-\\frac{\\lambda}{2}\\sum_{t=1}^{T}\\|\\mathbf{x}_{t}-\\mathbf{y}_{t}^{i}\\|^{2}-\\Delta_{T}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\operatorname{ER}(T)$ is the expert-regret, and $\\begin{array}{r}{\\Delta_{T}=\\sum_{t=1}^{T}\\mathbb{1}_{\\left\\{\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{v}_{t}\\rangle\\ge0\\right\\}}\\cdot\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{y}_{t}-\\mathbf{x}_{t}\\rangle\\ge0}\\end{array}$ is the negative term introduced in the surrogate loss . Compared to (18), the new upper bound (22) enjoys an additional negative term $-\\Delta_{T}$ , which is essential to achieve a favorable regret bound in the analysis. To utilize the negative quadratic term \u2212\u03bb2 tT=1 \u2225xt\u2212yti\u22252 in (22) for compensating the second-order bound in (19), we need to convert to $\\mathbf{x}_{t}$ , a place where (21) comes into play. From (19) and (21), we prove that for any $\\gamma\\in(0,\\frac{G}{2D}]$ it holds that (see Lemma 8 for details): ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{y}_{t}^{i}\\rangle\\leq O\\left(\\frac{G^{2}}{2\\gamma}\\right)+\\frac{\\gamma}{2G^{2}}\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{x}_{t}-\\mathbf{y}_{t}^{i}\\rangle^{2}+\\Delta_{T}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Substituting (23) into (22), the additional term $\\Delta_{T}$ is automatically canceled out, and we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle\\sum_{t=1}^{T}f_{t}(\\mathbf{x}_{t})-\\sum_{t=1}^{T}f_{t}(\\mathbf{x})\\leq\\mathtt{E R}(T)+O\\left(\\frac{G^{2}}{2\\gamma}\\right)+\\frac{\\gamma}{2G^{2}}\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{x}_{t}-\\mathbf{y}_{t}^{i}\\rangle^{2}-\\frac{\\lambda}{2}\\sum_{t=1}^{T}\\|\\mathbf{x}_{t}-\\mathbf{y}_{t}^{i}\\|^{2}}\\\\ &{}&{\\displaystyle\\leq\\mathtt{E R}(T)+O\\left(\\frac{G^{2}}{2\\gamma}\\right)+\\left(\\frac{\\gamma}{2}-\\frac{\\lambda}{2}\\right)\\sum_{t=1}^{T}\\|\\mathbf{x}_{t}-\\mathbf{y}_{t}^{i}\\|^{2}=O\\left(\\frac{1}{\\lambda}\\log T\\right)\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where the final regret bound is because we set $\\begin{array}{r}{\\gamma=\\operatorname*{min}\\{\\frac{G}{2D},\\lambda\\}}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "Remark 2 Section 2.3 describes two kinds of surrogate loss, as specified in (6) and (8). Indeed, they both are suitable for parameter-free online learning [Cutkosky, 2020] and non-stationary online learning [Zhao et al., 2022]. However, it is essential to adopt the new surrogate loss in our purpose: as established in Lemma 3, both negative terms and the mild difference between $\\mathbf{x}_{t}$ and $\\mathbf{y}_{t}$ are exploited in our regret analysis. By contrast, the old surrogate loss (6) lacks these advanced properties. $\\triangleleft$ ", "page_idx": 7}, {"type": "text", "text": "4 Efficient algorithm for universal online convex optimization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we present our efficient algorithms for universal OCO. To reduce the cost of projections, we deploy multiple experts on a ball $\\bar{\\mathcal{V}^{=}}\\left\\{\\mathbf{x}\\mid\\|\\mathbf{x}\\|\\leq D\\right\\}$ enclosing domain $\\mathcal{X}$ . After combining their decisions, we project the solution in $\\boldsymbol{\\wp}$ onto $\\mathcal{X}$ , which is the only projection onto $\\mathcal{X}$ per round. ", "page_idx": 7}, {"type": "text", "text": "4.1 Efficient algorithm for minimax universal regret ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To handle unknown parameters of strong convexity and exp-concavity, we construct two finite sets, i.e., $\\mathcal{P}_{\\mathrm{sc}}$ and $\\mathcal{P}_{\\exp}$ , to approximate their values [Zhang et al., 2022]. Taking $\\lambda$ -strongly convex functions as an example, we assume the unknown modulus $\\lambda$ is bounded by $\\lambda\\in[1/T,1]^{2}$ , and set $\\mathcal{P}_{\\mathrm{sc}}=\\{1/T,2/T,\\cdot\\cdot\\cdot^{\\cdot},2^{N}/T\\}$ , where $N=\\lceil\\log_{2}T\\rceil$ . In this way, for any $\\lambda\\in[1/T,1]$ , there exists a $\\widehat{\\lambda}\\in\\mathcal{P}_{\\mathrm{sc}}$ such that $\\widehat{\\lambda}\\leq\\lambda\\leq2\\widehat{\\lambda}$ . Moreover, we design three types of expert-losses. For general convex functions, we construct the expert-loss as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\ell_{t}^{\\mathrm{{cvx}}}(\\mathbf{y})=\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}-\\mathbf{y}_{t}\\rangle,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $g_{t}(\\mathbf{y})$ is defined in (8). Since $\\ell_{t}^{\\mathrm{cvx}}(\\cdot)$ is convex, we use OGD as the expert-algorithm to minimize it. To handle exp-concave functions, we construct the expert-loss for each $\\widehat{\\alpha}\\in\\mathcal{P}_{\\mathrm{exp}}$ as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\ell_{t,\\widehat{\\alpha}}^{\\mathrm{exp}}(\\mathbf{y})=\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}-\\mathbf{y}_{t}\\rangle+\\frac{\\widehat{\\beta}}{2}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}-\\mathbf{y}_{t}\\rangle^{2},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\begin{array}{r}{\\widehat{\\beta}=\\frac{1}{2}\\operatorname*{min}\\{\\frac{1}{4G D},\\widehat{\\alpha}\\}}\\end{array}$ . It is easy to verify that $\\ell_{t,\\widehat{\\alpha}}^{\\exp}(\\cdot)$ is $\\frac{\\widehat{\\beta}}{4}$ -exp-concave, so we use ONS as the expert-algorithm. For strongly convex functions, we construct the expert-loss for each $\\widehat{\\lambda}\\in\\mathcal{P}_{\\mathrm{sc}}$ as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\ell_{t,\\widehat{\\lambda}}^{\\mathrm{sc}}(\\mathbf{y})=\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}-\\mathbf{y}_{t}\\rangle+\\frac{\\widehat{\\lambda}}{2}\\|\\mathbf{y}-\\mathbf{x}_{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Since $\\ell_{t,\\widehat{\\lambda}}^{\\mathrm{sc}}(\\cdot)$ is $\\widehat{\\lambda}$ -strongly convex, we use OGD with step size $\\eta_{t}=1/[\\widehat{\\lambda}t]$ as the expert-algorithm.   \nFinally, we deploy a meta-algorithm to track the best expert on the fly. Following Zhang et al. ", "page_idx": 7}, {"type": "text", "text": "Algorithm 2 Efficient Algorithm for Universal OCO ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "1: Input: The modulus set $\\mathcal{P}_{\\mathrm{sc}}$ and $\\mathcal{P}_{\\exp}$ , the expert set ${\\mathcal{A}}=\\emptyset$ , the number of experts $k=0$   \n2: $k\\gets k+1$ , create an expert $E^{1}$ by running OGD with loss (24) over $\\boldsymbol{\\wp}$   \n3: for all $\\widehat{\\alpha}\\in\\mathcal{P}_{\\exp}$ do   \n4: $k\\gets k+1$ , create an expert $E^{k}$ by running ONS with loss (25) and parameter $\\widehat{\\alpha}$ over $\\boldsymbol{\\wp}$   \n5: end for   \n6: for all $\\widehat{\\lambda}\\in\\mathcal{P}_{\\mathrm{sc}}$ do   \n7: $k\\gets k+1$ , create an expert $E^{k}$ by running OGD with loss (26) and parameter $\\widehat{\\lambda}$ over $\\boldsymbol{\\wp}$   \n8: end for   \n9: Add all the experts to the set: $\\mathcal{A}=\\{E^{1},E^{2},\\cdot\\cdot\\cdot,E^{k}\\}$   \n10: for $t=1$ to $T$ do   \n11: Compute the weight $p_{t}^{i}$ of each expert $E^{i}$ by (27)   \n12: Receive the decision $\\mathbf{y}_{t}^{i}$ from each expert $\\dot{E}^{i}$ in $\\boldsymbol{\\mathcal{A}}$   \n13: Aggregate all the decisions by $\\begin{array}{r}{\\mathbf{y}_{t_{-}}=\\sum_{i=1}^{|A|}p_{t}^{i}\\mathbf{y}_{t}^{i}}\\end{array}$   \n14: Submit the decision $\\mathbf x_{t}=\\Pi_{\\mathcal{X}}[\\mathbf y_{t}]$ \u25b7The only step projects onto domain $\\mathcal{X}$ per round.   \n15: Suffer the loss $f_{t}(\\mathbf{x}_{t})$ and observe the gradient $\\nabla f_{t}(\\mathbf{x}_{t})$   \n16: Construct the expert-loss $\\ell_{t}^{\\mathrm{cvx}}(\\cdot)$ , $\\ell_{t}^{\\mathrm{sc}}(\\cdot)$ or $\\ell_{t}^{\\exp}(\\cdot)$ and sent it to corresponding expert in $\\boldsymbol{\\mathcal{A}}$   \n17: end for ", "page_idx": 8}, {"type": "text", "text": "[2022], we use the linearized surrogate loss to measure the performance of the experts, and choose Adapt-ML-Prod [Gaillard et al., 2014] as the meta-algorithm to yield a second-order bound. ", "page_idx": 8}, {"type": "text", "text": "Our efficient algorithm for universal OCO is summarized in Algorithm 2. From Steps 2 to 9, it creates a set of experts by running multiple online algorithms over the ball $\\boldsymbol{\\wp}$ , each specialized for a distinct function type. Then, it maintains a set $\\boldsymbol{\\mathcal{A}}$ consisting of all experts, and the $i$ -th expert is denoted by $E^{i}$ . In the $t$ -th round, it computes the weight $p_{t}^{i}$ of each expert $E^{i}$ in Step 11 according to Adapt-ML-Prod. After receiving all the predictions from the experts in Step 12, it aggregates them based on their weights to attain $\\mathbf{y}_{t}$ in Step 13. Next, it conducts the only projection onto the original domain $\\mathcal{X}$ to obtain the actual decision $\\mathbf{x}_{t}$ in Step 14. In Step 15, it evaluates the gradient $\\nabla f_{t}(\\mathbf{x}_{t})$ to construct the expert-losses in (24), (25), and (26). In Step 16, it sends the corresponding expert-loss to each expert so that it can make predictions for the next round. ", "page_idx": 8}, {"type": "text", "text": "Finally, we elucidate how our algorithm determines the weight of the $i$ -th expert $E^{i}$ . We measure the performance of expert $E^{i}$ by the linearized surrogate loss, i.e., $l_{t}^{i}=\\langle\\nabla g_{t}(\\bar{\\mathbf{y}_{t}}),\\mathbf{y}_{t}^{i}-\\mathbf{y}_{t}\\rangle$ . According to Lemma 2, we have $|l_{t}^{i}|\\leq\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|\\|\\mathbf{y}_{t}^{i}-\\mathbf{y}_{t}\\|\\leq2G D.$ . Since Adapt-ML-Prod requires the loss to fall within the range of $[0,1]$ , we normalize $l_{t}^{i}$ to construct the meta-loss as $\\ell_{t}^{i}=\\big(\\langle\\bar{\\nabla}g_{t}\\bigl(\\mathbf{y}_{t}\\bigr),\\mathbf{y}_{t}^{i}-$ $\\begin{array}{r}{\\mathbf{y}_{t}\\(\\mathbf{\\rho})/(4G D)+\\frac{1}{2}\\in[0,1]}\\end{array}$ . The loss of the meta-algorithm in the $t$ -th round is $\\begin{array}{r}{\\ell_{t}=\\sum_{i=1}^{|A|}p_{t}^{i}\\ell_{t}^{i}}\\end{array}$ , which is a constant $\\frac{1}{2}$ due to its construction and Step 13. For each expert $E^{i}$ , its weight is updated by: ", "page_idx": 8}, {"type": "equation", "text": "$$\np_{t}^{i}=\\frac{\\eta_{t-1}^{i}w_{t-1}^{i}}{\\sum_{j=1}^{|A|}\\eta_{t-1}^{j}w_{t-1}^{j}},\\;\\;w_{t-1}^{i}=\\left(w_{t-2}^{i}\\left(1+\\eta_{t-2}^{i}(\\ell_{t-1}-\\ell_{t-1}^{i})\\right)\\right)^{\\frac{\\eta_{t-1}^{i}}{\\eta_{t-2}^{i}}}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\begin{array}{r}{\\eta_{t-1}^{i}=\\operatorname*{min}\\left\\{\\frac{1}{2},\\sqrt{(\\ln|A|)/(1+\\sum_{s=1}^{t-1}(\\ell_{s}-\\ell_{s}^{i})^{2})}\\right\\}}\\end{array}$ . In the first round, we set $w_{0}^{i}=1/\\vert A\\vert$ . ", "page_idx": 8}, {"type": "text", "text": "Remark 3 While the surrogate loss in (8) involves the projection operation, our proposed meta-loss and expert-losses only access $g_{t}(\\mathbf{y})$ through $\\nabla g_{t}(\\mathbf{y}_{t})$ , which is given by Cutkosky [2020], ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\nabla g_{t}(\\mathbf{y}_{t})=\\nabla f_{t}(\\mathbf{x}_{t})-\\mathbb{1}_{\\{\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{v}_{t}\\rangle<0\\}}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{v}_{t}\\rangle\\cdot\\mathbf{v}_{t}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where vt =\u2225yt\u2212xt\u2225 . According to its formulation, the gradient can be directly computed from $\\mathbf{x}_{t}$ and $\\mathbf{y}_{t}$ , which means no additional projections are needed. Therefore, in each round, our algorithm requires only 1 projection onto domain $\\mathcal{X}$ . \u25c1 ", "page_idx": 8}, {"type": "text", "text": "Due to page limit, we provide the expert-algorithms, as well as all the proofs, in Appendix B. The theoretical guarantee of Algorithm 2 is given below. ", "page_idx": 8}, {"type": "text", "text": "Theorem 1 Under Assumptions 1 and 2, Algorithm 2 attains $O({\\sqrt{T}}),\\,O({\\frac{d}{\\alpha}}\\log T)$ and $\\textstyle O({\\frac{1}{\\lambda}}\\log T)$ regret for general convex functions, $\\alpha$ -exp-concave functions with $\\alpha\\,\\in\\,[1/T,1]$ , and $\\lambda$ -strongly convex functions with $\\lambda\\in[1/T,1]$ , respectively. ", "page_idx": 9}, {"type": "text", "text": "Remark 4 Similar to previous studies [Wang et al., 2019, Zhang et al., 2022], our universal algorithm also achieves the minimax optimal regret, but only requires 1 projection. \u25c1 ", "page_idx": 9}, {"type": "text", "text": "4.2 Efficient algorithm for small-loss universal regret ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Furthermore, we consider the small-loss regret for smooth and non-negative online functions. To this end, an additional assumption is required [Srebro et al., 2010]. ", "page_idx": 9}, {"type": "text", "text": "Assumption 3 All the online functions are non-negative, and $H$ -smooth over $_{\\mathcal{X}}$ ", "page_idx": 9}, {"type": "text", "text": "To exploit the smoothness, we enhance the expert-loss for strongly convex functions in (26) as ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\widehat{\\ell}_{t,\\widehat{\\lambda}}^{\\mathrm{sc}}(\\mathbf{y})=\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}-\\mathbf{y}_{t}\\rangle+\\frac{\\widehat{\\lambda}}{2G^{2}}\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|^{2}\\|\\mathbf{y}-\\mathbf{x}_{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Since $\\widehat{\\ell}_{t,\\widehat{\\lambda}}^{\\mathrm{sc}}(\\cdot)$ is strongly convex and smooth, we use $\\mathrm{S^{2}O G D}$ [Wang et al., 2020b] as the expertalgorithm. For general convex and exp-concave functions, we reuse (24) and (25) as the expert-losses, and employ ONS [Orabona et al., 2012] and SOGD [Zhang et al., 2019] as the expert-algorithms. The meta-algorithm remains unchanged. In this way, we obtain the following regret guarantee. ", "page_idx": 9}, {"type": "text", "text": "Theorem 2 Under Assumptions $^{\\,l}$ , 2 and 3, the improved version of Algorithm 2 attains $O(\\sqrt{L_{T}})$ , $\\textstyle O({\\frac{d}{\\alpha}}\\log L_{T})$ and $\\begin{array}{r}{O(\\frac{1}{\\lambda}\\log L_{T})}\\end{array}$ regret for general convex functions, $\\alpha$ -exp-concave functions with $\\alpha\\in[1/T,1]$ , and $\\lambda$ -strongly convex functions with $\\lambda\\in[1/T,1]$ , respectively, where the small-loss quantity $\\begin{array}{r}{L_{T}=\\operatorname*{min}_{\\mathbf{x}\\in\\mathcal{X}}\\sum_{t=1}^{T}f_{t}(\\mathbf{x})}\\end{array}$ is the cumulative loss of the best decision from the domain $\\mathcal{X}$ . ", "page_idx": 9}, {"type": "text", "text": "Remark 5 With only 1 projection in each round, our universal algorithm is able to deliver optimal small-loss regret bounds for multiple types of convex functions simultaneously. In contrast, Wang et al. [2020b] and Zhang et al. [2022] take ${\\cal O}(\\log T)$ projections to achieve the small-loss regret. $\\triangleleft$ ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion and future work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose a projection-efficient universal algorithm that achieves minimax optimal regret for three types of convex functions with only 1 projection per round. Furthermore, we enhance our algorithm to exploit the smoothness property and demonstrate that it attains small-loss regret for convex and smooth functions. To demonstrate the effectiveness of our proposed method, we also conduct empirical experiments, and the results are presented in Appendix E. ", "page_idx": 9}, {"type": "text", "text": "There are several directions for future research. First, one potentially unfavorable characteristic of our work is the requirements of domain and gradient boundedness. Motivated by the recent developments in parameter-free online learning for unbounded domains and gradients [Orabona, 2014, Orabona and P\u00e1l, 2016, Cutkosky and Boahen, 2016, 2017, Foster et al., 2017, Luo et al., 2022, Jacobsen and Cutkosky, 2022, 2023], we will investigate whether our algorithms can further avoid these prior knowledge in the future. Second, in addition to the small-loss bound, another important type of problem-dependent guarantee is the gradient-variation regret bound [Zhao et al., 2020, 2024], which has been actively studied recently due to its profound relationship to games and stochastic optimization. In the literature, recent studies [Yan et al., 2023, 2024, Xie et al., 2024, Wang et al., 2024a] achieve almost-optimal gradient-variation regret in universal online learning, but also suffer high projection complexity. Therefore, it remains challenging and important to develop a projection-efficient universal algorithm with optimal gradient-variation regret guarantees. Third, to deal with changing environments, adaptive regret has been proposed to minimize the regret over every interval in various setting of online learning [Hazan and Seshadhri, 2007, Daniely et al., 2015, Wan et al., 2021a, Wang et al., 2024b]. Existing universal algorithms [Zhang et al., 2021, Yang et al., 2024] typically conduct $O(\\log^{2}T)$ projections per round. In the future, we will investigate whether whether we can reduce the projection complexity of universal algorithms for adaptive regret. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was partially supported by NSFC (62361146852, 62122037), and the Collaborative Innovation Center of Novel Software Technology and Industrialization. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "J. Abernethy, P. L. Bartlett, A. Rakhlin, and A. Tewari. Optimal strategies and minimax lower bounds for online convex games. In Proceedings of the 21st Annual Conference on Learning Theory (COLT), pages 415\u2013423, 2008.   \nP. L. Bartlett, E. Hazan, and A. Rakhlin. Adaptive online gradient descent. In Advances in Neural Information Processing Systems 20 (NIPS), pages 65\u201372, 2008.   \nN. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press, 2006.   \nC.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2(3):27:1\u201327:27, 2011.   \nS. Chen, Y.-J. Z. W.-W. Tu, P. Zhao, and L. Zhang. Optimistic online mirror descent for bridging stochastic and adversarial online convex optimization. Journal of Machine Learning Research, pages 1 \u2013 62, 2024.   \nC.-K. Chiang, T. Yang, C.-J. Lee, M. Mahdavi, C.-J. Lu, R. Jin, and S. Zhu. Online optimization with gradual variations. In Proceedings of the 25th Annual Conference on Learning Theory (COLT), pages 1\u201320, 2012.   \nA. Cutkosky. Parameter-free, dynamic, and strongly-adaptive online learning. In Proceedings of the 37th International Conference on Machine Learning (ICML), pages 2250\u20132259, 2020.   \nA. Cutkosky and K. Boahen. Online learning without prior information. In Proceedings of the 30th Annual Conference on Learning Theory (COLT), pages 643\u2013677, 2017.   \nA. Cutkosky and K. A. Boahen. Online convex optimization with unconstrained domains and losses. In Advances in Neural Information Processing Systems 29 (NIPS), pages 748\u2013756, 2016.   \nA. Cutkosky and F. Orabona. Black-box reductions for parameter-free online learning in Banach spaces. In Proceedings of the 31st Conference On Learning Theory (COLT), pages 1493\u20131529, 2018.   \nA. Daniely, A. Gonen, and S. Shalev-Shwartz. Strongly adaptive online learning. In Proceedings of the 32nd International Conference on Machine Learning (ICML), pages 1405\u20131411, 2015.   \nJ. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. In Proceedings of the 23rd Annual Conference on Learning Theory (COLT), pages 257\u2013269, 2010.   \nD. J. Foster, S. Kale, M. Mohri, and K. Sridharan. Parameter-free online learning via model selection. In Advances in Neural Information Processing Systems 30 (NIPS), pages 6020\u20136030, 2017.   \nP. Gaillard, G. Stoltz, and T. van Erven. A second-order bound with excess losses. In Proceedings of the 27th Conference on Learning Theory (COLT), pages 176\u2013196, 2014.   \nD. Garber and B. Kretzu. Projection-free online exp-concave optimization. In Proceedings of Thirty Sixth Conference on Learning Theory (COLT), pages 1259\u20131284, 2023.   \nE. Hazan. Introduction to Online Convex Optimization. Foundations and Trends in Optimization, 2 (3-4):157\u2013325, 2016.   \nE. Hazan and S. Kale. Projection-free online learning. In Proceedings of the 29th International Conference on Machine Learning (ICML), pages 521\u2013528, 2012.   \nE. Hazan and E. Minasyan. Faster projection-free online learning. In Proceedings of Thirty Third Conference on Learning Theory (COLT), pages 1877\u20131893, 2020.   \nE. Hazan and C. Seshadhri. Adaptive algorithms for online decision problems. Electronic Colloquium on Computational Complexity, 88, 2007.   \nE. Hazan, A. Agarwal, and S. Kale. Logarithmic regret algorithms for online convex optimization. Machine Learning, 69(2-3):169\u2013192, 2007.   \nA. Jacobsen and A. Cutkosky. Parameter-free mirror descent. In Proceedings of 35th Conference on Learning Theory (COLT), pages 4160\u20134211, 2022.   \nA. Jacobsen and A. Cutkosky. Unconstrained online learning with unbounded losses. In Proceedings of the 40th International Conference on Machine Learning (ICML), pages 14590\u201314630, 2023.   \nD. P. Kingma and J. L. Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015.   \nI. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations (ICLR), 2019.   \nH. Luo and R. E. Schapire. Achieving all with no parameters: AdaNormalHedge. In Proceedings of the 28th Conference on Learning Theory (COLT), pages 1286\u20131304, 2015.   \nH. Luo, M. Zhang, P. Zhao, and Z.-H. Zhou. Corralling a larger band of bandits: A case study on switching regret for linear bandits. In Proceedings of the 35th Conference on Learning Theory (COLT), pages 3635\u20133684, 2022.   \nZ. Mhammedi, W. M. Koolen, and T. Van Erven. Lipschitz adaptivity with multiple learning rates in online learning. In Proceedings of the 32nd Conference on Learning Theory (COLT), pages 2490\u20132511, 2019.   \nM. Mohri and S. Yang. Accelerating online convex optimization via adaptive prediction. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AISTATS), pages 848\u2013856, 2016.   \nM. C. Mukkamala and M. Hein. Variants of RMSProp and Adagrad with logarithmic regret bounds. In Proceedings of the 34th International Conference on Machine Learning (ICML), pages 2545\u20132553, 2017.   \nF. Orabona. Simultaneous model selection and optimization through parameter-free stochastic learning. In Advances in Neural Information Processing Systems 27 (NIPS), pages 1116\u20131124, 2014.   \nF. Orabona and D. P\u00e1l. Coin betting and parameter-free online learning. In Advances in Neural Information Processing Systems 29 (NIPS), pages 577\u2013585, 2016.   \nF. Orabona and D. P\u00e1l. Scale-free online learning. Theoretical Computer Science, 716:50\u201369, 2018.   \nF. Orabona, N. Cesa-Bianchi, and C. Gentile. Beyond logarithmic bounds in online learning. In Proceedings of the 15th International Conference on Artificial Intelligence and Statistics (AISTATS), pages 823\u2013831, 2012.   \nE. Ordentlich and T. M. Cover. The cost of achieving the best portfolio in hindsight. Mathematics of Operations Research, 23(4):960\u2013982, 1998.   \nD. Prokhorov. IJCNN 2001 neural network competition. Technical report, Ford Research Laboratory, 2001.   \nS. J. Reddi, S. Kale, and S. Kumar. On the convergence of Adam and beyond. In International Conference on Learning Representations (ICLR), 2018.   \nS. Shalev-Shwartz. Online Learning: Theory, Algorithms, and Applications. PhD thesis, The Hebrew University of Jerusalem, 2007.   \nS. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: primal estimated sub-gradient solver for SVM. In Proceedings of the 24th International Conference on Machine Learning (ICML), pages 807\u2013814, 2007.   \nS. Shalev-Shwartz, Y. Singer, N. Srebro, and A. Cotter. Pegasos: primal estimated sub-gradient solver for svm. Mathematical Programming, 127(1):3\u201330, 2011.   \nN. Srebro, K. Sridharan, and A. Tewari. Smoothness, low-noise and fast rates. In Advances in Neural Information Processing Systems 23 (NIPS), pages 2199\u20132207, 2010.   \nT. Tieleman and G. Hinton. Lecture 6.5-RMSProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, pages 26\u201331, 2012.   \nT. van Erven and W. M. Koolen. MetaGrad: Multiple learning rates in online learning. In Advances in Neural Information Processing Systems 29 (NIPS), pages 3666\u20133674, 2016.   \nY. Wan and L. Zhang. Projection-free online learning over strongly convex sets. Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), pages 10076\u201310084, 2021.   \nY. Wan, W.-W. Tu, and L. Zhang. Strongly adaptive online learning over partial intervals. Science China Information Sciences, 2021a.   \nY. Wan, B. Xue, and L. Zhang. Projection-free online learning in dynamic environments. Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), pages 10067\u201310075, 2021b.   \nY. Wan, W.-W. Tu, and L. Zhang. Online frank-wolfe with arbitrary delays. In Advances in Neural Information Processing Systems (NeurIPS), pages 19703\u201319715, 2022.   \nG. Wang, S. Lu, and L. Zhang. Adaptivity and optimality: A universal algorithm for online convex optimization. In Proceedings of the 35th Conference on Uncertainty in Artificial Intelligence (UAI), pages 659\u2013668, 2019.   \nG. Wang, S. Lu, Q. Cheng, W.-W. Tu, and L. Zhang. SAdam: A variant of Adam for strongly convex functions. In International Conference on Learning Representations (ICLR), 2020a.   \nG. Wang, S. Lu, Y. Hu, and L. Zhang. Adapting to smoothness: A more universal algorithm for online convex optimization. In Proceedings of the 34th AAAI Conference on Artificial Intelligence (AAAI), pages 6162\u20136169, 2020b.   \nY. Wang, Y. Wan, S. Zhang, and L. Zhang. Distributed projection-free online learning for smooth and convex losses. In Proceedings of the 37th AAAI Conference on Artificial Intelligence (AAAI), pages 10226\u201310234, 2023.   \nY. Wang, S. Chen, W. Jiang, W. Yang, Y. Wan, and L. Zhang. Online composite optimization between stochastic and adversarial environments. In Advances in Neural Information Processing Systems 37 (NeurIPS), 2024a.   \nY. Wang, W. Yang, W. Jiang, S. Lu, B. Wang, H. Tang, Y. Wan, and L. Zhang. Non-stationary projection-free online learning with dynamic and adaptive regret guarantees. In Proceedings of the 38th AAAI Conference on Artificial Intelligence (AAAI), pages 15671\u201315679, 2024b.   \nY.-F. Xie, P. Zhao, and Z.-H. Zhou. Gradient-variation online learning under generalized smoothness. In Advances in Neural Information Processing Systems 37 (NeurIPS), 2024.   \nY.-H. Yan, P. Zhao, and Z.-H. Zhou. Universal online learning with gradient variations: A multi-layer online ensemble approach. In Advances in Neural Information Processing Systems 36 (NeurIPS), pages 37682\u201337715, 2023.   \nY.-H. Yan, P. Zhao, and Z.-H. Zhou. A simple and optimal approach for universal online learning with gradient variations. In Advances in Neural Information Processing Systems 37 (NeurIPS), 2024.   \nT. Yang, M. Mahdavi, R. Jin, and S. Zhu. Regret bounded by gradual variation for online convex optimization. Machine Learning, 95:183\u2013223, 2014.   \nW. Yang, W. Jiang, Y. Wang, P. Yang, Y. Hu, and L. Zhang. Small-loss adaptive regret for online convex optimization. In Proceedings of the 41st International Conference on Machine Learning (ICML), pages 56156\u201356195, 2024.   \nL. Zhang, T.-Y. Liu, and Z.-H. Zhou. Adaptive regret of convex and smooth functions. In Proceedings of the 36th International Conference on Machine Learning (ICML), pages 7414\u20137423, 2019.   \nL. Zhang, G. Wang, W.-W. Tu, W. Jiang, and Z.-H. Zhou. Dual adaptivity: A universal algorithm for minimizing the adaptive regret of convex functions. In Advances in Neural Information Processing Systems 34 (NeurIPS), pages 24968\u201324980, 2021.   \nL. Zhang, G. Wang, J. Yi, and T. Yang. A simple yet universal strategy for online convex optimization. In Proceedings of the 39th International Conference on Machine Learning (ICML), pages 26605\u2013 26623, 2022.   \nP. Zhao, Y.-J. Zhang, L. Zhang, and Z.-H. Zhou. Dynamic regret of convex and smooth functions. In Advances in Neural Information Processing Systems 33 (NeurIPS), pages 12510\u201312520, 2020.   \nP. Zhao, Y.-F. Xie, L. Zhang, and Z.-H. Zhou. Efficient methods for non-stationary online learning. In Advances in Neural Information Processing Systems 35 (NeurIPS), pages 11573\u201311585, 2022.   \nP. Zhao, Y.-J. Zhang, L. Zhang, and Z.-H. Zhou. Adaptivity and non-stationarity: Problem-dependent dynamic regret for online convex optimization. Journal of Machine Learning Research, 25(98):1 \u2013 52, 2024.   \nM. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the 20th International Conference on Machine Learning (ICML), pages 928\u2013936, 2003. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Algorithms for experts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we provide the detailed procedures of the expert-algorithms in our efficient algorithm. ", "page_idx": 14}, {"type": "text", "text": "A.1 Online gradient descent for convex functions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We use OGD [Zinkevich, 2003] to minimize $\\ell_{t}^{\\mathrm{cvx}}(\\cdot)$ in (24). The procedure of the expert-algorithm for general convex functions is summarized in Algorithm 3. ", "page_idx": 14}, {"type": "text", "text": "Algorithm 3 Expert $E^{i}$ : OGD for Convex Functions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1: Let $\\mathbf{y}_{1}^{i}$ be any point in $\\boldsymbol{\\wp}$   \n2: for $t=1$ to $T$ do   \n3: Submit $\\mathbf{y}_{t}^{i}$ to the meta-algorithm   \n4: Update ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\widehat{\\mathbf{y}}_{t+1}^{i}=\\mathbf{y}_{t}^{i}-\\frac{1}{\\sqrt{t}}\\nabla g_{t}(\\mathbf{y}_{t})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "5: Conduct a projection onto $\\boldsymbol{\\wp}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{y}_{t+1}^{i}=\\left\\{\\begin{array}{l l}{\\widehat{\\mathbf{y}}_{t+1}^{i},}&{\\mathrm{if~}\\Vert\\widehat{\\mathbf{y}}_{t+1}^{i}\\Vert\\leq D,}\\\\ {\\widehat{\\mathbf{y}}_{t+1}^{i}\\cdot\\frac{D}{\\Vert\\widehat{\\mathbf{y}}_{t+1}^{i}\\Vert},}&{\\mathrm{otherwise}\\;.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "6: end for ", "page_idx": 14}, {"type": "text", "text": "A.2 Online newton step for exp-concave (and smooth) functions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Lemma 4 Under Assumptions 1 and 2, $\\ell_{t,\\widehat{\\alpha}}^{\\exp}(\\cdot)$ in (25) is $\\frac{\\widehat{\\beta}}{4}$ -exp-concave, and $\\|\\nabla\\ell_{t,\\widehat{\\alpha}}^{\\mathrm{exp}}(\\mathbf{y})\\|^{2}\\leq2G^{2}$ . ", "page_idx": 14}, {"type": "text", "text": "Thus, we use ONS to minimize $\\ell_{t,\\widehat{\\alpha}}^{\\exp}(\\cdot)$ . Different from OGD, the projection of ONS onto $\\boldsymbol{\\wp}$ cannot be achieved through a simple resc aling like Step 5 in Algorithm 3. Here, we employ an efficient implementation of ONS [Mhammedi et al., 2019] that enhances the efficiency of its projection onto $\\boldsymbol{\\wp}$ . The procedure is summarized in Algorithm 4. ", "page_idx": 14}, {"type": "text", "text": "Algorithm 4 Expert $E^{i}$ : ONS for Exp-concave (and Smooth) Functions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1: Let $\\mathbf{y}_{1}^{i}$ be any point in $\\boldsymbol{\\wp}$ and $\\begin{array}{r}{\\Sigma_{1}=\\frac{1}{\\widehat{\\beta}^{2}D^{2}}\\mathbf{I}_{d}}\\end{array}$   \n2: for $t=1$ to $T$ do   \n3: Submit $\\mathbf{y}_{t}^{i}$ to the meta-algorithm   \n4: Update ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\boldsymbol{\\Sigma}}_{t+1}={\\boldsymbol{\\Sigma}}_{t}+\\nabla\\ell_{t,\\widehat{\\boldsymbol{\\alpha}}}^{\\mathrm{{exp}}}(\\mathbf{y}_{t}^{i})\\nabla\\ell_{t,\\widehat{\\boldsymbol{\\alpha}}}^{\\mathrm{{exp}}}(\\mathbf{y}_{t}^{i})^{\\top},\\quad\\widehat{\\mathbf{y}}_{t+1}^{i}=\\mathbf{y}_{t}^{i}-\\frac{1}{\\widehat{\\boldsymbol{\\beta}}}{\\boldsymbol{\\Sigma}}_{t+1}^{-1}\\nabla\\ell_{t,\\widehat{\\boldsymbol{\\alpha}}}^{\\mathrm{{exp}}}(\\mathbf{y}_{t}^{i})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nabla\\ell_{t,\\widehat{\\alpha}}^{\\mathrm{exp}}(\\mathbf{y}_{t}^{i})=\\nabla g_{t}(\\mathbf{y}_{t})+\\widehat{\\beta}\\nabla g_{t}(\\mathbf{y}_{t})\\nabla g_{t}(\\mathbf{y}_{t})^{\\top}(\\mathbf{y}_{t}^{i}-\\mathbf{y}_{t})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "5: Conduct a projection onto $\\boldsymbol{\\wp}$ in (29)   \n6: end for ", "page_idx": 14}, {"type": "text", "text": "Lemma 5 Let $\\mathbf{\\boldsymbol{\\Lambda}}_{t+1}:=\\mathrm{diag}((\\boldsymbol{\\lambda}_{t}^{k})_{k\\in[d]})$ and $\\mathbf{Q}_{t+1}$ are the matrices of eigenvalues and eigenvectors of $\\begin{array}{r}{\\left(\\boldsymbol{\\Sigma}_{t+1}-\\frac{1}{\\widehat{\\beta}^{2}D^{2}}\\mathbf{I}_{d}\\right)}\\end{array}$ , respectively. Then, the projection onto the ball $\\boldsymbol{\\wp}$ in Step 5 can be formulated as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{y}_{t+1}^{i}=\\left\\{\\begin{array}{l l}{\\widehat{\\mathbf{y}}_{t+1}^{i},}&{i f\\|\\widehat{\\mathbf{y}}_{t+1}^{i}\\|\\leq D,}\\\\ {\\mathbf{Q}_{t+1}^{\\top}\\left(x_{t+1}^{i}\\mathbf{I}+\\Lambda_{t+1}\\right)^{-1}\\mathbf{Q}_{t+1}\\Sigma_{t+1}\\widehat{\\mathbf{y}}_{t+1}^{i},}&{o t h e r w i s e\\;.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $x_{t+1}^{i}$ is the unique solution of $\\begin{array}{r}{\\rho(x):=\\sum_{k=1}^{d}\\frac{\\langle\\mathbf e_{k},\\mathbf Q_{t+1}\\Sigma_{t+1}\\widehat{\\mathbf y}_{t+1}^{i}\\rangle^{2}}{(x+\\lambda_{t}^{k})^{2}}=D^{2},}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "A.3 Online gradient descent for strongly convex functions ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We establish the following lemma for function $\\ell_{t}^{\\mathrm{sc}}(\\cdot)$ in (14). ", "page_idx": 15}, {"type": "text", "text": "Lemma 6 Under Assumptions $^{\\,l}$ and 2, the loss function $\\ell_{t}^{\\mathrm{sc}}(\\cdot)$ in (14) is $\\lambda$ -strongly convex, and $\\|\\nabla\\ell_{t}^{\\mathrm{sc}}(\\mathbf{y})\\|^{2}\\leq(G+2D)^{2}$ . ", "page_idx": 15}, {"type": "text", "text": "Since $\\ell_{t,\\widehat{\\lambda}}^{\\mathrm{sc}}(\\cdot)$ in (26) shares the same formulation as $\\ell_{t}^{\\mathrm{sc}}(\\cdot),\\ell_{t,\\widehat{\\lambda}}^{\\mathrm{sc}}(\\cdot)$ also benefits from the aforementioned properties, with the distinction being the substitution of $\\lambda$ for\u03bb. Therefore, we use a variant of OGD [Shalev-Shwartz et al., 2007] to minimize $\\ell_{t,\\widehat{\\lambda}}^{\\mathrm{sc}}(\\cdot)$ . The procedu re is summarized in Algorithm 5. ", "page_idx": 15}, {"type": "text", "text": "Algorithm 5 Expert $E^{i}$ : OGD for Strongly Convex Functions ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1: Let $\\mathbf{y}_{1}^{i}$ be any point in $\\boldsymbol{\\wp}$   \n2: for $t=1$ to $T$ do   \n3: Submit $\\mathbf{y}_{t}^{i}$ to the meta-algorithm   \n4: Update ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\widehat{\\mathbf{y}}_{t+1}^{i}=\\mathbf{y}_{t}^{i}-\\frac{1}{\\widehat{\\lambda}t}\\nabla\\ell_{t,\\widehat{\\lambda}}^{s c}(\\mathbf{y}_{t}^{i})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla\\ell_{t,\\widehat{\\lambda}}^{\\mathrm{sc}}(\\mathbf{y}_{t}^{i})=\\nabla g_{t}(\\mathbf{y}_{t})+\\widehat{\\lambda}(\\mathbf{y}_{t}^{i}-\\mathbf{x}_{t})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "5: Conduct a projection onto $\\boldsymbol{\\wp}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{y}_{t+1}^{i}=\\left\\{\\begin{array}{l l}{\\widehat{\\mathbf{y}}_{t+1}^{i},}&{\\mathrm{if~}\\Vert\\widehat{\\mathbf{y}}_{t+1}^{i}\\Vert\\leq D,}\\\\ {\\widehat{\\mathbf{y}}_{t+1}^{i}\\cdot\\frac{D}{\\Vert\\widehat{\\mathbf{y}}_{t+1}^{i}\\Vert},}&{\\mathrm{otherwise}\\;.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "6: end for ", "page_idx": 15}, {"type": "text", "text": "A.4 Scale-free online gradient descent for convex and smooth functions ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To exploit smoothness, we use scale-free online gradient descent (SOGD) [Zhang et al., 2019] to minimize $\\ell_{t}^{\\mathrm{cvx}}(\\cdot)$ in (24). The procedure is summarized in Algorithm 6. ", "page_idx": 15}, {"type": "text", "text": "Algorithm 6 Expert $E^{i}$ : Scale-free OGD for Convex and Smooth Functions ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1: Let $\\mathbf{y}_{1}^{i}$ be any point in $\\boldsymbol{\\wp}$   \n2: for $t=1$ to $T$ do   \n3: Submit $\\mathbf{y}_{t}^{i}$ to the meta-algorithm   \n4: Update ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\widehat{\\mathbf{y}}_{t+1}^{i}=\\mathbf{y}_{t}^{i}-\\eta_{t}\\nabla g_{t}(\\mathbf{y}_{t})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\eta_{t}=\\frac{\\alpha}{\\sqrt{\\delta+\\sum_{s=1}^{t}\\|\\nabla g_{s}(\\mathbf{y}_{s})\\|^{2}}},\\quad\\alpha,\\delta>0\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "5: Conduct a projection onto $\\boldsymbol{\\wp}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{y}_{t+1}^{i}=\\left\\{\\begin{array}{l l}{\\widehat{\\mathbf{y}}_{t+1}^{i},}&{\\mathrm{if~}\\Vert\\widehat{\\mathbf{y}}_{t+1}^{i}\\Vert\\leq D,}\\\\ {\\widehat{\\mathbf{y}}_{t+1}^{i}\\cdot\\frac{D}{\\Vert\\widehat{\\mathbf{y}}_{t+1}^{i}\\Vert},}&{\\mathrm{otherwise}\\;.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "6: end for ", "page_idx": 15}, {"type": "text", "text": "A.5 Smooth and strongly convex online gradient descent ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Recall that to exploit smoothness, we enhance the expert-loss for strongly convex functions as follows ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\widehat{\\ell}_{t,\\widehat{\\lambda}}^{\\mathrm{sc}}(\\mathbf{y})=\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}-\\mathbf{y}_{t}\\rangle+\\frac{\\widehat{\\lambda}}{2G^{2}}\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|^{2}\\|\\mathbf{y}-\\mathbf{x}_{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The above expert-loss enjoys the following property. ", "page_idx": 16}, {"type": "text", "text": "Lemma 7 Under Assumptions $^{\\,l}$ and 2, $\\widehat{\\ell}_{t,\\widehat{\\lambda}}^{\\mathrm{sc}}(\\cdot)$ in (28) is $\\frac{\\widehat{\\lambda}}{G^{2}}\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|^{2}$ -strongly convex, and $\\begin{array}{r}{\\|\\widehat{\\ell}_{t,\\widehat{\\lambda}}^{\\mathrm{sc}}(\\mathbf{y})\\|^{2}\\leq\\left(1+\\frac{2D}{G}\\right)^{2}\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|^{2}.}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "Due to the modulus of strong convexity is not fixed, we choose Smooth and Strongly Convex sOuGmDm $(\\mathrm{S^{2}O G D})$ Aalsg tohriet hexmp 7e.rt-algorithm [Wang et al., 2020b] to minimize $\\widehat{\\ell}_{t,\\widehat{\\lambda}}^{\\mathrm{sc}}(\\cdot)$ . The procedure is ", "page_idx": 16}, {"type": "text", "text": "Algorithm 7 Expert $E^{i}$ : Smooth and Strongly Convex OGD ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1: Let $\\mathbf{y}_{1}^{i}$ be any point in $\\boldsymbol{\\wp}$   \n2: for $t=1$ to $T$ do   \n3: Submit $\\mathbf{y}_{t}^{i}$ to the meta-algorithm   \n4: Update ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\widehat{\\mathbf{y}}_{t+1}^{i}=\\mathbf{y}_{t}^{i}-\\eta_{t}\\nabla g_{t}(\\mathbf{y}_{t})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\eta_{t}=\\frac{\\alpha}{\\delta+\\sum_{s=1}^{t}\\Vert\\nabla_{s,\\widehat{\\lambda}}^{\\widehat{\\ell}^{\\mathrm{sc}}}(\\mathbf{y}_{s}^{i})\\Vert^{2}},\\quad\\alpha,\\delta>0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "5: Conduct a projection onto $\\boldsymbol{\\wp}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{y}_{t+1}^{i}=\\left\\{\\begin{array}{l l}{\\widehat{\\mathbf{y}}_{t+1}^{i},}&{\\mathrm{if~}\\Vert\\widehat{\\mathbf{y}}_{t+1}^{i}\\Vert\\leq D,}\\\\ {\\widehat{\\mathbf{y}}_{t+1}^{i}\\cdot\\frac{D}{\\Vert\\widehat{\\mathbf{y}}_{t+1}^{i}\\Vert},}&{\\mathrm{otherwise}\\;.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "6: end for ", "page_idx": 16}, {"type": "text", "text": "B Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we provide the proofs of the theorems presented in the main paper (Theorem 1 and Theorem 2), as well as proofs of two important lemmas (Lemma 3 and Lemma 8). ", "page_idx": 16}, {"type": "text", "text": "B.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We present the exact bounds of the theoretical guarantee provided in Theorem 1. When functions are general convex, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{t=1}^{T}f_{t}(\\mathbf{x}_{t})-\\sum_{t=1}^{T}f_{t}(\\mathbf{x})\\leq4\\Gamma G D\\left(2+\\frac{1}{\\sqrt{\\ln|A|}}\\right)+\\left(\\frac{2\\Gamma G D}{\\sqrt{\\ln|A|}}+2D^{2}+G^{2}\\right)\\sqrt{T}-\\frac{G^{2}}{2}}}\\\\ &{}&{=O(\\sqrt{T})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $|A|=1+2{\\lceil}\\log_{2}T{\\rceil}$ and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Gamma=3\\ln|A|+\\ln\\left(1+{\\frac{|A|}{2e}}(1+\\ln(T+1))\\right)={\\cal O}(\\log\\log T).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "When functions are $\\alpha$ -exp-concave, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=1}^{T}f_{t}(\\mathbf{x}_{t})-\\sum_{t=1}^{T}f_{t}(\\mathbf{x})\\leq4\\Gamma G D\\left(2+\\frac{1}{\\sqrt{\\ln|A|}}\\right)+\\frac{\\Gamma^{2}}{\\beta\\ln|A|}+5\\left(\\frac{8}{\\beta}+2\\sqrt{2}G D\\right)d\\log T}\\\\ {\\displaystyle\\qquad\\qquad\\qquad=O\\left(\\frac{d}{\\alpha}\\log T\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "When functions are $\\lambda$ -strongly convex, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=1}^{T}f_{t}(\\mathbf{x}_{t})-\\sum_{t=1}^{T}f_{t}(\\mathbf{x})\\leq4\\Gamma G D\\left(2+\\frac{1}{\\sqrt{\\ln|A|}}\\right)+\\frac{\\Gamma^{2}G^{2}}{\\operatorname*{min}\\{\\frac{G}{D},\\lambda\\}\\ln|A|}+\\frac{(G+D)^{2}}{\\lambda}\\log T}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad=O\\left(\\frac{1}{\\lambda}\\log T\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "B.1.1 Analysis for general convex functions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We introduce the following decomposition for general convex functions, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}f_{t}(\\mathbf x_{t})-\\sum_{t=1}^{T}f_{t}(\\mathbf x)\\leq\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf x_{t}),\\mathbf x_{t}-\\mathbf x\\rangle\\overset{()}{\\leq}\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf y_{t}),\\mathbf y_{t}-\\mathbf x\\rangle}\\\\ {=\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf y_{t}),\\mathbf y_{t}-\\mathbf y_{t}^{i}\\rangle+\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf y_{t}),\\mathbf y_{t}^{i}-\\mathbf x\\rangle}\\\\ {\\overset{(24)}{=}\\underbrace{\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf y_{t}),\\mathbf y_{t}-\\mathbf y_{t}^{i}\\rangle}_{\\mathrm{meta\\!\\!}_{x}\\mathrm{orret}}+\\sum_{t=1}^{T}\\left(\\ell_{t}^{\\mathrm{vx}}(\\mathbf y_{t}^{i})-\\ell_{t}^{\\mathrm{vx}}(\\mathbf x)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "First, we start with the expert-regret. Since we are employing OGD to minimize $\\ell_{t}^{\\mathrm{cvx}}(\\cdot)$ , using standard OGD analysis [Zinkevich, 2003, Theorem 1] can obtain the following upper bound ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\ell_{t}^{\\mathrm{cvx}}(\\mathbf{y}_{t}^{i})-\\sum_{t=1}^{T}\\ell_{t}^{\\mathrm{cvx}}(\\mathbf{x})\\leq(2D^{2}+G^{2})\\sqrt{T}-\\frac{G^{2}}{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for any expert $\\mathbf{y}_{t}^{i}\\in\\mathcal{Y}$ and any $\\mathbf{x}\\in\\mathcal{X}$ . ", "page_idx": 17}, {"type": "text", "text": "Next, we move to bound the meta-regret. According to (48), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{y}_{t}^{i}\\rangle\\leq8\\Gamma G D+\\frac{\\Gamma}{\\sqrt{\\ln\\left|A\\right|}}\\sqrt{\\ln(6G^{2}D^{2}+\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{y}_{t}^{i}\\rangle^{2}}}\\\\ {\\displaystyle\\leq4\\Gamma G D\\left(2+\\frac{1}{\\sqrt{\\ln\\left|A\\right|}}\\right)+\\frac{\\Gamma}{\\sqrt{\\ln\\left|A\\right|}}\\sqrt{\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{y}_{t}^{i}\\rangle^{2}}}\\\\ {\\displaystyle\\leq4\\Gamma G D\\left(2+\\frac{1}{\\sqrt{\\ln\\left|A\\right|}}\\right)+\\frac{\\Gamma}{\\sqrt{\\ln\\left|A\\right|}}\\sqrt{\\displaystyle\\sum_{t=1}^{T}\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|^{2}\\|\\mathbf{y}_{t}-\\mathbf{y}_{t}^{i}\\|^{2}}}\\\\ {\\displaystyle}\\\\ {\\displaystyle\\leq4\\Gamma G D\\left(2+\\frac{1}{\\sqrt{\\ln\\left|A\\right|}}\\right)+\\frac{2\\Gamma G D}{\\sqrt{\\ln\\left|A\\right|}}\\sqrt{T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for all expert $E^{i}\\in A$ , where $\\Gamma$ is defined in (30) and the last set is due to ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|\\leq\\|\\nabla f_{t}(\\mathbf{x}_{t})\\|\\leq G.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Finally, substituting (32) and (33) into (31), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}f_{t}(\\mathbf{x}_{t})-\\sum_{t=1}^{T}f_{t}(\\mathbf{x})\\leq4\\Gamma G D\\left(2+\\frac{1}{\\sqrt{\\ln|A|}}\\right)+\\left(\\frac{2\\Gamma G D}{\\sqrt{\\ln|A|}}+2D^{2}+G^{2}\\right)\\sqrt{T}-\\frac{G^{2}}{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "B.1.2 Analysis for exp-concave functions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For $\\alpha$ -exp-concave functions, there exits $\\widehat{\\alpha}^{*}\\in\\mathcal{P}_{\\mathrm{exp}}$ that $\\widehat\\alpha^{*}\\leq\\alpha\\leq2\\widehat\\alpha^{*}$ , where $\\widehat{\\alpha}^{*}$ is the modulus of the $i$ -th expert $E^{i}$ . This inequality also indicates ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\widehat\\beta}^{*}\\leq\\beta\\leq2{\\widehat\\beta}^{*},\\quad{\\widehat\\beta}^{*}={\\frac{1}{2}}\\operatorname*{min}\\{{\\frac{1}{4G D}},{\\widehat\\alpha}^{*}\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $x-\\frac{\\widehat{\\beta}^{*}}{2}x^{2}$ is strictly increasing where $\\begin{array}{r}{\\widehat{\\beta}^{*}\\,=\\,\\frac{1}{2}\\operatorname*{min}\\{\\frac{1}{4G D},\\widehat{\\alpha}^{*}\\}}\\end{array}$ when $x\\in(-\\infty,2G D]$ , (9) implies that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}\\rangle-\\frac{\\widehat{\\beta}^{*}}{2}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}\\rangle^{2}\\leq\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{x}\\rangle-\\frac{\\widehat{\\beta}^{*}}{2}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{x}\\rangle^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, we introduce the following decomposition for $\\alpha$ -exp-concave functions, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}f(\\mathbf{x}_{t})-\\sum_{t=1}^{T}f_{t}(\\mathbf{x})\\leq\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}\\rangle-\\frac{\\beta}{2}\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}\\rangle^{2}}\\\\ &{\\le\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}\\rangle-\\frac{\\beta^{*}}{2}\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}\\rangle^{2}}\\\\ &{\\overset{(a)}{\\leq}\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{x}\\rangle-\\frac{\\beta^{*}}{2}\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{x}\\rangle^{2}}\\\\ &{=\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{y}_{t}^{*}\\rangle+\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}^{*}-\\mathbf{x}\\rangle-\\frac{\\beta^{*}}{2}\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{x}\\rangle^{2}}\\\\ &{\\overset{(a)}{\\leq}\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{y}_{t}^{*}\\rangle+\\displaystyle\\sum_{t=1}^{T}\\left(\\varepsilon_{t,t}^{\\mathrm{s}},(\\mathbf{y}_{t}^{*})-\\ell_{t,t}^{\\mathrm{s}},(\\mathbf{x})\\right)-\\frac{\\beta^{*}}{2}\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{y}_{t}^{*}\\rangle^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For the expert-regret, we can use the analysis of ONS [Hazan et al., 2007, Theorem 2] to obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\ell_{t,\\widehat{\\alpha}^{*}}^{\\mathrm{exp}}(\\mathbf{y}_{t}^{i})-\\sum_{t=1}^{T}\\ell_{t,\\widehat{\\alpha}^{*}}^{\\mathrm{exp}}(\\mathbf{x})\\leq5\\left(\\frac{4}{\\widehat{\\beta}^{*}}+2\\sqrt{2}G D\\right)d\\log T\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for any expert $\\mathbf{y}_{t}^{i}\\in\\mathcal{Y}$ and any $\\mathbf{x}\\in\\mathcal{X}$ , where ${\\widehat{\\beta}}^{*}$ is defined in (35). Next, we move to bound the meta-regret. According to (48), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{y}_{t}^{i}\\rangle\\leq8\\Gamma G D+\\frac{\\Gamma}{\\sqrt{\\ln{|\\cal{A}|}}}\\sqrt{16G^{2}D^{2}+\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{y}_{t}^{i}\\rangle^{2}}}\\\\ {\\displaystyle\\leq4\\Gamma G D\\left(2+\\frac{1}{\\sqrt{\\ln{|\\cal{A}|}}}\\right)+\\frac{\\Gamma}{\\sqrt{\\ln{|\\cal{A}|}}}\\sqrt{\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{y}_{t}^{i}\\rangle^{2}}}\\\\ {\\displaystyle\\leq4\\Gamma G D\\left(2+\\frac{1}{\\sqrt{\\ln{|\\cal{A}|}}}\\right)+\\frac{\\Gamma^{2}}{2\\hat{\\beta}^{*}\\ln{|\\cal{A}|}}+\\frac{\\hat{\\beta}^{*}}{2}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{y}_{t}^{i}\\rangle^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for all expert $E^{i}\\in A$ , where $\\Gamma$ is defined in (30) and the last step is due to ${\\sqrt{a b}}\\leq{\\frac{a}{2}}+{\\frac{b}{2}}$ . Substituting (38) and (39) into (37), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}f_{t}(\\mathbf x_{t})-\\sum_{t=1}^{T}f_{t}(\\mathbf x)\\leq4\\Gamma G D\\left(2+\\frac{1}{\\sqrt{\\ln|A|}}\\right)+\\frac{\\Gamma^{2}}{2\\hat{\\beta}^{*}\\ln|A|}+5\\left(\\frac{4}{\\hat{\\beta}^{*}}+2\\sqrt{2}G D\\right)d\\log T.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Finally, we use (35) to simplify the above bound. ", "page_idx": 18}, {"type": "text", "text": "B.1.3 Analysis for strongly convex functions ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For $\\lambda$ -strongly convex functions, there exits $\\widehat{\\lambda}^{*}\\in\\mathcal{P}_{\\mathrm{sc}}$ that $\\widehat{\\lambda}^{*}\\leq\\lambda\\leq2\\widehat{\\lambda}^{*}$ , where $\\widehat{\\lambda}^{*}$ is the modulus of the $i$ -th expert $E^{i}$ . Then, we introduce the f ollowing deco mposition for $\\lambda$ -stron gly convex functions ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}f_{t}(\\mathbf{x}_{t})-\\sum_{t=1}^{T}f_{t}(\\mathbf{x})\\leq\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}\\rangle-\\frac{\\lambda}{2}\\sum_{t=1}^{T}\\|\\mathbf{x}_{t}-\\mathbf{x}\\|^{2}}\\\\ &{\\le\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}\\rangle-\\frac{\\lambda^{*}}{2}\\sum_{t=1}^{T}\\|\\mathbf{x}_{t}-\\mathbf{x}\\|^{2}}\\\\ &{\\displaystyle\\overset{(20)}{\\leq}\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{x}\\rangle-\\Delta_{T}-\\frac{\\lambda^{*}}{2}\\sum_{t=1}^{T}\\|\\mathbf{x}_{t}-\\mathbf{x}\\|^{2}}\\\\ &{\\displaystyle\\overset{(26)}{\\leq}\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{y}_{t}^{*}\\rangle+\\sum_{t=1}^{T}\\Big(\\ell_{t,\\lambda}^{\\mathrm{'c}}(\\mathbf{y}_{t}^{*})-\\ell_{t,\\lambda}^{\\mathrm{'c}}(\\mathbf{x})\\Big)-\\frac{\\widehat{\\lambda}^{*}}{2}\\sum_{t=1}^{T}\\|\\mathbf{y}_{t}^{*}-\\mathbf{x}_{t}\\|^{2}-\\Delta_{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\begin{array}{r}{\\Delta_{T}=\\sum_{t=1}^{T}\\mathbb{1}_{\\left\\{\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{v}_{t}\\rangle\\geq0\\right\\}}\\cdot\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{y}_{t}-\\mathbf{x}_{t}\\rangle}\\end{array}$ . To bound the meta-regret, we derive the following the oretical guarantee. ", "page_idx": 19}, {"type": "text", "text": "Lemma 8 Under Assumptions $^{\\,l}$ and 2, the meta-regret of Algorithm 2 satisfies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{y}_{t}^{i}\\rangle\\leq8\\Gamma G D+\\frac{\\Gamma}{\\sqrt{\\ln{|\\cal A|}}}\\sqrt{16G^{2}D^{2}+\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{y}_{t}^{i}\\rangle^{2}}}\\\\ &{\\leq4\\Gamma G D\\left(2+\\frac{1}{\\sqrt{\\ln{|\\cal A|}}}\\right)+\\frac{\\Gamma^{2}G^{2}}{2\\gamma\\ln{|\\cal A|}}+\\frac{\\gamma}{2G^{2}}\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{x}_{t}-\\mathbf{y}_{t}^{i}\\rangle^{2}+\\Delta_{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for any $\\gamma\\in(0,\\frac{G}{2D}]$ , where $\\begin{array}{r}{\\Delta_{T}=\\sum_{t=1}^{T}\\mathbb{1}_{\\left\\{\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{v}_{t}\\rangle\\geq0\\right\\}}\\cdot\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{y}_{t}-\\mathbf{x}_{t}\\rangle}\\end{array}$ and $\\Gamma$ is in (30). ", "page_idx": 19}, {"type": "text", "text": "Remark 6 As mentioned in Section 3.2, Lemma 8 is pivotal in delivering optimal regret for strongly convex functions. Specifically, when the meta-algorithm enjoys a second-order bound in terms of the surrogate loss in (8), we can then convert the intermediate decision $\\mathbf{y}_{t}$ in the meta-regret bound to the actual one $\\mathbf{x}_{t}$ , at the cost of adding an addition positive term, as presented in the analysis in (23). \u25c1 ", "page_idx": 19}, {"type": "text", "text": "Combining Lemma 8 with (40) , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}f_{t}(\\mathbf x_{t})-\\sum_{t=1}^{T}f_{t}(\\mathbf x)}\\\\ &{\\le4\\Gamma G D\\left(2+\\frac{1}{\\sqrt{\\ln|A|}}\\right)+\\frac{\\Gamma^{2}G^{2}}{2\\gamma\\ln|A|}+\\frac{\\gamma}{2G^{2}}\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf y_{t}),\\mathbf x_{t}-\\mathbf y_{t}^{t}\\rangle^{2}}\\\\ &{\\quad+\\mathbb{E}\\mathbb{R}(T)-\\displaystyle\\frac{\\hat{\\lambda}^{\\star}}{2}\\sum_{t=1}^{T}\\|\\mathbf y_{t}^{t}-\\mathbf x_{t}\\|^{2}}\\\\ &{\\overset{(34)}{\\le}4\\Gamma G D\\left(2+\\frac{1}{\\sqrt{\\ln|A|}}\\right)+\\frac{\\Gamma^{2}G^{2}}{2\\gamma\\ln|A|}+\\left(\\frac{\\gamma}{2}-\\frac{\\hat{\\lambda}^{\\star}}{2}\\right)\\displaystyle\\sum_{t=1}^{T}\\|\\mathbf x_{t}-\\mathbf y_{t}^{t}\\|^{2}+\\mathbb{E}\\mathbb{R}(T)}\\\\ &{\\le4\\Gamma G D\\left(2+\\frac{1}{\\sqrt{\\ln|A|}}\\right)+\\frac{\\Gamma^{2}G^{2}}{2\\gamma\\ln|A|}+\\mathbb{E}\\mathbb{R}(T)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathrm{ER}(T)=\\sum_{t=1}^{T}(\\ell_{t,\\hat{\\lambda}^{*}}^{\\mathrm{sc}}(\\mathbf{y}_{t}^{i})-\\ell_{t,\\hat{\\lambda}^{*}}^{\\mathrm{sc}}(\\mathbf{x}))}\\end{array}$ and the last step is because we set $\\begin{array}{r}{\\gamma=\\operatorname*{min}\\{\\frac{G}{2D},\\widehat{\\lambda}^{*}\\}}\\end{array}$ Next, we bound the expert-regret [Shalev-Shwartz et al., 2011, Lemma 1] ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname{ER}(T)=\\sum_{t=1}^{T}\\ell_{t,\\widehat{\\lambda}^{*}}^{\\mathrm{sc}}(\\mathbf{y}_{t}^{i})-\\sum_{t=1}^{T}\\ell_{t,\\widehat{\\lambda}^{*}}^{\\mathrm{sc}}(\\mathbf{x})\\leq\\frac{(G+D)^{2}}{2\\widehat{\\lambda}^{*}}\\log T.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for any expert $\\mathbf{y}_{t}^{i}\\in\\mathcal{Y}$ and any $\\mathbf{x}\\in\\mathcal{X}$ . Substituting (42) into (41), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}f_{t}(\\mathbf{x}_{t})-\\sum_{t=1}^{T}f_{t}(\\mathbf{x})\\leq4\\Gamma G D\\left(2+\\frac{1}{\\sqrt{\\ln|A|}}\\right)+\\frac{\\Gamma^{2}G^{2}}{2\\gamma\\ln|A|}+\\frac{(G+D)^{2}}{2\\hat{\\lambda}^{*}}\\log T.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Finally, we use $\\widehat{\\lambda}^{*}\\leq\\lambda\\leq2\\widehat{\\lambda}^{*}$ to simplify the above bound. ", "page_idx": 20}, {"type": "text", "text": "B.2 Proof of Lemma 3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "According to (8), the (sub-)gradients of $g_{t}(\\cdot)$ can be formulated as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nabla g_{t}(\\mathbf{y})=\\left\\{\\begin{array}{l l}{\\nabla f_{t}(\\mathbf{x}_{t}),}&{\\mathrm{if}\\,\\,\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{v}_{t}\\rangle\\ge0,}\\\\ {\\nabla f_{t}(\\mathbf{x}_{t})-\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{v}_{t}\\rangle\\cdot\\frac{\\mathbf{y}-\\Pi_{X}[\\mathbf{y}]}{\\|\\mathbf{y}-\\Pi_{X}[\\mathbf{y}]\\|},}&{\\mathrm{if}\\,\\,\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{v}_{t}\\rangle<0.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "(i) When $\\langle\\nabla f_{t}({\\mathbf{x}}_{t}),{\\mathbf{v}}_{t}\\rangle\\ge0$ . We have $g_{t}(\\mathbf{y})=\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{y}\\rangle$ and $\\nabla g_{t}(\\mathbf{y})=\\nabla f_{t}(\\mathbf{x}_{t})$ . Thus, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\langle\\nabla f_{t}({\\bf x}_{t}),{\\bf x}_{t}-{\\bf x}\\rangle=\\langle\\nabla g_{t}({\\bf y}_{t}),{\\bf y}_{t}-{\\bf x}\\rangle-\\langle\\nabla f_{t}({\\bf x}_{t}),{\\bf y}_{t}-{\\bf x}_{t}\\rangle.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By the definition of $\\mathbf{v}_{t}=(\\mathbf{y}_{t}-\\mathbf{x}_{t})/\\|\\mathbf{y}_{t}-\\mathbf{x}_{t}\\|$ , we have $\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}\\rangle\\leq\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{y}_{t}\\rangle$ and thus ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{x}_{t}\\rangle\\leq\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}\\rangle\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "(ii) When $\\langle\\nabla f_{t}({\\mathbf x}_{t}),{\\mathbf v}_{t}\\rangle<0$ . According to Lemma 2, we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\langle\\nabla f_{t}({\\mathbf{x}}_{t}),{\\mathbf{x}}_{t}-{\\mathbf{x}}\\rangle\\leq\\langle\\nabla g_{t}({\\mathbf{y}}_{t}),{\\mathbf{y}}_{t}-{\\mathbf{x}}\\rangle.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Moreover, we derive the following equation ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\langle\\nabla g_{t}(\\mathbf y_{t}),\\mathbf y_{t}-\\mathbf x_{t}\\rangle=\\langle\\nabla f_{t}(\\mathbf x_{t}),\\mathbf y_{t}-\\mathbf x_{t}\\rangle-\\langle\\nabla f_{t}(\\mathbf x_{t}),\\mathbf v_{t}\\rangle\\cdot\\langle\\mathbf v_{t},\\mathbf y_{t}-\\mathbf x_{t}\\rangle}\\\\ &{=\\langle\\nabla f_{t}(\\mathbf x_{t}),\\mathbf y_{t}-\\mathbf x_{t}\\rangle-\\langle\\nabla f_{t}(\\mathbf x_{t}),\\mathbf y_{t}-\\mathbf x_{t}\\rangle\\cdot\\frac{1}{\\left\\Vert\\mathbf y_{t}-\\mathbf x_{t}\\right\\Vert}\\left\\langle\\frac{\\mathbf y_{t}-\\mathbf x_{t}}{\\left\\Vert\\mathbf y_{t}-\\mathbf x_{t}\\right\\Vert},\\mathbf y_{t}-\\mathbf x_{t}\\right\\rangle=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Finally, combining (44) and (46) obtains (20), further combining (45) and (47) yields (21). ", "page_idx": 20}, {"type": "text", "text": "B.3 Proof of Lemma 8 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "By the regret guarantee of Adapt-ML-Prod [Gaillard et al., 2014, Corollary 4], we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\left(\\ell_{t}-\\ell_{t}^{i}\\right)\\leq2\\Gamma+\\frac{\\Gamma}{\\sqrt{\\ln{|A|}}}\\sqrt{1+\\sum_{t=1}^{T}\\left(\\ell_{t}-\\ell_{t}^{i}\\right)^{2}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for all expert $E^{i}\\in A$ , where $\\Gamma=3\\ln\\vert A\\vert+\\ln(1+{\\textstyle{\\frac{\\vert A\\vert}{2e}}}(1+\\ln(T+1)))={\\cal O}(\\log\\log T)$ . By the definition of $\\ell_{t}$ and $\\ell_{t}^{i}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf y_{t}),\\mathbf y_{t}-\\mathbf y_{t}^{i}\\rangle\\leq8\\Gamma G D+\\frac{\\Gamma}{\\sqrt{\\ln{|\\cal A|}}}\\sqrt{16G^{2}D^{2}+\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf y_{t}),\\mathbf y_{t}-\\mathbf y_{t}^{i}\\rangle^{2}}}\\\\ {\\leq4\\Gamma G D\\left(2+\\frac{1}{\\sqrt{\\ln{|\\cal A|}}}\\right)+\\frac{\\Gamma^{2}G^{2}}{2\\gamma\\ln{|\\cal A|}}+\\frac{\\gamma}{2G^{2}}\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf y_{t}),\\mathbf y_{t}-\\mathbf y_{t}^{i}\\rangle^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for any $\\gamma>0$ , where the last step uses AM-GM inequality. ", "page_idx": 20}, {"type": "text", "text": "Next, we handle the term $\\langle\\nabla g_{t}(\\mathbf y_{t}),\\mathbf y_{t}-\\mathbf y_{t}^{i}\\rangle^{2}$ . We will consider two cases separately. ", "page_idx": 20}, {"type": "text", "text": "(i) When $\\langle\\nabla f_{t}({\\mathbf{x}}_{t}),{\\mathbf{v}}_{t}\\rangle\\ge0$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{y}_{t}^{i}\\rangle\\leq\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{y}_{t}-\\mathbf{y}_{t}^{i}\\rangle\\leq\\|\\nabla f_{t}(\\mathbf{x}_{t})\\|\\|\\mathbf{y}_{t}-\\mathbf{y}_{t}^{i}\\|\\leq2G D.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "As the function $\\begin{array}{r}{q(x)=x-\\frac{\\gamma}{2G^{2}}x^{2}}\\end{array}$ is strictly increasing when $\\begin{array}{r}{x\\in(-\\infty,\\frac{G^{2}}{\\gamma}]}\\end{array}$ , (49) implies that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{y}_{t}^{i}\\rangle-\\frac{\\gamma}{2G^{2}}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{y}_{t}^{i}\\rangle^{2}\\leq\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{y}_{t}-\\mathbf{y}_{t}^{i}\\rangle-\\frac{\\gamma}{2G^{2}}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{y}_{t}-\\mathbf{y}_{t}^{i}\\rangle^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for any $\\gamma\\in(0,\\frac{G}{2D}]$ . By rearranging terms, we obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\gamma}{2G^{2}}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{y}_{t}^{i}\\rangle^{2}\\overset{(43)}{=}\\frac{\\gamma}{2G^{2}}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{y}_{t}-\\mathbf{y}_{t}^{i}\\rangle^{2}}\\\\ &{\\le\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{y}_{t}-\\mathbf{x}_{t}\\rangle+\\displaystyle\\frac{\\gamma}{2G^{2}}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{y}_{t}^{i}\\rangle^{2}}\\\\ &{\\displaystyle\\overset{(43)}{=}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{y}_{t}-\\mathbf{x}_{t}\\rangle+\\displaystyle\\frac{\\gamma}{2G^{2}}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{x}_{t}-\\mathbf{y}_{t}^{i}\\rangle^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "(ii) When $\\langle\\nabla f_{t}({\\mathbf x}_{t}),\\mathbf v_{t}\\rangle<0,$ , (47) directly implies $\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{x}_{t}-\\mathbf{y}_{t}^{i}\\rangle=\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{y}_{t}^{i}\\rangle$ . Thus, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\gamma}{2G^{2}}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{y}_{t}^{i}\\rangle^{2}=\\frac{\\gamma}{2G^{2}}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{x}_{t}-\\mathbf{y}_{t}^{i}\\rangle^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Combining (50) and (51), we have ", "page_idx": 21}, {"type": "text", "text": "$\\frac{\\gamma}{2G^{2}}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}_{t}-\\mathbf{y}_{t}^{i}\\rangle^{2}\\leq\\mathbb{1}_{\\{\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{v}_{t}\\rangle\\geq0\\}}\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{y}_{t}-\\mathbf{x}_{t}\\rangle+\\frac{\\gamma}{2G^{2}}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{x}_{t}-\\mathbf{y}_{t}^{i}\\rangle^{2}$ (52) for any $\\gamma\\in(0,\\frac{G}{2D}]$ . Substituting (52) into (48), we finish the proof. ", "page_idx": 21}, {"type": "text", "text": "B.4 Proof of Theorem 2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The analysis is similar to Theorem 1. Also, we present the exact bounds of the theoretical guarantee provided in Theorem 2. When functions are general convex, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}f_{t}(\\mathbf{x}_{t})-\\sum_{t=1}^{T}f_{t}(\\mathbf{x})}\\\\ &{\\le4\\Gamma G D\\left(2+\\frac{1}{\\sqrt{\\ln|A|}}\\right)+\\sqrt{2D^{2}\\delta}+4H\\left(\\frac{2\\Gamma D}{\\sqrt{\\ln|A|}}+\\sqrt{2}(D+2G)\\right)^{2}}\\\\ &{\\quad+\\displaystyle2\\sqrt{H}\\left(\\frac{2\\Gamma D}{\\sqrt{\\ln|A|}}+\\sqrt{2}(D+2G)\\right)\\sqrt{L_{T}+4\\Gamma G D\\left(2+\\frac{1}{\\sqrt{\\ln|A|}}\\right)+\\sqrt{2D^{2}\\delta}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n=O(\\sqrt{L_{T}})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $|A|=1+2{\\lceil}\\log_{2}T{\\rceil}$ , $\\Gamma$ is defined in (30), and $\\begin{array}{r}{L_{T}=\\operatorname*{min}_{\\mathbf{x}\\in\\mathcal{X}}\\sum_{t=1}^{T}f_{t}(\\mathbf{x})}\\end{array}$ . When functions are $\\alpha$ -exp-concave, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathrm{e}^{-\\mathrm{i}k_{\\parallel}\\cdot\\mathrm{e}_{i}(x)}-\\mathrm{e}^{-\\mathrm{i}k_{\\parallel}(x)}}{\\sqrt{\\mathrm{i}k_{\\parallel}(x)}}}\\\\ &{\\quad\\le\\mathrm{e}^{-\\mathrm{i}k_{\\parallel}\\cdot\\mathrm{e}_{i}(x)}-\\frac{\\Gamma}{2k_{\\parallel}k_{\\parallel}}(\\sin{\\frac{\\pi}{2k_{\\parallel}}}+\\frac{2k_{\\parallel}}{3}\\cos{\\left(\\frac{\\sqrt{\\mathrm{i}k_{\\parallel}\\cdot\\mathrm{e}_{i}(x)}}{k_{\\parallel}}\\right)}\\frac{\\Gamma}{\\Gamma}/\\left(\\sin{\\frac{\\pi}{2k_{\\parallel}}}\\right)+1\\right)+\\frac{2}{\\beta_{0}}}\\\\ &{\\quad\\le\\mathrm{F}+\\frac{2k_{\\parallel}}{\\mathcal{N}}\\ln{\\left(\\frac{2\\pi/2+\\sqrt{\\mathrm{i}k_{\\parallel}\\cdot\\mathrm{e}_{i}(x)}}{\\sqrt{\\mathrm{i}k_{\\parallel}(x)}}\\frac{\\Gamma}{\\Gamma}\\right)}\\bigl[(\\sin{\\frac{\\pi}{2k_{\\parallel}}})^{2}-\\frac{2\\sqrt{\\mathrm{i}k_{\\parallel}}\\cdot\\mathrm{e}_{i}(x)^{2}\\pi/\\Gamma}{\\sqrt{\\mathrm{i}k_{\\parallel}}}+2\\mathcal{D}^{2}\\mathrm{F}\\bigr.\\bigr.}\\\\ &{\\quad\\left.-(\\frac{2}{\\sin{\\frac{\\pi}{2k_{\\parallel}}}\\pi/\\Gamma})\\right]\\sin{\\frac{\\pi}{2k_{\\parallel}}}}\\\\ &{\\quad\\hat{F}=\\mathrm{d}\\alpha\\mathrm{D}\\left(2+\\frac{1}{\\sqrt{\\mathrm{i}k_{\\parallel}\\cdot\\mathrm{e}_{i}(x)}}\\right)+\\frac{\\mathrm{e}^{\\mathrm{i}k_{\\parallel}\\cdot\\mathrm{e}_{i}(x)}-4\\sqrt{\\mathrm{i}k_{\\parallel}\\cdot\\mathrm{e}_{i}(x)}\\cos{\\alpha}\\cdot\\sin{\\alpha}\\cdot\\sin{\\beta}\\mathrm{e}^{\\mathrm{i}k_{\\parallel}(x)}\\cos{\\alpha}\\cdot\\sin{\\beta}\\mathrm{e}^{-\\mathrm{i}k_{\\parallel}\\cdot\\mathrm{e}_{i}(x)}}{\\sqrt{\\mathrm{i}k_{\\parallel}(x)}}}\\\\ &{\\quad\\overset{\\le}\\frac{\\Gamma}{\\Gamma}\\frac{\\\\,k_{\\parallel}(x)}{\\Gamma}\\frac{\\sqrt{1}}{\\sin{\\frac{\\pi}{2k_{\\parallel}}}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "B.4.1 Analysis for general convex functions ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We start with the meta-expert regret decomposition as presented in (31), ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}f_{t}(\\mathbf x_{t})-\\sum_{t=1}^{T}f_{t}(\\mathbf x)\\leq\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf y_{t}),\\mathbf y_{t}-\\mathbf y_{t}^{i}\\rangle+\\sum_{t=1}^{T}\\left(\\ell_{t}^{\\mathrm{cvx}}(\\mathbf y_{t}^{i})-\\ell_{t}^{\\mathrm{cvx}}(\\mathbf x)\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For the meta-regret, we reuse (33) to obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf y_{t}),\\mathbf y_{t}-\\mathbf y_{t}^{i}\\rangle\\leq4\\Gamma G D\\left(2+\\frac{1}{\\sqrt{\\ln|A|}}\\right)+\\frac{\\Gamma}{\\sqrt{\\ln|A|}}\\sqrt{\\displaystyle\\sum_{t=1}^{T}\\|\\nabla g_{t}(\\mathbf y_{t})\\|^{2}\\|\\mathbf y_{t}-\\mathbf y_{t}^{i}\\|^{2}}}}\\\\ &{}&{\\leq4\\Gamma G D\\left(2+\\frac{1}{\\sqrt{\\ln|A|}}\\right)+\\frac{2\\Gamma D}{\\sqrt{\\ln|A|}}\\sqrt{\\displaystyle\\sum_{t=1}^{T}\\|\\nabla g_{t}(\\mathbf y_{t})\\|^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for all expert $E^{i}\\in A$ , where $\\Gamma$ is defined in (30). For the expert-regret, we can use the analysis of SOGD [Zhang et al., 2019, Theorem 2] to obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\ell_{t}^{\\mathrm{cvx}}(\\mathbf{y}_{t}^{i})-\\sum_{t=1}^{T}\\ell_{t}^{\\mathrm{cvx}}(\\mathbf{x})\\leq\\sqrt{2D^{2}}\\sqrt{\\delta+\\left(1+\\frac{2G}{D}\\right)^{2}\\sum_{t=1}^{T}\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|^{2}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for any expert $\\mathbf{y}_{t}^{i}\\in\\mathcal{Y}$ and any $\\mathbf{x}\\in\\mathcal{X}$ . From the above formulation, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\ell_{t}^{\\mathrm{cvx}}(\\mathbf{y}_{t}^{i})-\\sum_{t=1}^{T}\\ell_{t}^{\\mathrm{cvx}}(\\mathbf{x})\\leq\\sqrt{2D^{2}\\delta}+\\sqrt{2\\left(D+2G\\right)^{2}\\sum_{t=1}^{T}\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|^{2}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Substituting (54) and (55) into (53), we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=1}^{T}f_{t}(\\mathbf x_{t})-\\sum_{t=1}^{T}f_{t}(\\mathbf x)}\\\\ {\\displaystyle\\overset{(34)}{\\leq}4\\Gamma G D\\left(2+\\frac{1}{\\sqrt{\\ln|A|}}\\right)+\\sqrt{2D^{2}\\delta}+\\left(\\frac{2\\Gamma D}{\\sqrt{\\ln|A|}}+\\sqrt{2}(D+2G)\\right)\\sqrt{\\displaystyle\\sum_{t=1}^{T}\\|\\nabla f_{t}(\\mathbf x_{t})\\|^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Next, we introduce the self-bounding property of smooth functions. ", "page_idx": 22}, {"type": "text", "text": "Lemma 9 (Lemma 3.1 of Srebro et al. [2010]) For an $H$ -smooth and nonnegative function, we have $\\|\\nabla f(\\mathbf{x})\\|\\leq\\sqrt{4H f(\\mathbf{x})}$ . ", "page_idx": 22}, {"type": "text", "text": "Thus, when functions are smooth, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=1}^{T}f_{t}(\\mathbf x_{t})-\\sum_{t=1}^{T}f_{t}(\\mathbf x)}\\\\ {\\displaystyle\\Xi4\\Gamma G D\\left(2+\\frac{1}{\\sqrt{\\ln|A|}}\\right)+\\sqrt{2D^{2}\\delta}+\\left(\\frac{2\\Gamma D}{\\sqrt{\\ln|A|}}+\\sqrt{2}(D+2G)\\right)\\sqrt{4H\\displaystyle\\sum_{t=1}^{T}f_{t}(\\mathbf x_{t})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "To simplify the above inequality, we use the following lemma. ", "page_idx": 22}, {"type": "text", "text": "L\u221aemma 10 (Lemma 19 \u221aof Shalev-Shwartz [2007]) Let $\\boldsymbol{x},\\boldsymbol{b},\\boldsymbol{c}\\,\\in\\,\\mathbb{R}^{+}$ . Then, we have $x\\mathrm{~-~}c\\leq$ $b{\\sqrt{x}}\\Rightarrow x-c\\leq b^{2}+b{\\sqrt{c}}$ . ", "page_idx": 22}, {"type": "text", "text": "By utilizing Lemma 10, we finish the proof. ", "page_idx": 22}, {"type": "text", "text": "B.4.2 Analysis for exp-concave functions ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The analysis is also similar to Theorem 1. We start with (37) ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}f_{t}(\\mathbf x_{t})-\\sum_{t=1}^{T}f_{t}(\\mathbf x)}\\\\ &{\\leq\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf y_{t}),\\mathbf y_{t}-\\mathbf y_{t}^{i}\\rangle+\\sum_{t=1}^{T}\\Big(\\ell_{t,\\hat{\\alpha}^{*}}^{\\mathrm{exp}}(\\mathbf y_{t}^{i})-\\ell_{t,\\hat{\\alpha}^{*}}^{\\mathrm{exp}}(\\mathbf x)\\Big)-\\frac{\\widehat{\\beta}^{*}}{2}\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf y_{t}),\\mathbf y_{t}-\\mathbf y_{t}^{i}\\rangle^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For the meta-regret, we also use (39) to bound. For the expert-regret, we can use the analysis of ONS under the smoothness condition [Orabona et al., 2012, Theorem 1] to get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\ell_{t,\\widehat{\\alpha}}^{\\mathrm{exp}}(\\mathbf{y}_{t}^{i})-\\sum_{t=1}^{T}\\ell_{t,\\widehat{\\alpha}}^{\\mathrm{exp}}(\\mathbf{x})\\leq\\frac{2d}{\\widehat{\\beta}^{*}}\\log\\left(\\frac{\\widehat{\\beta}^{*^{2}}D^{2}}{16d}\\sum_{t=1}^{T}\\|\\nabla\\ell_{t,\\widehat{\\alpha}}^{\\mathrm{exp}}(\\mathbf{y}_{t}^{i})\\|^{2}+1\\right)+\\frac{2}{\\widehat{\\beta}^{*}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for any expert $\\mathbf{y}_{t}^{i}\\in\\mathcal{Y}$ and any $\\mathbf{x}\\in\\mathcal{X}$ . Next, we provide an upper bound for $\\|\\nabla\\ell_{t,\\widehat{\\alpha}}^{\\exp}(\\mathbf{y}_{t}^{i})\\|^{2}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\nabla\\ell_{t,\\hat{\\alpha}}^{\\mathrm{exp}}(\\mathbf{y}_{t}^{i})\\|^{2}}\\\\ &{=\\langle\\nabla g_{t}(\\mathbf{y}_{t})+\\widehat{\\beta}^{*}\\nabla g_{t}(\\mathbf{y}_{t})\\nabla g_{t}(\\mathbf{y}_{t})^{\\top}(\\mathbf{y}-\\mathbf{y}_{t}),\\nabla g_{t}(\\mathbf{y}_{t})+\\widehat{\\beta}^{*}\\nabla g_{t}(\\mathbf{y}_{t})\\nabla g_{t}(\\mathbf{y}_{t})^{\\top}(\\mathbf{y}-\\mathbf{y}_{t})\\rangle}\\\\ &{=\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|^{2}+2\\widehat{\\beta}^{*}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}-\\mathbf{y}_{t}\\rangle\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|^{2}+\\widehat{\\beta}^{*^{2}}\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|^{4}\\|\\mathbf{y}-\\mathbf{y}_{t}\\|^{2}}\\\\ &{\\leq\\Big(1+2\\widehat{\\beta}^{*^{2}}G D\\Big)^{2}\\,\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|^{2}\\leq4\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\ell_{t,\\widehat{\\alpha}}^{\\mathrm{exp}}(\\mathbf{y}_{t}^{i})-\\sum_{t=1}^{T}\\ell_{t,\\widehat{\\alpha}}^{\\mathrm{exp}}(\\mathbf{x})\\leq\\frac{2d}{\\widehat{\\beta}^{*}}\\log\\left(\\frac{\\widehat{\\beta}^{*^{2}}D^{2}}{4d}\\sum_{t=1}^{T}\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|^{2}+1\\right)+\\frac{2}{\\widehat{\\beta}^{*}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Substituting (39) and (57) into (56), we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}f_{t}(\\mathbf{x}_{t})-\\sum_{t=1}^{T}f_{t}(\\mathbf{x})}\\\\ &{\\le4\\Gamma G D\\left(2+\\frac{1}{\\sqrt{\\ln|A|}}\\right)+\\frac{\\Gamma^{2}}{2\\hat{\\beta}^{*}\\ln|A|}+\\frac{2d}{\\hat{\\beta}^{*}}\\log\\left(\\frac{\\hat{\\beta}^{*^{2}}D^{2}}{4d}\\sum_{t=1}^{T}\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|^{2}+1\\right)+\\frac{2}{\\hat{\\beta}^{*}}}\\\\ &{\\overset{(34)}{\\le}4\\Gamma G D\\left(2+\\frac{1}{\\sqrt{\\ln|A|}}\\right)+\\frac{\\Gamma^{2}}{2\\hat{\\beta}^{*}\\ln|A|}+\\frac{2d}{\\hat{\\beta}^{*}}\\log\\left(\\frac{\\hat{\\beta}^{*^{2}}D^{2}H}{d}\\sum_{t=1}^{T}f_{t}(\\mathbf{x}_{t})+1\\right)+\\frac{2}{\\hat{\\beta}^{*}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last step is due to Lemma 9. Finally, we use the following lemma to simplify the bound. ", "page_idx": 23}, {"type": "text", "text": "Lemma 11 (Corollary 5 of Orabona et al. [2012]) Let $a,b,c,d,x>0$ satisfy $x\\!-\\!d\\leq a\\ln(b x\\!+\\!c)$ .   \nThen, we have $\\begin{array}{r}{x-d\\leq a\\ln(2(a b\\ln\\frac{2a b}{\\mathrm{e}}+d b+c))}\\end{array}$ . ", "page_idx": 23}, {"type": "text", "text": "B.4.3 Analysis for strongly convex functions ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Recall that we construct the expert-loss for strongly convex functions as follows ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\widehat{\\ell}_{t,\\widehat{\\lambda}}^{\\mathrm{sc}}(\\mathbf{y})=\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}-\\mathbf{y}_{t}\\rangle+\\frac{\\widehat{\\lambda}^{*}}{2G^{2}}\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|^{2}\\|\\mathbf{y}-\\mathbf{x}_{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then, we introduce a new decomposition for $\\lambda$ -strongly convex functions ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}f_{t}(\\mathbf x_{t})-\\frac{\\Gamma}{\\displaystyle\\sum_{t=1}^{T}f_{t}(\\mathbf x)}\\leq\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf x_{t}),\\mathbf x_{t}-\\mathbf x\\rangle-\\frac{\\lambda}{2}\\sum_{t=1}^{T}\\|\\mathbf x_{t}-\\mathbf x\\|^{2}}\\\\ &{\\leq\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf x_{t}),\\mathbf x_{t}-\\mathbf x\\rangle-\\frac{\\hat{\\lambda}^{*}}{2}\\sum_{t=1}^{T}\\|\\mathbf x_{t}-\\mathbf x\\|^{2}}\\\\ &{\\leq\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla f_{t}(\\mathbf x_{t}),\\mathbf x_{t}-\\mathbf x\\rangle-\\frac{\\hat{\\lambda}^{*}}{2\\langle t\\rangle^{2}}\\sum_{t=1}^{T}\\|\\nabla g_{t}(\\mathbf x_{t})\\|^{2}\\|\\mathbf x_{t}-\\mathbf x\\|^{2}}\\\\ &{\\overset{(a)}{\\leq}\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf y_{t}),\\mathbf y_{t}-\\mathbf x\\rangle-\\Delta r-\\frac{\\hat{\\lambda}^{*}}{2G^{2}}\\sum_{t=1}^{T}\\|\\nabla g_{t}(\\mathbf y_{t})\\|^{2}\\|\\mathbf x_{t}-\\mathbf x\\|^{2}}\\\\ &{\\overset{(b)}{\\leq}\\displaystyle\\sum_{t=1}^{T}\\langle\\nabla g_{t}(\\mathbf y_{t}),\\mathbf y_{t}-\\mathbf y_{t}^{*}\\rangle+\\sum_{t=1}^{T}\\left(\\hat{\\ell}_{t,\\lambda}^{*},(\\mathbf y_{t}^{*})-\\hat{\\ell}_{t,\\lambda}^{*},(\\mathbf x)\\right)-\\frac{\\hat{\\lambda}^{*}}{2G^{2}}\\sum_{t=1}^{T}\\|\\nabla g_{t}(\\mathbf y_{t})\\|^{2}\\|\\mathbf x_{t}-\\mathbf y_{t}^{*}\\|^{2}-\\Delta\\tau}\\\\ &{\\overset{(c)}{\\leq}\\displaystyle\\sum_{t=1}^{T}\\frac{\\operatorname*{sup}_{t}\\nabla f_{t}(\\mathbf x_{t})}{\\operatorname*{sup}_{t}\\mathbf x_{t}-\\mathbf y_{t}^{*}\\|}+\\sum_{t=1}^{T}\\left(\\hat{\\ell}_{t,\\lambda}^{*},(\\mathbf y_{t \n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\begin{array}{r}{\\Delta_{T}\\,=\\,\\sum_{t=1}^{T}\\mathbb{1}_{\\left\\{\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{v}_{t}\\rangle\\ge0\\right\\}}\\cdot\\langle\\nabla f_{t}(\\mathbf{x}_{t}),\\mathbf{y}_{t}-\\mathbf{x}_{t}\\rangle,}\\end{array}$ . To bound the meta-regret, we still incorporate wi th Lemma 8 to get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}f_{t}(\\mathbf x_{t})-\\sum_{t=1}^{T}f_{t}(\\mathbf x)\\leq4\\Gamma G D\\left(2+\\frac{1}{\\sqrt{\\ln|A|}}\\right)+\\frac{\\Gamma^{2}G^{2}}{2\\gamma\\ln|A|}+\\sum_{t=1}^{T}\\left(\\widehat{\\ell}_{t,\\hat{\\lambda}^{*}}^{\\mathrm{sc}}(\\mathbf y_{t}^{i})-\\widehat{\\ell}_{t,\\hat{\\lambda}^{*}}^{\\mathrm{sc}}(\\mathbf x)\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For the expert-regret, we derive a variant of theoretical guarantee of $\\mathrm{S^{2}O G D}$ . ", "page_idx": 24}, {"type": "text", "text": "Lemma 12 Under Assumptions $^{\\,l}$ and 2, for any expert $\\mathbf{y}_{t}^{i}\\in\\mathcal{Y}$ and any $\\mathbf{x}\\in\\mathcal{X}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\widehat{\\ell}_{t,\\widehat{\\lambda}^{*}}^{\\mathrm{sc}}(\\mathbf{y}_{t}^{i})-\\sum_{t=1}^{T}\\widehat{\\ell}_{t,\\widehat{\\lambda}^{*}}^{\\mathrm{sc}}(\\mathbf{x})\\leq1+\\frac{(G+2D)^{2}}{2\\widehat{\\lambda}^{*}}\\log\\left(\\frac{\\widehat{\\lambda}^{*}}{(G+2D)^{2}}\\sum_{t=1}^{T}\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|^{2}+1\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Combining the above bounds, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}f_{t}(\\mathbf{x}_{t})-\\sum_{t=1}^{T}f_{t}(\\mathbf{x})}\\\\ &{\\leq\\!4\\Gamma G D\\left(2+\\frac{1}{\\sqrt{\\ln|A|}}\\right)+\\frac{\\Gamma^{2}G^{2}}{2\\gamma\\ln|A|}+1+\\frac{(G+2D)^{2}}{2\\hat{\\lambda}^{*}}\\log\\left(\\frac{4H\\hat{\\lambda}^{*}}{(G+2D)^{2}}\\displaystyle\\sum_{t=1}^{T}f_{t}(\\mathbf{x})+1\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Finally, we simplify the above bound by utilizing Lemma 11. ", "page_idx": 24}, {"type": "text", "text": "C Supporting Lemmas ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "C.1 Proof of Lemma 4 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "According to the definition of $\\ell_{t,\\widehat{\\alpha}}^{\\mathrm{exp}}(\\cdot)$ in (25), we have $\\begin{array}{r l r}{\\nabla\\ell_{t,\\widehat{\\alpha}}^{\\mathrm{exp}}(\\mathbf{y})}&{{}=}&{\\nabla g_{t}(\\mathbf{y}_{t})\\ \\ +}\\end{array}$ $\\widehat{\\beta}\\nabla g_{t}(\\mathbf{y}_{t})\\nabla g_{t}(\\mathbf{y}_{t})^{\\top}(\\mathbf{y}-\\mathbf{y}_{t})$ . Thus, for all $\\mathbf y\\in\\mathcal D$ , it holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla\\ell_{t,\\widehat{\\alpha}}^{\\mathrm{exp}}(\\mathbf{y})\\nabla\\ell_{t,\\widehat{\\alpha}}^{\\mathrm{exp}}(\\mathbf{y})^{\\top}=\\nabla g_{t}(\\mathbf{y}_{t})\\nabla g_{t}(\\mathbf{y}_{t})^{\\top}+2\\widehat{\\beta}\\nabla g_{t}(\\mathbf{y}_{t})(\\mathbf{y}-\\mathbf{y}_{t})^{\\top}\\nabla g_{t}(\\mathbf{y}_{t})\\nabla g_{t}(\\mathbf{y}_{t})^{\\top}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\widehat{\\beta}^{2}\\nabla g_{t}(\\mathbf{y}_{t})\\nabla g_{t}(\\mathbf{y}_{t})^{\\top}(\\mathbf{y}-\\mathbf{y}_{t})(\\mathbf{y}-\\mathbf{y}_{t})^{\\top}\\nabla g_{t}(\\mathbf{y}_{t})\\nabla g_{t}(\\mathbf{y}_{t})^{\\top}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\,\\left(1+\\widehat{\\beta}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}-\\mathbf{y}_{t}\\rangle\\right)^{2}\\nabla g_{t}(\\mathbf{y}_{t})\\nabla g_{t}(\\mathbf{y}_{t})^{\\top}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\preceq4\\nabla g_{t}(\\mathbf{y}_{t})\\nabla g_{t}(\\mathbf{y}_{t})^{\\top}=\\frac{4}{\\widehat{\\beta}}\\nabla^{2}\\ell_{t,\\widehat{\\alpha}}^{\\mathrm{exp}}(\\mathbf{y})}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\nabla^{2}\\ell_{t,\\widehat{\\alpha}}^{\\mathrm{exp}}(\\mathbf{y})$ denotes the Hessian matrix of $\\ell_{t,\\widehat{\\alpha}}^{\\mathrm{exp}}(\\mathbf{y})$ and the last inequality is due to a, and the definition of $\\widehat\\beta$ . Therefore, $\\ell_{t,\\widehat{\\alpha}}^{\\mathrm{exp}}(\\cdot)$ is $\\frac{\\widehat{\\beta}}{4}$ -exp-concave [Hazan, 2016, Lemma 4.1]. Next, we provide the upper bound of the gradient of $\\ell_{t,\\widehat{\\alpha}}^{\\exp}(\\cdot)$ as follows ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\nabla\\ell_{t,\\widehat{\\alpha}}^{\\mathrm{exp}}(\\mathbf{y})\\|^{2}\\overset{(34)}{\\leq}(G+2\\widehat{\\beta}G^{2}D)^{2}\\overset{25}{\\leq}\\frac{25}{16}G^{2}\\leq2G^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This ends the proof. ", "page_idx": 25}, {"type": "text", "text": "C.2 Proof of Lemma 6 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "According to the definition of $\\ell_{t}^{\\mathrm{sc}}(\\cdot)$ in (14), it holds for any $\\mathbf{x},\\mathbf{y}\\in\\mathcal{X}$ that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\ell_{t}^{\\mathrm{sc}}(\\mathbf{x})\\geq\\ell_{t}^{\\mathrm{sc}}(\\mathbf{y})+\\langle\\nabla\\ell_{t}^{\\mathrm{sc}}(\\mathbf{y}),\\mathbf{x}-\\mathbf{y}\\rangle+\\frac{\\lambda}{2}\\|\\mathbf{x}-\\mathbf{y}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By Definition 1, it can be seen that $\\ell_{t}^{\\mathrm{sc}}(\\cdot)$ is $\\lambda$ -strongly convex. Next, we provide the upper bound of the gradient of $\\ell_{t}^{\\mathrm{sc}}(\\cdot)$ as follows ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\nabla\\ell_{t}^{\\mathrm{sc}}(\\mathbf{y})\\|^{2}\\leq\\|\\nabla g_{t}(\\mathbf{y}_{t})+\\lambda(\\mathbf{y}-\\mathbf{x}_{t})\\|^{2}\\overset{(34)}{\\leq}(G+2\\lambda D)^{2}\\leq(G+2D)^{2}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last step is due to our assumption that $\\lambda\\in[1/T,1]$ . ", "page_idx": 25}, {"type": "text", "text": "C.3 Proof of Lemma 7 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Similar to analysis of Lemma 6, for any $\\mathbf{x},\\mathbf{y}\\in{\\mathcal{X}}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\ell_{t,\\widehat{\\lambda}}^{\\mathrm{sc}}(\\mathbf{x})\\geq\\ell_{t,\\widehat{\\lambda}}^{\\mathrm{sc}}(\\mathbf{y})+\\langle\\nabla\\ell_{t,\\widehat{\\lambda}}^{\\mathrm{sc}}(\\mathbf{y}),\\mathbf{x}-\\mathbf{y}\\rangle+\\frac{\\widehat{\\lambda}}{2G^{2}}\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|^{2}\\|\\mathbf{x}-\\mathbf{y}\\|^{2}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By Definition 1, it is established that $\\ell_{t,\\widehat{\\lambda}}^{\\mathrm{sc}}(\\cdot)$ is $\\frac{\\widehat{\\lambda}}{G^{2}}\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|^{2}$ -strongly convex. Next, we upper bound the gradient of $\\ell_{t,\\widehat{\\lambda}}^{\\mathrm{sc}}(\\cdot)$ as follows ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\ell_{t,\\hat{\\lambda}}^{s c}(\\mathbf{y})\\|^{2}\\leq\\left\\langle\\nabla g_{t}(\\mathbf{y}_{t})+\\frac{\\hat{\\lambda}}{G^{2}}\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|^{2}(\\mathbf{y}-\\mathbf{x}_{t}),\\nabla g_{t}(\\mathbf{y}_{t})+\\frac{\\hat{\\lambda}}{G^{2}}\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|^{2}(\\mathbf{y}-\\mathbf{x}_{t})\\right\\rangle}\\\\ &{\\quad\\quad\\quad=\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|^{2}+\\frac{2\\hat{\\lambda}}{G^{2}}\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|^{2}\\langle\\nabla g_{t}(\\mathbf{y}_{t}),\\mathbf{y}-\\mathbf{x}_{t}\\rangle+\\frac{\\hat{\\lambda}^{2}}{G^{4}}\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|^{4}\\|\\mathbf{y}-\\mathbf{x}_{t}\\|^{2}}\\\\ &{\\quad\\quad\\quad\\overset{(34)}{\\leq}\\left(1+\\frac{2\\hat{\\lambda}D}{G}\\right)^{2}\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|^{2}\\leq\\left(1+\\frac{2D}{G}\\right)^{2}\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last step is due to our assumption that $\\widehat{\\lambda}\\in[1/T,1]$ . ", "page_idx": 25}, {"type": "text", "text": "C.4 Proof of Lemma 12 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The analysis is similar to Wang et al. [2020b]. Let $\\begin{array}{r}{\\widetilde{\\mathbf{y}}_{t+1}^{i}=\\mathbf{y}_{t}^{i}-\\frac{1}{\\eta_{t}}\\nabla\\ell_{t,\\widehat{\\alpha}}^{\\mathrm{sc}}(\\mathbf{y}_{t}^{i})}\\end{array}$ . According to the definition of (28), we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell_{t,k}^{\\mathrm{sc}}(\\mathbf{y}_{t}^{i})-\\ell_{t,k}^{\\mathrm{sc}}(\\mathbf{x})\\leq\\langle\\nabla\\ell_{t,k}^{\\mathrm{sc}}(\\mathbf{y}_{t}^{i}),\\mathbf{y}_{t}^{i}-\\mathbf{x}\\rangle-\\displaystyle\\frac{\\widehat{\\lambda}}{2G^{2}}\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|^{2}\\|\\mathbf{y}_{t}^{i}-\\mathbf{x}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\eta_{t}\\langle\\mathbf{y}_{t}^{i}-\\widetilde{\\mathbf{y}}_{t+1}^{i},\\mathbf{y}_{t}^{i}-\\mathbf{x}\\rangle-\\displaystyle\\frac{\\widehat{\\lambda}}{2G^{2}}\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|^{2}\\|\\mathbf{y}_{t}^{i}-\\mathbf{x}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For the first term, it can be verified that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathbf{y}_{t}^{i}-\\widetilde{\\mathbf{y}}_{t+1}^{i},\\mathbf{y}_{t}^{i}-\\mathbf{x}\\rangle=\\|\\mathbf{y}_{t}^{i}-\\mathbf{x}\\|^{2}+\\langle\\mathbf{x}-\\widetilde{\\mathbf{y}}_{t+1}^{i},\\mathbf{y}_{t}^{i}-\\mathbf{x}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\|\\mathbf{y}_{t}^{i}-\\mathbf{x}\\|^{2}-\\|\\widetilde{\\mathbf{y}}_{t+1}^{i}-\\mathbf{x}\\|^{2}-\\langle\\mathbf{y}_{t}^{i}-\\widetilde{\\mathbf{y}}_{t+1}^{i},\\widetilde{\\mathbf{y}}_{t+1}^{i}-\\mathbf{x}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\|\\mathbf{y}_{t}^{i}-\\mathbf{x}\\|^{2}-\\|\\widetilde{\\mathbf{y}}_{t+1}^{i}-\\mathbf{x}\\|^{2}+\\|\\widetilde{\\mathbf{y}}_{t+1}^{i}-\\mathbf{y}_{t}^{i}\\|^{2}+\\langle\\widetilde{\\mathbf{y}}_{t+1}^{i}-\\mathbf{y}_{t}^{i},\\mathbf{y}_{t}^{i}-\\mathbf{x}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which implies that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\langle\\mathbf{y}_{t}^{i}-\\widetilde{\\mathbf{y}}_{t+1}^{i},\\mathbf{y}_{t}^{i}-\\mathbf{x}\\right\\rangle=\\frac{1}{2}\\left(\\left\\Vert\\mathbf{y}_{t}^{i}-\\mathbf{x}\\right\\Vert^{2}-\\left\\Vert\\widetilde{\\mathbf{y}}_{t+1}^{i}-\\mathbf{x}\\right\\Vert^{2}+\\left\\Vert\\widetilde{\\mathbf{y}}_{t+1}^{i}-\\mathbf{y}_{t}^{i}\\right\\Vert^{2}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Thus, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell_{t,k}^{\\mathrm{sc}}(\\mathbf{y}_{t}^{i})-\\ell_{t,k}^{\\mathrm{sc}}(\\mathbf{w})\\leq\\displaystyle\\frac{\\eta_{t}}{2}\\left(\\|\\mathbf{y}_{t}^{i}-\\mathbf{x}\\|^{2}-\\|\\widetilde{\\mathbf{y}}_{t+1}^{i}-\\mathbf{x}\\|^{2}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\frac{1}{2\\eta_{t}}\\|\\nabla\\ell_{t,\\widehat{\\alpha}}^{\\mathrm{sc}}(\\mathbf{y}_{t}^{i})\\|^{2}-\\displaystyle\\frac{\\widehat{\\lambda}}{2G^{2}}\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|^{2}\\|\\mathbf{y}_{t}^{i}-\\mathbf{x}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Summing the above bound up over $t=1$ to $T$ , we attain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=1}^{T}\\ell_{t,\\hat{\\alpha}}^{\\mathrm{sc}}(\\mathbf{y}_{t}^{i})-\\sum_{t=1}^{T}\\ell_{t,\\hat{\\alpha}}^{\\mathrm{sc}}(\\mathbf{x})}\\\\ {\\displaystyle\\leq\\frac{\\eta_{1}}{2}\\|\\mathbf{y}_{1}^{i}-\\mathbf{x}\\|^{2}+\\sum_{t=1}^{T}\\left(\\eta_{t}-\\eta_{t-1}-\\frac{\\hat{\\lambda}}{G^{2}}\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|^{2}\\right)\\frac{\\|\\mathbf{y}_{t}^{i}-\\mathbf{x}\\|^{2}}{2}+\\sum_{t=1}^{T}\\frac{1}{2\\eta_{t}}\\|\\nabla\\ell_{t,\\hat{\\alpha}}^{\\mathrm{sc}}(\\mathbf{y}_{t}^{i})\\|^{2}}\\\\ {\\displaystyle\\leq1+\\sum_{t=1}^{T}\\frac{1}{2\\eta_{t}}\\|\\nabla\\ell_{t,\\hat{\\lambda}}^{\\mathrm{sc}}(\\mathbf{y}_{t}^{i})\\|^{2}\\leq1+\\frac{(G+2D)^{2}}{2\\hat{\\lambda}}\\sum_{t=1}^{T}\\frac{\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|^{2}}{(G+2D)^{2}/\\hat{\\lambda}+\\sum_{t=1}^{t}\\|\\nabla g_{t}(\\mathbf{y}_{t})\\|^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the last two inequalities is due to $\\begin{array}{r}{\\eta_{t}\\,=\\,(1+2D/G)^{2}+\\frac{\\widehat{\\lambda}}{G^{2}}\\sum_{i=1}^{t}\\|\\nabla g_{i}(\\mathbf{y}_{i})\\|^{2}}\\end{array}$ which is specifically set for new expert-loss. Further, we will use the following lemma to bound the last term. ", "page_idx": 26}, {"type": "text", "text": "Lemma 13 (Lemma 11 of Hazan et al. [2007]) Let $l_{1},\\cdot\\cdot\\cdot,l_{T}$ and $\\delta$ be non-negative real numbers.   \nThen, we have $\\begin{array}{r}{\\sum_{t=1}^{T}\\frac{l_{t}^{2}}{\\sum_{i=1}^{t}l_{i}^{2}+\\delta}\\leq\\log\\left(\\frac{1}{\\delta}\\sum_{t=1}^{T}l_{t}^{2}+1\\right)}\\end{array}$ . ", "page_idx": 26}, {"type": "text", "text": "This completes the proof of Lemma 12. ", "page_idx": 26}, {"type": "text", "text": "C.5 Proof of Lemma 5 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The analysis is similar to Mhammedi et al. [2019, Lemma 9]. When $\\|\\widehat{\\mathbf{y}}_{t+1}^{i}\\|\\geq D$ , then we need to solve the following quadratic problem: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{y}_{t+1}^{i}=\\underset{\\mathbf{y}\\in\\mathcal{Y}}{\\arg\\operatorname*{min}}(\\widehat{\\mathbf{y}}_{t+1}^{i}-\\mathbf{y})^{\\top}\\boldsymbol{\\Sigma}_{t+1}(\\widehat{\\mathbf{y}}_{t+1}^{i}-\\mathbf{y}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We use the Lagrangian multiplier to solve the above problem ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\mathbf{y},\\boldsymbol{\\mu})=(\\widehat{\\mathbf{y}}_{t+1}^{i}-\\mathbf{y})^{\\top}\\Sigma_{t+1}(\\widehat{\\mathbf{y}}_{t+1}^{i}-\\mathbf{y})+\\mu(\\mathbf{y}^{\\top}\\mathbf{y}-D^{2}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We set $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{y}}=\\mathbf{0}}\\end{array}$ to attain $\\Sigma_{t+1}(\\mathbf{y}-\\widehat{\\mathbf{y}}_{t+1}^{i})+\\mu\\mathbf{y}=0$ , which implies ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{y}=(\\mu\\mathbf{I}_{d}+\\Sigma_{t+1})^{-1}\\Sigma_{t+1}\\widehat{\\mathbf{y}}_{t+1}^{i}=\\mathbf{Q}_{t+1}^{\\top}\\left(x\\mathbf{I}+\\Lambda_{t+1}\\right)^{-1}\\mathbf{Q}_{t+1}\\Sigma_{t+1}\\widehat{\\mathbf{y}}_{t+1}^{i}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $x=\\mu+1/(\\widehat{\\beta}^{2}D^{2})$ . Due to $\\mathbf{y}^{\\top}\\mathbf{y}=D^{2}$ , $x$ is the solution of the following problem ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\rho(x):=\\sum_{k=1}^{d}\\frac{\\langle\\mathbf{e}_{k},\\mathbf{Q}_{t+1}\\Sigma_{t+1}\\widehat{\\mathbf{y}}_{t+1}^{i}\\rangle^{2}}{(x+\\lambda_{t}^{k})^{2}}=D^{2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "D Clarifications on bounded modulus ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we explain that bounded moduli are generally acceptable in practical scenarios, which is also stated in previous study [Zhang et al., 2022]. Taking $\\lambda$ -strongly convex functions as an example, we assume that $\\lambda\\in[1/T,1]$ , since other cases that $\\lambda<1/T$ and $\\lambda>1$ can be disregarded. (i) If $\\bar{\\lambda}<1/T$ , the regret bound for strongly convex functions becomes $\\Omega(T)$ , which cannot benefti from strong convexity. Therefore, we should treat these functions as general convex functions. (ii) If $\\lambda>1$ , $\\lambda$ -strongly convex functions are also 1-strongly convex according to Definition 1. Thus, we can treat these functions as 1-strongly convex functions. ", "page_idx": 26}, {"type": "image", "img_path": "xNncVKbwwS/tmp/efc399e54cc9c323df198fb888990fa30b97a8add5b9ccf47002a89993a510a8.jpg", "img_caption": ["Figure 1: Regret (first row) and running time (second row) of different methods. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "E Experiments ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we conduct empirical experiments to validate the effectiveness of our proposed methods, and present the details of experiments. ", "page_idx": 27}, {"type": "text", "text": "Settings We conduct experiments on the ijcnn1 dataset from LIBSVM Data [Chang and Lin, 2011, Prokhorov, 2001], where the dimension of features is $d=22$ . We consider the following online classification problem. In each round $t\\in[T]$ , the online learner chooses a decision $\\mathbf{x}_{t}\\in\\mathcal{X}$ . After submitting the decision, the online learner receives a batch of data samples $\\{(x_{t}^{(i)},y_{t}^{(i)})\\}_{i=1}^{m}$ are sampled from the dataset, where $\\boldsymbol x_{t}^{(i)}$ is the feature vector of the $i$ -th sample, and $y_{t}^{(i)}$ is the corresponding label. The learner can evaluate the model by the online convex loss and update the decision for the next round. In our study, we set $T=2000$ , the domain diameter as $D=20$ , and the gradient norm upper bound as $G={\\sqrt{22}}$ . Following the general setup of Zhao et al. [2022], we set the feasible domain to be an ellipsoid $\\mathcal{X}=\\{\\mathbf{x}\\in\\mathbb{R}^{\\bar{d}}\\,\\vert\\,\\mathbf{x}^{\\top}\\mathbf{Ex}\\leq\\lambda_{\\operatorname*{min}}\\mathbf{\\dot{(E)}}\\cdot(D/2)^{2}\\}$ , where $\\mathbf{E}$ is a certain diagonal matrix and $\\lambda_{\\operatorname*{min}}({\\bf E})$ denotes its minimum eigenvalue. We remark that the cost of one projection onto $\\mathcal{X}$ is generally expensive since it requires solving a convex programming. ", "page_idx": 27}, {"type": "text", "text": "In the following, we consider three types of online convex functions to simulate the unknown environment and demonstrate the universality of our method. First, for exp-concave functions, the online learner suffers a logistic loss: $\\begin{array}{r}{f_{t}(\\mathbf{x}_{t})\\;=\\;\\frac{1}{m}\\sum_{i=1}^{m}\\log\\Big(1+\\exp\\!\\big(\\!-\\!y_{t}^{(i)}\\mathbf{x}_{t}^{\\top}x_{t}^{(i)})\\Big)}\\end{array}$ . Second, for strongly convex functions, we choose the regularized hinge loss: $f_{t}(\\mathbf{x}_{t})\\;\\;=\\;\\;$ $\\begin{array}{r}{\\frac{1}{m}\\sum_{i=1}^{m}\\operatorname*{max}\\left(0,\\stackrel{\\cdot\\cdot}{1-}y_{t}^{(i)}{\\bf x}_{t}^{\\top}x_{t}^{(i)}\\right)+\\frac{\\lambda}{2}\\|{\\bf x}_{t}\\|^{2}}\\end{array}$ . Third, for general convex functions, the online learner suffers the absolute loss: $\\begin{array}{r}{f_{t}(\\mathbf{x}_{t})\\,=\\,\\frac{1}{m}\\sum_{i=1}^{m}\\left|\\mathbf{x}_{t}^{\\top}x_{t}^{(i)}-y_{t}^{(i)}\\right|}\\end{array}$ . Based on the above experimental settings, we conduct the empirical studies of our method, as well as other universal algorithms in the literature. ", "page_idx": 27}, {"type": "text", "text": "Algorithms We compare the performance of our proposed method for minimax universal regret with existing universal algorithms, including MetaGrad [van Erven and Koolen, 2016], Maler [Wang et al., 2019], efficient implementation of MetaGrad [Mhammedi et al., 2019], and USC [Zhang et al., 2022]. All the baselines share the same experimental setting as our method. ", "page_idx": 27}, {"type": "table", "img_path": "xNncVKbwwS/tmp/77927b9c8e85de56996fde58a5e2caa10ef88965004b847ac34b4e41433c3890.jpg", "table_caption": ["Table 2: A summary of state-of-the-art projection-free algorithms for different types of convex functions. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Results We repeat the experiments for five times and record the results in Figure 1. We conduct the experiments on a machine with a single CPU (Apple M1 pro) and 16GB memory. We record both regret and running time (in seconds) for all methods. As shown in Figure 1, the running time of our method is comparable to that of EffMetaGrad, yet it achieves better results for strongly convex functions. Compared to other algorithms which conduct ${\\cal O}(\\log T)$ projections, i.e., USC, Maler, and MetaGrad, the running time of our projection-efficient method is 5 to 20 times faster, and it also attains nearly optimal regret for three types of convex functions. In conclusion, the empirical results demonstrate the effectiveness of our method in achieving optimal regret guarantee and also significant enhancement in computational efficiency. ", "page_idx": 28}, {"type": "text", "text": "F Further discussion on projection-free algorithms ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In the literature, there exists a class of projection-free algorithms [Hazan and Kale, 2012, Hazan and Minasyan, 2020, Wan and Zhang, 2021, Wan et al., 2021b, 2022, Wang et al., 2023, Garber and Kretzu, 2023]. Therefore, it is natural to ask whether projection-free algorithms such as variants of Online Frank Wolfe could be used instead of OGD and ONS to remove all projections while still being adaptive to the smoothness. Here, we provide some targeted discussions on this matter. ", "page_idx": 28}, {"type": "text", "text": "In fact, we can choose projection-free algorithms as the expert-algorithms. However, given the current studies on projection-free algorithms, this approach will lead to a deterioration of the regret bound and can not handle certain cases. As is shown in Table 2, in the literature, there are no suitable projection-free algorithms for exp-concave functions, neither for strongly convex and smooth functions. Moreover, when functions are smooth, existing projection-free algorithms are unable to achieve problem-dependent bounds, such as the small-loss bounds in this work. ", "page_idx": 28}, {"type": "text", "text": "Finally, we would like to highlight that although using projection-free algorithms can remove all projections, they may not achieve greater efficiency based on the universal framework. Specifically, most projection-free algorithms, such as OFW and its variants, replace the original projection operation with a linear optimization step. Since the universal framework requires maintaining ${\\bar{O}}(\\log T)$ expert-algorithms, this approach needs to perform ${\\cal O}(\\log T)$ linear optimization steps per round, which can be time-consuming when $T$ is large. ", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our contributions and scope are clearly written in the abstract and introduction. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: See the future work in Section 5. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: See the assumptions in Section 2.1. The complete proofs can be found in Appendix B. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: All the information needed to reproduce the experimental results is provided. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [No] ", "page_idx": 31}, {"type": "text", "text": "Justification: Due to privacy concerns, we do not include the code. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: See the experiments in Appendix E. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: See the experiments in Appendix E. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: See the experiments in Appendix E. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have read the ethics carefully. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This work is mainly theoretical. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 32}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 33}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This work is mainly theoretical. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We cite the creators of the dataset used in our experiments properly. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This work does not release new assets. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]