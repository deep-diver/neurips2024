[{"type": "text", "text": "Understanding Transformer Reasoning Capabilities via Graph Algorithms ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Clayton Sanford1,2,\u2217 Bahare Fatemi1, Ethan $\\mathbf{Hall^{3}}$ , Anton Tsitsulin1, Mehran Kazemi4, Jonathan Halcrow1, Bryan Perozzi1, and Vahab Mirrokni1 1Google Research, 2Columbia University,3Google, 4Google DeepMind ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Which transformer scaling regimes are able to perfectly solve different classes of algorithmic problems? While tremendous empirical advances have been attained by transformer-based neural networks, a theoretical understanding of their algorithmic reasoning capabilities in realistic parameter regimes is lacking. We investigate this question in terms of the network\u2019s depth, width, and number of extra tokens for algorithm execution. Our novel representational hierarchy separates 9 algorithmic reasoning problems into classes solvable by transformers in different realistic parameter scaling regimes. We prove that logarithmic depth is necessary and sufficient for tasks like graph connectivity, while single-layer transformers with small embedding dimensions can solve contextual retrieval tasks. We also support our theoretical analysis with ample empirical evidence using the GraphQA benchmark. These results show that transformers excel at many graph reasoning tasks, even outperforming specialized graph neural networks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The transformer neural network architecture, which was initially introduced for neural machine translation [6, 77], quickly became the standard neural network architecture across many fields, powering recent breakthroughs in language modeling [20, 66, 12, 76, 3, 75, 67], computer vision [21, 49], and natural sciences [36, 53]. Across fields, transformers have superseded other architectures, surpassing them in downstream performance while maintaining reasonable computational footprint. ", "page_idx": 0}, {"type": "text", "text": "How can we analyze reasoning capabilities of neural networks? One approach is to study algorithms executable with their internal representations. Neural algorithmic reasoning [30, 89, 34, 37, 78] is a field of research dedicated to exploring such capabilities. Algorithmic execution is desirable because models use it to generalize out-of-distribution [92, 46] and scale to larger problem sizes [93]. ", "page_idx": 0}, {"type": "text", "text": "In this work, we focus on classes of transformers solving algorithmic reasoning problems on graphs. Why graph problems in particular? Recent research by Besta et al. [10] suggests that graphs are an ideal abstraction of complex reasoning with dependencies, including chain-of-thought [82] and its generalizations [86, 17, 41, 40]. Furthermore, we investigate graph reasoning in order to evaluate transformer capabilities compared to specialized models that explicitly capture the structure of data. Graph neural networks (GNNs) [72, 44, 28, 7, 15] offer strong baselines for algorithmic reasoning [79, 13] and comprehensive theoretical frameworks for their capabilities and limitations [51, 84]. The complexity of graph reasoning tasks varies greatly; some are easily solved by non-graph architectures and others require sophistication beyond standard GNNs [90]. This makes graph tasks a compelling testbed for both theoretical and empirical evaluations. ", "page_idx": 0}, {"type": "text", "text": "While transformer-based models are commonly optimized for downstream performance [38, 32, 64], theoretical investigations of transformer capabilities in realistic parameter regimes have been limited. ", "page_idx": 0}, {"type": "image", "img_path": "AfzbDw6DSp/tmp/9e5800053b7840427c4188a31320b8254c1c93779142f7bbbddb040242de637b.jpg", "img_caption": ["Figure 1: The graph encoding scheme employed in our theoretical and empirical analysis that presents a graph reasoning task (e.g. connectivity) as a tokenized input to a standard transformer model. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "We analyze the capabilities of standard transformer models to solve graph reasoning tasks by employing a graph tokenization scheme similar to [42]; see Figure 1 for a graph encoding example. We generalize the insights of [71]\u2014which established that graph connectivity can be computed in a more depth-efficient manner with transformers than GNNs\u2014to broader families of graph reasoning tasks. We also study the representational impacts of blank tokens, which can be thought of as a theoretically-tractable version of either chain-of-thought prompting [82], scratch space [61], or pause/filler tokens [29, 65]. ", "page_idx": 1}, {"type": "text", "text": "We conduct an extensive study of graph reasoning problems with transformers from both theoretical and empirical perspectives. We summarize our principal contributions as follows: ", "page_idx": 1}, {"type": "text", "text": "1. We introduce a novel representational hierarchy of graph reasoning tasks that formalizes reasoning capabilities of transformers in several realistic parameter scaling regimes. This includes two graph reasoning task classes\u2014which we refer to as the parallelizable and search tasks and include well-known algorithmic tasks like connectivity and shortest path respectively.   \n2. We prove that logarithmic-depth transformers are necessary and sufficient to solve parallelizable tasks in a highly parameter-efficient manner; similar constructions for search tasks employ much larger networks. We distinguish both of these classes from an easier family of problems\u2014retrieval tasks, such as node count and edge existence\u2014by showing that retrieval tasks can be efficiently computed by single-layer transformers, while parallelizable and search tasks cannot.   \n3. We empirically validate our representational contrast by showing that transformers outperform GNNs on tasks that require the analysis of long-range dependencies, including parallelizable tasks like connectivity. Our experiments suggest that GNNs perform better on simpler tasks that require only local analysis of neighboring nodes in low sample-complexity regimes, beneftiing from their well-aligned inductive bias. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Fundamental capabilities of transformers. Early representational research [88, 62, 81] established the universality of transformers by simulating Turing machines step-by-step, albeit in an unrealistic polynomial-depth scaling regime. These universality results were extended to boundeddepth transformers with chain-of-thought tokens [56, 52], but with a persistent gap between positive and negative results. A more fine-grained theoretical approach established the limitations of boundedsize transformers by relating them to threshold circuits, implying that L-complete tasks like graph connectivity are unsolvable by constant-depth transformers [55, 54, 57]. However, these circuit complexity reductions are not bidirectional and their positive results pertaining to classes of regular languages [31] reveal little about when tasks like connectivity can be expressed. ", "page_idx": 1}, {"type": "text", "text": "The closest analytical framework to ours characterizes transformers as distributed computing protocols by quantifying the informational bandwidth of their self-attention units. This line of work sharply separates transformers from other architectures and provides width and depth separations [70, 71]. The ability of transformers to simulate finite-state automata in logarithmic depth provides a further roadmap for how transformers efficiently leverage parallel computation [48]. ", "page_idx": 2}, {"type": "text", "text": "Recent work [18] similarly focuses on understanding the capabilities of transformers to solve graph algorithms, but they focus instead on looped transformers, where a constant-size transformer model is iterated up to a polynomial number of times in the size of the input graph. While their constructions for tasks like graph connectivity are more parameter-efficient than our own, this comes at the cost of a very high depth, which presents both run-time and learnability concerns. Our depth-efficient constructions emerge by simulating parallel graph algorithms, rather than well-known serial algorithms, such as Dijkstra\u2019s. ", "page_idx": 2}, {"type": "text", "text": "Empirical analysis of transformer reasoning capabilities. Multiple empirical investigations explore the algorithmic capabilities of transformer-based models [91, 47, 46]. Graph reasoning problems have been used to evaluate capabilities of transformer-based LLMs [24, 80, 87, 74, 33] including works with graph-specific GNN adapters [14, 63]. In the empirical part of our study, we use the GraphQA dataset [24]\u2014which was evaluated initially on LLM prompting appraoches and later on GNN-augmented prompts [62]\u2014to validate our complexity hierarchy and compare transformers with GNNs and prompt-based LLMs on a selection of graph reasoning tasks. ", "page_idx": 2}, {"type": "text", "text": "Transformers and graph neural networks. GNNs provide a useful benchmark for model expressivity on graph inputs. For instance, the inability of GNNs to efficiently solve \u201cglobal structure\u201d tasks like subgraph connectivity made evident by a connection to the CONGEST distributed computing model [51]. A further connection [59] to the Weisfeiler-Lehman (WL) isomorphism heuristic [83] captures the inability of feature-less GNNs to distinguish certain graph instances [84]. The featureless assumption is burdensome and representationally costly; GNNs with randomly intialized node features are known to be universal [2]. See Appendix D for a further discussion of the representational limitations of GNNs. ", "page_idx": 2}, {"type": "text", "text": "Graph transformers [22, 68, 58] integrate the transformer architecture with graph-structured data. While GNNs can simulate the WL test [1], transformers can simulate GNNs and exceed their WL limitations [90]; they are strictly more expressive than 2-WL [42]. Despite our focus on standard transformers, our empirical results in Section 4 include comparisons to a wide range of GNNs. ", "page_idx": 2}, {"type": "text", "text": "3 Hardness taxonomy of transformer graph reasoning tasks ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We provide our core result: a rigorous quantification of the hardness of graph reasoning tasks for transformer-based models. While graph reasoning tasks, like other algorithmic problems, can be categorized into well-known computational and circuit complexity classes (e.g. $\\mathrm{\\bar{T}C^{0}}$ , L, NL, NP, NP), the relationship between membership in these classes and the hardness of solving a task with a parameter-efficient neural network is not immediately obvious. Our hierarchy bridges the gap between these worst-case computational classes and the representational capabilities of bounded-size transformers of different parameter scaling regimes. These regimes include transformers whose depth $L$ scales with the input sequence length $N$ ; this contrasts with most theoretical results, which study on the constant-depth regime. ", "page_idx": 2}, {"type": "text", "text": "Our positive and negative theoretical results employ the transformer model of [71], which is presented in Appendix A.1 and assumes that the embedding dimension $m$ grows less rapidly than than the sequence length $N$ and that the multi-layer perceptrons (MLPs) are arbitrary functions. The result is a model of a transformer as a bounded-capacity communication protocol. In this model, arbitrary functions of each individual embedding vector can be computed, but the interactions between these vectors are restricted by the low-rank of the attention matrix. The relevance of this model is motivated by the rapid scaling in the context lengths of modern transformers in recent years and the high ratio of MLP parameter count to the embedding dimension each operates on. ", "page_idx": 2}, {"type": "text", "text": "This model also permits the inclusion of blank pause token inputs of [29], which provide additional computational power to the transformer model by extending the computational \u201ctape\u201d without introducing new information about the input. ", "page_idx": 2}, {"type": "image", "img_path": "AfzbDw6DSp/tmp/03ab42520b8f96ef994aa2b9155240b6a52ae59064e35818ade8ec5ef696ebf6.jpg", "img_caption": ["(a) The complexity hierarchy "], "img_footnote": [], "page_idx": 3}, {"type": "image", "img_path": "AfzbDw6DSp/tmp/168941e8bc158c8fe5f3e3ed50ed0a51dd2aa9a161ccbac7882fd693487b05be.jpg", "img_caption": ["(b) Example tasks and their complexity. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: A summary of the theoretical hierarchy of Section 3 that visualizes which type of graph reasoning tasks can be solved in which transformer scaling regime (Depth1 (D1), LogDepth (LD), LogDepthWide (LDW) and LogDepthPause (LDP)). ", "page_idx": 3}, {"type": "text", "text": "These results divide the tasks that are we empirically investigate in Section 4 into three difficulty families based on the hardness of solving the task with parallel computation. ", "page_idx": 3}, {"type": "text", "text": "1. Retrieval tasks\u2014including node count, edge count, edge existence, and node degree\u2014are tasks that can intuitively be solved by a single lookup step or global aggregation. These are the easiest tasks in our framework, and we show in Section 3.3 that these retrieval tasks can be computed by a single-layer transformer with small embedding dimension. (In contrast, all other examined tasks cannot be solved in that regime.) ", "page_idx": 3}, {"type": "text", "text": "2. Parallelizable tasks\u2014including connectivity, connected nodes, and cycle check from the experimental results and a host of other graph reasoning tasks, such as bipartiteness, planarity, and minimum spanning forest\u2014are non-trivial tasks that can be solved efficiently in a parallel computation setting. Section 3.1 establishes that these tasks can be solved by bounded-size transformers with logarithmic depth. ", "page_idx": 3}, {"type": "text", "text": "3. Search tasks\u2014including shortest path and other tasks like diameter and directed reachability\u2014 comprise a harder family of tasks that are less easily solved by parallel algorithms. In Section 3.2, we prove that these tasks belong in an equivalence class and exhibit large-model scaling regimes where they can be computed. ", "page_idx": 3}, {"type": "text", "text": "The representational hardness of these classes is quantified by several results that determine whether transformers that obey different parameter scaling rules can compute them. We define the following scaling regimes for the depth $L$ , embedding dimension $m$ , and number of \u201cpause\u201d tokens $N^{\\prime}$ of a family of transformers as a function of the size of the input graph, $N=O(|\\bar{V}|+|E|)$ . ", "page_idx": 3}, {"type": "text", "text": "\u2022 Depth1 (D1): Single-layer multi-headed transformers with small embedding dimension $m=$ ${\\cal O}(\\log N)$ without any pause tokens.   \n\u2022 LogDepth (LD): Transformers with depth $L=O(\\log N)$ , embedding dimension $m=O(N^{\\epsilon})$ for any fixed $\\epsilon>0$ , and no pause tokens.   \n\u2022 LogDepthPause (LDP): Transformers with the same depth and width constraints as LogDepth, with at most $N^{\\prime}=\\mathrm{{poly}}(N)$ blank \u201cpause\u201d tokens appended to the input sequence.   \n\u2022 LogDepthWide (LDW): Transformers with depth $L=O(\\log N)$ , embedding dimension $m=$ $O(N^{1/2+\\epsilon})$ , and no pause tokens. ", "page_idx": 3}, {"type": "text", "text": "Our positive and negative results that relate scaling regimes and graph reasoning tasks are displayed in Figure 2. We present high-level result summaries in the following sections with proofs in Appendix. ", "page_idx": 3}, {"type": "text", "text": "The most technically significant results concern LogDepth models and are provided in Appendix B. These bounds are consequences of Theorem 1, an improved analysis of the relationship between ", "page_idx": 3}, {"type": "text", "text": "transformers and the massively parallel computation (MPC) distributed computing model of $[39]^{2}$ .   \nThis connection between MPC and transformers is a sharp improvement of a similar result by [71]. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 (Simplified version of Theorem 8). For constant $\\delta,\\epsilon>0$ , any $R$ -round MPC protocol with $N$ machines with $O(N^{\\delta})$ bits of local memory each can be simulated by a transformer of depth $L=O(R)$ and embedding dimension $m=O(N^{\\delta+\\epsilon})$ . ", "page_idx": 4}, {"type": "text", "text": "Results pertaining to Depth1 are stated in detail and proved in Appendix C. In addition, we discuss the triangle counting task (and the more general clique counting task) in Section 3.4, where we show a distinct result for much smaller depth $(L=O(\\log\\log N))$ that includes pause tokens. ", "page_idx": 4}, {"type": "text", "text": "3.1 Parallelizable tasks and LogDepth transformers ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We define the family of parallelizable tasks to consist of graph reasoning tasks that are L-complete and are equivalent to graph connectivity in $O(1)$ -rounds, as proved by [60]. This family includes (but is not limited to): graph connectivity, minimum spanning forest, cycle detection, $^{s t\\!}$ -connectivity, number of connected components, graph bipartiteness, planarity testing, and one-cycle vs two-cycles testing. While graph connectivity and minimum spanning forest were shown to be computable by logarithmic depth transformers with small polynomial width (LogDepth) by previous work [71], this poses broader questions: Are all parallelizable graph reasoning tasks computable by a transformer of logarithmic depth and small embedding dimension? And do sub-logarithmic-depth transformers exist that solve any parallelizable tasks? ", "page_idx": 4}, {"type": "text", "text": "We first show that all parallelizable tasks can be solved in two logarithmic-depth scaling settings. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2. For any parallelizable task, there exists transformers in LogDepthPause and LogDepthPause that solve the task. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2 is stated formally as Theorem 18 and proved in Appendix B.2.1. Both components of the theorem are a consequence of a novel relationship between the MPC model and transformers (Theorem 1) and the analysis of MPC protocols for graph reasoning tasks by [60]. The LogDepthPause result is the direct implication of an $O(1)$ -round MPC equivalence (Theorem 9) between all parallelizable tasks and an MPC protocol that solves the connectivity task (Theorem 11). The LogDepthWide bound is a consequence of Theorem 15, which shows that all languages in $\\mathsf{N C}^{2}$ (including all languages in L and NL) can be evaluated by an MPC protocol with ${\\cal O}(\\log N)$ rounds and with local memory $O(N^{1/2+\\epsilon})$ [60]. ", "page_idx": 4}, {"type": "text", "text": "We further prove the conditional optimality of logarithmic depth. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3. Conditional on Conjecture 13, any transformer that solves some parallelizable task with width $m H=O(N^{\\epsilon})$ and pause tokens $N^{\\prime}=\\mathrm{{poly}}(N)$ must have depth $L=\\Omega(\\log N)$ . ", "page_idx": 4}, {"type": "text", "text": "This result (stated formally as Theorem 19 is a combination of the conditional depth lower bound on graph connectivity by [71] and the $O(1)$ -round equivalence between all parallel tasks. ", "page_idx": 4}, {"type": "text", "text": "3.2 Search tasks ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Search tasks are similarly defined to be those that are NL-complete and equivalent to shortest path in $O(1)$ -rounds of MPC and include shortest path, strong connectivity, $^{s t}$ -reachability, radius, diameter, and median. Like before, the $O(1)$ -round MPC equivalence translates to an $O(1)$ -depth equivalence in transformers. We give a similar positive result for LogDepthWide transformers; whether these tasks can be solved by LogDepthPause transformers is unknown. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4. For any search task, there exists a transformer in LogDepthWide that sovles the task. ", "page_idx": 4}, {"type": "text", "text": "This theorem, which is restated in Appendix B.2.2 as Theorem 21, is also an immediate consequence of Theorem 15. ", "page_idx": 4}, {"type": "text", "text": "While the minimum depth of a transformer with small embedding dimension that solves a search task is not identified, we prove that the minimum depth needed to solve some all search task is approximately equivalent in Theorem 22. ", "page_idx": 4}, {"type": "text", "text": "3.3 Retrieval tasks and Depth1 transformers ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Graph tasks whose algorithms consist of a single look-up or aggregation step can be efficiently solved by single-layer transformers. This result assumes that the graph $G=(\\bar{V},E)$ is encoded as some input sequence $X$ of length $N=O(|V|+|E|)$ that expresses each edge and vertex a single token. This encoding scheme is detailed in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "Theorem 5. For any retrieval task (including node count, edge count, edge existence, and node degree) there exists a transformer in Depth1 that solves the task. ", "page_idx": 5}, {"type": "text", "text": "We formalize this statement in Theorem 36 and prove it their in Appendix C.1. These rely on proving the existence of a useful input MLP $\\phi$ that precomputes embeddings with useful structure for all retrieval tasks. ", "page_idx": 5}, {"type": "text", "text": "In contrast, we show that a collection of parallelizable and search tasks cannot be efficiently solved by transformers in Depth1. ", "page_idx": 5}, {"type": "text", "text": "Theorem 6. Any single-layer transformer that solves the graph connectivity, shortest path, or cycle detection task has width satisfying $\\dot{m}H=\\tilde{\\Omega}(N)$ . ", "page_idx": 5}, {"type": "text", "text": "The proof of the formal counterpart of this statement (Theorem 38) appears in Appendix C.2 and is a consequence of a standard communication complexity argument. A more generalized result than Theorem 6 was proved by [57], which establishes that all problems outside of $\\mathsf{N C}^{1}$ \u2014which include all L-complete and NL-complete languages, and hence, all parallelizable and search tasks\u2014cannot be solved by constant-depth transformers with polynomial width because they cannot be computed by $\\mathsf{T}\\mathsf{C}^{0}$ (constant-depth threshold circuits). We nonetheless include this theorem to demonstrate a clean lower bound that applies to very simple input graph instances. ", "page_idx": 5}, {"type": "text", "text": "3.4 Triangle counting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We finally construct depth-efficient transformers for triangle counting due to the MPC algorithms of [11]. Unlike previous positive results, which applied uniformly across all graphs instances of bounded size, the complexity of the corresponding transformers for triangle counting is a function of the arboricity3 of the input graph. When the arboricity grows sub-polynomially with $N$ \u2014as is the case for bounded-degree graphs\u2014no pause tokens are necessary. Unlike the parallelizable and search classes of problems, strictly sub-logarithmic depth is attainable with pause tokens, even for worst-case graphs. ", "page_idx": 5}, {"type": "text", "text": "Theorem 7. There exists a transformer that computes the number of triangles in any input graph of arboricity $\\alpha$ and has embedding dimension $m={O}(N^{\\epsilon})$ , depth $L=O(\\log\\log N)$ , and pause tokens ", "page_idx": 5}, {"type": "equation", "text": "$$\nN^{\\prime}=\\binom{O(\\alpha N^{1-\\epsilon})}{0}\\quad i f\\alpha=\\Omega(N^{\\epsilon})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This result is a special case of Theorem 23, a more general result about clique counting that appears in Appendix B.2.3. ", "page_idx": 5}, {"type": "text", "text": "Theoretical conclusions. These results provide a tight characterization of the the reasoning capabilities of transformers whose depth, width, and input padding conform to different scaling regimes. They strengthen the established connection between transformers and massively parallel computation (MPC) [71] and generalize the resulting representational bounds to broader categories of graph tasks. We conclude that the logarithmic-depth regime is apt for for considering tasks in L and NL, which had previous illuminated the limitations of transformers with constant depth and a limited number of chain-of-thought tokens [56]. While expressivity does not imply learnability, these theoretical benchmarks sharply characterize the fundamental limitations of transformers and coincide with experimental results conveyed in the subsequent section. ", "page_idx": 5}, {"type": "text", "text": "Our theoretical model of a transformer has certain limitations, namely the universality of the multilayer perceptron units. When maximally exploited (e.g. by solving NP-hard problems within each MLP), this assumption exaggerates the computational capabilities of transformers. However, the local computational complexity of the MPC algorithms employed in this paper tend to be strongly sublinear in the size of the graph input (see, e.g. the connectivity algorithms of Andoni et al. [4]), which implies the existence of compact ReLU circuits for each of the model\u2019s MLPs. Furthermore, the ratio of model parameters that belong to MLPs in state-of-the-art language modelsis known to be high, which suggests that it is reasonable to assume that MLP units in our theoretical model are algorithmically rich. We would be interested in pursuing further theoretical research that combine the computational model [54] and the communication model of [71] to assess the capabilities of transformers whose MLPs are represented as bounded-size boolean circuits. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4 Empirical graph reasoning capabilities ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We further illuminate the reasoning capabilities of transformers by conducting an empirical investigation of the abilities of a variety of neural architecture and training settings to learn graph algorithmic tasks. We use the GraphQA benchmark tasks [24] for our experiments. We evaluate standard autoregressive transformers\u2014both small models (at most 60M parameters) trained from scratch and fine-tuned (FT) T5-11B model (with 11B parameters) [66]. For the fine-tuned models, we explore task-specific fine-tuning\u2014and contrast those results with graph neural networks (GNNs) and prompting-based methods on pre-trained LLMs. ", "page_idx": 6}, {"type": "text", "text": "These experimental results validate key tenets of our theoretical results and demonstrate the utility of transformers\u2019 algorithmic reasoning capabilities. Our principal empirical conclusions are as follows: ", "page_idx": 6}, {"type": "text", "text": "1. Transformers excel at global reasoning tasks. Transformers outperform GNNs on tasks that require efficiently aggregating information about distant nodes in a graph, such as connectivity and shortest path. ", "page_idx": 6}, {"type": "text", "text": "2. GNNs uncover local graph structure with few samples. While transformers are capable of efficiently expressing all graph learning tasks under investigating, the structural limitations of GNNs provide them with favorable inductive biases for intrinsically local tasks, such as cycle check and node degree, and permit them to outperform transformers in a low-sample regime. ", "page_idx": 6}, {"type": "text", "text": "3. Trained transformers outperform LLM prompting. Transformers trained to explicitly solve graph reasoning tasks consistently attain greater accuracy across tasks than a variety of prompting strategies applied to more recent larger LMs. ", "page_idx": 6}, {"type": "text", "text": "A comprehensive evaluation of each GraphQA task on every training setting appears in Appendix E, in addition details about transformer training, the GraphQA benchmark, and alternative GNN and prompting approaches. ", "page_idx": 6}, {"type": "text", "text": "4.1 Transformers excel at global reasoning tasks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As indicated in Section 3, graph reasoning algorithms can be categorized based on the extent to which they entail aggregating \u201clocal\u201d information about nodes and their immediate neighbors or modeling \u201cglobal\u201d connections between nodes separated by a long distances. This section investigates the following question about transformers and long-distance reasoning: ", "page_idx": 6}, {"type": "text", "text": "When do transformers outperform GNNs on tasks that require global reasoning? ", "page_idx": 6}, {"type": "text", "text": "We consider two tasks that require reasoning across long distances in a graph instance: evaluating connectivity and computing the shortest path between a pair of nodes. Neither of these tasks can be solved by only investigating the neighbors of the source and sink node, which therefore implies that some analysis of global graph structure is necessary. ", "page_idx": 6}, {"type": "text", "text": "Figure 3a displays the accuracy of a variety of trained transformers and GNNs on the connectivity task contrastsing the performance of all such models when trained on 1,000 and 100,000 graph connectivity instances. In the most restricted sample complexity regime, trained GNNs are consistently more accurate than the small transformer; however, increasing the number of training samples yields a far more substantial improvement in the performance of the small transformer, which outperforms all GNNs trained on 100,000 samples. Notably, the pre-trained transformer, fine-tuned on just 1000 training instances, nearly solves the connectivity task. This suggests significant enhancements due to the larger model size and the data-rich fine-tuning phase. Figure 3b plots the training and test error of ten small transformers trained on connectivity datasets of increasing size and reveals a sharp and ", "page_idx": 6}, {"type": "table", "img_path": "AfzbDw6DSp/tmp/895e00bdf9041b4146670507e7947e29781a1c65f19957ede0c787c3fde474f1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "(a) Connectivity classification accuracy for trained transformers and GNNs. ", "page_idx": 7}, {"type": "image", "img_path": "AfzbDw6DSp/tmp/650bb6036770ac2d59a185d0fa9e6742c1cb77233cd91a3702f6e673fa7a74b8.jpg", "img_caption": ["Number of Training Samples (log scale) (b) Connectivity classification accuracy of 60M transformers by training set size. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "continual improvement in accuracy. The fine-tuned T5 transformer has similar accuracy to the most sample-rich small transformer and exceeds that of all GNNs. ", "page_idx": 7}, {"type": "text", "text": "On the other hand, Table 1 demonstrates that the MPNN GNN models outperform small transformers when trained to compute the shortest path, even on larger training datasets. However, the fine-tuned T5 model has far higher accuracy than all alternatives, even when trained only 1000 samples. ", "page_idx": 7}, {"type": "table", "img_path": "AfzbDw6DSp/tmp/e02d411c997a806718f47c2c255b22aa91d108580a58d045d8b6965f61ef9a72.jpg", "table_caption": ["Figure 3: Accuracy of a variety of trained transformers and GNNs on the connectivity task. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Theoretical interpretation: Because connectivity is the prototypical example of a task in the parallelizable class and can thus be efficiently implemented by LogDepth transformers with very small width (Theorem 2), the fact that small transformers succeed in solving nearly all connectivity instances is illuminating but not surprising. In contrast, message-passing GNNs are ", "page_idx": 7}, {"type": "text", "text": "Table 1: Transformers vs GNNs on shortest path: Fine-tuned large transformers outperform other transformers and GNNs, even the alternatives are trained on much larger training sets. ", "page_idx": 7}, {"type": "text", "text": "unable to solve connectivity in a similarly depth- and width-efficient manner due to fundamental capacity limitations. ", "page_idx": 7}, {"type": "text", "text": "Shortest path belongs to the search class of graph reasoning tasks and is NL-complete. Theorem 4 implies that shortest path is computable by LogDepthWide transformers, which are likely to require very large embedding dimension to be learnable by finite samples. This task can only be computed in a depth- and parameter-efficient manner if a variety of search tasks, including all-pairs shortest-path and diameter, are as well (Theorem 22). The fact that only the pre-trained model has nearly optimal performance on shortest path reinforces the theoretical intuition that solving shortest path requires a very large number of model parameters. ", "page_idx": 7}, {"type": "text", "text": "4.2 GNNs uncover local graph structure with few samples ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "While small transformers outperform GNNs on graph reasoning algorithms that entail analysis of long-range graph structure, their empirical successes are not uniform. Here, we investigate the following question: ", "page_idx": 7}, {"type": "text", "text": "When do GNNs outperform transformers on graph reasoning tasks? ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Figure 3b and Table 1 demonstrate that GNNs outperform small transformers in the low-sample regime, despite the sufficient expressivity of transformers. This gap in performance, which is reinforced for the node degree and cycle check tasks in Table 2, suggests that GNNs have a beneficial ", "page_idx": 7}, {"type": "table", "img_path": "AfzbDw6DSp/tmp/d0213fc16914ceb6e701770155322c20b694502a2d1b86ce63cf3880cb874cb0.jpg", "table_caption": [], "table_footnote": ["Table 3: Comparison of GraphQA task accuracies of transformers explicitly trained for graph reasoning and LLMs with a variety of prompting strategies. "], "page_idx": 8}, {"type": "text", "text": "Just as the bounded kernel size of convolutional neural networks (CNNs) enables the sample-efficient learning of relevant textures and gradients for image analysis, message-passing GNNs are unable to send messages instantaneously across multiple edges, which simplifies a search of the space of \u201cone-hop\u201d graph algorithms and represents a positive inductive bias. In ", "page_idx": 8}, {"type": "table", "img_path": "AfzbDw6DSp/tmp/3ada4051decea61f60f85a01115b2937a3940a1fa57fc2ab40f16f2383532802.jpg", "table_caption": ["inductive bias for learning graph reasoning tasks that can be solved by attending exclusively to local heuristics. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "contrast, the ability of transformers to Table 2: Transformers vs GNNs on cycle check and node send information between any pair of degree: GNNs are favorably biased for local structure. input tokens\u2014and the alternative inductive biases suggested by the input positional encoding\u2014likely induces a steeper sample complexity to learn node degree. ", "page_idx": 8}, {"type": "text", "text": "Theoretical interpretation: While model expressivity is necessary for learnability, it is not sufficient. The locality constraints of message-passing GNNs likely provides a favorable inductive bias for learning tasks like node degree with an exclusively on local structure that makes learning these tasks possible in a sample-efficient manner. While cycle check is more representationally difficult for GNNs than transformers in the worst case (see Appendix A.2), the random graphs sampled for the GraphQA benchmark have very small cycles (Figure 7) and do not resemble the large-diameter worst-case instance. ", "page_idx": 8}, {"type": "text", "text": "4.3 Trained transformers outperform LLM prompting ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Large language models (LLMs) are regularly evaluated by their reasoning abilities, and it remains an open research question to determine what kinds of training data best teaches models to solve logical problems. We investigate the extent to which LLMs can already perform graph reasoning tasks without being trained explicitly to do so. ", "page_idx": 8}, {"type": "text", "text": "Do transformers trained explicitly to solve graph reasoning tasks outperform prompt-tuning approaches on much larger LLMs? ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In Table 3, we contrast the capabilities of trained transformer models with several prompt-based approaches to querying LLMs. Task-specific transformers\u2014including the fine-tuned 11B transformers\u2014 consistently dominated the prompt-based approaches, despite the vast difference in parameter count and the almost certain presence of graph reasoning in the LLM\u2019s corpus. ", "page_idx": 8}, {"type": "text", "text": "Theoretical interpretation: While the representational capabilities of LLMs to solve reasoning tasks is much greater than that of small transformers, this performance gap suggests that their effective reasoning capacity is much weaker and that it may be improved by a richer training corpus that includes synthetic tasks. ", "page_idx": 8}, {"type": "text", "text": "Finally, we observe that the near-perfect performance of trained transformers on the node count, edge count, and edge existence is consistent with the representational easy of those tasks, as suggested by the existence of efficient Depth1 transformer implementations. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper provides a comprehensive evaluation of transformer models\u2019 graph reasoning capabilities, shedding light on their effectiveness across diverse graph reasoning tasks. By introducing a novel representational hierarchy, the study distinguishes between retrieval, parallelizable, and search reasoning tasks and offers insights into the performance of transformers at varying levels of granularity. The empirical investigation reveals that transformers exhibit strong performance in graph-based reasoning problems, often matching or surpassing specialized graph models. Furthermore, the study highlights transformers\u2019 exceptional ability to capture global graph patterns effectively, showcasing their capability in understanding long-range dependencies, a critical factor in solving tasks involving global graph structures. Overall, this work crystallizes precise representational trade-offs that reflect the fundamental reasoning capabilities of transformers and demonstrates that the tasks used to quantify those capabilities are indeed learnable in a sample-efficient and parameter-efficient manner. ", "page_idx": 9}, {"type": "text", "text": "While the hierarchy introduced by this work effectively separates graph algorithmic tasks into distinct equivalence classes with significant implications for their computability by transformers, several questions remain for future research. We focused on graph reasoning tasks due to their relevance to the broader context of transformers, GNNs, and parallel algorithms. However, the complexity classes presented here could potentially be extended to a wider range of algorithmic problems. While our assumption of unbounded-size MLPs provides strong lower bounds, further research into whether parallelizable tasks can be represented by transformers with bounded-size MLP units would complement this existing work. Broader experimental results that empirically evaluate the scaling laws would more directly assess the relevance of representational theoretical results to learnability. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Anders Aamand, Justin Y. Chen, Piotr Indyk, Shyam Narayanan, Ronitt Rubinfeld, Nicholas Schiefer, Sandeep Silwal, and Tal Wagner. Exponentially improving the complexity of simulating the weisfeiler-lehman test with graph neural networks, 2022. Cited on page 3.   \n[2] Ralph Abboud, Ismail Ilkan Ceylan, Martin Grohe, and Thomas Lukasiewicz. The surprising power of graph neural networks with random node initialization, 2021. URL https://arxiv. org/abs/2010.01179. Cited on page 3.   \n[3] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Cited on page 1.   \n[4] Alexandr Andoni, Zhao Song, Clifford Stein, Zhengyu Wang, and Peilin Zhong. Parallel graph connectivity in log diameter rounds. In FOCS, October 2018. Cited on pages 7, 17, and 18.   \n[5] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023. Cited on page 42.   \n[6] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In ICLR, 2014. Cited on page 1.   \n[7] Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018. Cited on page 1.   \n[8] Paul Beame, Paraschos Koutris, and Dan Suciu. Communication steps for parallel query processing. JACM, 64(6), oct 2017. Cited on page 19.   \n[9] James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. JMLR, 13(2), 2012. Cited on page 41.   \n[10] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. Graph of thoughts: Solving elaborate problems with large language models. AAAI, 38 (16):17682\u201317690, March 2024. Cited on page 1.   \n[11] Amartya Shankha Biswas, Talya Eden, Quanquan C. Liu, Slobodan Mitrovi\u00b4c, and Ronitt Rubinfeld. Massively parallel algorithms for small subgraph counting, 2022. Cited on pages 6 and 20.   \n[12] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. NeurIPS, 2020. Cited on pages 1 and 42.   \n[13] Quentin Cappart, Didier Ch\u00e9telat, Elias B Khalil, Andrea Lodi, Christopher Morris, and Petar Velic\u02c7kovic\u00b4. Combinatorial optimization and reasoning with graph neural networks. JMLR, 24 (130):1\u201361, 2023. Cited on page 1.   \n[14] Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, Xuanwen Huang, and Yang Yang. GraphLLM: Boosting graph reasoning ability of large language model, 2023. Cited on page 3.   \n[15] Ines Chami, Sami Abu-El-Haija, Bryan Perozzi, Christopher R\u00e9, and Kevin Murphy. Machine learning on graphs: A model and comprehensive taxonomy. JMLR, 23(89):1\u201364, 2022. Cited on page 1.   \n[16] Sam Coy and Artur Czumaj. Deterministic massively parallel connectivity. In STOC, STOC \u201922, June 2022. Cited on page 19.   \n[17] Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning. In ICLR, 2023. Cited on page 1.   \n[18] Artur Back de Luca and Kimon Fountoulakis. Simulation of graph algorithms with looped transformers, 2024. URL https://arxiv.org/abs/2402.01107. Cited on page 3.   \n[19] Jeffrey Dean and Sanjay Ghemawat. Mapreduce: Simplified data processing on large clusters. In OSDI\u201904: Sixth Symposium on Operating System Design and Implementation, pages 137\u2013150, San Francisco, CA, 2004. Cited on page 18.   \n[20] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Cited on page 1.   \n[21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. Cited on page 1.   \n[22] Vijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs. arXiv preprint arXiv:2012.09699, 2020. Cited on page 3.   \n[23] Paul Erd\u02ddos and Alfred R\u00e9nyi. On random graphs. Publicationes Mathematicae Debrecen, 6: 290\u2013297, 1959. Cited on page 41.   \n[24] Bahare Fatemi, Jonathan Halcrow, and Bryan Perozzi. Talk like a graph: Encoding graphs for large language models. In ICLR, 2024. Cited on pages 3, 7, 9, 40, 41, 42, and 43.   \n[25] Fabian Frei and Koichi Wada. Efficient circuit simulation in mapreduce. ArXiv, abs/1907.01624, 2019. Cited on page 20.   \n[26] Roy Frostig, Matthew James Johnson, and Chris Leary. Compiling machine learning programs via high-level tracing. Systems for Machine Learning, 4(9), 2018. Cited on page 41.   \n[27] Mohsen Ghaffari, Fabian Kuhn, and Jara Uitto. Conditional hardness results for massively parallel computation from distributed lower bounds. In FOCS, 2019. Cited on page 19.   \n[28] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In ICML, 2017. Cited on pages 1, 8, 9, 42, and 43.   \n[29] Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think before you speak: Training language models with pause tokens. In ICLR, 2024. Cited on pages 2, 3, and 16.   \n[30] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014. Cited on page 1.   \n[31] Yiding Hao, Dana Angluin, and Robert Frank. Formal language recognition by hard attention transformers: Perspectives from circuit complexity. TACL, 10:800\u2013810, 2022. Cited on page 2.   \n[32] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Cited on page 1.   \n[33] Bowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji, and Jiawei Han. Large language models on graphs: A comprehensive survey. arXiv preprint arXiv:2312.02783, 2023. Cited on page 3.   \n[34] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. NIPS, 2015. Cited on page 1.   \n[35] Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle, Pierre-luc Cantin, Clifford Chao, Chris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami, Rajendra Gottipati, William Gulland, Robert Hagmann, C. Richard Ho, Doug Hogberg, John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski, Alexander Kaplan, Harshit Khaitan, Daniel Killebrew, Andy Koch, Naveen Kumar, Steve Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle Lucke, Alan Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorson, Bo Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard Walter, Walter Wang, Eric Wilcox, and Doe Hyun Yoon. In-datacenter performance analysis of a tensor processing unit. SIGARCH Comput. Archit. News, 2017. Cited on page 41.   \n[36] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin \u017d\u00eddek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583\u2013589, 2021. Cited on page 1.   \n[37] Lukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In ICLR, 2016. Cited on page 1.   \n[38] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Cited on page 1.   \n[39] Howard Karloff, Siddharth Suri, and Sergei Vassilvitskii. A Model of Computation for MapReduce, pages 938\u2013948. Cited on pages 5 and 18.   \n[40] Nora Kassner, Oyvind Tafjord, Ashish Sabharwal, Kyle Richardson, Hinrich Schuetze, and Peter Clark. Language models with rationality. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, EMNLP, December 2023. Cited on page 1.   \n[41] Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu, and Deepak Ramachandran. LAMBADA: Backward chaining for automated reasoning in natural language. In ACL, 2023. Cited on page 1.   \n[42] Jinwoo Kim, Tien Dat Nguyen, Seonwoo Min, Sungjun Cho, Moontae Lee, Honglak Lee, and Seunghoon Hong. Pure transformers are powerful graph learners, 2022. Cited on pages 2, 3, and 31.   \n[43] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Cited on page 41.   \n[44] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. Cited on pages 1, 8, 9, 42, and 43.   \n[45] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. NeurIPS, 35:22199\u201322213, 2022. Cited on page 42.   \n[46] Nayoung Lee, Kartik Sreenivasan, Jason D Lee, Kangwook Lee, and Dimitris Papailiopoulos. Teaching arithmetic to small transformers. In ICLR, 2024. Cited on pages 1 and 3.   \n[47] Yuxuan Li and James L. McClelland. Systematic generalization and emergent structures in transformers trained on structured tasks, 2022. Cited on page 3.   \n[48] Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers learn shortcuts to automata, 2022. Cited on page 3.   \n[49] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In CVPR, 2021. Cited on page 1.   \n[50] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Cited on page 41.   \n[51] Andreas Loukas. What graph neural networks cannot learn: depth vs width. In ICLR, 2020. Cited on pages 1, 3, and 39.   \n[52] Eran Malach. Auto-regressive next-token predictors are universal learners, 2023. Cited on page 2.   \n[53] Amil Merchant, Simon Batzner, Samuel S Schoenholz, Muratahan Aykol, Gowoon Cheon, and Ekin Dogus Cubuk. Scaling deep learning for materials discovery. Nature, 624(7990):80\u201385, 2023. Cited on page 1.   \n[54] William Merrill and Ashish Sabharwal. A logic for expressing log-precision transformers, 2022. Cited on pages 2 and 7.   \n[55] William Merrill and Ashish Sabharwal. The parallelism tradeoff: Limitations of log-precision transformers. TACL, 11:531\u2013545, 2023. Cited on pages 2 and 32.   \n[56] William Merrill and Ashish Sabharwal. The expressive power of transformers with chain of thought, 2023. Cited on pages 2 and 6.   \n[57] William Merrill, Ashish Sabharwal, and Noah A. Smith. Saturated transformers are constantdepth threshold circuits, 2021. Cited on pages 2 and 6.   \n[58] Erxue Min, Runfa Chen, Yatao Bian, Tingyang Xu, Kangfei Zhao, Wenbing Huang, Peilin Zhao, Junzhou Huang, Sophia Ananiadou, and Yu Rong. Transformer for graphs: An overview from architecture perspective, 2022. Cited on page 3.   \n[59] Christopher Morris, Yaron Lipman, Haggai Maron, Bastian Rieck, Nils M Kriege, Martin Grohe, Matthias Fey, and Karsten Borgwardt. Weisfeiler and Leman go machine learning: The story so far. JMLR, 24(1):15865\u201315923, 2023. Cited on pages 3 and 40.   \n[60] Danupon Nanongkai and Michele Scquizzato. Equivalence classes and conditional hardness in massively parallel computations. Distributed Computing, 35(2):165\u2013183, January 2022. Cited on pages 5, 18, 19, and 20.   \n[61] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021. Cited on page 2.   \n[62] Jorge P\u00e9rez, Pablo Barcel\u00f3, and Javier Marinkovic. Attention is turing complete. J. Mach. Learn. Res., 22(1), jan 2021. ISSN 1532-4435. Cited on pages 2 and 3.   \n[63] Bryan Perozzi, Bahare Fatemi, Dustin Zelle, Anton Tsitsulin, Mehran Kazemi, Rami Al-Rfou, and Jonathan Halcrow. Let your graph do the talking: Encoding structured data for LLMs, 2024. Cited on pages 3, 42, and 43.   \n[64] Jackson Petty, Sjoerd van Steenkiste, Ishita Dasgupta, Fei Sha, Dan Garrette, and Tal Linzen. The impact of depth and width on transformer language model generalization. NAACL, 2024. Cited on page 1.   \n[65] Jacob Pfau, William Merrill, and Samuel R Bowman. Let\u2019s think dot by dot: Hidden computation in transformer language models. arXiv preprint arXiv:2404.15758, 2024. Cited on pages 2 and 16.   \n[66] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR, 21(1):5485\u20135551, 2020. Cited on pages 1, 7, and 42.   \n[67] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jeanbaptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Cited on page 1.   \n[68] Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. Self-supervised graph transformer on large-scale molecular data. NeurIPS, 33, 2020. Cited on page 3.   \n[69] Tim Roughgarden, Sergei Vassilvitskii, and Joshua R. Wang. Shuffles and circuits (on lower bounds for modern parallel computation). J. ACM, 65(6), nov 2018. Cited on page 19.   \n[70] Clayton Sanford, Daniel Hsu, and Matus Telgarsky. Representational strengths and limitations of transformers, 2023. Cited on pages 3, 31, 32, and 36.   \n[71] Clayton Sanford, Daniel Hsu, and Matus Telgarsky. Transformers, parallel computation, and logarithmic depth, 2024. Cited on pages 2, 3, 5, 6, 7, 16, 18, 21, 22, 23, 31, 32, and 39.   \n[72] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE transactions on neural networks, 20(1):61\u201380, 2008. Cited on page 1.   \n[73] Noam Shazeer. GLU variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Cited on page 41.   \n[74] Kaya Stechly, Matthew Marquez, and Subbarao Kambhampati. GPT-4 doesn\u2019t know it\u2019s wrong: An analysis of iterative prompting for reasoning problems, 2023. Cited on page 3.   \n[75] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Cited on page 1.   \n[76] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Cited on page 1.   \n[77] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. Cited on pages 1, 16, and 42.   \n[78] Petar Veli\u02c7ckovi\u00b4c and Charles Blundell. Neural algorithmic reasoning. Patterns, 2(7), 2021. Cited on page 1. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "[79] Petar Velic\u02c7kovic\u00b4, Adri\u00e0 Puigdom\u00e8nech Badia, David Budden, Razvan Pascanu, Andrea Banino, Misha Dashevskiy, Raia Hadsell, and Charles Blundell. The CLRS algorithmic reasoning benchmark. In ICML, 2022. Cited on page 1. ", "page_idx": 14}, {"type": "text", "text": "[80] Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and Yulia Tsvetkov. Can language models solve graph problems in natural language? In NeurIPS, 2023. Cited on pages 3 and 42. ", "page_idx": 14}, {"type": "text", "text": "[81] Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: a case study on approximating turing machines with transformers, 2021. Cited on page 2. ", "page_idx": 14}, {"type": "text", "text": "[82] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS, 2022. Cited on pages 1, 2, 16, and 42. ", "page_idx": 14}, {"type": "text", "text": "[83] Boris Weisfeiler and Andrei Leman. The reduction of a graph to canonical form and the algebra which appears therein. nti, Series, 2(9):12\u201316, 1968. Cited on pages 3 and 40. ", "page_idx": 14}, {"type": "text", "text": "[84] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks?, 2018. Cited on pages 1, 3, 8, 9, 40, 42, and 43. ", "page_idx": 14}, {"type": "text", "text": "[85] Andrew Chi-Chih Yao. Some complexity questions related to distributive computing(preliminary report). In STOC, page 209\u2013213, New York, NY, USA, 1979. Cited on page 36. ", "page_idx": 14}, {"type": "text", "text": "[86] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. NeurIPS, 2024. Cited on page 1. ", "page_idx": 14}, {"type": "text", "text": "[87] Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, and Yongfeng Zhang. Language is all a graph needs, 2023. Cited on page 3. ", "page_idx": 14}, {"type": "text", "text": "[88] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J. Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions?, 2019. Cited on page 2. ", "page_idx": 14}, {"type": "text", "text": "[89] Wojciech Zaremba and Ilya Sutskever. Learning to execute. In ICLR, 2015. Cited on page 1. ", "page_idx": 14}, {"type": "text", "text": "[90] Bohang Zhang, Shengjie Luo, Liwei Wang, and Di He. Rethinking the expressive power of GNNs via graph biconnectivity. In ICLR, 2023. Cited on pages 1 and 3. ", "page_idx": 14}, {"type": "text", "text": "[91] Yi Zhang, Arturs Backurs, S\u00e9bastien Bubeck, Ronen Eldan, Suriya Gunasekar, and Tal Wagner. Unveiling transformers with lego: a synthetic reasoning task, 2022. Cited on page 3. ", "page_idx": 14}, {"type": "text", "text": "[92] Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi. Teaching algorithmic reasoning via in-context learning. arXiv preprint arXiv:2211.09066, 2022. Cited on page 1. ", "page_idx": 14}, {"type": "text", "text": "[93] Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Josh Susskind, Samy Bengio, and Preetum Nakkiran. What algorithms can transformers learn? a study in length generalization. In ICLR, 2024. Cited on page 1. ", "page_idx": 14}, {"type": "text", "text": "A Theoretical preliminaries ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For some vector $\\boldsymbol{v}\\in\\mathbb{R}^{N}$ , let ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname{softmax}(v)={\\frac{1}{\\sum_{i=1}^{N}\\exp(v_{i})}}\\left(\\exp(v_{1}),\\dots,\\exp(v_{N})\\right)\\in\\mathbb{R}^{N}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let $e_{i}\\in\\mathbb{R}^{m}$ denote the $i$ th elementary vector, $\\vec{1}=(1,\\dots,1)\\in\\mathbb{R}^{m}$ and $\\vec{0}=(0,\\dots,0)\\in\\mathbb{R}^{m}$ . Let $[n]=\\{1,\\ldots,n\\}$ . ", "page_idx": 14}, {"type": "text", "text": "A.1 Transformer models ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We use a similar theoretical definition of a multi-layer transformer model to the one employed by [71]. We use a bounded communication theoretical model of the standard bidirectional transformer of [77], which assumes that the principal limitation of the transformer\u2019s representational power is the low rank of the self-attention matrix. Several theoretical assumptions are necessary to model transformers in this regime: ", "page_idx": 15}, {"type": "text", "text": "\u2022 the interleaved element-wise multi-layer perceptron (MLP) units compute arbitrary functions;   \n\u2022 the embedding dimension $m$ and number of heads per layer $H$ are much smaller than the sequence length $N$ ; and   \n\u2022 all model parameters, inputs, and intermediate products can be represented with ${\\cal O}(\\log N)$ - bit numbers. ", "page_idx": 15}, {"type": "text", "text": "These assumptions are justified in greater detail by [71]. ", "page_idx": 15}, {"type": "text", "text": "Furthermore, we also model transformers as having $N^{\\prime}$ additional \u201cpause token\u201d inputs, which are embeddings that contain no information relevant information about the input instance, but which provide a longer \u201cwork tape\u201d that can be utilized for computational purposes [see e.g. 29, 65]. While structurally similar to chain-of-thought reasoning [82], these pause tokens are not determined by multiple auto-regressive passes over the transformer and do not encode useful information as input. Such pause tokens exist both in the theoretical results of Section 3 (as blank tokens at the end of the sequence) and in the empirical results of Section 4 (as placeholders between the graph representation and the task query for graphs that can be represented in fewer tokens than the maximum sequence length). ", "page_idx": 15}, {"type": "text", "text": "Definition 1. Let Transformer $\\stackrel{N,N^{\\prime}}{m,H,L}$ denote the family of all bidirectional transformers with embedding dimension $m$ , number of heads $H$ , and depth $L$ , which operate on input sequences on length $N$ with $N^{\\prime}$ blank pause tokens appended to the end5. Any transformer $f\\in$ Transforme rm,H,L is a function of the form $f:\\mathbb{R}^{N\\times d}\\rightarrow\\mathbb{R}^{N}$ for some input dimension $d$ , which parameterized by query, key, and value matrices ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q_{\\ell,h},K_{\\ell,h},V_{\\ell,h}\\in\\mathbb{R}^{m\\times m},\\;\\mathbf{for}\\;\\ell\\in[L],\\;h\\in[H],}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and arbitrary element-wise MLPs with positional embeddings $\\phi^{0},\\ldots,\\phi^{L}$ and $\\psi$ with ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi^{0}:\\mathbb{R}^{d}\\times\\mathbb{N}\\rightarrow\\mathbb{R}^{m},}\\\\ &{\\phi^{\\ell}:\\mathbb{R}^{m}\\times\\mathbb{N}\\rightarrow\\mathbb{R}^{m},\\;\\mathrm{for}\\;\\ell\\in\\{1,\\dots,L\\},}\\\\ &{\\psi:\\mathbb{R}^{m}\\times\\mathbb{N}\\rightarrow\\mathbb{R},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where for sequence $Y$ of length $N+N^{\\prime}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\phi^{\\ell}(Y)=(\\phi^{\\ell}(y_{1},1),\\ldots,\\phi^{\\ell}(y_{N+N^{\\prime}},N+N^{\\prime})).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To evaluate $f(X)$ on some input sequence $X\\,=\\,\\left(x_{1},\\ldots,x_{N}\\right)\\,\\in\\,\\mathbb{R}^{N\\times d}$ , we define intermediate embeddings $Y^{0},\\ldots,Y^{L}\\in\\mathbb{R}^{(N+N^{\\prime})\\times m}$ as follows: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The initial embedding is computed by applying the first MLP $\\phi^{0}$ to all elements of the input $X$ , along with $N^{\\prime}$ \u201cblank\u201d inputs: $Y^{0}=\\phi^{0}(X)=(\\phi^{0}(x_{1},1),\\ldots,\\phi^{0}(x_{N},N),\\phi^{0}(0,N+1),\\ldots,\\phi^{0}(0,N+N^{\\prime})).$ \u2022 The intermediate embeddings $Y^{\\ell}$ for $\\ell\\in\\{1,\\ldots,L\\}$ are computed by applying a unit of multi-headed attention (parameterized by $Q_{\\ell,h},K_{\\ell,h},V_{\\ell,h}$ for all $h\\in[H])$ to the previous embedding $Y^{\\ell-1}$ , followed by an element-wise application of the MLP $\\phi^{\\ell}$ . That is, ", "page_idx": 15}, {"type": "equation", "text": "$$\nY^{\\ell}=\\phi^{\\ell}\\left(\\sum_{h=1}^{H}\\mathrm{softmax}\\left(\\phi(Y^{\\ell-1})Q_{h}K_{h}^{\\mathsf{T}}\\phi(Y^{\\ell-1})^{\\mathsf{T}}\\right)\\phi(Y^{\\ell-1})V_{h}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "5If N \u2032 = 0, we denote the family as TransformerNm,H,L. ", "page_idx": 15}, {"type": "text", "text": "\u2022 The output $f(X)\\in\\mathbb{R}^{N}$ is computed to be the first $N$ outputs of $\\psi(Y^{L})$ . That is, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f(X)=(\\psi(Y_{1}^{L},1),\\dots,\\psi(Y_{N}^{L},N)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "While our theoretical results pertain to the bidirectional regime (where no masking is applied to the self-attention layer), all results in Appendix $\\mathbf{C}$ and all lower bounds in Appendix B apply to autoregressive transformers as well. Our empirical results utilize causal masking. ", "page_idx": 16}, {"type": "text", "text": "Furthermore, an $L$ -layer bidirectional transformer that operates on sequences of length $N$ with $N^{\\prime}$ pause tokens can be simulated by and $L$ -layer autoregressive transformer with $O(L\\cdot(\\bar{N}+N^{\\prime}))$ pause tokens. In the regime where $N^{\\prime}=\\mathrm{{poly}}(N)$ and $\\bar{L}=O(\\log N)$ , any positive results for bidirectional models likewise apply to autoregressive models. ", "page_idx": 16}, {"type": "text", "text": "A.2 Graph reasoning tasks ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We exclusively consider graph reasoning tasks that apply to undirected and unweighted graphs $G=(V,E)$ of bounded size, i.e. $|V|+|E|=O(N)$ for some size parameter $N$ . We define the tasks used for experimentation as follows: ", "page_idx": 16}, {"type": "text", "text": "\u2022 Node count: Given graph $G$ , compute $|V|$ .   \n\u2022 Edge count: Given graph $G$ , compute $|E|$ .   \n\u2022 Edge existence: Given graph $G$ and vertices $u,v\\in V$ , return $1\\left\\{(u,v)\\in E\\right\\}$ .   \n\u2022 Node degree: Given graph $G$ and vertex $u\\in V$ , return $\\deg(u)=|(u,v)\\in E|$ .   \n\u2022 Connectivity: Given graph $G$ and vertices $u,v\\in V$ , return 1 if there exists a path of edges between $u$ and $v$ and 0 otherwise.   \n\u2022 Cycle check: Given graph $G$ , return 1 if there exists a cycle of any length $\\geq\\;3$ and 0 otherwise.   \n\u2022 Shortest path: Given graph $G$ and vertices $u,v\\in V$ , return the smallest number of edges that forms a path between $u$ and $v$ if one exists and $-1$ otherwise.   \n\u2022 Triangle counting: Given graph $G$ , return the number of triangles in the graph: $|\\{(u,\\bar{v},w):(u,v),(v,w),(u,\\bar{w})^{*}\\in E\\}|$ . ", "page_idx": 16}, {"type": "text", "text": "These tasks are foundational to computer science and are solved by famous polynomial-time algorithms, such as Dijkstra\u2019s algorithm for shortest path and Kruskal\u2019s for spanning trees. Several of these tasks reap significant algorithmic benefits from parallel computing models, including graph connectivity, which can be evaluated in a logarithmic number of rounds by employing an \u201citerative merging\u201d strategy [4]. ", "page_idx": 16}, {"type": "text", "text": "The amenability of some of these tasks to parallel computing is the core principle underlying our theoretical results on transformer model capacity and the resulting contrasts with GNNs. For instance, the iterative merging algorithm for connectivity can be simulated by a logarithmic-depth transformer, but message-passing GNNs cannot do without a substantial width scaling (see Appendix D). Furthermore, this parallel computing algorithm is widely believed to be optimal, and a crystallization of this as Conjecture 13 is the bedrock of our transformer optimality conjectures. ", "page_idx": 16}, {"type": "text", "text": "The multi-layer results of Appendix B are representation-agnostic; the bounds apply to any fixed encoding of vertices and edges as a transformer input sequence $X\\in\\mathbb{R}^{N\\times d}$ . The input representation for Appendix C must satisfy the more specified node/edge encoding scheme, which represents each vertex and edge as a single embedding, followed by a query embedding (with optional blank embeddings as needed). This encoding scheme is reflected by Figure 1 and closely resembles the graph instance encoding used for the experimental results6. ", "page_idx": 16}, {"type": "text", "text": "Throughout the paper, we partition these and other graph reasoning tasks into three representational categories: retrieval, parallelizable, and search tasks. The retrieval category contains only four of the tasks used for experiments: ", "page_idx": 16}, {"type": "text", "text": "On the other hand, the other categories reflect two equivalence classes introduce by [60]: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Parallelizable tasks are defined to be L-complete and equivalent to connectivity in $O(1)$ rounds of MPC. These tasks include connectivity, cycle check, planarity testing, minimum cut, bipartiteness testing, minimum spanning forest, connected components, one-cycle versus two-cycles (see Conjecture 13), and # connected components. \u2022 Search tasks are defined to be NL-complete and equivalent to shortest path in $O(1)$ rounds of MPC. These tasks include shortest path, strong connectivity (for directed graphs), all-pairs shortest path, median, diameter, radius, directed cycle detection, and $^{s t}$ -reachability. ", "page_idx": 17}, {"type": "text", "text": "B Multi-layer transformers and parallelizable/search graph reasoning tasks ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this appendix, we formalize and prove all results in Sections 3.1 and 3.2 that pertain to the parallelizable and search categories of graph reasoning tasks. ", "page_idx": 17}, {"type": "text", "text": "The primary technical tool used to establish these results is an improved relationship between the MPC model of distributed communication of [39] and our theoretical model of a transformers (Appendix A.1). This result is presented informally as Theorem 1, and formally as Theorem 8. Because graph reasoning tasks are well studied in the MPC model of computation [e.g. 4, 60], this theorem enables the transfer of those positive results to transformer architectures. Similarly, negative results that pertain to the MPC model, including those conditioned on the well-known one-cycle versus two-cycle conjecture (Conjecture 13), imply negative results for transformers. ", "page_idx": 17}, {"type": "text", "text": "Theorem 8 (Formal version of Theorem 1; transformers simulate MPC). For constants $0<\\delta<$ $\\delta^{\\prime}\\ <\\ 1$ and $\\gamma\\;>\\;0$ , an $R$ -round deterministic $(\\gamma,\\delta)$ -MPC protocol with $n$ inputs can be simulated by a transformer $f\\ \\in$ Transformer $\\stackrel{N,N^{\\prime}}{m,H,L}$ with depth $L\\;=\\;O(R),$ , single heads $H\\,=\\,1$ embedding dimension $m\\,=\\,{\\cal{O}}(n^{\\delta^{\\prime}})$ , context length $N\\,=\\,n_{i}$ , and blank chain-of-thought tokens $N^{\\prime}=\\operatorname*{max}(0,O(n^{1+\\gamma-\\delta})-n)$ . ", "page_idx": 17}, {"type": "text", "text": "We formally introduce the MPC distributed computing model in Appendix B.1, along with the one-cycle versus two-cycle conjecture and a collection of positive results from [60]. In Appendix B.2, we formally present the task-specific graph reasoning results introduced in Sections 3.1 and 3.2 and prove them as implications of Theorem 8. We finally contextualize and prove Theorem 8 in Appendix B.3 by modifying the proof strategy of a similar result [71] and by showing that MPC protocols can be simulated by a weaker model of distributed computation. ", "page_idx": 17}, {"type": "text", "text": "B.1 MPC preliminaries ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The massively parallel computation (MPC) model of [39] formalizes distributed computing frameworks such as MapReduce [19] as theoretical models that are amenable to rigorous analysis. MPC pertains to a regime where an input that consists of $n$ \u201cwords\u201d is distributed across a very large number of machines $q$ (e.g. $q\\approx n^{0.95}$ ), each of which contains a bounded local memory $s$ (e.g. $s\\approx n^{0.1}$ ) where computation on individual machines is inexpensive but communication between machines is costly. We use the definition of MPC of [4], which quantifies the complexity of a protocol by the local memory $s=O(n^{\\delta})$ , the global memory $s q=\\dot{O}(n^{1+\\gamma})$ , and the number of communication rounds $R$ . ", "page_idx": 17}, {"type": "text", "text": "Definition 2. For global and local memory constants $\\gamma,\\delta>0$ and input size $n$ , an $R$ -round $(\\gamma,\\delta)$ - MPC protocol specifies a distributed computing protocol over $q=\\Theta(n^{1+\\gamma-\\delta})$ machines, each of which contains a local memory of size $s=O(n^{\\delta})$ . ", "page_idx": 17}, {"type": "text", "text": "\u2022 An input to the protocol, which is encoded as a length- ${\\cdot n}$ sequence of $p\\,=\\,\\Theta(\\log n)$ -bit words, is distributed across the local memories of the first $\\lceil n/s\\rceil$ machines. \u2022 In each of the $R$ rounds, each machine computes an arbitrary function its local memory at the time, which specifies at most $s$ words to be transmitted to other machines. \u2022 Messages are simultaneously transmitted (subject to the constraint that each machine sends and receives at most $s$ words of information), and each machine\u2019s new local memory is set equal to the messages received. ", "page_idx": 17}, {"type": "text", "text": "\u2022 After the final round, the output of the protocol is a concatenation of the local memories of the first $\\lceil n/s\\rceil$ machines. ", "page_idx": 18}, {"type": "text", "text": "We say that an MPC protocol computes some function $f:\\mathbb{Z}_{2^{p}}^{n}\\rightarrow\\mathbb{Z}_{2^{p}}^{n}$ if for any $X\\in\\mathbb{Z}_{2^{p}}^{n}$ encoded as input to the protocol, the output of the protocol is $f(X)$ . ", "page_idx": 18}, {"type": "text", "text": "B.1.1 MPC and graph reasoning tasks ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we introduce previously known results about the abilities of Massively Parallel Computation protocols to solve graph reasoning tasks. The novel results presented in the subsequent section are the primarily the immediate implications of these results and Theorem 8. ", "page_idx": 18}, {"type": "text", "text": "MPC round equivalence for parallelizable and search tasks These results largely reflect the contributions of [60], which establish a hierarchy of graph reasoning tasks that is reflected in our distinction between parallelizable and search tasks. They show that all parallelizable tasks are equivalent to connectivity in $O(1)$ rounds of MPC and that all search tasks are likewise equivalent to shortest path. (See their Figure 1.) Concretely, they prove the following for all graphs $\\bar{G}=(V,E)$ with $|V|^{\\frac{}{}}\\!+|E|=O(n)$ . ", "page_idx": 18}, {"type": "text", "text": "Theorem 9 (Theorem 4 of [60]; round-equivalence of parallelizable tasks). If there exists an $R$ - round $(\\gamma,\\delta)$ -MPC protocol for any parallelizable task\u2014including graph connectivity, $^{s t}$ -connectivity, cycle detection, formula evaluation, planarity testing, graph bipartiteness, connected components, minimum spanning forest, and minimum cut, one-cycle versus two-cycle testing\u2014for some $\\gamma>0$ and $\\delta\\in(0,1)$ , then there exists an $R^{\\prime}$ -round $(\\gamma^{\\prime},\\delta)$ -MPC protocol for any other other parallelizable task where $R^{\\prime}=R+O(1)$ and $\\gamma^{\\prime}=\\operatorname*{max}(\\gamma,3)$ . ", "page_idx": 18}, {"type": "text", "text": "Theorem 10 (Theorem 5 of [60]; round-equivalence of search tasks). If there exists an $R$ -round $(\\gamma,\\delta)$ -MPC protocol for any search task\u2014including $^{s t}$ -reachability (for directed graphs), strong connectivitity, directed cycle detection, shortest path, all-pairs shortest path, diameter, radius, and median\u2014for some $\\gamma>0$ and $\\delta\\in(0,1)$ , then there exists an $R^{\\prime}$ -round $(\\gamma^{\\prime},\\delta)$ -MPC protocol for any other other search task where $R^{\\prime}=R+O(1)$ and $\\gamma^{\\prime}=\\operatorname*{max}(\\gamma,2)$ . ", "page_idx": 18}, {"type": "text", "text": "The equivalance of Theorem 9 has more immediate implications for the round complexity of parallelizable tasks than Theorem 10 does for search tasks because the round complexity of graph connectivity is well-understood to be $O(\\log n)$ and thought to be optimal. We first present a deterministic MPC positive result for graph connectivity. ", "page_idx": 18}, {"type": "text", "text": "Theorem 11 (Theorem 6.2 of [16]; log-round connectivity MPC algorithm). For any $\\gamma>0$ and $\\delta~\\in~(0,1)$ , there exists a deterministic $O(\\log n)$ -round $(\\gamma,\\delta)$ -MPC protocol that solves graph connectivity on any graph $G=(V,E)$ of size $|V|+|E|={\\mathcal{O}}(n)$ . ", "page_idx": 18}, {"type": "text", "text": "Hence, all parallelizable tasks can be solved with a logarithmic number of MPC rounds with arbitrarily small polynomial local memory. ", "page_idx": 18}, {"type": "text", "text": "Corollary 12 (Log-round parallelizable MPC algorithms). For any $\\gamma>0$ and $\\delta\\in(0,1)$ and any parallelizable task, there exists a deterministic $O(\\log n)$ -round $(\\gamma^{\\prime},\\delta)$ -MPC protocol that solves the task on any graph $G=(V,E)$ of size $|V|+|E|=O(n).$ for $\\gamma^{\\prime}=\\operatorname*{max}(\\gamma,3)$ . ", "page_idx": 18}, {"type": "text", "text": "The optimality of these logarithmic-round protocols is suggested by a conjecture about the hardness of distinguishing between two graphs with $n$ vertices and $n$ edges, one of whose edges are arranged in a single cycle of length $n$ and the other in two disjoint cycles of length $\\begin{array}{l}{{\\frac{n}{2}}}\\end{array}$ . This is the well-known \u201cone-cycle versus two-cycle conjecture,\u201d which is widely employed as a condition for distributed computing hardness [see e.g. 8, 69, 27]. ", "page_idx": 18}, {"type": "text", "text": "Conjecture 13 (One-cycle versus two-cycle conjecture, see e.g. [27]). For any $\\gamma>0$ and $\\delta\\in(0,1)$ , any $(\\gamma,\\delta)$ -MPC protocol that distinguishes a single length-n cycle from two disjoint length- $\\begin{array}{l}{{\\frac{n}{2}}}\\end{array}$ cycles uses $R=\\Omega(\\log n)$ rounds. ", "page_idx": 18}, {"type": "text", "text": "Under this conjecture, Theorem 9 immediately implies the optimality of Corollary 12. ", "page_idx": 18}, {"type": "text", "text": "Corollary 14 (Optimality of log-round parallelizable MPC algorithms). Conditional on Conjecture 13, for any $\\gamma>0$ and $\\delta\\in(0,1)$ , any $(\\gamma,\\delta)$ -MPC protocol that solves a parallelizable graph task on all graphs $G=(V,E)$ of size $|V|+|E|=O(n)$ uses $R=\\Omega(\\log n)$ rounds. ", "page_idx": 18}, {"type": "text", "text": "The round complexity of search tasks is less understood, and it is unknown if search tasks can be solved by $O(\\log n)$ -round $(\\gamma,\\delta)$ -MPC protocols if $\\delta\\in(0,\\frac{1}{2}]$ . ", "page_idx": 18}, {"type": "text", "text": "MPC protocols for problems with bounded circuit size More general MPC constructions are known for problems that solved by bounded-size boolean circuits, which include both parallelizable and search tasks. The well-known ${\\mathsf{N C}}^{i}$ classes of Boolean circuits that take $N$ inputs and have poly $(n)$ gates and depth $O(\\log^{i}n)$ have been shown to be computable by bounded-round MapReduce-like computational protocols [25] and by MPC protocols in particular [60]. ", "page_idx": 19}, {"type": "text", "text": "Theorem 15 (Theorem 1 of [60]; log-round circuit MPC algorithms). For any problem in $\\mathsf{N C}^{i+1}$ and any $\\gamma>0$ and $\\begin{array}{r}{\\delta\\in\\left(\\frac{1}{2},1\\right)}\\end{array}$ , there exists a deterministic $O(\\log^{i}n)$ -round $(\\gamma,\\delta)$ -MPC protocol that solves the problem. ", "page_idx": 19}, {"type": "text", "text": "Since L and $\\mathsf{N L}$ are known to belong to $\\mathsf{N C}^{2}$ , the following corollary is an immediate consequence. ", "page_idx": 19}, {"type": "text", "text": "Corollary 16 (Log-round parallelizable and search MPC algorithms with high local memory). For any parallelizable or search graph reasoning task and any $\\gamma\\,>\\,0$ and $\\delta\\ \\in\\textstyle\\left(\\frac{1}{2},1\\right)$ , there exists $a$ deterministic $O(\\log n)$ -round $(\\gamma,\\delta)$ -MPC protocol that solves the task on all graphs $G=(V,E)$ of size $|V|+|E|=O(N)$ . ", "page_idx": 19}, {"type": "text", "text": "Note that these results p\u221aertain exclusively to the \u201clarge local memory regime,\u201d where each machine has memory at $s=\\omega({\\sqrt{n}})$ . Therefore, this does not guarantee the existence of a $O(\\log n)$ -round MPC solution for any search task or for any parallelizable task with $\\gamma<1$ . ", "page_idx": 19}, {"type": "text", "text": "MPC protocol for triangle counting Finally, the triangle counting task can be solved in the MPC framework by utilizing a special case of a parallel algorithm for clique counting. These pertain to graphs with bounded arboricity $\\alpha$ , a quantity that corresponds to the branching factor of a node that is bounded by the degree of the graph; these apply to arbitrary graphs by noting that $\\alpha\\leq|V|$ . ", "page_idx": 19}, {"type": "text", "text": "Theorem 17 (Theorem 1.5 of [11]; loglog-round triangle counting MPC algorithm). For any $k\\geq3$ , $\\delta\\ \\in\\ (0,1)$ , and $\\gamma\\,>\\,0$ , there exists an $O(\\log\\log n)$ -round $(\\gamma,\\delta)$ -MPC protocol that computes the number of $k$ -cliques in any graph $G\\,=\\,(V,E)$ with $|V|+|E|=O(n)$ and arboricity $\\alpha=$ $O(n^{\\gamma/(k-2)})$ . ", "page_idx": 19}, {"type": "text", "text": "B.2 Positive and negative graph reasoning results for multi-layer transformers ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We state formalized versions of the statements of Sections 3.1, 3.2 and 3.4 and prove them by invoking Theorem 8 jointly with the relationships between MPC and graph reasoning of Appendix B.1.1. ", "page_idx": 19}, {"type": "text", "text": "B.2.1 Parallelizable task results (Section 3.1) ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For the duration of this section, the class of parallelizable tasks includes all of those that are deemed equivalent to graph connectivity in $O(1)$ rounds of MPC by [60], as stated in Theorem 9. ", "page_idx": 19}, {"type": "text", "text": "We first present a statement that formalizes the existence of logarithmic-depth transformer constructions for solving parallelizable tasks. ", "page_idx": 19}, {"type": "text", "text": "Theorem 18 (Formal version of Theorem 2; log-depth transformers compute parallelizable tasks). For any $\\epsilon\\,\\in\\,(0,1)$ and any parallelizable task, there exists a transformer $f\\in$ Transformer $.N,N^{\\prime}$ $\\stackrel{\\scriptscriptstyle I\\,\\bullet\\,,\\iota\\,\\bullet}{m,H,L}$ such that $f(X)$ computes the task where $X\\in\\mathbb{R}^{N}$ is some encoding of any graph $G=(V,E)$ with $|V|+|E|=\\dot{O}(N)$ and $f$ has depth $L=O(\\log N)$ and heads $H=O(1)$ and embedding dimension $m$ and pause tokens $N^{\\prime}$ satisfying either ", "page_idx": 19}, {"type": "text", "text": "\u2022 LogDepthPause: $m=O(N^{\\epsilon})$ and $N^{\\prime}=O(N^{4-\\epsilon^{\\prime}})$ , where $\\epsilon^{\\prime}\\in(0,\\epsilon)$ ; or \u2022 LogDepthWide: $m=O(N^{\\epsilon})$ and $N^{\\prime}=0$ , $i f\\epsilon>\\frac{1}{2}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. The first bullet is an immediate implication of Corollary 12 and Theorem 8 with $\\gamma\\,=\\,3$ , $\\delta^{\\prime}=\\epsilon$ , and $\\delta=\\epsilon^{\\prime}$ . The second bullet follows from Corollary 16 and Theorem 8 with $\\gamma=\\delta=\\frac{\\epsilon}{2}$ and $\\delta^{\\prime}=\\epsilon$ . \u53e3 ", "page_idx": 19}, {"type": "text", "text": "We then establish that sub-logarithmic-depth solutions to any parallelizable task are impossible without having linear embedding dimension $m$ or super-polynomial number of pause tokens $N^{\\prime}$ , under the assumption that the one-cycle versus two-cycle conjecture holds. ", "page_idx": 19}, {"type": "text", "text": "Theorem 19 (Formal version of Theorem 3; log-depth optimality for parallelizable tasks). Conditional on Conjecture $I3$ , for any $\\epsilon\\in(0,1)$ and $\\gamma>0$ and any parallelizable task, if there exists $a$ transformer $f\\in$ Transforme $\\stackrel{N,N^{\\prime}}{m,H,L}$ that solves the task and has width $m H=O(N^{\\epsilon})$ and pause tokens $N+N^{\\prime}=O(N^{1+\\gamma})$ , then its depth satisfies $L=\\Omega(\\log N)$ . ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Proof. The proof is a consequence of Corollary 14 and a result of [71] that proves the to simulate transformers with MPC protocols, an inversion of Theorem 8. We restate that result as follows. ", "page_idx": 20}, {"type": "text", "text": "$\\stackrel{N,N^{\\prime}}{m,H,L}$ hf owr isdothm $m H\\;=\\;{\\cal O}(N^{\\delta})$ ffoorr  asnoym $\\delta~\\in~(0,1)$ $f~\\in$ $N+N^{\\prime}=O(N^{1+\\gamma})$ $\\gamma\\ge0$ $\\delta^{\\prime}\\,\\in\\,(\\delta,1)$ $\\gamma^{\\prime}\\,=\\,1\\,+\\,2\\gamma\\,+\\,\\delta^{\\prime}$ exists an $\\begin{array}{r l r}{\\lefteqn{O\\big(\\frac{L(1+\\gamma)}{\\delta^{\\prime}-\\delta}\\big)}}\\end{array}$ -round $(\\gamma^{\\prime},\\delta^{\\prime})$ -MPC protocol that simulates $f$ . ", "page_idx": 20}, {"type": "text", "text": "Consider a transformer $f\\in$ Transformer $\\stackrel{N,N^{\\prime}}{m,L,H}$ that solves the task with width $m H=O(N^{\\epsilon})$ and total sequence length $N+N^{\\prime}=O(N^{1+\\gamma}$ for some constant $\\epsilon\\,\\in\\,(0,1)$ and $\\gamma>1$ . Then, there exists an $O_{\\epsilon,\\gamma}(L)$ -round $(1+2\\gamma+\\sqrt{\\epsilon},\\sqrt{\\epsilon})$ )-MPC protocol that solves the parallelizable task. If Conjecture 13 is true, then $L=\\Omega(\\log N)$ . \u53e3 ", "page_idx": 20}, {"type": "text", "text": "B.2.2 Search task results (Section 3.2) ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We present the main result of Section 3.2 and show an equivalence between the minimum depth transformer needed to solve search tasks in a width-efficient manner. As before, the class of searchable tasks includes tasks that are equivalent to shortest path in $O(1)$ MPC rounds (Theorem 10). ", "page_idx": 20}, {"type": "text", "text": "Theorem 21 (Formal version of Theorem 4; LogDepthWide computes search tasks). For any $\\textstyle\\epsilon\\in({\\frac{1}{2}},1)$ and any search task, there exists a transformer $f\\in$ Transformer $\\stackrel{N,N^{\\prime}}{m,H,L}$ such that $f(X)$ computes the task where $X\\in\\mathbb{R}^{N}$ is some encoding of any graph $G=(V,E)$ with $|V|{+}|E|=O(N)$ , and $f$ has depth $L=O(\\log N)$ , heads $H=O(1)$ , embedding dimension $m=O(N^{\\epsilon}$ , and no pause tokens $'N^{\\prime}=0_{.}$ ). ", "page_idx": 20}, {"type": "text", "text": "Proof. As before, the proof is an immediate consequence of Corollary 16 and Theorem 8. ", "page_idx": 20}, {"type": "text", "text": "While both search and parallelizable tasks are computable logarithmic depth of considerable width, the minimum depth transformer of width $O(N^{\\epsilon})$ for small $\\epsilon$ that computes search tasks is unknown. Despite this deficit, we can still establish the representation similarity of all search tasks by showing that their minimum depths vary by at most a constant additive factor. ", "page_idx": 20}, {"type": "text", "text": "Theorem 22 (Depth-equivalence of search tasks). Suppose for some $\\epsilon\\,\\in\\,(0,1)$ and $\\gamma>0$ there exists a transformer $f\\ \\in$ Transformer $\\stackrel{N,N^{\\prime}}{m,H,L}$ with embedding dimension $m\\,=\\,{\\cal O}(N^{\\epsilon})$ and total sequence length $N+N^{\\prime}=O(N^{1+\\gamma})$ that computes the some search task on all graphs $G=(V,E)$ of size $|V|+|E|=O(N)$ . Then, for any other search task, there exists some transformer $f^{\\prime}\\in$ TransformerNm\u00af,, NH\u00af \u2032,L\u00af with embedding dimension m\u00af = O(m), depth L\u00af = L + O(1), and pause tokens $\\bar{N}^{\\prime}=N^{\\prime}+O(N^{3})$ that computes that search task. ", "page_idx": 20}, {"type": "text", "text": "Proof. This statement is an immediate consequence of Theorem 10 and Theorem 8. ", "page_idx": 20}, {"type": "text", "text": "B.2.3 Clique counting task results (Section 3.4) ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We prove the existing of doubly-logarithmic-depth transformer that solves the triangle counting task giving a more general result that counts the number of $k$ -cliques in any graph of bounded arboricity. ", "page_idx": 20}, {"type": "text", "text": "Theorem 23 (Generalization of Theorem 7; loglog-depth computes clique counting). For any fixed $\\epsilon\\in(0,1)$ and $k\\geq3,$ , there exists a transformer $f\\in$ Transforme $\\stackrel{N,N^{\\prime}}{m,H,L}$ with embedding dimension $m=O(n^{\\epsilon})$ , heads $H=O(1)$ , depth $L=O(\\log\\log n)$ , and chain-of-thought tokens ", "page_idx": 20}, {"type": "equation", "text": "$$\nN^{\\prime}=\\left\\{\\!\\!\\begin{array}{l l}{{O(\\alpha^{k-2}N^{1-\\epsilon})}}&{{i f\\alpha=\\Omega(N^{\\epsilon/(k-2)}),}}\\\\ {{0}}&{{o t h e r w i s e.}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "that counts the number of $k$ -cliques in all graphs $G=(V,E)$ of arboricity $\\alpha$ and size $|V|+|E|=$ $O(N)$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. This is the immediate implication of Theorem 8 and Theorem 23. ", "page_idx": 21}, {"type": "text", "text": "B.3 Proof of Theorem 8 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Our principal theoretical result establishes that any MPC protocol with sublinear local memory can be simulated by a transformer with sublinear embedding dimension. ", "page_idx": 21}, {"type": "text", "text": "Theorem 8 (Formal version of Theorem 1; transformers simulate MPC). For constants $0<\\delta<$ $\\delta^{\\prime}\\ <\\ 1$ and $\\gamma\\;>\\;0$ , an $R$ -round deterministic $(\\gamma,\\delta)$ -MPC protocol with n inputs can be simulated by $a$ transformer $f\\ \\in$ Transformer $\\stackrel{N,N^{\\prime}}{m,H,L}$ with depth $L\\;=\\;O(R),$ , single heads $H\\,=\\,1$ , embedding dimension $m\\,=\\,{\\cal{O}}(n^{\\delta^{\\prime}})$ , context length $N\\,=\\,n_{i}$ , and blank chain-of-thought tokens $N^{\\prime}=\\operatorname*{max}(0,O(n^{1+\\gamma-\\delta})-n)$ . ", "page_idx": 21}, {"type": "text", "text": "This offers an improvement on Theorem 3.1 of [71], which only guarantees that MPC protocols with local memory $s=O(n^{1/4-\\epsilon})$ (or with $\\delta<{\\textstyle{\\frac{1}{4}}}$ ) can be simulated by transformers with sublinear embedding dimension. This distinction is significant because several positive results for the MPC protocol (e.g. the ability to solve all problems in L and $\\mathsf{N L}$ in a logarithmic number of MPC rounds) require $s=\\Omega(N^{1/2})$ local memory, and hence could not be shown to be simulated by transformers of sublinear embedding dimension previously7. ", "page_idx": 21}, {"type": "text", "text": "Including an allowance of $N^{\\prime}$ blank pause tokens permits the simulation of MPC protocol with $\\gamma\\geq\\delta$ , i.e. where number of machines $q$ grows super-linearly with $n$ . If $\\gamma<\\delta$ , then the protocol can be simulated without pause tokens (i.e. $N^{\\prime}=0$ ) for sufficiently large input sizes $n$ . ", "page_idx": 21}, {"type": "text", "text": "To prove Theorem 8, we define a restriction of the MPC computational model that disallows each machine from communicating with more than $k$ other machines in each round of the protocol. ", "page_idx": 21}, {"type": "text", "text": "Definition 3. For constants $\\gamma,\\delta,\\rho>0$ , an $R$ -round $(\\gamma,\\delta,\\rho)$ -MPC protocol on input sequences of length $n$ is a $(\\gamma,\\delta)$ -MPC protocol (Definition 2) that obeys an additional constraint on the number of outgoing and incoming messages. Namely, for some capacity $k=O(n^{\\rho})$ , in each round, every machine can send its local memory to and receive information from at most distinct $k$ machines. ", "page_idx": 21}, {"type": "text", "text": "Theorem 8 is an immediate consequence of Proposition 24\u2014which establishes that any $(\\gamma,\\delta)$ -MPC protocol can be simulated by a $(\\gamma,\\delta,\\rho)$ -MPC protocol\u2014and Corollary 25\u2014which shows that any $(\\gamma,\\delta,\\rho)$ -MPC protocol can be simulated by a transformer. ", "page_idx": 21}, {"type": "text", "text": "Proposition 24 $\\left(\\gamma,\\delta,\\rho\\right)$ -MPC simulates $(\\gamma,\\delta)$ -MPC). For constants $\\gamma,\\delta>0$ and $\\rho\\in(0,\\frac{\\delta}{2})$ , $i f$ $f$ can be computed by an $R$ -round $(\\gamma,\\delta)$ -MPC protocol, then there exists a $\\begin{array}{r}{O(\\frac{R(1+\\gamma)^{2}}{\\rho^{2}})}\\end{array}$ -round $(\\gamma,\\delta,\\rho)$ -MPC protocol that computes $f$ as well. ", "page_idx": 21}, {"type": "text", "text": "Corollary 25 (Transformers simulate $(\\gamma,\\delta,\\rho)$ -MPC). For constants $\\delta\\,\\in\\,(0,1)$ and $\\gamma,\\rho>0$ , an $R$ -round deterministic $(\\gamma,\\delta,\\rho)$ -MPC protocol with $n$ inputs can be simulated by a transformer $f\\in$ Transformer $\\stackrel{N,N^{\\prime}}{m,H,L}$ with depth $L=R+1$ , heads $H=1$ , embedding dimension $m={\\cal O}(n^{\\delta+4\\rho}\\log n)$ , context length $N=n$ , and blank chain-of-thought tokens $N^{\\prime}=\\operatorname*{max}(0,O(n^{1+\\gamma-\\delta})-n)$ . ", "page_idx": 21}, {"type": "text", "text": "Proof of Theorem 8. Let $\\begin{array}{r}{\\rho:=\\operatorname*{min}(\\frac{\\delta}{2},\\frac{\\delta^{\\prime}-\\delta}{4})-\\epsilon}\\end{array}$ for some small constant $\\epsilon$ (e.g. $\\begin{array}{r}{\\epsilon:=\\operatorname*{min}(\\frac{\\delta}{4},\\frac{\\delta^{\\prime}-\\delta}{8}))}\\end{array}$ By Proposition 24, we can simulate the target -round -MPC protocol with an $R^{\\prime}$ -round $(\\gamma,\\delta,\\rho)$ -MPC protocol for ", "page_idx": 21}, {"type": "equation", "text": "$$\nR^{\\prime}=O\\left(\\frac{R(1+\\gamma^{2})}{\\operatorname*{min}((\\delta^{\\prime}-\\delta)^{2},\\delta^{2})}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We apply Corollary 25 to conclude that this latter protocol can be simulated by a transformer of depth $L=R^{\\prime}+1$ and embedding dimension ", "page_idx": 21}, {"type": "equation", "text": "$$\nm=O(n^{\\delta+4\\rho}\\log n)=O(n^{\\delta^{\\prime}}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We prove Proposition 24 in Appendix B.3.1 by simulating a single round of a standard MPC with multiple rounds of restricted MPC, where messages are sent to intermediate \u201cneighborhoods\u201d\u2014which contain collections of machines with similar destinations\u2014of increasing fine granularity. We prove Corollary 25 in Appendix B.3.2 by a minor adaptation of the proof of Theorem 3.1 of [71]. ", "page_idx": 22}, {"type": "text", "text": "B.3.1 Proof of Proposition 24 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Fix an $R$ -round $(\\gamma,\\delta)$ -MPC protocol $\\pi$ with input size $n$ , $q=\\Theta(n^{1+\\gamma-\\delta})$ machines, and $s=O(n^{\\delta})$ local memory. To prove Proposition 24, it suffices to show that a single round of MPC communication can be simulated an O( (1+\u03c12\u03b3) )-round (\u03b3, \u03b4, \u03c1)-MPC protocol. ", "page_idx": 22}, {"type": "text", "text": "To formalize the communication procedure to simulate, we let $0{\\mathrm{utbox}}=(0{\\mathrm{utbox}}_{1},\\dots,0{\\mathrm{utbox}}_{q})$ denote an encoding of the outgoing \u201cmessage packets\u201d from each source machine that obeys $\\pi$ \u2019s local memory constraints. Concretely, ", "page_idx": 22}, {"type": "equation", "text": "$$\n0{\\mathrm{utbox}}_{i}=\\{({\\mathrm{Msg}},{\\mathrm{Src}},{\\mathrm{Dst}}):{\\mathrm{Src}}=i,{\\mathrm{Dst}}\\in[q]\\}\\ {\\mathrm{s.t.}}\\ \\sum_{{\\mathrm{Msg}}\\in{\\mathrm{Outbox}}_{i}}|{\\mathrm{Msg}}|\\leq s.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let $\\mathtt{I n b o x}=(\\mathtt{I n b o x}_{1},\\mathtt{\\dots},\\mathtt{I n b o x}_{q})$ denote the same collection of message packets, organized by their destination machines. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathtt{I n b o x}_{i}=\\{(\\mathtt{N s g},\\mathtt{S r c},\\mathtt{D s t})\\in\\mathtt{O u t b o x}_{\\mathtt{S r c}}:\\mathtt{D s t}=i\\}\\ \\mathrm{~s.t.~}\\sum_{\\mathtt{M s g}\\in\\mathtt{I n b o x}_{i}}|\\mathtt{M s g}|\\leq s.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "It suffices to prove Lemma 26. ", "page_idx": 22}, {"type": "text", "text": "Lemma 26 $((\\gamma,\\delta,\\rho)$ -MPC simulates one communication round of $(\\gamma,\\delta)$ -MPC). If $\\rho\\,<\\,{\\frac{\\delta}{2}}$ , there exists an O( (1+\u03c12\u03b3) ) -round $(\\gamma,\\delta,\\rho)$ -MPC protocol that takes as input any Outbox and returns the corresponding Inbox. ", "page_idx": 22}, {"type": "text", "text": "Proof. We define $\\begin{array}{r}{r=O(\\frac{1+\\gamma}{\\rho})}\\end{array}$ intermediate steps and prove that each of those can be simulated. The intuition is that each an intermediate step routes each packet $(\\mathtt{M s g},\\mathtt{S r c},\\mathtt{D s t})$ to a machine that belongs to the same \u201cneighborhood\u201d as Dst. Each step maps each packet to a neighborhood of smaller radius than the step before, until all packets have been transmitted to their proper destination location. ", "page_idx": 22}, {"type": "text", "text": "We define a tree of neighborhoods as follows. Fix some branching factor $\\ell=\\Theta(n^{\\rho/2})$ and number of intermediate steps $\\begin{array}{r}{r=\\left\\lceil\\frac{\\log q}{\\log\\ell}\\right\\rceil=O(\\frac{1+\\gamma}{\\rho})}\\end{array}$ . For any step $t=0,\\ldots,r$ and neighborhood index $\\begin{array}{r}{j=1,\\ldots,b_{t}:=\\left\\lceil\\frac{q}{\\ell^{r-t}}\\right\\rceil}\\end{array}$ , we define ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{{Nbhd}}_{j}^{t}=\\left\\{(j-1)\\ell^{r-t}+1,(j-1)\\ell^{r-t}+2,\\ldots,j\\cdot\\ell^{r-t}\\right\\}\\cap[q]\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and observe that it satisfies the size constraint $|\\mathtt{N b h d}_{j}^{t}|\\,\\le\\,\\ell^{r-t}$ . We let $\\mathtt{N b h d}^{t}\\big(\\mathtt{D s t}\\big)$ denote the unique neighborhood ${\\tt N b h d}_{j}^{t}$ satisfying D $\\mathtt{s t}\\in\\mathtt{N b h d}_{j}^{t}$ . We say that $\\mathtt{N b h d}_{j}^{t}<\\mathtt{N b h d}_{j^{\\prime}}^{t}$ if $j<j^{\\prime}$ (and equivalently, for all $i\\in\\mathtt{N b h d}_{j}^{t}$ and $i^{\\prime}\\in\\mathtt{N b h d}_{j^{\\prime}}^{t}$ , $i<i^{\\prime}$ ). Let ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\quad~\\mathtt{Chi1dren}(\\mathtt{N b h d}^{\\it{t}}_{j})=\\left\\{\\mathtt{N b h d}_{(j-1)\\ell+1}^{\\it{t}+1},\\ldots,\\mathtt{N b h d}_{j\\ell}^{\\it{t}+1}\\right\\},}}\\\\ &{\\mathtt{D e s c e n d a n t s^{\\it{\\tau}}(\\mathtt{N b h d}_{j}^{\\it{t}})=\\left\\{\\mathtt{N b h d}^{\\it{t}}(i):i\\in\\mathtt{N b h d}_{j}^{\\it{t}}\\right\\},}}\\\\ &{\\qquad\\quad\\mathtt{P a r e n t}(\\mathtt{N b h d}_{j}^{\\it{t}})=\\mathtt{N b h d}_{\\lceil j/\\ell\\rceil}^{\\it{t}-1},}\\\\ &{\\mathtt{A n c e s t o r}^{\\it{t}}(\\mathtt{N b h d}_{j}^{\\it{\\tau}})=\\mathtt{N b h d}^{\\it{t}}(i)\\mathrm{\\;for\\}i\\in\\mathtt{N b h d}_{j}^{\\it{\\tau}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for some $\\tau\\geq t$ . ", "page_idx": 22}, {"type": "text", "text": "Note that the following are true of neighborhoods. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The initial \u201cstep $0^{\\circ}$ neighborhood contains all machines (i.e. $\\mathtt{N b h d}_{1}^{0}=[q])$ , and the final \u201cstep $r^{\\ast}$ neighborhoods contain a single machine (i.e. ${\\tt N b h d}_{j}^{r}=\\{j\\}$ for all $\\bar{j}\\in[q],$ . \u2022 For any $t<r$ and $j\\in[b_{t}],\\mathtt{N b h d}_{j}^{t}$ is the disjoint union of all sets in Children $\\left(\\mathtt{N b h d}_{j}^{t}\\right)$ and $\\lvert\\mathtt{C h i l d r e n}(\\mathtt{N b h d}_{j}^{t})\\rvert\\le\\ell$ . ", "page_idx": 22}, {"type": "image", "img_path": "AfzbDw6DSp/tmp/edfa1452426bbf667b067cbae1cfe1c7d17db77ae9bbf0d2b2fd3d12b28108c9.jpg", "img_caption": ["Figure 4: The neighborhood routing structure. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "\u2022 $\\left\\{\\mathsf{N b h d}_{1}^{t},\\ldots,\\mathsf{N b h d}_{b_{t}}^{t}\\right\\}$ comprise a disjoint union of $[q]$ . ", "page_idx": 23}, {"type": "text", "text": "\u2022 For any Dst $\\in[q]$ , there exist unique sets $\\mathtt{N b h d^{0}(D s t)},\\ldots,\\mathtt{N b h d^{r}(D s t)}$ that satisfy ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\mathrm{Nbhd}^{r}(\\mathrm{Dst})\\subset\\cdots\\subset\\mathrm{Nbhd}^{0}(\\mathrm{Dst})}}\\\\ {{\\mathrm{:})=\\mathrm{Parent}(\\mathrm{Nbhd}^{t}(\\mathrm{Dst})).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$\\mathtt{d}\\,\\mathtt{N b h d}^{t-1}(\\mathtt{D s t})=\\mathtt{P a r e n t}(\\mathtt{N b h d}^{t}(\\mathtt{D s t})).$ ", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We define a collection of MPC machine states as \u201cintermediate outboxes\u201d Outbox ${\\bf\\nabla}_{\\bf\\cdot}^{t}\\quad=$ $(0{\\tt u t b o x}_{1}^{t},\\dots,0{\\tt u t b o x}_{q}^{t})$ for each step $t$ to represent an assignment of message packets to machines in the same tth-level neighborhood as the packet destination. That is, we require that each Outbox $\\mathbf{\\boldsymbol{:}}_{i}^{t}$ satisfy ", "page_idx": 23}, {"type": "equation", "text": "$$\n0\\mathbf{utbox}_{i}^{t}=\\left\\{\\big(\\mathtt{M s g},\\mathtt{S r c},\\mathtt{D s t}\\big):i\\in\\mathtt{N b h d}^{t}\\big(\\mathtt{D s t}\\big)\\right\\}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We say that Outboxt are valid intermediate outboxes for Outbox if: ", "page_idx": 23}, {"type": "text", "text": "1. Outboxt satisfies Equation (1); ", "page_idx": 23}, {"type": "text", "text": "2. there is a one-to-one mapping between packets in Outbox and $\\mathtt{O u t b o x}^{t}$ ; and ", "page_idx": 23}, {"type": "text", "text": "3. local memory constraints are maintained up to a factor of two8 ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{\\mathbf{Msg}\\in\\mathbf{0utbox}_{i}^{t}}|\\mathtt{M s g}|\\leq2s.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that Outbox satisfies the conditions for $\\mathsf{O u t b o x}^{0}$ , and the only sequence of embeddings that satisfies the conditions for Outboxr is Inbox. An inductive argument that applies Lemma $27~r$ times completes the proof. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "Lemma 27 $((\\gamma,\\delta,\\rho)$ -MPC simulates one intermediate step). For any $t\\in\\{0,\\ldots,r-1\\}$ and $\\rho<{\\frac{\\delta}{2}}$ there exists an $O\\big(\\frac{1\\!+\\!\\gamma}{\\rho}\\big)$ -round $(\\gamma,\\delta,\\rho)$ -MPC protocol that takes as input any satisfactory Outboxt and returns some satisfactory Outbo $\\tau^{t+1}$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. The protocol operates in two phases: ", "page_idx": 24}, {"type": "text", "text": "1. The pre-computation phase, where relevant message size metadata is computed using $O(r-t)$ rounds of communication.   \n2. The message-passing phase, where all messages are propagated in $r-t$ rounds of communication in order to convert Outboxt to $0\\mathrm{utbox}^{t+1}$ . ", "page_idx": 24}, {"type": "text", "text": "Since $\\begin{array}{r}{r=O(\\frac{1+\\gamma}{\\rho})}\\end{array}$ , the bound on total number of rounds is satisfied. ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The algorithm maintains the invariant that all communication occurs within $t$ -level neighborhoods ${\\tt N b h d}_{j}^{t}$ without any mixing between neighborhoods; this is possible because of the assumed validity of Outboxt, which implies that all messages whose packets appear in $\\mathtt{N b h d}_{j}^{t}$ of Outboxt have ultimate destinations in $\\mathtt{N b h d}_{j}^{t}$ . Concretely, if $(\\mathtt{M s g},\\mathtt{S r c},\\mathtt{D s t})\\in\\mathtt{O u t b o x}_{i}^{t}$ and $i\\in\\mathtt{N b h d}_{j}^{t}$ , then $\\mathtt{D s t}\\in\\mathtt{N b h d}_{j}^{t}$ . We first explain the routing strategy for the message-passing phase by describing an itinerary of machines that each message packet will be routed to and proving that this itinerary meets the conditions needed to be executed by an $(r-t)$ -round $(\\gamma,\\delta,\\rho)^{\\!}$ -MPC protocol. We then describe how the metadata necessary for the itinerary computations can be obtained during the pre-computation phase. ", "page_idx": 24}, {"type": "text", "text": "Message-passing phase. We first introduce some packet notation to track all relevant metadata about any message in the tth intermediate step. Fix some particular input Outboxt. ", "page_idx": 24}, {"type": "text", "text": "\u2022 Let $\\mathbf{\\Psi}=\\mathbf{\\Psi}(\\mathbb{M}\\mathbf{s}\\mathbf{g},\\mathbf{S}\\mathbf{r}\\mathbf{c},\\mathbb{D}\\mathbf{s}\\mathbf{t},\\mathbf{S}\\mathbf{r}\\mathbf{c}^{t})$ denote a packet that contains a message Msg and metadata concerning its \u201cglobal\u201d source $\\mathtt{S r c}$ and destination Dst (i.e. $(\\mathtt{M s g},\\mathtt{S r c},\\mathtt{D s t})\\in$ Outbo $x_{\\mathrm{Src}}$ , $\\mathtt{I n b o x_{D s t}}$ ) and its \u201clocal\u201d source $\\mathtt{S r c}^{t}$ from within Outboxt (i.e. $(\\mathtt{M s g},\\mathtt{S r c},\\mathtt{D s t})\\in0\\mathtt{u t b o x}_{\\mathtt{S r c}^{t}}^{t})$ .   \n\u2022 We write $\\mathsf{P}\\in\\mathsf{O u t b o x}_{i}^{t}$ if $\\mathtt{S r c}^{t}=i$ and $\\textsf{P}\\in$ Nbhd if $\\mathsf{P}\\in\\mathsf{O u t b o x}_{i}^{t}$ for some $i\\in\\mathbb{N}{\\sf b h d}$ .   \n\u2022 Let $\\mathtt{S r c N b h d}^{t,\\tau}(\\mathsf{P})\\,=\\,\\mathtt{N b h d}^{\\tau}\\big(\\mathtt{S r c}^{t}\\big)$ represent the neighborhood of size $\\ell^{r-\\tau}$ contains $\\mathbb{P}$ (i.e. $\\mathsf{P}\\in\\mathsf{S r c N b h d}^{t,\\tau}(\\mathsf{P}))$ and $\\mathsf{D s t N b h d}^{\\bar{t}}(\\mathsf{P})=\\mathsf{N b h d}^{t+\\bar{1}}(\\mathsf{D s t})$ denote the neighborhood of size $\\ell^{r-t-1}$ that contains the ultimate destination Dst. Because $\\mathsf{P}\\in\\mathsf{N b h d}_{j}^{t}$ if and only if $\\mathtt{D s t}\\in\\mathtt{N b h d}_{j}^{t}$ , it follows that $\\mathtt{D s t N b h d}^{t}(\\mathtt{P})\\subset\\mathtt{S r c N b h d}^{t,t}(\\mathtt{P})$ .   \n\u2022 Let $|\\mathsf{P}|=|\\mathsf{M}\\mathbf{s}\\mathrm{g}|$ be the total number of bits needed to encode the packet.   \n\u2022 We establish a lexicographical ordering that depends first on $\\mathtt{S r c}^{t}$ and next on Dst. That is, we say that $P^{\\prime}<P$ for some $P^{\\prime}\\,\\stackrel{}{=}\\,\\big({\\tt M s g^{\\prime}},{\\tt S r c^{\\prime}},{\\tt D s t^{\\prime}},{\\tt S r c^{\\prime\\prime}}\\big)$ if $\\smash{\\mathrm{Src}^{t\\prime}\\,<\\,\\mathrm{Src}^{t}}$ , or if $\\mathtt{S r c}^{t\\prime}=\\dot{\\mathtt{S r c}}^{t}$ and $\\tt D s t^{\\prime}<\\tt D s t$ . ", "page_idx": 24}, {"type": "text", "text": "We develop a packet scoring function $z^{t,\\tau}(\\mathsf{P})$ , which in turn induces an itinerary function $b^{t,\\tau}(\\mathsf{P})\\in[q]$ that assigns an intermediate machine to hold packet $\\mathbb{P}$ after $r\\!-\\!\\tau$ communication steps. These functions are carefully chosen in order to ensure that the local memories of each individual machine are bounded by $O(s)$ and that the number of machine each sends messages to and receives messages from is bounded by $O(\\ell^{2})$ . Critically, we ensure that $b^{t,\\tau}\\big(\\mathsf{P}\\big)\\in\\mathsf{S r c N b h d}^{t,\\tau}\\big(\\mathsf{P}\\big)$ ; that is, we initially rearrange packets within solely within the smallest neighborhoods of $\\mathtt{O u t b o x}^{t}$ and gradually propagate packets more widely. That way, each packet $\\mathbb{P}$ obeys a trajectory of the following form over $r-t$ MPC ", "page_idx": 24}, {"type": "text", "text": "rounds: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{b^{t,r}(\\mathtt{P})=\\mathtt{S r c}^{t}\\in\\mathtt{S r c N b h d}^{t,r}(\\mathtt{P})\\Longrightarrow b^{t,r-1}(\\mathtt{P})\\in\\mathtt{S r c M b h d}^{t,r-1}(\\mathtt{P})\\Longrightarrow\\ \\cdot\\ \\cdot}\\\\ &{\\qquad\\qquad\\qquad\\implies b^{t,t+1}(\\mathtt{P})\\in\\mathtt{S r c N b h d}^{t,t+1}(\\mathtt{P})\\Longrightarrow b^{t,t}(\\mathtt{P})\\in\\mathtt{D s t M b h d}^{t}(\\mathtt{P}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "To define these functions, we first define partial sums of message sizes in order to later determine an itinerary of machines that $\\mathbb{P}$ will be routed to in between $\\mathtt{S r c}^{t}$ and some destination machine in $\\mathtt{D s t N b h d}^{t}(\\mathtt{P})$ . The first term, $\\mathtt{E q u a l D s t S u m}^{t,\\tau}(\\mathtt{P})$ , sums the size of all \u201clesser\u201d packets that share a destination neighborhood DstNbhd ${}^{t}({\\mathsf{P}})$ and a $\\tau$ th level input neighborhood SrcNbhd ${\\mathbf{}}^{t,\\tau}({\\mathbf{\\mathsf{P}}})$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathsf{E q u a l D s t S u m}^{t,\\tau}(\\mathsf{P})=\\sum_{\\mathsf{P}^{\\prime}\\in\\mathtt{S r c l b h d}^{t,\\tau}(\\mathsf{P})}\\mathsf{\\Gamma}\\big|\\mathrm{P}^{\\prime}\\big|.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The second term, $\\mathtt{L e s s D s t S u m}^{t,\\tau}(\\mathbf{P})$ , sums the size of all packets that share an input neighborhood but have a \u201csmaller\u201d destination neighborhood than P: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathtt{L e s s D s t S u m}^{t,\\tau}(\\mathtt{P})=\\mathtt{L e s s D s t S u m}^{t}(\\mathtt{S r c N b h d}^{t,\\tau}(\\mathtt{P}),\\mathtt{D s t N b h d}^{t}(\\mathtt{P})),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathtt{L e s s D s t S u m}^{t}(\\mathtt{S r c N b h d},\\mathtt{D s t M b h d})=\\sum_{\\mathtt{P}^{\\prime}\\in\\mathtt{S r c N b h d}\\atop\\mathtt{D s t M b h d}^{t}(\\mathbb{P}^{\\prime})<\\mathtt{D s t M b h d}}\\mathtt{|P^{\\prime}|}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We now define the packet scoring and itinerary functions for any $\\tau\\in\\{t,\\ldots,r\\}$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\nz^{t,\\tau}(\\mathtt{P})=\\left\\{\\begin{array}{l l}{2s\\cdot\\operatorname*{min}\\mathtt{S r c h b h d}^{t,\\tau}(\\mathtt{P})+\\mathtt{L e s s}\\mathtt{D s t}\\mathsf{S u m}^{t,\\tau}(\\mathtt{P})+\\mathtt{E q u a l D s t}\\mathsf{S u m}^{t,\\tau}(\\mathtt{P})}&{\\tau\\ge t+1,}\\\\ {2s\\cdot\\operatorname*{min}\\mathtt{D s t}\\mathtt{M b h d}^{t}(\\mathtt{P})+2\\cdot\\mathtt{E q u a l D s t}\\mathsf{S u m}^{t,\\tau}(\\mathtt{P})}&{\\tau=t.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\nb^{t,\\tau}(\\mathsf{P})=\\left\\lfloor\\frac{z^{t,\\tau}(\\mathsf{P})}{2s}\\right\\rfloor.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We prove a series of claims to establish that the packet scoring and itinerary functions are properly defined. ", "page_idx": 25}, {"type": "text", "text": "Claim 28 (Itinerary range). For any $\\tau\\in\\{t,\\ldots,r\\}$ , the itinerary function satisfies ", "page_idx": 25}, {"type": "equation", "text": "$$\nb^{t,\\tau}(\\mathsf{P})\\in\\left\\{\\!\\!\\!\\begin{array}{l l}{\\mathsf{S r c N b h d}^{t,\\tau}(\\mathsf{P})}&{\\tau\\geq t+1}\\\\ {\\mathsf{D s t N b h d}^{t}(\\mathsf{P})}&{\\tau=t.}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "As an immediate consequence, $b^{t,r}(\\mathsf{P})=\\mathbf{S}\\mathbf{r}\\mathsf{c}^{t}$ for $\\mathsf{P}=(\\mathtt{M s g},\\mathtt{S r c},\\mathsf{D s t},\\mathtt{S r c}^{t}).$ ", "page_idx": 25}, {"type": "text", "text": "Proof. We first bound the scoring function $z^{t,\\tau}$ . Note that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathrm{LessDstSum}^{t,\\tau}(\\mathtt{P})+\\mathtt{E q u a l D s t S u m}^{t,\\tau}(\\mathtt{P})\\le\\displaystyle\\sum_{\\mathtt{P}^{\\prime}\\in\\mathtt{S r c N b h d}^{t,\\tau}(\\mathtt{P})}\\mathtt{\\xi}|\\mathtt{P}^{\\prime}|-|\\mathtt{P}|}&{}\\\\ {\\le\\vert\\mathtt{S r c N b h d}^{t,\\tau}(\\mathtt{P})\\vert\\cdot2s-\\vert\\mathtt{P}\\vert.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore, for $\\tau>t$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\nz^{t,\\tau}(\\mathtt{P})\\in\\left[2s\\cdot\\operatorname*{min}\\mathtt{S r c N b d}^{t,\\tau}(\\mathtt{P}),2s\\cdot(\\operatorname*{min}\\mathtt{S r c N b d}^{t,\\tau}(\\mathtt{P})+|\\mathtt{S r c N b h d}^{t,\\tau}(\\mathtt{P})|)-|\\mathtt{P}|\\right],\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{b^{t,\\tau}(\\mathsf{P})\\in[\\operatorname*{min}\\mathsf{S r c N b h d}^{t,\\tau}(\\mathsf{P}),\\operatorname*{min}\\mathsf{S r c N b h d}^{t,\\tau}(\\mathsf{P})+|\\mathsf{S r c N b h d}^{t,\\tau}(\\mathsf{P})|),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which proves the first case of the claim. The second case follows by observing that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathtt{E q u a l D s t S u m}^{t,t}(\\mathtt{P})\\leq s\\cdot|\\mathtt{D s t N b h d}^{t}(\\mathtt{P})|-|\\mathtt{P}|.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Were it not true, there would exist at least one machine $i\\in\\mathsf{D s t N b h d}^{t}(\\mathsf{P})$ that must receive more an $s$ quantity of messages at the end of the entire round of the protocol $\\pi$ (i.e. $\\sum_{\\mathtt{M s g}\\in\\mathtt{I n b o x}_{i}}|\\mathtt{M s g}|>s)$ which contradicts the MPC assumption. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Claim 29 (Gaps between scores). If $\\mathrm{\\dot{P}_{1}\\neq P_{2}}$ and $z^{t,\\tau}\\big(\\mathsf{P}_{1}\\big)\\leq z^{t,\\tau}\\big(\\mathsf{P}_{2}\\big),$ , then ", "page_idx": 26}, {"type": "equation", "text": "$$\nz_{t,\\tau}(\\mathsf{P}_{1})+|\\mathsf{P}_{1}|\\leq z^{t,\\tau}(\\mathsf{P}_{2}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. First, let $\\tau\\mathrm{~\\,~>~}t$ . Consider the case where $\\mathtt{S r c N b h d}^{t,\\tau}(\\mathsf{P}_{1})\\neq\\mathtt{S r c N b h d}^{t,\\tau}(\\mathsf{P}_{2})$ . By Claim 28 and our assumption that $z^{t,\\tau}\\big(\\mathsf{P}_{1}\\big)\\leq z^{t,\\tau}\\big(\\mathsf{P}_{2}\\big)$ , it must be the case that SrcNbhd $t,\\tau(\\mathsf{P}_{1})<$ SrcNbhd $t,\\tau_{\\big(\\mathsf{P}_{2}\\big)}$ . Hence, we have the following by applying Equation (2). ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z^{t,\\tau}(\\mathtt{P}_{2})-z_{t,\\tau}(\\mathtt{P}_{1})}\\\\ &{\\quad\\ge2s\\cdot(\\operatorname*{min}\\mathtt{S r c N b d}^{t,\\tau}(\\mathtt{P}_{2})-(\\operatorname*{min}\\mathtt{S r c N b h d}^{t,\\tau}(\\mathtt{P}_{1})+|\\mathtt{S r c N b h d}^{t,\\tau}(\\mathtt{P}_{1})|-|\\mathtt{P}_{1}|))}\\\\ &{\\quad\\ge|\\mathtt{P}_{1}|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Otherwise, if $\\mathtt{S r c N b h d}^{t,\\tau}(\\mathtt{P}_{1})=\\mathtt{S r c N b h d}^{t,\\tau}\\big(\\mathtt{P}_{2}\\big)$ , then ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{\\Pi}_{\\mathsf{r}}^{t,\\tau}(\\mathsf{P}_{2})-z_{t,\\tau}(\\mathsf{P}_{1})=(\\mathtt{L e s s D s t S u m}^{t,\\tau}(\\mathsf{P}_{2})+\\mathtt{E q u a l D s t S u m}^{t,\\tau}(\\mathsf{P}_{2}))-(\\mathtt{L e s s D s t S u m}^{t,\\tau}(\\mathsf{P}_{1})+\\mathtt{E q u a l D s t S u m}^{t,\\tau}(\\mathsf{P}_{2}))}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\sum_{\\mathsf{P}^{\\prime}\\in S_{2}}|\\mathsf{P}^{\\prime}|-\\displaystyle\\sum_{\\mathsf{P}^{\\prime}\\in S_{1}}|\\mathsf{P}^{\\prime}|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for some packet subsets $S_{1},S_{2}\\;\\;\\subset\\;\\;{\\sf S r c N b h d}^{t,\\tau}\\big({\\sf P}_{1}\\big)$ . By further inspecting the respective LessDstSumt,\u03c4 and EqualDstSumt,\u03c4 terms, we observe that $S_{1}\\subset S_{2}$ and $\\mathsf{P}_{1}~\\in~S_{2}\\setminus S_{1}$ . The claim immediately follows. ", "page_idx": 26}, {"type": "text", "text": "The argument for the case $\\tau=t$ is nearly identical. ", "page_idx": 26}, {"type": "text", "text": "Claim 30 (Local memory bound). For any $b\\in\\mathbb{N},$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{\\mathbf{p}:b^{t,\\tau}(\\mathbb{P})=b}|\\mathbb{P}|\\leq\\binom{2s}{3s}\\quad\\tau\\in\\{t,r\\}}\\\\ {\\mathsf{P}{\\mathrm{:}\\,b^{t,\\tau}}(\\mathbb{P}){\\mathrm{=}}b}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. The case $\\tau\\,=\\,r$ is an immediate consequence of the inductive assumption that Outboxt satisfies the desired intermediate properties. ", "page_idx": 26}, {"type": "text", "text": "For all other cases, let $\\{\\mathsf{P}_{1},\\dotsc,\\mathsf{P}_{n}\\}$ denote all packets with $b^{t,\\tau}(\\mathsf{P}_{i})=b$ and let $z^{t,\\tau}({\\mathsf{P}}_{1})\\leq\\cdots\\leq$ $z^{t,\\tau}(\\mathsf{P}_{n})$ without loss of generality. We use Claim 29, the assumption that all $|\\mathsf{P}_{i}|\\,\\le\\,s$ , and the boundedness of $z^{t,\\tau}(\\mathsf{P}_{i})$ from Claim 28 to conclude the proof. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{i=1}^{n}\\left\\lvert\\mathtt{P}_{i}\\right\\rvert\\leq\\displaystyle\\sum_{i=1}^{n-1}(z^{t,\\tau}(\\mathtt{P}_{i+1})-z^{t,\\tau}(\\mathtt{P}_{i}))+\\left\\lvert\\mathtt{P}_{n}\\right\\rvert}&{}\\\\ {\\displaystyle\\leq z^{t,\\tau}(\\mathtt{P}_{n})-z^{t,\\tau}(\\mathtt{P}_{1})+s}&{}\\\\ {\\displaystyle\\leq\\left\\{2s\\ \\ \\ \\tau=t,\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Claim 31 (Intra-class distance preservation). If ${\\sf P}_{1}$ and ${\\sf P}_{2}$ satisfy $\\begin{array}{r l}{\\mathsf{S r c N b h d}^{t,\\tau+1}(\\mathsf{P}_{1})}&{{}=}\\end{array}$ $\\mathtt{S r c N b h d}^{t,\\tau+1}\\big(\\mathtt{P}_{2}\\big)$ and $\\mathtt{D s t N b h d}^{t}(\\mathtt{P}_{1})=\\mathtt{D s t N b h d}^{t}\\big(\\mathtt{P}_{2}\\big),$ , then ", "page_idx": 26}, {"type": "equation", "text": "$$\nz^{t,\\tau}(\\mathsf{P}_{1})-z^{t,\\tau}(\\mathsf{P}_{2})=z^{t,\\tau+1}(\\mathsf{P}_{1})-z^{t,\\tau+1}(\\mathsf{P}_{2}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. Since $\\begin{array}{r l r}{{\\sf S r c N b h d}^{t,\\tau+1}\\big(\\mathtt{P}_{1}\\big)}&{{}=}&{{\\sf S r c N b h d}^{t,\\tau+1}\\big(\\mathtt{P}_{2}\\big)}\\end{array}$ , it follows that $\\begin{array}{r l}{\\mathtt{S r c N b h d}^{t,\\tau}\\big(\\mathsf{P}_{1}\\big)}&{{}=}\\end{array}$ $\\mathtt{S r c N b h d}^{t,\\tau}(\\mathsf{P}_{2})$ and therefore, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{z^{t,\\tau}(\\mathsf{P}_{1})-z^{t,\\tau}(\\mathsf{P}_{2})=\\mathsf{E q u a l D s t S u m}^{t,\\tau}(\\mathsf{P}_{1})-\\mathsf{E q u a l D s t S u m}^{t,\\tau}(\\mathsf{P}_{2})}&{}\\\\ {=}&{\\displaystyle\\sum_{\\mathrm{p}^{\\prime}\\in\\mathrm{Srechbd}^{t,\\tau}(\\mathsf{P}_{1})}|\\mathsf{P}^{\\prime}|}\\\\ {\\mathsf{D e t m i n d}^{t}(\\mathsf{P}^{\\prime})\\mathsf{=\\jmath}_{\\mathsf{s t u b h d}^{t}(\\mathsf{P}_{1})}}\\\\ {\\mathsf{P}_{1}\\mathsf{c}^{\\mathsf{P}^{\\prime}\\setminus\\mathsf{P}_{2}}}\\\\ {z^{t,\\tau+1}(\\mathsf{P}_{1})-z^{t,\\tau+1}(\\mathsf{P}_{2})=\\mathsf{E q u a l D s t S u m}^{t,\\tau+1}(\\mathsf{P}_{1})-\\mathsf{E q u a l D s t S u m}^{t,\\tau+1}(\\mathsf{P}_{2})}\\\\ {=}&{\\displaystyle\\sum_{\\mathrm{p}^{\\prime}\\in\\mathrm{Srechbd}^{t,\\tau+1}(\\mathsf{P}_{1})}|\\mathsf{P}^{\\prime}|.}\\\\ {\\mathsf{D e t m i n d}^{t}(\\mathsf{P}^{\\prime})\\mathsf{=\\jmath}_{\\mathsf{s t u b h d}^{t}(\\mathsf{P}_{1})}}\\\\ {\\mathsf{P s t h d}\\mathsf{e}^{\\mathsf{P}\\cdot\\mathsf{C}\\mathsf{P}_{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Because $\\mathtt{P}_{1},\\mathtt{P}_{2}\\in\\mathtt{S r c N b h d}^{t,\\tau+1}(\\mathtt{P}_{1})$ , the defined packet ordering implies that any $\\mathsf{P}^{\\prime}\\in[\\mathsf{P}_{1},\\mathsf{P}_{2})$ must satisfy $\\mathsf{P}^{\\prime}\\in\\mathsf{S r c N b h d}^{t,\\tau+1}(\\mathsf{P}_{1})$ . Therefore, Equations (3) and (4) are equal and the claim holds. ", "page_idx": 27}, {"type": "text", "text": "Claim 32 (Distinct recipients bound). For any $b$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n|\\left\\{b^{t,\\tau}({\\mathsf{P}}):b^{t,\\tau+1}({\\mathsf{P}})=b\\right\\}|\\leq3\\ell.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. Within each root neighborhood $\\mathtt{N b h d}_{j}^{\\tau}$ , there exist at most $\\ell$ destination neighborhoods in $\\mathtt{D s t N b h d}^{t}(\\mathtt{P})$ for $\\mathsf{P}\\in\\mathbb{N}\\mathsf{b h}\\mathsf{d}_{j}^{\\tau}$ . ", "page_idx": 27}, {"type": "text", "text": "Fix some such DstNbhd and let $\\mathsf{P}_{1},\\ldots,\\mathsf{P}_{n}$ denote all packets with $b^{t,\\tau+1}(\\mathsf{P})\\;\\;=\\;\\;b$ and $\\mathtt{D s t N b h d}^{t}(\\mathtt{P}_{i})\\,=\\,\\mathtt{D s t N b h d}$ . Without loss of generality, assume that $z^{t,\\tau}\\big(\\mathsf{P}_{1}\\big)\\,\\leq\\,\\cdot\\,\\cdot\\,\\leq\\,z^{t,\\tau}\\big(\\mathsf{P}_{n}\\big)$ . Because all such packets belong to the same machine in step $r-\\tau-1$ (i.e. $b^{t,\\tau+1}(\\mathsf{P}_{i})=b$ for all $i\\in[n])$ , they belong share the same source neighborhood of size $\\ell^{r-\\tau-1}$ (i.e. SrcNbhd $\\mathsf{\\Omega}_{t,\\tau+1}^{t,\\tau+1}({\\mathsf{P}}_{i})=$ SrcNbhd $t,\\tau^{\\downarrow+1}({\\sf P}_{1}))$ . By Claim 31 and the definition of $b^{t,\\tau}$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{b^{t,\\tau}(\\mathsf{P}_{i})-b^{t,\\tau}(\\mathsf{P}_{1})\\leq1+\\displaystyle\\frac{1}{2s}(z^{t,\\tau}(\\mathsf{P}_{i})-z^{t,\\tau}(\\mathsf{P}_{1}))}\\\\ &{\\qquad\\qquad\\qquad=1+\\displaystyle\\frac{1}{2s}(z^{t,\\tau+1}(\\mathsf{P}_{i})-z^{t,\\tau+1}(\\mathsf{P}_{1}))}\\\\ &{\\qquad\\qquad\\qquad\\leq2+b^{t,\\tau+1}(\\mathsf{P}_{i})-b^{t,\\tau+1}(\\mathsf{P}_{1})=2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore, there are at most three possible values of $b^{t,\\tau}(\\mathsf{P}_{i})$ . ", "page_idx": 27}, {"type": "text", "text": "The claim follows by considering each of the $\\ell$ destination neighborhoods separately: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left\\{b^{t,\\tau}(\\mathbb{P}):b^{t,\\tau+1}(\\mathbb{P})=b\\right\\}\\right|=\\displaystyle\\sum_{\\mathrm{Bsthbhd}}\\left|\\left\\{b^{t,\\tau}(\\mathbb{P}):b^{t,\\tau+1}(\\mathbb{P})=b,\\:\\mathrm{DstMbhd}^{t}(\\mathbb{P})=\\mathrm{DstMbhd}\\right\\}\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq3\\ell.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Claim 33 (Distinct senders bound). For any $b$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n|\\left\\{b^{t,\\tau+1}(\\mathsf{P}):b^{t,\\tau}(\\mathsf{P})=b\\right\\}|\\leq3\\ell^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. Within each $\\mathtt{N b h d}_{j}^{\\tau}$ , there exist at most $\\ell^{2}$ distinct pairs of destination neighborhoods $\\mathtt{D s t N b h d}^{t}(\\mathtt{P})$ and source neighborhoods $\\mathtt{N b h d}^{t,\\tau}(\\mathtt{P})$ for $\\mathsf{P}\\in\\mathbb{N}\\mathsf{b h}\\mathsf{d}_{j}^{\\tau}$ . ", "page_idx": 27}, {"type": "text", "text": "As before, we fix some DstNbhd and SrcNbhd and let $\\mathsf{P}_{1},\\ldots,\\mathsf{P}_{n}$ all satisfy $b^{t,\\tau}(\\mathsf{P}_{i})\\ =\\ b$ , $\\mathtt{D s t N b h d}^{t}(\\mathsf{P}_{i})=\\mathtt{I}$ DstNbhd, and $\\mathtt{S r c N b h d}^{t,\\tau+1}=\\mathtt{S r c N b h d}^{t,\\tau+1}$ d. Using the same argument, we show that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left|\\left\\{b^{t,\\tau+1}(P_{i}):i\\in[n]\\right\\}\\right|\\leq3.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We conclude by considering all such pairs. ", "page_idx": 27}, {"type": "text", "text": "As a result of Claims 30, 32, and 33, we conclude that each packet $\\mathsf{P}=(\\mathsf{M s g},\\mathsf{S r c},\\mathsf{D s t},\\mathsf{S r c}^{t})$ can be equipped with some itinerary ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathsf{S r c}^{t}=b^{t,r}(\\mathsf{P}),\\ b^{t,r-1}(\\mathsf{P}),\\dots,b^{t,t+1}(\\mathsf{P}),\\ b^{t,t}(\\mathsf{P})\\in\\mathsf{D s t M b h d}^{t}(\\mathsf{P})\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "that properly translates an instances of $\\mathtt{O u t b o x}^{t}$ to $\\mathsf{O u t b o x}^{t+1}$ and does so without ever requiring local memory more than $3s=O(N^{\\delta})$ on any intermediate step or any machine to send or receive messages from more than $3\\ell^{2}=\\hat{O}(\\dot{N}^{\\rho})$ other machines. This itinerary can be executed using an $\\left(r-t\\right)$ -round $(\\gamma,\\delta,\\rho)$ -MPC protocol. ", "page_idx": 27}, {"type": "text", "text": "Pre-computation phase. It remains to show that each $b^{t,\\tau}$ can be computed for each packet. To do so, we prove that there exists an $O(r-t)$ -round $(\\gamma,\\delta,\\rho)$ -MPC protocol that ends with each machine $i$ knowing EqualDstSumt,\u03c4(P) and LessDstSum ${\\mathfrak{t}}^{,\\tau}(\\mathbb{P})$ for each $\\textsf{P}\\in\\mathsf{o u t b o x}_{i}^{t}$ and $\\tau\\,\\in\\,\\{t,\\ldots,r\\}$ . We design an MPC protocol that uses the tree structure to propagate information about individual child neighborhoods to their parents and vice versa. We describe recursive relationships that elucidate how to compute the two salient quantities. ", "page_idx": 27}, {"type": "text", "text": "First, we introduce a useful intermediate term. Let ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{EqualDstSum}^{t,\\tau}(i,\\mathsf{D s t M b h d})=\\sum_{\\substack{i^{\\prime}\\in\\mathrm{Mbhd}^{\\tau}(i)\\,\\sum_{\\substack{\\mathrm{p}^{\\prime}\\in\\mathrm{0utbox}_{i^{\\prime}}^{t}}}}}\\left|\\mathbb{P}^{\\prime}\\right|,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "denote the sum of all packet that are contained by \u201clesser\u201d machines that share source and destination neighborhoods. Note that EqualDstSum $t,\\tau_{\\mathsf{\\Omega}}({\\mathsf{P}})$ can be computed for any $\\mathtt{P}\\,\\in\\,0{\\mathtt{u t b o x}}_{i}^{t}$ locally by machine $i$ given prior knowledge of EqualDst $\\mathtt{S u m}^{t,\\tau}(i,\\mathtt{D s t N b h d}^{t}(\\mathtt{P}))$ . Thus, the pre-computation phase need only compute the latter term. ", "page_idx": 28}, {"type": "text", "text": "We also introduce a term that represents sum of the sizes of all packets that share source and destination neighborhoods: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathrm{\\DeltaNbhdSum}^{t}(\\mathrm{SrcNbhd},\\mathrm{DstNbhd})=\\sum_{\\mathrm{\\Deltap^{\\prime}\\in S r c N b h d}\\atop{\\mathrm{\\DeltaDstNbhd}^{t}(\\mathrm{P^{\\prime}})=\\mathrm{DstNbhd}}}\\left|\\mathrm{P}^{\\prime}\\right|.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Now, we provide the recurrences for any $\\tau<r$ (or for any SrcNbhd satisfying $|\\mathtt{S r c N b h d}|>1$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E q u a l D s t S u m}^{t,\\tau}(i,0.8{\\mathrm{thbhd}})}\\\\ &{\\quad=\\mathbb{E q u a l D s t S u m}^{t,\\tau+1}(i,0.0{\\mathrm{sthbhd}})+\\mathbb{E q u a l D s t S u m}^{t,\\tau}(\\mathrm{min~Whbhd}^{\\tau+1}(i),\\mathbb{P s t W h d})}\\\\ &{\\quad=\\mathbb{E q u a l D s t S u m}^{t,\\tau+1}(i,\\mathbb{D}{\\mathrm{stWhbd}})+\\underset{\\underset{\\mathbb{S}\\in\\mathrm{cmbd}}{\\mathrm{ScmalDsteam}}(\\mathrm{Bbhd}^{\\tau}(i))}{\\sum}\\underset{(i)}{\\mathrm{BbhdSum}}^{t}(\\{\\mathrm{Sre}\\mathrm{dbbhd},\\mathbb{D}{\\mathrm{stWbhd}}\\},}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{BerShalchate}^{\\prime}(i))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{BbhdSum}^{t}(\\{\\mathrm{Sre}\\mathrm{dbbhd},\\mathbb{D}{\\mathrm{stWbhd}}\\},\\qquad(5\\,\\mathrm{~cmbing})\\,,}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{Scmalcease}(\\mathrm{Bersonel},\\mathbb{D}{\\mathrm{stembind}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{Lesspst}\\,\\mathrm{Csmal}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{Scenst}\\,\\mathrm{Im}^{t}\\,\\mathrm{CSImbhd},\\,\\mathrm{DStUbhd}\\}=\\underset{\\mathrm{Scmbind}}{\\mathrm{Sredbal}}\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "When $\\tau\\ =\\ \\ r$ , the terms EqualDstSumt,\u03c4(i, DstNbhd), $\\mathtt{N b h d S u m}^{t}\\big(\\mathtt{N b h d}^{r}(i),\\mathtt{D s t N b h d}\\big)$ , and LessDstS $\\mathsf{\\Delta}^{\\mathsf{U m}^{t}}(\\mathsf{N b h d}^{r}(i)$ , DstNbhd) can be computed locally within machine $i$ . ", "page_idx": 28}, {"type": "text", "text": "We follow a tree-like communication pattern to compute all relevant sums. Each machine $i\\in[q]$ computes ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\{\\mathtt{E q u a l D s t S u m}^{t,\\tau}(i,\\mathtt{D s t M b h d}):\\tau\\ge t,\\;\\mathtt{D s t M b h d}\\in\\mathtt{C h i l d r e n}(\\mathtt{N b h d}^{t}(i))\\right\\}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\{\\mathtt{L e s s D s t S u m}^{t}(\\mathtt{N b h d}^{\\tau}(i),\\mathtt{D s t M b h d}):\\tau\\ge t,\\,\\mathtt{D s t M b h d}\\in\\mathtt{C h i l d r e n}(\\mathtt{N b h d}^{t}(i))\\right\\}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "by completing $r-t$ propagate-up rounds, $r-t$ aggregation rounds, and $r-t$ propagate-down rounds. ", "page_idx": 28}, {"type": "text", "text": "\u2022 The propagate-up rounds compute the neighborhood-wide message-size summations NbhdSumt $(\\bar{\\mathbb{N}}\\mathsf{b h d}^{\\top}(i)$ , DstNbhd) and LessDst $\\operatorname{Sum}^{t}(\\mathtt{N b h d}^{\\tau}(i)$ , DstNbhd) in each machine $i$ satisfying $i=\\operatorname*{min}\\mathbb{N}\\mathtt{b h d}^{\\tau}(i)$ for each $\\tau$ and DstNbhd.   \n\u2022 The aggregation rounds accumulate NbhdSum terms of the same level into specific EqualDstSum terms.   \n\u2022 The propagate-down rounds iteratively compute and share the EqualDstSum and LessDstSum terms with all relevant machines. ", "page_idx": 28}, {"type": "text", "text": "Propagate-up rounds: Fix some neighborhood $\\mathtt{N b h d}_{j}^{t}$ . After $r\\mathrm{~-~}\\tau$ propagate-up rounds, the goal is to compute the terms NbhdSumt(SrcNbhd, DstNbhd) and LessDst $\\operatorname{Sum}^{t}$ (SrcNbhd, DstNbhd) for each relevant destination neighborhood DstNbhd $.~\\in~\\cosh$ ildren $\\ (\\mathtt{N b h d}_{j}^{t})$ ) and source neighborhood SrcNbhd $\\in$ Descendants $\\tau(\\mathtt{N b h d}_{j}^{t})$ within a single machine in each source neighborhood, min SrcNbhd. We argue that this is possible inductively. ", "page_idx": 28}, {"type": "text", "text": "Before performing any computation (that is, after \u201cround $0^{\\circ}$ ), each machine $i$ individually computes $\\mathtt{N b h d S u m}^{t}(\\mathtt{N b h d}^{r}(i)$ , DstNbhd) and LessDstSumt $(\\mathbb{N}\\mathsf{b}\\mathsf{h}\\mathsf{d}^{r}(i)$ , DstNbhd) by aggregating the messages encoded in its own representation of Outbo $\\mathbf{\\boldsymbol{x}}_{i}^{t}$ . ", "page_idx": 28}, {"type": "text", "text": "We assume that the procedure works as specified for $r-\\tau$ rounds of communication. Fix some SrcNbhd $\\tau{-}1\\;\\in$ Descendants $\\tau\\!-\\!1_{\\left(\\mathbb{N}\\right)\\!\\circ\\!\\mathrm{h}\\!\\mathsf{d}_{j}^{t}}\\right)$ . Then, for every SrcNbhd\u03c4 $\\in$ ChildrenSrcNbhd $\\tau{-}1$ , the quantities ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\{\\mathrm{NbhdSum}^{t}(\\mathrm{SrcNbhd^{\\tau}},\\mathrm{DstMbhd}):\\mathrm{DstMbhd}\\in\\mathrm{Chi1dren}(\\mathrm{Nbhd}_{j}^{t})\\}}\\\\ &{\\quad\\cup\\,\\{\\mathrm{LessDstSum}^{t}(\\mathrm{SrcNbhd^{\\tau}},\\mathrm{DstMbhd}):\\mathrm{DstMbhd}\\in\\mathrm{Chi1dren}(\\mathrm{Nbhd}_{j}^{t})\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "have already been computed and stored in min SrcNbhd\u03c4. By the recurrence relations of Equations (5) and (6), $\\mathtt{N b h d S u m}^{t}(\\mathtt{S r c N b h d}^{\\tau-1}$ , DstNbhd) and LessDstSumt(SrcNbhd\u03c4\u22121, DstNbhd) are functions of those quantities. Thus, it suffices to have each machine min SrcNbhd\u03c4 machine transmit its relevant terms to min SrcNbhd $\\tau\\!-\\!1$ . A round of MPC communication that transmits such messages involves each machine sending most one message of size $\\ell$ and receiving at most $\\ell$ messages, each of size $O(\\ell)$ . ", "page_idx": 29}, {"type": "text", "text": "Inductively, we ensure that all neighborhood sums are computed after $r-t$ propagate-up rounds. Because each machine handles at most $\\ell$ distinct messages having total size $\\bar{O}(\\ell^{2})$ per MPC round, this protocol does not violate the bounded message size and bounded distinct message constraints (so long as $\\ell^{2}\\ll s)$ ), which can be guaranteed for sufficiently large $n$ , so long as $\\ell=O(n^{\\rho/2})$ . ", "page_idx": 29}, {"type": "text", "text": "Aggregation rounds: After the completion of the aggregation rounds, each machine $i$ computes terms of the form ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\{\\mathtt{E q u a l D s t S u m}^{t,\\tau}(\\operatorname*{min}\\mathtt{M b h d}^{\\tau+1}(i),\\mathtt{D s t M b h d}):\\mathtt{D s t M b h d}\\in\\mathtt{C h i l d r e n}(\\mathtt{N b h d}_{j}^{t})\\right\\}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "from relevant NbhdSumt terms if $i=\\operatorname*{min}\\mathbb{N}\\mathtt{b h d}^{\\tau+1}(i)$ . By the recurrence, it is sufficient for machine $i$ to following $\\ell^{2}$ distinct terms ", "page_idx": 29}, {"type": "text", "text": "$\\{{\\mathrm{NbhdSum}}^{t}({\\mathrm{SrcNbhd}},{\\mathrm{DstNbhd}}):{\\mathrm{SrcNbhd}}\\in{\\mathrm{Chi}}1{\\mathrm{dren}}({\\mathrm{Nbhd}}^{\\tau}(i)),{\\mathrm{DstNbhd}}\\}$ DstNbhd \u2208Children(Nbhdtj) ", "page_idx": 29}, {"type": "text", "text": "Since all machines $i$ already knows such terms for ${\\mathrm{SrcNbhd~}}={\\mathrm{~Nbhd}}^{\\tau+1}(i)$ , it can obtain the remaining NbhdSumt terms by simultaneously sharing information with its \u201ccousin machines:\u201d min SrcNbhd, for all Src ${\\mathrm{ibhd}}\\,\\in\\,{\\mathrm{Chi1dren}}({\\mathrm{Nbhd}}^{\\tau}(i))$ . This can be handled by a single round of communication where each \u201c $\\left(\\tau+1\\right)$ th-level neighborhood representative\u201d machine forwards its sums sums to up to $\\ell$ other first representatives, for a total messaging cost of $O(\\ell^{2})$ . ", "page_idx": 29}, {"type": "text", "text": "We use $\\boldsymbol{r}-t$ separate rounds to repeat this process for each $\\tau\\geq t$ . ", "page_idx": 29}, {"type": "text", "text": "Propagate-down rounds: It remains to compute each EqualDst $\\mathtt{S u m}^{t,\\tau}(i,\\cdot)$ and LessDst $\\mathtt{S u m}^{t}(\\mathtt{N b h d}^{\\tau}(i),\\cdot)$ term at each machine $i$ . The relevant LessDstSumt terms have already been computed by each respective min $\\mathtt{N b h d}^{\\tau}(i)$ machine and can be propagated to machine $i$ by using $r-t$ rounds of tree-propagation through intermediate min Nbhd $\\tau^{\\prime}(i)$ machines. ", "page_idx": 29}, {"type": "text", "text": "The same is possible for EqualDst $\\mathtt{S u m}^{t,\\tau}$ terms, although the individual terms need to be added in order to follow the recurrence. We propagate the terms in the same way as the LessDstSumt terms, but we take special care to carry out the extra additions. This can be accomplished simultaneously to the other propagate-down rounds. This protocol involves each first-child node sharing at most $\\ell$ distinct messages, each of size at most $O((r-t)\\ell)$ . As before, every node sends and receives at most $O((r-t)\\ell^{2})\\overset{\\mathcal{\\footnotesize{<}}}{\\ll}s$ words. ", "page_idx": 29}, {"type": "text", "text": "After these $3(r-t)$ rounds have elapsed, each machine $i$ computes $b^{t,\\tau}(\\mathbf{P})$ for each $\\mathsf{P}\\in\\mathsf{o u t b o x}_{i}^{t}$ . Using this itinerary, the machines routes tuples of the form ", "page_idx": 29}, {"type": "equation", "text": "$$\n(\\mathsf{P},b^{t,r}(\\mathsf{P}),\\ldots,b^{t,t}(\\mathsf{P}))\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "to the respective machine $b^{t,\\tau}(\\mathbf{P})$ in round $r-\\tau$ . Due to the arguments presented at the start of the section, this procedure terminates with each (Msg, Src, Dst) tuple being held by some machine $i$ such that the resulting $0{\\tt u t b o x}_{i}^{t+1}$ is valid, and the procedure involves at most $\\begin{array}{r}{O(r-t)=O(\\frac{1+\\gamma}{\\rho})}\\end{array}$ rounds of $(\\gamma,\\delta,\\rho)$ -MPC computation. \u53e3 ", "page_idx": 29}, {"type": "text", "text": "B.3.2 Proof outline of Corollary 25 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Corollary 25 (Transformers simulate $(\\gamma,\\delta,\\rho)$ -MPC). For constants $\\delta\\,\\in\\,(0,1)$ and $\\gamma,\\rho>0$ , an $R$ -round deterministic $(\\gamma,\\delta,\\rho)$ -MPC protocol with $n$ inputs can be simulated by a transformer $f\\in$ Transformer $\\stackrel{N,N^{\\prime}}{m,H,L}$ with depth $L=R+1$ , heads $H=1$ , embedding dimension $m={\\cal O}(n^{\\delta+4\\rho}\\log n)$ , context length $N=n$ , and blank chain-of-thought tokens $N^{\\prime}=\\operatorname*{max}(0,O(n^{1+\\gamma-\\delta})-n)$ . ", "page_idx": 29}, {"type": "text", "text": "The proof of Corollary 25 involves an adaptation to the proof of Theorem 3.1 of [71]. To avoid restating the proof in its entirety, we provide a brief outline of the proof of Theorem 3.1 and explain which modification is necessary. ", "page_idx": 30}, {"type": "text", "text": "Theorem 3.1 is a consequence of Lemmas B.4, B.5, and B.6 of [71], which establish that there exist single-layer transformers that simulate the initialization, a round of computation and communication, and the output formatting of any fixed $(\\gamma,\\delta)$ -MPC protocol. The input and output steps of $(\\gamma,\\delta)$ , and $(\\gamma,\\delta,\\rho)$ -MPC protocols are identical, only Lemma B.5 needs to be examined. ", "page_idx": 30}, {"type": "text", "text": "To simulate a single round of an MPC protocol with a transformer, all local computations are simulated in the element-wise multi-layer perceptron (MLP) units, and all communication is handled in a single multi-headed self-attention layer (Lemma B.7) Since $(\\gamma,\\delta,\\rho)$ -MPC protocols add no restrictions related to local computation, the former can be simulated in exactly the same manner with identical MLPs, and it remains to analyze the construction of Lemma B.7. We restate Lemma B.7 and provide an replacement lemma that suffices to prove Corollary 25. ", "page_idx": 30}, {"type": "text", "text": "Lemma 34 (Lemma B.7 of [71]; multi-headed attention simulates MPC communication). For any $R$ -round MPC protocol with local memory s and $q$ machines and any round $r\\in[R-1]$ , there exists a single-layer transformer $f\\,\\in$ Transforme $\\stackrel{q,0}{m,H,1}$ with $H=O(\\log\\log q)$ and $m\\,=\\,O(s^{4}\\log q)$ which, given as input a length- $q$ encoding of each machine\u2019s outgoing messages in round $r$ , returns an encoding of each machine\u2019s incoming messages in round $r+1$ . ", "page_idx": 30}, {"type": "text", "text": "Lemma 35 (Single-headed attention simulates bounded-message MPC communication). For any $R$ -round MPC protocol with local memory s, q machines, and a $k$ -machine communication limit and any round $r\\in[R-1],$ , there exists a single-layer single-headed transformer $f\\in$ Transforme $\\stackrel{q,0}{m,H,1}$ with $H=1$ and $m=O(k^{4}s\\log q)$ , which, given as input a length- $q$ encoding of each machine\u2019s outgoing messages in round $r$ , returns an encoding of each machine\u2019s incoming messages in round $r+1$ . ", "page_idx": 30}, {"type": "text", "text": "Lemma 35 is an immediate consequence of Lemma 3.2 of [71], their main technical result, which already applies to the regime with limits on the number of machines in communication. ", "page_idx": 30}, {"type": "text", "text": "By replacing Lemma 34 with Lemma 35, applying the remainder of the proof of Theorem 3.1 of [71], and letting $\\bar{k}=O(n^{\\rho})$ , the proof of Corollary 25 is complete. ", "page_idx": 30}, {"type": "text", "text": "C Single-layer transformers and graph reasoning tasks ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "This appendix presents the results of Section 3.3, which separates the collection of graph reasoning tasks into those retrieval tasks that can be efficiently solved by single-layer parameter-efficient transformers\u2014including node count, edge count, edge existence, and node degree\u2014and those parallelizable or search tasks that require deeper constructions\u2014including connectivity, shortest path, cycle check, and triangle count. Taken together, these results establish that the single-layer transformers of the Depth1 regime are capable of solving simple aggregation-based tasks, but that their known limitations in capacity as communication protocols of [70] apply to non-trivial graph reasoning tasks. ", "page_idx": 30}, {"type": "text", "text": "We specific a particular node/edge encoding of an input graph $G=(V,E)$ and a graph reasoning task using a consistent encoding scheme that closely resembles the encoding used in our graph reasoning experiments and those of [42]. This encoding is distinguished by the fact that each node and vertex of the graph $G$ is represented by exactly one token, rather than the pair of tokens utilized in our experiments. This choice ensures that any trivial pre-processing of graph inputs (e.g. using a positional embedding to associate each edge token pair) need not count towards the single-layer transformer model. ", "page_idx": 30}, {"type": "text", "text": "Definition 4. The node/edge encoding of a graph $G=(V,E)$ with $V\\subseteq[n]$ and $|V|+|E|\\le N-1$ and a graph reasoning task $P$ is a sequence ", "page_idx": 30}, {"type": "equation", "text": "$$\nX=X(G,P)=(x_{1},\\ldots,x_{N})\\in\\mathbb{R}^{N\\times d}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $d=5$ and each ", "page_idx": 30}, {"type": "equation", "text": "$$\nx_{i}=(x_{i}^{1},x_{i}^{2},\\mathbf{i}\\,\\mathbf{s}\\mathbf{Vertex}_{i},\\mathbf{i}\\,\\mathbf{s}\\mathbf{Edge}_{i},\\mathbf{i}\\,\\mathbf{s}\\mathbf{Task}_{i})\\in\\{0,\\ldots,n\\}^{2}\\times\\{0,1\\}^{3}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "satisfies the following conditions: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The token $x_{N}$ encodes a particular instance of the task $P$ , by encoding $\\mathtt{i s T a s k}_{N}=1$ with an optional edge or node encoding. That is, for tasks without arguments (such as triangle count), $x_{N}=\\bar{(0,0,0,0,1)}$ . For tasks with a single node argument $v\\in[n]$ (such as node degree), $x_{N}=(v,0,1,0,1)$ . For tasks with a pair of node arguments $\\bar{u,v}\\in[n]$ (such as shortest path and connectivity), $x_{N}=(u,v,0,1,1)$ .   \n\u2022 All other tokens satisfy ", "page_idx": 31}, {"type": "text", "text": "We say that a single-layer transformer $f\\in$ Transforme $\\stackrel{N}{m}_{,H,1}$ solves task $P$ on graph $G$ if the output corresponding to the task description $f(\\ensuremath{\\boldsymbol{X}}(G,P))_{N}$ encodes the output of the task. Since $f$ is a single-layer transformer, we can write this output as ", "page_idx": 31}, {"type": "equation", "text": "$$\nf(X)_{N}=\\psi\\left(\\sum_{h=1}^{H}\\mathrm{softmax}\\left(\\phi(x_{N})^{\\mathsf{T}}Q_{h}K_{h}^{\\mathsf{T}}\\phi(X)^{\\mathsf{T}}\\right)\\phi(X)V_{h}\\right)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for element-wise multi-layer perceptrons ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\phi:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{m},\\psi:\\mathbb{R}^{m}\\rightarrow\\mathbb{R}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "broadcasted across each input (i.e. $\\phi(X)=(\\phi(x_{1}),\\dots,\\phi(x_{N}))\\in\\mathbb{R}^{N\\times m})$ and weight matrices ", "page_idx": 31}, {"type": "equation", "text": "$$\nQ_{1},\\dots,Q_{H},K_{1},\\dots,K_{H},V_{1},\\dots,V_{H}\\in\\mathbb{R}^{m\\times m}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Throughout, we assume that all parameters in the transformer model and intermediate numerical quantities can be written using ${\\cal O}(\\log N)$ -bit floating point numbers. This assumption can be satisfied for the positive results of Appendix C.1 and is necessary to obtain the negative results of Appendix C.2. ", "page_idx": 31}, {"type": "text", "text": "We permit the element-wise MLPs $\\phi$ and $\\psi$ to be arbitrary functions for the negative results, while restricting them to be MLPs that can be approximated using bounded-size multi-layer ReLU networks for the positive results. While we do not make these ReLU networks explicit, we restrict ourselves to simple operations that can be computed using linear transformations and the application of smooth univariate functions. ", "page_idx": 31}, {"type": "text", "text": "Finally, we acknowledge that the negative results in are largely superseded by those of [55], which establishes that L-complete languages (including graph connectivity and cycle check) cannot be efficiently solved by constant-depth transformers, let alone single-layer transformers. We include these bounds anyway to mark a contrast with our positive results; draw further connections to the communication complexity lens on transformers; and exhibit simple task instances $(G,P)$ that require greater depth, including constant diameter and constant degree graphs. ", "page_idx": 31}, {"type": "text", "text": "C.1 Positive results for single-layer transformers ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Theorem 36 (Formal version of Theorem 5; Depth1 computes retrieval tasks). Fix any graph reasoning task among node count, edge count, edge existence, and node degree and any graph size $N$ . Then, there exists a single-layer single-headed transformer $f\\in$ Transforme $\\hat{\\mathbf{\\Phi}}_{m,1,1}^{N}$ with embedding dimension $m=O(\\log N)$ that solves the task on all graphs $G=(V,E)$ of size $|V|+|E|\\le N-1$ formatted as node/edge embedding sequences. ", "page_idx": 31}, {"type": "text", "text": "Proof. We prove that a single head of self-attention with input and output MLPs can solve these retrieval and aggregation tasks in a parameter-efficient manner by first carefully designing a universal input MLP $\\phi:\\bar{\\mathbb{R}}^{d}\\bar{\\to}\\mathbb{R}^{m}$ for some $m=O(\\log N)$ to produce embeddings that encode useful graph properties. Then, we define task-specific query, key, and value matrices $Q,K,V:\\in\\mathbb{R}^{m\\times\\bar{m}}$ and output MLPs $\\psi:\\mathbb{R}^{m}\\rightarrow\\mathbb{R}$ that produce the correct answers. While we do not explicitly account for finite-precision computations, all of the operations utilized can be carefully implemented to respect $O(\\log N)$ -bit floating-point numerical representations using the technical approaches of [70, 71]. ", "page_idx": 31}, {"type": "text", "text": "Shared sinusoidal embedding MLP. We denote the embedding output of the input MLP as ", "text_level": 1, "page_idx": 32}, {"type": "equation", "text": "$$\n\\phi(x_{i})=({\\mathtt{i s T a s k}}_{i},{\\mathtt{i s V e r t e x}}_{i},{\\mathtt{i s E d g e}}_{i},\\phi^{\\prime}(x_{i}))\\in\\mathbb{R}^{m}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "for some $\\phi^{\\prime}(x_{i})\\,:\\,\\mathbb{R}^{d}\\,\\to\\,\\mathbb{R}^{2m^{\\prime}}$ for some $m^{\\prime}\\,=\\,O(\\log N)$ and $m\\,=\\,2m^{\\prime}+3$ . For some fixed $a_{1},\\dotsc,a_{m^{\\prime}}\\in[0,1]$ to be determined, let ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\phi^{\\prime}(x_{i})=\\left\\{\\!\\!\\!\\begin{array}{l l}{\\eta(x_{i}^{1})}&{\\mathrm{if~isVertex}_{i}=1,}\\\\ {\\xi(x_{i}^{1},x_{i}^{2})}&{\\mathrm{if~isEdge}_{i}=1,}\\\\ {\\vec{0}}&{\\mathrm{otherwise},}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\eta$ is a sinusoidal embedding MLP for nodes with ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\eta(v)=(\\sin(2\\pi a_{1}v),\\cos(2\\pi a_{1}v),\\dots,\\sin(2\\pi a_{m^{\\prime}}v),\\cos(2\\pi a_{m^{\\prime}}v))\\in{\\mathbb R}^{2m^{\\prime}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and $\\xi$ is an edge embedding satisfying ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\xi(u,v)=\\frac{m^{\\prime}}{m^{\\prime}+\\eta(u)^{\\mathsf{T}}\\eta(v)}(\\eta(u)+\\eta(v)).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We first show that the node embeddings are approximately orthogonal. By employing standard trigonometric identities, we obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\eta(u)^{\\mathsf{T}}\\eta(v)=\\sum_{j=1}^{m^{\\prime}}\\cos(2\\pi a_{j}(u-v)),\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and note that $\\|\\boldsymbol{\\eta}(v)\\|_{2}^{2}=m^{\\prime}$ . We use a standard concentration argument to prove that $|\\eta(u)^{\\top}\\eta(v)|\\ll$ $m^{\\prime}$ if $u\\ne v$ with high probability. ", "page_idx": 32}, {"type": "text", "text": "Claim 37 (Near-orthogonality of embeddings $\\eta$ ). There exist coefficients $a_{1},\\dots,a_{m^{\\prime}}\\in[0,1]$ that comprise the embedding $\\eta:[n]\\rightarrow\\mathbb{R}^{m}$ of Equation (8) such that ", "page_idx": 32}, {"type": "equation", "text": "$$\n|\\eta(u)^{\\mathsf{T}}\\eta(v)|\\leq2\\sqrt{m^{\\prime}\\log n},\\,\\,i f\\,u\\neq v.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. We employ the probabilistic method. Consider $m^{\\prime}$ iid random variables $\\mathbf{a}_{1},\\ldots,\\mathbf{a}_{m^{\\prime}}\\mathbf{\\Omega}\\sim$ $\\mathrm{Unif}([0,1])$ and let $\\eta$ represent the respective node embedding. Fix some arbitrary $u,v\\in[n]$ with $u\\ne v$ and note that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\eta(u)^{\\top}\\eta(v)=\\sum_{j=1}^{m^{\\prime}}\\cos(2\\pi\\mathbf{a}_{j}(u-v)).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "For any $j\\in[m^{\\prime}]$ , the integrality of $u-v$ implies that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\mathbf{a}_{j}}{\\mathbb{E}}\\left[\\cos(2\\pi\\mathbf{a}_{j}(u-v))\\right]=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Hoeffding\u2019s inequality provides the following bound: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\left|\\eta(u)^{\\mathsf{T}}\\eta(v)\\right|\\geq2\\sqrt{m^{\\prime}\\log n}\\right]\\leq\\exp\\left(-\\frac{4m^{\\prime}\\log n}{2m^{\\prime}}\\right)\\leq\\frac{1}{n^{2}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By applying a union bound to all $\\binom{n}{2}$ choices of $u$ and $v$ , we have the following: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\forall u,v\\in[n],\\;u\\neq v:\\;|\\eta(u)^{\\top}\\eta(v)|\\geq2\\sqrt{m^{\\prime}\\log n}\\right]\\leq\\frac{n(n-1)}{2n^{2}}<1.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Hence, there exists a satisfactory set of coefficients $a_{1},\\dots,a_{m^{\\prime}}$ . ", "page_idx": 32}, {"type": "text", "text": "For some $\\eta$ satisfying Claim 37, let $\\begin{array}{r}{\\chi\\,=\\,\\operatorname*{max}_{u,v\\in[n],u\\neq v}\\,|\\eta(u)^{\\mathsf{T}}\\eta(v)|\\,\\leq\\,2\\sqrt{m^{\\prime}\\log n}\\,}\\end{array}$ . By taking $m^{\\prime}=O(\\log n)$ be sufficiently large, we guarantee that $\\chi\\leq\\frac{m^{\\prime}}{c}$ for any constant $c\\geq2$ . ", "page_idx": 32}, {"type": "text", "text": "We then bound all relevant inner products between vertex and edge embeddings for sufficently large $m^{\\prime}$ (and sufficiently small $\\chi$ ). We assume throughout that $u,u^{\\prime},v,v^{\\prime}$ are distinct and note that $\\xi(u,v)=\\xi(v,u)$ (and omit symmetric inequalities). ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left||\\phi(e)\\right|_{2}^{2}=m^{*},}\\\\ &{\\left|u(\\tau)^{*}(e)\\right|_{1}\\leq x\\leq\\frac{\\sigma^{2}}{4},}\\\\ {|u(\\tau)^{*}(e)-w^{2}|+\\frac{1}{2}\\frac{m^{*}(1+\\epsilon)}{2},}\\\\ &{\\left||u(\\tau)^{*}(e)-w^{2}|+\\left|\\frac{m^{*}(1+\\epsilon)}{2}(u(\\tau)^{*}(e)-w^{2})\\right|-m^{*}\\right|=0,}\\\\ &{\\left||u(\\tau)^{*}(e)\\right|_{1}^{2}\\leq\\frac{\\sigma^{2}}{4}\\leq\\frac{1}{2}\\epsilon^{\\epsilon},}\\\\ &{\\left||u(\\tau)^{*}(e)-w^{2}|\\right|=\\left|\\frac{m^{*}(1+\\epsilon)}{2}(u(\\tau)^{*}(e)-u(\\tau)^{*}(e)-x^{2})\\right|\\leq\\frac{2M_{C}^{*}}{2}\\leq4(x)\\frac{M_{C}^{*}}{4},}\\\\ &{\\left||u(\\tau)^{*}(e)-x^{2}|\\right|=\\frac{1}{2}\\frac{m^{*}(1+\\epsilon)}{2}(2(4\\tau)^{*}(e)-2\\alpha\\tau)\\frac{M_{C}^{*}}{2}\\alpha\\tau\\left(\\alpha\\tau+m(1+\\epsilon)\\frac{m^{*}(1+\\epsilon)}{2}(u(\\tau)^{*}(e)-\\alpha\\tau)^{*}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{\\epsilon^{2}}{4}\\left|\\frac{2M_{C}^{*}(1+\\epsilon)}{2}\\alpha(\\tau)^{*}(e)\\right|_{1}^{2}\\leq\\frac{2M_{C}^{*}}{2}\\leq4x\\leq\\frac{M_{C}^{*}}{3},}\\\\ &{\\xi(\\alpha,\\tau)^{*}(e),}\\\\ &{(\\alpha,\\tau)^{*}(e)=-\\frac{1}{2}\\frac{m^{*}(1+\\epsilon)}{2}(u(\\tau)^{*}(e)-u(\\tau)^{*}(e)-x^{2})\\frac{M_{C}^{*}}{2}\\leq\\frac{\\sigma^{2}}{4}(x)\\frac{M_{C}^{*}\\tau}{2},}\\\\ &{\\left|\\frac{m^{*}(1+\\epsilon)}{2}(u\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Therefore, we conclude the following bounds: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\eta(\\boldsymbol{u})^{\\mathsf{T}}\\eta(\\boldsymbol{v}),\\;\\eta(\\boldsymbol{u})^{\\mathsf{T}}\\xi(\\boldsymbol{u}^{\\prime},\\boldsymbol{v}^{\\prime}),\\;\\xi(\\boldsymbol{u},\\boldsymbol{v})^{\\mathsf{T}}\\xi(\\boldsymbol{u}^{\\prime},\\boldsymbol{v}^{\\prime})\\in\\left[-\\frac{m^{\\prime}}{4},\\frac{m^{\\prime}}{4}\\right],}\\\\ &{}&{\\Vert\\eta(\\boldsymbol{v})\\Vert_{2}^{2}\\,,\\;\\eta(\\boldsymbol{u})^{\\mathsf{T}}\\xi(\\boldsymbol{u},\\boldsymbol{v}),\\;\\xi(\\boldsymbol{u},\\boldsymbol{v})^{\\mathsf{T}}\\xi(\\boldsymbol{u},\\boldsymbol{v}^{\\prime})\\in\\left[\\frac{3m^{\\prime}}{4},\\frac{5m^{\\prime}}{4}\\right],}\\\\ &{}&{\\Vert\\xi(\\boldsymbol{u},\\boldsymbol{v})\\Vert_{2}^{2}\\in\\left[\\frac{7m^{\\prime}}{4},\\frac{9m^{\\prime}}{4}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We now apply the above bounds to prove that each task can be solved by a single-headed transformer with weights $Q,K,V\\in\\mathbb{R}^{m\\times m}$ , output MLP $\\psi\\,:\\mathbb{R}^{m}\\,\\rightarrow\\,\\mathbb{R}$ , and shared input MLP $\\phi$ discussed previously. ", "page_idx": 33}, {"type": "text", "text": "Node count task. If $P$ encodes the node count task, we specify weights matrices $Q,K,\\underline{{V}}$ in order to ensure that $Q^{\\mathsf{T}}\\phi(x_{N})\\,=\\,m^{\\prime}{e_{1}}^{9}$ ; $K^{\\mathsf{T}}\\phi(x_{i})\\,=\\,({\\sf i s V e r t e x}_{i}+{\\sf i s T a s k}_{i})\\cdot e_{1}$ ; and $V^{\\mathsf{T}}\\phi(x_{i})\\,=$ ${\\mathtt{i s T a s k}_{i}\\cdot e_{1}}^{\\mathtt{l0}}$ . Then, the following is true of exponentiated key/query inner products and scalings products of value vectors: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\exp(\\phi({\\boldsymbol x}_{N})^{\\mathsf{T}}Q K^{\\mathsf{T}}\\phi({\\boldsymbol x}_{i}))=\\left\\{\\begin{array}{l l}{e^{m^{\\prime}}}&{\\mathrm{if~is\\,vertex}_{i}=1\\;\\mathrm{or}\\;i=N,}\\\\ {1}&{\\mathrm{otherwise}.}\\end{array}\\right.}\\\\ {\\exp(\\phi({\\boldsymbol x}_{N})^{\\mathsf{T}}Q K^{\\mathsf{T}}\\phi({\\boldsymbol x}_{i}))V\\phi({\\boldsymbol x}_{i})=\\left\\{\\begin{array}{l l}{e^{m^{\\prime}}\\cdot e_{1}}&{\\mathrm{if~}i=N,}\\\\ {\\vec{0}}&{\\mathrm{otherwise}.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The output of the softmax is then ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathrm{softmax}\\left(\\phi(x_{N})^{\\top}Q K^{\\top}\\phi(X)^{\\top}\\right)\\phi(X)V=\\frac{\\sum_{i=1}^{N}\\exp(\\phi(x_{N})^{\\top}Q K^{\\top}\\phi(x_{i}))V\\phi(x_{i})}{\\sum_{i=1}^{N}\\exp(\\phi(x_{N})^{\\top}Q K^{\\top}\\phi(x_{i}))}\\qquad\\qquad}\\\\ &{}&{=\\frac{e^{m^{\\prime}}}{e^{m^{\\prime}}\\cdot(1+|V|)+1\\cdot(N-|V|-1)}\\cdot e_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Let ", "page_idx": 34}, {"type": "equation", "text": "$$\ny_{N}:=\\left(\\mathrm{softmax}\\left(\\phi(x_{N})^{\\mathsf{T}}Q K^{\\mathsf{T}}\\phi(X)\\right)\\phi(X)V\\right)_{1}\\in\\mathbb{R}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "denote the first coordinate of the softmax output. By taking $m^{\\prime}\\ge\\log(4N)$ , we guarantee that ", "page_idx": 34}, {"type": "equation", "text": "$$\ny_{N}\\in\\left[\\frac{1}{1+|V|+N/e^{m^{\\prime}}},\\frac{1}{1+|V|}\\right]\\subseteq\\left[\\frac{1}{\\frac{5}{4}+|V|},\\frac{1}{1+|V|}\\right].\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "By letting the output MLP $\\psi$ approximate the function $\\begin{array}{r}{\\psi(z)=\\left[\\frac{1}{z_{1}}-1\\right]}\\end{array}$ for $z_{1}\\in[1,N+2]$ , we can ensure that $f(X)_{N}=|V|$ . ", "page_idx": 34}, {"type": "text", "text": "Edge count task. We similarly let $Q^{\\mathsf{T}}\\phi(x_{N})=m^{\\prime}e_{1};K^{\\mathsf{T}}\\phi(x_{i})=(\\mathtt{i s E d g e}_{i}+\\mathtt{i s T a s k}_{i})\\cdot e_{1}$ ; and $V^{\\mathsf{T}}\\phi(x_{i})={\\mathsf{i s T a s k}}_{i}\\cdot e_{1}$ . By an identical analysis of the attention matrix and $\\psi$ construction, we ensure that $f(X)_{N}=|E|$ . ", "page_idx": 34}, {"type": "text", "text": "Edge existence task. Under the node/edge encoding, we assume that the edge existence task is encoded as $x_{N}~=~(x_{N}^{1},x_{N}^{2},0,1,1)$ for some $x_{N}^{1},x_{N}^{2}~\\in~V$ and should return $f(X)_{N}~=~$ $1\\left\\{(x_{N}^{1},x_{N}^{2})\\in E\\right\\}$ . We choose our weight matrices to ensure that $Q^{\\sf T}\\phi(x_{N})\\;\\;=\\;\\;\\phi^{\\prime}(x_{N})\\;\\;=\\;\\;$ $\\xi(x_{N}^{1},x_{N}^{2})$ ; $K^{\\mathsf{T}}\\phi(x_{i})\\,=\\,\\phi^{\\prime}(x_{i})$ , and $V^{\\mathsf{T}}\\phi(x_{i})\\,=\\,2(1\\,-\\,\\mathtt{i s T a s k}_{i})e_{1}$ . By applying Claim 37 and letting $m^{\\prime}=O(\\log N)$ to be sufficiently large, the following is true the query/key inner products: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\exp(\\phi({x_{N}})^{\\sf T}Q K^{\\sf T}\\phi({x_{i}}))=\\exp(\\big\\|\\xi(x_{N}^{1},x_{N}^{1})\\big\\|_{2}^{2})\\ge e^{7m^{\\prime}/4}}&{\\mathrm{~if~}\\big\\{x_{i}^{1},x_{i}^{2}\\big\\}=\\big\\{x_{N}^{1},x_{N}^{2}\\big\\},}\\\\ &{\\exp(\\phi({x_{N}})^{\\sf T}Q K^{\\sf T}\\phi({x_{i}}))\\le e^{5m^{\\prime}/4}\\le\\displaystyle\\frac{1}{8N}e^{7m^{\\prime}/4}}&{\\mathrm{otherwise.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We can therefore analyze the softmax output $y_{N}$ to obtain the following bound: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y_{N}\\leq\\frac{2\\exp(\\left\\vert\\left\\vert\\xi(x_{N}^{1},x_{N}^{1})\\right\\vert\\left\\vert2_{2}^{2}\\right\\vert1\\left\\{(x_{N}^{1},x_{N}^{2})\\in E\\right\\}+2N\\cdot\\frac{1}{8N}e^{7m^{\\prime}/4}}{\\exp(\\left\\vert\\left\\vert\\xi(x_{N}^{1},x_{N}^{1})\\right\\vert\\right\\vert2_{2}^{2})(1+1\\left\\{(x_{N}^{1},x_{N}^{2})\\in E\\right\\})}}\\\\ &{\\quad\\leq\\frac{21\\left\\{(x_{N}^{1},x_{N}^{2})\\in E\\right\\}+\\frac{1}{4}}{1+1\\left\\{(x_{N}^{1},x_{N}^{2})\\in E\\right\\}}\\leq1\\left\\{(x_{N}^{1},x_{N}^{2})\\in E\\right\\}+\\frac{1}{4},}\\\\ &{y_{N}\\geq\\frac{2\\exp(\\left\\vert\\left\\vert\\xi(x_{N}^{1},x_{N}^{1})\\right\\vert\\left\\vert2_{2}^{2})1\\left\\{(x_{N}^{1},x_{N}^{2})\\in E\\right\\}}{\\exp(\\left\\vert\\left\\vert\\xi(x_{N}^{1},x_{N}^{1})\\right\\vert\\right\\vert2_{2}^{2})(1+1\\left\\{(x_{N}^{1},x_{N}^{2})\\in E\\right\\})+N\\cdot\\frac{1}{8N}e^{7m^{\\prime}/4}}}\\\\ &{\\quad\\geq\\frac{21\\left\\{(x_{N}^{1},x_{N}^{2})\\in E\\right\\}}{1+1\\left\\{(x_{N}^{1},x_{N}^{2})\\in E\\right\\}+\\frac{1}{8}}\\geq1\\left\\{(x_{N}^{1},x_{N}^{2})\\in E\\right\\}-\\frac{1}{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Hence, ", "page_idx": 34}, {"type": "equation", "text": "$$\ny_{N}\\in\\left\\{\\!\\!\\begin{array}{l l}{\\left[-\\frac{1}{4},\\frac{1}{4}\\right]}&{\\mathrm{if~}(x_{N}^{1},x_{N}^{2})\\not\\in E,}\\\\ {\\left[\\frac{3}{4},\\frac{5}{4}\\right]}&{\\mathrm{if~}(x_{N}^{1},x_{N}^{2})\\in E.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Therefore, it suffices to design a threshold output MLP $\\psi$ that satisfies ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\psi(z)=\\left\\{1\\begin{array}{l l}{{z_{1}}}&{{z_{1}\\ge\\frac{3}{4},}}\\\\ {{0}}&{{z_{1}\\le\\frac{1}{4},}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "in order to distinguish between the two possible output ranges. This can be easily constructed by taking a linear combination of two ReLU neurons. ", "page_idx": 34}, {"type": "text", "text": "Node degree task. We assume that the task is encoded as $x_{N}=(x_{N}^{1},0,1,0,1)$ and should return $f(X)_{N}\\,=\\,\\deg(x_{N}^{1})\\,:=\\,\\big|\\big\\{(x_{N}^{1},v)\\in E\\big\\}\\big|$ . We use weight matrices with $Q^{\\mathsf{T}}\\phi(x_{N})\\,=\\,\\phi^{\\prime}(x_{N})\\,=$ $\\eta(\\boldsymbol{x}_{N});K^{\\mathsf{T}}\\phi(\\boldsymbol{x}_{i})=\\phi^{\\prime}(\\boldsymbol{x}_{i})$ ; and $V^{\\mathsf{T}}\\phi(x_{i})={\\mathsf{i s T a s k}}_{i}$ . This ensure that the following is true about the query/key inner products for sufficiently large $m^{\\prime}$ : ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\exp(\\phi(x_{N})^{\\mathsf{T}}Q K^{\\mathsf{T}}\\phi(x_{i}))=e^{m^{\\prime}}}&{\\qquad\\mathrm{if~}i=N\\;\\mathrm{or}\\;x_{N}^{1}\\in\\big\\{x_{i}^{1},x_{i}^{2}\\big\\},}\\\\ &{\\exp(\\phi(x_{N})^{\\mathsf{T}}Q K^{\\mathsf{T}}\\phi(x_{i}))\\leq e^{m^{\\prime}/4}\\leq\\displaystyle\\frac{1}{4N}e^{m^{\\prime}}}&{\\qquad\\qquad\\qquad\\mathrm{otherwise}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "We similarly ensure that the following holds about the softmax output $y_{N}$ . ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y_{N}\\leq\\cfrac{e^{m^{\\prime}}}{e^{m^{\\prime}}\\cdot(\\deg(x_{N}^{1})+2)}=\\cfrac{1}{\\deg(x_{N}^{1})+2},}\\\\ &{y_{N}\\geq\\cfrac{e^{m^{\\prime}}}{(\\deg(x_{N}^{1})+2)e^{m^{\\prime}}+N\\cdot\\frac{1}{4N}e^{m^{\\prime}}}}\\\\ &{\\geq\\cfrac{1}{\\deg(x_{N}^{1})+\\frac{9}{4}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "We then use the similar approach employed in the node count task to choose an output MLP that approximately computes $\\begin{array}{r}{\\phi(z)=\\left[\\frac{1}{z_{1}}-2\\right]}\\end{array}$ . ", "page_idx": 35}, {"type": "text", "text": "We thus conclude that there exist single-layer transformers that solve all of the described tasks. ", "page_idx": 35}, {"type": "text", "text": "C.2 Negative results for single-layer transformers ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Theorem 38 (Formal version of Theorem 6; Depth1 cannot compute search or retrieval tasks). Fix any graph reasoning task among graph connectivity, shortest path, and cycle detection. Any single-layer transformer $f\\in$ Transforme $\\bar{\\mathsf{r}}_{m,H,1}^{N}$ with ${\\cal O}(\\log N)$ -bit precision that solves the task on all graphs $G=(V,E)$ of size $|V|+|E|\\le\\dot{N}-1$ formatted as node/edge embedding sequences has width satisfying $m H=\\Omega(N/\\log N)$ . ", "page_idx": 35}, {"type": "text", "text": "Our negative results generalizes and applies the approach of [70] to prove negative results for singlelayer transformers by communication complexity reductions. The bounds hinge on the following fundamental fact about the hardness of a two-player game where two agents jointly attempt to compute a set disjointness quantity that depends on both of their inputs with bounded communication. ", "page_idx": 35}, {"type": "text", "text": "Fact 1 (Disjointness communication complexity lower bound [85]). Suppose Alice and Bob are given inputs $a,b\\in\\{0,1\\}^{r}$ and wish to jointly compute $\\mathrm{DISJ}(a,b)\\,=\\,\\mathrm{max}_{i}\\,a_{i}b_{i}$ by alternately sending single-bit messages to one another. Any deterministic protocol that computes DISJ $(a,b)$ requires at least $r$ rounds of communication, or r bits of information. ", "page_idx": 35}, {"type": "text", "text": "We first generalize Theorem 7 of [70] to show that no transformer can efficiently solve an embedded set disjointness problem without having a width that scales linearly in $r$ . We prove the theorem and later apply to it prove negative results about relevant graph reasoning tasks. To demonstrate that the graph reasoning tasks do not require pathological input graphs $G$ to be hard, we exhibit particular input graph instances with constant graph diameter or constant degree where the task cannot be efficiently solved. ", "page_idx": 35}, {"type": "text", "text": "Lemma 39 (Generic Depth1 communication complexity negative result). For some sequence length, fix two disjoint subsets $A,B\\,\\subset\\,[N\\,-\\,1],$ , and consider a single-layer transformer $f\\ \\in$ Transformer $\\stackrel{N}{m}_{,H,1}$ with ${\\cal O}(\\log N)$ -bit precision that solves set disjointness, i.e. $f(X)_{N}=\\operatorname{DISJ}(a,b)$ for any input $X$ where $X_{A}$ is $a$ function of Alice\u2019s input $a\\,\\in\\,\\{0,1\\}^{r}$ , $X_{B}$ is a function of Bob\u2019s input $b\\in\\{0,1\\}^{r}$ , and $X_{[N]\\backslash(A\\cup B)}$ is fixed regardless of a and $b$ . Then, $f$ has width satisfying $m H=\\Omega(r/\\log N)$ . ", "page_idx": 35}, {"type": "text", "text": "Proof. Consider any transformer $f$ that solves DISJ as specified in the theorem statement. We show that this implies the existence of a $O(m H\\log N)$ -round communication protocol that solves $\\mathrm{DISJ}(a,b)$ for any inputs $a,b\\in\\{0,1\\}^{r}$ . An application of Fact 1 immediately proves the theorem statement. ", "page_idx": 35}, {"type": "text", "text": "If such a transformer $f$ exists, then the following is true for some $\\phi,\\psi$ , and $Q,K,V$ : ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{DISJ}(a,b)=\\psi\\left(\\displaystyle\\sum_{h=1}^{H}\\mathrm{softmax}\\left(\\phi(x_{N})^{\\mathsf{T}}Q_{h}K_{h}^{\\mathsf{T}}\\phi(X)\\right)\\phi(X)V_{h}\\right)}\\\\ &{\\qquad\\qquad=\\psi\\left(\\displaystyle\\sum_{h=1}^{H}\\frac{Z_{h,A}\\exp(L_{h,A})+Z_{h,B}\\exp(L_{h,B})+Z_{h,[N]\\backslash(A\\cup B)}\\exp(L_{h,[N]\\backslash(A\\cup B)})}{\\exp(L_{h,A})+\\exp(L_{h,B})+\\exp(L_{h,[N]\\backslash(A\\cup B)})}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "for partial softmax and normalization terms11 defined as follows for $S\\subset[N]$ and $h\\in[H]$ : ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Z_{h,S}=\\displaystyle\\frac{\\sum_{i\\in S}\\exp(\\phi(x_{N})^{\\mathsf{T}}Q_{h}K_{h}^{\\mathsf{T}}\\phi(x_{i}))V_{h}\\phi(x_{i})}{\\sum_{i\\in S}\\exp(\\phi(x_{N})^{\\mathsf{T}}Q_{h}K_{h}^{\\mathsf{T}}\\phi(x_{i}))}\\in\\mathbb{R}^{m},}\\\\ &{L_{h,S}=\\log\\left(\\displaystyle\\sum_{i\\in S}\\exp(\\phi(x_{N})^{\\mathsf{T}}Q_{h}K_{h}^{\\mathsf{T}}\\phi(x_{i}))\\right)\\in\\mathbb{R}_{+}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The definition of input instances $X$ implies that Alice can compute $Z_{h,A}$ and $L_{h,A}$ as a function of her input $a$ ; Bob can similarly compute $Z_{h,B}$ and $L_{h,B}$ from $b$ ; and $\\begin{array}{r}{Z_{h,[N]\\setminus(A\\cup B)},L_{h,[N]\\setminus(A\\cup B)}}\\end{array}$ , and all implementation details of $f$ are known by all players. Therefore, Bob can compute DISJ $(a,b)$ by an $O({\\bar{m}}H\\log N)$ -round protocol where Alice sends him $\\left(Z_{h,A},L_{h,A}\\right)$ bit-by-bit. \u53e3 ", "page_idx": 36}, {"type": "text", "text": "It remains to apply Lemma 39 to each graph reasoning task by defining graph instance encodings $X$ that encode DISJ $(a,b)$ in the solution to the task for some $r=\\Theta(N)$ . ", "page_idx": 36}, {"type": "text", "text": "Proof of Theorem 38. For each task, we provide a pair of \u201chard instances:\u201d one with constant degree and one with constant diameter, in order to obey different notions of graph simplicity. ", "page_idx": 36}, {"type": "text", "text": "Graph connectivity task. For both instances, on any disjointness input $a,b\\in\\{0,1\\}^{r}$ , we define a graph $G=(V,E)$ with $|V|+|E|=O(r)$ whose edges encode the input, i.e. $E=E(a,b)$ . We define three fixed disjoint sets of potential edges $\\bar{E}_{A},\\bar{E}_{B},\\bar{E}_{*}$ with $|\\bar{E_{A}^{\\,}}|+|\\bar{E}_{B}|+|\\bar{E}_{*}|=^{^{\\prime}}O(r)$ such that $\\underline{{E}}=E_{A}(a)^{\\prime}\\cup E_{B}(b)\\cup\\bar{E}_{*}$ , where $\\dot{E_{A}}(a)\\subset\\bar{E}_{A}$ is a function of only Alice\u2019s input $a$ and $E_{B}(b)\\subset{\\bar{E}}_{B}$ is a function of Bob\u2019s $b$ . ", "page_idx": 36}, {"type": "text", "text": "We define an enumeration of all vertices in $V$ and potential edges in $\\bar{E}_{A},\\bar{E}_{B},\\bar{E}_{*}$ . We first fix their sizes $|V|,|\\bar{E}_{A}|,|\\bar{E}_{B}|,|\\bar{E}_{*}|$ and then index the vertices and edges in the order $V,\\bar{E}_{A},\\bar{E}_{B},\\bar{E}_{*}$ . That is, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\,\\,\\,V=\\{1,\\dots,|V|\\}\\,,}\\\\ &{\\bar{E}_{*}=\\{(u_{i},v_{i}):i\\in S_{*}\\}\\,,\\mathrm{~for~}S_{*}=\\left\\{|V|+1,\\dots,|V|+|\\bar{E}_{*}|\\right\\},}\\\\ &{\\bar{E}_{A}=\\{(u_{i},v_{i}):i\\in A\\}\\,,\\mathrm{~for~}A=\\left\\{|V|+|\\bar{E}_{*}|+1,\\dots,|V|+|\\bar{E}_{*}|+|\\bar{E}_{A}|\\right\\},}\\\\ &{\\bar{E}_{B}=\\left\\{(u_{i},v_{i}):i\\in B\\right\\},\\mathrm{~for~}B=\\left\\{|V|+|\\bar{E}_{*}|+|\\bar{E}_{A}|+1,\\dots,|V|+|\\bar{E}_{*}|+|\\bar{E}_{A}|+|\\bar{E}_{B}|\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "This implies that the following transformer input $X\\in R^{N\\times d}$ for $N=|V|+|S_{*}|+|A|+|B|+1$ is a valid node/edge encoding such that inputs $X_{A}$ is a function of $a$ , $X_{B}$ is a function of $b$ , and $X_{[N]\\backslash(A\\cup B)}$ is constant: ", "page_idx": 36}, {"type": "text", "text": "\u2022 For $i\\in V$ , $x_{i}=(i,0,1,0,0)$ encodes a fixed vertex. ", "page_idx": 36}, {"type": "text", "text": "\u2022 For $i\\in S_{*}$ , $x_{i}=(u_{i},v_{i},0,1,0)$ encodes a fixed edge. ", "page_idx": 36}, {"type": "text", "text": "\u2022 For $i\\in A$ , $x_{i}$ represents edge $(u_{i},v_{i})$ if it exists: ", "page_idx": 36}, {"type": "equation", "text": "$$\nx_{i}=\\left\\{\\!\\!\\begin{array}{l l}{(u_{i},v_{i},0,1,0)}&{\\mathrm{if}\\ (u_{i},v_{i})\\in E_{A}(a),}\\\\ {(0,0,0,0,0)}&{\\mathrm{otherwise}.}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "\u2022 For $i\\in B$ , $x_{i}$ represents edge $(u_{i},v_{i})$ if it exists: ", "page_idx": 37}, {"type": "equation", "text": "$$\nx_{i}=\\left\\{\\!\\!\\begin{array}{l l}{(u_{i},v_{i},0,1,0)}&{{\\mathrm{if~}}(u_{i},v_{i})\\in E_{B}(b),}\\\\ {(0,0,0,0,0)}&{{\\mathrm{otherwise}}.}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "\u2022 $x_{N}=(u,v,0,1,1)$ encodes the task token for some fixed $(u,v)\\in V^{2}$ . ", "page_idx": 37}, {"type": "text", "text": "Constant-diameter instance: We define a graph $G$ of diameter at most 8 that encodes an disjointness instance $a,b\\in\\{0,1\\}^{r}$ in an instance of connectivity. ", "page_idx": 37}, {"type": "text", "text": "\u2022 Let $\\vert V\\vert=3r+2$ , and let nodes 1 and $3r\\!+\\!2$ denote the \u201csource\u201d and \u201csink\u201d node respectively. That is, $x_{N}=(1,3r+2,0,1,1)$ .   \n\u2022 Edges between the source node 1 and nodes $\\{2,\\ldots,r+1\\}$ and between the end node $3r+2$ and $\\{2r+2,\\ldots,3r+1\\}$ always are included. That is, $E_{*}=\\left\\{(1,i):i\\in\\{2,\\ldots,r+1\\}\\right\\}\\cup\\left\\{(3r+2,i):i\\in\\{2r+2,\\ldots,3r+1\\}\\right\\}.$ ", "page_idx": 37}, {"type": "text", "text": "\u2022 Alice\u2019s input is encoded as edges $(i+1,i+r+1)$ for $i\\in[r]$ . That is, ", "page_idx": 37}, {"type": "equation", "text": "$$\n{E}_{A}(a)=\\left\\{(i+1,i+r+1):a_{i}=1\\right\\}\\subset\\bar{E}_{A}=\\left\\{(i+1,i+r+1):i\\in[r]\\right\\}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "\u2022 Bob\u2019s inputs are similarly encoded as $(i+r+1,i+2r)$ : ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{E}_{B}(b)=\\left\\{(i+r+1,i+2r+1):b_{i}=1\\right\\}\\subset\\bar{E}_{B}=\\left\\{(i+r+1,i+2r+1):i\\in[r]\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We visualize this construction of $G$ in Figure 5. ", "page_idx": 37}, {"type": "text", "text": "There exists a path between node 1 and node $3r+2$ if and only if there exists some $i\\in[r]$ such that $(i+1,i+r+\\stackrel{\\cdot}{1}),(i+r+1,i+2r+1)\\in E,$ , which corresponds to $a_{i}=b_{i}=1$ . Thus, the graph $G$ is connected if and only if DISJ $(a,b)=1$ . Any transformer $f$ that computes ", "page_idx": 37}, {"type": "equation", "text": "$$\nf(X)_{N}=1\\left\\{1{\\mathrm{~and~}}3r+2{\\mathrm{~are~connected~in}}G\\right\\}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "also solves disjointness. Since $N=|V|+|S_{*}|+|A|+|B|+1=\\Theta(r)$ , we conclude the proof of hardness of solving graph connectivity on this instance by applying Lemma 39. ", "page_idx": 37}, {"type": "image", "img_path": "AfzbDw6DSp/tmp/86eea2f0a35c28b7fc065224f036a94b262bcb18a5759c7a04f0c25c436f0ee1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "Figure 5: The constant diameter graph construction for $r=3$ , $A=(1,0,1)$ and $B=(1,1,0)$ . The source 1 and sink 11 are connected which is equivalent to DI $\\operatorname{SJ}(A,B)=1$ by construction. ", "page_idx": 37}, {"type": "text", "text": "Constant-degree instance: We can define a degree-3 graph $G$ that similarly encodes a disjointness instance $a,b$ . To do so, we modify the previous construction by replacing the fixed edges $E_{*}$ with two binary trees, each of depth $Q(\\log r)$ , between the source and end nodes as roots and Alice and Bob\u2019s nodes incident to $\\bar{E}_{A}$ and $\\bar{E}_{B}$ respectively as leaves. The remainder of the construction\u2014including the encoding of $E_{A}(a)$ and $E_{B}(b)$ and the connectivity analysis\u2014is identical. Since a binary tree with $r$ leaves has $O(r)$ nodes and edges, the graph $G$ can be similarly encoded as a length- $\\mathcal{N}$ transformer input for $N=O(r)$ . See Figure 6 for a visualization. ", "page_idx": 37}, {"type": "image", "img_path": "AfzbDw6DSp/tmp/708bd23056649d563e4887f61f4b39d5bd173565eb629a5474eb74891b47c355.jpg", "img_caption": ["Figure 6: The degree 3 graph construction for $r=4$ , $A=(0,1,0,1)$ , $B=(1,1,0,0)$ . The source node $S$ and sink node $X$ are connected, which is equivalent to DI $\\operatorname{sJ}(A,B)=1$ by construction. "], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "Shortest path task. We can modify the graph connectivity constructions $G$ to create a decision variant of shortest path, rather than graph connectivity. Let $D(G)$ be the length of a path between the source and sink node in each connectivity construction, and note that $D(G)=4$ for the constantdiameter instance and $D(G)=O(\\log r)$ for constant-degree. We design a new graph $G^{\\prime}$ of size $O(r)$ then appends a path of $D(G)+1$ vertices to $G$ . Then, the shortest path between the source and sink node is $D(G)$ if they are connected and $D(G)+1$ if not. ", "page_idx": 38}, {"type": "text", "text": "If there exists a transformer $f^{\\prime}$ solves shortest path on graphs of the form $G^{\\prime}$ , then there exists a minor modification $f$ that solves graph connectivity on graph $G$ . ", "page_idx": 38}, {"type": "text", "text": "Cycle check task. We again modify the graph connectivity constructions. We produce $G^{\\prime}$ by adding a single edge to $G$ between the source node 1 and the sink node $3r+2$ , which ensures that $G^{\\prime}$ has a cycle if and only if $G$ is connected between the source and sink nodes. Therefore, a transformer $f^{\\prime}$ can only solve check check if there exists a transformer $f$ that solves connectivity. \u53e3 ", "page_idx": 38}, {"type": "text", "text": "D Representational gaps between transformers and GNNs ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Our experiments in Section 4 highlight the differences in graph reasoning abilities between vanilla transformers with a naive input graph tokenization and graph neural networks (GNNs). These distinctions can be understood as the consequences of model capacity or inductive bias between transformers and GNNs. This appendix contrasts the novel analysis of the capacities of vanilla transformers in Section 3 with previously established limitations on GNNs. We present two theoretical tools for deriving negative results on GNN capabilities: CONGEST distributed computing model and the Weisfeiler-Leman isomorphism test (WL-test). We discuss the results implied by these frameworks and contrast them with transformer capabilities and limitations. ", "page_idx": 38}, {"type": "text", "text": "D.1 Limitations of GNNs via CONGEST analogy ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "The bidirectional relationship between transformers and the massively parallel computation (MPC) distributed computing model of [71] was partially inspired by a similar analogy between GNNs and the CONGEST model by [51]. ", "page_idx": 38}, {"type": "text", "text": "The MPC and CONGEST distributed computing protocols differ in their models of how messages are passed between machines. While MPC protocols permit any machine to send a message to any other machine subject to capacity constraints, CONGEST protocols operate on a graph topology that restricts machines to send messages exclusively to and from neighbors. As a consequence, two nodes in a CONGEST graph that are separated by a path of length $p$ cannot share information with one another without $\\Omega(p)$ rounds of communication; no similar notion of \u201clong-range communication\u201d exists for MPC. ", "page_idx": 38}, {"type": "text", "text": "All message-passing GNNs where each node initially knows only its own features can be simulated by a CONGEST protocol whose rounds scales linearly in the depth of the GNN [51]. This reduction implies a lower bound on solving several parallelizable and search tasks discussed in Appendix A.2. ", "page_idx": 38}, {"type": "text", "text": "Theorem 40. Any graph neural network $f$ with width (message size) m and depth L that computes any of the following tasks on $n$ -node graphs has the following size lower bounds: ", "page_idx": 38}, {"type": "text", "text": "In contrast, the quantitative bounds in Section 3 establish sharp trade-offs between transformers and GNNs for parallelizable tasks and suggest a possible equivalence for search tasks. ", "page_idx": 39}, {"type": "text", "text": "All parallelizable tasks\u2014including (subgraph) connectivity, minimum spanning forest, minimum cut, and cycle detection\u2014can be solved by transformers of depth $L=O(\\log n)$ and width $m=O(n^{\\epsilon})$ for any constant $\\epsilon\\in(0,1)$ due to Theorem 2. In contrast, Theorem 40 requires that a similar-depth GNNs have width $m=\\tilde{\\Omega}(n)$ , and a GNN of comparable width requires depth $L=\\tilde{\\Omega}(n^{1-\\epsilon})$ . ", "page_idx": 39}, {"type": "text", "text": "On the other hand, search tasks, such as shortest path and diamter, are only guaranteed to be solvable by transformers of depth $O(\\log n)$ and width $\\dot{O}(n^{1+\\epsilon})$ (for graphs with $|{\\bar{E}}|=n^{2}$ ) by Theorem 4. This statement compares to the GNN negative results of Theorem 40. ", "page_idx": 39}, {"type": "text", "text": "D.2 Limitations of GNNs via the Weisfeiler-Leman test ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "A relationship between GNNs and the Weisfeiler-Leman heuristic graph isomorphism test [83] (WL test) further establishes representational limitations of message-passing GNNs. This connection and the rich literature surrounding it is presented in greater detail by [59]. ", "page_idx": 39}, {"type": "text", "text": "The 1-WL test is a permutation-invariant test for predicting whether two candidate graphs are isomorphic that works by first labeling each node with the empty set $\\varnothing$ and then repeatedly replacing each label with a multiset of its neighbors\u2019 labels. A hierarchy of WL test variants exists where the $k$ -WL test maintains a label for each $k$ -tuple of vertices. The failure models of these heuristic solutions are well-understood; critically, the 1-WL test cannot distinguish between connected and disconnected graphs. ", "page_idx": 39}, {"type": "text", "text": "The abilities of message-passing GNNs without unique node identifiers to determine whether two graphs are isomorphic are limited by the graphs distinguishable by the 1-WL test [84, 59]. As an immediate consequence, such GNNs cannot solve graph connectivity at all, unlike transformers, which can do so with a logarithmic-depth parameter-efficient representation. The relationship is bidirectional; message-passing GNNs (and transformers as well) admit an efficient approximation of the 1-WL test. ", "page_idx": 39}, {"type": "text", "text": "The effectiveness of these bounds is tempered by the assumption that no node includes identifying features, which is easily overcome by standard GNN models. The analogy is further limited by the fact that graph embeddings of transformers must have some (possibly arbitrary) node identifier as input in order to tokenize a graph using the node/edge encoding without losing the ability to associate each node with its incident edges. However, the juxtaposition of the 1-WL and CONGEST-based limitations on the abilities of GNNs to solve connectivity-based tasks suggests a fundamental gap in capabilities between models that is apparent in multiple theoretical lenses. ", "page_idx": 39}, {"type": "text", "text": "E Experimental Details ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "E.1 Datasets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "We evaluate our model on the diverse graph reasoning tasks presented in GraphQA [24]. We used the public code of the dataset available at https://github.com/google-research/ google-research/tree/master/graphqa. The code to generate the datasets is licensed under the Apache License, Version 2.0. The tasks in the dataset range in difficulty and encompass the following categories: ", "page_idx": 39}, {"type": "text", "text": "\u2022 Graph-level: node counting (counting the number of nodes in a graph), edge counting (counting the number of edges in a graph), cycle check (determining whether a graph contains a cycle), and triangle counting (counting the number of triangles in a graph). \u2022 Node-level: node degree (calculating the degree of a given node in a graph). ", "page_idx": 39}, {"type": "text", "text": "\u2022 Edge-level: connectivity (finding if there is a path from one node to another), edge existence (whether a given edge exists in a graph, and shortest path (finding the length of the shortest path from one node to another). ", "page_idx": 40}, {"type": "text", "text": "The graphs used in the experiments in this paper and the corresponding graph reasoning tasks are taken from [24]. There are 1, 000 graphs in the original train set, 500 graphs in the dev set, and 500 graphs in the test set. The graphs are generated randomly using Erd\u02ddos-R\u00e9nyi (ER) random graph model [23]. Graph size ranges from 5 to 20 nodes. ", "page_idx": 40}, {"type": "text", "text": "Train set statistics. Average number of nodes: 11.90; average number of edges: 37.01; average node degree: 5.43. ", "page_idx": 40}, {"type": "text", "text": "Test set statistics. Average number of nodes: 12.37; average number of edges: 39.79; average node degree: 5.70. ", "page_idx": 40}, {"type": "text", "text": "While random instances of graph reasoning tasks provide a valuable assessment of the task complexity on realistic graphs, they do not necessarily reflect the \u201cworst case\u201d graph inputs that convey negative results like Theorem 6 and Theorem 3. For example, the reduction that establishes that cycle check is \u201cas hard as\u201d graph connectivity and the consequential logarithmicdepth hardness results hinge on the consideration of graph instances with $n$ nodes and polynomial cycle length. However, as witnessed by Figure 7, the shortest cycles observed in 1000 instances of GraphQA cycle check is almost always of length three, and only $3.2\\%$ of instances are larger. As a consequence, identifying the existence of a cycle on the GraphQA dataset is inherently local, which is reflected by a strong performance by heuristic-based GNN solutions ", "page_idx": 40}, {"type": "image", "img_path": "AfzbDw6DSp/tmp/f83197db660e63471d135e33419e14d16c32b56e1835b9ba564820013c05da0d.jpg", "img_caption": ["Figure 7: Histogram of minimum cycle lengths for cycle check instances. "], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "(Table 2)\u2014despite the fact that efficient GNNs for worst-case cycle check do not exist (Theorem 40). ", "page_idx": 40}, {"type": "text", "text": "For our experiments on the effect of the scale of the number of training data points in the final results we obtain, we use the open-source code of GraphQA available to generate a larger training dataset of 100K examples. We follow the original instructions and parameters to create this larger training dataset. ", "page_idx": 40}, {"type": "text", "text": "E.2 Implementation Details ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Model Hyperparameters. We fixed the number of iterations as 1,000,000 and train standard decoderonly transformers with $L=12$ layers, $m=768$ embedding dimension, $H=12$ heads, learning rate $5\\cdot\\dot{1}0^{-4}$ , and dropout 0.1. These models have an approximate parameter count of 60,000,000. ", "page_idx": 40}, {"type": "text", "text": "We used random search [9] over the following set of hyperparameters to select a universal architecture for all tasks: The range provided for the learning rate and dropout rate are $[10^{-4},10^{-1}]$ and [0, 0.5]. The number of layers $L$ and embedding dimension $m$ is selected from $L\\in\\{4,6,8,10,12,14,16\\}$ and $m\\in\\{192,384,576,768,960,1152,1344,1536\\}$ . We employed the GLU [73] activation as a non-linearity. ", "page_idx": 40}, {"type": "text", "text": "Model Selection. We implemented our model in JAX [26] and used AdamW [43, 50] as the optimizer. Optimal hyperparameters for each task and model were determined by training on the GraphQAT rain dataset and evaluating performance on the $\\mathrm{GraphQA}_{D e v}$ dataset. The results presented in the paper are based on the held-out $\\mathrm{GraphQA}_{T e s t}$ dataset. ", "page_idx": 40}, {"type": "text", "text": "Hardware Acceleration. All experiments were conducted using Google\u2019s TPUv3 and TPUv5e accelerators [35]. ", "page_idx": 40}, {"type": "text", "text": "E.3 Baseline Results ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "To rigorously evaluate the performance of transformers on graph reasoning tasks, we compare them against three established categories of baselines: ", "page_idx": 41}, {"type": "text", "text": "1. Prompting-based methods. These methods provide the LLM with a textual descriptions of the graph and question within the prompt. We consider the following variations and copy the results from the original papers: ", "page_idx": 41}, {"type": "text", "text": "\u2022 ZERO-SHOT. In this approach, the model is given a task description and immediately asked to produce the desired output. No additional examples or demonstrations are provided.   \n\u2022 FEW-SHOT. This approach provides the model with a few examples of the task and their desired outputs [12]. Unlike traditional training, these examples are included directly in the prompt, allowing the model to learn and adapt during the inference.   \n\u2022 COT. Chain-of-thought (CoT) prompting [82] provides examples each showing step-by-step reasoning, teaching the LLM to generate its own thought processes for tackling new tasks.   \n\u2022 ZERO-COT. Zero-shot CoT [45] builds upon Chain-of-Thought (CoT) prompting by eliminating the need for training examples. The LLM generates its own step-by-step reasoning process using a simple trigger phrase like \u201cLet\u2019s think step by step\u201d.   \n\u2022 COT-BAG. BAG prompting [80] extends COT to improve the performance of LLMs on graphrelated tasks by appending \u201cLet\u2019s construct a graph with the nodes and edges first\u201d to the prompt. ", "page_idx": 41}, {"type": "text", "text": "2. Graph-based methods. These models are specifically designed to process graphs as input and are trained task-specific. They leverage the connections between nodes to learn patterns and make predictions, making them ideal for tasks where a graph is involved. We use GCN [44], MPNN [28], and GIN [84] from this category. GraphToken [63] is a GNN-based model that processes the graph and feed the output of the GNN as soft-tokens to an LLM. We consider both GNN models with and without identity tokenizations, as the former is known to be theoretically limited by WL tests, while the latter poses a \u201cfairer\u201d comparison with the positional encodings of transformers. ", "page_idx": 41}, {"type": "text", "text": "3. Transformer models (Ours). The last class of model are task-specific vanilla transformer models [77]. The 60M transformer- $I K$ model is the one described above trained on 1, 000 training examples from the GraphQA training set. To investigate the impact of training data scale, we generated a larger dataset containing 100, 000 examples, ensuring the same distribution as the original training set by using the official GraphQA code and trained 60M transformer-100K on that. The 11B transformer $(F T)$ -1K is a vanilla transformer model that is started with a pre-trained checkpoint of T5 [66] and is fine-tuned on the 1K training dataset. We also include two fine-tuned PaLM 2 [5] transformers of size XXS and XS. Similar to prompting baselines, this model receives a textual description of the graph as input to leverage its textual reasoning capabilities. ", "page_idx": 41}, {"type": "text", "text": "The results for ZERO-SHOT, ZERO-COT, FEW-SHOT, COT, and COT-BAG are taken from Fatemi et al.   \n[24]. Results for SOFT-PROMPT and GraphToken are sourced from Perozzi et al. [63]. ", "page_idx": 41}, {"type": "text", "text": "We independently evaluated GCN, MPNN, and GIN models on these tasks. We used the original architectures proposed in their respective papers and performed hyperparameter tuning on the GraphQA $.D e v$ dataset. ", "page_idx": 41}, {"type": "text", "text": "E.4 Further Experimental results ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Table 4 presents a comprehensive comparison of the graph reasoning capabilities across various baseline models and our proposed transformer architectures. The results highlight several key findings, which we summarize below: ", "page_idx": 41}, {"type": "text", "text": "Transformers Exhibit Strong Performance on Graph-based Reasoning Problems. While transformers are not explicitly designed for graph reasoning tasks like graph-based models, they demonstrate surprisingly strong performance in this domain. The results of this study indicate that transformers, despite their versatility as a general architecture, can often match or even surpass specialized graph models on a variety of graph reasoning benchmarks. ", "page_idx": 41}, {"type": "text", "text": "Transformers Excel at Retrieval Tasks. As proved in Theorem 36, retrieval tasks can be solved by transformers. The obtained results confirm that such tasks are relatively easy for transformers as they obtained the full accuracy on most of such tasks. One exception here is the node degree task that GNNs outperform transformers but still transformers perform relatively well. We discuss why GNNs outperform transformers well for this task. ", "page_idx": 41}, {"type": "table", "img_path": "AfzbDw6DSp/tmp/ffd9fb0d24891f42a4b2d035179e0b0b4678e546b0e8379f09fe3260beb5e012.jpg", "table_caption": [], "table_footnote": ["Table 4: Comparison of various methods in different categories on graph reasoning tasks of GraphQA. Here, we categorize the tasks using the taxonomy proposed in Section 3. "], "page_idx": 42}, {"type": "image", "img_path": "AfzbDw6DSp/tmp/c9c504cd8e2e1c435621a7e672739727fb0f9bdb206be37d4fedea26abbb0b11.jpg", "img_caption": ["Figure 8: Comparison of train and test scaling on all tasks. "], "img_footnote": [], "page_idx": 42}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "Larger Transformers Excel at Solving Search Tasks. As discussed in Theorem 4, transformers are effective for search tasks, albeit requiring a larger number of parameters compared to retrieval tasks. This is empirically evident in the comparison between Transformer-1K and Transformer1K (pretrained). It\u2019s worth noting that the pretrained transformer used here has 11 billion parameters, a significant increase from the 1 million parameters in Transformer-1K. ", "page_idx": 42}, {"type": "text", "text": "Transformers Excel at Capturing Global Patterns. An interesting observation here is the performance gap between Transformers and GNNs across tasks with varying emphasis on local versus global graph structure. Notably: ", "page_idx": 42}, {"type": "table", "img_path": "AfzbDw6DSp/tmp/d99e3ad652059d802f7aa7a984284b3d621c63684ace22288bfa97f1f0b9de03.jpg", "table_caption": [], "table_footnote": ["Table 5: Mean accuracy (and standard deviation) of trained 60M-parameter transformers over five random seeds. "], "page_idx": 43}, {"type": "text", "text": "1. Local Structure: The node degree task, which relies heavily on local node information, is best handled by MPNN, a GNN-based model. ", "page_idx": 43}, {"type": "text", "text": "2. Global Structure: In contrast, tasks like connectivity, triangle counting and shortest path, which require understanding global graph patterns, are dominated by Transformer models. Notably, vanilla Transformers achieve a remarkable $45\\%$ relative improvement over even much larger LLMs augmented with GNN-generated soft prompts (GraphToken) on the shortest path task. This showcases the exceptional ability of Transformers to capture long-range dependencies, a critical factor in understanding global graph structure. ", "page_idx": 43}, {"type": "text", "text": "E.4.1 Sample complexity ablations ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "To develop a more comprehensive understanding of graph reasoning tasks learnability by small transformers, we train a variety of transformers for 1,000,000 steps on each task on a range of sample sizes. In Figure 8, we demonstrate how the model performance improves as a function of sample complexity. By doing so, we witness the relative hardness of each task in terms of the marginal benefit of new samples. ", "page_idx": 43}, {"type": "text", "text": "\u2022 Edge count and node count are \u201ceasy\u201d retrieval tasks that can be solved perfectly with as few as 100 samples.   \n\u2022 Edge existence attains near-perfect train and test classification accuracy as the number of training samples approaches 100,000.   \n\u2022 Connectivity, shortest path, and node degree demonstrate a sharp improvement in evaluation error as a function of the sample size. These models perfectly fit the training set in most sample size regimes, but yield a closer correspondence between training and testing error when trained on 100,000 samples.   \n\u2022 Cycle check and triangle count have persistent gaps between training and testing error and overfit even in the large sample setting. ", "page_idx": 43}, {"type": "text", "text": "E.4.2 Experimental stability ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "While we lack computational resources to train all models multiple times to obtain error bars, we repeated the full experiments on 60M-parameter transformers with 1000 and 100,000 training samples on each task with five different random seeds in order to estimate the standard deviation of the resulting task accuracy and displayed those results in Table 5. With the exception of the node degree experiments trained on 100,000 samples (which included a single far-outlier), all tasks and sample regimes had no more than two percentage points of standard deviation. Critically, this implies the robustness of the superiority of trained transformers over GNNs and prompting methods on \u201cintrinsically global\u201d tasks, such as graph connectivity. ", "page_idx": 43}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: The empirical and theoretical claims discussed accurately represent the detailed theoretical analysis and the wide-ranging experimentation on graph reasoning benchmarks. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 44}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Justification: Theoretical limitations are discussed in the presentation of the results in Section 3 and detailed more comprehensively in Appendix A. We discuss limitations of experimental results in Section 4. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 44}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Justification: The theoretical results follow all of the criteria. The proofs are in Appendix. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 45}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We comprehensively detail the models used and the training hyper-parameters in the appendix. The training dataset is generated by a publicly available source code. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 45}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 46}, {"type": "text", "text": "Answer: [No] ", "page_idx": 46}, {"type": "text", "text": "Justification: While code is not made available, the data can be generated using the GraphQA dataset linked in the appendix. Full training details are made available, which makes the code easily reproducible. The code will be open sourced upon acceptance of the paper. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 46}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: All training and evaluation details and model hyper-parameters are included in the paper appendix. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 46}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: The main claims of the paper are supported by error bars and analysis present in Table 5. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 46}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 47}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: The type of the accelerator used in the experiments is in Appendix ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 47}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: We reviewed the guidelines and found no relevant violations to our work.   \nSince the experimental datasets are synthetically generated, privacy concerns are moot. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 47}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: The paper is focused on reasoning capabilities of transformers using theoretical and empirical evaluation. To the best of our knowledge, there is no societal impact for the work. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 48}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: This is because the datasets used are synthetically generated and reflect random graphs, no risks of data leakage are present. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 48}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: The papers are cited, the link to the source codes used is states along with their license. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 48}, {"type": "text", "text": "", "page_idx": 49}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: No new assets is introduced in this paper. We have used public data and no code has been released yet. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 49}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: No human subjects were involved. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 49}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: No human subjects were involved. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 49}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 50}]