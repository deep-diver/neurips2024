{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-03", "reason": "This paper introduced the transformer architecture, the foundation of the models studied in this paper."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2018-10-11", "reason": "This paper introduced BERT, a significant advance in language model pre-training that is highly relevant to the study of transformer capabilities."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper demonstrated the few-shot learning capabilities of large language models, which is relevant to this paper's investigation of how transformers solve algorithmic tasks."}, {"fullname_first_author": "Jordan Hoffmann", "paper_title": "Training compute-optimal large language models", "publication_date": "2022-03-17", "reason": "This paper explores the scaling laws of large language models, which provides a valuable context for understanding how transformer capabilities change with increasing scale."}, {"fullname_first_author": "Clayton Sanford", "paper_title": "Transformers, parallel computation, and logarithmic depth", "publication_date": "2024-01-01", "reason": "This paper, also authored by several authors of the target paper, provides crucial theoretical foundations for understanding the relationship between transformer depth, width, and algorithmic complexity."}]}