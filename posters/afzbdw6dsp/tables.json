[{"figure_path": "AfzbDw6DSp/tables/tables_7_1.jpg", "caption": "Table 3: Comparison of GraphQA task accuracies of transformers explicitly trained for graph reasoning and LLMs with a variety of prompting strategies.", "description": "This table compares the performance of various models on several graph reasoning tasks from the GraphQA benchmark.  The models include explicitly trained transformers of different sizes (60M parameters and 11B parameters), as well as LLMs using several prompting strategies (ZERO-SHOT, FEW-SHOT, COT, ZERO-COT, and COT-BAG).  The table allows for a direct comparison of the accuracy of different techniques in solving various graph reasoning tasks, highlighting the strengths and limitations of each approach. The results are categorized by task difficulty (retrieval, parallelizable, and search tasks), allowing for a nuanced understanding of model capabilities.", "section": "4 Empirical graph reasoning capabilities"}, {"figure_path": "AfzbDw6DSp/tables/tables_7_2.jpg", "caption": "Table 1: Transformers vs GNNs on shortest path: Fine-tuned large transformers outperform other transformers and GNNs, even the alternatives are trained on much larger training sets.", "description": "The table compares the performance of different transformer models and Graph Neural Networks (GNNs) on the shortest path prediction task.  It shows the accuracy of each model with different numbers of training samples (1K and 100K). The results demonstrate that fine-tuned large language models (LLMs) significantly outperform smaller transformers and GNNs, highlighting the benefits of larger model sizes and fine-tuning for complex graph reasoning tasks.  Even with a smaller number of training samples, the fine-tuned large model shows high accuracy.", "section": "4 Empirical graph reasoning capabilities"}, {"figure_path": "AfzbDw6DSp/tables/tables_8_1.jpg", "caption": "Table 4: Comparison of various methods in different categories on graph reasoning tasks of GraphQA. Here, we categorize the tasks using the taxonomy proposed in Section 3.", "description": "This table compares the performance of various models on different graph reasoning tasks from the GraphQA benchmark dataset.  The models are categorized into three groups: prompting-based methods (using LLMs with different prompting strategies), graph-based methods (using GNN architectures), and transformer models (trained from scratch and fine-tuned). The tasks are further categorized based on a novel representational hierarchy proposed in the paper (retrieval, parallelizable, search, and subgraph counting). The table shows the accuracy of each model on each task category, providing a comprehensive comparison of their capabilities.", "section": "4 Empirical graph reasoning capabilities"}, {"figure_path": "AfzbDw6DSp/tables/tables_8_2.jpg", "caption": "Table 2: Transformers vs GNNs on cycle check and node degree: GNNs are favorably biased for local structure.", "description": "The table compares the performance of transformers and graph neural networks (GNNs) on two graph reasoning tasks: node degree and cycle check.  The results show that GNNs, particularly MPNN and GIN, significantly outperform the 60M parameter transformer, especially in the low-sample regime (1K training examples).  This suggests that GNNs have a favorable inductive bias for tasks that are intrinsically local, such as node degree and cycle check, enabling them to learn effectively from a small number of samples.  The larger 11B parameter fine-tuned transformer shows substantially improved accuracy, but still doesn't match the GNN performance on node degree with only 1K training examples.", "section": "4.2 GNNs uncover local graph structure with few samples"}, {"figure_path": "AfzbDw6DSp/tables/tables_42_1.jpg", "caption": "Table 4: Comparison of various methods in different categories on graph reasoning tasks of GraphQA. Here, we categorize the tasks using the taxonomy proposed in Section 3.", "description": "This table compares the performance of various models on different graph reasoning tasks from the GraphQA benchmark.  The models are categorized into three groups: prompting-based methods (using LLMs with various prompting techniques), graph-based methods (GNNs such as GCN, MPNN, and GIN), and transformer models (the authors' models).  The table shows the accuracy of each method on each task, categorized by the task difficulty (retrieval, parallelizable, search) as defined in Section 3 of the paper.  The results demonstrate the strengths and weaknesses of different models on different types of graph reasoning tasks.", "section": "4 Empirical graph reasoning capabilities"}, {"figure_path": "AfzbDw6DSp/tables/tables_43_1.jpg", "caption": "Table 5: Mean accuracy (and standard deviation) of trained 60M-parameter transformers over five random seeds.", "description": "This table presents the mean accuracy and standard deviation results for trained 60M parameter transformers across five different random seeds. The results are categorized by task type (retrieval, parallelizable, search, and subgraph counting) and training data size (1K or 100K samples).  It demonstrates the model's performance consistency and the impact of training data size on accuracy across different graph reasoning task complexities.", "section": "4 Empirical graph reasoning capabilities"}]