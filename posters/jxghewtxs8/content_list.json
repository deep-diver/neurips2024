[{"type": "text", "text": "High-Resolution Image Harmonization with Adaptive-Interval Color Transformation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Quanling Meng1, Qinglin Liu1, Zonglin $\\mathbf{Li}^{1}$ , Xiangyuan Lan2, Shengping Zhang1,\u2217, Liqiang Nie1 ", "page_idx": 0}, {"type": "text", "text": "1School of Computer Science and Technology, Harbin Institute of Technology, China 2Peng Cheng Laboratory, China quanling.meng@hit.edu.cn, qinglin.liu@outlook.com, zonglin.li@hit.edu.cn lanxy@pcl.ac.cn, s.zhang@hit.edu.cn, nieliqiang@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Existing high-resolution image harmonization methods typically rely on global color adjustments or the upsampling of parameter maps. However, these methods ignore local variations, leading to inharmonious appearances. To address this problem, we propose an Adaptive-Interval Color Transformation method (AICT), which predicts pixel-wise color transformations and adaptively adjusts the sampling interval to model local non-linearities of the color transformation at high resolution. Specifically, a parameter network is first designed to generate multiple position-dependent 3-dimensional lookup tables (3D LUTs), which use the color and position of each pixel to perform pixel-wise color transformations. Then, to enhance local variations adaptively, we separate a color transform into a cascade of sub-transformations using two 3D LUTs to achieve the non-uniform sampling intervals of the color transform. Finally, a global consistent weight learning method is proposed to predict an image-level weight for each color transform, utilizing global information to enhance the overall harmony. Extensive experiments demonstrate that our AICT achieves state-of-the-art performance with a lightweight architecture. The code is available at https://github.com/aipixel/AICT. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Image composition [3, 2, 24] aims to combine a foreground object with a background image to create a realistic composite, which holds significant potential across various domains, including art, entertainment, commerce [3, 43, 51], and data augmentation [11, 30, 26]. However, since the foreground and background may be captured under different conditions, directly pasting the foreground onto the background usually results in an inconsistent appearance. To address this problem, image harmonization endeavors to adjust the color of the foreground to seamlessly integrate with the background, which plays a pivotal role in image editing. ", "page_idx": 0}, {"type": "image", "img_path": "jXgHEwtXs8/tmp/48ee09fb13c672a9a6274e033ace221fb65ce230a5139ff79a27e76fee7ab5b7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Figure 1: Our method predicts pixel-wise color transformations at low resolution and adaptively the adjusts sampling interval to model local nonlinearities of the color transform at high resolution. ", "page_idx": 0}, {"type": "text", "text": "Traditional image harmonization methods [5, 19, 27, 28, 29, 36, 37] primarily concentrate on aligning the color statistics of the foreground to match the background using hand-crafted features. Since these methods lack consideration for the content of the composite images, they often yield suboptimal results when there are substantial differences in appearance between the foreground and background. With the rapid advance of deep learning, learning-based methods [38, 17, 9, 8, 22, 15, 33, 6, 44, 13] have become dominant and achieved remarkable progress. These methods usually adopt encoderdecoder based structures to learn the dense pixel-to-pixel transformation between composite images and ground-truth images at a low resolution (e.g., $256\\times256$ pixels), while real-world applications increasingly demand high-resolution images. Although these methods can process images with any size theoretically, the computational cost required for high-resolution images is extremely expensive. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Recently, several methods [20, 21, 46, 12] have emerged to tackle the challenge of high-resolution image harmonization. To reduce computational costs, these methods usually use a low-resolution composite image to predict transformation parameters for processing the corresponding high-resolution composite image instead of directly generating the final image. These methods can be mainly categorized into two groups. Harmonizer [20] and $\\bar{\\mathrm{S^{2}C R N e t}}$ [21] focus on predicting image-level parameters to perform global color adjustments. However, these adjustments do not contain any semantic and local information, which leads to identical changes for pixels in different regions with the same color value. On the other hand, DCCF [46] and PCT-Net [12] predict low-resolution parameter maps and then directly upsample them to align with high-resolution composite images for pixel-wise color transformation. However, upsampling low-resolution parameter maps may introduce errors, which fails to model local non-linearities of the color transform at high resolution. In summary, these four methods ignore local color transformations across the foreground regions, which are prone to generate inharmony results in local regions. ", "page_idx": 1}, {"type": "text", "text": "To address this problem, we propose an Adaptive-Interval Color Transformation method (AICT) for high-resolution image harmonization. As shown in Figure 1, AICT predicts parameter maps for pixel-wise color transformations, rather than performing global color adjustments. Additionally, it adaptively adjusts the sampling interval to model local non-linearities of the color transformation at high resolution. To implement this complex transformation, we formulate the task as an image-based multiple curve estimation problem. Specifically, a parameter network is first proposed to generate multiple curves as position-dependent 3- dimensional lookup tables (3D LUTs), which use the color and position of each pixel to perform pixel-wise color transformations. Then, to enhance local variations adaptively, we propose an adaptive interval learning method, which separates a color transform into a cascade of subtransformations using two position-dependent 3D LUTs to achieve the non-uniform sampling ", "page_idx": 1}, {"type": "image", "img_path": "jXgHEwtXs8/tmp/4536f0ffc4bcac8d3392ce8208c5e89c94c6abdb75dc7d610bfb0d78611b733c.jpg", "img_caption": ["Figure 2: Model size vs. performance (fMSE score) comparison on the full-resolution images of the iHarmony4 dataset [8]. The proposed method achieves state-of-the-art performance while maintaining a lightweight architecture. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "intervals of the color transform. Finally, a global consistent weight learning method is proposed to predict an image-level weight for each color transform, utilizing global information to enhance the overall harmony by modeling the influence of background brightness on the foreground. As shown in Figure 2, AICT achieves state-of-the-art performance in foreground-normalized MSE (fMSE) on the full-resolution images of the iHarmony4 dataset [8] while maintaining a model size comparable to PCT-Net [12]. Our main contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose an Adaptive-Interval Color Transformation method to perform pixel-wise color transformations and model local non-linearities of the color transformation for highresolution image harmonization. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose an adaptive interval learning method to achieve a flexible sampling point allocation and a global consistent weight method to utilize global information to enhance the overall harmony. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance in high-resolution image harmonization while maintaining a lightweight and simple network architecture. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Image Harmonization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To reduce computational costs, existing high-resolution image harmonization methods [20, 21, 46, 12, 7, 40] usually takes a low-resolution image as input to predict transformation parameters for processing the corresponding high-resolution image instead of directly outputting the final image. For example, Harmonizer [20] employs a neural network to regress filter arguments based on lowresolution images, which are then used for several white-box filters to adjust various aspects of the foreground, including brightness, contrast, and other characteristics. $\\mathrm{S^{2}C R N e t}$ [21] focuses on extracting spatial-separated embeddings from low-resolution images to predict parameters of the piece-wise curve mapping for performing color-wise transformations. Similarly, Wang et al. [40] utilize down-sampled images to predict global RGB curves for performing color correction at higher resolutions. Furthermore, they propose to predict and upsample low-resolution shading maps to address local tonal variations. These three methods predict image-level parameters to perform global color adjustments. Unlike these methods, DCCF [46] processes low-resolution images to acquire human comprehensible neural filter maps, which are subsequently upsampled and applied to the original input image. PCT-Net [12] predicts affine transformation parameter maps based on low-resolution images and upsamples them to match high-resolution images. Both methods predict low-resolution parameter maps and then upsample them directly to align with high-resolution images for pixel-wise transformations. In addition, CDTNet [7] performs pixel-to-pixel transformation at low resolution and color-to-color transformation at high resolution in parallel. It subsequently utilizes a refinement module to integrate the two intermediate outputs. The above methods typically rely on global color adjustments or the upsampling of parameter maps, which ignore local variations. Our method performs pixel-wise color transformations and models local non-linearities for high-resolution image harmonization. ", "page_idx": 2}, {"type": "text", "text": "2.2 LUT-based Image Enhancement ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A lookup table (LUT) defines a set of tables consisting of output values, where each value can be addressed by a set of indices. When provided with an input combination, it can output a corresponding value by performing lookup and interpolation operations, so it usually serves as an effective and efficient representation of a univariate or multivariate function. Zeng et al. [49] propose to learn multiple basis 3D LUTs and predict content-dependent weights to fuse them into an image-adaptive LUT for photo enhancement. To consider global scenarios and local spatial information, Wang et al. [41] introduce a lightweight two-head weight predictor for image-level scenario adaptation and pixel-wise category fusion. Yang et al. [47] achieve a more flexible sampling point allocation in 3D LUTs by adaptively learning the non-uniform sampling intervals in the 3D color space. To enhance the expressiveness of the LUT, Yang et al. [48] decompose a single color transform into a cascade of sub-transforms and use 1D LUTs to increase cell utilization within the 3D LUT. Zhang et al. [50] employ hash techniques to reduce the space complexity of 3D LUTs. Liu et al. [23] propose to learn a context map for the pixel-level category and a group of image-adaptive coefficients for achieving context-aware 4D LUT. These methods typically predict several weight parameters to fuse pre-trained LUTs for global RGB-to-RGB transformations, while our method predicts position-dependent 3D LUTs for pixel-wise color transformations. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given a composite image $\\tilde{I}\\,\\in\\,\\mathbb{R}^{H\\times W\\times3}$ and a binary mask $M\\,\\in\\,\\{0,1\\}^{H\\times W}$ that indicates the foreground region to be harmonized, image harmonization aims to adjust the color of the foreground region to obtain a harmonized image $\\bar{I}\\in\\mathbb{R}^{H\\times W\\times3}$ close to the ground truth image $I\\in\\mathbb{R}^{H\\times W\\times3}$ . To achieve high-resolution image harmonization at a low computational cost, we formulate the task as an image-based multiple curve estimation problem and propose an Adaptive-Interval Color Transformation method (AICT) as shown in Figure 3. AICT consists of a high resolution (HR) ", "page_idx": 2}, {"type": "image", "img_path": "jXgHEwtXs8/tmp/d3a8f0912f80a0b2ce9c337c9209d189c799d9fccfe446511b555d38f8865bd4.jpg", "img_caption": ["Figure 3: The framework of the proposed method. AICT consists of a high resolution (HR) branch and a low resolution (LR) branch. In the LR branch, the composite image I\u02dc and composite mask $M$ are downsampled to predict two parameter maps $C$ and $F$ . In the HR branch, $C$ is used to redistribute the color values of $\\tilde{I}$ into specific ranges of the following $F$ , which achieves adaptively adjusting the sampling interval of the color transformation. Finally, the new color values and pixel coordinates are mapped to final color values using $F$ . "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "branch and a low resolution (LR) branch. In the LR branch, $\\tilde{I}$ and $M$ are first downsampled to obtain a low-resolution composite image I\u02dclow \u2208RH\u2032\u00d7W \u2032\u00d73 and foreground mask Mlow \u2208{0, 1}H\u2032\u00d7W \u2032, where $H^{\\prime}<H$ and $W^{\\prime}<W$ . They are then fed into a parameter network to predict two parameter maps $C\\in\\mathbb{R}^{H^{\\prime}\\times W^{\\prime}\\times Q}$ and $F\\in\\mathbb{R}^{H^{\\prime}\\times W^{\\prime}\\times Q}$ and they are regarded as curves in the form of positiondependent 3D LUTs, where $Q$ is equal to three times the number of knot points in a curve. In the HR branch, $C$ is used to process color values and pixel coordinates to redistribute the color values of $\\tilde{I}$ into specific ranges of the following $F$ for achieving pixel-wise adaptive adjustment of the sampling interval. $F$ is used to map the redistributed color values and pixel coordinates to final color values, achieving pixel-wise color transformation. Therefore, the color transformation in our method includes two cascaded pixel-wise sub-transformations. ", "page_idx": 3}, {"type": "text", "text": "3.1 Adaptive Interval Learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the LR branch, $\\tilde{I}_{\\mathrm{low}}$ and $M_{\\mathrm{low}}$ are fed into the parameter network to produce $C$ and $F$ . As shown in Figure 3, we use the Transformer encoder [13] to construct the parameter network, which consists of multiple attention layers. Firstly, the low-resolution composite image $\\tilde{I}_{\\mathrm{low}}$ and foreground mask $M_{\\mathrm{low}}$ are divided into patches, which are then projected into the embedding space. Positional encodings are added to the embedded patches, which are processed by the Transformer encoder. The output of the Transformer encoder is reassembled to obtain the feature $T_{1}\\in\\mathbb{R}^{64\\times64\\times256}$ , and a deconvolution layer is applied to reduce the feature channel number and perform upsampling to obtain the feature $\\dot{T_{2}}\\in\\mathbb{R}^{H^{\\prime}\\times W^{\\prime}\\times12}$ . Subsequently, $T_{2}$ is processed through a $1\\times1$ convolution to obtain the parameter map $C\\in\\mathbb{R}^{H^{\\prime}\\times W^{\\prime}\\times Q}$ . We individually process each color channel and divide $C$ into three parameter maps $C^{\\mathrm{R}}\\in\\mathbb{R}^{H^{\\prime}\\times W^{\\prime}\\times K}$ , $C^{\\mathrm{G}}\\in\\mathbb{R}^{H^{\\prime}\\times\\Bar{W}^{\\prime}\\times K}$ , and $C^{\\mathrm{B}}\\in\\mathbb{R}^{H^{\\prime}\\times W^{\\prime}\\times K}$ , where $Q$ is equal to $3\\times K$ Taking the red channel of an RGB image as an example, the corresponding parameter map is $C^{\\tt R}$ . To achieve adaptive interval learning, we need to reconstruct the parameter map $C^{\\tt R}$ . The softmax function is used to obtain the normalized interval $V^{\\mathbb{R}}\\in[0,1]^{H^{\\prime}\\times W^{\\prime}\\times M}=\\mathsf{S o f t m a x}(C^{\\mathbb{R}},\\mathsf{a x i s}=\\mathsf{3}$ ), where axis $=3$ indicates that normalization is performed along the third dimension [47] of $C^{\\tt R}$ . We perform cumulative summation in the third dimension of $V^{\\bar{\\mathbf{R}}}$ and then add an origin for each position to obtain the sampling coordinates $K^{\\mathrm{R}}\\in[0,1]^{H^{\\prime}\\times W^{\\prime}\\times(M+1)}$ , which can be expressed as $\\bar{K}^{\\mathrm{R}}=[Z;\\mathrm{Cumsum}(V^{\\mathrm{R}},\\mathrm{a}\\bar{\\mathrm{x}}\\mathrm{is}=3)]$ ], where $Z$ is a $H^{\\prime}\\times W^{\\prime}\\times1$ matrix filled with zero values, and ttho e1 $[\\cdot;\\cdot]$ d  demnaoitnetsa itnhse  tchoen cmaotennoattoionne  ionpcerreaatisoinn.g  Ipnr soupcehr tia esw aaly,o enagc thh ev atlhuier di nd $K^{\\tt R}$ inss iwoint $(K_{c,b,i}^{\\tt R}<\\check{K}_{c,b,j}^{\\tt R}$ for $c,b\\in\\mathbb{I}_{0}^{255}$ , $i,j\\in\\mathbb{I}_{0}^{M}$ , and $i<j$ ). ", "page_idx": 3}, {"type": "text", "text": "We use position-dependent 3D LUTs to achieve pixel-wise adaptive adjustment of the sampling interval. The position-dependent 3D LUT maps spatial coordinates and color values to new color values [35]. We treat $K^{\\tt R}$ as a position-dependent 3D LUT, which utilizes lookup and interpolation operations to serve as a color mapping curve. For a given coordinate $(i,j)$ and its color value $x(i,j)$ in the red channel of I\u02dc, we can find the corresponding position (u, v, w) in KR, where u = i HH\u2032\u2212\u221211 , $v=j\\frac{W^{\\prime}-1}{W-1}$ , $\\begin{array}{r}{w=x(i,j)\\frac{K-1}{C_{\\mathrm{max}}}}\\end{array}$ and $C_{\\mathrm{max}}$ represents the maximum color value in the image. Based on $(u,v,w)$ , we can obtain adjacent 8 sampling points in and perform trilinear interpolation to produce a new color value $\\hat{x}(i,j)$ . The entire process can be formulated as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{x}(i,j)=t(l(i,j,x(i,j),K^{\\mathrm{R}}))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $t$ and $l$ denote trilinear interpolation and lookup operations, respectively. The upper section of Figure 4 demonstrates the color redistribution process. Here, each coordinate and its corresponding color value are mapped to a new color value using $K^{\\tt R}$ . Similarly, we can obtain redistributed color values for the other color channels in the same way. ", "page_idx": 4}, {"type": "text", "text": "3.2 Pixel-Wise Color Transform ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "After obtaining the redistributed color values, we utilize the parameter map $F$ to achieve pixel-wise color transformation. As shown in Figure 3, we apply a $1\\times1$ convolution to the feature $T_{2}$ for obtaining the intermediate parameter map R \u2208RH\u00d7W \u00d7Q, which are divided into three parameter maps $R^{\\mathrm{R}}\\,\\in\\,\\mathbb{R}^{H^{\\prime}\\times W^{\\prime}\\times K}$ , $R^{\\mathrm{G}}\\in$ $\\mathbb{R}^{H^{\\prime}\\times\\Bar{W^{\\prime}}\\times K}$ , and $R^{\\mathrm{B}}\\,\\in\\,\\mathbb{R}^{H^{\\prime}\\times W^{\\prime}\\times K}$ according to the color channel. When inserting the foreground into a new background, the average brightness of the foreground will be influenced by the background. For example, the inserted foreground may experience overall darkening, brightening, or a bias toward a certain color. As shown in Figure 5, the foreground region un", "page_idx": 4}, {"type": "image", "img_path": "jXgHEwtXs8/tmp/56abd0ce42d61462f3783e069fcb40498e9c1a9dfd5942cc950140f29eada0c9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 4: The illustration of the color redistribution and result prediction. The coordinates and color values are first mapped to new color values using $K^{\\tt R}$ . Then, the final color values are obtained by using F R according to the coordinates and redistributed color values. ", "page_idx": 4}, {"type": "text", "text": "dergoes darkening after image harmonization. Therefore, we propose a globally consistent weight learning method and design a weight learning module to learn image-level parameters for controlling overall color transformations. These parameters can also be regarded as scene classification, which are adjusted based on different scenes. The weight learning module consists of two $3\\!\\times\\!3$ convolutional layers, Batch Normalization (BN) layers [18], max-pooling layers, ReLU activation functions, and one fully-connected layer, making it lightweight. It processes $T_{1}$ to predict weight vectors $\\mathbf{w}^{\\mathrm{R}}\\in\\mathbb{R}^{K}$ , $\\mathbf{w}^{\\mathrm{G}}\\in\\dot{\\mathbb{R}}^{K}$ , and $\\mathbf{w^{B}}\\in\\mathbb{R}^{K}$ , which are used to multiply with $R^{\\mathrm{R}}$ , $R^{\\mathrm{G}}$ , and $\\breve{R}^{{\\tt B}}$ to obtain the final parameter maps $F^{\\mathrm{R}}\\in\\mathbb{R}^{H^{\\prime}\\times W^{\\prime}\\times K}$ , $F^{\\mathrm{G}}\\in\\mathbb{R}^{H^{\\prime}\\times W^{\\prime}\\times\\bar{K}}$ , and $F^{\\mathrm{B}}\\in\\mathbb{R}^{H^{\\prime}\\times W^{\\prime}\\times K}$ , respectively. ", "page_idx": 4}, {"type": "text", "text": "We also treat $F^{\\tt R}$ as a positiondependent 3D LUT. For a given coordinate $(i,j)$ and its redistributed color value $\\hat{x}(i,j)$ in the red channel of $\\tilde{I}$ , lookup and interpolation operators are performed in $F^{\\tilde{\\mathrm{R}}}$ to produce the final output color value $\\hat{y}(i,j)$ . The bottom part of Figure 4 illustrates the result prediction process. Due to the difference in resolution, each pixel in the parameter maps $K^{\\tt R}$ and $\\bar{F^{\\mathrm{R}}}$ corresponds to a local region of the composite image. However, the color values in a ", "page_idx": 4}, {"type": "image", "img_path": "jXgHEwtXs8/tmp/be074f42d0b838d76b8a00e8b5c5c2f0f4541d906f1bcf95105a18ac1b3b2ff8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 5: The illustration of the overall impact of the background on the foreground. The foreground is outlined in red. The pixel histogram statistics indicate that the overall foreground region undergoes darkening after the process of image harmonization. ", "page_idx": 4}, {"type": "text", "text": "region are usually distributed within certain ranges. Therefore, $K^{\\tt R}$ aims to redistribute these color values across the entire color value range, thereby improving the utilization of sampling points in $F^{\\tt R}$ and enhancing the expressiveness of the curve. By combining these two curves ( $\\overset{\\cdot}{K}^{\\bar{\\mathsf{R}}}$ and $F^{\\tt R}$ ), our method achieves the capability of adaptively adjusting the sampling intervals. Finally, the result images corresponding to each channel are obtained by transforming the color values at each position of $\\tilde{I}$ , which are then concatenated to produce the harmonized image $\\hat{I}$ . As we process each color channel separately for an RGB image, the parameter network needs to predict six curves. ", "page_idx": 5}, {"type": "text", "text": "3.3 Loss Functions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "During the training phase, the loss $\\mathcal{L}_{\\mathrm{high}}$ is calculated in the HR branch based on the difference between $\\hat{I}$ and $I$ . To improve the performance of AICT for images with small foregrounds, we adopt the foreground-normalized MSE loss [33], which is formulated as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{high}}=\\frac{\\displaystyle\\sum_{i=1}^{H}\\sum_{j=1}^{W}\\left\\|\\hat{I}_{i,j}-I_{i,j}\\right\\|_{2}^{2}}{\\operatorname*{max}\\{A_{\\mathrm{min}},\\displaystyle\\sum_{i=1}^{H}\\sum_{j=1}^{W}M_{i,j}\\}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $A_{\\mathrm{min}}$ is a hyper-parameter to stabilize training. By replacing $\\tilde{I}$ with $\\tilde{I}_{\\mathrm{low}}$ in the framework, we can also obtain the low-resolution harmonized image $\\hat{I}_{\\mathrm{low}}$ and the loss $\\mathcal{L}_{\\mathrm{low}}$ . We also apply the foreground-normalized MSE loss to minimize the difference between $\\hat{I}_{\\mathrm{low}}$ and the low-resolution version of the ground truth $I_{\\mathrm{low}}$ . Overall, our network can be trained by optimizing the combination of the losses above ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathcal{L}_{\\mathrm{high}}+\\lambda\\mathcal{L}_{\\mathrm{low}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\lambda$ is a hyper-parameter that controls the weight of $\\mathcal{L}_{\\mathrm{low}}$ . Our parameter network is trained in an end-to-end manner. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Setting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1.1 Dataset and Evaluation Metrics ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Following the same settings as previous methods [20, 46, 12], we train and evaluate our method on the iHarmony4 dataset [8], which consists of four subsets (HAdobe5k, HCOCO, HDay2night, and HFlickr) and includes 73146 samples for image harmonization. Each sample contains a composite image, a corresponding foreground mask, and a ground truth image. The HAdobe5k, HCOCO, HDay2night, and HFlickr subsets consist of 2160, 4283, 133, and 828 test images, respectively. These subsets have resolutions ranging from $312\\times230$ to $6048\\times4032$ pixels. The width and height of images in the HCOCO, HDay2night, and HFlickr subsets are all below 1024 pixels, and only the HAdobe5k subset is composed of images with a width or height larger than 1024 pixels. ", "page_idx": 5}, {"type": "text", "text": "We also train and evaluate our method on the ccHarmony dataset [25], which is constructed by transferring each image across different illumination conditions to simulate natural illumination variations. This dataset contains 3080 training samples and 1180 test samples, and each sample includes a composite image, a corresponding foreground mask, and a ground truth image. ", "page_idx": 5}, {"type": "text", "text": "Additionally, we evaluate our method against other methods on a real composite dataset [7], which consists of 100 test samples. Each sample contains only a composite image and a corresponding foreground mask. Note that all datasets used in our experiments are publicly available. ", "page_idx": 5}, {"type": "text", "text": "For the quantitative performance metrics, we calculate several key indicators, including Mean Square Error (MSE), foreground Mean Square Error (fMSE), Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity Index (SSIM) for each individual image in the dataset. Subsequently, we compute the average values for the entire dataset and for each specific subset. While MSE serves as a crucial evaluation metric, it tends to be biased towards images that contain larger foreground regions due to the variations in foreground sizes across the dataset. This limitation makes MSE less reliable for a comprehensive assessment of image quality. In contrast, fMSE offers a more balanced and equitable evaluation of overall quality, making it a more suitable choice for our analysis [12]. ", "page_idx": 5}, {"type": "text", "text": "Table 1: Quantitative comparison on the full-resolution test images of the iHarmony4 dataset. The best results are marked in bold, and the second best are underlined. ", "page_idx": 6}, {"type": "table", "img_path": "jXgHEwtXs8/tmp/d99ed8827566f57e5fbb10d2804a1fab99c6b16a3a724e5d4e88d598035f98c3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.1.2 Implementation Details ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "For the iHarmony4 dataset [8], our network is trained from scratch by using Adam optimizer with $\\beta_{1}\\,=\\,0.9$ , $\\beta_{2}=0.999$ , and $\\epsilon=e^{-8}$ . The batch size is set to 4 and the model is trained for 100 epochs. We set the learning rate as $5e^{-5}$ for the first 50 epochs and linearly decay it to zero over the next 50 epochs. For the network design, we set $K=8$ , $Q=24$ , $H^{\\prime}=25\\bar{6}$ , and $W^{\\prime}=256$ . For the loss function, $\\lambda$ is set to 0.01, and $A_{\\mathrm{min}}$ is set to 1000 and 100 in the HR branch and LR branch, respectively. We resize training images to ensure that the length of their ", "page_idx": 6}, {"type": "table", "img_path": "jXgHEwtXs8/tmp/d42d3490cd3cd7badd46c1b328abcc6d88c1906ee794697a5838b200617d2975.jpg", "table_caption": ["Table 2: Quantitative comparison on the HAdobe5k subset at a $2048\\times2048$ resolution. The best results are marked in bold, and the second best are underlined. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "sides does not exceed 2048 pixels due to memory limitations. During the testing phase, we perform image harmonization using full-resolution images. Following [7], we also conduct tests on the HAdobe5k subset at two different resolutions $\\langle1024\\times1024$ and $2048\\times2048)$ . For the ccHarmony dataset [25], we use the parameters trained on the iHarmony4 dataset as the initial parameters, and then fine-tune our network on the training set of the ccHarmony dataset. Following [25], the test image size in this dataset is set as $256\\times256$ . To augment training samples, we crop the composite image according to a random bounding box, the width and height of which are not smaller than the halved width and height of the composite image, respectively. The random horizontal flip is also applied to training samples. Our network is implemented based on the PyTorch framework and trained over approximately 75 hours on a computer equipped with two EPYC 7513 CPUs, 256GB of memory, and two 3090 GPUs. ", "page_idx": 6}, {"type": "text", "text": "4.2 Comparison with the State-of-the-arts ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We conduct experiments and comparisons on the full-resolution test images of the iHarmony4 dataset [8]. AdaInt [47] and SepLUT [48], originally designed for image enhancement, are modified and trained from scratch on the iHarmony4 dataset for the image harmonization task. Our AICT has three key differences compared to these methods. Firstly, they predict several weight parameters to fuse pre-trained LUTs, while our method dynamically predicts entire LUTs based on input. Secondly, they focus on global RGB-to-RGB transformations, whereas our AICT enables pixel-wise color transformations. Lastly, instead of applying adaptive interval learning and separable lookup tables for global sampling adjustments, our AICT achieves pixel-wise sampling interval adjustment to model local non-linearities in color transformation. To the best of our knowledge, only Harmonizer [20], DCCF [46], and PCT-Net [12] perform image harmonization on full-resolution images. Since Xu et al. [46] do not provide fMSE and SSIM values, we evaluate DCCF by running the provided code for comparison. Guerreiro et al. [12] propose two types of architectures: a CNN-based encoder-decoder network and a network based on a Visual Transformer (ViT) [10]. We choose the ViT-based network for comparison as it demonstrates better performance. The quantitative comparison results are shown in Table 1. We observe that AICT outperforms other methods on most metrics. Due to the limited amount of training samples in the HDay2night subset, our method achieves lower performance on this subset. To evaluate performance on low-resolution images, we also report quantitative results on the iHarmony4 dataset at a $256\\times256$ resolution (see the Appendix). Additionally, we evaluate the harmonization performance on the HAdobe $\\operatorname{5k}$ subset at a $2048\\times2048$ resolution. As shown in Table 2, our method achieves the best performance across all metrics. Furthermore, we compare our method with others on this subset at a $1024\\times1024$ resolution (see the Appendix). ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "As shown in Figure 6, we present the qualitative results of AICT against state-of-the-art methods on the full-resolution test images of the iHarmony4 dataset [8]. Compared to other methods, the images generated by AICT are closer to the ground truth images, making the composite images more realistic. More qualitative results are presented in the Appendix. Additionally, we also present the quantitative comparison results on the real composite dataset [25] to demonstrate the superiority and generalizability of our method (see the Appendix). ", "page_idx": 7}, {"type": "text", "text": "To further demonstrate the effectiveness of our method, we also present quantitative comparison results on the ccHarmony dataset [25] at $256\\times256$ resolution. As shown in Table 3, our method also achieves the best performance across all metrics. ", "page_idx": 7}, {"type": "text", "text": "4.3 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We study the influence of the adaptive interval learning method on image harmonization performance using the iHarmony4 dataset [8], as shown in Table 4. We first remove the weight learning module and the adaptive interval learning method in AICT, denoted as \u201cSingle\u201d. Compared to AICT, this method achieves lower performance. We then incorporate the channel-crossing strategy [34, 35] into \u201cSingle\u201d, denoted as \u201cCross\u201d. The parameter model uses an RGB image to predict parameter maps, which inherently include information from all color channels. Thus, the channel-crossing strategy introduces redundancy without improving performance. We also remove the adap", "page_idx": 7}, {"type": "table", "img_path": "jXgHEwtXs8/tmp/27dc6ec47089e24ef85d245570901b532b621d7575d573bf2b14c02e97044745.jpg", "table_caption": ["Table 3: Quantitative comparison on the ccHarmony dataset at a $256\\times256$ resolution. The best results are marked in bold, and the second best are underlined. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "tive interval learning method in AICT, denoted as \u201cw/o Int\u201d. Compared to AICT, the performance of this method decreases due to the reduced expressiveness of the curves. To investigate whether using more LUTs can improve performance, we first cascade 2 and 3 LUTs for adaptive interval learning, denoted as ${}^{\\bullet\\bullet}\\mathrm{Int}\\times2^{\\bullet}$ and ${}^{\\bullet\\bullet}\\mathrm{Int}\\times3^{\\bullet}$ , respectively. Then, we cascade 2 and 3 LUTs for color transformation, denoted as $\\mathrm{^{66}T r a}\\times2^{\\circ}$ and $\\mathrm{^{66}T r a}\\times3^{\\circ}$ , respectively. Finally, we cascade 4 and 6 LUTs for alternating adaptive interval learning and color transformation, denoted as \u201cAlt $\\times\\ 2^{\\bullet}$ and ${^{\\bullet}\\!\\mathrm{Alt}}\\times3^{\\circ}$ , respectively. The experimental results show that increasing the number of LUTs for adaptive interval learning does not improve performance. Furthermore, using more LUTs for color transformation decreases performance due to increased training difficulty. Additionally, we combine AdaInt [47] and SepLUT [48] for global RGB-to-RGB transformations, denoted as \u201cAdaInt $^+$ SepLUT\u201d. Compared to AICT, one reason for the performance decline of \u201cAdaInt $^+$ SepLUT\u201d is the neglect of local context. ", "page_idx": 7}, {"type": "image", "img_path": "jXgHEwtXs8/tmp/e2485b63245029f324853b4635ee50a39fae138fc44437169e13ac519b2bf3d5.jpg", "img_caption": ["Figure 6: Qualitative comparison results and error maps. We visualize the error between the harmonized images and the ground truth images. The error maps are normalized for display, and the foreground is outlined in red. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "We conduct ablation studies on the global consistent weight learning method, as shown in Table 5. First, we remove the weight vector corresponding to the R channel, denoted as \u201cw/o R\u201d. Next, we remove the weight vectors for both the R and $\\mathrm{G}$ channels, denoted as \u201cw/o RG\u201d. Finally, we remove the entire weight learning module, denoted as \u201cw/o Weight\u201d. As the number of removed weight vectors increases, the harmonization performance decreases, highlighting the importance of the global consistent weight learning method. We also predict spatially varying weight parameters to weight each sampling point of the parameter map $R$ , denoted as \u201cSpatial\u201d. The results for \u201cSpatial\u201d indicate that learning ", "page_idx": 8}, {"type": "table", "img_path": "jXgHEwtXs8/tmp/3f7e8a79635c49826d86a605259cbfab2d56fbb95aa942c6bb7f1cc9f8f0ace8.jpg", "table_caption": ["Table 4: Ablation studies on the adaptive interval learning method. The best results are marked in bold. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "spatially varying weight parameters does not improve performance, as the LUT is responsible for pixel-wise color transformations. These observations demonstrate that the proposed adaptive interval learning method and global consistent weight learning method are effective. ", "page_idx": 8}, {"type": "text", "text": "4.4 Hyper-parameter Analyses ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We conduct studies on key parameters using the iHarmony4 dataset [8], including the number of knots $K$ , the coefficient of the LR branch loss $\\lambda$ , and the hyper-parameter $A_{\\mathrm{min}}$ in the foregroundnormalized MSE loss, as shown in Table 6. We set $K$ to 6, 8, and 10 to analyze the influence of curves with different numbers of knots for the image harmonization task, where $K$ is set to 8 in AICT. By comparing $\\bf{\\dot{K}}{=}6^{\\circ}$ with AICT, we observe that increasing the number of knots reduces fMSE and ", "page_idx": 9}, {"type": "table", "img_path": "jXgHEwtXs8/tmp/2d72481178260a43e1ac90ba7ad5090d9fa6c714d686925d7244cdf192dee247.jpg", "table_caption": ["Table 5: Ablation studies on the global consistent weight learning method. The best results are marked in bold. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "MSE while improving PSNR, indicating that a larger number of knots enhances the harmonization ability of the curves. However, as the number of knots increases, the performance of $\\mathrm{^{\\bullet\\bullet}K\\!=\\!10^{\\circ\\bullet}}$ decreases, suggesting that an excessive number of knots increases network parameters. This complicates the training process of the network, making it more challenging to achieve optimal performance. To investigate the effect of the LR branch loss, we set $\\lambda$ to 0, 0.01, and 0.1, where $\\lambda$ set to 0.01 in AICT. When $\\lambda$ is set to 0.01, AICT achieves the best performance on all metrics, demonstrating that an optimal weight for the LR branch loss is crucial for the high-resolution image harmonization. For the foreground-normalized MSE loss, we set $A_{\\mathrm{min}}$ to $H\\times W$ and $H^{\\prime}\\times W^{\\prime}$ in the HR and LR branch, respectively, which means these objective functions are equivalent to MSE functions, denoted as \u201cMSE\u201d. The results indicate that using the foreground-normalized MSE loss improves harmonization performance, as it prevents training samples with foregrounds of different sizes from being trained with varying loss magnitudes, ensuring effective training for small foreground images. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we formulate image harmonization as an image-based multiple curve estimation problem and propose an Adaptive-Interval Color Transformation method, which predicts pixel-wise color transformation and adaptively adjusts the sampling interval to model local nonlinearities of the color transformation at high resolution. Specifically, a parameter network is first designed to generate multiple curves as position-dependent 3D LUTs, which use the color and position of each pixel to perform pixel-wise color transformation. Then, we separate a color transform into a cascade of sub-transformations ", "page_idx": 9}, {"type": "table", "img_path": "jXgHEwtXs8/tmp/510f3d824cf5bfdc518bf73e39a9268a2a490989e53ac706460745bb171eae28.jpg", "table_caption": ["Table 6: Hyper-parameter analyses on the number of knots $K$ , the coefficient of the LR branch loss $\\lambda$ , and the hyper-parameter $A_{\\mathrm{min}}$ in the foreground-normalized MSE loss. The best results are marked in bold. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "using two position-dependent 3D LUTs to achieve the non-uniform sampling intervals of the color transform. Finally, a global consistent weight learning method is proposed to predict an image-level weight for each color transform, utilizing global information to enhance the overall harmony. Experimental results demonstrate that our method achieves state-of-the-art performance in high-resolution image harmonization with a lightweight architecture. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by the National Natural Science Foundation of China under Grants 62272134, 62236003 and 62072141, in part by the National Science and Technology Major Project under Grant 2021ZD0110901 and in part by Shenzhen Colleges and Universities Stable Support Program under Grant GXWD20220817144428005. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Ivan Anokhin, Pavel Solovev, Denis Korzhenkov, Alexey Kharlamov, Taras Khakhulin, Aleksei Silvestrov, Sergey Nikolenko, Victor Lempitsky, and Gleb Sterkin. High-resolution daytime translation without domain labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7488\u20137497, 2020. 14   \n[2] Samaneh Azadi, Deepak Pathak, Sayna Ebrahimi, and Trevor Darrell. Compositional GAN: learning image-conditional binary composition. International Journal of Computer Vision, 128:2570\u20132585, 2020. 1   \n[3] Bor-Chun Chen and Andrew Kae. Toward realistic image compositing with adversarial learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8415\u20138424, 2019. 1   \n[4] Qifeng Chen and Vladlen Koltun. Photographic image synthesis with cascaded refinement networks. In Proceedings of the IEEE International Conference on Computer Vision, pages 1511\u20131520, 2017. 14   \n[5] Daniel Cohen-Or, Olga Sorkine, Ran Gal, Tommer Leyvand, and Ying-Qing Xu. Color harmonization. ACM Transactions on Graphics, 25(3):624\u2013630, 2006. 1   \n[6] Wenyan Cong, Li Niu, Jianfu Zhang, Jing Liang, and Liqing Zhang. Bargainnet: Background-guided domain translation for image harmonization. In IEEE International Conference on Multimedia and Expo, pages 1\u20136, 2021. 2, 14   \n[7] Wenyan Cong, Xinhao Tao, Li Niu, Jing Liang, Xuesong Gao, Qihao Sun, and Liqing Zhang. Highresolution image harmonization via collaborative dual transformations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18470\u201318479, 2022. 3, 6, 7, 8, 14, 15   \n[8] Wenyan Cong, Jianfu Zhang, Li Niu, Liu Liu, Zhixin Ling, Weiyuan Li, and Liqing Zhang. Dovenet: Deep image harmonization via domain verification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8394\u20138403, 2020. 2, 6, 7, 8, 10, 14, 15   \n[9] Xiaodong Cun and Chi-Man Pun. Improving the harmony of the composite image by spatial-separated attention module. IEEE Transactions on Image Processing, 29:4759\u20134771, 2020. 2, 14   \n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 8   \n[11] Debidatta Dwibedi, Ishan Misra, and Martial Hebert. Cut, paste and learn: Surprisingly easy synthesis for instance detection. In Proceedings of the IEEE International Conference on Computer Vision, pages 1301\u20131310, 2017. 1   \n[12] Julian Jorge Andrade Guerreiro, Mitsuru Nakazawa, and Bj\u00f6rn Stenger. Pct-net: Full resolution image harmonization using pixel-wise color transformations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5917\u20135926, 2023. 2, 3, 6, 7, 8, 14, 15, 16   \n[13] Zonghui Guo, Zhaorui Gu, Bing Zheng, Junyu Dong, and Haiyong Zheng. Transformer for image harmonization and beyond. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(11):12960\u2013 12977, 2023. 2, 4, 14   \n[14] Zonghui Guo, Dongsheng Guo, Haiyong Zheng, Zhaorui Gu, Bing Zheng, and Junyu Dong. Image harmonization with transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14870\u201314879, 2021. 8, 14   \n[15] Zonghui Guo, Haiyong Zheng, Yufeng Jiang, Zhaorui Gu, and Bing Zheng. Intrinsic image harmonization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16367\u2013 16376, 2021. 2, 8, 14   \n[16] Yucheng Hang, Bin Xia, Wenming Yang, and Qingmin Liao. SCS-Co: Self-consistent style contrastive learning for image harmonization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19678\u201319687, 2022. 14   \n[17] Guoqing Hao, Satoshi Iizuka, and Kazuhiro Fukui. Image harmonization with attention-based deep feature modulation. In Proceedings of the British Machine Vision Conference, volume 1, page 2, 2020. 2   \n[18] Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the International Conference on Machine Learning, volume 37, pages 448\u2013456, 2015. 5   \n[19] Jiaya Jia, Jian Sun, Chi-Keung Tang, and Heung-Yeung Shum. Drag-and-drop pasting. ACM Transactions on Graphics, 25(3):631\u2013637, 2006. 1   \n[20] Zhanghan Ke, Chunyi Sun, Lei Zhu, Ke Xu, and Rynson WH Lau. Harmonizer: Learning to perform white-box image and video harmonization. In Proceedings of the European Conference on Computer Vision, pages 690\u2013706, 2022. 2, 3, 6, 7, 8, 14, 15, 16   \n[21] Jingtang Liang, Xiaodong Cun, Chi-Man Pun, and Jue Wang. Spatial-separated curve rendering network for efficient and high-resolution image harmonization. In Proceedings of the European Conference on Computer Vision, pages 334\u2013349, 2022. 2, 3, 14   \n[22] Jun Ling, Han Xue, Li Song, Rong Xie, and Xiao Gu. Region-aware adaptive instance normalization for image harmonization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9361\u20139370, 2021. 2, 8, 14   \n[23] Chengxu Liu, Huan Yang, Jianlong Fu, and Xueming Qian. 4d lut: learnable context-aware 4d lookup table for image enhancement. IEEE Transactions on Image Processing, 32:4742\u20134756, 2023. 3   \n[24] Li Niu, Wenyan Cong, Liu Liu, Yan Hong, Bo Zhang, Jing Liang, and Liqing Zhang. Making images real again: A comprehensive survey on deep image composition. arXiv preprint arXiv:2106.14490, 2021. 1   \n[25] Li Niu, Linfeng Tan, Xinhao Tao, Junyan Cao, Fengjun Guo, Teng Long, and Liqing Zhang. Deep image harmonization with globally guided feature transformation and relation distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7723\u20137732, 2023. 6, 7, 8, 14   \n[26] Xi Ouyang, Yu Cheng, Yifan Jiang, Chun-Liang Li, and Pan Zhou. Pedestrian-synthesis-gan: Generating pedestrian data in real scene and beyond. arXiv preprint arXiv:1804.02047, 2018. 1   \n[27] Patrick P\u00e9rez, Michel Gangnet, and Andrew Blake. Poisson image editing. ACM Transactions on Graphics, 22(3):313\u2013318, 2003. 1   \n[28] Francois Pitie, Anil C Kokaram, and Rozenn Dahyot. N-dimensional probability density function transferand its application to colour transfer. In Proceedings of the IEEE International Conference on Computer Vision, pages 1434\u20131439, 2005. 1   \n[29] Erik Reinhard, Michael Adhikhmin, Bruce Gooch, and Peter Shirley. Color transfer between images. IEEE Computer Graphics and Applications, 21(5):34\u201341, 2001. 1   \n[30] Tal Remez, Jonathan Huang, and Matthew Brown. Learning to segment via cut-and-paste. In Proceedings of the European Conference on Computer Vision, pages 37\u201352, 2018. 1   \n[31] Xuqian Ren and Yifan Liu. Semantic-guided multi-mask image harmonization. In Proceedings of the European Conference on Computer Vision, volume 13697, pages 564\u2013579, 2022. 14   \n[32] Xintian Shen, Jiangning Zhang, Jun Chen, Shipeng Bai, Yue Han, Yabiao Wang, Chengjie Wang, and Yong Liu. Learning global-aware kernel for image harmonization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7535\u20137544, 2023. 14   \n[33] Konstantin Sofiiuk, Polina Popenova, and Anton Konushin. Foreground-aware semantic representations for image harmonization. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1620\u20131629, 2021. 2, 6, 7, 8, 14   \n[34] Yuda Song, Hui Qian, and Xin Du. Starenhancer: Learning real-time and style-aware image enhancement. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4126\u20134135, 2021. 8   \n[35] Yuda Song, Hui Qian, and Xin Du. Multi-curve translator for high-resolution photorealistic image translation. In European Conference on Computer Vision, pages 126\u2013143. Springer, 2022. 5, 8   \n[36] Kalyan Sunkavalli, Micah K Johnson, Wojciech Matusik, and Hanspeter Pfister. Multi-scale image harmonization. ACM Transactions on Graphics, 29(4):1\u201310, 2010. 1   \n[37] Michael W Tao, Micah K Johnson, and Sylvain Paris. Error-tolerant image compositing. International Journal of Computer Vision, 103(2):178\u2013189, 2013. 1   \n[38] Yi-Hsuan Tsai, Xiaohui Shen, Zhe Lin, Kalyan Sunkavalli, Xin Lu, and Ming-Hsuan Yang. Deep image harmonization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3789\u20133797, 2017. 2   \n[39] Jeya Maria Jose Valanarasu, He Zhang, Jianming Zhang, Yilin Wang, Zhe Lin, Jose Echevarria, Yinglan Ma, Zijun Wei, Kalyan Sunkavalli, and Vishal M Patel. Interactive portrait harmonization. In International Conference on Learning Representations, 2023. 16   \n[40] Ke Wang, Micha\u00ebl Gharbi, He Zhang, Zhihao Xia, and Eli Shechtman. Semi-supervised parametric real-world image harmonization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5927\u20135936, 2023. 3   \n[41] Tao Wang, Yong Li, Jingyang Peng, Yipeng Ma, Xian Wang, Fenglong Song, and Youliang Yan. Real-time image enhancer via learnable spatial-aware 3d lookup tables. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2471\u20132480, 2021. 3   \n[42] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. Highresolution image synthesis and semantic manipulation with conditional gans. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8798\u20138807, 2018. 14   \n[43] Shuchen Weng, Wenbo Li, Dawei Li, Hongxia Jin, and Boxin Shi. MISC: multi-condition injection and spatially-adaptive compositing for conditional person image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7741\u20137749, 2020. 1   \n[44] Yazhou Xing, Yu Li, Xintao Wang, Ye Zhu, and Qifeng Chen. Composite photograph harmonization with complete background cues. In Proceedings of the ACM International Conference on Multimedia, pages 2296\u20132304, 2022. 2   \n[45] Ke Xu, Gerhard Petrus Hancke, and Rynson WH Lau. Learning image harmonization in the linear color space. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12570\u201312579, 2023. 14   \n[46] Ben Xue, Shenghui Ran, Quan Chen, Rongfei Jia, Binqiang Zhao, and Xing Tang. DCCF: deep comprehensible color filter learning framework for high-resolution image harmonization. In Proceedings of the European Conference on Computer Vision, pages 300\u2013316. Springer, 2022. 2, 3, 6, 7, 8, 14, 15, 16   \n[47] Canqian Yang, Meiguang Jin, Xu Jia, Yi Xu, and Ying Chen. Adaint: Learning adaptive intervals for 3d lookup tables on real-time image enhancement. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17522\u201317531, 2022. 3, 4, 7, 8, 14, 15   \n[48] Canqian Yang, Meiguang Jin, Yi Xu, Rui Zhang, Ying Chen, and Huaida Liu. Seplut: Separable imageadaptive lookup tables for real-time image enhancement. In Proceedings of the European Conference on Computer Vision, volume 13678, pages 201\u2013217, 2022. 3, 7, 8, 14, 15   \n[49] Hui Zeng, Jianrui Cai, Lida Li, Zisheng Cao, and Lei Zhang. Learning image-adaptive 3d lookup tables for high performance photo enhancement in real-time. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(4):2058\u20132073, 2020. 3   \n[50] Fengyi Zhang, Lin Zhang, Tianjun Zhang, and Dongqing Wang. Adaptively hashing 3dluts for lightweight real-time image enhancement. In IEEE International Conference on Multimedia and Expo, pages 2771\u2013 2776. IEEE, 2023. 3   \n[51] Song-Hai Zhang, Zheng-Ping Zhou, Bin Liu, Xi Dong, and Peter Hall. What and where: A context-based recommendation system for object insertion. Computational Visual Media, 6(1):79\u201393, 2020. 1 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 More Quantitative Results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table 7: Quantitative comparison on the iHarmony4 dataset at a $256\\times256$ resolution. The best results are marked in bold, and the second best are underlined. ", "page_idx": 13}, {"type": "table", "img_path": "jXgHEwtXs8/tmp/a2be8c7b94da1959a75bbf0a8ba22f63595e94b94e47eda3a77e26c896a086a9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "In this section, we provide supplementary quantitative results to further demonstrate the advantages of our proposed method. First, we evaluate AICT against more state-of-the-art methods using lowresolution test images from the iHarmony4 dataset [8]. Following previous work, we downsample these images to $256\\times256$ pixels to obtain low-resolution versions. It is important to note that AICT employs parameters trained on high-resolution images from the iHarmony4 dataset [8] and uses low-resolution images as input to perform pixel-wise color transformation instead of downsampling high-resolution output images for comparison. As shown in Table 7, AICT achieves superior performance across most metrics, which demonstrates the effectiveness of our method on lowresolution images. However, our method performs slightly worse on the HDay2night subset due to the limited number of training images in this subset. ", "page_idx": 13}, {"type": "text", "text": "We also compare AICT with stateof-the-art methods on the HAdobe5k subset at a resolution of $1024~\\times$ 1024. As shown in Table 8, our method achieves the best performance in terms of MSE and PSNR, and performs slightly worse than CDTNet256 [7] in terms of fMSE and SSIM. It is important to note that CDTNet [7] requires retraining for images of varying resolutions. In contrast, AICT demonstrates flexibility by using parameters trained on high-resolution images from the iHarmony4 dataset [8] to effectively process images at various resolutions. ", "page_idx": 13}, {"type": "text", "text": "In order to compare AICT with stateof-the-art methods at ultra-high resolutions, we collect two ultra highresolution benchmarks (over4K and over5K) by selecting images with res", "page_idx": 13}, {"type": "table", "img_path": "jXgHEwtXs8/tmp/82f2d8f25bfa94bf230e4568c050f7ffe56486867acab5f43b5ed51a5ea0947e.jpg", "table_caption": ["Table 8: Quantitative comparison on the HAdobe5k subset at a $1024\\times1024$ resolution. The best results are marked in bold, and the second best are underlined. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "olutions exceeding 4K $(4096\\times2160)$ and 5K $(5120\\times2880)$ from the HAdobe5k subset. As shown in Table 9, our method achieves the best performance on the over4K benchmark in terms of fMSE, MSE, and SSIM, and the second-best performance in terms of SSIM. As shown in Table 10, on the over5K benchmark, our method achieves the best performance in terms of MSE and PSNR and the second-best performance in terms of fMSE and SSIM. ", "page_idx": 13}, {"type": "text", "text": "We further compare the FLOPs, memory cost, and inference time of our method with state-of-the-art methods on the HAdobe5k subset. These metrics are crucial for evaluating the computational efficiency and scalability of the models. Our tests were conducted on a computer equipped with 32GB of memory and a GeForce GTX 1070 Ti GPU. Tables 11 and 12 show the comparison results at $1024\\!\\times\\!1024$ and $2048\\times2048$ resolutions, respectively. ", "page_idx": 14}, {"type": "text", "text": "Table 9: Quantitative comparison on the HAdobe5k subset at resolutions exceeding 4K $(4096\\times2160)$ ). The best results are marked in bold, and the second best are underlined. ", "page_idx": 14}, {"type": "table", "img_path": "jXgHEwtXs8/tmp/d401d8c107e85f48bef55efce8af9c0522454d8858f25cc1c7f94ef18dbbbbde.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "At both resolutions, Harmonizer [20] has a significant advantage in terms of FLOPs, while DCCF [46] has the lowest memory cost. At a resolution of $1024\\times1024$ , our method is comparable to PCTNet [12] in FLOPs, memory cost, and inference time. However, at a resolution of $2048\\times2048$ , our method demonstrates lower memory cost and shorter inference time compared to PCT-Net. One important reason is that the parameter maps in our method have a fixed size, whereas PCT-Net needs to align the parameter maps with the composite images, resulting in higher memory consumption and longer inference time. ", "page_idx": 14}, {"type": "text", "text": "A.2 More Qualitative Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We present additional qualitative comparison results on the iHarmony4 dataset [8], shown in Figure 7. Specifically, we compare our proposed method with Harmonizer [20], DCCF [46], and PCT-Net [12]. The qualitative results demonstrate that our method achieves superior harmonization performance, generating images that closely resemble the ground truth. Our method excels in adjusting ", "page_idx": 14}, {"type": "text", "text": "Table 10: Quantitative comparison on the HAdobe5k subset at resolutions exceeding 5K $(5120\\times2880)$ ). The best results are marked in bold, and the second best are underlined. ", "page_idx": 14}, {"type": "table", "img_path": "jXgHEwtXs8/tmp/06878ba0f09c902dcb5f72a715f5aacf83a1b53089481a0d31ca572b802223e7.jpg", "table_caption": [], "table_footnote": ["the overall brightness and preserving local details in the foreground, leading to more realistic and visually pleasing composite images. "], "page_idx": 14}, {"type": "table", "img_path": "jXgHEwtXs8/tmp/524490f859fd0e0b3116b42976f95e6cf0ce29f4cbed60425eafce7b80887c57.jpg", "table_caption": ["Table 11: Comparison of inference time, memory cost, and FLOPs at a $1024\\times1024$ resolution. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "jXgHEwtXs8/tmp/2dd71f995f352eb08192abf40b222e32a02e40c0143f0729861b0bc8d26b610e.jpg", "table_caption": ["Table 12: Comparison of inference time, memory cost, and FLOPs at a $2048\\times2048$ resolution. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.3 Evaluation on Real Composite Images ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We provide qualitative results on a real composite dataset [7], which presents a significant challenge by combining foreground regions from different light fields with background regions. The resolution of this dataset ranges from $1024\\times1024$ to $6016\\times4000$ pixels. As shown in Figure 8, AICT outperforms other methods in producing visually superior results in real-world scenarios. The images generated by AICT show a more harmonious appearance, with both overall brightness and local foreground details appearing more realistic. Following [39], we first randomly select 20 high-resolution real composite images from this dataset to conduct a user study. Then, 20 volunteers independently rank the predictions from 1 to 3 based on visual quality. Scores of 3, 2, and 1 are assigned for ranks 1, 2, and 3, respectively. The mean scores for each method are presented in the table below. As shown in Table 13, our method achieves the highest score. The above experimental results demonstrate the superiority and generalizability of our method. ", "page_idx": 14}, {"type": "image", "img_path": "jXgHEwtXs8/tmp/817657b9a571e5e9d1bfb2833404dbcec5e3dea551efbf6963814aeb26c7f7a5.jpg", "img_caption": ["Figure 7: Qualitative comparison results and error maps. We visualize the error between the harmonized images and the ground truth images. The error maps are normalized for display, and the foreground is outlined in red. "], "img_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "jXgHEwtXs8/tmp/1f4d88dc5b02c36858040b0d2164da1cea7fe8ecd742f327cac3756fce3f5291.jpg", "table_caption": ["Table 13: User study results. The best result is marked in bold. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "image", "img_path": "jXgHEwtXs8/tmp/0f56869e34c4b3708503a1e4f32e68e40f57da414a6fe0274ecfc2c0832f9cde.jpg", "img_caption": ["Figure 8: Qualitative comparison results against state-of-the-art methods on the real composite dataset. The foreground is outlined in red. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "A.4 Broader Impacts ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "This paper proposes a pixel-wise high-resolution image harmonization method aimed at adjusting the color of the foreground to seamlessly integrate with the background, thereby enhancing the realism of composite images and making a significant contribution to the image composition community. Simultaneously, our research holds paramount importance in domains such as art, entertainment, and commerce. However, it is worth noting that image harmonization techniques could potentially be exploited to create deceptive or misleading visual content. To address this concern, there is a pressing need for the development of image manipulation detection methods. Additionally, we emphasize the importance of raising awareness about the capabilities and limitations of such methods to effectively mitigate their adverse impacts. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "A.5 Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A primary limitation of our method lies in its incapability to effectively address foreground elements like mirrors and glass. This is because the model cannot accurately simulate the interplay between light and various materials in such scenarios. Figure 9 illustrates some failure cases where our method struggles to harmonize images containing such elements. ", "page_idx": 17}, {"type": "image", "img_path": "jXgHEwtXs8/tmp/78722904c4cf1a82fbd5e24c1373c2385e30cc3df8620b37f5005dda91237390.jpg", "img_caption": ["Figure 9: Examples of failed harmonization on the iHarmony4 dataset. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: The paper discusses the limitations of the work performed by the authors in Section A.5. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper fully discloses all the information needed to reproduce the main experimental results of the paper in Section 3. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The data used are publicly available, and the code can be found at https: //github.com/aipixel/AICT, along with sufficient instructions to faithfully reproduce the main experimental results. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper specifies all the training and test details in Section 4.1.2. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: The code can be found at https://github.com/aipixel/AICT. The results can be reproduced using the provided code and the implementation details described in the paper, so no error bars are reported in this paper. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 20}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper provides sufficient information on the computer resources in Section 4.1.2. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper discusses both potential positive societal impacts and negative societal impacts of the work performed in Section A.4. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: All assets used in the paper, including code, data, and models, have been properly credited to their original creators. The licenses are explicitly mentioned in Section 4.1.1 and 4.2. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper proposes a new model, and the documentation is provided in Section 3. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]