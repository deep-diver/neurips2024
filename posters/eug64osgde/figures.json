[{"figure_path": "eUg64OsGDE/figures/figures_0_1.jpg", "caption": "Figure 1: COUNTGD is capable of taking both visual exemplars and text prompts to produce highly accurate object counts (a), but also seamlessly supports counting with only text queries or only visual exemplars (b). The multi-modal visual exemplar and text queries bring extra flexibility to the open-world counting task, such as using a short phrase (c), or adding additional constraints (the words 'left' or 'right') to select a sub-set of the objects (d). These examples are taken from the FSC-147 [42] and CountBench [39] test sets. The visual exemplars are shown as yellow boxes. (d) visualizes the predicted confidence map of the model, where a high color intensity indicates a high level of confidence.", "description": "This figure demonstrates the capabilities of the COUNTGD model in open-world object counting.  It showcases the model's ability to accurately count objects using various combinations of input: text descriptions only, visual exemplars only, and a combination of both.  The multi-modal approach (text and exemplars) offers enhanced flexibility, allowing for more precise object selection using constraints like spatial location ('left', 'right'). Examples are drawn from the FSC-147 and CountBench datasets, with visual exemplars highlighted by yellow bounding boxes.  The heatmap in (d) visually represents the model's confidence in its predictions.", "section": "1 Introduction"}, {"figure_path": "eUg64OsGDE/figures/figures_3_1.jpg", "caption": "Figure 2: The COUNTGD architecture. At inference the object to be counted can be specified by visual exemplars or text prompts or both. The input image is passed through the image encoder, feswint to obtain spatial feature maps at different scales. The visual exemplar tokens are cropped out of this feature map using RoIAlign (as shown in Figure 3). The text is passed through the text encoder, ferr to obtain text tokens. In the feature enhancer, f, the visual exemplar tokens and text tokens are fused together with self-attention and cross-attend to the image features, producing the fused visual exemplar and text features, zv,t, and new image features, z\u0131. The k image features z\u2081 that have the highest cosine similarity with the fused features zv,t are passed to the cross-modality decoder, fy, as \u201ccross-modality queries\u201d. Finally, the similarity matrix, \u00dd between the outputs of the cross-modality decoder, fy, and zv,t is calculated, and outputs that achieve a maximum similarity with the Zvt above a confidence threshold o are identified as final detections and enumerated to estimate the final count. Our model is built on top of GroundingDINO [33] architecture with the additional modules indicated by blue shading.", "description": "This figure illustrates the COUNTGD architecture, which is a multi-modal open-world object counting model.  The model takes as input an image, text description of the target object, and/or visual exemplars (bounding boxes around example instances).  The image is processed through a Swin Transformer-based image encoder to extract multi-scale feature maps. Visual exemplars are extracted using RoIAlign, and the text is processed through a BERT-based text encoder.  A feature enhancer module fuses the image, text, and visual exemplar features using self- and cross-attention mechanisms.  A cross-modality decoder then processes the fused features to generate a similarity map, where high-similarity regions correspond to instances of the target object.  These instances are enumerated to obtain the final object count. The architecture extends GroundingDINO by incorporating modules for handling visual exemplars and fusing multi-modal information.", "section": "3.2 COUNTGD Architecture Components"}, {"figure_path": "eUg64OsGDE/figures/figures_3_2.jpg", "caption": "Figure 3: The visual feature extraction pipeline for images and visual exemplars. (a) For the input image, a standard Swin Transformer model is used to extract visual feature maps at multiple spatial resolutions. (b) For the visual exemplars with their corresponding bounding boxes, we first up-scale the multiple visual feature maps of the input image to the same resolution, then concatenate these feature maps, and project them to 256 channels with a 1 \u00d7 1 convolution. Finally, we apply a RoIAlign with the bounding box coordinates to get the visual features for the exemplars.", "description": "This figure shows how the model extracts visual features from both the input image and the visual exemplars.  For the input image (a), a Swin Transformer extracts multi-scale feature maps, which are then projected to 256 dimensions. For the visual exemplars (b), the image features are first upscaled to the same resolution, then concatenated, projected to 256 dimensions, and finally, RoIAlign is used to extract features within the bounding boxes of the exemplars.", "section": "3.2 COUNTGD Architecture Components"}, {"figure_path": "eUg64OsGDE/figures/figures_6_1.jpg", "caption": "Figure 1: COUNTGD is capable of taking both visual exemplars and text prompts to produce highly accurate object counts (a), but also seamlessly supports counting with only text queries or only visual exemplars (b). The multi-modal visual exemplar and text queries bring extra flexibility to the open-world counting task, such as using a short phrase (c), or adding additional constraints (the words 'left' or 'right') to select a sub-set of the objects (d). These examples are taken from the FSC-147 [42] and CountBench [39] test sets. The visual exemplars are shown as yellow boxes. (d) visualizes the predicted confidence map of the model, where a high color intensity indicates a high level of confidence.", "description": "This figure shows examples of COUNTGD's counting performance using different input modalities.  (a) demonstrates accurate counting with both text and visual exemplars. (b) shows that COUNTGD can also count using text-only or visual exemplar-only prompts. (c) illustrates the flexibility of using short phrases as input. Finally, (d) shows how COUNTGD can use text to refine the selection of objects specified by visual exemplars, visualizing the confidence map of the model's predictions.", "section": "1 Introduction"}, {"figure_path": "eUg64OsGDE/figures/figures_8_1.jpg", "caption": "Figure 5: Studying visual exemplar and text interactions. We plot the confidence scores of the instances for each image. In (a) and (b) we show we can specify shape with the exemplar and modify color with text. In (c) we show we can specify spatial location with text, and shape with the exemplar.", "description": "This figure demonstrates the interaction between visual exemplars and text in COUNTGD.  It visually shows how using both modalities allows for more specific object counting than either method alone. (a) and (b) illustrate how shape can be specified via an exemplar, while color is modified by additional text input, showing that the model only counts objects of the specified color matching the visual exemplars. (c) shows how spatial location (top/bottom row) can be selected via text, while shape is still specified by the visual exemplar, resulting in counting only the specific objects within the specified location and shape.", "section": "4.5 Language and Exemplar Interactions"}, {"figure_path": "eUg64OsGDE/figures/figures_14_1.jpg", "caption": "Figure 1: COUNTGD is capable of taking both visual exemplars and text prompts to produce highly accurate object counts (a), but also seamlessly supports counting with only text queries or only visual exemplars (b). The multi-modal visual exemplar and text queries bring extra flexibility to the open-world counting task, such as using a short phrase (c), or adding additional constraints (the words 'left' or 'right') to select a sub-set of the objects (d). These examples are taken from the FSC-147 [42] and CountBench [39] test sets. The visual exemplars are shown as yellow boxes. (d) visualizes the predicted confidence map of the model, where a high color intensity indicates a high level of confidence.", "description": "This figure shows examples of the COUNTGD model's ability to count objects accurately using different input modalities.  Subfigure (a) demonstrates the model's ability to accurately count objects using both visual exemplars (yellow boxes) and text prompts. Subfigure (b) showcases the model's functionality when using only text queries or visual exemplars. Subfigure (c) illustrates the flexibility of using short text phrases to specify the objects to count. Lastly, subfigure (d) visualizes the predicted confidence map, illustrating the model's high confidence in its predictions by means of color intensity.", "section": "1 Introduction"}, {"figure_path": "eUg64OsGDE/figures/figures_17_1.jpg", "caption": "Figure 7: Visualizing CountGD\u2019s output when the exemplar and text conflict. In the top row we show the input image, text input, and visual exemplars provided to COUNTGD. In the bottom row, we visualize COUNTGD\u2019s output. For the butterflies (leftmost example) and the fruit (example in the middle), COUNTGD correctly outputs a count of 0. For the flowers (rightmost example), COUNTGD incorrectly outputs a count of 9.", "description": "This figure demonstrates the results of an experiment designed to test the model's ability to handle conflicting information from visual exemplars and text prompts.  Three scenarios are presented: butterflies with text prompt \u201cwhite plant pot\u201d, strawberries and blueberries with text prompt \u201cstrawberry\u201d, and roses with text prompt \u201cyellow\u201d.  In the first two cases, the model correctly identifies no instances matching both the visual exemplar and text description. However, in the third case, the model incorrectly counts nine roses even though the text specified only yellow roses, demonstrating a limitation in handling conflicting information.  The top row shows the input, with red boxes highlighting the exemplar selections. The bottom row displays the model's output with numbers indicating the detected objects.", "section": "G. Diving Deeper Into Language and Exemplar Interactions"}, {"figure_path": "eUg64OsGDE/figures/figures_18_1.jpg", "caption": "Figure 4: Qualitative counting results on FSC-147 [42] and CountBench [39] using the multi-modal COUNTGD. The model is trained and tested on FSC-147 visual exemplars and text. Input text is written above each image, and visual exemplars are indicated by the red boxes. On CountBench, we test the same model trained on the FSC-147 in a zero-shot way with only text (there are no visual exemplars for CountBench). Blue words indicate the subject of each caption input to the model. In both cases, COUNTGD predicts the count in all images shown with 100% accuracy. Note on the CountBench examples, the model counts the specified objects correctly when there are multiple types of objects in the image, such as the tomatoes with cucumbers, and the girls with bubbles. Detected points are filtered with a Gaussian and plotted under the input images for visualization purposes.", "description": "This figure shows qualitative examples of object counting results using the COUNTGD model.  The top row shows examples from the FSC-147 dataset, where the model was trained and tested with both text and visual exemplars.  The input text is provided above each image, and red boxes indicate the visual exemplars. The bottom row presents zero-shot results from the CountBench dataset; here, only the text prompts were used.  The model achieves 100% accuracy across all examples. The figure highlights the model's ability to accurately count objects even when multiple object types are present.", "section": "4 Experiments"}, {"figure_path": "eUg64OsGDE/figures/figures_18_2.jpg", "caption": "Figure 9: Additional qualitative examples showing CountGD's performance on the FSC-147 test set. In these examples, CountGD predicts the count with 100% accuracy.", "description": "This figure shows four examples from the FSC-147 test set where the COUNTGD model correctly identifies the number of objects in each image.  The top row displays the input images, with the corresponding text prompts shown above each image. The bottom row shows the heatmap of predictions overlaid onto the input images. The red bounding boxes indicate the objects that were detected by the model, and the numbers in the bottom right corner of each image represent the final object counts predicted by the model.  The model accurately counts eggs, apples, deer, and nail polish in each of the four example images.", "section": "4.3 Comparison to State-of-the-art on Standard Benchmarks"}, {"figure_path": "eUg64OsGDE/figures/figures_19_1.jpg", "caption": "Figure 10: Text is sometimes not enough to specify the object to count. In (a), given only text, CountGD accurately estimates the number of crystals. In (b), CountGD cannot accurately estimate the number of crystals in the X-ray image using text alone, since they look unfamiliar. In (c), providing an additional visual exemplar alleviates the issue. Input images are in the top row. Detected instances from CountGD are shown in the bottom row.", "description": "This figure shows three examples where the model's counting performance is affected by the input modality and whether a visual exemplar is provided. In the first example, only text is provided, and the model accurately counts the number of crystals. In the second example, only text is provided, and the model fails to accurately count the crystals in an X-ray image where they appear less distinct. In the third example, both text and a visual exemplar are provided, and the model successfully counts the crystals.", "section": "4.5 Language and Exemplar Interactions"}, {"figure_path": "eUg64OsGDE/figures/figures_20_1.jpg", "caption": "Figure 1: COUNTGD is capable of taking both visual exemplars and text prompts to produce highly accurate object counts (a), but also seamlessly supports counting with only text queries or only visual exemplars (b). The multi-modal visual exemplar and text queries bring extra flexibility to the open-world counting task, such as using a short phrase (c), or adding additional constraints (the words 'left' or 'right') to select a sub-set of the objects (d). These examples are taken from the FSC-147 [42] and CountBench [39] test sets. The visual exemplars are shown as yellow boxes. (d) visualizes the predicted confidence map of the model, where a high color intensity indicates a high level of confidence.", "description": "This figure demonstrates the capabilities of the COUNTGD model in open-world object counting.  It showcases four scenarios:\n(a) Using both visual exemplars (yellow boxes) and text to achieve highly accurate object counts.\n(b) Demonstrating the model's ability to count using text-only or visual exemplar-only prompts.\n(c) Highlighting flexibility by using short phrases for object specification.\n(d) Illustrating refined counting by adding spatial constraints (e.g., \"left\", \"right\") to select subsets of objects, along with a visualization of the model's confidence map.", "section": "1 Introduction"}]