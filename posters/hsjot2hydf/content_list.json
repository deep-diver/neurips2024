[{"type": "text", "text": "Initializing Services in Interactive ML Systems for Diverse Users ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Avinandan Bose University of Washington avibose@cs.washington.edu ", "page_idx": 0}, {"type": "text", "text": "Mihaela Curmei University of California Berkeley mcurmei@berkeley.edu ", "page_idx": 0}, {"type": "text", "text": "Daniel L. Jiang University of Washington danji@cs.washington.edu ", "page_idx": 0}, {"type": "text", "text": "Jamie Morgenstern University of Washington jamiemmt@cs.washington.edu ", "page_idx": 0}, {"type": "text", "text": "Sarah Dean Cornell University sdean@cornell.edu ", "page_idx": 0}, {"type": "text", "text": "Lillian J. Ratliff University of Washington ratliff $@$ uw.edu ", "page_idx": 0}, {"type": "text", "text": "Maryam Fazel University of Washington mfazel $@$ uw.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper investigates ML systems serving a group of users, with multiple models/services, each aimed at specializing to a sub-group of users. We consider settings where upon deploying a set of services, users choose the one minimizing their personal losses and the learner iteratively learns by interacting with diverse users. Prior research shows that the outcomes of learning dynamics, which comprise both the services\u2019 adjustments and users\u2019 service selections, hinge significantly on the initial conditions. However, finding good initial conditions faces two main challenges: (i) Bandit feedback: Typically, data on user preferences are not available before deploying services and observing user behavior; (ii) Suboptimal local solutions: The total loss landscape (i.e., the sum of loss functions across all users and services) is not convex and gradient-based algorithms can get stuck in poor local minima. ", "page_idx": 0}, {"type": "text", "text": "We address these challenges with a randomized algorithm to adaptively select a minimal set of users for data collection in order to initialize a set of services. Under mild assumptions on the loss functions, we prove that our initialization leads to a total loss within a factor of the globally optimal total loss with complete user preference data, and this factor scales logarithmically in the number of services. This result is a generalization of the well-known $k$ -means++ guarantee to a broad problem class, which is also of independent interest. The theory is complemented by experiments on real as well as semi-synthetic datasets. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider a setting where a provider wants to design $k$ services for $n$ users with diverse preferences. Each service uses a model parameterized by a vector $\\theta\\in\\ensuremath{\\mathbb{R}}^{d}$ to predict users\u2019 preferences, and users pick a service that yields the smallest loss for them. The loss incurred by user $i$ when choosing a service parameterized by $\\theta$ is denoted by $\\mathcal{L}_{i}(\\theta,\\phi_{i})$ , where $\\phi_{i}\\ \\in\\ \\mathbb{R}^{d}$ parameterizes the user\u2019s preference. We want to design $k$ services by minimizing the sum of all losses; i.e., an optimization ", "page_idx": 0}, {"type": "text", "text": "problem of the form: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{minimize}_{\\theta_{1},\\dots,\\theta_{k}\\in\\mathbb{R}^{d}}\\ \\sum_{i=1}^{n}\\operatorname*{min}\\{\\mathcal{L}_{i}(\\theta_{1},\\phi_{i}),\\dots\\mathcal{L}_{i}(\\theta_{k},\\phi_{i})\\}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "This problem formulation is broad and includes the classical $k$ -means clustering problem [28] (where $\\mathcal{L}_{i}$ are Euclidean distances and the inner \u2018min\u2019 selects the closest among $k$ centroids), mixed-linear regression [42], generalized principal component analysis (GPCA) or subspace clustering [1], in addition to our new, motivating problem of designing $k$ services for $n$ users. Even if the losses ${\\mathcal{L}}_{i}$ are convex in $\\theta$ , this objective is generally not convex (even in the special case of the $k$ -means problem). ", "page_idx": 1}, {"type": "text", "text": "Our goal is to find a local minimum of this optimization problem with an approximation ratio (a worst-case guarantee on the achieved total loss with respect to the global optimum) under suitable yet broad assumptions on $\\mathcal{L}_{i}$ . Further, an important limitation in many practical settings is that the provider/designer has only bandit feedback (zeroth-order oracle access) to the loss functions (i.e., the designer doesn\u2019t know the function $\\mathcal{L}_{i}(\\cdot,\\phi_{i})$ , but can only evaluate its value for some $\\theta$ corresponding to the service chosen by the user among the ones deployed), which further complicates solution methods, compared to the classical cases of clustering and facility location problems [10] which typically assume full information. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we seek a novel and effective initialization scheme that vastly extends the celebrated $k$ -means $\\mathrel{\\downarrow}++$ algorithm and its analysis [5]. This scheme should retain the simplicity and ease of implementation of the original algorithm, yet be able to (1) handle general loss families (assumptions on $\\mathcal{L}_{i}$ are discussed in Section 2), (2) provide a tight, instance-dependent approximation ratio (details in Section 3), and (3) handle realistic information limitations such as access only to (noisy) bandit feedback, in a sample-efficient manner. Next, we describe in more detail the important use case of initialization of services for diverse users in multi-service ML systems (yet as noted above, our main result has other applications as well, and can be of independent interest). ", "page_idx": 1}, {"type": "text", "text": "Motivation. In a variety of contexts such as federated learning [27], crowd-sourcing [36] and online recommendation systems [35], data about user preferences is acquired through iterative interactions. This data is then used to improve the model and serve the individual needs of users. Given that users\u2019 preferences are typically heterogeneous, recent works demonstrate that using multiple specialized models can be more effective than the one-size-fits-all approach of employing a single large shared model, e.g., for clustered federated learning [30, 34, 17], meta learning [25, 7], fine-tuning for specific groups of users or tasks [9, 37], and in the context of fair classifiers [39]. Here we tackle the crucial yet under-explored phase of initializing services in ML systems that learn interactively from diverse users. The initialization process is crucial as it sets the stage for how effectively these systems can adapt and specialize with future user interactions. Once initialized, the services interact with users, who, in turn, choose among services based on their loss. These \u201clearning dynamics\u201d typically lead to the specialization of services to groups of users [18, 12], and [12] shows experimentally that the overall social welfare achieved by the services depends on the initialization of the learning dynamics. We note that in the context of problem (1), the learning dynamics in [12] can be seen as updates of an alternating minimization algorithm, iteratively updating users\u2019 choices (the inner minimization) and services\u2019 parameters (update to each $\\theta_{j}$ ). Our goal is to initialize a set of services to minimize the sum of losses for all users (or equivalently, maximize total welfare), tackling the following challenges: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Bandit loss feedback: In practice, offering a service often precedes data collection. Specifically, in contexts like online recommendations, it is usually not feasible to gather user preference data (knowledge about $\\phi_{i}$ and evaluations of $\\mathcal{L}_{i}(\\cdot,\\phi_{i})$ at various different parameter values in problem (1)) without first deploying the services parameterized by $\\{\\theta_{1},\\ldots,\\theta_{k}\\}$ and observing user interactions. This means that data collection is inherently conditional on the existence of services, challenging the conventional \u201cdata-first, model-second\u201d paradigm. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Suboptimal local solutions: Since users select the service with the lowest loss $(\\operatorname*{min}\\{\\mathcal{L}_{i}(\\theta_{1},\\phi_{i}),\\ldots\\mathcal{L}_{i}(\\theta_{k},\\phi_{i})\\}$ in problem (1)), minimizing over the parameters $\\{\\theta_{1},\\ldots,\\theta_{k}\\}$ leads to a nonconvex problem in general, as mentioned earlier. Gradient-based learning dynamics can get stuck in local minima where the total loss can be significantly worse than the globally minimum loss. Thus, the outcomes of learning dynamics are heavily affected by initialization of service parameters. ", "page_idx": 1}, {"type": "text", "text": "Contributions. The following summarizes the contributions of this paper. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We design a computationally and statistically efficient algorithm for initialization of services prior to learning dynamics. The algorithm works by adaptively selecting a small number of users to collect data from (via queries of their loss function) in order to initialize the set of services.   \n\u2022 We establish an approximation ratio for the designed algorithm: the expected total loss achieved by the algorithm right after initialization is within a factor of the globally optimal total loss in the presence of complete user preference data, and this factor scales logarithmically in the number of services. Furthermore this bound is tight, and recovers the known $\\boldsymbol{\\mathrm{k}}$ -means $^{++}$ approximation ratio as a special case (cf. Section 3).   \n\u2022 When users belong to a set of demographic groups, it is desirable that the services do not result in unfavorable outcomes towards certain demographics (e.g., based on gender or racial groups). One fair objective is to minimize the maximum average loss of users across different groups. We provide an approximation ratio for this fair objective that scales logarithmically in the number of services (cf. Section 3).   \n\u2022 In the context of linear prediction models, we study the problem of generalizing to users the provider has not interacted with before (cf. Section 4).   \n\u2022 We empirically demonstrate the strengths of our initialization scheme via experiments on a prediction task using 2021 US Census data, and online movie recommendation task using the Movielens10M dataset (cf. Section 5). ", "page_idx": 2}, {"type": "text", "text": "1.1 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Multiple Model Specialization. In distributed learning, where data sources are users\u2019 personal devices, utilizing multiple specialized models, where users are grouped into clusters representing interests, can yield improved predictions and outcomes. For instance, in recommendation systems these clusters could represent users interested in different movie genres, or different combination of features (see Appendix B for a concrete example on Netfilx recommendation clusters). This approach has been adopted recently in clustered federated learning [34, 30, 17] and online interactive learning [32], facility location problems [6], where users choose models/services and for which they provide updates. ", "page_idx": 2}, {"type": "text", "text": "Clustering. Multiple model specialization leads to clustering the users into groups and centering a specialized model on each group. We provide a brief review of the $k$ -means clustering problem and establish the connection to specialization. The $k$ -means clustering problem is one of the most commonly encountered unsupervised learning problems. Given a set of known $n$ points in Euclidean space, the goal is to partition them into $k$ clusters (each characterized by a center), such that the sum of square of distances to their closest center is minimized. Dasgupta [11] and Aloise et al. [3] showed that the $k$ -means problem is NP-Hard. The most popular heuristic for $k$ -means is Lloyd\u2019s algorithm [28], which proceeds by randomly initializing $k$ centers and then uses iterative updates to find a locally optimal $k$ -means clustering, which can be arbitrarily bad compared to the globally optimal clustering. The performance of the $k$ -means algorithm relies crucially on the initialization. Arthur and Vassilvitskii [5] and Ostrovsky et al. [33] proposed an elegant polynomial time algorithm for initializing centers, known as $k$ -means $^{++}$ . Arthur and Vassilvitskii [5] proved that the expected cost of the initial clustering obtained by $k$ -means $^{++}$ is at most $8(2+\\log k)$ times the cost of optimal $k$ -means clustering. Our work generalizes the analysis of Arthur and Vassilvitskii [5] to the setting where user\u2019s preferences are represented as unknown points and the loss functions are unknown with only bandit access, not necessarily identical, and general as long they satisfy Assumptions 2.1 and 2.2, with important examples given in Appendix C. ", "page_idx": 2}, {"type": "text", "text": "For a detailed discussion on more related works please see Appendix A. ", "page_idx": 2}, {"type": "text", "text": "Notation and Terminology. For\u221a a symmetric matrix A and any vector $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , we denote its Mahalanobis norm by $\\|x\\|_{\\mathbf{A}}\\,=\\,\\sqrt{x^{\\top}\\mathbf{A}x}$ . The generalized eigenvalues for a pair of symmetric matrices $\\mathbf{A}$ and $\\mathbf{B}$ are denoted by $\\lambda(\\mathbf{A},\\mathbf{B})$ , defined as the solutions of $\\lambda$ for the generalized eigenvalue problems ${\\bf A}v\\,=\\,\\lambda{\\bf B}v$ [16, 14]. Specifically we use $\\lambda_{\\mathrm{min}}({\\bf A},{\\bf B})$ to denote minimum generalized eigenvalue for the matrix pair $\\mathbf{A},\\mathbf{B}$ . The loss for a user $i$ given service $\\theta\\in\\ensuremath{\\mathbb{R}}^{d}$ is denoted by $\\mathcal{L}_{i}(\\theta,\\phi_{i})$ where $\\phi_{i}$ parameterizes the user\u2019s preference. For a set of users $\\boldsymbol{\\mathcal{A}}$ (e.g., ${\\mathcal{A}}=[n]$ denotes a set of $n$ users) and a set of services $\\Theta\\,=\\,\\{\\theta_{1},\\dots,\\theta_{k}\\}\\,\\subset\\,\\mathbb{R}^{d}$ , the total loss is defined as $\\mathcal{L}(\\Theta,\\mathcal{A})=$ $\\begin{array}{r}{\\sum_{i\\in{\\cal A}}\\operatorname*{min}_{j\\in[k]}{\\mathcal{L}}_{i}\\big(\\theta_{j},\\phi_{i}\\big)}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "2 Problem Setup ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We make the following assumptions about the functional form of $\\mathcal{L}_{i}$ , and state several examples of function classes satisfying these properties. Note that the designer/provider doesn\u2019t need to know the functional form of $\\mathcal{L}_{i}$ , and knowledge about ${\\mathcal{L}}_{i}$ is obtained through bandit feedback via observing the scalar values $\\mathcal{L}_{i}(\\theta,\\phi_{i})$ for different $\\theta$ , where $\\theta$ parameterizes the services. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.1 (Unique Minimizer). The loss function satisfies the following equivalence: ${\\mathcal{L}}_{i}(\\theta,{\\bar{\\phi_{i}}})=0\\iff\\theta=\\phi_{i}$ . ", "page_idx": 3}, {"type": "text", "text": "This assumption implies that unless all users have identical preference parameters, there doesn\u2019t exist a single service parameter $\\theta$ that simultaneously minimizes every user\u2019s loss. Thus providing multiple services (multiple $\\theta$ \u2019s) where the users choose the one best for them is strictly better than one service for all users. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.2 (Approximate Triangle Inequalities). For a pair of users $i,j$ there exists a finite constant $c_{i j}>0$ such that for all $\\boldsymbol{\\theta}\\in\\bar{\\mathbb{R}}^{d}$ the following hold: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(i)~c_{i j}\\mathcal{L}_{i}(\\theta,\\phi_{i})\\leq\\mathcal{L}_{j}(\\theta,\\phi_{j})+\\mathcal{L}_{j}(\\phi_{i},\\phi_{j}).}\\\\ {(i i)~c_{i j}\\mathcal{L}_{i}(\\phi_{j},\\phi_{i})\\leq\\mathcal{L}_{j}(\\theta,\\phi_{j})+\\mathcal{L}_{i}(\\theta,\\phi_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here $c_{i j}$ (equal to $c_{j i}$ ) captures the alignment between the preference parameters and loss geometries of two users. Lower values of $c_{i j}$ indicate less similarity. Item $(i)$ implies that the loss for user $i$ on any service $\\theta\\in\\ensuremath{\\mathbb{R}}^{d}$ is no worse than (up to a constant factor) the sum of (a) loss of another user $j$ on using the same service, and (b) the loss of user $j$ if they were to use user $i$ \u2019s preference parameter. The latter term (b) can be seen as measuring the similarity between the users\u2019 preferences. ", "page_idx": 3}, {"type": "text", "text": "For condition $(i i)$ to hold for all $\\theta\\in\\ensuremath{\\mathbb{R}}^{d}$ , it must also hold for the service that minimizes the sum of losses of both users using the same service. Suppose that users $i$ and $j$ were to exchange preference parameters. Then their loss would be no worse than (up to a constant factor) their minimum total loss, i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\nc_{i j}\\mathcal{L}_{i}(\\phi_{j},\\phi_{i})\\leq\\operatorname*{min}_{\\theta\\in\\mathbb{R}^{d}}\\left(\\mathcal{L}_{j}(\\theta,\\phi_{j})+\\mathcal{L}_{i}(\\theta,\\phi_{i})\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Some examples of loss functions and the corresponding constants that satisfy these assumptions include the following (see Appendix C for additional examples and derivations): ", "page_idx": 3}, {"type": "text", "text": "\u2022 Squared error loss for linear predictors (cf. Section 4). \u2022 The Huber loss on the prediction error: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{i}(\\theta,\\phi_{i})=\\left\\{\\frac{1}{2}\\|\\theta-\\phi_{i}\\|^{2},\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ {\\mathcal{S}(\\|\\theta-\\phi_{i}\\|-\\frac{1}{2}\\delta),\\quad\\mathrm{otherwise}.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This loss is used typically in robust estimation tasks. Here $\\Vert\\cdot\\Vert$ could be any norm, and we show $c_{i j}=1/3$ . ", "page_idx": 3}, {"type": "text", "text": "\u2022 The normalized cosine distance: $\\mathcal{L}_{i}(\\theta,\\phi_{i})=1-\\theta^{\\top}\\phi_{i}$ where $\\|\\theta\\|_{2}=\\|\\phi_{i}\\|_{2}=1$ , with $\\begin{array}{r}{c_{i j}=\\frac{1}{2}}\\end{array}$ . This is commonly used as a similarity measure in natural language processing applications, for example finding similarity between two documents.   \n\u2022 The Mahalanobis distance: $\\begin{array}{r c l}{{{\\mathcal L}_{i}(\\theta,\\phi_{i})}}&{{=}}&{{\\|\\theta\\;-\\;\\phi_{i}\\|_{\\Sigma_{i}}}}\\end{array}$ . Different users can have different $\\Sigma_{i}$ capturing their diverse loss variation, as long as $\\Sigma_{i}$ is full rank. Here $\\begin{array}{r l}{c_{i j}}&{{}=}\\end{array}$ $\\operatorname*{min}\\{\\lambda_{\\operatorname*{min}}\\bar{(}\\Sigma_{i},\\Sigma_{j}^{^{-}}),\\lambda_{\\operatorname*{min}}(\\Sigma_{j},\\Sigma_{i})\\}$ .   \n\u2022 Any distance metric: This naturally follows from triangle inequality, hence $c_{i j}=1$ .   \n\u2022 Any arbitrary function $\\mathcal{L}_{i}(\\theta,\\phi_{i})$ that is $L_{i}$ -Lipschitz and $\\mu_{i}$ -strongly convex in $\\theta$ with $c_{i j}=$ $\\operatorname*{min}(\\mu_{i},\\mu_{j})/\\operatorname*{max}(L_{i},L_{j})$ . Objective. Suppose the users have access to $k$ services parameterized by $\\Theta=\\{\\theta_{1},\\dots,\\theta_{k}\\}\\subset\\mathbb{R}^{d}$ .   \nThen, each user $i$ selects service $\\theta_{l}$ that minimizes their loss, i.e. $\\begin{array}{r}{\\mathcal{L}_{i}(\\boldsymbol{\\Theta},\\boldsymbol{\\phi}_{i})=\\operatorname*{min}_{l\\in[k]}\\mathcal{L}_{i}(\\boldsymbol{\\theta}_{l},\\boldsymbol{\\phi}_{i})}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "As discussed earlier, our goal is to design $\\Theta$ , such that the sum of losses across users and services is minimized. We define the objective as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\Theta,[n])=\\sum_{i\\in[n]}\\operatorname*{min}_{j\\in[k]}\\mathcal{L}_{i}(\\theta_{j},\\phi_{i})=\\sum_{i\\in[n]}\\mathcal{L}_{i}(\\Theta,\\phi_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Definition 2.3. Define the unknown optimal set of $k$ services that minimizes the objective to be ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Theta_{\\mathrm{OPT}}:=\\operatorname{argmin}_{|\\Theta|=k}\\mathcal{L}(\\Theta,[n]).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "1: Input: Set of users $[n]$ , number of services $k$ .   \n2: Choose a user $i$ uniformly randomly from $[n]$ .   \n3: Query user $i$ \u2019s preference $\\phi_{i}$ , set the first service $\\Theta_{1}=\\phi_{i}$ .   \n4: for $t\\in\\{2,\\ldots,k\\}$ do   \n5: User behavior: Collect user losses on existing services $\\Theta_{t-1}:\\{\\mathcal{L}_{i}(\\Theta_{t-1},\\phi_{i})\\}_{i\\in[n]}$ .   \n6: User Selection: Sample $l$ from $[n]$ with probability $P(l=i)\\propto\\mathcal{L}_{i}(\\Theta_{t-1},\\phi_{i})$ .   \n7: New service: Query user $l^{\\dagger}$ \u2019s preference $\\phi_{l}$ .   \n8: $\\Theta_{t}=\\Theta_{t-1}\\cup\\phi_{l}$ .   \n9: end for   \n10: Return $\\Theta_{k}$ ", "page_idx": 4}, {"type": "text", "text": "Specifically, $\\Theta_{\\mathrm{OPT}}$ defines a \u201cclustering\u201d, meaning a partitioning of the $n$ users into $k$ clusters. The cluster $B_{m}$ is the set of all users that prefer the service $\\theta_{m}$ among all the services in the optimal set $\\Theta_{\\mathrm{OPT}}$ . In other words, $B_{m}$ is defined as the set of all points such that $\\mathcal{B}_{m}=\\{i\\in[n]\\ |\\ \\}\\dag\\theta_{m}=$ arg $\\begin{array}{r}{\\operatorname*{min}_{\\theta_{l}\\in\\Theta_{\\mathrm{OPT}}}\\mathcal{L}_{i}(\\theta_{l},\\phi_{i})\\right\\}}\\end{array}$ . If multiple services are equally preferred by a subpopulation, the ties are broken arbitrarily. The resulting set of clusters is denoted by $\\mathcal{C}(\\Theta_{\\mathrm{OPT}})=\\{\\mathcal{B}_{1},\\dots,\\mathcal{B}_{k}\\}$ . ", "page_idx": 4}, {"type": "text", "text": "The are several statistical and computational challenges to this problem. ", "page_idx": 4}, {"type": "text", "text": "Challenge 1. Since preferences $\\{\\phi_{i}\\}_{i\\in[n]}$ and loss functions $\\{{\\mathcal{L}}_{i}\\}_{i\\in[n]}$ are unknown and the provider only has zeroth order or bandit feedback access, estimating the objective function $\\mathcal{L}(\\Theta,[n])$ usually needs a lot of data collected uniformly across the users. This large amount of data is needed before services can be deployed, yet as stated earlier, we are in the situation where we have no data until we deploy services and observe user interactions. Our limited access to user information (via limited queries) makes our setting challenging. ", "page_idx": 4}, {"type": "text", "text": "Challenge 2. The loss function is non-convex and iterative minimization approaches from a random initialization are susceptible to getting stuck in arbitrarily poor local optima. This means computing the optimal clustering first and then finding the best service for each cluster is NP-Hard. ", "page_idx": 4}, {"type": "text", "text": "Challenge 3. We do not assume any data separability conditions, for example user preference parameters are drawn from $k$ well separated distributions. Thus we are unable to exploit underlying structure to reduce sample complexity. ", "page_idx": 4}, {"type": "text", "text": "Despite the challenges, in Section 3, we propose an algorithm that is both statistically and computationally efficient, and admits an approximation ratio with respect to the globally optimal value, i.e., $\\mathcal{L}(\\Theta_{\\mathrm{OPT}},[n])$ . ", "page_idx": 4}, {"type": "text", "text": "3 Algorithm & Main Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we present our initialization algorithm with guarantees, Algorithm 1, and describe how the steps of the algorithm arise naturally in the interactive systems under consideration. Since collecting data uniformly across all the $n$ users can be prohibitively expensive, our goal is to get data from a minimal number of users. ", "page_idx": 4}, {"type": "text", "text": "Each iteration of the loop in the algorithm adds a service sequentially and the loop terminates when there are $k$ services, where $k$ is a predetermined parameter for the algorithm. We focus on the loop (lines 4-8) in Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "Suppose at time $t-1$ , the set $\\Theta_{t-1}$ is the set of current $t-1$ services. Then, at time step $t$ the following steps take place. ", "page_idx": 4}, {"type": "text", "text": "\u2022 User behavior (line 5). Given the list of services $\\Theta_{t-1}$ users are assumed to choose the best service that minimizes user loss. Users report their losses with respect to the service they choose from the set of existing services $\\{\\mathcal{L}_{i}(\\Theta_{t-1},\\bar{\\phi}_{i})\\}_{i\\in[n]}$ . In practice, this step requires deploying the services and collecting signals of engagement and utility to determine the loss associated with each user, under the behavioral assumption that users are rational agents that choose the best available service. The provider thus needs to measure each user\u2019s loss only in their single chosen service. ", "page_idx": 4}, {"type": "text", "text": "\u2022 User selection (line 6) A new user $l$ is selected with probability proportional to $\\mathcal{L}_{i}(\\Theta_{t-1},\\phi_{l})$ . This ensures that users that are currently poorly served by existing services are more likely to be selected. ", "page_idx": 4}, {"type": "text", "text": "\u2022 New Service (line 7-8). Given a selected user $l$ , the algorithm queries the preference $\\phi_{l}$ of the user and centers the new service at that preference $\\theta_{t}=\\phi_{l}$ . In practice, this step requires acquiring data about the user in order to learn their preference parameter (this is needed for only $k$ total users throughout the algorithm). For example, data may be acquired by incentivizing the selected users, via offering discount coupons or free premium subscriptions [21]. ", "page_idx": 5}, {"type": "text", "text": "With each iteration the loss of each user is non-increasing; the previous services remained fixed and a user would switch to a new service only if it improves quality or equivalently decreases loss. Since at each iteration, a new service is added, the process terminates after $k$ steps. Since it is costly to offer and maintain too many different services, we typically have $k\\ll n$ . ", "page_idx": 5}, {"type": "text", "text": "We now discuss the theoretical properties of the set of services we get at the termination of Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.1. Consider n users with unknown preferences $\\{\\phi_{1},\\dots,\\phi_{n}\\}\\,\\subset\\,\\mathbb{R}^{d}$ , and associated loss functions $\\mathcal{L}_{i}(\\cdot,\\cdot)$ satisfying Assumptions 2.1 and 2.2, with bandit access. Let $\\Theta_{\\mathrm{OPT}}\\,\\subset\\,\\mathbb{R}^{d}$ be the set of $k$ services minimizing the total loss and $\\mathcal{C}(\\Theta_{\\mathrm{OPT}})$ the resulting partitioning of users (Definition 2.3). If Algorithm $^{\\,l}$ is used to obtain $k$ services $\\Theta_{k}$ , then the following bound holds: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\Theta_{k}}[\\mathcal{L}(\\Theta_{k},[n])]\\le K_{\\mathrm{OPT}}(2+\\log k)\\cdot\\mathcal{L}(\\Theta_{\\mathrm{OPT}},[n]),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the expectation is taken over the randomization of the algorithm and $K_{\\mathrm{OPT}}$ is equal to ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{max}_{\\mathcal{B}\\in\\mathcal{C}(\\Theta_{\\mathrm{OPT}})}\\frac{4}{\\underset{j\\in B}{\\operatorname*{min}}\\underset{i\\in B}{\\sum}c_{i j}}\\left(\\operatorname*{max}_{j\\in\\mathcal{B}}\\sum_{i\\in B}\\frac{1}{c_{i j}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "A detailed proof is presented in Appendix D; we summarize the main ideas here. The intuition is that a chosen user\u2019s preference parameter is typically a good representative for other users in its cluster. Thus adding a service parameterized by the chosen user\u2019s preferences generally reduces the losses of users in this cluster. Subsequently we are less likely to pick another user from the same cluster. The $\\log k$ factor is due to clusters from which users were never picked. ", "page_idx": 5}, {"type": "text", "text": "A similar proof approach was used by Arthur and Vassilvitskii [5] in the context of the $k$ -means problem, by sequentially placing centers on known points sampled with probability proportional to the point\u2019s squared distance to its closest existing center. A key novelty of our analysis is to capture the alignment of diverse loss geometries across users in a large class of functions, specifically understanding how user similarities $c_{i j}$ affect the approximation ratio.1 ", "page_idx": 5}, {"type": "text", "text": "Key characteristics of $K_{\\mathrm{OPT}}$ : The following are essential characteristics of the term $K_{\\mathrm{OPT}}$ . ", "page_idx": 5}, {"type": "text", "text": "(i) All terms in $K_{\\mathrm{OPT}}$ depend on the local clusters in the unknown optimal clustering $\\mathcal{C}(\\Theta_{\\mathrm{OPT}})$ .   \n(ii) The constant $\\begin{array}{r}{\\operatorname*{min}_{j\\in\\mathcal{B}}\\frac{1}{|\\mathcal{B}|}\\sum_{i\\in\\mathcal{B}}c_{i j}}\\end{array}$ captures the user whose loss geometry is least similar to the average loss geometry of the cluster they belong to (recall Assumption 2.2.i).   \n(iii) The constant maxj\u2208B|B1| $\\begin{array}{r}{\\operatorname*{max}_{j\\in\\mathcal{B}}\\frac{1}{|\\mathcal{B}|}\\sum_{i}\\frac{1}{c_{i j}}}\\end{array}$ captures the user whose preference is least similar to the optimal service parameter of the cluster they belong to (recall Assumption 2.2.ii).   \n(iv) Even within a cluster all terms are averages, so a few poorly aligned pairs of users don\u2019t hurt the bound if the cluster sizes are large. ", "page_idx": 5}, {"type": "text", "text": "Fair objective. While minimizing the total loss is beneficial from the provider\u2019s point of view in keeping users satisfied on average, it is undesirable in human-centric applications if the provided services result in unfavorable or harmful outcomes towards some demographic groups. ", "page_idx": 5}, {"type": "text", "text": "Suppose the $n$ users come from $m$ different demographic groups ( $m$ is typically small, say racial groups, gender). We denote the groups as $A=\\{A_{1},\\dots,A_{m}\\}\\,\\subset\\,[n]$ . The fairness objective is defined as the maximum average loss suffered by any group: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Phi(\\Theta,\\mathcal{A})=\\operatorname*{max}_{i\\in[m]}\\mathcal{L}(\\Theta,\\mathcal{A}_{i})/|\\mathcal{A}_{i}|.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "[15] defined this objective in the context of fair $k$ -means where the points and group identities are known and gave a non-constructive proof that if a $c-$ approximate solution for $k$ -means exists, it is $m\\cdot c$ -approximate for fair $k$ -means. For the fairness objective, we slightly modify our algorithm, by simply reweighting the probability to select an user by the inverse of the size of their demographic group to result in Fair AcQUIre (Algorithm 2 in Appendix E). ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Theorem 3.2. Consider $n$ users with unknown preferences $\\{\\phi_{1},\\dots,\\phi_{n}\\}\\,\\subset\\,\\mathbb{R}^{d}$ , and associated loss functions $\\mathcal{L}_{i}(\\cdot,\\cdot)$ satisfying Assumptions 2.1 and 2.2 with bandit access. Suppose these users belong to m demographic groups $\\mathcal{A}=\\{\\mathcal{A}_{1},...\\,,\\mathcal{A}_{m}\\}\\subset[n]$ . Let $\\Theta_{\\mathrm{fair}}\\subset\\mathbb{R}^{d}$ be the set of $k$ services minimizing the fairness objective $\\Phi$ given in (4). If Algorithm 2 is used to obtain $k$ services $\\Theta_{k}$ , then the following bound holds: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\Theta_{k}}[\\Phi(\\Theta_{k},\\mathcal{A})]\\leq m K_{\\mathrm{fair}}(2+\\log k)\\cdot\\Phi(\\Theta_{\\mathrm{fair}},\\mathcal{A}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the expectation is taken over the randomization of the algorithm and $K_{\\mathrm{fair}}$ is defined in (10). ", "page_idx": 6}, {"type": "text", "text": "4 Generalization in Linear Predictors ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In practical settings, a provider would want to design services that not only keep the subscribed users satisfied but also attract new users to subscribe by generalizing the services to users it has never interacted with before. Now instead of considering $n$ users, suppose that each $i\\in[N]$ represents a subpopulation with its own (sub-Gaussian) distribution of features, and the provider can interact with finite samples $n_{i}$ from these distributions. A question that arises is whether we can deal with this finite-sample-from-subpopulations scenario, and how does the number of samples affect the algorithm\u2019s output to unseen users. In this section, we answer this question for the special case of linear predictors (i.e., regression loss). ", "page_idx": 6}, {"type": "text", "text": "In this section we restrict ourselves to the special case of linear prediction tasks, where the goal is to accurately predict the score of a user as a linear function of their features. The score for a user in the $i^{\\mathrm{th}}$ subpopulation with zero-mean random feature $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ is generated as $y=\\phi_{i}^{\\top}x$ where both the true linear regressor $\\phi_{i}\\in\\mathbb{R}^{d}$ and the feature covariance $\\mathbb{E}_{x}[x x^{\\top}]=\\Sigma_{i}$ are unknown. Suppose a service uses a linear regressor $\\theta\\in\\mathbb{R}^{d}$ , to predict the score for this user as $\\theta^{\\top}x\\in\\mathbb{R}$ . The loss for this subpopulation for this service is defined as the expected squared error between the predicted and actual scores, i.e., $\\mathcal{L}_{i}(\\theta,\\phi_{i})=\\mathbb{E}_{(x,y)}[(\\theta^{\\top}x-y)^{2}]=\\^{\\cdot}\\|\\theta-\\phi_{i}\\|_{\\Sigma_{i}}^{2}$ . ", "page_idx": 6}, {"type": "text", "text": "Assumption 4.1. For subpopulation $i$ , features are independent draws from a zero-mean sub-Gaussian distribution. For a random feature $x\\in\\mathbb{R}^{d}$ and for any $u\\in\\mathbb{R}^{d}$ , such that $\\|u\\|_{\\Sigma_{i}}=1$ , $u^{\\top}x\\in\\mathbb{R}$ is sub-Gaussian with variance proxy $\\sigma_{i}^{2}$ .2 ", "page_idx": 6}, {"type": "text", "text": "Assumption 4.2. We assume that the decision to choose between different services happens at a subpopulation level and not an individual level. ", "page_idx": 6}, {"type": "text", "text": "To illustrate Assumption 4, consider the example of a provider that offers personalized services to schools (subpopulations) such as online library resources wherein the service provider queries students about the experience. Each school typically has a considerable number of students, but only a subset of them may actively respond to such queries. Once a school selects the service, it is made available to all students, and the provider could implement a system where students are encouraged to fill out a feedback form after using their service. ", "page_idx": 6}, {"type": "text", "text": "Suppose only $n_{i}$ users from subpopulation $i$ are subscribed to the services. Thus, upon choosing a service parameterized by $\\boldsymbol{\\theta}\\in\\mathbb{R}^{\\dot{d}}$ the provider observes an empirical loss, which is given by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\mathcal{L}}_{i}(\\theta,\\phi_{i})=\\frac{1}{n_{i}}\\sum_{j\\in[n_{i}]}(\\theta^{\\top}x_{i}^{j}-y_{i}^{j})^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\{(x_{i}^{j},y_{i}^{j})\\}_{j\\in[n_{i}]}$ are private unknown features and scores of the users. We stress that the service gets to see the value of the user loss function at the deployed $\\theta$ (bandit feedback), but not the features of each subpopulation. ", "page_idx": 6}, {"type": "text", "text": "Assumption 4.3. The number of users from each subpopulation is greater than the dimension of the linear predictor, i.e. $n_{i}\\geq d$ for all $i\\in[N]$ . ", "page_idx": 6}, {"type": "text", "text": "In this setting, given a set of services parameterized by $\\Theta_{t-1}=\\{\\theta_{1},\\ldots,\\theta_{t-1}\\}$ the Steps 5-6 of Algorithm 1 proceed with these finite sample averages $\\begin{array}{r}{\\widehat{\\mathcal{L}}_{i}(\\Theta_{t-1},\\phi_{i})=\\operatorname*{min}_{j\\in[t-1]}\\widehat{\\mathcal{L}}_{i}(\\theta_{j},\\phi_{i})}\\end{array}$ . In ", "page_idx": 6}, {"type": "image", "img_path": "HSJOt2hyDf/tmp/0f003065e41d1f731cc12b3ea497f32ed190f05f7754345e65030c6fed1add93.jpg", "img_caption": ["(a) Avg. loss (all Census groups) (b) Avg. loss (worst demographic) (c) Avg. excess error (ML10M task) Figure 1: Fig. 1a and1b show the performance of various user selection strategies on the travel time prediction task on the Census data. Notably, our findings reveal that the greedy and epsilon-greedy baselines exhibit strong performance for $k<10$ . However, as the value of $k$ grows, these strategies prove myopic, with random sampling surpassing their effectiveness. AcQUIre and Fair AcQUIre consistently emerge as the two best baselines for both tasks. Fig. 1c presents the average excess error for the movie recommendation task. Remarkably, the greedy algorithm demonstrates efficacy when $k$ is small. Epsilon-greedy, employing an explore-vs-exploit approach, successfully overcomes myopic tendencies. Nevertheless, AcQUIre continues to be the best baseline for data collection. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Step 7, multiple ways can be adopted by the provider to estimate $\\phi_{i}$ . Users can be given incentives to provide a batch of feature score pairs, or gradient free methods can be used to estimate the optimal solution to the regression problem. The generalization guarantee of Algorithm 1 to the total expected loss is stated below. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.4. Suppose users belong to $N$ subpopulations satisfying Assumptions 4.1, 4.2, and 4.3. Let $\\{n_{i}\\}_{i\\in[N]}$ denote the number of samples per subpopulation. Let $\\Theta_{k}$ be the output of Algorithm 1 using finite sample estimates $\\widehat{\\mathcal{L}}_{i}(\\cdot,\\phi_{i})$ and $\\Theta_{\\mathrm{OPT}}$ be the optimal solution of the expected loss. Then, for any $\\epsilon\\,\\in\\,(0,1)$ , i $\\begin{array}{r}{{}^{c}\\,n_{i}\\,=\\,\\Omega\\big(\\frac{\\sigma_{i}^{4}\\sqrt{N}\\log\\big(2/\\delta\\big)}{\\epsilon^{2}}\\big)}\\end{array}$ for all $i\\,\\in\\,[N]$ , the following inequality holds with probability at least $1-\\delta$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\cdots\\stackrel{\\mathrm{\\Large~w~r~i~}}{\\cdots}\\stackrel{\\mathrm{\\Large~w~r~}\\,+\\,-\\,1-\\epsilon}{\\cdots}\\stackrel{\\mathrm{\\Large~w~\\cdots~}}{\\longrightarrow}\\,\\left.\\right\\rangle,\\quad w i t h\\;\\widehat{\\Sigma}_{i}=\\frac{1}{n_{i}}\\sum_{l\\in[n_{i}]}x_{i}^{l}(x_{i}^{l})^{\\top},\\;\\widehat{\\Sigma}_{j}=\\frac{1}{n_{j}}\\sum_{l\\in[n_{j}]}x_{j}^{l}(x_{j}^{l})^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The proof is presented in Appendix F; we provide a brief overview here. We apply the Chernoff bound to the difference between the empirical loss and expected loss. Note that $(i)$ even if the same set of services are provided, the loss minimizing service for the empirical loss may be different from the expected loss for any subpopulation, and $(i i)$ the optimal set of services for the total empirical loss and the total expected loss are different. Handling these carefully, and utilizing Theorem 3.1 concludes the proof. ", "page_idx": 7}, {"type": "text", "text": "Remark 4.5. Note that ${\\widehat{\\Sigma}}_{i}$ is the empirical feature covariance of subpopulation $i$ . The term $c_{i j}$ captures the alignment between two subpopulation\u2019s loss geometry, and here is equal to half of the minimum generalized eigenvalue of the empirical feature covariances of the the respective subpopulations. This quantity is the largest constant satisfying Assumption 2.2 (cf. Appendix F). ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We empirically demonstrate3 the beneftis of our algorithm on a commute time prediction task based on 2021 US Census data4 and a semi-synthetic movie recommendation task on the MovieLens10M dataset. In each task the multi-service provider has initially no access to data. Our goal is to evaluate the effectiveness of the iterative data collection and service initialization procedure in comparison to established baselines. Below we first describe both the tasks and then discuss the baselines we consider for our evaluation. ", "page_idx": 7}, {"type": "text", "text": "Census Data. We consider the task of predicting daily work commute times, based on 2021 US census data from FOLKTABLES [13]. We illustrate a potential use case: the provider is a transport authority offering services in the form of personalized podcasts. If the duration of a service is similar to the commute time of a user, that user will be able to consume the media while travelling to work. Hence an accurate prediction of the commute time may be useful in providing services tailored to the users. ", "page_idx": 7}, {"type": "text", "text": "The dataset has $N=7025$ subpopulations defined by area zip codes. We use $k$ linear predictors (as services) with user features (education, income, transportation mode, age etc). Refer to Appendix G for details on data pre-processing. The features and commute times are unknown a priori to the provider. At time step $t\\leq k$ , suppose that the provider offers a set of services parameterized by $\\Theta_{t-1}$ . The provider observes the losses (squared prediction error of their commute times as discussed in Section 4) across different subpopulations $\\{\\widehat{\\mathcal{L}}_{i}(\\Theta_{t-1},\\phi_{i})\\}_{i\\in[N]}$ . Then the provider selects a subpopulation $l\\,\\in\\,[N]$ , to get the user feature and commute time data and then estimates $\\phi_{l}$ via least squares regression. This selection can be done via our proposed method or one of the baseline strategies discussed later. The provider then centers its next service on $\\phi_{l}$ and updates the list of offered services $\\Theta_{t}=\\Theta_{t-1}\\cup\\phi_{l}$ . Note that in this process the provider only observes features and commute times of users in the $k$ selected subpopulations and $k\\ll N$ . In Figure 2 we compare runtimes, and observe that even with 1 billion users, AcQUIre takes $300\\;\\mathrm{sec}$ , whereas the greedy and epsilon greedy methods take $>10^{5}$ sec even for 10 million users. With 5000 services, AcQUIre takes $<900$ secs, whereas the runtimes for greedy and epsilon greedy are in the range of $10^{5}$ secs. ", "page_idx": 8}, {"type": "image", "img_path": "HSJOt2hyDf/tmp/b67c838435a14656b0ede084290f64f741ca1f9727b18d0e07fcc71da8288fc2.jpg", "img_caption": ["Figure 2: Runtimes for AcQUIre and baselines as number of users $(N)$ and services $(K)$ vary. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Movie Recommendations. We conduct a semi synthetic experiment based on the widely used Movielens10M data set [19] containing 10000054 ratings across 10681 movies by 71567 viewers. We hold out the top $m=200$ movies and pre-process the data set to divide viewers into $N=1000$ user subpopulations based on similarity of their ratings on the remaining movies (cf. Appendix G). Our goal is to evaluate the generalization performance of the baselines to viewers that the provider has never interacted with before. Thus during the service initialization phase, only half of the users in each of the $N$ subpopulations interact with the provider (train set), and we evaluate the performance of the algorithm by the loss incurred by the initialized services on data of the other half of the users that no prior interaction with the provider (test set). For subpopulation $i\\in[N]$ , the solution to the following optimization problem denotes the user and item embedding: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(U_{i},\\phi_{i})=\\arg\\operatorname*{min}_{(U,\\phi)\\in\\mathbb{R}^{n_{i}\\times d}\\times\\mathbb{R}^{d\\times m}}\\|(U\\phi-r_{i})_{\\Omega_{i}^{\\mathrm{train}}}\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $r_{i}$ is the true user ratings and $\\Omega_{i}^{\\mathrm{train}}$ is the list of movies rated by the users in subpopulation $i$ . Since $r_{i}$ is a sparse matrix we consider the prediction error only on the movies they rated, i.e. $\\Omega_{i}^{\\mathrm{train}}$ . User loss $/$ dissatisfaction for a recommendation model, parameterized by $\\theta\\in\\mathbb{R}^{d\\times m}$ , is captured by the excess error, namely ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathcal{L}_{i}(\\theta,\\phi_{i})=\\lVert(U_{i}\\theta-r_{i})_{\\Omega_{i}^{\\mathrm{train}}}\\rVert_{2}^{2}-\\lVert(U_{i}\\phi_{i}-r_{i})_{\\Omega_{i}^{\\mathrm{train}}}\\rVert_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "This value typically indicates how unhappy users are with the suggested movies with respect to their preferred movies. The provider initially doesn\u2019t know the user ratings. At time step $t\\leq k$ , suppose the provider offers a set of recommendation models $\\Theta_{t-1}$ . Users choose the service with the best recommendations and the provider observes the losses across different subpopulations $\\{\\mathcal{L}_{i}(\\Theta_{t-1},\\phi_{i})\\}_{i\\in[N]}$ . Then the provider selects a subpopulation $l\\in[N]$ to estimate $\\phi_{l}$ via (5). This selection can be done via our proposed method or one of the baseline strategies discussed below. The provider then centers its next model on $\\phi_{l}$ and updates the list of offered models $\\Theta_{t}=\\Theta_{t-1}\\cup\\phi_{l}$ . In this process the provider only observes movie ratings of the users in the $k$ selected subpopulations. denoted by Once the services are initialized we evaluate the performance on the movies rated in the test set $\\{\\Omega_{1}^{\\mathrm{test}},\\ldots,\\Omega_{N}^{\\mathrm{test}}\\}$ . ", "page_idx": 8}, {"type": "text", "text": "Baselines. Both our tasks iterate through the steps of observing User Behavior, User Selection to gather data, designing New Service to update set of offered services. Through our experiments we wish to empirically evaluate different User Selection strategies with respect to AcQUIre (line ", "page_idx": 8}, {"type": "text", "text": "6 in Algorithm 1). The different user selection strategies result in the following baselines: (i) Random: $P(l\\;=\\;i)\\;=\\;1/n$ , (ii) Greedy: $l\\;=\\;\\mathrm{argmax}_{i\\in[n]}\\:{\\mathcal L}_{i}(\\Theta_{t-1},\\phi_{i})$ , (iii) Epsilon Greedy: $l=\\operatorname*{argmax}_{i\\in[n]}(\\mathcal{L}_{i}(\\Theta_{t-1},\\phi_{i})+\\epsilon_{i})$ where $\\epsilon_{1},\\ldots,\\epsilon_{n}$ denotes zero mean i.i.d. noise. Given that the Census Dataset comprises various racial demographic groups of varying sizes (with the smallest group being ten times smaller than the largest group), and considering our interest in the fairness objective (4), we explore incorporating these size imbalances into our algorithms. Consequently, we introduce three additional baselines, wherein the selection criteria are scaled by the corresponding group sizes: (iv) Balanced Random, (v) Balanced Greedy, and (vi) Balanced Epsilon Greedy. We benchmark them against Fair AcQUIre (Algorithm 2) which has guarantees as as stated in Theorem 3.2. ", "page_idx": 9}, {"type": "text", "text": "Evaluation: Each algorithm is run for 500 initialization seeds, the averages are reported in Figure 1. ", "page_idx": 9}, {"type": "text", "text": "Runtimes: We compare the runtimes of AcQUIre with the baselines, and study the affect of the number of users $(N)$ and number of services $(K)$ in Figure 2. We find the runtimes of AcQUIre to be of the similar order of magnitude of random initialization, meanwhile performing much better than random, and the much slower greedy and epislon greedy initialization schemes. ", "page_idx": 9}, {"type": "text", "text": "Impact of Initialization: Once a set of services are initialized, with more user interactions, the provider updates the services on new data to improve the quality (indicated by the reduction in total loss). To evaluate the importance of initialization, we conducted experiments using two different optimization algorithms: (i) Generalized $\\boldsymbol{\\mathrm{k}}$ -means: The services are iteratively updated by training each service on the current group of subpopulations selecting it. After updating the service parameters, the subpopulations reselect their best service. This process repeats until convergence. (ii) Multiplicative weights update [12]: Similar to k-means, but each subpopulation can have users choosing different services simultaneously. Both generalized $\\mathbf{k}$ -means and the multiplicative weights update guarantee that the total loss reduces over time [12]. ", "page_idx": 9}, {"type": "text", "text": "In our experiments, we initialize a set of services using AcQUIre and other baseline methods, then let both optimization algorithms run until convergence. We plot the total loss versus the number of iterations (Figure 3). Our results demonstrate that AcQUIre leads to: (1) faster convergence, and (2) lower final loss (initializing with AcQUIre converges to lower losses; other initialization schemes are prone to being stuck in suboptimal local minima). These findings highlight the significance of a robust initialization strategy. By starting with a better initial configuration, the optimization algorithms can more effectively reach higher quality solutions. ", "page_idx": 9}, {"type": "image", "img_path": "HSJOt2hyDf/tmp/275d047a8e972b589b5356c8e7f3afe6dc487980bc9776be417074da26743373.jpg", "img_caption": ["Figure 3: We study the importance of initialization in both the convergence rate and quality of converged solution of optimization algorithms. We find AcQUIre converges both faster and to a lower total loss across optimization methods (kmeans and multiplicative weights) as well as datasets. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We study the problem of initializing services for a provider catering to a user base with diverse preferences. We address the challenges of unknown user preferences, only bandit (zeroth-order) feedback from the losses, and the non-convexity of the optimization problem, by proposing an algorithm that designs services by adaptively querying data from a small set of users. We also consider the fairness aspect of such design in human centric applications. Our proposed algorithm has theoretical guarantees on both the average and fair loss objectives. There are open questions relating to quantifying the robustness of the proposed initialization algorithm to noisy observations, perturbations, or outliers in the finite sample case when the feature distribution is heavy-tailed, which is a direction for future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Dean, Fazel, Morgenstern and Ratliff were supported in part by NSF CCF-AF 2312774/2312775. Additionally, Morgenstern\u2019s work was supported by NSF CCF 2045402. Dean\u2019s research was supported by a gift from Wayfair, a LinkedIn research award, and NSF OAC 2311521. Ratliff\u2019s research was supported by NSF CNS 1844729 and Office of Naval Research YIP Award N000142012571. Fazel was supported in part by awards NSF TRIPODS II 2023166, CCF 2007036, CCF 2212261. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "[1] Generalized principal component analysis (GPCA). IEEE Trans. on Pattern Analysis and Machine Intelligence, (12):1945\u2014-1959, 2005.   \n[2] Sara Ahmadian, Ashkan Norouzi-Fard, Ola Svensson, and Justin Ward. Better guarantees for k-means and euclidean k-median by primal-dual algorithms. SIAM Journal on Computing, 49 (4):FOCS17\u201397, 2019.   \n[3] Daniel Aloise, Amit Deshpande, Pierre Hansen, and Preyas Popat. Np-hardness of euclidean sum-of-squares clustering. Machine learning, 75(2):245\u2013248, 2009.   \n[4] Preeti Arora, Shipra Varshney, et al. Analysis of $\\boldsymbol{\\mathrm{k}}$ -means and k-medoids algorithm for big data. Procedia Computer Science, 78:507\u2013512, 2016.   \n[5] David Arthur and Sergei Vassilvitskii. K-means++ the advantages of careful seeding. In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, pages 1027\u20131035, 2007. [6] Avinandan Bose, Arunesh Sinha, and Tien Mai. Scalable distributional robustness in a class of non-convex optimization with guarantees. Advances in Neural Information Processing Systems, 35:13826\u201313837, 2022.   \n[7] Avinandan Bose, Simon Shaolei Du, and Maryam Fazel. Offline multi-task transfer rl with representational penalization. arXiv preprint arXiv:2402.12570, 2024.   \n[8] Gregory Canal, Blake Mason, Ramya Korlakai Vinayak, and Robert Nowak. One for all: Simultaneous metric and preference learning over multiple users. arXiv e-prints, pages arXiv\u2013 2207, 2022.   \n[9] Kurtland Chua, Qi Lei, and Jason D. Lee. How fine-tuning allows for effective meta-learning. In Proc. of Advances in Neural Information Processing Systems, volume 34, 2021.   \n[10] G\u00e9rard Cornu\u00e9jols, George Nemhauser, and Laurence Wolsey. The uncapicitated facility location problem. Technical report, Cornell University Operations Research and Industrial Engineering, 1983.   \n[11] Sanjoy Dasgupta. The hardness of k-means clustering. Department of Computer Science and Engineering, University of California, San Diego, 2008.   \n[12] Sarah Dean, Mihaela Curmei, Lillian J Ratliff, Jamie Morgenstern, and Maryam Fazel. Multi-learner risk reduction under endogenous participation dynamics. arXiv preprint arXiv:2206.02667, 2022.   \n[13] Frances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt. Retiring adult: New datasets for fair machine learning. Advances in Neural Information Processing Systems, 34:6478\u20136490, 2021.   \n[14] Wolfgang F\u00f6rstner and Boudewijn Moonen. A metric for covariance matrices. Geodesy-the Challenge of the 3rd Millennium, pages 299\u2013309, 2003.   \n[15] Mehrdad Ghadiri, Samira Samadi, and Santosh Vempala. Socially fair k-means clustering. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 438\u2013448, 2021.   \n[16] Benyamin Ghojogh, Fakhri Karray, and Mark Crowley. Eigenvalue and generalized eigenvalue problems: Tutorial. arXiv preprint arXiv:1903.11240, 2019.   \n[17] Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework for clustered federated learning. Advances in Neural Information Processing Systems, 33: 19586\u201319597, 2020.   \n[18] Tony Ginart, Eva Zhang, Yongchan Kwon, and James Zou. Competing ai: How does competition feedback affect machine learning? In International Conference on Artificial Intelligence and Statistics, pages 1693\u20131701. PMLR, 2021.   \n[19] F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. Acm transactions on interactive intelligent systems (tiis), 5(4):1\u201319, 2015.   \n[20] Tatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness without demographics in repeated loss minimization. In International Conference on Machine Learning, pages 1929\u20131938. PMLR, 2018.   \n[21] Christoph Hirnschall, Adish Singla, Sebastian Tschiatschek, and Andreas Krause. Learning user preferences to incentivize exploration in the sharing economy. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.   \n[22] Jean Honorio and Tommi Jaakkola. Tight bounds for the expected risk of linear classifiers and pac-bayes finite-sample guarantees. In Artificial Intelligence and Statistics, pages 384\u2013392. PMLR, 2014.   \n[23] Nicolas Hug. Surprise: A python library for recommender systems. Journal of Open Source Software, 5(52):2174, 2020. doi: 10.21105/joss.02174. URL https://doi.org/10.21105/ joss.02174.   \n[24] Tapas Kanungo, David M Mount, Nathan S Netanyahu, Christine D Piatko, Ruth Silverman, and Angela Y Wu. A local search approximation algorithm for $\\mathbf{k}$ -means clustering. In Proceedings of the eighteenth annual symposium on Computational geometry, pages 10\u201318, 2002.   \n[25] Weihao Kong, Raghav Somani, Zhao Song, Sham Kakade, and Sewoong Oh. Meta-learning for mixed linear regression. In International Conference on Machine Learning, pages 5394\u20135404. PMLR, 2020.   \n[26] Silvio Lattanzi and Christian Sohler. A better k-means $^{++}$ algorithm via local search. In International Conference on Machine Learning, pages 3662\u20133671. PMLR, 2019.   \n[27] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges, methods, and future directions. IEEE signal processing magazine, 37(3):50\u201360, 2020.   \n[28] Stuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28 (2):129\u2013137, 1982.   \n[29] Konstantin Makarychev, Aravind Reddy, and Liren Shan. Improved guarantees for k-means++ and k-means++ parallel. Advances in Neural Information Processing Systems, 33:16142\u201316152, 2020.   \n[30] Yishay Mansour, Mehryar Mohri, Jae Ro, and Ananda Theertha Suresh. Three approaches for personalization with applications to federated learning. arXiv preprint arXiv:2002.10619, 2020.   \n[31] Saeed Masoudnia and Reza Ebrahimpour. Mixture of experts: a literature survey. The Artificial Intelligence Review, 42(2):275, 2014.   \n[32] Adhyyan Narang, Omid Sadeghi, Lillian J Ratliff, Maryam Fazel, and Jeff Bilmes. Online submodular+ supermodular (bp) maximization with bandit feedback. arXiv preprint arXiv:2207.03091, 2022.   \n[33] Rafail Ostrovsky, Yuval Rabani, Leonard J Schulman, and Chaitanya Swamy. The effectiveness of lloyd-type methods for the k-means problem. Journal of the ACM (JACM), 59(6):1\u201322, 2013.   \n[34] Felix Sattler, Klaus-Robert M\u00fcller, and Wojciech Samek. Clustered federated learning: Modelagnostic distributed multitask optimization under privacy constraints. IEEE transactions on neural networks and learning systems, 32(8):3710\u20133722, 2020.   \n[35] Linqi Song, Cem Tekin, and Mihaela Van Der Schaar. Online learning in large-scale contextual recommender systems. IEEE Transactions on Services Computing, 9(3):433\u2013445, 2014.   \n[36] Jacob Steinhardt, Gregory Valiant, and Moses Charikar. Avoiding imposters and delinquents: Adversarial crowdsourcing and peer prediction. Advances in Neural Information Processing Systems, 29, 2016.   \n[37] Yue Sun, Adhyyan Narang, Ibrahim Gulluk, Samet Oymak, and Maryam Fazel. Towards sample-efficient overparameterized meta-learning. In Proc. of Advances in Neural Information Processing Systems, volume 34, 2021.   \n[38] Gokcan Tatli, Rob Nowak, and Ramya Korlakai Vinayak. Learning preference distributions from distance measurements. In 2022 58th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 1\u20138. IEEE, 2022.   \n[39] Berk Ustun, Yang Liu, and David Parkes. Fairness without harm: Decoupled classifiers with preference guarantees. In International Conference on Machine Learning, pages 6373\u20136382. PMLR, 2019.   \n[40] Roman Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press, 2018.   \n[41] Xueru Zhang, Mohammadmahdi Khaliligarekani, Cem Tekin, and Mingyan Liu. Group retention when using machine learning in sequential decision making: the interplay between user dynamics and fairness. Advances in Neural Information Processing Systems, 32, 2019.   \n[42] Kai Zhong, Prateek Jain, and Inderjit S Dhillon. Mixed linear regression with multiple components. Advances in neural information processing systems, 29, 2016. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A More Discussion on Related Works ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Retention. User retention in machine learning systems is closely related to the decision dynamics between the provider and users studied in the single service setting by Hashimoto et al. [20], Zhang et al. [41] and multiple services setting by Dean et al. [12], Ginart et al. [18]. In settings with multiple sub-populations of users of different types, the question of retention has been explored in parallel with the issue of fairness. These works typically focus on the stability of the dynamics at equilibrium. However, the final outcome is heavily influenced by the initial data the provider has and the initial configuration (partitioning) of the users across the offered services. Our work addresses the impact of initialization on the final outcome with theoretical guarantees. ", "page_idx": 14}, {"type": "text", "text": "Mixture of Experts. This model specialization is also explored in the mixture of experts literature (see, e.g., [31]), which uses multiple \u2018expert\u2019 models to enhance accuracy and robustness, assigning inputs to the most suitable expert based on their features through a gating mechanism. ", "page_idx": 14}, {"type": "text", "text": "Clustering. [29] improved the analysis of [5] to show an approximation ratio $5(2+\\log k)$ and showed a family of instances with $5\\log k$ approximation ratio, thus showing $k$ -means $^{++}$ is tight. Recent works in clustering [24, 2, 26] provide a constant approximation ratio, and although they are polynomial time algorithms, these methods are data inefficient and rely on knowing the points $a$ priori. ", "page_idx": 14}, {"type": "text", "text": "Facility Location Problem. Our algorithm has some resemblances with the facility location problem where there are $n$ users, and a provider can set up at most $k$ facilities at one of $m$ candidate locations (also known as the $k$ -medoids objective [4]). One key difference is the provider can choose from $m$ predecided locations, compared to our setting where the optimization space is infinite. However our algorithm initializes only at $k$ of the $n$ user preferences, hence it can be viewed as the candidate locations simply being the user preferences and thus $m\\,=\\,n$ . A typical greedy algorithm for the $k$ -medoids objective proceeds by evaluating the marginal decrement in total loss for all possible candidates and selects the candidate with maximal loss reduction. Thus (i) the algorithm would need to know all $\\{\\phi_{1},\\ldots,\\phi_{n}\\}$ apriori, and (ii) deploy $n$ services to obtain all $n^{2}$ function evaluations $\\mathcal{L}_{j}(\\phi_{i},\\phi_{j})$ , $\\forall i,j\\in[n]$ . This is infeasible in applications like online recommendation systems where $n$ is very large and typically a provider has the capacity to deploy only $k\\ll n$ services. Hence a $k$ -medoids like objective will not be reasonable in our setup with incomplete information. ", "page_idx": 14}, {"type": "text", "text": "[6] studied the case where the provider had prior access to only $N<n$ users\u2019 utility functions before deploying services and the goal was to minimize the worst case total (over all $n$ users) loss. However their results assume they can solve a computationally hard mixed integer program optimally. ", "page_idx": 14}, {"type": "text", "text": "Preference Learning. Given a fixed set of items or services, [38] focus on learning preference distributions of users. [8] extend this to the setting where the user losses are given by an identical metric, and they learn both preferences and the metric efficiently. Our setting focuses on the design of services rather than learning preferences over a fixed set of services, and we also allow each user to have a different loss function and loss geometry. ", "page_idx": 14}, {"type": "text", "text": "B Motivating Example ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We conceptualize \u2018services\u2019 broadly, encompassing both sets of independent learners\u2014such as various service providers collaborating on initialization\u2014and single learners with multiple models, like companies with diverse platforms or multi-model servers in federated learning settings. This initialization process is further relevant for social planners aiming to facilitate the coordination of service initialization. ", "page_idx": 14}, {"type": "text", "text": "Netflix Example. All details are borrowed from the actual working of Netflix (refer https: //recoai.net/netflix-recommendation-system-how-it-works/ for a detailed description and more references). The Netflix homepage displays several rows of suggestions. Each row is a collection of movies and the rows are arranged from top to bottom in decreasing order of likelihood that the user will pick movies to watch. The user has a complete choice of which row to select a movie to watch from. The different rows are generated by different underlying recommendation models parametrized by $\\theta_{1},\\dots,\\theta_{K}$ . Our model abstracts these rows out as services. Netflix maintains $K=1300$ such different recommendation models. Users\u2019 decision of watching a movie from the available rows of recommendations on the Netfilx homepage informs the provider (Netfilx) which of the rows (parameterized recommendation model) the user prefers most. This let\u2019s them finetune their recommendation models. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "C Mappings satisfying Assumptions 2.1 and 2.2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this appendix, we provide more details on examples that satisfy Assumptions 2.1 and 2.2. For the squared error loss for linear predictors, we refer the reader to Lemma F.1 for a derivation of $c_{i j}$ . The remainder of the example classes are detailed below. ", "page_idx": 15}, {"type": "text", "text": "C.1 Huber Loss ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Recall that the Huber loss is defined by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{i}(\\theta,\\phi_{i})=\\left\\{\\frac{1}{2}\\|\\theta-\\phi_{i}\\|^{2}\\right.\\quad\\quad\\quad\\quad\\left.\\mathrm{if}\\,\\,\\|\\theta-\\phi_{i}\\|\\leq\\delta,}\\\\ {\\delta(\\|\\theta-\\phi_{i}\\|-\\frac{1}{2}\\delta)\\quad\\mathrm{otherwise}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that the Huber loss varies quadratically when the error $\\lVert{\\boldsymbol{\\theta}}-{\\boldsymbol{\\phi}}_{i}\\rVert\\leq\\delta$ and linearly otherwise.   \nWe will refer to these as quadratic and linear regime of the Huber loss subsequently. ", "page_idx": 15}, {"type": "text", "text": "We divide the derivation into 2 subcases. ", "page_idx": 15}, {"type": "text", "text": "Case 1. $\\lVert{\\boldsymbol{\\theta}}-{\\boldsymbol{\\phi}}_{i}\\rVert\\leq2\\delta$ . Within Case 1, there are several subcases to consider. We analyze each one separately. ", "page_idx": 15}, {"type": "text", "text": "\u2022 At least one of the terms on the right is linear: Under the condition of Case 1, ${\\mathcal{L}}_{i}(\\theta,\\phi_{i})\\leq$ ${\\textstyle\\frac{3}{2}}\\delta^{2}$ . Note that the Huber loss is monotonicly increasing in the prediction error. The function value in the linear in $\\lVert\\boldsymbol{\\theta}-\\boldsymbol{\\phi}_{i}\\rVert$ is at least ${\\textstyle\\frac{1}{2}}\\delta^{2}$ . Thus, if one of the terms in $\\mathcal{L}_{j}(\\theta,\\phi_{j})+\\mathcal{L}_{j}(\\phi_{i},\\phi_{j})$ were in the linear regime, the $\\begin{array}{r}{\\bar{\\mathcal{L}}_{j}(\\theta,\\phi_{j})+\\mathcal{L}_{j}(\\phi_{i},\\phi_{j})\\geq\\frac{1}{2}\\delta^{2}}\\end{array}$ and $\\begin{array}{r}{\\frac{1}{3}\\mathcal{L}_{i}(\\theta,\\phi_{i})\\leq\\mathcal{L}_{j}(\\theta,\\phi_{j})+\\mathcal{L}_{j}(\\phi_{i},\\phi_{j}).}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "\u2022 Both terms on the right are quadratic: In this sub-case, it needs to be shown when both terms of $\\mathcal{L}_{j}(\\theta,\\phi_{j})+\\bar{\\mathcal{L}}_{j}(\\phi_{i},\\bar{\\phi_{j}})$ are in the quadratic regime the bound still holds. By the triangle inequality on norms, we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\theta-\\phi_{i}\\|\\leq\\|\\theta-\\phi_{j}\\|+\\|\\phi_{i}-\\phi_{j}\\|.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using the power mean inequality, namely ", "page_idx": 15}, {"type": "equation", "text": "$$\na\\leq b+c\\implies a^{2}\\leq2(b^{2}+c^{2}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "on the triangle inequality, we deduce the following implication: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\theta-\\phi_{i}\\|\\leq\\|\\theta-\\phi_{j}\\|+\\|\\phi_{i}-\\phi_{j}\\|\\implies\\frac{1}{2}\\|\\theta-\\phi_{i}\\|^{2}\\leq\\|\\theta-\\phi_{j}\\|^{2}+\\|\\phi_{i}-\\phi_{j}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "If the $\\mathcal{L}_{i}(\\theta,\\phi_{i})$ is quadratic, we have that $\\begin{array}{r}{\\frac{1}{2}\\mathcal{L}_{i}(\\theta,\\phi_{i})\\leq\\mathcal{L}_{j}(\\theta,\\phi_{j})+\\mathcal{L}_{j}(\\phi_{i},\\phi_{j})}\\end{array}$ . Now, let $\\mathcal{L}_{i}(\\theta,\\phi_{i})$ be linear. Then, we deduce that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}_{j}(\\theta,\\phi_{j})+\\mathcal{L}_{j}(\\phi_{i},\\phi_{j})=\\frac{1}{2}(\\|\\theta-\\phi_{j}\\|^{2}+\\|\\phi_{i}-\\phi_{j}\\|^{2})\\geq\\frac{1}{4}\\|\\theta-\\phi_{i}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since the equation $\\begin{array}{r}{\\frac{1}{4}x^{2}-\\frac{1}{3}x+\\frac{1}{6}=0}\\end{array}$ has no real roots, the expression is always positive. Hence, by substituting x = \u2225\u03b8\u2212\u03b4\u03d5i\u2225, we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1}{4}\\|\\theta-\\phi_{i}\\|^{2}\\geq\\frac{1}{3}\\delta(\\|\\theta-\\phi_{i}\\|-\\frac{1}{2}\\delta)=\\frac{1}{3}\\mathcal L_{i}(\\theta,\\phi_{i}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, we deduce that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1}{3}\\mathcal{L}_{i}(\\theta,\\phi_{i})\\leq\\mathcal{L}_{j}(\\theta,\\phi_{j})+\\mathcal{L}_{j}(\\phi_{i},\\phi_{j}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Case 2. $\\lVert{\\boldsymbol{\\theta}}-{\\boldsymbol{\\phi}}_{i}\\rVert>2\\delta$ . First, observe that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac13\\mathcal L_{i}(\\theta,\\phi_{i})=\\frac13\\delta(\\|\\theta-\\phi_{i}\\|-\\frac12\\delta)}\\\\ &{\\quad\\quad\\quad\\quad=\\frac13\\delta\\|\\theta-\\phi_{i}\\|+\\frac16\\delta\\|\\theta-\\phi_{i}\\|-\\frac16\\delta\\|\\theta-\\phi_{i}\\|-\\frac16\\delta^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\frac13\\delta\\|\\theta-\\phi_{i}\\|+\\frac16\\delta\\|\\theta-\\phi_{i}\\|-\\frac13\\delta^{2}-\\frac16\\delta^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\frac12\\delta\\|\\theta-\\phi_{i}\\|-\\frac12\\delta^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the second to last inequality follows from the fact that $\\lVert\\theta-\\phi_{i}\\rVert>2\\delta$ . By the triangle inequality on norms, we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|\\theta-\\phi_{i}\\|\\leq\\|\\theta-\\phi_{j}\\|+\\|\\phi_{i}-\\phi_{j}\\|.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In turn, this implies either $\\lVert\\boldsymbol{\\theta}-\\boldsymbol{\\phi}_{j}\\rVert$ or $\\lVert\\phi_{i}-\\phi_{j}\\rVert$ is greater than $\\textstyle{\\frac{1}{2}}\\|\\theta-\\phi_{i}\\|$ . Without loss of generality, assume $\\begin{array}{r}{\\|\\theta-\\phi_{j}\\|\\geq\\frac{1}{2}\\|\\theta-\\phi_{i}\\|\\geq\\delta.}\\end{array}$ . Thus ", "page_idx": 16}, {"type": "equation", "text": "$$\nL_{j}(\\theta,\\phi_{j})=\\delta(\\|\\theta-\\phi_{j}\\|-\\frac{1}{2}\\delta)\\geq\\frac{1}{2}\\delta\\|\\theta-\\phi_{i}\\|-\\frac{1}{2}\\delta^{2}\\geq\\frac{1}{3}\\mathcal L_{i}(\\theta,\\phi_{i}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $\\mathcal{L}_{j}\\big(\\phi_{i},\\phi_{j}\\big)\\geq0$ , we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1}{3}\\mathcal{L}_{i}(\\theta,\\phi_{i})\\leq\\mathcal{L}_{j}(\\theta,\\phi_{j})+\\mathcal{L}_{j}(\\phi_{i},\\phi_{j}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "C.2 The normalized cosine distance ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Recall that the normalized cosine distance is given by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{L}_{i}(\\theta,\\phi_{i})=1-\\theta^{\\top}\\phi_{i}\\quad\\mathrm{where}\\quad\\|\\theta\\|_{2}=\\|\\phi_{i}\\|_{2}=1.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n1-\\theta^{\\top}\\phi_{i}=\\frac{1}{2}(\\|\\theta\\|_{2}^{2}+\\|\\phi_{i}\\|_{2}^{2}-2\\theta^{\\top}\\phi_{i})=\\frac{1}{2}\\|\\theta-\\phi\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Assumption 2.2 is satisfied with $\\begin{array}{r}{c_{i j}\\;=\\;\\frac{1}{2}}\\end{array}$ by using triangle inequality followed by power mean inequality. ", "page_idx": 16}, {"type": "text", "text": "C.3 Mahalanobis distance ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Consider the Mahalanobis distance which is defined by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{i}(\\boldsymbol{\\theta},\\boldsymbol{\\phi}_{i})=\\|\\boldsymbol{\\theta}-\\boldsymbol{\\phi}_{i}\\|_{\\Sigma_{i}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\Sigma_{i}$ is full rank and $\\begin{array}{r}{c_{i j}=\\operatorname*{min}\\bigr\\{\\lambda_{\\operatorname*{min}}\\mathopen{}\\mathclose\\bgroup\\left(\\sum_{i},\\sum_{j}\\aftergroup\\egroup\\right),\\frac{1}{\\lambda_{\\operatorname*{min}}\\mathopen{}\\mathclose\\bgroup\\left(\\sum_{i},\\sum_{j}\\aftergroup\\egroup\\right)}}\\end{array}$ . The derivation is similar to proof of Lemma F.1. ", "page_idx": 16}, {"type": "text", "text": "D Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We note that the parameters of the services initialized by Algorithm 1 are a subset of the users\u2019 unknown preferences. This allows us to define the notion of covering. ", "page_idx": 16}, {"type": "text", "text": "Definition D.1. Let $\\Theta\\subset\\{\\phi_{1},\\ldots,\\phi_{n}\\}$ be a set of services. A cluster $B\\in{\\mathcal{C}}(\\Theta_{\\mathrm{OPT}})$ is said to be covered by $\\Theta$ if there exists $i\\in{\\cal B}$ such that $\\phi_{i}\\in\\Theta$ . If no such $i\\in{\\cal B}$ exists, then the cluster $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ is said to be uncovered. ", "page_idx": 16}, {"type": "text", "text": "The proof idea from here on is to show that there exists an approximation ratio $K_{\\mathrm{OPT}}$ for the covered clusters, and is shown in Lemma D.2 and D.3. ", "page_idx": 16}, {"type": "text", "text": "Lemma D.2. Let $B\\in{\\mathcal{C}}(\\Theta_{\\mathrm{OPT}})$ be an arbitrary cluster in $\\mathcal{C}(\\Theta_{\\mathrm{OPT}})$ , and let $\\theta\\in\\ensuremath{\\mathbb{R}}^{d}$ be a service centered on the preference of a user $j$ chosen uniformly at random from $[n]$ . The expected loss of users in $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ , conditioned on $j\\in\\mathcal{B}$ , satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta}[\\mathcal{L}(\\{\\theta=\\phi_{j}\\},\\mathcal{B})\\mid j\\in\\mathcal{B}]\\le\\left(\\operatorname*{max}_{j\\in\\mathcal{B}}\\frac{2}{|\\mathcal{B}|}\\sum_{i}\\frac{1}{c_{i j}}\\right)\\mathcal{L}(\\Theta_{\\mathrm{OPT}},\\mathcal{B}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Since we choose a user from $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ , the conditional probability that we choose some fixed $\\phi_{j}$ as the parameter for the service is precisely $\\begin{array}{r}{P(\\theta=\\phi_{j}\\mid\\theta\\in B)=\\frac{1}{|B|}}\\end{array}$ . Let $\\mathcal{I}(B)$ denote the best service covering all the points in $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ , and then compute ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta}\\big[\\mathcal{L}\\big(\\{\\theta=\\phi_{j}\\},\\mathcal{B}\\big)\\mid j\\in\\mathcal{B}\\big]=\\sum_{j\\in\\mathcal{B}}P\\big(\\theta=\\phi_{j}\\big)\\mathcal{L}\\big(\\theta,\\mathcal{B}\\big)=\\sum_{j\\in\\mathcal{B}}\\frac{1}{|\\mathcal{B}|}\\sum_{i\\in\\mathcal{B}}\\mathcal{L}_{i}\\big(\\phi_{j},\\phi_{i}\\big).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using Assumption 2.2.(ii) with $\\theta=\\mathcal{I}(B)$ , we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\upxi_{\\theta}[\\mathcal{L}(\\{\\theta=\\phi_{j}\\},B)\\mid j\\in B]\\le\\sum_{j\\in B}\\sum_{i\\in B}\\frac{1}{|B|}\\left(\\frac{1}{c_{i j}}\\left(\\mathcal{L}_{j}(\\mathcal{I}(B),\\phi_{j})+\\mathcal{L}_{i}(\\mathcal{I}(B),\\phi_{i})\\right)\\right),}\\\\ &{\\displaystyle=\\sum_{j\\in B}\\left(\\frac{1}{|B|}\\sum_{i\\in B}\\frac{1}{c_{i j}}\\right)\\mathcal{L}_{j}(\\mathcal{I}(B),\\phi_{j})+\\sum_{i\\in B}\\left(\\frac{1}{|B|}\\sum_{j\\in B}\\frac{1}{c_{i j}}\\right)\\mathcal{L}_{i}(\\mathcal{I}(B),\\phi_{i}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Noting that $c_{i j}=c_{j i}$ , by swapping the indices of the second term in the above summand, the second term is identical to the first term. Therefore, we deduce that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta}[\\mathcal{L}(\\{\\theta=\\phi_{j}\\},\\mathcal{B})\\mid j\\in\\mathcal{B}]\\le2\\sum_{j\\in\\mathcal{B}}\\left(\\left(\\frac{1}{|\\mathcal{B}|}\\sum_{i\\in\\mathcal{B}}\\frac{1}{c_{i j}}\\right)\\mathcal{L}_{j}(\\mathcal{J}(\\mathcal{B}),\\phi_{j})\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now, using the fact that $\\begin{array}{r}{\\operatorname*{max}_{j\\in\\mathcal{B}}\\left(\\frac{1}{\\left|\\mathcal{B}\\right|}\\sum_{i\\in\\mathcal{B}}\\frac{1}{c_{i j}}\\right)}\\end{array}$ is a uniform upper bound for the multiplier in the above expression, we bring it outside the summation as a constant. Therefore, we deduce that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta}[\\mathcal{L}(\\{\\theta=\\phi_{j}\\},\\mathcal{B})\\mid j\\in\\mathcal{B}]\\le\\left(\\operatorname*{max}_{j\\in\\mathcal{B}}\\frac{2}{|\\mathcal{B}|}\\sum_{i}\\frac{1}{c_{i j}}\\right)\\mathcal{L}(\\mathcal{I}(\\mathcal{B}),\\mathcal{B}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we have used the fact that $\\begin{array}{r}{\\mathcal{L}(\\mathcal{I}(\\mathcal{B}),\\mathcal{B})=\\sum_{i\\in\\mathcal{B}}\\mathcal{L}_{i}(\\mathcal{I}(\\mathcal{B}),\\phi_{i})}\\end{array}$ . Since $B\\in\\mathcal{C}(\\Theta_{\\mathrm{OPT}})$ , the covering ${\\mathcal{I}}(B)$ is the loss minimizing service among all services in $\\Theta_{\\mathrm{OPT}}$ for all points in $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ . Thus, we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta}[\\mathcal{L}(\\{\\theta=\\phi_{j}\\},\\mathcal{B})\\mid j\\in\\mathcal{B}]\\le\\left(\\operatorname*{max}_{j\\in\\mathcal{B}}\\frac{2}{|\\mathcal{B}|}\\sum_{i}\\frac{1}{c_{i j}}\\right)\\mathcal{L}(\\Theta_{\\mathrm{OPT}},\\mathcal{B}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 17}, {"type": "text", "text": "Lemma D.3. Let $B\\in{\\mathcal{C}}(\\Theta_{\\mathrm{OPT}})$ be an arbitrary cluster in $\\mathcal{C}(\\Theta_{\\mathrm{OPT}})$ , and let $\\Theta_{t}\\subset\\mathbb{R}^{d}$ denote the parameters for a set of preexisting $t$ arbitrary services. Consider a new clustering $\\Theta_{t+1}=\\Theta_{t}\\cup\\theta$ where $\\theta=\\phi_{j}$ is a random service centered on user $j\\in[n]$ selected with probability $P(\\theta=\\phi_{j})\\propto$ $\\mathcal{L}_{j}(\\Theta_{t},\\phi_{j})$ . Then, the expected loss of $\\boldsymbol{\\beta}$ , conditioned on $j\\in\\mathcal B$ , satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta}[\\mathcal{L}(\\Theta_{t}\\cup(\\theta=\\phi_{j}),\\mathcal{B})\\mid j\\in\\mathcal{B}]\\le\\frac{1}{\\operatorname*{min}_{j\\in\\mathcal{B}}\\frac{1}{|\\mathcal{B}|}\\sum_{i\\in B}c_{j i}}\\left(\\operatorname*{max}_{j\\in\\mathcal{B}}\\frac{2}{|\\mathcal{B}|}\\sum_{i}\\frac{1}{c_{i j}}\\right)\\mathcal{L}(\\Theta_{\\mathrm{OPT}},\\mathcal{B}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Given that we are choosing a user from $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ , the conditional probability that we center the new service on some fixed $\\phi_{j}$ is precisely $\\begin{array}{r}{\\mathcal{L}(\\Theta_{t},\\phi_{j})/(\\sum_{i\\in\\mathcal{B}}\\mathcal{L}(\\Theta_{t},\\bar{\\phi_{i}}))}\\end{array}$ . After adding $\\phi_{j}$ to the list of services, a user $i$ will have loss $\\operatorname*{min}\\{\\mathcal{L}_{i}(\\Theta_{t},\\phi_{i}),\\mathcal{L}_{i}(\\Tilde{\\phi_{j}},\\Tilde{\\phi_{i}})\\}$ . Therefore we deduce that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\theta}[\\mathcal{L}(\\Theta_{t}\\cup(\\theta=\\phi_{j}),B)\\mid j\\in B]=\\displaystyle\\sum_{j\\in B}P(\\theta=\\phi_{j}\\mid\\theta\\in B)\\displaystyle\\sum_{i\\in B}\\mathcal{L}_{i}(\\Theta_{t}\\cup\\theta,\\phi_{i}),}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\sum_{j\\in B}\\frac{\\mathcal{L}_{j}(\\Theta_{t},\\phi_{j})}{\\sum_{l\\in B}\\mathcal{L}_{l}(\\Theta_{t},\\phi_{l})}\\displaystyle\\sum_{i\\in B}\\operatorname*{min}\\{\\mathcal{L}_{i}(\\Theta_{t},\\phi_{i}),\\mathcal{L}_{i}(\\phi_{j},\\phi_{i})\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By Assumption $2.2.(i)$ , for any $\\theta\\in\\ensuremath{\\mathbb{R}}^{d}$ we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\nc_{j i}\\mathcal{L}_{j}(\\theta,\\phi_{j})\\leq\\mathcal{L}_{i}(\\theta,\\phi_{i})+\\mathcal{L}_{i}(\\phi_{j},\\phi_{i}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We utilize the fact that given two functions $f,g:\\,\\Theta\\ \\to\\ \\mathbb{R}$ , if $f(\\theta)~\\leq~g(\\theta)~~\\forall\\theta~\\in~\\Theta$ , then $\\begin{array}{r}{\\operatorname*{min}_{\\theta\\in\\Theta}f(\\theta)\\leq\\operatorname*{min}_{\\theta\\in\\Theta}\\bar{g}(\\theta)}\\end{array}$ . Hence, the following implication holds: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\because_{\\beta\\mid\\frac{\\operatorname*{min}}{\\theta\\in\\Theta}}\\mathcal{L}_{j}(\\theta,\\phi_{j})\\leq\\operatorname*{min}_{\\theta\\in\\Theta}\\mathcal{L}_{i}(\\theta,\\phi_{i})+\\mathcal{L}_{i}(\\phi_{j},\\phi_{i})\\implies c_{j i}\\mathcal{L}_{j}(\\Theta_{t},\\phi_{j})\\leq\\mathcal{L}_{i}(\\Theta_{t},\\phi_{i})+\\mathcal{L}_{i}(\\phi_{j},\\phi_{i}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Summing over all $i\\in\\mathcal{B}$ , we get that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}_{j}(\\Theta_{t},\\phi_{j})\\leq\\frac{1}{\\sum_{i\\in B}c_{j i}}\\left(\\sum_{i\\in B}\\mathcal{L}_{i}(\\Theta_{t},\\phi_{i})+\\mathcal{L}_{i}(\\phi_{j},\\phi_{i})\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Applying this to $\\mathcal{L}_{j}(\\Theta_{t},\\phi_{j})$ on the right hand side of (8), we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathfrak{c}_{\\theta}[\\mathcal{L}(\\Theta_{t}\\cup(\\theta=\\phi_{j}),\\mathcal{B})\\mid j\\in\\mathcal{B}]\\le\\sum_{j\\in\\mathcal{B}}\\frac{1}{\\sum_{i\\in B}c_{j i}}\\left(1+\\frac{\\sum_{i\\in B}\\mathcal{L}_{i}(\\phi_{j},\\phi_{i})}{\\sum_{l\\in B}\\mathcal{L}_{l}(\\Theta_{t},\\phi_{l})}\\right)\\sum_{i\\in B}\\operatorname*{min}\\{\\mathcal{L}_{i}(\\Theta_{t},\\phi_{i}),\\mathcal{L}_{i}(\\Theta_{t},\\phi_{i})\\},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $\\operatorname*{min}\\{a,b\\}\\leq a$ , we further deduce that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\theta}\\big[\\mathcal{L}(\\Theta_{t}\\cup(\\theta=\\phi_{j}),B)\\mid j\\in B\\big]\\leq\\displaystyle\\sum_{j\\in B}\\frac{1}{\\sum_{i\\in B}c_{j i}}\\sum_{i\\in B}\\mathcal{L}_{i}(\\phi_{j},\\phi_{i})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\sum_{j\\in B}\\frac{1}{\\sum_{i\\in B}c_{j i}}\\frac{\\sum_{i\\in B}\\mathcal{L}_{i}(\\phi_{j},\\phi_{i})}{\\sum_{l\\in B}\\mathcal{L}_{l}(\\Theta_{t},\\phi_{l})}\\sum_{i\\in B}\\mathcal{L}(\\Theta_{t},\\phi_{i}),}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\sum_{j\\in B}\\frac{2}{\\sum_{i\\in B}c_{i j}}\\sum_{i\\in B}\\mathcal{L}_{i}(\\phi_{j},\\phi_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now, we use the upper bound $\\frac{1}{\\operatorname*{min}_{j\\in B}\\sum_{i\\in B}c_{j i}}$ for the multiplier to get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta}[\\mathcal{L}(\\Theta_{t}\\cup(\\theta=\\phi_{j}),\\mathcal{B})\\mid j\\in\\mathcal{B}]\\le\\frac{2}{\\frac{1}{|\\mathcal{B}|}\\operatorname*{min}_{j\\in\\mathcal{B}}\\sum_{i\\in\\mathcal{B}}c_{j i}}\\sum_{j\\in\\mathcal{B}}\\frac{1}{i\\in\\mathcal{B}}\\frac{\\mathcal{L}_{i}(\\phi_{j},\\phi_{i})}{|\\mathcal{B}|}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that $\\begin{array}{r}{\\sum_{j\\in\\mathcal{B}}\\sum_{i\\in\\mathcal{B}}\\frac{1}{|\\mathcal{B}|}\\mathcal{L}_{i}(\\phi_{j},\\phi_{i})}\\end{array}$ is essentially the loss on choosing users from $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ uniformly randomly and centering the service on the chosen user. Plugging in the expression in (7) from the proof of Lemma D.2, we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta}\\big[\\mathcal{L}\\big(\\Theta_{t}\\cup(\\theta=\\phi_{j}),\\mathcal{B}\\big)\\bigm|j\\in\\mathcal{B}\\big]\\le\\frac{4}{\\underset{j\\in\\mathcal{B}}{\\operatorname*{min}}\\frac{1}{|\\mathcal{B}|}\\sum_{i\\in\\mathcal{B}}c_{j i}}\\left(\\underset{j\\in\\mathcal{B}}{\\operatorname*{max}}\\frac{1}{|\\mathcal{B}|}\\sum_{i\\in\\mathcal{B}}\\frac{1}{c_{i j}}\\right)\\mathcal{L}\\big(\\mathcal{J}(\\mathcal{B}),\\mathcal{B}\\big).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $B\\in{\\mathcal{C}}(\\Theta_{\\mathrm{OPT}})$ , the covering $\\mathcal{I}(B)$ is the loss minimizing service among all available services in $\\Theta_{\\mathrm{OPT}}$ for all points in $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ . Therefore, we deduce that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta}[\\mathcal{L}(\\Theta_{t}\\cup(\\theta=\\phi_{j}),\\mathcal{B})\\mid j\\in\\mathcal{B}]\\le\\frac{4}{\\underset{j\\in\\mathcal{B}}{\\operatorname*{min}}\\,\\frac{1}{|\\mathcal{B}|}\\sum_{i\\in\\mathcal{B}}c_{j i}}\\left(\\underset{j\\in\\mathcal{B}}{\\operatorname*{max}}\\,\\frac{1}{|\\mathcal{B}|}\\sum_{i\\in\\mathcal{B}}\\frac{1}{c_{i j}}\\right)\\mathcal{L}(\\Theta_{\\mathrm{OPT}},\\mathcal{B}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 18}, {"type": "text", "text": "The following Lemma is an induction relating the losses on covered and uncovered clusters. ", "page_idx": 18}, {"type": "text", "text": "Lemma D.4. Let $\\Theta_{t}\\subset\\mathbb{R}^{d}$ denote the parameters of a set of t arbitrary services. Consider $u>0$ uncovered clusters from $\\mathcal{C}(\\Theta_{\\mathrm{OPT}})$ , denoted by $\\mathcal{U}_{t}$ , and let $\\mathcal{H}_{t}$ denote the covered clusters. Suppose we add $v\\leq u$ random services to $\\Theta_{t}$ , chosen with probability proportional to their current loss\u2014i.e., $\\mathcal{L}(\\Theta_{t},\\phi_{j})$ \u2014and let $\\Theta_{t+v}$ denote the the resulting set of services. The following estimate holds: ", "page_idx": 18}, {"type": "text", "text": "E\u0398t $_{+v}[\\mathcal{L}(\\Theta_{t+v},[n])]\\leq\\left(\\mathbb{E}_{\\Theta_{t}}[\\mathcal{L}(\\Theta_{t},\\mathcal{H}_{t})]+K_{\\mathrm{OPT}}\\cdot\\mathcal{L}(\\Theta_{\\mathrm{OPT}},\\mathcal{U}_{t})\\right)\\cdot(1+S_{v})+\\frac{u-v}{u}\\cdot\\mathbb{E}_{\\Theta_{t}}[\\mathcal{L}(\\Theta_{t},\\mathcal{U}_{t})]$ where $K_{\\mathrm{OPT}}$ is as defined in (3) and $S_{v}=\\left(1+{\\textstyle{\\frac{1}{2}}}+\\ldots+{\\textstyle{\\frac{1}{v}}}\\right)$ is the harmonic series. ", "page_idx": 18}, {"type": "text", "text": "With the preceding technical lemmas, we are now ready to prove the main theorem which we restate for convenience. ", "page_idx": 19}, {"type": "text", "text": "Theorem 3.1. Consider n users with unknown preferences $\\{\\phi_{1},\\dots,\\phi_{n}\\}\\,\\subset\\,\\mathbb{R}^{d}$ , and associated loss functions $\\mathcal{L}_{i}(\\cdot,\\cdot)$ satisfying Assumptions 2.1 and 2.2, with bandit access. Let $\\Theta_{\\mathrm{OPT}}\\,\\subset\\,\\mathbb{R}^{d}$ be the set of $k$ services minimizing the total loss and $\\mathcal{C}(\\Theta_{\\mathrm{OPT}})$ the resulting partitioning of users (Definition 2.3). If Algorithm $^{\\,l}$ is used to obtain $k$ services $\\Theta_{k}$ , then the following bound holds: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\Theta_{k}}[\\mathcal{L}(\\Theta_{k},[n])]\\le K_{\\mathrm{OPT}}(2+\\log k)\\cdot\\mathcal{L}(\\Theta_{\\mathrm{OPT}},[n]),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the expectation is taken over the randomization of the algorithm and $K_{\\mathrm{OPT}}$ is equal to ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{max}_{\\mathcal{B}\\in\\mathcal{C}(\\Theta_{\\mathrm{OPT}})}\\frac{4}{\\underset{j\\in B}{\\operatorname*{min}}\\underset{i\\in B}{\\sum}c_{i j}}\\left(\\operatorname*{max}_{j\\in\\mathcal{B}}\\sum_{i\\in B}\\frac{1}{c_{i j}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Consider $t\\,=\\,1$ , and $\\mathcal{B}\\,\\in\\,\\mathcal{C}(\\Theta_{\\mathrm{OPT}})$ , the cluster in which the first chosen user belongs. Applying Lemma D.4 with $v=u=k-1$ and the fact that $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ is the only covered cluster, we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\Theta_{k}}[\\mathcal{L}(\\Theta_{k},[n])]\\le(\\mathbb{E}_{\\Theta_{1}}[\\mathcal{L}\\left(\\Theta_{1},\\mathcal{B}\\right)]+K_{\\mathrm{OPT}}\\cdot\\mathcal{L}\\left(\\Theta_{\\mathrm{OPT}},[n]-\\mathcal{B}\\right))\\cdot\\left(1+S_{k-1}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Observe that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\Theta_{1}}[\\mathcal{L}\\left(\\Theta_{1},\\mathcal{B}\\right)]\\leq K_{\\mathrm{OPT}}\\cdot\\mathcal{L}\\left(\\Theta_{\\mathrm{OPT}},\\mathcal{B}\\right)],\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "by Lemma D.2. Moreover, we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{L}\\left(\\Theta_{\\mathrm{OPT}},[n]-\\mathcal{B}\\right)=\\mathcal{L}\\left(\\Theta_{\\mathrm{OPT}},[n]\\right)-\\mathcal{L}\\left(\\Theta_{\\mathrm{OPT}},\\mathcal{B}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using these two expression and noting $S_{k-1}\\leq1+\\log k$ , we get the stated result. ", "page_idx": 19}, {"type": "text", "text": "E Fair Initialization ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We assume the scenario when the user demographic identities are known a priori. Let $\\gamma:[n]\\rightarrow A$ be a mapping, which maps a user $i$ to its demographic group $\\gamma(i)$ , and let $|\\gamma(i)|$ denote the size of the demographic group user $i$ belongs to. ", "page_idx": 19}, {"type": "text", "text": "We define the weighted total loss (where each users\u2019 loss is divided by its group size) as: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{G}(\\Theta,[n])=\\sum_{i\\in[n]}\\frac{\\mathcal{L}_{i}(\\Theta,\\phi_{i})}{|\\gamma(i)|}=\\sum_{i\\in[m]}\\frac{\\mathcal{L}(\\Theta,\\mathcal{A}_{i})}{|\\mathcal{A}_{i}|}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that this can also be interpreted as the sum of average losses across different demographic groups. ", "page_idx": 19}, {"type": "text", "text": "We introduce Algorithm 2 with a couple of changes from Algorithm 1. ", "page_idx": 19}, {"type": "text", "text": "\u2022 Instead of uniformly picking a user at random, we are picking a user $i$ with probability proportional to $\\frac{1}{|\\gamma(i)|}$ . This is equivalent to saying, the probability of the first user being in some group $A_{j}\\in{\\mathcal{A}}$ is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\sum_{i\\in A_{j}}{\\frac{1}{|\\gamma(i)|}}}{\\sum_{j\\in[m]}\\sum_{i\\in A_{j}}{\\frac{1}{|\\gamma(i)|}}}={\\frac{1}{m}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "That is, the first user is equally likely to belong to one of the $m$ groups. ", "page_idx": 19}, {"type": "text", "text": "\u2022 A user $i$ at any time step $t$ is selected with probability proportional to $\\mathcal{L}_{i}(\\Theta_{t-1},\\phi_{i})/|\\gamma(i)|$ . This implies that a user from some group $A_{j}\\in A$ is likely to be picked with probability proportional to the average demographic loss\u2014i.e., ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{i\\in\\mathcal{A}_{j}}\\mathcal{L}_{i}(\\Theta_{t-1},\\phi_{i})/|\\gamma(i)|=\\frac{\\mathcal{L}(\\Theta_{t-1},\\mathcal{A}_{j})}{|\\mathcal{A}_{j}|}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "1: Input: Set of users $[n]$ , number of services $k$ , Demographic groups $\\mathcal{A}=\\{\\mathcal{A}_{1},\\ldots,\\mathcal{A}_{m}\\}$ , Map   \nusers to demographic groups $\\gamma:[n]\\rightarrow A$   \n2: Choose a user i uniformly randomly from [n] with probability \u221d|\u03b3(1i)|.   \n3: Query user $i$ \u2019s preference $\\phi_{i}$ , set the first service $\\Theta_{1}=\\phi_{i}$ .   \n4: for $t\\in\\{2,\\ldots,k\\}$ do   \n5: User behavior: Collect user losses on existing services $\\Theta_{t-1}:\\{\\mathcal{L}_{i}(\\Theta_{t-1},\\phi_{i})\\}_{i\\in[n]}$ .   \n6: User Selection: Sample $l$ from $[n]$ with probability $P(l=i)\\propto\\mathcal{L}_{i}(\\Theta_{t-1},\\phi_{i})/|\\gamma(i)|$ .   \n7: New service: Query user $l^{\\ast}$ \u2019s preference $\\phi_{l}$ .   \n8: $\\Theta_{t}=\\Theta_{t-1}\\cup\\phi_{l}$ .   \n9: end for   \n10: Return $\\Theta_{k}$ ", "page_idx": 20}, {"type": "text", "text": "We first state the approximation ratio of Algorithm 2 on the objective ${\\mathcal{G}}(\\Theta,[n])$ . ", "page_idx": 20}, {"type": "text", "text": "Lemma E.1. Consider n users with unknown preferences $\\{\\phi_{1},\\ldots,\\phi_{n}\\}\\ \\subset\\ \\mathbb{R}^{d},$ , and unknown associated loss functions $\\mathcal{L}_{i}(\\cdot,\\cdot)$ satisfying Assumptions 2.1 and 2.2. Let $\\Theta_{\\mathrm{scaled}}\\subset\\mathbb{R}^{d}$ be the set of $k$ services minimizing the total loss and $\\mathcal{C}(\\Theta_{\\mathrm{scaled}})$ the resulting partitioning of users. If Algorithm 2 is used to obtain $k$ services $\\Theta_{k}$ , then the following bound holds: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\Theta_{k}}[\\mathcal{G}(\\Theta_{k},[n])]\\le K_{\\mathrm{fair}}(2+\\log k)\\cdot\\mathcal{G}(\\Theta_{\\mathrm{scaled}},[n]),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the expectation is taken over the randomization of the algorithm and $K_{\\mathrm{fair}}$ is equal to ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\substack{B\\in\\mathcal{C}(\\Theta_{\\mathrm{scaled}})\\,}}\\frac{4}{\\operatorname*{min}_{j\\in B}\\sum_{i\\in B}\\frac{c_{i j}}{|\\gamma(i)|}}\\left(\\operatorname*{max}_{j\\in B}\\sum_{i\\in B}\\frac{1}{c_{i j}|\\gamma(i)|}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The proof is similar as Theorem 3.1, with the introduction of $\\frac{1}{|\\gamma(i)|}$ \u2019s when deriving Lemma D.2 and D.3. ", "page_idx": 20}, {"type": "text", "text": "Theorem 3.2. Consider $n$ users with unknown preferences $\\{\\phi_{1},\\dots,\\phi_{n}\\}\\,\\subset\\,\\mathbb{R}^{d}$ , and associated loss functions $\\mathcal{L}_{i}(\\cdot,\\cdot)$ satisfying Assumptions 2.1 and 2.2 with bandit access. Suppose these users belong to m demographic groups $\\mathcal{A}=\\{\\mathcal{A}_{1},...\\,,\\mathcal{A}_{m}\\}\\subset[n]$ . Let $\\Theta_{\\mathrm{fair}}\\subset\\mathbb{R}^{d}$ be the set of $k$ services minimizing the fairness objective $\\Phi$ given in (4). If Algorithm 2 is used to obtain $k$ services $\\Theta_{k}$ , then the following bound holds: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\Theta_{k}}[\\Phi(\\Theta_{k},\\mathcal{A})]\\leq m K_{\\mathrm{fair}}(2+\\log k)\\cdot\\Phi(\\Theta_{\\mathrm{fair}},\\mathcal{A}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the expectation is taken over the randomization of the algorithm and $K_{\\mathrm{fair}}$ is defined in (10). ", "page_idx": 20}, {"type": "text", "text": "Proof. For any $\\Theta\\subset\\mathbb{R}^{d}$ , we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Phi(\\Theta,\\mathcal{A})=\\operatorname*{max}_{i\\in[m]}\\frac{\\mathcal{L}(\\Theta,\\mathcal{A}_{i})}{|\\mathcal{A}_{i}|}\\leq\\sum_{i\\in[m]}\\frac{\\mathcal{L}(\\Theta,\\mathcal{A}_{i})}{|\\mathcal{A}_{i}|}=\\mathcal{G}(\\Theta,[n]).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For $\\Theta_{k}$ \u2014namely the output of Algorithm 1\u2014the above expression implies that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\Theta_{k}}[\\Phi(\\Theta_{k},\\mathcal{A})]\\le\\mathbb{E}_{\\Theta_{k}}[\\mathcal{G}(\\Theta_{k},[n])].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We also have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{G}(\\Theta_{\\mathrm{fair}},[n])=\\sum_{i\\in[m]}\\frac{\\mathcal{L}(\\Theta_{\\mathrm{fair}},A_{i})}{\\lvert A_{i}\\rvert}\\leq\\sum_{i\\in[m]}\\operatorname*{max}_{i\\in[m]}\\frac{\\mathcal{L}(\\Theta_{\\mathrm{fair}},A_{i})}{\\lvert A_{i}\\rvert}=m\\cdot\\Phi(\\Theta_{\\mathrm{fair}},A).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let $\\Theta_{\\mathrm{scaled}}$ be the minimizer of $\\mathcal{G}(\\cdot,[n])$ and $\\Theta_{\\mathrm{fair}}$ be the minimizer of $\\Phi(\\cdot,A)$ . Then, we have that $\\mathcal{G}(\\Theta_{\\mathrm{scaled}},[n])\\le\\mathcal{G}(\\Theta_{\\mathrm{fair}},[n])\\le m\\cdot\\Phi(\\Theta_{\\mathrm{fair}},\\mathcal{A})\\le m\\cdot\\mathbb{E}_{\\Theta_{k}}[\\Phi(\\Theta_{k},\\mathcal{A})]\\le m\\cdot\\mathbb{E}_{\\Theta_{k}}[\\mathcal{G}(\\Theta_{k},[n])]$ . Now using Lemma E.1 for the guarantee on $\\Theta_{k}$ for $\\mathcal{G}(\\cdot,[n])$ , we conclude the proof. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Remark E.2. If all $c_{i j}$ \u2019s are identically equal to some $c>0$ , $\\begin{array}{r}{K_{\\mathrm{fair}}=\\frac{4}{c^{2}}}\\end{array}$ and the approximation ratio of Algorithm 2 for the fair objective is $4m(2+\\log k)/c^{2}$ . Meanwhile, Algorithm 1 would have an approximation ratio ", "page_idx": 20}, {"type": "equation", "text": "$$\n4m\\cdot\\frac{\\operatorname*{max}_{i\\in[m]}\\left|\\mathcal{A}_{i}\\right|}{\\operatorname*{min}_{i\\in[m]}\\left|\\mathcal{A}_{i}\\right|}\\cdot\\frac{(2+\\log k)}{c^{2}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "F Proof of Theorem 4.4 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Let the empirical loss\u2014which is a finite sample average regression loss\u2014be denoted ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{L}}_{i}(\\theta,\\phi_{i})=\\frac{1}{n_{i}}\\sum_{j\\in[n_{i}]}(\\theta^{\\top}x_{i}^{j}-y_{i}^{j})^{2},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\{(x_{i}^{j},y_{i}^{j})\\}_{j\\in[n_{i}]}$ are private unknown features and scores of the users, respectively. ", "page_idx": 21}, {"type": "text", "text": "Lemma F.1. Under Assumptions 4.1 and 4.3, each subpopulation empirical loss ${\\widehat{\\mathcal{L}}}_{i}$ satisfies Assumption 2.1 and Assumption 2.2 with ", "page_idx": 21}, {"type": "equation", "text": "$$\nc_{i j}=\\frac{1}{2}\\lambda_{\\operatorname*{min}}\\left(\\frac{\\sum_{l\\in[n_{i}]}x_{i}^{l}(x_{i}^{l})^{\\top}}{n_{i}},\\frac{\\sum_{l\\in[n_{j}]}x_{j}^{l}(x_{j}^{l})^{\\top}}{n_{j}}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Fix a subpopulation index $i$ . We start by showing ${\\widehat{\\mathcal{L}}}_{i}$ satisfies Assumption 2.1. We use the following result: if $p\\leq d$ random vectors in $\\bar{\\mathbb{R}^{d}}$ are inde pendently drawn from a distribution that is absolutely continuous with respect to the Lebesgue measure, then they are almost surely linearly independent. By Assumption 4.3, applying this result to our scenario, if we draw $n_{i}\\geq d$ features independently from a sub-Gaussian distribution to form a feature matrix $\\mathbf{X}_{i}\\,\\in\\,\\mathbb{R}^{n_{i}\\times d}$ , then it is almost surely full column rank. ", "page_idx": 21}, {"type": "text", "text": "Therefore, we compute $\\phi_{i}=\\mathbf{X}_{i}^{\\dagger}y_{i}$ since $\\mathbf{X}_{i}^{\\dagger}\\mathbf{X}_{i}=\\mathbf{I}_{p}$ when $\\mathbf{X}_{i}$ is full column rank. The subpopulation $i$ \u2019s empirical loss is compactly written as $\\widehat{\\mathcal{L}}_{i}(\\theta,\\phi_{i})=\\|\\theta-\\phi_{i}\\|_{{\\mathbf{A}}_{i}}^{2}$ , where $\\begin{array}{r}{\\mathbf{A}_{i}=\\frac{1}{n_{i}}\\sum_{l\\in[n_{i}]}x_{i}^{l}(x_{i}^{l})^{\\top}}\\end{array}$ . Thus $\\widehat{\\mathscr{L}}_{i}(\\theta,\\phi_{i})$ satisfies Assumption 2.1. ", "page_idx": 21}, {"type": "text", "text": "We now show that ${\\widehat{\\mathcal{L}}}_{i}$ satisfies Assumption 2.2 with $\\begin{array}{r}{c_{i j}=\\frac{1}{2}\\lambda_{\\mathrm{min}}({\\bf A}_{i},{\\bf A}_{j})}\\end{array}$ . To this end, we find the largest $c\\in\\mathbb{R}_{+}$ such that $c(u^{\\top}\\mathbf{A}_{i}u)\\leq u^{\\top}\\mathbf{A}_{j}u$ and $c(u^{\\top}\\mathbf{A}_{j}u)\\leq u^{\\top}\\mathbf{A}_{i}u$ for all $u\\in\\mathbb{R}^{d}$ . ", "page_idx": 21}, {"type": "text", "text": "Rearranging the inequality, this problem is the same as finding the largest $c\\in\\mathbb{R}_{+}$ such that ", "page_idx": 21}, {"type": "equation", "text": "$$\nu^{\\top}(\\mathbf{A}_{i}-c\\mathbf{A}_{j})u\\geq0\\quad\\mathrm{and}\\quad\\mathbf{A}_{j}-c\\mathbf{A}_{i})u\\geq0\\quad\\forall\\:u\\in\\mathbb{R}^{d},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "or equivalently, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{A}_{i}-c\\mathbf{A}_{j}\\succeq0\\quad\\mathrm{and}\\quad\\mathbf{A}_{j}-c\\mathbf{A}_{i}\\succeq0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, finding such a constant $c$ is easily reformulated as the following optimization problem: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\{c\\,|\\,\\mathbf{A}_{i}-c\\mathbf{A}_{j}\\succeq0,\\mathbf{A}_{j}-c\\mathbf{A}_{i}\\succeq0\\}=\\lambda_{\\operatorname*{min}}(\\mathbf{A}_{i},\\mathbf{A}_{j}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "There are two key tools we use to finish the argument. ", "page_idx": 21}, {"type": "text", "text": "$\\begin{array}{r}{\\mathrm{For~any~}u\\in\\mathbb{R}^{d},\\,\\frac{1}{2}\\lambda_{\\operatorname*{min}}({\\bf A}_{i},{\\bf A}_{j})\\|u\\|_{{\\bf A}_{i}}^{2}\\leq\\|u\\|_{{\\bf A}_{j}}^{2}.}\\end{array}$ \u2022 For any norm triangle inequality gives us $\\lVert{\\boldsymbol{\\theta}}-{\\boldsymbol{\\phi}}_{i}\\rVert\\leq\\lVert{\\boldsymbol{\\theta}}-{\\boldsymbol{\\phi}}_{j}\\rVert+\\lVert{\\boldsymbol{\\phi}}_{i}-{\\boldsymbol{\\phi}}_{j}\\rVert$ . Then we use power mean inequality, i.e. $a\\leq b+c\\implies a^{2}\\leq2(b^{2}+c^{2})$ . ", "page_idx": 21}, {"type": "text", "text": "The following shows Assumption $2.2.(i)$ holds with $\\begin{array}{r}{c_{i j}=\\frac{1}{2}\\lambda_{\\mathrm{min}}({\\bf A}_{i},{\\bf A}_{j})}\\end{array}$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac12\\lambda_{\\operatorname*{min}}(\\mathbf{A}_{i},\\mathbf{A}_{j})\\widehat{\\mathcal{L}}_{i}(\\theta,\\phi_{i})=\\frac{1}{2}\\lambda_{\\operatorname*{min}}(\\mathbf{A}_{i},\\mathbf{A}_{j})\\|\\theta-\\phi_{i}\\|_{\\mathbf{A}_{i}}^{2}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\leq\\frac{1}{2}\\|\\theta-\\phi_{i}\\|_{\\mathbf{A}_{j}}^{2}}\\\\ &{\\displaystyle\\qquad\\qquad\\leq\\|\\theta-\\phi_{j}\\|_{\\mathbf{A}_{j}}^{2}+\\|\\phi_{i}-\\phi_{j}\\|_{\\mathbf{A}_{j}}^{2}}\\\\ &{=\\widehat{\\mathcal{L}}_{j}(\\theta,\\phi_{j})+\\widehat{\\mathcal{L}}_{j}(\\phi_{i},\\phi_{j}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Analogously, the following shows Assumption $2.2.(i i)$ holds with $\\begin{array}{r}{c_{i j}=\\frac{1}{2}\\lambda_{\\mathrm{min}}({\\bf A}_{i},{\\bf A}_{j})}\\end{array}$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac12\\lambda_{\\operatorname*{min}}(\\mathbf{A}_{i},\\mathbf{A}_{j})\\widehat{\\mathcal{L}}_{i}(\\phi_{j},\\phi_{i})=\\displaystyle\\frac12\\lambda_{\\operatorname*{min}}(\\mathbf{A}_{i},\\mathbf{A}_{j})\\|\\phi_{j}-\\phi_{i}\\|_{\\mathbf{A}_{i}}^{2}}&{}\\\\ {\\displaystyle\\leq\\lambda_{\\operatorname*{min}}(\\mathbf{A}_{i},\\mathbf{A}_{j})\\left(\\|\\theta-\\phi_{j}\\|_{\\mathbf{A}_{i}}^{2}+\\|\\theta-\\phi_{i}\\|_{\\mathbf{A}_{i}}^{2}\\right)}&{}\\\\ {\\displaystyle\\leq\\|\\theta-\\phi_{j}\\|_{\\mathbf{A}_{j}}^{2}+\\|\\theta-\\phi_{i}\\|_{\\mathbf{A}_{i}}^{2}}&{}\\\\ {\\displaystyle=\\mathcal{L}_{j}(\\theta,\\phi_{j})+\\mathcal{L}_{i}(\\theta,\\phi_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 21}, {"type": "text", "text": "With the preceding technical lemma in place, we now are ready to prove Theorem 4.4. ", "page_idx": 22}, {"type": "text", "text": "Theorem 4.4. Suppose users belong to $N$ subpopulations satisfying Assumptions 4.1, 4.2, and 4.3. Let $\\{n_{i}\\}_{i\\in[N]}$ denote the number of samples per subpopulation. Let $\\Theta_{k}$ be the output of Algorithm 1 using finite sample estimates $\\widehat{\\mathcal{L}}_{i}(\\cdot,\\phi_{i})$ and $\\Theta_{\\mathrm{OPT}}$ be the optimal solution of the expected loss. Then, for any $\\epsilon\\,\\in\\,(0,1)$ , if $\\begin{array}{r}{\\dot{n}_{i}=\\Omega(\\frac{\\sigma_{i}^{4}\\sqrt{N}\\log{(2/\\delta)}}{\\epsilon^{2}})}\\end{array}$ for all $i\\,\\in\\,[N]$ , the following inequality holds with probability at least $1-\\delta$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{z}_{\\ominus_{k}}[\\mathcal{L}(\\Theta_{k},[N])]\\le\\frac{1+\\epsilon}{1-\\epsilon}K_{\\mathrm{OPT}}(2+\\log k)\\mathcal{L}(\\Theta_{\\mathrm{OPT}},[N]),\\ w h e r e\\ K_{\\mathrm{OPT}}\\ i s\\,a s\\,d e\\!f i n e d\\,i n\\left(3\\right)\\,a n d}\\\\ &{\\mathfrak{z}_{j}=\\frac{1}{2}\\operatorname*{min}\\ \\left\\{\\lambda_{\\operatorname*{min}}(\\widehat{\\Sigma}_{i},\\widehat{\\Sigma_{j}}),\\frac{1}{\\lambda_{\\operatorname*{min}}(\\widehat{\\Sigma}_{i},\\widehat{\\Sigma_{j}})}\\right\\},\\ w i t h\\ \\widehat{\\Sigma}_{i}=\\frac{1}{n_{i}}\\sum_{l\\in[n_{i}]}x_{i}^{l}(x_{i}^{l})^{\\top},\\ \\widehat{\\Sigma}_{j}=\\frac{1}{n_{j}}\\sum_{l\\in[n_{j}]}x_{j}^{l}(x_{j}^{l})^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Lemma F.1 gives us that the subpopulation $i$ \u2019s empirical loss for a regressor $\\theta\\in\\mathbb{R}^{d}$ can be written as $\\lVert\\theta-\\bar{\\phi}_{i}\\rVert_{\\mathbf{A}_{i}}^{2}$ . This is a random quantity since $\\mathbf{A}_{i}$ is a sample average covariance of randomly chosen features. We analyse this term next. ", "page_idx": 22}, {"type": "text", "text": "For a random feature $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ in subpopulation $i$ , given a fixed center $\\theta\\in\\ensuremath{\\mathbb{R}}^{d}$ , the term (\u03b8\u2212\u03d5i)\u22a4xis subGaussian with variance proxy $\\sigma_{i}^{2}$ by Assumption 4.1. The square of a sub-Gaussian random variable, (\u03b8\u2212\u03d5i\u2225)\u03b8\u22a4\u2212x\u03d5xi\u2225\u22a42(\u03b8\u2212\u03d5i)is sub-exponential5 with parameters \u03bdi2 = 32\u03c3i4 , \u03b1i = 4\u03c3i2 (see [22][Appendix B]). ", "page_idx": 22}, {"type": "text", "text": "Since subpopulation $i$ \u2019s empirical loss is an average of $n_{i}$ samples, the empirical loss $\\lVert\\theta-\\phi_{i}\\rVert_{\\mathbf{A}_{i}}^{2}$ is a subexponential variable with parameters $\\begin{array}{r}{\\frac{\\nu_{i}^{2}}{n_{i}}\\|\\theta-\\phi_{i}\\|_{\\Sigma_{i}}^{4},\\frac{\\alpha_{i}}{\\sqrt{n_{i}}}\\|\\theta-\\phi_{i}\\|_{\\Sigma_{i}}^{2}}\\end{array}$ . This is because the variance of sample average scales as \u221a1ni , and the constant \u2225\u03b8 \u2212\u03d5i\u22252\u03a3i scales the subexponential parameters appropriately. ", "page_idx": 22}, {"type": "text", "text": "Now, given a set of $k$ arbitrary services $\\Theta\\,=\\,\\{\\theta_{1},\\dots,\\theta_{k}\\}\\,\\subset\\,\\mathbb{R}^{d}$ , let $\\theta_{\\gamma(i)}\\,\\in\\,\\Theta$ be an arbitrary service chosen by subpopulation $i$ . The sum of empirical losses on based on such choices is also subexponential with parameters $\\begin{array}{r}{\\nu^{2}=\\sum_{i\\in[N]}\\frac{\\nu_{i}^{2}}{n_{i}}\\lVert\\boldsymbol{\\theta}-\\boldsymbol{\\phi}_{i}\\rVert_{\\Sigma_{i}}^{4}}\\end{array}$ and $\\begin{array}{r}{\\alpha=\\operatorname*{max}_{i\\in[N]}\\frac{\\alpha_{i}}{\\sqrt{n_{i}}}\\|\\theta-\\phi_{i}\\|_{\\Sigma_{i}}^{2}}\\end{array}$ . Using Chernoff bound [40], we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{o}\\left\\{\\vert\\sum_{i\\in[N]}\\|\\theta_{\\gamma(i)}-\\phi_{i}\\|_{\\mathbf{A}_{i}}^{2}-\\sum_{i\\in[N]}\\|\\theta_{\\gamma(i)}-\\phi_{i}\\|_{\\mathbf{\\Sigma}_{i}}^{2}\\right\\}\\geq t\\rfloor\\leq2\\exp\\left(-\\frac{t^{2}}{2\\nu^{2}}\\right)\\quad\\mathrm{where~}0\\leq t\\leq\\frac{\\nu^{2}}{\\alpha}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that the probability bound is is increasing in $\\nu$ , we thus take an upper bound on $\\nu^{2}$ to upper bound this probability: ", "page_idx": 22}, {"type": "equation", "text": "$$\n,^{2}=\\sum_{i\\in[N]}\\frac{\\nu_{i}^{2}}{n_{i}}\\|\\theta-\\phi_{i}\\|_{\\Sigma_{i}}^{4}=32\\sum_{i\\in[N]}\\frac{\\sigma_{i}^{4}}{n_{i}}\\|\\theta_{\\gamma(i)}-\\phi_{i}\\|_{\\Sigma_{i}}^{4}\\leq32\\left(\\sum_{i\\in[N]}\\frac{\\sigma_{i}^{8}}{n_{i}^{2}}\\right)^{\\frac{1}{2}}\\left(\\sum_{i\\in[N]}\\|\\theta_{\\gamma(i)}-\\phi_{i}\\|_{\\Sigma_{i}}^{8}\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last inequality follows from Cauchy-Schwarz. Further, noticing that $\\textstyle\\sum_{i\\in n}a_{i}^{4}\\ \\leq$ $(\\textstyle\\sum_{i\\in n}a_{i}^{2})^{2}$ , and applying it for the last term twice, we deduce that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nu^{2}\\leq32\\left(\\sum_{i\\in[N]}\\frac{\\sigma_{i}^{8}}{n_{i}^{2}}\\right)^{\\frac{1}{2}}\\left(\\sum_{i\\in[n]}\\|\\theta_{\\gamma(i)}-\\phi_{i}\\|_{\\Sigma_{i}}^{2}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "5A random variable $_x$ is subexponential with parameters $(v^{2},\\alpha)$ if $\\begin{array}{r}{\\mathbb{E}[\\exp(\\lambda x)]\\leq\\exp(\\frac{\\nu^{2}\\lambda^{2}}{2})\\;\\forall|\\lambda|\\leq\\frac{1}{\\alpha}}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "Choosing $\\begin{array}{r}{t=\\epsilon\\sum_{i\\in[N]}\\|\\theta_{\\gamma(i)}-\\phi_{i}\\|_{\\Sigma_{i}}^{2}}\\end{array}$ in (11), where $\\begin{array}{r}{0\\leq\\epsilon\\leq\\mathcal{O}(\\operatorname*{min}_{i\\in[N]}\\frac{\\sigma_{i}^{2}}{\\sqrt{n_{i}}})}\\end{array}$ , we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P\\left[|\\displaystyle\\sum_{i\\in[N]}\\|\\theta_{\\gamma(i)}-\\phi_{i}\\|_{\\mathbf{A}_{i}}^{2}-\\displaystyle\\sum_{i\\in[N]}\\|\\theta_{\\gamma(i)}-\\phi_{i}\\|_{\\mathbf{\\Delta}_{i}}^{2}|\\geq\\epsilon\\displaystyle\\sum_{i\\in[N]}\\|\\theta_{\\gamma(i)}-\\phi_{i}\\|_{\\mathbf{\\Delta}_{i}}^{2}\\right]}\\\\ &{\\quad\\geq2\\exp\\left(-\\displaystyle\\frac{\\epsilon^{2}}{64}\\left(\\displaystyle\\sum_{i\\in[N]}\\frac{\\sigma_{i}^{8}}{n_{i}^{2}}\\right)^{-1/2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "With high probability, at least $\\begin{array}{r}{1-2\\exp\\left(-\\frac{\\epsilon^{2}}{64}\\left(\\sum_{i\\in[N]}\\frac{\\sigma_{i}^{8}}{n_{i}^{2}}\\right)^{-1/2}\\right)}\\end{array}$ , for all $\\Theta\\subset\\mathbb{R}^{d}$ , we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n(1-\\epsilon)\\sum_{i\\in[N]}\\|\\theta_{\\gamma(i)}-\\phi_{i}\\|_{\\Sigma_{i}}^{2}\\leq\\sum_{i\\in[N]}\\|\\theta_{\\gamma(i)}-\\phi_{i}\\|_{\\mathbf{A}_{i}}^{2}\\leq(1+\\epsilon)\\sum_{i\\in[N]}\\|\\theta_{\\gamma(i)}-\\phi_{i}\\|_{\\Sigma_{i}}^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The rest of the proof is deterministic, thus it is assumed everything follows with probability at $\\begin{array}{r}{1-2\\exp\\left\\}-\\frac{\\epsilon^{2}}{64}\\left(\\sum_{i\\in[N]}\\frac{\\sigma_{i}^{8}}{n_{i}^{2}}\\right)^{-1/2}\\right)}\\end{array}$ . Define $\\Theta^{N}\\,=\\,\\Theta\\,\\times\\,\\cdot\\,\\cdot\\,\\times\\,\\Theta.$ \u2014i.e., Cartesian product of the set $\\Theta\\textsl{N}$ -times. Given two functions $f,g:\\Theta^{N}\\to\\mathbb{R}$ , if $f(\\theta)\\leq g(\\theta)$ for all $\\theta\\in\\Theta^{N}$ , then $\\begin{array}{r}{\\operatorname*{min}_{\\theta\\in\\Theta^{N}}f(\\theta)\\le\\operatorname*{min}_{\\theta\\in\\Theta^{N}}g(\\theta)}\\end{array}$ . Therefore, we deduce that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left(1-\\epsilon\\right)\\sum_{i\\in[N]}\\operatorname*{min}_{j\\in[k]}\\|\\theta_{j}-\\phi_{i}\\|_{\\Sigma_{i}}^{2}\\leq\\sum_{i\\in[N]}\\operatorname*{min}_{j\\in[k]}\\|\\theta_{j}-\\phi_{i}\\|_{\\mathbf{A}_{i}}^{2}\\leq\\left(1+\\epsilon\\right)\\sum_{i\\in[N]}\\operatorname*{min}_{j\\in[k]}\\|\\theta_{j}-\\phi_{i}\\|_{\\Sigma_{i}}^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Hence, given the same set of services $\\Theta$ , the total empirical loss can be bounded with respect to the total expected loss as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n(1-\\epsilon)\\mathcal{L}(\\Theta,[N])\\leq\\widehat{\\mathcal{L}}(\\Theta,[N])\\leq(1+\\epsilon)\\mathcal{L}(\\Theta,[N]).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Recall $\\Theta_{\\mathrm{OPT}}$ denotes the optimal solution for the total expected loss and let $\\widehat{\\Theta}_{\\mathrm{OPT}}$ denote the optimal solution for the total empirical loss. Let $\\Theta_{k}$ be the output of Algorithm 1 on finite samples per subpopulations. Using the first inequality of (12), we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(1-\\epsilon)\\mathbb{E}_{\\Theta_{k}}[\\mathcal{L}(\\Theta_{k},[N])]\\leq\\mathbb{E}_{\\Theta_{k}}[\\widehat{\\mathcal{L}}(\\Theta_{k},[N])].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Also, noting that $\\widehat{\\Theta}_{\\mathrm{OPT}}$ is the minimizer for $\\widehat{\\mathcal{L}}(\\Theta,[N])$ and using the second inequality of (12), we get that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\mathcal{L}}(\\widehat{\\Theta}_{\\mathrm{OPT}},[N])\\leq\\widehat{\\mathcal{L}}(\\Theta_{\\mathrm{OPT}},\\Phi)\\leq(1+\\epsilon)\\mathcal{L}(\\Theta_{\\mathrm{OPT}},\\Phi).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now combining (13) and (14) with Theorem 3.1, we get the desired result\u2014i.e., ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\Theta_{k}}[\\mathcal{L}(\\Theta_{k},[N])]\\le\\frac{1+\\epsilon}{1-\\epsilon}K_{\\mathrm{OPT}}(2+\\log k)\\cdot\\mathcal{L}(\\Theta_{\\mathrm{OPT}},[N]).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Setting $\\begin{array}{r}{\\delta=1-2\\exp\\left(-\\frac{\\epsilon^{2}}{64}\\left(\\sum_{i\\in[N]}\\frac{\\sigma_{i}^{8}}{n_{i}^{2}}\\right)^{-1/2}\\right)}\\end{array}$ , we get that $\\begin{array}{r}{n_{i}=\\Omega(\\frac{\\sigma_{i}^{4}\\sqrt{N}\\log{(2/\\delta)}}{\\epsilon^{2}})}\\end{array}$ . This concludes the proof. ", "page_idx": 23}, {"type": "text", "text": "G Experiment Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Census Dataset. The categorical features (schooling, marital status, migration status, citizenship) were first converted to one hot vectors and the continuous features (age, income) were scaled appropriately for better conditioning. We performed singular value decomposition on the features and retained the top ten components. The scores were taken as the log transform of the daily commute time in minutes. To form subpopulations, we split users based on their Public Use Microdata Area codes (zip code) and ensure each subpopulation belongs to one of the demographic groups considered. The percentage improvements of our algorithm are plotted in Figure 4. ", "page_idx": 23}, {"type": "image", "img_path": "HSJOt2hyDf/tmp/4ce4d07781489d720fe8a12f2f09965b3a7e3808249d83adfb0658369ae70080.jpg", "img_caption": ["Figure 4: Fair objective improvement for AcQUIre over the baseline across different demographics. We observe that there is atleast $15\\%$ improvement across sex demographics for a wide range of number of services. For racial demographics the improvement is $7.26\\%$ . "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "HSJOt2hyDf/tmp/3a2be965036765e0e1f4e0293c8d6541cbd64ce9b44d4af160d1ad8bd4e28e39.jpg", "img_caption": ["Figure 5: Average losses across different demographic groups for Fair AcQUIre (left,middle). Percentage improvement over baseline (right). We observe that Fair AcQUIre reduces disparity across different groups compared to the baseline. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "In Figure 5 we plot the average losses on the individual demographic groups. We benchmark against the weighted random baseline and note that Fair AcQUIre improves not only the fair objective value but the loss for every demographic group. ", "page_idx": 24}, {"type": "text", "text": "Movie Recommendation Dataset. We use Surprise (a Python toolkit [23]) to perform our experiments. We split the total 10 million ratings into top (a) 200 movies, and (b) all other movies. We use the inbuilt nonnegative matrix factorization function of Surprise on (b) to get user and item embeddings. We cluster the users into 1000 subpopulations by running $k$ -means on the obtained user embeddings we get. We evaluate algorithms for this experiment on the held out set (a) of the top 200 movies. ", "page_idx": 24}, {"type": "text", "text": "Ablation: We use a 2 layer Neural Network with ReLU activations that takes as input the user features and outputs their score. We still use the standard squared prediction error. However note that in this modeling scenario, the loss no longer satisfies our assumptions in the parameters of the neural network. Given a user\u2019s features and true score, since there are no unique minimizers, we run gradient descent to compute a local minimizer and then use this trained neural network as a service to predict other user\u2019s scores. We run AcQUIre and other baselines under this modeling and report our results in Figure 6. We find that the performance of AcQUIre even when violating assumptions is almost similar to using AcQUIre under modeling which satisfies assumptions. ", "page_idx": 24}, {"type": "text", "text": "Additionally, we would like to emphasize that the implementation of our algorithm itself does not rely on these assumptions. It only requires the loss values to be observable. Therefore, one can model very complex precision models and only supply the loss values of these models to our algorithm. ", "page_idx": 24}, {"type": "image", "img_path": "HSJOt2hyDf/tmp/485c21ee37705df0e9e5f5625d52a441da83914e05f6ca357aa96c54fb541f71.jpg", "img_caption": ["Figure 6: We evaluate the performance of initialization methods when Assumptions 2.1 and 2.2 are violated. \"AcQUIre (NN)\" refers to using AcQUIre with a Neural Network (NN) to predict scores. This terminology is consistently applied to all other baselines as well. Notably, the performance of AcQUIre doesn\u2019t degrade when using Neural Network to predict the scores, thereby demonstrating its robustness to violating the assumptions made in the paper. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We abstract out the problem statement, challenges, and novelty in solution ideas in the abstract. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: Yes, we clearly state all the assumptions that the theoretical results rely on.   \nWe also briefly discuss open directions which were not covered in this paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Proof sketches are in the main paper, with full proofs in the appendix. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We state experimental setup and details, and provide anonymous link to our code base and links to the two publicly available datasets that we used in the experiments. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Yes, we provide both code link and upload codebase in the supplemental. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The focus of the paper is not experimental, but experiments are used to illustrate our algorithmic and theoretical contributions. All details are specified in the experiments section and the appendix. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We report number of trials and error bars. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: All our experiments can be run on personal devices. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have read the code of ethics and verified we align with it. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. \u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 28}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: While it is not the main focus of the paper, our paper has a devoted section that considers the fair version of the total loss, and discusses the fairness aspects of the proposed methods. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper is mostly theoretical in nature and poses no such risk. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We cite the dataset papers. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This paper is theoretical in nature. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: No human subjects were needed for this paper. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] Justification: No. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]