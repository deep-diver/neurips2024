[{"figure_path": "dQ9ji8e9qQ/figures/figures_1_1.jpg", "caption": "Figure 1: An illustration of our proposed ARE framework. This example showcases how ARE can enhance the concept of \"angry\" within an LLM. The process involves an iterative dance between the generator and the discriminator. The generator produces outputs, while the discriminator refines its internal representation of \"angry\" based on these outputs. Through this back-and-forth training, the LLM gradually learns to produce outputs that align better with the concept of \"angry.\"", "description": "This figure illustrates the Adversarial Representation Engineering (ARE) framework proposed in the paper. It shows how ARE iteratively refines a language model's understanding of a concept (in this case, \"angry\") through a back-and-forth process between a generator (the language model) and a discriminator. The generator produces outputs, and the discriminator evaluates these outputs and provides feedback to refine the internal representation of the concept. This iterative process allows the model to generate outputs that better reflect the target concept.", "section": "4 Proposed Method"}, {"figure_path": "dQ9ji8e9qQ/figures/figures_4_1.jpg", "caption": "Figure 2: Comparison between the basic structures of GAN and ARE.", "description": "This figure compares the architectures of Generative Adversarial Networks (GANs) and the proposed Adversarial Representation Engineering (ARE) framework.  Both frameworks involve a generator and a discriminator.  In GANs, the generator produces data, and the discriminator distinguishes between real and generated data.  In ARE, the generator (a decoder model) produces representations, and the discriminator distinguishes between target and generated representations.  The ARE framework uses adversarial training between the generator and discriminator to refine the representations and achieve the desired editing goal.", "section": "4.1 Adversarial Representation Engineering"}, {"figure_path": "dQ9ji8e9qQ/figures/figures_4_2.jpg", "caption": "Figure 3: t-SNE visualization of aligned model's response to normal and malicious prompts over iterative training epochs.", "description": "This figure shows the visualization of the training process of the Adversarial Representation Engineering (ARE) framework using t-SNE.  It displays the changes in the representation of normal and malicious prompts over 30 epochs. In the beginning (Epoch 0), the representations of normal and malicious prompts are distinctly clustered. As training progresses (Epoch 1), the malicious prompt representations start moving towards the normal prompt cluster. Finally (Epoch 30), the two clusters have merged almost completely, indicating that the model has learned to generate similar responses for both types of prompts.  This demonstrates the effectiveness of ARE in aligning the model's responses to a desired concept.", "section": "4.1 Adversarial Representation Engineering"}, {"figure_path": "dQ9ji8e9qQ/figures/figures_5_1.jpg", "caption": "Figure 1: An illustration of our proposed ARE framework. This example showcases how ARE can enhance the concept of \"angry\" within an LLM. The process involves an iterative dance between the generator and the discriminator. The generator produces outputs, while the discriminator refines its internal representation of \"angry\" based on these outputs. Through this back-and-forth training, the LLM gradually learns to produce outputs that align better with the concept of \"angry.\"", "description": "This figure illustrates the Adversarial Representation Engineering (ARE) framework.  It shows how an iterative process between a generator (the LLM) and a discriminator refines the LLM's internal representation of a concept (\"angry\" in this example).  The generator produces text, the discriminator evaluates how well it matches the target concept, and feedback from the discriminator guides the generator to better align its output with the concept. The process repeats across epochs, leading to gradual improvement in generating outputs aligned with the intended concept.", "section": "4 Proposed Method"}]