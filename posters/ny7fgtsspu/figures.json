[{"figure_path": "nY7fGtsspU/figures/figures_6_1.jpg", "caption": "Figure 1: Simulations and GP prior of a GCN on a complete graph with N = 5 nodes, shift operator A\u03b1\u03b2 = N\u22121 + \u03b4\u03b1\u03b2(1\u22121/N), vanishing bias b(l) = 0 and \u03c6(x) = erf(x). a) The phase diagram dependent on \u03c32w and g. The equilibrium feature distance \u03bc(X) obtained from computing the GCN GP prior for L = 4,000 layers is shown as a heatmap, the red line is the theoretical prediction for the transition to the non-oversmoothing phase. b) Same as in a) but color coding shows whether \u03bc(X) is close to zero (black) or not (white) with precision 10\u22125. The red line again shows the theoretically predicted phase transition. c) Feature distance \u03bc(X(l)) for a random input X(0) i.i.d. N(0, 1) as a function of layer l. Parameters are written in the panel in matching colors and marked with color coded crosses in the phase diagram in panel b). Feature dimension of the hidden layers is d1 = 200, crosses show the mean of 50 network realizations, solid curves the theoretical predictions.", "description": "This figure shows a phase diagram for a graph convolutional network (GCN) on a complete graph with 5 nodes. The phase diagram shows the equilibrium feature distance as a function of the weight variance (\u03c32w) and a parameter (g) that controls the balance between diagonal and off-diagonal elements in the shift operator. The red line indicates the theoretical prediction for the transition to a non-oversmoothing phase, where the feature distances remain informative even at large depths. The figure also includes plots showing the evolution of feature distance with the number of layers for different parameter choices.", "section": "4 Results"}, {"figure_path": "nY7fGtsspU/figures/figures_7_1.jpg", "caption": "Figure 2: The non-oversmoothing phase in a contextual stochastic block model instance with parameters N = 100, d = 5, \u03bb = 1. The shift operator is chosen according to (6) with g = 0.3, and \u03c3\u03c4 = 0 and \u03c6(x) = erf(x). a) The maximum feature distance between any pair of nodes in equilibrium obtained from computing the GCN GP prior for L = 4,000 layers (blue) and the largest eigenvalue of the linearized GCN GP dynamics at the zero distance state as a function of weight variance \u03c32. The red line marks the point where max\u00bf{||} = 1. b) Heatmap of the equilibrium distance matrix with entries da\u00df = d(x\u03b1, x\u03b2) (Equation (13)) at \u03c32 = 1.3, marked as point A in panel a). Colorbar shared with the plot in c). c) Same as b) but at point B with \u03c32 = 2. d) Features distances d = d(xa, x\u03b2) as a function of layers for random inputs x(0) i.id. N(0, 1) and a finite-size GCN with d1 = 200, averaged for distances for pairs of nodes within the same community (red) and across communities (purple).", "description": "This figure shows an analysis of the non-oversmoothing phase in a contextual stochastic block model.  Panel a) shows the transition point between oversmoothing and non-oversmoothing phases, determined by the maximum feature distance and the largest eigenvalue of the linearized GCN GP dynamics. Panels b) and c) visualize the equilibrium distance matrices at different points (A and B) in the phase diagram. Panel d) shows the average feature distance over layers for finite-size GCNs, comparing within-class and between-class distances. The results illustrate how the non-oversmoothing phase is reached by tuning the weight variance, leading to informative features even in deep GCNs.", "section": "4.2.2 General graphs"}, {"figure_path": "nY7fGtsspU/figures/figures_8_1.jpg", "caption": "Figure 3: Generalization error (mean squared error) of the Gaussian process for a CSBM with parameters N = 20, d = 5, \u03bb = 1, \u03b3 = 1 and \u03bc = 4. The shift operator is defined in (6) with g = 0.1, other parameters are \u03c3\u03c4 = 0, \u03c6(x) = erf(x) and \u03c3ro = 0.01. In all panels we use Ntrain = 10 training nodes and Ntest = 10 test nodes, five training nodes from each of the two communities. Labels are \u00b11 for the two communities, respectively. For all panels, we show averages over 50 CSBM instances. a) Heatmap of the generalization error of the GCN GP dependent on number of layers L and weight variance \u03c32. The red line shows the transition to the non-oversmoothing phase. b) Generalization error dependent on weight variance \u03c32 and depths L = 1,4,16, 64, 256, 1024 from turquoise to dark blue. c) Generalization error dependent on the layer for the GCN GP at the critical line \u03c3\u00b2 = \u03c3\u03c9,crit, in the oversmoothing phase \u03c3\u00b2 = \u03c3\u03c9,crit \u2212 1 and the non-oversmoothing phase \u03c3\u00b2 = \u03c3\u03c9,crit + 1. d) Performance of randomly initialized finite-size GCNs with d\u2081 = 200 for l = 1,..., L where only the linear readout layer is trained with gradient descent (details in Appendix F) at the critical line \u03c3\u00b2 = \u03c3\u03c9,crit, in the oversmoothing phase \u03c3\u00b2 = \u03c3\u03c9,crit \u2212 1 and the non-oversmoothing phase \u03c3\u00b2 = \u03c3\u03c9,crit + 1.", "description": "This figure shows the generalization error of Gaussian process for a Contextual Stochastic Block Model (CSBM) with different parameters, specifically focusing on the impact of weight variance (\u03c3\u00b2) and the number of layers (L) on the generalization performance. The figure is divided into four subplots, each demonstrating a different aspect of the relationship between these parameters and generalization error. It also compares the performance of Gaussian Process (GP) and finite-size Graph Convolutional Networks (GCNs).", "section": "4.3 Implications for performance"}, {"figure_path": "nY7fGtsspU/figures/figures_8_2.jpg", "caption": "Figure 3: Generalization error (mean squared error) of the Gaussian process for a CSBM with parameters N = 20, d = 5, \u03bb = 1, \u03b3 = 1 and \u03bc = 4. The shift operator is defined in (6) with g = 0.1, other parameters are \u03c3\u03c4 = 0, \u03c6(x) = erf(x) and \u03c3ro = 0.01. In all panels we use Ntrain = 10 training nodes and Ntest = 10 test nodes, five training nodes from each of the two communities. Labels are \u00b11 for the two communities, respectively. For all panels, we show averages over 50 CSBM instances. a) Heatmap of the generalization error of the GCN GP dependent on number of layers L and weight variance \u03c32. The red line shows the transition to the non-oversmoothing phase. b) Generalization error dependent on weight variance \u03c32 and depths L = 1,4,16, 64, 256, 1024 from turquoise to dark blue. c) Generalization error dependent on the layer for the GCN GP at the critical line \u03c3\u00b2 = \u03c3\u03c9,crit, in the oversmoothing phase \u03c3\u00b2 = \u03c3\u03c9,crit \u2212 1 and the non-oversmoothing phase \u03c3\u00b2 = \u03c3\u03c9,crit + 1. d) Performance of randomly initialized finite-size GCNs with d\u2081 = 200 for l = 1,..., L where only the linear readout layer is trained with gradient descent (details in Appendix F) at the critical line \u03c3\u00b2 = \u03c3\u00b2,crit, in the oversmoothing phase \u03c3\u00b2 = \u03c3w,crit \u2212 1 and the non-oversmoothing phase \u03c3\u00b2 = \u03c3w,crit + 1.", "description": "This figure shows the generalization error of Gaussian process for a Contextual Stochastic Block Model (CSBM) with different parameters. It demonstrates how the generalization error changes with the number of layers and weight variance. The figure also highlights the transition point between oversmoothing and non-oversmoothing phases.", "section": "4.3 Implications for performance"}, {"figure_path": "nY7fGtsspU/figures/figures_16_1.jpg", "caption": "Figure 5: Histogram of \u03c32w,crit for the 50 CSBM instances used in the experiment of Figure 3. The point 1 is marked for comparison to related work.", "description": "This histogram visualizes the distribution of critical weight variances (\u03c32w,crit) obtained from 50 simulations of the Contextual Stochastic Block Model (CSBM) used in the experiments of Figure 3.  Each critical weight variance represents the threshold beyond which the graph convolutional network (GCN) transitions from an oversmoothing to a non-oversmoothing phase. The vertical line at \u03c32w,crit = 1 is included for comparison with results found in related research. The distribution shows that many of the critical variances are above 1, indicating that oversmoothing can be avoided by initializing the weight variance beyond a certain threshold.", "section": "4.2.2 General graphs"}, {"figure_path": "nY7fGtsspU/figures/figures_17_1.jpg", "caption": "Figure 6: Oversmoothing in GCN GPs with the commonly used shift operator (48) (called Kipf & Welling in the label) and our row-stochastic shift operator (6). a) Feature distance \u03bc(X) in equilibrium dependent on weight variance \u03c32 obtained from simulating the GCN GP priors for Leq = 4,000 layers. Shown are the averages (solid lines) and standard deviations (shaded areas) over 20 CSBM initializations with \u03bb = 1, d = 5 and N = 30 nodes. b) Scatter plot of maximal variance of all nodes max{Kaa} and feature distance \u03bc(X) in equilibrium for the same data as in plot a). c) Same as b) but zoomed into lower left corner.", "description": "This figure compares the oversmoothing behavior of GCNs using two different shift operators: a row-stochastic operator and the commonly used Kipf & Welling operator.  Panel (a) shows the equilibrium feature distance (\u03bc(X)) as a function of weight variance (\u03c3w2).  Panels (b) and (c) present scatter plots relating the equilibrium feature distance to the maximum variance among nodes, offering a closer look at the transition between oversmoothing and non-oversmoothing phases.", "section": "4.2.2 General graphs"}, {"figure_path": "nY7fGtsspU/figures/figures_18_1.jpg", "caption": "Figure 7: Node distance measure \u03bc(X) at equilibrium obtained from computing the GCN GP prior for L = 4,000 layers as a function of \u03c32. The transition to the non-oversmoothing regime is estimated by checking where the node distance measure is larger than = 10\u22125, marked as the red line.", "description": "This figure shows the equilibrium node distance (\u03bc(X)) as a function of weight variance (\u03c3w2) obtained from a Gaussian process approximation of a graph convolutional network (GCN) with 4000 layers.  The red line marks the transition point to the non-oversmoothing regime, where the equilibrium node distance becomes greater than a small threshold (\u2208 = 10\u207b\u2075).  This demonstrates how increasing weight variance leads to a phase transition where the GCN no longer oversmooths.", "section": "4.2.2 General graphs"}]