[{"figure_path": "FJlrSZBMCD/tables/tables_6_1.jpg", "caption": "Table 1: Downstream evaluation results for full methods, comparing Phi-Mamba against open-source models of similar sizes pretrained on standard language modeling corpuses. Phi-Mamba attains performance close to the teacher model and better than all pretrained models, while using less than 1% of the training data.", "description": "This table presents a comparison of the performance of various language models on several downstream tasks.  The key model being evaluated is Phi-Mamba, a distilled model trained with significantly less data than the other models.  The table shows that Phi-Mamba achieves comparable performance to the original (teacher) model and outperforms other open-source models of similar size. This highlights the effectiveness of the knowledge distillation approach used in training Phi-Mamba.", "section": "Empirical Validation"}, {"figure_path": "FJlrSZBMCD/tables/tables_6_2.jpg", "caption": "Table 2: MOHAWK distillation from Phi-1.5 teacher model to Phi-Mamba-1.5B. \"Stages Applied\" details which of the three MOHAWK stages was performed, highlighting the importance of each stage. All experiments executed using a fixed amount of 5B tokens for the entire distillation process.", "description": "This table presents the results of applying different stages of the MOHAWK distillation process. It shows the impact of each stage on several downstream tasks, highlighting the importance of using all three stages for optimal performance. A fixed token budget was used across all experiments.", "section": "5 Empirical Validation"}, {"figure_path": "FJlrSZBMCD/tables/tables_8_1.jpg", "caption": "Table 3: Attention matrix approximation by structured matrix mixers (Frobenius distance; lower is better). Structures are Toeplitz, low-rank (LR), state space dual (SSD) model (3.2) and general semi-separable matrices (SSM), all causal. We used 1,000 samples, each 512 tokens. Samples were passed through Llama2-7B-Chat, and one attention head from each layer was randomly chosen for approximation. We evaluated (LR) and SSD families with 10,000 gradient descent steps per sample.", "description": "This table shows the Frobenius distance between self-attention matrices from Llama2-7B-Chat and their approximations using different structured matrix families: Toeplitz, low-rank, state-space dual (SSD), and semi-separable matrices (SSM). Lower values indicate better approximation quality. The experiment used 1000 samples of 512 tokens each, and the (LR) and SSD families were optimized using 10,000 gradient descent steps.", "section": "6 Self-Attention Approximation with Structured Matrix Mixers"}, {"figure_path": "FJlrSZBMCD/tables/tables_9_1.jpg", "caption": "Table 4: Ablations of matrix structure using the same training recipe (Stages 2 and 3). While many efficient sequence models (e.g. global convolutions, linear attention, and state space models) can be represented as structured matrix mixers (e.g. Toeplitz, low-rank, and semi-separable matrices respectively), more expressive structured matrix families can match the attention matrix more closely.", "description": "This table presents an ablation study comparing different matrix structures (Toeplitz, low-rank, SSD, and semi-separable) used in the Phi-Mamba model's sequence mixer.  It shows that more expressive structures, like the semi-separable matrices, better approximate the original Transformer's attention matrix, leading to improved performance on downstream tasks (Winogrande, ARC-E, ARC-C, PIQA, HellaSwag).  The L2 distance metric quantifies the difference between the student model's matrix mixer and the teacher model's attention matrix. Lower L2 distance implies a better approximation.", "section": "5 Empirical Validation"}, {"figure_path": "FJlrSZBMCD/tables/tables_9_2.jpg", "caption": "Table 5: MOHAWK distillation for Phi-Mamba-1.5B on the entire model vs just the Mamba-2 component, i.e., frozen MLP, embedding, etc. MOHAWK can be used on just the sequence mixer blocks while keeping all other components frozen without compromising performance (Section 5.1).", "description": "This table compares the performance of Phi-Mamba-1.5B model when fine-tuning the entire model versus only the Mamba-2 component using the MOHAWK method.  It shows that MOHAWK can be effectively applied by only fine-tuning the sequence mixer blocks while keeping other components frozen, demonstrating the method's efficiency and modularity.", "section": "4.4 Phi-Mamba architecture"}, {"figure_path": "FJlrSZBMCD/tables/tables_15_1.jpg", "caption": "Table 3: Attention matrix approximation by structured matrix mixers (Frobenius distance; lower is better). Structures are Toeplitz, low-rank (LR), state space dual (SSD) model (3.2) and general semi-separable matrices (SSM), all causal. We used 1,000 samples, each 512 tokens. Samples were passed through Llama2-7B-Chat, and one attention head from each layer was randomly chosen for approximation. We evaluated (LR) and SSD families with 10,000 gradient descent steps per sample.", "description": "This table presents the results of approximating attention matrices using various structured matrix families.  The Frobenius distance, a measure of the difference between the original attention matrix and its approximation, is used to evaluate the quality of each approximation method.  Lower Frobenius distances indicate better approximations. The table compares Toeplitz, low-rank (LR), state space dual (SSD), and general semi-separable matrices (SSM) for their ability to approximate attention matrices.  The results are averaged across all layers of a Llama2-7B-Chat model.", "section": "6 Self-Attention Approximation with Structured Matrix Mixers"}, {"figure_path": "FJlrSZBMCD/tables/tables_17_1.jpg", "caption": "Table 1: Downstream evaluation results for full methods, comparing Phi-Mamba against open-source models of similar sizes pretrained on standard language modeling corpuses. Phi-Mamba attains performance close to the teacher model and better than all pretrained models, while using less than 1% of the training data.", "description": "This table presents a comparison of the Phi-Mamba model's performance against other open-source language models on various downstream tasks.  It shows that Phi-Mamba, despite being trained with significantly less data (less than 1% of the training data used for other models), achieves comparable or better performance than other models of similar size on tasks such as Winogrande, ARC-E, ARC-C, PIQA, HellaSwag, and LAMBADA.  This highlights the effectiveness of the knowledge distillation method used in the paper.", "section": "5 Empirical Validation"}, {"figure_path": "FJlrSZBMCD/tables/tables_17_2.jpg", "caption": "Table 1: Downstream evaluation results for full methods, comparing Phi-Mamba against open-source models of similar sizes pretrained on standard language modeling corpuses. Phi-Mamba attains performance close to the teacher model and better than all pretrained models, while using less than 1% of the training data.", "description": "This table presents a comparison of the Phi-Mamba model's performance against several other open-source language models on various downstream tasks.  These tasks assess common sense reasoning and language understanding capabilities.  The key takeaway is that Phi-Mamba, despite being trained with significantly less data (less than 1% of the others), achieves comparable or superior performance to the other models.  This highlights the effectiveness of the knowledge distillation method used to train Phi-Mamba.", "section": "5 Empirical Validation"}, {"figure_path": "FJlrSZBMCD/tables/tables_18_1.jpg", "caption": "Table 1: Downstream evaluation results for full methods, comparing Phi-Mamba against open-source models of similar sizes pretrained on standard language modeling corpuses. Phi-Mamba attains performance close to the teacher model and better than all pretrained models, while using less than 1% of the training data.", "description": "This table compares the performance of the Phi-Mamba model (trained using the MOHAWK method) against other open-source language models on several downstream tasks.  The key takeaway is that Phi-Mamba achieves comparable or better performance than models trained with significantly more data (over 100x more tokens). The tasks assessed include commonsense reasoning and language understanding.", "section": "5 Empirical Validation"}]