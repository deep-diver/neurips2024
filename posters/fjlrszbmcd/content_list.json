[{"type": "text", "text": "Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Aviv $\\mathbf{Bick^{1*}}$ , Kevin Y. ${\\bf{L i}}^{1*}$ , Eric P. $\\mathbf{X}\\mathbf{in}\\mathbf{g}^{12}$ , J. Zico Kolter1, Albert $\\mathbf{Gu}^{13}$ 1Carnegie Mellon University, 2MBZUAI, 3Cartesia.ai {abick, kyl2}@cs.cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Transformer architectures have become a dominant paradigm for domains like language modeling but suffer in many inference settings due to their quadratic-time self-attention. Recently proposed subquadratic architectures, such as Mamba, have shown promise, but have been pretrained with substantially less computational resources than the strongest Transformer models. In this work, we present a method that is able to distill a pretrained Transformer architecture into alternative architectures such as state space models (SSMs). The key idea to our approach is that we can view both Transformers and SSMs as applying different forms of mixing matrices over the token sequences. We can thus progressively distill the Transformer architecture by matching different degrees of granularity in the SSM: first matching the mixing matrices themselves, then the hidden units at each block, and finally the end-to-end predictions. Our method, called MOHAWK, is able to distill a Mamba-2 variant based on the Phi-1.5 architecture (Phi-Mamba) using only 3B tokens. Despite using less than $1\\%$ of the training data typically used to train models from scratch, Phi-Mamba boasts substantially stronger performance compared to all past open-source non-Transformer models. MOHAWK allows models like SSMs to leverage computational resources invested in training Transformer-based architectures, highlighting a new avenue for building such models. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models based upon Transformer architectures have become a staple of natural language processing but suffer from their reliance on quadratic self-attention \u2014 the need to compute inner products between tokens at all positions up to the context length. This has motivated the development of several alternative subquadratic models, either approximations of self-attention [23] or entirely different architectures, such as state space models (SSMs) [13, 12, 30, 35]. Training strong subquadratic models such as SSMs can benefti the community through their cheaper finetuning and inference costs; however, they have not beneftited from the same amount of community effort in the form of training and compute as for Transformers. This raises a natural question: is it possible to leverage the vast amounts of resources that have been invested in training quadratic-time Transformers and use these models to produce stronger alternative models, such as state-space models? ", "page_idx": 0}, {"type": "text", "text": "In this paper, we present an approach for training subquadratic state-space models (specifically from the class of Mamba SSMs [12]) through the distillation of different elements of a pretrained Transformer model. The key intuition is viewing both Attention and SSMs as sequence transformations that mix different token embeddings by applying different classes of matrices across them. Sequence model architectures can then be factored into separate (i) sequence mixing and (ii) channel mixing blocks, e.g., a Transformer is composed of Attention (sequence mixer) and MLP (channel mixer) ", "page_idx": 0}, {"type": "image", "img_path": "FJlrSZBMCD/tmp/6f14af5c63bd138a5a4dd66cbdf6b6d3fc06a0c0a78309bcf7cd1ad5ba5fbe8d.jpg", "img_caption": ["Figure 1: Plot of trained token budget to averaged accuracy on Winogrande, Arc-E, Arc-C, PIQA, and Hellaswag on various open-source models (mainly non-Transformer-based models). Our model (Phi-Mamba) uses more than $33\\times$ less token budget to achieve $5\\%$ higher average accuracy than the next best model. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "blocks. Using this breakdown, we can separately distill the mixing elements of each model explicitly at different levels of granularity. Specifically, we propose a three-phase distillation process that progressively targets higher levels of supervision from the teacher model: (1) a matrix orientation phase that aligns the sequence transformation matrices themselves; (2) a hidden-state distillation that aligns the hidden-state representations of each individual layer of the network without sacrificing preexisting learned representations; and (3) an end-to-end training phase with weight transfer that finally distills the final output of the network using only a fraction of training data. We term our approach MOHAWK after these three stages (Matrix Orientation, Hidden-State Alignment, Weight-Transfer and Knowledge Distillation). ", "page_idx": 1}, {"type": "text", "text": "We apply our approach to a modified instantiation of the Mamba-2 architecture [8], termed PhiMamba, which is aimed at more directly corresponding to the different architectural blocks of the Phi-1.5 language model [15] \u2014 a very strong Transformer model at the 1.3B parameter scale. Using our approach, the Phi-Mamba model achieves performance on benchmarks stronger than any previous Mamba models of similar size. Although performance still lags behind that of the base Phi-1.5 model on these benchmarks, the model is distilled with only 3.0B tokens, less than $1\\%$ of the data used to train either the previously best-performing Mamba models and $2\\%$ for the Phi-1.5 model itself. For instance, our Phi-Mamba achieves a $71.7\\%$ accuracy on the Winogrande dataset, compared to the pretrained Mamba-2 model\u2019s $60.9\\%$ accuracy, and $44.1\\%$ accuracy on the ARC-C dataset, compared to Mamba-2\u2019s $33.3\\%$ accuracy. Our results highlight the benefti of our three-phase distillation approach: we show in ablation experiments that each phase is highly beneficial for the eventual performance of the model, and that, e.g., only attempting to directly distill the Phi-1.5 model (i.e., Phase 3 alone) substantially underperforms the full MOHAWK method. Moreover, our findings emphasize the benefits of state-space models while training on fewer than $100\\times$ tokens than the original pretrained Mamba model. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Sequence Models. State-of-the-art autoregressive language models have been pretrained on massive amounts of data, resulting in models that exhibit extensive downstream capabilities, such as zeroshot translation and long-range reasoning [3, 15, 37]. Recent work has focused on addressing the quadratic complexity of Transformers by developing subquadratic alternatives based on RNN [30, 1], SSM [12, 35], and linear attention mechanisms [23, 43, 25, 7, 32], highlighting the importance of efficient sequence models in the era of large-scale autoregressive language models. ", "page_idx": 1}, {"type": "text", "text": "Distillation. Knowledge distillation can be used to transfer knowledge from a large teacher model to a smaller student model, resulting in a more efficient model that retains the performance of the teacher model [18]. Distillation has been applied to various language modeling tasks, such as text generation [5, 17], machine translation [16, 48, 36], and question-answering system [19, 44]. ", "page_idx": 2}, {"type": "text", "text": "Distillation in language models has been largely focused on compression: turning a larger pretrained Transformer into a smaller one by utilizing the weights of the teacher model [41, 21, 42]. Some of the techniques proposed look similar to ours; for example, [41] match attention matrices in a step similar to our matrix orientation, and [24] align outputs of each block (i.e., the hidden states). However, these differ in subtle and important ways because of our setting; for example, the former uses a different loss function than us that relies on softmax attention, and the latter is an end-to-end objective while our hidden state alignment occurs completely independently block-per-block. Consequently, prior work has observed that combining these objectives does not actually help or even hurts distillation [21], whereas we show that our techniques all significantly help improve the student model. ", "page_idx": 2}, {"type": "text", "text": "A smaller body of work has focused on our objective of distilling across architectures, in particular, turning a pretrained Transformer into a different architecture (usually some recurrent model) of the same size. [22] converted a pretrained softmax attention into linear attention by directly transferring weights and continuing fine-tuning. A similar approach was taken by concurrent works for converting Attention into linear RNNs [27, 40]. Recently, [47, 46] also proposed distilling into linear attention by first matching attention matrices. Our approach differs by using a different loss function that works beyond linear attention; incorporating more stages (e.g., the hidden state alignment step); and using recent, more expressive classes of efficient student models (Mamba-2), which we show are significantly easier to distill into (Table 4). ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To facilitate a clear understanding of our distillation approach, we start with the necessary background and definitions. An overview of the Mamba-2 architecture, which forms the foundation of our Phi-Mamba model, is also provided. ", "page_idx": 2}, {"type": "text", "text": "3.1 Matrix Mixers ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Following [8], we refer to an equivalent function that represents the input and output of a sequence model as a sequence transformation or a sequence mixer. Formally, ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (Sequence Transformation). We use the term to refer to a parameterized map on sequences $Y\\,=\\,f_{\\theta}(X)$ where $\\mathbf{X},\\mathbf{Y}\\in\\mathbb{R}^{(T,P)}$ and $\\theta$ is an arbitrary collection of parameters. $T$ represents the sequence or time axis; subscripts index into the first dimension, e.g. $\\bar{X}_{t},Y_{t}\\in\\mathbb{R}^{P}$ . ", "page_idx": 2}, {"type": "text", "text": "To put it differently, sequence mixers combine tokens at various time steps, facilitating the model\u2019s comprehension of temporal information and interactions. Sequence transformations form the foundation of deep sequence models, being integral components of neural network frameworks such as Transformers. A particular family of sequence transformations can be represented by $\\mathbf{Y}=\\mathbf{M}\\mathbf{X}$ for a matrix $\\mathbf{M}\\in\\mathbb{R}^{(T,T)}$ , which we refer to as a sequence transformation matrix or matrix mixer. An example of such a matrix mixer is the vanilla self-attention, Softmax $(\\mathbf{Q}\\mathbf{K}^{\\top})$ , which is applied to the input-dependent $\\mathbf{V}$ resulting in the familiar Softmax $(\\mathbf{Q}\\mathbf{K}^{\\top})\\mathbf{V}$ . Similarly, Linear Attention [23] has a sequence transformation matrix of the form $\\mathbf{K}^{\\top}$ . In addition, we can easily obtain their causal variants by multiplying by $\\mathbf{L}$ , a lower triangular matrix filled with 1s, to obtain $\\mathbf{L}\\circ\\operatorname{Softmax}(\\mathbf{Q}\\mathbf{K}^{\\top})$ and $\\mathbf{L}\\circ\\mathbf{Q}\\mathbf{K}^{\\top}$ , respectively. Another example is a Toeplitz matrix $\\mathbf{T}$ used to perform discrete convolution on input $\\mathbf{X}$ , resulting in TX [31]. ", "page_idx": 2}, {"type": "text", "text": "A naive approach to computing the output of a sequence transformation is to multiply the input sequence $\\mathbf{X}$ by the matrix M. However, this approach has a time complexity of $O(\\bar{T}^{2})$ , which is prohibitive for long sequences. Subquadratic sequence transformations, such as Mamba-2, have been developed to address such inefficiencies through structured matrix multiplication. ", "page_idx": 2}, {"type": "text", "text": "3.2 Mamba-2 ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Mamba-2 [8], a type of structured state space models (SSMs) [13, 11], was recently introduced. Similarly to the original Mamba model [12], Mamba-2 uses a time-varying state-space model which ", "page_idx": 2}, {"type": "text", "text": "can selectively focus on or ignore inputs due to its input-dependent parameterization of the system components. The time-varying SSM is defined as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{h_{t+1}=\\mathbf{A}_{t}h_{t}+\\mathbf{B}_{t}x_{t}}}\\\\ &{}&{y_{t}=\\mathbf{C}_{t}h_{t}\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $\\mathbf{B}_{t}$ and $\\mathbf{C}_{t}$ are input-dependent projections of the system, as in Mamba-1; however, ${\\bf A}_{t}$ is the identity matrix I multiplied by a scalar $\\alpha_{t}$ . The above formulation also differs from the previous one by treating the underlying sequence as originating from a discrete signal instead of a continuous one and therefore omits the sampling component $\\Delta t$ from the original Mamba model. ", "page_idx": 3}, {"type": "text", "text": "Importantly, Mamba-2 draws a new connection between SSMs and Transformers, termed Structured State Space Duality $(S S D)$ , which shows that a special case of SSMs can be viewed as a form of causal linear attention. In particular, fixing $\\mathbf{A}_{t}=I$ (a further restriction of Mamba-2 to $\\alpha_{t}=1$ ) results in the formulation of causal linear attention [23] with the matrices $\\mathbf{B}$ and $\\mathbf{C}$ representing the projections of the key and the query, respectively, while the input projection $\\mathbf{X}$ corresponds to the projection of the value. ", "page_idx": 3}, {"type": "text", "text": "Mamba-2 as a matrix sequence transformation. Inspired by the aforementioned connection between SSMs and Transformers, [8] shows that Mamba-2\u2019s $S S D$ mixer family is equivalent to sequentially-semi-separable matrices [4]. Formally, the SSD mixer family can be represented as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\begin{array}{r}{h_{t+1}=\\alpha_{t}\\cdot I h_{t}+\\mathbf{B}x_{t}\\quad\\Rightarrow\\quad{\\left[\\begin{array}{l l l l l}{\\alpha_{1}}&{0}&{0}&{\\cdots}&{0}\\\\ {\\alpha_{2:1}}&{\\alpha_{2}}&{0}&{\\cdots}&{0}\\\\ {\\alpha_{3:1}}&{\\alpha_{3:2}}&{\\alpha_{3}}&{\\cdots}&{0}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\alpha_{n:1}}&{\\alpha_{n:2}}&{\\alpha_{n:3}}&{\\cdots}&{\\alpha_{n}}\\end{array}\\right]}\\circ(C\\cdot B^{\\top})\\cdot X}\\end{array}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\alpha_{t:i}=\\alpha_{t-1}\\cdot\\alpha_{t-2}\\cdot\\cdot\\cdot\\alpha_{i}$ . An interesting observation is that the Mamba-2 architecture can be viewed as a causal linear attention with a learnable causal mask. ", "page_idx": 3}, {"type": "text", "text": "4 Methods ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Throughout this section, we will describe each phase of MOHAWK. Specifically, we will cover the stages of matrix orientation, hidden-state alignment, and knowledge distillation, all three of which are crucial for developing an effective student model from the pretrained Transformer model. Unlike traditional distillation techniques, the student model retains the overall architecture of the teacher model, differing only in the replacement of the attention matrix mixer with a subquadratic alternative. We will progressively unveil our architecture, Phi-Mamba, along with the specifics of its distillation process. This section concludes with an in-depth description of the Phi-Mamba architecture and its hybrid version, which surpasses the performance of other subquadratic matrix mixers. Further examinations of the effectiveness of the method and ablation studies are discussed in Section 5. ", "page_idx": 3}, {"type": "text", "text": "For clarity, the term block refers to the repeating components that form the end-to-end model. The blocks are composed of layers, such as the self-attention layer (including projections), the SSM layer (including the mixer and convolution), and the convolutional layer. In this manner, many Transformer models, like Llama [37], are viewed as a stack of alternating self-attention and MLP blocks, whereas the Phi and Phi-Mamba models are comprised of Phi blocks that have parallel Attention/SSM and MLP blocks. ", "page_idx": 3}, {"type": "text", "text": "4.1 Stage 1: Matrix Orientation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The first stage of MOHAWK aims to align the student matrix mixer with the teacher\u2019s self-attention matrix. Achieving this alignment is a two-step process: first, at every mixing layer, the student components preceding the matrix mixer are set to match the teacher\u2019s components. This ensures that each layer\u2019s input undergoes the same transformation up to the matrix mixer section. Consequently, the only variation from the input to the mixing process is the matrix calculation. We then minimize the distance between the matrix mixer, e.g., the self-attention matrix and the materialized SSM matrix (2), of each layer within the student and teacher models: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\phi}\\|\\mathrm{TeacherMixer}(\\mathbf{u})-\\mathrm{StudentMixer}_{\\phi}(\\mathbf{u})\\|_{F}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\phi$ denotes the parameters within the student\u2019s sequence mixing layer, and u indicates any arbitrary input. In our experimental setup, u was chosen as the output from the teacher model\u2019s preceding layer to better mimic the input distribution to the layer. This stage ensures that the student and teacher models have roughly similar mixing layers and sets the foundation for the subsequent stages of the distillation process. In particular, this stage can be done in parallel across all the student layers, as the inputs to the student and teacher blocks are identical. ", "page_idx": 4}, {"type": "text", "text": "For Mamba-2, we begin by setting the convolution to an identity function, effectively nullifying its initial impact. This results in the computation of the semi-separable matrix being the sole distinction between the layers. We then proceed to minimize the distance between the two matrix mixers: the semiseparable scalar identity and the attention matrix (see Figure 2). Figure 3 demonstrates the importance of this stage in the distillation process. Furthermore, Table 3 shows that the Mamba-2 matrix mixer is more expressive than popular alternatives and can closely approximate the selfattention matrix of various data samples across all layers of a Transformer model through gradient descent, solidifying it as a strong sequence mixer. ", "page_idx": 4}, {"type": "text", "text": "4.2 Stage 2: Hidden-State Alignment ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Following the optimization of Equation (3), we must still address the differences between the outputs of the student and teacher blocks. To achieve this, we further align the components of the two blocks using initialization and distillation. Specifically, our goal is to match each student and teacher mixing blocks by minimizing the L2 norm of their output (e.g., the entire Mamba block with the self-attention block): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\phi}\\|\\mathrm{AttnBlock}(\\mathbf{u})-\\mathrm{StudentMixerBlock}_{\\phi}(\\mathbf{u})\\|_{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where similar to Section 4.1, $\\phi$ represents student\u2019s block parameters, and $\\mathbf{u}$ is an input. Once again, this stage can be done in parallel across all the student layers. ", "page_idx": 4}, {"type": "text", "text": "In the case of Mamba-2, we modify the remaining components to be identical to the Phi-1.5\u2019s Attention block, so that the overall functionality is preserved from Stage 1. Concretely, we initialize the gate (see Figure 2) to a constant value of 1 to \u201copen\u201d the gate, canceling its initial effect. In addition, we remove the normalization prior to the output projection, as it cannot be set to align with the Attention block. We then minimize the distance between the output of the Mamba-2 block and the output of the teacher\u2019s self-attention block. Our analysis indicates that the distance between the Mamba-2 block and the self-attention block is strongly correlated with the model\u2019s ability to learn the teacher\u2019s distribution, as shown in Table 3. Furthermore, Figure 3 shows that a better independent alignment of the student and teacher blocks results in performance improvements, highlighting the importance of this stage in the distillation process. ", "page_idx": 4}, {"type": "text", "text": "4.3 Stage 3: Weight-Transfer and Knowledge Distillation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The final stage of the distillation process aims to fine-tune the student model to match the performance of the teacher model. Although each student mixing block is aligned with its corresponding teacher mixing block, discrepancies are still present between consecutive blocks throughout the network To bridge these gaps and address the remaining components of the language model, we transfer the remaining weights of the teacher model to the student\u2019s respective components. For Phi-Mamba, this involves the token embedding, the final layer normalization, the Language Model head, and the MLP and input norm at each block (see Figure 2). We then fine-tune the complete end-to-end student model under teacher supervision. Concretely, we use a distillation loss to encourage the student model to mimic the distribution of the teacher model\u2019s logits, also known as knowledge distillation [18]: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\phi}\\mathcal{L}_{\\mathrm{CE}}\\big(\\mathrm{TeacherModel}(\\mathbf{x}),\\mathrm{StudentModel}_{\\phi}(\\mathbf{x})\\big)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{x}$ is the input tokens to the models. ", "page_idx": 4}, {"type": "text", "text": "It has been hypothesized that much of the information stored in language models resides in MLP blocks [28]. To utilize the work already done pretraining the teacher, MOHAWK adjusts the structure of the student blocks to utilize the MLP in the same way as the teacher model, effectively swapping the teacher\u2019s matrix mixer with that of the student. ", "page_idx": 4}, {"type": "text", "text": "Interestingly, during this step, the MLP weights can be kept frozen while keeping the model performant. This showcases Mamba-2\u2019s powerful expressiveness crucial for replacing Attention, cuts the number of trained parameters by more than half, and, in larger models, helps prevent the student model from experiencing catastrophic forgetting of the teacher model\u2019s information. We validate Mamba-2\u2019s ability to do so in Table 5. ", "page_idx": 4}, {"type": "image", "img_path": "FJlrSZBMCD/tmp/4937619ce68489293ad0ed0c89e4e1ca6a177abc037a2a5f623404997ac6fbef.jpg", "img_caption": ["Figure 2: The Phi-Mamba architecture consists of a stack of blocks, each of which contains a Mamba block and an MLP block. The Mamba block is a simplified version of the Mamba-2 block [8] that omits the non-linear activation function after the convolutional operation and the layer normalization present before the output projection, so that the parts of the model outside the matrix mixer can be transferred from the teacher model. The MOHAWK distillation process involves progressively matching fine-to-coarse parts of the model to the corresponding part of the teacher model: (1) the mixer mixer itself (2) the full Mamba vs. Attention blocks, and (3) the end-to-end model. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "4.4 Phi-Mamba architecture ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Combining the three stages of MOHAWK, we introduce the Phi-Mamba architecture, which merges the Mamba-2 model of [8] with the Phi-1.5 Transformer model of [15]. It consists of a stack of Phi-Mamba blocks (Figure 2), initialized and distilled as described in previous sections. ", "page_idx": 5}, {"type": "text", "text": "Overall, the Phi-Mamba architecture, as depicted in Figure 2, differs from the vanilla Mamba-2 architecture by modifying the structure of the SSM matrix mixer, removing components from the SSM block and incorporating dense layers from the teacher model. In particular, each Mamba-2 block was modified by removing post-convolution activation and pre-output projection normalization, while setting the gate and convolution to be identity functions. Interestingly, although these components were found to be beneficial for performance when Mamba-2 was trained from scratch [8], we find that they are unnecessary for our distillation process. ", "page_idx": 5}, {"type": "text", "text": "Two key changes were made to the Mamba-2 matrix mixer. The first was converting the SSM head structure from multi-value to multi-head, much like the multi-head attention mechanism found in Transformers [39], enabling the independent distillation of each Transformer head into a Mamba head. Moreover, we handle the sequence mixer as entirely discrete-time by making the A matrix a projection of the input and eliminating the $\\Delta$ discretization parameter. Although this formulation slightly differs from Mamba-2, the original algorithm can still be applied as a black-box method. ", "page_idx": 5}, {"type": "text", "text": "5 Empirical Validation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We empirically validate the MOHAWK framework is able to achieve better performance on various downstream benchmarks compared to previous subquadratic models of similar size. Our final PhiMamba-1.5B model is distilled on 3 billion tokens (distributed as 80M in Stage 1, 160M in Stage 2, and 2.76B tokens in Stage 3 as described in Appendix A) from the C4 dataset, with a sequence length of 2048. This constitutes less than $1\\%$ of the resources used by many top-performing subquadratic open-source models (e.g., the original Mamba-1/2 models pretrain on 315B tokens). ", "page_idx": 5}, {"type": "text", "text": "Table 1 presents a comprehensive breakdown of downstream evaluation results for our models and multiple baselines on a standard set of commonsense reasoning and language understanding tasks: WinoGrande [33], HellaSwag [45], PIQA [2], ARC-Challenge and ARC-Easy [6], and ", "page_idx": 5}, {"type": "table", "img_path": "FJlrSZBMCD/tmp/2f6286e4c07763b51377a34f5bd72fdb0ae2811ea4de71705281de442060e0a9.jpg", "table_caption": ["Table 1: Downstream evaluation results for full methods, comparing Phi-Mamba against open-source models of similar sizes pretrained on standard language modeling corpuses. Phi-Mamba attains performance close to the teacher model and better than all pretrained models, while using less than $\\bar{1}\\%$ of the training data. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "FJlrSZBMCD/tmp/f461209e2813c0f14637748185bb1f3020cfb045e9e99d95655e53ce95577647.jpg", "table_caption": ["Table 2: MOHAWK distillation from Phi-1.5 teacher model to Phi-Mamba-1.5B. \u201cStages Applied\u201d details which of the three MOHAWK stages was performed, highlighting the importance of each stage. All experiments executed using a fixed amount of 5B tokens for the entire distillation process. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "LAMBADA [29]. Figure 1 shows the performance versus the training cost of Phi-Mamba compared to many open-source baselines from the literature at similar model sizes. ", "page_idx": 6}, {"type": "text", "text": "For the remainder of this section, we will analyze the impact of the 3 stages of MOHAWK one by one. Throughout the experiments detailed in this section, we use the AdamW optimizer with $\\beta=(0.9,0.95)$ , a weight decay of 0.1, and a learning rate of $1\\times10^{-4}$ , combined with a WarmupStable-Decay (WSD) scheduler featuring $10\\%$ warmup and $10\\%$ decay. The training law figures and the final Phi-Mamba model use the regime detailed in Appendix A. ", "page_idx": 6}, {"type": "text", "text": "5.1 Stage 3 (Weight-Transfer and Knowledge Distillation) ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As described in Section 4.3, this phase employs a simple end-to-end distillation of teacher-model logits. It leverages the alignment among all sequence mixers and successive blocks to jointly fine-tune all components of the network. Experiments shown in Table 2 highlight the relevance of implementing this end-to-end alignment, with all three architectures achieving their highest scores only after this phase. Predictably, the impact of end-to-end alignment varies by architecture: models with more mixing layers similar to the teacher model see a reduced importance of this phase. ", "page_idx": 6}, {"type": "text", "text": "Stage 3 is the only stage in MOHAWK that trains the student model end-to-end and can be seen as the \u201cmain\u201d stage. Many distillation methods employ only this stage; however, Table 2 shows that using only end-to-end knowledge distillation is less than ideal. Although it is slightly advantageous to use only Stage 3 compared to only Stage 2, there is a significant gap between using only Stage 2 versus using Stage $2+3$ . As elaborated in Section 6, this phase can freeze all network components except the Mamba-2 sequence mixer without a significant performance drop. This in particular indicates that the third stage (like the other stages of MOHAWK) can operate in computationally limited settings. ", "page_idx": 6}, {"type": "text", "text": "5.2 Stage 2 (Hidden-State Alignment) ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Following the analysis of the model\u2019s end-to-end distillation in Stage 3, we evaluate the impact of aligning the hidden-state outputs of mixer blocks (Stage 2) on both the subsequent Stage 3 process ", "page_idx": 6}, {"type": "image", "img_path": "FJlrSZBMCD/tmp/40a47796dec8c5b5bfe27abac0182d6992b02eb50cb889f836fccf812fa5e805.jpg", "img_caption": ["Stage 2 Initialization Stage 3 Finetuned Stage 3 Pretrained "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 3: Training laws comparing the token budget between Stages 2 and 3, as measured by the Stage 2 metric (hidden state distance) and Stage 3 metric (perplexity). Stage 2 initializations are used as the starting checkpoint for their respective Stage 3 finetuning models. Stage 3 pretrained is trained from scratch only with weight transfer and knowledge distillation. Despite training for less tokens on Stage 3 than the Stage 3 from scratch, almost all Stage 2 initialized models eventually outperform the baseline in perplexity on a fixed budget. In general, better aligned Stage 2 initializations improve post-Stage 3 performance. ", "page_idx": 7}, {"type": "text", "text": "and overall downstream model performance. We accomplish this by training Phi-Mamba instances from scratch using Stage 2 to various token counts. From these checkpoints, we proceed to Stage 3 training, ending with different total budgets to allow us to analyze how the degree of Stage 2 \u201cpretraining\u201d impacts Stage 3 performance at various token budgets. ", "page_idx": 7}, {"type": "text", "text": "Figure 3 demonstrates that given an adequate training budget, models beginning with weights with lower hidden state distances (after Stage 2) outperform those that depend exclusively on knowledge distillation (Stage 3). These lower hidden states are also correlated with lower starting perplexities, which in turn are correlated with downstream performance, as shown in Figure 5. Furthermore, Table 2 shows the synergy between Stage 2 and Stage 3, as applying Stage 3 on top of Stage 2 outperforms vanilla knowledge distillation, highlighting the importance of incorporating both hidden-state alignment and knowledge distillation methods for the tested architectures. ", "page_idx": 7}, {"type": "text", "text": "5.3 Stage 1 (Matrix Mixer Orientation) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Motivated by our previous finding, we then analyze how matching the matrix mixers can decrease the overall mixer block\u2019s hidden-state distance with the teacher model even further. Similarly to our previous protocol, we assess the positive impact of the current stage on the following phase\u2019s metrics and final model\u2019s performance by comparing models with varying amount of Stage 1 and Stage 2 training on both stage metrics. ", "page_idx": 7}, {"type": "text", "text": "Figure 4 shows that even with constrained budgets, performing Stage 1 for a small period can help with subsequent stages and their performances. Thus, even a small amount of Stage 1 training can help their respective Stage 2 models reach better hidden-state distances compared to the fromscratch counterpart. This is despite the phenomenon that the teacher and student mixers diverge and then re-converge in Stage 2 after mixer similarity is no longer directly optimized. Coupled with Section 5.2, which discovers that lower hidden state initializations lead to better perplexity and downstream performance, it can be inferred that Stage 1 aids the overall distillation process. We further empirically validate this intuition in Table 2, which indicates that this stage aligns the matrix mixers to a stronger degree than only the hidden state alignment. ", "page_idx": 7}, {"type": "text", "text": "6 Self-Attention Approximation with Structured Matrix Mixers ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We start by testing the ability of various matrix mixer families to match the empirical self-attention matrices of a pretrained Transformer. We take 1000 samples from each layer of a Llama2-7b-Chat model [37], materialize the attention matrices, and project them onto given classes of structured matrices. The results in Table 3 are averaged across all layers. ", "page_idx": 7}, {"type": "image", "img_path": "FJlrSZBMCD/tmp/6fcd2782d052233bdc98b05c7d4b1549bb9bc3e71cd8508bb73cb5d17d65d33f.jpg", "img_caption": ["Stage 1 Initialization Stage 2 Finetuned Stage 2 Pretrained "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 4: Training laws comparing the token budget between Stages 1 and 2, as measured by the Stage 1 metric (matrix mixer distance) and Stage 2 metric (hidden state distance). Even a small amount of Stage 1 training can improve the model\u2019s hidden-state distances in subsequent stages. Notably, this improvement occurs despite an increase in matrix mixer distance during Stage 2. This suggests that early Stage 1 training provides a foundational benefti that enhances the model\u2019s performance in later stages, demonstrating the importance of initial training phases in model optimization. ", "page_idx": 8}, {"type": "table", "img_path": "FJlrSZBMCD/tmp/0c0e17589a41f628d2ce086fe9766466b4291dd76576c15a0030fc047f42c3ad.jpg", "table_caption": ["Table 3: Attention matrix approximation by structured matrix mixers (Frobenius distance; lower is better). Structures are Toeplitz, low-rank (LR), state space dual (SSD) model (3.2) and general semi-separable matrices (SSM), all causal. We used 1,000 samples, each 512 tokens. Samples were passed through Llama2-7B-Chat, and one attention head from each layer was randomly chosen for approximation. We evaluated (LR) and SSD families with 10,000 gradient descent steps per sample. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "In particular, to describe the class of linear attention matrices (3.1), we use the fact that $\\mathbf{Q}$ and $\\mathbf{K}$ are projections of the input $\\boldsymbol{x}\\in\\mathbb{R}^{d_{i n}}$ onto $\\mathbb{R}^{d_{o u t}}$ , and therefore their rank is bounded by min $\\{d_{i n},d_{o u t}\\}$ For multihead linear attention, $d_{o u t}$ (also known as head dimension) is typically a small value (e.g., Phi-1.5 and Llama2-7b-Chat have head dimensions of 64 and 128, respectively). Thus, we approximate this family of sequence mixers using causal low-rank matrices $\\bar{\\mathbf{L}}\\circ\\mathbf{Q}\\mathbf{K}^{\\dag}$ , where $\\mathbf{L}$ is a lower-triangular causal mask of 1s, and $\\mathbf{Q}$ , $\\mathbf{K}$ are in $\\mathbb{R}^{n\\times d}$ with $d\\ll n$ (indicating that the head dimension is substantially smaller than the sequence length). ", "page_idx": 8}, {"type": "text", "text": "To describe the multi-head Mamba-2 matrix family, we utilize the state space dual (SSD) layer (3.2) in a manner similar to the previous linear attention, but now the causal matrix $\\mathbf{L}$ possesses an $n$ -degree rolling multiplicative structure for SSD which can be seen as a more expressive mask that generalizes the causal mask (Section 3.2). ", "page_idx": 8}, {"type": "text", "text": "Both causal low-rank and SSD matrix families were approximated with 10,000 steps of gradient descent per sample. To approximate the general class of SSM matrix mixers, we utilize balanced truncation, a gradient-free projection algorithm. This method is mainly known in the field of time-invariant Dynamical System model reduction [14] and has been modified for use in timevarying systems [34]. Similarly, for the family of causal Toeplitz matrices, representing convolution operations, we employ a simple heuristic that minimizes the error for each attention matrix. ", "page_idx": 8}, {"type": "text", "text": "Table 3 shows that while the SSM matrix family provides the closest approximation to the selfattention matrix mixer, the Mamba-2 mixer family (SSD) has just twice the distance from the SSM matrices. This is in contrast to Linear Attention, which has three times the distance, all while keeping a computational cost on par with Linear Attention. More details can be found in Appendix C. ", "page_idx": 8}, {"type": "text", "text": "We further validate the ability of a Mamba-2 block to replace an Attention layer within a language model. Firstly, we create two variants of our architecture, Phi-Toeplitz and Phi-LR, and run the MOHAWK process for 1B tokens at each stage (see Table 4) to verify that the previous finding ", "page_idx": 8}, {"type": "text", "text": "Table 4: Ablations of matrix structure using the same training recipe (Stages 2 and 3). While many efficient sequence models (e.g. global convolutions, linear attention, and state space models) can be represented as structured matrix mixers (e.g. Toeplitz, low-rank, and semi-separable matrices respectively), more expressive structured matrix families can match the attention matrix more closely. ", "page_idx": 9}, {"type": "table", "img_path": "FJlrSZBMCD/tmp/e9b435bdbd206e137e989803c3a72421ef8eac531ba4af70c04124e0450bff1e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "FJlrSZBMCD/tmp/3d855bc594d515d696afd13a76c455dc02527a6ee864ec4253e67b5b655596d9.jpg", "table_caption": ["Table 5: MOHAWK distillation for Phi-Mamba-1.5B on the entire model vs just the Mamba-2 component, i.e., frozen MLP, embedding, etc. MOHAWK can be used on just the sequence mixer blocks while keeping all other components frozen without compromising performance (Section 5.1). "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "hold in a multilayer, end-to-end model case. Secondly, we run MOHAWK while freezing various parts of the Phi-Mamba modules (refer to Table 5), revealing that limiting the trainable elements to the Mamba-2 blocks (excluding the embedding, head and all MLP layers) results in only a minor performance decrease during MOHAWK distillation. ", "page_idx": 9}, {"type": "text", "text": "Interestingly, in all of the aforementioned experiments, we have found a consistent correlation between the projection distances of the matrix (Frobenius distance) in Table 3 and the downstream performance metrics (accuracy) in Table 4. Essentially, a better matrix approximation (lower Frobenius distance) is correlated with better model performance (higher accuracy) on various tasks. This connection highlights the relationship between the quality of the matrix approximation and the performance of the model. Such findings are echoed in [20], which find that more expressive matrix mixers lead to more performant models, e.g., Low-rank-based BERT models outperform Toeplitz-based ones. ", "page_idx": 9}, {"type": "text", "text": "7 Discussion and Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our experiments shows that the Mamba-2 model can be successfully distilled from a pretrained Transformer teacher model, utilizing its extensive knowledge learned from custom datasets and higher computational resources. Despite using less than $100\\times$ data compared to many open-source models, including Mamba, our subquadratic model outperforms other subquadratic models in various benchmark tests by a wide margin. ", "page_idx": 9}, {"type": "text", "text": "The MOHAWK framework\u2019s multi-stage process which gradually increased the scope of distillation is essential extracting the teacher model\u2019s knowledge to the fullest extent as shown in our ablations and training laws. We continue to find the effectiveness of MOHAWK when distilling hybrid Attention-SSM models and provide ablations on the number and position of Attention layers. ", "page_idx": 9}, {"type": "text", "text": "Additionally, we demonstrate that Mamba-2\u2019s relationship to Transformers is evident not only in theory, but also in practice, as it captures interactions similar to those of Transformers, and is able to replace Attention with little drop in performance. Coupled with past research which has posited that much of a language model\u2019s knowledge is embedded in the MLP blocks, we believe that any subquadratic model with a sufficiently expressive matrix mixer can replicate the behavior of pretrained Transformers, bringing quadratic knowledge to subquadratic models. We recommend further research to explore the role of sequence mixing layers in subquadratic models and their impact on performance. Advancements in both the distillation process and the sequence mixer architecture could lead to further improved performance in a range of tasks. We propose that \u201ctrainability\u201d and \u201cdistillability\u201d are distinct properties of the models, and therefore, distillation techniques should be more appropriately tailored to the model. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] M. Beck, K. P\u00f6ppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter. xlstm: Extended long short-term memory, 2024.   \n[2] Y. Bisk, R. Zellers, J. Gao, Y. Choi, et al. PIQA: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on Artificial Intelligence, volume 34, pages 7432\u20137439, 2020.   \n[3] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners, 2020.   \n[4] S. Chandrasekaran, P. Dewilde, M. Gu, T. Pals, and A.-J. van der Veen. Fast stable solver for sequentially semi-separable linear systems of equations. In International Conference on High-Performance Computing, pages 545\u2013554. Springer, 2002.   \n[5] Y.-C. Chen, Z. Gan, Y. Cheng, J. Liu, and J. Liu. Distilling knowledge learned in bert for text generation, 2020.   \n[6] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try ARC, the AI2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.   \n[7] T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. R\u00e9. Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022.   \n[8] T. Dao and A. Gu. Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality. In International Conference on Machine Learning (ICML), 2024.   \n[9] P. Dewilde and A. J. van der Veen. On the hankel-norm approximation of upper-triangular operators and matrices. Integral Equations and Operator Theory, 17(1):1\u201345, 1993.   \n[10] P. Dewilde and A.-J. Veen. Time-Varying Systems and Computations. Springer Book Archive. Springer New York, NY, 1 edition, 1998. Springer Science+Business Media Dordrecht 1998.   \n[11] A. Gu. Modeling Sequences with Structured State Spaces. Phd thesis, Stanford University, 2023.   \n[12] A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2023.   \n[13] A. Gu, K. Goel, and C. R\u00e9. Efficiently modeling long sequences with structured state spaces, 2022.   \n[14] S. Gugercin and A. C. Antoulas. A survey of model reduction by balanced truncation and some new results. International Journal of Control, 77(8):748\u2013766, 2004.   \n[15] S. Gunasekar, Y. Zhang, J. Aneja, C. C. T. Mendes, A. D. Giorno, S. Gopi, M. Javaheripi, P. Kauffmann, G. de Rosa, O. Saarikivi, A. Salim, S. Shah, H. S. Behl, X. Wang, S. Bubeck, R. Eldan, A. T. Kalai, Y. T. Lee, and Y. Li. Textbooks are all you need, 2023.   \n[16] S. Hahn and H. Choi. Self-knowledge distillation in natural language processing, 2019.   \n[17] M. A. Haidar and M. Rezagholizadeh. Textkd-gan: Text generation using knowledgedistillation and generative adversarial networks, 2019.   \n[18] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network, 2015.   \n[19] M. Hu, Y. Peng, F. Wei, Z. Huang, D. Li, N. Yang, and M. Zhou. Attention-guided answer distillation for machine reading comprehension, 2018.   \n[20] S. Hwang, A. Lahoti, T. Dao, and A. Gu. Hydra: Bidirectional state space models through generalized matrix mixers, 2024.   \n[21] A. H. Jha, D. Groeneveld, E. Strubell, and I. Beltagy. Large language model distillation doesn\u2019t need a teacher. arXiv preprint arXiv:2305.14864, 2023.   \n[22] J. Kasai, H. Peng, Y. Zhang, D. Yogatama, G. Ilharco, N. Pappas, Y. Mao, W. Chen, and N. A. Smith. Finetuning pretrained transformers into rnns. arXiv preprint arXiv:2103.13076, 2021.   \n[23] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention, 2020.   \n[24] C. Liang, H. Jiang, Z. Li, X. Tang, B. Yin, and T. Zhao. Homodistil: Homotopic task-agnostic distillation of pre-trained transformers. arXiv preprint arXiv:2302.09632, 2023.   \n[25] H. Liu, M. Zaharia, and P. Abbeel. Ring attention with blockwise transformers for near-infinite context, 2023.   \n[26] S. A. Melchior, P. Van Dooren, and K. A. Gallivan. Model reduction of linear time-varying systems over finite horizons. Applied Numerical Mathematics, 77:72\u201381, 2014.   \n[27] J. Mercat, I. Vasiljevic, S. Keh, K. Arora, A. Dave, A. Gaidon, and T. Kollar. Linearizing large language models, 2024.   \n[28] J. Niu, A. Liu, Z. Zhu, and G. Penn. What does the knowledge neuron thesis have to do with knowledge?, 2024.   \n[29] D. Paperno, G. Kruszewski, A. Lazaridou, N.-Q. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda, and R. Fern\u00e1ndez. The LAMBADA dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1525\u20131534, 2016.   \n[30] B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, S. Biderman, H. Cao, X. Cheng, M. Chung, M. Grella, K. K. GV, X. He, H. Hou, J. Lin, P. Kazienko, J. Kocon, J. Kong, B. Koptyra, H. Lau, K. S. I. Mantri, F. Mom, A. Saito, G. Song, X. Tang, B. Wang, J. S. Wind, S. Wozniak, R. Zhang, Z. Zhang, Q. Zhao, P. Zhou, Q. Zhou, J. Zhu, and R.-J. Zhu. Rwkv: Reinventing rnns for the transformer era, 2023.   \n[31] Z. Qin, X. Han, W. Sun, B. He, D. Li, D. Li, Y. Dai, L. Kong, and Y. Zhong. Toeplitz neural network for sequence modeling, 2023.   \n[32] Z. Qin, S. Yang, W. Sun, X. Shen, D. Li, W. Sun, and Y. Zhong. Hgrn2: Gated linear rnns with state expansion, 2024.   \n[33] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial Winograd schema challenge at scale. Communications of the ACM, 64(9):99\u2013106, 2021.   \n[34] H. Sandberg and A. Rantzer. Balanced truncation of linear time-varying systems. IEEE Transactions on Automatic Control, 49(2):217\u2013229, 2004.   \n[35] Y. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue, J. Wang, and F. Wei. Retentive network: A successor to transformer for large language models, 2023.   \n[36] X. Tan, Y. Ren, D. He, T. Qin, Z. Zhao, and T.-Y. Liu. Multilingual neural machine translation with knowledge distillation, 2019.   \n[37] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient foundation language models, 2023.   \n[38] A. van der Veen and P. Dewilde. On low-complexity approximation of matrices. Linear Algebra and its Applications, 205-206:1145\u20131201, 1994.   \n[39] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need, 2023.   \n[40] J. Wang, D. Paliotta, A. May, A. M. Rush, and T. Dao. The mamba in the llama: Distilling and accelerating hybrid models, 2024.   \n[41] W. Wang, F. Wei, L. Dong, H. Bao, N. Yang, and M. Zhou. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. Advances in Neural Information Processing Systems, 33:5776\u20135788, 2020.   \n[42] M. Xia, T. Gao, Z. Zeng, and D. Chen. Sheared llama: Accelerating language model pre-training via structured pruning. arXiv preprint arXiv:2310.06694, 2023.   \n[43] S. Yang, B. Wang, Y. Shen, R. Panda, and Y. Kim. Gated linear attention transformers with hardware-efficient training, 2024.   \n[44] Z. Yang, L. Shou, M. Gong, W. Lin, and D. Jiang. Model compression with two-stage multiteacher knowledge distillation for web question answering system, 2019.   \n[45] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019.   \n[46] M. Zhang, S. Arora, R. Chalamala, A. Wu, B. Spector, A. Singhal, K. Ramesh, and C. R\u00e9. Lolcats: On low-rank linearizing of large language models, 2024.   \n[47] M. Zhang, K. Bhatia, H. Kumbong, and C. R\u00e9. The hedgehog & the porcupine: Expressive linear attentions with softmax mimicry, 2024.   \n[48] C. Zhou, G. Neubig, and J. Gu. Understanding knowledge distillation in non-autoregressive machine translation, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Experiments and Experimental Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Hyperparameter Search To construct Appendix A, we performed grid searches for training in Stages 1, 2, and 3 independently from scratch to find the optimal hyperparameters. We explored learning rates $\\mathrm{lr}\\,=\\,\\{1,2,5\\}\\times10^{\\{-3,-4\\}}$ and batch sizes $2^{\\{15,16,17,18\\}}$ . AdamW Optimizer was used with $\\beta\\,=\\,(0.9,0.95)$ , incorporating a weight decay of 0.1, gradient clipping at 1.0, and a Warmup-Stable-Decay (WSD) scheduler with $10\\%$ warmup and $10\\%$ decay utilizing linear warmup and cooldown functions. Automatic mixed precision training to bf16 was used in all stages. For Stages 1 and 2, we initially fixed the batch size at $2^{16}$ , then varied the learning rates. After identifying the optimal learning rate, we adjusted the batch sizes and subsequently finalized the learning rate after fixing the batch size. Consequently, Stage 1 used bs $=2^{15}$ , $\\mathrm{lr}=\\dot{5}\\times10^{-4}$ and Stage 2 used $\\mathrm{bs=2^{15},\\bar{l r}=2\\times10^{-3}}$ . In Stage 3, we set the batch size to $2^{19}\\approx0.5\\mathrm{M}$ and focused solely on varying the learning rate, resulting in $5\\times10^{-4}$ . Stages 1 and 2 were trained to 200M steps each while Stage 3 extended to 1B steps. For the Phi-Mamba ultimate model, the Stage 3 learning rate was reduced to $2\\times10^{-4}$ to enhance stability. ", "page_idx": 13}, {"type": "text", "text": "Multi-Stage Distillation Procedure In the development of the training law (see Figure 3), we executed a single \"continuous\" run initialized from a state that included several checkpoints. The warm-up period was determined as $10\\%$ of the tokens processed during the continuous run. For instance, if the model\u2019s goal was to process 640 million tokens, and it started from a run that had processed 40 million tokens, then the warm-up would be set at 60 million tokens. The checkpoints recorded during the warm-up phase were preserved as they were, while subsequent checkpoints underwent a cooling of $10\\%$ of the current phase. To illustrate, in the scenario mentioned earlier, a checkpoint at 320 million tokens during the 40M to 640M run would maintain the original warmup, while the cooldown would span 28 million tokens. Conversely, a checkpoint at 80 million tokens within the warm-up phase would be saved without any cooldown. ", "page_idx": 13}, {"type": "text", "text": "Training Laws on Downstream Metrics Figure 5 extends the Stage 2 versus Stage 3 comparison in Figure 3, except we measure average accuracy on downstream metrics instead of perplexity. We observe a strong correlation between the training laws of perplexity and downstream evaluation metrics. While the general trend indicates that models exposed to more tokens during the prior stage initialization tend to perform better on both perplexity and downstream metrics, the relationship is not perfectly aligned. Specifically, the order of model performance based on perplexity does not always match the order based on downstream metrics, highlighting some differences in how these metrics capture model effectiveness. ", "page_idx": 13}, {"type": "text", "text": "Training the Final Phi-Mamba Model After confirming the importance of the stages in Section 5.1, Section 5.2, and Section 5.3, we proceed to distill the final Phi-Mamba model using the three elements of MOHAWK. We use 80M tokens for Stage 1, due to the strong performance of the token count in both the matrix and hidden state distances (Figure 4). Stage 2 was distilled for 160M tokens given the apparent saturation of both hidden state distance and perplexity compared to the other initialization states, such as 10M, 20M, 40M, etc. (Figure 3). We employed Stage 3 to a total of 3B tokens across all stages and observed that the previously optimal learning rate applied for training training laws led to instabilities in training, particularly spikes in evaluation perplexity. Decreasing the learning rate for Stage 3 mitigated this issue as mentioned above. We hypothesize that the instability is due to the Stage $1+2$ initialization\u2019s Mamba component being quite similar to that of the teacher model, so a large learning rate coupled with disconnect between blocks, which are mended in Stage 3, can cause training instabilities. The performance of the final model is reported in Table 1. ", "page_idx": 13}, {"type": "text", "text": "B Applying Mamba-2 as a Black Box ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "As noted previously Section 4.4, our Mamba-based sequence mixer is slightly modified from the original to make it more amenable for distilling from a Transformer architecture. In particular, the Mamba-2 sequence mixer is treated entirely in discrete time by projecting the input onto the matrix A and removing the discretization parameter $\\Delta$ . Even though this formulation is somewhat different from Mamba-2, the original algorithm remains applicable through a reduction expressed in Appendix B. ", "page_idx": 13}, {"type": "image", "img_path": "FJlrSZBMCD/tmp/408fb98835d6fff17fbacbdad1487953c27bf74c0249c0770c380dd59af4b5cb.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 5: Training laws comparing the amount of token budget between Stages 2 and 3, as measured by the average accuracy of downstream evaluation metrics. ", "page_idx": 14}, {"type": "text", "text": "Listing 1 PyTorch example for using the Mamba algorithm for a Delta-free variation. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "X: (batch, seqlen, nheads, headdim) A_log: (batch, seqlen, nheads) B: (batch, seqlen, nheads, dstate) C: (batch, seqlen, nheads, dstate) D: (nheads) ", "page_idx": 14}, {"type": "text", "text": "y = Mamba( X = X / A_log.unsqueeze(-1), $\\tt d t=$ rearrange(A_log, \"b c h -> b h c\"), ${\\texttt A}=$ torch.ones(self.nheads), $\\texttt{B}=\\texttt{B}$ , ${\\tt C}~=~{\\tt C}~$ , ", "page_idx": 14}, {"type": "text", "text": "Du $=$ torch.einsum(\"h,blhp->blhp\", D, X) $\\texttt{y}=$ rearrange(y $^+$ Du, \"b l h p -> b l (h p)\") ", "page_idx": 14}, {"type": "text", "text": "C Attention Matrix Approximation Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This section serves as a complement to Section 6 and outlines the methods employed to create Table 3. Appendices C.1 to C.5 describe our strategies for finding a matrix within the specified families that closely approximates the original attention matrix using a selected distance metric. Formally, we consider the following optimization problem: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}\\in\\mathcal{M}}||\\mathbf{M}-\\mathbf{X}||\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\mathcal{M}$ is the subspace of a specific matrix family, $\\mathbf{M}$ is the attention matrix, and $\\left\\Vert\\cdot\\right\\Vert$ corresponds to a selected distance metric. In the following sections, we explore different methods and matrix families for this optimization problem. ", "page_idx": 14}, {"type": "text", "text": "C.1 Semi-Separable Matrix Approximation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Considering a time-varying system denoted by $\\{\\mathbf{A_{k}},\\mathbf{B_{k}},\\mathbf{C_{k}},\\mathbf{D_{k}}\\}_{k\\in[l]}$ , we can describe it using the matrix mixer $T$ (also known as the transfer matrix) as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\nT=\\left[\\begin{array}{c c c c c c c c}{D_{1}}&{0}&{0}&{0}&{0}&{\\cdots\\cdots}&{0}\\\\ {C_{2}B_{1}}&{D_{2}}&{0}&{0}&{0}&{\\cdots\\cdots}&{0}\\\\ {C_{3}A_{2}B_{1}}&{C_{3}B_{2}}&{D_{3}}&{0}&{0}&{\\cdots\\cdots}&{0}\\\\ {C_{4}A_{3:2}B_{1}}&{C_{4}A_{3}B_{2}}&{C_{4}B_{3}}&{D_{4}}&{0}&{\\cdots}&{0}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {C_{l}A_{l-1:2}B_{1}}&{C_{l}A_{l-1:3}B_{2}}&{C_{l}A_{l-1:4}B_{3}}&{C_{l}A_{l-1:5}B_{4}}&{\\cdots}&{C_{l}B_{l-1}}&{D_{l}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "With $\\mathbf{A}_{\\mathbf{k}}\\in\\mathbb{R}^{n\\times n}$ , $\\mathbf{B_{k}}\\in\\mathbb{R}^{m\\times n}$ , $\\mathbf{C_{k}}\\in\\mathbb{R}^{p\\times n}$ , and $\\mathbf{D_{k}}\\in\\mathbb{R}^{p\\times m}$ , where $n$ is the state dimension, $m$ the input dimension, and $p$ the output dimension. ", "page_idx": 14}, {"type": "text", "text": "Table 6: Full attention matrix approximation by structured matrix mixers Structures are Toeplitz, causal low-rank (LR), RetNet, state space dual (SSD) model (3.2) with and without the diagonal D term and general semi-separable matrices (SSM). We have used 1,000 samples, each consisting of 512 tokens. Llama2-7B-Chat was applied on every sample, and one attention head from each layer was randomly chosen for approximation. We evaluated (LR), RetNet, and SSD families with 10,000 gradient descent steps per sample. ", "page_idx": 15}, {"type": "table", "img_path": "FJlrSZBMCD/tmp/dd4c9d5ba7a73381542a356585b3d7f860e4beb1e1cabbe97eedad7d058d7960.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "As T\u2019s form corresponds to a semi-separable matrix (i.e., each sub-matrix has a rank of up to $n$ ), we will label this matrix form as SSM with state size $n$ throughout the remainder of this appendix, representing a state-space model with state size $n$ or a semi-separable matrix of order $n$ . ", "page_idx": 15}, {"type": "text", "text": "Every matrix can be represented as a linear combination of rank-one matrices. Thus, the attention matrix $\\mathbf{M}\\in\\mathbb{R}^{l\\times l}$ can be interpreted as an SSM with a state size of up to $l\\gg n$ . Consequently, we can employ prior research on time-varying model order reduction [9, 26, 38] to reduce $\\mathbf{M}$ to an SSM with a smaller state size $n$ . Specifically, we utilize the following SVD-based approximation: ", "page_idx": 15}, {"type": "text", "text": "Input: Attention matrix $\\mathbf{M}\\in\\mathbb{R}^{L\\times L}$ , state size $n$ Output: Approximated attention matrix $\\mathbf{\\tilde{M}}\\in\\mathbb{R}^{L\\times L}$ Procedure: ", "page_idx": 15}, {"type": "text", "text": "1.1 Define $H_{k}$ as the submatrix of $\\mathbf{M}$ below and to the left of entry $M_{k,k}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\nH_{k}=\\left[\\stackrel{\\cdot M_{k,1}}{\\vdots}\\right.\\cdot..\\quad M_{k,k-1}\\atop\\cdot\\quad\\cdot\\quad\\vdots}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "1.2 Perform the SVD on $\\mathbf{H_{k}}$ and truncate it to rank $n$   \n1.3 Integrate the truncated $\\mathbf{H_{k}}$ back into the new matrix \u02dcM ", "page_idx": 15}, {"type": "text", "text": "Note that the diagonal elements of $\\mathbf{M}$ were not subject to approximation in Algorithm 1 as they remain unchanged. ", "page_idx": 15}, {"type": "text", "text": "Although this approximation method for the semi-separable matrix is heuristic, it has been empirically shown to deliver good results. For further details on approximation methods for semi-separable matrices, and the theoretical background behind them, we refer the reader to [10, 26] ", "page_idx": 15}, {"type": "text", "text": "C.2 Causal Low-rank Matrix Approximation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Given a set of self-attention matrices, we tried to find how close an causal low-rank matrix could approximate $M=\\operatorname{Softmax}(\\mathbf{Q}\\mathbf{K}^{\\top})$ . To ensure the state size $N$ , or in this case rank of $N$ , of the approximation $\\widetilde{M}$ , we composed $\\widetilde{\\mathbf{M}}=\\mathbf{L}\\circ\\mathbf{A}\\mathbf{B}^{\\top}$ where $\\mathbf{A},\\mathbf{B}\\in\\mathbb{R}^{D,N}$ , $\\mathbf{L}$ is a $\\mathbb{R}^{D,D}$ lower triangular mask, and $D=512$ . ", "page_idx": 16}, {"type": "text", "text": "We used the results from our causal low-rank (LR) experiments to inform much of our experimental design for later gradient descent-based approximations, which include both SSD classes (with and without $\\mathbf{D}$ matrix) and RetNet. We experimented with various different low-rank approximation solvers. We found that gradient descent performed better than alternating gradient descent. Both types of gradient descent were better than alternating least-squares which often times reached less than optimal local minima. Causal low-rank matrix approximation can also be seen as a softer version of the low-rank matrix completion problem, but a semi-definite programming (SDP) approach was not able to outperform standard gradient descent. ", "page_idx": 16}, {"type": "text", "text": "Due to our LR approximation requiring gradient descent, we selected the number of steps in relation to the time required to calculate the semi-separable approximation of the same matrix. Given the heuristic approach for converting self-attention matrices to a semi-separable form (Appendix C.1) and its ability to be parallelized, we selected the number of steps for gradient descent based on the time it took to run an entire batch of matrices (32) using gradient decent on causal low-rank versus one matrix using the semi-separable heuristic. After testing with the state sizes $N=16$ , 32, 64, we found that 10,000 steps suitable as it was around a factor of $5\\times$ compared to SSM. The 10,000 steps was maintained across all gradient based approximation classes (SSD, SSD without D, and RetNet). Experiments using the finalized step count showed AdamW provided better results compared to SGD/Adam, and the use of a scheduler provided little gain. ", "page_idx": 16}, {"type": "text", "text": "During the experiments, we also found that initialization of the matrices $\\mathbf{A},\\mathbf{B}$ played a significant role in the resulting approximation difference. The original $\\mathbf{A},\\mathbf{B}$ values were sampled from $[0,1)$ ; however, given $\\forall M_{i j}\\leq1,i,j\\in[D]$ due to the SoftMax operator, $\\mathbf{A},\\mathbf{B}$ values was then sampled from $\\textstyle\\left[0,{\\frac{1}{\\sqrt{512N}}}\\right)$ to have the last row of the self-attention matrix be uniform probability. We then proceeded to vary the factor of the range exponentially, testing $\\begin{array}{r}{\\left[0,\\frac{1}{\\sqrt{512N}}\\right)*2^{\\{-2,-1,0,1,2,4,8\\}}}\\end{array}$ where we found $\\left[0,{\\frac{1}{\\sqrt{512N}}}\\right)*2^{4}$ provided the best initialization across multiple datasets. A normal distribution with $\\mu\\,=\\,0$ and $\\sigma^{2}$ with the above tested values performed worse than the uniform distribution. Initialization experiments were conducted using the AdamW optimizer with a learning rate of 0.001 and the standard 10,000 steps. This and subsequent gradient descent classes use the same initialization for their A, B matrices. ", "page_idx": 16}, {"type": "text", "text": "For all gradient descent experiments in Table 6, Three learning rates 0.1, 0.01, 0.001 and AdamW were used for each combination of matrix class, state size, and dataset, with the best approximation being documented. The Frobenius matrix norm was used as the loss function. ", "page_idx": 16}, {"type": "text", "text": "C.3 State Space Dual (SSD) Approximation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For the SSD approximation, we utilize the scalar SSM recurrent (1SS) representation introduced in [8]. A key component is the values of $a$ , which we will refer to $l$ from here on out to avoid confusion with matrix A, that constitute the final matrix mixer M. ", "page_idx": 17}, {"type": "table", "img_path": "FJlrSZBMCD/tmp/4f221a45c38eff09130bd8ef9a93a891b02ff9eccd4ddc1c7f5ca38f8d049c1e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Given the rolling multiplicative property of $L$ and the size of n_states, initialization of $l$ was important to prevent the bottom-right values of $L$ quickly reaching 0. We explored the uniform initialization of $[0,1)+\\{-10,-8,-6,-4,-2,0,2\\}$ where smaller values of $l$ leads to less \u201cdecay\u201d within the $L$ matrix. We found sampling $l$ from $[-8,-7)$ resulted in the best performance and use this initialization in the SSD family and RetNet class. As expected, adding the D component helps reduce the error between the approximation and actual attention matrix Table 6. ", "page_idx": 17}, {"type": "text", "text": "C.4 RetNet Matrix Approximation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The Retention mechanism, introduced by [35], is a key component in RetNet models and can be represented mathematically as $(\\mathbf{Q}\\mathbf{K}^{\\top}\\cdot\\bar{\\mathbf{L}})\\bar{\\mathbf{V}}$ . Here, the matrix $\\mathbf{L}$ is defined element-wise by ", "page_idx": 17}, {"type": "equation", "text": "$$\nL_{n m}=\\binom{\\gamma^{n-m},}{0,}\\ \\ \\ n\\geq m\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\gamma$ is a decay factor. This lower triangular matrix $\\mathbf{L}$ captures the temporal dependencies by decaying past values with respect to the current position. ", "page_idx": 17}, {"type": "text", "text": "In our approximation, we replace the product QK with matrices A and $\\mathbf{B}$ . The matrix $\\mathbf{L}$ can be efficiently constructed in PyTorch using the following code, which generates a RetNet approximation: ", "page_idx": 17}, {"type": "table", "img_path": "FJlrSZBMCD/tmp/8848e7ca5c5dfe07bf060e06a5d608cb2461a33f861669418260dc4b771a6482.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "This implementation provides a practical method for simulating the Retention mechanism, crucial for reducing computational complexity in RetNet models. ", "page_idx": 17}, {"type": "text", "text": "C.5 Toeplitz Approximation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Our Toeplitz approximation technique calculates the matrix approximation by setting the value of each band of the Toeplitz matrix as the average of the values of the respective band in the attention matrix. Since each band in a Toeplitz matrix is constant along its diagonal, this method ensures that the approximation preserves the structure of the original matrix while maintaining computational efficiency. ", "page_idx": 18}, {"type": "text", "text": "To justify this approach, we observe that taking the mean per band minimizes the L2 norm (i.e., the sum of squared differences) between the original attention matrix and the approximated Toeplitz matrix. Specifically, for each band, the optimal value that minimizes the L2 difference between the two matrices is the average of the elements in that band. This is because the mean is the value that minimizes the sum of squared deviations for a set of numbers. As such, using the mean ensures that the approximation is as close as possible to the original matrix in terms of L2 distance, thereby providing a robust and efficient approximation method. ", "page_idx": 18}, {"type": "text", "text": "As before, we assume that the approximation is input-dependent, meaning that each attention matrix has its own unique Toeplitz approximation. ", "page_idx": 18}, {"type": "text", "text": "C.6 Segsum Operator ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The segsum operator computes the sum of elements across specified segments of a matrix, which, as applied in Appendices C.3 and C.4, corresponds to summing over the columns. This operation is crucial for various matrix manipulations, including the computation of the state-space dual (refer to Equation (2)). Below is the Python implementation of the \u2018segsum\u2018 operator using PyTorch. ", "page_idx": 18}, {"type": "table", "img_path": "FJlrSZBMCD/tmp/89c4e49f4a3de3ced47e7c2087eae632eb49de189ddf188f907938d687678f1d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Our claims are supported by concrete results and thorough ablations for each stage of our proposed method. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: See Section 7. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: N/A. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Our proposed method is quite simple and all significant details are included. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our proposed method is quite simple and all important details are reported.   \nThe datasets used are reported and are all standard, open source datasets. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Experimental details are reported. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our experiments are expensive for academic results; furthermore, it is common knowledge that language modeling results are reproducible across runs, when only the model initialization varies and the dataset order is fixed. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [No] ", "page_idx": 22}, {"type": "text", "text": "Justification: Experiments were run across different types of hardware with frequent checkpointing and resuming, but all using commercially available standard resources (e.g. singlenode training on A100 or H100 GPUs). ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: No additional harms beyond standard concerns exist from this line of research. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: No additional harms beyond standard concerns exist from this line of research. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: N/A ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The open source assets that we use have been credited. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 23}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] Justification: N/A Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] Justification: N/A ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] Justification: N/A ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]