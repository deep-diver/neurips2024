[{"type": "text", "text": "Toward Real Ultra Image Segmentation: Leveraging Surrounding Context to Cultivate General Segmentation Model ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sai Wang, Yutian Lin, Yu Wu,\u2217 Bo Du School of Computer Science, Wuhan University {wangsai23, wuyucs, yutian.lin, dubo}@whu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Existing ultra image segmentation methods suffer from two major challenges, namely the scalability issue (i.e. they lack the stability and generality of standard segmentation models, as they are tailored to specific datasets), and the architectural issue (i.e. they are incompatible with real-world ultra image scenes, as they compromise between image size and computing resources). To tackle these issues, we revisit the classic sliding inference framework, upon which we propose a Surrounding Guided Segmentation framework (SGNet) for ultra image segmentation. The SGNet leverages a larger area around each image patch to refine the general segmentation results of local patches. Specifically, we propose a surrounding context integration module to absorb surrounding context information and extract specific features that are beneficial to local patches. Note that, SGNet can be seamlessly integrated to any general segmentation model. Extensive experiments on five datasets demonstrate that SGNet achieves competitive performance and consistent improvements across a variety of general segmentation models, surpassing the traditional ultra image segmentation methods by a large margin. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "With the rapid development of computing and imaging equipment, ultra-high resolution images with millions or even billions of pixels emerge in endlessly and driving the need for more advanced analytical techniques. Accurate understanding of information conveyed by images [1, 47, 43] has become imperative in diverse fields such as remote sensing [29, 48, 44, 50] and medical analysis [37, 36, 31], and ultra image segmentation has emerged as an essential tool for achieving this goal. ", "page_idx": 0}, {"type": "text", "text": "To achieve ultra image segmentation, most of the methods adhere to the concept of integrating global and local information, utilizing the global context clues to aid local region refinement [6, 20]. Typically, these networks have two branches: one receives the down-sampled ultra image and extracts global context information of whole image, while the other extracts local information from sliced patches or the complete image, as shown in Fig. 1(a). The final result is generated by fusing the global and local features. Despite numerous improvements made to ultra image segmentation, we observe that it still suffer from issues in both their scalability and architecture: ", "page_idx": 0}, {"type": "text", "text": "(1) Scalability Issue. The existing ultra image segmentation methods (UIS) often rely on datasetspecific parameters, which limit their model capacity and scalability. The UIS methods struggle to scale up to larger image sizes, with performance significantly degrading as image resolution increases. Moreover, UIS has complex training procedures that need multi-stage parameter tuning [8, 24]. In contrast, general semantic segmentation (GSS) methods are more effective and demonstrate greater scalability compared to UIS. With a straightforward training process, GSS can be adaptable to various ultra image datasets. ", "page_idx": 0}, {"type": "image", "img_path": "nU4lvlMwrt/tmp/84842c8eff975524ccb9f5b996b34890b5eda2b834d7e43a928bf57c11d2e919.jpg", "img_caption": ["Figure 1: Comparison with specific (a) and (b). TheFigure 2: Directly adapting the general segmentation previous architecture all suffered from Scalability Issuemodel to ultra image scene will cause fragmentation and Architectural Issue due to specific designs and aphenomenon (a): the prediction results of the edge lack of tailored context, whereas ours is designed toareas between adjacent patches (even in overlapped leverage any general segmentation model to addresspatches) are inconsistent. ultra image segmentation. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "(2) Architectural Issue. The architecture of existing UIS is not suitable for processing ultra image scenes in real-world scenarios, as it compromises between dataset size and computing resources. Most methods utilize either the entire image or the downsampled version to capture global information. However, the two strategies are both constrained by the size of input image. When the input image is excessively large, the former strategy cannot process it directly, while the later will suffer great information loss during compression. ", "page_idx": 1}, {"type": "text", "text": "Building upon the above discussions, a simple solution is to introduce general segmentation model [27, 12, 18, 39] into the ultra image segmentation task using the sliding window approach, which takes only isolated patches as input due to memory limitations. However, directly adapting GSS to ultra image scenes also raises two challenges: (1) Information bottleneck. Using isolated patches as input prevents the model from capturing the correlations between patches, which blocks the information flow and affects the model\u2019s perception of the surrounding information. (2) Fragmentation phenomenon. With isolated patch input, even if overlapped patches are used, the prediction results of each patch are independent. This leads to inconsistent prediction between patches, especially at the edges of adjacent patches, as shown in Fig. 2. ", "page_idx": 1}, {"type": "text", "text": "Based on above considerations, we propose an end-to-end framework called Surrounding Guided Segmentation Framework (SGNet), which takes advantage of the general segmentation method and utilizes surrounding information near the local patch to guide the model. As shown in Fig. 1(b), SGNet includes two decoupled parts: a general segmentation module and a surrounding contextguided branch. Specifically, SGNet takes both the local patch and the corresponding surrounding patch as input. The surrounding patch covers a larger area around the local patch and provides more context information. In the surrounding context-guided branch, we model the context information required to segment the local patch from a larger perspective to drive the flow of information between regions of whole image. Besides, to alleviate the fragmentation phenomenon, we propose a boundary consistency loss to improve the consistency of the prediction results of adjacent patches, thereby alleviating the inconsistent predictions. Note that, unlike existing tightly coupled ultra images segmentation methods, our method can be seamlessly incorporated into any general segmentation models, and brings stable performance improvements. ", "page_idx": 1}, {"type": "text", "text": "Our contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We excavate two essential but largely overlooked issues in UIS, which hold great value for the community. In addressing these challenges, we are the first to tackle the ultra image segmentation task from the general segmentation model perspective. \u2022 We present a novel end-to-end ultra image segmentation framework named SGNet, which leverage surrounding context information to guide patch-based model segmentation. Our method is flexible to be added to any general segmentation model. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Experiments show that our method achieves competitive performance and consistently improves over different general segmentation models on five public datasets, outperforming previous methods by a large margin. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 General Semantic Segmentation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "With the development of deep learning [33, 32], the two mainstream methods based on convolution neural network [52, 35, 34, 2] and Transformer [42, 17, 51, 28] have achieved excellent performance. FCN [27] is the first fully convolutional architecture and a lot of work has been extended, such as DeeplabV3 [3] and HRNet [38]. Compared with CNN-based methods, representative Transformerbased works include SegFormer [46], Mask2Foremr [7] and SAM [22]. BiSeNetV1 [49] and STDC [12] are designed for real-time segmentation to reduce the computational overhead. The general segmentation model shows excellent stability and scalability, and we aim to leverage its strengths to address both the Scalability and Architectural issues in existing UIS methods. ", "page_idx": 2}, {"type": "text", "text": "2.2 Ultra Image Semantic Segmentation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "GLNet [6] introduces a novel global-local architecture. PPN [45] builds on the top of GLNet by integrating a classification network to distinguish valuable patches. Furthermore, Magnet [16] employs a multi-stage pipeline where each stage corresponds to a specific magnification level. FCtL [24] exploits locality-aware contextual correlation to effectively integrate and associate contextual information of local patches. Compared to previous methods, ISDNet [14] proposes a novel framework that combines shallow and deep networks, enabling directly whole-image inference, thereby improving the segmentation effect while increasing the speed. ElegantSeg [5] proposes a end-to-end holistic learning framework from the perspective of engineering optimization. Recently, WSDNet [20] follows the architecture of ISDNet, using DWT-IWT to preserve spatial details. GPWFormer [19] also employs the global-local architecture, and propose the wavelet transformer to model semantic relations. However, the aforementioned methods all suffer from inherent framework flaws, and cannot be applied to ultra images of extremely large scale. On the contrary, our simple yet generalized solution is more adaptable and applicable than specific UIS methods in real-world scenarios. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Overview ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we present SGNet, a novel framework that enables existing general segmentation models for ultra image segmentation. As shown in Fig. 3, SGNet consists of two major components, the general segmentation module and surrounding context-guided branch (SCB). The two branches take the local patch and its surrounding larger area as inputs, respectively and extract their features (Section Architecture). In the surrounding context-guided branch, we introduce a surrounding context integration module (Section Surrounding Context Integration Module) to enable interaction between local and surrounding features, and selectively learn contextual information that is helpful for patch segmentation. Furthermore, we propose a boundary consistency loss to maintain the consistency of prediction results across adjacent patches in Section Loss Function. ", "page_idx": 2}, {"type": "text", "text": "3.2 Architecture ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The general segmentation approaches include global inference and slide inference. The former approaches could result in a significant quality drop during the image resolution compression. Therefore, we attempt to adapt the slide inference based methods into ultra image segmentation tasks. Given an ultra image $\\dot{\\boldsymbol{I}}\\,\\in\\,\\mathbb{R}^{H\\times W\\times C}$ , we divide it into $\\Nu$ non-overlapping local patches $I_{l o c a l}\\in\\mathbb{R}^{h\\times w\\times c}$ and feed them into general segmentation module for prediction as our target. Next, we start from a random position within the local patch as the center and obtain a surrounding patch that is $\\alpha(\\alpha>1)$ times larger than its side length. The surrounding patch includes more contextual details, which is helpful to guide the local patch training. ", "page_idx": 2}, {"type": "image", "img_path": "nU4lvlMwrt/tmp/16ce1d1a1ba5791432d001d1ef07e07aa937fd20bcb0cce584dddad316f42180.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 3: The Architecture of SGNet. An ultra image $I$ is randomly cropped to obtain a local patch $I_{l o c a l}$ and a surrounding patch $I_{g l o b a l}$ containing more context information of any size, which are respectively sent to the general segmentation module and the surrounding context-guided branch to extract features. The resulting features are aggregated through simple concatenation and used to generate high-quality predictions. General segmentation module can be applied to any segmentation model, and surrounding context-guided branch consistently achieves stable improvements on it. LN, W-MSA, and GAP stand for layer normalization, window-based multi-head self-attention, and global average pooling, respectively. ", "page_idx": 3}, {"type": "text", "text": "Afterward, the general segmentation module receives the local patch to extract features. Meanwhile, the surrounding context-guided branch (SCB) receives the surrounding patch to extract the surrounding information to guide the segmentation of local patch. In order to boost the processing speed of the SCB, a lightweight backbone is employed for feature extraction. The resulting feature map are subsequently transmitted to the surrounding context integration module for further relationship modeling and focused acquisition of contextual information essential for local patch prediction. ", "page_idx": 3}, {"type": "text", "text": "The output feature map is aligned using coordinate relative relationships, retaining only the portion corresponding to the local patch. This portion is then combined with the feature map generated by the general segmentation module to obtain the final prediction through a standard segmentation head. To achieve a more complete optimization of the SCB, we add an extra auxiliary segmentation head to predict the result of the surrounding patch. This helps us to further calculate the boundary consistency loss between the local and surrounding patches based on this prediction. ", "page_idx": 3}, {"type": "text", "text": "Compared to previous ultra image segmentation methods [6, 45, 24, 14, 19] that cannot handle extremely large images well, our architecture is more flexible and integrated, as it can handle an ANY size large image, equip with ANY general segmentation model, thereby well-suited for real-world scenarios. ", "page_idx": 3}, {"type": "text", "text": "3.3 Surrounding Context Integration Module ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The information contained in the surrounding patch can serve as an extension of the local patch, providing it with more abundant decision-making guidance. Therefore, we introduce the Surrounding Context Integration Module (SCI), from the perspective of absorbing the information contained in each region, and integrating context across all the windows of surrounding patch. The structure of SCI is shown in Fig. 3. ", "page_idx": 3}, {"type": "text", "text": "Following [26, 25, 13], the feature map $F\\,\\in\\,\\mathbb{R}^{H\\times W\\times C}$ is partitioned into a set of $R\\times R$ nonoverlapping regions as $F_{r e g i o n}\\in\\mathbb{R}^{\\frac{H}{R}\\times\\frac{\\mathbf{\\bar{W}}}{R}\\times R^{2}\\times C}$ , with each region being subsequently subdivided into w \u00d7 w non-overlapping windows as Fw \u2208RRHw \u00d7 RWw \u00d7R2\u00d7w2\u00d7C. ", "page_idx": 3}, {"type": "text", "text": "To facilitate information exchange within the region as well as absorb the contextual information that is helpful for segmentation, we perform layer normalization and window-based multi-head self-attention (W-MSA) on all patch tokens within $F_{w}$ as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nF_{w}^{'}=W\\!\\!-\\!\\!M S\\!A(L N(F_{w}))+F_{w}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Currently, the information of all patch tokens within the region have been fully incorporated into $F_{w}^{'}$ Subsequently, we apply global average pooling (GAP) to acquire the global feature representation $\\begin{array}{r}{F_{w}^{g l o b a l}\\in\\mathbb{R}^{\\frac{H}{R w}\\times\\frac{W}{R w}\\times R^{2}\\times1\\times C}}\\end{array}$ for each window, so as to facilitate the information exchange among the windows in later stage: ", "page_idx": 4}, {"type": "equation", "text": "$$\nF_{w}^{g l o b a l}=G A P(F_{w}^{'}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This process enables the absorption and integration of information from every position within the window into a unified vector representation that encapsulates the overall information of the window. ", "page_idx": 4}, {"type": "text", "text": "To leverage the complementarity of information and encourage information sharing among windows within the surrounding patch, we utilize self-attention (SA) mechanism to capture the interdependence among the global feature representations $F_{w}^{g l o b a l}$ of each window: ", "page_idx": 4}, {"type": "equation", "text": "$$\nF_{w}^{g l o b a l^{\\prime}}=S A(F_{w}^{g l o b a l}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Upon completion of the information exchange, the global feature representation of the current window incorporates the information of other windows. Subsequently, the global feature representation $F_{w}^{g l o b a l^{\\bar{\\prime}}}$ is added to $F_{w}^{'}$ by broadcasting, while the information of the remaining windows is transferred to $F_{w}^{'}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nF_{g l o b a l}=F_{w}^{'}+F_{w}^{g l o b a l^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To enhance the compatibility and generalization for subsequent operation, we apply feed forward layer (FFN) to further refine $F_{g l o b a l}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nF_{g l o b a l}^{'}=F F N(F_{g l o b a l})+F_{g l o b a l}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "At this time, Fg\u2032lobal integrates the context information of the surrounding patch, which can strengthen the features of the local patch region as a complement and solve the challenge of information bottleneck. ", "page_idx": 4}, {"type": "text", "text": "3.4 Loss Function ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "3.4.1 Boundary Consistency Loss ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Despite the incorporation of local patch features and surrounding contextual information, there are still inconsistent prediction results between adjacent patches due to the lack of explicit constraints. Therefore, we propose a boundary consistency loss $\\mathcal{L}_{C o n s i s t e n c y}$ to improve the consistency of prediction results in neighboring regions and alleviate the fragmentation phenomenon. $\\mathcal{L}_{C}$ onsistency compels the prediction results of both the general segmentation module and SCB to be as similar as possible, thus promoting consistency in results of neighboring regions across different patches. This helps create a smoother transition between predictions of adjacent patches, harmonizing the prediction of the entire ultra image. ", "page_idx": 4}, {"type": "text", "text": "Concretely, we crop the predicted mask Pglobal corresponding to the local patch from the prediction of the surrounding patch $P_{g l o b a l}$ . Then, we apply L1 constraints on both $P_{g l o b a l}^{'}$ and the prediction result of local patch $P_{l o c a l}$ to encourage similarity between them, even when different contexts are utilized. The loss function is defined as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{C o n s i s t e n c y}=||P_{g l o b a l}^{'}-P_{l o c a l}||.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.4.2 Overall Loss ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The cross-entropy loss is used for both the general segmentation module $(\\mathcal{L}_{C E})$ and SCB $\\left(\\mathcal{L}_{S C B}\\right)$ . The overall loss $\\mathcal{L}$ is a weighted sum of all the losses mentioned above: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}=\\lambda_{1}\\mathcal{L}_{C E}+\\lambda_{2}\\mathcal{L}_{S C B}+\\lambda_{3}\\mathcal{L}_{C o n s i s t e n c y}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "table", "img_path": "nU4lvlMwrt/tmp/34afc485552eef49d509d72a0f60cfb615b292cf571ef14b90a450f4a97049a8.jpg", "table_caption": ["Table 1: Comparison with baseline on five datasets. The \u201cMode\u201d column denotes the specific inference modes associated with each method. The \u201cTest size\u201d column relates to the DeepGlobe dataset. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Datasets ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To comprehensively evaluate our method, we conduct experiments on five public ultra image datasets involving general, medical and remote sensing scenarios: Cityscapes [10], DeepGlobe [11], Inria Aerial [30], Five-Billion-Pixels [40], and Gleason [21]. Since we do not have data partition details from [16], we randomly split the Gleason dataset into a training set of 195 images and a testing set of 49 images, and retrained the relevant models. All other datasets followed the official division. ", "page_idx": 5}, {"type": "text", "text": "4.2 Implementation Details ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In both training and testing, the local patch has a size of $512\\!\\times\\!512$ in all datasets, while the surrounding patch is twice as large without any resizing. During sliding inference process, we do not preserve any overlapping regions and the center of local patch and surrounding patch are the same. The first four stages of STDC, used as the lightweight backbone following [14], are initialized with ImageNet weights. For all experiments, we set $\\lambda_{1}=1,\\lambda_{2}=0.4,\\lambda_{3}=0.1.$ . For the DeepGlobe dataset, the \"unknown\" category is ignored during training as it is not included in the evaluation [11]. We also exclude \"unlabeled\" category in the FBP dataset following [5]. ", "page_idx": 5}, {"type": "text", "text": "We adopt MMSegmentation [9] as our toolbox and use AdamW optimizer, which initial learning rate is set to $2\\times10^{-4}$ . All the models are trained on 4 Tesla V100 GPUs with batch size of 8, except for the Inria Aerial and Gleason dataset, which are trained for 10k iterations while the rest are trained for $30\\mathrm{k}$ iterations. Apart from regular operations such as multi-scale training, filpping, and rotating, we do not do any special data augmentation, and the final result does not use any test time augmentation. ", "page_idx": 5}, {"type": "text", "text": "4.3 Comparison Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We classify our comparison methods into two groups: general semantic segmentation and ultra image segmentation. To perform a comprehensive evaluation, we select semantic segmentation methods that rely on CNN (FCN, DeepLabV3Plus, HRNet), Transformer (SegFormer), and lightweight architecture (STDC) for comparison. In addition, we verify the performance of classic ultra image segmentation methods, including GLNet, ISDNet, GPWFoermer, etc. For fair comparison, we adopt ResNet-50 [15] or its equivalent parameter amount as the backbone for all the methods we compared. ", "page_idx": 5}, {"type": "text", "text": "4.3.1 General VS Ultra Image Segmentation Methods ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "According to the results presented in Table 1, UIS methods only work well on specific datasets, while GSS methods achieve relatively satisfying performance on all datasets. This confirms the aforementioned scalability issue. The existing UIS methods show a noticeable performance decrease when transitioning from handling images in DeepGlobe dataset $(2448\\!\\times\\!2448)$ ) to larger ultra images like FBP $(6800\\!\\times\\!7200)$ , particularly for those utilizing whole images as input, such as ISDNet. This demonstrates that the current UIS architecture lacks flexibility and faces a trade-off between models and datasets. This reveals the aforementioned architectural issue. ", "page_idx": 5}, {"type": "image", "img_path": "nU4lvlMwrt/tmp/b0ffa62a1a5ce9f08267c369ea00a87824de52da35f3a4cc46ece4d5bb598533.jpg", "img_caption": ["Figure 4: The visualization of different methods in DeepGlobe (top row, $2448\\!\\times\\!2448)$ ) and Inria Aerial (bottom row, $5000\\!\\times\\!5000)$ . We use DeepLabV3Plus as our general segmentation model and add SCB on top of it. We apply red color to mark regions where the background is misclassified as foreground and green color to denote regions where foreground is misclassified as background. "], "img_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "nU4lvlMwrt/tmp/ebaa8a0ffc89ad38482d38e302ea4c967c1cfbdce0774ffc97700a5d25a304e1.jpg", "table_caption": ["Table 2: Efficacy of proposed module. Align, SCI, Aux, Loss respectively correspond to the feature alignment operation, surrounding context integration module, auxiliary head and boundary consistency loss. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "nU4lvlMwrt/tmp/cf69055822197bfbe10f4cdb6558273d5d6a712b8eb4ca2563574326b9b2308a.jpg", "table_caption": ["Table 3: Analysis of Surrounding Context Integration. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4.3.2 Improvement over General Segmentation Model ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Compared with above methods, our method obtains consistent gains upon all general segmentation methods. Since our method can seamlessly be incorporated into GSS and utilizes the sliding inference strategy for prediction, it can handle ultra images of any scale while leveraging the advantages of scalability from GSS. This perfectly resolves the scalability issue and architectural issue. Taking the DeepGlobe dataset as an example, our method yields a minimum improvement of $1.69\\%$ across all general models, and a maximum of $75.44\\%$ mIoU, which is $2.22\\%$ higher than using only DeepLabV3Plus. This demonstrates that our method can make full use of the contextual information in the surrounding patch to guide local patch segmentation. It is worth emphasizing that our method is flexible and applicable to any general segmentation models. It also should be noted that all the methods reported in Table 1 are sliding window without overlap. A sliding window with overlap is essentially a test time augmentation method, where multiple predictions on overlapping regions are averaged to enhance the model\u2019s performance. By using overlapping regions that cover half of the input patch for predictions, our method can further improve the performance by $0.24\\%$ to $75.68\\%$ mIoU on DeepGlobe dataset. ", "page_idx": 6}, {"type": "text", "text": "Qualitative results of representative works are shown in Fig. 4, while the results of WSDNet and GPWFormer are not available due to no public code. Since we leverage the contextual clues around the current patch as a guide, our method exhibits fewer false positives and delivers more precise segmentation results than other methods. ", "page_idx": 6}, {"type": "image", "img_path": "nU4lvlMwrt/tmp/9d744e8f4f074ddc5159f3ee15d4ca38b1c14929103c34331fa6e83f99d0374f.jpg", "img_caption": ["Figure 5: Comparison of adding SCB on different mod- Figure 6: Comparison of adding SCB on different models (from top to bottom are DeeplabV3Plus, HRNet els in Gleason $(5120\\!\\times\\!5120)$ ). The predicted patches of and FCN in 5(a) and 5(b)) in Cityscapes $(2048\\!\\times\\!1024)$ . model are divided by the white line, while the red box The mIoU values are calculated on each shown images. indicates the area where the model failed to predict. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "nU4lvlMwrt/tmp/1a3f8bf0b005a85ba3003d3b0bef5097570854afd14307a8ef8bc916221e8ef1.jpg", "table_caption": ["Table 4: Analysis of feature fusion scheme. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "nU4lvlMwrt/tmp/b1f17eab6859e68b2f6fa7b295bc0ceed13e028a431bbb6a0822f0f89858e2ec.jpg", "table_caption": ["Table 5: Analysis of surrounding patch size. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.4 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "4.4.1 Effectiveness of Proposed Module ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To thoroughly confirm the efficacy of each module, we substitute SGNet (DeepLabV3Plus version) with six variations denoted as Group ABCDEF. As shown in Table 2, we employ only a lightweight backbone to individually extract features from surrounding patch, and then concatenated them on last features in group $\\mathbb{B}$ . Group $\\mathbb{B}$ achieves a mIoU of $73.76\\%$ , which exceeds group A that only used general segmentation module by $0.54\\%$ . In group $\\mathbb{C}$ , we utilize the crop operation to align features, resulting in an improvement of $0.12\\%$ compared to $\\mathbb{B}$ . By applying the surrounding context integration module to model the context information, and the auxiliary head to enhance the convergence of SCB in groups $\\mathbb{D}$ and $\\mathbb{E}$ , we are able to improve performance by $0.88\\%$ and $1.35\\%$ compared to group $\\mathbb{C}$ . Finally, group $\\mathbb{F}$ incorporates the boundary consistency loss from group $\\mathbb{E}$ , and we can conclude that this loss can effectively enhance the consistency between the bordering regions of adjacent patches, up to $75.44\\%$ mIoU. Additionally, when combining the logits maps from the surrounding branch and the local branch, the performance is further improved to $75.59\\%$ mIoU. The improvement in results essentially belongs to a model ensembling method. This indicates that our surrounding branch has learned effective and complementary information to the local branch, further demonstrating the validity of our approach. ", "page_idx": 7}, {"type": "text", "text": "Other than analyzing the effectiveness of individual components within the module, we also demonstrate its generality across different methods, as shown in Fig. 5. As observed, on the general dataset such as Cityscapes, our method still lead to significant and consistent improvements upon various models. In addition, we select objects whose area is less than 900 pixels $(0.0036\\%$ of the whole image) from Inria Aerial dataset to verify the effect on small objects. Adding SCB improved the mIoU from $57.12\\%$ to $57.73\\%$ , which shows SCB also improve the tiny objects. ", "page_idx": 7}, {"type": "text", "text": "4.4.2 Efficacy of Surrounding Context Integration Module ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To explore how the attention mechanism and global interaction affect the performance, we conduct ablation studies on surrounding context integration module in Table 3. We first analyze the difference between the naive self-attention (Naive SA) and the window self-attention (W-MSA) in modeling the attention information of the surrounding patch. We apply six residual layers to replace attention module as the baseline, achieving $74.40\\%$ mIoU. Building on this, we further utilize Naive SA and W-MSA, which improve the mIoU by $0.46\\%$ and $0.32\\%$ , respectively. This indicates that the attention mechanism can capture the correlation between different positions, promoting the flow of surrounding information within the feature map. As naive self-attention operates on every pixel of the feature map, it inherently incorporates surrounding interaction operation. We further analyze the impact of incorporating surrounding information into the W-MSA mechanism, and we utilize convolution and GAP operations to extract surrounding information from the windows. After adopting either convolution or GAP, we can get consistent improvement, indicating that introducing surrounding context is essential for W-MSA, which enables the current window to acquire information of other regions. In comparison to the convolution, using GAP to extract the surrounding representation of the window is more suitable for W-MSA, which can reach up to $75.44\\%$ mIoU. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.4.3 Efficacy of Feature Fusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We perform experiments to analyze the scheme of feature fusion of SCB and general segmentation module, as shown in Table 4. The fusion position including before the decoder head (early fusion) and after the decoder head (late fusion). The fusion method including ADD and CONCAT. We observe that the performance of late fusion is better than early fusion. We believe that late fusion is more flexible, shielding the complex processing of different models in the input part of the decoder head. Both ADD and CONCAT operations yield satisfactory outcomes, where the latter have a leading of $0.51\\%$ . We believe that the segmentation head captures a greater amount of information, allowing it to dynamically select the optimal feature for prediction. Hence, modifying the segmentation head of GSS can further enhance the performance of model, but it deviates from our starting point of simplicity and is outside the scope of our method. ", "page_idx": 8}, {"type": "text", "text": "4.4.4 Comparison of Surrounding Patch Size ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We perform ablation studies on the surrounding patch size in Table 5. We observe that, as the surrounding size increases, the performance initially improves and then gradually decreases. This is attributed to the fact that only those close nearby area could provide valuable contextual guidance. Too far-away pixels in the entire images are not relevant to the local patch and may introduce additional noise. This confirms the motivation that contextual information around the local patch is beneficial. ", "page_idx": 8}, {"type": "text", "text": "4.4.5 Efficiency Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Table 6: Comparison of speed on DeepGlobe. We measure GPU memory using the command line tool \u201cgpustat\u201d. \u201c\u2217\u201d represents results we reproduced in our setting. \u201c-\u201d indicates that there is no publicly available result or code. ", "page_idx": 8}, {"type": "table", "img_path": "nU4lvlMwrt/tmp/95363e2eb23341f33b46ac9448ba95b63f8fffd6c52785e78e1326c196969cb9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "nU4lvlMwrt/tmp/af7c7e4285851089eeaec22e1af02d627c041a0813d7a9cf252498168f18551a.jpg", "table_caption": ["Table 7: Comparison of normal image and JPEG compressed image. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "We conduct experiments to examine the speed of different methods. Frames-per-second (FPS) and Memory are measured on a Tesla V100 GPU with a batch size of 1. The variation in FPS among different methods is primarily attributed to the inference framework, that is, whole inference and slide inference. Without considering the overlap, the latter theoretically requires $\\left\\lceil\\frac{\\widetilde{W}}{w}\\right\\rceil\\,\\times\\,\\left\\lceil\\frac{H}{h}\\right\\rceil\\,.$ more operations than the former. Methods such as ISDNet primarily focus on model efficiency and achieve higher FPS, based on a shallow-deep architecture that directly employs the whole image for inference. Consequently, we also develop a fast version named SGNet (ISDNet-Style) by incorporating a modified SCB for a fair comparison. SGNet (ISDNet-Style) applies a single surrounding context integration module to the output feature map of the last stage of STDC to model global information. To maintain consistency with ISDNet and exclude speed differences caused by inference frameworks, we set our local patch size as same as ISDNet to simulate the whole inference. As shown in Table 6, the fast version not only surpasses the mIoU of ISDNet but also achieves a higher FPS of up to 25.59. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.4.6 Robustness Study ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We compressed the DeepGlobe dataset to $10\\%$ of its original image quality using JPEG compression and retrained SGNet and DeepLabV3Plus on it. As shown in Table 7, the results show that SGNet significantly outperforms DeepLabV3Plus by $3.92\\%$ (from $64.16\\%$ mIoU versus $60.24\\%$ mIoU), and this improvement is almost twice that of normal images (from $73.22\\%$ mIoU to $75.44\\%$ mIoU). This indicates that our method is relatively insensitive to noise compared to baseline models and can use surrounding information to infer damaged pixel information within the object. It also demonstrates that our method is particularly robust in scenarios involving image degradation. ", "page_idx": 9}, {"type": "text", "text": "4.4.7 Fragmentation Phenomenon Study ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To show our advantage to alleviate the fragmentation phenomenon, we compare our module against GSS in Fig. 6. We simulate all predicted patches (non-overlapping) generated during slide inference process with white lines. It is evident that GSS result in a steep change in the boundary between adjacent patches, and the predicted results of each patch are relatively independent, lacking coherence. Our method is capable of modeling the correlation between patches, leading to smoother prediction results for adjacent patches. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose the Surrounding Guided Segmentation Framework (SGNet) to address the scalability and architectural issues in existing UIS methods. SGNet leverages surrounding context to guide local patch segmentation and can be incorporated into any general segmentation model. Our method consistently improves performance across five datasets and demonstrates greater adaptability and applicability than existing methods in real-world scenarios. This work not only contributes a novel solution to the UIS domain but also emphasizes the potential of integrating general segmentation techniques to advance the field. We hope that SGNet inspires further exploration in ultra image segmentation, fostering innovations that enhance model performance and scalability. ", "page_idx": 9}, {"type": "text", "text": "Limitations. Noise and artifacts in ultra-high-resolution images can hinder segmentation accuracy, necessitating further research to address these challenges. ", "page_idx": 9}, {"type": "text", "text": "Social Impact. The proposed method has the potential to advance various fields, including medical image analysis and remote sensing image processing. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment. This work was supported in part by the National Key Research and Development Program of China under grant 2023YFC2705700, in part by the National Natural Science Foundation of China under grant 62372341, and in part by the Fundamental Research Funds for the Central Universities under grant 2042024kf0040. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Xiaojun Chang, Pengzhen Ren, Pengfei Xu, Zhihui Li, Xiaojiang Chen, and Alex Hauptmann. A comprehensive survey of scene graphs: Generation and application. IEEE TPAMI, 45(1):1\u201326, 2021.   \n[2] Chun-Fu Chen, Rameswar Panda, and Quanfu Fan. Regionvit: Regional-to-local attention for vision transformers. arXiv preprint arXiv:2106.02689, 2021.   \n[3] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. arxiv 2017. arXiv preprint arXiv:1706.05587, 2, 2019.   \n[4] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In ECCV, pages 801\u2013818, 2018.   \n[5] Wei Chen, Yansheng Li, Bo Dang, and Yongjun Zhang. Elegantseg: End-to-end holistic learning for extra-large image semantic segmentation. arXiv preprint arXiv:2211.11316, 2022. [6] Wuyang Chen, Ziyu Jiang, Zhangyang Wang, Kexin Cui, and Xiaoning Qian. Collaborative global-local networks for memory-efficient segmentation of ultra-high resolution images. In CVPR, pages 8924\u20138933, 2019.   \n[7] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. In CVPR, pages 1290\u2013 1299, 2022. [8] Ho Kei Cheng, Jihoon Chung, Yu-Wing Tai, and Chi-Keung Tang. Cascadepsp: Toward classagnostic and very high-resolution segmentation via global and local refinement. In CVPR, pages 8890\u20138899, 2020.   \n[9] MMSegmentation Contributors. MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark. https://github.com/open-mmlab/mmsegmentation, 2020.   \n[10] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, pages 3213\u20133223, 2016.   \n[11] Ilke Demir, Krzysztof Koperski, David Lindenbaum, Guan Pang, Jing Huang, Saikat Basu, Forest Hughes, Devis Tuia, and Ramesh Raskar. Deepglobe 2018: A challenge to parse the earth through satellite images. In CVPRW, pages 172\u2013181, 2018.   \n[12] Mingyuan Fan, Shenqi Lai, Junshi Huang, Xiaoming Wei, Zhenhua Chai, Junfeng Luo, and Xiaolin Wei. Rethinking bisenet for real-time semantic segmentation. In CVPR, pages 9716\u2013 9725, 2021.   \n[13] Jiemin Fang, Lingxi Xie, Xinggang Wang, Xiaopeng Zhang, Wenyu Liu, and Qi Tian. Msgtransformer: Exchanging local spatial information by manipulating messenger tokens. In CVPR, pages 12063\u201312072, 2022.   \n[14] Shaohua Guo, Liang Liu, Zhenye Gan, Yabiao Wang, Wuhao Zhang, Chengjie Wang, Guannan Jiang, Wei Zhang, Ran Yi, Lizhuang Ma, et al. Isdnet: Integrating shallow and deep networks for efficient ultra-high resolution segmentation. In CVPR, pages 4361\u20134370, 2022.   \n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770\u2013778, 2016.   \n[16] Chuong Huynh, Anh Tuan Tran, Khoa Luu, and Minh Hoai. Progressive semantic segmentation. In CVPR, pages 16755\u201316764, 2021.   \n[17] Jitesh Jain, Jiachen Li, Mang Tik Chiu, Ali Hassani, Nikita Orlov, and Humphrey Shi. Oneformer: One transformer to rule universal image segmentation. In CVPR, pages 2989\u20132998, 2023.   \n[18] Jitesh Jain, Anukriti Singh, Nikita Orlov, Zilong Huang, Jiachen Li, Steven Walton, and Humphrey Shi. Semask: Semantically masked transformers for semantic segmentation. In ICCV, pages 752\u2013761, 2023.   \n[19] Deyi Ji, Feng Zhao, and Hongtao Lu. Guided patch-grouping wavelet transformer with spatial congruence for ultra-high resolution segmentation. In IJCAI, pages 920\u2013928, 2023.   \n[20] Deyi Ji, Feng Zhao, Hongtao Lu, Mingyuan Tao, and Jieping Ye. Ultra-high resolution segmentation with ultra-rich context: A novel benchmark. In CVPR, pages 23621\u201323630, 2023.   \n[21] Davood Karimi, Guy Nir, Ladan Fazli, Peter C Black, Larry Goldenberg, and Septimiu E Salcudean. Deep learning-based gleason grading of prostate cancer from histopathology images\u2014role of multiscale decision aggregation and data augmentation. IEEE JBHI, 24(5):1413\u2013 1426, 2019.   \n[22] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023.   \n[23] Alexander Kirillov, Yuxin Wu, Kaiming He, and Ross Girshick. Pointrend: Image segmentation as rendering. In CVPR, pages 9799\u20139808, 2020.   \n[24] Qi Li, Weixiang Yang, Wenxi Liu, Yuanlong Yu, and Shengfeng He. From contexts to locality: Ultra-high resolution image segmentation via locality-aware contextual correlation. In ICCV, pages 7252\u20137261, 2021.   \n[25] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In CVPR, pages 12009\u201312019, 2022.   \n[26] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, pages 10012\u201310022, 2021.   \n[27] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In CVPR, pages 3431\u20133440, 2015.   \n[28] Huaishao Luo, Junwei Bao, Youzheng Wu, Xiaodong He, and Tianrui Li. Segclip: Patch aggregation with learnable centers for open-vocabulary semantic segmentation. In ICML, pages 23033\u201323044. PMLR, 2023.   \n[29] Lei Ma, Yu Liu, Xueliang Zhang, Yuanxin Ye, Gaofei Yin, and Brian Alan Johnson. Deep learning in remote sensing applications: A meta-analysis and review. ISPRS, 152:166\u2013177, 2019.   \n[30] Emmanuel Maggiori, Yuliya Tarabalka, Guillaume Charpiat, and Pierre Alliez. Can semantic labeling methods generalize to any city? the inria aerial image labeling benchmark. In IGARSS, pages 3226\u20133229. IEEE, 2017.   \n[31] Maciej A Mazurowski, Haoyu Dong, Hanxue Gu, Jichen Yang, Nicholas Konz, and Yixin Zhang. Segment anything model for medical image analysis: an experimental study. Medical Image Analysis, 89:102918, 2023.   \n[32] Gaurav Menghani. Efficient deep learning: A survey on making deep learning models smaller, faster, and better. ACM Computing Surveys, 55(12):1\u201337, 2023.   \n[33] Shervin Minaee, Yuri Boykov, Fatih Porikli, Antonio Plaza, Nasser Kehtarnavaz, and Demetri Terzopoulos. Image segmentation using deep learning: A survey. IEEE TPAMI, 44(7):3523\u2013 3542, 2021.   \n[34] Tuan Duc Ngo, Binh-Son Hua, and Khoi Nguyen. Isbnet: a 3d point cloud instance segmentation network with instance-aware sampling and box-aware dynamic convolution. In CVPR, pages 13550\u201313559, 2023.   \n[35] Yaolei Qi, Yuting He, Xiaoming Qi, Yuan Zhang, and Guanyu Yang. Dynamic snake convolution based on topological geometric constraints for tubular structure segmentation. In ICCV, pages 6070\u20136079, 2023.   \n[36] Muhammad Imran Razzak, Saeeda Naz, and Ahmad Zaib. Deep learning for medical image processing: Overview, challenges and the future. Classification in BioApps: Automation of Decision Making, pages 323\u2013350, 2018.   \n[37] Dinggang Shen, Guorong Wu, and Heung-Il Suk. Deep learning in medical image analysis. Annu Rev Biomed Eng, 19:221\u2013248, 2017.   \n[38] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for human pose estimation. In CVPR, pages 5693\u20135703, 2019.   \n[39] Hans Thisanke, Chamli Deshan, Kavindu Chamith, Sachith Seneviratne, Rajith Vidanaarachchi, and Damayanthi Herath. Semantic segmentation using vision transformers: A survey. EAAI, 126:106669, 2023.   \n[40] Xin-Yi Tong, Gui-Song Xia, and Xiao Xiang Zhu. Enabling country-scale land cover mapping with meter-resolution satellite imagery. ISPRS Journal of Photogrammetry and Remote Sensing, 196:178\u2013196, 2023.   \n[41] Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. SCI DATA, 5(1):1\u20139, 2018.   \n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurlPS, 30, 2017.   \n[43] Sai Wang, Yutian Lin, and Yu Wu. Omni-q: Omni-directional scene understanding for unsupervised visual grounding. In CVPR, pages 14261\u201314270, 2024.   \n[44] Yi Wang, Syed Muhammad Arsalan Bashir, Mahrukh Khan, Qudrat Ullah, Rui Wang, Yilin Song, Zhe Guo, and Yilong Niu. Remote sensing image super-resolution and object detection: Benchmark and state of the art. Expert Syst. Appl, page 116793, 2022.   \n[45] Tong Wu, Zhenzhen Lei, Bingqian Lin, Cuihua Li, Yanyun Qu, and Yuan Xie. Patch proposal network for fast semantic segmentation of high-resolution images. In AAAI, volume 34, pages 12402\u201312409, 2020.   \n[46] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. NeurlPS, 34:12077\u201312090, 2021.   \n[47] Shuo Xu, Sai Wang, Xinyue Hu, Yutian Lin, Bo Du, and Yu Wu. Mac: A benchmark for multiple attributes compositional zero-shot learning. arXiv preprint arXiv:2406.12757, 2024.   \n[48] Huang Yao, Rongjun Qin, and Xiaoyu Chen. Unmanned aerial vehicle for remote sensing applications\u2014a review. Remote Sens, 11(12):1443, 2019.   \n[49] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, and Nong Sang. Bisenet: Bilateral segmentation network for real-time semantic segmentation. In ECCV, pages 325\u2013341, 2018.   \n[50] Haitao Yuan, Sai Wang, Zhifeng Bao, and Shangguang Wang. Automatic road extraction with multi-source data revisited: completeness, smoothness and discrimination. Proceedings of the VLDB Endowment, 16(11):3004\u20133017, 2023.   \n[51] Hao Zhang, Feng Li, Xueyan Zou, Shilong Liu, Chunyuan Li, Jianwei Yang, and Lei Zhang. A simple framework for open-vocabulary segmentation and detection. In ICCV, pages 1020\u20131031, 2023.   \n[52] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In CVPR, pages 2881\u20132890, 2017. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Existing Architecture Analysis ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In Figure 7, we show the detail architectures of existing ultra image segmentation methods. Whole inference 7(a) and slide inference 7(b) are the two most fundamental ultra image segmentation architectures. While whole inference can be applied directly to general segmentation models [27, 4, 38, 46, 12, 49], it utilizes the down-sampled image as input, which results in a loss of essential information. The latter divides the image into multiple patches by means of sliding window, and predicts the patches one by one, aggregating the results together. However, due to the isolated prediction of patches and the lack of global information, it is prone to cause information bottleneck and fragmentation phenomenon. ", "page_idx": 13}, {"type": "image", "img_path": "nU4lvlMwrt/tmp/1eb3682bb0ead38fda4f4c0c3ca0d958643b35595285b1c3496f0b96028ecc40.jpg", "img_caption": ["Figure 7: Comparison of existing architectures of ultra image segmentation. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "To address the issues above and leverage the beneftis of both whole inference and slice inference, some studies [6, 45, 24, 16, 19] have introduced the Global & Local architecture 7(c). This architecture involves one branch that takes in the down-sampled ultra image as the global cue, while the other branch extracts local information from sliced patches. However, these tasks usually require sequential processing of each patch and complex integration strategies to combine global information, resulting in poor scalability of the architecture. Alternatively, some study[14, 20] have introduced the Shallow & Deep architecture 7(d), which inputs the original image and the resized image into different branches for processing. Despite its fast speed, it is essentially a trade-off in dataset size and computing resources, and cannot be used for real ultra image segmentation scenarios. ", "page_idx": 13}, {"type": "text", "text": "The architectures depicted in Figure 7(c) and Figure 7(d) both use the original image after resize in an input branch, which may limit the capability of the architecture to process ultra images. On the one hand, when the image is extremely large, the resized image will inevitably lose a lot of detail information to fit the GPU memory. On the other hand, the resized ultra image cannot guarantee to provide specific global information that is helpful for local patch segmentation. To address these issues, we designed a novel and effective ultra image segmentation architecture 7(e) based on slide window. Our architecture can handle ultra images of any scale and provide surrounding information that aids in local patch segmentation. Furthermore, our architecture is highly flexible, not restricted by computing resources, and can be seamlessly integrated to any encoder-decoder based segmentation model. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "B Dataset Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To comprehensively evaluate our method, we conduct experiments on five public ultra image datasets involving general, medical and remote sensing scenarios: ", "page_idx": 14}, {"type": "text", "text": "Cityscapes [10]. The Cityscapes dataset is a popular street scene dataset for generic semantic segmentation. It contains 3475 images with the resolution of $1024\\!\\times\\!2048$ and a total of 19 categories. We use 2975 images for training and 500 images for testing. ", "page_idx": 14}, {"type": "text", "text": "Gleason [21]. The Gleason dataset is a high resolution medical image dataset with the resolution of $5120\\!\\times\\!5120$ . It contains 244 H&E-stained histopathology images for automatic Gleason grading of prostate cancer. Since we do not have data partition details from [16], we randomly split dataset into training and testing set with 195 and 49 images. ", "page_idx": 14}, {"type": "text", "text": "DeepGlobe [11]. DeepGlobe is a satellite image dataset that contains 803 ultra-high resolution images $(2448\\!\\times\\!2448$ pixels). It contains 7 categories, of which the class named \"unknown\" is excluded. Following [6], we divide the training, validation, and testing sets into 454, 207, and 142 images, respectively. ", "page_idx": 14}, {"type": "text", "text": "Inria Aerial [30]. The Inria Aerial dataset comprises 180 images, each with a resolution of $5000\\!\\times\\!5000$ pixels. Following [6], we divide the training, validation, and testing sets into 126, 27, and 27 images, respectively. ", "page_idx": 14}, {"type": "text", "text": "Five-Billion-Pixels [40]. The FBP dataset includes 150 high-resolution images, each with a size of $7200\\!\\times\\!6800$ pixels and labeled with 24 categories. We use the same test set as in [40], and randomly divide the remaining images into training set and validation set, including 90 and 30 images. ", "page_idx": 14}, {"type": "text", "text": "C Comparative Analysis of SCB Branch ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In order to demonstrate the proposed SCB Branch\u2019s efficacy, we used sixteen conventional convolution layers followed by four transformer blocks to form a trivial replacement branch for extracting surrounding image feature. It serving as a functionally analogous replacement for the proposed SCB branch. As shown in Table 8, we added this trivial replacement branch to all general segmentation models in Table 1 and conducted experiments on the DeepGlobe dataset using the same settings. The results show that our proposed SCB branch significantly outperforms this branch, proving the efficacy of our proposed component. ", "page_idx": 14}, {"type": "table", "img_path": "nU4lvlMwrt/tmp/9bd48c302d57e034353d61461e5c29fa3d83883303b03ec668bb2bd5ffc026f1.jpg", "table_caption": ["Table 8: Comparison of SCB Branch with it trivial replacement. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "D Large Scale Human Subject Segmentation Study ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We conducted experiments on the well-known CelebAMask-HQ dataset to further verify the effectiveness of our method in large scale human subject segmentation. Due to the lack of extremely high-resolution human datasets, we simulate ultra-high resolution by resizing the images from the CelebAMask-HQ dataset from 1024 to 2448 pixels. We compared our SGNet with DeepLabV3Plus, which is a highly popular and widely used image segmentation model across various domains. The performance of our method significantly outperformed DeepLabV3Plus by $1.61\\%$ (from $62.93\\%$ mIoU to $64.54\\%$ mIoU). We provide more examples of qualitative results in Figure 8. ", "page_idx": 14}, {"type": "image", "img_path": "nU4lvlMwrt/tmp/17489ff81c07b0b01b9191e670d019141dbdc8f229821f88cd5cf171b263ed97.jpg", "img_caption": ["Figure 8: The visualization of different methods in CelebAMask-HQ. We resized the images from 1024 to 2448 pixels to simulate ultra-high resolution and used sliding window of size 512 without overlap for inference. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "E More Qualitative Result ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Figure 9, we provide more examples of qualitative results between our method and existing ultra image segmentation methods like FCtL [24] and ISDNet [14]. These results indicate that our method is capable of achieving satisfactory quality on various challenging datasets [11, 30]. Figure 10 and Figure 11 illustrate the effectiveness of SCB by comparing it to general segmentation models such as DeepLabV3Plus and HRNet on Cityscapes [10] and Gleason [41]. ", "page_idx": 15}, {"type": "image", "img_path": "nU4lvlMwrt/tmp/f4dd63223ec08b65a280521171aeae99ae3ea880af4f1f9a6594431e6742faef.jpg", "img_caption": ["Figure 9: The qualitative results of different methods in DeepGlobe (top row, $2448\\!\\times\\!2448)$ ) and Inria Aerial (bottom row, $5000\\!\\times\\!5000)$ ). "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "nU4lvlMwrt/tmp/5f370f6159618f0aff18e1824adfe91b873d3bd0be4d65279905fdac30bc1e5d.jpg", "img_caption": ["Figure 10: The comparison of adding SCB on different models in Cityscapes $(2048\\!\\times\\!1024)$ ). "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "nU4lvlMwrt/tmp/4428ec76e74cb4c3f659880215339c43eb8c9f71159ec21d381ba9c6e260faa5.jpg", "img_caption": ["Figure 11: The comparison of adding SCB on different models in Gleason $(5120\\!\\times\\!5120)$ ). "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The main claims accurately summarize the our contributions. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: In Conclusion, we discuss the limitations of the proposed method ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We provide the necessary information required to replicate the main experimental results, including the model architecture and experimental settings. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We have included our core code in an anonymized zip as part of the supplementary, and will release the complete code upon acceptance. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The training and test details are provided in Sec. 4.2. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: Not applicable. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: In Section 4.2, we provided detailed information about the specific computer resources. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We have conducted. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We discuss the societal impacts in conclusion. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We cite all the original papers that provided the code and dataset. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 21}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}]