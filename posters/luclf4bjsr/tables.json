[{"figure_path": "LuCLf4BJsr/tables/tables_1_1.jpg", "caption": "Table 1: Comparison between Chain-of-Agents and prior methods for long-context tasks. Rec./Foc.: being able to mitigate inaccurate receptive field/long context focusing issues. Read: the number of tokens as model input, where n is the total input length, k is the context window limit of LLMs. Inter.: the interpretability of the approach. Note that RAG is 'medium interpretable' because of the re-ranked chunks.", "description": "This table compares Chain-of-Agents (CoA) with existing methods for handling long-context tasks.  It contrasts approaches based on input reduction (truncation, RAG), window extension (position interpolation, long context models) and multi-agent LLMs.  The comparison considers the ability to mitigate issues of focusing on the relevant portion of a long context, whether the method requires training, the amount of input text processed, agent type (single vs. multiple), task applicability (generic vs. query-based), and the level of interpretability.", "section": "1 Introduction"}, {"figure_path": "LuCLf4BJsr/tables/tables_3_1.jpg", "caption": "Table 1: Comparison between Chain-of-Agents and prior methods for long-context tasks. Rec./Foc.: being able to mitigate inaccurate receptive field/long context focusing issues. Read: the number of tokens as model input, where n is the total input length, k is the context window limit of LLMs. Inter.: the interpretability of the approach. Note that RAG is 'medium interpretable' because of the re-ranked chunks.", "description": "This table compares Chain-of-Agents (CoA) with existing methods for handling long-context tasks.  It contrasts approaches based on input reduction (truncation and RAG) and window extension, highlighting CoA's unique ability to mitigate issues related to receptive field and context focus while maintaining high interpretability and training-free nature.", "section": "1 Introduction"}, {"figure_path": "LuCLf4BJsr/tables/tables_4_1.jpg", "caption": "Table 3: Dataset Statistics. Avg. Input/Agents is the average words/agents (8k) for source input.", "description": "This table presents statistics for nine datasets used in the experiments, categorized into question answering, summarization, and code completion tasks.  For each dataset, it shows the average input length in words, the average number of agents used in the Chain-of-Agents framework, and whether the task is query-based or not. The average input length is given in words, and the average number of agents represents how many agents were used on average to process the input for each dataset using an 8k word limit per agent. Query-based indicates whether the task requires a specific query to guide the processing of the input text.", "section": "4.1 Experiment Setup"}, {"figure_path": "LuCLf4BJsr/tables/tables_4_2.jpg", "caption": "Table 2: Time complexity.", "description": "This table compares the time complexity of three different methods for handling long-context tasks: Full-Context, CoA (Chain-of-Agents), and RAG (Retrieval-Augmented Generation).  The time complexity is broken down into encoding and decoding phases.  The notation used shows that Full-Context has a quadratic time complexity for encoding (O(n\u00b2)), while CoA and RAG show linear time complexity in terms of n, the input length, but with different dependencies on the context window size (k for CoA, k' for RAG). Decoding time complexity for all three methods is linear (O(nr)), indicating a dependence on the response length (r).", "section": "3.3 Time Complexity Analysis"}, {"figure_path": "LuCLf4BJsr/tables/tables_5_1.jpg", "caption": "Table 4: Overall results of CoA. CoA significantly outperforms Vanilla and RAG using various backbone LLMs on all datasets.", "description": "This table presents the performance comparison of the proposed Chain-of-Agents (CoA) framework against two baseline methods: Vanilla (Full-Context) and RAG (Retrieval-Augmented Generation) across nine different datasets encompassing three task types: Question Answering, Summarization, and Code Completion.  The results are shown for six different Large Language Models (LLMs) as backbones for each method.  The table highlights CoA's significant performance improvements over the baselines in all datasets and across all LLMs.", "section": "4.2 Overall Results of CoA"}, {"figure_path": "LuCLf4BJsr/tables/tables_6_1.jpg", "caption": "Table 5: Comparison between CoA and other multi-agent frameworks. CoA with sequential agents outperforms other designs with multiple parallel agents including Merge and Hierarchical.", "description": "This table compares the performance of Chain-of-Agents (CoA) against two other multi-agent approaches: Merge and Hierarchical.  The comparison is made across eight different datasets, focusing on the performance of each method using various Language Models (LLMs) as the backbone.  The results demonstrate that CoA, with its sequential agent communication, outperforms the parallel approaches of Merge and Hierarchical, highlighting the effectiveness of CoA's unique communication strategy for long context tasks.", "section": "4.2 Overall Results of CoA"}, {"figure_path": "LuCLf4BJsr/tables/tables_6_2.jpg", "caption": "Table 4: Overall results of CoA. CoA significantly outperforms Vanilla and RAG using various backbone LLMs on all datasets.", "description": "This table presents the overall performance comparison of the proposed Chain-of-Agents (CoA) framework against two strong baseline methods: Vanilla (Full-Context) and RAG (Retrieval-Augmented Generation), across nine different datasets encompassing three types of tasks: Question Answering, Summarization, and Code Completion.  The results are shown for six different large language models (LLMs) as backbones, demonstrating the consistent superior performance of CoA across various LLMs and tasks.", "section": "4.2 Overall Results of CoA"}, {"figure_path": "LuCLf4BJsr/tables/tables_8_1.jpg", "caption": "Table 7: Ablation on CoA. Manager plays an important role in CoA, and left-to-right yields the best performance among other reading orders including Right-to-Left and Permutation.", "description": "This table presents the ablation study results on the Chain-of-Agents (CoA) framework. It shows the impact of removing the manager agent, and it compares the performance of three different reading orders: left-to-right, right-to-left, and permutation.  The results demonstrate the importance of the manager agent in the CoA framework and the effectiveness of a left-to-right reading order.", "section": "5.5 Ablation Study: Effectiveness of Manager and Alternative Design Choices"}, {"figure_path": "LuCLf4BJsr/tables/tables_8_2.jpg", "caption": "Table 6: Comparison between CoA and other multi-agent frameworks. CoA with sequential agents outperforms other designs with multiple parallel agents including Merge and Hierarchical.", "description": "This table compares the performance of Chain-of-Agents (CoA) against two other multi-agent approaches: Merge and Hierarchical.  The comparison is made across nine datasets, focusing on the performance of each method on various question answering, summarization, and code completion tasks. CoA consistently outperforms both other methods, highlighting the effectiveness of its sequential communication structure compared to parallel strategies.", "section": "4.2 Overall Results of CoA"}, {"figure_path": "LuCLf4BJsr/tables/tables_9_1.jpg", "caption": "Table 9: Practical time analysis on HotpotQA dataset. Avg. # of Input/Output shows the total input and output tokens for each model.", "description": "This table shows the average running time, the average number of input tokens, the average number of output tokens, and the average number of agent outputs for three different models: Vanilla (8k), RAG (8k), and CoA (8k) on the HotpotQA dataset.  It demonstrates the time efficiency and token usage of each model, highlighting the trade-offs between model complexity and performance.", "section": "5.7 Practical Time Complexity"}, {"figure_path": "LuCLf4BJsr/tables/tables_9_2.jpg", "caption": "Table 10: Information loss of text-bison model on different datasets.", "description": "This table presents the performance of the Chain-of-Agents (CoA) framework using the text-bison model on five different datasets and the corresponding information loss during the communication process between agents.  The information loss metric quantifies the difference between the highest score achieved by any communication unit (CUi) and the final prediction score against the gold standard, indicating the amount of information lost during the sequential communication among the agents.", "section": "5.8 Information Loss"}, {"figure_path": "LuCLf4BJsr/tables/tables_18_1.jpg", "caption": "Table 1: Comparison between Chain-of-Agents and prior methods for long-context tasks. Rec./Foc.: being able to mitigate inaccurate receptive field/long context focusing issues. Read: the number of tokens as model input, where n is the total input length, k is the context window limit of LLMs. Inter.: the interpretability of the approach. Note that RAG is 'medium interpretable' because of the re-ranked chunks.", "description": "This table compares Chain-of-Agents (CoA) with existing methods for handling long-context tasks.  It contrasts different approaches based on their ability to manage receptive field issues (Rec./Foc.), the amount of input tokens processed (Read), whether they require training (No Train), the type of agent used (Single/Multiple), the method's general applicability (Applicability), and the method's interpretability (Inter.).  The table highlights CoA's advantages in mitigating issues related to both receptive field and focusing, and its ability to process the entire input with multiple agents while maintaining high interpretability. ", "section": "1 Introduction"}, {"figure_path": "LuCLf4BJsr/tables/tables_18_2.jpg", "caption": "Table 1: Comparison between Chain-of-Agents and prior methods for long-context tasks. Rec./Foc.: being able to mitigate inaccurate receptive field/long context focusing issues. Read: the number of tokens as model input, where n is the total input length, k is the context window limit of LLMs. Inter.: the interpretability of the approach. Note that RAG is 'medium interpretable' because of the re-ranked chunks.", "description": "This table compares Chain-of-Agents (CoA) with existing methods for handling long-context tasks.  It contrasts approaches based on input reduction (truncation and RAG) and window extension, highlighting CoA's ability to mitigate issues with receptive field and context focus. The table also shows the number of tokens processed by each approach, its trainability, the type of agent (single or multiple), its applicability, and its interpretability.", "section": "1 Introduction"}, {"figure_path": "LuCLf4BJsr/tables/tables_19_1.jpg", "caption": "Table 16: Compare between CoA and previous state-of-the-art models on nine datasets. * indicates the model needs further training.", "description": "This table compares the performance of Chain-of-Agents (CoA) with the best results reported in previous studies across nine different datasets.  The datasets cover question answering, summarization, and code completion tasks.  The table shows that CoA achieves either better or comparable performance compared to previous state-of-the-art methods, with notable improvements in some categories. The asterisk (*) indicates that certain prior results required further model training, while CoA is a training-free method.", "section": "4.2 Overall Results of CoA"}, {"figure_path": "LuCLf4BJsr/tables/tables_19_2.jpg", "caption": "Table 3: Dataset Statistics. Avg. Input/Agents is the average words/agents (8k) for source input.", "description": "This table presents the statistics of nine datasets used in the experiments, categorized into three task types: Question Answering, Summarization, and Code Completion. For each dataset, it lists the average input length (in words), the average number of agents used (considering an 8k word limit per agent), and an indicator specifying whether the dataset is query-based or not.  The table provides context on the scale and nature of the datasets used to evaluate the Chain-of-Agents (CoA) framework.", "section": "4.1 Experiment Setup"}, {"figure_path": "LuCLf4BJsr/tables/tables_20_1.jpg", "caption": "Table 16: Compare between CoA and previous state-of-the-art models on nine datasets. * indicates the model needs further training.", "description": "This table compares the performance of Chain-of-Agents (CoA) against the best previously reported results on nine different datasets, encompassing question answering, summarization, and code completion tasks.  It highlights CoA's performance relative to existing state-of-the-art models, indicating whether those models required further training to achieve their reported scores. The table provides a quantitative assessment of CoA's effectiveness across various long-context tasks.", "section": "4.2 Overall Results of CoA"}, {"figure_path": "LuCLf4BJsr/tables/tables_20_2.jpg", "caption": "Table 16: Compare between CoA and previous state-of-the-art models on nine datasets. * indicates the model needs further training.", "description": "This table compares the performance of Chain-of-Agents (CoA) with the best results reported in previous studies on nine different datasets.  Each dataset represents a different task (question answering, summarization, and code completion). The numbers represent the performance scores achieved, indicating that CoA either matches or surpasses the state-of-the-art in most cases.  The asterisk (*) denotes that a model required further training to achieve the reported score.", "section": "4.2 Overall Results of CoA"}, {"figure_path": "LuCLf4BJsr/tables/tables_21_1.jpg", "caption": "Table 18: Comparison between vanilla and CoA with various context lengths using text-bison-32k.", "description": "This table presents a comparison of the performance of the vanilla model and the proposed Chain-of-Agents (CoA) model on the BookSum dataset.  The performance is measured using different context window sizes (4k, 8k, 16k, and 32k tokens) for the text-bison-32k model. The table shows that CoA consistently outperforms the vanilla model across all context window sizes. The improvement is most significant at 8k and 16k context windows.", "section": "F.2 Robustness against Context Window Size"}, {"figure_path": "LuCLf4BJsr/tables/tables_22_1.jpg", "caption": "Table 1: Comparison between Chain-of-Agents and prior methods for long-context tasks. Rec./Foc.: being able to mitigate inaccurate receptive field/long context focusing issues. Read: the number of tokens as model input, where n is the total input length, k is the context window limit of LLMs. Inter.: the interpretability of the approach. Note that RAG is 'medium interpretable' because of the re-ranked chunks.", "description": "This table compares Chain-of-Agents (CoA) with existing methods for handling long-context tasks.  It contrasts different approaches based on three criteria: whether they address issues of focusing on relevant information within long contexts, whether they require training, and the number of tokens the model processes (Read).  The interpretability of each method is also assessed.", "section": "1 Introduction"}, {"figure_path": "LuCLf4BJsr/tables/tables_23_1.jpg", "caption": "Table 1: Comparison between Chain-of-Agents and prior methods for long-context tasks. Rec./Foc.: being able to mitigate inaccurate receptive field/long context focusing issues. Read: the number of tokens as model input, where n is the total input length, k is the context window limit of LLMs. Inter.: the interpretability of the approach. Note that RAG is 'medium interpretable' because of the re-ranked chunks.", "description": "This table compares Chain-of-Agents (CoA) with existing methods for handling long-context tasks.  It contrasts approaches based on input reduction (truncation and RAG) and window extension.  The comparison focuses on the ability to address inaccurate receptive fields, the number of tokens processed, whether the method requires training, the applicability of the method to different task types, and the interpretability of the method.", "section": "1 Introduction"}]