[{"figure_path": "YGTVEmBXtV/figures/figures_1_1.jpg", "caption": "Figure 1: Performance of FILM-7B, Mistral-7B-Instruct-v0.2, and GPT4-Turbo on our three probing tasks. FILM-7B significantly overcomes the problem of information loss in the middle of the context.", "description": "This figure presents the results of three probing tasks designed to assess the ability of different LLMs to utilize long contexts.  The x-axis represents the relative position of information within a long context (800 sentences, 800 functions, or 750 entities). The y-axis shows the performance (%).  FILM-7B (the model introduced in this paper), Mistral-7B-Instruct-v0.2 (a baseline model), and GPT4-Turbo (a strong commercial model) are compared.  The results demonstrate that FILM-7B significantly outperforms the baselines in retrieving information from various positions within the long context, especially in the middle of the context, thus addressing the \"lost-in-the-middle\" challenge.", "section": "3 Long-Context Probing"}, {"figure_path": "YGTVEmBXtV/figures/figures_2_1.jpg", "caption": "Figure 2: The data construction process for IN2 training, aimed at enhancing the fine-grained information awareness (upper), and the integration and reasoning of information (lower).", "description": "This figure illustrates the process of creating the dataset for Information-Intensive (IN2) training.  The top half shows the creation of question-answer pairs focusing on fine-grained information awareness. A raw text is split into 128-token segments. One segment is used to prompt GPT-4 to generate a question and answer focusing only on the information within that segment. This QA pair, along with other randomly selected segments, forms the long context. The bottom half shows the generation of QA pairs focused on integration and reasoning.  Multiple segments are selected, and GPT-4 is prompted to generate a QA pair requiring integration of information across these segments. Again, the generated QA pair is combined with randomly selected segments to create the long context.  The goal is to create data where the answer requires information from specific positions within a long context, forcing the model to fully utilize all information.", "section": "2 Information-Intensive Training"}, {"figure_path": "YGTVEmBXtV/figures/figures_3_1.jpg", "caption": "Figure 1: Performance of FILM-7B, Mistral-7B-Instruct-v0.2, and GPT4-Turbo on our three probing tasks. FILM-7B significantly overcomes the problem of information loss in the middle of the context.", "description": "This figure displays the performance of three different large language models (FILM-7B, Mistral-7B-Instruct-v0.2, and GPT4-Turbo) on three probing tasks designed to evaluate their ability to utilize information from various positions within a long context.  The x-axis represents the relative position of the information within the context, while the y-axis shows the performance (likely accuracy or F1 score). The results indicate that FILM-7B significantly outperforms the other two models, especially in retrieving information from the middle of the context, demonstrating its improved ability to avoid the \"lost-in-the-middle\" problem.", "section": "1 Introduction"}, {"figure_path": "YGTVEmBXtV/figures/figures_4_1.jpg", "caption": "Figure 1: Performance of FILM-7B, Mistral-7B-Instruct-v0.2, and GPT4-Turbo on our three probing tasks. FILM-7B significantly overcomes the problem of information loss in the middle of the context.", "description": "This figure displays the performance of three different large language models (FILM-7B, Mistral-7B-Instruct-v0.2, and GPT4-Turbo) on three probing tasks designed to evaluate their ability to utilize information from long contexts.  The x-axis represents the relative position of the information within the context, and the y-axis represents the performance. FILM-7B shows a significant improvement over the other models, particularly in retrieving information from the middle of the long context, thus addressing the \"lost-in-the-middle\" problem.", "section": "3 Long-Context Probing"}, {"figure_path": "YGTVEmBXtV/figures/figures_6_1.jpg", "caption": "Figure 5: Compare the performance of IN2 training and general instruction tuning (IT). Both two training process takes the same number of training instances (20% of the full data size, 300K examples). Compare with Mistral + IN2 training, the gains from normal instruction tuning are marginal and unstable.", "description": "This figure compares the performance of the Information-Intensive (IN2) training method with standard instruction tuning (IT) on three probing tasks.  Both methods used the same number of training examples (20% of the full dataset, or 300,000 examples).  The results show that IN2 training significantly improves performance, especially in addressing the lost-in-the-middle problem, whereas instruction tuning provides only marginal and inconsistent improvements.", "section": "4 Experiments and Analysis"}, {"figure_path": "YGTVEmBXtV/figures/figures_8_1.jpg", "caption": "Figure 6: Performances of FILM-7B and the backbone model on short-context tasks.", "description": "This figure shows the performance comparison between FILM-7B and its base model, Mistral-7B-Instruct-v0.2, across eight common short-context tasks.  The tasks include MMLU, BoolQ, RACE-H, CSQA, ARC-C, HellaSwag, GSM8K, and MATH.  The bar chart visually represents the accuracy achieved by each model on each task, highlighting whether FILM-7B maintains or improves upon the performance of the base model on these tasks that are typically used to evaluate general language model capabilities.", "section": "4.2 Main Results and Analysis"}, {"figure_path": "YGTVEmBXtV/figures/figures_15_1.jpg", "caption": "Figure 7: Performances of FILM-7B on Needle-in-the-Haystack.", "description": "This heatmap visualizes the performance of the FILM-7B model on the Needle-in-the-Haystack task. The x-axis represents the token limit (length of the context), while the y-axis shows the depth percent (the relative position of the answer within the context).  The color intensity represents the model's accuracy in finding the answer.  Darker colors indicate better performance. This figure demonstrates that FILM-7B achieves near-perfect performance on this task, even with a long context window.", "section": "3.1 Near-Perfect Performance on Needle-in-the-Haystack: Are We There Yet?"}, {"figure_path": "YGTVEmBXtV/figures/figures_16_1.jpg", "caption": "Figure 8: Performance of FILM-7B with a 4K sliding window (SW). PT-IN2: apply the sliding window in both pre-training and IN2 training. IN2: apply the sliding window only in IN2 training.", "description": "This figure shows the performance comparison of FILM-7B under different training strategies with a 4K sliding window. The x-axis represents the relative positions of information in the context. The y-axis represents the performance. Three lines represent three training strategies: FILM-7B (20%) without a sliding window; FILM-7B (20%) + SW (IN2), applying the sliding window during IN2 training only; and FILM-7B (20%) + SW (PT-IN2), applying the sliding window during both pre-training and IN2 training.  The results demonstrate that using sliding windows during training significantly hurts the model's ability to retrieve information, especially when the distance between the retrieval keyword and the information exceeds the sliding window size.", "section": "3 Long-Context Probing"}, {"figure_path": "YGTVEmBXtV/figures/figures_18_1.jpg", "caption": "Figure 9: Performance of FILM-7B on 64K context length. The position embeddings are extended through YaRN.", "description": "This figure shows the performance of FILM-7B model with 64K context window length on a document sentence retrieval task (bi-directional retrieval). The x-axis represents the relative positions of the sentences in the context and y-axis represents the performance in terms of percentage. Two models are being compared, FILM-7B and Mistral-7B-Instruct-v0.2.  The position embeddings of FILM-7B are extended using YaRN (Yet Another Retrieval Network) technique to handle longer contexts. The figure demonstrates that FILM-7B significantly outperforms the baseline Mistral model, especially in the middle sections of the long context, indicating the effectiveness of the FILM-7B model in addressing the \"lost-in-the-middle\" problem.", "section": "3 Long-Context Probing"}]