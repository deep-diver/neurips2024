[{"type": "text", "text": "Gradient-Free Methods for Nonconvex Nonsmooth Stochastic Compositional Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhuanghua Liu Department of Computer Science, National University of Singapore CNRS@CREATE LTD, 1 Create Way, #08-01 CREATE Tower, Singapore 138602 liuzhuanghua9@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Luo Luo\u2217 School of Data Science, Fudan University Shanghai Key Laboratory for Contemporary Applied Mathematics luoluo@fudan.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Bryan Kian Hsiang Low Department of Computer Science, National University of Singapore lowkh@comp.nus.edu.sg ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Stochastic compositional optimization (SCO) problems are popular in many realworld applications, including risk management, reinforcement learning, and metalearning. However, most of the previous methods for SCO require the smoothness assumption on both the outer and inner functions, which limits their applications to a wider range of problems. In this paper, we study the SCO problem in that both the outer and inner functions are Lipschitz continuous but possibly nonconvex and nonsmooth. In particular, we propose gradient-free stochastic methods for finding the $(\\delta,\\epsilon)$ -Goldstein stationary points of such problems with non-asymptotic convergence rates. Our results also lead to an improved convergence rate for the convex nonsmooth SCO problem. Furthermore, we conduct numerical experiments to demonstrate the effectiveness of the proposed methods. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we consider the following stochastic compositional optimization (SCO) problem: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{d}}\\Phi(\\mathbf{x})\\triangleq f(g(\\mathbf{x})),\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where the outer and inner functions $f\\colon\\ensuremath{\\mathbb{R}}^{m}\\to\\ensuremath{\\mathbb{R}}$ and $g\\colon\\ensuremath{\\mathbb{R}}^{d}\\to\\ensuremath{\\mathbb{R}}^{m}$ has the form of ", "page_idx": 0}, {"type": "equation", "text": "$$\nf(\\mathbf{y})\\triangleq\\mathbb{E}_{\\boldsymbol\\xi}[F(\\mathbf{y};\\boldsymbol{\\xi})]\\qquad\\mathrm{and}\\qquad g(\\mathbf{x}):=\\mathbb{E}_{\\boldsymbol\\xi}[G(\\mathbf{x};\\boldsymbol{\\xi})],\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "and the stochastic components $F(\\mathbf{y};\\pmb{\\xi})$ and $G(\\mathbf{x};\\zeta)$ are Lipschitz continuous but possibly nonconvex and nonsmooth. Random variables $\\xi$ and $\\zeta$ are independent. Such formulation is popular in many realworld applications, including risk management [1], statistical learning [2], reinforcement learning [3], and model agnostic meta-learning [4]. ", "page_idx": 0}, {"type": "text", "text": "Most of the existing work [2, 5, 6, 7, 8] for nonconvex SCO problem is based on the assumption that both functions $f(\\cdot)$ and $g(\\cdot)$ are smooth. Unfortunately, many modern machine learning ", "page_idx": 0}, {"type": "table", "img_path": "", "table_caption": ["Table 1: We present the stochastic zeroth-order complexity of proposed algorithms for solving nonsmooth stochastic compositional optimization problems. "], "table_footnote": [], "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathrm{METHODS}\\ }&{\\,\\mathrm{PRobLEM}}&{\\quad\\mathrm{CoMPLEXITY}\\quad}&{\\mathrm{REFERENCE}}\\\\ {\\mathrm{GFCOM}\\ }&{\\mathrm{NoncoNVEX}}&{\\mathcal{O}(d^{3.5}\\delta^{-3}\\epsilon^{-6})}&{\\mathrm{CoROLLARY~4.2}}\\\\ {\\mathrm{GFCOM^{+}}\\ }&{\\mathrm{NoncoNVEX}}&{\\mathcal{O}(d^{3.5}\\delta^{-3}\\epsilon^{-5})}&{\\mathrm{CoROLLARY~4.4}}\\\\ {\\mathrm{WS-GFCOM^{2}}\\ }&{\\mathrm{CoNvEX}}&{\\mathcal{O}(d^{3}\\delta^{-2.4}\\epsilon^{-4.8}+d^{3.5}\\delta^{-2}\\epsilon^{-6})}&{\\mathrm{CoROLLARY~5.3}}\\\\ {\\mathrm{WS-GFCOM^{+}}\\ }&{\\mathrm{CoNvEX}}&{\\mathcal{O}(d^{3}\\delta^{-2.4}\\epsilon^{-4}+d^{3.5}\\delta^{-2}\\epsilon^{-5})\\ }&{\\mathrm{CoROLLARY~5.4}}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "models including deep neural networks do not satisfy the smoothness condition. Ruszczynski [9] proposed a single time-scale stochastic subgradient method for solving the Problem (1). However, the author only provided asymptotic convergence analysis for the approach. In recent work, Liu and Davanloo Tajbakhsh [10], Hu et al. [11] presented the non-asymptotic convergence for the nonconvex nonsmooth SCO problem, while their analysis requires additional assumptions such as the weak-convexity and the relative smoothness condition. ", "page_idx": 1}, {"type": "text", "text": "The non-smoothness in the Problem (1) implies the classical gradient-based approaches and the convergence measure in terms of the gradient norm cannot be applied. The Clarke subdifferential [12] for the Lipshitz continuous functions is a natural extension of gradients for the smooth functions. However, hard instances have shown that no deterministic or randomized algorithms can find an \u03f5- stationary point with respect to the Clarke subdifferential of a Lipschitz function in finite time [13, 14]. To address this issue, Zhang et al. [13] proposed a refined notion of the $(\\delta,\\epsilon)$ -Goldstein stationary point in terms of the Goldstein $\\delta$ -subdifferential, which considers the convex hull of the Clarke subdifferential at points in the $\\delta$ -neighbourhood [15]. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose a zeroth-order stochastic method called gradient-free compositional optimization method (GFCOM) for solving Problem (1) in finite time. In particular, we show that the GFCOM can find a $(\\delta,\\epsilon)$ -Goldstein stationary point of the objective function $\\Phi(\\cdot)\\,=\\,f(g(\\cdot))$ within the stochastic zeroth-order oracle complexity of $O(d^{3.5}\\delta^{-3}\\bar{\\epsilon}^{-6})$ . Furthermore, we improve the GFCOM by using the variance reduction technique [16, 17, 18, 19] to establish a more efficient first-order oracle estimator, leading to the algorithm $\\mathrm{GFCOM^{+}}$ which achieves a tighter upper complexity bound of $\\mathcal{O}(d^{3.5}\\delta^{-3}\\epsilon^{-5})$ . In addition, we study convex nonsmooth SCO problems. In this regime, prior methods [2, 20, 21] suffer two major limitations: (i) Their convergence analysis is based on the smoothness condition of the outer function. (ii) Their convergence result is measured by the sub-optimality of the function value gap, while the non-asymptotic convergence rate for finding the stationary point has not been studied. We overcome these issues by involving a warm-start strategy into $\\mathrm{GFCOM^{+}}$ , which is guarantee to find a $(\\delta,\\epsilon)$ -Goldstein stationary point within $\\mathcal{O}(d^{3}\\delta^{-2.\\bar{4}}\\dot{\\epsilon}^{-4}+d^{3.5}\\delta^{-2}\\epsilon^{-5})$ stochastic zeroth-order oracle complexity. We summarize the complexity of proposed methods in Table 1. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we review prior work for stochastic compositional optimization and classical nonconvex nonsmooth optimization. ", "page_idx": 1}, {"type": "text", "text": "2.1 Stochastic Compositional Optimization ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In a pioneer work, Wang et al. [2] studied the non-asymptotic convergence of nonconvex smooth stochastic compositional optimization by proposing the stochastic compositional gradient descent (SCGD), which contains two sequences of stepsizes for different time scales to update the variable and track the inner function value, respectively. The authors also heuristically extended their methods to zeroth-order optimization. Wang et al. [5] proposed an accelerated variant of SCGD using an extrapolation-smoothing scheme, and Ghadimi et al. [6] proposed a single time-scale approach to accelerate the convergence further. Additionally, Hu et al. [7], Lin et al. [20], Yuan et al. [22] incorporated the variance reduction technique into the first-order iteration, achieving a tight stochastic first-order complexity under the mean-squared smoothness assumption. ", "page_idx": 1}, {"type": "text", "text": "Compared with the smooth counterpart, the study of nonsmooth compositional optimization is relatively scarce. Ruszczynski [9] proposed a single time-scale stochastic subgradient method for nonconvex nonsmooth SCO problems, while the theoretical analysis only provided the asymptotic convergence rate. Liu and Davanloo Tajbakhsh [10] introduced the stochastic composition Bregman gradient method and provided a non-asymptotic convergence analysis under the relative smoothness condition. Vladarean et al. [23] proposed a Frank-Wolfe algorithm for constrained nonconvex nonsmooth SCO problems. Their analysis assumes that the outer function is convex but possibly non-differentiable, and the inner function is smooth. Very recently, Hu et al. [11] studied stochastic methods for the finite-sum coupled compositional optimization problem. Their convergence rate is established by assuming both the outer and inner functions are weakly convex, and the outer function is non-decreasing. In addition, Kalogerias and Powell [24] studied the zeroth-order stochastic optimization for a specific compositional optimization problem in risk-aware learning. ", "page_idx": 2}, {"type": "text", "text": "2.2 Non-Asymptotic convergence Analysis of Nonconvex Nonsmooth Optimization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this subsection, we present a literature review for classical nonconvex nonsmooth optimization. The study of this field has a long history [12, 25], but the non-asymptotic convergence analysis of nonsmooth optimization has only emerged in recent years. Zhang et al. [13] provided the nonasymptotic complexity analysis of the interpolated normalized gradient descent method to achieve a $(\\delta,\\epsilon)$ -Goldstein stationary point of a Lipschitz function with a nonstandard subgradient oracle. Davis et al. [26], Tian et al. [27] improved this method by introducing random perturbations in each iteration to remove the assumptions. Recently, Cutkosky et al. [28] proposed the optimal algorithm via the reduction from nonconvex nonsmooth optimization to online learning. ", "page_idx": 2}, {"type": "text", "text": "The development of non-asymptotic convergence analysis of zeroth-order methods for nonsmooth optimization was initiated by Nesterov and Spokoiny [29]. Later, Lin et al. [30] proposed gradientfree methods for this problem by establishing a relationship between the Goldstein $\\delta$ -subdifferential and randomized smoothing. Chen et al. [31], Liu et al. [32] improved their results by leveraging the variance-reduction technique. Kornowski and Shamir [33] obtained a sharper bound by applying the reduction technique introduced by Cutkosky et al. [28] to the gradient-free setting. Liu et al. [34], Grimmer and Jia [35] further extends the methodology to the constrained setting. However, these methods do not apply to the nonconvex nonsmooth SCO Problem (1). ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we first present the notations and assumptions used in this paper, then introduce the convergence criteria for nonsmooth optimization and the randomized smoothing technique. ", "page_idx": 2}, {"type": "text", "text": "3.1 Notations and Assumptions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We use $\\left\\Vert\\cdot\\right\\Vert$ to denote the Euclidean norm of a vector. We define $\\mathbb{B}_{\\delta}(\\mathbf{x})\\triangleq\\{\\mathbf{y}\\in\\mathbb{R}^{d}:\\|\\mathbf{y}-\\mathbf{x}\\|\\leq\\delta\\}$ as the Euclidean ball centered at $\\mathbf{x}\\in\\mathbb{R}^{d}$ with a radius $\\delta>0$ . We let $\\operatorname{conv}(A)$ be the convex hull of the set $A$ . For two given sets $A$ and $B$ , we define $A\\times B$ as their Cartesian product. In addition, we denote $f\\circ g$ as the function composition such that $(f\\circ g)(\\mathbf{x})\\triangleq f(g(\\mathbf{x}))$ . ", "page_idx": 2}, {"type": "text", "text": "Throughout this paper, we assume the objective function (1) satisfies the following assumptions. ", "page_idx": 2}, {"type": "text", "text": "Assumption 3.1. We assume the stochastic component $F(\\cdot,\\pmb\\xi)$ is $L_{f}(\\pmb\\xi)$ -Lipschitz for any given $\\xi$ , and the stochastic component $G(\\cdot,\\zeta)$ is $L_{g}(\\zeta)$ -Lipschitz for any given $\\zeta$ . That is, it holds ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|F(\\mathbf{x},\\pmb{\\xi})-F(\\mathbf{y},\\pmb{\\xi})|\\leq L_{f}(\\pmb{\\xi})\\left\\|\\mathbf{x}-\\mathbf{y}\\right\\|\\quad a n d\\quad\\|G(\\hat{\\mathbf{x}},\\pmb{\\zeta})-G(\\hat{\\mathbf{y}},\\pmb{\\zeta})\\|\\leq L_{g}(\\pmb{\\zeta})\\left\\|\\hat{\\mathbf{x}}-\\hat{\\mathbf{y}}\\right\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "for any x, $\\textbf y\\in\\mathbb{R}^{d}$ and $\\hat{\\mathbf{x}},\\hat{\\mathbf{y}}\\,\\in\\,\\mathbb{R}^{m}$ . We also assume the Lipschitz parameters $L_{f}(\\pmb\\xi)$ and $L_{g}(\\zeta)$ have bounded second-order moments such that $\\mathbb{E}_{\\pmb{\\xi}}[L_{f}(\\pmb{\\xi})^{2}]\\leq G_{f}^{2}$ and $\\mathbb{E}_{\\zeta}[L_{g}(\\zeta)^{2}]\\leq G_{g}^{2}$ for some constants $G_{f},G_{g}>0$ . ", "page_idx": 2}, {"type": "text", "text": "Remark 3.2. We can verify that Assumption 3.1 implies the function $f(\\cdot)$ is $G_{f}$ -Lipschitz, and the function $g(\\cdot)$ is $G_{g}$ -Lipschitz by Jensen\u2019s inequality. ", "page_idx": 2}, {"type": "text", "text": "Assumption 3.3. We assume that there exists a constant $\\sigma_{0}$ as the upper bound on the variance of the functions $G(\\cdot,\\zeta)$ , such that for any $\\mathbf{x}\\in\\mathbb{R}^{d}$ , we have $\\mathbb{E}_{\\zeta}\\left[\\left\\|G(\\mathbf{x},\\bar{\\zeta})-g(\\mathbf{x})\\right\\|^{2}\\right]\\leq\\sigma_{0}^{2}$ . ", "page_idx": 2}, {"type": "text", "text": "Assumption 3.4. We assume that the composite function $\\Phi(\\cdot)\\triangleq(f\\circ g)(\\cdot)$ is lower bounded such that $\\begin{array}{r}{\\Phi^{*}\\triangleq\\operatorname*{inf}_{\\mathbf{x}\\in\\mathbb{R}^{d}}\\Phi(\\mathbf{x})>-\\infty}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 Convergence Criteria for Nonsmooth Functions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We introduce the definitions of the Clarke subdifferential and approximate Clarke stationary points. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.5 (Clarke [12]). The Clarke subdifferential of a Lipschitz function $f$ at a point $\\mathbf{x}$ is defined as $\\begin{array}{r}{\\partial f(\\mathbf{x})\\triangleq\\operatorname{conv}\\left\\{g:g=\\operatorname*{lim}_{\\mathbf{x}_{k}\\to\\mathbf{x}}\\nabla f(\\mathbf{x}_{k})\\right\\}}\\end{array}$ . Furthermore, we call a point $\\mathbf{x}$ an $\\epsilon$ -Clarke stationary point of $f$ if it holds that $\\operatorname*{min}\\{\\|g\\|:g\\in\\partial f(\\mathbf{x})\\}\\leq\\epsilon$ . ", "page_idx": 3}, {"type": "text", "text": "Zhang et al. [13], Kornowski and Shamir [14] showed that no deterministic or randomized algorithm could find an $\\epsilon$ -Clarke stationary point in finite time. Consequently, Zhang et al. [13] considered a refined notion of approximate stationary point in terms of the Goldstein $\\delta$ -subdifferential. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.6 (Zhang et al. [13]). Given a Lipschitz function $f\\colon\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ and $\\delta>0$ , the Goldstein $\\delta$ -subdifferential of $f$ at point $\\mathbf{x}\\in\\mathbb{R}^{d}$ is defined as $\\partial_{\\delta}f\\big(\\mathbf{x}\\big):=\\mathrm{conv}\\big(\\cup_{\\mathbf{y}\\in\\mathbb{B}_{\\delta}(\\mathbf{x})}\\partial f\\big(\\mathbf{y}\\big)\\big)$ , which is the convex hull of the Clarke subdifferential at the points in the $\\delta$ -neighbourhood of x. Additionally, a point $\\mathbf{x}\\in\\dot{\\mathbb{R}}^{d}$ is called $a$ $(\\delta,\\epsilon)$ -Goldstein stationary point of $f(\\cdot)$ if it holds that $\\operatorname*{min}\\{\\|g\\|:g\\in$ $\\bar{\\partial_{\\delta}}f(\\mathbf{x})\\}\\leq\\epsilon$ . ", "page_idx": 3}, {"type": "text", "text": "Recent work [13, 26, 28] has shown that it is possible to find a $(\\delta,\\epsilon)$ -Goldstein stationary point of a nonsmooth problem without a composition structure in finite time. However, these theories are not applicable to nonsmooth SCO. In particular, we can infer from Rademacher\u2019s theorem and Assumption 3.1 that the composite function $\\Phi(\\cdot)$ is differentiable almost everywhere. Let $\\mathcal{Q}\\subseteq\\mathbb{R}^{d}$ be the set on which $\\Phi$ is differentiable, then $\\mathbb{R}^{d}\\setminus\\mathcal{Q}$ is of measure zero. Recent work assumes they have access to the unbiased stochastic gradient estimator of the objective function for any $\\mathbf{x}\\in\\mathcal{Q}$ . In our setting, the unbiased gradient estimator of the composite function $\\Phi(\\mathbf{x})$ is $\\mathcal{J}_{G}(\\mathbf{x};\\zeta)\\nabla F(g(\\mathbf{x});\\pmb{\\xi})$ , where $\\mathcal{I}_{G}$ is the Jacobian matrix of the function $G(\\cdot;\\zeta)$ . However, such an estimator is hard to obtain because the function value $g\\mathbf{(x)}$ is an expectation. ", "page_idx": 3}, {"type": "text", "text": "3.3 Randomized Smoothing ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The randomized smoothing is a popular technique for nonsmooth analysis [36] and gradient-free optimization [29]. Concretely, given a Lipschitz function $f$ and a uniform distribution $\\mathcal{P}$ on a unit ball, we define its smoothed surrogate as $f_{\\delta}(\\bar{\\mathbf{x}})=\\mathbb{E}_{\\mathbf{u}\\sim\\mathcal{P}}[f(\\mathbf{x}+\\delta\\mathbf{u})]$ , which has the following properties. ", "page_idx": 3}, {"type": "text", "text": "Lemma 3.7 (Lin et al. [30, Proposition 2.3]). Let $f_{\\delta}(\\mathbf{x})=\\mathbb{E}_{\\mathbf{u}\\sim\\mathcal{P}}[f(\\mathbf{x}+\\delta\\mathbf{u})]$ where $\\mathcal{P}$ is a uniform distribution on a unit ball in $\\ell_{2}$ -norm. Suppose the function $f$ is $L$ -Lipschitz, th\u221aen we have (a) $|f_{\\delta}(\\mathbf{x})-f(\\mathbf{x})|\\leq\\delta L$ ; (b) $f_{\\delta}(\\cdot)$ is differentiable everywhere and $L$ -Lipschitz with $c L\\sqrt{d}\\delta^{-1}$ -Lipschitz gradient, where c is some positive constant; (c) $\\nabla f_{\\delta}(\\mathbf{x})\\in\\partial_{\\delta}f(\\mathbf{x}).$ for any $\\mathbf{x}\\in\\mathbb{R}^{d}$ . ", "page_idx": 3}, {"type": "text", "text": "Moreover, we consider the following unbiased gradient estimator of the smoothed surrogate function $f_{\\delta}(\\cdot)$ , which can be obtained from two function query oracle calls on points uniformly sampled from a unit sphere [37]. ", "page_idx": 3}, {"type": "text", "text": "Lemma 3.8 (Lin et al. [30, Lemma D.1]). Let $f(\\mathbf{x})=\\mathbb{E}[F(\\mathbf{x};\\pmb{\\xi})]$ be a $L$ -Lipschitz function. We denote ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\iota(\\mathbf{x};\\mathbf{u},\\pmb{\\xi})\\triangleq\\frac{d}{2\\delta}(F(\\mathbf{x}+\\delta\\mathbf{u};\\pmb{\\xi})-F(\\mathbf{x}-\\delta\\mathbf{u};\\pmb{\\xi}))\\mathbf{u},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where u is uniformly sampled from a distribution on\u221a a unit sphere in $\\mathbb{R}^{d}$ space. Then, we have $\\mathbb{E}[\\iota(\\mathbf{x};\\mathbf{u},\\pmb{\\xi})]=\\nabla f_{\\delta}(\\mathbf{x})$ and $\\mathbb{E}[\\left\\|\\iota(\\mathbf{x};\\mathbf{u},\\pmb{\\xi})\\right\\|^{2}]\\leq16\\sqrt{2\\pi}d L^{2}$ . ", "page_idx": 3}, {"type": "text", "text": "4 Algorithms for Nonconvex Nonsmooth SCO ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we propose zeroth-order stochastic algorithms for solving the nonconvex nonsmooth SCO problem. We also provide non-asymptotic convergence analysis for the proposed methods. ", "page_idx": 3}, {"type": "text", "text": "1 for $t=0,1,\\ldots,T-1$ do   \n2 Sample $\\{\\xi_{t,i},\\mathbf{w}_{t,i}\\}_{i=1}^{b_{f}}$ and $\\{\\zeta_{t,i}\\}_{i=1}^{b_{g}}$ .   \n3 Generate $G(\\mathbf{x}_{t}\\pm\\delta\\mathbf{w}_{t,j};\\pmb{\\zeta}_{t,i})$ for every $(i,j)\\in[b_{g}]\\times[b_{f}]$ .   \n4 Let $\\begin{array}{r}{\\mathbf{y}_{t,j}=\\frac{1}{b_{g}}\\sum_{i\\in[b_{g}]}G(\\mathbf{x}_{t}+\\delta\\mathbf{w}_{t,j};\\zeta_{t,i})}\\end{array}$ .   \n5 Let $\\begin{array}{r}{\\mathbf{z}_{t,j}=\\frac{1}{b_{g}}\\sum_{i\\in[b_{g}]}G(\\mathbf{x}_{t}-\\delta\\mathbf{w}_{t,j};\\zeta_{t,i})}\\end{array}$ .   \n6 Let $\\begin{array}{r}{\\mathbf{v}_{t}=\\frac{1}{b_{f}}\\sum_{j\\in[b_{f}]}\\frac{d}{2\\delta}(F(\\mathbf{y}_{t,j};\\pmb{\\xi}_{t,j})-F(\\mathbf{z}_{t,j};\\pmb{\\xi}_{t,j}))\\mathbf{w}_{t,j}.}\\end{array}$   \n7 Update $\\mathbf{x}_{t+1}=\\mathbf{x}_{t}-\\eta\\mathbf{v}_{t}$ .   \n8 end ", "page_idx": 4}, {"type": "text", "text": "9 Return: $\\mathbf{x}_{R}$ where $R$ is uniformly sampled from $[T]$ ", "page_idx": 4}, {"type": "equation", "text": "$\\mathbf{Algorithm\\:2\\!:\\,GFCOM^{+}(x_{0},\\eta,T,b_{f},b_{f}^{\\prime},b_{g},b_{g}^{\\prime},m)}$ ", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "1 for $t=0,1,\\ldots,T-1$ do   \n2 if t mod $m=0$ then   \n3 Sample $\\{\\xi_{t,i},\\mathbf{w}_{t,i}\\}_{i=1}^{b_{f}}$ and $\\{\\zeta_{t,i}\\}_{i=1}^{b_{g}}$ .   \n4 Generate $G(\\mathbf{x}_{t}\\pm\\delta\\mathbf{w}_{t,j};\\pmb{\\zeta}_{t,i})$ for every $(i,j)\\in[b_{g}]\\times[b_{f}]$ .   \n5 Let $\\begin{array}{r}{\\mathbf{y}_{t,j}=\\frac{1}{b_{g}}\\sum_{i\\in[b_{g}]}G(\\mathbf{x}_{t}+\\delta\\mathbf{w}_{t,j};\\zeta_{t,i})}\\end{array}$ .   \n6 Let $\\begin{array}{r}{\\mathbf{z}_{t,j}=\\frac{1}{b_{g}}\\sum_{i\\in[b_{g}]}G(\\mathbf{x}_{t}-\\delta\\mathbf{w}_{t,j};\\zeta_{t,i})}\\end{array}$ .   \n7 Let $\\begin{array}{r}{\\mathbf{v}_{t}=\\frac{1}{b_{f}}\\sum_{j\\in[b_{f}]}\\frac{d}{2\\delta}(F(\\mathbf{y}_{t,j};\\pmb{\\xi}_{t,j})-F(\\mathbf{z}_{t,j};\\pmb{\\xi}_{t,j}))\\mathbf{w}_{t,j}.}\\end{array}$   \n8 else   \n9 Sample $\\{\\xi_{t,i},\\mathbf{w}_{t,i}\\}_{i=1}^{b_{f}^{\\prime}}$ and $\\{\\zeta_{t,i}\\}_{i=1}^{b_{g}^{\\prime}}$ .   \n10 Generate $G(\\mathbf{x}_{t}\\pm\\delta\\mathbf{w}_{t,j};\\zeta_{t,i})$ and $G(\\mathbf{x}_{t-1}\\pm\\delta\\mathbf{w}_{t,j};\\pmb{\\zeta}_{t,i})$ for every $(i,j)\\in[b_{g}^{\\prime}]\\times[b_{f}^{\\prime}]$ .   \n11 Let $\\begin{array}{r}{\\mathbf{y}_{k,j}=\\frac{1}{b_{g}^{\\prime}}\\sum_{i\\in[b_{g}^{\\prime}]}G(\\mathbf{x}_{k}+\\delta\\mathbf{w}_{t,j};\\zeta_{k,i})}\\end{array}$ for $k\\in\\{t-1,t\\}$ .   \n12 Let $\\begin{array}{r}{\\mathbf{z}_{k,j}=\\frac{1}{b_{g}^{\\prime}}\\sum_{i\\in[b_{g}^{\\prime}]}G(\\mathbf{x}_{k}-\\delta\\mathbf{w}_{t,j};\\zeta_{k,i})}\\end{array}$ for $k\\in\\{t-1,t\\}$ .   \n13 Let $\\begin{array}{r}{\\mathbf{q}_{k}=\\frac{1}{b_{f}^{\\prime}}\\sum_{j\\in[b_{f}^{\\prime}]}\\frac{d}{2\\delta}(F(\\mathbf{y}_{k,j};\\pmb{\\xi}_{t,j})-F(\\mathbf{z}_{k,j};\\pmb{\\xi}_{t,j}))\\mathbf{w}_{t,j}}\\end{array}$ for $k\\in\\{t-1,t\\}$ .   \n14 Let $\\mathbf{v}_{t}=\\mathbf{q}_{t}-\\mathbf{q}_{t-1}+\\mathbf{v}_{t-1}$ .   \n15 end   \n16 Update $\\mathbf{x}_{t+1}=\\mathbf{x}_{t}-\\eta\\mathbf{v}_{t}$ .   \n17 end   \n18 Return: $\\mathbf{x}_{R}$ where $R$ is uniformly sampled from $[T]$ . ", "page_idx": 4}, {"type": "text", "text": "4.1 The Algorithms ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this subsection, we propose the gradient-free compositional optimization method (GFCOM) and its accelerated variant $\\mathrm{GFCOM^{+}}$ . We first introduce the main intuition of the GFCOM. Consider the following hypothetical zeroth-order gradient estimator ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{v}}_{t}=\\frac{1}{b_{f}}\\sum_{j\\in[b_{f}]}\\frac{d}{2\\delta}(F(g(\\mathbf{x}_{t}+\\delta\\mathbf{w}_{t,j});\\pmb{\\xi}_{t,j})-F(g(\\mathbf{x}_{t}-\\delta\\mathbf{w}_{t,j});\\pmb{\\xi}_{t,j}))\\mathbf{w}_{t,j},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $b_{f}\\,>\\,0$ is the mini-batch size of the gradient estimator. By Lemma 3.8, the vector $\\bar{\\mathbf{v}}_{t}$ is an unbiased estimator of $\\nabla\\Phi_{\\delta}(\\mathbf{x}_{t})$ . Unfortunately, it is intractable to obtain the function values $g(\\mathbf{x}_{t}\\pm\\delta\\mathbf{w}_{t,j})$ because $g(\\cdot)$ is an expectation of stochastic component functions $G(\\cdot;\\zeta)$ . To remedy this issue, we introduce auxiliary variables $\\mathbf{y}_{t,j}$ and $\\mathbf{z}_{t,j}$ to approximate the inner function values $g(\\mathbf{x}_{t}+\\delta\\mathbf{w}_{t,j})$ and $g(\\mathbf{x}_{t}-\\delta\\mathbf{w}_{t,j})$ , respectively. In particular, the vectors $\\mathbf{y}_{t,j}$ and $\\mathbf{z}_{t,j}$ are mini-batch function estimators defined as follows ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{y}_{t,j}=\\frac{1}{b_{g}}\\sum_{i\\in[b_{g}]}G(\\mathbf{x}_{t}+\\delta\\mathbf{w}_{t,j};\\zeta_{t,i}),\\quad\\mathrm{and}\\quad\\mathbf{z}_{t,j}=\\frac{1}{b_{g}}\\sum_{i\\in[b_{g}]}G(\\mathbf{x}_{t}-\\delta\\mathbf{w}_{t,j};\\zeta_{t,i}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $b_{g}>0$ is the mini-batch size of the function estimator. Accordingly, we use these two variables to replace the function calls of $g(\\cdot)$ in the gradient estimator $\\mathbf{v}_{t}$ of Eq. (2). The complete procedure of the GFCOM is presented in Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "For the $\\mathrm{GFCOM^{+}}$ , we leverage the variance reduction technique to approximate $\\nabla\\Phi_{\\delta}(\\mathbf{x}_{t})$ . In particular, we consider the following hypothetical recursive gradient estimator ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{v}}_{t}=\\bar{\\mathbf{q}}_{t}-\\bar{\\mathbf{q}}_{t-1}+\\mathbf{v}_{t-1},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\bar{\\bf q}_{t}$ and $\\bar{\\bf q}_{t-1}$ are mini-batch gradient estimator defined as follows. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{q}}_{k}=\\frac{1}{b_{f}^{\\prime}}\\sum_{j\\in[b_{f}^{\\prime}]}\\frac{d}{2\\delta}(F(g(\\mathbf{x}_{k}+\\delta\\mathbf{w}_{t,j});\\xi_{t,j})-F(g(\\mathbf{x}_{k}-\\delta\\mathbf{w}_{t,j});\\xi_{t,j}))\\mathbf{w}_{t,j},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $b_{f}^{\\prime}>0$ is the mini-batch size and $k\\in\\{t-1,t\\}$ . Compared with the mini-batch gradient estimator (2), the recursive gradient estimator (4) has been shown to achieve a sharper complexity bound in nonconvex optimization literature [17, 18, 31]. However, the gradient estimator is computationally intractable due to the unknown function $g(\\cdot)$ . Similar to the development of Algorithm 1, we define $\\mathbf{y}_{t},\\,\\mathbf{z}_{t}$ to estimate the inner function values $g(\\mathbf{x}_{t}\\pm\\delta\\mathbf{w}_{t,j})$ . We also introduce variables ${\\bf y}_{t-1},{\\bf z}_{t-1}$ to approximate the inner function values $g(\\mathbf{x}_{t-1}\\pm\\delta\\mathbf{w}_{t,j})$ at the previous iteration. Then we define stochastic gradient estimators $\\mathbf{q}_{t}$ and $\\mathbf q_{t-1}$ in terms of ${\\bf y}_{t},{\\bf y}_{t-1},{\\bf z}_{t}$ and $\\mathbf{Z}_{t-1}$ . ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{q}_{k}=\\frac{1}{b_{f}^{\\prime}}\\sum_{j\\in[b_{f}^{\\prime}]}\\frac{d}{2\\delta}(F(\\mathbf{y}_{k,j};\\pmb{\\xi}_{t,j})-F(\\mathbf{z}_{k,j};\\pmb{\\xi}_{t,j}))\\mathbf{w}_{t,j},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for $k\\in\\{t-1,t\\}$ . We replace the minibatch gradient estimator $\\bar{\\bf q}_{t}$ and $\\bar{\\bf q}_{t-1}$ in the recursive gradient estimator $\\bar{\\mathbf{v}}_{t}$ of Eq. (4) with the refined gradient estimators $\\mathbf{q}_{t}$ and $\\mathbf q_{t-1}$ . The complete procedure of $\\mathrm{GFCOM^{+}}$ is presented in Algorithm 2. ", "page_idx": 5}, {"type": "text", "text": "4.2 Convergence Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this subsection, we consider the complexity analysis of the proposed algorithms introduced in Section 4.1. We assume that $\\Phi(x_{0})-\\Phi^{*}\\leq R$ , where $R>0$ is some constant. ", "page_idx": 5}, {"type": "text", "text": "The following theorem shows the convergence rate of solving the Problem (1) with the GFCOM method presented in Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1. Unde\u221ar Assumption 3.1, 3.3 and 3.4, running the GFCOM algorithm (Algorithm 1) with $\\eta\\leq\\delta/(c G_{f}G_{g}{\\sqrt{d}})$ where $c>0$ is some constant, then the output $\\mathbf{x}_{R}$ satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert\\nabla\\Phi_{\\delta}(\\mathbf{x}_{R})\\Vert^{2}\\right]=\\mathcal{O}\\left(\\frac{G_{f}G_{g}\\sqrt{d}R}{\\delta T}+\\frac{G_{f}^{2}G_{g}^{2}\\sqrt{d}}{T}+\\frac{d G_{f}^{2}G_{g}^{2}}{b_{f}}+\\frac{d^{2}G_{f}^{2}\\sigma_{0}^{2}}{\\delta^{2}b_{g}}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Using Theorem 4.1 with the parameter setting ", "page_idx": 5}, {"type": "equation", "text": "$$\nT=\\Theta\\left(\\frac{G_{f}G_{g}\\sqrt{d}R}{\\delta\\epsilon^{2}}+\\frac{G_{f}^{2}G_{g}^{2}\\sqrt{d}}{\\epsilon^{2}}\\right),\\quad b_{f}=\\Theta\\left(\\frac{d G_{f}^{2}G_{g}^{2}}{\\epsilon^{2}}\\right)\\quad\\mathrm{and}\\quad b_{g}=\\Theta\\left(\\frac{d^{2}G_{f}^{2}\\sigma_{0}^{2}}{\\delta^{2}\\epsilon^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "we obtain the following oracle complexity result for Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "Corollary 4.2. Under Assumption 3.1, 3.3 and 3.4, the GFCOM algorithm (Algorithm 1) requires at most $\\mathcal{O}\\big(\\dot{d}^{3.5}G_{f}^{5}G_{g}^{3}\\sigma_{0}^{2}R\\delta^{-3}\\epsilon^{-6}+d^{3.5}G_{f}^{6}G_{g}^{4}\\sigma_{0}^{2}\\delta^{-2}\\epsilon^{-6}\\big)$ stochastic zeroth-order function query calls to obtain a $(\\delta,\\epsilon)$ -Goldstein stationary point of $\\Phi$ . ", "page_idx": 5}, {"type": "text", "text": "After giving the complexity bound of GFCOM in Corollary 4.2, we now consider the convergence analysis of $\\mathrm{GFCOM^{+}}$ . We will show that it enjoys a sharper complexity bound due to the utilization of the recursive gradient estimator. The following theorem shows the convergence rate of solving Problem (1) with the $\\mathrm{GFCOM^{+}}$ (Algorithm 2). ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.3. Under\u221a Assumption 3.1, 3.3 and 3.4, running the $G F C O M^{+}$ algorithm (Algorithm 2) with $\\eta\\!=\\!\\delta/(2c G_{f}G_{g}{\\sqrt{d}})$ , $b_{f}^{\\prime}\\!=\\!\\Theta(d G_{f}G_{g}\\epsilon^{-1})$ and $m\\!=\\!\\Theta(G_{f}G_{g}\\epsilon^{-1})$ , then the output $\\mathbf{x}_{R}$ satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert\\nabla\\Phi_{\\delta}(\\mathbf{x}_{R})\\Vert^{2}\\right]=\\mathcal{O}\\left(\\frac{\\sqrt{d}G_{f}G_{g}R}{\\delta T}+\\frac{\\sqrt{d}G_{f}^{2}G_{g}^{2}}{T}+\\frac{d G_{f}^{2}G_{g}^{2}}{b_{f}}+\\frac{d^{2}G_{f}^{2}\\sigma_{0}^{2}}{\\delta^{2}b_{g}}+\\frac{d^{2}G_{f}^{2}\\sigma_{0}^{2}}{\\delta^{2}b_{g}^{\\prime}}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Using Theorem 4.3 with the parameter setting ", "page_idx": 6}, {"type": "equation", "text": "$$\nT\\!=\\!\\Theta\\!\\left(\\frac{\\sqrt{d}G_{f}G_{g}R}{\\delta\\epsilon^{2}}\\!+\\!\\frac{\\sqrt{d}G_{f}^{2}G_{g}^{2}}{\\epsilon^{2}}\\right),~~b_{f}\\!=\\!\\Theta\\!\\left(\\frac{d G_{f}^{2}G_{g}^{2}}{\\epsilon^{2}}\\right),~~b_{g}\\!=\\!b_{g}^{\\prime}\\!=\\!\\Theta\\!\\left(\\frac{d^{2}G_{f}^{2}\\sigma_{0}^{2}}{\\delta^{2}\\epsilon^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "we obtain the following oracle complexity result for Algorithm 2. ", "page_idx": 6}, {"type": "text", "text": "Corollary 4.4. Under Assumption 3.1, 3.3 and 3.4, the GFCOM+ algorithm (Algorithm 2) requires at most $\\bar{\\mathcal{O}}\\big(d^{3.5}G_{f}^{4}G_{g}^{2}\\sigma_{0}^{2}R\\delta^{-\\hat{3}}\\epsilon^{-5}+d^{3.5}G_{f}^{5}G_{g}^{3}\\sigma_{0}^{2}\\delta^{-2}\\epsilon^{-5}\\big)$ stochastic zeroth-order function query calls to obtain $a$ $(\\delta,\\epsilon)$ -Goldstein stationary point of $\\Phi$ . ", "page_idx": 6}, {"type": "text", "text": "For both Theorem 4.1 and 4.3, we take $c=1$ according to Lemma 8 of Duchi et al. [36]. ", "page_idx": 6}, {"type": "text", "text": "4.3 Discussion ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In Algorithm 2, we only apply the variance reduction technique to the outer function $f(\\cdot)$ to accelerate our algorithm. In contrast, existing methods [2, 7, 22] for nonconvex smooth SCO also apply the technique on the inner function estimator to obtain an improved complexity bound. Here we briefly discuss the cause that leads to such a difference. For smooth optimization, the main intuition of the variance reduction technique is to establish a connection between the bound of the meansquare error term $\\mathbb{E}\\big[\\left\\|\\mathbf{v}_{t}-\\nabla\\Phi(\\mathbf{x}_{t})\\right)\\big\\|^{2}\\big]$ and the expected distance of iterates at successive iterations $\\mathbb{E}[\\left\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\right\\|^{2}]$ , which diminishes asymptotically. In our algorithm, we exploit the randomized smoothing with perturbed iterates $\\mathbf{x}_{t}\\pm\\delta\\mathbf{w}_{t,j}$ to approximate the gradient of the smoothed surrogate function $\\Phi_{\\delta}(\\mathbf{x}_{t})$ . If we apply the variance reduction to the inner function estimator, the meansquare error $\\mathbb{E}\\big[\\big\\|\\mathbf{v}_{t}-\\nabla\\Phi_{\\delta}(\\mathbf{x}_{t})\\big)\\big\\|^{2}\\big]$ is bounded by the expected distance of the perturbed iterates at successive iterations $\\mathbb{E}\\big[\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\pm\\delta\\big(\\mathbf{w}_{t,j}-\\mathbf{w}_{t-1,j}\\big)\\big\\|^{2}\\big]$ , which does not vanish asymptotically. ", "page_idx": 6}, {"type": "text", "text": "5 Extensions to Convex Nonsmooth SCO ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we extend the result in Section 4.1 to study the convex nonsmooth SCO problem.   \nFirstly, we introduce an additional assumption as follows. ", "page_idx": 6}, {"type": "text", "text": "Assumption 5.1. We suppose the function $f(\\mathbf{x})$ is convex and non-decreasing, $G(\\mathbf{x};\\zeta)$ is convex for any given $\\zeta$ , and the solution set $\\begin{array}{r}{\\mathcal{X}^{*}=\\arg\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{d}}\\Phi(\\mathbf{x})}\\end{array}$ is non-empty. ", "page_idx": 6}, {"type": "text", "text": "From this assumption and Section 3.2.4 by Boyd and Vandenberghe [38], we can deduce that $\\Phi(\\mathbf{x})$ is a convex function. We will show that stochastic algorithms obtain an improved convergence rate for the nonsmooth SCO problem with Assumption 5.1. To obtain a $(\\delta,\\epsilon)$ -Goldstein stationary point of the problem, we propose a two-phase gradient-free stochastic method called warm-started GFCOM (WS-GFCOM) in Algorithm 3. The intuition is that we use the GFCOM method to get a sufficiently small sub-optimality in the first phase, and then we apply the proposed methods in Section 4.1 to find the stationary point in the second phase. We remark that using the GFCOM method for the first phase is justified by the observation that it can achieve optimal convergence rate in terms of the sub-optimality of function value gap given the access to the exact function value $g\\mathbf{(x)}$ [39]. ", "page_idx": 6}, {"type": "text", "text": "5.1 Convergence Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this subsection, we consider the complexity analysis of the proposed WS-GFCOM method. Let $\\begin{array}{r}{\\hat{R}\\triangleq\\operatorname*{min}_{\\mathbf x\\in\\mathcal{X}^{*}}\\|x-x_{0}\\|}\\end{array}$ . We characterize the convergence rate of the WS-GFCOM method for the convex nonsmooth SCO problem at the first phase with the following theorem. ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.2. Under Assumption 3.1, 3.3 and 5.1, running the WS-GFCOM algorithm (Algorithm 3) with parameters $\\eta_{0}=\\Theta\\big(\\hat{R_{}}/(G_{f}G_{g}\\sqrt{d T_{0}})\\big)$ , $T_{0}\\,=\\,\\Theta\\big(d\\bar{G}_{f}^{2}G_{g}^{2}\\hat{R}^{2}\\rho^{-2}\\big)$ , $b_{g,0}=\\Theta\\big(G_{f}^{2}\\sigma_{0}^{2}\\rho^{-2}\\big)$ and $\\delta=\\Theta\\big(\\rho G_{f}^{-1}G_{g}^{-1}\\big)$ , then the output $\\mathbf{x}_{1}$ satisfies $\\mathbb{E}[\\Phi(\\mathbf{x}_{1})-\\Phi^{*}]\\leq\\rho$ . In addition, the total zerothorder stochastic oracle complexity is at most $\\mathcal{O}\\big(d G_{f}^{4}G_{g}^{2}\\sigma_{0}^{2}\\hat{R}^{2}\\rho^{-4}\\big)$ . ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.2 implies that the initial suboptimality at the beginning of the second phase is bounded by $\\rho$ . Consequently, the total complexity of $\\scriptstyle\\mathrm{WS-GFCOM}^{2}$ (Algorithm 3 with Option I) is bounded by $\\mathcal{O}\\big(d G_{f}^{4}G_{g}^{2}\\sigma_{0}^{2}\\hat{R}^{2}\\rho^{-4}+d^{3.5}G_{f}^{5}G_{g}^{3}\\sigma_{0}^{2}\\rho\\delta^{-3}\\epsilon^{-6}+d^{3.5}G_{f}^{6}G_{g}^{4}\\sigma_{0}^{2}\\delta^{-2}\\epsilon^{-6}\\big)$ . An appropriate choice of $\\rho$ leads to the following oracle complexity of Algorithm 3 with Option I. ", "page_idx": 7}, {"type": "text", "text": "Corollary 5.3. Under Assumption 3.1, 3.3 and 5.1, the WS-GFCOM2 algorithm (Algorithm $^3$ with Option I) requires at most $\\mathcal{O}\\big(d^{3}G_{f}^{4.8}G_{g}^{2.8}\\sigma_{0}^{2}\\hat{R}^{0.4}\\delta^{-2.4}\\epsilon^{-4.8}+d^{3.5}G_{f}^{6}G_{g}^{4}\\sigma_{0}^{2}\\delta^{-2}\\epsilon^{-6}\\big)$ stochastic zeroth-order function query calls to obtain a $(\\delta,\\epsilon)$ -Goldstein stationary point of $\\Phi$ . ", "page_idx": 7}, {"type": "text", "text": "With a similar deduction, we can show that using the $\\mathrm{GFCOM^{+}}$ for the second phase can obtain an improved complexity bound. The following theorem shows the oracle complexity of Algorithm 3 with Option II. ", "page_idx": 7}, {"type": "text", "text": "Corollary 5.4. Under Assumption 3.1, 3.3 and 5.1, the WS-GFCOM+ algorithm (Algorithm $^3$ with Option II) requires at most $\\mathcal{O}\\big(d^{3}G_{f}^{4}G_{g}^{2}\\sigma_{0}^{2}\\hat{R}^{0.4}\\delta^{-2.4}\\epsilon^{-4}+d^{3.5}G_{f}^{5}G_{g}^{3}\\sigma_{0}^{2}\\delta^{-2}\\epsilon^{-5}\\big)$ stochastic zeroth-order function query calls to obtain $a$ $(\\delta,\\epsilon)$ -Goldstein stationary point of $\\Phi$ . ", "page_idx": 7}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compare the proposed methods GFCOM and $\\mathrm{GFCOM^{+}}$ with a Kiefer-Wolfowitz style zeroth-order baseline method [2, 40]. In particular, the baseline gradient estimator is defined as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{v}_{t}=\\frac{1}{b_{f}}\\sum_{j\\in[b_{f}]}\\frac{d}{2\\delta}\\left(F(\\mathbf{y}_{t,j};\\pmb{\\xi}_{t,j})-F(\\mathbf{z}_{t,j};\\pmb{\\xi}_{t,j}^{\\prime})\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where ${\\bf y}_{t,j}$ and $\\mathbf{z}_{t,j}$ are function estimators defined in Eq. (3). $\\boldsymbol{\\xi}_{t,j}$ and $\\xi_{t,j}^{\\prime}$ are independent random variables. We test all the methods on the nonconvex penalized risk-averse portfolio management problem and a reinforcement learning (RL) problem. We set $\\delta=0.1$ for the GFCOM and GFCOM+ methods. ", "page_idx": 7}, {"type": "text", "text": "6.1 Nonconvex Penalized Portfolio Management ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We consider the portfolio management problem with capped- $\\ell_{1}$ regularizer [41]. Let x denote the investment quantity corresponding to $N$ assets and $\\mathbf{r}_{t}\\,\\in\\,\\mathbb{R}^{N}$ denote the returns of $N$ assets at timestamp $t$ . We can formulate the portfolio management problem as the following nonsmooth compositional optimization problem ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{N}}-\\frac{1}{T}\\sum_{t=1}^{T}\\langle\\mathbf{r}_{t},\\mathbf{x}\\rangle+\\frac{1}{T}\\sum_{i=1}^{T}\\left(\\langle\\mathbf{r}_{t},\\mathbf{x}\\rangle-\\frac{1}{T}\\sum_{s=1}^{T}\\langle\\mathbf{r}_{s},\\mathbf{x}\\rangle\\right)^{2}+\\beta(\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where \u03b2(x) = \u03bb iN= $\\begin{array}{r}{\\beta(\\mathbf{x})=\\lambda\\sum_{i=1}^{N}\\operatorname*{min}\\{|x_{i}|,\\alpha\\}}\\end{array}$ and $\\lambda,\\alpha>0$ are tunable hyperparameters. Specifically, the inner function $G(\\mathbf{x};\\boldsymbol{\\xi})$ and outer function $F(\\mathbf{w};\\zeta)$ can be formulated as ", "page_idx": 7}, {"type": "equation", "text": "$$\nG(\\mathbf{x};\\pmb{\\xi})=\\left[x_{1},\\ldots,x_{N},\\langle r_{\\xi},\\mathbf{x}\\rangle\\right]^{\\intercal}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and ", "page_idx": 7}, {"type": "equation", "text": "$$\nF(\\mathbf{w};\\zeta)=-\\langle\\mathbf{r}_{\\zeta},w_{[N]}\\rangle+(\\langle\\mathbf{r}_{\\zeta},\\mathbf{w}_{[N]}\\rangle-\\mathbf{w}_{N+1})^{2}+\\beta(\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Both random variables $\\xi$ and $\\zeta$ are uniformly sampled from $\\{1,\\ldots,T\\}$ . We choose $\\lambda=10^{-5}$ and $\\alpha=2$ in our experiments. The goal of the Problem (6) is to maximize the return while controlling the variance of the portfolio. ", "page_idx": 7}, {"type": "text", "text": "We compare all the methods on 6 different portfolio datasets formed on Size and Operating Proftiability2. For all algorithms, we tune the stepsize among $\\{1\\times10^{-5},3\\times10^{-5},\\ldots,1\\times\\Bar{1}0^{-3},\\stackrel{\\circ}{3}\\times10^{-3}\\}$ . ", "page_idx": 7}, {"type": "image", "img_path": "UVAq3uJ0gc/tmp/c96db53161b951fdc71c58825f7af9aa6e9103b844f6c7382229ec0f58d95f0b.jpg", "img_caption": ["Figure 1: We present the loss vs. complexity on several portfolio management datasets. The plot of GFCOM and the Kiefer-Wolfowitz method are overlapped as their performance are close to each other. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "UVAq3uJ0gc/tmp/fb0fff715b262ae469255ba383110f76c56ffcd307a04ac9db546f7b6300e612.jpg", "img_caption": ["Figure 2: For the RL task, we present the loss vs. complexity on datasets with states of different sizes. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "We choose the mini-batch size $b_{f}\\,=\\,b_{g}\\,=\\,1000$ . In addition, we set $b_{f}^{\\prime}\\,=\\,100$ , $b_{g}^{\\prime}\\,=\\,1000$ and $m=b_{f}/b_{f}^{\\prime}=10$ for the $\\mathrm{GFCOM^{+}}$ algorithm. Figure 1 shows that the $\\mathrm{GFCOM^{+}}$ algorithm converges much faster than the GFCOM and the baseline method across all datasets. ", "page_idx": 8}, {"type": "text", "text": "6.2 Application to Reinforcement Learning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We demonstrate an experiment on RL and verify the effectiveness of the proposed methods on value function evaluation. Let $V^{\\pi}(s)$ be the value function of a state $s$ under a policy $\\pi$ for all state $s\\in S$ where $\\vert{\\cal S}\\vert=n$ . Let $r_{s^{\\prime},s}$ be the reward transition from $s^{\\prime}$ to $s$ , and $\\gamma>0$ is a discounting factor. Furthermore, we assume that the value of each state can be parameterized as a linear map of some feature map $\\psi_{s}\\in\\mathbb{R}^{d}$ of the state $s$ such that $V^{\\pi}(s)=\\langle\\psi_{s},\\bar{\\bf w}\\rangle$ . Then we formulate the RL problem as a Bellman residual minimization problem ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{w}\\in\\mathbb{R}^{d}}\\sum_{s=1}^{n}h\\left(\\langle\\psi_{s},\\mathbf{w}\\rangle-\\sum_{s^{\\prime}}P_{s s^{\\prime}}(r_{s,s^{\\prime}}+\\gamma\\langle\\psi_{s^{\\prime}},\\mathbf{w}\\rangle)\\right),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $P_{s s^{\\prime}}$ is the probability transition matrix and $h(x)=1\\!-\\!\\exp(-|x|/\\sigma)$ is a nonconvex nonsmooth loss which is more robust to adversarial outliers than the squared loss [42, 43]. Specifically, the inner function $G(\\mathbf{x};\\pmb{\\xi})$ and outer function $F(\\mathbf{w};\\zeta)$ can be formulated as ", "page_idx": 8}, {"type": "equation", "text": "$$\nG(\\mathbf{w};{\\pmb\\xi})=[\\langle\\psi_{1},{\\mathbf{w}}\\rangle,r_{1,{\\pmb\\xi}_{1}}+\\gamma\\langle\\psi_{{\\pmb\\xi}_{1}},{\\mathbf{w}}\\rangle,\\dots,\\langle\\psi_{n},{\\mathbf{w}}\\rangle,r_{n,{\\pmb\\xi}_{n}}+\\gamma\\langle\\psi_{{\\pmb\\xi}_{n}},{\\mathbf{w}}\\rangle]^{\\intercal}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "equation", "text": "$$\nF({\\bf z};\\zeta)=h(z_{2\\zeta}-z_{2\\zeta+1}).\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "In the above formulation, each $\\xi_{i}$ is uniformly sampled from $\\{P_{i1},\\ldots,P_{i n}\\}$ and $\\zeta$ is uniformly sampled from $\\{1,\\ldots,T\\}$ . We follow a similar experiment setup by Yuan et al. [22]. Specifically, we generate a Markov decision process with different numbers of states $n\\in\\{400,600,800\\}$ and 10 actions at each state. The transition probability matrix is generated from the uniform distribution from $[0,1]$ . In addition, the rewards are sampled uniformly from $[0,1]$ . In terms of hyperparameter setting, we choose $b_{f}=b_{g}=100$ for all algorithms. In addition, we set $b_{f}^{\\prime}=10$ , $b_{g}^{\\prime}=100$ and $m=b_{f}/b_{f}^{\\prime}=10$ for the $\\mathrm{GFCOM^{+}}$ algorithm. For other hyperparameters, we use the same setting for the portfolio management problem. The experimental results in Figure 2 show that the $\\mathrm{GFCOM^{+}}$ significantly outperforms other methods. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we propose novel zeroth-order algorithms for nonconvex nonsmooth stochastic compositional optimization. We present the non-asymptotic convergence rate of the proposed algorithms for obtaining a $(\\delta,\\epsilon)$ -Goldstein point of the problem. Furthermore, we extend our methods with a warmstart phase to solve the convex nonsmooth SCO problem with improved convergence guarantees. We conduct numerical experiments on portfolio management and reinforcement learning problems to demonstrate the effectiveness of the proposed algorithms. ", "page_idx": 9}, {"type": "text", "text": "In future work, it is interesting to study the lower bound of the zeroth-order algorithms on nonconvex nonsmooth SCO. It is also interesting to investigate whether the complexity bound of zeroth-order algorithms can be further improved. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research/project is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-PhD-2023-08-043T-J). This research is part of the programme DesCartes and is supported by the National Research Foundation, Prime Minister\u2019s Office, Singapore under its Campus for Research Excellence and Technological Enterprise (CREATE) programme. Luo Luo is supported by National Natural Science Foundation of China (No. 62206058), Shanghai Sailing Program (22YF1402900), Shanghai Basic Research Program (23JC1401000), and the Major Key Project of PCL under Grant PCL2024A06. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Darinka Dentcheva, Spiridon Penev, and Andrzej Ruszczy\u00b4nski. Statistical estimation of composite risk functionals and risk optimization problems. Annals of the Institute of Statistical Mathematics, 69:737\u2013760, 2017.   \n[2] Mengdi Wang, Ethan X Fang, and Han Liu. Stochastic compositional gradient descent: algorithms for minimizing compositions of expected-value functions. Mathematical Programming, 161:419\u2013449, 2017.   \n[3] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.   \n[4] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pages 1126\u20131135. PMLR, 2017.   \n[5] Mengdi Wang, Ji Liu, and Ethan X Fang. Accelerating stochastic composition optimization. Journal of Machine Learning Research, 18(105):1\u201323, 2017.   \n[6] Saeed Ghadimi, Andrzej Ruszczynski, and Mengdi Wang. A single timescale stochastic approximation method for nested stochastic optimization. SIAM Journal on Optimization, 30 (1):960\u2013979, 2020. [7] Wenqing Hu, Chris Junchi Li, Xiangru Lian, Ji Liu, and Huizhuo Yuan. Efficient smooth non-convex stochastic compositional optimization via stochastic recursive gradient descent. Advances in Neural Information Processing Systems, 32, 2019. [8] Tianyi Chen, Yuejiao Sun, and Wotao Yin. Solving stochastic compositional optimization is nearly as easy as solving stochastic optimization. IEEE Transactions on Signal Processing, 69: 4937\u20134948, 2021. [9] Andrzej Ruszczynski. A stochastic subgradient method for nonsmooth nonconvex multilevel composition optimization. SIAM Journal on Control and Optimization, 59(3):2301\u20132320, 2021.   \n[10] Yin Liu and Sam Davanloo Tajbakhsh. Stochastic composition optimization of functions without lipschitz continuous gradient. Journal of Optimization Theory and Applications, 198 (1):239\u2013289, 2023.   \n[11] Quanqi Hu, Dixian Zhu, and Tianbao Yang. Non-smooth weakly-convex finite-sum coupled compositional optimization. Advances in Neural Information Processing Systems, 36, 2024.   \n[12] Frank H. Clarke. Optimization and nonsmooth analysis. SIAM, 1990.   \n[13] Jingzhao Zhang, Hongzhou Lin, Stefanie Jegelka, Ali Jadbabaie, and Suvrit Sra. Complexity of finding stationary points of nonsmooth nonconvex functions. In Proc. ICML, pages 11173\u2013 11182, 2020.   \n[14] Guy Kornowski and Ohad Shamir. Oracle complexity in nonsmooth nonconvex optimization. Journal of Machine Learning Research, 23(314):1\u201344, 2022.   \n[15] AA Goldstein. Optimization of lipschitz continuous functions. Mathematical Programming, 13: 14\u201322, 1977.   \n[16] Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Tak\u00e1\u02c7c. Sarah: A novel method for machine learning problems using stochastic recursive gradient. In International conference on machine learning, pages 2613\u20132621. PMLR, 2017.   \n[17] Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal nonconvex optimization via stochastic path-integrated differential estimator. Advances in neural information processing systems, 31, 2018.   \n[18] Zhe Wang, Kaiyi Ji, Yi Zhou, Yingbin Liang, and Vahid Tarokh. Spiderboost and momentum: Faster variance reduction algorithms. Advances in Neural Information Processing Systems, 32, 2019.   \n[19] Zhize Li, Hongyan Bao, Xiangliang Zhang, and Peter Richt\u00e1rik. Page: A simple and optimal probabilistic gradient estimator for nonconvex optimization. In International conference on machine learning, pages 6286\u20136295. PMLR, 2021.   \n[20] Tianyi Lin, Chengyou Fan, Mengdi Wang, and Michael I Jordan. Improved sample complexity for stochastic compositional variance reduced gradient. In 2020 American Control Conference (ACC), pages 126\u2013131. IEEE, 2020.   \n[21] Yibo Xu and Yangyang Xu. Katyusha acceleration for convex finite-sum compositional optimization. INFORMS Journal on Optimization, 3(4):418\u2013443, 2021.   \n[22] Huizhuo Yuan, Xiangru Lian, and Ji Liu. Stochastic recursive variance reduction for efficient smooth non-convex compositional optimization. arXiv preprint arXiv:1912.13515, 2019.   \n[23] Maria-Luiza Vladarean, Nikita Doikov, Martin Jaggi, and Nicolas Flammarion. Linearization algorithms for fully composite optimization. In The Thirty Sixth Annual Conference on Learning Theory, pages 3669\u20133695. PMLR, 2023.   \n[24] Dionysios S Kalogerias and Warren B Powell. Zeroth-order stochastic compositional algorithms for risk-aware learning. SIAM Journal on Optimization, 32(2):386\u2013416, 2022.   \n[25] Marko M Makela and Pekka Neittaanmaki. Nonsmooth optimization: analysis and algorithms with applications to optimal control. World Scientific, 1992.   \n[26] Damek Davis, Dmitriy Drusvyatskiy, Yin Tat Lee, Swati Padmanabhan, and Guanghao Ye. A gradient sampling method with complexity guarantees for lipschitz functions in high and low dimensions. Advances in neural information processing systems, 35:6692\u20136703, 2022.   \n[27] Lai Tian, Kaiwen Zhou, and Anthony Man-Cho So. On the finite-time complexity and practical computation of approximate stationarity concepts of lipschitz functions. In International Conference on Machine Learning, pages 21360\u201321379. PMLR, 2022.   \n[28] Ashok Cutkosky, Harsh Mehta, and Francesco Orabona. Optimal stochastic non-smooth nonconvex optimization through online-to-non-convex conversion. In International Conference on Machine Learning, pages 6643\u20136670. PMLR, 2023.   \n[29] Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions. Foundations of Computational Mathematics, 17(2):527\u2013566, 2017.   \n[30] Tianyi Lin, Zeyu Zheng, and Michael Jordan. Gradient-free methods for deterministic and stochastic nonsmooth nonconvex optimization. Advances in Neural Information Processing Systems, 35:26160\u201326175, 2022.   \n[31] Lesi Chen, Jing Xu, and Luo Luo. Faster gradient-free algorithms for nonsmooth nonconvex stochastic optimization. In International Conference on Machine Learning, pages 5219\u20135233. PMLR, 2023.   \n[32] Chengchang Liu, Chaowen Guan, Jianhao He, and John Lui. Quantum algorithms for nonsmooth non-convex optimization. arXiv preprint arXiv:2410.16189, 2024.   \n[33] Guy Kornowski and Ohad Shamir. An algorithm with optimal dimension-dependence for zero-order nonsmooth nonconvex stochastic optimization. arXiv preprint arXiv:2307.04504, 2023.   \n[34] Zhuanghua Liu, Cheng Chen, Luo Luo, and Bryan Kian Hsiang Low. Zeroth-order methods for constrained nonconvex nonsmooth stochastic optimization. In Forty-first International Conference on Machine Learning, 2024.   \n[35] Benjamin Grimmer and Zhichao Jia. Goldstein stationarity in lipschitz constrained optimization. arXiv preprint arXiv:2310.03690, page 9, 2023.   \n[36] John C Duchi, Peter L Bartlett, and Martin J Wainwright. Randomized smoothing for stochastic optimization. SIAM Journal on Optimization, 22(2):674\u2013701, 2012.   \n[37] John C Duchi, Michael I Jordan, Martin J Wainwright, and Andre Wibisono. Optimal rates for zero-order convex optimization: The power of two function evaluations. IEEE Transactions on Information Theory, 61(5):2788\u20132806, 2015.   \n[38] Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.   \n[39] Ohad Shamir. An optimal algorithm for bandit and zero-order convex optimization with two-point feedback. Journal of Machine Learning Research, 18(52):1\u201311, 2017.   \n[40] Jack Kiefer and Jacob Wolfowitz. Stochastic estimation of the maximum of a regression function. The Annals of Mathematical Statistics, pages 462\u2013466, 1952.   \n[41] Tong Zhang. Analysis of multi-stage convex relaxation for sparse regularization. Journal of Machine Learning Research, 11(3), 2010.   \n[42] Chao Qu, Yan Li, and Huan Xu. Non-convex conditional gradient sliding. In international conference on machine learning, pages 4208\u20134217. PMLR, 2018.   \n[43] Zebang Shen, Cong Fang, Peilin Zhao, Junzhou Huang, and Hui Qian. Complexities in projection-free stochastic non-convex minimization. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 2868\u20132876. PMLR, 2019. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "The appendix is organized as follows. Section A introduces supporting lemmas that are essential for the analysis of the proposed gradient-free SCO methods. Section B proves the convergence rate of the GFCOM method introduced in Section 4. Section C proves the convergence rate of the $\\mathrm{GFCOM^{+}}$ method which enjoys a better function oracle call complexity. Section $\\mathrm{D}$ provides the convergence analysis of the WS-GFCOM2 and WS-GFCOM+ proposed in Section 5. ", "page_idx": 12}, {"type": "text", "text": "A Supporting Lemmas ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Throughout the work, we define the variable $\\begin{array}{r}{g_{t}(\\mathbf{x})=\\frac{1}{b_{g}}\\sum_{i\\in[b_{g}]}G(\\mathbf{x};\\pmb{\\zeta}_{t,i})}\\end{array}$ for the GFCOM algorithm, and we denote ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{g_{t}(\\mathbf{x})=\\left\\{\\begin{array}{l l}{\\frac{1}{b_{g}}\\sum_{i\\in[b_{g}]}G(\\mathbf{x};\\zeta_{t,i}),\\;\\;\\;t\\bmod m=0}\\\\ {\\frac{1}{b_{g}^{\\prime}}\\sum_{i\\in[b_{g}^{\\prime}]}G(\\mathbf{x};\\zeta_{t,i}),\\;\\;\\;\\mathrm{Otherwise}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "for the $\\mathrm{GFCOM^{+}}$ method. Now we introduce an important lemma which is useful for the analysis of both GFCOM and $\\mathrm{GFCOM^{+}}$ algorithms. ", "page_idx": 12}, {"type": "text", "text": "Lemma A.1. Under Assumption 3.1 and 3.3, for both Algorithms $^{\\,l}$ and 2 it holds that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}[(f\\circ g)_{\\delta}(\\mathbf{x}_{t+1})-(f\\circ g)_{\\delta}(\\mathbf{x}_{t})]}\\\\ &{\\leq-\\frac{\\eta}{2}\\mathbb{E}\\left[\\|\\nabla(f\\circ g)_{\\delta}(\\mathbf{x}_{t})\\|^{2}\\right]-\\left(\\frac{\\eta}{2}\\!-\\!\\frac{c\\eta^{2}G_{f}G_{g}\\sqrt{d}}{2\\delta}\\right)\\mathbb{E}\\left[\\|\\mathbf{v}_{t}\\|^{2}\\right]\\!+\\!\\frac{\\eta}{2}\\mathbb{E}\\left[\\|\\mathbf{v}_{t}-\\nabla(f\\circ g)_{\\delta}(\\mathbf{x}_{t})\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof. From the smoothness of $\\Phi_{\\delta}=(f\\circ g)_{\\delta}$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad(f\\circ g)_{\\delta}(\\mathbf{x}_{t+1})-(f\\circ g)_{\\delta}(\\mathbf{x}_{t})}\\\\ &{\\leq\\langle\\nabla(f\\circ g)_{\\delta}(\\mathbf{x}_{t}),\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\rangle+\\frac{c G_{f}G_{g}\\sqrt{d}}{2\\delta}\\left\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\right\\|^{2}}\\\\ &{=-\\eta[\\langle\\nabla(f\\circ g)_{\\delta}(\\mathbf{x}_{t}),\\mathbf{v}_{t}\\rangle]+\\frac{c\\eta^{2}G_{f}G_{g}\\sqrt{d}}{2\\delta}\\left\\|\\mathbf{v}_{t}\\right\\|^{2}}\\\\ &{=-\\frac{\\eta}{2}\\left\\|\\nabla(f\\circ g)_{\\delta}(\\mathbf{x}_{t})\\right\\|^{2}-\\left(\\frac{\\eta}{2}-\\frac{c\\eta^{2}G_{f}G_{g}\\sqrt{d}}{2\\delta}\\right)\\left\\|\\mathbf{v}_{t}\\right\\|^{2}+\\frac{\\eta}{2}\\left\\|\\mathbf{v}_{t}-\\nabla(f\\circ g)_{\\delta}(\\mathbf{x}_{t})\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Taking expectations on both sides of the inequality, we get the desired result. ", "page_idx": 12}, {"type": "text", "text": "B Convergence Analysis of Algorithm 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Before giving the analysis of the convergence rate of Algorithm 1, we first present the bound of the mean-square error term $\\mathbb{E}\\big[\\|\\mathbf{v}_{t}-\\nabla\\Phi_{\\delta}\\bar{(\\mathbf{x}}_{t})\\|^{2}\\big]$ . ", "page_idx": 12}, {"type": "text", "text": "Lemma B.1. Under Assumption 3.1 and 3.3, for Algorithm $^{\\,I}$ it holds that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert\\mathbf{v}_{t}-\\nabla\\Phi_{\\delta}(\\mathbf{x}_{t})\\Vert^{2}\\right]\\leq\\frac{2d^{2}G_{f}^{2}\\sigma_{0}^{2}}{\\delta^{2}b_{g}}+\\frac{32\\sqrt{2\\pi}d G_{f}^{2}G_{g}^{2}}{b_{f}}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof. Recall that $\\begin{array}{r}{\\mathbf{v}_{t}=\\frac{1}{b_{f}}\\sum_{j\\in[b_{f}]}\\frac{d}{2\\delta}(F(\\mathbf{y}_{t,j};\\pmb{\\xi}_{t,j})-F(\\mathbf{z}_{t,j};\\pmb{\\xi}_{t,j}))}\\end{array}$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\left\\|\\mathbf{v}_{t}-\\mathbf{v}\\boldsymbol{\\Psi}_{\\delta}(\\boldsymbol{x}_{t})\\right\\|\\right]}\\\\ &{\\le2\\mathbb{E}\\left[\\left\\|\\mathbf{v}_{t}-\\frac{1}{b_{f}}\\sum_{j\\in[b_{f}]}\\frac{d}{2\\delta}(F(g(\\mathbf{x}_{t}+\\delta\\mathbf{w}_{t,j});\\pmb{\\xi}_{t,j})-F(g(\\mathbf{x}_{t}-\\delta\\mathbf{w}_{t,j});\\pmb{\\xi}_{t,j}))\\right\\|^{2}\\right]}\\\\ &{\\quad+\\,2\\mathbb{E}\\left[\\left\\|\\frac{1}{b_{f}}\\sum_{j\\in[b_{f}]}\\frac{d}{2\\delta}(F(g(\\mathbf{x}_{t}+\\delta\\mathbf{w}_{t,j});\\pmb{\\xi}_{t,j})-F(g(\\mathbf{x}_{t}-\\delta\\mathbf{w}_{t,j});\\pmb{\\xi}_{t,j}))-\\nabla(f\\circ g)_{\\delta}(x_{t})\\right\\|^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\le2\\left(\\displaystyle\\frac{d^{2}}{2\\delta^{2}b_{f}}\\sum_{j\\in[b_{f}]}\\mathbb{E}\\left[\\|F(g(\\mathbf{x}_{t}+\\delta\\mathbf{w}_{t,j});\\boldsymbol{\\xi}_{t,j})-F(g_{t}(\\mathbf{x}_{t}+\\delta\\mathbf{w}_{t,j});\\boldsymbol{\\xi}_{t,j})\\|^{2}\\right]\\right.}\\\\ &{\\quad\\left.+\\displaystyle\\frac{d^{2}}{2\\delta^{2}b_{f}}\\sum_{j\\in[b_{f}]}\\mathbb{E}\\left[\\|F(g(\\mathbf{x}_{t}-\\delta\\mathbf{w}_{t,j});\\boldsymbol{\\xi}_{t,j})-F(g_{t}(\\mathbf{x}_{t}-\\delta\\mathbf{w}_{t,j});\\boldsymbol{\\xi}_{t,j})\\|^{2}\\right]\\right)}\\\\ &{\\quad+\\displaystyle\\frac{32\\sqrt{2\\pi}d G_{f}^{2}G_{g}^{2}}{b_{f}}}\\\\ &{\\le\\displaystyle\\frac{2d^{2}G_{f}^{2}\\sigma_{0}^{2}}{\\delta^{2}b_{g}}+\\frac{32\\sqrt{2\\pi}d G_{f}^{2}G_{g}^{2}}{b_{f}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The first inequality is due to $\\left\\|\\mathbf{a}+\\mathbf{b}\\right\\|^{2}\\leq2\\left\\|\\mathbf{a}\\right\\|^{2}+2\\left\\|\\mathbf{b}\\right\\|^{2}$ for any a, $\\mathbf{b}\\in\\mathbb{R}^{d}$ . The second inequality is due to Lemma 3.8. The last inequality follows from the $G_{f}$ -Lipchitzness of $f(\\cdot)$ and Assumption 3.3. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "We present the formal proof of Theorem 4.1 and Corollary 4.2 below. ", "page_idx": 13}, {"type": "text", "text": "B.1 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof. If we take \u03b7 =cGf G\u03b4g\u221ad and rearrange the formula in Lemma A.1, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{\\eta}{2}\\mathbb{E}[\\|\\nabla(f\\circ g)_{\\delta}(\\mathbf{x}_{t})\\|^{2}]\\leq\\mathbb{E}[(f\\circ g)_{\\delta}(\\mathbf{x}_{t})-(f\\circ g)_{\\delta}(\\mathbf{x}_{t+1})]+\\frac{\\eta}{2}\\mathbb{E}[\\|\\mathbf{v}_{t}-\\nabla(f\\circ g)_{\\delta}(\\mathbf{x}_{t})\\|^{2}].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Sum the above inequality from $t=0$ to $T-1$ and divide both sides by $\\textstyle{\\frac{\\eta T}{2}}$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\|\\nabla(f\\circ g)_{\\delta}(\\mathbf{x}_{t})\\|^{2}\\right]\\leq\\frac{2c G_{f}G_{g}\\sqrt{d}\\mathbb{E}\\left[(f\\circ g)_{\\delta}(\\mathbf{x}_{0})-(f\\circ g)_{\\delta}(\\mathbf{x}_{T})\\right]}{\\delta T}}\\\\ {+\\,\\frac{32\\sqrt{2\\pi}d G_{f}^{2}G_{g}^{2}}{b_{f}}+\\frac{2d^{2}G_{f}^{2}\\sigma_{0}^{2}}{\\delta^{2}b_{g}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In addition, by Lemma 3.7, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[(f\\circ g)_{\\delta}(\\mathbf{x}_{0})-(f\\circ g)_{\\delta}(\\mathbf{x}_{T})\\right]\\leq\\mathbb{E}[(f\\circ g)(\\mathbf{x}_{0})-(f\\circ g)(\\mathbf{x}_{T})]+2G_{f}G_{g}\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Combining the last two inequalities, we get the desired result. ", "page_idx": 13}, {"type": "text", "text": "B.2 Proof of Corollary 4.2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof. The total zeroth-order oracle calls can be bounded by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{O}(T b_{f}b_{g})}\\\\ &{=\\mathcal{O}\\left(\\left(\\frac{G_{f}G_{g}\\sqrt{d}R}{\\delta\\epsilon^{2}}+\\frac{G_{f}^{2}G_{g}^{2}\\sqrt{d}}{\\epsilon^{2}}\\right)\\cdot\\frac{d G_{f}^{2}G_{g}^{2}}{\\epsilon^{2}}\\cdot\\frac{d^{2}G_{f}^{2}\\sigma_{0}^{2}}{\\delta^{2}\\epsilon^{2}}\\right)}\\\\ &{=\\mathcal{O}\\left(\\frac{d^{3.5}G_{f}^{5}G_{g}^{3}\\sigma_{0}^{2}R}{\\delta^{3}\\epsilon^{6}}+\\frac{d^{3.5}G_{f}^{6}G_{g}^{4}\\sigma_{0}^{2}}{\\delta^{2}\\epsilon^{6}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "C Convergence Analysis of Algorithm 2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we consider the formal proof of the convergence rate of the $\\mathrm{GFCOM^{+}}$ method. First, we introduce the following lemma to bound the mean-square error between the recursive gradient estimator and the gradient of the surrogate function. ", "page_idx": 13}, {"type": "text", "text": "Lemma C.1. Let $n_{t}=\\lfloor t/m\\rfloor\\,r$ m. Under Assumption 3.1 and 3.3, for Algorithm 2 it holds that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\left\\Vert\\mathbf{v}_{t}-\\nabla(f\\circ g)_{\\delta}(\\mathbf{x}_{t})\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\!\\frac{10d^{2}G_{f}^{2}G_{g}^{2}\\eta^{2}}{\\delta^{2}b_{f}^{\\prime}}\\displaystyle\\sum_{i=n_{t}}^{t}\\mathbb{E}\\left[\\left\\Vert\\mathbf{v}_{i}\\right\\Vert^{2}\\right]+\\frac{10m d^{2}G_{f}^{2}\\sigma_{0}^{2}}{\\delta^{2}b_{f}^{\\prime}b_{g}^{\\prime}}+\\frac{32\\sqrt{2\\pi}d G_{f}^{2}G_{g}^{2}}{b_{f}}}\\\\ &{\\quad+\\,\\frac{2c d^{2}G_{f}^{2}\\sigma_{0}^{2}}{\\delta^{2}b_{g}}+\\frac{2c d^{2}G_{f}^{2}\\sigma_{0}^{2}}{\\delta^{2}b_{g}^{\\prime}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. By $\\left\\|\\mathbf{a}+\\mathbf{b}\\right\\|^{2}\\leq2\\left\\|\\mathbf{a}\\right\\|^{2}+2\\left\\|\\mathbf{b}\\right\\|^{2}$ , we can infer that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\left\\Vert\\mathbf{v}_{t}-\\nabla(f\\circ g)_{\\delta}(\\mathbf{x}_{t})\\right\\Vert^{2}\\right]}\\\\ &{\\leq2\\mathbb{E}\\left[\\left\\Vert\\mathbf{v}_{t}-\\nabla(f\\circ g_{t})_{\\delta}(\\mathbf{x}_{t})\\right\\Vert^{2}\\right]+2\\mathbb{E}\\left[\\left\\Vert\\nabla(f\\circ g_{t})_{\\delta}(\\mathbf{x}_{t})-\\nabla(f\\circ g)_{\\delta}(\\mathbf{x}_{t})\\right\\Vert^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To bound the first term of R.H.S., we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\big\\|\\mathbf{y}_{t}\\!-\\!\\nabla(f\\circ g_{t})(\\mathbf{x}_{t})\\big\\|^{2}\\Big\\}}\\\\ &{\\!=\\!\\mathbb{E}\\Bigg[\\Bigg\\|\\frac{1}{b_{f}}\\!\\Bigg\\|\\frac{\\!\\big(d\\big(F(\\mathbf{y}_{t},\\xi_{t}),\\xi_{t})\\big)\\!-\\!\\big(F(\\mathbf{z}_{t},\\xi_{t}),\\xi_{t}\\big))\\mathbf{w}_{t,j}\\!-\\!\\frac{d}{2\\delta}\\big(F(\\mathbf{y}_{t-1,j};\\xi_{t})\\big)\\!-\\!F(\\mathbf{z}_{t-1,j};\\xi_{t,j})\\big)\\mathbf{w}_{t,j}\\Bigg\\|}{\\!\\Bigg.}}\\\\ &{\\quad+\\!\\nu_{t-1}\\!-\\!\\nabla(f\\circ g_{t})\\!\\big\\|^{2}\\Bigg]}\\\\ &{\\!=\\!\\mathbb{E}\\Bigg[\\Bigg\\|\\frac{1}{b_{f}}\\!\\Bigg\\|\\!\\frac{\\big(d\\big(F(\\mathbf{y}_{t},\\xi_{t})\\big)\\big)\\!-\\!\\nabla\\big(\\mathbf{z}_{t,j};\\xi_{t,j}\\big)\\big)\\mathbf{w}_{t,j}\\!-\\!\\frac{d}{2\\delta}\\big(F(\\mathbf{y}_{t-1,j};\\xi_{t,j})\\big)\\!-\\!F\\big(\\mathbf{z}_{t-1,j};\\xi_{t,j}\\big)\\big)\\mathbf{w}_{t,j}\\Bigg\\|}{\\!\\Bigg.}}\\\\ &{\\quad-\\big(\\nabla(f\\circ g_{t})\\!\\big\\|\\!\\big(\\mathbf{x}_{t})\\!-\\!\\nabla(f\\circ g_{t-1})(\\mathbf{x}_{t-1})\\big)\\big\\|^{2}\\Bigg]\\!+\\!\\mathbb{E}\\Big[\\big\\|\\mathbf{v}_{t-1}\\!-\\!\\nabla(f\\circ g_{t-1,j})(\\mathbf{x}_{t-1})\\big\\|^{2}\\Bigg]}\\\\ &{\\leq\\!\\frac{1}{b_{f}^{2}\\!\\int_{\\mathbb{R}}\\!\\Bigg[\\big(\\frac{d}{\\delta}\\big(F(\\mathbf{y}_{t},\\xi_{t})\\big)\\big)\\!-\\!\\nabla\\big(\\mathbf{z}_{t,j};\\xi_{t,j}\\big)\\!-\\!F\\big(\\mathbf{z}_{t,j};\\xi_{t,j}\\big)\\big)\\mathbf{w}_{t,j}\\!-\\!\\frac{d}{2\\delta}\\big(F(\\mathbf{y}_{t\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The second equality follows from Lemma 3.8. The last inequality is due to $\\mathbb{E}[\\left\\lVert\\mathbf{x}-\\mathbb{E}[\\mathbf{x}]\\right\\rVert^{2}]\\ \\leq$ $\\mathbb{E}[\\|\\mathbf{x}\\|^{2}]$ . Observe that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|\\frac{d}{d s}(F(g_{t+\\cdot},\\xi,\\xi_{t}))-F(g_{t,j};\\xi_{t,j})\\right\\|_{W,j}-\\frac{d}{2\\delta}\\big(F(y_{t-1,j};\\xi,\\xi_{t})-F(g_{t-1,j};\\xi_{t,j})\\big)\\mathbb{w}_{t,j}\\right\\|_{X}^{2}\\right]}\\\\ &{\\leq\\leq\\mathbb{E}\\Bigg[\\bigg|\\frac{d}{d s}\\big(F(g(\\mathbf{x}_{t+\\cdot}~\\delta\\mathbf{w}_{t,j});\\xi_{t,j})-F(g(\\mathbf{x}_{t}-\\delta\\mathbf{w}_{t,j});\\xi_{t,j})-}\\\\ &{\\qquad(F(g(\\mathbf{x}_{t-1,j}+\\delta\\mathbf{w}_{t,j});\\xi_{t,j})-F(g(\\mathbf{x}_{t-1}-\\delta\\mathbf{w}_{t,j});\\xi_{t,j})\\big)\\mathbb{w}_{t,j}\\bigg|^{2}\\Bigg]}\\\\ &{\\qquad+\\mathbb{E}\\left[\\left\\|\\frac{d}{d s}\\big(F(g(\\mathbf{x}_{t}+\\delta\\mathbf{w}_{t,j});\\xi_{t,j})-F(g_{t,j};\\xi_{t,j})\\big)\\mathbb{w}_{t,j}\\right\\|^{2}\\Bigg]}\\\\ &{\\qquad+\\mathbb{E}\\left[\\bigg|\\frac{d}{d s}\\Big(F(g(\\mathbf{x}_{t}-\\delta\\mathbf{w}_{t,j});\\xi_{t,j})-F(g_{t,j};\\xi_{t,j})\\big)\\mathbb{w}_{t,j}\\bigg|^{2}\\right]}\\\\ &{\\qquad+\\mathbb{E}\\left[\\bigg|\\frac{d}{d s}\\Big(F(g(\\mathbf{x}_{t-1}+\\delta\\mathbf{w}_{t,j});\\xi_{t,j})-F(g_{t-1,j};\\xi_{t,j})\\big)\\mathbb{w}_{t,j}\\bigg|^{2}\\right]}\\\\ &{\\qquad+\\mathbb{E}\\Bigg[\\bigg|\\frac{d}{d s}\\Big(F(g(\\mathbf{x}_{t-1}+\\delta\\mathbf{w}_{t,j});\\xi_{t,j})-F(g_{t-1,j};\\xi_{t,j})\\big)\\mathbb{w}_{t,j}\\bigg|^{2}\\Bigg]}\\\\ &{\\qquad+\\mathbb{E}\\mathbb{E}\\left[\\bigg|\\frac{d}{d s}\\Big(F(g(\\mathbf{x}_{t-1}-\\delta\\mathbf{w}_{t,j});\\xi_{t \n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\!\\!\\frac{5d^{2}G_{f}^{2}G_{g}^{2}}{\\delta^{2}}\\mathbb{E}\\left[\\left\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\right\\|^{2}\\right]+\\frac{5d^{2}G_{f}^{2}\\sigma_{0}^{2}}{\\delta^{2}b_{g}^{\\prime}}}\\\\ &{=\\!\\!\\frac{5d^{2}G_{f}^{2}G_{g}^{2}\\eta^{2}}{\\delta^{2}}\\mathbb{E}\\left[\\left\\|\\mathbf{v}_{t-1}\\right\\|^{2}\\right]+\\frac{5d^{2}G_{f}^{2}\\sigma_{0}^{2}}{\\delta^{2}b_{g}^{\\prime}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The first inequality is due to $\\left\\|a_{1}+\\cdot\\cdot+a_{n}\\right\\|^{2}\\leq n\\left\\|a_{1}\\right\\|^{2}+\\cdot\\cdot\\cdot+n\\left\\|a_{n}\\right\\|^{2}$ . The second inequality is due to the Lipschitzness of both inner and outer functions with Assumption 3.3. Consequently, one has ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\left\\Vert\\mathbf{v}_{t}-\\nabla(f\\circ g_{t})_{\\delta}(\\mathbf{x}_{t})\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\!\\frac{5d^{2}G_{f}^{2}G_{g}^{2}\\eta^{2}}{\\delta^{2}b_{f}^{\\prime}}\\mathbb{E}\\left[\\left\\Vert\\mathbf{v}_{t-1}\\right\\Vert^{2}\\right]+\\frac{5d^{2}G_{f}^{2}\\sigma_{0}^{2}}{\\delta^{2}b_{f}^{\\prime}b_{g}^{\\prime}}+\\mathbb{E}\\left[\\left\\Vert\\mathbf{v}_{t-1}-\\nabla(f\\circ g_{t-1})_{\\delta}(\\mathbf{x}_{t-1})\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\!\\frac{5d^{2}G_{f}^{2}G_{g}^{2}\\eta^{2}}{\\delta^{2}b_{f}^{\\prime}}\\sum_{i=n_{t}}^{t}\\mathbb{E}\\left[\\left\\Vert\\mathbf{v}_{i}\\right\\Vert^{2}\\right]+\\frac{5m d^{2}G_{f}^{2}\\sigma_{0}^{2}}{\\delta^{2}b_{f}^{\\prime}b_{g}^{\\prime}}+\\frac{16\\sqrt{2\\pi}d G_{f}^{2}G_{g}^{2}}{b_{f}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The last inequality follows from Lemma 3.8. In addition, for $t$ mod $m=0$ we can bound ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[\\left\\Vert\\nabla(f\\circ g_{t})_{\\delta}(\\mathbf{x}_{t})-\\nabla(f\\circ g)_{\\delta}(\\mathbf{x}_{t})\\right\\Vert^{2}\\right]}\\\\ &{=\\!\\!\\mathbb{E}\\left[\\Bigg\\Vert\\mathbb{E}_{\\mathbf{u}}\\left[\\frac{d}{\\delta}\\left((f\\circ g_{t})(\\mathbf{x}_{t}+\\delta\\mathbf{u})-(f\\circ g)(\\mathbf{x}_{t}+\\delta\\mathbf{u})\\right)\\mathbf{u}\\right]\\Bigg\\Vert^{2}\\Bigg]}\\\\ &{\\le\\!\\frac{d^{2}}{\\delta^{2}}\\mathbb{E}_{\\mathbf{u}}\\left[\\left\\Vert(f\\circ g_{t})(\\mathbf{x}_{t}+\\delta\\mathbf{u})-(f\\circ g)(\\mathbf{x}_{t}+\\delta\\mathbf{u})\\right\\Vert^{2}\\left\\Vert\\mathbf{u}\\right\\Vert^{2}\\right]}\\\\ &{\\le\\!\\frac{c d^{2}G_{f}^{2}\\sigma_{0}^{2}}{\\delta^{2}b_{g}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The last inequality follows from the Lipschitzness of $f$ and Assumption 3.3. Similarly, for $t$ mod $m\\neq$ 0 we can bound ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert\\nabla(f\\circ g_{t})_{\\delta}(\\mathbf{x}_{t})-\\nabla(f\\circ g)_{\\delta}(\\mathbf{x}_{t})\\Vert^{2}\\right]\\leq\\frac{c d^{2}G_{f}^{2}\\sigma_{0}^{2}}{\\delta^{2}b_{g}^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Putting everything together, we get the desired bound. ", "page_idx": 15}, {"type": "text", "text": "We present the formal proof of Theorem 4.3 and Corollary 4.4 below. ", "page_idx": 15}, {"type": "text", "text": "C.1 Proof of Theorem 4.3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. Rearrange the terms in Lemma A.1, sum $t$ from 0 to $T-1$ , and divide both sides by $\\textstyle{\\frac{\\eta T}{2}}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{T}\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\|\\nabla(f\\circ g)\\delta(\\mathbf{x}_{t})\\|^{2}\\right]}\\\\ &{\\leq\\!\\frac{2\\mathbb{E}\\left[\\big(f\\circ g)\\delta(\\mathbf{x}_{0})-2(f\\circ g)\\delta(\\mathbf{x}_{T})\\big]}{\\eta T}-\\frac{1}{T}\\left(1-\\frac{c\\eta G_{f}G_{g}\\sqrt{d}}{\\delta}\\right)\\displaystyle\\sum_{i=0}^{T-1}\\mathbb{E}\\left[\\|\\mathbf{v}_{i}\\|^{2}\\right]}\\\\ &{\\quad+\\frac{1}{T}\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\|\\mathbf{v}_{t}-\\nabla(f\\circ g)\\delta(\\mathbf{x}_{t})\\|^{2}\\right]}\\\\ &{\\leq\\!\\frac{2\\mathbb{E}\\left[\\big(f\\circ g)\\delta(\\mathbf{x}_{0})-2(f\\circ g)\\delta(\\mathbf{x}_{T})\\big]}{\\eta T}-\\frac{1}{T}\\left(1-\\frac{c\\eta G_{f}G_{g}\\sqrt{d}}{\\delta}-\\frac{10m d^{2}G_{f}^{2}G_{g}^{2}\\eta^{2}}{\\delta^{2}b_{f}}\\right)\\displaystyle\\sum_{i=0}^{T-1}\\mathbb{E}\\left[\\|\\mathbf{v}_{i}\\|^{2}\\right]}\\\\ &{\\quad+\\frac{10m d^{2}G_{f}^{2}\\sigma_{0}^{2}}{\\delta^{2}b_{i}^{\\prime}b_{g}}+\\frac{32\\sqrt{2}\\pi d G_{f}^{2}G_{g}^{2}}{b_{f}}+\\frac{2c d^{2}G_{f}^{2}\\sigma_{0}^{2}}{\\delta^{2}b_{g}}+\\frac{2c d^{2}G_{f}^{2}\\sigma_{0}^{2}}{\\delta^{2}b_{g}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The last inequality follows from Lemma C.1. If we choose hyperparameters as follows ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\eta=\\frac{\\delta}{2c G_{f}G_{g}\\sqrt{d}},\\quad b_{f}^{\\prime}=\\Theta\\left(\\frac{d G_{f}G_{g}}{\\epsilon}\\right),\\quad m=\\Theta\\left(\\frac{G_{f}G_{g}}{\\epsilon}\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then we can deduce that 1 \u2212c\u03b7Gf \u03b4Gg d $\\begin{array}{r}{1-\\frac{c\\eta G_{f}G_{g}\\sqrt{d}}{\\delta}-\\frac{10m d^{2}G_{f}^{2}G_{g}^{2}\\eta^{2}}{\\delta^{2}b_{f}^{\\prime}}\\leq0}\\end{array}$ . By Lemma 3.7, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[(f\\circ g)_{\\delta}(\\mathbf{x}_{0})-(f\\circ g)_{\\delta}(\\mathbf{x}_{T})\\right]\\leq\\mathbb{E}[(f\\circ g)(\\mathbf{x}_{0})-(f\\circ g)(\\mathbf{x}_{T})]+2G_{f}G_{g}\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, we obtain the following result ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert\\nabla\\Phi_{\\delta}(\\mathbf{x}_{R})\\Vert^{2}\\right]=\\mathcal{O}\\left(\\frac{\\sqrt{d}G_{f}G_{g}R}{\\delta T}+\\frac{\\sqrt{d}G_{f}^{2}G_{g}^{2}}{T}+\\frac{d G_{f}^{2}G_{g}^{2}}{b_{f}}+\\frac{d^{2}G_{f}^{2}\\sigma_{0}^{2}}{\\delta^{2}b_{g}}+\\frac{d^{2}G_{f}^{2}\\sigma_{0}^{2}}{\\delta^{2}b_{g}^{\\prime}}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "C.2 Proof of Corollary 4.4 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. The total zeroth-order oracle calls can be bounded by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{O}(T b_{f}^{\\prime}b_{g}^{\\prime}+T b_{f}b_{g}/m)}\\\\ &{=\\!\\mathcal{O}\\left(\\left(\\frac{\\sqrt{d}G_{f}G_{g}R}{\\delta\\epsilon^{2}}\\!+\\!\\frac{\\sqrt{d}G_{f}^{2}G_{g}^{2}}{\\epsilon^{2}}\\right)\\!\\cdot\\!\\frac{d G_{f}G_{g}}{\\epsilon}\\cdot\\frac{d^{2}G_{f}^{2}\\sigma_{0}^{2}}{\\delta^{2}\\epsilon^{2}}\\right)}\\\\ &{=\\!\\mathcal{O}\\left(\\frac{d^{3.5}G_{f}^{4}G_{g}^{2}\\sigma_{0}^{2}R}{\\delta^{3}\\epsilon^{5}}+\\frac{d^{3.5}G_{f}^{5}G_{g}^{3}\\sigma_{0}^{2}}{\\delta^{2}\\epsilon^{5}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "D Extensions to Convex Nonsmooth Functions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we present the formal proof of theorems presented in Section 5. ", "page_idx": 16}, {"type": "text", "text": "D.1 Proof of Theorem 5.2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. Since $G(\\cdot;\\zeta)$ is convex function, $g_{t}(\\cdot)$ is also convex. Using the result of Section 3.2.4 of [38] and Assumption 5.1, we can deduce that $f\\circ g_{t}$ is a convex function. Let $\\begin{array}{r}{\\mathbf{x}^{\\ast}=\\arg\\operatorname*{min}_{\\mathbf{x}}(f\\circ g)_{\\delta}(\\mathbf{x})}\\end{array}$ , then we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}[[\\left(f\\circ g_{t})_{\\delta}(\\mathbf{x}_{t})-(f\\circ g_{t})_{\\delta}(\\mathbf{x}^{*})]\\right.}\\\\ &{\\left.\\leq\\mathbb{E}[\\langle\\nabla(f\\circ g_{t})_{\\delta}(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle]\\right.}\\\\ &{=\\mathbb{E}\\left[\\left\\langle\\frac{d}{2\\delta}(F(g_{t}(\\mathbf{x}_{t}+\\delta\\mathbf{u}),\\xi_{t})-F(g_{t}(\\mathbf{x}_{t}-\\delta\\mathbf{u}),\\xi_{t})),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle\\right]}\\\\ &{\\left.\\leq\\frac{1}{\\eta_{0}}\\mathbb{E}\\left[\\left\\langle\\mathbf{x}_{t}-\\mathbf{x}_{t+1},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle\\right]}\\\\ &{=\\frac{1}{2\\eta_{0}}\\mathbb{E}\\left[\\left\\Vert\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\Vert^{2}-\\left\\Vert\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\right\\Vert^{2}+\\left\\Vert\\mathbf{x}_{t}-\\mathbf{x}_{t+1}\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\frac{1}{2\\eta_{0}}\\mathbb{E}\\left[\\left\\Vert\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\Vert^{2}-\\left\\Vert\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\right\\Vert^{2}\\right]+8\\sqrt{2\\pi}d G_{f}^{2}G_{g}^{2}\\eta_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The first inequality follows from the convexity of $f\\circ g_{t}$ . The last inequality is due to Lemma 3.8. Observe that for $\\forall\\mathbf{x}\\in\\mathbb{R}^{d}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad|(f\\circ g)_{\\delta}(\\mathbf{x})-(f\\circ g_{t})_{\\delta}(\\mathbf{x})|}\\\\ &{=\\!|\\mathbb{E}_{\\mathbf{u}}[(f\\circ g)(\\mathbf{x}+\\delta\\mathbf{u})]-\\mathbb{E}_{\\mathbf{u}}[(f\\circ g_{t})(\\mathbf{x}+\\delta\\mathbf{u})]|}\\\\ &{=\\!|\\mathbb{E}_{\\mathbf{u}}[(f\\circ g)(\\mathbf{x}+\\delta\\mathbf{u})-(f\\circ g_{t})(\\mathbf{x}+\\delta\\mathbf{u})]|}\\\\ &{\\le\\!\\mathbb{E}_{\\mathbf{u}}[|(f\\circ g)(\\mathbf{x}+\\delta\\mathbf{u})-(f\\circ g_{t})(\\mathbf{x}+\\delta\\mathbf{u})|]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\le\\!\\frac{c_{1}G_{f}\\sigma_{0}}{\\sqrt{b_{g,0}}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $c_{1}$ is some constant. The second equality is due to the linearity of expectations. The last inequality follows from Assumption 3.3 and Lipschitzness of $f$ . Consequently, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad\\mathbb{E}[(f\\circ g)_{\\delta}(\\mathbf{x}_{1})-(f\\circ g)_{\\delta}(\\mathbf{x}^{*})]}\\\\ &{\\quad\\le\\!\\frac{\\hat{R}^{2}}{2\\eta_{0}T_{0}}+8\\!\\sqrt{2\\pi}d G_{f}^{2}G_{g}^{2}\\eta_{0}+\\frac{2c_{1}G_{f}\\sigma_{0}}{\\sqrt{b_{g,0}}}}\\\\ &{\\quad\\le\\!\\frac{12\\hat{R}G_{f}G_{g}\\sqrt{d}}{\\sqrt{T_{0}}}+\\frac{2c_{1}G_{f}\\sigma_{0}}{\\sqrt{b_{g,0}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By Lemma 3.7, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[(f\\circ g)_{\\delta}\\big(\\mathbf{x}_{1}\\big)-(f\\circ g)_{\\delta}\\big(\\mathbf{x}^{*}\\big)\\right]\\leq\\mathbb{E}\\big[(f\\circ g)\\big(\\mathbf{x}_{1}\\big)-(f\\circ g)\\big(\\mathbf{x}^{*}\\big)\\big]+2G_{f}G_{g}\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Consequently, one has ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}[(f\\circ g)_{\\delta}(\\mathbf{x}_{1})-(f\\circ g)_{\\delta}(\\mathbf{x}^{*})]}\\\\ &{\\leq\\!\\frac{12\\hat{R}G_{f}G_{g}\\sqrt{d}}{\\sqrt{T_{0}}}+\\frac{2c_{1}G_{f}\\sigma_{0}}{\\sqrt{b_{g,0}}}+2G_{f}G_{g}\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To obtain $\\mathbb{E}[(f\\circ g)(\\mathbf{x}_{1})-(f\\circ g)(\\mathbf{x}^{*})]\\leq\\rho.$ , we choose ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\eta_{0}=\\frac{\\hat{R}}{G_{f}G_{g}\\sqrt{d T_{0}}},\\quad T_{0}=\\Theta\\left(\\frac{\\hat{R}^{2}G_{f}^{2}G_{g}^{2}d}{\\rho^{2}}\\right),\\quad b_{g,0}=\\Theta\\left(\\frac{G_{f}^{2}\\sigma_{0}^{2}}{\\rho^{2}}\\right),\\quad\\delta=\\Theta\\left(\\frac{\\rho}{G_{f}G_{g}}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "D.2 Proof of Corollary 5.3 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. The total stochastic function oracle calls can be bounded by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{O}\\left(T b_{f}b_{g}+T_{0}b_{g,0}\\right)}\\\\ &{=\\!\\mathcal{O}\\left(\\frac{G_{f}G_{g}\\sqrt{d}(\\rho+G_{f}G_{g}\\delta)}{\\delta\\epsilon^{2}}\\cdot\\frac{d G_{f}^{2}G_{g}^{2}}{\\epsilon^{2}}\\cdot\\frac{d^{2}G_{f}^{2}\\sigma_{0}^{2}}{\\delta^{2}\\epsilon^{2}}+\\frac{d\\hat{R}^{2}G_{f}^{4}G_{g}^{2}\\sigma_{0}^{2}}{\\rho^{4}}\\right)}\\\\ &{=\\!\\mathcal{O}\\left(\\frac{d^{3.5}G_{f}^{5}G_{g}^{3}\\sigma_{0}^{2}\\rho}{\\delta^{3}\\epsilon^{6}}+\\frac{d^{3.5}G_{f}^{6}G_{g}^{4}\\sigma_{0}^{2}}{\\delta^{2}\\epsilon^{6}}+\\frac{d\\hat{R}^{2}G_{f}^{4}G_{g}^{2}\\sigma_{0}^{2}}{\\rho^{4}}\\right)}\\\\ &{=\\!\\mathcal{O}\\left(\\frac{d^{3}\\hat{R}^{0.4}G_{f}^{4.8}G_{g}^{2.8}\\sigma_{0}^{2}}{\\delta^{2.4}\\epsilon^{4.8}}+\\frac{d^{3.5}G_{f}^{6}G_{g}^{4}\\sigma_{0}^{2}}{\\delta^{2}\\epsilon^{6}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "D.3 Proof of Corollary 5.4 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. The total stochastic function oracle calls can be bounded by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{O}\\left(T b_{f}^{\\prime}b_{g}^{\\prime}+T b_{f}b_{g}/m+T_{0}b_{g,0}\\right)}\\\\ &{=\\!\\mathcal{O}\\left(\\frac{G_{f}G_{g}\\sqrt{d}\\left(\\rho+G_{f}G_{g}\\right)}{\\delta\\epsilon^{2}}\\cdot\\frac{d G_{f}G_{g}}{\\epsilon}\\cdot\\frac{d^{2}\\sigma_{0}^{2}G_{f}^{2}}{\\delta^{2}\\epsilon^{2}}+\\frac{d\\hat{R}^{2}G_{f}^{4}G_{g}^{2}\\sigma_{0}^{2}}{\\rho^{4}}\\right)}\\\\ &{=\\!\\mathcal{O}\\left(\\frac{d^{3.5}G_{f}^{4}G_{g}^{2}\\sigma_{0}^{2}\\rho}{\\delta^{3}\\epsilon^{5}}+\\frac{d^{3.5}G_{f}^{5}G_{g}^{3}\\sigma_{0}^{2}}{\\delta^{2}\\epsilon^{5}}+\\frac{d\\hat{R}^{2}G_{f}^{4}G_{g}^{2}\\sigma_{0}^{2}}{\\rho^{4}}\\right)}\\\\ &{=\\!\\mathcal{O}\\left(\\frac{d^{3}\\hat{R}^{0.4}G_{f}^{4}G_{g}^{2}\\sigma_{0}^{2}}{\\delta^{2.4}\\epsilon^{4}}+\\frac{d^{3.5}G_{f}^{5}G_{g}^{3}\\sigma_{0}^{2}}{\\delta^{2}\\epsilon^{5}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: We are clearing the code with internal compliance and will release it upon approval. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: There are no potential positive or negative societal impacts of this work. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 22}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]