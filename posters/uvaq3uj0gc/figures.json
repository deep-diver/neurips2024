[{"figure_path": "UVAq3uJ0gc/figures/figures_8_1.jpg", "caption": "Figure 1: We present the loss vs. complexity on several portfolio management datasets. The plot of GFCOM and the Kiefer-Wolfowitz method are overlapped as their performance are close to each other.", "description": "This figure compares the performance of three algorithms for portfolio management: GFCOM, GFCOM+, and a Kiefer-Wolfowitz baseline method.  The x-axis represents the number of function calls (a measure of computational complexity), and the y-axis represents the loss (or error) of the portfolio optimization.  The figure shows that GFCOM+ generally converges faster (i.e., lower loss with fewer function calls) compared to GFCOM and Kiefer-Wolfowitz.  The similar performance of GFCOM and the Kiefer-Wolfowitz method is highlighted by overlapping the lines in the plot.", "section": "6 Experiments"}, {"figure_path": "UVAq3uJ0gc/figures/figures_8_2.jpg", "caption": "Figure 2: For the RL task, we present the loss vs. complexity on datasets with states of different sizes. We choose the mini-batch size bf = bg = 1000. In addition, we set b'f = 100, b'g = 1000 and m = bf/b'f = 10 for the GFCOM+ algorithm. Figure 1 shows that the GFCOM+ algorithm converges much faster than the GFCOM and the baseline method across all datasets.", "description": "This figure compares the performance of GFCOM, GFCOM+, and a Kiefer-Wolfowitz baseline method on a reinforcement learning (RL) task.  The x-axis represents the number of function calls (a measure of computational cost), and the y-axis shows the loss.  Three subplots show results for different problem sizes (number of states: 400, 600, and 800). GFCOM+ consistently demonstrates faster convergence than GFCOM and the baseline.", "section": "6.2 Application to Reinforcement Learning"}]