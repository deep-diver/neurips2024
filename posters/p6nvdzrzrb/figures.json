[{"figure_path": "P6nVDZRZRB/figures/figures_5_1.jpg", "caption": "Figure 1: Behavior of Uncertainties Learned by EDL methods on Real Data. (a) EDL methods learn spurious epistemic uncertainty, wherein uncertainty does not vanish with an increasing number of observed samples, contrary to the fundamental definition of epistemic uncertainty. (b) Instead of a constant, EDL methods learn model-dependent aleatoric uncertainty that depends on hyper-parameter \u03bb, contrary to the fundamental definition of aleatoric uncertainty.", "description": "This figure shows the behavior of epistemic and aleatoric uncertainty learned by four different EDL methods (BM, RPriorNet, PostNet, NatPN) on the CIFAR10 dataset.  The left side (a) illustrates the epistemic uncertainty which, contrary to its definition, does not vanish as the sample size increases. The right side (b) shows the aleatoric uncertainty, which unexpectedly changes with hyperparameter \u03bb instead of remaining constant.", "section": "Rethinking the Success of EDL Methods"}, {"figure_path": "P6nVDZRZRB/figures/figures_6_1.jpg", "caption": "Figure 2: OOD Detection Performance v.s. Hyper-parameter \u03bb on CIFAR10. The x-axis represents the increasing \u03bb value, and the y-axis represents the Average AUROC score of OOD detection tasks. EDL Methods' uncertainty quantification performance are sensitive to hyper-parameter \u03bb, while generally benefit from small \u03bb.", "description": "This figure shows the impact of the hyperparameter \u03bb on the performance of different EDL methods in out-of-distribution (OOD) detection tasks using CIFAR-10 as the in-distribution dataset.  The average AUROC (Area Under the Receiver Operating Characteristic curve), a common metric for evaluating OOD detection, is plotted against different values of \u03bb.  The shaded area represents the standard deviation across multiple runs. The results indicate that the performance of EDL methods in OOD detection is sensitive to the choice of \u03bb, with generally better performance observed for smaller values of \u03bb.", "section": "5 Rethinking the Success of EDL Methods"}, {"figure_path": "P6nVDZRZRB/figures/figures_8_1.jpg", "caption": "Figure 3: Comparison of Different EDL Methods on OOD Detection. Distillation based methods, including new proposed Bootstrap-Distill method, demonstrate clear advantage over other classical EDL methods. Similar behavior holds for selective classification task.", "description": "This figure compares the performance of various EDL methods on out-of-distribution (OOD) detection.  The x-axis represents the different EDL methods.  The left bar graph shows AUROC scores for CIFAR-10, and the right bar graph shows AUROC scores for CIFAR-100.  The results indicate that distillation-based methods, particularly the new Bootstrap-Distill method, significantly outperform other EDL approaches in OOD detection.  The improved performance is consistent across both CIFAR-10 and CIFAR-100 datasets.", "section": "7 Comprehensive Empirical Evaluation"}, {"figure_path": "P6nVDZRZRB/figures/figures_8_2.jpg", "caption": "Figure 1: Behavior of Uncertainties Learned by EDL methods on Real Data. (a) EDL methods learn spurious epistemic uncertainty, wherein uncertainty does not vanish with an increasing number of observed samples, contrary to the fundamental definition of epistemic uncertainty. (b) Instead of a constant, EDL methods learn model-dependent aleatoric uncertainty that depends on hyper-parameter \u03bb, contrary to the fundamental definition of aleatoric uncertainty.", "description": "This figure shows the behavior of epistemic and aleatoric uncertainties learned by four different EDL methods (BM, RPriorNet, PostNet, NatPN) on the CIFAR-10 dataset.  The left panel (a) demonstrates that epistemic uncertainty (the uncertainty due to the model's knowledge) does not decrease even with an increasing number of training samples, which is unexpected. The right panel (b) illustrates that the aleatoric uncertainty (the uncertainty inherent in the data) depends on a hyperparameter \u03bb, contradicting the fundamental definition of aleatoric uncertainty as being data-dependent, not model-dependent.", "section": "5 Rethinking the Success of EDL Methods"}, {"figure_path": "P6nVDZRZRB/figures/figures_21_1.jpg", "caption": "Figure 1: Behavior of Uncertainties Learned by EDL methods on Real Data. (a) EDL methods learn spurious epistemic uncertainty, wherein uncertainty does not vanish with an increasing number of observed samples, contrary to the fundamental definition of epistemic uncertainty. (b) Instead of a constant, EDL methods learn model-dependent aleatoric uncertainty that depends on hyper-parameter \u03bb, contrary to the fundamental definition of aleatoric uncertainty. Similar behavior holds for 2D Gaussian data (see Figure 5 in Appendix H.1).", "description": "This figure shows the behavior of epistemic and aleatoric uncertainties learned by four representative EDL methods (BM, RPriorNet, PostNet, and NatPN) on CIFAR-10 dataset.  The left panel (a) demonstrates that epistemic uncertainty, as quantified by mutual information, does not decrease and vanish with increasing sample sizes. This is contrary to the expected behavior, where uncertainty should decrease with more data. The right panel (b) illustrates that the aleatoric uncertainty, quantified by the expected Shannon entropy,  is not a constant but depends on the hyperparameter lambda (\u03bb).  This is not in line with the definition of aleatoric uncertainty which should be constant and independent of model parameters.", "section": "Rethinking the Success of EDL Methods"}, {"figure_path": "P6nVDZRZRB/figures/figures_22_1.jpg", "caption": "Figure 6: Visualization of Epistemic Uncertainty on 2D Gaussian Data. The first row (a)-(d) corresponds to differential entropy quantified by EDL models trained using their original proposed training objective (reverse-KL), and the second row (e)-(h) corresponds to epistemic uncertainty quantified by same EDL methods ablated with MSE objective in Eq. (18). This ablation study implies that the specific training objectives has no actual impact on the methods' ability to quantify uncertainty. Other techniques, such as using density estimation (PostNet (c, g), NatPN (d, h)), and using OOD data during training (RPriorNet (b, f)), play a more significant role in EDL methods' UQ performance. Without auxiliary techniques, BM (a, e) cannot distinguish in-distribution and OOD regions. Similar behavior holds for total uncertainty quantification (see Fig. 8).", "description": "This figure shows an ablation study on the effect of different training objectives and auxiliary techniques on the ability of EDL methods to quantify epistemic uncertainty.  The results indicate that the choice of training objective (reverse KL vs. MSE) has little impact, whereas the use of auxiliary techniques like density estimation and OOD data during training significantly affects performance.  Without these techniques, the basic EDL method struggles to distinguish between in-distribution and out-of-distribution data.", "section": "H.2 Ablation Study in Sec. 5.2"}, {"figure_path": "P6nVDZRZRB/figures/figures_23_1.jpg", "caption": "Figure 1: Behavior of Uncertainties Learned by EDL methods on Real Data. (a) EDL methods learn spurious epistemic uncertainty, wherein uncertainty does not vanish with an increasing number of observed samples, contrary to the fundamental definition of epistemic uncertainty. (b) Instead of a constant, EDL methods learn model-dependent aleatoric uncertainty that depends on hyper-parameter \u03bb, contrary to the fundamental definition of aleatoric uncertainty. Similar behavior holds for 2D Gaussian data (see Figure 5 in Appendix H.1).", "description": "This figure shows that Evidential Deep Learning (EDL) methods fail to accurately capture epistemic and aleatoric uncertainty.  The left panel demonstrates that epistemic uncertainty (which should decrease with more data) remains high even with a large number of samples. The right panel shows that aleatoric uncertainty (which should be constant) changes with a hyperparameter (\u03bb). This indicates EDL methods do not reliably represent uncertainty.", "section": "Rethinking the Success of EDL Methods"}, {"figure_path": "P6nVDZRZRB/figures/figures_23_2.jpg", "caption": "Figure 1: Behavior of Uncertainties Learned by EDL methods on Real Data. (a) EDL methods learn spurious epistemic uncertainty, wherein uncertainty does not vanish with an increasing number of observed samples, contrary to the fundamental definition of epistemic uncertainty. (b) Instead of a constant, EDL methods learn model-dependent aleatoric uncertainty that depends on hyper-parameter \u03bb, contrary to the fundamental definition of aleatoric uncertainty.", "description": "This figure displays the behavior of epistemic and aleatoric uncertainty learned by four representative EDL methods (BM, RPriorNet, PostNet, NatPN) on the CIFAR10 dataset.  Subfigure (a) shows that epistemic uncertainty, which should decrease with more data, remains constant, even with an increasing number of samples. Subfigure (b) reveals that aleatoric uncertainty, which should be constant, is dependent on the hyperparameter \u03bb.  This inconsistent behavior contradicts the fundamental definitions of both types of uncertainty, suggesting a flaw in how these EDL methods quantify uncertainty.", "section": "5 Rethinking the Success of EDL Methods"}, {"figure_path": "P6nVDZRZRB/figures/figures_24_1.jpg", "caption": "Figure 1: Behavior of Uncertainties Learned by EDL methods on Real Data. (a) EDL methods learn spurious epistemic uncertainty, wherein uncertainty does not vanish with an increasing number of observed samples, contrary to the fundamental definition of epistemic uncertainty. (b) Instead of a constant, EDL methods learn model-dependent aleatoric uncertainty that depends on hyper-parameter \u03bb, contrary to the fundamental definition of aleatoric uncertainty.", "description": "This figure shows the behavior of epistemic and aleatoric uncertainty learned by four different EDL methods (BM, RPriorNet, PostNet, NatPN) on the CIFAR-10 dataset.  The left panel (a) demonstrates that epistemic uncertainty, as measured by mutual information, does not decrease with increasing sample size, contradicting the theoretical definition of epistemic uncertainty. The right panel (b) shows that aleatoric uncertainty varies with the hyperparameter \u03bb, again contradicting the theoretical definition which posits it should be constant.  These findings suggest that the EDL methods do not reliably quantify uncertainties.", "section": "5 Rethinking the Success of EDL Methods"}, {"figure_path": "P6nVDZRZRB/figures/figures_24_2.jpg", "caption": "Figure 1: Behavior of Uncertainties Learned by EDL methods on Real Data. (a) EDL methods learn spurious epistemic uncertainty, wherein uncertainty does not vanish with an increasing number of observed samples, contrary to the fundamental definition of epistemic uncertainty. (b) Instead of a constant, EDL methods learn model-dependent aleatoric uncertainty that depends on hyper-parameter \u03bb, contrary to the fundamental definition of aleatoric uncertainty.", "description": "This figure shows the behavior of epistemic and aleatoric uncertainty learned by different Evidential Deep Learning (EDL) methods on the CIFAR-10 dataset.  The left subfigure (a) demonstrates that epistemic uncertainty, as measured by mutual information, does not decrease and vanish with increasing sample size as it theoretically should, indicating a spurious learning effect. The right subfigure (b) illustrates that the aleatoric uncertainty, as quantified by the expected entropy of predictive distribution, varies with the hyperparameter \u03bb, contradicting the definition of aleatoric uncertainty as a fixed quantity inherent to the data.", "section": "Rethinking the Success of EDL Methods"}, {"figure_path": "P6nVDZRZRB/figures/figures_25_1.jpg", "caption": "Figure 1: Behavior of Uncertainties Learned by EDL methods on Real Data. (a) EDL methods learn spurious epistemic uncertainty, wherein uncertainty does not vanish with an increasing number of observed samples, contrary to the fundamental definition of epistemic uncertainty. (b) Instead of a constant, EDL methods learn model-dependent aleatoric uncertainty that depends on hyper-parameter \u03bb, contrary to the fundamental definition of aleatoric uncertainty.", "description": "This figure shows the behavior of epistemic and aleatoric uncertainty learned by four representative EDL methods (BM, RPriorNet, PostNet, NatPN) on CIFAR10 dataset.  The left panel (a) demonstrates that epistemic uncertainty, as measured by mutual information, remains almost constant regardless of the increasing sample size and test accuracy, contradicting the expectation that epistemic uncertainty should decrease and vanish with more data.  The right panel (b) illustrates that aleatoric uncertainty, represented by the expected Shannon entropy of the predictive distribution, varies with the hyperparameter \u03bb instead of being a constant, contrary to the fundamental definition of aleatoric uncertainty.  This suggests EDL methods fail to accurately quantify these types of uncertainty.", "section": "Rethinking the Success of EDL Methods"}, {"figure_path": "P6nVDZRZRB/figures/figures_25_2.jpg", "caption": "Figure 1: Behavior of Uncertainties Learned by EDL methods on Real Data. (a) EDL methods learn spurious epistemic uncertainty, wherein uncertainty does not vanish with an increasing number of observed samples, contrary to the fundamental definition of epistemic uncertainty. (b) Instead of a constant, EDL methods learn model-dependent aleatoric uncertainty that depends on hyper-parameter \u03bb, contrary to the fundamental definition of aleatoric uncertainty.", "description": "This figure shows the behavior of epistemic and aleatoric uncertainties learned by four representative EDL methods (BM, RPriorNet, PostNet, NatPN) on the CIFAR10 dataset.  The left-hand side (a) demonstrates that the epistemic uncertainty, which should decrease and vanish with increasing data, remains largely constant for ID data across different sample sizes. The right-hand side (b) demonstrates that the aleatoric uncertainty depends on the hyperparameter \u03bb instead of being constant, contradicting the definition of aleatoric uncertainty. These findings challenge the reliability of uncertainty quantification using the existing EDL methods.", "section": "5 Rethinking the Success of EDL Methods"}, {"figure_path": "P6nVDZRZRB/figures/figures_25_3.jpg", "caption": "Figure 1: Behavior of Uncertainties Learned by EDL methods on Real Data. (a) EDL methods learn spurious epistemic uncertainty, wherein uncertainty does not vanish with an increasing number of observed samples, contrary to the fundamental definition of epistemic uncertainty. (b) Instead of a constant, EDL methods learn model-dependent aleatoric uncertainty that depends on hyper-parameter \u03bb, contrary to the fundamental definition of aleatoric uncertainty.", "description": "This figure shows the behavior of epistemic and aleatoric uncertainty learned by different EDL methods (BM, RPriorNet, PostNet, NatPN) on CIFAR-10 dataset.  Subfigure (a) demonstrates that epistemic uncertainty, as quantified by the EDL methods, does not vanish even with an increasing number of training samples, which contradicts the theoretical definition of epistemic uncertainty.  Subfigure (b) shows that the EDL methods do not learn a constant aleatoric uncertainty, but rather learn an uncertainty dependent on the hyperparameter \u03bb, a finding that contradicts the theoretical definition of aleatoric uncertainty.  This suggests that the uncertainty estimations produced by the EDL methods are unreliable.", "section": "5 Rethinking the Success of EDL Methods"}, {"figure_path": "P6nVDZRZRB/figures/figures_26_1.jpg", "caption": "Figure 1: Behavior of Uncertainties Learned by EDL methods on Real Data. (a) EDL methods learn spurious epistemic uncertainty, wherein uncertainty does not vanish with an increasing number of observed samples, contrary to the fundamental definition of epistemic uncertainty. (b) Instead of a constant, EDL methods learn model-dependent aleatoric uncertainty that depends on hyper-parameter \u03bb, contrary to the fundamental definition of aleatoric uncertainty.", "description": "This figure demonstrates the behavior of epistemic and aleatoric uncertainties learned by four representative EDL methods (BM, RPriorNet, PostNet, and NatPN) on the CIFAR-10 dataset.  The left panel (a) shows that epistemic uncertainty, contrary to its definition, does not vanish as the number of training samples increases.  The right panel (b) shows that aleatoric uncertainty depends on the hyperparameter \u03bb, which is inconsistent with the definition of aleatoric uncertainty as independent of the model.", "section": "5 Rethinking the Success of EDL Methods"}]