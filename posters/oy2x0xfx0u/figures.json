[{"figure_path": "Oy2x0Xfx0u/figures/figures_19_1.jpg", "caption": "Figure 1: Overview of our results. Formulating ReLU MPNNs as tropical geometric objects allows us to shed light on several important aspects where WL falls short.", "description": "This figure summarizes the key theoretical contributions of the paper.  It highlights the use of tropical geometry to analyze ReLU message-passing neural networks (MPNNs).  The figure shows that the work characterizes the class of functions learned by ReLU MPNNs, establishes bounds on their geometric complexity, introduces new architectures, and characterizes their decision boundaries.  These contributions address limitations of existing methods, such as the Weisfeiler-Lehman (WL) test, for understanding the capabilities of GNNs.", "section": "Theoretical contributions of this work"}, {"figure_path": "Oy2x0Xfx0u/figures/figures_22_1.jpg", "caption": "Figure 2: Orange, green, and magenta nodes represent input, hidden (with activation max{\u00b7, 0}) and output (with activation max{\u00b7, -\u221e}) units respectively. (Left) ( can be used as a control to either let \u03be pass or filter it through the network. Note that in practice a sufficiently small negative weight can be used instead of -\u221e. (Right) A gadget that yields the greater of its two inputs as the output.", "description": "This figure shows two neural network gadgets. The left gadget uses a control input (\u03b6) to either pass or filter an input (\u03be) through the network. The right gadget outputs the maximum of its two inputs.", "section": "6.3 Selection gadget"}, {"figure_path": "Oy2x0Xfx0u/figures/figures_22_2.jpg", "caption": "Figure 2: Orange, green, and magenta nodes represent input, hidden (with activation max{\u00b7, 0}) and output (with activation max{\u00b7, -\u221e}) units respectively. (Left) ( can be used as a control to either let & pass or filter it through the network. Note that in practice a sufficiently small negative weight can be used instead of -\u221e. (Right) A gadget that yields the greater of its two inputs as the output.", "description": "This figure shows two neural network gadgets used in the paper's algorithms. The left gadget is a selection gadget that uses a control input to either pass or filter an input value. The right gadget is a comparison gadget that outputs the maximum of two input values.", "section": "6.3 Selection gadget"}, {"figure_path": "Oy2x0Xfx0u/figures/figures_22_3.jpg", "caption": "Figure 3: (Left)  in Broadcast gadget (shown here for m = 3 and r = 4 for node A\u2081) replicates all the monomials p1, ..., pr across the nodes of a graph G (not shown). See Algorithm 4 in Appendix for weights and other details. (Right) Selection gadget builds on the base gadget from Fig. 2a, and filters out specific monomials at each node of G, yielding a partition of the monomials across nodes.", "description": "This figure shows the Broadcast and Selection gadgets used in the paper's proposed algorithms. The left panel illustrates the Broadcast gadget, which replicates input monomials across nodes in a fully connected graph.  The right panel shows the Selection gadget, a neural network that filters these replicated monomials to distribute them evenly among nodes, facilitating efficient comparisons within the algorithms.", "section": "New ReLU MPNNs architectures and complexity tradeoffs"}]