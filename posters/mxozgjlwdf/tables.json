[{"figure_path": "MXOzgjlWDF/tables/tables_6_1.jpg", "caption": "Table 1: ViT-experiments : Baseline numbers are taken from [24]. The best numbers are highlighted in bold and the second-best numbers are underlined. Hyperparameter settings are followed from [24]. We find that SURM consistently outperform very strong baselines with 2-3x reduction in parameters.", "description": "This table presents the results of experiments using Vision Transformers (ViT) on various image classification datasets (CIFAR-10, CIFAR-100, SUN397, DTD, STL10). It compares the performance of the proposed Structured Unrestricted-Rank Matrices (SURM) method with several other parameter-efficient fine-tuning (PEFT) baselines, including Attention Tuning, Linear Probing, BitFit, Adapter, AdapterDrop, LoRA, LoRA-FIX, LN Tuning, LEPE, RPB, KAdaptation.  The table shows the number of parameters used by each method and the accuracy achieved on each dataset.  The results demonstrate that SURM consistently achieves higher accuracy than the baselines while using significantly fewer parameters.", "section": "Experiments"}, {"figure_path": "MXOzgjlWDF/tables/tables_7_1.jpg", "caption": "Table 1: ViT-experiments : Baseline numbers are taken from [24]. The best numbers are highlighted in bold and the second-best numbers are underlined. Hyperparameter settings are followed from [24]. We find that SURM consistently outperform very strong baselines with 2-3x reduction in parameters.", "description": "This table presents the results of experiments on various image classification datasets using Vision Transformers (ViTs).  The table compares the performance of the proposed Structured Unrestricted-Rank Matrices (SURM) method against several baseline parameter-efficient fine-tuning (PEFT) techniques.  The performance is measured in terms of accuracy across different datasets (CIFAR-10, CIFAR-100, SUN397, DTD, STL10). The number of parameters used by each method is also reported.  The table highlights that SURM achieves better accuracy than the baselines while using significantly fewer parameters.", "section": "6 Experiments"}, {"figure_path": "MXOzgjlWDF/tables/tables_9_1.jpg", "caption": "Table 1: ViT-experiments : Baseline numbers are taken from [24]. The best numbers are highlighted in bold and the second-best numbers are underlined. Hyperparameter settings are followed from [24]. We find that SURM consistently outperform very strong baselines with 2-3x reduction in parameters.", "description": "This table presents the results of experiments using Vision Transformers (ViTs) on various image classification datasets (CIFAR-10, CIFAR-100, SUN397, DTD, STL-10).  It compares the performance of the proposed Structured Unrestricted-Rank Matrices (SURM) method against several baselines, including fine-tuning, attention tuning, linear probing, adapter methods, and LoRA. The table shows the accuracy achieved by each method, along with the number of parameters used.  SURM demonstrates improved accuracy with significantly fewer parameters compared to baseline methods.", "section": "6 Experiments"}, {"figure_path": "MXOzgjlWDF/tables/tables_22_1.jpg", "caption": "Table 1: ViT-experiments : Baseline numbers are taken from [24]. The best numbers are highlighted in bold and the second-best numbers are underlined. Hyperparameter settings are followed from [24]. We find that SURM consistently outperform very strong baselines with 2-3x reduction in parameters.", "description": "This table presents the results of experiments conducted on various image classification datasets using Vision Transformers (ViTs).  It compares the performance of the proposed Structured Unrestricted-Rank Matrices (SURM) method against several baseline parameter-efficient fine-tuning (PEFT) techniques.  The table shows accuracy results on multiple datasets (CIFAR-10, CIFAR-100, SUN397, DTD, STL10) for both ViT-B and CLIP architectures.  The number of parameters used by each method is also provided, highlighting SURM's efficiency in achieving competitive or better accuracy with significantly fewer parameters.", "section": "6 Experiments"}, {"figure_path": "MXOzgjlWDF/tables/tables_23_1.jpg", "caption": "Table 5: Performance of SURM and other baselines on GLUE benchmark. We report the MCC score for COLA, F1 score for MRPC, Spearman correlation for STSB, and accuracy scores for the other tasks. All results are obtained by averaging over 3 seeds. Best numbers are highlighted in bold and the second best numbers is underline.", "description": "This table presents the performance comparison of the proposed Structured Unrestricted-Rank Matrices (SURMs) method against various other parameter-efficient fine-tuning (PEFT) techniques on the GLUE benchmark dataset.  The table shows the scores (MCC, F1, Spearman correlation, and Accuracy) achieved by each method across different GLUE tasks.  The number of parameters used by each method is also indicated.", "section": "Experiments"}, {"figure_path": "MXOzgjlWDF/tables/tables_24_1.jpg", "caption": "Table 1: ViT-experiments : Baseline numbers are taken from [24]. The best numbers are highlighted in bold and the second-best numbers are underlined. Hyperparameter settings are followed from [24]. We find that SURM consistently outperform very strong baselines with 2-3x reduction in parameters.", "description": "This table presents the results of image classification experiments using Vision Transformers (ViT).  It compares the performance of the proposed Structured Unrestricted-Rank Matrices (SURM) method against several baseline parameter-efficient fine-tuning (PEFT) techniques across five image datasets (CIFAR-10, CIFAR-100, SUN397, DTD, STL10). The table shows the accuracy achieved by each method, the number of parameters used, and highlights the best and second-best results for each dataset.  The results demonstrate that SURM achieves higher accuracy than baselines while using significantly fewer parameters.", "section": "Experiments"}, {"figure_path": "MXOzgjlWDF/tables/tables_25_1.jpg", "caption": "Table 5: Performance of SURM and other baselines on GLUE benchmark. We report the MCC score for COLA, F1 score for MRPC, Spearman correlation for STSB, and accuracy scores for the other tasks. All results are obtained by averaging over 3 seeds. Best numbers are highlighted in bold and the second best numbers is underline.", "description": "This table presents the results of various parameter-efficient fine-tuning (PEFT) methods on the GLUE benchmark.  It compares the performance of Structured Unrestricted-Rank Matrices (SURMs) against several other PEFT techniques, including LoRA and adapter-based methods.  The metrics used include Matthews Correlation Coefficient (MCC), F1-score, Spearman correlation, and accuracy, depending on the specific task.  The number of parameters used by each method is also shown, demonstrating the parameter efficiency of SURMs.", "section": "Experiments"}, {"figure_path": "MXOzgjlWDF/tables/tables_26_1.jpg", "caption": "Table 8: CKA between full finetuned weights and the SURM weights", "description": "This table shows the Centered Kernel Alignment (CKA) scores between the full fine-tuned weights and the weights obtained using different structured unrestricted rank matrices (SURMs) \u2013 LoRA, Circulant, Symmetric Toeplitz, and Toeplitz.  CKA measures the similarity between two sets of feature vectors. Higher CKA scores indicate greater similarity.  The table demonstrates that SURM methods, particularly the circulant variant, achieve substantially higher CKA scores compared to LoRA, suggesting that SURMs learn representations that are more similar to those learned by full fine-tuning.", "section": "B.4 Analysis of Weight Matrices in Fine-tuned Models"}]