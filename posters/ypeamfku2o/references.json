{"references": [{"fullname_first_author": "Haoyi Zhou", "paper_title": "Informer: Beyond efficient transformer for long sequence time-series forecasting", "publication_date": "2021-00-00", "reason": "This paper is a foundational work that significantly advances the state-of-the-art in long sequence time series forecasting, and is directly cited and compared to by the current paper."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-00-00", "reason": "This paper introduced the Transformer architecture, which has become a dominant paradigm in sequence modeling and is fundamental to many models discussed and compared in the current paper."}, {"fullname_first_author": "Sepp Hochreiter", "paper_title": "Long short-term memory", "publication_date": "1997-00-00", "reason": "This paper introduced the LSTM architecture, a significant improvement to the RNN architecture that addresses many of its limitations and is relevant to the novel PGN architecture proposed in the current paper."}, {"fullname_first_author": "Junyoung Chung", "paper_title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "publication_date": "2014-00-00", "reason": "This paper provided an important empirical evaluation of gated recurrent neural networks which are directly relevant to the PGN architecture proposed in the current paper."}, {"fullname_first_author": "Razvan Pascanu", "paper_title": "On the difficulty of training recurrent neural networks", "publication_date": "2013-00-00", "reason": "This paper highlighted the challenges in training RNNs that are directly addressed in the current paper's proposed PGN architecture"}]}