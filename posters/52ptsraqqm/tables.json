[{"figure_path": "52PTSrAQQM/tables/tables_6_1.jpg", "caption": "Table 1: Comparison with DINOSAUR [33] on synthetic datasets: MOVI-C [15] and MOVI-E [15]. We include both the reported and reproduced performance of DINOSAUR for fair comparison.", "description": "This table compares the performance of the proposed self-modulating slot attention method with the baseline method, DINOSAUR, on two synthetic datasets, MOVI-C and MOVI-E.  The metrics used are FG-ARI, mBO, and mIoU.  The table shows that the proposed method outperforms DINOSAUR on all metrics across both datasets.", "section": "4 Experiments"}, {"figure_path": "52PTSrAQQM/tables/tables_6_2.jpg", "caption": "Table 2: Comparison with DINOSAUR [33] on real-world datasets: COCO [29] and VOC [14]. We include both the reported and reproduced performance of DINOSAUR for fair comparison.", "description": "This table compares the performance of the proposed method with the DINOSAUR baseline on two real-world datasets: COCO and VOC.  It presents results for FG-ARI, mBO (mean best overlap, calculated using both semantic and instance segmentation ground truth), and mIoU (mean Intersection over Union).  The table includes both the reported results from the original DINOSAUR paper and the authors' own reproduction of those results, to ensure a fair comparison.", "section": "4.1 Experimental Settings"}, {"figure_path": "52PTSrAQQM/tables/tables_7_1.jpg", "caption": "Table 3: Comparison with state of the arts [33] on COCO [29], VOC [14], MOVI-C [15], and MOVI-E [15]. Performances of Slot Attention on MOVI-C and -E are reproduced by [33] and that of SLATE by [19]. Those on COCO and VOC are from [46].", "description": "This table compares the performance of the proposed self-modulating slot attention method with several state-of-the-art object-centric learning methods across four benchmark datasets: COCO, VOC, MOVI-C, and MOVI-E.  The metrics used for comparison are FG-ARI, mBO, and mIoU.  The results demonstrate the superiority of the proposed method, particularly on the more challenging COCO and VOC datasets.", "section": "4.3 Qualitative Results"}, {"figure_path": "52PTSrAQQM/tables/tables_8_1.jpg", "caption": "Table 5: Comparison with DINOSAUR [33] using six iterations in slot attention on COCO [29].", "description": "This table compares the performance of the proposed self-modulating slot attention model with the DINOSAUR baseline model on the COCO dataset.  Both models use six iterations of slot attention. The table shows that the proposed model significantly outperforms DINOSAUR in terms of FG-ARI and mBO, demonstrating the effectiveness of the proposed top-down pathway.", "section": "4.2 Quantitative Analysis"}, {"figure_path": "52PTSrAQQM/tables/tables_8_2.jpg", "caption": "Table 4: Comparison between different code-book sizes on COCO [29].", "description": "This table shows the performance of the proposed self-modulating slot attention model on the COCO dataset [29] using different codebook sizes (E = 128, 256, 512, 1024). The results are measured using two metrics: FG-ARI and mBO. The best performance is achieved with a codebook size of 512.", "section": "4.4 In-depth Analysis"}, {"figure_path": "52PTSrAQQM/tables/tables_9_1.jpg", "caption": "Table 6: Ablation studies on COCO dataset for each module consisting of the proposed top-down pathway: channel-wise modulation (m\u00b2), vector quantization (VQ), spatial-wise modulation (m\u00b3), and shifting attention map (shift).", "description": "This table presents the ablation study results conducted on the COCO dataset.  It shows the performance of the model when different components of the proposed top-down pathway are removed. The columns represent the inclusion (\u2713) or exclusion of specific modules: channel-wise modulation, vector quantization, spatial-wise modulation, and attention map shifting. The last row shows the full model's performance, while the others progressively remove components to assess their individual contributions.", "section": "4.4 In-depth Analysis"}, {"figure_path": "52PTSrAQQM/tables/tables_14_1.jpg", "caption": "Table A7: Comparison with slot attention [30] on CLEVR6", "description": "This table shows the comparison result between the reproduced slot attention and the proposed method with top-down pathway on CLEVR6 dataset.  The metrics used are FG-ARI and mBO. The results show that while FG-ARI decreases, mBO shows significant improvement, which indicates the robustness of the proposed method. ", "section": "Additional Experiments"}, {"figure_path": "52PTSrAQQM/tables/tables_14_2.jpg", "caption": "Table 2: Comparison with DINOSAUR [33] on real-world datasets: COCO [29] and VOC [14]. We include both the reported and reproduced performance of DINOSAUR for fair comparison.", "description": "This table compares the performance of the proposed method with the DINOSAUR baseline on two real-world datasets, COCO and VOC.  It includes both the originally reported results from the DINOSAUR paper and the authors' reproduction of those results to ensure a fair comparison.  The metrics used are FG-ARI, mBO, and mIoU, providing a comprehensive evaluation of object discovery performance on more complex datasets than the synthetic ones used in Table 1.", "section": "4.1 Experimental Settings"}, {"figure_path": "52PTSrAQQM/tables/tables_14_3.jpg", "caption": "Table 2: Comparison with DINOSAUR [33] on real-world datasets: COCO [29] and VOC [14]. We include both the reported and reproduced performance of DINOSAUR for fair comparison.", "description": "This table compares the performance of the proposed self-modulating slot attention method with the DINOSAUR baseline on two real-world object-centric learning datasets: COCO and VOC.  It shows the FG-ARI, mBO, and mIoU metrics for both the reported results from the original DINOSAUR paper and reproduced results. The comparison highlights the improvement achieved by incorporating the top-down pathway into the slot attention mechanism.", "section": "4.1 Experimental Settings"}]