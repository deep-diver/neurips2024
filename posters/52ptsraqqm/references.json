{"references": [{"fullname_first_author": "F. Locatello", "paper_title": "Object-centric learning with slot attention", "publication_date": "2020-12-01", "reason": "This paper introduces the foundational slot attention mechanism, which is extended in the current work to incorporate top-down information."}, {"fullname_first_author": "M. Seitzer", "paper_title": "Bridging the gap to real-world object-centric learning", "publication_date": "2023-01-01", "reason": "This paper provides a strong baseline method (DINOSAUR) for comparison, using a similar architecture to the current work, but without top-down information."}, {"fullname_first_author": "J. Jiang", "paper_title": "Object-centric slot diffusion", "publication_date": "2023-01-01", "reason": "This paper explores a different approach to object-centric learning using diffusion models, showing the state-of-the-art in the field that the current paper compares to."}, {"fullname_first_author": "A. Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-01-01", "reason": "This paper introduces the vision transformer (ViT) architecture, which is a key component of the encoder used in the current work, showing its effectiveness for image representation."}, {"fullname_first_author": "G. Singh", "paper_title": "Illiterate dall-e learns to compose", "publication_date": "2021-01-01", "reason": "This paper introduces an autoregressive decoder for slot attention, which is used in the current work to improve the quality of slot representations."}]}