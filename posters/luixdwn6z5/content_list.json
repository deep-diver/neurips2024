[{"type": "text", "text": "Risk-Sensitive Control as Inference with R\u00e9nyi Divergence ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kaito Ito The University of Tokyo kaito@g.ecc.u-tokyo.ac.jp ", "page_idx": 0}, {"type": "text", "text": "Kenji Kashima Kyoto University kk@i.kyoto-u.ac.jp ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper introduces the risk-sensitive control as inference (RCal) that extends CaI by using R\u00e9nyi divergence variational inference. RCaI is shown to be equivalent to log-probability regularized risk-sensitive control, which is an extension of the maximum entropy (MaxEnt) control. We also prove that the risk-sensitive optimal policy can be obtained by solving a soft Bellman equation, which reveals several equivalences between RCa1, MaxEnt control, the optimal posterior for CaI, and linearly-solvable control. Moreover, based on RCaI, we derive the risk-sensitive reinforcement learning (RL) methods: the policy gradient and the soft actor-critic. As the risk-sensitivity parameter vanishes, we recover the risk-neutral CaI and RL, which means that RCaI is a unifying framework. Furthermore, we give another risksensitive generalization of the MaxEnt control using R\u00e9nyi entropy regularization. We show that in both of our extensions, the optimal policies have the same structure even though the derivations are very different. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Optimal control theory is a powerful framework for sequential decision making [1]. In optimal control problems, one seeks to find a control policy that minimizes a given cost functional and typically assumes the full knowledge of the system's dynamics. Optimal control with unknown or partially known dynamics is called reinforcement learning (RL) [2], which has been successfully applied to highly complex and uncertain systems, e.g., robotics [3], self-driving vehicles [4]. However, solving optimal control and RL problems is still challenging, especially for continuous spaces. ", "page_idx": 0}, {"type": "text", "text": "Control as Inference (Cal), which connects optimal control and Bayesian inference, is a promising paradigm for overcoming the challenges of RL [5]. In CaI, the optimality of a state and control trajectory is defined by introducing optimality variables rather than explicit costs. Consequently, an optimal control problem can be formulated as a probabilistic inference problem. In particular, maximum entropy (MaxEnt) control [6, 7] is equivalent to a variational inference problem using the Kullback-Leibler (KL) divergence. MaxEnt control has entropy regularization of a control policy, and as a result, the optimal policy is stochastic. Several works have revealed the advantages of the regularization such as robustness against disturbances [8], natural exploration induced by the stochasticity [7, 9], fast convergence of the MaxEnt policy gradient method [10]. ", "page_idx": 0}, {"type": "text", "text": "On the other hand, the KL divergence is not the only option available for variational inference. In [11], the variational inference was extended to R\u00e9nyi $\\alpha$ -divergence [12], which is a rich family of divergences including the KL divergence. Similar to the traditional variational inference, this extension optimizes a lower bound of the evidence, which is called the variational R\u00e9nyi bound. The parameter $\\alpha$ of R\u00e9nyi divergence controls the balance between mass-covering and zero-forcing effects for approximate inference [13]. However, if we use R\u00e9nyi divergence for CaI, it remains unclearhow $\\alpha$ affects the optimal policy, and a natural question arises: what objective does CaI using Renyi divergence optimize? ", "page_idx": 0}, {"type": "text", "text": "1. We reveal that CaI with R\u00e9nyi divergence solves a log-probability (LP) regularized risksensitive control problem with exponential utility [14] (Theorem 2). The order parameter $\\alpha$ of R\u00e9nyi divergence plays a role of the risk-sensitivity parameter, which determines whether the resulting policy is risk-averse or risk-seeking. Based on the result, we refer to CaI using R\u00e9nyi divergence as risk-sensitive CaI (RCaI). Since R\u00e9nyi divergence includes the KL divergence, RCaI is a unifying framework of CaI. Additionally, we show that the risk-sensitive optimal policy takes the form of the Gibbs distribution whose energy is given by the Q-function, which can be obtained by solving a soft Bellman equation (Theorem 3). Furthermore, this reveals several equivalence results between RCaI, MaxEnt control, the optimal posterior for CaI, and linearly-solvable control [15, 16]. ", "page_idx": 1}, {"type": "text", "text": "2. Based on RCal, we derive risk-sensitive RL methods. First, we provide a policy gradient method [17-19] for the regularized risk-sensitive RL (Proposition 7). Next, we derive the risk-sensitive counterpart of the soft actor-critic algorithm [7] through the maximization of the variational R\u00e9nyi bound (Subsection 4.2). As the risk-sensitivity parameter vanishes, the proposed methods converge to REINFORCE [19] with entropy regularization and risk-neutral soft actor-critic [7], respectively. One of their advantages over other risksensitive approaches, including distributional RL [20, 21], is that they require only minor modifications to the standard REINFORCE and soft actor-critic. The behavior of the risk-sensitive soft actor-critic is examined via an experiment.   \n3. Although the risk-sensitive control induced by RCal has LP regularization of the policy, it is not entropy, unlike the MaxEnt control with the Shannon entropy regularization. To bridge this gap, we provide another risk-sensitive generalization of the MaxEnt control using R\u00e9nyi entropy regularization. We prove that the resulting optimal policy and the Bellman equation have the same structure as the LP regularized risk-sensitive control (Theorem 6). The derivation differs significantly from that for the LP regularization, and for the analysis, we establish the duality between exponential integrals and Renyi entropy (Lemma 5). ", "page_idx": 1}, {"type": "text", "text": "The established relations between several control problems in this paper are summarized in Fig. 1. ", "page_idx": 1}, {"type": "text", "text": "Related work  The duality between control and inference has been extensively studied [15, 22-26]. Inspired by Cal, [27, 28] reformulated model predictive control (MPC) as a variational inference problem. In [29], variational inference MPC using Tsallis divergence, which is equivalent to R\u00e9nyi divergence, was proposed. The difference between our results and theirs is that variational inference MPC infers feedforward optimal control while RCaI infers feedback optimal control. Consequently, the equivalence of risk-sensitive control ", "page_idx": 1}, {"type": "image", "img_path": "LUIXdWn6Z5/tmp/0123a81eabf78a51ac9c51f18e2ed2947ac09e80ced9b20eb4f56080fb3f6376.jpg", "img_caption": ["Figure 1: Relations of control problems. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "and Tsallis variational inference MPC is not derived, unlike RCaI. The work [30] proposed an EM-style algorithm for RL based on Cal, where the resulting policy is risk-seeking. However, riskaverse policies cannot be derived from Cal by this approach. Our framework provides the equivalence between CaI and risk-sensitive control both for risk-seeking and risk-averse cases. ", "page_idx": 1}, {"type": "text", "text": "Risk-averse policies are known to yield robust control [31, 32], and risk-seeking policies are useful for balancing exploration and exploitation for RL_ [33]. Because of these merits, many efforts have been devoted to risk-sensitive RL [19, 34-36]. In [37], risk-sensitive RL with Shannon entropy regularization was investigated. However, their theoretical results are valid only for almost risk-neutral cases. Our results imply that LP and R\u00e9nyi entropy regularization are suitable for the risk-sensitive RL. ", "page_idx": 1}, {"type": "text", "text": "In [16], risk-sensitive control whose control cost is defined by R\u00e9nyi divergence was investigated, and it was shown that the associated Bellman equation can be linearized. However, it is assumed that the transition distribution can be controlled as desired, which is not satisfied in general as pointed out in [38]. On the other hand, our result shows that when the dynamics is deterministic, LP and R\u00e9nyi entropy regularized risk-sensitive control problems are linearly solvable without the full controllability assumption of the transition distribution. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Notation  For simplicity, by abuse of notation, we write the density (or probability mass) functions of random variables $x,y$ as $p(x),p(y)$ , and the expectation with respect to $p(x)$ is denoted by $\\mathbb{E}_{p(x)}$ For a set $S$ , the set of all densities on $S$ is denoted by ${\\mathcal{P}}(S)$ . R\u00e9nyi entropy and divergence with parameter $\\alpha>0$ $\\alpha\\neq1$ are defined as $\\begin{array}{r}{\\mathcal{H}_{\\alpha}(p):=\\frac{1}{\\alpha(1-\\alpha)}\\log\\left[\\int_{\\{u:p(u)>0\\}}p(u)^{\\alpha}\\mathrm{d}u\\right]\\!,}\\end{array}$ $D_{\\alpha}(p_{1}\\|p_{2}):=$ $\\begin{array}{r}{\\frac{1}{\\alpha-1}\\log\\left[\\int_{\\{u:p_{1}(u)p_{2}(u)>0\\}}p_{1}(u)^{\\alpha}p_{2}(u)^{1-\\alpha}\\mathrm{d}u\\right]}\\end{array}$ For te faetor $\\scriptstyle{\\frac{1}{\\alpha(1-\\alpha)}}$ of $\\mathcal{H}_{\\alpha}$ , we follow [39, 40] because this choice is convenient for the analysis in Subsection 3.2 rather than another common choice $1/(1-\\alpha)$ .We formally extend the definition of $\\mathcal{H}_{\\alpha}$ to $\\alpha\\ <\\ 0$ . Denote the Shannon entropy and KL divergence by $\\dot{\\mathcal{H}}_{1}(p)$ \uff0c $D_{1}(p_{1}\\|p_{2})$ , respectively because $\\begin{array}{r}{\\operatorname*{lim}_{\\alpha\\to1}\\mathcal{H}_{\\alpha}(p)=\\mathcal{H}_{1}(p)}\\end{array}$ $\\begin{array}{r}{\\operatorname*{lim}_{\\alpha\\rightarrow1}D_{\\alpha}(p_{1}\\|p_{2})\\,\\stackrel{*}{=}\\,D_{1}(p_{1}\\|p_{2})}\\end{array}$ : For further properties of the Renyi entropy and divergence, see e.g., [41]. The set of integers $\\{k,k+1,\\ldots,s\\}$ $k<s$ is denoted by $[\\![k,s]\\!]$ . A sequence $\\{x_{k},x_{k+1},\\ldots,x_{s}\\}$ is denoted by $x_{k:s}$ . The set of non-negative real numbers is denoted by $\\mathbb{R}_{\\geq0}$ ", "page_idx": 2}, {"type": "text", "text": "2   Brief introduction to control as inference ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "First, we briefly introduce the framework of CaI. For the detailed derivation, see Appendix A and [5]. Throughout the paper, $x_{t}$ and $u_{t}$ denote $\\mathbb{X}$ -valued state and $\\mathbb{U}$ -valued control variables at time $t$ , respectively, where $\\mathbb{X}\\subseteq\\mathbb{R}^{n_{x}}$ $\\mathbb{U}\\subseteq\\mathbb{R}^{n_{u}}$ , and $\\mu_{L}(\\mathbb{U})\\,>\\,0$ . Here, $\\mu_{L}$ denotes the Lebesgue measure on $\\mathbb{R}^{n_{u}}$ The initial distribution is $p(x_{0})$ , and the transition density is denoted by $p(x_{t+1}|x_{t},u_{t})$ , which depends only on the current state and control input. Let $T~>~0$ be a finite time horizon. CaI connects control and probabilistic inference problems by introducing optimalityvariables $\\mathcal{O}_{t}\\,\\in\\,\\{0,1\\}$ as in Fig. 2. For $c_{t}:\\mathbb{X}\\times\\mathbb{U}\\to\\mathbb{R}_{>0}$ \uff0c $c_{T}:\\mathbb{X}\\to\\mathbb{R}_{>0}$ , which will serve as cost functions, the distribution of $O_{t}$ is given by $p(\\mathcal{O}_{t}=1|x_{t},\\stackrel{-}{u}_{t})=\\exp(-c_{t}\\overline{{(x}}_{t},u_{t}))$ \uff0c $t\\in[0,T-1]$ and $p(\\mathcal{O}_{T}=1|x_{T})=\\exp(-c_{T}(x_{T}\\bar{)})$ .If $O_{t}=1$ , then $(x_{t},u_{t})$ at time $t$ is said to be \u201coptimal.\" The control posterior $p(u_{t}|x_{t},\\mathcal{O}_{t:T}=1)$ is called the optimal policy. Let the prior of $u_{t}$ be uniform: $p(u_{t})=1/\\bar{\\mu}_{L}(\\mathbb{U}),\\forall\\bar{u}_{t}\\in\\mathbb{U}$ .Althoughthis choice is common for Cal, the arguments in this paper may be extended to non-uniform priors. Then, for the graphical model in Fig. 2, the distribution of the optimal state and control input trajectory $\\tau:=(x_{0:T},u_{0:T-1})$ satisfies ", "page_idx": 2}, {"type": "image", "img_path": "LUIXdWn6Z5/tmp/0a10a408611141a8ea385624f91714bafeb7c7870eec1e74fb7544c1c8147793.jpg", "img_caption": ["Figure 2: Graphical model for Cal. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{p(\\tau|\\mathcal{O}_{0:T}=1)\\propto\\displaystyle\\left[p(x_{0})\\prod_{t=0}^{T-1}p(x_{t+1}|x_{t},u_{t})\\right]\\left[p(\\mathcal{O}_{T}=1|x_{T})\\prod_{t=0}^{T-1}p(\\mathcal{O}_{t}=1|x_{t},u_{t})\\right]}}\\\\ {{=\\displaystyle\\left[p(x_{0})\\prod_{t=0}^{T-1}p(x_{t+1}|x_{t},u_{t})\\right]\\exp\\left(-c_{T}(x_{T})-\\sum_{t=0}^{T-1}c_{t}(x_{t},u_{t})\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "For notational simplicity, we will $\\mathrm{drop}=1$ for $O_{t}$ in the remainder of this paper. ", "page_idx": 2}, {"type": "text", "text": "The optimal policy $p(u_{t}|x_{t},\\mathcal{O}_{t:T})$ can be computed in a recursive manner. To this end, defne ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathsf{Q}_{t}(x_{t},u_{t}):=-\\log\\frac{p(\\mathcal{O}_{t:T}|x_{t},u_{t})}{\\mu_{L}(\\mathbb{U})},\\:\\:\\forall_{t}(x_{t}):=-\\log p(\\mathcal{O}_{t:T}|x_{t}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "which play a role of value functions. Then, the following result holds. ", "page_idx": 2}, {"type": "text", "text": "Proposition 1. Assume that $\\mu_{L}(\\mathbb{U})<\\infty$ and let $\\mathsf{c}_{t}(x_{t},u_{t})\\,:=\\,c_{t}(x_{t},u_{t})+\\log\\mu_{L}(\\mathbb{U}).$ Assume further the existence of density functions $p(x_{0})$ and $p(x_{t+1}|x_{t},u_{t})$ for any $t\\in[0,T-1]^{1}$ . Then, $i t$ holds that ", "page_idx": 2}, {"type": "equation", "text": "$$\np(u_{t}|x_{t},\\mathcal{O}_{t:T}=1)=\\exp\\left(-\\mathsf{Q}_{t}(x_{t},u_{t})+\\mathsf{V}_{t}(x_{t})\\right),\\;\\;\\forall x_{t}\\in\\mathbb{X},\\;\\forall u_{t}\\in\\mathbb{U},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\vee_{t}(x_{t})=-\\log\\left[\\int_{\\mathbb{U}}\\exp(-\\mathsf{Q}_{t}(x_{t},u_{t}))\\mathrm{d}u_{t}\\right],\\,\\forall t\\in\\mathbb{J}0,T-1\\mathbb{J},\\,\\,\\mathsf{V}_{T}(x_{T})=c_{T}(x_{T}),}\\\\ &{\\mathsf{Q}_{t}(x_{t},u_{t})=\\mathsf{c}_{t}(x_{t},u_{t})-\\log\\mathbb{E}_{p(x_{t+1}|x_{t},u_{t})}\\left[\\exp(-\\mathsf{V}_{t+1}(x_{t+1}))\\right],\\,\\forall t\\in\\mathbb{J}0,T-1\\mathbb{J}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The recursive computation (4), (5) is similar to the Bellman equation for the risk-seeking control. However, it is not still clear what kind of performance index the optimal trajectory $p(\\tau|\\mathcal{O}_{t:T})$ optimizes because (4) does not coincide with that of the conventional risk-seeking control. An indirect way to make this clear is variational inference. Let us consider finding the closest trajectory distribution $p^{\\pi}(\\tau)$ to the optimal distribution $p(\\tau|\\mathcal{O}_{0:T})$ . The variational distribution is chosen as ", "page_idx": 3}, {"type": "equation", "text": "$$\np^{\\pi}(\\tau)=p(x_{0})\\prod_{t=0}^{T-1}p(x_{t+1}\\vert x_{t},u_{t})\\pi_{t}(u_{t}\\vert x_{t}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\pi_{t}(\\cdot|x_{t})\\in\\mathcal{P}(\\mathbb{U})$ is the conditional density of $u_{t}$ given $x_{t}$ and corresponds to a control policy. Then, the minimization of the KL divergence $D_{1}(p^{\\pi}(\\tau)||p(\\tau|\\mathcal{O}_{0:T}))$ is known to be equivalent to the following MaxEnt control problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{minimize}_{\\{\\pi_{t}\\}_{t=0}^{T-1}}\\;\\mathbb{E}_{p^{\\pi}(\\tau)}\\left[c_{T}(x_{T})+\\sum_{t=0}^{T-1}\\Bigl(c_{t}(x_{t},u_{t})-\\mathcal{H}_{1}(\\pi_{t}(\\cdot|x_{t}))\\Bigr)\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Especially  when  the  system $p(x_{t+1}|x_{t},u_{t})$ is   deterministic, the   minimum  value  of $\\bar{D_{1}}\\bar{(p^{\\pi}(\\tau)}||p(\\tau|\\mathcal{O}_{0:T}))$ is $0$ , and the posterior $p(\\boldsymbol{u}_{t}|\\boldsymbol{x}_{t},\\mathcal{O}_{t:T})$ yields the optimal control of (7). As mentioned in Introduction, this work uses R\u00e9nyi divergence rather than the KL divergence. Moreover, we characterize the optimal posterior $p(u_{t}|x_{t},\\mathcal{O}_{t:T})$ more directly even for stochastic systems. ", "page_idx": 3}, {"type": "text", "text": "3  Control as Renyi divergence variational inference ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we address the question of what kind of control problem is solved by Cal with Renyi divergence and characterize the optimal policy. ", "page_idx": 3}, {"type": "text", "text": "3.1  Equivalence between CaI with Renyi divergence and risk-sensitive control ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Let $\\eta>-1$ $\\eta\\neq0$ . Then, Cal using Renyi variational inference is formulated as the minimization of $D_{1+\\eta}(p^{\\pi}(\\tau)\\Vert p(\\tau\\vert\\mathcal{O}_{0:T}))$ withrespect to $p^{\\pi}$ in (6). Now, we have ", "page_idx": 3}, {"type": "equation", "text": "$$\nD_{1+\\eta}(p^{\\pi}\\|p(\\cdot|\\mathcal{O}_{0:T}))=\\underbrace{\\frac{1}{\\eta}\\log\\left[\\int p^{\\pi}(\\tau)^{1+\\eta}p(\\tau,\\mathcal{O}_{0:T})^{-\\eta}\\mathrm{d}\\tau\\right]}_{-(\\mathrm{Variational}\\,\\mathrm{Renyi}\\,\\mathrm{bound})}+\\log p(\\mathcal{O}_{0:T}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "That is, CaI with R\u00e9nyi divergence is equivalent to maximizing the above variational R\u00e9nyi bound. Moreover, by (1), it holds that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{og}\\left[\\displaystyle\\int p^{\\pi}(\\tau)^{1+\\eta}p(\\tau,\\mathcal{O}_{0:T})^{-\\eta}\\mathrm{d}\\tau\\right]}\\\\ &{=\\log\\left[\\displaystyle\\int p^{\\pi}(\\tau)\\left(\\frac{p(x_{0})\\prod_{t=0}^{T-1}p(x_{t+1}\\vert x_{t},u_{t})\\pi_{t}(u_{t}\\vert x_{t})}{\\displaystyle\\frac{1}{\\mu_{L}(\\tau)}p(x_{0})\\left[\\prod_{t=0}^{T-1}p(x_{t+1}\\vert x_{t},u_{t})\\right]\\exp\\left(-c_{T}(x_{T})-\\sum_{t=0}^{T-1}c_{t}(x_{t},u_{t})\\right)}\\right)^{\\eta}\\mathrm{d}\\tau\\right]}\\\\ &{=\\log\\left[\\displaystyle\\int p^{\\pi}(\\tau)\\exp\\!\\left(\\eta c_{T}(x_{T})+\\eta\\sum_{t=0}^{T-1}\\!\\left(c_{t}(x_{t},u_{t})+\\log\\pi_{t}(u_{t}\\vert x_{t})\\right)\\right)\\mathrm{d}\\tau\\right]+\\eta\\log\\mu_{L}(\\mathbb{U}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Consequently, we obtain the first equivalence result in this paper. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2. Suppose that the assumptions in Proposition $^{\\,l}$ hold. Then, for any $\\eta>-1$ \uff0c $\\eta\\neq0$ the minimization of $D_{1+\\eta}(p^{\\pi}\\|p(\\cdot|\\mathcal{O}_{0:T}=1))$ with respect to $p^{\\pi}$ in (6) is equivalent to ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{minimize}_{\\{\\pi_{t}\\}_{t=0}^{T-1}}\\ \\frac{1}{\\eta}\\log\\mathbb{E}_{p^{\\pi}(\\tau)}\\left[\\exp\\left(\\eta c_{T}(x_{T})+\\eta\\sum_{t=0}^{T-1}\\Bigl(c_{t}(x_{t},u_{t})+\\log\\pi_{t}(u_{t}|x_{t})\\Bigr)\\right)\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Problem (9) is a risk-sensitive control problem with the log-probability regularization $\\log\\pi_{t}(u_{t}|x_{t})$ of the control policy. Let $\\eta\\Phi(\\tau)$ be the exponent in (9). Then, $\\begin{array}{r}{\\frac{1}{\\eta}\\log\\mathbf{\\dot{E}}[\\exp(\\eta\\Phi(\\tau))]=\\mathbf{\\check{E}}[\\Phi(\\tau)]+}\\end{array}$ $\\begin{array}{r}{\\frac{\\eta}{2}\\mathrm{Var}[\\Phi(\\tau)]+O(\\eta^{2})}\\end{array}$ where $\\mathrm{Var}[\\cdot]$ denotes the variance [42]. Hence, $\\eta>0$ (resp. $\\eta<0$ leads to risk-averse (resp. risk-seeking) policies. As $\\eta$ goes to zero, the objective in (9) converges to the risk-neutral MaxEnt control problem (7). ", "page_idx": 4}, {"type": "text", "text": "3.2  Derivation of optimal control and further equivalence results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this subsection, we derive the optimal policy of (9) and give its characterizations. For the analysis we do not need the non-negativity of the cost $c_{t}$ . We only sketch the derivation, and the detailed proof is given in Appendix B. Similar to the conventional optimal control problems, we adopt the dynamic programming. Another approach based on variational inference will be given in Subsection 4.2. Define the optimal (state-)value function $V_{t}:\\mathbb{X}\\to\\mathbb{R}$ and the Q-function $\\mathcal{Q}_{t}:\\mathbb{X}\\times\\mathbb{U}\\rightarrow\\mathbb{R}$ asfollows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nV_{t}(x_{t}):=\\operatorname*{inf}_{\\{\\pi_{s}\\}_{s=t}^{T-1}}\\frac{1}{\\eta}\\log\\mathbb{E}_{p^{\\pi}(\\tau|x_{t})}\\left[\\exp\\left(\\eta c_{T}(x_{T})+\\eta\\sum_{s=t}^{T-1}(c_{s}(x_{s},u_{s})+\\log\\pi_{s}(u_{s}|x_{s}))\\right)\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{Q}_{t}(x_{t},u_{t}):=c_{t}(x_{t},u_{t})+\\frac{1}{\\eta}\\log\\mathbb{E}_{p(x_{t+1}|x_{t},u_{t})}\\left[\\exp\\bigl(\\eta V_{t+1}(x_{t+1})\\bigr)\\right],\\quad t\\in[0,T-1],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and $V_{T}(x_{T}):=c_{T}(x_{T})$ . Then, it can be shown that the Bellman equation for Problem (9) is ", "page_idx": 4}, {"type": "equation", "text": "$$\nV_{t}(x_{t})=-\\log\\left[\\int_{\\mathbb{U}}\\exp\\left(-\\mathcal{Q}_{t}(x_{t},u^{\\prime})\\right)\\mathrm{d}u^{\\prime}\\right]+\\operatorname*{inf}_{\\pi_{t}(\\cdot|x_{t})\\in\\mathcal{P}(\\mathbb{U})}D_{1+\\eta}(\\pi_{t}(\\cdot|x_{t})\\|\\pi_{t}^{*}(\\cdot|x_{t})),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\pi_{t}^{*}(u_{t}|x_{t}):=\\exp\\left(-\\,\\mathcal{Q}_{t}(x_{t},u_{t})\\right)/\\mathcal{Z}_{t}(x_{t})$ , and the normalizing constant is assumed to fulfill $\\begin{array}{r}{\\mathcal{Z}_{t}(x_{t})\\,:=\\,\\int_{\\mathbb{U}}\\exp\\left(-\\,\\mathcal{Q}_{t}(x_{t},u^{\\prime})\\right)\\mathrm{d}u^{\\prime}\\,<\\,\\infty}\\end{array}$ Since $D_{1+\\eta}(\\pi_{t}(\\cdot|x_{t})\\|\\bar{\\pi}_{t}^{*}(\\cdot|x_{t}))$ attains its minimum value O if and only if $\\pi_{t}(\\cdot|x_{t})=\\pi_{t}^{*}(\\cdot|x_{t})$ , the unique optimal policy that minimizes the right-hand side of (12) is given by $\\pi_{t}^{*}(\\cdot|x_{t})$ and ", "page_idx": 4}, {"type": "equation", "text": "$$\nV_{t}(x_{t})=-\\log\\left[\\int_{\\mathbb{U}}\\exp\\left(-\\mathcal{Q}_{t}(x_{t},u^{\\prime})\\right)\\mathrm{d}u^{\\prime}\\right],\\;\\;\\pi_{t}^{*}(u_{t}|x_{t})=\\exp\\left(-\\mathcal{Q}_{t}(x_{t},u_{t})+V_{t}(x_{t})\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Because of the softmin operation above, the left equation in (13) is called the soft Bellman equation. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.Assume that $\\begin{array}{r}{\\int_{\\mathbb{U}}\\exp\\left(-\\mathscr{Q}_{t}(x,u^{\\prime})\\right)\\mathrm{d}u^{\\prime}<\\infty}\\end{array}$ holds for any $t\\in[0,T-1]$ and $x\\in\\mathbb{X}$ Let $\\eta>-1,$ $\\eta\\neq0$ .Then, the unique optimal policy of Problem (9) is given by (13). Especially when the dynamics is deterministic, i.e., $p(\\Bar{x_{t+1}}|x_{t},\\Bar{u_{t}})=\\Bar{\\delta}(x_{t+1}-\\Bar{f_{t}}(x_{t},\\Bar{u_{t}}))$ for some $\\bar{f}_{t}\\colon\\mathbb{X}\\times\\dot{\\mathbb{U}}\\rightarrow\\mathbb{X}$ andtheDiracdeltafunction $\\delta$ ,it holds that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Q}_{t}(x_{t},u_{t})=c_{t}(x_{t},u_{t})+V_{t+1}\\left(\\bar{f}_{t}(x_{t},u_{t})\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and the optimal policy of the MaxEnt control problem (7) solves the $L P$ -regularized risk-sensitive control problem (9) for any $\\eta>-1$ $\\eta\\neq0$ $\\diamondsuit$ ", "page_idx": 4}, {"type": "text", "text": "Assumption $\\begin{array}{r}{\\int_{\\mathbb{U}}\\exp\\left(-\\mathscr{Q}_{t}(x,u^{\\prime})\\right)\\mathrm{d}u^{\\prime}\\,<\\,\\infty}\\end{array}$ is satisfied for example when $c_{t}$ is bounded for any $t\\in[0,T]$ and $\\mu_{L}(\\mathbb{U})<\\infty$ . The linear quadratic setting also fulfills this assumption; see (16). ", "page_idx": 4}, {"type": "text", "text": "Theorem 3 suggests several equivalence results: ", "page_idx": 4}, {"type": "text", "text": "RCaI and MaxEnt control for deterministic systems. First, we emphasize that even though the equivalence between unregularized risk-neutral and risk-sensitive controls for deterministic systems is already known, our equivalence result for MaxEnt and regularized risk-sensitive controls is nontrivial. This is because the regularized policy $\\pi_{t}^{*}$ makes a system stochastic even though the original system is deterministic, and for stochastic systems, the unregularized risk-sensitive control does not coincide with the risk-neutral control. This implies that the optimal randomness introduced by the regularization does not affect the risk sensitivity of the policy. This provides insight into the robustness of MaxEnt control [8]. Note that [43] mentioned that the MaxEnt control objective can be reconstructed by the risk-sensitive control objective under the heuristic assumption that the cost follows a uniform distribution. However, this assumption is not satisfied in general. Our equivalence result does not require such an unrealistic assumption. ", "page_idx": 4}, {"type": "text", "text": "RCaI and optimal posterior. Although the optimal posterior $p(u_{t}|x_{t},\\mathcal{O}_{t:T})$ yields the MaxEnt control for deterministic systems as mentioned in Section 2, it is not known what objective $p(u_{t}|x_{t},\\mathcal{O}_{t:T})$ ", "page_idx": 4}, {"type": "text", "text": "optimizes for stochastic systems. Theorem 3 gives a new characterization of $p(u_{t}|x_{t},\\mathcal{O}_{t:T})$ . By formally substituting $\\eta=-1$ into (11), the Bellman equation for computing $\\pi_{t}^{*}$ becomes (4), (5) for the optimal posterior $p(u_{t}|x_{t},\\mathcal{O}_{t:T})$ . Note that even if the cost function $c_{t}$ in (9) is replaced by $c_{t}$ in Proposition 1, $\\{\\pi_{t}^{*}\\}$ is still optimal. Therefore, by taking the limit as $\\eta\\searrow-1$ , the policy $\\pi_{t}^{*}(u_{t}|x_{t})$ in Theorem 3 converges to $p(\\bar{u}_{t}|x_{t},\\mathcal{O}_{t:T})$ , and in this sense, the policy $p(u_{t}|x_{t},\\mathcal{O}_{t:T})$ is risk-seeking. ", "page_idx": 5}, {"type": "text", "text": "Corollary 4. Under the assumptions in Proposition $^{\\,l}$ it holdsthat ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\eta\\searrow-1}\\pi_{t}^{*}(u_{t}|x_{t})=\\exp(-\\,Q_{t}(x_{t},u_{t})+V_{t}(x_{t}))=p(u_{t}|x_{t},\\mathcal{O}_{t:T}=1),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $V_{t}$ and $\\mathcal{Q}_{t}$ are given by (11), (13) with $\\eta=-1$ ", "page_idx": 5}, {"type": "text", "text": "RCaI for deterministic systems and linearly-solvable control. For deterministic systems, by thetransformation $E_{t}(x_{t}):=\\exp(-V_{t}(x_{t}))$ , the Bellman equation (14) becomes linear: $E_{t}(x_{t})=$ $\\begin{array}{r}{\\int\\mathrm{exp}(-c_{t}(x_{t},u^{\\prime}))E_{t+1}(\\bar{f}_{t}(x_{t},\\bar{u^{\\prime}}))\\mathrm{d}u^{\\prime}}\\end{array}$ . That is, when the system is deterministic, the LP-regularized risk-sensitive control, or equivalently, the MaxEnt control is linearly solvable [15, 16, 44], which enables efficient computation of RL. Even for the MaxEnt control, this fact seems not to be mentioned explicitly in the literature. ", "page_idx": 5}, {"type": "text", "text": "RCaI and unregularized risk-sensitive control in linear quadratic setting. Similar to the unregularized and MaxEnt problems [45, 46], Problem (9) with a linear system $p(x_{t+1}|x_{t},u_{t})\\,=$ $\\mathcal{N}(x_{t+1}|A_{t}x_{t}+B_{t}u_{t},\\Sigma_{t})$ and quadratic costs $c_{t}(x_{t},u_{t})\\,=\\,(x_{t}^{\\top}Q_{t}x_{t}\\,+\\,\\overbar{u}_{t}^{\\top}R_{t}{u}_{t})/2$ $c_{T}(x_{T})\\;=\\;$ $x_{T}^{\\top}Q_{T}x_{T}/2$ admits an explicit form of the optimal policy: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\pi_{t}^{*}(u|x)=\\mathcal{N}\\bigg(u\\big|-\\big(R_{t}+B_{t}^{\\top}\\Pi_{t+1}\\big(I-\\eta\\Sigma_{t}\\Pi_{t+1}\\big)^{-1}B_{t}\\big)^{-1}B_{t}^{\\top}\\Pi_{t+1}\\big(I-\\eta\\Sigma_{t}\\Pi_{t+1}\\big)^{-1}A_{t}x,}&{}&\\\\ {\\big(R_{t}+B_{t}\\Pi_{t+1}\\big(I-\\eta\\Sigma_{t}\\Pi_{t+1}\\big)^{-1}B_{t}\\big)^{-1}\\bigg).}&{}&{\\mathrm{~(~L~~1~)~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, $\\mathcal{N}(\\cdot|\\mu,\\Sigma)$ denotes the Gaussian density with mean $\\mu$ and covariance $\\Sigma$ . The definition of $\\Pi_{t}$ and the proof are given in Appendix C. In general, the mean of the regularized risk-sensitive control deviates from the unregularized risk-sensitive control. However, in the linear quadratic Gaussian (LQG) case, the mean of the optimal policy (16) coincides with the optimal control of risk-sensitive LQG control without the regularization [47]. ", "page_idx": 5}, {"type": "text", "text": "3.3  Another risk-sensitive generalization of MaxEnt control via Renyi entropy ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The Shannon entropy regularization $\\mathbb{E}[-\\mathcal{H}_{1}(\\pi_{t}(\\cdot|x_{t}))]$ of the MaxEnt control problem (7) can be rewritten as $\\mathbb{E}[\\log\\pi_{t}(u_{t}|x_{t})]$ . In this sense, the risk-sensitive control (9) is a natural extension of (7). Nevertheless, for the risk-sensitive case, the interpretation of $\\log\\pi_{t}(u_{t}|x_{t})$ as entropy is no longer available. In this subsection, we provide another risk-sensitive extension of the MaxEnt control. Inspired by the R\u00e9nyi divergence utilized so far, we employ R\u00e9nyi entropy regularization: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{minimize}_{\\{\\pi_{t}\\}_{t=0}^{T-1}}\\ \\frac{1}{\\eta}\\log\\mathbb{E}_{p^{\\pi}(\\tau)}\\left[\\exp\\left(\\eta c_{T}(x_{T})+\\eta\\sum_{t=0}^{T-1}\\Bigl(c_{t}(x_{t},u_{t})-\\mathcal{H}_{1-\\eta}(\\pi_{t}(\\cdot|x_{t}))\\Bigr)\\right)\\right],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\eta\\in\\mathbb{R}\\setminus\\{0,1\\}$ , and $\\begin{array}{r}{\\pi_{t}(\\cdot|x)\\,\\in\\,L^{1-\\eta}(\\mathbb{U})\\,:=\\,\\{\\rho\\in\\mathcal{P}(\\mathbb{U})|\\int_{\\mathbb{U}}\\rho(u)^{1-\\eta}\\mathrm{d}u<\\infty\\},\\forall x}\\end{array}$ which implies $|\\mathcal{H}_{1-\\eta}(\\pi_{t}(\\cdot|x_{t}))|<\\infty$ .As $\\eta$ tends to zero, (17) converges to the MaxEnt control problem (7). Define the value function $\\mathcal{V}_{t}$ and the Q-function $\\mathcal{L}_{t}$ associated with (17) like (10) and (11). Then, as in Subsection 3.2, the following Bellman equation holds. The derivation is given in Appendix $\\boldsymbol{\\mathrm E}$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{V}_{t}(x_{t})=\\operatorname*{inf}_{\\pi_{t}\\in L^{1-\\eta}(\\mathbb{U})}\\left\\{\\frac{1}{\\eta}\\log\\left[\\int_{\\mathbb{U}}\\pi_{t}(u^{\\prime}|x_{t})\\exp(\\eta\\mathcal{D}_{t}(x_{t},u^{\\prime}))\\mathrm{d}u^{\\prime}\\right]-\\mathcal{H}_{1-\\eta}(\\pi_{t}(\\cdot|x_{t}))\\right\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For the minimization in (18), we establish the duality between exponential integrals and Renyi entropy like in [40] because the same procedure as for (12) cannot be applied. ", "page_idx": 5}, {"type": "text", "text": "Lemma 5 (Informal). For $\\beta,\\gamma\\in\\mathbb{R}\\setminus\\{0\\}$ such that $\\beta<\\gamma$ and for $g:\\mathbb{U}\\to\\mathbb{R},$ it holds that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{1}{3}\\log\\left[\\int_{\\mathbb{U}}\\exp(\\beta g(u))\\mathrm{d}u\\right]=\\operatorname*{inf}_{\\rho\\in L^{1-\\frac{\\gamma}{\\gamma-\\beta}}(\\mathbb{U})}\\left\\{\\frac{1}{\\gamma}\\log\\left[\\int_{\\mathbb{U}}\\exp(\\gamma g(u))\\rho(u)\\mathrm{d}u\\right]-\\frac{1}{\\gamma-\\beta}\\mathcal{H}_{1-\\frac{\\gamma}{\\gamma-\\beta}}(\\rho)\\right\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and the unique optimal solution that minimizes the right-hand side of (19) is given by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\rho(u)=\\frac{\\exp\\left(-(\\gamma-\\beta)g(u)\\right)}{\\int_{\\mathbb{U}}\\exp(-(\\gamma-\\beta)g(u^{\\prime}))\\mathrm{d}u^{\\prime}},\\ \\forall u\\in\\mathbb{U}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "For the precise statement and the proof, see Appendix D. By applying Lemma 5 with $\\beta=\\eta-1$ $\\gamma=\\eta$ to (18), we obtain the optimal policy of (17) as follows. ", "page_idx": 6}, {"type": "text", "text": "Theorem 6. Assume that $c_{t}$ is bounded below for any $t\\in[0,T]$ Assume further that for any $x\\in\\mathbb{X}$ and $t\\in[0,T-1]$ , it holds that $\\begin{array}{r}{\\int_{\\mathbb{U}}\\exp\\left(-\\mathscr{Q}_{t}(x,u^{\\prime})\\right)\\mathrm{d}u^{\\prime}\\,\\bar{<}\\infty}\\end{array}$ \uff0c $\\begin{array}{r}{\\int_{\\mathbb{U}}\\exp\\left(-(1-\\eta)\\mathcal{Q}_{t}(x,u^{\\prime})\\right)\\mathrm{d}u^{\\prime}<}\\end{array}$ $\\infty$ .Then, the unique optimal policy of Problem (17) is given by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\pi_{t}^{\\star}(u_{t}|x_{t})=\\frac{1}{\\mathcal{Z}(x_{t})}\\exp\\left(-\\mathcal{Q}_{t}(x_{t},u_{t})\\right),\\;\\;\\forall t\\in[0,T-1],\\;\\forall x_{t}\\in\\mathbb{X},\\;\\forall u_{t}\\in\\mathbb{U},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{Z}_{t}(x_{t}):=\\int_{\\mathbb{U}}\\exp(-\\mathcal{Z}_{t}(x_{t},u^{\\prime}))\\mathrm{d}u^{\\prime},}\\end{array}$ and it holds that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{H}_{t}(x_{t})=\\frac{-1}{1-\\eta}\\log\\left[\\int_{\\mathbb{U}}\\exp\\left(-(1-\\eta)\\mathcal{Q}_{t}(x_{t},u^{\\prime})\\right)\\mathrm{d}u^{\\prime}\\right],\\;\\;\\forall t\\in\\mathbb{I}0,T-1\\mathbb{J},\\;\\forall x_{t}\\in\\mathbb{X}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Recall that the LP regularized risk-sensitive optimal control is given by (11), (13) while the R\u00e9nyi entropy regularized control is determined by (21), (22), and ${\\mathcal{Q}}_{t}(x_{t},u_{t})\\;=\\;c_{t}(x_{t},u_{t})\\;+$ $\\begin{array}{r}{\\frac{1}{\\eta}\\log\\mathbb{E}_{p(x_{t+1}\\vert x_{t},u_{t})}\\overline{{\\vert\\exp(\\eta\\mathcal{V}_{t+1}(x_{t+1}))\\vert}}}\\end{array}$ Hence, the onlydifference between thersk-sesitive conmtrols for the LP and R\u00e9nyi regularization is the coeffcient in the soft Bellman equations (13), (22). ", "page_idx": 6}, {"type": "text", "text": "4   Risk-sensitive reinforcement learning via RCaI ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Standard RL methods can be derived from CaI using the KL divergence [5]. In this section, we derive risk-sensitive policy gradient and soft actor-critic methods from RCaI. ", "page_idx": 6}, {"type": "text", "text": "4.1  Risk-sensitive policy gradient ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this subsection, we consider minimizing the cost (9) by a time-invariant policy parameterized as $\\pi_{t}(u|x)=\\pi^{(\\theta)}(u|x)$ $\\theta\\in\\mathbb{R}^{n_{\\theta}}$ . Let $\\begin{array}{r}{C_{\\theta}(\\tau):=c_{T}(x_{T})+\\sum_{t=0}^{T-1}(c_{t}(x_{t},u_{t})+\\log\\pi^{(\\theta)}(u_{t}|x_{t}))}\\end{array}$ and $p_{\\theta}$ be the density of the trajectory $\\tau$ under the policy $\\pi^{(\\theta)}$ . Then, Problem (9) can be reformulated as the minimization of $J(\\theta)/\\eta$ where $J(\\theta):=\\bar{\\int_{}p_{\\theta}(\\bar{\\tau})\\exp(\\eta C_{\\theta}(\\tau))\\mathrm{d}\\tau}$ . To optimize $J(\\theta)/\\eta$ by gradient descent, we give the gradient $\\nabla_{\\theta}J(\\theta)$ . The proof is shown in Appendix $\\boldsymbol{\\mathrm{F}}$ ", "page_idx": 6}, {"type": "text", "text": "Proposition 7. Assume the existence of densities $p(x_{t+1}|x_{t},u_{t})$ $p(x_{0})$ .Assume further that $\\pi^{(\\theta)}$ is differentiable in $\\theta$ \uff1aand the derivative and the integral can be interchanged as $\\nabla_{\\theta}J(\\theta)\\;=\\;$ $\\begin{array}{r}{\\int\\nabla_{\\theta}[p_{\\theta}(\\tau)\\exp(\\eta C_{\\theta}(\\tau))]\\mathrm{d}\\tau}\\end{array}$ Then, for any function $b:\\mathbb{R}^{n_{x}}\\rightarrow\\mathbb{R},$ it holds that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\theta}J(\\theta)=(\\eta+1)\\mathbb{E}_{p_{\\theta}(\\tau)}\\Bigg[\\underset{t=0}{\\overset{T-1}{\\sum}}\\nabla_{\\theta}\\log\\pi^{(\\theta)}(u_{t}|x_{t})}\\\\ &{\\qquad\\qquad\\quad\\times\\Bigg\\{\\exp\\Bigg(\\eta c_{T}(x_{T})+\\eta\\underset{s=t}{\\overset{T-1}{\\sum}}\\Big(c_{s}(x_{s},u_{s})+\\log\\pi^{(\\theta)}(u_{s}|x_{s})\\Big)\\Bigg)-b(x_{t})\\Bigg\\}\\Bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Thefunction $b$ is referred to as a baseline function, which can be used for reducing the variance of an estimateof $\\nabla_{\\theta}J$ . The following gradient estimate of $J(\\theta)/\\eta$ is unbiased: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{\\jmath+1}{\\eta}\\sum_{t=0}^{T-1}\\nabla_{\\theta}\\log\\pi^{(\\theta)}(u_{t}|x_{t})\\biggl\\{\\exp\\biggl(\\eta c_{T}(x_{T})+\\eta\\sum_{s=t}^{T-1}\\Bigl(c_{s}(x_{s},u_{s})+\\log\\pi^{(\\theta)}(u_{s}|x_{s})\\Bigr)\\biggr)-b(x_{t})\\biggr\\}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This is almost the same as risk-sensitive REINFORCE [19] except for the additional term $\\log\\pi^{(\\theta)}(u_{s}|x_{s})$ . In the risk-neutral limit $\\eta\\rightarrow0$ , this estimator converges to the MaxEnt policy gradient estimator [5]. ", "page_idx": 6}, {"type": "text", "text": "4.2  Risk-sensitive soft actor-critic ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In Subsection 3.2, we used dynamic programming to obtain the optimal policy $\\{\\pi_{t}^{*}\\}$ . Rather, in this section, we adopt a standard procedure of variational inference [48]. First, we find the optimal factor $\\pi_{t}$ for fixed $\\pi_{s}$ $s\\neq t$ as follows. The proof is deferred to Appendix G. ", "page_idx": 7}, {"type": "text", "text": "Proposition 8. For $t\\in[0,T-1]$ let $\\pi_{s},s\\neq t$ be fixed. Let $\\eta>-1$ $\\eta\\neq0$ Then, the optimal factor $\\begin{array}{r}{\\pi_{t}^{\\bullet}:=\\arg\\operatorname*{min}_{\\pi_{t}\\in\\mathcal{P}(\\mathbb{U})}D_{1+\\eta}(p^{\\pi}\\|p(\\cdot|\\mathcal{O}_{0:T}=1))}\\end{array}$ is given by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\pi_{t}^{\\bullet}(u_{t}|x_{t})=\\frac{1}{Z_{t}(x_{t})}\\left(\\mathbb{E}_{p^{\\pi}(x_{t+1:T},u_{t+1:T-1}|x_{t},u_{t})}\\left[\\left(\\frac{\\prod_{s=t+1}^{T-1}\\pi_{s}(u_{s}|x_{s})}{p(\\mathcal{O}_{t}|x_{T})\\prod_{s=t}^{T-1}p(\\mathcal{O}_{s}|x_{s},u_{s})}\\right)^{\\eta}\\right]\\right)^{-1/\\eta},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $Z_{t}(x_{t})$ is the normalizing constant. ", "page_idx": 7}, {"type": "text", "text": "By (24), the optimal factor $\\pi_{t}^{\\bullet}$ is independent of the past factors $\\pi_{s}$ $s\\in[0,t-1]$ . Therefore, the variational R\u00e9nyi bound in (8) is maximized by optimizing $\\pi_{t}$ in backward order from $t=T-1$ to $t=0$ , which is consistent with the dynamic programming. Associated with (24), we define ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{V_{t}^{\\pi}(x_{t}):=\\displaystyle\\frac{1}{\\eta}\\log\\mathbb{E}_{p^{\\pi}(x_{t+1:T},u_{t:T-1}|x_{t})}\\left[\\left(\\frac{\\prod_{s=t}^{T-1}\\pi_{s}(u_{s}|x_{s})}{p(\\mathcal{O}_{t}|x_{T})\\prod_{s=t}^{T-1}p(\\mathcal{O}_{s}|x_{s},u_{s})}\\right)^{\\eta}\\right]}}\\\\ {{=\\displaystyle\\frac{1}{\\eta}\\log\\mathbb{E}_{p^{\\pi}(x_{t+1:T},u_{t:T-1}|x_{t})}\\left[\\exp\\left(\\eta c_{T}(x_{T})+\\eta\\sum_{s=t}^{T-1}(c_{s}(x_{s},u_{s})+\\log\\pi_{s}(u_{s}|x_{s})\\right)\\right)\\right],}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "which is the value function for the policy $\\{\\pi_{s}\\}_{s=t}^{T-1}$ satisfying the following Bellman equation. ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{t}^{\\pi}(x_{t})=\\displaystyle\\frac{1}{\\eta}\\log\\mathbb{E}_{\\pi_{t}(u_{t}|x_{t})}\\left[\\left(\\frac{\\pi_{t}(u_{t}|x_{t})}{p(\\mathcal{O}_{t}|x_{t},u_{t})}\\right)^{\\eta}\\mathbb{E}_{p(x_{t+1}|x_{t},u_{t})}\\left[\\exp(\\eta V_{t+1}^{\\pi}(x_{t+1}))\\right]\\right]}\\\\ &{\\qquad\\quad=\\displaystyle\\frac{1}{\\eta}\\log\\mathbb{E}_{\\pi_{t}(u_{t}|x_{t})}\\left[\\exp\\left(\\eta c_{t}(x_{t},u_{t})+\\eta\\log\\pi_{t}(u_{t}|x_{t})\\right)\\mathbb{E}_{p(x_{t+1}|x_{t},u_{t})}\\left[\\exp(\\eta V_{t+1}^{\\pi}(x_{t+1}))\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "By the value function, $\\pi_{t}^{\\bullet}(u_{t}|x_{t})$ can be written as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\pi_{t}^{\\bullet}(u_{t}|x_{t})=\\frac{p(\\mathcal{O}_{t}|x_{t},u_{t})}{Z_{t}(x_{t})}\\mathbb{E}_{p(x_{t+1:T},u_{t+1:T-1}|x_{t},u_{t})}\\left[\\left(\\frac{\\prod_{s=t+1}^{T-1}\\pi_{s}(u_{s}|x_{s})}{p(\\mathcal{O}_{t}|x_{T})\\prod_{s=t+1}^{T-1}p(\\mathcal{O}_{s}|x_{s},u_{s})}\\right)^{\\eta}\\right]^{-1/\\eta}}&{{}}&{}\\\\ {=\\frac{p(\\mathcal{O}_{t}|x_{t},u_{t})}{Z_{t}(x_{t})}\\mathbb{E}_{p(x_{t+1:T},u_{t})}\\left[\\exp(\\eta V_{t+1}^{\\pi}(x_{t+1}))\\right]^{-1/\\eta}.}&{{}}&{\\left(2\\pi_{s}(u_{t})\\prod_{s=t+1}^{T-1}\\pi_{s}(u_{t})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Next w dene theQ-function for $\\{\\pi_{s}\\}_{s=t+1}^{T-1}$ ", "page_idx": 7}, {"type": "equation", "text": "$$\nQ_{t}^{\\pi}(x_{t},u_{t}):=-\\log p(\\mathcal{O}_{t}|x_{t},u_{t})+\\frac{1}{\\eta}\\log\\mathbb{E}_{p(x_{t+1}|x_{t},u_{t})}\\left[\\exp(\\eta V_{t+1}^{\\pi}(x_{t+1}))\\right].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Then, it follows from (26) and (27) that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad V_{t}^{\\pi}(x_{t})=\\displaystyle\\frac{1}{\\eta}\\log\\mathbb{E}_{\\pi_{t}(u_{t}|x_{t})}\\left[\\pi_{t}(u_{t}|x_{t})^{\\eta}\\exp(\\eta Q_{t}^{\\pi}(x_{t},u_{t}))\\right],}\\\\ &{\\pi_{t}^{\\bullet}(u_{t}|x_{t})=\\displaystyle\\frac{1}{Z_{t}(x_{t})}\\exp(-Q_{t}^{\\pi}(x_{t},u_{t})),\\,\\,\\,Z_{t}(x_{t})=\\displaystyle\\int_{\\mathbb{U}}\\exp\\left(-Q_{t}^{\\pi}(x_{t},u^{\\prime})\\right)\\mathrm{d}u^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Especially when $\\pi_{t}(u_{t}|x_{t})=\\pi_{t}^{\\bullet}(u_{t}|x_{t})$ , it holds that $\\begin{array}{r}{V_{\\underline{{t}}}^{\\pi}(x_{t})=-\\log\\left[\\int\\exp(-Q_{t.}^{\\pi}(x_{t},u^{\\prime}))\\mathrm{d}u^{\\prime}\\right]}\\end{array}$ which coincides with the soft Bellman equation in (13). In summary, in order to obtain the optimal factor $\\pi_{t}^{\\bullet}$ , it is sufficient to compute $V_{t}^{\\pi}$ and $Q_{t}^{\\pi}$ in a backward manner. ", "page_idx": 7}, {"type": "text", "text": "Next, we consider the situation when the policy is parameterized as $\\pi_{t}^{(\\theta)}(u_{t}|x_{t})$ $\\theta\\in\\mathbb{R}^{n_{\\theta}}$ and there is no parameter $\\theta$ that gives the optimal factor $\\pi_{t}^{(\\theta)}=\\pi_{t}^{\\bullet}$ . To accommodate this situation, we utilize the variational R\u00e9nyi bound. One can easily see that the maximization of the R\u00e9nyi bound in (8) with respect to a single factor $\\pi_{t}$ is equivalent to the following problem. ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{minimize}_{\\pi_{t}}\\ \\frac{1}{\\eta}\\log\\mathbb{E}_{p^{\\pi}(x_{t})}\\left[\\mathbb{E}_{\\pi_{t}(u_{t}|x_{t})}\\left[\\pi_{t}(u_{t}|x_{t})^{\\eta}\\exp(\\eta Q_{t}^{\\pi}(x_{t},u_{t}))\\right]\\right].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "This suggests choosing $\\theta$ that minimizes (31) whose $\\pi_{t}$ is replaced by $\\pi_{t}^{(\\theta)}$ . Note that this is further equivalent to ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{minimize}_{\\theta}\\ \\mathbb{E}_{p^{\\pi}(x_{t})}\\left[D_{1+\\eta}\\left(\\pi_{t}^{(\\theta)}(\\cdot|x_{t})\\middle|\\right|\\frac{\\exp(-Q_{t}^{\\pi}(x_{t},\\cdot))}{Z_{t}(x_{t})}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We also parameterize $V_{t}^{\\pi}$ and $Q_{t}^{\\pi}$ as $V^{(\\psi)},\\,Q^{(\\phi)}$ and optimize $\\psi,\\phi$ so that the relations (28), (29) approximately hold. To obtain unbiased gradient estimators later, we minimize the following squared residual error based on (28), (29), and the transformation $T_{\\eta}(v):=(\\mathrm{e}^{\\eta v}-1)/\\eta,v\\in\\mathbb{R}$ ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathcal{I}}_{Q}(\\phi):=\\mathbb{E}_{p^{\\pi}(x_{t},u_{t})}\\left[\\frac{1}{2}\\left\\{T_{\\eta}\\left(Q^{(\\phi)}(x_{t},u_{t})-c(x_{t},u_{t})\\right)-\\mathbb{E}_{p(x_{t+1}|x_{t},u_{t})}\\left[T_{\\eta}(V^{(\\psi)}(x_{t+1}))\\right]\\right\\}^{2}\\right],}\\\\ &{{\\mathcal{I}}_{V}(\\psi):=\\mathbb{E}_{p^{\\pi}(x_{t})}\\left[\\frac{1}{2}\\left\\{T_{\\eta}(V^{(\\psi)}(x_{t}))-\\mathbb{E}_{\\pi^{(\\theta)}(u_{t}|x_{t})}\\left[T_{\\eta}\\left(Q^{(\\phi)}(x_{t},u_{t})+\\log\\pi^{(\\theta)}(u_{t}|x_{t})\\right)\\right]\\right\\}^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Using ${Q^{(\\phi)}}$ and $T_{\\eta}$ , we replace (31) with the following equivalent objective: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathcal{T}_{\\pi}(\\theta):=\\mathbb{E}_{p^{\\pi}(x_{t})}\\left[\\mathbb{E}_{\\pi^{(\\theta)}(u_{t}|x_{t})}\\big[T_{\\eta}\\big(Q^{(\\phi)}(x_{t},u_{t})+\\log\\pi^{(\\theta)}(u_{t}|x_{t})\\big)\\big]\\right].\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Noting that $\\begin{array}{r}{\\operatorname*{lim}_{\\eta\\to0}T_{\\eta}(\\kappa(\\eta))\\,=\\,\\kappa(0)}\\end{array}$ for $\\kappa:\\mathbb{R}\\rightarrow\\mathbb{R}$ , as the risk sensitivity $\\eta$ goes to zero, the objectives $\\mathcal{I}_{Q},\\mathcal{I}_{V},\\mathcal{I}_{\\pi}$ converge to those used for the risk-neutral soft actor-critic [7]. Now, we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\phi}\\mathcal{I}_{Q}(\\phi)=\\mathbb{E}_{p^{\\pi}(x_{t},u_{t})}\\Big[\\big(\\nabla_{\\phi}Q^{(\\phi)}(x_{t},u_{t})\\big)\\exp\\big(\\eta Q^{(\\phi)}(x_{t},u_{t})-\\eta c(x_{t},u_{t})\\big)}\\\\ &{\\qquad\\qquad\\qquad\\times\\big\\{T_{\\eta}\\big(Q^{(\\phi)}(x_{t},u_{t})-c(x_{t},u_{t})\\big)-\\mathbb{E}_{p(x_{t+1}|x_{t},u_{t})}\\big[T_{\\eta}\\big(V^{(\\psi)}(x_{t+1})\\big)\\big]\\big\\}\\Big],}\\\\ &{\\nabla_{\\psi}\\mathcal{I}_{V}(\\psi)=\\mathbb{E}_{p^{\\pi}(x_{t})}\\Big[\\big(\\nabla_{\\psi}V^{(\\psi)}(x_{t})\\big)\\exp(\\eta V^{(\\psi)}(x_{t})\\big)}\\\\ &{\\qquad\\qquad\\quad\\times\\big\\{T_{\\eta}\\big(V^{(\\psi)}(x_{t})\\big)-\\mathbb{E}_{\\pi^{(\\theta)}(u_{t}|x_{t})}\\big[T_{\\eta}\\big(Q^{(\\phi)}(x_{t},u_{t})+\\log\\pi^{(\\theta)}(u_{t}|x_{t})\\big)\\big]\\big\\}\\Big],}\\\\ &{\\nabla_{\\theta}\\mathcal{I}_{\\pi}(\\theta)=(\\eta+1)\\mathbb{E}_{p^{\\pi}(x_{t},u_{t})}\\Big[\\big(\\nabla_{\\theta}\\log\\pi^{(\\theta)}(u_{t}|x_{t})\\big)T_{\\eta}\\big(Q^{(\\phi)}(x_{t},u_{t})+\\log\\pi^{(\\theta)}(u_{t}|x_{t})\\big)\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Thanks to the transformation $T_{\\eta}$ , the expectations appear linearly, and an unbiased gradient estimator can be obtained by removing them. By simply replacing the gradients of the soft actor-critic [7] with (34)-(36), we obtain the risk-sensitive soft actor-critic (RSAC). It is worth mentioning that since RSAC requires only minor modifications to SAC, techniques for stabilizing SAC, e.g., reparameterization, minibatch sampling with a replay buffer, target networks, double Q-network, can be directly used forRSAC. ", "page_idx": 8}, {"type": "text", "text": "5 Experiment ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Unregularized risk-averse control is known to be robust against perturbations in systems [32]. Since the robustness of the regularized cases has not yet been established theoretically, we verify the robustness of policies learned by RSAC through a numerical example. The environment is Pendulum-v1 in OpenAI Gymnasium. We trained control policies using the hyperparameters shown in Appendix H. There were no significant differences in the control performance obtained or the behavior during training. On the other hand, for each $\\eta$ , one control policy was selected and was applied to a slightly different environment without retraining. To be more precise, the pendulum length l, which is 1.0 during training, is changed to 1.25 and 1.5; See Fig. 3. In this example, it can be seen that the control policy obtained withlarger $\\eta$ has a smaller performance degradation due to environmental changes. This robustness can be considered a benefit of risk-sensitive control. ", "page_idx": 8}, {"type": "image", "img_path": "LUIXdWn6Z5/tmp/378fe2f6fd9d72e7a0e9ed8e925a1376bba4c855ed5829c3006c2c53777ff31e.jpg", "img_caption": ["Figure 3: Average episode cost for RSAC with some $\\eta$ and standard SAC. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "In Fig. 4, empirical distributions of the costs for different risk-sensitivity parameters $\\eta$ are plotted. Only the distribution for $\\eta=0.02$ does not change so much under the system perturbations. The ", "page_idx": 8}, {"type": "image", "img_path": "LUIXdWn6Z5/tmp/27233241a9867ddf691e0193e7a2993355f4a228626912532e39a1f10d6add50.jpg", "img_caption": ["(a) Pendulum length $l=1.0$ during  (b) System perturbation $l=1.25$ (c) System perturbation $l=1.5$ training "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 4: Empirical distributions of the costs for different risk-sensitivity parameters $\\eta$ distributionfor SAC $\\mathit{\\Pi}_{\\mathit{\\Pi}}^{\\prime}\\mathit{\\Pi}_{\\mathit{\\Pi}}=\\mathrm{\\Delta}0]$ with $l=1.5$ deviates from the original one $(l=1.0)$ O,and another peak of the distribution appears in the high-cost area. This means that there is a high probability of incurring a high cost, which clarifies the advantage of RSAC. The more risk-seeking the policy becomes, the less robust it becomes against the system perturbation. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we proposed a unifying framework of CaI, named RCaI, using R\u00e9nyi divergence variational inference. We revealed that RCaI yields the LP regularized risk-sensitive control with exponential performance criteria. Moreover, we showed the equivalences for risk-sensitive control, MaxEnt control, the optimal posterior for CaI, and linearly-solvable control. In addition to these connections, we derived the policy gradient method and the soft actor-critic method for the risksensitive RL via RCal. Interestingly, Renyi entropy regularization also results in the same form of the risk-sensitive optimal policy and the soft Bellman equation as the LP regularization. ", "page_idx": 9}, {"type": "text", "text": "From a practical point of view, a major limitation of the proposed risk-sensitive soft actor-critic is its numerical instability for large $|\\eta|$ cases. Since $\\eta$ appears, for example, as $\\exp(\\eta Q^{(\\phi)}(x_{t},u_{t}))$ in the gradients (34)-(36), the magnitude of $\\eta$ that does not cause the numerical instability depends on the scale of costs. Therefore, we need to choose $\\eta$ depending on environments. In the experiment using Pendulum-v1, $|\\eta|$ that is larger than 0.03 results in the failure of learning due to the numerical instability. Although it is an important future work to address this issue, we would like to note that this issue is not specific to our algorithms, but occurs in general risk-sensitive RL with exponential utility. It is also important how to choose a specific value of the order parameter $1+\\eta$ of R\u00e9nyi divergence. Since we showed that $\\eta$ determines the risk sensitivity of the optimal policy, we can follow previous studies on the choice of the sensitivity parameter of the risk-sensitive control without regularization. The properties of the derived algorithms also need to be explored in future work, e.g., the compatibility of a function approximator for RSAC [49]. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors thank Ran Wang for his valuable help in conducting the experiment. This work was supported in part by JSPS KAKENHI Grant Numbers JP23K19117, JP24K17297, JP21H04875. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] On\u00e9simo Hernandez-Lerma and Jean B. Lasserre, Discrete-time Markov Control Processes: Basic Optimality Criteria, vol. 30, Springer-Verlag New York, 1996.   \n[2] Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction, MIT Press, second edition, 2018.   \n[3] Tuomas Haarnoja, Sehoon Ha, Aurick Zhou, Jie Tan, George Tucker, and Sergey Levine, \"Learning to walk via deep reinforcement learning\", in Robotics: Science and Systems, 2019.   \n[4] B. Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A. Al Sallab, Senthil Yogamani, and Patrick Perez, \u201cDeep reinforcement learning for autonomous driving: A survey\", IEEE Transactions on Intelligent Transportation Systems, vol. 23, no. 6, pp. 4909-4926, 2022.   \n[5]  Sergey Levine, \u201cReinforcement learning and control as probabilistic inference: Tutorial and review\", arXiv preprint arXiv: 1805.00909, 2018.   \n[6]  Brian D. Ziebart, Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy, PhD thesis, Carnegie Mellon University, 2010.   \n[7] Tuomas Harnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine, \u201cSoft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor'\", in International Conference on Machine Learning. PMLR, 2018, pp. 1861-1870.   \n[8] Benjamin Eysenbach and Sergey Levine, \u201cMaximum entropy RL (provably) solves some robust RL problems\", in International Conference on Learning Representations, 2022.   \n[9] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abeel, and Sergey Levine, \u201cSoft actor-critic algorithms and applications\", arXiv preprint arXiv: 1812.05905, 2018.   \n[10] Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans, \u201cOn the global convergence rates of softmax policy gradient methods\", in International Conference on Machine Learning. PMLR, 2020, vol. 119, p. 6820-6829.   \n[11] Yingzhen Li and Richard E. Turner, \u201cRenyi divergence variational inference\", in Advances in Neural Information Processing Systems, 2016, vol. 29, pp. 1073-1081.   \n[12]  Alfred Renyi, \u201cOn measures of entropy and information\", in Proceedings of the fourth Berkeley Symposium on Mathematical Statistics and Probability, 1961, vol. 1, pp. 547-561.   \n[13] Cheng Zhang, Judith Butepage, Hedvig Kjelstrom, and Stephan Mandt, \u201cAdvances in variational inference\", IEEE Transactions on Pattern Analysis and Machine Inteligence, vol. 41, no. 8, Pp. 2008-2026, 2019.   \n[14] Peter Whittle, Risk-Sensitive Optimal Control, John Wiley & Sons, Ltd., 1990.   \n[15] Emanuel Todorov, \u201cLinearly-solvable Markov decision problems\", in Advances in Neural Information Processing Systems, 2006, vol. 19, pp. 1369-1376.   \n[16]  Krishnamurthy Dvijotham and Emanuel Todorov, \u201cA unifying framework for linearly solvable control\", in 27th Conference on Uncertainty in Artificial Intelligence, 2011, pp. 179-186.   \n[17]  Ronald J. Williams, \u201c\"Simple statistical gradient-following algorithms for connectionist reinforcement learning\", Machine Learning, vol. 8, p. 229-256, 1992.   \n[18] David Nass, Boris Belousov, and Jan Peters, \u201cEntropic risk measure in policy search\", in 2019 IEEE/RSJ International Conference on Inteligent Robots and Systems (IROS). IEEE, 2019, pp. 1101-1106.   \n[19] Erfaun Noorani and John S. Baras, \u201cRisk-sensitive REINFORCE: A Monte Carlo policy gradient algorithm for exponential performance criteria\", in 2021 60th IEEE Conference on Decision and Control (CDC). IEEE, 2021, pp. 1522-1527.   \n[20] Jingliang Duan, Yang Guan, Shengbo Eben Li, Yangang Ren, Qi Sun, and Bo Cheng, \u201cDistributional soft actor-critic: Off-policy reinforcement learning for addressing value estimation errors\", IEEE Transactions on Neural Networks and Learning Systems, vol. 33, no. 11, pp. 6584-6598,2022.   \n[21] Jinyoung Choi, Christopher Dance, Jung-Eun Kim, Seulbin Hwang, and Kyung-sik Park, \u201cRisk-conditioned distributional soft actor-critic for risk-sensitive navigation\", in 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2021, pp. 8337-8344.   \n[22] Hilbert J. Kappen, \u201cPath integrals and symmetry breaking for optimal control theory\", Journal of Statistical Mechanics: Theory and Experiment, vol. 2005, no. 11, pp. P11011, 2005.   \n[23] Emanuel Todorov, \u201cGeneral duality between optimal control and estimation\", in 2008 47th IEEE Conference on Decision and Control. IEEE, 2008, pp. 4286-4292.   \n[24] Hilbert J. Kappen, Vicenc Gomez, and Manfred Opper, \u201cOptimal control as a graphical model inference problem\", Machine Learning, vol. 87, p. 159-182, 2012.   \n[25] Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar,\u201cOn stochastic optimal control and reinforcement learning by approximate inference\", in Proceedings of Robotics: Science and Systems, 2012.   \n[26] Marc Toussaint, \u201cRobot trajectory optimization using approximate inference\u201d\", in International Conference on Machine Learning, 2009, p. 1049-1056.   \n[27]  Masashi Okada and Tadahiro Taniguchi, \u201cVariational inference MPC for Bayesian model-based reinforcement learning\", in Conference on Robot Learning. PMLR, 2020, pp. 258-272.   \n[28] Alexander Lambert, Fabio Ramos, Byron Boots, Dieter Fox, and Adam Fishman, \u201c\"Stein variational model predictive control\", in Conference on Robot Learning. PMLR, 2021, vol. 155, pp. 1278-1297.   \n[29] Ziyi Wang, Oswin So, Jason Gibson, Bogdan Vlahov, Manan S. Gandhi, Guan-Horng Liu, and Evangelos A. Theodorou, \u201c\"Variational inference MPC using Tsallis divergence\", in Robotics: Science and Systems, 2021.   \n[30] Yinlam Chow, Brandon Cui, MoonKyung Ryu, and Mohammad Ghavamzadeh, \u201cVariational model-based policy optimization\", arXiv preprint arXiv:2006.05443, 2020.   \n[31] Marco C. Campi and Matthew R. James, \u201cNonlinear discrete-time risk-sensitive optimal control\", International Journal of Robust and Nonlinear Control, vol. 6, no. 1, pp. 1-19, 1996.   \n[32] Ian R Petersen, Matthew R James, and Paul Dupuis, \u201cMinimax optimal control of stochastic uncertain systems with relative entropy constraints\", IEEE Transactions on Automatic Control, vol. 45, no. 3, Pp. 398-412, 2000.   \n[33] Brendan O'Donoghue, \u201c\"Variational Bayesian reinforcement learning with regret bounds\", in Advances in Neural Information Processing Systems, 2021, vol. 34, pp. 28208-28221.   \n[34]  Vivek S. Borkar, \u201cQ-learning for risk-sensitive control\", Mathematics of Operations Research, vol. 27, no. 2, Pp. 294-311, 2002.   \n[35] Yingjie Fei, Zhuoran Yang, Yudong Chen, and Zhaoran Wang, \u201cExponential Bellman equation and improved regret bounds for risk-sensitive reinforcement learning\", in Advances in Neural Information Processing Systems, 2021, vol. 34, pp. 20436-20446.   \n[36] Javier Garcia and Fernando Fernandez,\u03b2 \u201cA comprehensive survey on safe reinforcement learning\", Journal of Machine Learning Research, vol. 16, no. 1, pp. 1437-1480, 2015.   \n[37]  Tobias Enders, James Harrison, and Maximilian Schiffer, \u201cRisk-sensitive soft actor-critic for robust deep reinforcement learning under distribution shifts\", arXiv preprint arXiv:2402.09992, 2024.   \n[38] Kaito Ito and Kenji Kashima, \u201cKullback-Leibler control for discrete-time nonlinear systems on continuous spaces\", SICE Journal of Control, Measurement, and System Integration, vol. 15, no. 2, pp. 119-129, 2022.   \n[39] Friedrich Liese and Igor Vajda, Convex Statistical Distances, Teubner, Leipzig, 1987.   \n[40] Rami Atar, Kenny Chowdhary, and Paul Dupuis, \u201c\"Robust bounds on risk-sensitive functionals yia Renyi divergence\", SIAM/ASA Journal on Uncertainty Quantification, vol. 3, no. 1, pp. 18-33, 2015.   \n[41] Tim Van Erven and Peter Harremos, \u201cRenyi divergence and Kullback-Leibler divergence\u201d, IEEE Transactions on Information Theory, vol. 60, no. 7, pp. 3797-3820, 2014.   \n[42] Oliver Mihatsch and Ralph Neuneier, \u201cRisk-sensitive reinforcement learning\", Machine Learning, vol. 49, pp. 267-290, 2002.   \n[43] Erfaun Noorani, Christos Mavridis, and John Baras, \u201cRisk-sensitive reinforcement learning with exponential criteria\", arXiv preprint arXiv:2212.09010, 2023.   \n[44] Krishnamurthy Dvijotham and Emanuel Todorov, \u201cInverse optimal control with linearlysolvable MDPs\", in Proceedings of the 27th International Conference on Machine Learning, 2010, pPp. 335-342.   \n[45] Kaito Ito and Kenji Kashima, \u201cMaximum entropy optimal density control of discrete-time linear systems and Schrodinger bridges\", IEEE Transactions on Automatic Control, vol. 69, no. 3, pp.1536-1551, 2023.   \n[46] Kaito Ito and Kenji Kashima, \u201cMaximum entropy density control of discrete-time linear systems with quadratic cost\", To appear in IEEE Transactions on Automatic Control, 2025, arXiv preprint arXiv:2309.10662.   \n[47] Peter Whittle, \u201cRisk-sensitive linear/quadratic/Gaussian control', Advances in Applied Probability, vol. 13, no. 4, pp. 764-777, 1981.   \n[48] Christopher M. Bishop, Pattern Recognition and Machine Learning, Springer, 2006.   \n[49] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour, \u201c\"Policy gradient methods for reinforcement learning with function approximation\", in Advances in Neural Information Processing Systems, 1999, vol. 12, pp. 1057-1063.   \n[50] Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann, \u201cStable-baselines3: Reliable reinforcement learning implementations\", Journal of Machine Learning Research, vol. 22, no. 268, pp. 1-8, 2021.   \n[51] Diederik P. Kingma and Jimmy Ba, \u201cAdam: A method for stochastic optimization\", arXiv preprint arXiv:1412.6980, 2014. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A   More details on Control as Inference ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this appendix, we give more details on Cal. As mentioned in (1), the distribution of the state and control input trajectory given optimality variables satisfies ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle p(\\tau|\\mathcal{O}_{0:T})\\propto p(\\tau,\\mathcal{O}_{0:T})}\\ ~}\\\\ {{\\displaystyle~~~~~~~~~~~=\\left[p(\\mathcal{O}_{T}|x_{T})\\prod_{t=0}^{T-1}p(\\mathcal{O}_{t}|x_{t},u_{t})\\right]\\left[p(x_{0})\\prod_{t=0}^{T-1}p(x_{t+1}|x_{t},u_{t})p(u_{t})\\right],}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $p(u_{t})=1/\\mu_{L}(\\mathbb{U})$ and $p(\\tau,\\mathcal{O}_{0:T})$ is defined so that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\tau\\in\\mathcal{B},\\ \\mathcal{O}_{0:T}=\\mathbf{o}_{0:T})=\\int_{\\mathcal{B}}p(\\tau,\\mathbf{o}_{0:T})\\mathrm{d}\\tau\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "for any $\\mathbf{o}_{0:T}\\in\\{0,1\\}^{T+1}$ and any Borel set $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ where $\\mathbb{P}$ denotes the probability. Therefore, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\np(\\tau|\\mathcal{O}_{0:T}=1)\\propto\\left[p(x_{0})\\prod_{t=0}^{T-1}p(x_{t+1}|x_{t},u_{t})\\right]\\exp\\left(-c_{T}(x_{T})-\\sum_{t=0}^{T-1}c_{t}(x_{t},u_{t})\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The posterior $p(u_{t}|x_{t},\\mathcal{O}_{t:T}=1)$ given the optimality condition $O_{t:T}=1$ is called the optimal policy. We emphasize that the optimality of $p(u_{t}|x_{t},\\mathcal{O}_{t:T}=1)$ is defined by the condition $O_{t:T}=1$ rather than by introducing a cost functional, unlike $\\pi^{*}(u_{t}|x_{t})$ in (13). In the following, we $\\mathrm{drop}=1$ for $O_{t}$ ", "page_idx": 13}, {"type": "text", "text": "The optimal policy can be computed as follows. Define ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\beta_{t}(x_{t},u_{t}):=p(\\mathcal{O}_{t:T}|x_{t},u_{t}),}\\\\ {\\zeta_{t}(x_{t}):=p(\\mathcal{O}_{t:T}|x_{t}).\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then, it holds that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\zeta_{t}(x_{t})=\\int_{\\mathbb{U}}p(\\mathcal{O}_{t:T}|x_{t},u_{t})p(u_{t}|x_{t})\\mathrm{d}u_{t}=\\int_{\\mathbb{U}}\\beta_{t}(x_{t},u_{t})p(u_{t})\\mathrm{d}u_{t}=\\frac{1}{\\mu_{L}(\\mathbb{U})}\\int_{\\mathbb{U}}\\beta_{t}(x_{t},u_{t})\\mathrm{d}u_{t}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In addition, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta_{t}(x_{t},u_{t})=p(\\mathcal{O}_{t:T}|x_{t},u_{t})=p(\\mathcal{O}_{t}|x_{t},u_{t})p(\\mathcal{O}_{t+1:T}|x_{t},u_{t})}\\\\ &{\\qquad\\qquad=p(\\mathcal{O}_{t}|x_{t},u_{t})\\displaystyle\\int_{\\mathbb{X}}p(\\mathcal{O}_{t+1:T}|x_{t+1})p(x_{t+1}|x_{t},u_{t})\\mathrm{d}x_{t+1}}\\\\ &{\\qquad\\qquad=p(\\mathcal{O}_{t}|x_{t},u_{t})\\displaystyle\\int_{\\mathbb{X}}\\zeta_{t+1}(x_{t+1})p(x_{t+1}|x_{t},u_{t})\\mathrm{d}x_{t+1},}\\\\ &{\\zeta_{T}(x_{T})=p(\\mathcal{O}_{T}|x_{T})=\\exp(-c_{T}(x_{T})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where we used ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(\\mathcal{O}_{t+1:T}|x_{t},u_{t})=\\displaystyle\\int_{\\mathbb{X}}p(\\mathcal{O}_{t+1:T},x_{t+1}|x_{t},u_{t})\\mathrm{d}x_{t+1}}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\int_{\\mathbb{X}}p(\\mathcal{O}_{t+1:T}|x_{t+1},x_{t},u_{t})p(x_{t+1}|x_{t},u_{t})\\mathrm{d}x_{t+1}}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\int_{\\mathbb{X}}p(\\mathcal{O}_{t+1:T}|x_{t+1})p(x_{t+1}|x_{t},u_{t})\\mathrm{d}x_{t+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In terms of $\\beta_{t}$ and $\\zeta_{t}$ , the optimal policy can be written as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(u_{t}|x_{t},\\mathcal{O}_{t:T})=\\frac{p(x_{t},u_{t},\\mathcal{O}_{t:T})}{p(x_{t},\\mathcal{O}_{t:T})}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{p(\\mathcal{O}_{t:T}|x_{t},u_{t})}{p(\\mathcal{O}_{t:T}|x_{t})}p(u_{t}|x_{t})}\\\\ &{\\qquad\\qquad\\qquad=\\frac{\\beta_{t}(x_{t},u_{t})}{\\mu_{L}(\\mathbb{U})\\zeta_{t}(x_{t})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Next, by the logarithmic transformation, we define ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{Q}_{t}(x_{t},u_{t}):=-\\log\\frac{\\beta_{t}(x_{t},u_{t})}{\\mu_{L}(\\mathbb{U})},}\\\\ {\\mathsf{V}_{t}(x_{t}):=-\\log\\zeta_{t}(x_{t}).\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, by (41), the optimal policy satisfies ", "page_idx": 14}, {"type": "equation", "text": "$$\np(u_{t}|x_{t},\\mathcal{O}_{t:T})=\\exp\\left(-\\mathsf{Q}_{t}(x_{t},u_{t})+\\mathsf{V}_{t}(x_{t})\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By (39), it holds that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathsf{V}_{t}(x_{t})=-\\log\\left[\\int_{\\mathbb{U}}\\exp(-\\mathsf{Q}_{t}(x_{t},u_{t}))\\mathrm{d}u_{t}\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By using (40), we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\exp(-\\mathsf{Q}_{t}(x_{t},u_{t}))\\mu_{L}(\\mathbb{U})=\\exp(-c_{t}(x_{t},u_{t}))\\int_{\\mathbb{X}}\\zeta_{t+1}(x_{t+1})p(x_{t+1}|x_{t},u_{t})\\mathrm{d}x_{t+1},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which yields ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathsf{Q}_{t}(x_{t},u_{t})=\\mathsf{c}_{t}(x_{t},u_{t})-\\log\\mathbb{E}_{p(x_{t+1}|x_{t},u_{t})}\\left[\\exp(-\\mathsf{V}_{t+1}(x_{t+1}))\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here, we defined $\\mathsf{c}_{t}(x_{t},u_{t}):=c_{t}(x_{t},u_{t})+\\log\\mu_{L}(\\mathbb{U})$ . In summary, Proposition 1 holds. ", "page_idx": 14}, {"type": "text", "text": "B Proof of Theorem 3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This appendix is devoted to the analysis of the following problem: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\{\\pi_{t}\\}_{t=0}^{T-1}}{\\mathrm{minimize}}}&{\\quad\\frac{1}{\\eta}\\log\\mathbb{E}\\left[\\exp\\left(\\eta c_{T}(x_{T})+\\eta\\sum_{t=0}^{T-1}(c_{t}(x_{t},u_{t})+\\varepsilon\\log\\pi_{t}(u_{t}|x_{t}))\\right)\\right],}\\\\ {\\mathrm{subject~to}}&{\\quad x_{t+1}=f_{t}(x_{t},u_{t},w_{t}),\\;\\;u_{t}\\in\\mathbb{U},\\;\\;\\forall t\\in[0,T-1],}\\\\ &{\\quad u_{t}\\sim\\pi_{t}(\\cdot|x)\\mathrm{~given~}x_{t}=x,}\\\\ &{\\quad x_{0}\\sim\\mathbb{P}_{x_{0}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here, $\\{w_{t}\\}_{t=0}^{T-1}$ is an independent seqguene, $x_{0}$ is independent of $\\{w_{t}\\},\\varepsilon>0$ is the reglarization parameter, and $\\eta$ is the risk-sensitivity parameter satisfying $\\eta>-\\varepsilon^{-1}$ \uff0c $\\eta\\neq0$ . Note that we do not assume the existence of densities $p(x_{t+1}|x_{t},u_{t})$ \uff0c $p(x_{0})$ . To perform dynamic programming for Problem (47), define the value function and the Q-function as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle V_{t}(x):=\\operatorname*{inf}_{\\left\\{\\pi_{s}\\right\\}_{s=t}^{T-1}}\\frac{1}{\\eta}\\log\\mathbb{E}\\left[\\exp\\left(\\eta c_{T}(x_{T})+\\eta\\sum_{s=t}^{T-1}\\Bigl(c_{s}(x_{s},u_{s})+\\varepsilon\\log\\pi_{s}(u_{s}|x_{s})\\Bigr)\\right)\\right]\\left.\\middle|x_{t}=x\\right].}}\\\\ {{\\displaystyle V_{T}(x):=c_{T}(x),\\;\\;x\\in\\mathbb{X},}}\\\\ {{\\displaystyle2_{t}(x,u):=c_{t}(x,u)+\\frac{1}{\\eta}\\log\\mathbb{E}\\left[\\exp\\bigl(\\eta V_{t+1}\\bigl(f_{t}(x,u,w_{t})\\bigr)\\bigr)\\right],\\;\\;t\\in[0,T-1],\\;x\\in\\mathbb{X},\\;u\\in\\mathbb{U}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, under the assumptio that $\\begin{array}{r}{\\int_{\\mathbb{U}}\\exp\\left(-\\frac{\\mathscr{Q}_{t}(x,u^{\\prime})}{\\varepsilon}\\right)\\mathrm{d}u^{\\prime}<\\infty}\\end{array}$ , we prove that the unique optimal policy of Problem (47) is given by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\pi_{t}^{*}(u|x):=\\frac{\\exp\\left(-\\frac{Q_{t}(x,u)}{\\varepsilon}\\right)}{\\int_{\\mathbb{U}}\\exp\\left(-\\frac{Q_{t}(x,u^{\\prime})}{\\varepsilon}\\right)\\mathrm{d}u^{\\prime}},\\;\\;t\\in\\mathbb{J}0,T-1\\mathbb{J},\\;u\\in\\mathbb{U},\\;x\\in\\mathbb{X}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "First, by definition, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{V_{t}}(x)=\\underbrace{\\operatorname*{inf}_{{\\{\\pi_{k}\\}}_{\\tau=1}^{T}}\\frac{1}{\\eta}\\log\\left[\\int_{\\mathcal{T}}\\pi_{t}(u\\,\\vert x)\\mathbb{E}\\biggl[\\exp\\biggl(\\eta c_{\\epsilon}(x,u)+\\varepsilon\\eta\\log\\pi_{t}(u\\,\\vert x)+\\eta c_{T}(x_{T})}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\eta\\sum_{s=t+1}^{T-1}\\Big(c_{s}(x_{s},u_{s})+\\varepsilon\\log\\pi_{s}(u_{s})\\vert x_{s})\\Big)\\biggr)\\right]\\,\\bigg|}_{x_{t}=x}\\,=x,u_{t}=u\\bigg]\\mathrm{d}u\\Bigg]}\\\\ {\\,=\\underbrace{\\operatorname*{inf}_{{\\{\\pi_{k}\\}}_{\\tau=1}^{T}\\frac{1}{\\eta}\\log\\left[\\int_{\\mathbb{V}}\\pi_{t}(u\\,\\vert x)\\exp\\Big(\\eta c_{\\epsilon}(x,u)+\\varepsilon\\eta\\log\\pi_{t}(u\\,\\vert x\\rangle)}\\\\ {\\qquad\\quad\\times\\mathbb{E}\\biggl[\\exp\\biggl(\\eta c_{T}(x_{T})+\\eta\\underbrace{T^{-1}}_{s=t+1}\\Big(c_{s}(x_{s},u_{s})+\\varepsilon\\log\\pi_{s}(u_{s})\\vert x_{s})\\Big)\\biggr)\\right]\\,\\pi_{t}=x,u_{t}=u\\bigg]\\mathrm{d}u\\Bigg]}_{\\mathrm{d}u}}\\\\ {\\,=\\underbrace{\\operatorname*{inf}_{{\\ \\pi_{t}}\\tau}\\log\\left[\\int_{\\mathbb{V}}\\pi_{t}(u\\,\\vert x\\rangle\\exp\\bigl(\\eta c_{\\epsilon}(x,u)+\\varepsilon\\eta\\log\\pi_{t}(u\\,\\vert x\\rangle)\\bigr)\\mathbb{E}\\left[\\exp\\bigl(\\eta V_{t+1}(f_{t}(x,u,w_{t})\\bigr)\\bigr)\\right]\\mathrm{d}u\\right]}_{\\mathrm{d}u}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By the definition of the Q-function (52), we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{t}(x)=\\underset{\\pi_{t}(\\cdot\\vert x)\\in\\mathcal{P}(\\mathbb{U})}{\\operatorname*{inf}}\\frac{1}{\\eta}\\log\\left[\\int_{\\mathbb{U}}\\pi_{t}(u\\vert x)\\exp(\\varepsilon\\eta\\log\\pi_{t}(u\\vert x))\\exp(\\eta Q_{t}(x,u))\\mathrm{d}u\\right]}\\\\ &{\\qquad=\\underset{\\pi_{t}(\\cdot\\vert x)\\in\\mathcal{P}(\\mathbb{U})}{\\operatorname*{inf}}\\frac{1}{\\eta}\\log\\left[\\int_{\\mathbb{U}}\\left(\\pi_{t}(u\\vert x)\\right)^{1+\\varepsilon\\eta}\\left(\\exp\\left(\\frac{-Q_{t}(x,u)}{\\varepsilon}\\right)\\right)^{-\\varepsilon\\eta}\\mathrm{d}u\\right]}\\\\ &{\\qquad=\\underset{\\pi_{t}(\\cdot\\vert x)\\in\\mathcal{P}(\\mathbb{U})}{\\operatorname*{inf}}\\frac{1}{\\eta}\\log\\left[\\left(\\int_{\\mathbb{U}}\\exp\\left(\\frac{-Q_{t}(x,u^{\\prime})}{\\varepsilon}\\right)\\mathrm{d}u^{\\prime}\\right)^{-\\varepsilon\\eta}\\int_{\\mathbb{U}}\\pi_{t}(u\\vert x)^{1+\\varepsilon\\eta}\\pi_{t}^{*}(u\\vert x)^{-\\varepsilon\\eta}\\mathrm{d}u\\right]}\\\\ &{\\qquad=-\\varepsilon\\log\\left[\\int_{\\mathbb{U}}\\exp\\left(-\\frac{Q_{t}(x,u^{\\prime})}{\\varepsilon}\\right)\\mathrm{d}u^{\\prime}\\right]+\\underset{\\pi_{t}(\\cdot\\vert x)\\in\\mathcal{P}(\\mathbb{U})}{\\operatorname*{inf}}\\varepsilon D_{1+\\varepsilon\\eta}(\\pi_{t}(\\cdot\\vert x)\\vert\\vert\\pi_{t}^{*}(\\cdot\\vert x)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $D_{1+\\varepsilon\\eta}(\\pi_{t}(\\cdot|x)\\|\\pi_{t}^{*}(\\cdot|x))$ attains its minimum value O if and only if $\\pi_{t}(\\cdot|x)\\,=\\,\\pi_{t}^{*}(\\cdot|x)$ we conclude that ", "page_idx": 15}, {"type": "equation", "text": "$$\nV_{t}(x)=-\\varepsilon\\log\\left[\\int_{\\mathbb{U}}\\exp\\left(-\\frac{\\mathscr{Q}_{t}(x,u^{\\prime})}{\\varepsilon}\\right)\\mathrm{d}u^{\\prime}\\right],\\:\\:\\forall x\\in\\mathbb{X},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and the unique optimal policy of Problem (47) is given by (53). Moreover, $\\pi_{t}^{*}$ can be rewritten as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pi_{t}^{*}(u|x)=\\exp\\left(-\\frac{Q_{t}(x,u)}{\\varepsilon}+\\frac{V_{t}(x)}{\\varepsilon}\\right),\\;\\;t\\in[\\![0,T-1]\\!],\\;u\\in\\mathbb{U},\\;x\\in\\mathbb{X}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "When considering the deterministic system $x_{t+1}=\\bar{f}_{t}{(x_{t},u_{t})}$ , we immediately obtain the relation ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Q}_{t}(x,u)=c_{t}(x,u)+V_{t+1}(\\bar{f}_{t}(x,u)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "On the other hand, the unique optimal policy of the MaxEnt control problem: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{minimize}_{\\{\\pi_{t}\\}_{t=0}^{T-1}}\\;\\mathbb{E}\\left[c_{T}(x_{T})+\\sum_{t=0}^{T-1}\\Bigl(c_{t}(x_{t},u_{t})-\\varepsilon\\mathcal{H}_{1}(\\pi_{t}(\\cdot|x_{t}))\\Bigr)\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "is also given by (55) whose Q-function (52) is replaced by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Q}_{t}(x,u)=c_{t}(x,u)+\\mathbb{E}\\bigl[V_{t+1}\\bigl(f_{t}(x,u,w_{t})\\bigr)\\bigr].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, when the system is deterministic, the Q-function of the LP regularized risk-sensitive control problem (47) coincides with that of the MaxEnt control problem (57). Consequently, the optimal policy of Problem (57) solves Problem (47) for any $\\eta>-\\varepsilon^{-1}$ $\\eta\\neq0$ for deterministic systems. ", "page_idx": 15}, {"type": "text", "text": "C  Linear quadratic Gaussian setting ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this appendix, we derive the regularized risk-sensitive optimal policy in the linear quadratic Gaussian setting. ", "page_idx": 16}, {"type": "text", "text": "Theorem 9. Let $\\begin{array}{r c l}{p(x_{t+1}|x_{t},u_{t})}&{=}&{\\mathcal{N}(A_{t}x_{t}\\ +\\ B_{t}u_{t},\\Sigma_{t})}\\end{array}$ and $\\begin{array}{r l r}{c_{t}(x_{t},u_{t})}&{{}=}&{(x_{t}^{\\top}Q_{t}x_{t}\\;\\;+}\\end{array}$ $u_{t}^{\\top}R_{t}u_{t})/2$ \uff0c $c_{T}(x_{T})=x_{T}^{\\top}Q_{T}x_{T}/2$ where $\\Sigma_{t}$ \uff0c $Q_{t}$ ,and $R_{t}$ are positive definitematricesfor any $t$ and $\\mathcal{N}(\\mu,\\Sigma)$ denotestheGaussiandistributionwithmean $\\mu$ and covariance $\\Sigma$ .Let $\\mathbb{X}=\\mathbb{R}^{n_{x}}$ $\\mathbb{U}=\\mathbb{R}^{n_{u}}$ Assume that there exists a solution $\\{\\Pi_{t}\\}_{t=0}^{T}$ to the following Riccati difference equation: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Pi_{t}=Q_{t}+A_{t}^{\\top}\\Pi_{t+1}\\big(I-\\eta\\Sigma_{t}\\Pi_{t+1}+B_{t}R_{t}^{-1}B_{t}^{\\top}\\Pi_{t+1}\\big)^{-1}A_{t},\\ \\forall t\\in\\ensuremath{[\\![0,T-1\\!]\\!]},}\\\\ &{\\Pi_{T}=Q_{T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "suchthat $\\Sigma_{t}^{-1}-\\eta\\Pi_{t+1}$ is positivedefiniteforany $t\\in[0,T-1]$ Here, $I$ denotes theidentitymatrix of appropriate dimension. Then, the unique optimal policy of Problem (9) is given by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\pi_{t}^{*}(u|x)=\\mathcal{N}\\bigg(u\\big|-(R_{t}+B_{t}^{\\top}\\Pi_{t+1}\\big(I-\\eta\\Sigma_{t}\\Pi_{t+1}\\big)^{-1}B_{t}\\big)^{-1}B_{t}^{\\top}\\Pi_{t+1}\\big(I-\\eta\\Sigma_{t}\\Pi_{t+1}\\big)^{-1}A_{t}x,}&{}&\\\\ {(R_{t}+B_{t}\\Pi_{t+1}\\big(I-\\eta\\Sigma_{t}\\Pi_{t+1}\\big)^{-1}B_{t}\\big)^{-1}\\bigg).}&{}&{\\quad{\\mathrm{(Codiss~)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. In this proof, for notational simplicity, we often drop the time index $t$ as $A,B$ .First, for $t=T-1$ , the Q-function in (11) is ", "page_idx": 16}, {"type": "equation", "text": "$$\nQ_{T-1}(x,u)=\\frac{1}{2}\\Vert x\\Vert_{Q_{T-1}}^{2}+\\frac{1}{2}\\Vert u\\Vert_{R_{T-1}}^{2}+\\frac{1}{\\eta}\\log\\mathbb{E}\\left[\\exp\\left(\\frac{\\eta}{2}\\Vert A_{T-1}x+B_{T-1}u+w_{T-1}\\Vert_{\\Pi_{T}}^{2}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\|x\\|_{P}^{2}:=x^{\\top}P x$ for a symmetric matrix $P$ . Here, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\exp\\left(\\frac{\\eta}{2}\\|A x+B u+w_{T-1}\\|_{\\Pi_{T}}^{2}\\right)\\right]}\\\\ {\\displaystyle=\\frac{1}{\\sqrt{(2\\pi)^{n_{x}}|\\Sigma_{T-1}|}}\\int_{\\mathbb R^{n_{x}}}\\exp\\left(-\\frac{1}{2}\\|w\\|_{\\Sigma_{T-1}^{-1}}^{2}+\\frac{\\eta}{2}\\|A x+B u+w\\|_{\\Pi_{T}}^{2}\\right)\\mathrm{d}w,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $|\\Sigma_{T-1}|$ denotes the determinant of $\\Sigma_{T-1}$ , and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\displaystyle\\frac{1}{2}\\|w\\|_{\\Sigma_{T-1}^{-1}}^{2}+\\frac{\\eta}{2}\\|A x+B u+w\\|_{\\Pi_{T}}^{2}}\\\\ &{=-\\displaystyle\\frac{1}{2}\\left(\\|w\\|_{\\Sigma^{-1}-\\eta\\Pi}^{2}-2\\eta w^{\\top}\\Pi(A x+B u)-\\|A x+B u\\|_{\\eta\\Pi}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By the assumption that $\\Sigma_{T-1}^{-1}-\\eta\\Pi_{T}$ is positive defnite and a completion of squares argument ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\displaystyle\\frac{1}{2}\\|w\\|_{\\Sigma_{T-1}^{-1}}^{2}+\\frac{\\eta}{2}\\|A x+B u+w\\|_{\\Pi_{T}}^{2}}\\\\ &{=-\\displaystyle\\frac{1}{2}\\left(\\|w-(\\Sigma^{-1}-\\eta\\Pi)^{-1}\\eta\\Pi(A x+B u)\\|_{\\Sigma^{-1}-\\eta\\Pi}^{2}-\\|\\eta\\Pi(A x+B u)\\|_{(\\Sigma^{-1}-\\eta\\Pi)^{-1}}^{2}-\\|A x+B u\\|_{\\Sigma_{T-1}^{0}}^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int_{\\mathbb R^{n_{x}}}\\exp\\left(-\\frac12\\|w\\|_{\\Sigma_{T-1}^{-1}}^{2}+\\frac{\\eta}{2}\\|A x+B u+w\\|_{\\Pi_{T}}^{2}\\right)\\mathrm{d}w}\\\\ &{=\\sqrt{(2\\pi)^{n_{x}}\\left|(\\Sigma^{-1}-\\eta\\Pi)^{-1}\\right|}\\exp\\left(\\frac12\\|\\eta\\Pi(A x+B u)\\|_{(\\Sigma^{-1}-\\eta\\Pi)^{-1}}^{2}+\\frac12\\|A x+B u\\|_{\\eta\\Pi}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Consequently, by (61)-(63), the Q-function can be written as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{Q}_{T-1}(x,u)=\\displaystyle\\frac{1}{2}\\|x\\|_{Q_{T-1}}^{2}+\\frac{1}{2}\\|u\\|_{R_{T-1}}^{2}+\\frac{1}{2\\eta}\\|\\eta\\Pi(A_{T-1}x+B_{T-1}u)\\|_{(\\Sigma_{T-1}^{-1}-\\eta\\Pi_{T})^{-1}}^{2}}\\\\ &{\\qquad\\qquad+\\displaystyle\\frac{1}{2}\\|A_{T-1}x+B_{T-1}u\\|_{\\Pi}^{2}+C_{\\mathcal{Q}_{T-1}}}\\\\ &{\\qquad=\\displaystyle\\frac{1}{2}\\|x\\|_{Q}^{2}+\\frac{1}{2}\\|u\\|_{R}^{2}+\\frac{1}{2}\\|A x+B u\\|_{\\eta\\Pi(\\Sigma^{-1}-\\eta\\Pi)^{-1}\\Pi+\\Pi}^{2}+C_{\\mathcal{Q}_{T-1}}}\\\\ &{\\qquad=\\displaystyle\\frac{1}{2}\\|x\\|_{Q}^{2}+\\frac{1}{2}\\|u\\|_{R}^{2}+\\frac{1}{2}\\|A x+B u\\|_{\\Pi(I-\\eta\\Sigma\\Pi)^{-1}}^{2}+C_{\\mathcal{Q}_{T-1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the constant $C_{\\mathcal{Q}_{T-1}}$ is independent of $(x,u)$ . Now, we adopt a completion of squares argument again: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2_{T-1}(x,u)=\\frac{1}{2}\\left(\\|u\\|_{H^{\\bot}B^{\\top}\\Pi(I-\\eta\\Sigma\\Pi)^{-1}B}^{2}+2x^{\\top}A^{\\top}\\Pi(I-\\eta\\Pi\\Sigma)^{-1}B u+\\|x\\|_{Q+A^{\\top}\\Pi(I-\\eta\\Sigma\\Pi)^{-1}A}^{2}\\right)}\\\\ &{\\qquad\\qquad\\qquad+C_{Q-r_{-1}}}\\\\ &{\\qquad=\\frac{1}{2}\\bigg(\\|u+(R+B^{\\top}\\Pi(I-\\eta\\Sigma\\Pi)^{-1}B)^{-1}B^{\\top}(I-\\eta\\Pi\\Sigma)^{-1}\\Pi A x\\|_{R+B^{\\top}\\Pi(I-\\eta\\Sigma\\Pi)^{-1}B}^{2}}\\\\ &{\\qquad\\qquad\\quad-\\|B^{\\top}(I-\\eta\\Pi\\Sigma)^{-1}\\Pi A x\\|_{(R+B^{\\top}\\Pi(I-\\eta\\Sigma\\Pi)^{-1}B)^{-1}}^{2}+\\|x\\|_{Q+A^{\\top}\\Pi(I-\\eta\\Sigma\\Pi)^{-1}A}^{2}\\bigg)}\\\\ &{\\qquad\\qquad\\quad+C_{Q-r_{-1}}}\\\\ &{\\qquad=\\frac{1}{2}\\|u+(R+B^{\\top}\\Pi_{T}(I-\\eta\\Sigma\\Pi_{T})^{-1}B)^{-1}B^{\\top}\\Pi_{T}(I-\\eta\\Sigma\\Pi_{T})^{-1}A x\\|_{R+B^{\\top}\\Pi_{T}(I-\\eta\\Sigma\\Pi)^{-1}}^{2}}\\\\ &{\\qquad\\qquad\\quad+\\frac{1}{2}\\|x\\|_{\\Pi_{T-1}}^{2}+C_{Q-r_{-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here, we used $\\Pi_{T}(I-\\eta\\Sigma_{T-1}\\Pi_{T})^{-1}=(I-\\eta\\Pi_{T}\\Sigma_{T-1})^{-1}\\Pi_{T}$ and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Pi_{T-1}=Q_{T-1}+A_{T-1}^{\\top}\\Pi_{T}\\big(I-\\eta\\Sigma_{T-1}\\Pi_{T}+B_{T-1}R_{T-1}^{-1}B_{T-1}^{\\top}\\Pi_{T}\\big)^{-1}A_{T-1}}\\\\ &{\\qquad=Q+A^{\\top}\\Pi_{T}\\big(I-\\eta\\Sigma_{T-1}\\Pi_{T}\\big)^{-1}A-A^{\\top}\\Pi_{T}\\big(I-\\eta\\Sigma_{T-1}\\Pi_{T}\\big)^{-1}B}\\\\ &{\\qquad\\quad\\times\\left(R_{T-1}+B^{\\top}\\Pi_{T}\\big(I-\\eta\\Sigma_{T-1}\\Pi_{T}\\big)^{-1}B\\right)^{-1}B^{\\top}\\big(I-\\eta\\Pi_{T}\\Sigma_{T-1}\\big)^{-1}\\Pi_{T}A.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, the optimal policy at $t=T-1$ is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\pi_{T-1}^{*}(u|x)=\\mathcal{N}\\big(u\\big|-\\big(R_{T-1}+B^{\\top}\\Pi_{T}(I-\\eta\\Sigma_{T-1}\\Pi_{T})^{-1}B\\big)^{-1}B^{\\top}\\Pi_{T}\\big(I-\\eta\\Sigma_{T-1}\\Pi_{T}\\big)^{-1}A x,}&{}&\\\\ {(R_{T-1}+B^{\\top}\\Pi_{T}(I-\\eta\\Sigma_{T-1}\\Pi_{T})^{-1}B\\big)^{-1}\\big).}&{}&{\\mathrm{(64)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The value function is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\nV_{T-1}(x)=-\\log\\left[\\int_{{\\mathbb R}^{n_{u}}}\\exp(-{\\mathscr Q}_{T-1}(x,u))\\mathrm d u\\right]=\\frac{1}{2}\\|x\\|_{{\\Pi}_{T-1}}^{2}+C_{V_{T-1}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $C_{V_{T-1}}$ does not depend on $x$ ", "page_idx": 17}, {"type": "text", "text": "By applying the same argument as above for $t=T-2,\\ldots,0$ , we arrive at the optimal policy (60) and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle V_{t}(x)=\\frac{1}{2}\\|x\\|_{\\Pi_{t}}^{2}+C_{V_{t}}},}\\\\ {{\\displaystyle Q_{t}(x,u)}}\\\\ {{\\displaystyle=\\frac{1}{2}\\|u+(R_{t}+B^{\\top}\\Pi_{t+1}(I-\\eta\\Sigma_{t}\\Pi_{t+1})^{-1}B)^{-1}B^{\\top}\\Pi_{t+1}(I-\\eta\\Sigma_{t}\\Pi_{t+1})^{-1}A x\\|_{R_{t}+B^{\\top}\\Pi_{t+1}}^{2}(I-\\eta\\Sigma_{t}\\Pi_{t+1})^{-1}B^{\\top}}}\\\\ {{\\displaystyle\\quad+\\\\frac{1}{2}\\|x\\|_{\\Pi_{t}}^{2}+C_{Q_{t}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $C_{V_{t}}$ and $C_{\\mathcal{Q}_{t}}$ are independent of $(x,u)$ . This completes the proof. ", "page_idx": 17}, {"type": "text", "text": "By the same argument as above, the optimal policy of the Renyi entropy regularized risk-sensitive control problem (17) in the linear quadratic Gaussian setting is also given by (60). ", "page_idx": 17}, {"type": "text", "text": "D Proof of Lemma 5 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "First, we give the precise statement of Lemma 5. To this end, for $a,b\\in\\mathbb{R}$ ,define ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathfrak{Z}_{a,b}(\\mathbb{U}):=\\left\\{g:\\mathbb{U}\\to\\mathbb{R}\\,\\left|\\,g\\mathrm{~is~bounded~below,~}\\int_{\\mathbb{U}}\\exp(a g(u))\\mathrm{d}u<\\infty,\\,\\int_{\\mathbb{U}}\\exp(b g(u))\\mathrm{d}u<\\infty\\right.\\right\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similarly, define $\\overline{{B}}_{a,b}(\\mathbb{U})$ for upper bounded functions. For given $g~:~\\mathbb{U}~\\rightarrow~\\mathbb{R}$ $a~\\in~\\mathbb{R}$ , and $\\alpha\\in\\mathbb{R}\\setminus\\{0,1\\}$ ,define ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{P}_{a,g}(\\mathbb{U}):=\\bigg\\{\\rho\\in\\mathcal{P}(\\mathbb{U})\\ \\bigg|\\,\\int_{\\mathbb{U}}\\exp(a g(u))\\rho(u)\\mathrm{d}u<\\infty\\bigg\\}\\,,}\\\\ &{\\quad L^{\\alpha}(\\mathbb{U}):=\\bigg\\{\\rho\\in\\mathcal{P}(\\mathbb{U})\\ \\bigg|\\,\\int_{\\mathbb{U}}\\rho(u)^{\\alpha}\\mathrm{d}u<\\infty\\bigg\\}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "If $\\rho\\,\\in\\,L^{\\alpha}(\\mathbb{U})$ and $\\alpha\\in(0,1)$ , then it holds that $\\mathcal{H}_{\\alpha}(\\rho)<\\infty$ . If $\\alpha\\in(-\\infty,0)\\cap(1,\\infty)$ , we have $\\mathcal{H}_{\\alpha}(\\rho)>-\\infty$ ", "page_idx": 18}, {"type": "text", "text": "Now, we are ready to state the duality lemma. ", "page_idx": 18}, {"type": "text", "text": "Lemma 10. For $\\beta,\\gamma\\in\\mathbb{R}\\setminus\\{0\\}$ such that $\\beta<\\gamma$ and for $g\\in\\underline{{\\mathcal{B}}}_{\\{\\beta,-(\\gamma-\\beta)\\}}(\\mathbb{U}),$ it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{3}\\log\\left[\\int_{\\mathbb{U}}\\exp(\\beta g(u))\\mathrm{d}u\\right]=\\operatorname*{inf}_{\\rho\\in L^{1-\\frac{\\gamma}{\\gamma-\\beta}}(\\mathbb{U})}\\left\\{\\frac{1}{\\gamma}\\log\\left[\\int_{\\mathbb{U}}\\exp(\\gamma g(u))\\rho(u)\\mathrm{d}u\\right]-\\frac{1}{\\gamma-\\beta}\\mathcal{H}_{1-\\frac{\\gamma}{\\gamma-\\beta}}(\\rho)\\right\\},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and the unique optimal solution that minimizes the right-hand side of (68) is given by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\rho(u)=\\frac{\\exp\\left(-(\\gamma-\\beta)g(u)\\right)}{\\int_{\\mathbb{U}}\\exp(-(\\gamma-\\beta)g(u^{\\prime}))\\mathrm{d}u^{\\prime}},\\;\\;u\\in\\mathbb{U}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In addition, for $h\\in\\overline{{B}}_{\\{\\gamma,\\gamma-\\beta\\}}(\\mathbb{U})$ it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{\\gamma}\\log\\left[\\int\\exp(\\gamma h(u))\\mathrm{d}u\\right]=\\operatorname*{sup}_{\\rho\\in L^{\\frac{\\gamma}{\\gamma-\\beta}}(\\mathbb{U})}\\left\\{\\frac{1}{\\beta}\\log\\left[\\int\\exp(\\beta h(u))\\rho(u)\\mathrm{d}u\\right]+\\frac{1}{\\gamma-\\beta}\\mathcal{H}_{\\frac{\\gamma}{\\gamma-\\beta}}(\\rho)\\right\\},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and the unique optimal solution that maximizes the right-hand side of (70) is given by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\rho(u)=\\frac{\\exp({(\\gamma-\\beta)h(u)})}{\\int\\exp({(\\gamma-\\beta)h(u^{\\prime})})\\mathrm{d}u^{\\prime}},\\,\\,\\,u\\in\\mathbb{U}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Although the proof is similar to that of the duality between exponential integrals and R\u00e9nyi divergence [40], it requires more careful analysis because we do not assume the upper boundedness of $g$ and the lower boundedness of $h$ , unlike in [40]. ", "page_idx": 18}, {"type": "text", "text": "Proof. For notational simplicity, we often drop $\\mathbb{U}$ as $L^{\\alpha}$ . First, we note that it is sufficient to prove that for $\\alpha>0,\\alpha\\neq1$ \uff0c $g\\in\\underline{{B}}_{\\{\\alpha-1,-1\\}}$ , and $h\\in\\overline{{\\mathcal{B}}}_{\\{\\alpha,1\\}}$ , it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{\\alpha-1}\\log\\left[\\int\\exp((\\alpha-1)g(u))\\mathrm{d}u\\right]=\\operatorname*{inf}_{\\rho\\in L^{1-\\alpha}}\\left\\{\\frac{1}{\\alpha}\\log\\left[\\int\\exp(\\alpha g(u))\\rho(u)\\mathrm{d}u\\right]-\\mathcal{H}_{1-\\alpha}(\\rho)\\right\\},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{\\alpha}\\log\\left[\\int\\exp(\\alpha h(u))\\mathrm{d}u\\right]=\\operatorname*{sup}_{\\rho\\in L^{\\alpha}}\\left\\{\\frac{1}{\\alpha-1}\\log\\left[\\int\\exp((\\alpha-1)h(u))\\rho(u)\\mathrm{d}u\\right]+\\mathcal{H}_{\\alpha}(\\rho)\\right\\},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\rho^{*}(u):=\\frac{\\exp(-g(u))}{\\int\\exp(-g(u^{\\prime}))\\mathrm{d}u^{\\prime}},\\;\\;\\rho^{**}(u):=\\frac{\\exp(h(u))}{\\int\\exp(h(u^{\\prime}))\\mathrm{d}u^{\\prime}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "are the unique optimal solutions to (72), (73), respectively. To see this, note that if (72), (73) hold for $\\alpha>0,\\alpha\\neq1$ they hold for any $\\alpha\\in\\mathbb{R}\\setminus\\{0,1\\}$ . Indeed, when $\\alpha<0$ , let $\\bar{\\alpha}:=1-\\alpha>1$ and for $h\\in\\overline{{\\mathcal{B}}}_{\\{\\alpha,1\\}}$ , let $\\bar{g}:=-h$ . Since $\\bar{g}\\in\\underline{{\\mathcal{B}}}_{\\{\\bar{\\alpha}-1,-1\\}}$ , by (72), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{\\bar{\\alpha}-1}\\log\\left[\\int\\exp((\\bar{\\alpha}-1)\\bar{g}(u))\\mathrm{d}u\\right]=\\operatorname*{inf}_{\\rho\\in L^{1-\\bar{\\alpha}}}\\left\\{\\frac{1}{\\bar{\\alpha}}\\log\\left[\\int\\exp(\\bar{\\alpha}\\bar{g}(u))\\rho(u)\\mathrm{d}u\\right]-\\mathcal{H}_{1-\\bar{\\alpha}}(\\rho)\\right\\}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{-\\frac{1}{\\alpha}\\log\\left[\\int\\exp(\\alpha h(u))\\mathrm{d}u\\right]=\\operatorname*{inf}_{\\rho\\in L^{\\alpha}}\\left\\{\\frac{1}{1-\\alpha}\\log\\left[\\int\\exp((\\alpha-1)h(u))\\rho(u)\\mathrm{d}u\\right]-\\mathcal{H}_{\\alpha}(\\rho)\\right\\}}}&{}&\\\\ &{=-\\operatorname*{sup}_{\\rho\\in L^{\\alpha}}\\left\\{\\frac{1}{\\alpha-1}\\log\\left[\\int\\exp((\\alpha-1)h(u))\\rho(u)\\mathrm{d}u\\right]+\\mathcal{H}_{\\alpha}(\\rho)\\right\\},}&{}&\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which means that for any $\\alpha\\,<\\,0$ and any $h\\,\\in\\,\\overline{{\\beta}}_{\\alpha,1}$ , (73) holds. Similarly, by considering $\\bar{h}:=$ $-g\\in\\underline{{B}}_{\\{\\bar{\\alpha},1\\}}$ for $g\\in\\underline{{B}}_{\\{\\alpha-1,-1\\}}$ , we can see that for any $\\alpha<0$ and any $g\\in\\underline{{B}}_{\\{\\alpha-1,-1\\}}$ , (72) holds. Additionally, (72) and (73) with $\\begin{array}{r}{\\alpha=\\frac{\\gamma}{\\gamma-\\beta}}\\end{array}$ \u03b2, g = ( - \u03b2)g, h = (- \u03b2)h coincide with(68), (70) where $g$ and $h$ are replaced by $\\widetilde{g},\\widetilde{h}$ ", "page_idx": 19}, {"type": "text", "text": "In what follows, for $\\alpha>0$ $\\alpha\\neq1$ we prove (72). Note that when $\\rho\\in L^{1-\\alpha}$ \uff0c $|\\mathcal{H}_{1-\\alpha}(\\rho)|<\\infty$ holds. Hence, for the minimization of (72), it is sufficient to consider $\\rho\\in\\mathcal{P}_{\\alpha,g}\\cap L^{1-\\alpha}$ . The density $\\rho^{*}$ defined in (74) fulfils $\\rho^{*}\\in\\mathcal{P}_{\\alpha,g}\\cap L^{1-\\alpha}$ because $g\\in\\underline{{B}}_{\\{\\alpha-1,-1\\}}$ , and it can be easily seen that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{\\alpha-1}\\log\\left[\\int\\exp((\\alpha-1)g(u))\\mathrm{d}u\\right]=\\frac{1}{\\alpha}\\log\\left[\\int\\exp(\\alpha g(u))\\rho^{\\ast}(u)\\mathrm{d}u\\right]-\\mathcal{H}_{1-\\alpha}(\\rho^{\\ast}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "First, we consider the case $\\alpha>1$ Define $\\widetilde{\\rho}(u):=\\exp((\\alpha-1)g(u))$ \uff0c $\\varphi(u):=\\exp(-g(u))$ . Then, by Holder's inequality, for any $\\rho\\in\\mathcal{P}_{\\alpha,g}\\cap L^{1-\\alpha}$ , it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int\\widetilde\\rho(u)\\mathrm{d}u=\\int\\left(\\frac{\\varphi(u)}{\\rho(u)}\\right)^{\\frac{\\alpha-1}{\\alpha}}\\left(\\frac{\\rho(u)}{\\varphi(u)}\\right)^{\\frac{\\alpha-1}{\\alpha}}\\widetilde\\rho(u)\\mathrm{d}u}\\\\ &{\\qquad\\qquad\\le\\left(\\int\\left(\\frac{\\varphi(u)}{\\rho(u)}\\right)^{\\alpha-1}\\widetilde\\rho(u)\\mathrm{d}u\\right)^{\\frac{1}{\\alpha}}\\left(\\int\\frac{\\rho(u)}{\\varphi(u)}\\widetilde\\rho(u)\\mathrm{d}u\\right)^{\\frac{\\alpha-1}{\\alpha}}}\\\\ &{\\qquad\\qquad=\\left(\\int\\rho(u)^{1-\\alpha}\\mathrm{d}u\\right)^{\\frac{1}{\\alpha}}\\left(\\int\\exp(\\alpha g(u))\\rho(u)\\mathrm{d}u\\right)^{\\frac{\\alpha-1}{\\alpha}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Noting that $\\alpha-1>0$ and taking the logarithm of (76), we get for any $\\rho\\in\\mathcal{P}_{\\alpha,g}\\cap L^{1-\\alpha}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{\\alpha-1}\\log\\left[\\int\\exp((\\alpha-1)g(u))\\mathrm{d}u\\right]\\leq\\frac{1}{\\alpha}\\log\\left[\\int\\exp(\\alpha g(u))\\rho(u)\\mathrm{d}u\\right]-\\mathcal{H}_{1-\\alpha}(\\rho).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combining this with (75), the relation (72) holds, and by (75), $\\rho^{*}$ in (74) is an optimal solution. The equality of Holder's inequality (76) holds if and only if there exist $a_{1},a_{2}\\geq0$ $a_{1}a_{2}\\neq0$ such that $\\begin{array}{r}{a_{1}\\left(\\frac{\\varphi(u)}{\\rho(u)}\\right)^{1-\\alpha}=a_{2}\\frac{\\rho(u)}{\\varphi(u)}}\\end{array}$ holds $\\widetilde{\\mu}$ almost everywhere. Here, $\\widetilde{\\mu}$ is the measure defined by $\\widetilde{\\rho}$ This condition is satisfied only for $\\rho^{*}$ , that is, it is an unique optimal solution. ", "page_idx": 19}, {"type": "text", "text": "Next, we analyze the case $\\alpha\\in(0,1)$ ByHoler's inequalty, fr ay $\\rho\\in\\mathcal P_{\\alpha,g}$ \uff0c ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int\\left(\\frac{\\varphi(u)}{\\rho(u)}\\right)^{\\alpha-1}\\widetilde{\\rho}(u)\\mathrm{d}u\\leq\\left(\\int1^{1/\\alpha}\\widetilde{\\rho}(u)\\mathrm{d}u\\right)^{\\alpha}\\left(\\int\\left[\\left(\\frac{\\varphi(u)}{\\rho(u)}\\right)^{\\alpha-1}\\right]^{\\frac{1}{1-\\alpha}}\\widetilde{\\rho}(u)\\mathrm{d}u\\right)^{1}}\\\\ &{\\qquad\\qquad\\qquad=\\left(\\int\\widetilde{\\rho}(u)\\mathrm{d}u\\right)^{\\alpha}\\left(\\int\\frac{\\rho(u)}{\\varphi(u)}\\widetilde{\\rho}(u)\\mathrm{d}u\\right)^{1-\\alpha},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which yields ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{\\alpha-1}\\log\\left[\\int\\exp((\\alpha-1)g(u))\\mathrm{d}u\\right]\\leq\\frac{1}{\\alpha}\\left[\\int\\exp(\\alpha g(u))\\rho(u)\\mathrm{d}u\\right]-\\mathcal{H}_{1-\\alpha}(\\rho),\\;\\;\\forall\\rho\\in\\mathcal{P}_{\\alpha,g}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, similar to the case $\\alpha\\,>\\,1$ , it can be seen that for $\\alpha\\in(0,1)$ , (72) holds and $\\rho^{*}$ is a unique optimal solution. ", "page_idx": 19}, {"type": "text", "text": "Next, we show (73) for $\\alpha>1$ . Since $\\alpha>1$ and $h$ is upper bounded, it holds that $\\rho\\in\\mathcal P_{\\alpha-1,h}$ . The density $\\rho^{**}$ defined in (74) satisfies $\\rho^{**}\\in\\mathcal{P}_{\\alpha-1,h}\\cap L^{\\alpha}$ because $h\\in B_{\\{\\alpha,1\\}}$ , and one can easily see that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{1}{\\alpha}\\log\\left[\\int\\exp(\\alpha h(u))\\mathrm{d}u\\right]=\\frac{1}{\\alpha-1}\\log\\left[\\int\\exp((\\alpha-1)h(u))\\rho^{**}(u)\\mathrm{d}u\\right]+\\mathcal{H}_{\\alpha}(\\rho^{**}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Define $\\widehat{\\rho}(u):=\\exp((\\alpha-1)h(u))\\rho(u),\\lambda(u):=\\exp(-h(u))\\rho(u)$ . Then, by Holder's inequality, for any $\\rho\\in L^{\\alpha}$ , it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int\\widehat{\\rho}(u)\\mathrm{d}u=\\int\\lambda(u)^{\\frac{\\alpha-1}{\\alpha}}\\lambda(u)^{-\\frac{\\alpha-1}{\\alpha}}\\widehat{\\rho}(u)\\mathrm{d}u}\\\\ &{\\qquad\\qquad\\leq\\left(\\int\\lambda(u)^{\\alpha-1}\\widehat{\\rho}(u)\\mathrm{d}u\\right)^{\\frac{1}{\\alpha}}\\left(\\int\\lambda(u)^{-1}\\widehat{\\rho}(u)\\mathrm{d}u\\right)^{\\frac{\\alpha-1}{\\alpha}}}\\\\ &{\\qquad\\qquad=\\left(\\int\\rho(u)^{\\alpha}\\mathrm{d}u\\right)^{\\frac{1}{\\alpha}}\\left(\\int\\exp(\\alpha h(u))\\mathrm{d}u\\right)^{\\frac{\\alpha-1}{\\alpha}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "It follows from the above that for any $\\rho\\in L^{\\alpha}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{1}{\\alpha-1}\\log\\left[\\int\\exp((\\alpha-1)h(u))\\rho(u)\\mathrm{d}u\\right]\\leq\\frac{1}{\\alpha}\\log\\left[\\int\\exp(\\alpha h(u))\\mathrm{d}u\\right]-\\mathcal{H}_{\\alpha}(\\rho).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence, by the same argument as for (72), we can show that (73) holds for $\\alpha>1$ , and $\\rho^{**}$ is a unique optimal solution. ", "page_idx": 20}, {"type": "text", "text": "Lastly, we show (73) for $\\alpha\\in(0,1)$ . For $\\rho\\in L^{\\alpha}$ , it holds that $|\\mathcal{H}_{\\alpha}(\\rho)|<\\infty$ . Then, noting that $\\alpha-1<0$ , it is sufficient to perform the maximization in (73) for $\\rho\\in\\mathcal{P}_{\\alpha-1,h}\\cap L^{\\alpha}$ . By Holder's inequality, for any $\\rho\\in\\mathcal P_{\\alpha-1,h}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int\\rho^{\\alpha}\\mathrm{d}u=\\int\\lambda(u)^{\\alpha-1}\\hat{\\rho}(u)\\mathrm{d}u\\le\\left(\\int1^{1/\\alpha}\\hat{\\rho}(u)\\mathrm{d}u\\right)^{\\alpha}\\left((\\lambda(u)^{\\alpha-1})^{\\frac{1}{1-\\alpha}}\\hat{\\rho}(u)\\mathrm{d}u\\right)^{1-\\alpha}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\left(\\int\\exp((\\alpha-1)h(u))\\rho(u)\\mathrm{d}u\\right)^{\\alpha}\\left(\\int\\exp(\\alpha h(u))\\mathrm{d}u\\right)^{1-\\alpha}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{1}{\\alpha-1}\\log\\left[\\int\\exp((\\alpha-1)h(u))\\rho(u)\\mathrm{d}u\\right]\\leq\\frac{1}{\\alpha}\\log\\left[\\int\\exp(\\alpha h(u))\\mathrm{d}u\\right]-\\mathcal{H}_{\\alpha}(\\rho),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and similar to the case $\\alpha>1$ , we arrive at (73) for $\\alpha\\in(0,1)$ , and the unique optimal solution is $\\rho^{**}$ This completes the proof. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "E Proof of Theorem 6 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this appendix, we analyze the following problem: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{minimize}_{\\{\\pi_{t}\\}_{t=0}^{T-1}}\\;\\frac{1}{\\eta}\\log\\mathbb{E}\\left[\\exp\\left(\\eta c_{T}(x_{T})+\\eta\\sum_{t=0}^{T-1}\\Bigl(c_{t}(x_{t},u_{t})-\\varepsilon\\mathcal{H}_{1-\\varepsilon\\eta}(\\pi_{t}(\\cdot|x_{t}))\\Bigr)\\right)\\right],\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\varepsilon>0$ $\\eta\\in\\mathbb{R}\\setminus\\{0,\\varepsilon^{-1}\\}$ , the system is given by (48)-(50), and $\\pi_{t}(\\cdot|x)\\in L^{1-\\varepsilon\\eta}(\\mathbb{U}):=\\{\\rho\\in$ $\\begin{array}{r}{\\mathcal{P}(\\mathbb{U})\\mid\\int_{\\mathbb{U}}\\rho(\\dot{u})^{1-\\varepsilon\\eta}\\mathrm{d}\\dot{u}<\\infty\\}}\\end{array}$ for any $x\\in\\mathbb{X}$ and $t\\in[0,T-1]$   \nDefine the value function and the Q-function associated with (78) as   \n$\\begin{array}{l l}{\\displaystyle\\mathcal{H}_{t}(x):=\\operatorname*{inf}_{\\{\\pi_{s}\\}_{s=t}^{T-1}}\\frac{1}{\\eta}\\log\\mathbb{E}\\left[\\exp\\left(\\eta c_{T}(x_{T})+\\eta\\sum_{s=t}^{T-1}\\Bigl(c_{s}(x_{s},u_{s})-\\varepsilon\\mathcal{H}_{1-\\varepsilon\\eta}(\\pi_{s}(\\cdot|x_{s}))\\Bigr)\\right)\\Bigg|\\ x_{t}=x\\right]}\\\\ {\\displaystyle\\mathcal{H}_{T}(x):=c_{T}(x),\\ \\ x\\in\\mathbb{X},}\\\\ {\\displaystyle\\mathcal{Z}_{t}(x,u):=c_{t}(x,u)+\\frac{1}{\\eta}\\log\\mathbb{E}\\left[\\exp\\bigl(\\eta\\mathcal{H}_{t+1}\\bigl(f_{t}(x,u,w_{t})\\bigr)\\bigr)\\right],\\ \\ t\\in[0,T-1],\\ x\\in\\mathbb{X},\\ u\\in\\mathbb{U}.}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "For the analysis, we assume the following conditions. ", "page_idx": 20}, {"type": "text", "text": "Assumption 12. The Q-function $\\mathcal{L}_{t}$ in (80) satisfies ", "text_level": 1, "page_idx": 21}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{U}}\\exp\\left(-\\frac{\\mathscr{Q}_{t}(x,u)}{\\varepsilon}\\right)\\mathrm{d}u<\\infty,~~\\int_{\\mathbb{U}}\\exp\\left(-(1-\\varepsilon\\eta)\\frac{\\mathscr{Q}_{t}(x,u)}{\\varepsilon}\\right)\\mathrm{d}u<\\infty\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for any $x\\in\\mathbb{X}$ and $t\\in[0,T-1]$ ", "page_idx": 21}, {"type": "text", "text": "For example, when $c_{t}$ is bounded for any $t\\ \\in\\ [0,T]$ \uff0c $\\mathcal{L}_{t}$ is also bounded, and in addition, if $\\mu_{L}(\\mathbb{U})\\,<\\,\\infty$ , (81) holds. In the linear quadratic setting, Assumption 12 also holds without the boundedness of $c_{t}$ and $\\mathbb{U}$ ", "page_idx": 21}, {"type": "text", "text": "Now, we prove Theorem 6 by induction. First, for $t=T-1$ ,wehave ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\ell_{T-1}(x)=\\underset{\\pi_{T-1}(\\cdot\\vert x)\\in L^{1-\\varepsilon\\eta}(\\mathbb{U})}{\\operatorname*{inf}}\\Biggl\\{-\\varepsilon\\mathcal{H}_{1-\\varepsilon\\eta}(\\pi_{T-1}(\\cdot\\vert x))}\\\\ {\\quad\\qquad+\\,\\frac{1}{\\eta}\\log\\left[\\int_{\\mathbb{U}}\\pi_{T-1}(u\\vert x)\\mathbb{E}\\bigl[\\exp\\left(\\eta c_{T-1}(x,u)+\\eta c_{T}(x_{T})\\right)\\ \\big\\vert\\ x_{T-1}=x,\\ u_{T-1}=u\\bigr]\\ensuremath{\\mathrm{d}}u\\Bigr]\\Biggr\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The derivation is same as (85) and (86). By the definition of the Q-function in (80), it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\ell_{T-1}(x)=\\operatorname*{inf}_{\\pi_{T-1}(\\cdot|x)\\in L^{1-\\varepsilon\\eta}(\\mathbb{U})}\\left\\{\\frac{1}{\\eta}\\log\\left[\\int_{\\mathbb{U}}\\pi_{T-1}(u|x)\\exp(\\eta\\mathcal{Q}_{T-1}(x,u))\\mathrm{d}u\\right]-\\varepsilon\\mathcal{H}_{1-\\varepsilon\\eta}(\\pi_{T-1}(\\cdot|x))\\right\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $c_{T}$ and $c_{T-1}$ are bounded below, $\\mathcal{Q}_{T-1}$ is also bounded below. Therefore, by Assumption 12, $\\mathcal{Q}_{T-1}(x,\\cdot)\\in\\underline{{\\mathcal{B}}}_{-(\\varepsilon^{-1}-\\eta),-\\varepsilon^{-1}}(\\mathbb{U})$ (see (67) for the defnition of $\\underline{{\\beta}}_{a,b}$ ), and we can apply Lemma 10 with $\\beta=-(\\varepsilon^{-1}-\\eta)$ \uff0c $\\gamma=\\eta$ to (82). As a result, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{V}_{T-1}(x)=\\frac{-1}{\\varepsilon^{-1}-\\eta}\\log\\left[\\int_{\\mathbb{U}}\\exp\\left(-(\\varepsilon^{-1}-\\eta)\\mathcal{Q}_{T-1}(x,u)\\right)\\mathrm{d}u\\right],\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and the unique optimal policy that minimizes the right-hand side of (82) is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\pi_{T-1}^{\\star}(u|x)=\\frac{\\exp\\left(-\\frac{\\mathcal{Q}_{T-1}(x,u)}{\\varepsilon}\\right)}{\\int_{\\mathbb{U}}\\exp\\left(-\\frac{\\mathcal{Q}_{T-1}(x,u^{\\prime})}{\\varepsilon}\\right)\\mathrm{d}u^{\\prime}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Moreover, since $\\mathcal{Q}_{T-1}$ is bounded below, $\\mathcal{V}_{T-1}$ is also bounded below. ", "page_idx": 21}, {"type": "text", "text": "Next, we assume the indution hypothstha for some $t\\in[0,T-2]$ \uff0c $\\{\\pi_{s}^{\\star}\\}_{s=t+1}^{T-1}$ isthe unqgue optimal policy of the minimization in the definition of $\\mathcal{V}_{t+1}$ , and $\\mathcal{V}_{t+1}$ is bounded below. By definition, ", "page_idx": 21}, {"type": "text", "text": "we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{\\theta}(x)=\\underset{(u_{k},u_{k}^{*})\\in\\mathcal{V}_{r}}{\\sum}\\Bigg[\\mathrm{sg}_{r}\\Bigg[\\mathrm{exp}_{\\theta}\\Big(p_{r}(x_{k},u_{k})-c\\theta\\Big)\\mathrm{f}_{r}(x_{k},u_{k})+\\mathrm{pr}(x_{k})}\\\\ &{\\qquad\\qquad\\quad+\\mathrm{p}_{\\theta}\\sum_{i=1}^{r_{k}}\\bigg(c_{k}(x_{k},u_{k})-c R_{1-1}(u_{k},\\cdot(x_{k}))\\bigg)\\Bigg]\\Bigg]_{1}=x\\Bigg]}\\\\ &{\\!=\\!\\!\\!\\!\\frac{1}{r_{k}\\!\\!\\!\\prod_{0}^{n}{-1}\\sigma}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Moreover, noting that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\exp(\\eta\\mathcal{V}_{t+1}(x))=\\mathbb{E}_{\\{\\pi_{s}^{\\star}\\}_{s=t+1}^{T-1}}\\left[\\exp\\left(\\eta c_{T}(x_{T})+\\eta\\sum_{s=t+1}^{T-1}\\left(c_{s}(x_{s},u_{s})-\\varepsilon\\mathcal{H}_{1-\\varepsilon\\eta}(\\pi_{s}^{\\star}(\\cdot|x_{s}))\\right)\\right)\\Bigg|\\;x_{t+1}=0\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathcal{V}_{t}(x)=\\operatorname*{inf}_{\\pi_{t}(\\cdot\\vert x)\\in L^{1-\\varepsilon\\eta}(\\mathbb{U})}-\\varepsilon\\mathcal{H}_{1-\\varepsilon\\eta}(\\pi_{t}(\\cdot\\vert x))}\\\\ {\\phantom{\\frac{1}{\\eta_{t}}}+\\frac{1}{\\eta}\\log\\left[\\int\\pi_{t}(u\\vert x)\\exp(\\eta c_{t}(x,u))\\mathbb{E}\\left[\\exp\\bigl(\\eta\\mathcal{H}_{t+1}(f_{t}(x,u,w_{t}))\\bigr)\\right]\\mathrm{d}u\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By using $\\mathcal{L}_{t}$ , the above equation can be written as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{H}(x)=\\operatorname*{inf}_{\\pi_{t}(\\cdot|x)\\in L^{1-\\varepsilon\\eta}(\\mathbb{U})}\\frac{1}{\\eta}\\log\\left[\\int_{\\mathbb{U}}\\pi_{t}(u|x)\\exp(\\eta\\mathcal{Q}_{t}(x,u))\\mathrm{d}u\\right]-\\varepsilon\\mathcal{H}_{1-\\varepsilon\\eta}(\\pi_{t}(\\cdot|x)).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since we assumed that $\\mathcal{V}_{t+1}$ is bounded below, $\\mathcal{L}_{t}$ is also bounded below. By combining this with Assumption 12, it holds that $\\mathcal{Q}_{t}(x,\\cdot)\\in\\underline{{\\mathcal{B}}}_{-(\\varepsilon^{-1}-\\eta),-\\varepsilon^{-1}}(\\mathbb{U})$ Thus, by Lemma 10, the unique optimal policy that minimizes the right-hand side of the above equation is ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\pi_{t}^{\\star}(u|x)=\\frac{\\exp\\left(-\\frac{\\mathcal{Q}_{t}(x,u)}{\\varepsilon}\\right)}{\\int_{\\mathbb{U}}\\exp\\left(-\\frac{\\mathcal{Q}_{t}(x,u^{\\prime})}{\\varepsilon}\\right)\\mathrm{d}u^{\\prime}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{V}_{t}(x)=\\frac{-1}{\\varepsilon^{-1}-\\eta}\\log\\left[\\int_{\\mathbb{U}}\\exp\\left(-(\\varepsilon^{-1}-\\eta)\\mathcal{Q}_{t}(x,u)\\right)\\mathrm{d}u\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lastly, since $\\mathcal{L}_{t}$ is bounded below, $\\mathcal{V}_{t}$ is also bounded below. This completes the induction step, and we obtain Theorem 6. ", "page_idx": 22}, {"type": "text", "text": "F Proof of Proposition 7 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "By using the relation $\\nabla_{\\theta}\\log p_{\\theta}(\\tau)=\\nabla_{\\theta}p_{\\theta}(\\tau)/p_{\\theta}(\\tau)$ , we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}J(\\theta)=\\int p_{\\theta}(\\tau)\\exp(\\eta C_{\\theta}(\\tau))\\big(\\eta\\nabla_{\\theta}C_{\\theta}(\\tau)+\\nabla_{\\theta}\\log p_{\\theta}(\\tau)\\big)\\mathrm{d}\\tau.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In addition, by the expression ", "page_idx": 23}, {"type": "equation", "text": "$$\np_{\\theta}(\\tau)=p(x_{0})\\prod_{t=0}^{T-1}p(x_{t+1}|x_{t},u_{t})\\pi^{(\\theta)}(u_{t}|x_{t}),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{T}_{\\theta}J(\\theta)=\\displaystyle\\int p_{\\theta}(\\tau)\\exp(\\eta C_{\\theta}(\\tau))\\left(\\eta\\sum_{t=0}^{T-1}\\nabla_{\\theta}\\log\\pi^{(\\theta)}(u_{t}|x_{t})+\\sum_{t=0}^{T-1}\\nabla_{\\theta}\\log\\pi^{(\\theta)}(u_{t}|x_{t})\\right)\\mathrm{d}\\tau}\\\\ &{=(\\eta+1)\\mathbb{E}_{p_{\\theta}(\\tau)}\\left[\\left(\\sum_{t=0}^{T-1}\\nabla_{\\theta}\\log\\pi^{(\\theta)}(u_{t}|x_{t})\\right)\\exp\\left(\\eta c_{T}(x_{T})+\\eta\\sum_{t=0}^{T-1}(c_{t}(x_{t},u_{t})+\\log\\pi^{(\\theta)}(u_{t}|x_{t})\\right)\\right)\\prod_{s=t}^{T-1}\\left(\\prod_{s=t}^{T-1}\\prod_{s=t}^{(s)}\\prod_{s=t}^{(s)}\\left(\\prod_{s=t}^{(s)}\\prod_{s=s}^{(s)}\\prod_{s=t}^{(s)}\\prod_{s=s}^{(s)}\\prod_{s=t}^{(s)}\\prod_{s=s}^{(s)}\\prod_{s=t}^{(s)}\\prod_{s=s}^{(s)}\\prod_{s=t}^{(s)}\\prod_{s=s}^{(s)}\\prod_{s=t}^{(s)}\\prod_{s=s}^{(s)}\\prod_{s=s}^{(s)}\\prod_{s=s}^{(s)}\\left(\\prod_{s=t}^{(s)}\\prod_{s=s}^{(s)}\\prod_{s=s}^{(s)}\\prod_{s=s}^{(s)}\\prod_{s=s}^{(s)}\\prod_{s=s}^{(s)}\\prod_{s=s}^{(s)}\\prod_{s=s}^{(s)}\\prod_{s=s}^{(s)}\\prod_{s=s}^{(s)}\\prod_{s=s}^{(s)}\\prod_{s=s}^{(s)}\\prod_{s=s}^{(s)}\\prod_{s=s}^{(s)}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that for any $h:(\\mathbb{X})^{t+1}\\times(\\mathbb{U})^{t+1}\\to\\mathbb{R}$ , it holds that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[h(\\alpha_{0:t},u_{0:t})\\right]=\\int h(x_{0:t},u_{0:t})p(x_{0:t})\\prod_{\\substack{s=1\\atop s=0}}^{T-1}\\|r(x_{s+1}|x_{s},u_{s})\\pi^{(\\theta)}(u_{s_{1}}|x_{s})\\mathrm{d}x_{0:T}\\mathrm{d}u_{0:T-1}}\\\\ &{\\quad=\\int h(x_{0:t},u_{0:t})p(x_{0:t})\\prod_{\\substack{s=0}}^{T-2}}\\\\ &{\\quad\\quad\\times\\left[\\int p(x_{T}|x_{T-1},u_{T-1})\\pi^{(\\theta)}(u_{T-1}|x_{T-1})\\mathrm{d}x_{T}\\mathrm{d}u_{T-1}\\right]\\mathrm{d}x_{0:T-1}\\mathrm{d}u_{0:T-2}}\\\\ &{\\quad=\\int h(x_{0:t},u_{0:t})p(x_{0:t})\\prod_{\\substack{s=0}}^{T-2}\\prod_{\\substack{s=1\\atop s=0}}^{T-2}\\Big(\\int_{(1:T_{s})}\\Big(\\!x_{s}\\Big_{s}\\Big_{s}\\Big)\\mathrm{d}u_{0:T-1}\\mathrm{d}u_{0:T-2}}\\\\ &{\\quad\\quad\\vdots}\\\\ &{\\quad\\quad=\\int h(x_{0:t},u_{0:t})\\pi^{(\\theta)}(u_{t}|x_{t})p(x_{0:1})\\prod_{\\substack{s=0}}^{t-1}\\int_{(1:T_{s})}^{v}(x_{s+1}|x_{s},u_{s})\\pi^{(\\theta)}(u_{s_{1}}|x_{s})\\mathrm{d}x_{0:T-1}\\mathrm{d}u_{0:t}\\mathrm{d}u_{0:t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "It follows from the above that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{p(\\tau)}\\bigg[\\nabla_{\\Phi}\\|\\mathbf{g}_{\\alpha}\\|_{L^{p}}\\exp\\Bigg(\\prod_{m=0}^{n-1}(c_{*},u_{*})+\\log\\pi^{(\\theta)}(u_{*}|x_{*})\\Bigg)\\Bigg]}\\\\ &{=\\int\\mathbb{C}_{\\Phi}\\log\\pi^{(\\theta)}(u_{*}|x_{*})\\exp\\Bigg(\\prod_{m=0}^{n-1}(c_{*},(x_{*},u_{*})+\\log\\pi^{(\\theta)}(u_{*}|x_{*}))\\Bigg)}\\\\ &{\\quad\\quad\\times\\pi^{(\\theta)}(u_{*}|x_{*})\\exp\\frac{\\hat{t}}{\\displaystyle\\sum_{l=0}^{n-1}(c_{*},(x_{*},u_{*}))^{m}}\\Theta^{(\\theta)}(u_{*}|x_{*})\\log\\pi^{(\\theta)}(u_{*}|x_{*})}\\\\ &{=\\int\\mathbb{C}_{\\Phi}\\pi^{(\\theta)}(u_{*}|x_{*})\\exp\\frac{\\hat{t}}{\\displaystyle\\sum_{s=0}^{n-1}(c_{*},(x_{*},u_{*})+\\log\\pi^{(\\theta)}(u_{*}|x_{*}))}}\\\\ &{\\quad\\quad\\times\\rho(x)\\displaystyle\\prod_{s=0}^{n-1}(c_{*}+|x_{*},u_{*}|)\\pi^{(\\theta)}(u_{*}|x_{*})\\log\\pi^{(\\theta)}}\\\\ &{=\\int\\Big(\\prod_{m=0}^{n-1}\\sum_{\\tau\\in\\mathcal{T}_{s}}\\Big(\\prod_{m=0}^{n-1}\\sum_{\\tau\\in\\mathcal{T}_{s}}\\Big(\\eta_{\\tau}\\Big)_{*}\\Big)\\Big(r_{*}\\Big)_{*}\\Big(c_{*}(x_{*},u_{*})+\\log\\pi^{(\\theta)}(u_{*}|x_{*})\\Big)}\\\\ &{\\quad\\quad\\quad\\times\\rho(x)\\displaystyle\\prod_{m=0}^{n-1}(c_{*}+|x_{*},u_{*}|)\\pi^{(\\theta)}(u_{*}|x_{*})\\log\\pi^{(\\theta)}(u_{*}|x_{*})}\\\\ &{\\quad\\quad\\quad\\times\\rho(x)\\displaystyle\\prod_{m=1}^{n-1}(c_{*}+|x_{*},u \n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By combining this with (90), we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{T}_{\\theta}J(\\theta)}\\\\ &{=(\\eta+1)\\mathbb{E}_{p_{\\theta}(\\tau)}\\left[\\displaystyle\\sum_{t=0}^{T-1}\\nabla_{\\theta}\\log\\pi^{(\\theta)}(u_{t}|x_{t})\\exp\\left(\\eta c_{T}(x_{T})+\\eta\\displaystyle\\sum_{s=t}^{T-1}\\bigl(c_{s}(x_{s},u_{s})+\\log\\pi^{(\\theta)}(u_{s}|x_{s})\\bigr)\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lastly, for any function $b:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ , it holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{p_{\\theta}(\\tau)}[\\nabla_{\\theta}\\log\\pi^{(\\theta)}(u_{t}|x_{t})b(x_{t})]=\\displaystyle\\int p_{\\theta}(x_{t},u_{t})\\frac{\\nabla_{\\theta}\\pi^{(\\theta)}(u_{t}|x_{t})}{\\pi^{(\\theta)}(u_{t}|x_{t})}b(x_{t})\\mathrm{d}x_{t}\\mathrm{d}u_{t}}\\\\ &{\\ =\\displaystyle\\int p(x_{t})b(x_{t})\\nabla_{\\theta}\\pi^{(\\theta)}(u_{t}|x_{t})\\mathrm{d}u_{t}\\mathrm{d}x_{t}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This completes the proof. ", "page_idx": 24}, {"type": "text", "text": "G Proof of Proposition 8 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "By definition, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\pi_{t}^{\\bullet}=\\underset{\\pi_{t}\\in\\mathcal{P}(\\mathbb{U})}{\\arg\\operatorname*{min}}\\,\\frac{1}{\\eta}\\log\\left[\\int p^{\\pi}(\\tau)\\left(\\frac{\\prod_{s=0}^{T-1}\\pi_{s}(u_{s}|x_{s})}{p(O_{T}|x_{T})\\prod_{s=0}^{T-1}p(\\mathcal{O}_{s}|x_{s},u_{s})}\\right)^{\\eta}\\mathrm{d}\\tau\\right].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The term between the brackets is ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int p^{\\pi}(x_{0:t},u_{0:t})}\\\\ &{\\times\\,\\left(\\int p^{\\pi}(x_{t+1:T},u_{t+1:T}|x_{t},u_{t})\\left[\\frac{\\prod_{s=0}^{T-1}\\pi_{s}(u_{s}|x_{s})}{p(O_{T}|x_{T})\\prod_{s=0}^{T-1}p(\\mathcal{O}_{s}|x_{s},u_{s})}\\right]^{\\eta}\\mathrm{d}x_{t+1:T}\\mathrm{d}u_{t+1:T}\\right)\\mathrm{d}x_{0:t}\\mathrm{d}u_{0:t}}\\\\ &{=\\int p^{\\pi}(x_{0:t},u_{0:t})\\left[\\frac{\\prod_{s=0}^{t-1}\\pi_{s}(u_{s}|x_{s})}{\\prod_{s=0}^{t-1}p(\\mathcal{O}_{s}|x_{s},u_{s})}\\right]^{\\eta}}\\\\ &{\\times\\,\\underbrace{\\left(\\int p^{\\pi}(x_{t+1:T},u_{t+1:T}|x_{t},u_{t})\\left[\\frac{\\prod_{s=t}^{T-1}\\pi_{s}(u_{s}|x_{s})}{p(\\mathcal{O}_{t}|x_{T})\\prod_{s=t}^{T-1}p(\\mathcal{O}_{s}|x_{s},u_{s})}\\right]^{\\eta}\\mathrm{d}x_{t+1:T}\\mathrm{d}u_{t+1:T}\\right)}_{=:M}\\mathrm{d}u_{0:t}\\mathrm{d}u_{0:t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where ", "page_idx": 25}, {"type": "equation", "text": "$$\nU=\\pi_{t}(u_{t}|x_{t})^{\\eta}\\int p^{\\pi}(x_{t+1:T},u_{t+1:T}|x_{t},u_{t})\\left[\\frac{\\prod_{s=t+1}^{T-1}\\pi_{s}(u_{s}|x_{s})}{p(\\mathcal{O}_{T}|x_{T})\\prod_{s=t}^{T-1}p(\\mathcal{O}_{s}|x_{s},u_{s})}\\right]^{\\eta}\\mathrm{d}x_{t+1:T}\\mathrm{d}u_{t+1:T}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In addition, by the expression $\\begin{array}{r}{p^{\\pi}(x_{0:t},u_{0:t})=p(x_{0})\\pi_{t}(u_{t}|x_{t})\\prod_{s=0}^{t-1}p(x_{s+1}|x_{s},u_{s})\\pi_{s}(u_{s}|x_{s})}\\end{array}$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi_{t}^{\\bullet}=\\underset{\\pi_{t}}{\\arg\\operatorname*{min}}\\,\\frac{1}{\\eta}\\log\\left[\\int\\pi_{t}(u_{t}|x_{t})^{1+\\eta}\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.\\times\\left.\\mathbb{E}_{p^{\\pi}(x_{t+1:T},u_{t+1:T}|x_{t},u_{t})}\\left[\\left(\\frac{\\prod_{s=t+1}^{T-1}\\pi_{s}(u_{s}|x_{s})}{p(\\mathcal{O}_{t}|x_{T})\\prod_{s=t}^{T-1}p(\\mathcal{O}_{s}|x_{s},u_{s})}\\right)^{\\eta}\\right]\\mathrm{d}x_{t}\\mathrm{d}u_{t}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now, define ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widehat{\\pi}_{t}(u_{t}|x_{t}):=\\frac{1}{Z_{t}(x_{t})}\\left(\\mathbb{E}_{p^{\\pi}(x_{t+1:T},u_{t+1:T}|x_{t},u_{t})}\\left[\\left(\\frac{\\prod_{s=t+1}^{T-1}\\pi_{s}(u_{s}|x_{s})}{p(\\mathcal{O}_{t}|x_{T})\\prod_{s=t}^{T-1}p(\\mathcal{O}_{s}|x_{s},u_{s})}\\right)^{\\eta}\\right]\\right)^{-1/\\eta},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\nZ_{t}(x_{t}):=\\int\\left(\\mathbb{E}_{p^{\\pi}(x_{t+1:T},u_{t+1:T}|x_{t},u_{t})}\\left[\\left(\\frac{\\prod_{s=t+1}^{T-1}\\pi_{s}(u_{s}|x_{s})}{p(\\mathcal{O}_{t}|x_{T})\\prod_{s=t}^{T-1}p(\\mathcal{O}_{s}|x_{s},u_{s})}\\right)^{\\eta}\\right]\\right)^{-1/\\eta}\\mathrm{d}u_{t}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then, (94) can be rewritten as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\pi_{t}^{\\bullet}=\\arg\\operatorname*{min}_{\\pi_{t}}\\frac{1}{\\eta}\\log\\left[\\int_{\\mathbb{X}}Z_{t}(x_{t})^{\\eta}\\int_{\\mathbb{U}}\\widehat{\\pi}_{t}(u_{t}|x_{t})\\left(\\frac{\\pi_{t}(u_{t}|x_{t})}{\\widehat{\\pi}_{t}(u_{t}|x_{t})}\\right)^{1+\\eta}\\mathrm{d}u_{t}\\mathrm{d}x_{t}\\right].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By Jensen's inequality, for any $\\eta>-1$ $\\eta\\neq0$ , it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac1\\eta\\log\\left[\\displaystyle\\int_{\\mathbb X}Z_{t}(x_{t})^{\\eta}\\int_{\\mathbb U}\\widehat\\pi_{t}(u_{t}|x_{t})\\left(\\frac{\\pi_{t}\\left(u_{t}|x_{t}\\right)}{\\widehat\\pi_{t}\\left(u_{t}|x_{t}\\right)}\\right)^{1+\\eta}\\mathrm{d}u_{t}\\mathrm{d}x_{t}\\right]}\\\\ &{\\ge\\frac1\\eta\\log\\left[\\displaystyle\\int_{\\mathbb X}Z_{t}(x_{t})^{\\eta}\\left(\\displaystyle\\int_{\\mathbb U}\\widehat\\pi_{t}(u_{t}|x_{t})\\frac{\\pi_{t}(u_{t}|x_{t})}{\\widehat\\pi_{t}(u_{t}|x_{t})}\\mathrm{d}u_{t}\\right)^{1+\\eta}\\mathrm{d}x_{t}\\right]}\\\\ &{=\\frac1\\eta\\log\\left[\\displaystyle\\int_{\\mathbb X}Z_{t}(x_{t})^{\\eta}\\mathrm{d}x_{t}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the equality holds if and only if $\\pi(\\cdot|x_{t})\\ll\\widehat{\\pi}_{t}(\\cdot|x_{t})$ , and $\\pi_{t}(\\cdot|x_{t})/\\widehat{\\pi}_{t}(\\cdot|x_{t})$ is constant $\\widehat{\\mathbb{P}}_{x_{t}}$ almost everywhere. Here, $\\widehat{\\mathbb{P}}_{x_{t}}$ is the probability distribution associated with $\\widehat{\\pi}_{t}\\big(\\cdot|x_{t}\\big)$ . Hence, the infimum (98) is attained only by $\\pi_{t}=\\widehat{\\pi}_{t}$ . This completes the proof. ", "page_idx": 25}, {"type": "text", "text": "H  Details of the experiment ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The implementation of the risk-sensitive SAC (RSAC) algorithm follows the stable-baselines3 [50] version of the SAC algorithm, which means that the RSAC algorithm also implements some tricks including reparameterization, minibatch sampling with a replay buffer, target networks, and double Q-network. Now, we introduce a series of hyperparameters listed in Table 1 shared for both SAC and RSAC algorithms. ", "page_idx": 26}, {"type": "table", "img_path": "LUIXdWn6Z5/tmp/32e36370f260ec88a909bce0b2bcd210cd59bde900b87ce869553185d621caa2.jpg", "table_caption": ["Table 1: SAC and RSAC Hyperparameters "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "As mentioned in Section 5, there were no significant differences in the control performance obtained or the behavior during training shown in Fig. 5 with those hyperparameters. However, when $\\eta$ is too small or too large, the training process becomes unstable due to the gradient vanishing problem and the gradient exponential growth problem, respectively, leading to training failure. To this end, we compare the robustness of the trained policies with RSAC $(\\eta\\in\\left\\{-0.02,-\\bar{0}.01,0.01,0.02\\right\\})$ and the standard SAC, which corresponds to $\\eta\\,=\\,0$ , in the experiment. For each learned policy, we do trail for 20 times. For each trail, we take 100 sampling paths to calculate the average episode cost. In Fig. 3, the error bars depict the max and min values, and the points depict the mean value among the 20 trails. We change the length of the pole $l$ in the Pendulum-v1 environment to test the robustness of the learned policies $\\mathcal{l}\\,=\\,1.0~\\mathrm{m}$ in the original environment). For the training, we used an Ubuntu 20.04 server (GPU: NVIDIA GeForce RTX 2080Ti). The code is available at https://github.com/kaito-1111/risk-sensitive-sac.git. ", "page_idx": 26}, {"type": "image", "img_path": "LUIXdWn6Z5/tmp/c335cc9b957651378e6f67d64a0f993ebae81854092a3df7aab60d2a36098bc0.jpg", "img_caption": ["Figure 5: Training process of RSAC (with different $\\eta$ ) and SAC in terms of average episode cost. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The main claims are made based on our theoretical results (Theorems 2, 3, 6, and Propositions 7, 8). ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The limitations are discussed in Section 6. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u25cf The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u25cf The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that infuence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Assumptions and a complete proof of all our results (Theorems 2, 3, 6, and Propositions 7, 8) are provided in the main paper and appendix. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u25cf The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u25cf Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: All the information is disclosed in Appendix H. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide open access to the code via GitHub. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips .cc/ public/guides/CodeSubmissionPolicy) for more details.   \nWhile we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All the training and test details are given in Appendix HI ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u25cf The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We report error bars in Fig. 3. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of themean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The information on the computer resources is provided in Appendix H. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: This work does not involve human subjects or participants, and there are no data-related concerns such as privacy issues. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u25cf The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The contribution of this paper is theoretical and we do not anticipate any direct societal impact of the work. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u25cf If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: In this work, we do not need data or models that have a high risk for misuse. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u25cf Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u25cf We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: For the experiment, we use OpenAI Gym, and it is properly mentioned. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u25cf The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 31}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We submit the documentation as a supplementary material. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset isused.   \n\u25cf At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u25cf The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u25cf Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]