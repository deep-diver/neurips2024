[{"type": "text", "text": "SpaFL: Communication-Efficient Federated Learning with Sparse Models and Low Computational Overhead ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Minsu Kim \u2217 Virginia Tech ", "page_idx": 0}, {"type": "text", "text": "Walid Saad Virginia Tech ", "page_idx": 0}, {"type": "text", "text": "Merouane Debbah Khalifa University ", "page_idx": 0}, {"type": "text", "text": "Choong Seon Hong Kyung Hee University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The large communication and computation overhead of federated learning (FL) is one of the main challenges facing its practical deployment over resourceconstrained clients and systems. In this work, SpaFL: a communication-efficient FL framework is proposed to optimize sparse model structures with low computational overhead. In SpaFL, a trainable threshold is defined for each fliter/neuron to prune its all connected parameters, thereby leading to structured sparsity. To optimize the pruning process itself, only thresholds are communicated between a server and clients instead of parameters, thereby learning how to prune. Further, global thresholds are used to update model parameters by extracting aggregated parameter importance. The generalization bound of SpaFL is also derived, thereby proving key insights on the relation between sparsity and performance. Experimental results show that SpaFL improves accuracy while requiring much less communication and computing resources compared to sparse baselines. The code is available at https://github.com/news-vt/SpaFL_NeruIPS_2024 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Federated learning (FL) is a distributed machine learning framework in which clients collaborate to train a machine learning (ML) model without sharing private data [1]. In FL, clients perform multiple epochs of local training using their own datasets and communicate model updates with a server. Different from a classical, centralized ML, FL systems are typically deployed on edge devices such as mobile or Internet of Things (IoT) devices, which have limited computing and communication resources. However, current ML models are typically too large and complex to be trained and deployed for inference by edge devices. Moreover, large model sizes can induce significant FL communication costs on both devices and communication networks. Hence, the practical deployment of FL over resource-constrained devices and systems requires optimized computation and communication costs for both edge devices and communication networks. This has motivated lines of research focused on reducing communication overhead in FL [2, 3], training sparse models in FL [4, 5, 6, 7, 8, 9], and optimizing model architectures to find a compact model for inference [10, 11, 12]. The works in [2, 3] proposed training algorithms such as quantization, gradient compression, and transmitting the subset of models in order to reduce the communication costs of FL. However, the associated computational overhead of these existing algorithms remains high since devices have to train a dense model. In [4, 5, 6, 7, 8, 9], FL algorithms in which devices train and communicate sparse models are proposed. However, the works in [4, 5] used unstructured pruning, which is difficult to gain the computation efficiency in practice. Moreover, the computation and communication overhead can still be large if model sparsity is not high. In [6, 7, 8, 9], the authors investigated the structured sparsity, however, the solutions therein either fixed the channel sparsity patterns for clients or did not optimize the pruning process. Furthermore, the FL approaches of [10, 11, 12] can significantly increase computation resource usage by training multiple models for resource-constrained devices. Clearly, despite a surge of literature on sparsity in FL, there is still a need to develop new FL algorithms that can find sparse model structures with optimized communication efficiency and low computational overhead to operate on resource-constrained devices. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The main contribution of this paper is SpaFL: a communication-efficient FL framework for optimizing sparse models with low computational overhead achieved by performing structured pruning through trainable thresholds. Here, a trainable threshold is defined for each filter/neuron to prune all of its connected parameters. To optimize the pruning process, only thresholds are communicated between clients and the FL server. Hence, clients can learn how to prune their model from global thresholds and can significantly reduce communication costs. Since parameters are not communicated, the clients\u2019 parameters and sparse model structures will remain personalized while only global thresholds are shared. We show that global thresholds can capture the aggregated parameter importance of clients. We further update the clients\u2019 model parameters by extracting aggregated parameter importance from global thresholds to improve performance. We analyze the generalization ability of SpaFL and provide insights on the relation between sparsity and performance. We summarize our contributions as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a new communication-efficient FL framework called SpaFL, in which clients optimize their sparse model structures with low computing costs through trainable thresholds. \u2022 We show how SpaFL can significantly reduce communication overhead for both clients and the server by only exchanging thresholds, the number of which is less than two orders of magnitude smaller than the number of model parameters. \u2022 We provide the generalization performance of SpaFL. Moreover, the impact of sharing thresholds on the model performance is theoretically and experimentally analyzed. \u2022 Experimental results demonstrate the performance, computation costs, and communication efficiency of SpaFL compared with both dense and sparse baselines. For instance, the results show that SpaFL uses only $0.17\\%$ of communication and $12.0\\%$ of computation resources compared to a dense baseline FedAvg while improving accuracy. Additionally, SpaFL improves accuracy by $2.92\\%$ compared to a sparse baseline while consuming only $0.35\\%$ of this baseline\u2019s communication resources, and only $24\\%$ of its computing resources. ", "page_idx": 1}, {"type": "text", "text": "2 Background and Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Federated Learning ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Distributed machine learning has consistently progressed and achieved success. However, it mostly focuses on training with independent and identically distributed (i.i.d.) data [13, 14]. The FL frameworks along with the FedAvg [1] enables clients to collaboratively train while preserving data privacy without data sharing. Due to privacy constraints and individual preferences, FL clients often collect non-iid data. As such, data can exhibit differences and imbalances in distribution across clients. This variability poses significant challenges in achieving efficient convergence. For a more detailed literature review, we refer to [15, 16]. Although most of state-of-the-art FL methods are effective in mitigating data heterogeneity, they often neglect the computational and communication costs involved in the training process. ", "page_idx": 1}, {"type": "text", "text": "2.2 Training and Finding Sparse Models in FL ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "To reduce the computation and communication overhead of complex ML models during training, the idea of embedding FL algorithms with pruning has recently attracted attention. In [4, 5, 6, 7, 8, 9, 17, 18, 19, 20, 21, 22, 23, 24, 25], the clients train sparse models and communicate sparse model parameters to reduce computation and communication overhead. To improve the aggregation phase with sparse models, the works in [17, 20, 21] perform averaging only between overlapping parameters to avoid information dilution by excluding zero value parameters. The authors in [18] obtained a sparse model by selecting a particular client to prune an initial dense model and then performed training in a similar way to FedAvg. In [4, 24], the authors presented binary masks adjustment strategy to improve the performance of sparse models and communication efficiency. The work in [25] progressively pruned a dense model for sparsification and analyzed its convergence. In [19, 22], the clients optimized personalized sparse models by exchanging lottery tickets [26] at every communication round. The work in [5] obtained personalized sparse models by $l_{1}$ norms constraints and the correlation between local and global models. In [8], the authors proposed dual pruning scheme for both local and global models to reduce the communication costs. The FL framework of [23] allows clients to train personalized sparse models in a decentralized setting without a central server. Although these works [17, 18, 4, 24, 25, 19, 22, 5, 23] adopted sparse models during training, they used unstructured pruning, which is difficult to improve the computation efficiency in practice. Meanwhile, with structured sparsity, the authors [7] proposed a training scheme that allows clients to train smaller submodels of a global model. In [9], clients train set of submodels with fixed channel sparsity patterns depending on their computing capabilities. The work in [6] studied structured sparsity by adjusting clients\u2019 channel activation probabilities. However, the works in [7, 9] fixed sparsity patterns and did not optimize sparse model structures. Although [6] optimized channel activation probabilities, the communication cost of downlink still remains high as a server broadcasts whole parameters. Similar to our work, in [27, 28], only binary masks are communicated and optimized by training auxiliary variables to learn sparse model structures. However, the work in [27] approximated binarization step using a sigmoid function during forward propagation. In [28], the downlink communication costs remained the same as that of FedAvg. In [10, 11, 29], clients perform neural-architecture-search by training multiple models to find optimized and sparse models to improve computational and memory efficiency at inference phase. However, in practice, clients often have limited resources to support the computationally intensive architecture search process [30]. Therefore, most prior works either adopted unstructured pruning or they still required extensive computing and communication costs for finding optimal sparse models. In contrast to the prior art, in the proposed SpaFL framework, we find sparse model structures with structured sparsity by optimizing and communicating trainable thresholds for filter/neurons. ", "page_idx": 1}, {"type": "image", "img_path": "dAXuir2ets/tmp/7dad0aeaa274cbd313fa87670eae42c141af4dbb3de026add597f95cb6beb6af.jpg", "img_caption": ["Figure 1: Illustration of SpaFL framework that performs model pruning through thresholds. Only the thresholds are communicated between the server and clients. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3 SpaFL Algorithm ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we first present the proposed pruning scheme for structured sparsity and formulate our FL problem to find optimal sparse models. Then, we present SpaFL to solve the proposed problem with low computation and communication overhead. ", "page_idx": 2}, {"type": "text", "text": "3.1 Structured Pruning with Trainable Thresholds ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "lWaye edrse.f iTneh ea  ntreauirnaal bnleet twhorreks hoofl dcl ifeonr t $k$ cwhi lnl ecuornosni sitn  olfi $L$ alra ylaeryse rass $\\{W_{k}^{1},\\ldots,W_{k}^{L}\\}$ .i nF coro npvaorlaumtieotnerasl $W_{k}^{l}\\in\\mathbb{R}^{n_{\\mathrm{out}}^{l}\\times n_{\\mathrm{in}}^{l}}$ in a linear layer $l$ , we define trainable thresholds $\\tau^{l}\\in\\mathbb{R}^{n_{\\mathrm{out}}^{l}}$ for output neurons. If it is a convolutional layer $\\boldsymbol{W}_{k}^{l}\\in\\mathbb{R}^{n_{\\mathrm{out}}^{l}\\times c_{\\mathrm{in}}^{l}\\times k^{l}\\times h^{l}}$ , where $c_{\\mathrm{in}}^{l}$ is the number of input channels and $k^{l}\\times h^{l}$ are the kernel sizes, we can change $\\boldsymbol{W}_{k}^{l}$ as $W_{k}^{l}\\,\\in\\,\\mathbb{R}^{n_{\\mathrm{out}}^{l}\\times n_{\\mathrm{in}}^{l}}$ with $n_{\\mathrm{in}}^{l}=c_{\\mathrm{in}}^{l}\\times k^{l}\\times h^{l}$ . Similarly, we can define the corresponding thresholds $\\tau^{l}\\in\\mathbb{R}^{n_{\\mathrm{out}}^{l}}$ for filters in that layer. Then, for each client $k$ , we define a set of total thresholds $\\boldsymbol{\\tau}=\\{\\tau^{1},\\dots,\\tau^{L}\\}$ . Note that the number of these additional thresholds will be at most $1\\%$ of the number of model parameters $d$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "For threshold $\\tau_{i}^{l}$ of fliter/neuron $i$ in layer $l$ , we compare the average magnitude of its connected parameters $\\begin{array}{r}{\\mu_{k,i}^{l}=1/n_{\\mathrm{in}}^{l}\\sum_{j=1}^{n_{\\mathrm{in}}^{l}}|w_{k,i j}^{l}|}\\end{array}$ to its value $\\tau_{i}^{l}$ . If $\\mu_{k,i}^{l}<\\tau_{i}^{l}$ , we prune all connected parameters to this fliter/neuron. Hence, our pruning can induce structured sparsity unlike [31]. Thus, we do not need to compute the gradients of parameters in a pruned filter/neuron [32] during backpropagation. We can obtain a binary mask $\\ensuremath{\\boldsymbol{p}}_{k}^{l}$ for $\\boldsymbol{W}_{k}^{l}$ , as follows ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{k,i j}^{l}=S(\\mu_{k,i}-\\tau_{i}^{l}),\\;1\\leq i\\leq n_{\\mathrm{out}}^{l},1\\leq j\\leq n_{\\mathrm{in}}^{l},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $S(\\cdot)$ is a unit step function. Hence, we can obtain the binary masks $\\{p_{k}^{1},\\ldots,p_{k}^{L}\\}$ by performing (1) at each layer. To facilitate the pruning, we constrain the parameters and thresholds to be within $[-1,1]$ and $[0,1]$ , respectively [31]. For simplicity, we unroll $\\{W_{k}^{1},\\ldots,W_{k}^{L}\\}$ and $\\{p_{k}^{1},\\ldots,p_{k}^{L}\\}$ to $\\pmb{w}_{k}\\,\\in\\,\\mathbb{R}^{d}$ and $\\pmb{p}_{k}\\in\\mathbb{R}^{d}$ , respectively as done in [33]. Thresholds represent the importance of their connected parameters (see more details in Section 3.3.1). Hence, clients can know which filter/neuron is important by training thresholds, thereby optimizing sparse model structures. Then, the key question becomes: Can clients benefti by collaborating to optimize shared thresholds in order to find optimal sparse models? We partially answer this question in Table 1. Following the same configurations in Section 5, clients with non-iid datasets only train and communicate thresholds $\\tau$ while freezing model parameters. ", "page_idx": 3}, {"type": "table", "img_path": "dAXuir2ets/tmp/ce8b6c6e2fa38794537a0a48eac24c0cd559ce46f90a716043be7ada46c52841.jpg", "table_caption": ["Table 1: Only thresholds are trained and communicated while parameters are kept frozen. "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "We can see that learning sparse structures can improve the performance even without training parameters. This also corroborates the result of [28]. Motivated by this observation, we aim to find optimal sparse models of clients in an FL setting by communicating only thresholds in order to reduce the communication costs in both clients and server sides while keeping parameters locally. The communication cost will decrease drastically because the number of thresholds will be at most $1\\%$ of the number of model parameters $d$ . Essentially, we optimize the sparse models of clients with small computing and communication resources by communicating thresholds. ", "page_idx": 3}, {"type": "text", "text": "3.2 Problem Formulation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We aim to optimize each client\u2019s model parameters and sparse model structures jointly in a personalized $\\mathrm{FL}$ setting by only communicating thresholds. This can be formulated as the following optimization problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{min}_{\\tau,w_{1},\\dots,w_{N}}\\quad\\frac{1}{N}\\sum_{k=1}^{N}F_{k}({\\tilde{w}_{k}},\\tau),}\\\\ {\\displaystyle\\mathrm{s.t.}\\quad\\quad F_{k}({\\tilde{w}_{k}},\\tau)=\\frac{1}{D_{k}}\\sum_{i=1}^{D_{k}}\\mathcal{L}(w_{k}\\odot p_{k}(\\tau);\\{x_{i},y_{i}\\}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\tilde{\\pmb{w}}_{k}\\,=\\,\\pmb{w}_{k}\\odot\\pmb{p}_{k}(\\pmb{\\tau})$ is a pruned model, $F_{k}(\\cdot)$ is a empirical risk associated with local data of client $k$ , $\\mathcal{L}$ is a loss function, $D_{k}$ is the number of data samples, $\\{\\pmb{x},\\boldsymbol{y}\\}$ is an input-label pair, $\\pmb{w}_{k}$ captures the model parameters, and $\\odot$ is the Hadamard product. If an element of $p_{k}(\\tau)$ is zero, then the corresponding parameter of $\\pmb{w}_{k}$ will be pruned. Our goal is to obtain the optimal $\\pmb{w}_{k}$ and $\\tau$ for each client in order to reduce the computation and communication overhead during training. However, solving (2) is not trivial because $\\pmb{w}_{k}$ and $\\tau$ are highly correlated. Moreover, structured sparsity can induce a large performance drop due to coarse-grained sparsity patterns compared to unstructured pruning [34]. ", "page_idx": 3}, {"type": "text", "text": "3.3 Algorithm Overview ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now describe the proposed algorithm, SpaFL, that can solve (2) while maintaining communicationefficiency with low computational cost. In SpaFL, every client jointly optimizes its personalized sparse model structure and model parameters with trainable thresholds, which can be used to prune filters/neurons. To save communication resources, only thresholds will be aggregated at a server to generate global thresholds for the next round. Here, global thresholds can represent the aggregated parameter importance of clients. Hence, at the beginning of each round, every client extracts the aggregated parameter importance from the global thresholds so as to update its model parameters. The overall algorithm is illustrated in Fig 1. and summarized in Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "3.3.1 Local Training for Parameters and Thresholds ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "At each round, a server samples a set of clients $\\mathcal{S}_{t}$ such that $|S_{t}|\\;=\\;K$ for local training. For given global thresholds $\\tau(t)$ at round $t$ , client $k\\in S_{t}$ generates a binary mask $\\mathbf{\\nabla}p_{k}(\\tau(t))$ using (1). Subsequently, it obtains the sparse model $\\pmb{\\tilde{w}}_{k}(t)=\\pmb{w}_{k}(t)\\odot\\pmb{p}_{k}(\\pmb{\\tau}(t))$ . To improve the communication efficiency, each sampled client performs $E$ epochs using mini-batch stochastic gradient to update parameters and thresholds as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{k}^{e+1}(t)\\gets w_{k}^{e}(t)-\\eta(t)g_{k}(\\tilde{w}_{k}^{e}(t)),\\;\\tilde{w}_{k}^{0}(t)=\\tilde{w}_{k}(t),\\;0\\leq e\\leq E-1,}\\\\ &{\\tau_{k}^{e+1}(t)\\gets\\tau_{k}^{e}(t)-\\eta(t)h_{k}(\\tilde{w}_{k}^{e}(t)),\\;\\tau_{k}^{0}(t)=\\tau(t),\\;0\\leq e\\leq E-1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{g_{k}(\\tilde{w}_{k}^{e}(t))=\\nabla_{\\tilde{w}_{k}^{e}}F_{k}(\\tilde{w}_{k}^{e}(t),\\tau(t);\\xi_{k}^{e}(t)),h_{k}(\\tilde{w}_{k}^{e}(t))\\,=\\nabla_{\\tau}F_{k}(\\tilde{w}_{k}^{e}(t),\\tau(t);\\xi_{k}^{e}(t))}\\end{array}$ with a mini-batch $\\xi$ and $\\eta(t)$ is a learning rate. Parameters of unpruned fliter/neurons and thresholds will be jointly updated via backpropagation. To enforce sparsity, we add a regularization term $R(t)$ to (4) in order to penalize small threshold values. To this end, client $k$ first calculates the following sparsity regularization term $\\begin{array}{r}{R(t)=\\sum_{l=1}^{L}\\sum_{i=1}^{n_{\\mathrm{out}}^{l}}\\exp(-\\tau_{i})}\\end{array}$ . Then, the loss function can be rewritten as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nF_{k}(\\tilde{\\pmb{w}}_{k}^{e}(t),\\pmb{\\tau}(t);\\xi_{k}^{e}(t))\\gets F_{k}(\\tilde{\\pmb{w}}_{k}^{e}(t),\\pmb{\\tau}(t);\\xi_{k}^{e}(t))+\\alpha R(t),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $0\\,\\leq\\,\\alpha\\,\\leq\\,1$ is the coefficient that controls $R(t)$ . From (5), we can give thresholds $\\tau(t)$ performance feedback on the current sparse model while also progressively increasing $\\tau(t)$ through the sparsity regularization term $R(t)$ [31]. From (5), client $k$ then updates the received global thresholds $\\tau(t)$ via backpropagation as follows ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tau_{k}^{e+1}(t)\\gets\\tau_{k}^{e}(t)-\\eta(t)h_{k}(\\tilde{w}_{k}^{e}(t))+\\alpha\\eta(t)\\exp\\{-\\tau_{k}^{e}(t)\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "After local training, each client $k\\in S_{t}$ , transmits the updated thresholds $\\tau_{k}(t)$ to the server. Here, the communication overhead will be less than one percent of that of transmitting the entire parameters. Subsequently, the server performs aggregation and broadcasts new global thresholds, i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{\\tau}(t+1)=\\frac{1}{K}\\sum_{k\\in S_{t}}\\pmb{\\tau}_{k}(t).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, in SpaFL, clients communicate only thresholds. Then, what will clients learn from sharing trained thresholds? Next, we show that thresholds represent the importance of their associated filter/neurons. ", "page_idx": 4}, {"type": "text", "text": "3.3.2 Learning Parameter Importance From Thresholds ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Clients can know which fliter/neurons are important by sharing trained thresholds. For the threshold of filter/neuron $i$ at layer $l$ of client $k$ , its gradient can be written as below ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{h_{k,i}^{t}(\\tilde{\\boldsymbol{w}}_{k}^{e}(t))=\\frac{F_{k}(\\tilde{\\boldsymbol{w}}_{k}^{e}(t))}{\\partial\\tau_{k,i}^{e}(t)}=\\displaystyle{\\sum_{j=1}^{n_{t}}\\frac{\\partial\\tilde{\\boldsymbol{w}}_{k,i}^{e,l}(t)}{\\partial\\tau_{k,i}^{e}(t)}}\\frac{\\partial F_{k}(\\tilde{\\boldsymbol{w}}_{k}(t),\\tau(t))}{\\partial\\tilde{\\boldsymbol{w}}_{k,i}^{e,l}(t)}=\\displaystyle{\\sum_{j=1}^{n_{t}}\\frac{\\partial\\tilde{\\boldsymbol{w}}_{k,i j}^{e,l}(t)}{\\partial\\tau_{k,i}^{e,l}(t)}}\\{g_{k}(\\tilde{\\boldsymbol{w}}_{k}^{e}(t))\\}_{i j}^{l}}\\\\ &{\\quad}&{\\quad=\\displaystyle{\\sum_{j=1}^{n_{t}}\\frac{\\partial\\tilde{\\boldsymbol{w}}_{k,i j}^{e,l}(t)}{\\partial Q_{k,i}^{e,l}(t)}\\frac{\\partial Q_{k,i}^{e,l}(t)}{\\partial\\tau_{k,i}^{e}(t)}\\{g_{k}(\\tilde{\\boldsymbol{w}}_{k}^{e}(t))\\}_{i j}^{l}}}\\\\ &{\\quad}&{\\quad=\\displaystyle{\\sum_{j=1}^{n_{t}}\\frac{\\partial\\boldsymbol{w}_{k,i j}^{e,l}(t)}{\\partial S(Q_{k,i j}^{e,l}(t))}\\odot\\boldsymbol{g}_{k,i}^{e,l}(t)}\\frac{\\partial\\boldsymbol{g}_{k}^{e,l}(t)}{\\partial\\tau_{k,i}^{e,l}(t)}\\frac{\\partial Q_{k,i}^{e,l}(t)}{\\partial\\tau_{k,i}^{e,l}(t)}\\{g_{k}(\\tilde{\\boldsymbol{w}}_{k}^{e}(t))\\}_{i j}^{l}}}\\\\ &{\\quad}&{\\quad=\\displaystyle{-\\sum_{j=1}^{n_{t}}\\{g_{k}(\\tilde{\\boldsymbol{w}}_{k}^{e}(t))\\}_{i j}^{l}\\boldsymbol{w}_{k,i j}^{e,l}(t)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $Q_{k,i}^{e,l}(t)=\\mu_{k,i}^{e}(t)-\\tau_{k,i}^{e,l}(t)$ in (1), (8) is from the definition of pruned parameters in (2) and the unit step function $S(\\cdot)$ , and (9) is from the identity straight-through estimator [35] to approximate the gradient of the step functions in (8). ", "page_idx": 5}, {"type": "text", "text": "From (9), we can see that threshold $\\tau_{k,i}^{e,l}$ corresponds to the importance of its connected parameters $w_{k,i j}^{e,l},1\\leq j\\leq n_{\\mathrm{in}}^{l}$ , in its filter/neuron. This is because the importance of a parameter $w_{i j}^{l}$ can be estimated by [36] ", "page_idx": 5}, {"type": "equation", "text": "$$\nF(\\pmb{w},\\pmb{\\tau})-F(\\pmb{w},\\pmb{\\tau};\\pmb{w}_{i j}^{l}=0)\\approx g(\\pmb{w})_{i j}^{l}w_{i j}^{l},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $F(\\pmb{w},\\pmb{\\tau};w_{i j}^{l}=0)$ is the loss function when $w_{i j}^{l}$ is masked and the approximation is obtained from the first Taylor expansion at $w_{i j}^{l}=0$ . Therefore, if connected parameters were important, the sign of (10) of those parameters will be negative, and the corresponding threshold will decrease as in (9). Otherwise, the threshold will be increased to enforce sparsity. Hence, prematurely pruned parameters will be automatically recovered via a joint optimization of $\\tau$ and $\\mathbf{\\nabla}w$ . ", "page_idx": 5}, {"type": "text", "text": "3.3.3 Extracting Parameter Importance from Global Thresholds ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Since thresholds represent the importance of the connected parameters at each filter/neuron, clients can learn how to prune their parameters from the global thresholds. Moreover, the difference between two consecutive global thresholds $\\Delta\\tau(t)\\,=\\,\\tau(t+1)\\,-\\,\\tau(t)$ captures the history of aggregated parameter importance, which can be further used to improve model performance. For instance, from (10), if $\\Delta\\tau_{i}^{l}(\\dot{t})<0$ , then the parameters connected to threshold $i$ in layer $l$ were globally important. If $\\Delta\\tau_{i}^{l}(t)\\geq0$ , then the connected parameters were globally less important. Hence, from $\\Delta\\tau(t)$ , clients can deduce which parameter is globally important or not and further update their model parameters. After generating new global thresholds $\\tau(t+1)$ , the server broadcasts $\\tau(t+1)$ to client $k\\in S_{t+1}$ , and then clients calculate $\\Delta\\tau(t)=\\tau(t+1)-\\dot{\\tau}(t)$ . ", "page_idx": 5}, {"type": "text", "text": "We then present how clients can update their model parameters from $\\Delta\\tau(t)$ . For given $\\Delta\\tau(t)$ , we need to decide on the: 1) update direction and 2) update amount. Clients can know the update direction of parameters by considering $\\Delta\\tau(t)$ and the dominant sign of parameters connected to each threshold. For simplicity, assume that each parameter has a threshold. Then, the gradient of the thresholds in (9) can be rewritten as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{k}(\\tilde{\\boldsymbol{w}}_{k}(t))=-g_{k}(\\tilde{\\boldsymbol{w}}_{k}(t))\\boldsymbol{w}_{k}(t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The gradient of the loss $F_{k}(\\tilde{\\pmb{w}}_{k}(t),\\pmb{\\tau}(t))$ with respect to the whole parameters $w_{k}(t)$ is given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{\\partial F_{k}(\\tilde{\\mathbf{w}}_{k}(t),\\pmb{\\tau}(t))}{\\partial\\mathbf{w}_{k}(t)}=\\pmb{g}_{k}(\\tilde{\\mathbf{w}}_{k}(t))|\\mathbf{w}_{k}(t)|.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "From (11) and (12), the gradient direction of a parameter $w$ is opposite of that of its connected threshold if $w>0$ . Otherwise, both the threshold and the parameter have the same gradient direction. ", "page_idx": 5}, {"type": "text", "text": "Input: Total number of clients $N$ ; Total communication rounds $T$ ; Local number of epochs $E$   \nOutput: Global thresholds $\\tau$ and personalized models $\\tilde{w}_{k}$   \n1 The server initializes $\\tau(0)$ and ${\\pmb w}(0)$ and broadcasts them to every client ;   \n2 for $t=0$ to $T-1$ do   \n3 Server randomly samples $\\mathcal{S}_{t}$ ;   \n4 for Client $k\\in S_{t}$ do   \n5 Receive $\\tau(t+1)$ from the server and calculate $\\Delta\\tau(t)$ ;   \n6 Update the current local model using $\\Delta\\tau(t)$ with (13);   \n7 for $e=0$ to $E-1$ do   \n8 Update $\\begin{array}{r}{\\pmb{w}_{k}^{e+1}(t)\\gets\\pmb{w}_{k}^{e}(t)-\\eta(t)\\pmb{g}_{k}(\\tilde{\\pmb{w}}_{k}^{e}(t)),\\ \\tilde{\\pmb{w}}_{k}^{0}(t)=\\tilde{\\pmb{w}}_{k}(t);}\\end{array}$   \n9 Update $\\tau_{k}^{e+1}(t)\\gets\\tau_{k}^{e}(t)-\\eta(t)h_{k}(\\tilde{w}_{k}^{e}(t))$ , \u03c4 0k(t) = \u03c4(t)   \n10 Transmit the updated threshold $\\tau_{k}(t)$ to the server   \n1 Generate a new global threshold $\\tau(t+1)$ using (7) ", "page_idx": 6}, {"type": "text", "text": "Hence, we can deduce the following: If $w>0$ , the gradient direction of $w$ and the sign of $\\Delta\\tau$ will have the same sign; otherwise, the gradient direction of $w$ and the sign of $\\Delta\\tau$ are opposite. In SpaFL, each threshold has multiple connected parameters to its fliter/neuron. As such, we decide the update direction of connected parameters by finding the dominant sign among them. To this end, we simply add the connected parameters of each threshold. For instance, consider threshold $i$ in layer $l$ of client $k$ , if $\\textstyle\\sum_{j=1}^{n_{\\mathrm{in}}^{l}}w_{k,i j}^{l}(t)>0$ , then the gradient direction of the connected parameters will be the same as the sign of $\\Delta\\tau_{i}^{l}(t)$ . Otherwise, it is the opposite of the sign of $\\Delta\\tau_{i}^{l}(t)$ . Thus, the update direction can be simply expressed with a XOR operation between the sign of $\\Delta\\tau_{i}^{l}(t)$ and the sign of connected parameters sum. Next, we decide how much a parameter should be updated. From (11) and (12), we can see that a threshold and a parameter have the same magnitude for their gradients. Hence, we simply divide $\\Delta\\tau_{i}^{l}(t)$ by the number of connected parameters $n_{\\mathrm{in}}^{l}$ . We finally provide the update equation using $\\Delta\\tau(t)$ as follows ", "page_idx": 6}, {"type": "equation", "text": "$$\nw_{k,i j}^{l}(t+1)=w_{k,i j}^{l}(t)+\\frac{1}{n_{\\mathrm{in}}^{l}}\\Delta\\tau_{i}^{l}(t)\\,\\mathrm{XOR}\\left\\{\\mathrm{sign}\\left(\\sum_{j=1}^{n_{\\mathrm{in}}^{l}}w_{k,i j}^{l}(t)\\right),\\mathrm{sign}(\\Delta\\tau_{i}^{l}(t))\\right\\},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathrm{sign}(\\cdot)$ is a sign function. This parameter update corresponds to line 7 in Algorithm 1. Note that this additional parameter update is not computationally intensive because it happens only once before local training. We also provide the number of used FLOPs during training with inclusion of this operation in Section 5. ", "page_idx": 6}, {"type": "text", "text": "4 Theoretical Analysis of SpaFL ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "$\\begin{array}{r l}&{\\frac{1}{N}\\sum_{k=1}^{N}\\frac{1}{D_{k}}\\sum_{i=1}^{D_{k}}\\mathcal{L}(\\tilde{\\boldsymbol{w}_{k}},\\bar{\\boldsymbol{\\tau}_{;}}\\,z_{i})}\\end{array}$ l,ii sz waaetn i coionnnp suiatd-neoar lutythpseiu set  xppoaefirc .t eSSdpu rapiFspkLo $\\begin{array}{r}{\\mathcal{R}=\\frac{1}{N}\\sum_{k=1}^{N}\\bar{\\mathbb{E}}_{z_{k}\\sim\\mathcal{D}_{k}}\\mathcal{L}(\\tilde{\\pmb{w}}_{k},\\pmb{\\tau};z_{k})}\\end{array}$ $\\begin{array}{r l}{\\hat{\\mathcal{R}}}&{{}=}\\end{array}$ where $\\mathcal{L}$ is a loss function and $z$ $\\rho_{k}$ is the ratio of remaining model parameters of client $k$ and $\\bar{\\rho}$ is the average model density across clients. Then, for the hypothesis $A(\\mathcal{D})$ with global thresholds $\\tau$ from Algorithm 1 on the joint training dataset $\\begin{array}{r}{\\mathcal{D}=\\cup_{k=1}^{N}\\dot{\\mathcal{D}_{k}}}\\end{array}$ with $\\bar{\\rho}$ , we have the following generalization bound as follows: ", "page_idx": 6}, {"type": "text", "text": "Theorem 1. For the loss function $||\\mathcal{L}||_{\\infty}\\leq1$ , the training data size $\\begin{array}{r}{D\\ge\\frac{2}{\\epsilon^{\\prime2}}\\ln\\left(\\frac{16}{\\exp\\left(-\\epsilon^{\\prime}\\delta^{\\prime}\\right)}\\right)}\\end{array}$ and the total number of communication rounds $T$ , we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\hat{\\mathcal{R}}(A(\\mathcal{D}))-\\mathcal{R}(A(\\mathcal{D}))\\right|<9\\epsilon^{\\prime}\\right]>1-\\frac{\\exp(-\\epsilon^{\\prime})\\delta^{\\prime}}{\\epsilon^{\\prime}}\\ln\\frac{2}{\\epsilon^{\\prime}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\delta^{\\prime}=\\exp\\left(-\\displaystyle\\frac{\\epsilon^{\\prime}+T\\tilde{\\epsilon}}{2}\\right)\\left(\\displaystyle\\frac{1}{1+\\exp(\\tilde{\\epsilon})}\\left(\\frac{2T\\tilde{\\epsilon}}{T\\tilde{\\epsilon}-\\epsilon^{\\prime}}\\right)\\right)^{T}\\left(\\displaystyle\\frac{T\\tilde{\\epsilon}+\\epsilon^{\\prime}}{T\\tilde{\\epsilon}-\\epsilon^{\\prime}}\\right)^{-\\frac{\\epsilon^{\\prime}+T\\tilde{\\epsilon}}{2\\tilde{\\epsilon}}}-\\left(1-\\displaystyle\\frac{\\delta}{1+\\exp(\\tilde{\\epsilon})}\\right)^{T}}\\\\ {+\\displaystyle2-\\left(1-\\exp(\\tilde{\\epsilon})\\displaystyle\\frac{\\delta}{1+\\exp(\\tilde{\\epsilon})}\\right)^{\\lceil\\frac{\\epsilon^{\\prime}}{\\tilde{\\epsilon}}\\rceil}\\left(1-\\displaystyle\\frac{\\delta}{1+\\exp(\\tilde{\\epsilon})}\\right)^{T-\\lceil\\frac{\\epsilon^{\\prime}}{\\tilde{\\epsilon}}\\rceil},}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "equation", "text": "$$\n\\tilde{\\epsilon}=\\log\\left(\\frac{D-\\xi}{D}+\\frac{\\xi}{D}\\exp\\left(\\frac{\\sqrt{2}\\bar{\\rho}M_{g}\\sigma\\sqrt{\\log\\frac{1}{\\delta}}+\\bar{\\rho}^{2}M_{g}^{2}}{2\\sigma^{2}}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\xi$ is the size of a mini-batch, $\\sigma$ is the variance of Gaussian noise, and $M_{g}$ is the maximum diameter of thresholds\u2019 gradients (11). The proof and the definition of $\\delta$ are provided in the Appendix 1.2 and (12), respectively. ", "page_idx": 7}, {"type": "text", "text": "From Theorem 1, we can see that, as the average model density $\\bar{\\rho}$ decreases, the generalization bounds becomes smaller, thereby achieving better generalization performance. This is because $\\epsilon^{\\prime}$ and \u03f5\u02dc decrease as the average model density $\\bar{\\rho}$ decreases. Hence, SpaFL can improve the generalization performance with sparse models by optimizing and sharing global thresholds. ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now present experimental results to demonstrate the performance, computation costs and communication efficiency of SpaFL. Implementation details are provided in the Supplementary document. ", "page_idx": 7}, {"type": "text", "text": "5.1 Experiments Configuration ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct experiments on three image classification datasets: FMNIST [37], CIFAR-10, and CIFAR-100 [38] datasets with NVIDA A100 GPUs. To distribute datasets in a non-iid fashion, we use Dirichlet (0.2) for FMNIST and Dirichlet (0.1) for CIFAR-10 and CIFAR-100 datasets as done in [39] with $N=100$ clients. We set the total communication round $T=500$ and 1500 for FMNIST/CIFAR10 and CIFAR100, respectively. At each round, we randomly sample $K=10$ clients. Unless stated otherwise, we average all the results over at least 10 different random seeds. We also calculate the best accuracy by averaging each client\u2019s performance on its test dataset. For FMNIST dataset, we use the Lenet-5-Caffe. For the Lenet model, we set $\\eta(t)=0.001$ , $E=5$ , $\\alpha=0.002$ , and a batch size to be 64. For CIFAR-10 dataset, we use a convolutional neural network (CNN) model with seven layers used in [40] with $\\eta(t)=0.01$ , $E=5$ , $\\alpha=0.00015$ , and a batch size of 16. We adopt the ResNet-18 model for CIFAR-100 dataset with $\\eta(t)=0.01$ , $E=7$ , $\\alpha=0.0007$ , and a batch size of 64. The learning rate of CIFAR-100 is decayed by 0.993 at each communication round. ", "page_idx": 7}, {"type": "text", "text": "5.2 Baselines ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compare SpaFL with multiple state of the art baselines that studied sparse model structures in FL. In FedAvg [1], every client trains a global dense model and communicates whole model parameters. FedPM [28] trains and communicates a binary mask while freezing model parameters. In HeteroFL [7], each client trains and communicates $p$ -reduced models, which remove the last $1-p$ output channels in each layer. In Fjord [9], each client randomly samples a model from a set of $p$ -reduced models, which drops out $p\\%$ of filter/neurons in each layer. FedP3 [41] communicates a subset of sparse layers that are pruned by the server for downlink and personalize the remaining layers. Clients only upload the updated remaining layers to the server. FedSpa [4] trains personalized sparse models for clients while maintaining fixed model density during training. Local only performs local training with the introduced pruning method without any communications. For the sparse FL baselines, the average target sparsity is set to 0.5 following the configurations in [28, 7, 9, 41, 4]. ", "page_idx": 7}, {"type": "text", "text": "5.3 Main Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In Table 2 and Fig. 2, we present the averaged accuracy, communication costs, number of FLOPs during training, and convergence rate for each algorithm. We consider all uplink and downlink communications to calculate the communication cost of each algorithm. We also provide the details of the FLOPs measure in the Supplementary document. We average the model densities of SpaFL when a model achieved the best accuracy during training. From these results, we observe that SpaFL outperforms all baselines while using the least amount of communication costs and number of FLOPs. The achieved model densities are $35.36\\%$ , $30.57\\%$ , and $35.38\\%$ , for FMNIST, CIFAR-10, and CIFAR-100, respectively. We also observe that SpaFL uses less resources and performs better than FedP3, HetroFL and Fjord, which deployed structured sparse models across clients. For FedP3, clients only upload subset of layers, but the server still needs to send the remaining layers. Although FedPM reduced uplink communication costs by communicating only binary masks, its downlink cost is the same as FedAvg. In SpaFL, since the clients and the server only exchange thresholds, we can significantly reduce the communication costs compared to baselines that exchange the subset of model parameters such as HeteroFL and Fjord. Moreover, SpaFL significantly achieved better performance than Local, which did not communicate trained thresholds. Local achieved $51.2\\%$ , $50.1\\%$ , and $53.6\\%$ model densities for each dataset, respectively. We can see that communicating trained thresholds can make models sparser and achieve better performance. This also corroborates the analysis of Theorem 1. Hence, SpaFL can efficiently improve model performance with small computation and communication costs. In Fig. 2, we show the convergence rate of each algorithm. We can see that the accuracy of SpaFL decreases and then keeps increasing. The initial accuracy drop is from pruning while global thresholds are not trained enough. As thresholds keep being trained and communicated, clients learn how to prune their model, thereby gradually improving the performance with less active filter/neurons. ", "page_idx": 7}, {"type": "table", "img_path": "dAXuir2ets/tmp/b37df000aaaaa4dbdb58a5cda08bd6bec48abd85ce8eb91388ba52946ce7f994.jpg", "table_caption": ["Table 2: Performance of SpaFL and other baselines along with their used communication costs (Comm) and computation (FLOPs) resources during whole training. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "dAXuir2ets/tmp/2746e636b47dfcf92c04e229225bae3b072f7d109f1f19798183653f86cc6a9a.jpg", "img_caption": ["(a) Learning curve on FMNIST (b) Learning curve on CIFAR-10 (c) Learning curve on CIFAR-100 Figure 2: Learning curves on FMNIST, CIFAR-10, and CIFAR-100 ", "Table 3: Impact of extracting parameter importance from global thresholds "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "dAXuir2ets/tmp/97742ecb513477aeb187751a9632e55c576f84406c519fb47eb9ab9cbc642d46.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "We provide an empirical comparison between SpaFL and the baseline that does not use the update in Section 3.3.3 in Table. 3. We can see that the update (13) can provide a clear improvement compared to the baseline by extracting parameter importance from global thresholds. ", "page_idx": 8}, {"type": "image", "img_path": "dAXuir2ets/tmp/ad19ee1ed199fb0630e627d02664bb368f132df925c6912cd436f6eb26b96d8f.jpg", "img_caption": ["(a) Sparsity pattern at round 40 (b) Sparsity pattern at round 150 (c) Sparsity pattern at round 500 ", "Figure 3: Sparsity pattern of conv1 layer on CIFAR-10 "], "img_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "dAXuir2ets/tmp/c142296abfb6458098705725b35a3120830cd7119bdec505c83f600124106dc4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Table 4: Performance of SpaFL with the ViT architecture on CIFAR-10 ", "page_idx": 9}, {"type": "text", "text": "In Fig. 3, we show the change of structured sparsity of the first convolutional layer with 64 filters with three input channels on CIFAR-10. We color active filters as black and pruned filters as white. We can see that clients learn common sparse structures across training round. For instance, the 31th and 40th filters are all pruned at round 40. Meanwhile, the 20th filter is recovered at rounds 150 and 500. We can know that SpaFL enables clients to learn optimized sparse model structures by optimizing thresholds. In SpaFL, pruned filter/neurons can be recovered by sharing thresholds. At round 40, filters are pruned with high sparsity. Since premature pruning damages the performance, most fliters are recovered at round 150. Then, clients gradually enforce more sparsity to fliters along with training rounds as shown in Fig. 3c. ", "page_idx": 9}, {"type": "text", "text": "In Tab. 4, we show the performance of SpaFL on a vision transformer using the ViT [42] on CIFAR-10 dataset. We used the same data distribution as done in Tab. 2. We apply our pruning scheme to multiheads attention layers. Since a multiheads attention layer essentially consists of stacked linear layers, we can simply use (1), thereby making sparse attention. We can see that SpaFL can be applied to transformer architectures by achieving the density of around $42\\%$ while outperforming FedAvg. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we have developed a communication-efficient FL framework SpaFL that allows clients to optimize sparse model structures with low computing costs. We have reduced computational overhead by performing structured pruning through trainable thresholds. To optimize the pruning process, we have communicated only thresholds between clients and a server. We have also presented the parameter update method that can extract parameter importance from global thresholds. Furthermore, we have provided theoretical insights on the generalization performance of SpaFL. ", "page_idx": 9}, {"type": "text", "text": "Limitations and Broader Impact One limitation of SpaFL is that it cannot explicitly control the sparsity of clients. Since we enforce sparsity through the regularizer term, we need to run multiple experiments to find values for desired sparsity. Another limitation is that our analysis requires a bounded loss function. Meanwhile, in practice, most loss functions may admit bounds that have a large value. For broader impact, SpaFL can reduce not only the computation and communication costs of FL training, but also those of inference phase due to sparsity. Hence, in general, SpaFL can improve the sustainability of FL deployments, and more broadly, of AI. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pages 1273\u20131282. PMLR, 2017.   \n[2] Sunwoo Lee, Anit Kumar Sahu, Chaoyang He, and Salman Avestimehr. Partial model averaging in federated learning: Performance guarantees and beneftis. arXiv e-prints, pages arXiv\u20132201, 2023.   \n[3] Pretom Roy Ovi, Emon Dey, Nirmalya Roy, and Aryya Gangopadhyay. Mixed quantization enabled federated learning to tackle gradient inversion attacks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5045\u20135053, 2023.   \n[4] Tiansheng Huang, Shiwei Liu, Li Shen, Fengxiang He, Weiwei Lin, and Dacheng Tao. Achieving personalized federated learning with sparse local models. arXiv preprint arXiv:2201.11380, 2022.   \n[5] Xiaofeng Liu, Yinchuan Li, Qing Wang, Xu Zhang, Yunfeng Shao, and Yanhui Geng. Sparse personalized federated learning. IEEE Transactions on Neural Networks and Learning Systems, 2023.   \n[6] Dongping Liao, Xitong Gao, Yiren Zhao, and Cheng-Zhong Xu. Adaptive channel sparsity for federated learning under system heterogeneity. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20432\u201320441, 2023.   \n[7] Enmao Diao, Jie Ding, and Vahid Tarokh. Heterofl: Computation and communication efficient federated learning for heterogeneous clients. International Conference on Learning Representations, 2021.   \n[8] Kai Yi, Nidham Gazagnadou, Peter Richt\u00e1rik, and Lingjuan Lyu. Fedp3: Federated personalized and privacy-friendly network pruning under model heterogeneity. arXiv preprint arXiv:2404.09816, 2024.   \n[9] Samuel Horvath, Stefanos Laskaridis, Mario Almeida, Ilias Leontiadis, Stylianos Venieris, and Nicholas Lane. Fjord: Fair and accurate federated learning under heterogeneous targets with ordered dropout. Advances in Neural Information Processing Systems, 34:12876\u201312889, 2021.   \n[10] Taehyeon Kim and Se-Young Yun. Supernet training for federated image classification under system heterogeneity. arXiv preprint arXiv:2206.01366, 2022.   \n[11] Mi Luo, Fei Chen, Zhenguo Li, and Jiashi Feng. Architecture personalization in resourceconstrained federated learning. In NeurIPS Workshop on New Frontiers in Federated Learning, 2021.   \n[12] Won Joon Yun, Yunseok Kwak, Hankyul Baek, Soyi Jung, Mingyue Ji, Mehdi Bennis, Jihong Park, and Joongheon Kim. Slimf:l Federated learning with superposition coding over slimmable neural networks. IEEE/ACM Transactions on Networking, 2023.   \n[13] Zhenheng Tang, Shaohuai Shi, Wei Wang, Bo Li, and Xiaowen Chu. Communication-efficient distributed deep learning: A comprehensive survey. arXiv preprint arXiv:2003.06307, 2020.   \n[14] Joost Verbraeken, Matthijs Wolting, Jonathan Katzy, Jeroen Kloppenburg, Tim Verbelen, and Jan S Rellermeyer. A survey on distributed machine learning. Acm computing surveys, 53(2): 1\u201333, 2020.   \n[15] Wenke Huang, Mang Ye, Zekun Shi, Guancheng Wan, He Li, Bo Du, and Qiang Yang. Federated learning for generalization, robustness, fairness: A survey and benchmark. arXiv preprint arXiv:2311.06750, 2023.   \n[16] Qinbin Li, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu, and Bingsheng He. A survey on federated learning systems: Vision, hype and reality for data privacy and protection. IEEE Transactions on Knowledge and Data Engineering, 35(4):3347\u20133366, 2021.   \n[17] Sameer Bibikar, Haris Vikalo, Zhangyang Wang, and Xiaohan Chen. Federated dynamic sparse training: Computing less, communicating less, yet learning better. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 6080\u20136088, 2022.   \n[18] Yuang Jiang, Shiqiang Wang, Victor Valls, Bong Jun Ko, Wei-Han Lee, Kin K Leung, and Leandros Tassiulas. Model pruning enables efficient federated learning on edge devices. IEEE Transactions on Neural Networks and Learning Systems, 2022.   \n[19] Ang Li, Jingwei Sun, Binghui Wang, Lin Duan, Sicheng Li, Yiran Chen, and Hai Li. Lotteryf:l empower edge intelligence with personalized and communication-efficient federated learning. In 2021 IEEE/ACM Symposium on Edge Computing (SEC), pages 68\u201379. IEEE, 2021.   \n[20] Xinchi Qiu, Javier Fernandez-Marques, Pedro PB Gusmao, Yan Gao, Titouan Parcollet, and Nicholas Donald Lane. Zerofl: Efficient on-device training for federated learning with local sparsity. arXiv preprint arXiv:2208.02507, 2022.   \n[21] Ang Li, Jingwei Sun, Pengcheng Li, Yu Pu, Hai Li, and Yiran Chen. Hermes: an efficient federated learning framework for heterogeneous mobile clients. In Proceedings of the 27th Annual International Conference on Mobile Computing and Networking, pages 420\u2013437, 2021.   \n[22] Vaikkunth Mugunthan, Eric Lin, Vignesh Gokul, Christian Lau, Lalana Kagal, and Steve Pieper. Fedltn: Federated learning for sparse and personalized lottery ticket networks. In Computer Vision\u2013ECCV 2022: 17th European Conference, pages 69\u201385. Springer, 2022.   \n[23] Rong Dai, Li Shen, Fengxiang He, Xinmei Tian, and Dacheng Tao. Dispfl: Towards communication-efficient personalized federated learning via decentralized sparse training. In International Conference on Machine Learning, pages 4587\u20134604. PMLR, 2022.   \n[24] Sara Babakniya, Souvik Kundu, Saurav Prakash, Yue Niu, and Salman Avestimehr. Revisiting sparsity hunting in federated learning: Why does sparsity consensus matter? Transactions on Machine Learning Research, 2023.   \n[25] Dimitris Stripelis, Umang Gupta, Greg Ver Steeg, and Jose Luis Ambite. Federated progressive sparsification (purge-merge-tune) $^+$ . In Workshop on Federated Learning: Recent Advances and New Challenges (in Conjunction with NeurIPS, 2022.   \n[26] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018.   \n[27] Ang Li, Jingwei Sun, Xiao Zeng, Mi Zhang, Hai Li, and Yiran Chen. Fedmask: Joint computation and communication-efficient personalized federated learning via heterogeneous masking. In Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems, pages 42\u201355, 2021.   \n[28] Berivan Isik, Francesco Pase, Deniz Gunduz, Tsachy Weissman, and Michele Zorzi. Sparse random networks for communication-efficient federated learning. arXiv preprint arXiv:2209.15328, 2022.   \n[29] Erum Mushtaq, Chaoyang He, Jie Ding, and Salman Avestimehr. Spider: Searching personalized neural architecture for federated learning. arXiv preprint arXiv:2112.13939, 2021.   \n[30] Minh Tri L\u00ea, Pierre Wolinski, and Julyan Arbel. Efficient neural networks for tiny machine learning: A comprehensive review. arXiv preprint arXiv:2311.11883, 2023.   \n[31] Junjie Liu, Zhe Xu, Runbin Shi, Ray CC Cheung, and Hayden KH So. Dynamic sparse training: Find efficient sparse network from scratch with trainable masked layers. arXiv preprint arXiv:2005.06870, 2020.   \n[32] Xiao Zhou, Weizhong Zhang, Zonghao Chen, Shizhe Diao, and Tong Zhang. Efficient neural network training via forward and backward propagation sparsification. Advances in neural information processing systems, 34:15216\u201315229, 2021.   \n[33] Amirkeivan Mohtashami, Martin Jaggi, and Sebastian Stich. Masked training of neural networks with partial gradients. In International Conference on Artificial Intelligence and Statistics, pages 5876\u20135890. PMLR, 2022.   \n[34] Lu Yin, Gen Li, Meng Fang, Li Shen, Tianjin Huang, Zhangyang Wang, Vlado Menkovski, Xiaolong Ma, Mykola Pechenizkiy, Shiwei Liu, et al. Dynamic sparsity is channel-level sparsity learner. Advances in Neural Information Processing Systems, 36, 2023.   \n[35] Penghang Yin, Jiancheng Lyu, Shuai Zhang, Stanley Osher, Yingyong Qi, and Jack Xin. Understanding straight-through estimator in training activation quantized neural nets. arXiv preprint arXiv:1903.05662, 2019.   \n[36] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance estimation for neural network pruning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11264\u201311272, 2019.   \n[37] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.   \n[38] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[39] Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data distribution for federated visual classification. arXiv preprint arXiv:1909.06335, 2019.   \n[40] Hattie Zhou, Janice Lan, Rosanne Liu, and Jason Yosinski. Deconstructing lottery tickets: Zeros, signs, and the supermask. Advances in neural information processing systems, 32, 2019.   \n[41] Kai Yi, Nidham Gazagnadou, Peter Richt\u00e1rik, and Lingjuan Lyu. Fedp3: Federated personalized and privacy-friendly network pruning under model heterogeneity. In The Twelfth International Conference on Learning Representations.   \n[42] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[43] Fengxiang He, Bohan Wang, and Dacheng Tao. Tighter generalization bounds for iterative differentially private learning algorithms. In Uncertainty in Artificial Intelligence, pages 802\u2013 812. PMLR, 2021.   \n[44] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg on non-iid data. arXiv preprint arXiv:1907.02189, 2019.   \n[45] Atilim Gunes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic differentiation in machine learning: a survey. Journal of machine learning research, 18(153):1\u201343, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Experiments ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Implementation Detail ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We run all experiments on NVIDIA A100 GPUs with PyTorch. In Table 6, we provide detailed information of model architectures for each dataset. For the FMNIST dataset, we use the Lenet-5- Caffe model, which is Caffe variant of Lenet-5. The Lenet model has 430500 of model parameters and 580 of trainable thresholds. For the CIFAR-10 dataset, we use a CNN model of seven layers used in [40]. It has 807366 of model parameters and 1418 of trainable thresholds. The ResNet-18 model is adopted for the CIFAR-100 dataset with 11159232 of model parameters and 4800 of thresholds. We use a stochastic gradient optimizer with momentum of 0.9. For FMNIST with the Lenet model, we use $\\eta(t)=0.001$ , $E=5$ , a batch size of 64, and $\\alpha=0.002$ . For CIFAR-10, we use $\\eta(t)=0.01$ , $E=5$ , a batch size of 16, and $\\alpha=0.00015$ . For CIFAR-100, we use $\\eta(t)=0.01$ , $E=7$ decayed by 0.993 at each communication round, a batch size of 64, and $\\alpha=0.0007$ . All trainable thresholds are initialized to zero. We noticed that too large sparsity coefficient $\\alpha$ can dominate the training loss, resulting in masking whole parameters in a certain layer. Following the implementation of [31], if a certain layer\u2019s density becomes less than $1\\%$ , the corresponding trainable thresholds will be reset to zero to avoid masking whole parameters. ", "page_idx": 13}, {"type": "text", "text": "For the ViT, we use the patch size of 4, embedding dimension of 128, depth of 6, 8 heads, and set the dimension of linear layers as 256. We use the same setting with the above CIFAR-10 experiments except $\\alpha=0.0001$ and $E=1$ . ", "page_idx": 13}, {"type": "table", "img_path": "dAXuir2ets/tmp/f5afdafebb7166ff5e2e4ef65a7cffe1870a2ec4a24fe06cecbdcb32f2808d98.jpg", "table_caption": ["Table 6: Model architectures used in our experiments "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.1.1 More details about baselines ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We compare SpaFL with sparse baselines that investigated structured sparsity. In FedAvg [1], every client trains a global dense model and communicates whole model parameters. We used the equal weighted average for the model aggregation. FedPM [28] optimizes a binary mask while freezing model parameters. Clients only transmit their arithmetically coded binary masks to the server, and the server broadcasts real-valued probability masks to the clients. We use Adam optimizer with learning rate of 0.1 as done in [28]. HeteroFL [7] selects $\\lceil p C\\rceil$ channels of each layer, where $0\\le\\le1$ and $C$ is the number of channels, to make $p$ reduced submodels. Clients train and communicate $p$ reduced submodels during training. We set $p\\,=\\,0.5$ following [7]. Fjord [9] samples $p$ from a uniform distribution $\\mathcal{U}(p_{\\mathrm{min}},p_{\\mathrm{max}})$ . After sampling $p$ , clients train $p$ reduced submodel by selecting the first $\\lceil p C\\rceil$ channels of each layer. We set $p_{\\mathrm{min}}=0.4$ and $p_{\\mathrm{max}}=0.6$ [9]. We provide the learning rates of the baselines in the following table. FedP3 [41] communicates a subset of sparse layers that are pruned by the server for downlink and personalize the remaining layers. Clients only upload the updated remaining layers to the server. We choose \u2019OPU2\u2019 method, which uniformly selects two layers for clients from the entire network. Hence, clients only upload these chosen layers to the server. For the pruning methods, we adopted the ordered dropout for structured sparsity. FedSpa [4] trains personalized sparse models for clients while maintaining fixed model density during training. The initial pruning rate is set to be 0.5 and decayed using cosine annealing. ", "page_idx": 13}, {"type": "table", "img_path": "dAXuir2ets/tmp/ad08fc71827f6fca29bec080b224492f7e7da3ee2315d905c0e949dfbd250157.jpg", "table_caption": [], "table_footnote": ["Table 7: learning rates used by the baselines "], "page_idx": 14}, {"type": "text", "text": "A.2 Proof of Theorem 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We next present the detailed proof of Theorem 1. The proof is inspired by [23] and [43] To facilitate the proof, we first provide the definition of differential privacy and key lemmas from [43]. ", "page_idx": 14}, {"type": "text", "text": "Definition 1. (Differential privacy). A hypothesis $\\boldsymbol{\\mathcal{A}}$ is $(\\epsilon,\\delta)$ - differentially private for any hypothesis subset $\\mathcal{A}_{0}$ and adjacent datasets $S$ and $S^{\\prime}$ which differ by only one example such that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\log\\left[\\frac{\\mathbb{P}_{A(S)}(A(S)\\in\\mathcal{A}_{0})-\\delta}{\\mathbb{P}_{A(S^{\\prime})}(A(S^{\\prime})\\in\\mathcal{A}_{0})}\\right]\\le\\epsilon.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma 1. (Theorem $^{4}$ in $[43],$ For an iterative algorithm $A_{i}$ at round $i$ , define the update rule as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{M}_{i}:(\\mathcal{A}_{i-1(S),S})->\\mathcal{A}_{i}(S).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "If for any fixed $\\boldsymbol{A}_{i-1}$ , $\\mathcal{M}_{i}$ is $(\\epsilon_{i},\\delta)$ private, then $\\{A_{i}\\}_{i=0}^{T}$ is $(\\epsilon^{\\prime},\\delta^{\\prime})$ differntially private such that \u03f5\u2032 = 2  iT=0 \u03f5i2 log \u03b41\u02dc +  iT=0 \u03f5ieexxpp((\u03f5\u03f5ii))+\u221211, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\delta^{\\prime}=\\exp\\left(-\\frac{\\epsilon^{\\prime}+T\\epsilon}{2}\\right)\\left(\\frac{1}{1+\\exp(\\epsilon)}\\left(\\frac{2T\\epsilon}{T\\epsilon-\\epsilon^{\\prime}}\\right)\\right)^{T}\\left(\\frac{T\\epsilon+\\epsilon^{\\prime}}{T\\epsilon-\\epsilon^{\\prime}}\\right)^{-\\frac{\\epsilon^{\\prime}+T\\epsilon}{2\\epsilon}}-\\left(1-\\frac{\\delta}{1+\\exp(\\epsilon)}\\right)^{T}}\\\\ {+\\displaystyle2-\\left(1-\\exp(\\epsilon)\\frac{\\delta}{1+\\exp(\\epsilon)}\\right)^{\\lceil\\frac{\\epsilon^{\\prime}}{\\epsilon}\\rceil}\\left(1-\\frac{\\delta}{1+\\exp(\\epsilon)}\\right)^{T-\\lceil\\frac{\\epsilon^{\\prime}}{\\epsilon}\\rceil},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma 2. (Theorem $^{\\,l}$ in $[43],$ ) For an $(\\epsilon,\\delta)$ private hypothesis $\\mathcal{A}$ , the training dataset size $D\\leq$ nexp(16\u2212\u03f5)\u03b4, and the loss function ||L||\u221e< 1, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\hat{\\mathcal{R}}(A(\\mathcal{D}))-\\mathcal{R}(A(\\mathcal{D}))\\right|<9\\epsilon\\right]>1-\\frac{\\exp(-\\epsilon)\\delta}{\\epsilon}\\ln\\frac{2}{\\epsilon},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. The overall proof follows [23] by showing that SpaFL is an iterative machine learning algorithm that satisfies differential privacy at each round. Then, we can use lemmas from [43] that provide generalization bound to differential private algorithm. One major difference from [23] is that we have global thresholds not global parameters. ", "page_idx": 14}, {"type": "text", "text": "We first define notations for the proof. The diameter of the gradient space is defined as $\\nonumber M_{g}\\,=$ $\\begin{array}{r l}{\\operatorname*{max}_{w,z,z^{\\prime},\\tau}||\\nabla F(w,\\tau;z)\\!-\\!\\nabla F(\\bar{w},\\tau;z^{\\prime})||}&{{}}\\end{array}$ , where $z$ is an input-output pair. We also denote $G_{k,B}=$ $\\begin{array}{r}{\\frac{1}{|\\boldsymbol{B}|}\\sum_{\\boldsymbol{z}\\in\\boldsymbol{B}}h_{k}(\\tilde{\\boldsymbol{w}}_{k};\\boldsymbol{z})}\\end{array}$ as the average of $h_{k}(\\tilde{w}_{k})$ over $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ . We use $\\mathbb{P}$ as probability distribution and $\\mathbb{P}^{A}$ as the probability distribution conditioned on $A$ . ", "page_idx": 14}, {"type": "text", "text": "From Algorithm 1, it is clear that SpaFL is iteratively optimizing global thresholds $\\tau$ in each client at every round. We now derive the differential privacy of (9) in Algorithm 1. Here, each client calculates $\\boldsymbol{h}_{k}$ using its subset of local data. As done in [23], we assume that additive Gaussian noise sample is added in (9) in Algorithm 1 for the analysis. Since we always have global thresholds at round $t$ , (9) can be seen as sampling a mini-batch $\\mathcal{T}(t)$ from $\\mathcal{D}=\\cup_{k}\\mathcal{D}$ with mini-batch size $\\xi$ and we let ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\mathbb{P}^{S_{\\mathbb{Z}(t)}}(\\tau(t)=\\tau|\\tau(t-1))}{\\mathbb{P}^{S_{\\mathbb{Z}(t)}^{\\prime}}(\\tau(t)=\\tau|\\tau(t-1))}=\\frac{\\mathbb{P}^{S_{\\mathbb{Z}(t)}}(\\eta(t-1)G_{S_{\\mathbb{Z}(t-1)}}+\\mathcal{N}(0,\\sigma^{2}\\mathbb{I})=-\\tau+\\tau(t-1))}{\\mathbb{P}^{S_{\\mathbb{Z}(t)}^{\\prime}}(\\eta(t-1)G_{S_{\\mathbb{Z}(t-1)}^{\\prime}}+\\mathcal{N}(0,\\sigma^{2}\\mathbb{I})=-\\tau+\\tau(t-1))},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\pmb{\\tau}(t)=\\pmb{\\tau}(t-1)-\\eta(t-1)\\left(G_{S_{\\mathbb{Z}(t-1)}}+\\mathscr{N}(0,\\sigma^{2}\\mathbb{I})\\right)$ and $\\begin{array}{r}{G_{S_{\\mathbb{Z}(t-1)}}=\\frac{1}{N}\\sum_{k=1}^{N}G_{k,S_{\\mathbb{Z}_{k}(t-1)}}}\\end{array}$ . We defin $:\\eta(t-1)\\tau^{\\prime}=\\pmb{\\tau}(t-1)-\\pmb{\\tau}(t)-\\eta(t-1)G_{S_{\\mathbb{Z}(t-1)}}$ , then we can rewrite (21) as below ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{\\rho}(A)=\\frac{\\mathbb{P}^{S_{\\mathbb{Z}(t)}}(\\mathcal{N}(0,\\sigma^{2}\\mathbb{I})=\\tau^{\\prime})}{\\mathbb{P}^{S_{\\mathbb{Z}(t)}^{\\prime}}(G_{S_{\\mathbb{Z}(t-1)}^{\\prime}}-G_{S_{\\mathbb{Z}(t-1)}}+\\mathcal{N}(0,\\sigma^{2}\\mathbb{I})=\\tau^{\\prime})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $\\pmb{\\tau}\\sim\\pmb{\\tau}(t-1)-\\eta(t-1)(G_{S_{\\mathbb{Z}(t-1)}}+\\mathcal{N}(0,\\sigma^{2}\\mathbb{I}))$ due to added Gaussian noise samples, $\\tau^{\\prime}\\sim\\mathcal{N}(0,\\sigma^{2}\\mathbb{I})$ . Then, following the definition of differential privacy, we define ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{p}(\\tau^{\\prime})=\\log\\frac{\\mathbb{P}^{S_{\\tau(t)}}(\\mathcal{N}(0,\\sigma^{2}\\mathbb{I})=\\tau^{\\prime})}{\\mathbb{P}^{S_{\\tau(t)}^{\\prime}}(G_{S_{\\tau(t-1)}^{\\prime}}-G_{S_{\\tau(t-1)}}+\\mathcal{N}(0,\\sigma^{2}\\mathbb{I})=\\tau^{\\prime})}}\\\\ &{\\qquad=-\\frac{||\\tau^{\\prime}||^{2}}{2\\sigma^{2}}+\\frac{||\\tau^{\\prime}-G_{S_{\\tau(t-1)}}-G_{S_{\\tau(t-1)}^{\\prime}}||^{2}}{2\\sigma^{2}}}\\\\ &{\\qquad=\\frac{2\\langle\\tau^{\\prime},G_{S_{\\tau(t-1)}}-G_{S_{\\tau(t-1)}^{\\prime}}\\rangle+||G_{S_{\\tau(t-1)}}-G_{S_{\\tau(t-1)}^{\\prime}}||^{2}}{2\\sigma^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where (23) is from the definition of Gaussian distribution. We now denote GSI(t\u22121) \u2212GS\u2032I(t\u22121) in (24) as $\\pmb{v}$ . We derive the bound of $||\\pmb{v}||$ as follows ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle||\\boldsymbol{v}||=||G_{S_{\\mathcal{Z}(t-1)}}-G_{S_{\\mathcal{Z}(t-1)}^{\\prime}}||=||\\frac{1}{N}\\sum_{k=1}^{N}G_{k,S_{\\mathcal{Z}_{k}(t-1)}}-G_{k,S_{\\mathcal{Z}_{k}(t-1)}^{\\prime}}||}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\leq\\frac{1}{N}\\sum_{k=1}^{N}||G_{k,S_{\\mathcal{Z}_{k}(t-1)}}-G_{k,S_{\\mathcal{Z}_{k}(t-1)}^{\\prime}}||}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\leq\\frac{1}{N}\\sum_{k=1}^{N}||\\frac{1}{S_{\\mathcal{Z}(t-1)}}|\\sum_{z\\in S_{\\mathcal{Z}(t-1)}}h_{k}(\\bar{w}_{k}(t-1);z)-\\frac{1}{|S_{\\mathcal{Z}(t-1)}^{\\prime}|}\\sum_{z\\in S_{\\mathcal{Z}_{(t-1)}^{\\prime}}^{\\prime}}h_{k}(\\bar{w}_{k}(t-1);z^{\\prime})||}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\leq\\frac{1}{N}\\sum_{k=1}^{N}\\rho_{k}{M_{g}}=\\bar{\\rho}M_{g},}&{(2\\ell)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where (25) is from the definition of $G$ and (26) is from the definition of the diameter of gradient $M_{g}$ . Note that some elements of $h_{k}(\\tilde{w}_{k};z)$ will be zero since we do not calculate gradients of pruned fliter/neurons due to structured sparsity. Hence, we multiply the current model density to derive (26). ", "page_idx": 15}, {"type": "text", "text": "We next bound $\\langle\\tau^{\\prime},v\\rangle$ in (24). Since $\\langle\\pmb{\\tau}^{\\prime},\\pmb{v}\\rangle\\sim\\mathcal{N}(0,||\\pmb{v}||^{2}\\sigma^{2})$ , we have the following inequality using Chernoff Bound as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left[\\langle\\tau^{\\prime},v\\rangle\\geq\\sqrt{2}||v|\\,\\sigma\\sqrt{\\log{1/\\delta}}\\right]\\leq\\underset{x}{\\mathrm{min}}\\exp\\left(-\\sqrt{2}x||v||\\sigma\\sqrt{\\log{1/\\delta}}\\mathbb{E}[\\exp(x\\langle\\tau^{\\prime},v\\rangle)]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We define $\\delta$ as follows ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\delta=\\operatorname*{min}_{x}\\exp\\left(-\\sqrt{2}x||v||\\sigma\\sqrt{\\log{1/\\delta}}\\mathbb{E}[\\exp(x\\langle\\tau^{\\prime},v\\rangle)]\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, with the probability of $1-\\delta$ with respect to $\\tau^{\\prime}$ , we can derive the bound of (24) as follows ", "page_idx": 15}, {"type": "equation", "text": "$$\nD_{p}(\\tau^{\\prime})\\leq\\frac{\\sqrt{2}\\bar{\\rho}M_{g}\\sigma\\sqrt{\\log{1/\\delta}+\\bar{\\rho}^{2}M_{g}^{2}}}{2\\sigma^{2}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Following Lemma 1 and (13) in [23], we can derive that each round in Algorithm 1 is $(\\tilde{\\epsilon},\\frac{\\xi}{D}\\delta)$ differntially private, where $\\tilde{\\epsilon}$ is given as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tilde{\\epsilon}=\\log\\left(\\frac{D-\\xi}{D}+\\frac{\\xi}{D}\\exp\\left(\\frac{\\sqrt{2}\\bar{\\rho}M_{g}\\sigma\\sqrt{\\log\\frac{1}{\\delta}}+\\bar{\\rho}^{2}M_{g}^{2}}{2\\sigma^{2}}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\xi$ is the size of $S_{\\mathcal{T}(t-1)}$ . Subsequently, we apply Lemma 1 to have $(\\epsilon^{\\prime},\\delta^{\\prime})$ differential privacy for $T$ communication rounds. Lastly, we finish the proof by using Lemma 2. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "A.3 Convergence Rate Analysis ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We derive the convergence rate of SpaFL. Since we only communicate thresholds $\\tau$ , we derive the convergence rate of the global thresholds. In SpaFL, we simultaneously update $\\tau$ and $\\mathbf{\\nabla}w$ , and it is analytically challenging to track the update of $\\tau$ for multiple local epochs $E$ . As such, we analyze the convergence of SpaFL under the special case with $E=1$ . We leave a more general convergence analysis with multiple local epochs for future works. We now make two assumptions [44] as follows ", "page_idx": 16}, {"type": "text", "text": "Assumption 1. (smoothness) $F_{k}(\\cdot)$ is $M$ -smooth for $\\tau$ and client $k,\\,\\forall k$ ", "page_idx": 16}, {"type": "equation", "text": "$$\nF_{k}({\\pmb w},{\\pmb\\tau}^{\\prime})\\leq F_{k}({\\pmb w},{\\pmb\\tau})+\\langle\\nabla_{\\tau}F_{k}({\\pmb w},{\\pmb\\tau}),{\\pmb\\tau}^{\\prime}-{\\pmb\\tau}\\rangle+\\frac{M}{2}\\|{\\pmb\\tau}^{\\prime}-{\\pmb\\tau}\\|^{2},\\,\\forall\\tau.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Assumption 2. (Unbiased stochastic gradient) The stochastic gradient $\\boldsymbol{h}_{k}$ is an unbiased estimator of the gradient $\\nabla_{\\tau}F_{k}$ , respectively, for client $k,\\forall k$ , such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}h_{k}({\\boldsymbol{w}}_{k})=\\nabla_{\\tau}F_{k}({\\boldsymbol{w}}_{k},\\tau).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, we have the following convergence rate ", "page_idx": 16}, {"type": "text", "text": "Theorem 2. For $\\begin{array}{r}{\\gamma(t)=\\eta(t)(1-\\frac{\\alpha(1-M\\eta(t))}{2})}\\end{array}$ and the largest number of parameters connected to a neuron or filter $n_{i n}^{m a x}>0$ in a given model, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{1}{\\sqrt{T}}\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}\\|\\displaystyle\\sum_{k=1}^{N}\\nabla_{\\tau}F_{k}(\\bar{w}_{k}(t),\\tau(t))\\|^{2}\\leq\\displaystyle\\sum_{t=0}^{T-1}\\sum_{k=1}^{N}\\underline{{\\mathbb{E}\\|\\nabla_{\\tau}F_{k}(\\tilde{w}_{k}(t),\\tau(t))-\\nabla_{\\tau_{k}}F_{k}(\\tilde{w}_{k}(t),\\tau_{k}(t))\\|^{2}}}}\\\\ &{\\qquad+\\displaystyle\\sum_{t=0}^{T-1}\\frac{2\\alpha\\eta(t)}{T\\gamma(t)}\\left\\{1-M\\eta(t)(1-\\alpha)\\right\\}\\|\\exp(-\\tau(t))\\|^{2}}\\\\ &{\\qquad+\\displaystyle\\sum_{t=0}^{T-1}\\sum_{k=1}^{N}\\frac{M^{2}\\eta(t)^{2}n_{i n}^{m a x}}{N T\\gamma(t)}\\mathbb{E}F_{k}(\\tilde{w}_{k}(t),\\tau(t))}\\\\ &{\\qquad+\\displaystyle\\sum_{t=0}^{T-1}\\sum_{k=1}^{N}\\underline{{\\mathbb{E}\\|\\tau(t)-\\tau_{k}(t)\\|^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "From (9), thresholds $\\tau(t)$ are updated using parameter gradients $\\pmb{g}_{k}(t),k\\in S$ . We can expect that the thresholds will converge when parameters $w_{k},\\forall k$ , converge. We can see that the sparsity regularizer coefficient $\\alpha$ impacts convergence. As $\\alpha$ increases, we can quickly enforce more sparsity to the model. However, a very large $\\alpha$ will damage the performance as $\\gamma(t)$ decreases in (1). We can also observed that the convergence depends on the difference between the received global thresholds $\\tau(t)$ and the updated thresholds $\\tau_{k}(t)$ . Hence, a very large change to the global thresholds will lead to a significantly different binary mask in the next round. Then, local training can be unstable as parameters have to adapt to the new mask. Therefore, from Theorem 2, we can capture the tradeoff between the computing cost and the learning performance in terms of $\\alpha$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. We first consider the case in which global thresholds converge. We have the following update rule for global thresholds as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tau(t+1)=\\frac{1}{K}\\sum_{k\\in\\mathcal{S}}\\tau_{k}(t)=\\tau(t)-\\frac{1}{K}\\eta(t)\\sum_{k\\in\\mathcal{S}}h_{k}(\\tilde{w}_{k}(t))+\\alpha\\eta(t)\\exp(-\\tau(t)).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We take the expectation over the randomness in client scheduling and stochastic gradients as follows ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\pmb{\\tau}(t+1)=\\pmb{\\tau}(t)-\\frac{\\eta(t)}{K}\\mathbb{E}\\sum_{k\\in\\mathcal{S}}\\pmb{h}_{k}(\\pmb{\\tilde{w}}_{k}(t))+\\alpha\\eta(t)\\exp(-\\pmb{\\tau}(t)).}\\\\ &{\\qquad\\qquad=\\pmb{\\tau}(t)-\\frac{\\eta(t)}{N}\\mathbb{E}\\sum_{k=1}^{N}\\nabla_{\\pmb{\\tau}}F_{k}(\\pmb{\\tilde{w}}_{k}(t),\\pmb{\\tau}(t))+\\alpha\\eta(t)\\exp(-\\pmb{\\tau}(t)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence, clearly $\\tau$ will eventually converge if $\\begin{array}{r}{\\frac{1}{N}\\mathbb{E}||\\sum_{k=1}^{N}\\nabla_{\\tau}F_{k}(\\tilde{\\pmb{w}}_{k}(t),\\pmb{\\tau}(t))||^{2}}\\end{array}$ converges. We next show that this conditional statement holds in our  SpaFL framework. ", "page_idx": 17}, {"type": "text", "text": "From the $M$ -smoothness of the loss function in Assumption 1, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\nF_{k}(\\tilde{w}_{k}(t),\\tau_{k}(t))\\!\\leq\\!F_{k}(\\tilde{w}_{k}(t),\\tau(t))\\!+\\!\\langle\\nabla_{\\tau}F_{k}(\\tilde{w}_{k}(t),\\tau(t)),\\tau_{k}(t)\\!-\\!\\tau(t)\\rangle\\!+\\!\\frac{M}{2}||\\tau_{k}(t)-\\tau(t)||^{2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To facilitate the analysis, we first derive $\\pmb{\\tau}_{k}(t)-\\pmb{\\tau}(t)$ as below ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\pmb{\\tau}_{k}(t)-\\pmb{\\tau}(t)=-\\eta(t)\\pmb{h}_{k}(\\pmb{\\tilde{w}}_{k}(t))+\\alpha\\eta(t)\\exp(-\\pmb{\\tau}(t)).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, we can change (36) as follows ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{k}(\\tilde{w}_{k}(t),\\tau_{k}(t))\\leq F_{k}(\\tilde{w}_{k}(t),\\tau(t))+\\langle\\nabla_{\\tau}F_{k}(\\tilde{w}_{k}(t),\\tau(t)),-\\eta(t)h_{k}(\\tilde{w}_{k}(t))\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad+\\langle\\nabla_{\\tau}F(\\tilde{w}_{k}(t),\\tau(t)),\\alpha\\eta(t)\\exp(-\\tau(t))\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad+\\displaystyle\\frac{M\\eta(t)^{2}}{2}\\|h_{k}(\\tilde{w}_{k}(t))-\\alpha\\eta(t)\\exp(-\\tau(t))\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We next take the expectation to the above inequality and use Assumption 2 as below ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}F_{k}(\\tilde{w}_{k}(t),\\tau_{k}(t))\\leq\\mathbb{E}F_{k}(\\tilde{w}_{k}(t),\\tau(t))+\\langle\\nabla_{\\tau}F_{k}(\\tilde{w}_{k}(t),\\tau(t)),-\\eta(t)\\nabla_{\\tau}F_{k}(\\tilde{w}_{k}(t),\\tau(t))\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\langle\\nabla_{\\tau}F_{k}(\\tilde{w}_{k}(t),\\tau(t)),\\alpha\\eta(t)\\exp(-\\tau(t))\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\frac{M\\eta(t)^{2}}{2}\\mathbb{E}\\|h_{k}(\\tilde{w}_{k}(t))-\\alpha\\exp(-\\tau(t))\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbb{E}F_{k}(\\tilde{w}_{k}(t),\\tau(t))-\\eta(t)\\|\\nabla_{\\tau}F_{k}(\\tilde{w}_{k}(t),\\tau(t))\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\underbrace{\\alpha\\eta(t)(1-M\\eta(t))\\langle\\nabla_{\\tau}F_{k}(\\tilde{w}_{k}(t),\\tau(t)),\\exp(-\\tau(t))\\rangle}_{A}}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\underbrace{M\\eta(t)^{2}\\mathbb{E}\\|h_{k}(\\tilde{w}_{k}(t))\\|^{2}}_{B}+\\frac{M\\alpha^{2}\\eta(t)^{2}}{2}\\|\\exp(-\\tau(t))\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We first bound A using \u27e8a, b\u27e9\u2264 ||a||22+||b||2 as below ", "page_idx": 17}, {"type": "equation", "text": "$$\nA\\leq\\frac{\\alpha\\eta(t)(1-M\\eta(t))}{2}\\left[||\\nabla_{\\tau}F_{k}(\\tilde{w}_{k}(t),\\tau(t))||^{2}+||\\exp(-\\tau(t))||^{2}\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We now further bound $B$ as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B=\\frac{M\\eta(t)^{2}}{2}\\underset{l=1}{\\overset{L}{\\sum}}\\underset{l=1}{\\overset{n_{u}^{\\prime}}{\\sum}}\\underset{j=1}{\\overset{n_{u}^{\\prime}}{\\sum}}\\{\\underset{j=1}{\\sum}(\\boldsymbol{g}_{k}(\\tilde{w}_{k}(t)))_{i j}^{l}\\boldsymbol{w}_{k,j}^{E-1,l}(t)\\}|^{2}}\\\\ &{\\leq\\frac{M\\eta(t)^{2}}{2}\\underset{l=1}{\\overset{n_{u}^{\\prime}}{\\sum}}\\underset{i=1}{\\overset{n_{u}^{\\prime}}{\\sum}}\\underset{j=1}{\\overset{n_{u}^{\\prime}}{\\sum}}\\{\\underset{\\left\\vert\\boldsymbol{g}_{k}(\\tilde{w}_{k}(t))\\right\\vert}{\\sum}\\}_{i j}^{l}\\boldsymbol{w}_{k,j}^{E-1,l}(t)\\vert^{2}}\\\\ &{\\leq\\frac{M\\eta(t)^{2}n_{u}^{\\mathrm{max}}}{2}\\underset{l=1}{\\overset{n_{u}^{\\prime}}{\\sum}}\\underset{i=1}{\\overset{n_{u}^{\\prime}}{\\sum}}\\underset{j=1}{\\overset{n_{u}^{\\prime}}{\\sum}}\\{\\underset{\\left\\vert\\boldsymbol{g}_{k}(\\tilde{w}_{k}(t))\\right\\vert}{\\sum}}_{i j}^{l}\\boldsymbol{w}_{k,j}^{E-1,l}(t)\\vert^{2}}\\\\ &{\\overset{(a)}{\\leq}\\frac{M\\eta(t)^{2}n_{u}^{\\mathrm{max}}}{2}\\underset{l=1}{\\overset{n_{u}^{\\prime}}{\\sum}}\\underset{i=1}{\\overset{n_{u}^{\\prime}}{\\sum}}\\underset{j=1}{\\overset{n_{u}^{\\prime}}{\\sum}}\\{\\underset{\\left\\vert\\boldsymbol{g}_{k}(\\tilde{w}_{k}(t))\\right\\vert}{\\sum}}_{i j}^{l}\\vert^{2}}\\\\ &{=\\frac{M\\eta(t)^{2}n_{u}^{\\mathrm{max}}}{2}\\underset{l=1}{\\overset{n_{u}^{\\prime}}{\\sum}}\\{\\underset{i=1}{\\overset{n_{u}^{\\prime}}{\\sum}}\\{\\left\\vert\\boldsymbol{g}_{k}(\\tilde{w}_{k}(t))\\right\\vert^{2}\\}}\\leq M^{2}\\eta(t)^{2}n_{u}^\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $n_{\\mathrm{in}}^{\\mathrm{max}}$ is the largest number of parameters connected to a neuron or filter in a given model, $(a)$ is from $|w|\\leq1$ in Section 3.2.1, and the last inequality is from the $M$ -smoothness of $F_{k}$ . By combining $A$ and $B$ with taking expectation, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}F_{k}(\\tilde{w}_{k}(t),\\tau_{k}(t))\\le\\mathbb{E}F_{k}(\\tilde{w}_{k}(t),\\tau(t))-\\eta(t)\\left\\{1\\!-\\!\\frac{\\alpha\\left(1-M\\eta(t)\\right)}{2}\\right\\}||\\nabla_{\\tau}F_{k}(\\tilde{w}_{k}(t),\\tau(t))||^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad+\\frac{\\alpha\\eta(t)\\left(1-M\\eta(t)(1-\\alpha)\\right)}{2}||\\mathrm{exp}(-\\tau(t))||^{2}+M^{2}\\eta(t)^{2}n_{\\mathrm{in}}^{\\mathrm{max}}\\mathbb{E}F_{k}(\\tilde{w}_{k},\\tau(t))}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By arranging the above inequality, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lvert\\nabla_{\\tau}F(\\tilde{w}_{k}(t),\\tau(t))\\rvert\\rvert^{2}\\le\\frac{1}{\\gamma(t)}\\left[\\mathbb{E}\\underbrace{F_{k}(\\tilde{w}_{k}(t),\\tau(t))-\\mathbb{E}F_{k}(\\tilde{w}_{k}(t),\\tau_{k}(t))}_{(A)}\\right]}&{}\\\\ {+\\frac{\\alpha\\eta(t)}{\\gamma(t)}\\left\\{1-M\\eta(t)(1-\\alpha)\\}\\lvert|\\exp(-\\tau(t))\\rvert\\rvert^{2}+\\frac{M^{2}\\eta(t)^{2}n_{\\operatorname*{in}}^{\\operatorname*{max}}}{\\gamma(t)}\\mathbb{E}F_{k}(\\tilde{w}_{k},\\tau(t))\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "hwahveer et $\\begin{array}{r}{\\gamma(t)=\\eta(t)(1-\\frac{\\alpha(1-M\\eta(t))}{2})}\\end{array}$ . We now further bound $(A)$ in (43). From Assumption 1, we ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\boldsymbol{A})\\leq\\langle\\nabla_{\\tau}F_{k}(\\tilde{w}_{k}(t),\\tau(t)),\\tau(t)-\\tau_{k}(t)\\rangle+\\frac{1}{2M}||\\nabla_{\\tau}F_{k}(\\tilde{w}_{k}(t),\\tau(t))-\\nabla_{\\tau_{k}}F_{k}(\\tilde{w}_{k}(t),\\tau_{k}(t))||^{2}}}\\\\ &{}&{\\leq\\frac{\\gamma(t)}{2}||\\nabla_{\\tau}F_{k}(\\tilde{w}_{k}(t),\\tau(t))||^{2}+\\frac{1}{2\\gamma(t)}||\\tau(t)-\\tau_{k}(t)||^{2}}\\\\ &{}&{\\ \\ \\ +\\frac{1}{2M}||\\nabla_{\\tau}F_{k}(\\tilde{w}_{k}(t),\\tau(t))-\\nabla_{\\tau_{k}}F_{k}(\\tilde{w}_{k}(t),\\tau_{k}(t))||^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Based on (44), we can bound (43) as below ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{|\\nabla_{\\tau}F(\\tilde{w}_{k}(t),\\tau(t))||^{2}\\leq\\frac{1}{M\\gamma(t)}\\mathbb{E}||F_{k}(\\nabla_{\\tau(t)}\\tilde{w}_{k}(t),\\tau(t))-\\nabla_{\\tau_{k}(t)}F_{k}(\\tilde{w}_{k}(t),\\tau_{k}(t))||^{2}}&\\\\ &{\\quad}&{+\\frac{2\\alpha\\eta(t)}{\\gamma(t)}\\left\\{1-M\\eta(t)(1-\\alpha)\\right\\}||\\exp(-\\tau(t))||^{2}+\\frac{2M^{2}\\eta(t)^{2}n_{\\mathrm{in}}^{\\mathrm{max}}}{\\gamma(t)}\\mathbb{E}F_{k}(\\tilde{w}_{k},\\tau_{k}(t),\\tau_{k}(t))}\\\\ &{\\quad}&{+\\,\\frac{||\\tau(t)-\\tau_{k}(t)||^{2}}{\\gamma(t)^{2}}.}&{(45)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "From (45), we can bound the averaged aggregated gradients with respect to thresholds as below ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{1}{\\mathrm{V}}\\mathbb{E}\\|\\sum_{k=1}^{N}\\nabla_{\\tau}F(\\tilde{w}_{k}(t),\\tau(t))\\|^{2}\\leq\\frac{1}{N}\\sum_{k=1}^{N}\\mathbb{E}\\|\\nabla_{\\tau}F(\\tilde{w}_{k}(t),\\tau(t))\\|^{2}}}\\\\ &{\\leq\\frac{1}{N M\\gamma(t)}\\left(\\sum_{k=1}^{N}\\mathbb{E}\\|F_{k}(\\nabla_{\\tau(t)}\\tilde{w}_{k}(t),\\tau(t))-\\nabla_{\\tau_{k}(t)}F_{k}(\\tilde{w}_{k}(t),\\tau_{k}(t),\\tau(t))\\|^{2}\\right)}\\\\ &{\\quad+\\frac{2\\alpha\\eta(t)}{\\gamma(t)}\\left\\{1-M\\eta(t)(1-\\alpha)\\right\\}\\|\\exp(-\\tau(t))\\|^{2}}\\\\ &{\\quad+\\frac{1}{N}\\sum_{k=1}^{N}\\frac{2M^{2}\\eta(t)^{2}n_{\\operatorname*{in}}^{\\operatorname*{max}}}{\\gamma(t)}\\mathbb{E}F_{k}(\\tilde{w}_{k},\\tau(t))+\\frac{1}{N}\\sum_{k=1}^{N}\\frac{\\mathbb{E}\\|\\tau(t)-\\tau_{k}(t)\\|^{2}}{\\gamma(t)_{\\operatorname*{in}}^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By summing the above inequality from $t=0$ to $T-1$ , we can obtain the result of Theorem 2. ", "page_idx": 18}, {"type": "text", "text": "Based on Theorem 2, we can derive t\u221ahe convergence rate with the Big-O notation by following the steps in [8]. We first assume $\\eta=1/\\sqrt{T}$ and $0\\leq\\alpha<1$ . Then, we can bound $\\begin{array}{r}{\\frac{1}{\\gamma(t)}\\leq\\frac{2}{\\eta(t)(1-\\alpha)}}\\end{array}$ . By ", "page_idx": 18}, {"type": "text", "text": "replacing $\\eta(t)$ and $\\gamma(t)$ with their assumed value and bound into the above convergence rate, we have the following bound ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{O}(\\frac{A}{\\sqrt{T}(1-\\alpha)})+\\mathcal{O}(\\frac{B}{T(1-\\alpha)})+\\mathcal{O}(\\frac{C}{\\sqrt{T}})+\\mathcal{O}(\\frac{D}{\\sqrt{T}}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\begin{array}{r}{A=\\sum_{t=0}^{T-1}\\sum_{k=1}^{N}\\frac{\\mathbb{E}\\|\\nabla_{\\tau}F_{k}(\\tilde{w}_{k}(t),\\tau(t))-\\nabla_{\\tau_{k}}F_{k}(\\tilde{w}_{k}(t),\\tau(t))\\|^{2}}{M N},B=\\sum_{t=0}^{T-1}4\\alpha||\\exp(-\\tau(t))||^{2}}\\end{array}$ 2, C = M 2nimnaxG2/2, and D = tT =\u221201 $\\begin{array}{r}{D=\\sum_{t=0}^{T-1}\\sum_{k=1}^{N}\\frac{||\\tau(t)-\\tau_{k}(t)||^{2}}{N}}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "A.4 Communication Costs Measure ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We calculate the communication cost of SpaFL considering both uplink and downlink communications. At each round $t$ , sampled clients transmit their updated thresholds to the server. Hence, the uplink communication costs can be given by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{Comm}_{\\mathrm{Up}}=K\\times\\tau_{\\mathrm{num}}\\times32~[\\mathrm{bits}],\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\tau_{\\mathrm{num}}$ is the number of thresholds of a given model. In downlink, the server broadcasts the updated global threshold to sampled clients. Hence, the downlink communication costs can be given as below ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{{Comm}_{d o w n}}=K\\times\\tau_{\\mathrm{{num}}}\\times32~[\\mathrm{{bits}].}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, total communication costs can be given by $T\\times(\\mathrm{Comm}_{\\mathrm{Up}}+\\mathrm{Comm}_{\\mathrm{down}}).$ . ", "page_idx": 19}, {"type": "text", "text": "A.5 FLOPs Measure ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We calculate the number of FLOPs during training using the framework introduced in [32]. We consider a convolutional layer with an input tensor $\\boldsymbol{X}\\:\\stackrel{\\smile}{\\in}\\:\\mathbb{R}^{N\\times C\\times X\\times Y}$ , parameter tensor $W\\,\\in$ $\\mathbb{R}^{F\\times C\\times R\\times S}$ , and output tensor $O\\in\\mathbb{R}^{N\\times\\bar{F^{\\star}}\\times H\\times W}$ . Here, the input tensor $X$ consists of $N$ number of samples, each of which has $X\\times Y$ dimension. The parameter tensor $W$ has $F$ fliters of $C$ channels with kernel size $R\\times S$ . The output tensor $O$ will have $F$ output channels with dimension $H\\times W$ for $N$ samples. During forward propagation, a filter in $W$ performs convolution operation with the input tensor $X$ to produce a single value in the output tensor $O$ . Hence, we can approximate the number of FLOPs as $N\\times(C\\times R\\times S)\\times F\\times H\\times W$ . Since we use a sparse model during forward propagation, the number of FLOPs can be reduced to $\\rho\\times N\\times(C\\times R\\stackrel{\\cdot}{\\times}S)\\times F\\times H\\times\\Bar{W}$ , where $\\rho={\\frac{||p||_{0}}{||W||_{0}}}$ ||||Wp| |||00 is the density of the parameter matrix W. For the backpropagation, we calculate it as 2 times of that of forward propagation following [45]. ", "page_idx": 19}, {"type": "text", "text": "For a fully connected layer with input tensor $X\\in\\mathbb{R}^{N\\times X}$ and parameter tensor $W\\in\\mathbb{R}^{X\\times Y}$ , the input tensor $X$ is multiplied with $W$ during the forward propagation. Hence, with the density of $W$ , we can calculate the number of FLOPs for the forward propagation as $\\rho\\times N\\times X\\times Y$ . In backpropagation, we follow the same process for convolutional layers. ", "page_idx": 19}, {"type": "text", "text": "We also consider the number of FLOPs to perform line 6 in Algorithm 1 for updating the local models from global thresholds. Sampled clients first have to decide update directions by doing summation of connected parameters at each neuron/filter (sum operation). Then, they update their local models using the received global thresholds (sum and multiply operations). This corresponds to $1.5\\times d$ FLOPs, where $d$ is the number of model parameters. Then, the total number of FLOPs during one local epoch at round $t$ can be approximately given by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{FLOP}(t)=\\displaystyle\\sum_{l=1}^{L}3N\\times(C_{l}\\times R_{l}\\times S_{l})\\times F_{l}\\times H_{l}\\times W_{l}\\times\\mathbb{1}\\{\\mathrm{layer}\\,l==\\mathrm{conv}\\}}\\\\ &{\\qquad\\qquad\\qquad+\\,3\\times N\\times X_{l}\\times Y_{l}\\times\\mathbb{1}\\{\\mathrm{layer}\\,l==\\mathrm{fc}\\}+1.5d}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "B Change of sparsity patterns on CIFAR-10 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Here, we present the change of sparsity patterns of different layers on CIFAR-10. ", "page_idx": 19}, {"type": "image", "img_path": "dAXuir2ets/tmp/09704f55de9e4ab8c73dcad56f8dddbeeb931610f5396414da2965ee64fa6061.jpg", "img_caption": ["Figure 4: Sparsity patterns of conv2 layer on CIAFR-10 "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "dAXuir2ets/tmp/a1930edf5c8032a94dd7f76ec298ef511d2be77616889e96ff4e16cad5609498.jpg", "img_caption": ["Figure 5: Sparsity patterns of dense1 layer on CIAFR-10 "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "dAXuir2ets/tmp/9f1f6c4e7af1db50217bd3f9fc31612865e2678de5036c6e692611e3cb3b9c8b.jpg", "img_caption": ["Figure 6: Sparsity patterns of conv1 layer on CIAFR-10 "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "B.1 Change of Model Sparsity patterns in conv2 ", "page_idx": 20}, {"type": "text", "text": "B.2 Change of Model Sparsity patterns in dense1 ", "page_idx": 20}, {"type": "text", "text": "From Figs. 4 and 5, we can observe that clients learn common sparsity patterns across layers by communicating thresholds. ", "page_idx": 20}, {"type": "text", "text": "B.3 Change of Model Sparsity patterns with different data heterogeneity ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 21}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 21}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] . ", "page_idx": 21}, {"type": "text", "text": "\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available. \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 21}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 21}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification:   \nGuidelines: \u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] Justification: ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 22}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] Justification: ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] NA ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We do not include pretraiend language models or image generators. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. ", "page_idx": 25}, {"type": "text", "text": "\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] Justification: Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] Justification: we do not have crowdsourcing experiments. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. ", "page_idx": 26}, {"type": "text", "text": "\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] Justification: We do not have human subjects. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]