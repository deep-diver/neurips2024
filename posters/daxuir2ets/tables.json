[{"figure_path": "dAXuir2ets/tables/tables_3_1.jpg", "caption": "Table 1: Only thresholds are trained and communicated while parameters are kept frozen.", "description": "This table shows the results of an experiment where only the thresholds were trained and communicated, while the model parameters were kept frozen.  The experiment was conducted on three datasets (FMNIST, CIFAR-10, and CIFAR-100). The table shows that even without training the model parameters, learning sparse structures through training only the thresholds, improves performance. The accuracy achieved by training thresholds alone is reported, along with the initial values of the thresholds used before training.", "section": "3 SpaFL Algorithm"}, {"figure_path": "dAXuir2ets/tables/tables_8_1.jpg", "caption": "Table 2: Performance of SpaFL and other baselines along with their used communication costs (Comm) and computation (FLOPs) resources during whole training.", "description": "This table presents a comparison of SpaFL's performance against several baseline federated learning algorithms across three benchmark datasets (FMNIST, CIFAR-10, and CIFAR-100).  The metrics compared include accuracy, communication costs (in Gbits), and computational costs (in e+11, e+13, and e+14 FLOPs for the respective datasets).  The table highlights SpaFL's superior performance in terms of accuracy while significantly reducing communication and computational overhead compared to the baselines. This demonstrates the effectiveness of SpaFL in optimizing sparse models with low computational overhead.", "section": "5 Experiments"}, {"figure_path": "dAXuir2ets/tables/tables_8_2.jpg", "caption": "Table 2: Performance of SpaFL and other baselines along with their used communication costs (Comm) and computation (FLOPs) resources during whole training.", "description": "This table compares the performance of SpaFL against other federated learning algorithms across three datasets: FMNIST, CIFAR-10, and CIFAR-100.  The metrics presented are the accuracy achieved, the amount of communication in gigabits, the number of floating point operations (FLOPs) in e+11, e+13, and e+14, respectively for each dataset.  The results highlight SpaFL's efficiency in achieving high accuracy while using significantly fewer communication and computational resources compared to the baselines.", "section": "5 Experiments"}, {"figure_path": "dAXuir2ets/tables/tables_9_1.jpg", "caption": "Table 4: Performance of SpaFL with the ViT architecture on CIFAR-10", "description": "This table compares the performance of SpaFL and FedAvg on the CIFAR-10 dataset using the Vision Transformer (ViT) architecture.  It shows that SpaFL achieves a significantly higher accuracy (69.78% \u00b1 2.62%) compared to FedAvg (59.20% \u00b1 0.4%), while maintaining a much sparser model (42.2% \u00b1 4.8% density) compared to the dense model of FedAvg (100% density). This highlights SpaFL's effectiveness in achieving high accuracy with low computational cost by optimizing sparse model structures.", "section": "5 Experiments"}, {"figure_path": "dAXuir2ets/tables/tables_13_1.jpg", "caption": "Table 2: Performance of SpaFL and other baselines along with their used communication costs (Comm) and computation (FLOPs) resources during whole training.", "description": "This table compares the performance of SpaFL against other baseline methods (FedAvg, FedPM, HeteroFL, Fjord, FedSpa, FedP3, and Local) across three datasets: FMNIST, CIFAR-10, and CIFAR-100.  For each method, it presents the accuracy achieved, the communication cost in gigabits (Gbit), and the number of floating-point operations (FLOPs) in e+11, e+13, and e+14, respectively.  The results demonstrate SpaFL's superior performance in terms of accuracy while maintaining significantly lower communication and computation costs.", "section": "5 Experiments"}, {"figure_path": "dAXuir2ets/tables/tables_14_1.jpg", "caption": "Table 2: Performance of SpaFL and other baselines along with their used communication costs (Comm) and computation (FLOPs) resources during whole training.", "description": "This table compares the performance of SpaFL against several other baseline algorithms across three datasets (FMNIST, CIFAR-10, and CIFAR-100).  The metrics presented are accuracy, communication costs (in Gbits), FLOPs (floating point operations, a measure of computational cost in e+11, e+13, and e+14), and the model density (percentage of non-zero parameters).  The table demonstrates SpaFL's efficiency in achieving high accuracy with significantly reduced communication and computation resources compared to the baselines.", "section": "5 Experiments"}]