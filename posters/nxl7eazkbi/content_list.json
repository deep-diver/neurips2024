[{"type": "text", "text": "Model LEGO: Creating Models Like Disassembling and Assembling Building Blocks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jiacong $\\mathbf{H}\\mathbf{u}^{1,5}\\,\\mathbf{v}$ , Jing Gao2 \u2660, Jingwen $\\mathbf{Y}\\mathbf{e}^{3}$ , Yang Gao7, Xingen Wang1,7, Zunlei Feng4,5,6 \u2663, Mingli Song1,5,6 ", "page_idx": 0}, {"type": "text", "text": "1College of Computer Science and Technology, Zhejiang University, 2 Robotics Institute, School of Computer Science, Carnegie Mellon University, 3Electrical and Computer Engineering, National University of Singapore 4School of Software Technology, Zhejiang University, ", "page_idx": 0}, {"type": "text", "text": "5State Key Laboratory of Blockchain and Data Security, Zhejiang University, 6Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data Security, 7Bangsheng Technology Co., Ltd. ", "page_idx": 0}, {"type": "text", "text": "jiaconghu@zju.edu.cn,jinggao2@andrew.cmu.edu,jingweny@nus.edu.sg, {roygao,newroot,zunleifeng,brooksong}@zju.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "With the rapid development of deep learning, the increasing complexity and scale of parameters make training a new model increasingly resource-intensive. In this paper, we start from the classic convolutional neural network (CNN) and explore a paradigm that does not require training to obtain new models. Similar to the birth of CNN inspired by receptive fields in the biological visual system, we draw inspiration from the information subsystem pathways in the biological visual system and propose Model Disassembling and Assembling (MDA). During model disassembling, we introduce the concept of relative contribution and propose a component locating technique to extract task-aware components from trained CNN classifiers. For model assembling, we present the alignment padding strategy and parameter scaling strategy to construct a new model tailored for a specific task, utilizing the disassembled task-aware components. The entire process is akin to playing with LEGO bricks, enabling arbitrary assembly of new models, and providing a novel perspective for model creation and reuse. Extensive experiments showcase that task-aware components disassembled from CNN classifiers or new models assembled using these components closely match or even surpass the performance of the baseline, demonstrating its promising results for model reuse. Furthermore, MDA exhibits diverse potential applications, with comprehensive experiments exploring model decision route analysis, model compression, knowledge distillation, and more. The code is available at https://github.com/jiaconghu/Model-LEGO. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Convolutional Neural Networks (CNNs), as the predominant architecture in deep learning, play a crucial role in image, video, and audio processing [1, 2, 3]. CNNs were originally inspired by the concept of receptive fields in the biological visual system [4], and our focus is to explore and leverage similar characteristics within CNNs. Various studies [5, 6, 7] have delved into unraveling the intricacies of biological visual information processing systems. Notably, Livingstone et al. [6] substantiated that the intermediate visual cortex comprises relatively independent subdivisions. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Some studies [8, 9, 10, 11, 12, 13, 14] have endeavored to visualize critical layers and neurons within CNNs. These visualizations demonstrate that in the more shallow layers of CNNs, form, color, texture, and edge features are processed by distinct convolutional kernels, whereas deeper layers are responsible for category-aware features. The above phenomena of feature processing within CNNs demonstrates the similarity to the parallel processing mechanism [5, 6] and the information integration theory [7] proposed in biological visual systems. Consequently, this paper is dedicated to the exploration and practical utilization of these distinct subsystems within CNNs. ", "page_idx": 1}, {"type": "text", "text": "In pursuit of this exploration, we introduce a pioneering task named Model Disassembling and Assembling (MDA), a novel approach to construct and combine subsystems with LEGO-like flexibility. The conceptual framework behind MDA posits that, much like assembling and disassembling LEGO structures, deep learning models can undergo such operations without incurring significant training overhead or compromising performance. This task is designed to be universally applicable, spanning various existing Deep Neural Networks (DNNs), such as Convolutional Neural Networks (CNNs), Graph Neural Networks (GNNs), Transformers, and others. ", "page_idx": 1}, {"type": "text", "text": "However, constructing the MDA framework presents several challenges, notably in determining the minimal disassembling unit and devising an assembly process with minimal impact on performance. Existing works either require the predefinition of task units during the initial training stage, or the disassembled unit cannot be directly used for inference or assembly [15, 16, 17]. In contrast, our approach distinguishes itself as the inaugural attempt to directly disassemble a trained network into task-aware components, avoiding the need for additional networks or fine-tuning. This ensures efficiency and interpretability throughout the disassembling and assembling processes. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we illustrate our approach using CNN classifiers, with the assurance that its applicability extends to other Deep Neural Network (DNN) architectures. In the disassembling phase, we define the task-aware component by introducing the concept of relative contribution and a mechanism for contribution aggregation and allocation. This is seamlessly applied throughout the forward propagation process of the network. Building on these principles, we introduce a component locating technique that discerns and extracts task-aware components. In the assembling phase, we propose a simple yet effective alignment padding strategy. This involves padding empty kernels onto each convolutional filter to ensure uniform kernel counts across all filters in each layer. Additionally, to account for varying feature magnitudes across different components, we implement a parameter scaling strategy. The resulting MDA framework facilitates recombination among different pre-trained models, providing a vital technique for model reuse. ", "page_idx": 1}, {"type": "text", "text": "Our contributions in this study are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce a novel task, MDA, which aims to disassemble and assemble deep models in an interpretable manner reminiscent of playing with LEGOs. This task is motivated by the subdivision of the biological visual system [6].   \n\u2022 We present the inaugural method for MDA, specifically applied to CNN classifiers. In the model disassembling phase, we introduce a component locating technique to disassemble task-aware components from the models. For model assembling, we propose an alignment padding strategy and a parameter scaling strategy to assemble task-aware components into a new model.   \n\u2022 Extensive experiments validate the efficacy of our proposed MDA method, showing that the performance of the disassembled and assembled models closely matches or even surpasses that of the baseline models.   \n\u2022 MDA introduces a fresh perspective for model reuse. Additionally, we explore diverse applications of MDA, including decision route analysis, model compression, knowledge distillation, and more. ", "page_idx": 1}, {"type": "text", "text": "2 Model Disassembling and Assembling ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we present the definition of Model Disassembling and Assembling (MDA). Let us consider a set of $N$ pre-trained deep learning models denoted as $\\{\\mathcal{M}^{(n)}\\}_{n=1}^{N}$ . Each model $\\mathcal{M}^{(n)}$ comprises $K^{(n)}$ subtasks, represented as $\\{t_{k}^{(n)}\\}_{k=1}^{K^{(n)}}$ . Our objective in model disassembling is to extract the model components $\\mathcal{M}[t_{k}^{(n)}]$ corresponding to a specific subtask $t_{k}^{(n)}$ from the source model $\\mathcal{M}^{(n)}$ . These extracted components $\\mathcal{M}[t_{k}^{(n)}]$ are intended to encapsulate only the parameters critically relevant to the subtask $t_{k}^{(n)}$ . In essence, the disassembled components $\\mathcal{M}[t_{k}^{(n)}]$ should function as an independent model, preserving the full capability for the subtask $t_{k}^{(n)}$ without redundant capability for other subtasks. Furthermore, the assembling process involves combining disassembled components from different models. For example, combining components $\\mathcal{M}[t_{k_{1}}^{(n_{1})},\\ldots,t_{k_{2}}^{(n_{1})}]$ from $\\mathcal{M}^{(n_{1})}$ and $\\mathcal{M}[t_{k_{3}}^{(n_{2})},\\ldots,t_{k_{4}}^{(n_{2})}]$ from $\\mathcal{M}^{(n_{2})}$ results in a new model $\\mathcal{M}^{(n e w)}$ . This assembled model $\\mathcal{M}^{(n e w)}=\\bar{\\mathcal{M}}[t_{k_{1}}^{(n_{1})},\\bar{\\dots}...,t_{k_{2}}^{(n_{1})},t_{k_{3}}^{(n_{2})},\\dots,t_{k_{4}}^{(n_{2})}]$ , t(kn42 )] is expected to retain full functionality for the subtasks {t(kn1 ), $\\{t_{k_{1}}^{(n_{1})},\\dots,t_{k_{2}}^{(n_{1})}\\}$ and $\\{t_{k_{3}}^{(n_{2})},\\ldots,t_{k_{4}}^{(n_{2})}\\}$ ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "MDA is applicable to various existing deep learning architectures, including Convolutional Neural Networks (CNNs), Graph Neural Networks (GNNs), and Transformers. The focus of this paper is to explore the implementation and efficacy of MDA specifically in the context of CNN classifiers. ", "page_idx": 2}, {"type": "text", "text": "3 Model Disassembling ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Contribution Aggregation and Allocation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given a CNN classifier with $K$ categories, each category is treated as an individual subtask within the classifier. Conventionally, a Softmax operation is employed on the output features of the final fully connected layer, transforming these features into a probabilistic distribution that sums to unity. The feature with a relatively higher value corresponds to a higher probability, and the category with the highest probability serves as the predicted result of the classifier. Consequently, the feature exhibiting relatively greater magnitude plays a decisive role in determining the final classification result. We term the relative degree to which features influence the result as relative contribution. It is crucial to note that the concept of relative contribution is not confined solely to the final layer of the network. Features from preceding layers play a pivotal role in shaping the features of subsequent layers. Consequently, we ", "page_idx": 2}, {"type": "image", "img_path": "nxL7eazKBI/tmp/aa6627f8c0c7b1ea13380900bab108231c782081162847c1b7457b7ccd97c2c9.jpg", "img_caption": ["Figure 1: Disassembling process at the $l$ -th layer of a CNN model, where the red solid line represents the contribution aggregation process, and the black dashed line represents the contribution allocation process. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "extend the concept of relative contribution to encompass all layers of the CNN classifier, thereby establishing a comprehensive contribution system that spans the entirety of the network. ", "page_idx": 2}, {"type": "text", "text": "To illustrate this process more specifically, we focus on the $l.$ -th convolutional layer of a CNN (the case of the fully connected layer is discussed in Appendix A). The $l$ -th layer has $P$ input channels aasn $\\{\\mathbb{C}_{q}^{(l)}\\}_{q=1}^{Q}$ $Q$ t.  cEhaacnhn eflilst.e r $\\mathbb{C}_{q}^{(l)}$ ecsopnosnidstisn golf $P$ t choe $l$ v-tohl ultaiyoenr  kceornmeplrsi, stehsu $\\b{Q}$ $\\mathbb{C}_{q}^{(l)}=\\{c_{q,p}^{(l)}\\}_{p=1}^{P}.$ .r s,T hdee ninotpeudt feature maps for the -th layer are represented as $\\{a_{p}^{(l)}\\}_{p=1}^{P}$ . With the convolution filter $\\mathbb{C}_{q}^{(l)}$ , the hidden feature maps $\\mathbb{A}_{q}^{(l)}$ for the $q$ -th channel in the $l$ -th layer are computed as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{A}_{q}^{(l)}=\\{a_{q,p}^{(l)}\\}_{p=1}^{P},a_{q,p}^{(l)}=c_{q,p}^{(l)}\\otimes a_{p}^{(l)},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\otimes$ denotes the convolution operation. From these hidden feature maps $\\mathbb{A}_{q}^{(l)}$ , the output feature map aq $a_{q}^{(l+1)}$ in the $l$ -th layer (which serves as the input feature map $a_{q}^{(l+1)}$ in the $(l+1)$ -th layer) is determined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\na_{q}^{(l+1)}=\\sum_{p=1}^{P}a_{q,p}^{(l)}+b_{q}^{(l)},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $b_{q}^{(l)}$ is the bias for the $q$ -th channel. From Eqn.(2), it is evident that the output feature map a(ql+1)is the summation of the hidden feature maps A(ql ) . This implies that the value of $a_{q}^{(l+1)}$ (l+1)is influenced by the value of each individual hidden feature map $a_{q,p}^{(l)}$ . Similar to the Softmax operation in the final layer, the larger the individual hidden feature map, the greater its contribution to the output $a_{q}^{(l+1)}$ ", "page_idx": 3}, {"type": "text", "text": "3.1.1 Contribution Aggregation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The red solid line in Fig. 1 represents the contribution aggregation process. In this process, the contributions from the input feature maps $\\{a_{p}^{(l)}\\}_{p=1}^{P}$ are aggregated to the hidden feature maps $\\mathbb{A}_{q}^{(l)}$ via the convolution fliter $\\mathbb{C}_{q}^{(l)}$ of the $q_{\\mathrm{~\\,~}}$ -th channel. To quantify the contribution of each hidden feature map aq, $a_{q,p}^{(l)}$ p to the output a(ql $a_{q}^{(l+1)}$ , we introduce a metric $s_{q,p}^{(l)}$ defined as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\ns_{q,p}^{(l)}=\\sum_{h=1}^{H^{(l)}}\\sum_{w=1}^{W^{(l)}}a_{q,p}^{(l)}[h,w],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $H^{(l)}$ and $W^{(l)}$ denote the height and width, respectively, of the feature map $a_{q,p}^{(l)}$ . The term $a_{q,p}^{(l)}[h,w]$ represents the pixel value at the $h$ -th row and $w$ -th column of $a_{q,p}^{(l)}$ . Considering the presence of activation functions such as ReLU, negative contributions are treated as having zero impact on the result. Consequently, the contribution $\\hat{s}_{q,p}^{(l)}$ of the hidden feature map $a_{q,p}^{(l)}$ is recalculated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{s}_{q,p}^{(l)}=\\operatorname*{max}(s_{q,p}^{(l)},0).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In line with the principle of the Softmax operation, the contribution of each hidden feature map is relative. Hence, we employ min-max normalization to obtain the relative contribution $r_{q,p}^{(l)}$ of each hidden feature map aq,p: ", "page_idx": 3}, {"type": "equation", "text": "$$\nr_{q,p}^{(l)}=\\frac{\\hat{s}_{q,p}^{(l)}-\\operatorname*{min}(\\{\\hat{s}_{q,p}^{(l)}\\}_{p=1}^{P})}{\\operatorname*{max}(\\{\\hat{s}_{q,p}^{(l)}\\}_{p=1}^{P})-\\operatorname*{min}(\\{\\hat{s}_{q,p}^{(l)}\\}_{p=1}^{P})+\\varepsilon},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\varepsilon$ is a small constant added to prevent division by zero. This normalization process ensures that the contributions are scaled relative to each other, facilitating the identification of the most influential hidden feature maps in the layer. ", "page_idx": 3}, {"type": "text", "text": "3.1.2 Contribution Allocation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The black dashed line in Fig. 1 represents the process of contribution allocation, where the contribution from the feature map ap is allocated to various hidden feature maps $\\{a_{q,p}^{(l)}\\}_{q=1}^{Q}$ through different convolution kernels $\\{c_{q,p}^{(l)}\\}_{q=1}^{Q}$ . Consequently, the overall contribution s(pl) of the feature map a(pl) is calculated using the following equation: ", "page_idx": 3}, {"type": "equation", "text": "$$\ns_{p}^{(l)}=\\sum_{q=1}^{Q}r_{q,p}^{(l)}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In this calculation, similar to the previous steps, negative contributions are considered as zero: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{s}_{p}^{(l)}=\\operatorname*{max}(s_{p}^{(l)},0).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Moreover, acknowledging that the significance of each feature map is relative, the relative contribution r(pl) of the feature map a(pl) is computed as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nr_{p}^{(l)}=\\frac{\\hat{s}_{p}^{(l)}-\\operatorname*{min}(\\{\\hat{s}_{p}^{(l)}\\}_{p=1}^{P})}{\\operatorname*{max}(\\{s_{p}^{(l)}\\}_{p=1}^{P})-\\operatorname*{min}(\\{\\hat{s}_{p}^{(l)}\\}_{p=1}^{P})+\\varepsilon}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.2 Component Locating Technique ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Building upon the concept of relative contribution and the mechanism of contribution aggregation and allocation, we introduce a novel approach termed the component locating technique. This method is designed to identify task-aware components, that is, the parameters most relevant to a given task, within a CNN. Taking the $l_{\\cdot}$ -th layer as an example, the initial stage of this technique involves identifying the most relevant feature maps for the targeted task. Subsequently, the next step is to pinpoint the most relevant parameters by discerning which parameters are linked to the identified most relevant feature maps. For a specific task, please refer to Appendix B. ", "page_idx": 4}, {"type": "text", "text": "3.2.1 Relevant Feature Identifying ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In essence, the most relevant feature maps are those which exhibit a relatively larger contribution to the result of the model. In the process of contribution aggregation, a threshold value denoted as $\\alpha$ is employed to discern whether a relative contribution $r_{q,p}^{(l)}$ (calculated in Eqn. 5) is large or small: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{r}_{q,p}^{(l)}=\\left\\{\\begin{array}{l l}{1}&{r_{q,p}^{(l)}\\ge\\alpha}\\\\ {0}&{r_{q,p}^{(l)}<\\alpha}\\end{array}\\right.,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\alpha$ is a chosen value within the range (0,1]. Through the above equation, the soft relative contribution $r_{q,p}^{(l)}$ , which are continuous values indicating the degree of contribution, are transformed into hard relative contribution r\u02c6q(l,)p. The s(pl) in Eqn. 6 is now the sum over the hard relative contribution $\\hat{r}_{q,p}^{(l)}$ . Eqn.7 and Eqn.8 remain unchanged. ", "page_idx": 4}, {"type": "text", "text": "Further, a second threshold value $\\beta$ is used to determine whether the relative contribution $r_{p}^{(l)}$ (as defined in Eqn.8) of a feature map is large or small: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{r}_{p}^{(l)}=\\left\\{{1}\\quad r_{p}^{(l)}\\geq\\beta\\right.}\\\\ {0}&{\\left.r_{p}^{(l)}<\\beta\\right.}\\end{array},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "rien $\\beta$ ihs aar dc hroelsaetni vvea lcuoen twriitbhuitni othn , gjeu (st0 ,a1s] .i nT thhise  ecqausaet ioof n.sforms the soft relative contribution $r_{p}^{(l)}$ $\\hat{r}_{p}^{(l)}$ $\\hat{r}_{q,p}^{(l)}$ ", "page_idx": 4}, {"type": "text", "text": "3.2.2 Parameter Linking ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In Eqn. 10, the hard relative contribution $\\hat{r}_{p}^{(l)}$ , being either 0 or 1, indicates whether the feature map a(pl) is most relevant to the predicted result. Similarly, the hard relative contribution r\u02c6q(l+ 1)can also reflect whether the feature map a(ql+1)is most relevant to the predicted result. Integrating Eqn.1 with Eqn.2, we can represent the convolutional process as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\na_{q}^{(l+1)}=\\sum_{p=1}^{P}c_{q,p}^{(l)}\\otimes a_{p}^{(l)}+b_{q}^{(l)},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This equation signifies that the output feature map a(ql+1)is generated by the convolution operation of the input feature maps $\\{a_{p}^{(l)}\\}_{p=1}^{P}$ with the respective convolution kernels $\\{c_{q,p}^{(l)}\\}_{p=1}^{P}$ , which collectively freolremv atnhte  tcoo tnhveo lpurteiodinc tfeildt err $\\mathbb{C}_{q}^{(l)}$ ,.  tThehnu si,t  ilf otghiec aolluyt pfuotl lfoeawtsu rteh amt athp a(ql+1) d to oilsu tdieotne rfmiltienre baen dm tohset $\\mathbb{C}_{q}^{(l)}$ associated bias $b_{q}^{(l)}$ are also most relevant to the predicted result. Furthermore, each input feature map $a_{p}^{(l)}$ engages in the convolution operation exclusively with the kernels $\\{c_{q,p}^{(l)}\\}_{q=1}^{Q}$ across different convolution filters. Therefore, if the feature map $a_{p}^{(l)}$ is identified as most relevant to the predicted result, then the convolution kernels $\\{c_{q,p}^{(l)}\\}_{q=1}^{Q}$ associated with it are also deemed most relevant to the predicted result. The component locating technique is also applicable to fully connected layers, enabling the identification of the most relevant filters, kernels, and biases. ", "page_idx": 4}, {"type": "text", "text": "In summary, through the component locating technique, we can effectively identify and discriminate the most relevant components to a specific task. This includes pinpointing the most relevant filters, kernels, and biases. Subsequently, the identified parameters most relevant to a specific task can be extracted from the source model through a process known as structure pruning [18]. ", "page_idx": 4}, {"type": "text", "text": "4 Model Assembling ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the process of CNN model assembling, our objective is to combine disassembled, task-aware components, typically derived from different source models, into a new model. This assembling is performed without necessitating retraining or incurring performance loss. To achieve this goal, we propose an alignment padding strategy and a parameter scaling strategy. It is important to note that the focus of this paper is specifically on assembling different models that share isomorphic network architectures. ", "page_idx": 5}, {"type": "image", "img_path": "nxL7eazKBI/tmp/5f2bee71854a8d9718d5a563d5d62461c4ebd6bae3350697dfcea95bfef9f56f.jpg", "img_caption": ["Figure 2: Assembling process at the $l$ -th layer of CNN models: (a) and ${\\bf(b)}$ represent two distinct disassembled models, respectively; (c) illustrates the assembled model. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.1 Alignment Padding Strategy ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the assembling of CNN models, we combine models layer by layer along the dimension of the fliters. A challenge in this process is that fliters from different models may have varying numbers of kernels. To address this, we introduce a simple yet effective alignment padding strategy. This strategy involves padding empty kernels to each fliter, ensuring that all fliters in a given layer have a uniform number of kernels. ", "page_idx": 5}, {"type": "text", "text": "Consider the $l$ -th layer as an example. As depicted in Fig.2(a), a disassembled model consists of $J$ convolution filters, denoted as $\\bar{\\{\\mathbb{C}_{j}^{(l)}\\}}_{j=1}^{J}$ . Each convolution filter $\\mathbb{C}_{j}^{(l)}$ comprises $I$ convolutional kernels, expressed as C(jl) $\\mathbb{C}_{j}^{(l)}=\\{c_{j,i}^{(l)}\\}_{i=1}^{I}$ . Another disassembled model, shown in Fig.2(b), contains $V$ convolution filters $\\{\\mathbb{C}_{v}^{(l)}\\}_{v=1}^{V}$ , with each filter $\\mathbb{C}_{v}^{(l)}$ consisting of $U$ convolutional kernels, expressed ians $\\mathbb{C}_{v}^{(l)}=\\{c_{v,u}^{(l)}\\}_{u=1}^{U}$ .t hI scsoenmvbolliuntgi opnr ofcilteesrs,s .i llEuasctrha tfeiltde ir ni nF itgh.i2s( cn)e, wt hemsoe dtewl ois  maoudgemlse natree d mweirtghe da to a new model wi $J+V$   \ncomplementary number of empty kernels. Specifically, if a convolution fliter C\u02c6(jl) originates from $\\mathbb{C}_{j}^{(l)}$ , it is padded with $U$ empty kernels at the end, forming $\\hat{\\mathbb{C}}_{j}^{(l)}=\\{c_{j,1}^{(l)},c_{j,2}^{(l)},\\bar{\\cdot}\\bar{\\cdot}\\cdot,c_{j,I}^{(l)},0_{1},0_{2},\\bar{\\cdot}\\cdot\\cdot,0_{U}\\}$ . Conversely, if a convolution filter $\\hat{\\mathbb{C}}_{v}^{(l)}$ comes from $\\mathbb{C}_{v}^{(l)}$ , it is padded with $I$ empty kernels at the beginning, resulting in $\\hat{\\mathbb{C}}_{v}^{(l)}=\\{0_{1},0_{2},\\ldots,0_{I},c_{v,1}^{(l)},c_{v,2}^{(l)},\\ldots,c_{v,U}^{(l)}\\}$ . Through this alignment padding strategy, every convolution filter in the assembled model is standardized to have the same total of $I+U$ convolution kernels. ", "page_idx": 5}, {"type": "text", "text": "The alignment padding strategy can be readily extended to incorporate multiple disassembled models and applied across all layers of a CNN. A key feature of this approach is that the assembled model is ready for inference immediately, without the need for any retraining. ", "page_idx": 5}, {"type": "text", "text": "4.2 Parameter Scaling Strategy ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In addition to the alignment padding strategy, we address another critical issue in model assembling: the potential disparity in the magnitude of features output by disassembled components from different source models. If left unaddressed, this disparity could cause the assembled model to be biased towards the disassembled components with larger feature magnitudes, impacting the balance and effectiveness of the assembled model. To resolve this, we propose a parameter scaling strategy. ", "page_idx": 5}, {"type": "text", "text": "Taking the $l$ -th layer as an example, as shown in Fig.2(a) and (b), let\u2019s consider the output feature maps in the $l$ -th layer of the disassembled models, denoted as $\\{a_{j}^{(l+1)}\\}_{j=1}^{J}$ and $\\{a_{v}^{(l+1)}\\}_{v=1}^{\\bar{V}}$ , respectively. The magnitude of each feature map a(jl+1)is quantified as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\ne_{j}^{(l+1)}=\\sum_{h=1}^{H^{(l+1)}}\\sum_{w=1}^{W^{(l+1)}}a_{j}^{(l+1)}[h,w],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "table", "img_path": "nxL7eazKBI/tmp/b129dd69bfe18349f2d78dc5a463631d0a7b65ea49b07abaceb0c1a942c99946.jpg", "table_caption": ["Table 1: Comparison of disassembling performance. In the classifier, each category corresponds to a task. \u2018Base.\u2019 refers to the average accuracy for \u2018Disassembled Task\u2019 in the source model. In \u2018Disa.\u2019, \u2018Score1 $(+S c o r e2)^{*}$ represents two metrics: \u2018Score1\u2019 is the accuracy and \u2018Score2\u2019 is the improved accuracy compared to \u2018Base.\u2019 for the \u2018Disassembled Task\u2019 in the disassembled model. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Similarly, the magnitude $e_{v}^{(l+1)}$ for feature map a(vl+1)is calculated using the same equation. We then compute the average magnitude of the feature maps from all disassembled models in the $l$ -th layer: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\bar{e}^{(l+1)}=\\frac{1}{J+V}\\left(\\sum_{j=1}^{J}e_{j}^{(l+1)}+\\sum_{v=1}^{V}e_{v}^{(l+1)}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then, the convolution filter $\\mathbb{C}_{j}^{(l)}$ is scaled as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbb{C}}_{j}^{(l)}=(\\bar{e}^{(l+1)}/e_{j}^{(l+1)})\\mathbb{C}_{j}^{(l)}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In practical applications, this parameter scaling strategy is particularly crucial in the last fully connected layer to ensure that the magnitude differences in the final outputs of disassembled models from different source models are not excessively large. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Experimental Settings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Dataset and Network. We select three datasets and three mainstream CNN classifiers to evaluate our MDA method. The datasets include CIFAR-10 [19], CIFAR-100 [19], and Tiny-ImageNet [20]. The chosen CNN classifiers are VGG-16 [21], ResNet-50 [22], and GoogleNet [23]. ", "page_idx": 6}, {"type": "text", "text": "Parameter Configuration. In our MDA method, the key parameters are $\\alpha$ and $\\beta$ , as defined in Eqn.9 and Eqn.10, respectively. By default, we set $\\alpha=0.3$ and $\\beta=0.2$ in convolutional layers, and $\\alpha=0.4$ and $\\beta=0.3$ in fully connected layers, unless specified otherwise. The model training is conducted using the SGD optimizer, with a learning rate of 0.01. To ensure the reliability and reproducibility of our results, we report the average of three independent experimental runs for each result. Comprehensive details and the source code can be found in the Supplementary Material. ", "page_idx": 6}, {"type": "text", "text": "5.2 MDA Applied to CNN Models ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.2.1 Model Disassembling Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We present the results of model disassembling in Table 1. The results in Table 1 reveal that both single-task and multi-task disassembling, as executed by our method (denoted as \u2019Disa.\u2019), exhibit accuracies comparable to, or even surpassing, those of the source model (denoted as \u2019Base.\u2019). Notably, disassembling single tasks $\\because0^{\\circ}$ or $\\because1\\,\\cdot$ from CIFAR-10 when using GoogleNet achieved a $100\\%$ accuracy rate. Similarly, the accuracy for disassembling multiple tasks \u201970-169\u2019 from Tiny-ImageNet on ResNet-50 showed an improvement of over $2.15\\%$ compared to the source model. What\u2019s more, to go deeper into the performance of our proposed disassembled method, we present the disassembling results on ImageNet [24] and the comparison of parameter size and Floating Point Operations Per Second (FLOPs) in the Supplementary Material. ", "page_idx": 6}, {"type": "table", "img_path": "nxL7eazKBI/tmp/e52bd19d34ebc9b296c0b5d789b6ca115032071ab1f47c939a208d2f43fbb4a3.jpg", "table_caption": ["Table 2: Comparison of assembling performance. \u2018Base.\u2019 indicates the average accuracy for the \u2018Assembled Task\u2019 in the source models. In \u2018Asse.\u2019, \u2018Score1 / Score2\u2019 represent the average accuracy scores for the \u2018Assembled Task\u2019 in the assembled models without fine-tuning and with ten epochs of fine-tuning, respectively. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.2.2 Model Assembling Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The performance of model assembling across different datasets is presented in Table 2. It is observed that the assembled models generally achieve comparable performance to the source models in both single-task and multi-task assembling settings. Notably, the assembled models combining $^{\\ast}0{-}2+0{-}69^{\\ast}$ from \u2018CIFAR- $^{10+}$ Tiny-ImageNet\u2019 on GoogleNet surpass the source model in terms of accuracy. However, there are instances, such as with $\\phantom{0}{20-69}+70{-179}^{\\circ}$ from \u2018CIFAR- $100+$ Tiny-ImageNet\u2019 on ResNet-50, where a decrease in accuracy is noted. This could be attributed to the interaction and interference among the numerous parameters from the different models being assembled, particularly when the number of tasks is large, leading to less stable predictions in the new model. ", "page_idx": 7}, {"type": "text", "text": "5.3 MDA Applied to GCN Model ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The proposed MDA method extends beyond CNN models and is equally applicable to Graph Convolutional Network (GCN) models [25]. We demonstrate this by conducting a disassembling experiment for node classification using a GCN model [25] on the Cora dataset [26]. The results of this experiment are presented in Table 3. ", "page_idx": 7}, {"type": "text", "text": "The results clearly indicate that the accuracy of the disassembled GCN model surpasses that of the source model. For instance, the accuracy of the disassembled model for categories $\"1-2\"$ shows an improvement of $1.\\bar{4}7\\%$ over the source GCN model. ", "page_idx": 7}, {"type": "text", "text": "Table 3: Performance of the GCN model disassembling on the Cora Dataset. \u2018Base.\u2019 represents the average accuracy for the \u2018Disassembled Task\u2019 in the source model. In \u2018Disa.\u2019, \u2018Score1 (+Score2)\u2019 indicates the accuracy and the improvement in accuracy (\u2018Score2\u2019) compared to \u2018Base.\u2019 for the \u2018Disassembled Task\u2019 of the disassembled model. ", "page_idx": 7}, {"type": "table", "img_path": "nxL7eazKBI/tmp/828ac41ad464c6363b0413b5176429b5b76ca31bcc725b941285d77a53eb8c21.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.4 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Parameters $\\alpha$ and $\\beta$ . Fig. 3 presents an ablation study on the thresholds $\\alpha$ and $\\beta$ in the fully connected layer and convolutional layer. With the increase of the thresholds $\\alpha$ and $\\beta$ , fewer parameters will be regarded as relevant to the specific tasks. Therefore, in Fig. 3(a), we observe that as the thresholds $\\alpha$ and $\\beta$ increase, the accuracy of the disassembled model decreases, with thresholds in the convolutional layer being more sensitive than in the fully connected layer. Fig. 3(b) illustrates that thresholds $\\alpha$ and $\\beta$ induce more significant changes in FLOPs in the convolutional layer compared to the fully connected layer. In Fig. 3(c), the model parameter size ratio of the disassembled model decreases with the increase in thresholds. ", "page_idx": 7}, {"type": "image", "img_path": "nxL7eazKBI/tmp/b67dce743cea33fd4984d5f0c71d846bcda3fb17a8fed3f3700ad577346cc28c.jpg", "img_caption": ["Figure 4: Accuracy curve (a), FLOPs ratio curve Figure 3: Accuracy curve (a), FLOPs ratio curve (b), and model parameter size ratio curve (c) for (b), and model parameter size ratio curve (c) the disassembled model as the number of disasfor the disassembled model, varying with hyper- sembled layers varies. The number of disassemparameters in the fully connected layers $(\\alpha_{f},\\beta_{f})$ bled layers accumulates from deep layers to shaland convolutional layers $(\\alpha_{c},\\beta_{c})$ . low layers in the model. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Number of Disassembled Layers. Fig. 4 shows the ablation study on the number of disassembled layers. As depicted in Fig. 4(a), the accuracy initially increases and then decreases with the rising number of disassembled layers, but consistently remains higher than the source model. Additionally, Fig. 4(b, c) reveals that both the FLOPs ratio and parameter size ratio decline as more layers are disassembled. This ablation study validates that our proposed disassembled method can effectively disassemble all layers of the model. ", "page_idx": 8}, {"type": "text", "text": "Assembling Strategy. The impact of different assembling strategies is detailed in Table 4. It is observed that the method combining padding alignment and parameter scaling ( $\\mathrm{^{\\circ}+P a d d}$ . +Para.\u2019) results in higher accuracy compared to the strategy employing only padding alignment ( $\\mathrm{^{\\bullet}+P a d d.^{\\circ})}$ . In the case of $\\mathbf{\\dot{\\nabla}}0+0^{\\circ}$ , the $\\mathrm{\\dot{\\Sigma}+P a d d}$ . $+\\mathrm{Para}$ .\u2019 approach leads to a significant accuracy increase of $37.00\\%$ . These results affirm the effectiveness of the assembling strategies proposed in this paper. ", "page_idx": 8}, {"type": "text", "text": "Table 4: Comparison of assembling strategies. \u2018Base.\u2019 represents the average accuracy for the \u2018Assembled Task\u2019 in the source models. $\\cdot+\\mathrm{Padd}$ .\u2019 and $\\mathrm{\\dot{+}P a d d}$ . $+\\mathrm{Para}$ .\u2019 denote the accuracy of the assembled classifier using only the padding alignment strategy and both the padding alignment and parameter scaling strategies, respectively. ", "page_idx": 8}, {"type": "table", "img_path": "nxL7eazKBI/tmp/99959f6e9da8a06dc313809ded8008053ea54376151afdf7786d396a06cb0e5c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "6 Related Works ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "6.1 Model Explanation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Model explanation methods can generally be categorized into four types: activation-based, gradientbased, perturbation-based techniques, and Layer-wise Relevance Propagation (LRP). Activation-based approaches [27, 28, 29, 30, 31, 32, 33, 34] involve calculating a set of weights and then aggregation feature maps to highlight crucial features. Gradient-based methods [35, 36, 37, 38, 39, 40, 21, 41, 42] utilize gradients to identify key features. Perturbation-based techniques [43, 44, 45, 46] discern important features by altering or masking them and observing the resultant changes in output. LRP involves backward propagation of the final prediction through the network using specific local propagation rules, grounded in the principle of conservation [47]. Montavon et al. [48] provided a comprehensive review of LRP rules, including LRP-0, LRP- $\\cdot\\epsilon$ , $\\mathrm{LRP-}\\gamma$ , LRP- $\\alpha\\beta$ , flat, $\\omega^{2}$ -rules, and $z^{\\mathfrak{B}}$ -rule, and discussed their distinctions and interconnections. Additionally, Ancona et al. [49] examined various gradient-based techniques (such as Gradient $\\times$ Input, Integrated Gradients, and DeepLIFT) and LRP from both theoretical and practical angles, highlighting their similarities and conditions for equivalence or approximation. While these model explanation techniques are primarily employed to locate important features in the original input contributing to the final prediction, our focus is distinct. We aim to identify task-aware components, namely the relevant parameters for specific tasks, for model disassembling. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6.2 Subnetwork Identification ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Subnetwork identification approaches can be divided into ante-hoc and post-hoc techniques. Antehoc techniques, such as those proposed by Li et al. [50] and Liang et al. [15], incorporate novel architectural control modules to select specific filters or employ category-specific gating during training, mainly for network interpretation and adversarial sample detection. Post-hoc techniques are further subdivided. The first family requires additional learnable modules and extra training steps. For instance, Hu et al. [16] introduced Neural Architecture Disentanglement (NAD) for disentangling pre-trained DNNs into task-specific sub-architectures, while Wang et al. [51] and Frankle et al. [52] focused on data routing paths and network acceleration, respectively. Furthermore, Yu et al. [53] and Yang et al. [17] used knowledge distillation to dissect and reassemble models. The second family aligns more closely with feature attribution, with techniques such as those of Khakzar et al. [54] employing concepts similar to perturbation-based methods for pathway selection. In summary, while existing subnetwork identification techniques are commonly applied for network interpretation and adversarial sample detection, our work centers on MDA. We focus on disassembling task-aware components from trained CNN classifiers and reassembling them into a new model, akin to playing with LEGOs, without requiring additional training. ", "page_idx": 9}, {"type": "text", "text": "7 Limitation and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our experiments, as detailed in Table 1 of the main text, demonstrate that models disassembled using the proposed method can surpass the source model in terms of accuracy. However, the performance of the assembled models, as shown in Table 2 of the main text, indicates a decrease in accuracy in certain cases (e.g., $\\mathrm{~\\dot{~}{0}-19+0-69}^{\\circ}$ , $\\phantom{0}{20-69}+70{-179}^{\\circ}$ , ${\\bf\\nabla}^{\\bullet}0{-}99+0{-}199^{\\circ}$ for \u2018CIFAR- $100+$ Tiny-ImageNet\u2019). This decline in performance could be attributed to the interference of irrelevant components, which may adversely affect the correct prediction of samples. Looking ahead, our research will concentrate on addressing the disturbance caused by irrelevant components and enhancing the effectiveness of our model disassembling and assembling technique, particularly for targeted tasks. Additionally, while this paper has focused exclusively on CNN classifiers, future research will explore the disassembling and assembling of models in other domains, including object detection and segmentation. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduce a novel Model Disassembling and Assembling (MDA) task, inspired by the subdivision of the visual system [6], with the objective of disassembling and assembling deep models in a manner akin to playing with LEGOs. The primary focus of this paper centers on the application of MDA to CNN classifiers. During model disassembling, we introduce the concept of relative contribution and propose a component locating technique to extract task-aware components from trained CNN classifiers. For model assembling, we introduce the alignment padding strategy and parameter scaling strategy to construct a new model tailored for a specific task using the disassembled task-aware components. Extensive experiments conducted in this study reveal that the performance of the disassembled and assembled models closely aligns with or even surpasses that of the baseline models. In addition to offering a fresh perspective for model reuse, our research extends to the diverse applications of MDA, including decision route analysis, model compression, knowledge distillation, and more. In future work, we will focus on the MDA applied to other models, such as multi-modal models, large language models etc. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work is funded by National Key Research and Development Project (Grant No: 2022YFB2703100), Ningbo Natural Science Foundation (2022J182) and the Fundamental Research Funds for the Central Universities (226-2024-00145). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Zhi-Xuan Chen, Cheng Jin, Tian-Jing Zhang, Xiao Wu, and Liang-Jian Deng. Spanconv: A new convolution via spanning kernel space for lightweight pansharpening. In Proc. 31st Int. Joint Conf. Artif. Intell, pages 1\u20137, 2022.   \n[2] Bowei Du, Yecheng Huang, Jiaxin Chen, and Di Huang. Adaptive sparse convolutional networks with global context enhancement for faster object detection on drone images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13435\u201313444, 2023.   \n[3] Xuan Shen, Yaohua Wang, Ming Lin, Yilun Huang, Hao Tang, Xiuyu Sun, and Yanzhi Wang. Deepmad: Mathematical architecture design for deep convolutional neural network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6163\u20136173, 2023.   \n[4] David H Hubel and Torsten N Wiesel. Receptive fields, binocular interaction and functional architecture in the cat\u2019s visual cortex. The Journal of physiology, 160(1):106, 1962.   \n[5] D. V. Essen and E. A. Deyoe. Concurrent processing in the primate visual cortex. Cognitive Neuroence, pages 383\u2013400, 1995.   \n[6] M. S. Livingstone and D. H. Hubel. Psychophysical evidence for separate channels for the perception of form, color, movement, and depth. Journal of Neuroscience, 7(11):3416\u20133468, 1987. [7] Treisman and M. Anne. Selective attention in man. Br Med Bull, 20(1):12\u201316, 1963.   \n[8] D. Erhan, Y. Bengio, A. Courville, and P. Vincent. Visualizing higher-layer features of a deep network. University of Montreal, 2009.   \n[9] A. Mahendran and A. Vedaldi. Understanding deep image representations by inverting them. CVPR, 2015.   \n[10] A. Nguyen, J. Yosinski, and J. Clune. Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks. CVPR, 2016.   \n[11] Christoph Feichtenhofer, Axel Pinz, Richard P Wildes, and Andrew Zisserman. Deep insights into convolutional networks for video recognition. International Journal of Computer Vision, 128:420\u2013437, 2020.   \n[12] Anh Nguyen, Alexey Dosovitskiy, Jason Yosinski, Thomas Brox, and Jeff Clune. Synthesizing the preferred inputs for neurons in neural networks via deep generator networks. NeurIPS, 2016.   \n[13] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. Computer Science, 2013.   \n[14] B. Sterling. Inceptionism: going deeper into neural networks. Google Research Blog, 2011.   \n[15] Haoyu Liang, Zhihao Ouyang, Yuyuan Zeng, Hang Su, Zihao He, Shu-Tao Xia, Jun Zhu, and Bo Zhang. Training interpretable convolutional neural networks by differentiating class-specific filters. In ECCV, pages 622\u2013638. Springer, 2020.   \n[16] Jie Hu, Liujuan Cao, Tong Tong, Qixiang Ye, Shengchuan Zhang, Ke Li, Feiyue Huang, Ling Shao, and Rongrong Ji. Architecture disentanglement for deep neural networks. In ICCV, pages 672\u2013681, 2021.   \n[17] Xingyi Yang, Zhou Daquan, Songhua Liu, Jingwen Ye, and Xinchao Wang. Deep model reassembly. arXiv preprint arXiv:2210.17409, 2022.   \n[18] Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, and Xinchao Wang. Depgraph: Towards any structural pruning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16091\u201316101, 2023.   \n[19] Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.   \n[20] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015.   \n[21] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. In ICLR Workshop, 2013.   \n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770\u2013778, 2016.   \n[23] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, pages 1\u20139, 2015.   \n[24] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, pages 248\u2013255. Ieee, 2009.   \n[25] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.   \n[26] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data. AI magazine, 29(3):93\u201393, 2008.   \n[27] Saurabh Desai and Harish G. Ramaswamy. Ablation-cam: Visual explanations for deep convolutional network via gradient-free localization. In WACV, pages 983\u2013991, 2020.   \n[28] Rakshit Naidu and Joy Michael. Ss-cam: Smoothed score-cam for sharper visual feature localization. arXiv preprint arXiv:2006.14255, 2020.   \n[29] Aravindh Mahendran and Andrea Vedaldi. Visualizing deep convolutional neural networks using natural pre-images. International Journal of Computer Vision, 120:233\u2013255, 2016.   \n[30] Haofan Wang, Zifan Wang, Mengnan Du, Fan Yang, Zijian Zhang, Sirui Ding, Piotr Mardziel, and Xia Hu. Score-cam: Score-weighted visual explanations for convolutional neural networks. In CVPR Workshops, pages 111\u2013119, 2020.   \n[31] Shunyu Liu, Jie Song, Yihe Zhou, Na Yu, Kaixuan Chen, Zunlei Feng, and Mingli Song. Interaction pattern disentangling for multi-agent reinforcement learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.   \n[32] Jianming Zhang, Sarah Adel Bargal, Zhe Lin, Jonathan Brandt, Xiaohui Shen, and Stan Sclaroff. Top-down neural attention by excitation backprop. International Journal of Computer Vision, 126(10):1084\u20131102, 2018.   \n[33] Lixiang Ru, Bo Du, Yibing Zhan, and Chen Wu. Weakly-supervised semantic segmentation with visual words learning and hybrid pooling. International Journal of Computer Vision, 130(4):1127\u20131144, 2022.   \n[34] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning deep features for discriminative localization. IEEE Computer Society, 2016.   \n[35] Aditya Chattopadhay, Anirban Sarkar, Prantik Howlader, and Vineeth N Balasubramanian. Grad-cam $^{++}$ : Generalized gradient-based visual explanations for deep convolutional networks. In WACV, pages 839\u2013847, 2018.   \n[36] Z. Feng, J. Hu, S. Wu, X. Yu, J. Song, and M. Song. Model doctor: A simple gradient aggregation strategy for diagnosing and treating cnn classifiers. AAAI, 2022.   \n[37] Daniel Omeiza, Skyler Speakman, Celia Cintas, and Komminist Weldemariam. Smooth grad-cam $^{++}$ : An enhanced inference level visualization technique for deep convolutional neural network models. arXiv preprint arXiv:1908.01224, 2019.   \n[38] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. International Journal of Computer Vision, 128(2):336\u2013359, 2020.   \n[39] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagating activation differences. In ICML, 2017.   \n[40] Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, and Anshul Kundaje. Not just a black box: Learning important features through propagating activation differences. arXiv preprint arXiv:1605.01713, 2016.   \n[41] Jie Song, Yixin Chen, Xinchao Wang, Chengchao Shen, and Mingli Song. Deep model transferability from attribution maps. In NeurIPS, volume 32, pages 6179\u20136189, 2019.   \n[42] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In ICML, 2017.   \n[43] Benjamin J. Lengerich, Sandeep Konam, Eric P. Xing, Stephanie Rosenthal, and Manuela M. Veloso. Visual explanations for convolutional neural networks via input resampling. arXiv preprint arXiv:1707.09641, 2017.   \n[44] Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In ECCV, 2014.   \n[45] Jian Zhou and Olga G Troyanskaya. Predicting effects of noncoding variants with deep learning\u2013based sequence model. Nature Methods, 12(10):931\u2013934, 2015.   \n[46] Luisa M Zintgraf, Taco S Cohen, Tameem Adel, and Max Welling. Visualizing deep neural network decisions: Prediction difference analysis. In ICLR, 2017.   \n[47] S. Bach, A. Binder, G. Montavon, F. Klauschen, KR M\u00fcller, and W. Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PLOS ONE, 10, 2015.   \n[48] Gr\u00e9goire Montavon, Alexander Binder, Sebastian Lapuschkin, Wojciech Samek, and Klaus-Robert M\u00fcller. Layer-wise relevance propagation: an overview. Explainable AI: interpreting, explaining and visualizing deep learning, pages 193\u2013209, 2019.   \n[49] Marco Ancona, Enea Ceolini, Cengiz ztireli, and Markus Gross. Towards better understanding of gradientbased attribution methods for deep neural networks. In ICLR, 2018.   \n[50] Yuchao Li, Rongrong Ji, Shaohui Lin, Baochang Zhang, Chenqian Yan, Yongjian Wu, Feiyue Huang, and Ling Shao. Interpretable neural network decoupling. In ECCV, pages 653\u2013669. Springer, 2020.   \n[51] Yulong Wang, Hang Su, Bo Zhang, and Xiaolin Hu. Interpret neural networks by identifying critical data routing paths. In CVPR, pages 8906\u20138914, 2018.   \n[52] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018.   \n[53] Sihao Yu, Fei Sun, Jiafeng Guo, Ruqing Zhang, and Xueqi Cheng. Legonet: A fast and exact unlearning architecture. arXiv preprint arXiv:2210.16023, 2022.   \n[54] Ashkan Khakzar, Soroosh Baselizadeh, Saurabh Khanduja, Christian Rupprecht, Seong Tae Kim, and Nassir Navab. Neural response interpretation through the lens of critical pathways. In CVPR, pages 13528\u201313538, 2021.   \n[55] Yann Lecun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. ISP, pages 306\u2013351, 2001.   \n[56] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.   \n[57] Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang. Filter pruning via geometric median for deep convolutional neural networks acceleration. In CVPR, pages 4340\u20134349, 2019.   \n[58] Mingbao Lin, Rongrong Ji, Yan Wang, Yichen Zhang, Baochang Zhang, Yonghong Tian, and Ling Shao. Hrank: Filter pruning using high-rank feature map. In CVPR, pages 1529\u20131538, 2020.   \n[59] Chengchao Shen, Xinchao Wang, Jie Song, Li Sun, and Mingli Song. Amalgamating knowledge towards comprehensive classification. In AAAI, volume 33, pages 3068\u20133075, 2019.   \n[60] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Contribution Aggregation and Allocation in the Fully Connected Layer ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Figure 5 illustrates the scenario of contribution aggregation and allocation in the fully connected (FC) layer within Convolutional Neural Networks (CNNs). The FC layer is composed of $\\b{Q}$ filters $\\{\\mathbb{C}_{q}^{(l)}\\}_{q=1}^{Q}$ , each comprising $P$ kernels $\\mathbb{C}_{q}^{(l)}=\\{c_{q,p}^{(l)}\\}_{p=1}^{P}$ . Notably, in the FC layer, both kernels and features are single real numbers, i.e., $a_{p}^{(l)}\\in\\mathbb{R}$ and $c_{q,p}^{(l)}\\in\\mathbb{R}$ , contrasting with the convolutional layer where kernels and features are two-dimensional matrices of real numbers, denoted as $a_{p}^{(l)}\\in\\mathbb{R}^{H\\times W}$ and $c_{q,p}^{(l)}\\in\\mathbb{R}^{H_{k}^{(l)}\\times W_{k}^{(l)}}$ (with $H^{(l)}$ and $W^{(l)}$ representing the height and width of features, and $H_{k}^{(l)}$ and $W_{k}^{(l)}$ representing the height and width of kernels). ", "page_idx": 13}, {"type": "text", "text": "Given this structural discrepancy, the method for computing contributions in fully connected layers requires adaptation. Specifically, the contribution $s_{q,p}^{(l)}$ defined in Eqn.(3) can be directly equated to the hidden feature $a_{q,p}^{(l)}$ as expressed below: ", "page_idx": 13}, {"type": "equation", "text": "$$\ns_{q,p}^{(l)}=a_{q,p}^{(l)}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The remaining equations in the main text remain unchanged. ", "page_idx": 13}, {"type": "image", "img_path": "nxL7eazKBI/tmp/796bc4ed65e37841d5fd8e1945a2c55308781d0c2cba2d1eec9b01ae32c87689.jpg", "img_caption": ["Figure 5: The process of contribution aggregation and allocation at the $l$ -th fully connected layer, wherein the red solid line delineates the contribution aggregation process, and the black dashed line signifies the contribution allocation process. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "B Component Locating Technique for a Specific Task ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Figure 6 illustrates a visualization of the soft relative contribution $r_{p}^{(l)}$ for input feature maps in an intermediate convolutional layer of a model. Additional visualizations of relative contribution are available in $\\S\\mathrm{I}$ . Notably, in Fig. 6(a), an intriguing observation emerges: different input samples belonging to the same category display similar patterns in terms of soft relative contribution. This consistency suggests that channels making significant contributions to the classification of a particular category tend to exhibit consistency across diverse samples within that category. Essentially, specific categories are closely associated with fixed convolution filters and kernels. ", "page_idx": 13}, {"type": "text", "text": "Conversely, as depicted in Fig. 6(b), input samples from distinct categories manifest distinct patterns of soft relative contribution. This variability implies that channels contributing substantially to classification vary across different categories. In essence, each category is linked to a unique set of convolution filters and kernels. ", "page_idx": 13}, {"type": "text", "text": "Therefore, considering the $l.$ -th layer as an example, to identify components associated with a specific task (category), we initially select $1\\%$ of samples accurately classified for this category with the highest predicted probability. Subsequently, we average the features of these selected samples and calculate the relative contribution following the description in the main body of the paper. ", "page_idx": 13}, {"type": "text", "text": "C Relevant Feature Identifying with Backward Consideration ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The computation of the relative contribution $\\hat{r}_{p}^{(l)}$ for the input feature map $a_{p}^{(l)}$ in the $l$ -th layer employs a backward approach. Consequently, it is imperative to account for the relative contribution of the input feature maps in the subsequent $(l+1)$ -th layer when computing the relative contribution in the $l$ -th layer. Specifically, ", "page_idx": 13}, {"type": "image", "img_path": "nxL7eazKBI/tmp/9fc9a8d773929121b5be925d464a32f932487817527395fa2b42446c3247abc9.jpg", "img_caption": ["Figure 6: Visualization of the soft relative contribution $r_{p}^{(l)}$ (as defined in Eqn.(8)) in the 13-th convolutional layer of the VGG-16 model trained on the CIFAR-10 dataset. Subfigure (a) depicts different input samples belonging to the same category, while subfigure (b) showcases input samples from different categories. Bright and dark colors respectively represent large and small values of relative contribution. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "the input feature maps in the $(l+1)$ -th layer correspond to the output feature maps $\\{a_{q}^{(l+1)}\\}_{q=1}^{Q}$ of the $l$ -th layer. Therefore, the relative contribution $\\{\\hat{r}_{q}^{(l+1)}\\}_{q=1}^{Q}$ )}qQ=1 of these output feature maps in the l-th layer is equivalent to the relative contribution of the input feature maps in the $(l+1)$ -th layer. ", "page_idx": 14}, {"type": "text", "text": "In accordance with Eqn.(2), if the relative contribution r\u02c6(ql+1)for the output feature map a(ql+1)is zero, then the relative contribution $\\{r_{q,p}^{(l)}\\}_{p=1}^{P}$ of the hidden feature maps $\\{a_{q,p}^{(l)}\\}_{p=1}^{P}$ is also considered as zero. Consequently, in Eqn.(9), the hard relative contribution $\\hat{r}_{q,p}^{(l)}$ for the hidden feature map $a_{q,p}^{(l)}$ is recalculated as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{r}_{q,p}^{(l)}=\\left\\{\\!\\!\\begin{array}{l l}{{1}}&{{r_{q,p}^{(l)}\\geq\\alpha\\;\\;\\mathrm{and}\\;\\;\\hat{r}_{q}^{(l+1)}=1}}\\\\ {{0}}&{{r_{q,p}^{(l)}<\\alpha\\;\\;\\mathrm{or}\\;\\;\\hat{r}_{q}^{(l+1)}=0}}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The remaining equations in the main text remain unchanged. ", "page_idx": 14}, {"type": "image", "img_path": "nxL7eazKBI/tmp/24c1c866f99f6e036cd5785f5b63adce76f07293ec8d1fdf59452dd9738f66e7.jpg", "img_caption": ["Figure 7: (a) The default alignment padding strategy and (b) the improved alignment padding strategy in the inception module. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "D Alignment Padding Strategy for Inception Module ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Due to the distinctive multi-branch concatenation architecture, specifically the inception module, the default alignment padding strategy encounters challenges when applied to GoogleNet [23]. To address this, we have enhanced the alignment padding strategy specifically tailored for the inception module. ", "page_idx": 15}, {"type": "text", "text": "As illustrated in Fig. 7(a), the conventional alignment padding is employed in each convolutional fliter by default, resulting in blank features calculated by zero kernels being dispersed throughout the channel at various locations after the concatenate operation. In our improved alignment padding strategy, depicted in Fig. 7(b), the alignment padding is applied after the concatenate operation. ", "page_idx": 15}, {"type": "text", "text": "E More Experiments of CNN Model Disassembling ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To further investigate the efficacy of the proposed disassembling method, we present the parameter size and Floating Point Operations Per Second (FLOPs) in Table 5. It is evident that both the parameter size (referred to as \u2018Para.\u2019) and FLOPs increase with the number of disassembled categories. For instance, the parameter size of the model disassembled for categories \u20183-9\u2019 from MNIST using VGG-16 is higher than that of the model disassembled for categories $\\scriptstyle{\\sqrt{0-2}}\\,>$ from the same dataset and network. ", "page_idx": 15}, {"type": "text", "text": "Table 5: Comparison of disassembling performance using additional metrics. In \u2018Para.\u2019, \u2018Score1 / Score2\u2019 represent the parameter sizes (in millions, M) for the \u2018Disassembled Task\u2019 in the source and disassembled models, respectively. In \u2018FLOPs\u2019, \u2018Score1 / Score2\u2019 indicate the FLOPs (Floating Point Operations per Second) for the \u2018Disassembled Task\u2019 in the source model and the disassembled model, respectively. ", "page_idx": 15}, {"type": "table", "img_path": "nxL7eazKBI/tmp/542a8ef3fcc601a68d48aa829ff7616b2a2a705d04b94ef46d9ebb13d270ab41.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Furthermore, our observations reveal that as the source model encompasses more categories, the individual components associated with each category tend to be smaller. For example, both the FLOPs and \u2018Para.\u2019 for the model disassembled component for category $\\surd0\\ '$ from Tiny-ImageNet on VGG-16 are less than those for the same category $\\acute{\\mathbf{\\Delta}}\\mathbf{0}^{\\circ}$ from MNIST on the same network. This suggests that the complexity and resource requirements of disassembled models are influenced not only by the number of categories they comprise but also by the inherent diversity and complexity of the datasets from which they are derived. ", "page_idx": 15}, {"type": "text", "text": "F More Experiments of CNN Model Assembling ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Table 6 provides detailed insights into the comparative performance of models with and without assembling. The results indicate that models with assembling generally exhibit reduced performance compared to those without assembling. For instance, the accuracy of $^{*}0{-}2+0{-}19^{*}$ from CIFAR- $10+\\mathrm{CIFAR-100}$ on VGG16 decreases after assembling. This decline in accuracy may be attributed to the possibility that samples from a specific category could inadvertently attain high confidence in other unrelated components of the assembled models. A notable example involves similar features across different categories, such as a monkey (from model $\\mathcal{M}^{(1)}$ ) and a gorilla ", "page_idx": 15}, {"type": "table", "img_path": "nxL7eazKBI/tmp/4d8bd00758b1909c9eed78fa2e53fbf66ace5a4b31c113ee354206714bb38831.jpg", "table_caption": ["Table 6: Performance comparison between models with and without assembling. \u2018 $\\mathcal{M}^{(1)}$ \u2019 and $\\mathcal{M}^{(2)}$ \u2019 represent the accuracy of the two disassembled models in the \u2018Assembled Task\u2019, respectively. In \u2018Asse.\u2019, \u2018Score1 / Score2\u2019 indicate the average accuracy scores for the \u2018Assembled Task\u2019 in the assembled models, presented without fine-tuning and with ten epochs of fine-tuning, respectively. All results are expressed as percentages. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 7: The disassembling performance on ImageNet with VGG-16. \u2018Base.\u2019 denotes the average accuracy for specific categories from the source model. For \u2018Disa.\u2019, \u2018Score1 (+Score2)\u2019 denotes the average accuracy \u2018Score1\u2019 (the improved average accuracy \u2018Score2\u2019 compared to \u2018Base.\u2019) for specific categories from the disassembled model. ", "page_idx": 16}, {"type": "table", "img_path": "nxL7eazKBI/tmp/3ad35cc51d1aabbd2b28b9185040dc2448c259538142af7a739d7dc72e69208d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "(from model $\\mathcal{M}^{(2)}$ ). In such cases, a sample from one category might receive a higher confidence score from the model trained on the other category. ", "page_idx": 16}, {"type": "text", "text": "However, a key advantage of model assembling lies in the reduction of inference time. As the number of disassembled components increases, running each component independently becomes more time-consuming compared to utilizing an assembled model. This underscores the trade-off between model performance and computational efficiency in the assembling process. ", "page_idx": 16}, {"type": "text", "text": "G More Experiments of CNN Model on Large-scale Datasets ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "It is crucial to investigate the scalability of the proposed framework on large-scale datasets, such as ImageNet [24]. Table 7 presents the disassembling performance on ImageNet with VGG-16, demonstrating that the performance of the disassembled model surpasses that of the source model in all cases. Specifically, when disassembling categories $\\mathrm{^\\circ-99^{\\circ}}$ , the accuracy of the disassembled model is higher by $3.18\\%$ compared to the source model. These experiments underscore the scalability of the proposed method, which extends its applicability to mainstream benchmark datasets. ", "page_idx": 16}, {"type": "text", "text": "H MDA Applied to Other Domains ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The proposed MDA method demonstrates significant flexibility and utility beyond its primary application. Specifically, it enables the customization and reuse of pre-trained CNN classifiers for specific tasks without ", "page_idx": 16}, {"type": "image", "img_path": "nxL7eazKBI/tmp/14a2301500e44fed6aecb1fb02b6233590d7f39cdc6b65e640f50d61ceed2ff6.jpg", "img_caption": ["Figure 8: Decision routes of the categories \u2018dog\u2019 and \u2018automobile\u2019 in the LeNet-5 model on the CIFAR-10 dataset, where the channels of the \u2018dog\u2019 and \u2018automobile\u2019 in layer 1 are the same, while in later layers, such as layer 2, 3, 4, and 5, they are totally different. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 8: Comparison of model compression performance. \u2018Base.\u2019 refers to the source model, while \u2018Ours.\u2019 represents the model compressed using the proposed method. \u2018Acc.\u2019, \u2018FLOPs\u2019, and \u2018Para.\u2019 indicate the accuracy, floating point operations, and parameter size of the model, respectively. All accuracy scores are presented as percentages. ", "page_idx": 17}, {"type": "table", "img_path": "nxL7eazKBI/tmp/d7c761ef2261a15880b10b0537a3d27e456d96a65ffdf88e846b6d0a864c28e7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "necessitating additional training. This adaptability extends to various other tasks, including but not limited to model decision route analysis, model compression, knowledge distillation. ", "page_idx": 17}, {"type": "text", "text": "H.1 Model Decision Route Visualization ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The concept of a \u2018decision route\u2019 refers to the specific data flow pathways utilized for each category within a deep learning model. These pathways are typically sparse and remain fixed. As elucidated in $\\S B$ , the prediction for different categories within a model hinges on the large contributions from distinct feature maps, meaning each category is associated with its own set of relevant model parameters. The proposed framework for contribution allocating and aggregating, combined with component locating techniques, facilitates the construction of unique decision routes for each category. ", "page_idx": 17}, {"type": "text", "text": "Such decision route analysis offers several benefits. Firstly, it enhances the explainability of deep models. By comparing the decision route of a misclassified sample with the corresponding correct category\u2019s route, researchers can identify where the data flow diverged incorrectly. Additionally, these visualizations serve as powerful tools for a deeper understanding and exploration of deep classifiers. ", "page_idx": 17}, {"type": "text", "text": "For example, Fig.8 illustrates decision routes for specific categories using LeNet-5 [55], which includes two convolutional layers and three linear layers. It is observable that each category possesses distinct decision routes, yet there is some overlap among them. This overlap can be attributed to the fact that shallower convolutional filters often process fundamental features like color, texture, and edges\u2014aligning with existing research [8, 9, 10, 12, 13, 14] and the mechanisms of biological visual information processing [5, 6, 7]. In the deeper layers, particularly the final linear layer, we notice that connections between neurons are sparse yet fixed, reflecting the emergence of category-specific features at deeper levels of the CNN. ", "page_idx": 17}, {"type": "text", "text": "In summary, visualizing the decision routes for specific classes within a CNN classifier not only aids in dissecting the classifier\u2019s underlying mechanisms but also proves instrumental in debugging and enhancing the model\u2019s performance. ", "page_idx": 17}, {"type": "text", "text": "H.2 Model Compression ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The proposed MDA presents a novel approach to model compression for CNN classifiers. Utilizing our component locating technique, we effectively disassemble category-specific parameters from the source model. This process entails separating parameters utilized across all categories and discarding those that are redundant, i.e., not used by any category. Consequently, this method offers a unique avenue for model compression. ", "page_idx": 17}, {"type": "table", "img_path": "nxL7eazKBI/tmp/e0d8eb5a739a816ae1cbf31712b220346d7d6687a7d58e2582d3f74a2512866b.jpg", "table_caption": ["Table 9: Comparison of knowledge distillation performance. \u2018Base.\u2019 represents the average accuracy for the \u2018Assembled Task\u2019 obtained from the source models. All accuracy scores are expressed as percentages. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "For empirical validation, we conducted compression experiments using three mainstream classifiers: VGG16 [56], ResNet-50 [22], and GoogleNet [23] on the CIFAR-10 dataset [19], as detailed in Table 8. Additionally, we benchmarked our method against state-of-the-art (SOTA) model compression techniques, specifically two pruning-based methods: FPGM [57] and HRank [58]. ", "page_idx": 18}, {"type": "text", "text": "The results, as shown in Table 8, indicate that our method achieves comparable accuracy to the SOTA methods, FPGM and HRank. However, it is observed that our method exhibits higher Floating Point Operations Per Second (FLOPs) and parameter size compared to these pruning-based methods. This distinction likely stems from our method\u2019s focus on disassembling parameters relevant to each category, as opposed to pruning methods which aim to fliter out parameters irrelevant to all categories. Consequently, while our method retains parameters commonly used across categories, pruning methods like FPGM may discard some of these components without significantly affecting the final prediction, leading to fewer FLOPs and a lower parameter size. ", "page_idx": 18}, {"type": "text", "text": "In future research, we aim to delve deeper into the capabilities and potential of our disassembling and assembling approach in the realm of model compression, exploring ways to enhance its efficiency and effectiveness in reducing model complexity while preserving or even enhancing performance. ", "page_idx": 18}, {"type": "text", "text": "H.3 Knowledge Distillation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The proposed MDA focuses on assembling task-aware components disassembled from different models into a new, unified model. This process bears resemblance to Knowledge Amalgamating (KA) as described in Shen et al. [59], where knowledge from multiple \u2018teacher\u2019 models is distilled into a single \u2018student\u2019 model. While the overarching goals of MDA and KA are similar, the techniques employed in each approach differ significantly. ", "page_idx": 18}, {"type": "text", "text": "To assess the efficacy of our MDA method in the context of knowledge distillation, we conducted comparative experiments against KA using the VGG-16 [21] model on five benchmark datasets: MNIST [55], FashionMNIST [60], CIFAR-10 [19], CIFAR-100 [19], and Tiny-ImageNet [20]. The results of these experiments are summarized in Table 9. ", "page_idx": 18}, {"type": "text", "text": "The results indicate that the proposed MDA method generally achieves higher accuracy than KA, particularly in scenarios with a smaller number of assembled categories, such as the combination of \u2018MNIST $^+$ FASHIONMNIST\u2019. Conversely, as the number of assembled categories increases\u2014for instance, in the case of \u2018CIFAR-100 $^+$ Tiny-ImageNet\u2019, the performance of our method tends to decline, even falling below that of the source model and KA. This decrease in accuracy could be attributed to the increased complexity and potential interference when assembling a larger number of task-aware model components. Specifically, for a test image from an unknown category, the flow through all decision routes in the assembled model can lead to confusion and incorrect predictions, particularly if the components are primarily tailored for categories in a different dataset (e.g., dataset \u2018A\u2019), thus adversely affecting the accuracy of predictions for samples in dataset \u2018B\u2019. ", "page_idx": 18}, {"type": "image", "img_path": "nxL7eazKBI/tmp/cfc5645a1d4beb232de085463dcc0adb5c17eb5f73d994ebbb1de23724c77845.jpg", "img_caption": ["Figure 9: Visualization of the soft relative contribution $r_{p}^{(l)}$ (as defined in Eqn.(8)) for input samples from different categories in layer 13 of VGG-16 on different datasets. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "nxL7eazKBI/tmp/344770732ee52a6c902f20844e5c05612d60ae4b649ac2abe1c78bd3969c7e5f.jpg", "img_caption": ["Figure 10: Visualization of the soft relative contribution $r_{p}^{(l)}$ (as defined in Eqn.(8)) for input samples from different categories in layer 52 of ResNet-50 on different datasets. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "I More Visualization of Relative Contribution $(\\S B)$ ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, Figs. 9-12 show additional visualization results of the relative contribution. As depicted in Figs. 9-11, for a specific layer of the networks on different datasets, different categories exhibit distinct patterns of the soft relative contribution. From Fig. 12, it is evident that in different layers of VGG-16, different categories demonstrate varied patterns of the soft relative contribution. These visualizations offer additional insights into the associations between categories and specific filters across various layers of the neural networks. In certain instances, such as in the case of Layer 52 of ResNet50 on CIFAR-10, distinct categories exhibit comparable substantial relative contributions across diverse channels. The potential explanation for this phenomenon lies in the insufficient formation of category-related features within this layer, attributable to the limited number of categories in CIFAR-10 (only ten), coupled with the residual structure and depth of the network. Nonetheless, other layers manifest disparate substantial relative contributions for distinct categories across diverse channels. This phenomenon stands as a pivotal factor in ensuring the accurate prediction performance of ResNet50 on the CIFAR-10 dataset. ", "page_idx": 19}, {"type": "image", "img_path": "nxL7eazKBI/tmp/601e00da6a74224f1e227a8cc284bd264d8dd15fc417fe23f20c7fd19d6a17a4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 11: Visualization of the soft relative contribution $r_{p}^{(l)}$ (as defined in Eqn.(8)) for input samples from different categories in layer 66 of GoogleNet on different datasets. ", "page_idx": 20}, {"type": "image", "img_path": "nxL7eazKBI/tmp/4b6b6882e6c4f52ba53716b6f477d8becf6b4955cb86c90a2d9f4281f56db420.jpg", "img_caption": ["Figure 12: Visualization of the soft relative contribution $r_{p}^{(l)}$ (as defined in Eqn.(8)) for input samples from different categories in different layers of VGG-16 on CIFAR-10. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Please refer to the main text and the appendix. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Please refer to the main text and the appendix. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: Please refer to the main text and the appendix. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: Please refer to the main text and the appendix. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Please refer to the main text and the appendix. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: Please refer to the main text and the appendix. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Please refer to the main text and the appendix. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Please refer to the main text and the appendix. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] Justification: Please refer to the main text and the appendix. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] Justification: Please refer to the main text and the appendix. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]