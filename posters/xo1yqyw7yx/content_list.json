[{"type": "text", "text": "Enabling Adaptive Agent Training in Open-Ended Simulators by Targeting Diversity ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Robby Costales\u2217 Stefanos Nikolaidis ", "page_idx": 0}, {"type": "text", "text": "Department of Computer Science University of Southern California ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The wider application of end-to-end learning methods to embodied decisionmaking domains remains bottlenecked by their reliance on a superabundance of training data representative of the target domain. Meta-reinforcement learning (meta-RL) approaches abandon the aim of zero-shot generalization\u2014the goal of standard reinforcement learning (RL)\u2014in favor of few-shot adaptation, and thus hold promise for bridging larger generalization gaps. While learning this meta-level adaptive behavior still requires substantial data, efficient environment simulators approaching real-world complexity are growing in prevalence. Even so, hand-designing sufficiently diverse and numerous simulated training tasks for these complex domains is prohibitively labor-intensive. Domain randomization (DR) and procedural generation (PG), offered as solutions to this problem, require simulators to possess carefully-defined parameters which directly translate to meaningful task diversity\u2014a similarly prohibitive assumption. In this work, we present DIVA, an evolutionary approach for generating diverse training tasks in such complex, open-ended simulators. Like unsupervised environment design (UED) methods, DIVA can be applied to arbitrary parameterizations, but can additionally incorporate realistically-available domain knowledge\u2014thus inheriting the flexibility and generality of UED, and the supervised structure embedded in well-designed simulators exploited by DR and PG. Our empirical results showcase DIVA\u2019s unique ability to overcome complex parameterizations and successfully train adaptive agent behavior, far outperforming competitive baselines from prior literature. These findings highlight the potential of such semi-supervised environment design (SSED) approaches, of which DIVA is the first humble constituent, to enable training in realistic simulated domains, and produce more robust and capable adaptive agents. Our code is available at https://github.com/robbycostales/diva. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Despite the broadening application of reinforcement learning (RL) methods to real-world problems [1, 2], generalization to new scenarios\u2014ones not explicitly supported by the training set\u2014remains a fundamental challenge [3]. Meta-reinforcement learning (meta-RL), an extension of the RL framework, is formulated specifically for training adaptive agents, and is thus well-suited for overcoming these generalization gaps [4]. One recent work has demonstrated that meta-RL agents can be trained at scale to achieve adaptation capabilities on par with human subjects [5]. However, learning this human-like adaptive behavior naturally requires a large amount of data representative of the downstream (or target) distribution. For task distributions approaching real-world complexity\u2014precisely the ones of interest\u2014designing each scenario by hand is prohibitively expensive. ", "page_idx": 0}, {"type": "image", "img_path": "Xo1Yqyw7Yx/tmp/0a161be0ecb9f8c207af7ab78abb5c6896b45c2d9ccb49dfc8da7c03c5ca9759.jpg", "img_caption": ["Figure 1: Highly structured environment simulators assume access to parameterizations $E_{\\mathrm{S}}(\\pmb\\theta)$ for which random seeds $\\theta_{i}$ directly produce meaningfully diverse features (e.g. RACING tracks with challenging turns). Open-ended environments with flexible, unstructured parameterizations $E_{\\mathrm{U}}(\\pmb\\theta)$ \u2014though enabling more complex emergent features\u2014lack direct control over high-level features of interest. We introduce DIVA, an approach that effectively creates a more workable parameterization $E_{\\mathrm{QD}}(\\pmb\\theta)$ by evolving levels beyond the minimally diverse population from $E_{\\mathrm{U}}(\\pmb\\theta)$ . By training on these discovered levels, DIVA enables superior performance on downstream tasks. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Prior works have explored the use of domain randomization (DR) and procedural generation (PG) techniques to produce diverse training data for learning agents [6]. Despite eliminating the need for hand-designing each task individually, human labor is still required to carefully design an environment generator that can produce diverse, high-quality tasks. As environments become more complex and open-ended, the ability to hand-design such a robust generator becomes increasingly infeasible. Some methods, like PLR [7], attempt to ameliorate this limitation by learning a curriculum over the generated levels, but these works still operate under the assumption that the generator produces meaningfully diverse levels with a high probability. ", "page_idx": 1}, {"type": "text", "text": "Unsupervised environment design (UED) [8] are a broad class of appproaches which use performancebased metrics to adaptively form a curriculum of training levels. ACCEL [9], a state-of-the-art UED method, uses an evolutionary process to discover more interesting regions of the simulator\u2019s parameter space (i.e. appropriately challenging tasks) than can be found by random sampling. While UED approaches are designed to be generally applicable and require little domain knowledge, they implicitly require a very constrained environment generator\u2014one in which all axes of difficulty correspond to meaningful learning potential for the downstream distribution. Moreover, when faced with complex open-ended environments with arbitrary parameterizations, even ACCEL is not able to efficiently explore the solution space, as it is still bottlenecked by the speed of agent evaluations. ", "page_idx": 1}, {"type": "text", "text": "In this work, we introduce DIVA, an approach for generating diverse training tasks in open-ended simulators to train adaptive agents. By using quality diversity (QD) optimization to efficiently explore the solution space, DIVA bypasses the problem of needing to evaluate agents on all generated levels. QD also enables fine-grained control over the axes of diversity to be captured in the training tasks, allowing the flexible integration of task-related prior knowledge from both domain experts and learning approaches. We demonstrate that DIVA, with limited supervision in the form of feature samples from the target distribution, significantly outperforms state of the art UED approaches\u2014 despite the UED approaches being provided with significantly more interactions. We further show that UED techniques can be integrated into DIVA. Preliminary results with this combination (which we call $\\mathrm{DIVA+}$ ) are promising, and suggest an exciting avenue for future work. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Meta-reinforcement learning. We use the meta-reinforcement learning (meta-RL) framework to train adaptive agents, which involves learning an adaptive policy $\\pi_{\\phi}$ over a distribution of tasks $\\tau$ . ", "page_idx": 1}, {"type": "image", "img_path": "Xo1Yqyw7Yx/tmp/8546fbbf00fe3f9473eeccf1162250847a8553dda8e1cceb3a9d473670847d87.jpg", "img_caption": ["Figure 2: DIVA archive updates on ALCHEMY. The first stage (a) begins with bounds that encapsulate initial solutions, and the target region. As the first stage progresses (b), and QD discovers more of the solution space, the sampling region for the emitters gradually shrinks towards the target region. The second stage begins by redefining the archive bounds to be the target region and including some extra feature dimensions (c). QD fills out just the target region now (d), using sample weights from the target-derived prior (e), the same distribution used to sample levels during meta-training. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Each $\\mathcal{M}_{i}\\,\\in\\,\\mathcal{T}$ is a Markov decision process (MDP) defined by a tuple $\\langle S,A,P,R,\\gamma,T\\rangle$ , where $\\boldsymbol{S}$ is the set of states, $\\boldsymbol{\\mathcal{A}}$ is the set of actions, $P(s_{t+1}|s_{t},a_{t})$ is the transition distribution between states given the current state and action, $R(s_{t},a_{t})$ is the reward function, $\\gamma\\in[0,1]$ is the discount $\\begin{array}{r}{\\mathcal{D}\\,=\\,\\{\\tau^{h}\\}_{h=0}^{H}}\\end{array}$ $T$ t\u2014hew hheorriez $H$ .i sM tehtea -tnruaimnbinerg  ionfv eolpviseos dseasm ipnl ienagc tha strkisa $\\mathcal{M}_{i}\\sim\\mathcal{T}$ $\\tau$ i, ncionlgle tcoti tnhge t $\\mathcal{M}_{i}$ ct\u2014oraineds optimizing policy parameters $\\phi$ to maximize the expected discounted returns across all episodes. ", "page_idx": 2}, {"type": "text", "text": "VariBAD [10] is a context variable-based meta-RL approach which belongs to the wider class of RNNbased methods [11, 12]. While prior methods [13, 14] also use context variables to assist in task adaptation, VariBAD uniquely learns within a belief-augmented MDP (BAMDP) $\\langle S,{\\mathcal{A}},{\\mathcal{Z}},P,R,\\gamma,{\\bar{T^{\\rangle}}}$ where the context variables $z\\in{\\mathcal{Z}}$ encodes the agent\u2019s uncertainty about the task, promoting Bayesian exploration. VariBAD utilizes an RNN-based variational autoencoder (VAE) to model a posterior belief over possible tasks given the full agent trajectory, permitting efficient updates to prior beliefs. ", "page_idx": 2}, {"type": "text", "text": "Quality diversity. For a given problem, quality diversity (QD) optimization framework aims to generate a set of diverse, high-quality solutions. Formally, a problem instance of QD [15] specifies an objective function $J:\\mathbb{R}^{n}\\,\\rightarrow\\,\\mathbb{R}$ and $k$ features $f_{i}:\\mathbb{R}^{n}\\,\\rightarrow\\,\\mathbb{R}$ . Let $S\\,=\\,f(\\mathbb{R}^{n})$ be the feature space formed by the range of $f$ , where $\\pmb{f}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{k}$ is the joint feature vector. For each $s\\in S$ , the QD objective is to find a solution $\\pmb\\theta\\,\\in\\,\\mathbb{R}^{n}$ where $\\pmb{f}(\\pmb{\\theta})\\stackrel{-}{=}\\pmb{s}$ and $J(\\theta)$ is maximized. Since $\\mathbb{R}^{k}$ is continuous, an algorithm solving the QD problem definition above would require unbounded memory to store all solutions. QD algorithms in the MAP-Elites [16] family therefore discretize $S$ via a tessellation method, where $\\mathcal{G}$ is a tessellation of the continuous feature space $S$ into $N_{\\mathcal{G}}$ cells. In employing a MAP-Elites algorithm, we relax the QD objective to find a set of solutions $\\pmb{\\theta}_{i},i\\in\\{1,\\dotsc,\\dot{N}_{\\mathcal{G}}\\}$ , such that each $\\theta_{i}$ occupies one unique cell in $\\mathcal{G}$ . We call the occupants $\\theta_{i}$ of all $M$ cells, each with its own position ${\\pmb f}({\\pmb\\theta}_{i})$ and objective value $J(\\pmb\\theta_{i})$ , the archive of solutions. ", "page_idx": 2}, {"type": "text", "text": "3 Problem setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "One assumption underlying UED methods is that random parameters\u2014or parameter perturbations for ACCEL \u2014produce meaningfully different levels to justify the expense of computing objectives on each newly generated level. However, when the genotype is not well-behaved\u2014when meaningful diversity is rarely generated through random sampling or mutations\u2014these algorithms waste significant time evaluating redundant levels. In our work, we discard the assumption of well-behaved genotypes in favor of making fewer, more realistic assumptions about complex environment generators. There are several assumptions we make about the simulated environments DIVA has access to. ", "page_idx": 2}, {"type": "text", "text": "Genotypes. We assume access to an unstructured environment parameterization function $E_{U}(\\pmb\\theta)$ , where each $\\pmb{\\theta}$ is a genotype (corresponding to the QD solutions $\\theta_{i}$ ) describing parameters to be fed into the environment generator. QD algorithms can support both continuous and discrete genotype spaces, and in this work we evaluate on domains with both kinds. Crucially, we make no assumption of the quality of the training tasks produced by this random generator. We only assume that (1) there is some nonzero (and for practical purposes, nontrivial) probability that this generator will produce a valid level for training\u2014one in which success is possible and positive rewards are in reach; and (2) that it is computationally feasible to discover meaningful feature diversity through an intelligent search over the parameter space\u2014an assumption implicit in all QD applications. ", "page_idx": 2}, {"type": "text", "text": "Features. We assume access to a pre-defined set of features, $S\\,=\\,f(\\mathbb{R}^{n})$ , that capture axes of diversity which accurately characterize the diversity to be expected within the downstream task distribution. It is also possible to learn or select good environment features from a sample of tasks from the downstream distribution, which we discuss in Section 7. For the sake of simplicity, we use a grid archive as our tessellation $\\mathcal{G}$ , where the $k$ dimensions of the discrete archive correspond to the defined features. The number of bins for each feature is a hyperparameter, and can be learned or adapted over the course of training. We generally find it to be helpful to use moderately high resolutions to ease the search, since smaller leaps in feature-level diversity are required to uncover new cells. By default, we use 100 sample feature values across all domains, but demonstrate in ablation studies that that significantly fewer may be used (see Appendix C). ", "page_idx": 3}, {"type": "text", "text": "4 DIVA ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "DIVA assumes access to a small set of feature samples representative of the target domain. It does not, however, require access to the underlying levels themselves. This is a key distinction, as the former is a significantly weaker assumption. Consider the problem of training in-home assistive robots in simulation with the objective of adapting to real-world houses. It is more likely we have access to publicly available data describing typical houses\u2014dimensions, stylistic features, etc.\u2014than we have access to corresponding simulator parameters which produce those exact feature values. ", "page_idx": 3}, {"type": "text", "text": "Feature density estimation. DIVA begins by constructing a QD archive with appropriate bounds and resolution. Given a set of specified features $\\{f_{i}\\}^{k}$ and a handful of downstream feature samples, we first infer each feature\u2019s underlying distribution. These can be approximated with kernel density estimation (KDE), or we can work with certain families of parameterized distributions. For our experiments, we assume each feature is either (independently) normally or uniformly distributed. We use a statistical test2 to evaluate the fti of each distribution family, and select the best-ftiting. Setting the resolution for discrete feature dimensions is straightforward, and depends only on the range. For continuous features, the resolution should enable enough signal for discovering new cells, while avoiding practical issues that arise with too many cells3. See Section 5 for domain-specific details. ", "page_idx": 3}, {"type": "text", "text": "Two-stage QD updates. Once the feature-specific target distributions are determined, we can use these to set bounds for each archive dimension. A na\u00efve approach would be to set the archive ranges for each feature based on the confidence bounds of the target distribution. However, random samples from $E_{\\mathrm{U}}$ may not produce feature values that fall within the target range. We found this to be a major issue in the ALCHEMY domain (see Figure 2), and for some features in RACING. We solve this problem by setting the initial archive bounds to include both randomly generated samples from $E_{\\mathrm{U}}$ , as well as the full target region. As the updates progress, we gradually update the sample mask\u2014which is used to inform the sampling of new solutions\u2014towards the target region. We observe empirically that updating and applying this mask provides an enormous speed-up in guiding solutions towards the target region (see Figure 15). After this first stage, solutions are inserted into a new archive defined by the proper target bounds. See Appendix A for more specifics on these two QD update stages. ", "page_idx": 3}, {"type": "text", "text": "Overview. DIVA consists of three stages. Stage 1 (S1) begins by initializing the archive with bounds that include both the downstream feature samples (the target region), as well as the initial population generated from $E_{U}(\\theta)$ . S1 then proceeds with alternating $\\it Q D$ updates, to discover new solutions, and sample mask updates, to guide the population towards the target region. In Stage 2 (S2), the archive is reinitialized with existing solutions, but is now bounded by the target region. QD updates continue to further diversify the population, now targeting the downstream feature values specifically. The last stage is standard meta-training, where training task parameters are now drawn from $P_{\\mathcal{G}}(\\pmb{\\theta})$ , a distribution over the feature space approximated using the downstream feature samples, discretized over the archive cells. See Appendix A for detailed pseudocode. ", "page_idx": 3}, {"type": "table", "img_path": "Xo1Yqyw7Yx/tmp/49f14df7927259580412911352e291642dcf20db1f5d0450d0b77305db90d399.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "5 Empirical results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Baselines. We implement the following baselines to evaluate their relative performance to DIVA. ODS is the \u201coracle\" agent trained over the downstream environment distribution $E_{\\mathrm{S}}(\\pmb\\theta)$ , used for evaluation. With this baseline, we are benchmarking the upper bound in performance from the perspective of a learning algorithm that has access to the underlying data distribution.4 DR is the meta-learner trained over a task distribution defined by performing domain randomization over the space of valid genotypes, $\\pmb{\\theta}$ , under the training parameterization, $E_{\\mathrm{U}}(\\pmb\\theta)$ . Robust PLR $(\\underline{{\\mathbf{P}}}\\underline{{\\mathbf{L}}}\\underline{{\\mathbf{R}}}^{\\perp})$ [17] is the improved and theoretically grounded version of PLR [7], where agents\u2019 performance-based PLR objectives are evaluated on each level before using them for training. ACCEL [9] is the same as $\\mathrm{PLR^{\\perp}}$ but instead of randomly sampling over the genotype space to generate levels for evaluation, levels are mutated from existing solutions. All baselines use VariBAD [10] as their base meta-learner. ", "page_idx": 4}, {"type": "text", "text": "Experimental setup. The oracle agent (ODS) is first trained over the each environment\u2019s downstream distribution to tune VariBAD\u2019s hyperparameters. These environment-specific VariBAD settings are then fixed while hyperparameters for DIVA and the other baselines are tuned. For fairness of comparison\u2014since DIVA is allowed $N_{\\mathrm{QD}}$ QD update steps to flil its archive before meta-training\u2014 we allow each UED approach $\\scriptstyle(\\mathrm{{PLR}^{\\perp}}$ and ACCEL) to use significantly more environment steps for agent evaluations (details discussed below per environment). All empirical results were run with 5 seeds unless otherwise specified, and error bars indicate a $95\\%$ confidence region for the metric in question. The QD archive parameters were set per environment, and for ALCHEMY and RACING, relied on some hand-tuning to find the right combinations of features and objectives. We leave it to future work to perform a deeper analysis on what constitutes good archive design, and how to better automate this process. ", "page_idx": 4}, {"type": "text", "text": "5.1 GRIDNAV ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our first evaluation domain is a modified version of GRIDNAV (Figure 3), originally introduced to motivate and benchmark VariBAD [10]. The agent spawns at the center of the grid at the start of each episode, and receives a slight negative reward ${'}r=-0.1)$ each step until it discovers (inhabits) the goal cell, at which point it also receives a larger positive reward $\\left.r=1.0\\right]$ ). ", "page_idx": 4}, {"type": "text", "text": "Parameterization. We parameterize the task space (i.e. the goal location) to reduce the likelihood of generating meaningfully diverse goals. Specifically, each $E_{\\mathrm{U}_{k}}$ (or $E_{k}$ ) introduces $k$ genes to the solution genotype which together define the final $y$ location. Each gene $j$ can assume the values $\\theta_{j}\\in\\{-1,0,1\\}$ , and the final $y$ location is determined by summing these values, and performing a floor division to map the bounds back to the original range of the grid. As $k$ increases, $y$ values are increasingly biased towards 0, as shown on the right side of Figure 3. For more details on the GRIDNAV domain, see Appendix B.1. ", "page_idx": 4}, {"type": "image", "img_path": "Xo1Yqyw7Yx/tmp/8e9e3ec9e11b9cd800ce35f56ce2aff70cc35c1667ae596df37f08123185808b.jpg", "img_caption": ["Figure 3: Left: A GRIDNAV agent attempting to locate the goal across two episodic rollouts. Right: The marginal probability of sampled goals inhabiting each $y$ for different complexities $k$ of $E_{k}(\\pmb\\theta)$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "QD updates. We define the archive features to be the $x$ and $y$ coordinates of the goal location. The objective is set to the current iteration, so that newer solutions are prioritized (additional details in Appendix B.1). DIVA is provided $N_{52}=8.0\\times10^{4}$ $'N_{\\mathrm{S1}}=\\,0]$ ) QD update iterations for filling the archive. To compensate, $\\mathrm{PLR^{\\perp}}$ and ACCEL are each provided with an additional $9.6\\times10^{\\overline{{6}}}$ environment steps for evaluating PLR scores, which amounts to three times as many total interactions\u2014since all methods are provided $N_{E}=4.8\\times10^{6}$ interactions for training. If each \u201creset\" call counts as one environment step5, the UED baselines are effectively granted $2.4\\times$ more additional step data than what DIVA additionally receives through its QD updates (details in Appendix E.1). ", "page_idx": 4}, {"type": "text", "text": "Results. From Figure 4a, we see that increasing genotype complexity (i.e. larger $k$ ) reduces goal diversity for DR\u2014which is expected given the parameterization defined for $E_{\\mathrm{U}}$ . We can also see that DIVA, as a result of its QD updates, can effectively capture goal diversity, even as complexity increases. When we fix the complexity $\\left[k=24\\right]$ ) and train over the $E_{\\mathrm{U}}$ distribution, we see that the UED approaches are unable to incidentally discover and capture diversity over the course of training (Figure 4b). DIVA\u2019s explicit focus on capturing meaningful level diversity enables it to significantly outperform these baselines in terms of episodic return (Figure $4\\mathrm{c}$ ) and success rate (Figure 4d). ", "page_idx": 4}, {"type": "image", "img_path": "Xo1Yqyw7Yx/tmp/70919661e86f3cbb4d8298fcb196b2c2f732a1a296f5d882ef1bf1015ca93960.jpg", "img_caption": ["Figure 4: GRIDNAV analysis and results. (a) Target region coverage produced by DIVA and DR over different genotype complexities $k$ . DR represents the average coverage of batches corresponding to the size of the QD archive. $\\mathbf{D}\\mathbf{R}^{*}$ represents the total number of unique levels discovered over the course of parameter randomization steps which equal in number to the additional environments $\\mathtt{P L R^{\\perp}}$ is provided for evaluation. $\\mathbf{D}\\mathbf{R}^{*}$ is thus an upper bound on the diversity that $\\mathrm{PLR^{\\perp}}$ can capture. $500\\mathrm{k}$ iterations (QD or otherwise) are used across all results. (b) The diversity produced by $\\dot{\\mathrm{PLR^{\\perp}}}$ and ACCEL over the course of training (later updates omitted due to no change in trend). (c) Final episode return curves for DIVA and baselines. (d) Final method success rates across each episode. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "5.2 ALCHEMY ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "ALCHEMY [18] is an artificial chemistry environment with a combinatorially complex task distribution. Each task is defined by some latent chemistry, which influences the underlying dynamics, as well as agent observations. To successfully maximize returns over the course of a trial, the agent must infer and exploit this latent chemistry. At the start of each episode, the agent is provided a new set of (1-12) potions and (1-3) stones, where each stone has a latent state defined by a specific vertex of a cube, i.e. $(\\{0,1\\},\\{0,1\\},\\{0,1\\})$ , and each potion has a latent effect, or specific manner in which it transforms stone latent states (see Figure 5a). The agent observes only salient artifacts of this latent information, and must use interactions to identify the ground-truth mechanics. At each step, the agent can apply any remaining potion to any remaining stone. Each stone\u2019s value is maximized the closer its latent state is to $(1,1,1)$ , and rewards are produced when stones are cast into the cauldron. ", "page_idx": 5}, {"type": "text", "text": "To make training feasible on academic resources, we perform evaluations on the symbolic version of ALCHEMY, as opposed to the full Unity-based version. Symbolic ALCHEMY contains the same mechanistic complexity, minus the visuomotor challenges which are irrelevant to this project\u2019s aims. ", "page_idx": 5}, {"type": "text", "text": "Parameterization. $E_{\\mathrm{S}}(\\pmb\\theta)$ is the downstream distribution containing maximal stone diversity. For training, implement $E_{\\mathrm{U}_{k}}$ where $k$ controls the level of difficulty in generating diverse stones. Specifically, we introduce a larger set of coordinating genes $\\theta_{j}\\in\\{0,1\\}$ that together specify the initial stone latent states, similar to the mechanism we used in GRIDNAV to limit goal diversity. Each stone latent coordinate is specified with $k$ genes, and only when all $k$ genes are set to 1 is the latent coordinate is set to 1. When any of the genes are 0, the latent coordinate is 0. For our experiments we set $k=8$ , and henceforth use $E_{\\mathrm{U}}$ to signify $E_{\\mathrm{U_{8}}}$ . ", "page_idx": 5}, {"type": "text", "text": "QD updates. We use features LATENTSTATEDIVERSITY and MANHATTANTOOPTIMAL \u2014both of which target stone latent state diversity from different angles. See Appendix B.2 for more specifics on these features and other details surrounding ALCHEMY\u2019s archive construction. Like GRIDNAV, the objective is set to bias new solutions. DIVA is provided with $N_{51}\\,=\\,8.0\\,\\times\\,10^{4}$ and $N_{\\mathrm{S}2}$ $=3.0\\ \\overset{.}{\\times}10^{4}$ QD update iterations. $\\mathrm{PLR^{\\perp}}$ and ACCEL are compensated such that they receive $3.5\\times$ more additional step data than what DIVA receives via QD updates (see Appendix E.1 for details). ", "page_idx": 5}, {"type": "text", "text": "Results. Our empirical results demonstrate that DIVA is able to generate latent stone states with diversity representative of the target distribution. We see this both quantitatively in Figure 5b, and qualitatively in Figure 6. In Figure 5c, we see this diversity translates to significantly better results ", "page_idx": 5}, {"type": "image", "img_path": "Xo1Yqyw7Yx/tmp/c5903a1a7ea563ccd1fb45cf871b1fe4ce88fea588e1cb240ab99db889f4f8f7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 5: ALCHEMY environment and results. (a) A visual representation of ALCHEMY\u2019s structured stone latent space. $P_{1}$ and $P_{2}$ represent potions acting on stones. Only $P_{1}$ results in a latent state change, because $P_{2}$ would push the stone outside of the valid latent lattice. (b) Marginal feature distributions for $E_{\\mathrm{S}}$ (the structured target distribution), DIVA, and $E_{\\mathrm{U}}$ (the unstructured distribution used directly for DR, and to initialize DIVA\u2019s archive). (c) Final episode return curves for DIVA and baselines. (d) Number of unique genotypes used by each method over the course of meta-training. ", "page_idx": 6}, {"type": "image", "img_path": "Xo1Yqyw7Yx/tmp/a40834e31e493e76113b8e1986a85c060b2cb2856c34eba21bb53c3906cc9b2e.jpg", "img_caption": ["Figure 6: ALCHEMY level diversity. Early on in DIVA\u2019s QD updates (left), the levels in the archive do not posses much latent stone diversity\u2014all are close to $(1,1,1)$ . As samples begin populating the target region in later QD updates (right), we see stone diversity is significantly increased. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "on $E_{\\mathrm{S}}$ over baselines. Despite generating roughly as many unique genotypes as DIVA (Figure 5d), $\\mathrm{PLR^{\\perp}}$ and ACCEL are unable to generate training stone sets of significant phenotypical diversity to enable success on the downstream distribution. ", "page_idx": 6}, {"type": "text", "text": "5.3 RACING ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Lastly, we evaluate DIVA on the RACING domain introduced by [17]. In this environment, the agent controls a race car via simulated steering and gas pedal mechanisms, and is rewarded for efficiently completing the track, $\\mathcal{M}_{i}\\in\\mathcal{T}$ . We adapt this RL environment to the meta-RL setting by lowering the resolution of the observation space significantly. By increasing the challenge of perception, even competent agents benefti from multiple episodes to better understand the underlying track. For all of our experiments, we use $H=2$ episodes per trial, and a flattened $15\\times15$ pixel observation space. ", "page_idx": 6}, {"type": "text", "text": "Setup. We use three different parameterizations in our experiments: (1) $E_{\\mathrm{S}}(\\pmb\\theta)$ is the downstream distribution we use for evaluating all methods, training ODS, and setting archive bounds for DIVA. Parameters $\\pmb{\\theta}$ are used to seed the random generation of control points which in turn parameterize a sequence of B\u00e9zier curves designed to smoothly transition between the control locations. Track diversity is further enforced by rejecting levels with control points that possess a standard deviation below a certain threshold. (2) $E_{\\mathrm{U}_{k}}(\\pmb\\theta)$ is a reparameterization of $E_{\\mathrm{S}}(\\pmb\\theta)$ that makes track diversity harder to generate, with the difficulty proportional to the value of $k\\in\\mathbb{N}$ . For our experiments, we use $k=32$ (which we will denote simply as $E_{\\mathrm{U}}(\\pmb\\theta))$ , which roughly means that meaningful diversity is $32\\times$ less likely to randomly occur than when $k=1$ (which is equivalent to $E_{\\mathrm{S}}(\\pmb\\theta))$ . This is achieved by defining a small region in the center, 32 (or $k$ , in general) times smaller than the track boundaries, where all points outside the region are projected onto the unit square, and scaled to the track size. (3) $E_{\\mathrm{F1}}(\\pmb\\theta)$ uses $\\pmb{\\theta}$ as an RNG seed to select between a set of 20 hand-crafted levels official Formula-1 tracks [17], and is used to benchmark DIVA\u2019s zero-shot generalization to a new target distribution. ", "page_idx": 6}, {"type": "text", "text": "QD updates. We define features TOTALANGLECHANGES (TAC) and CENTEROFMASSX (CX) for the archive dimensions. Levels from $E_{\\mathrm{U}}$ lack curvature (see Figure 8) so TAC, which is defined as the sum of angle changes between track segments, is useful for directly targeting this desired curvature. ", "page_idx": 6}, {"type": "image", "img_path": "Xo1Yqyw7Yx/tmp/ab767eab8c5fc03be6da32e950e4677ed27a234e220af1151e825f571bab5329.jpg", "img_caption": ["Figure 7: RACING features and main results. Left: Marginal feature distributions for $E_{\\mathrm{S}}$ (target distribution), $E_{\\mathrm{F1}}$ (human-designed F1 tracks), DIVA, and $E_{\\mathrm{U}}$ (the unstructured distribution used for DR, the original levels that DIVA evolves)\u2014cropped for readability. Center: Final episode return curves for DIVA and baselines on $E_{\\mathrm{S}}$ . Right: Track completion rates by method, evaluated on $E_{\\mathrm{S}}$ . "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "Xo1Yqyw7Yx/tmp/ff90183c0ffec2f06051423c60a0662a5575b2f50273dec1c78521c1c8b47403.jpg", "img_caption": ["Figure 8: RACING level diversity. We see that random $E_{\\mathrm{U}}$ levels, used by DR, and which form the initial population of DIVA, are unable to produce qualitatively diverse tracks (left). After the two-stage QD-updates, DIVA is able to produce tracks of high qualitative diversity (right). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "CX, or the average location of the segments, targets diversity in the location of these high-density (high-curvature) regions. We compute an alignment objective over features CENTEROFMASSY and VARIANCEY to further target downstream diversity. See Appendix B.3 for more details relevant to the archive construction process for RACING. DIVA is provided with $2.5\\times10^{5}$ initial QD updates on RACING. $\\mathrm{PLR^{\\perp}}$ and ACCEL are compensated with $4.0\\times$ more additional step data than what DIVA receives through QD updates (see Appendix E.1 for more details). ", "page_idx": 7}, {"type": "text", "text": "Main results. Results are shown in Figure 7. DIVA outperforms all baselines, including the UED approaches, which have access to three times as many environment interactions. From Figure 8, we see that final DIVA levels contain significantly more diversity than randomization over $E_{\\mathrm{U}}$ . ", "page_idx": 7}, {"type": "text", "text": "Transfer to F1 tracks. Next, we evaluate the ability of these trained policies to zero-shot transfer to humandesigned F1 levels [17], $E_{\\mathrm{F1}}$ . Though qualitative differences are apparent (see Figure 9), from Figure 7a we can additionally see how these levels differ quantitatively. Even though DIVA uses feature samples from $E_{\\mathrm{S}}$ to define its archive, we see from the results in Figure 9 that DIVA is not only able to complete many of these tracks, but is also able to significantly outperform ODS. This result may seem unlikely, given that DIVA bases its axes of diversity on $E_{\\mathrm{S}}$ . One possible explanation is that while DIVA successfully matches its TOTALANGLECHANGES distribution to $E_{\\mathrm{S}}$ (see Figure 7), because it is less likely for all 12 control points to be mutated to the diversity-enabling region than just a few control points with sharp angles, DIVA \u201copts\" for the latter, and thus produces fewer, sharper angles, which is evidently useful for transferring to (these) human-designed tracks. This hypothesis matches what we see qualitatively from the DIVA-produced levels in Figure 8. ", "page_idx": 7}, {"type": "image", "img_path": "Xo1Yqyw7Yx/tmp/e4d6ff8d300dc47c7a0358ca99deb22b00d0c4d86867111e534d51323c1b94ff.jpg", "img_caption": ["Figure 9: Sample F1 levels (top), and track completion rates by methods targeting $E_{\\mathrm{S}}$ , evaluated on $E_{\\mathrm{F1}}$ (bottom). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Combining DIVA and UED. While $\\mathrm{PLR^{\\perp}}$ and ACCEL struggle on our evaluation domains, they still have utility of their own, which we hypothesize may be compatible with DIVA\u2019s. As a preliminary experiment to evaluate the potential of such a combination, we introduce $\\underline{{\\mathbf{D}\\mathbf{I}\\mathbf{V}\\mathbf{A}\\mathbf{+}}}$ , which still uses DIVA to generate diverse training samples via QD, but additionally uses $\\mathrm{PLR^{\\perp}}$ to define a new distribution over these levels based on approximate learning potential. Instead of randomly sampling levels from $E_{\\mathrm{U}}$ , the $\\mathrm{PLR^{\\perp}}$ evaluation mechanism samples levels from the DIVA-induced distribution over the archive. We perform experiments on two different archives generated by DIVA: (1) an archive that is slightly misspecified (see Appendix B.3 for details), and (2) the archive used in our main results. From Figure 10, we see that while performance does not significantly improve for (2), the combination of DIVA and $\\mathrm{PLR^{\\perp}}$ is able to significantly improve performance on (1), and even statistically match the original DIVA results. These results highlight the potential of such hybrid $(\\mathrm{QD+UED})$ semi-supervised environment design (SSED) approaches, a promising area for future work. ", "page_idx": 8}, {"type": "image", "img_path": "Xo1Yqyw7Yx/tmp/ad4acfbac3f2dd8aff95027abe45a4aab8d75e2da037afc04919bf4d528208e8.jpg", "img_caption": ["Figure 10: $\\mathbf{DIVA+}$ results compared to DIVA, for (1) misspecified, and (2) wellspecified archives, evaluated on $E_{\\mathrm{S}}$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "6 Related work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Meta-reinforcement learning. Meta-reinforcement learning methods range from gradient-based approaches (e.g. MAML) [19], RNN context-based approaches [12, 11] (e.g. RL2), and the slew of emerging works utilizing transformers [20, 5, 21]. We use VariBAD [10], a state-of-the-art context variable-based approach that extends $\\mathrm{RL^{2}}$ by using variational inference to incorporate task uncertainty into its beliefs. HyperX [22], an extension that uses reward-bonuses, was not found to improve performance on our domains. In each of these works, the training distribution is given; none address the problem of generating diverse training scenarios in absence of such a distribution. ", "page_idx": 8}, {"type": "text", "text": "Procedural environment generation. Procedural (content) generation (PCG / PG) [6] is a vast field. Many RL and meta-RL domains themselves have PG baked-in (e.g. ProcGen [23], Meta-World, [24], Alchemy [18], and XLand [5]). Each of these works rely on human engineering to produce levels with meaningfully diverse features. A related stream of works apply scenario generation to robotics\u2014some works essentially perform PCG [25, 26], while others integrate more involved search mechanics [27, 28, 29, 30]. One prior work [31] defines a formal but generic parameterization for applying PG to generate meta-RL tasks. It is yet to be shown, however, if such an approach can scale to domains with vastly different dynamics, and greater complexity. ", "page_idx": 8}, {"type": "text", "text": "Unsupervised environment design. UED approaches\u2014which use behavioral metrics to automatically define and adapt a curriculum of suitable tasks for agent training\u2014form the frontier of research on open-endedness. The recent stream of open-ended agent/environment co-evolution works (e.g. [32, 33, 34]) was kickstarted by the POET [35, 36] algorithm. The \u201cUED\u201d term itself originated in PAIRED [8], which uses the performance of an \u201cantagonist\u201d agent to define the curriculum for the main (protagonist) agent. PLR [7] introduces an approach for weighting training levels based on learning potential, using various proxy metrics to capture this high-level concept. [17] introduces $\\mathrm{PLR^{\\perp}}$ , which only trains on levels that have been previously evaluated, and thus enabling certain theoretical robustness guarantees. AdA [5] uses PLR as a cornerstone of their approach for generating diverse training levels for adaptive agents in a complex, open-ended task space. ACCEL [9] borrows $\\mathrm{PLR^{\\perp}}$ \u2019s scoring procedure, but the best-performing solutions are instead mutated, so the buffer not only collects and prioritizes levels of higher learning potential, but evolves them. We use ACCEL as our main baseline because it has demonstrated state-of-the art results on relevant domains, and like DIVA, evolves a population of levels. The main algorithmic differences between ACCEL and DIVA are that ACCEL (1) performs additional evaluation rollouts to produce scores during training and (2) uses a 1-d buffer instead of DIVA\u2019s multi-dimensional archive. $\\mathrm{PLR^{\\perp}}$ serves as a secondary baseline in this work; its non-evolutionary nature makes it a useful comparison to DR. ", "page_idx": 8}, {"type": "text", "text": "Scenario generation via QD. A number of recent works apply QD to simulated environments in order to generate diverse scenarios, with distinct aims. Some works, like DSAGE [37], uses QD to develop diverse levels for the purpose of probing a pretrained agent for interesting behaviors. In another line of work applies QD to human-robot interaction (HRI), and ranges from generating diverse scenarios [38], to finding failure modes in shared autonomy systems [39] and human-aware planners [40]. DIVA\u2019s application of QD inspired by these approaches, as they produce meaningfully diverse environment scenarios, but no prior work exists which applies QD to define a task distribution for agent training, much less adaptive agent training, or overcoming difficult parameterizations in open-ended environments. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "7 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The present work enables adaptive agent training on open-ended environment simulators by integrating the unconstrained nature of unsupervised environment design (UED) approaches, with the implicit supervision baked into procedural generation (PG) and domain randomization (DR) methods. Unlike PG and DR, which requires domain knowledge to be carefully incorporated into the environment generation process, DIVA is able to flexibly incorporate domain knowledge, and can discover new levels representative of the downstream distribution. And instead of relying on behavioral metrics to infer a general, ungrounded form of \u201clearning potential\u201d, like UED\u2014which becomes increasingly unconstrained and therefore less useful a signal as environments become more complex and open-ended\u2014DIVA is able to directly incorporate downstream feature samples to target specific, meaningful axes of diversity. With only a handful of downstream feature samples to set the parameters of the QD archive, our experiments (Section 5) demonstrate DIVA\u2019s ability to outperform competitive baselines compensated with three times as many environment steps during training. ", "page_idx": 9}, {"type": "text", "text": "In its current form, the most obvious limitation of DIVA is that, in addition to assuming access to downstream feature samples, the axes of diversity themselves must be specified. However, we imagine these axes of diversity could be learned automatically from a set of sample levels, or selected from a larger set of candidate features; it may be possible to adapt existing QD works to automate this process in related settings [41]. The present work also lacks a more thorough analysis of what constitutes good archive design. While some amount of heuristic decision-making is unavoidable when applying learning algorithms to specific domains, a promising future direction would be to study how to approach DIVA\u2019s archive design from a more algorithmic perspective. ", "page_idx": 9}, {"type": "text", "text": "DIVA currently performs QD iterations over the environment parameter space defined by $E_{U}(\\pmb\\theta)$ , where each component of the genotype $\\pmb{\\theta}$ represents some salient input parameter to the simulator. Prior works in other domains (e.g. [42]) have demonstrated QD\u2019s ability to explore the latent space of generative models. One natural direction for future work would therefore be to apply DIVA to neural environment generators (rather than algorithmic generators), where $\\pmb{\\theta}$ would instead correspond to the latent input space of the generative model. If the latent space of these models is more convenient to work with than the raw environment parameters\u2014e.g. due to greater smoothness with respect to meaningful axes of diversity\u2014this may help QD more efficeintly discover samples within the target region. Conversely, DIVA\u2019s ability to discover useful regions of the parameter space means these neural environment generators do not need to be \u201cwell-behaved\", or match a specific target distribution. Since these generative models are also likely to be differentiable, DIVA can additionally incorporate gradient-based QD works (e.g. DQD [15]) to accelerate its search. ", "page_idx": 9}, {"type": "text", "text": "Preliminary results with $\\mathrm{DIVA+}$ demonstrate the additional potential of combining UED and DIVA approaches. The F1 transfer results (i.e. DIVA outperforming ODS trained directly on $E_{\\mathrm{S}}$ ) further suggest that agents benefti from flexible incorporation of downstream knowledge. In future work, we hope to study more principle integrations of UED and DIVA-like approaches, and to more generally explore this exciting new area of semi-supervised environment design (SSED). ", "page_idx": 9}, {"type": "text", "text": "More broadly, now equipped with DIVA, researchers can develop more general-purpose, open-ended simulators, without concerning themselves with constructing convenient, well-behaved parameterizations. Evaluations in this work required constructing our own contrived paramterizations, since domains are rarely released without carefully designed parameterizations. It is no longer necessary to accomodate the assumption made my DR, PG, and UED approaches\u2014that either randomization over the parameter space should produce meaningful diversity, or that all forms of level difficulty ought to correspond to meaningful learning potential. So long as diverse tasks are possible to generate, even if sparsely distributed within the paramter space, QD may be used to discover these regions, and exploit them for agent training. Based on the promising empirical results presented in this work, we are hopeful that DIVA will enable future works to tackle even more complicated domains, and assist researchers in designing more capable and behaviorally interesting adaptive agents. ", "page_idx": 9}, {"type": "text", "text": "8 Reproducibility statement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The source code, along with thorough documentation for reproducing each result in this paper, is publicly available on Github6. Even without this code, researchers should be able to fully reproduce the algorithm from the details in the main body, the pseudocode provided in Appendix A, and training details (hyperparameters and hardware information) provided in Appendix E. ", "page_idx": 10}, {"type": "text", "text": "9 Ethics statement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Like all fundamental technologies, this work has the potential to be misapplied for malicious purposes. The authors do not believe, however, that the methods introduced in this work present a significant or unique risk for misuse or abuse. The authors intend for DIVA to be applied to use-cases that have the best interests of humanity (including concern for the earth and other sentient creatures) at heart. ", "page_idx": 10}, {"type": "text", "text": "10 Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was partially supported by NSF CAREER (#2145077) and the DARPA EMHAT project. We thank Tjanaka et al., the developers of pyribs [43], whose library served as the basis for our QD implementations. We thank Zintgraf et al., the authors of VariBAD [10], whose codebase served as the basis for our meta-RL agent. We thank Jiang et al. and Parker-Holder et al., the authors of PLR [7] and ACCEL [9], respectively, for their implementations which served as the basis for our UED baselines. We specifically thank Minqi Jiang for answering questions related to the PLR codebase in the early stages of development, and Varun Bhatt for helpful discussion at various stages of this work. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] X. Wang, S. Wang, X. Liang, D. Zhao, J. Huang, X. Xu, B. Dai, and Q. Miao, \u201cDeep reinforcement learning: A survey,\u201d IEEE Trans. Neural Netw. Learn. Syst., vol. 35, pp. 5064\u20135078, Apr. 2024.   \n[2] K. Sivamayil, E. Rajasekar, B. Aljafari, S. Nikolovski, S. Vairavasundaram, and I. Vairavasundaram, \u201cA systematic study on reinforcement learning based applications,\u201d Energies, vol. 16, p. 1512, Feb. 2023.   \n[3] R. Kirk, A. Zhang, E. Grefenstette, and T. Rockt\u00e4schel, \u201cA survey of zero-shot generalisation in deep reinforcement learning,\u201d jair, vol. 76, pp. 201\u2013264, Jan. 2023.   \n[4] J. Beck, R. Vuorio, E. Z. Liu, Z. Xiong, L. Zintgraf, C. Finn, and S. Whiteson, \u201cA survey of metareinforcement learning,\u201d arXiv [cs.LG], Jan. 2023.   \n[5] J. Bauer, K. Baumli, F. Behbahani, A. Bhoopchand, N. Bradley-Schmieg, M. Chang, N. Clay, A. Collister, V. Dasagi, L. Gonzalez, K. Gregor, E. Hughes, S. Kashem, M. Loks-Thompson, H. Openshaw, J. ParkerHolder, S. Pathak, N. Perez-Nieves, N. Rakicevic, T. Rockt\u00e4schel, Y. Schroecker, S. Singh, J. Sygnowski, K. Tuyls, S. York, A. Zacherl, and L. M. Zhang, \u201cHuman-timescale adaptation in an open-ended task space,\u201d in Proceedings of the 40th International Conference on Machine Learning (A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, eds.), vol. 202 of Proceedings of Machine Learning Research, pp. 1887\u20131935, PMLR, 23\u201329 Jul 2023.   \n[6] N. Shaker, J. Togelius, and M. J. Nelson, Procedural Content Generation in Games. Computational Synthesis and Creative Systems, Springer, 2016.   \n[7] M. Jiang, E. Grefenstette, and T. Rockt\u00e4schel, \u201cPrioritized Level Replay,\u201d in Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event (M. Meila and T. Zhang, eds.), vol. 139 of Proceedings of Machine Learning Research, pp. 4940\u20134950, PMLR, 2021.   \n[8] M. Dennis, N. Jaques, E. Vinitsky, A. Bayen, S. Russell, A. Critch, and S. Levine, \u201cEmergent complexity and zero-shot transfer via unsupervised environment design,\u201d Advances in neural information processing systems, vol. 33, pp. 13049\u201313061, 2020.   \n[9] J. Parker-Holder, M. Jiang, M. Dennis, and others, \u201cEvolving curricula with regret-based environment design,\u201d International Conference on Machine Learning, 2022.   \n[10] L. M. Zintgraf, K. Shiarlis, M. Igl, S. Schulze, Y. Gal, K. Hofmann, and S. Whiteson, \u201cVariBAD: A Very Good Method for Bayes-adaptive Deep RL via Meta-learning,\u201d in 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, OpenReview.net, 2020.   \n[11] Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and P. Abbeel, \u201cRL2: Fast Reinforcement Learning via Slow Reinforcement Learning,\u201d arXiv:1611.02779 [cs, stat], Nov. 2016.   \n[12] J. Wang, Z. Kurth-Nelson, H. Soyer, J. Z. Leibo, D. Tirumala, R. Munos, C. Blundell, D. Kumaran, and M. M. Botvinick, \u201cLearning to reinforcement learn,\u201d in Proceedings of the 39th Annual Meeting of the Cognitive Science Society, CogSci 2017, London, UK, 16-29 July 2017 (G. Gunzelmann, A. Howes, T. Tenbrink, and E. J. Davelaar, eds.), cognitivesciencesociety.org, 2017.   \n[13] L. Zintgraf, K. Shiarli, V. Kurin, K. Hofmann, and S. Whiteson, \u201cFast context adaptation via meta-learning,\u201d in International Conference on Machine Learning, pp. 7693\u20137702, PMLR, 2019.   \n[14] K. Rakelly, A. Zhou, C. Finn, S. Levine, and D. Quillen, \u201cEfficient Off-policy Meta-reinforcement Learning via Probabilistic Context Variables,\u201d in Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA (K. Chaudhuri and R. Salakhutdinov, eds.), vol. 97 of Proceedings of Machine Learning Research, pp. 5331\u20135340, PMLR, 2019.   \n[15] M. C. Fontaine and S. Nikolaidis, \u201cDifferentiable Quality Diversity,\u201d in Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual (M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, and J. W. Vaughan, eds.), pp. 10040\u201310052, 2021.   \n[16] J. Mouret and J. Clune, \u201cIlluminating search spaces by mapping elites,\u201d CoRR, vol. abs/1504.04909, 2015.   \n[17] M. Jiang, M. Dennis, J. Parker-Holder, J. N. Foerster, E. Grefenstette, and T. Rockt\u00e4schel, \u201cReplay-guided Adversarial Environment Design,\u201d in Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual (M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, and J. W. Vaughan, eds.), pp. 1884\u20131897, 2021.   \n[18] J. X. Wang, M. King, N. P. M. Porcel, Z. Kurth-Nelson, T. Zhu, C. Deck, P. Choy, M. Cassin, M. Reynolds, H. F. Song, et al., \u201cAlchemy: A benchmark and analysis toolkit for meta-reinforcement learning agents,\u201d in Thirty-ffith Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.   \n[19] C. Finn, P. Abbeel, and S. Levine, \u201cModel-agnostic Meta-learning for Fast Adaptation of Deep Networks,\u201d in Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017 (D. Precup and Y. W. Teh, eds.), vol. 70 of Proceedings of Machine Learning Research, pp. 1126\u20131135, PMLR, 2017.   \n[20] L. C. Melo, \u201cTransformers are Meta-Reinforcement Learners,\u201d in Proceedings of the 39th International Conference on Machine Learning (K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, eds.), vol. 162 of Proceedings of Machine Learning Research, pp. 15340\u201315359, PMLR, 2022.   \n[21] J. Grigsby, L. Fan, and Y. Zhu, \u201cAMAGO: Scalable in-context reinforcement learning for adaptive agents,\u201d in The Twelfth International Conference on Learning Representations, 2024.   \n[22] L. M. Zintgraf, L. Feng, C. Lu, M. Igl, K. Hartikainen, K. Hofmann, and S. Whiteson, \u201cExploration in Approximate Hyper-state Space for Meta Reinforcement Learning,\u201d in Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event (M. Meila and T. Zhang, eds.), vol. 139 of Proceedings of Machine Learning Research, pp. 12991\u201313001, PMLR, 2021.   \n[23] K. Cobbe, C. Hesse, J. Hilton, and J. Schulman, \u201cLeveraging Procedural Generation to Benchmark Reinforcement Learning,\u201d in Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, vol. 119 of Proceedings of Machine Learning Research, pp. 2048\u20132056, PMLR, 2020.   \n[24] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine, \u201cMeta-World: A Benchmark and Evaluation for Multi-task and Meta Reinforcement Learning,\u201d in 3rd Annual Conference on Robot Learning, CoRL 2019, Osaka, Japan, October 30 - November 1, 2019, Proceedings (L. P. Kaelbling, D. Kragic, and K. Sugiura, eds.), vol. 100 of Proceedings of Machine Learning Research, pp. 1094\u20131100, PMLR, 2019.   \n[25] J. Arnold and R. Alexander, \u201cTesting autonomous robot control software using procedural content generation,\u201d in Lecture Notes in Computer Science, Lecture notes in computer science, pp. 33\u201344, Berlin, Heidelberg: Springer Berlin Heidelberg, 2013.   \n[26] D. J. Fremont, T. Dreossi, S. Ghosh, X. Yue, A. L. Sangiovanni-Vincentelli, and S. A. Seshia, \u201cScenic: a language for scenario specification and scene generation,\u201d in Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI 2019, (New York, NY, USA), p. 63\u201378, Association for Computing Machinery, 2019.   \n[27] G. E. Mullins, P. G. Stankiewicz, R. C. Hawthorne, and S. K. Gupta, \u201cAdaptive generation of challenging scenarios for testing and evaluation of autonomous vehicles,\u201d J. Syst. Softw., vol. 137, pp. 197\u2013215, Mar. 2018.   \n[28] Y. Abeysirigoonawardena, F. Shkurti, and G. Dudek, \u201cGenerating adversarial driving scenarios in highfidelity simulators,\u201d in 2019 International Conference on Robotics and Automation (ICRA), pp. 8271\u20138277, IEEE, May 2019.   \n[29] A. Gambi, M. Mueller, and G. Fraser, \u201cAutomatically testing self-driving cars with search-based procedural content generation,\u201d in Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis, (New York, NY, USA), ACM, July 2019.   \n[30] Y. Zhou, S. Booth, N. Figueroa, and J. Shah, \u201cRocus: Robot controller understanding via sampling,\u201d in Proceedings of the 5th Conference on Robot Learning (A. Faust, D. Hsu, and G. Neumann, eds.), vol. 164 of Proceedings of Machine Learning Research, pp. 850\u2013860, PMLR, 08\u201311 Nov 2022.   \n[31] T. Miconi, \u201cProcedural generation of meta-reinforcement learning tasks,\u201d Feb. 2023.   \n[32] T. Gabor, A. Sedlmeier, M. Kiermeier, T. Phan, M. Henrich, M. Pichlmair, B. Kempter, C. Klein, H. Sauer, R. S. Ag, and J. Wieghardt, \u201cScenario co-evolution for reinforcement learning on a grid world smart factory domain,\u201d in Proceedings of the Genetic and Evolutionary Computation Conference, (New York, NY, USA), ACM, July 2019.   \n[33] D. M. Bossens and D. Tarapore, \u201cQED: Using quality-environment-diversity to evolve resilient robot swarms,\u201d IEEE Trans. Evol. Comput., vol. 25, pp. 346\u2013357, Apr. 2021.   \n[34] A. Dharna, J. Togelius, and L. B. Soros, \u201cCo-generation of game levels and game-playing agents,\u201d in Proceedings of the Sixteenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE\u201920, AAAI Press, 2020.   \n[35] R. Wang, J. Lehman, J. Clune, and K. O. Stanley, \u201cPaired open-ended trailblazer (POET): Endlessly generating increasingly complex and diverse learning environments and their solutions,\u201d arXiv [cs.NE], Jan. 2019.   \n[36] R. Wang, J. Lehman, A. Rawal, J. Zhi, Y. Li, J. Clune, and K. Stanley, \u201cEnhanced POET: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions,\u201d in Proceedings of the 37th International Conference on Machine Learning (H. D. III and A. Singh, eds.), vol. 119 of Proceedings of Machine Learning Research, pp. 9940\u20139951, PMLR, 13\u201318 Jul 2020.   \n[37] V. Bhatt, B. Tjanaka, M. Fontaine, and S. Nikolaidis, \u201cDeep surrogate assisted generation of environments,\u201d Advances in Neural Information Processing Systems, vol. 35, pp. 37762\u201337777, 2022.   \n[38] D. Gravina, A. Khalifa, A. Liapis, J. Togelius, and G. N. Yannakakis, \u201cProcedural content generation through quality diversity,\u201d in 2019 IEEE Conference on Games (CoG), pp. 1\u20138, ieeexplore.ieee.org, Aug. 2019.   \n[39] M. C. Fontaine and S. Nikolaidis, \u201cA Quality Diversity Approach to Automatically Generating Humanrobot Interaction Scenarios in Shared Autonomy,\u201d in Robotics: Science and Systems XVII, Virtual Event, July 12-16, 2021 (D. A. Shell, M. Toussaint, and M. A. Hsieh, eds.), 2021.   \n[40] M. C. Fontaine, Y. Hsu, Y. Zhang, B. Tjanaka, and S. Nikolaidis, \u201cOn the Importance of Environments in Human-robot Coordination,\u201d in Robotics: Science and Systems XVII, Virtual Event, July 12-16, 2021 (D. A. Shell, M. Toussaint, and M. A. Hsieh, eds.), 2021.   \n[41] L. Grillotti and A. Cully, \u201cUnsupervised behavior discovery with quality-diversity optimization,\u201d IEEE Trans. Evol. Comput., 2022.   \n[42] A. Khalifa, P. Bontrager, S. Earle, and J. Togelius, \u201cPCGRL: Procedural Content Generation via Reinforcement Learning,\u201d in Proceedings of the Sixteenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE 2020, virtual, October 19-23, 2020 (L. Lelis and D. Thue, eds.), pp. 95\u2013101, AAAI Press, 2020.   \n[43] B. Tjanaka, M. C. Fontaine, Y. Zhang, S. Sommerer, and others, \u201cpyribs: A bare-bones python library for quality diversity optimization,\u201d 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Algorithmic details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Algorithm. The pseudocode below walks through the entire training process for DIVA in abstract. All of the new components that DIVA introduces is written in green, and all $\\mathbf{D}\\mathbf{I}\\mathbf{V}\\mathbf{A}+$ modifications are in blue. Original VariBAD training steps are in black, and all inline comments are in orange. ", "page_idx": 13}, {"type": "text", "text": "Algorithm 2 DIVA (detailed) ", "text_level": 1, "page_idx": 13}, {"type": "table", "img_path": "Xo1Yqyw7Yx/tmp/79fa0c72b9a8185bf83cd11196057999c64dde3fab7c81a97683dd58ae83970b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Algorithm 3 QD update ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "1: # Perform a single QD update on archive $\\mathcal{G}$ with batch size $B$ .   \n2: function qd_upda $\\mathtt{r E}(\\mathcal{G},J,M,B)$   \n3: $\\tilde{\\Theta}^{B\\times n}=[\\tilde{\\theta}_{1},\\dots,\\tilde{\\theta}_{B}]\\gets$ sample_from_emitters $(\\mathcal{G},M,B)$ \u25b7Get mutated batch of solutions   \n4: $F^{B\\times k}=[f(\\Tilde{\\theta}_{1}),\\dots,f(\\Tilde{\\theta}_{B})]\\leftarrow$ compute_features $\\tilde{\\Theta})$ $\\triangleright$ Compute env. features   \n5: $J^{B\\times1}=[J(\\widetilde{\\pmb{\\theta}}_{1}),\\dots,J(\\widetilde{\\pmb{\\theta}}_{B})]\\leftarrow$ compute_objectives( $(\\tilde{\\Theta})$ \u25b7Compute env. objectives   \n6: G\u2032 \u2190add_solutions $(\\mathcal{G},(\\tilde{\\Theta},F,J)$ ) \u25b7Add new solutions to archive if they are elites   \n7: return G\u2032 ", "page_idx": 13}, {"type": "text", "text": "Details on the two-stage QD updates Here we provide more details on the process described in Section Section 4. Hyperparameters $N_{\\mathrm{S1}}$ and $N_{\\mathrm{S}2}$ are set to define the number of QD updates to perform in each stage (see Appendix D). In proportion to how many updates in S1 have elapsed, if the sample mask is enabled, the mask is moved at a linear pace from encapsulating the full S1 archive, to covering only the target region. We also set a hyperparameter, $N_{\\mathrm{SM}}$ (see Appendix D), which specifies the minimum number of solutions which must exist within the mask\u2019s new bounds for it to be updated. This is to ensure the mask never outpaces the search process. The mask was only found to be necessary in the ALCHEMY environment. In S1 we sample solutions uniformly from within the mask. In S2, we begin sampling from the discretized target density distribution approximated from the downstream feature samples. Two stages are used for RACING as well, since many initial samples fall outside of the target region, but masking was not found to be necessary. The sample mask has a relatively straightforward implementation for MAP-Elites, which we use for ALCHEMY\u2019s discrete genotype (and GRIDNAV, where no mask is required). Since MAP-Elite updates entail performing mutations on solutions directly sampled from the archive, the mask is implemented to only consider solutions that fall within the mask bounds. However, since the CMA-ES-based emitter we use for RACING operates by sampling from a parameterized distribution, instead of sampling from the archive directly, the mask would need to be applied to these parameters instead of the archive. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "B Domain details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 GRIDNAV ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "GRIDNAV features. The following features are defined for the GRIDNAV environment: ", "page_idx": 14}, {"type": "table", "img_path": "Xo1Yqyw7Yx/tmp/6215435a48a3097116cfa6aa7821f702e812f8a34d7cf49c16e1c9ff0934984d.jpg", "table_caption": ["Table 1: GRIDNAV features. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B.2 ALCHEMY ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "ALCHEMY features. We defined the following features for the ALCHEMY environment: ", "page_idx": 14}, {"type": "table", "img_path": "Xo1Yqyw7Yx/tmp/24a9bdec77dd9b0373fed3bf380b4f5a29df75f370b8d6834461ccb11aac7176.jpg", "table_caption": ["Table 2: ALCHEMY features. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 11 contains the feature distributions for the structured and unstructured environment parameterizations on ALCHEMY, computed over 100 feature samples. Figure 14 shows the covariance between feature values for RACING, computed over 100 feature samples. ", "page_idx": 14}, {"type": "image", "img_path": "Xo1Yqyw7Yx/tmp/19a08019fc7a45a9e3867d61dedb93b6dbaf9a61b1a6bf65c7637e1e8a5d8d75.jpg", "img_caption": ["Figure 11: ALCHEMY all feature distributions. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "Xo1Yqyw7Yx/tmp/40182868558c6b1de8425768e6275b0ae013c0f01b1320f53c54d7facca3e13f.jpg", "img_caption": ["Figure 12: ALCHEMY measure covariances. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Archive hyperparameters for ALCHEMY were determined based on some knowledge about the domain, as well as the feature distributions (Figure 13). We noticed a major deviation between $E_{\\mathrm{U}}$ and $E_{\\mathrm{S}}$ in the feature LATENTSTATEDIVERSITY (LSD), and an even greater on in MANHATTANTOOPTIMAL (MTO). These two constitute the initial dimensions of the archive, and we found the sample mask updates to be crucial to reach and flil the target region (see Figure 15). We use PARITYFIRSTSTONE (PFS) in the second stage to encourage more diversity once the target is reached; it is excluded from the first stage, which is focused on simply reaching the target. The archive for the first stage is of shape [100, 300, 1], corresponding to LSD, MTO, and PFS. The second stage shape is [150, 150, 5]. We found this archive to produce diverse enough solutions, evidenced by the number and spread in the target region, so we used this setting to train our DIVA agents. The only objective we found useful for ALCHEMY was a slight bias for newly generated solutions, which we also used for GRIDNAV. We hypothesize this prevents the archive from getting \u201cstuck\u201d with a suboptimal set of solutions, in absense of other objectives. ", "page_idx": 15}, {"type": "text", "text": "B.3 RACING ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "RACING features. See Table 13 for all features defined on RACING. Figure 13 contains the feature distributions for the structured and unstructured environment parameterizations on RACING, computed over 100 feature samples. Figure 14 shows the covariance between feature values for RACING, computed over 100 samples. ", "page_idx": 15}, {"type": "text", "text": "Archive hyperparameters for RACING were determined through trial and error, by viewing the samples produced by the archives at the end of the QD updates, as well as the target coverage metrics. After a few iterations, it became clear that Total Angle Change TOTALANGLECHANGES (TAC) was the most useful feature, and so we tried pairing it with a number of others, prioritizing other features with low absolute covariance (see Figure 14). ", "page_idx": 15}, {"type": "text", "text": "The best performing archive used by DIVA on RACING uses TAC and CX as its features, and used a measure alignment objective over CY and VY. The measure alignment objective rewards solutions for having measure values over the specific measures that are similar to the target distribution. We also found that randomly sampling these objective values according to the target distribution provided some additional support in covering the target region efficiently. ", "page_idx": 15}, {"type": "table", "img_path": "Xo1Yqyw7Yx/tmp/27b387a3caba75cb0c8f04bafa252a39d59e3031a810d08b303cb429095ffa1e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "Xo1Yqyw7Yx/tmp/a0e27990fe12704e046a1dd9bcccf1cfe7c4db5b8ea2dd9651351cb8c65e2ba8.jpg", "img_caption": ["Figure 13: RACING all feature distributions. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "The slightly \u201cmisspecified\u201d archive was chosen because its solutions generated some diversity, but not as much as the aforementioned one. This archive uses TAC and ATLR as its features, and uses a measure diversity objective over just CY. Instead of prioritizing alignment to the target distribution, the diversity objective samples a handful of solutions from the archive, and uses the current solutions deviation from these as its objective. ", "page_idx": 16}, {"type": "text", "text": "We use final archive dimensions of $500\\times500$ for both S1 and S2. ", "page_idx": 16}, {"type": "text", "text": "C Ablation analysis ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Sample mask ablation. Figure 15 shows the benefit of updating the sample mask bounds during the first archive fliling stage on ALCHEMY. Not only does this approach produce significantly more total archive solutions, but more importantly, progress towards fliling the target region specifically is accelerated. ", "page_idx": 16}, {"type": "text", "text": "D Hyperparameter sensitivity analysis ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Varying QD mutation rate. We perform an ablation on the QD mutation rate, which is the probability that a given gene will be mutated (for MAP-Elites). We perform this ablation on ALCHEMY because the its search is the most challenging of the three environments we consider (it is the sole environment that required a longer S1 and the sample mask trick for accelerating to accelerate the search). We see from Figure 16 that ALCHEMY results are not very sensitive to the setting of the mutation rate. ", "page_idx": 16}, {"type": "image", "img_path": "Xo1Yqyw7Yx/tmp/e5fd3901083c1ca542121dcd331fbf6b212c793e68f4ef417e5b82c434ad23b4.jpg", "img_caption": ["Figure 14: RACING measure covariances. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "Xo1Yqyw7Yx/tmp/0c893f438785a959e99069a4441bbc12daa3a5ba323885ff32e474aafdb10959.jpg", "img_caption": ["Figure 15: ALCHEMY sample mask ablation curves. This specific result is the result of two seeds instead of five, as we found the variance to be very low for this ablation (validated across other parameter settings). "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Varying number of QD updates. We perform a similar ablation to test how robust DIVA is to the number of QD updates performed. We see from Figure 17 that ALCHEMY results suffer somewhat from fewer updates (e.g. for only $10\\mathbf{k}$ in each stage), still significantly outperform baselines in each case. The trend is clear, however: more QD updates produces more solutions, which generally translates to better performance, even if slightly. ", "page_idx": 17}, {"type": "text", "text": "Varying number of downstream samples. Next we test how robust DIVA is to the number of downstream samples used to compute the target distribution. In Figure 18 we see that, despite the errors increasing with fewer samples, DIVA still significantly outperforms baselines with as few as five samples. ", "page_idx": 17}, {"type": "text", "text": "E Training details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "E.1 DIVA hyperparameters ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Table 4 displays the hyperparameters used for DIVA across all domains. ", "page_idx": 17}, {"type": "text", "text": "A note on $N_{\\mathrm{TRS}}$ computation The initial QD population $\\left(n_{0}\\right)$ is implemented such that the first set of QD updates simply generates $n_{0}$ random levels from $E_{U}$ , before performing the actual mutations (for ME) or intelligent sampling (for ES). Thus, the formula we use for computing $N_{\\mathrm{TRS}}$ , the total reset steps provided to DIVA (see Table 4), which we use to compare the extra steps we provide $\\mathtt{P L R^{\\perp}}$ and ACCEL (discussed in Section 5), does not include $n_{0}$ ; it is simply the product of the batch size and the total number of QD iterations. ", "page_idx": 17}, {"type": "image", "img_path": "Xo1Yqyw7Yx/tmp/36fa69e6266cbcfb213c84d3075d45c7f928a3ee1745f48089dd0e1070e22cae.jpg", "img_caption": ["Figure 16: Effect of varying QD mutation rate in ALCHEMY. Left: The returns for the final episode by mutation rate, after training on archives produced with each mutation rate. Right: The final number of solutions in the archive after performing QD updates with each mutation rate. This result was produced by running three different seeds for each mutation rate. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "Xo1Yqyw7Yx/tmp/9c00bdac282066c3b879f0e2f6b4c39d0592a761a3ff7868220bbbdb5683b6cc.jpg", "img_caption": ["Figure 17: Effect of varying the number of QD updates in ALCHEMY. Left: The returns for the final episode by number of QD updates in each stage $\\left.N_{\\mathrm{S}1}=N_{\\mathrm{S}2}\\right)$ ). Right: The final number of solutions in the archive after performing each number of QD updates. This result was produced by running three different seeds for each setting. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "E.2 VariBAD hyperparameters ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Table 5 displays the hyperparameters used for VariBAD across all domains. ", "page_idx": 18}, {"type": "text", "text": "E.3 Baseline hyperparameters ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Table 6 displays the hyperparameters used for $\\mathrm{PLR^{\\perp}}$ across all domains. ", "page_idx": 18}, {"type": "text", "text": "E.3.2 ACCEL ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "ACCEL uses the same hyperparameters as $\\mathtt{P L R^{\\perp}}$ (see Table 6), combined with the same evolutionary hyperparameters used for DIVA\u2019s QD archive (see Table 4). ", "page_idx": 18}, {"type": "text", "text": "E.4 Computational details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "All results were produced on a handful of Titan X or Xp GPUs. Environments were parallelized across multiple CPU cores to accelerate training. While the experiment time varies by method and environment, most experiments take less than a day to run to completion. $\\mathtt{P L R^{\\perp}}$ and ACCEL take the longest, as they required twice as many environment steps as the other methods\u2014on the two latter domains, these methods take well over a day to run to completion. ", "page_idx": 18}, {"type": "image", "img_path": "Xo1Yqyw7Yx/tmp/ea95c12ad8e5fbd13acccadfb4069d2e98d8d3c46ea1c7e1147a9d274f94bdda.jpg", "img_caption": ["Figure 18: Effect of varying number of samples in ALCHEMY. Left: DIVA evaluation returns for the final episode by number of downstream samples, after training on archives produced with by using each number of samples to produce archive bounds and prior. Center/Right: Errors for mean and variance parameters of the normal distribution based on number of samples used for computation; for MANHATTANTOOPTIMAL and LATENTSTATEDIVERSITY. For all plots, five seeds were used for each hyperparameter setting. "], "img_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "Xo1Yqyw7Yx/tmp/c650b788ded8b8cc4d128b8b4a08891d8f63559f37405bebf9510527036daea6.jpg", "table_caption": ["Table 4: DIVA hyperparameter settings. "], "table_footnote": ["\u2020GRIDNAV, \u22c6ALCHEMY, \u22c4RACING "], "page_idx": 19}, {"type": "table", "img_path": "Xo1Yqyw7Yx/tmp/63193fa93ee19e7f4955805d0cdd035e10effcf107bad6651d48d5cd6baf6687.jpg", "table_caption": ["Table 5: VariBAD hyperparameter settings. "], "table_footnote": ["\u2020GRIDNAV, \u22c6ALCHEMY, \u22c4RACING "], "page_idx": 20}, {"type": "table", "img_path": "Xo1Yqyw7Yx/tmp/e4866fd5e9e032d55065611d152a4df7e909f0ac93919e67fd484cc471910713.jpg", "table_caption": ["Table 6: PLR hyperparameter settings. "], "table_footnote": ["\u2020GRIDNAV, \u22c6ALCHEMY, \u22c4RACING "], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The two main claims in the abstract are as follows: \u201cOur empirical results demonstrate DIVA\u2019s [1] unique ability to leverage ill-parameterized simulators to train adaptive behavior in meta-RL agents, [2] far outperforming competitive baselines.\" (1): This ability is indeed \u201cunique\u201d, as far as the authors are aware, and is supported by the discussion of related works in Section 6. (2): The empirical results in this paper presented in Section 5 support the claim that DIVA far outperforms other baselines for training meta-RL agents. Specific pieces of evidence include the GRIDNAV results in Figure 4, the ALCHEMY results in Figure 5, and the results in RACING, contained in Figure 7, 9, and 10. The final sentence in the abstract makes a more general, weaker claim about the presented method\u2019s potential: \u201cThese findings highlight the potential of approaches like DIVA to enable training in complex open-ended domains, and to produce more robust and adaptable agents.\" The authors believe the same set of evidence supports this claim as well, but is ultimately up to the reader\u2014and more importantly, future work\u2014to decide. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The authors discuss limitations of the present work in Section 7. This Discussion section covers (1) key takeaways from the paper, (2) limitations, and (3) promising avenues for future work. The authors combine these sections because the points are all interrelated, and having them in the same place preserves cohesion and flow of the writing. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. ", "page_idx": 22}, {"type": "text", "text": "\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. \u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not include any formalized theoretical results. The authors did not find reason to provide any theorems, formulas, or proofs to support the claims made. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: In addition to the provided code (see Section 8), which is self-contained and includes documentation for reproducing all empirical results in this paper, the authors additionally include all training details, including hyperparameter settings for all methods, in Appendix E. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The code is available and well-documented (see Section 8), and contains sufficient instructions to faithfully reproduce the main experimental results. Additionally, all necessary details required to independently reproduce the results are self contained in the paper, with hyperparameter settings and other details fully described in Appendix E. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: See Appendix E for all relevant training and test details not covered in the main body. Additionally, details about each evaluation domain, if likewise not covered in the main body, are available in Appendix B. Lastly, all of these details are self-contained and documented in the code (see Section 8). ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: All results (excepting visualizations, and the trend curve in Figure 4 (a), for which they are not needed) are accompanied with error bars that capture the factors of variability, mostly due to random seeding (affecting many different random settings of the algorithm, e.g. initial model weights, sampling, etc.). The significance of all error bars, and number of seeds used for each experiment, are detailed in Section 5. Other training details are available in Appendix E. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: All compute resources (workers, memory, time of execution) are detailed in Appendix E.4. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The authors have read and understood the NeurIPS Code of Ethics, and the paper conforms to the code in every respect. ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The authors specifically discuss the broader impacts of the present work in Section 9. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: The authors feel that such safeguards are unnecessary for the present work, as the risk for misuse is estimated by the authors to be very low\u2014it is unclear how this work and its artifacts can be directly used for malicious purposes. The authors are willing to adjust this position if members of the community feel otherwise. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: All used resources are either cited properly in the paper and/or are documented and credited in the codebase (see Section 8). ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The new asset introduced in this paper is the codebase (see Section 8), which is well-documented for the purpose of reproducibility, and contains details about training, the license, and limitations, etc. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]