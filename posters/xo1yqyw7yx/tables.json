[{"figure_path": "Xo1Yqyw7Yx/tables/tables_3_1.jpg", "caption": "Table 4: DIVA hyperparameter settings.", "description": "This table lists the hyperparameters used in the DIVA algorithm.  It breaks down hyperparameters for the two stages of quality diversity (QD) updates (Ns1 and Ns2), the total number of QD updates (NQD), the initial population size (no), the number of QD solution emitters (Ne), and the sampling batch size of each QD emitter (Be).  Further hyperparameters are detailed for the effective QD batch size per update, the total number of reset steps, the type of QD emitter (MAP-Elites or CMA-ES), and various other settings related to the quality diversity approach and the meta-learning algorithm (VariBAD). The table also provides values for each hyperparameter used for the GRIDNAV, ALCHEMY, and RACING experiments.", "section": "E DIVA training details"}, {"figure_path": "Xo1Yqyw7Yx/tables/tables_13_1.jpg", "caption": "Table 4: DIVA hyperparameter settings.", "description": "This table lists all the hyperparameters used by the DIVA algorithm across all three experimental domains (GRIDNAV, ALCHEMY, RACING).  It includes hyperparameters for both stages of the QD process (S1 and S2), parameters controlling the meta-RL training (VariBAD), and settings specific to the chosen QD algorithm (MAP-Elites or CMA-ES) in each domain.  Understanding these settings is crucial for reproducing the experiments detailed in the paper.", "section": "E DIVA hyperparameters"}, {"figure_path": "Xo1Yqyw7Yx/tables/tables_14_1.jpg", "caption": "Table 1: GRIDNAV features.", "description": "This table lists two features used in the GRIDNAV environment: XPOSITION (XP), representing the x-coordinate of the goal, and YPOSITION (YP), representing the y-coordinate of the goal.", "section": "B.1 GRIDNAV"}, {"figure_path": "Xo1Yqyw7Yx/tables/tables_14_2.jpg", "caption": "Table 2: ALCHEMY features.", "description": "This table lists and describes the features used in the ALCHEMY environment.  Each feature provides a different aspect of the complexity or diversity of the chemical reactions simulated in the environment, relevant to the overall training task.  Understanding these features is crucial to interpreting the results of the DIVA algorithm and the comparative analysis with baselines.", "section": "B.2 ALCHEMY"}, {"figure_path": "Xo1Yqyw7Yx/tables/tables_16_1.jpg", "caption": "Table 3: RACING features.", "description": "This table lists all features used in the RACING environment.  Each feature provides a different perspective on the characteristics of the racetrack, capturing aspects such as the shape's complexity, size, and curvature. These features are used by DIVA to generate diverse training tracks, and they are carefully chosen to represent the key characteristics that distinguish challenging and interesting racetracks from simpler ones.", "section": "B.3 RACING"}, {"figure_path": "Xo1Yqyw7Yx/tables/tables_19_1.jpg", "caption": "Table 4: DIVA hyperparameter settings.", "description": "This table shows the hyperparameters used in the DIVA algorithm across different environments (GRIDNAV, ALCHEMY, RACING).  It details settings for both stages of the quality diversity (QD) optimization process (Ns1, Ns2), the overall number of QD updates, population size, emitter parameters (mutation rates, initial sigma), and objective functions used.  It also includes the number of meta-training steps and a comparison of additional steps provided to baselines. Note that the values vary across the different domains indicated by the superscripts. ", "section": "E DIVA training details"}, {"figure_path": "Xo1Yqyw7Yx/tables/tables_20_1.jpg", "caption": "Table 5: VariBAD hyperparameter settings.", "description": "This table shows the hyperparameters used for the VariBAD algorithm across all the domains in the experiments.  It includes parameters related to the policy network, value function, the variational autoencoder (VAE), and the Proximal Policy Optimization (PPO) algorithm.  The table specifies settings for learning rates, optimization methods, network architectures, regularization techniques, and other key parameters.  Each parameter is clearly defined and its corresponding value listed.", "section": "E.2 VariBAD hyperparameters"}, {"figure_path": "Xo1Yqyw7Yx/tables/tables_21_1.jpg", "caption": "Table 6: PLR hyperparameter settings.", "description": "This table lists the hyperparameters used for the Robust PLR (PLR+) baseline in the RACING, ALCHEMY, and GRIDNAV experiments.  The hyperparameters control aspects of the level replay mechanism, such as the number of levels stored in the buffer (NPLR), the replay scoring function (score(\u03c4,\u03c0)), and the exploration-exploitation trade-off through temperature parameters (\u03b2s, \u03b2E, and \u03b2c).  The table also includes parameters related to staleness (\u03c1c, fc, and \u03b2c).", "section": "E.3.1 PLR+"}]