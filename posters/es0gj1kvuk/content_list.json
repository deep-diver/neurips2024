[{"type": "text", "text": "Data subsampling for Poisson regression with pth-root-link ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Han Cheng Lie ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Alexander Munteanu ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Institut f\u00fcr Mathematik   \nUniversit\u00e4t Potsdam   \nGermany   \nhanlie@uni-potsdam.de ", "page_idx": 0}, {"type": "text", "text": "Department of Statistics TU Dortmund University Germany alexander.munteanu@tu-dortmund.de ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We develop and analyze data subsampling techniques for Poisson regression, the standard model for count data $y\\,\\in\\,\\mathbb{N}$ . In particular, we consider the Poisson generalized linear model with ID- and square root-link functions. We consider the method of coresets, which are small weighted subsets that approximate the loss function of Poisson regression up to a factor of $1\\pm\\varepsilon$ . We show $\\Omega(n)$ lower bounds against coresets for Poisson regression that continue to hold against arbitrary data reduction techniques up to logarithmic factors. By introducing a novel complexity parameter and a domain shifting approach, we show that sublinear coresets with $1\\pm\\varepsilon$ approximation guarantee exist when the complexity parameter is small. In particular, the dependence on the number of input points can be reduced to polylogarithmic. We show that the dependence on other input parameters can also be bounded sublinearly, though not always logarithmically. In particular, we show that the square root-link admits an $O(\\log(y_{\\mathrm{max}}))$ dependence, where $y_{\\mathrm{max}}$ denotes the largest count presented in the data, while the ID-link requires a $\\Theta(\\sqrt{y_{\\mathrm{max}}/\\log(y_{\\mathrm{max}})})$ dependence. As an auxiliary result for proving the tightness of the bound with respect to $y_{\\mathrm{max}}$ in the case of the ID-link, we show an improved bound on the principal branch of the Lambert $W_{0}$ function, which may be of independent interest. We further show the limitations of our analysis when $p$ th degree root-link functions for $p\\,\\geq\\,3$ are considered, which indicate that other analytical or computational methods would be required if such a generalization is even possible. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Random sampling is arguably one of the most popular approaches to reduce large amounts of data to save memory, runtime, and further downstream resources such as communication bandwidth and energy. In contrast, classic statistical learning theory often uses uniform sampling and provides only asymptotic approximation guarantees. These guarantees often require strict assumptions such as i.i.d. data and for model assumptions to be met exactly. However, data collected from real applications often violate these conditions: only finite samples are available, independence might not be satisfied, and the model may deviate from reality. When model ftiting algorithms are applied to such data, we are not only interested in reducing the above-mentioned resource requirements, but also in providing rigorous worst-case guarantees on approximation. ", "page_idx": 0}, {"type": "text", "text": "Arguably, the most popular approach is the Sensitivity Framework [26, 19], which provides a generalpurpose importance sampling scheme that yields a weighted subsample \u2014 or coreset \u2014 that given a data matrix $X$ approximates some loss function $f(X{\\boldsymbol{\\beta}})$ within a factor $(1\\pm\\varepsilon)$ for any query point $\\beta$ . This guarantee can be stated as follows: a significantly smaller subset $K\\subseteq X,k:=|K|\\ll|X|$ together with corresponding weights $w\\in\\mathbb{R}^{k}$ is a $(1\\pm\\varepsilon)$ -coreset for $X$ if it satisfies ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\forall\\beta\\in\\mathbb{R}^{d}\\colon|f(X\\beta)-f_{w}(K\\beta)|\\leq\\varepsilon\\cdot f(X\\beta).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "We point the interested reader to [37, 35, 32] for a gentle introduction and overview on coresets. ", "page_idx": 1}, {"type": "text", "text": "Our aim is of course not only to obtain good approximation accuracy as stated in Equation (1), but also for the subsample achieving the bound to be sublinear in the input size. Unfortunately, most generalized linear models do not admit strongly sublinear data summaries with reasonable approximation guarantees [36, 31]. This also holds for the Poisson models considered in this paper. ", "page_idx": 1}, {"type": "text", "text": "To go beyond the worst-case setting and enable meaningful data reduction, a natural approach is to parameterize the analysis with a quantity that captures the fit of data to the statistical model and quantifies the achievable size of succinct data summaries [36]. Another ingredient that is commonly used to tackle data reduction for generalized linear models is to relate their loss to $\\ell_{p}$ norms, for which $\\ell_{p}$ sensitivities or leverage scores yield viable importance sampling distributions [16, 12, 36, 34]. ", "page_idx": 1}, {"type": "text", "text": "1.1 Our contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We provide the first rigorous analysis for $(1\\pm\\varepsilon)$ -approximate data reduction for Poisson models: ", "page_idx": 1}, {"type": "text", "text": ". We show $\\Omega(n)$ lower bounds against coresets for Poisson regression (Lemma 6.1), showing that changing the link function alone does not resolve the problem of bounding the complexity of coresets for the log-link, which is incompressible. Our lower bound extends to arbitrary data reduction techniques up to a $\\log(n)$ factor (Lemma 6.2). ", "page_idx": 1}, {"type": "text", "text": "2. We introduce a novel complexity parameter $\\rho$ that captures the compressibility of data under a Poisson pth-root-link model (Equation (2)). This parameter corresponds naturally to the statistical model assumptions and establishes a relationship between these assumptions and an optimization perspective of the compressibility problem.   \n3. We conduct a parameterized analysis, showing that sublinear coresets exist under the statistically natural assumption of small $\\rho$ parameter (Theorem 3.8), and using a novel domain shift idea for their optimization (Theorem 4.2).   \n4. We prove a square root upper bound for the Lambert $W_{0}$ function over $[-1/e,0)$ (Lemma 6.3) that allows us to prove tight bounds for the slope of a linear lower envelope (Lemma 6.4). This justifies an $\\tilde{\\Theta}(\\sqrt{y_{\\mathrm{max}}})^{1}$ dependence for the ID-link, which is contrasted by an $O(\\log{(y_{\\mathrm{{max}}})})$ dependence for the square root-link.   \n5. We show the limitation of our domain shifting approach, showing that the error of this method cannot be bounded to give the required $(1+\\varepsilon)$ approximation for $p\\geq3$ (Lemma 6.5). This indicates a limitation to the common choices $p\\in\\{1,2\\}$ , and suggests that different techniques than the ones we develop below may be needed to overcome this limitation. ", "page_idx": 1}, {"type": "text", "text": "1.2 Our techniques ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The general outline of our analysis follows the established method of sensitivity sampling [26, 19]. Several steps along this outline require novel ideas due to peculiarities of the Poisson loss function defined in Equations (3) and (4) below. A VC dimension bound of $d^{2}$ is easy to obtain by counting the number of arithmetic operations required to compare an individual loss to a given threshold as a measure of complexity [4]. A near-linear $\\tilde{O}(d/\\varepsilon)$ bound is obtained by a more fine-grained analysis, by grouping and rounding the associated count data (respectively, sensitivity scores) to powers of $(1+\\varepsilon)$ (resp. to powers of 2). This results in a surrogate loss function that admits group-wise linear VC dimension, by splitting the domain of each loss function at its unique global minimum into two regions such that the restriction of the function to each region is monotone, and connecting the resulting construction to hyperplane classifiers. Approximating the surrogate finally implies the desired $(1\\pm\\varepsilon)$ approximation for the original loss as well. ", "page_idx": 1}, {"type": "text", "text": "For bounding the sensitivities, the domain of the loss function $g(z)$ is split into three intervals: 1) one interval consisting of \u2018moderate\u2019 values of $z$ , such that the $g(z)$ values are bounded above and below within constants and can be treated using simple uniform sampling; 2) one interval where $z$ has large values, in which case $g(z)$ is closely related to $z^{p};3)$ ) one interval, where $z$ is close to 0, in which case the negative logarithm dominates the loss. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Tackling the interval in 2) requires relating the loss function $g_{y}(z)$ to $z^{p}$ . Specifically we would like to bound $z^{p}\\,\\geq\\,g_{y}(z)\\,\\geq\\,{\\bar{z}}^{p}/\\lambda$ for sufficiently large $z$ and for some value of $\\lambda$ . This requires special care, since the loss function $g_{y}(z)$ is translated polynomially towards larger values of $z$ with growing $y$ , but the minimum of $g_{y}(z)$ grows only logarithmically with $y$ . Informally speaking, the loss function \u2018widens\u2019, and its minimum \u2018moves mainly to the right\u2019, so for large $y$ , we would need a very flat lower bound, which requires large $\\lambda\\approx y$ (ignoring polylogarithmic terms). However, this is undesirable, since $\\lambda$ appears to be a crucial parameter for bounding the subsample size. Specifically, this would yield a $\\textstyle\\sum_{i\\in[n]}y_{i}=\\Omega(n)$ dependence in the coreset size. So instead, we bound the loss roughly as $z^{p}\\,\\geq\\,g_{y}(z)\\,\\geq\\,(z-y^{1/p})^{p}/\\lambda$ , which amounts to translating the lower envelope with growing $y$ as well. Additionally, we introduce a novel complexity parameter $\\rho$ which balances the translated $(z-y^{1/p})^{p}$ lower bound with the $z^{p}$ upper bound. We stress that these translation and balancing arguments are not artificial or just used to make calculations go through, but they are naturally consistent with the statistical model (see the discussion below Equation (2)). This proof strategy eventually captures the loss function within interval 2) more closely and yields sublinear bounds for $\\lambda$ as well. ", "page_idx": 2}, {"type": "text", "text": "We remark that in contrast to $\\mu$ -complexity in previous work [36, 34], a bounded balancing complexity parameter $\\rho$ does not handle the asymmetry between intervals 2) and 3). Tackling the interval in 3) thus also requires completely new ideas, as the negative logarithm has an infinite asymptote at 0, which we exploit to prove $\\tilde{\\Omega}(\\boldsymbol{n})$ lower bounds on subsample size in Lemma 6.1 and Lemma 6.2. Such asymptotes have not been mentioned or analyzed in previous related work on sensitivity sampling for GLMs. To circumvent the lower bound, we avoid this interval by introducing a novel domain shifting approach, requiring all feasible solutions to satisfy $z\\,>\\,\\eta$ for a suitable $\\eta\\,>\\,0$ for optimization. Choosing $\\eta$ in the order of $\\varepsilon$ , we can argue that the solution in the shifted domain is a $(1+\\varepsilon)$ approximation. Avoiding the asymptote enables a coreset construction for the shifted domain. ", "page_idx": 2}, {"type": "text", "text": "We believe that the domain shifting approach is necessary: if an instance consists of the extreme points on the convex hull, and all but a small (sublinear) number of points are separated by an $\\varepsilon$ distance to the boundary, then required structure is already in the data. But if non-extremal points are allowed to be arbitrarily close to the boundary, and we do not shift the domain, then we will not avoid high sensitivity points that are strictly inside the convex hull. Then the coreset size would necessarily depend on the distance of these non-extremal points to the boundary, and crucially on the number of points that are very close to the boundary of the convex hull, which again can be $\\mathrm{{\\Omega}}^{\\Omega(n)}$ . ", "page_idx": 2}, {"type": "text", "text": "Exploiting the asymmetry between the intervals 2) and 3) where the loss exhibits $z^{p}$ and \u2212 $\\log(z)$ growth respectively, we prove $\\tilde{\\Omega}(n)$ lower bounds on subsample size, by adapting known reduction techniques [31, 36, 41]. We also provide lower bounds on parameters used in our analysis, showing their tightness. In particular, the aforementioned $\\lambda$ slope parameter is of size $\\Theta({\\sqrt{y/\\log(y)}})$ in the case $p=1$ . The proof is conducted by an exact characterization of the tangent point between our linear lower envelope and the loss function. Since this requires balancing between $z$ and $\\log(z)$ and examining the $\\log(y!)$ function, further bounds rely on Stirling\u2019s approximation and the principal branch of the Lambert $W_{0}$ function. Recent bounds in [38] for the Lambert $W_{0}$ function imply our square root upper bound but only a cubic root lower bound. We thus significantly tighten their bound in an appropriate interval. This result may be of independent interest, since the Lambert function cannot be expressed in terms of elementary functions and has many important applications in various fields. As a result, we obtain a matching square root lower bound on $\\lambda$ in our context. ", "page_idx": 2}, {"type": "text", "text": "1.3 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Classic work on data subsampling started with linear $\\ell_{2}$ regression [16] and was extended to linear $\\ell_{p}$ regression [12]. More recently, the study continued with non-linear transformations such as in generalized linear regression models. The first guaranteed finite subsample bounds for logistic regression appeared in [36], while impossibility results for Poisson regression were given in [31]. Research on generalized linear models was continued for Probit regression [34]. Asymptotic properties of subsampling for generalized linear models, including for Poisson regre\u221assion, were studied in [3, 27]. A finite-sample size result is given in [3, Theorem 5] that exhibits $O({\\sqrt{d}})$ approximation error. [13] studied a sampling-based feature space reduction for a wide array of generalized linear models with additive errors. However, parts of their assumptions specifically do not apply to Poisson regression. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "2 Preliminaries and the Poisson pth-root-link model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Poisson regression models aim to predict a count variable $Y\\in\\ensuremath{\\mathbb{N}}_{0}$ using a generalized linear model with link function $h:\\mathbb{R}\\to\\mathbb{R}$ , i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\nh(\\mathbb{E}(Y\\mid x))=x\\beta\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $x=(1,x^{(1)},\\ldots,x^{(d-1)})\\in\\mathbb{R}^{d}$ is a row vector, and $\\beta\\in\\mathbb{R}^{d}$ is a column vector carrying the model parameters that in particular include an intercept $\\beta_{1}$ [30]. Common choices for $h$ are the canonical log-link $h(v)=\\ln(v)$ , the ID-link $h(v)=v$ , and the root-link $h(v)=v^{1/2}$ . The latter two can be cast into a general framework by introducing the $p$ th-root-link $h(v)\\,=\\,v^{1/p}$ , for any $\\mathbb{R}\\ni p\\geq1$ , where the ID-link and root-link correspond to $p\\in\\{1,2\\}$ [10]. ", "page_idx": 3}, {"type": "text", "text": "Subsampling for the log-link is not possible with the multiplicative $(1+\\varepsilon)$ error guarantees that we aim for, since it entails preserving the $\\exp(x\\beta)$ function [31]. We will also show impossibility results for the pth-root-link. However, we parameterize our analysis with a data-dependent parameter that reflects naturally how well the realized data distribution is captured by the Poisson model. This parameter is inspired from previous work [36]: we will refer to data $X,y$ as being $\\cdot_{\\rho}$ -complex\u2019, if there exists a $0<\\rho<\\infty$ such that denoting by $x_{j}$ the $j$ -th row of the data matrix $\\bar{X}\\in\\mathbb{R}^{n\\times\\bar{d}}$ and by $y_{j}$ the $j$ -th entry of the vector $y\\in\\ensuremath{\\mathbb{N}}_{0}^{n}$ , it holds that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\beta\\in\\mathbb{R}^{d}}\\frac{\\sum_{j=1}^{n}\\left|x_{j}\\beta\\right|^{p}}{\\sum_{j=1}^{n}\\left|x_{j}\\beta-y_{j}^{1/p}\\right|^{p}}\\leq\\rho.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We may interpret the parameter $\\rho$ as follows. The rate parameter of the predicted Poisson distribution is $\\mathbb{E}(Y\\mid x)\\stackrel{=}{=}(x\\beta)^{p}$ . Hence, the mean and variance of $Y_{j}$ given $x_{j}$ is $(x_{j}\\beta)^{p}$ , for each $j=1,\\dots,n$ . To obtain the maximum likelihood estimator of $\\beta$ , we seek $\\beta$ such that for every $j=1,\\dots,n,x$ $x_{j}\\beta$ is as close as possible to $y_{j}^{1/p}$ , since $y_{j}^{1/p}$ minimizes the $j$ th summand $g_{y_{j}}$ of the loss function specified in Equations (3) and (4) below. However, choosing $\\beta$ so that $|x_{j}\\beta-y_{j}^{1/p}|$ is small will imply that each summand in the numerator of Equation (2) will be close to $y_{j}^{1/p}$ . In this case, the variance of the $(Y_{j})_{j=1}^{n}$ will not be captured effectively by the Poisson model, and $\\rho$ will be large. Thus, smaller values of $\\rho$ , i.e., values of $\\rho$ that are closer to 1, indicate that the true data distribution is better captured by the Poisson model. Thus the $\\rho$ parameter in Equation (2) plays a similar role of quantifying model fti as the $\\mu_{w}(X)$ parameter from [36, Section 2]; see in particular the comments at the end of that section. ", "page_idx": 3}, {"type": "text", "text": "Assuming that the value of $\\rho$ is small allows us to use the proximity of the negative log-likelihood to $\\ell_{p}$ norms, together with some novel optimization ideas involving a shifted domain. This yields the first provable finite and sublinear subsample size with rigorous $(1+\\varepsilon)$ approximation guarantee. We focus on the special cases $p\\in\\{1,2\\}$ since they are the most popular (in fact the only practical) alternatives to the intractable log-link [10, 31]. The ID-link has been used in epidemiology [40, 29]. The root-link function has been applied to forecasting for queueing systems [39] and to account for misspecification bias in maximum likelihood estimation [17]. When the estimated mean count of the data is zero, then the canonical log-link causes problems that can be avoided by using the root-link; see e.g. [28, Section 5.4]. We also discuss in our lower bounds section other choices for $p$ , and show that for any natural number $p\\geq3$ the bound implied by our novel shifting idea must fail. This bound is crucial to obtain our final approximation, indicating that other methods would be required to tackle a generalization for $p\\geq3$ , if this is even possible. ", "page_idx": 3}, {"type": "text", "text": "Given parameters $\\beta$ and an input $x$ , the rate parameter of the predicted Poisson distribution is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mu:=\\mathbb{E}(Y\\mid x)=(x\\beta)^{p},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which corresponds to its mean and variance. Its probability mass function is $\\mathbb{P}(Y=y)=\\operatorname{Poisson}(y\\mid$ $\\begin{array}{r}{x{\\beta})=\\frac{\\mu^{y}e^{-\\mu}}{y!}=\\frac{(x{\\beta})^{p y}\\,e^{-(x{\\beta})^{p}}}{y!}}\\end{array}$ . Given a set of i.i.d. observations expressed as the rows $x_{i}$ of a data matrix $X\\in\\mathbb{R}^{n\\times d}$ with corresponding labels $y\\in\\ensuremath{\\mathbb{N}}_{0}^{n}$ we can obtain a maximum likelihood estimate of the parameter $\\beta$ by minimizing the negative log-likelihood, which takes the form ", "page_idx": 3}, {"type": "equation", "text": "$$\nf_{y}(X\\beta):=\\sum_{i=1}^{n}g_{y_{i}}(x_{i}\\beta)=\\sum_{i=1}^{n}(x_{i}\\beta)^{p}-p y_{i}\\ln(x_{i}\\beta)+\\ln(y_{i}!)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\ng_{y_{i}}(x_{i}\\beta):=(x_{i}\\beta)^{p}-p y_{i}\\log(x_{i}\\beta)+\\log(y_{i}!).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For any $p$ th-root-link, the loss function includes a $\\log(x/3)$ term, which restricts the feasible set to all $\\beta$ such that for all $x_{i},i\\in[n]$ it holds that $x_{i}\\beta>0$ . We note that for summands corresponding to $y_{i}=0$ , the function $g_{0}(z)$ simplifies to $g_{0}(z)=z^{p}>0$ with well-known properties of the $\\ell_{p}$ norm. We thus focus on summands $g_{y_{i}}$ for $y_{i}\\in\\mathbb{N}$ below. ", "page_idx": 4}, {"type": "text", "text": "For arbitrary $y\\in\\mathbb N$ , the function $g_{y}(z)\\,=\\,z^{p}-p y\\ln(z)+\\ln(y)$ on $\\mathbb{R}_{>0}$ is strictly convex with first and second derivatives $\\begin{array}{r}{g_{y}^{\\prime}(z)=p z^{p-1}-\\frac{p y}{z}}\\end{array}$ and $\\begin{array}{r}{g_{y}^{\\prime\\prime}(z)=p(p-1)z^{p-2}+\\frac{p y}{z^{2}}>0}\\end{array}$ respectively. Thus $g_{y}(z)$ decreases on the interval $z\\in(0,y^{1/p})$ , increases on $z\\in(y^{1/p},\\infty)$ , and has a unique minimizer at $z^{*}=y^{1/p}$ with corresponding value $\\begin{array}{r}{y-y\\log(y)+\\log(y!)\\;\\approx\\frac{1}{2}\\log(y)+\\Theta(1)\\geq1}\\end{array}$ by Stirling\u2019s approximation. We shall use the following lower bounds to capture the $y$ -dependence. ", "page_idx": 4}, {"type": "text", "text": "Lemma 2.1. It holds for all $z\\in\\mathbb{R}_{>0},p\\in[1,\\infty),y\\in\\mathbb{N}$ that ", "page_idx": 4}, {"type": "equation", "text": "$$\ng_{y}(z)=z^{p}-p y\\log(z)+\\log(y!)\\geq\\operatorname*{max}\\left\\{1,{\\frac{1}{3}}(1+p\\log(z))\\right\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The next two results bound the individual loss contributions from above and below by roughly a value of $z^{p}$ . For the lower bound, however, we note that as the value of $y$ grows, the loss function is translated polynomially towards larger $z$ values, since its minimum is attained at $z^{*}\\,=\\,y^{1/p}$ . However, by Lemma 2.1, and the properties above, the increase of $g_{y}(z^{*})$ is only logarithmic in $z^{*}$ , and thus also logarithmic in $y$ . Denote by $\\lambda$ the scaling parameter of the lower bound on $g_{y}$ given in Lemma 2.2. Then the logarithmic growth of $g_{y}(z^{*})$ implies that we would need $\\lambda\\approx y/\\log(y)$ . Unfortunately, the value of $\\lambda$ will affect the coreset size, which is undesirable, since it can become linear simply due to large values of $y$ . We thus require a sublinear dependence on $y_{\\mathrm{max}}$ , i.e., the largest value of $y$ presented in the data. To this end, we shift the lower envelope by the minimizer $\\bar{z}^{\\ast}=y^{1/p}$ . The value of $\\lambda$ can subsequently be bounded in a desirable way, but differs significantly depending on the value of $p$ : in the case $p=1$ we prove $\\lambda\\in O(\\sqrt{y/\\log(y)})$ to be sufficient, while the case $p\\,=\\,2$ even constant $\\lambda=1$ will suffice. In Section 6, we will show a separation by a superconstant and matching square root lower bound on the value of $\\lambda$ in the dominating case $p=1$ . ", "page_idx": 4}, {"type": "text", "text": "Lemma 2.2. For any $p\\geq1$ and $y\\in\\mathbb N$ it holds that $z^{p}\\ge g_{y}(z)\\,f o r\\,z>y^{1/p}$ . If $p=1$ , then for some $\\lambda\\in O(\\sqrt{y/\\log(y)})$ , it holds that $\\begin{array}{r}{g_{y}(z)\\ge\\frac{(z-y^{1/p})^{p}}{\\lambda}}\\end{array}$ for $z>y^{1/p}$ . ", "page_idx": 4}, {"type": "text", "text": "Lemma 2.3. Let $p\\geq2$ and $y\\in\\mathbb{N}$ , and $\\lambda=1$ . Then $\\begin{array}{r}{g_{y}(z)\\ge\\frac{(z-y^{1/p})^{p}}{\\lambda}}\\end{array}$ for $z>y^{1/p}$ . ", "page_idx": 4}, {"type": "text", "text": "3 Coreset construction ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We begin by summarizing some key aspects of the sensitivity framework. Formal definitions for the sensitivity framework (including sensitivities, the VC dimension, and the main subsampling theorem) are given in Appendix A. In the sensitivity framework, the goal is to obtain coresets using importance sampling techniques to approximate loss functions [26]. Given a loss function whose value depends on a collection of input points, the main idea of the framework is to measure the sensitivity of any input point in terms of its worst-case contribution to the loss function. More precisely, given a point $x_{j}$ , its sensitivity for the loss function of the form $\\begin{array}{r}{f(X\\eta)=\\sum_{j\\in[n]}g(x_{j}\\eta)}\\end{array}$ is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\sigma_{j}=\\operatorname*{sup}_{\\eta}\\frac{g(x_{j}\\eta)}{f(X\\eta)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The main subsampling theorem, Proposition A.5, combines the sensitivities together with the theory of VC dimension. The idea is to sample points according to probabilities that are proportional to the sensitivities, in order to create an appropriately reweighted subsample of the initial collection of points. Suppose the total sensitivity $\\begin{array}{r}{S=\\sum_{j\\in[n]}\\sigma_{j}}\\end{array}$ of the points and the VC dimension $\\Delta$ associated with a set system based on the summands $g(x_{i}\\eta)$ in the loss function $f(X\\eta)$ are bounded, and choose a failure probability $\\delta$ . If the subsample is of size $\\begin{array}{r}{k=O(\\frac{S}{\\varepsilon^{2}}(\\Delta\\log(S)+\\log(\\frac{1}{\\delta})))}\\end{array}$ , then it is in fact a $(1+\\varepsilon)$ -coreset [19] with probability at least $1-\\delta$ . Unfortunately, it is often just as difficult to compute the exact sensitivities as it is to solve the original problem. The remedy is to exploit the fact that one does not need the exact sensitivities themselves: it suffices to use any upper bounds on the sensitivities, provided that the upper bounds are not too loose, since a larger upper bound will lead to a larger coreset. Thus, we can reduce the task of coreset construction to two tasks: control of the VC dimension and sensitivity estimation of the loss function. This is handled in the following sections. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "3.1 Bounding the VC dimension ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We prove two different bounds on the VC dimension. The first one is a simple quadratic bound of $O(\\dot{d}^{2})$ . Our proof in the appendix simply counts the number of operations required to compare the loss function to a given threshold. The VC dimension bound then follows from a standard result in the context of bounding the VC dimension of neural networks [4, Thm. 8.14]. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.1. The VC dimension of the range space associated with the class of Poisson loss functions as in Equation (3) is bounded by $\\Delta(\\mathfrak{R}_{\\mathcal{F}^{\\ast}})\\le O(d^{2})$ . ", "page_idx": 5}, {"type": "text", "text": "It is noteworthy that the quadratic dependence is implied already only from one single application of the exponential function, which is sufficient but likely not necessary in our context. Hence, we show in the remainder a more refined near-linear bound of $\\begin{array}{r}{O(\\frac{d}{\\varepsilon}\\log(n)\\log(y_{\\mathrm{max}}))=\\tilde{O}(d/\\varepsilon)}\\end{array}$ , while keeping the dependence on other input parameters\u2014namely, on $n$ and $y_{\\mathrm{max}}$ \u2014logarithmic. ", "page_idx": 5}, {"type": "text", "text": "To this end, we subdivide the set of input functions into groups of growing values of their response parameter $y_{i}$ , and of their sensitivity $\\varsigma_{i}$ in a geometric progression. By rounding these values in each group to their next power in the geometric progression, we obtain disjoint sets of functions that closely approximate the original weighted loss functions, and whose VC-dimension can be bounded in $O(\\bar{d})$ . Since there is only a logarithmic number of groups in both progressions, we obtain the claimed VC dimension bound. ", "page_idx": 5}, {"type": "text", "text": "Recall the responses $(y_{i})_{i}$ are nonnegative integers. We define their largest value (for a given input) to be $y_{\\operatorname*{max}}\\,=\\,\\operatorname*{max}\\{y_{i}\\mid i\\,\\in\\,[n]\\}$ . Therefore, they are naturally bounded between $0\\leq y_{i}\\leq y_{\\mathrm{max}}$ for all $i\\in[n]$ and there are at most $y_{\\mathrm{{max}}}+1$ different values of $y_{i}$ . Also note that the sensitivity values are naturally bounded by $0\\le\\varsigma_{i}\\le1$ , but since they are continuous, they must be bounded away from 0 in order for the geometric progression to end in a finite (logarithmic) number of steps. If we increase each sensitivity by $1/n$ then the total sensitivity grows only by a constant, since $\\begin{array}{r}{S^{\\prime}=\\sum_{i\\in[n]}(\\varsigma_{i}+1/n)=S\\,\\overset{.}{+}1}\\end{array}$ . For these reasons, we can thus assume that $1/n\\leq\\varsigma_{i}\\leq1$ . ", "page_idx": 5}, {"type": "text", "text": "Now, we would like to increase the sensitivities even more to their next power of 2, which will clearly increase the total sensitivity by no more than ", "page_idx": 5}, {"type": "equation", "text": "$$\n2S^{\\prime}=2S+2\\leq3S.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "By our above upper and lower bounds on the sensitivities, this will result in ${\\cal O}(\\log(n))$ groups, where in each group all sensitivities are equal. Note that in Proposition A.5, the reweighting of points depends only on fixed terms except for the sensitivities. Thus, in each group, all weights are constant. We have the following bound that applies to each group and for any fixed $y\\in\\ensuremath{\\mathbb{N}}_{0}$ . ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.2. The VC dimension of the range space induced by the set of functions $\\mathcal{F}_{c}=\\{g_{i}(\\beta)=$ $c\\cdot g_{y_{i}}(x_{i}\\beta)\\mid i\\in[n]\\}$ with equal weight $c\\in\\mathbb{R}_{\\geq0}$ , and equal $y_{i}=y\\in\\mathbb{N}_{0}$ for all $i\\in[n]$ satisfies $\\Delta\\left(\\dot{\\Re}_{\\mathcal{F}_{c}}\\right)=O(d)$ . ", "page_idx": 5}, {"type": "text", "text": "For the values of $y_{i}$ , we proceed in a very similar way. However, unlike the sensitivities, a constant approximation as in Equation (5) provided by powers of 2 will not suffice. Instead, we group the values of $y_{i}$ into powers of $(1+\\varepsilon)$ and round all $y_{i}$ that belong to the same group to the next larger power. We argue that this preserves a $(1\\pm O(\\varepsilon))$ approximation to the original loss function. Indeed, this claim even holds for each summand $g_{y_{i}}$ if $y_{i}$ is large enough, as the following lemma shows. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.3. Let $y\\geq8$ and $1\\geq\\varepsilon>0$ . Let $y<y^{\\prime}\\leq(1+\\varepsilon)y_{i}$ . Then for arbitrary $z>0$ it holds that $(1-3\\varepsilon)g_{y}(z)\\leq g_{y^{\\prime}}(z)\\leq(1+3\\varepsilon)g_{y}(z)$ . ", "page_idx": 5}, {"type": "text", "text": "A direct consequence of Lemma 3.3 is that any coreset for the rounded version is a coreset for the original loss function and vice versa, up to an additional $(1\\pm O(\\varepsilon))$ error. We can therefore work with the rounded version of the loss function, which yields better bounds for the VC dimension. ", "page_idx": 5}, {"type": "text", "text": "A general theorem for bounding the VC dimension of the union or intersection of $t$ range spaces, each of bounded VC dimension at most $D$ , was given in [6]. Their result yields $O(t D\\log(t))$ . Here, we give a bound of $O(t D)$ for the special case that the range spaces are disjoint2. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.4. Let $\\mathcal{F}$ be any family of functions, and let $F_{1},\\dots,F_{t}\\subseteq{\\mathcal{F}}$ be nonempty sets that form a partition of $\\mathcal{F}$ , i.e., their disjoint union satisfies $\\begin{array}{r}{\\dot{\\bigcup_{i\\in[t]}}=\\mathcal{F}}\\end{array}$ . Let the VC dimension of the range space induced by $F_{i}$ be bounded by $D$ for all $i\\in[t]$ . Then the VC dimension of the range space induced by $\\mathcal{F}$ satisfies $\\Delta\\left(\\Re_{\\mathcal{F}}\\right)\\leq t D$ . ", "page_idx": 6}, {"type": "text", "text": "As a result of our previous partition into groups and the $O(d)$ bound on each group, we obtain the desired result. ", "page_idx": 6}, {"type": "text", "text": "Lemma 3.5. Let $\\mathcal{F}$ be the set of functions in the Poisson model. We can round and group the values of $y_{i}$ and the associated sensitivities $\\varsigma_{i}$ to obtain $\\mathcal{F}^{*}$ such that each function in ${\\mathcal{F}}^{*}$ is weighted by $0<w_{i}\\in W:=\\{u_{1},\\ldots,u_{t}\\}$ for $t\\in O(\\varepsilon^{-1}\\log(n)\\cdot\\log(y_{\\operatorname*{max}}))$ . The range space induced by $\\mathcal{F}^{*}$ satisfies $\\begin{array}{r}{\\Delta\\left(\\Re_{\\mathcal{F}^{*}}\\right)\\leq O(\\frac{d}{\\varepsilon}\\log(n)\\log(y_{\\operatorname*{max}}))}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "As a direct corollary of Lemmas 3.1 and 3.5, we obtain the following combined bound. ", "page_idx": 6}, {"type": "text", "text": "Corollary 3.6. The VC dimension $\\Delta(\\mathfrak{R}_{\\mathcal{F}^{\\ast}})$ of the range space associated with the class of Poisson loss functions as in Equation (3) is bounded by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Delta\\left(\\Re_{\\mathcal{F}^{*}}\\right)\\leq O\\left(d\\cdot\\operatorname*{min}\\left\\{d,\\frac{\\log(n)\\log(y_{\\operatorname*{max}})}{\\varepsilon}\\right\\}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "3.2 Bounding the sensitivities ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We split the loss function into two parts: ", "page_idx": 6}, {"type": "equation", "text": "$$\nf_{y}(X\\beta):=\\sum_{i:x_{i}\\beta\\leq\\eta}g_{y_{i}}(x_{i}\\beta)+\\sum_{i:x_{i}\\beta>\\eta}g_{y_{i}}(x_{i}\\beta)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We will ignore the first sum, since we will see later in the main approximation of Section 4, that by shifting the hyperplanes defined by parameter vectors in the solution space, everything can be shifted to the second sum where $x_{i}\\beta\\geq\\eta$ . In this way, we preserve a $\\left(1+\\varepsilon\\right)$ -approximation, if $\\eta=\\Theta(\\varepsilon)$ is small enough. We note that this shifting technique still requires the extreme points on the convex hull $\\operatorname{Ext}(X)$ to be maintained; we address this issue in Section 5. We will focus on bounding the sensitivities for the remaining points with $x_{i}\\beta\\geq\\eta$ . In the next lemma we require the concept of a well-conditioned-basis [12]. Let $q\\in\\{2,\\infty\\}$ denote the dual norm of $p\\in\\{1,2\\}$ , respectively. We say that $U$ is a \u2018 $(\\alpha,\\gamma,p)$ -well-conditioned basis\u2019 for the column span of $X=U R$ if $U\\,\\in\\,\\mathbb{R}^{n\\times d}$ satisfies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|U\\|_{p}\\leq\\alpha,\\quad,\\forall z\\in\\mathbb{R}^{d}:\\|z\\|_{q}\\leq\\gamma\\|U z\\|_{p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Lemma 3.7. Let $X\\in\\mathbb{R}^{n\\times d},y\\in\\mathbb{N}_{0}^{n}$ be a $\\rho$ -complex dataset, i.e., Equation (2) holds. Let $p\\in\\{1,2\\}$ . Let $\\lambda\\geq1$ be the slope parameter from either Lemma 2.2 or Lemma 2.3 depending on the value of $p$ . Let $\\gamma$ be a conditioning parameter and $\\eta>0$ be arbitrary. Then the sensitivity for each $x_{i}$ with $x_{i}\\beta\\,>\\,\\eta$ is bounded by $\\varsigma_{i}\\,\\leq\\,\\lambda\\rho\\gamma^{p}\\|U_{i}\\|_{p}^{p}+2/n$ . Their total sensitivity is bounded by $\\mathfrak{S}\\le$ $O\\left(\\rho d\\sqrt{y_{\\mathrm{max}}/\\mathrm{log}(y_{\\mathrm{max}})}+\\mathrm{log}\\log(1/\\eta)\\right)f o r\\,p=1,$ and $\\mathfrak{S}\\le O(\\rho d+\\log{(y_{\\operatorname*{max}})}+\\log\\log(1/\\eta))$ for $p=2$ . ", "page_idx": 6}, {"type": "text", "text": "3.3 Combining the results into the sensitivity framework ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Putting all steps (VC dimension, total sensitivity) together into the sensitivity framework, Proposition A.5 yields the following computational result, where in particular $\\ell_{p}$ well-conditioning is established constructively using $\\ell_{2}$ subspace embeddings [9], resp. using $\\ell_{1}$ spanning sets [44]. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.8. Let $X\\,\\in\\,\\mathbb{R}^{n\\times d},y\\,\\in\\,\\mathbb{N}_{0}^{n}$ be a $\\rho$ -complex dataset, i.e., Equation (2) holds. We can compute a weighted coreset $(K,w)\\in\\mathbb{R}^{k\\times d}\\times\\mathbb{R}_{>0}^{k}$ for the pth-root-link Poisson regression problem with $p\\,\\in\\,\\{1,2\\}$ on $D(\\eta)\\,:=\\,\\{\\beta\\,\\in\\,\\mathbb{R}^{d}\\;:\\;\\forall i$ , $x_{i}\\beta\\,>\\,\\eta\\}$ . The size of the coreset is bounded by $k=\\tilde{O}(\\varepsilon^{-2}d\\cdot\\operatorname*{min}\\{d,\\varepsilon^{-1}\\log(n)\\log(y_{\\mathrm{max}})\\}\\cdot m)$ , where ", "page_idx": 6}, {"type": "equation", "text": "$$\nm=\\left\\{\\begin{array}{l l}{\\rho d\\log\\log(d)\\sqrt{y_{\\operatorname*{max}}/\\log(y_{\\operatorname*{max}})}+\\log\\log(1/\\eta)}&{p=1}\\\\ {\\rho d+\\log\\left(y_{\\operatorname*{max}}\\right)+\\log\\log(1/\\eta)}&{p=2.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "4 Main approximation result ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In the previous section, we developed a coreset for the sum of individual losses where $x_{i}\\beta\\geq\\eta$ , i.e., for the second sum of Equation (6). Since we cannot bound the remaining first sum where $x_{i}{\\beta}<\\eta$ , we choose to simply avoid it instead, by shifting each solution by $\\eta$ . Define for any $\\eta\\geq0$ ", "page_idx": 7}, {"type": "equation", "text": "$$\nD(\\eta):=\\{\\beta\\in\\mathbb{R}^{d}\\ :\\ \\forall i,\\ x_{i}\\beta>\\eta\\}.^{3}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The original domain of optimization is $D(0)$ and the shifted domain is $D(\\eta)$ . Shifting the domain does not remove the need to store the extreme points on the convex hull of the input, since we need these points to determine the feasible domain $D(\\eta)$ during optimization. However, shifting removes the need to approximate the first sum in Equation (6) over points with unbounded sensitivity located in a small slab of width $\\eta$ within the convex hull. ", "page_idx": 7}, {"type": "text", "text": "Since we have a $(1\\pm\\varepsilon)$ coreset for $D(\\eta)$ , we need to find a suitable choice for $\\eta$ and show that the optimizer $(\\beta^{\\prime})^{*}\\in D(\\eta)$ is a $(1+O(\\varepsilon))$ approximation for the optimizer in the original domain $\\beta^{*}\\in D(0)$ . We thus define ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\beta}^{*}:=\\operatorname*{argmin}_{\\beta^{\\prime}\\in D(\\eta)}f(X\\beta^{\\prime}),\\quad\\beta^{*}:=\\operatorname*{argmin}_{\\beta\\in D(0)}f(X\\beta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Lemma 4.1. It holds for sufficiently small $\\eta>0$ that ", "page_idx": 7}, {"type": "equation", "text": "$$\nf(X{\\boldsymbol{\\beta}}^{*})\\leq f(X{\\boldsymbol{\\tilde{\\beta}}}^{*})\\leq(1+O(\\eta))f(X{\\boldsymbol{\\beta}}^{*}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Now we combine the preceding results, namely the coreset and the domain shifting bound, for our main theorem. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.2. Let $\\varepsilon\\,\\in\\,(0,1/14)$ . Let $(C,w)$ be a coreset according to Theorem 3.8. Let ${\\tilde{\\beta}}:=$ $\\begin{array}{r}{\\operatorname*{argmin}_{\\beta\\in D(\\varepsilon)}f_{w}(C\\beta),\\,\\beta^{*}:=\\operatorname*{argmin}_{\\beta\\in D(0)}f(X\\beta).}\\end{array}$ . Then ", "page_idx": 7}, {"type": "equation", "text": "$$\nf(X{\\beta}^{*})\\leq f(X{\\tilde{\\beta}})\\leq(1+\\varepsilon)f(X{\\beta}^{*}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "5 Extreme points on the convex hull ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In the previous sections, we argued that for obtaining a $\\left(1+\\varepsilon\\right)$ approximation, it suffices to calculate a coreset that is valid for all $\\beta\\in D(\\varepsilon)$ , and then to minimize our loss function over $\\beta\\in D(\\varepsilon)$ using the coreset instead of the full data. Note that for the optimizer to stay in the feasible set $D(\\varepsilon)$ , one must store the extreme points on the convex hull of the input points denoted by $\\operatorname{Ext}(X)$ . This is true even when $\\varepsilon=0$ , i.e., even when the original function is considered and no shifting occurs. Note that there exist datasets such as our \u2018points on a circle\u2019 example considered in the lower bounds of Section 6, such that $|\\mathrm{Ext}(X)|=n$ . ", "page_idx": 7}, {"type": "text", "text": "There are several ways to either characterize $\\vert\\mathrm{Ext}(X)\\vert$ for typical inputs in a sublinear way, or to approximate the convex hull by a smaller sublinear subset, called an $\\varepsilon$ -kernel, with an error of at most $\\varepsilon$ . Since these methods are usually relative to the diameter of the data, and since we need an additive error for our shifting approach, we first normalize the data to be within the unit ball. We note that this does not change the value of the loss function, since this involves scaling by a fixed value $C\\geq1$ , and since we can use the fact that ", "page_idx": 7}, {"type": "equation", "text": "$$\nf(X\\beta)=f\\left(\\left({\\frac{X}{C}}\\right)(C\\beta)\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "as well as the one-to-one correspondence between any $\\beta$ and $C\\beta$ . Thus, we can run the algorithm on the rescaled data, obtain a good or optimal solution $C\\beta^{*}$ , and rescale $C\\beta^{*}$ to obtain the corresponding $\\beta^{*}$ . Rescaling steps such as normalizing data to zero mean and unit variance are standard in statistical data analysis [22]. ", "page_idx": 7}, {"type": "text", "text": "Smoothed complexity of the convex hull Instead of the worst case $\\vert\\mathrm{Ext}(X)\\vert$ over all possible datasets $X\\in\\mathbb{R}^{n\\times d}$ , in smoothed complexity we consider ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{X\\in\\mathbb{R}^{n\\times d}}\\mathbb{E}_{\\Xi\\sim\\mathcal{N}}|\\mathrm{Ext}(X+\\Xi)|,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\mathcal{N}$ denotes the distribution over matrices $\\Xi$ with the same dimensions as $X$ and with i.i.d. Gaussian entries with mean 0 and variance $\\sigma^{2}$ , i.e., $\\Xi_{i,j}\\sim N(0,\\sigma^{2})$ for all $i\\in[n],j\\in[d]$ [11]. This is motivated by the fact that many datasets are recorded with measurement errors, which can often be assumed to be Gaussian. Specifically for the convex hull, [11, Chapter 4] showed that for normalized data in the unit cube, the supremum above is bounded by ", "page_idx": 8}, {"type": "equation", "text": "$$\nO\\left(\\frac{\\log^{\\frac{3}{2}d-1}(n)}{\\sigma^{d}}+\\log^{d-1}(n)\\right),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "which is sublinear in $n$ , though exponential in $d$ . ", "page_idx": 8}, {"type": "text", "text": "$\\varepsilon$ -kernels The purpose of $\\varepsilon$ -kernels is to approximate the extent of a point set up to an error of $1-\\varepsilon$ for any direction in $\\mathbb{R}^{d}$ based on a subset of the data. They were introduced by [1] and improved by [8] to optimal $\\Theta(1/\\varepsilon^{(d-1)/2})$ size, see the survey [2]. Since we assume our data to be normalized within the unit ball, this translates to an additive error that is bounded by $\\varepsilon$ . Thus, the boundary of the convex hull of the $\\varepsilon$ -kernel can be smaller than the boundary of the original convex hull by at most $\\varepsilon$ . ", "page_idx": 8}, {"type": "text", "text": "Improvement for structured data It is known that $\\varepsilon$ -kernels can have size up to $\\Omega(1/\\varepsilon^{(d-1)/2})$ in the worst case. Beyond the worst case, the structure of the given data may allow for much smaller $\\varepsilon$ -kernels to exist, even in high dimensions. Motivated by this, [5] developed a \u2018greedy clustering\u2019 approach that produces an $\\varepsilon$ -kernel of size $O(k_{\\mathrm{min}}/\\varepsilon^{2})$ , where $k_{\\mathrm{min}}\\,=\\,k_{\\mathrm{min}}(X,\\varepsilon)$ denotes the smallest possible size of a subset that gives the required $\\varepsilon$ -kernel guarantee for the original input $X$ . ", "page_idx": 8}, {"type": "text", "text": "Using $\\varepsilon$ -kernels for optimizing Poisson models The above options include the possibility to calculate the convex hull and rely on the sublinear smoothed complexity bound, if this is reasonable in the given context. Otherwise, if we have access to any of the above $\\varepsilon$ -kernel constructions, we shift the hyperplane away from the approximation of the convex hull, provided by the $\\varepsilon$ -kernel, by a distance of $\\eta=2\\varepsilon$ instead of just $\\varepsilon$ . As a result, the hyperplane will be shifted away from the original convex hull by at least $\\varepsilon$ and at most $2\\varepsilon$ . Thus, we still compute a $(1+\\Theta(\\varepsilon))$ approximation in this way. ", "page_idx": 8}, {"type": "text", "text": "6 Lower bounds ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We complement our coreset constructions for the variants of the Poisson model by a series of lower bounds. Our bounds in Lemma 6.1 and Lemma 6.2 are specifically for the $p$ th-root-link, and not for the log-link, which was studied in [31]. We use similar constructions of the bad dataset as those used frequently in previous literature, e.g., in [31, Theorem 6], and in [36, 41, 23, 21]. However, each of these references require specific adaptations to the respective loss function that do not directly apply to our setting. Our arguments are thus adapted to the Poisson pth-root-link model to show that it does not admit sublinear coresets without imposing assumptions on the data or restricting the model. ", "page_idx": 8}, {"type": "text", "text": "The hard instance consists of $n$ equidistant points on the unit circle. Recall that in the Poisson regression formulation, every point has an additional intercept coordinate which is 1, and the corresponding parameter of $\\beta$ determines the affine translation; see Section 2. For every $i\\in[n]$ , let $\\begin{array}{r}{x_{i}=(\\bar{1},\\cos(\\frac{\\bar{2}\\bar{\\pi}i}{n}),\\sin(\\frac{2\\pi i}{n}))}\\end{array}$ , and $y_{i}=1$ . Recall that any feasible $\\beta$ must satisfy $x_{i}\\beta>0,\\forall i\\in[n]$ The hyperplane parameterized by $\\beta$ is thus always outside the point set and points in the direction of the (xi)i\u2208[n]. ", "page_idx": 8}, {"type": "text", "text": "The idea for showing that any point has sensitivity 1 is that if $\\beta$ points to the center of the point set, and the hyperplane is translated to just \u2018touch\u2019 point $x_{i}$ , then $x_{i}\\beta$ is arbitrarily close to 0, implying that the cost is arbitrarily large. All other points are sufficiently bounded away from the hyperplane, but also not too far away, so their cost is bounded. This implies that the sensitivity of point $x_{i}$ can be made arbitrarily close to 1. By symmetry of our construction, this holds for any point. ", "page_idx": 8}, {"type": "text", "text": "Lemma 6.1. Consider a number $n\\geq8$ of points equidistant on a unit circle in a 2-dimensional affine subspace embedded in $\\mathbb{R}^{d},d\\geq3$ , each with label $y_{i}=1$ . Then the sensitivity of each point for the Poisson model with pth-root-link for $p\\in\\{1,2\\}$ is arbitrarily close to 1. Consequently, any coreset for the Poisson regression model must comprise all $\\Omega(n)$ input points. ", "page_idx": 8}, {"type": "text", "text": "Below, we prove an even more interesting statement, i.e., that no compression is possible below $\\Omega(n)$ bits. While this statement appears to give a weaker $\\Omega(n/\\log(n))$ bound against coresets, it in fact gives a stronger bound in some sense. This is because the bound holds against any possible data reduction algorithm and against any data structure that answers negative log-likelihood queries to within a small error, independent of what (possibly randomized) operations the data reduction algorithm performs. For example, the algorithm could subsample, it could select input points as coreset constructions do, or it could take linear combinations as in linear sketching. More generally, the bound holds against any sort of bit encoding that represents the reduced data. The reduction is based on the same data example of equidistant points on a circle embedded in $d\\geq3$ dimensions. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Lemma 6.2. Let $\\Sigma_{D}$ be a data structure for $D=[X,y]\\in\\mathbb{R}^{n\\times d}\\times\\mathbb{R}^{n},$ $d\\geq3,$ , that approximates negative log-likelihood queries $\\Sigma_{D}(\\beta)$ for Poisson regression with the pth-root-link for $p\\in\\{1,2\\}$ , such that for some $\\varphi\\geq1$ it holds that ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\forall\\beta\\in\\mathbb{R}^{d}:f(X\\beta)\\leq\\Sigma_{D}(\\beta)\\leq\\varphi\\cdot f(X\\beta).\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "I $\\begin{array}{r}{\\gamma_{\\varphi}<\\frac{n}{8\\log(n)}}\\end{array}$ then $\\Sigma_{D}$ requires $\\Omega(n)$ bits of memory. ", "page_idx": 9}, {"type": "text", "text": "Next, we prove that the parts of our analysis that are specific to the Poisson model are tight. In particular, the scale parameter $\\lambda$ is only a constant for $p=2$ , but in the case $p=1$ we only have a $\\bar{\\sqrt{y/\\log(y)}}$ upper bound. Since $y_{\\mathrm{max}}$ , i.e., the largest $y$ , can potentially be very large, one may ask if we can do better. Our next result exactly characterizes the smallest possible parameter $\\lambda$ such that our linear lower envelope approximation is tangent to the actual loss function, in order to show a tight $\\lambda=\\Theta(\\sqrt{y/\\log(y)})$ bound. This characterization of tangent points relies on properties of the principal branch $W_{0}$ of the Lambert function, which is defined by the equation ", "page_idx": 9}, {"type": "equation", "text": "$$\nW_{0}(x)e^{W_{0}(x)}=x,\\;\\mathrm{for}\\;x\\ge-1/e.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "The only approximations that the final bounds obey follow from Stirling\u2019s approximation and from the following upper bound on the Lambert function $W_{0}$ by a square root function. This upper bound improves the recent cubic root bound of [38, Theorem 3.2] within a small region, and is crucial to obtaining a tight square root (rather than cubic root) lower bound on $\\lambda$ . ", "page_idx": 9}, {"type": "text", "text": "Lemma 6.3. For all $x\\in[-1/e,0)$ , it holds that $W_{0}(x)\\leq\\sqrt{2(1+e x)}-1.$ . ", "page_idx": 9}, {"type": "text", "text": "The above bound on the $W_{0}$ function is novel and may be of independent interest. In our context, it allows us to prove the following tight bound on $\\lambda$ that resembles the same asymptotic upper bound as in Lemma 2.2, and establishes a matching lower bound. ", "page_idx": 9}, {"type": "text", "text": "Lemma 6.4. Let $y\\,\\in\\,\\mathbb{N}$ be arbitrary, $p\\,=\\,1$ , and $\\tau\\,=\\,y^{1/p}$ in the definition Equation (4) of $g_{y}$ $\\begin{array}{r}{h_{\\lambda}(z):=\\,\\frac{(z-y^{1/p})^{p}}{\\lambda}}\\end{array}$ $z\\,>\\,0$ ,  iTnh ewnh $g_{y}$ caansde $h_{\\lambda}$ e  aurnei qtuaen tgaenngt etnot  epaoicnht  iost $i f$ $\\begin{array}{r}{\\lambda=\\lambda^{*}(y)=(W_{0}(\\frac{-y}{(y!)^{1/y}\\exp(2)})+1)^{-1},}\\end{array}$ $\\begin{array}{r}{z^{\\ast}(y)=\\frac{y\\lambda^{\\ast}(y)}{\\lambda^{\\ast}(y)-1}}\\end{array}$ In addition, $\\lambda^{*}(y)=\\Theta(\\sqrt{y_{\\operatorname*{max}}/\\log(y_{\\operatorname*{max}})})$ . ", "page_idx": 9}, {"type": "text", "text": "The next lemma shows for $p\\,\\geq\\,3$ that there exists no constant $C$ such that the domain shifting approach that we developed in Section 4 yields a $1+C\\varepsilon$ error bound. As this error bound is a crucial sufficient condition for our main approximation results given in Lemma 4.1 and Theorem 4.2 to hold, the lemma suggests that different techniques may be needed. It indicates a limitation of our analysis to the most common values $p\\in\\{1,2\\}$ , which are the main parameterizations considered in our work. ", "page_idx": 9}, {"type": "text", "text": "Lemma 6.5. Let $\\mathbf{\\Delta}\\cdot p\\in\\mathbb{N},\\,p\\geq3$ . Then there does not exist an absolute constant $C\\geq0$ such that for all sufficiently small $\\eta>0$ and for all $\\beta\\in D(0)$ , $\\beta^{\\prime}:=\\beta+\\eta e_{1}\\in D(\\eta)$ satisfies ", "page_idx": 9}, {"type": "equation", "text": "$$\nf(X\\beta^{\\prime})\\le f(X\\beta)+\\eta^{p}n+\\eta C f(X\\beta).\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "7 Concluding remarks ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In Section 1.3, we recalled that previous finite sample size results had either unbounded or $O({\\sqrt{d}})$ error instead of our $(1+\\varepsilon)$ approximation. Our lower bounds on the parameters, together with linear VC dimension, linear sensitivity, and linear $\\rho$ dependence in our main quantitative bounds of Theorem 3.8, leave no room for improvement (up to polylogarithmic factors) if one uses a black-box application of the sensitivity framework. We remark that recent improvements on $\\ell_{p}$ sensitivity sampling [33] suggest that the dimension dependence can be improved to linear as well. Exploiting the fact that our coreset gives a guarantee for all $\\beta~\\in~D(\\varepsilon)$ , it would be interesting to extend the statistical treatment to the Bayesian setting, inferring the distribution of parameters over this (sub-)domain, as was recently accomplished in the case of logistic and probit regression [14]. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors thank the reviewers for their constructive feedback and discussions. We also thank Simon Omlor for valuable discussions that inspired the domain shift idea and Tim Novak for help with experiments. The research of HCL has been partially funded by the DFG \u2014 Project-ID 318763901 \u2014 SFB1294. AM was mainly supported by the German Research Foundation (DFG) \u2014 grant MU 4662/2-1 (535889065) and by the TU Dortmund - Center for Data Science and Simulation (DoDaS). AM acknowledges additional travel funding by the University of Cologne. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Pankaj K. Agarwal, Sariel Har-Peled, and Kasturi R. Varadarajan. Approximating extent measures of points. J. ACM, 51(4):606\u2013635, 2004.   \n[2] Pankaj K Agarwal, Sariel Har-Peled, Kasturi R Varadarajan, et al. Geometric approximation via coresets. Combinatorial and computational geometry, 52(1):1\u201330, 2005.   \n[3] Mingyao Ai, Jun Yu, Huiming Zhang, and HaiYing Wang. Optimal subsampling algorithms for Big Data regressions. Statistica Sinica, 31(2):pp. 749\u2013772, 2021.   \n[4] Martin Anthony and Peter L. Bartlett. Neural Network Learning - Theoretical Foundations. Cambridge University Press, 2002.   \n[5] Avrim Blum, Sariel Har-Peled, and Benjamin Raichel. Sparse approximation via generating point sets. ACM Trans. Algorithms, 15(3):32:1\u201332:16, 2019.   \n[6] Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. Learnability and the Vapnik-Chervonenkis dimension. J. ACM, 36(4):929\u2013965, 1989.   \n[7] Vladimir Braverman, Dan Feldman, and Harry Lang. New frameworks for offline and streaming coreset constructions. CoRR, abs/1612.00889, 2016.   \n[8] Timothy M Chan. Faster core-set constructions and data stream algorithms in fixed dimensions. In Proceedings of the twentieth annual symposium on Computational geometry, pages 152\u2013159, 2004.   \n[9] Kenneth L Clarkson and David P Woodruff. Low-rank approximation and regression in input sparsity time. Journal of the ACM, 63(6):1\u201345, 2017.   \n[10] W. G. Cochran. The analysis of variance when experimental errors follow the Poisson or binomial laws. The Annals of Mathematical Statistics, 11(3):335\u2013347, 1940.   \n[11] Valentina Damerow. Average and smoothed complexity of geometric structures. PhD thesis, University of Paderborn, 2006.   \n[12] Anirban Dasgupta, Petros Drineas, Boulos Harb, Ravi Kumar, and Michael W. Mahoney. Sampling algorithms and coresets for $\\ell_{p}$ regression. SIAM J. Comput., 38(5):2060\u20132078, 2009.   \n[13] Gregory Dexter, Rajiv Khanna, Jawad Raheel, and Petros Drineas. Feature space sketching for logistic regression. CoRR, abs/2303.14284, 2023.   \n[14] Zeyu Ding, Simon Omlor, Katja Ickstadt, and Alexander Munteanu. Scalable Bayesian p-generalized probit and logistic regression. Advances in Data Analysis and Classification, pages 1\u201335, 2024.   \n[15] Petros Drineas, Malik Magdon-Ismail, Michael W. Mahoney, and David P. Woodruff. Fast approximation of matrix coherence and statistical leverage. J. Mach. Learn. Res., 13:3475\u20133506, 2012.   \n[16] Petros Drineas, Michael W. Mahoney, and S. Muthukrishnan. Sampling algorithms for $l_{2}$ regression and applications. In Proceedings of the Seventeenth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1127\u20131136, 2006.   \n[17] Bradley Efron. Poisson overdispersion estimates based on the method of asymmetric maximum likelihood. Journal of the American Statistical Association, 87(417):98\u2013107, 1992.   \n[18] Dan Feldman and Michael Langberg. A unified framework for approximating and clustering data. In Proceedings of the 43rd ACM Symposium on Theory of Computing (STOC), pages 569\u2013578. ACM, 2011.   \n[19] Dan Feldman, Melanie Schmidt, and Christian Sohler. Turning Big Data into tiny data: Constant-size coresets for $k$ -means, PCA, and projective clustering. SIAM J. Comput., 49(3):601\u2013657, 2020.   \n[20] Susanne Frick, Amer Krivosija, and Alexander Munteanu. Scalable learning of item response theory models. In Proceedings of The 27th International Conference on Artificial Intelligence and Statistics (AISTATS), pages 1234\u20131242, 2024.   \n[21] Sariel Har-Peled, Dan Roth, and Dav Zimak. Maximum Margin Coresets for Active and Noise Tolerant Learning. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), page 836\u2013841, 2007.   \n[22] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning. Springer Series in Statistics. Springer New York Inc., 2001.   \n[23] Jonathan Huggins, Trevor Campbell, and Tamara Broderick. Coresets for scalable Bayesian logistic regression. In Advances in Neural Information Processing Systems (NIPS), volume 29, 2016.   \n[24] Michael J. Kearns and Umesh V. Vazirani. An Introduction to Computational Learning Theory. MIT Press, 1994.   \n[25] Ilan Kremer, Noam Nisan, and Dana Ron. On randomized one-round communication complexity. Computational Complexity, 8(1):21\u201349, 1999.   \n[26] Michael Langberg and Leonard J. Schulman. Universal epsilon-approximators for integrals. In Proceedings of the 21st Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 598\u2013607, 2010.   \n[27] JooChul Lee, Elizabeth D. Schifano, and HaiYing Wang. Fast optimal subsampling probability approximation for generalized linear models. Econometrics and Statistics, 29:224\u2013237, 2024.   \n[28] John Maindonald and W. John Braun. Data analysis and graphics using R \u2013 an example-based approach, volume 10 of Camb. Ser. Stat. Probab. Math. Cambridge: Cambridge University Press, 3rd edition, 2010.   \n[29] Ian C. Marschner. Stable computation of maximum likelihood estimates in identity link Poisson regression. Journal of Computational and Graphical Statistics, 19(3):666\u2013683, 2010.   \n[30] P. McCullagh and J. A. Nelder. Generalized Linear Models. Chapman & Hall, London, 1989.   \n[31] Alejandro Molina, Alexander Munteanu, and Kristian Kersting. Core dependency networks. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI), pages 3820\u20133827, 2018.   \n[32] Alexander Munteanu. Coresets and sketches for regression problems on data streams and distributed data. In Machine Learning under Resource Constraints, Volume 1 - Fundamentals, pages 85\u201398. De Gruyter, 2023.   \n[33] Alexander Munteanu and Simon Omlor. Optimal bounds for $\\ell_{p}$ sensitivity sampling via $\\ell_{2}$ augmentation. In Forty-first International Conference on Machine Learning (ICML), 2024.   \n[34] Alexander Munteanu, Simon Omlor, and Christian Peters. $p$ -Generalized probit regression and scalable maximum likelihood estimation via sketching and coresets. In Proceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS), pages 2073\u20132100, 2022.   \n[35] Alexander Munteanu and Chris Schwiegelshohn. Coresets-methods and history: A theoreticians design pattern for approximation and streaming algorithms. K\u00fcnstliche Intell., 32(1):37\u201353, 2018.   \n[36] Alexander Munteanu, Chris Schwiegelshohn, Christian Sohler, and David P. Woodruff. On coresets for logistic regression. In Advances in Neural Information Processing Systems 31 (NeurIPS), pages 6562\u20136571, 2018.   \n[37] Jeff M Phillips. Coresets and sketches. In Handbook of discrete and computational geometry, pages 1269\u20131288. Chapman and Hall/CRC, 2017.   \n[38] Biel Roig-Solvas and Mario Sznaier. Euclidean distance bounds for LMI analytic centers using a novel bound on the Lambert function, 2022. arXiv:2004.01115.   \n[39] Haipeng Shen and Jianhua Z. Huang. Forecasting time series of inhomogeneous Poisson processes with application to call center workforce management. The Annals of Applied Statistics, 2(2):601 \u2013 623, 2008.   \n[40] Donna Spiegelman and Ellen Hertzmark. Easy SAS Calculations for Risk or Prevalence Ratios and Differences. American Journal of Epidemiology, 162(3):199\u2013200, 2005.   \n[41] Elad Tolochinsky, Ibrahim Jubran, and Dan Feldman. Generic coreset for scalable learning of monotonic kernels: Logistic regression, sigmoid and more. In International Conference on Machine Learning (ICML), pages 21520\u201321547, 2022.   \n[43] Ruosong Wang and David P. Woodruff. Tight bounds for $\\ell_{1}$ oblivious subspace embeddings. ACM Trans. Algorithms, 18(1):8:1\u20138:32, 2022.   \n[44] David P. Woodruff and Taisuke Yasuda. New subset selection algorithms for low rank approximation: Offilne and online. In Proceedings of the 55th Annual ACM Symposium on Theory of Computing (STOC), pages 1802\u20131813, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Details on the Sensitivity Framework ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Definition A.1 (Coreset, cf. [19]). Let $X\\ \\in\\ \\mathbb{R}^{n\\times d}$ be a set of points $\\{x_{1},\\ldots,x_{n}\\}$ , weighted by $w\\,\\in\\,\\mathbb{R}_{>0}^{n}$ . For any $\\eta\\,\\in\\,\\mathbb{R}^{d}$ , let the cost of $\\eta$ w.r.t. the point $x_{i}$ be described by a function $w_{i}\\cdot f\\left(x_{i}\\eta\\right)$ mapping from $\\mathbb{R}$ to $(0,\\infty)$ . Thus, the cost of \u03b7 w.r.t. the (weighted) set $X$ is $f_{w}\\left(X\\eta\\right)=$ $\\textstyle\\sum_{i}w_{i}\\cdot f\\left(w_{i}\\eta\\right)$ . Then a set $K\\in\\mathbb{R}^{k\\times d}$ , (re)weighted by $u\\in\\mathbb{R}_{>0}^{k}$ is a $(1+\\varepsilon)$ -coreset of $X$ for the function $f_{w}$ if $k\\ll n$ and ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\forall\\beta\\in\\mathbb{R}^{d}\\colon\\left|f_{w}\\left(X\\eta\\right)-f_{u}\\left(K\\eta\\right)\\right|\\leq\\varepsilon\\cdot f_{w}\\left(X\\eta\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In our analysis we use sampling based on so-called sensitivity scores, the range space induced by the set of functions, and the VC-dimension. We define these notions next. ", "page_idx": 13}, {"type": "text", "text": "Definition A.2 (Sensitivity, [26]). Consider a family of functions $\\mathcal{F}=\\{g_{1},\\ldots,g_{n}\\}$ mapping from $\\mathbb{R}^{d}$ to $[0,\\infty)$ and weighted by $w\\in\\mathbb{R}_{>0}^{n}$ . The sensitivity of g\u2113for the function $\\begin{array}{r}{f_{w}(\\eta)\\stackrel{}{=}\\sum_{\\ell\\in[n]}w_{\\ell}g_{\\ell}(\\eta)}\\end{array}$ , where $\\eta\\in\\mathbb{R}^{d}$ , is ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\varsigma_{\\ell}=\\operatorname*{sup}\\frac{w_{\\ell}g_{\\ell}(\\eta)}{f_{w}(\\eta)}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The total sensitivity is S = \u2113\u2208[n] \u03c2\u2113. ", "page_idx": 13}, {"type": "text", "text": "Definition A.3 (Range space; VC dimension). A range space is a pair $\\boldsymbol{\\mathfrak{R}}=(\\mathcal{F},\\mathtt{r a n g e s})$ , where $\\mathcal{F}$ is a set and ranges is a family of subsets of $\\mathcal{F}$ . The VC dimension $\\Delta(\\mathfrak{R})$ of $\\Re$ is the size $|G|$ of the largest subset $G\\subseteq{\\mathcal{F}}$ such that $G$ is shattered by ranges, i.e., $|\\{G\\cap R:R\\in{\\tt r a n g e s}\\}|=2^{|G|}$ . ", "page_idx": 13}, {"type": "text", "text": "Definition A.4 (Induced range space). Let $\\mathcal{F}$ be a finite set of functions mapping from $\\mathbb{R}^{d}$ to $\\mathbb{R}_{\\geq0}$ . For every $x\\,\\in\\,\\mathbb{R}^{d}$ and $r\\,\\in\\,\\mathbb{R}_{\\geq0}$ , let r $\\mathbf{ange}_{\\mathcal{F}}(x,r)\\,=\\,\\{f\\,\\in\\,\\mathcal{F}\\,:\\,f(x)\\,\\geq\\,r\\}$ , and $\\mathtt{r a n g e s}({\\mathcal F})=$ $\\{\\mathbf{range}_{\\mathcal{F}}(x,r):x\\in\\mathbb{R}^{d},r\\in\\mathbb{R}{\\ge}0\\}$ . Let $\\Re_{\\mathcal{F}}=(\\mathcal{F},\\mathtt{r a n g e s}(\\mathcal{F}))$ be the range space induced by $\\mathcal{F}$ . ", "page_idx": 13}, {"type": "text", "text": "To construct coresets for Poisson models, we use a framework that combines sensitivity scores with the theory of VC dimension, originally proposed by [18, 7]. We use a more recent and slightly updated version as stated in the following theorem. ", "page_idx": 13}, {"type": "text", "text": "Proposition A.5 ([19], Theorem 31). Consider a family of functions $\\mathcal{F}=\\{f_{1},\\ldots,f_{n}\\}$ mapping from $\\mathbb{R}^{d}$ to $[0,\\infty)$ and a vector of weights $w\\ \\in\\ \\mathbb{R}_{>0}^{n}$ . Let $\\varepsilon,\\delta\\ \\in\\ (0,1/2)$ . Let $s_{i}~\\geq~\\varsigma_{i}$ . Let $\\textstyle S=\\sum_{i=1}^{n}s_{i}\\geq{\\mathfrak{S}}$ . Given $s_{i}$ one can compute in time $O(|\\mathcal{F}|)$ a set $\\mathcal{R}\\subset\\mathcal{F}$ of ", "page_idx": 13}, {"type": "equation", "text": "$$\nO\\left(\\frac{S}{\\varepsilon^{2}}\\left(\\Delta\\log(S)+\\log\\left(\\frac{1}{\\delta}\\right)\\right)\\right)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "weighted functions such that with probability $1-\\delta$ we have for all $\\eta\\in\\mathbb{R}^{d}$ simultaneously ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left|\\sum_{f\\in{\\mathcal{F}}}w_{i}f_{i}(\\eta)-\\sum_{f\\in{\\mathcal{R}}}u_{i}f_{i}(\\eta)\\right|\\le\\varepsilon\\sum_{f\\in{\\mathcal{F}}}w_{i}f_{i}(\\eta),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where each element of R is sampled i.i.d. with probability pj = sSj from F, ui =|SRw|sjj denotes the weight of a function $f_{i}\\in\\mathcal{R}$ that corresponds to $f_{j}\\in\\mathcal{F}$ , and where $\\Delta$ is an upper bound on the VC dimension of the range space $\\Re_{\\mathcal{F}^{*}}$ induced by ${\\mathcal{F}}^{*}$ that can be obtained by defining $\\mathcal{F}^{*}$ to be the set of functions fj \u2208F where each function is scaled by|SRw|sjj . ", "page_idx": 13}, {"type": "text", "text": "B Proofs for Poisson pth-root-link model ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma 2.1. It holds for all $z\\in\\mathbb{R}_{>0},p\\in[1,\\infty),y\\in\\mathbb{N}$ that ", "page_idx": 13}, {"type": "equation", "text": "$$\ng_{y}(z)=z^{p}-p y\\log(z)+\\log(y!)\\geq\\operatorname*{max}\\left\\{1,{\\frac{1}{3}}(1+p\\log(z))\\right\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof of Lemma 2.1. For $y=1$ a direct calculation yields $g_{y}(z)\\ge g_{y}(y^{1/p})=1$ . ", "page_idx": 13}, {"type": "text", "text": "For all other $y\\in\\mathbb{N}\\setminus\\{1\\}$ we get from Stirling\u2019s approximation ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\log(y!)\\geq y\\log(y)-y+\\frac{1}{2}\\log(2\\pi y)+\\frac{1}{12y+1}}\\\\ {\\geq\\displaystyle y\\log(y)-y+\\frac{1}{2}\\log(y)+\\frac{1}{2}\\log(2\\pi)}\\\\ {\\geq y\\log(y)-y+\\frac{1}{2}\\log(y)+\\frac{9}{10}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since $g_{y}$ has a unique minimizer at $z^{*}=y^{1/p}$ , it follows that ", "page_idx": 14}, {"type": "equation", "text": "$$\ng_{y}(z)\\geq g_{y}(y^{1/p})=y-y\\log(y)+\\log(y!)\\geq\\frac{9}{10}+\\frac{1}{2}\\log(y)>1\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To prove the claimed bound generalizing to all $z\\in\\mathbb{R}_{>0}$ , we first argue that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{9}{10}+\\frac{1}{3}\\log(y)\\geq\\frac{1}{3}(1+\\log(y+1)),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which is equivalent to showing for arbitrary $y\\in\\mathbb N$ it holds that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{1}{3}\\left(1+\\log\\left(\\frac{y+1}{y}\\right)\\right)=\\frac{1}{3}\\left(1+\\log\\left(1+\\frac{1}{y}\\right)\\right)\\leq\\frac{1}{3}+\\frac{1}{3y}\\leq\\frac{2}{3}\\leq\\frac{9}{10}\\,,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the first inequality follows from the inequality $1+x\\leq e^{x}$ for all values of $x$ . We see that by monotonicity of the functions $\\begin{array}{r}{h(z)=\\frac{1}{3}(1+\\log(z^{p}))}\\end{array}$ and the above properties of $g_{y}(z)$ , we have for all $z\\le(y+1)^{1/p}$ that ", "page_idx": 14}, {"type": "equation", "text": "$$\ng_{y}(z)\\geq\\frac{9}{10}+\\frac{1}{2}\\log(y)\\geq\\frac{9}{10}+\\frac{1}{3}\\log(y)\\overset{\\mathrm{Equaion}\\,(11)}{\\geq}\\frac{1}{3}(1+\\log(y+1))\\geq h(z),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the first inequality above follows from the inequality one line above Equation (11). Finally for $z\\ge(y+1)^{1/p}$ the function $g_{y}$ grows at least as fast as the lower bound, since ", "page_idx": 14}, {"type": "equation", "text": "$$\ng_{y}^{\\prime}(z)=p z^{p-1}-{\\frac{p y}{z}}=p\\left(z^{p-1}-{\\frac{y}{z}}\\right)=p\\left({\\frac{z^{p}-y}{z}}\\right)\\geq p{\\frac{y+1-y}{z}}\\geq{\\frac{p}{3z}}=h^{\\prime}(z).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma 2.2. For any $p\\geq1$ and $y\\in\\mathbb N$ it holds that $z^{p}\\geq g_{y}(z)$ for $z>y^{1/p}$ . If $p=1$ , then for some $\\lambda\\in O(\\sqrt{y/\\log(y)})$ , it holds that $\\begin{array}{r}{g_{y}(z)\\ge\\frac{(z-y^{1/p})^{p}}{\\lambda}}\\end{array}$ for $z>y^{1/p}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof of Lemma 2.2. We start with the upper bound. Using the fact that the assumption $z>y^{1/p}$ implies $\\log z>0$ , and setting $y_{i}=1$ in Equation (4) yields ", "page_idx": 14}, {"type": "equation", "text": "$$\ng_{1}(z)=z^{p}-p\\log(z)+\\log1<z^{p}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "It remains to prove the claim for $y\\geq2$ . From Stirling\u2019s approximation we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\log(y!)\\leq y\\log(y)-y+{\\frac{1}{2}}\\log(y)+{\\frac{1}{2}}\\log(2\\pi)+{\\frac{1}{12}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now, note that since the derivative of the function $y\\mapsto{\\textstyle\\frac{1}{2}}\\log y-y$ is strictly negative over the interval $[2,\\infty)$ , it follows that the function itself is decreasing over the same interval. By rearranging terms and replacing $y_{i}$ in Equation (4) with an arbitrary $y\\geq2$ , it follows that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle g_{y}(z)=z^{p}-y\\log(z^{p})+\\log(y!)\\leq z^{p}-y\\log(y)+\\log(y!)}\\\\ {\\displaystyle\\leq z^{p}+\\frac{1}{2}\\log(y)-y+\\frac{1}{2}\\log(2\\pi)+\\frac{1}{12}}\\\\ {\\displaystyle\\leq z^{p}+\\frac{1}{2}\\log(2)-2+\\frac{1}{2}\\log(2\\pi)+\\frac{1}{12}}\\\\ {\\displaystyle=z^{p}+\\frac{1}{2}\\log(\\pi)-\\frac{23}{12}<z^{p},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the last inequality holds since $\\pi<e^{2}$ , and thus $\\textstyle{\\frac{1}{2}}\\log(\\pi)<\\log(e)=1$ . ", "page_idx": 15}, {"type": "text", "text": "In the remainder, let $p=1$ . Let ", "page_idx": 15}, {"type": "equation", "text": "$$\nL B(y):=\\operatorname*{max}\\left\\{1,{\\frac{1}{3}}\\left(1+\\log(y)\\right)\\right\\}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "be the lower bound given in Lemma 2.1. Now, we want to prove that $\\begin{array}{r}{g_{y}(z)\\geq\\frac{z-y}{\\lambda}=:h(z)}\\end{array}$ for $\\lambda\\geq1$ as small as possible. ", "page_idx": 15}, {"type": "text", "text": "To this end, we consider the derivatives $\\begin{array}{r}{g_{y}^{\\prime}(z)=1-\\frac{y}{z}}\\end{array}$ and $\\begin{array}{r}{h^{\\prime}(z)=\\frac{1}{\\lambda}}\\end{array}$ , and find that ", "page_idx": 15}, {"type": "equation", "text": "$$\n1-{\\frac{y}{z}}\\geq{\\frac{1}{\\lambda}}\\Longleftrightarrow z\\geq y\\left(1+{\\frac{1}{\\lambda-1}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Next, we see that since $h(y)=0$ , we can guarantee $h(z)\\leq L B(y)\\leq g_{y}(z)$ for all $z\\in[y,y+\\Delta]$ , where $\\Delta:=\\lambda\\cdot L B(y)$ . ", "page_idx": 15}, {"type": "text", "text": "To obtain a general lower bound on $g_{y}(z)$ , we want both conditions to hold simultaneously, which is true whenever ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{y}{\\lambda-1}\\leq\\Delta\\Longleftrightarrow y\\leq\\lambda(\\lambda-1)L B(y)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Solving for $\\lambda$ yields that $\\begin{array}{r}{g_{y}(z)\\geq\\frac{z-y}{\\lambda}}\\end{array}$ holds for all $\\begin{array}{r}{\\lambda\\geq\\frac{1}{2}\\left(\\sqrt{\\frac{4y}{L B(y)}+1}+1\\right)}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma 2.3. Let $p\\geq2$ and $y\\in\\mathbb N$ , and $\\lambda=1$ . Then $\\begin{array}{r}{g_{y}(z)\\ge\\frac{(z-y^{1/p})^{p}}{\\lambda}\\,f\\!o r\\,z>y^{1/p}.}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma 2.3. First we define for $\\tau>0$ the function $h_{\\lambda}$ , and compute its derivatives: ", "page_idx": 15}, {"type": "equation", "text": "$$\nh_{\\lambda}(z):=\\frac{(z-\\tau)^{p}}{\\lambda},\\quad h_{\\lambda}^{\\prime}(z)=p\\frac{(z-\\tau)^{p-1}}{\\lambda},\\quad h_{\\lambda}^{\\prime\\prime}(z)=p(p-1)\\frac{(z-\\tau)^{p-2}}{\\lambda},\\quad z\\in\\mathbb{R}_{>0}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Setting $\\tau=y^{1/p}$ in Equation (12), we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\nh_{\\lambda}(y^{1/p})=\\frac{(y^{1/p}-y^{1/p})^{p}}{\\lambda}=0<\\frac{1}{2}\\log(2\\pi y)<g_{y}(y^{1/p}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the second inequality follows from the fact that the minimizer of $g_{y}$ is $y^{1/p}$ and from Stirling\u2019s approximation: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\log(2\\pi y)+\\frac{1}{12y+1}<\\operatorname*{min}_{z>0}g_{y}(z)=y-y\\log y+\\log y!.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, $g_{y}(y^{1/p})\\geq h_{\\lambda}(y^{1/p})$ , and a sufficient condition for $g_{y}(z)\\ge h_{\\lambda}(z)$ to hold for $z>y^{1/p}$ is that $g_{y}(z)-h(z)$ is nondecreasing on $z>y^{1/p}$ , i.e. ", "page_idx": 15}, {"type": "equation", "text": "$$\n0\\leq g_{y}^{\\prime}(z)-h^{\\prime}(z)=p\\left[\\left(z^{p-1}-{\\frac{y}{z}}\\right)-{\\frac{(z-y^{1/p})^{p-1}}{\\lambda}}\\right]\\Longleftrightarrow z^{p-1}\\geq{\\frac{y}{z}}+{\\frac{(z-y^{1/p})^{p-1}}{\\lambda}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that ", "page_idx": 15}, {"type": "equation", "text": "$$\nz>y^{1/p}\\Longrightarrow\\frac{y}{z}+\\frac{(z-y^{1/p})^{p-1}}{\\lambda}\\leq\\frac{y}{y^{1/p}}+\\frac{(z-y^{1/p})^{p-1}}{\\lambda},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "so a sufficient condition for $g_{y}^{\\prime}(z)-h_{\\lambda}^{\\prime}(z)$ to be nonnegative for all $z>y^{1/p}$ is ", "page_idx": 15}, {"type": "equation", "text": "$$\nz^{p-1}\\geq\\frac{(z-y^{1/p})^{p-1}}{\\lambda}+(y^{1/p})^{p-1},\\quad\\forall z>y^{1/p}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "If $\\lambda\\geq1$ , then a further sufficient condition for $g_{y}^{\\prime}(z)-h_{\\lambda}^{\\prime}(z)$ to be nonnegative for all $z>y^{1/p}$ is ", "page_idx": 15}, {"type": "equation", "text": "$$\nz^{p-1}\\geq(z-y^{1/p})^{p-1}+(y^{1/p})^{p-1},\\quad\\forall z>y^{1/p},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which is equivalent to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left(\\frac{z}{y^{1/p}}\\right)^{p-1}\\geq\\left(\\frac{z}{y^{1/p}}-1\\right)^{p-1}+1,\\quad\\forall z>y^{1/p}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For $x\\in\\mathbb R$ , let $\\lfloor x\\rfloor:=\\operatorname*{sup}\\{p\\in\\mathbb{Z}\\ :\\ p\\leq x\\}$ be the floor of $x$ . We will prove that Equation (13) holds. We have ", "page_idx": 16}, {"type": "text", "text": "binomial thm., $p\\geq2$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{z}{y^{1/p}}>1\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left(\\frac{z}{y^{2}|y|}\\right)^{p-1}}\\\\ &{=\\left(\\frac{z^{2}(y)}{y^{2}|y|}\\right)^{p-1-\\{1\\}}\\left(\\frac{z}{y^{2}|y|}\\right)^{\\{p-1\\}}}\\\\ &{=\\left(\\frac{z}{y^{2}|y|}\\right)^{p-1+\\{1\\}-1}\\left(\\frac{z}{y^{2}|z|}-1+\\right)^{\\{p-1\\}}}\\\\ &{=\\left(\\frac{z^{2}(y)}{y^{2}|y|}\\right)^{p-1-\\{1\\}-1}\\sum_{s=0}^{\\lfloor\\frac{p}{p-1}\\rfloor}\\left(\\frac{z^{2}}{y^{2}|y|}-1\\right)^{s}}\\\\ &{=\\left(\\frac{z^{2}(y)}{y^{2}|y|}\\right)^{p-1-\\{1\\}-1}\\left(\\left(\\frac{z^{2}}{y^{2}|y|^{2}}-1\\right)^{\\{p-1\\}}+\\right)}\\\\ &{\\geq\\left(\\frac{z^{2}}{y^{2}|y|}\\right)^{p-1-\\{1\\}-1}\\left(\\left(\\frac{z^{2}}{y^{2}|y|^{2}}-1\\right)^{\\{p-1\\}}+\\right)}\\\\ &{=\\left(\\frac{z^{2}(y)}{y^{2}|y|}\\right)^{p-1-\\{1\\}-1}\\left(\\frac{z^{2}}{y^{2}|z|}-1\\right)^{\\{p-1\\}-1}+\\left(\\frac{z^{2}}{y^{2}|y|}\\right)^{p-1-\\{1\\}}}\\\\ &{\\geq\\left(\\frac{z^{2}(y)}{y^{2}|y|}-1\\right)^{p-1-\\{1\\}-1}\\left(\\frac{z^{2}}{y^{2}|y|}-1\\right)^{\\{p-1\\}-1\\}+\\left(\\frac{z^{2}}{y^{2}|y|}\\right)^{p-1-\\{1\\}-1}}\\\\ &{>\\left(\\frac{z^{2}(y)}{y^{2}|y|}-1\\right)^{p-1+\\{1\\}}+}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{r\\geq0\\Rightarrow\\frac{\\mathrm{d}}{\\mathrm{d}x}x^{r}\\geq0}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{z}{y^{1/p}}>1,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "as desired. ", "page_idx": 16}, {"type": "text", "text": "C Proofs for coreset construction ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma 3.1. The VC dimension of the range space associated with the class of Poisson loss functions as in Equation (3) is bounded by $\\bar{\\Delta}(\\mathfrak{R}_{\\mathcal{F}^{*}})\\stackrel{-}{\\le}O(d^{2})$ . ", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma 3.1. Because we use the coreset approach, we need to consider weighted subsets of the data, and thus weighted sums $\\textstyle\\sum_{i=1}^{n}w_{i}g_{y_{i}}(x_{i}{\\bar{\\beta\\bar{)}}}$ that allow for different weights in Equation (3). In determining the VC dimension we range over all $r\\,\\in\\,\\mathbb{R},\\beta\\,\\in\\,\\mathbb{R}^{d}$ , and need to check whether $w_{i}g_{y_{i}}(x_{i}\\beta)>\\bar{r}$ . Note that the log factorial terms are independent of $x_{i}\\beta$ . In particular, for each $r\\in\\mathbb{R}$ , there exists $\\mathbb{R}\\ni s:=r-w_{i}\\log(y_{i}!)$ . We can thus instead count the number of operations required for evaluating each summand ", "page_idx": 16}, {"type": "equation", "text": "$$\nw_{i}((x_{i}\\beta)^{p}-p y_{i}\\ln(x_{i}\\beta))\\geq s.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This can be rearranged to ", "page_idx": 16}, {"type": "equation", "text": "$$\nx_{i}\\beta\\leq\\exp\\left(\\frac{(x_{i}\\beta)^{p}-s/w_{i}}{p y_{i}}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which can be accomplished using (at most) the following operations ", "page_idx": 16}, {"type": "text", "text": "\u2022 1 division to compute $s/w_{i}$   \n\u2022 $d$ multiplications and $d-1$ additions to compute $x_{i}\\beta$   \n\u2022 $p-1$ multiplications of $x_{i}\\beta$ to compute $(x_{i}\\beta)^{p}$   \n\u2022 1 subtraction to compute $(x_{i}\\beta)^{p}-s/w_{i}$   \n\u2022 1 multiplication to compute $p y_{i}$ (0 for $p=1$ )   \n\u2022 1 division by pyi   \n\u2022 1 exponential function evaluation to compute $\\exp\\left({\\frac{(x_{i}\\beta)^{p}\\!-\\!s/w_{i}}{p y_{i}}}\\right)$   \n\u2022 1 operation to verify whether $\\begin{array}{r}{x_{i}\\beta\\le\\exp\\left(\\frac{(x_{i}\\beta)^{p}-s/w_{i}}{p y_{i}}\\right)}\\end{array}$ holds ", "page_idx": 16}, {"type": "text", "text": "\u2022 1 operation to output 0 or 1 depending on whether $\\begin{array}{r}{x_{i}\\beta\\le\\exp\\left(\\frac{(x_{i}\\beta)^{p}-s/w_{i}}{p y_{i}}\\right)}\\end{array}$ holds for a total of $t=2d+p+5$ operations, exactly $q=1$ of which is an exponential function evaluation. Putting these quantities into the final conclusion of [4, Thm. 8.14] yields ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta(\\Re_{\\mathcal{F}^{*}})\\leq(d(q+1))^{2}+11d(q+1)(t+\\log_{2}((9d(q+1)))}\\\\ {=(2d)^{2}+22d(2d+p+5+\\log_{2}(18d))=O(d^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma 3.2. The VC dimension of the range space induced by the set of functions $\\mathcal{F}_{c}=\\{g_{i}(\\beta)=$ $c\\cdot g_{y_{i}}(x_{i}\\beta)\\mid i\\in[n]\\}$ with equal weight $c\\in\\mathbb{R}_{\\geq0}$ , and equal $y_{i}=y\\in\\mathbb{N}_{0}$ for all $i\\in[n]$ satisfies $\\Delta\\left(\\Re_{\\mathcal{F}_{c}}\\right)=O(d)$ . ", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma 3.2. We split the functions $g_{i}$ into subfunctions $g_{\\leq y_{i}^{1/p}}(x_{i}\\beta)$ and $g_{>y_{i}^{1/p}}(x_{i}\\beta)$ depending on whether $x_{i}\\beta\\leq y_{i}^{1/p}$ or $x_{i}\\beta>y_{i}^{1/p}$ . Since $g_{i}$ is minimizedi when $x_{i}\\beta=y_{i}^{1/p}$ , and due to strict convexity, the two subfunctions $g_{\\leq y_{i}^{1/p}}\\colon(0,y_{i}^{1/p}]\\to[g(y_{i}^{1/p}),\\infty)$ and $g_{>y_{i}^{1/p}}\\colon(y_{i}^{1/p},\\infty)\\to$ $[g(y_{i}^{1/p}),\\infty)$ are strictly monotonic and invertible on their respective ranges and domains. ", "page_idx": 17}, {"type": "text", "text": "Now fix an arbitrary subset $G\\subseteq{\\mathcal{F}}_{c}$ . Let $\\Omega=\\mathbb{R}^{d}\\times\\mathbb{R}_{\\geq0}$ . We have the following bound: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\vert\\big\\langle G\\cap R\\mid R\\in\\mathbf{ranges}(\\mathcal{F}_{c})\\big\\rangle\\right\\vert=\\left\\vert\\{\\mathbf{range}_{G}(\\beta,r)\\mid\\beta\\in\\mathbb{R}^{d},r\\in\\mathbb{R}_{\\ge0}\\right\\}\\right\\vert}\\\\ &{=\\Bigg\\vert\\bigcup_{(\\beta,r)\\in\\Omega}\\left\\{\\left\\{g_{i}\\in G\\mid g_{i}(\\beta)\\ge r\\right\\}\\right\\}\\Bigg\\vert}\\\\ &{=\\Bigg\\vert\\bigcup_{(\\beta,r)\\in\\Omega}\\left\\{\\left\\{g_{i}\\in G\\mid c\\cdot g_{\\leq\\beta_{i}^{1/p}}(x_{i}\\beta)\\ge r\\vee c\\cdot g_{\\gg\\beta_{i}^{1/p}}(x_{i}\\beta)\\ge r\\right\\}\\right\\}\\Bigg\\vert}\\\\ &{\\leq\\Bigg\\vert\\bigcup_{(\\beta,r)\\in\\Omega}\\left\\{\\left\\{g_{i}\\in G\\mid x_{i}\\beta\\ge g_{\\leq\\beta_{i}^{1/p}}^{-1}(r/c)\\right\\}\\right\\}\\Bigg\\vert\\cdot\\Bigg\\vert\\bigcup_{(\\beta,r)\\in\\Omega}\\left\\{\\left\\{g_{i}\\in G\\mid x_{i}\\beta\\ge g_{\\gg\\beta_{i}^{1/p}}^{-1}(r/c)\\right\\}\\right\\}\\Bigg\\vert.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The inequality holds, since each non-empty set in the collection on the LHS satisfies either of the conditions of the sets in the collections on the RHS, or both, and is thus the union of two of the sets, one from each collection. It can thus comprise at most all unions obtained from combining any two of these sets. ", "page_idx": 17}, {"type": "text", "text": "Now, note that both sets are of the form $\\{g_{i}\\in G\\mid x_{i}\\beta\\geq s_{1}\\}$ where $s_{1}=g_{\\le y_{i}^{1/p}}^{-1}(r/c)$ maps any real $r$ to a value of some subset of the reals $s\\in D\\subset\\mathbb{R}$ as specified above. Extending the domain of $s$ and $x_{i}\\beta$ to the reals, we obtain exactly the points that are shattered by the affine hyperplane classifier $x_{i}\\mapsto\\mathbf{1}_{\\{x_{i}\\beta-s\\geq0\\}}$ . The VC dimension of the set of hyperplane classifiers is $d+1$ [24, 42]. The argument holds verbatim for $s_{2}=g_{>y_{i}^{1/p}}^{-1}(r/c)$ . ", "page_idx": 17}, {"type": "text", "text": "We conclude the claimed bound on $\\Delta(\\mathfrak{R}_{\\mathcal{F}_{c}})$ by showing that the above term Equation (14) is strictly less than $2^{|G|}$ for $|G|=10(d+1)$ . By a bound on the growth of the sets (see [6, 24]), we have for this particular choice ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\ \\ \\left|\\bigcup_{(\\beta,r)\\in\\Omega}\\{\\{g_{i}\\in G\\;|\\;x_{i}\\beta\\ge g_{\\le y_{i}^{1/p}}^{-1}(r/c)\\}\\}\\right|\\cdot\\left|\\bigcup_{(\\beta,r)\\in\\Omega}\\{\\{g_{i}\\in G\\;|\\;x_{i}\\beta\\ge g_{>y_{i}^{1/p}}^{-1}(r/c)\\}\\}\\right|}\\\\ &{\\le\\left|\\left\\{\\{g_{i}\\in G\\;|\\;x_{i}\\beta-s_{1}\\ge0\\}\\;|\\;\\beta\\in\\mathbb{R}^{d},s_{1}\\in\\mathbb{R}\\right\\}\\cdot\\left|\\left\\{\\{g_{i}\\in G\\;|\\;x_{i}\\beta-s_{2}\\ge0\\}\\;|\\;\\beta\\in\\mathbb{R}^{d},s_{2}\\in\\mathbb{R}\\right\\}\\right|}\\\\ &{\\le\\left(\\left(\\frac{e|G|}{d+1}\\right)^{(d+1)}\\right)^{2}<\\left(30^{(d+1)}\\right)^{2}=2^{2(d+1)\\log_{2}(30)}\\le2^{10(d+1)}=2^{|G|}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which implies that $\\Delta(\\mathfrak{R}_{\\mathcal{F}_{\\ell_{1}}})<10(d+1)$ . ", "page_idx": 17}, {"type": "text", "text": "Lemma 3.3. Let $y\\geq8$ and $1\\geq\\varepsilon>0$ . Let $y<y^{\\prime}\\leq(1+\\varepsilon)y_{i}$ . Then for arbitrary $z>0$ it holds that $(1-3\\varepsilon)g_{y}(z)\\leq g_{y^{\\prime}}(z)\\leq(1+3\\varepsilon)g_{y}(z)$ . ", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma 3.3. Recall that ", "page_idx": 18}, {"type": "equation", "text": "$$\ng_{y^{\\prime}}(z)=\\underbrace{(z)^{p}-p y^{\\prime}\\log(z)}_{(i)}+\\underbrace{\\log(y^{\\prime}!)}_{(i i)}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We bound the two parts separately. The first part $(i)$ is straightforward from the assumption on $y^{\\prime}$ . We only need to distinguish two cases, depending on which either the upper or the lower bound of the assumption $y<y^{\\prime}\\bar{\\leq}(1+\\varepsilon)y_{i}$ applies: If $\\log(z)\\geq0$ , then there is nothing to prove since in that case ", "page_idx": 18}, {"type": "equation", "text": "$$\nz^{p}-p y^{\\prime}\\log(z)\\leq z^{p}-p y\\log(z),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "given the hypothesis that $y<y^{\\prime}$ . Suppose that $\\log(z)<0$ . Then ", "page_idx": 18}, {"type": "equation", "text": "$$\nz^{p}-p y^{\\prime}\\log(z)\\leq z^{p}-(1+\\varepsilon)p y\\log(z)\\leq(1+3\\varepsilon)(z^{p}-p y\\log(z)),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the first inequality follows from the hypothesis that $y^{\\prime}\\le(1+\\varepsilon)y_{i}$ and the second inequality follows since $1+3\\varepsilon>1+\\varepsilon>1$ . ", "page_idx": 18}, {"type": "text", "text": "For the second part $(i i)$ we need some technical claims. Since $y,y^{\\prime}\\in\\mathbb{N}_{0}$ , it must hold that $y^{\\prime}\\geq y+1$ . Then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{12y^{\\prime}}\\leq\\frac{1}{12(y+1)}\\leq\\frac{1}{12y+1}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Further, for any $x\\geq e$ it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\log((1+\\varepsilon)x)=\\log(x)+\\log(1+\\varepsilon)\\leq\\log(x)+\\varepsilon\\cdot1\\leq(1+\\varepsilon)\\log(x),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last inequality uses the condition that $x\\geq e$ . ", "page_idx": 18}, {"type": "text", "text": "With this in place, and using $y^{\\prime}>y\\geq8>e^{2}$ , we apply Stirling\u2019s approximation with a constant $\\begin{array}{r}{C:=\\frac{1}{2}\\log(\\dot{2}\\pi)}\\end{array}$ that is independent of $y,y^{\\prime}$ , and that is common to both the upper and lower bounds from Stirling\u2019s approximation for the factorial function. We thus obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log(y^{\\prime}!)\\leq y^{\\prime}\\log(y^{\\prime})-y^{\\prime}+\\frac{1}{2}\\log(y^{\\prime})+\\frac{1}{12y^{\\prime}}+C}\\\\ &{\\qquad\\qquad=y^{\\prime}(\\log(y^{\\prime})-1)+\\frac{1}{2}\\log(y^{\\prime})+\\frac{1}{12y^{\\prime}}+C}\\\\ &{\\qquad\\qquad=y^{\\prime}(\\log(y^{\\prime}/e))+\\frac{1}{2}\\log(y^{\\prime})+\\frac{1}{12y^{\\prime}}+C}\\\\ &{\\qquad\\qquad\\leq(1+\\varepsilon)^{2}y\\log(y/e)+(1+\\varepsilon)\\frac{1}{2}\\log(y)+\\frac{1}{12y+1}+C}\\\\ &{\\qquad\\qquad\\leq(1+\\varepsilon)^{2}\\left(y\\log(y/e)+\\frac{1}{2}\\log(y)+\\frac{1}{12y+1}+C\\right)}\\\\ &{\\qquad\\qquad\\leq(1+3\\varepsilon)\\log(y!)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the first inequality follows from the upper bound in Stirling\u2019s approximation for the factorial, the second inequality follows from the hypothesis that y\u2032 \u2264(1+\u03b5)y and the inequality121y\u2032 \u226412y1+1 proven above, and the last inequality follows from the lower bound in Stirling\u2019s approximation of the factorial and from the fact that $(1+\\varepsilon)^{2}=1+2\\varepsilon+\\varepsilon^{2}\\leq1+3\\varepsilon$ . The lower bound can be treated in a similar way. Overall, our claim follows. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Lemma 3.4. Let $\\mathcal{F}$ be any family of functions, and let $F_{1},\\dots,F_{t}\\subseteq{\\mathcal{F}}$ be nonempty sets that form a partition of ${\\mathcal F}_{\\mathrm{c}}$ , i.e., their disjoint union satisfies $\\begin{array}{r}{\\dot{\\bigcup_{i\\in[t]}}=\\mathcal{F}}\\end{array}$ . Let the $V C$ dimension of the range space induced by $F_{i}$ be bounded by $D$ for all $i\\in[t]$ . Then the VC dimension of the range space induced by $\\mathcal{F}$ satisfies $\\Delta\\left(\\Re_{\\mathcal{F}}\\right)\\leq t D$ . ", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma 3.4. We prove the claim by contradiction. To this end suppose the VC dimension for $\\mathcal{F}$ is strictly larger than $t D$ . Then there exists a set $G$ of size $\\left|G\\right|>t{\\bar{D}}$ that is shattered by the ranges of $\\Re_{\\mathcal{G}}$ . Consider its intersections $G_{i}=G\\cap F_{i},i\\in[t]$ with the sets $F_{i}$ . By their disjointness, each $G_{i}$ must be shattered by the ranges of $\\Re_{F_{i}}$ . Note, that at least one $G_{i}$ must therefore satisfy $|G_{i}|/t>D$ , which contradicts the assumption that their VC dimension is bounded by $D$ . Our claim thus follows. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Lemma 3.5. Let $\\mathcal{F}$ be the set of functions in the Poisson model. We can round and group the values of $y_{i}$ and the associated sensitivities $\\varsigma_{i}$ to obtain $\\mathcal{F}^{*}$ such that each function in ${\\mathcal{F}}^{*}$ is weighted by $0<w_{i}\\in W:=\\{u_{1},\\ldots,u_{t}\\}$ for $t\\in O(\\varepsilon^{-1}\\log(n)\\cdot\\log(y_{\\operatorname*{max}}))$ . The range space induced by $\\mathcal{F}^{*}$ satisfies $\\begin{array}{r}{\\Delta\\left(\\Re_{\\mathcal{F}^{*}}\\right)\\leq O(\\frac{d}{\\varepsilon}\\log(n)\\log(y_{\\operatorname*{max}}))}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma 3.5. We partition our input functions $g_{y_{i}},i\\in[n]$ into disjoint sets with boundaries that increase in a geometric progression, depending on the sensitivities resp. weights, and on the response values ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G_{i j}=\\{g_{y_{k}}\\mid8\\cdot(1+\\varepsilon)^{i}\\leq y_{k}<\\lfloor8\\cdot(1+\\varepsilon)^{i+1}\\rfloor,2^{j}\\varsigma_{\\operatorname*{min}}\\leq\\varsigma_{k}<2^{j+1}\\varsigma_{\\operatorname*{min}}\\},}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad i\\in[0,O(\\log_{1+\\varepsilon}(y_{\\operatorname*{max}}))],j\\in[0,O(\\log(n))].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Additionally, we put the remaining values of $y$ into a constant number of disjoint sets ", "page_idx": 19}, {"type": "equation", "text": "$$\nH_{y j}=\\{g_{y_{k}}\\mid y_{k}=y,\\,2^{j}\\varsigma_{\\operatorname*{min}}\\leq\\varsigma_{k}<2^{j+1}\\varsigma_{\\operatorname*{min}}\\},y\\in\\{0,1,2,\\ldots,7\\},j\\in[0,O(\\log(n))].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In particular, we note that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{F}=\\left(\\dot{\\bigcup_{i j}}G_{i j}\\right)\\,\\dot{\\cup}\\,\\left(\\dot{\\bigcup_{y j}}H_{y j}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "forms a partition of the whole function family, since the sets are disjoint and cover all functions by construction. ", "page_idx": 19}, {"type": "text", "text": "Each member of a set is of the same form, i.e., after rounding the weights and $y_{i}$ , all members of a subset have equal $y_{i}=y$ , and they have equal weight. The assumptions are thus satisfied for invoking Lemma 3.2 to bound the VC dimension for each of the induced range spaces by $O(d)$ . By construction, the subsets are disjoint and their number is bounded by ", "page_idx": 19}, {"type": "equation", "text": "$$\nt=O(\\log_{2}(n)\\cdot\\log_{1+\\varepsilon}(y_{\\operatorname*{max}}))=O\\left(\\log(n)\\cdot{\\frac{\\log(y_{\\operatorname*{max}})}{\\log(1+\\varepsilon)}}\\right)=O(\\varepsilon^{-1}\\log(n)\\log(y_{\\operatorname*{max}})).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We can thus invoke Lemma 3.4 to obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Delta\\left(\\Re_{\\mathcal{F}^{*}}\\right)\\leq O(d t)=O\\left(\\frac{d}{\\varepsilon}\\log(n)\\log(y_{\\operatorname*{max}})\\right)=\\tilde{O}\\left(\\frac{d}{\\varepsilon}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Recall the condition Equation (7) for an $(\\alpha,\\gamma,p)$ -well-conditioned basis. ", "page_idx": 19}, {"type": "text", "text": "Lemma 3.7. Let $X\\in\\mathbb{R}^{n\\times d},y\\in\\mathbb{N}_{0}^{n}$ be a $\\rho$ -complex dataset, i.e., Equation (2) holds. Let $p\\in\\{1,2\\}$ . Let $\\lambda\\geq1$ be the slope parameter from either Lemma 2.2 or Lemma 2.3 depending on the value of $p$ . Let $\\gamma$ be a conditioning parameter and $\\eta>0$ be arbitrary. Then the sensitivity for each $x_{i}$ with $x_{i}\\beta\\,>\\,\\eta$ is bounded by $\\varsigma_{i}\\,\\leq\\,\\lambda\\rho\\gamma^{p}\\|U_{i}\\|_{p}^{p}+2/n$ . Their total sensitivity is bounded by $\\mathfrak{S}\\le$ $O\\left(\\rho d\\sqrt{y_{\\operatorname*{max}}/\\log(y_{\\operatorname*{max}})}+\\log\\log(1/\\eta)\\right).$ for $p=1$ , and $\\mathfrak{S}\\le O(\\rho d+\\log{(y_{\\operatorname*{max}})}+\\log\\log(1/\\eta))$ for $p=2$ . ", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma 3.7. Case 1: $y_{i}\\,=\\,0$ : We start with the special case where $y_{i}\\,=\\,0$ . Recall that $x_{i}\\beta>0$ . Then, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{y_{i}}(x_{i}\\beta)=(x_{i}\\beta)^{p}\\leq\\|U_{i}\\|_{p}^{p}\\|R\\beta\\|_{q}^{p}\\leq\\|U_{i}\\|_{p}^{p}(\\gamma\\|U R\\beta\\|_{q})^{p}}\\\\ &{\\qquad\\qquad=\\gamma^{p}\\|U_{i}\\|_{p}^{p}\\|U R\\beta\\|_{p}^{p}=\\gamma^{p}\\|U_{i}\\|_{p}^{p}\\|X\\beta\\|_{p}^{p}=\\gamma^{p}\\|U_{i}\\|_{p}^{p}\\sum_{j=1}^{n}g_{y_{j}}(x_{j}\\beta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Next, we consider $0\\neq y_{i}\\in\\mathbb{N}$ , and divide this into two sub-cases. ", "page_idx": 19}, {"type": "text", "text": "Case 2(i): $y_{i}\\in\\mathbb{N},\\eta<x_{i}\\beta\\le y_{i}^{1/p}$ : ", "page_idx": 19}, {"type": "text", "text": "We start with the case $0<\\eta\\leq x_{i}\\beta\\leq y_{i}^{1/p}$ . ", "page_idx": 20}, {"type": "text", "text": "For $g_{y_{i}}$ defined in Equation (4), ", "page_idx": 20}, {"type": "equation", "text": "$$\ng_{y_{i}}(x_{i}\\beta):=(x_{i}\\beta)^{p}-p y_{i}\\log(x_{i}\\beta)+\\log(y_{i}!)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "it holds using Lemma 2.1, the bounds on $x_{i}\\beta$ and the monotonicity of $g_{y}(z)$ in the interval that ", "page_idx": 20}, {"type": "equation", "text": "$$\n1\\leq g_{y_{i}}(x_{i}\\beta)\\leq\\eta^{p}+y_{i}\\log\\left(\\frac{1}{\\eta^{p}}\\right)+\\log(y_{i}!)=:U B(y_{i}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let $\\mathcal{G}=\\{i\\in[n]\\mid1\\leq g_{y_{i}}(x_{i}\\beta)\\leq\\,U B(y_{\\operatorname*{max}})\\}$ . We subdivide $\\textstyle\\mathcal{G}=\\dot{\\bigcup_{j=1}^{l}}\\mathcal{G}_{j}$ into disjoint sets ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{G}_{j}=\\{i\\in\\mathcal{G}\\ |\\ U B(y_{\\operatorname*{max}})\\cdot2^{-j}<g_{y_{i}}(x_{i}\\beta)\\leq U B(y_{\\operatorname*{max}})\\cdot2^{-j+1}\\},\\ j\\in\\{1,\\dots,l\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $g_{y_{i}}(x_{i}\\beta)\\in[1,\\,U B(y_{\\operatorname*{max}})]$ , there can be at most $l\\leq\\lceil\\log_{2}(U B(y_{\\operatorname*{max}}))\\rceil$ groups. So ", "page_idx": 20}, {"type": "equation", "text": "$$\nl\\leq\\log_{2}\\left(\\eta^{p}+y_{\\operatorname*{max}}\\log\\left(\\frac{1}{\\eta^{p}}\\right)+\\log(y_{\\operatorname*{max}}!)\\right)\\leq O\\left(\\log(y_{\\operatorname*{max}})+\\log\\log\\left(\\frac{y_{\\operatorname*{max}}}{\\eta}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now let $n_{j}=|\\mathcal{G}_{j}|$ . We can bound the sensitivity (see Definition A.2) for each summand $g_{y_{i}}(x_{i}\\beta)$ for $i\\in\\mathcal{G}_{j}$ by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\varsigma_{i}:=\\operatorname*{sup}_{\\beta}\\frac{g_{y_{i}}(x_{i}\\beta)}{\\sum_{i=1}^{n}g_{y_{i}}(x_{i}\\beta)}\\leq\\operatorname*{sup}_{\\beta}\\frac{g_{y_{i}}(x_{i}\\beta)}{\\sum_{i\\in{\\mathcal{G}}_{j}}g_{y_{i}}(x_{i}\\beta)}\\leq\\frac{U B(y_{\\operatorname*{max}})\\cdot2^{-j+1}}{n_{j}\\,U B(y_{\\operatorname*{max}})\\cdot2^{-j}}=\\frac{2}{n_{j}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Summing over $i\\in\\mathcal G$ yields ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{i\\in\\mathcal{G}}\\varsigma_{i}=\\sum_{j=1}^{l}\\sum_{i\\in\\mathcal{G}_{j}}\\frac{2}{n_{j}}=\\sum_{j=1}^{l}\\frac{2n_{j}}{n_{j}}=2l\\leq O\\left(\\log(y_{\\operatorname*{max}})+\\log\\log\\left(\\frac{y_{\\operatorname*{max}}}{\\eta}\\right)\\right)}\\\\ {\\leq O\\left(\\log(y_{\\operatorname*{max}})+\\log\\log\\left(\\frac{1}{\\eta}\\right)\\right).\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Case 2(ii): $y_{i}\\in\\mathbb{N},x_{i}\\beta>y_{i}^{1/p}$ : ", "page_idx": 20}, {"type": "text", "text": "Now we take care of the remaining region where $x_{i}\\beta>y^{1/p}$ . ", "page_idx": 20}, {"type": "text", "text": "In particular, by Lemmas 2.2 and 2.3 for some scaling $\\lambda=\\lambda_{p}$ that depends on $p\\in\\{1,2\\}$ , we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{(z-y_{i}^{1/p})^{p}}{\\lambda}\\leq g_{y_{i}}(z)\\leq z^{p}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "in that region. ", "page_idx": 20}, {"type": "text", "text": "Let $U R$ be a decomposition of $X$ , so that $x_{i}\\beta=U_{i}R\\beta$ , and $U$ is again a $p$ -well conditioned basis, in the sense of Equation (7). ", "page_idx": 20}, {"type": "text", "text": "Now, using our assumption given in Equation (2), we have the following inequalities ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{y_{i}}(x_{i}\\beta)\\leq(x_{i}\\beta)^{p}\\leq\\|U_{i}\\|_{p}^{p}\\|R\\beta\\|_{q}^{p}\\leq\\gamma^{p}\\|U_{i}\\|_{p}^{p}\\|U R\\beta\\|_{p}^{p}}\\\\ &{\\qquad\\qquad=\\gamma^{p}\\|U_{i}\\|_{p}^{p}\\sum_{j=1}^{n}\\left(x_{j}\\beta\\right)^{p}\\leq\\rho\\gamma^{p}\\|U_{i}\\|_{p}^{p}\\sum_{j=1}^{n}\\left(x_{j}\\beta-y_{j}^{1/p}\\right)^{p}}\\\\ &{\\qquad\\qquad\\leq\\lambda\\rho\\gamma^{p}\\|U_{i}\\|_{p}^{p}\\sum_{j=1}^{n}g_{y_{j}}(x_{j}\\beta)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Summing over all sensitivities, we get that the total sensitivity is bounded by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathfrak{S}\\le\\rho\\lambda(\\alpha\\gamma)^{p}+O\\left(\\log(y_{\\operatorname*{max}})+\\log\\log\\left(\\frac{y_{\\operatorname*{max}}}{\\eta}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For the first summand, there exists for each $p\\in[1,2]$ , a so-called Auerbach basis attaining $\\alpha=d,\\gamma=$ 1, see \u221a[43, Lemma 2.22]. For the \u221aspecial case $p=2$ , we even have tha\u221at any orthonorm\u221aal basis satisfies $\\alpha=\\sqrt{d},\\gamma=1$ since $\\|U\\|_{F}={\\sqrt{d}}$ , and for any $z\\in\\mathbb{R}^{d}\\colon\\|U z\\|_{2}=\\sqrt{z^{T}U^{T}U z}=\\sqrt{z^{T}z}=\\|z\\|_{2}$ . Thus, in both cases we have suitable bases with $(\\alpha\\gamma)^{p}=d$ . ", "page_idx": 21}, {"type": "text", "text": "For $p=1$ Lemma 2.2 yields $\\lambda\\leq O(\\sqrt{y_{\\mathrm{max}}/\\log y_{\\mathrm{max}}})$ . With this, the overall bound simplifies to ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathfrak{S}\\leq O\\left(\\rho d\\sqrt{y_{\\operatorname*{max}}/\\log y_{\\operatorname*{max}}}+\\log\\log(1/\\eta)\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For $p=2$ we have that $\\lambda=1$ suffices by Lemma 2.3. With this, the overall bound simplifies to ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathfrak{S}\\le O(\\rho d+\\log{(y_{\\operatorname*{max}})}+\\log{\\log(1/\\eta)}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Theorem 3.8. Let $X\\,\\in\\,\\mathbb{R}^{n\\times d},y\\,\\in\\,\\mathbb{N}_{0}^{n}$ be a $\\rho$ -complex dataset, i.e., Equation (2) holds. We can compute a weighted coreset $(K,w)\\in\\mathbb{R}^{k\\times d}\\times\\mathbb{R}_{>0}^{k}$ for the pth-root-link Poisson regression problem with $p\\,\\in\\,\\{1,2\\}$ on $D(\\eta)\\,:=\\,\\{\\beta\\,\\in\\,\\mathbb{R}^{d}\\;:\\;\\forall i$ , $x_{i}\\beta\\,>\\,\\eta\\}$ . The size of the coreset is bounded by $k=\\tilde{O}(\\varepsilon^{-2}d\\cdot\\operatorname*{min}\\{d,\\varepsilon^{-1}\\log(n)\\log(y_{\\mathrm{max}})\\}\\cdot m)$ , where ", "page_idx": 21}, {"type": "equation", "text": "$$\nm=\\left\\{\\begin{array}{l l}{\\rho d\\log\\log(d)\\sqrt{y_{\\operatorname*{max}}/\\log(y_{\\operatorname*{max}})}+\\log\\log(1/\\eta)}&{p=1}\\\\ {\\rho d+\\log\\left(y_{\\operatorname*{max}}\\right)+\\log\\log(1/\\eta)}&{p=2.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof of Theorem 3.8. We put our bounds from Corollary 3.6 and Lemma 3.7 together into the main theorem of the sensitivity framework Proposition A.5. That is, we calculate the sensitivity upper bounds $s_{i}$ , take a sample according to the distribution $\\begin{array}{r}{p_{i}=s_{i}/\\sum_{j=1}^{n}s_{j}}\\end{array}$ of the respective size, and reweight them accordingly. Then, the calculated bounds on the VC dimension and total sensitivity yield a bound on the required size, such that Proposition A.5 yields with constant probability that the reweighted subsample gives a $(1\\pm\\eta)$ -approximation uniformly over $\\beta\\in D(\\eta)$ . ", "page_idx": 21}, {"type": "text", "text": "Since the Auerbach basis used in the sensitivity calculations of Lemma 3.7 can be expensive to compute depending on the value of $p$ , we use more efficient approximation techniques here. ", "page_idx": 21}, {"type": "text", "text": "In the case $p=2$ , we use a sparse obliv\u221aious $\\ell_{2}$ subspace embedding by [9], which was explicitly proven in [34, Lemma 2\u221a.14], to give a $({\\sqrt{2d}},{\\sqrt{2}},2)$ -well-conditioned basis. This is within absolute constant factors to the $(\\sqrt{d},1,2)$ -conditioning of the Auerbach basis. Thus, the complexities given in Lemma 3.7 do not change in $O$ -notation. ", "page_idx": 21}, {"type": "text", "text": "In the case $p=1$ , we use a more recent technique introduced in [44], called $(\\alpha,\\gamma,p)$ -well-conditioned spanning sets. This is a relaxation of the well-conditioned basis $U$ given in Equation (7), where $U\\in\\mathbb{R}^{n\\times s}$ and $z\\in\\mathbb{R}^{s}$ , are allowed to have slightly increased dimension $s>d$ . We also note that we only need to bound norms of vectors of the form $X\\beta$ in the columnspan of the data matrix whose rank is bounded by $d$ . We thus require the bounds to hold only for $y\\in\\mathbb{R}^{s},s>d$ that actually represent vectors $X\\beta$ in a different basis. Other aspects of Equation (7) remain unchanged. ", "page_idx": 21}, {"type": "text", "text": "Our proof is nearly verbatim to [44, Theorem 1.11]. Their algorithm constructs a matrix $R\\in\\mathbb{R}^{d\\times s}$ . We set $U=X R\\in\\mathbb{R}^{n\\times s}$ . By [44, Lemma 4.1], this can be done with $s=O(d\\log\\log(d))$ , such that each column $U^{(i)}$ , for $i\\in[s]$ satisfies $\\|U^{(i)}\\|_{p}=1$ , and for every $\\|X\\beta\\|_{p}=1$ there exists a vector $y\\in\\mathbb{R}^{s}$ , such that $X\\beta=U y$ and $\\|y\\|_{2}=O(\\bar{1})$ . ", "page_idx": 21}, {"type": "text", "text": "For $\\textit{p}=\\textit{1}$ , this yields that $U$ is an $(\\alpha,\\gamma,1)$ -well-conditioned spanning set, where $\\alpha=$ $O(d\\log\\log(d))$ , and $\\gamma=O(1)$ . For the bound on $\\alpha$ , it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|U\\|_{1}=\\sum_{i=1}^{s}\\|U^{(i)}\\|_{1}=s=O(d\\log\\log(d)).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For the bound on $\\gamma$ , note that the H\u00f6lder dual for $p=1$ is $q=\\infty$ . Now, it follows for every $y\\in\\mathbb{R}^{s}$ that represents any $X\\beta$ as a linear combination of columns of $U$ that, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|y\\|_{\\infty}\\leq\\|y\\|_{2}\\leq O(1)=O(1)\\|X\\beta\\|_{1}=O(1)\\|U y\\|_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "As a consequence in the case $p=1$ , this computational result replaces the $d$ factor from the Auerbach basis in the proof of Lemma 3.7 by a factor $\\bar{(\\alpha\\gamma)}^{p}=O(d\\log\\bar{\\log}(d))$ , as we have claimed. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "D Proofs for main approximation result ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Recall Equation (8): ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\beta}^{*}:=\\operatorname*{argmin}_{\\beta^{\\prime}\\in D(\\eta)}f(X\\beta^{\\prime}),\\quad\\beta^{*}:=\\operatorname*{argmin}_{\\beta\\in D(0)}f(X\\beta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma 4.1. It holds for sufficiently small $\\eta>0$ that ", "page_idx": 22}, {"type": "equation", "text": "$$\nf(X{\\boldsymbol{\\beta}}^{*})\\leq f(X{\\boldsymbol{\\tilde{\\beta}}}^{*})\\leq(1+O(\\eta))f(X{\\boldsymbol{\\beta}}^{*}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof of Lemma 4.1. Recall from Section 2 that we choose $X$ to be the design matrix that includes an intercept, i.e., every row of $X$ is of the form $x=(1,x^{(1)},x^{(2)},\\ldots,x^{(d-1)})$ . Note that by definition we have that $D(\\eta)\\subset D(0)$ is a proper subset. Thus, $f(X{\\boldsymbol{\\beta}}^{*})\\leq f(X{\\tilde{\\beta}}^{*})$ follows immediately. ", "page_idx": 22}, {"type": "text", "text": "Next, define for every $\\beta\\in D(0)$ its shifted version to be in one-to-one correspondence with a unique $\\beta^{\\prime}\\in D(\\eta)$ via the translation ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\beta^{\\prime}:=\\beta+\\eta e_{1},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $e_{1}$ is the first standard basis vector. Recall that we choose the first column of $X$ to be $(1,\\dots,1)\\in\\mathbb{R}^{n}$ . ", "page_idx": 22}, {"type": "text", "text": "Now consider $(\\beta^{*})^{\\prime}\\in D(\\eta)$ to be the shifted version of the global optimizer $\\beta^{*}\\in D(0)$ . Since ${\\tilde{\\beta}}^{*}$ minimizes the loss function over $D(\\eta)$ , it follows that $f(X{\\tilde{\\beta}}^{*})\\leq f(X(\\beta^{*})^{\\prime})$ . ", "page_idx": 22}, {"type": "text", "text": "Finally, we claim that there exists an absolute constant $C\\geq0$ such that for all sufficiently small $\\eta>0$ we have for all $\\beta$ that Equation (9) holds, i.e., ", "page_idx": 22}, {"type": "equation", "text": "$$\nf(X\\beta^{\\prime})\\le f(X\\beta)+\\eta^{p}n+\\eta C f(X\\beta).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By summing the result of Lemma 2.1 over all $n$ inputs, we have that $f(X{\\boldsymbol{\\beta}}^{*})\\geq n$ . Applying our claim to $\\beta^{*}$ thus yields ", "page_idx": 22}, {"type": "equation", "text": "$$\nf(X(\\beta^{*})^{\\prime})\\leq f(X\\beta^{*})+\\eta^{p}n+\\eta C f(X\\beta^{*})\\leq(1+\\eta+C\\eta)f(X\\beta^{*})=(1+O(\\eta))f(X\\beta^{*}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This proves that Lemma 2.1 and Equation (9) imply the upper bound of the lemma. ", "page_idx": 22}, {"type": "text", "text": "It remains to prove our claim of Equation (9) for $p=1$ , and $p=2$ separately. ", "page_idx": 22}, {"type": "text", "text": "Case $p=1$ : We have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{f(X\\beta^{\\prime})=\\sum_{i\\in[n]}(x_{i}\\beta+\\eta)-y_{i}\\log(x_{i}\\beta+\\eta)+\\log(y_{i}!)}}\\\\ &{\\le\\displaystyle\\sum_{i\\in[n]}x_{i}\\beta-y_{i}\\log(x_{i}\\beta)+\\log(y_{i}!)+\\eta=f(X\\beta)+\\eta n,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which satisfies Equation (9) with $C=0$ . ", "page_idx": 22}, {"type": "text", "text": "Case $p=2$ : We have ", "text_level": 1, "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{f(X\\beta^{\\prime})=\\sum_{i\\in[n]}(x_{i}\\beta)^{2}+2\\eta x_{i}\\beta+\\eta^{2}-2y_{i}\\log(x_{i}\\beta+\\eta)+2y_{i}\\log(x_{i}\\beta)-2y_{i}\\log(x_{i}\\beta)+\\log y_{i}\\mathcal{M}}}\\\\ &{=\\sum_{i\\in[n]}(x_{i}\\beta)^{2}+2\\eta x_{i}\\beta+\\eta^{2}-2y_{i}\\log\\frac{x_{i}\\beta+\\eta}{x_{i}\\beta}-2y_{i}\\log(x_{i}\\beta)+\\log y_{i}\\mathcal{M}}\\\\ &{=f(X\\beta)+\\sum_{i\\in[n]}2\\eta x_{i}\\beta+\\eta^{2}-2y_{i}\\log\\frac{x_{i}\\beta+\\eta}{x_{i}\\beta}}\\\\ &{=f(X\\beta)+\\sum_{i\\in[n]}2\\eta x_{i}\\beta+\\eta^{2}-2y_{i}\\log\\left(1+\\frac{\\eta}{x_{i}\\beta}\\right)}\\\\ &{=f(X\\beta)+\\sum_{i\\in[n]}2\\eta x_{i}\\beta+\\eta^{2}-2y_{i}\\log\\left(1+\\frac{\\eta}{x_{i}\\beta}\\right)}\\\\ &{=f(X\\beta)+n\\eta^{2}+2\\eta\\sum_{i\\in[n]}x_{i}\\beta-\\frac{y_{i}\\beta}{\\eta}\\log\\left(1+\\frac{\\eta}{x_{i}\\beta}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now using that $\\begin{array}{r}{\\log(1+x)\\geq\\frac{x}{1+x},\\forall x>-1}\\end{array}$ , we bound the error for every $y\\in\\mathbb N$ by a function $\\phi=\\phi_{y}$ . ", "page_idx": 23}, {"type": "equation", "text": "$$\nz-\\frac{y}{\\eta}\\log\\left(1+\\frac{\\eta}{z}\\right)\\leq z-\\frac{y}{\\eta}\\frac{\\eta}{z}\\frac{z}{z+\\eta}=z-\\frac{y}{z+\\eta}=:\\phi(z),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The first and second derivatives are given by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{c c c}{\\displaystyle{\\phi^{\\prime}(z)=1+\\frac{y}{(z+\\delta)^{2}}\\geq1,}}\\\\ {\\displaystyle{\\phi^{\\prime\\prime}(z)=-\\frac{2y(z+\\delta)}{(z+\\delta)^{4}}<0,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "from which we know that the function is monotonically increasing and concave. On the other hand, we know that $g_{y}(z)$ is convex, monotonically increasing on $z\\in[\\bar{y}^{1/2},\\infty)$ and is bounded below by 1. We can thus show the claim for $C=3$ by comparing the functions as well as their derivatives at $z=y^{1/2}+1$ . ", "page_idx": 23}, {"type": "text", "text": "First note that by monotonicity, we have for all $z\\le y^{1/2}+1$ that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\phi(z)\\le\\phi(y^{1/2}+1)=\\frac{(y^{1/2}+1)^{2}+\\eta(y^{1/2}+1)-y}{y^{1/2}+1+\\eta}=\\frac{y+2y^{1/2}+1+\\eta y^{1/2}+\\eta-y}{y^{1/2}+1+\\eta}}\\\\ {=\\frac{y^{1/2}+1+\\eta+(1+\\eta)y^{1/2}}{y^{1/2}+1+\\eta}=1+\\frac{(1+\\eta)y^{1/2}}{y^{1/2}+1+\\eta}}\\\\ {\\le1+\\frac{(1+\\eta)y^{1/2}}{y^{1/2}}=1+1+\\eta\\le3\\cdot1\\le3g(z).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In particular this holds for $z=y^{1/2}+1$ as well. ", "page_idx": 23}, {"type": "text", "text": "It remains to compare the derivatives for the choice of $z=y^{1/2}+1$ . We have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\phi^{\\prime}(y^{1/2}+1)=1+\\frac{y}{(y^{1/2}+1+\\delta)^{2}}\\leq1+\\frac{y}{(y^{1/2})^{2}}=2,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which implies that we also have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{3g^{\\prime}(y^{1/2}+1)=3\\cdot2\\frac{(y^{1/2}+1)^{2}-y}{y^{1/2}+1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=3\\cdot2\\frac{y+2y^{1/2}+1-y}{y^{1/2}+1}}\\\\ &{\\qquad\\qquad\\qquad=3\\cdot2\\frac{2y^{1/2}+1}{y^{1/2}+1}\\geq3\\cdot2>2\\geq\\phi^{\\prime}(y^{1/2}+1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This completes the proof of $\\phi(z)\\leq3g(z)$ for all $z$ by convexity of $g$ and concavity of $\\phi$ ", "page_idx": 23}, {"type": "text", "text": "Overall, the claim of Equation (9) follows with $C\\,=\\,3(p-1)$ , for both $p\\,\\in\\,\\{1,2\\}$ , which also concludes the proof of the lemma. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Theorem 4.2. Let $\\varepsilon\\,\\in\\,(0,1/14)$ . Let $(C,w)$ be a coreset according to Theorem 3.8. Let ${\\tilde{\\beta}}:=$ $\\begin{array}{r}{\\operatorname*{argmin}_{\\beta\\in D(\\varepsilon)}f_{w}(C\\beta),\\,\\beta^{*}:=\\operatorname*{argmin}_{\\beta\\in D(0)}f(X\\beta).}\\end{array}$ . Then ", "page_idx": 23}, {"type": "equation", "text": "$$\nf(X{\\beta}^{*})\\leq f(X{\\tilde{\\beta}})\\leq(1+\\varepsilon)f(X{\\beta}^{*}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof of Theorem 4.2. We invoke Lemma 4.1 with $\\eta=\\varepsilon$ to show that the shifted version of $\\beta^{*}$ , i.e., $\\beta_{g o o d}:=(\\beta^{*})^{\\prime}=\\beta^{*}+\\eta e_{1}$ is a $(1+\\varepsilon)$ -approximation and $\\beta_{g o o d}\\,\\in\\,D(\\eta)$ . Thus, the optimizer $(\\bar{\\beta}^{\\prime})^{*}\\in D(\\eta)$ cannot be worse than a $(1+\\varepsilon)$ -approximation. ", "page_idx": 23}, {"type": "text", "text": "The coreset construction of Theorem 3.8 works uniformly over $D(\\eta)$ . It thus yields a coreset $C\\subset X$ of size $k$ with weights $w\\in\\mathbb{R}^{k}$ such that if we denote the weighted loss on the coreset by $f_{w}(C\\beta)$ , it satisfies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\forall\\beta\\in D(\\eta):(1-\\varepsilon)f(X\\beta)\\leq f_{w}(C\\beta)\\leq(1+\\varepsilon)f(X\\beta)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then defining $\\beta_{g o o d}:=\\beta^{*}+\\eta e_{1}$ , we have $f(X{\\tilde{\\beta}})\\,\\geq\\,f(X{\\beta}^{*})$ since $D(\\eta)\\,\\subset\\,D(0)$ . Moreover, assuming $0<\\varepsilon\\le\\frac{1}{2}$ we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(X\\tilde{\\beta})\\leq\\displaystyle\\frac{1}{1-\\varepsilon}f_{w}(C\\tilde{\\beta})\\leq\\displaystyle\\frac{1}{1-\\varepsilon}f_{w}(C\\beta_{g o o d})}\\\\ &{\\qquad\\quad\\leq\\displaystyle\\frac{1+\\varepsilon}{1-\\varepsilon}f(X\\beta_{g o o d})\\leq\\displaystyle\\frac{(1+\\varepsilon)^{2}}{1-\\varepsilon}f(X\\beta^{*})\\leq(1+7\\varepsilon)f(X\\beta^{*})}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "rescaling $\\varepsilon$ finishes our main result. ", "page_idx": 24}, {"type": "text", "text": "E Proofs for lower bounds ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Lemma 6.1. Consider a number $n\\geq8$ of points equidistant on a unit circle in a 2-dimensional affine subspace embedded in $\\mathbb{R}^{d},d\\geq3$ , each with label $y_{i}=1$ . Then the sensitivity of each point for the Poisson model with pth-root-link for $p\\in\\{1,2\\}$ is arbitrarily close to 1. Consequently, any coreset for the Poisson regression model must comprise all $\\Omega(n)$ input points. ", "page_idx": 24}, {"type": "text", "text": "Proof of Lemma 6.1. We first note that our construction can be embedded arbitrarily in $d\\geq\\ 3$ dimensional spaces. For simplicity, we describe the construction for $d=3$ , where the first dimension corresponds to the affine translation and the other two describe the location in the 2-dimensional subspace. ", "page_idx": 24}, {"type": "text", "text": "Recall that our point set is given by $\\begin{array}{r}{x_{i}\\,=\\,(1,\\cos(\\frac{2\\pi i}{n}),\\sin(\\frac{2\\pi i}{n}))}\\end{array}$ , $i\\,\\in\\,[n]$ , and $y_{i}=1$ for every $i\\,\\in\\,[n]$ . By symmetry of the construction, it suffices to analyze w.l.o.g. the sensitivity of point $x_{n}\\doteq(1,\\bar{\\cos(2\\pi)},\\sin(2\\pi))=(1,1,0)$ . Since the sensitivity is defined as the supremum over all $\\beta$ , it also suffices to find one $\\beta$ for which the sensitivity is arbitrarily close to 1. To this end, for a small $\\eta>0$ yet to be determined, we choose ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\beta=(1+\\eta,-\\cos(2\\pi),-\\sin(2\\pi))^{T}=(1+\\eta,-1,0)^{T},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $1+\\eta$ represents a translation term and $(-1,0)^{T}$ represents a \u2018normal\u2019 term that lives within the 2-dimensional subspace mentioned above. This normal term defines a hyperplane $H$ (which is in fact a line) within the 2-dimensional subspace. The normal points towards the center of the point set, and the hyperplane $H$ is at distance exactly $x_{n}\\beta=1+\\eta-1=\\eta$ from $x_{n}$ . ", "page_idx": 24}, {"type": "text", "text": "A simple trigonometric calculation yields that the separation between $x_{n}$ and the neighboring points $x_{1}$ and $x_{n-1}$ along the direction orthogonal to $H$ is exactly $1-\\cos(2\\pi/n)$ . Since $n\\geq8$ , it holds that $(2\\pi/n)^{2}/3\\le^{\\cdot}1-\\cos(2\\pi/n)\\le(\\bar{2}\\pi/n)^{2}/2$ by a second order Taylor series expansion of the cosine function. All other points are even farther away from $x_{n}$ than $x_{1}$ and $x_{n-1}$ , and therefore also further from $H$ . Also note that if we let $x_{n}^{\\prime}=(1,-1,0)$ be the antipodal point of $x_{n}$ on the circle, we see that the distances of all points from $H$ are less than $x_{n}^{\\prime}\\beta=1+\\eta+1=2+\\eta<3$ . ", "page_idx": 24}, {"type": "text", "text": "Recall that for arbitrary $p\\geq1$ , the function $g_{y}$ defined in Equation (4) is minimized at $y^{1/p}$ , which in this case equals $y_{i}=1$ . We have that roughly half of the points are at distance at least $1+\\eta$ and distance at most 3 from $H$ . By strict convexity, we have that $g_{1}$ is also strictly increasing on the interval $\\lbrack1,\\infty)$ . We can thus upper bound the contribution of each of these points by at most ", "page_idx": 24}, {"type": "equation", "text": "$$\ng_{1}(x_{i}\\beta)\\leq{\\frac{n}{2}}g_{1}(3)=(3^{p}-p\\log(3))\\leq9-1=8\\leq8\\log(n).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For the other half of the points (except $x_{n}$ ) the contribution is upper bounded by the loss that occurs closest to $H$ . By strict convexity again, we have that $g_{1}$ is also strictly decreasing on the interval $(0,1]$ . We argued that the points are sufficiently separated, so we get that each of their contributions is bounded by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{1}(x_{i}\\beta)\\leq g_{1}(1-\\cos(2\\pi/n))\\leq g_{1}((2\\pi/n)^{2}/3)\\leq g_{1}(1/n^{2})}\\\\ &{\\qquad\\qquad\\leq1/n^{2p}-2p\\log(1/n)=1/n^{2}+2p\\log(n)}\\\\ &{\\qquad\\qquad\\leq1+4\\log(n)\\leq8\\log(n).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now, choosing $\\eta=\\exp(-n^{2})$ , we have that the cost of the point $x_{n}$ is lower bounded by ", "page_idx": 24}, {"type": "equation", "text": "$$\ng_{1}(x_{n}\\beta)=g_{1}(\\eta)\\geq\\left({\\frac{1}{\\exp(n^{2})}}\\right)^{p}+p\\log(\\exp(n^{2}))\\geq n^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\varsigma_{n}=\\operatorname*{sup}_{\\beta^{\\prime}}\\frac{g_{1}(x_{n}\\beta^{\\prime})}{\\sum_{i=1}^{n}g_{1}(x_{i}\\beta^{\\prime})}\\ge\\frac{n^{2}}{n^{2}+8n\\log(n)}\\stackrel{n\\to\\infty}{\\longrightarrow}1.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since there is no sensitivity upper bound below 1 that holds for arbitrarily large $n$ and for each point, [41, Lemma A.1] implies that the coreset must comprise all $\\Omega(n)$ points. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Lemma 6.2. Let $\\Sigma_{D}$ be a data structure for $D=[X,y]\\in\\mathbb{R}^{n\\times d}\\times\\mathbb{R}^{n},\\,d\\geq$ $d\\geq3,$ , that approximates negative log-likelihood queries $\\Sigma_{D}(\\beta)$ for Poisson regression with the pth-root-link for $p\\in\\{1,2\\}$ , such that for some $\\varphi\\geq1$ it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\forall\\beta\\in\\mathbb{R}^{d}:f(X\\beta)\\leq\\Sigma_{D}(\\beta)\\leq\\varphi\\cdot f(X\\beta).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "$\\begin{array}{r}{I f\\varphi<\\frac{n}{8\\log(n)}}\\end{array}$ then $\\Sigma_{D}$ requires $\\Omega(n)$ bits of memory. ", "page_idx": 25}, {"type": "text", "text": "Proof of Lemma 6.2. We reduce from the indexing problem for which we know that it has one-way randomized communication complexity $\\Omega(n)$ [25]. We construct a protocol as follows. Alice is given a vector $b\\in\\{0,1\\}^{n}$ . She produces for every $i$ with $b_{i}=1$ the points $\\begin{array}{r}{x_{i}=(1,\\cos(\\frac{2\\pi i}{n}),\\sin(\\frac{2\\pi i}{n}))}\\end{array}$ in canonical order. The corresponding counts are set to $y_{i}=1$ . She builds and sends $\\Sigma_{D}$ to Bob, whose task is to guess the bit $b_{j}$ . Let the size of $\\Sigma_{D}$ in bit complexity be $s(n)$ bits, and note that $s(n)$ corresponds to the amount of bits that have been communicated. Bob chooses to query $\\begin{array}{r}{\\beta=(1+\\eta,-\\cos(\\frac{2\\pi j}{n}),-\\sin(\\frac{2\\pi j}{n}))}\\end{array}$ . ", "page_idx": 25}, {"type": "text", "text": "By symmetry of the construction, we can assume w.l.o.g. that the upper bounds Equations (18) and (19) on the costs in the proof of Lemma 6.1 continue to hold. ", "page_idx": 25}, {"type": "text", "text": "Thus, if $b_{j}=0$ , then $x_{j}$ does not exist and the cost of all other points is bounded from above by ", "page_idx": 25}, {"type": "equation", "text": "$$\nf(X{\\boldsymbol{\\beta}})\\leq8n\\log(n).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "If $b_{j}=1$ , then $x_{j}$ is at distance exactly ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{j}\\beta=\\left(1,\\cos\\left(\\frac{2\\pi j}{n}\\right),\\sin\\left(\\frac{2\\pi j}{n}\\right)\\right)\\cdot\\left(1+\\eta,-\\cos\\left(\\frac{2\\pi j}{n}\\right),-\\sin\\left(\\frac{2\\pi j}{n}\\right)\\right)^{T}}\\\\ &{\\quad\\quad=1+\\eta-\\cos\\left(\\frac{2\\pi j}{n}\\right)^{2}-\\sin\\left(\\frac{2\\pi j}{n}\\right)^{2}=1+\\eta-1=\\eta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus, choosing $\\eta=\\exp(-n^{2})$ , the cost is bounded below by $f(X\\beta)\\ge g_{1}(\\eta)\\ge n^{2}$ as in the proof of Lemma 6.1. ", "page_idx": 25}, {"type": "text", "text": "Given that $\\begin{array}{r}{\\varphi<\\frac{n^{2}}{8n\\log(n)}=\\frac{n}{8\\log(n)}}\\end{array}$ , Bob can distinguish these two cases based on the data structure only, by deciding whether $\\Sigma_{D}(\\beta)$ is strictly smaller or larger than $n^{2}$ . Consequently, it holds that $s(\\bar{n)}\\geq\\bar{\\Omega}(n)$ , since this solves the indexing problem. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Lemma 6.3. For all $x\\in[-1/e,0)$ , it holds that $W_{0}(x)\\leq\\sqrt{2(1+e x)}-1$ . ", "page_idx": 25}, {"type": "text", "text": "Proof of Lemma 6.3. First we claim that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\tau-\\log(1+\\tau)\\geq-\\frac{1}{2}\\log(1-\\tau^{2})=-\\log(\\sqrt{1-\\tau^{2}}),\\quad\\tau\\in(-1,0].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Define $\\begin{array}{r}{l(\\tau):=\\tau-\\log(1+\\tau)+\\frac{1}{2}\\log(1-\\tau^{2})}\\end{array}$ . Then $l(0)=0$ . The derivative of $l$ is ", "page_idx": 25}, {"type": "equation", "text": "$$\nl^{\\prime}(\\tau)=1-\\frac{1}{1+\\tau}-\\frac{1}{2}\\cdot\\frac{2\\tau}{1-\\tau^{2}}=\\frac{1-\\tau^{2}-1+\\tau-\\tau}{1-\\tau^{2}}=\\frac{-\\tau^{2}}{1-\\tau^{2}}<0,\\quad\\forall\\tau\\in(-1,0]\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which implies that for every $\\tau\\in(-1,0]$ we have $l(\\tau)\\geq0$ . This proves Equation (20). Next, we follow the proof strategy of [38, Theorem 3.2]: define a new variable $\\tau=\\bar{\\tau_{}}(x):=-(W_{0}(x)+1)$ , so that $-W_{0}(x)=1+\\tau$ . Since $W_{0}(x)e^{W_{0}(x)}=x$ for $x\\geq-1/e$ , the definition of $\\tau$ implies that $(1+\\tau)e^{-(1+\\tau)}=-x.$ . Let $x\\in[-1/e,0)$ be arbitrary. Then since $W_{0}(0)=0$ and $W_{0}(-1/e)=-1$ , we have $\\tau(x)\\in(-1,0]$ , and thus ", "page_idx": 25}, {"type": "equation", "text": "$$\n(1+\\tau)e^{-(1+\\tau)}=-x\\Longleftrightarrow\\qquad\\tau-\\log(1+\\tau)=-\\log(-x)-1\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\implies}&&{-\\log(\\sqrt{1-\\tau^{2}})\\leq-\\log(-x)-1}&&{\\mathrm{by~Equation~}(20\\leq x)}\\\\ &{\\iff}&&{1+\\log(-x)\\leq\\log(\\sqrt{1-\\tau^{2}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The last inequality is equivalent to ", "page_idx": 26}, {"type": "equation", "text": "$$\n1\\leq\\log\\left({\\frac{\\sqrt{1-\\tau^{2}}}{-x}}\\right)\\Longleftrightarrow e\\leq{\\frac{\\sqrt{1-\\tau^{2}}}{-x}}\\Longleftrightarrow-e x\\leq{\\sqrt{1-\\tau^{2}}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Now since $\\begin{array}{r}{(\\sqrt{1-\\tau^{2}})^{2}=1-\\tau^{2}\\leq1-\\tau^{2}+(\\frac{\\tau^{2}}{2})^{2}=(1-\\frac{\\tau^{2}}{2})^{2}}\\end{array}$ for every $\\tau\\in(-1,0]$ , it follows that ", "page_idx": 26}, {"type": "equation", "text": "$$\n-e x\\leq1-\\frac{\\tau^{2}}{2}\\Longleftrightarrow\\frac{\\tau^{2}}{2}\\leq1+e x\\Longleftrightarrow|W_{0}(x)+1|\\leq\\sqrt{2(1+e x)},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the rightmost equivalence above follows from the definition $\\tau:=-(W_{0}(x)\\!+\\!1)$ . The inequality on the right-hand side above implies the desired conclusion. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "Recall the function $g_{y_{i}}$ defined in Equation (4). ", "page_idx": 26}, {"type": "text", "text": "Lemma 6.4. Let $y\\,\\in\\,\\mathbb{N}$ be arbitrary, $p\\,=\\,1$ , and $\\tau\\,=\\,y^{1/p}$ in the definition Equation (4) of $g_{y}$ . Let $\\begin{array}{r}{h_{\\lambda}(z):=\\,\\frac{(z-y^{1/p})^{p}}{\\lambda}}\\end{array}$ for $z\\,>\\,0$ . Then $g_{y}$ and $h_{\\lambda}$ are tangent to each other if and only $i f$ $\\begin{array}{r}{\\lambda=\\lambda^{*}(y)=(W_{0}(\\frac{-y}{(y!)^{1/y}\\exp(2)})+1)^{-1}}\\end{array}$ , in which case the unique tangent point is $\\begin{array}{r}{z^{\\ast}(y)=\\frac{y\\lambda^{\\ast}(y)}{\\lambda^{\\ast}(y)-1}}\\end{array}$ In addition, $\\lambda^{*}(y)=\\Theta(\\sqrt{y_{\\operatorname*{max}}/\\log(y_{\\operatorname*{max}})})$ . ", "page_idx": 26}, {"type": "text", "text": "Proof of Lemma 6.4. A point of tangency $\\tilde{z}$ of the curves $g_{y}$ and $h_{\\lambda}$ is defined as a point where the functions agree and their derivatives agree. To identify the point where the derivatives of $g_{y}$ and $h_{\\lambda}$ agree, we observe ", "page_idx": 26}, {"type": "equation", "text": "$$\ng_{y}^{\\prime}(\\tilde{z})=1-\\frac{y}{z}=\\frac{1}{\\lambda}=h_{\\lambda}^{\\prime}(\\tilde{z})\\Longleftrightarrow1-\\frac{1}{\\lambda}=\\frac{y}{\\tilde{z}}\\Longleftrightarrow\\tilde{z}=\\frac{\\lambda y}{\\lambda-1}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since $\\begin{array}{r}{g_{y}(z)\\,\\ge\\,\\frac{1}{2}\\log(2\\pi y)}\\end{array}$ and $h_{\\lambda}(z)<0$ for $z<y$ if $\\tau=y$ , the tangent point cannot lie in the interval $(0,y]$ . Hence, $\\tilde{z}>y$ must hold. Combining this observation with the equation $\\begin{array}{r}{\\tilde{z}=\\frac{\\lambda y}{\\lambda-1}}\\end{array}$ for the point where the derivatives agree, we conclude that $\\lambda>1$ . ", "page_idx": 26}, {"type": "text", "text": "Now suppose that $\\tilde{z}$ is a point where the functions $g_{y}$ and $h_{\\lambda}$ agree: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{h_{\\lambda}(\\bar{z})=g_{y}(\\bar{z})}\\\\ &{\\Longleftrightarrow}&{y\\log(\\bar{z})-\\bar{z}+\\frac{\\bar{z}}{\\lambda}=\\log(y!)}\\\\ &{\\Longleftrightarrow}&{y\\log(\\bar{z})-\\bar{z}\\frac{\\lambda-1}{\\lambda}=\\frac{y}{\\lambda}+\\log(y!)}\\\\ &{\\Longleftrightarrow}&{\\log(\\bar{z})-\\bar{z}\\left(\\frac{\\lambda y}{\\lambda-1}\\right)^{-1}=\\frac{1}{\\lambda}+\\frac{1}{y}\\log(y!)}\\\\ &{\\Longleftrightarrow}&{\\bar{z}\\exp\\left(-\\bar{z}\\left(\\frac{\\lambda y}{\\lambda-1}\\right)^{-1}\\right)=\\exp\\left(\\frac{1}{\\lambda}\\right)(y!)^{1/y}}\\\\ &{\\Longleftrightarrow}&{\\left(-\\bar{z}\\left(\\frac{\\lambda y}{\\lambda-1}\\right)^{-1}\\right)\\exp\\left(-\\bar{z}\\left(\\frac{\\lambda y}{\\lambda-1}\\right)^{-1}\\right)=-\\left(\\frac{\\lambda y}{\\lambda-1}\\right)^{-1}\\exp\\left(\\frac{1}{\\lambda}\\right)(y!)^{1/y}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "If $\\tilde{z}$ is a point of tangency, then by the equivalent condition above for $g_{y}^{\\prime}\\,=\\,h_{\\lambda}^{\\prime}$ , it follows that $\\begin{array}{r}{\\tilde{z}=\\frac{\\lambda y}{\\lambda-1}}\\end{array}$ . Substituting this into the last equation above yields ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{-1\\exp(-1)=-\\left(\\displaystyle\\frac{\\lambda y}{\\lambda-1}\\right)^{-1}\\exp\\left(\\displaystyle\\frac{1}{\\lambda}\\right)(y!)^{1/y}}\\\\ {\\iff}&{}&{\\qquad-\\displaystyle\\frac{y}{(y!)^{1/y}}\\exp(-1)=-\\displaystyle\\frac{\\lambda-1}{\\lambda}\\exp\\left(\\displaystyle\\frac{1}{\\lambda}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\iff\\qquad\\qquad-{\\frac{y}{(y!)^{1/y}}}\\exp(-2)=\\left({\\frac{1}{\\lambda}}-1\\right)\\exp\\left({\\frac{1}{\\lambda}}-1\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The last equation above yields that ", "page_idx": 27}, {"type": "equation", "text": "$$\n{\\frac{1}{\\lambda}}-1=W_{k}\\left({\\frac{-y}{(y!)^{1/y}\\exp(2)}}\\right),\\quad k\\in\\mathbb{Z}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We can further specify the branches of the Lambert W function as follows. By definition of the factorial, $1\\leq{\\frac{y}{(y!)^{1/y}}}$ for all $y\\in\\mathbb N$ . On the other hand, by Stirling\u2019s approximation, ", "page_idx": 27}, {"type": "equation", "text": "$$\n{\\frac{\\exp(1-{\\frac{1}{12y^{2}}})}{(2\\pi y)^{1/(2y)}}}<{\\frac{y}{(y!)^{1/y}}}<{\\frac{\\exp(1-{\\frac{1}{12y^{2}+y}})}{(2\\pi y)^{1/(2y)}}}<\\exp(1),\\quad y\\in\\mathbb{N}\\setminus\\{1\\}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Hence, ", "page_idx": 27}, {"type": "equation", "text": "$$\n-\\exp(-1)<-\\frac{y}{(y!)^{1/y}}\\exp(-2)\\leq-\\exp(-2),\\quad y\\in\\mathbb{N}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This implies that the argument of the Lambert $\\mathrm{w}$ function in Equation (21) lies in the interval $(-e^{-1},\\overleftarrow{-e}^{-2}]$ . By definition of the Lambert $\\mathrm{w}$ function, this in turn implies that in Equation (21), we need to consider only $k=0$ and $k=-1$ , which means that ", "page_idx": 27}, {"type": "equation", "text": "$$\n{\\frac{1}{\\lambda^{*}}}={\\frac{1}{\\lambda^{*}(y)}}\\in\\left\\{W_{k}\\left({\\frac{-y}{(y!)^{1/y}\\exp(2)}}\\right)+1\\,:\\,k\\in\\{0,-1\\}\\right\\}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For all $x\\ \\in\\ [-\\exp(-1),-\\exp(-2)]$ , $W_{0}(x)\\;\\geq\\;W_{-1}(x)$ , with equality holding only for $x=$ $-\\exp(-1)$ . This follows from the definition of $W_{0}$ as the principal branch of the Lambert $\\mathrm{W}$ function. Hence, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\lambda^{*}(y)=\\frac{1}{W_{0}\\left(\\frac{-y}{(y!)^{1/y}\\exp(2)}\\right)+1},\\quad y\\in\\mathbb{N},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which proves the first statement of Lemma 6.4. ", "page_idx": 27}, {"type": "text", "text": "Next, we show that $\\lambda^{*}(y)=\\Theta(\\sqrt{y_{\\operatorname*{max}}/\\log(y_{\\operatorname*{max}})})$ . We use a lower bound for the principal branch $W_{0}$ of the Lambert W function for negative arguments from [38, Theorem 3.2]: ", "page_idx": 27}, {"type": "equation", "text": "$$\n(e x+1)^{1/2}-1\\leq W_{0}(x),\\quad\\forall x\\in[-e^{-1},0].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since we showed above that the argument of the Lambert $\\mathrm{w}$ function in Equation (21) lies in the interval $(-e^{-1},-e^{-2}]$ , we may apply Equation (23). ", "page_idx": 27}, {"type": "text", "text": "Combining Lemma 6.3 with Equation (23), implies that for all $x\\in[-e^{-1},0]$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sqrt{e x+1}-1\\leq W_{0}(x)\\leq\\sqrt{2(1+e x)}-1\\Leftrightarrow\\frac{1}{\\sqrt{2(1+e x)}}\\leq\\frac{1}{W_{0}(z)+1}\\leq\\frac{1}{\\sqrt{(1+e x)}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By Equation (22), we may thus set \u2212(y!)y1/y e\u22122 in the above inequalities, which yields ", "page_idx": 27}, {"type": "equation", "text": "$$\n2^{-1/2}\\left(\\frac{-y}{(y!)^{1/y}}e^{-1}+1\\right)^{-1/2}\\leq\\frac{1}{W_{0}\\left(\\frac{-y}{(y!)^{1/y}}e^{-2}\\right)+1}=\\lambda^{*}(y)\\leq\\left(\\frac{-y}{(y!)^{1/y}}e^{-1}+1\\right)^{-1/2}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for all $y\\in\\mathbb{N}$ . Again by Equation (22), ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\cdot-\\exp\\left(-\\frac{1}{12y^{2}+y}-\\frac{1}{2y}\\log(2\\pi y)\\right)<\\frac{-y}{(y!)^{1/y}}e^{-1}+1<1-\\exp\\left(-\\frac{1}{12y^{2}}-\\frac{1}{2y}\\log(2\\pi y)\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Note that ", "page_idx": 27}, {"type": "equation", "text": "$$\n0<\\frac{1}{12y^{2}+y}+\\frac{1}{2y}\\log(2\\pi y)<\\frac{1}{12y^{2}}+\\frac{1}{2y}\\log(2\\pi y).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since $\\begin{array}{r}{y\\mapsto\\frac{1}{12y^{2}}+\\frac{1}{2y}\\log(2\\pi y)}\\end{array}$ is decreasing on $\\lbrack1,\\infty)$ and has the value $\\begin{array}{r}{\\frac{1}{12}+\\frac{\\log(2\\pi)}{2}<\\frac{1}{2}}\\end{array}$ at $y=1$ it suffices to consider the quantity $1-\\exp(-x)$ for $x\\in[0,\\frac{1}{2}]$ . By Taylor\u2019s approximation, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{x}{2}<1-\\exp(-x)<x,\\quad\\forall x\\in[0,1]\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and applying the lower bound in Equation (25) to the lower bound for \u2212(y!)y1/y e\u22121 + 1 below Equation (24) yields ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{-y}{(y!)^{1/y}}e^{-1}+1>1-\\exp\\left(-\\frac{1}{12y^{2}+y}-\\frac{1}{2y}\\log(2\\pi y)\\right)}}\\\\ &{\\leq\\frac{1}{2}\\left(\\frac{1}{12y^{2}+y}+\\frac{1}{2y}\\log(2\\pi y)\\right)}\\\\ &{=\\frac{1}{2}\\cdot\\frac{1+6(y+1/12)\\log(2\\pi y)}{12y^{2}+y}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for all $y\\in\\mathbb{N}$ . Applying this to the upper bound for $\\lambda^{*}(y)$ in Equation (24) yields for all $y\\in\\mathbb N$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\lambda^{*}(y)\\leq\\left(2\\cdot\\frac{12y^{2}+y}{1+6(y+1/12)\\log(2\\pi y)}\\right)^{1/2}\\leq\\left(\\frac{26y^{2}}{6y\\log(2\\pi y)}\\right)^{1/2}=O\\left(\\sqrt{\\frac{y}{\\log(y)}}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Applying the upper bound in Equation (25) to the upper bound for $-{\\frac{y}{(y!)^{1/y}}}e^{-1}+1$ below Equation (24) yields ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{-y}{(y!)^{1/y}}e^{-1}+1<\\frac{1}{12y^{2}}+\\frac{1}{2y}\\log(2\\pi y)=\\frac{1+6y\\log(2\\pi y)}{12y^{2}}<\\frac{12y\\log(2\\pi y)}{12y^{2}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the rightmost inequality follows since the function $y\\mapsto6y\\log(2\\pi y)$ is increasing on $\\lbrack1,\\infty)$ and has a value strictly larger than 1 at $y=1$ . Applying the inequality above to the lower bound for $\\lambda^{*}(y)$ in Equation (24) yields ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\lambda^{*}(y)\\geq\\left(\\frac{1}{2}\\frac{y^{2}}{y\\log(2\\pi y)}\\right)^{1/2}=\\Omega\\left(\\sqrt{\\frac{y}{\\log(y)}}\\right),\\quad\\forall y\\in\\mathbb{N}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "This completes the proof that $\\lambda^{*}(y)=\\Theta(\\sqrt{y_{\\operatorname*{max}}/\\log(y_{\\operatorname*{max}})})$ . ", "page_idx": 28}, {"type": "text", "text": "Lemma 6.5. Let $p\\in\\mathbb N$ , $p\\geq3$ . Then there does not exist an absolute constant $C\\geq0$ such that for all sufficiently small $\\eta>0$ and for all $\\beta\\in D(0)$ , $\\beta^{\\prime}:=\\beta+\\eta e_{1}\\in D(\\eta)$ satisfies ", "page_idx": 28}, {"type": "equation", "text": "$$\nf(X\\beta^{\\prime})\\le f(X\\beta)+\\eta^{p}n+\\eta C f(X\\beta).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof of Lemma 6.5. First note that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(X\\beta^{\\prime})=\\displaystyle\\sum_{i\\in[n]}(x_{i}\\beta+\\eta)^{p}-p y_{i}\\log(x_{i}\\beta+\\eta)+\\log(y_{i}!)}\\\\ &{\\qquad=\\displaystyle\\sum_{i\\in[n]}\\left((x_{i}\\beta)^{p}+\\eta^{p}+\\sum_{\\ell=1}^{p-1}\\binom{p}{\\ell}\\left(x_{i}\\beta\\right)^{\\ell}\\eta^{p-\\ell}\\right)-p y_{i}\\log(x_{i}\\beta+\\eta)+\\log(y_{i}!)}\\\\ &{\\qquad=f(X\\beta)+\\eta^{p}n+\\displaystyle\\sum_{i\\in[n]}\\sum_{\\ell=1}^{p-1}\\binom{p}{\\ell}\\left(x_{i}\\beta\\right)^{\\ell}\\eta^{p-\\ell}-p y_{i}\\log\\left(\\frac{x_{i}\\beta+\\eta}{x_{i}\\beta}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the last equation follows by the definition of $f(X{\\boldsymbol{\\beta}})$ , given the hypothesis on $p$ . Define for every $y\\in\\mathbb N$ the auxiliary function ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\varphi_{1,y,p}(z):=\\frac{1}{\\eta}\\left(\\sum_{\\ell=1}^{p-1}\\binom{p}{\\ell}z^{\\ell}\\eta^{p-\\ell}-p y\\log\\left(\\frac{z+\\eta}{z}\\right)\\right),\\quad z>0.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then the inequality Equation (9) is equivalent to ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{i\\in[n]}\\varphi_{1,y_{i},p}(x_{i}\\beta)\\leq C\\sum_{i\\in[n]}g_{y_{i}}(x_{i}\\beta)\\Longleftrightarrow0\\leq\\sum_{i\\in[n]}C g_{y_{i}}(x_{i}\\beta)-\\varphi_{1,y_{i},p}(x_{i}\\beta),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which implies the following statement: there exists $C,\\eta^{*}>0$ such that for every $n\\in\\mathbb{N},(y_{i})_{i\\in[n]}\\in$ $\\mathbb{N}^{n}$ , $\\eta\\leq\\eta^{*}$ , and $\\begin{array}{r}{(z_{i})_{i\\in[n]}\\in\\prod_{i\\in[n]}[y_{i}^{1/p},\\infty)}\\end{array}$ , it holds that ", "page_idx": 28}, {"type": "equation", "text": "$$\n0\\leq\\sum_{i\\in[n]}C g_{y_{i}}(z_{i})-\\varphi_{1,y_{i},p}(z_{i}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "This statement yields the following necessary condition for Equation (9): there exists $C,\\eta^{*}>0$ such that for every $n\\in\\mathbb N$ , $(y_{i})_{i\\in[n]}\\in\\mathbb{N}^{n}$ , $\\eta\\leq\\eta^{*}$ , and $\\begin{array}{r}{(z_{i})_{i\\in[n]}\\in\\prod_{i\\in[n]}[y_{i}^{1/p},\\infty)}\\end{array}$ , at least one summand on the right-hand side must satisfy $0\\leq C g_{y_{i}}(z_{i})-\\varphi_{1,y_{i},p}(z_{i})$ . This is because if every summand in the sum in Equation (27) were strictly negative, then the sum itself must be strictly negative. If the necessary condition above does not hold, then by considering the contrapositive, we conclude that Equation (9) does not hold either. ", "page_idx": 29}, {"type": "text", "text": "We now show that the necessary condition does not hold, by proving that for every $C,\\eta>0$ and $n\\,\\in\\,\\mathbb{N}$ , there exists $(y_{i})_{i\\in[n]}\\,\\in\\,\\mathbb{N}^{n}$ such that for every $i\\,\\in\\,[n]$ $:[n],{\\mathit{C g}}_{y_{i}}(y_{i}^{1/p})-{\\phi}_{1,y_{i},p}(y_{i}^{1/p})<0$ . Indeed, for every $i\\in[n]$ , suppose that every $y_{i}\\in\\mathbb{N}$ satisfies ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{2C}{p(p-1)\\eta}<\\frac{y^{(p-2)/p}}{\\frac{1}{2}\\log(2\\pi y)+\\frac{1}{12y}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By the hypothesis that $p\\geq3$ and by the fact that the denominator of the right-hand side grows more slowly than the numerator, it follows that there exist infinitely many values of $y_{i}$ that satisfy Equation (28). Thus it remains to show that Equation (28) implies $C g_{y}(y^{1/p})-\\phi_{1,y,p}(y^{1/p})<0$ . ", "page_idx": 29}, {"type": "text", "text": "By the inequality $\\textstyle{\\frac{x}{1+x}}\\leq\\log(1+x)$ , $x>0$ , we obtain ", "page_idx": 29}, {"type": "equation", "text": "$$\n-\\frac{y}{z}=-\\frac{y}{\\eta}\\frac{\\eta}{z}\\leq-\\frac{y}{\\eta}\\log\\left(1+\\frac{\\eta}{z}\\right),\\quad\\forall\\eta,z>0.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Rewrite the auxiliary function $\\varphi_{1,y,p}$ from Equation (26) and bound it from below, first by using the lower bound above, and then by using the hypothesis that $p\\in\\mathbb{N},\\,p\\geq3$ : ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\varphi_{1,y,p}(z)=\\displaystyle\\sum_{\\ell=1}^{p-1}\\binom{p}{\\ell}z^{\\ell}\\eta^{p-\\ell-1}-p\\displaystyle\\frac{y}{\\eta}\\log\\bigg(\\frac{z+\\eta}{z}\\bigg)}\\\\ &{\\qquad\\qquad\\geq\\displaystyle\\sum_{\\ell=1}^{p-1}\\binom{p}{\\ell}z^{\\ell}\\eta^{p-\\ell-1}-p\\displaystyle\\frac{y}{z}>p z^{p-1}+\\frac{p(p-1)}{2}z^{p-2}\\eta^{1}-p\\displaystyle\\frac{y}{z}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Setting $z=y^{1/p}$ , it thus suffices to show that ", "page_idx": 29}, {"type": "equation", "text": "$$\nC g_{y}(y^{1/p})<p\\left(y^{(p-1)/p}+\\eta\\frac{p-1}{2}y^{(p-2)/p}-y^{1-1/p}\\right)=p\\eta\\frac{p-1}{2}y^{(p-2)/p}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Using (4) to evaluate $g_{y}(y^{1/p})$ and using the upper bound on $\\log(y!)$ from Stirling\u2019s approximation, we conclude that $\\begin{array}{r}{g_{y}(y^{1/p})<\\frac{1}{2}\\log(2\\pi y)+\\frac{1}{12y}}\\end{array}$ . Now Equation (28) implies Equation (29), because ", "page_idx": 29}, {"type": "equation", "text": "$$\nC\\left(\\frac12\\log(2\\pi y)+\\frac{1}{12y}\\right)<p\\eta\\frac{p-1}2y^{(p-2)/p}\\Longleftrightarrow\\frac{2C}{p(p-1)\\eta}<\\frac{y^{(p-2)/p}}{\\frac12\\log(2\\pi y)+\\frac{1}{12y}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This completes the proof. ", "page_idx": 29}, {"type": "text", "text": "F On the shifted domain and feasibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section, we show that the restriction to $D(\\eta)$ in Theorem 3.8 and Section 4 do not lead to feasibility issues. ", "page_idx": 29}, {"type": "text", "text": "First recall that we had defined for any $\\eta\\geq0$ ", "page_idx": 29}, {"type": "equation", "text": "$$\nD(\\eta):=\\{\\beta\\in\\mathbb{R}^{d}\\,:\\,\\forall i,\\,\\,x_{i}\\beta>\\eta\\}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Also recall that for the ID- and square root-link \u2014 and in fact for any $p$ th-root-link \u2014 the loss function as defined in Equations (3) and (4) includes a $\\log(x/3)$ term, which restricts the feasible region to $\\beta$ such that for all $x_{i},i\\in[n]$ it holds that $x_{i}\\beta>0$ . Thus $\\beta\\in D(0)$ is the natural domain induced by the model. In particular, this restriction is not our choice. ", "page_idx": 29}, {"type": "text", "text": "Our domain shift idea restricts the domain even further to $\\beta\\in D(\\eta)\\subset D(0)$ , for $\\eta>0$ . Clearly, some solutions that are feasible in the problem formulated over $D(0)$ are no longer feasible in the problem formulated over $D(\\eta)$ . But as we prove in Appendix D, we can construct a coreset that holds for all $\\beta\\in D(\\eta)$ , and $D(\\eta)$ contains at least one $\\beta$ that is a $(1+\\varepsilon)$ -approximation for the optimal solution $\\beta^{*}$ of the problem on the original domain $D(0)$ , and evaluated on the full dataset. These two parts are combined to prove that the final minimizer $\\tilde{\\beta}\\in D(\\eta)$ optimized on the coreset is a $(1+\\bar{O}(\\varepsilon))$ -approximation compared to the value of $\\beta^{*}$ , when both are evaluated on the full dataset. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "In the other direction, no infeasible solution can become feasible because of the proper subset relation $D(\\eta)\\,\\subset\\,D(0)$ . Finally, note that for any data and any fixed $\\eta\\,<\\,\\infty$ , both, $D(\\eta)$ and $D(0)$ are non-empty, since they consist of all $\\beta$ that parameterize hyperplanes that put the convex hull of input points (respectively, the additive $\\eta$ -inflation of the convex hull of input points) in the positive open halfspace. Thus there always exist feasible solutions, which means that no instance can become completely infeasible by means of our methods. ", "page_idx": 30}, {"type": "text", "text": "G Pseudocode, data and experimental results ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "G.1 Pseudocode ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Here we give pseudocode for our coreset construction Algorithm 1 and for the subsequent optimization procedure Algorithm 2: ", "page_idx": 30}, {"type": "table", "img_path": "ES0Gj1KVUk/tmp/dbdbe9164be72338daaf67645171dd6fba00b4c9c3786e6622d684ca9d1bf0de.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Input: data $X\\in\\mathbb{R}^{n\\times d},Y\\in\\mathbb{N}_{0}^{n}$ , error parameter $\\varepsilon\\in(0,\\frac{1}{3})$ . Output: $(1+\\varepsilon)$ -approximate solution $\\tilde{\\beta}$ . 1: Run Algorithm 1 with input $X,Y$ and $k$ as specified to obtain the coreset $(X^{\\prime},Y^{\\prime},w)$ . Let $X_{C H}$ be defined as in Algorithm 1. 2: Run any convex optimization algorithm to find the optimal solution $\\tilde{\\beta}$ for the $p$ th-root-link Poisson regression objective on $(X^{\\prime},Y^{\\prime},w)$ under the constraint that $\\forall x_{i}\\in X_{C H}\\colon x_{i}\\beta>\\varepsilon$ (see Theorem 4.2) 3: return $\\tilde{\\beta}$ . ", "page_idx": 30}, {"type": "text", "text": "G.2 Synthetic data generation ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We generated for each $p\\,\\in\\,\\{1,2\\}$ a dataset with dimensions $n\\,=\\,100\\,000,d\\,=\\,7$ with $n$ labels corresponding to each point. ", "page_idx": 31}, {"type": "text", "text": "\u2022 Construction of $X$ : 1. Start with 6 standard basis vectors $(z_{i})_{i=1}^{6}$ and add the all zero vector $z_{0}=0$ to be the extreme points on the convex hull. 2. Construct a matrix in $\\mathbb{R}^{(n-7)\\times d}$ with i.i.d. standard Gaussian entries. Translate and rescale the rows of this matrix so that the resulting rows lie in the interior of the convex hull of the points $(z_{i})_{i=0}^{6}$ . Concatenate the matrix with the resulting to the matrix with rows given by $(z_{i})_{i=0}^{6}$ generated by the first step. Call the resulting matrix $Z\\in\\mathbb{R}^{n\\times d}$ . 3. Horizontally concatenate a column of length $n$ consisting only of ones to the left of the matrix $Z$ (add an intercept) to get $X\\in\\bar{\\mathbb{R}^{n\\times(d+1)}}$ .   \n\u2022 Construction of $\\beta$ : 1. Draw one sample $\\widetilde{\\beta}\\sim10^{1/p}\\cdot N(0,I_{d})$ , with $d=6$ as above. 2. Find $\\mathrm{min}_{i\\in[n]}(Z{\\widetilde{\\beta}})_{i}$ for $Z$ from the construction of $X$ 3. Compute $b:=\\operatorname*{max}\\{1,2^{1/p}\\cdot|\\operatorname*{min}_{i}(Z\\widetilde{\\beta})_{i}|\\}$ 4. Define $\\beta:=(b,\\widetilde{\\beta})\\in\\mathbb{R}^{d+1}$ .   \n\u2022 Construction of $y$ : 1. Compute $\\lambda:=(X\\beta)^{p}\\in\\mathbb{R}^{n}$ 2. For each $i=1,\\hdots,n$ , draw $Y_{i}\\sim\\operatorname{Poisson}(\\lambda_{i})$ , and store the resulting vector as $y$ . ", "page_idx": 31}, {"type": "text", "text": "G.3 Experimental illustration ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "All experiments were run on a commodity machine with Intel Core i7-7700K processor (4 cores, 4.2GHz, 32GB RAM) and took overall around 50 minutes to complete. The Python code of [34] was adapted to the Poisson regression setting.4 We applied it with the appropriate $p\\in\\{1,2\\}$ to the datasets with dimensions $n=100\\,000,d=7$ generated as detailed in the previous section. ", "page_idx": 31}, {"type": "text", "text": "We compared our method to uniform sampling as a baseline, which is widely popular due to its simplicity and general applicability. ", "page_idx": 31}, {"type": "text", "text": "We varied the reduced size between 50 and 600 in equal increments of size 50. For each reduced size and each method, we performed 201 independent repetitions. ", "page_idx": 31}, {"type": "text", "text": "Our results are shown in Figure 1. The red (Poisson with $p$ th-root-link) and blue (uniform sampling) solid lines display the median approximation ratio across 201 independent repetitions for each reduced size. The shaded areas below and above the solid lines indicate $2\\times$ standard errors of the respective medians. ", "page_idx": 31}, {"type": "text", "text": "For both $p~\\in~\\{1,2\\}$ the results look widely similar, although the case $p\\,=\\,1$ is slightly more distinctive. We thus focus our further description on the case $p=1$ . ", "page_idx": 31}, {"type": "text", "text": "Our novel Poisson subsampling method generally outperforms uniform sampling, and their $2\\times$ standard error intervals are very narrow. ", "page_idx": 31}, {"type": "text", "text": "The shaded blue area under the blue solid line indicates that the $2\\times$ standard error for uniform sampling is slightly more narrow than the $2\\times$ standard error for 1-Poisson regression, i.e., Poisson regression with ID-link. However, this error is optimistically calculated only on repetitions that succeed in providing a valid approximation when applied to the original full data. ", "page_idx": 31}, {"type": "text", "text": "The shaded blue area above the blue solid line is unbounded, which indicates that some of the repetitions yield solutions that are infeasible for the original full data problem, and thus fail to give an appropriate approximation. Even in a few feasible cases, approximation ratios were $1.5-2.5\\overset{\\circ}{\\times}10^{9}$ , which may be explained by missing points that are very close to the boundary of the convex hull, thus causing huge errors. The fraction of repetitions leading to infeasible solutions was always non-negligible and we note that the solid blue line was interrupted below a reduced size of 250 (respectively 150 for $p=2$ ) meaning that even the median was infinite, indicating that more than half of the repetitions of uniform sampling were infeasible for the original problem. ", "page_idx": 31}, {"type": "image", "img_path": "ES0Gj1KVUk/tmp/0ca3a340dde80b4b92de9f01eb3b3307cc5b1dc41bbe89031b50a6b72c348adf.jpg", "img_caption": ["Figure 1: Experimental results for two synthetic data sets with $p=1$ (left), respectively $p=2$ (right). Our method is presented in red and compared against uniform sampling, which is presented in blue. Solid lines indicate the median and shaded areas indicate $\\pm2$ standard errors around the median taken across 201 independent repetitions for each reduced size between 50 and 600 in equal increment steps of 50. For the blue shaded area below the blue solid line, only feasible repetitions were counted, while the blue shaded area above represents the unbounded standard error without this restriction. For some lower reduced sizes, even the median was infinite, which results in an interrupted blue solid line. This indicates that more than half of the repetitions gave infeasible results when using uniform sampling with low sample sizes, while our method never produced infeasible results. "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "In contrast to that, our method produced feasible results in all repetitions, across all reduced sizes, confirming our discussion in Appendix F, and giving approximation ratios very close to 1. ", "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We state the contributions of our work in the abstract and also in Section 1.1.   \nWe support our claims with proven theoretical results. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: We prove several lower bounds in Section 6 that indicate the limitations of constructing coresets for Poisson models in general, as well as limitations of our specific approach and analysis. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 33}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The proof of every lemma and theorem is given in the supplementary material. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: This information is provided in Appendix G. Experiments and data can be reproduced statistically as well as exactly using the code in the public repository. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: See the Git code repository https://github.com/Tim907/ poisson-regression/, which contains all required code and instructions to install and reproduce all experiments. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: This information is provided in Appendix G. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: We report 2SE of the median in our plots. See the caption for Figure 1 and the detailed description in Appendix G. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 35}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Computing resources and total runtime are detailed in Appendix G. We only used standard commodity hardware on which all experiments could be carried out in around 50 minutes. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our research does not involve human subjects or human participants, and addresses all data-related concerns in the NeurIPS Code of Ethics. We do not anticipate any potentially harmful consequences of our research. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: We consider the research in this paper to be foundational research that is not tied to any particular application or deployment. We do not anticipate any negative application unique to the research presented in this paper. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The risks posed by the Poisson model analyzed in this paper are no greater than the risks posed by other statistical models used in regression, for example. No datasets were released as part of this paper. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We only used existing code for which one of the authors has full rights to modify and reuse. The original source was appropriately cited. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: The new codes are published in an open GitHub repository and documented so as to allow installing, running and reusing them. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 38}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}]