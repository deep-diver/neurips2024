{"importance": "This paper is crucial for researchers in text-to-video generation because it introduces a novel approach to improve the quality and alignment of generated videos with textual prompts. By utilizing large language models for annotation and developing a new reward model, the research addresses the limitations of current methods and opens new avenues for enhancing video generation quality. This is highly relevant given the increasing interest in high-fidelity video generation and the need for more efficient and effective training methods.", "summary": "MLLMs enhance text-to-video generation by providing 135k fine-grained video preferences, creating VIDEOPREFER, and a novel reward model, VIDEORM, boosting video quality and alignment.", "takeaways": ["Leveraging MLLMs for generating video preference annotations proves highly effective and cost-efficient, resulting in a large-scale dataset (VIDEOPREFER).", "The novel reward model (VIDEORM) effectively captures video preferences, improving text-to-video alignment and generation quality.", "Extensive experiments validate the efficacy of VIDEOPREFER and VIDEORM, showcasing significant improvements in the field of text-to-video generation."], "tldr": "Current text-to-video models struggle to produce high-quality videos that accurately reflect input text prompts due to limitations in training data and reward models.  Manually annotating video preference data is expensive and time-consuming, hindering progress. This research tackles this issue by exploring the use of Multimodal Large Language Models (MLLMs) for creating a large-scale video preference dataset. \nThe researchers utilized MLLMs to create VIDEOPREFER, a dataset comprising 135,000 preference annotations across two dimensions (Prompt-Following and Video Quality).  Building upon VIDEOPREFER, they developed VIDEORM, a general-purpose reward model specifically designed for video preference in the text-to-video domain.  The model is tailored to capture temporal information, enhancing quality assessment.  Experiments confirmed the effectiveness of VIDEOPREFER and VIDEORM, representing a significant step forward in the field, leading to improved generation quality and alignment.", "affiliation": "Microsoft Research", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "3ivnixHy16/podcast.wav"}