[{"figure_path": "LKdCkV31T7/figures/figures_1_1.jpg", "caption": "Figure 1: NeuroGauss4D-PCI robustly outperforms existing methods [3; 9] across multiple datasets, frame intervals, and point cloud densities, consistently achieving lower interpolation errors. NeuroGauss4D-PCI robustly handles minute non-rigid deformations, large-scale unstructured scenes, dynamic environments with non-uniform data, and extensive motions. The proposed method consistently achieves precise local and global point cloud predictions.", "description": "This figure compares the performance of NeuroGauss4D-PCI against other state-of-the-art methods for point cloud interpolation.  The left panel shows interpolation error on the DHB dataset for different numbers of interval frames. The right panel displays interpolation error on the NL-Drive dataset for different point cloud densities.  In both cases, NeuroGauss4D-PCI significantly outperforms other methods, demonstrating its robustness to various challenges like non-rigid deformations, large-scale scenes and data sparsity.", "section": "1 Introduction"}, {"figure_path": "LKdCkV31T7/figures/figures_4_1.jpg", "caption": "Figure 3: Temporal radial basis function Gaussian residual (RBF-GR) and 4D Gaussian Deformation Field. The normalized RBF weights \u03darbf (t2) are used to compute the residuals of Gaussian means \u0394\u03bc\u2082, rotations R2, and features Pt1 feat t2 between time t1 and t2. The covariance matrix Et2 is then updated using the learned rotation residual. Wrbf = Osoftmax(MLP(feat)), where Wrbf denotes the attention weights for RBF activations, adaptively adjusted based on Gaussian features. The Temporal-RBF-GR employs radial basis functions (RBFs) with learnable centers ci and scales si to capture the temporal evolution of Gaussian parameters.", "description": "This figure illustrates the Temporal Radial Basis Function Gaussian Residual (RBF-GR) module and the 4D Gaussian Deformation Field.  The RBF-GR module uses radial basis functions to interpolate Gaussian parameters over time, capturing temporal dynamics.  The 4D Gaussian Deformation Field uses Gaussian means, covariances, and features to model spatiotemporal changes in the Gaussian distributions, which are then used to update Gaussian ellipsoid information.  The figure shows the different components of the module and how they interact to generate updated ellipsoid information.", "section": "3.3 Temporal Radial Basis Function Gaussian Residual (RBF-GR) Module"}, {"figure_path": "LKdCkV31T7/figures/figures_8_1.jpg", "caption": "Figure 5: Compared to advanced point cloud interpolation algorithms [3; 9], our method aligns better with the ground truth in predicting point cloud positions and geometry on the NL Drive autonomous driving dataset [33; 34; 32; 3].", "description": "This figure compares the performance of the proposed NeuroGauss4D-PCI method against other state-of-the-art point cloud interpolation algorithms (NeuralPCI [3] and 3DSFLabelling [9]) on the NL Drive autonomous driving dataset.  It visually demonstrates that NeuroGauss4D-PCI achieves better alignment with the ground truth point cloud positions and geometry when interpolating between frames. The better alignment between the predicted points (orange) and the ground truth points (green) highlights the improved accuracy of NeuroGauss4D-PCI in handling the complexities of large-scale LiDAR scenes.", "section": "4.3 Result Comparison on Point Cloud Scene Flow"}, {"figure_path": "LKdCkV31T7/figures/figures_15_1.jpg", "caption": "Figure 6: Visualization of NeuroGauss4D-PCI for multi-sensor time synchronization and point cloud densification applications. In point cloud densification, sparse ground truth points are shown in green, while the predicted dense point cloud is shown in red, exhibiting good overlap with the sparse ground truth.", "description": "This figure demonstrates two applications of the NeuroGauss4D-PCI model: temporal synchronization and point cloud densification.  The left side shows how the model can interpolate intermediate frames between sparse LiDAR point clouds (P1, P3) and image frames (I1, I3) to produce temporally consistent sequences (P2, I2).  The right side illustrates how the model can densify sparse point clouds (green) by predicting additional points (red), increasing the point density while maintaining spatial accuracy.  The close overlap between green and red points indicates accurate predictions.", "section": "C Applications"}, {"figure_path": "LKdCkV31T7/figures/figures_15_2.jpg", "caption": "Figure 7: Qualitative visualizations on the DHB dataset demonstrate the evident superiority of the proposed method in reconstructing fine details when compared to existing state-of-the-art open-source models [37; 2; 3].", "description": "This figure shows a qualitative comparison of point cloud interpolation results on the DHB dataset.  It compares the ground truth (GT) with results from PointINet, IDEA-Net, NeuralPCI, and the proposed NeuroGauss4D-PCI method.  The focus is on highlighting the superior ability of NeuroGauss4D-PCI to accurately reconstruct fine details and handle complex non-rigid deformations, particularly in areas indicated by the orange dashed circles.", "section": "4.3 Result Comparison on Point Cloud Scene Flow"}, {"figure_path": "LKdCkV31T7/figures/figures_16_1.jpg", "caption": "Figure 8: Quantitative comparison with pure neural field method [3] on the NL-Drvie dataset.", "description": "This figure shows a quantitative comparison of the proposed NeuroGauss4D-PCI method against a pure neural field method [3] on the NL-Drive dataset.  It visually compares the ground truth point clouds to the results from both methods. The color-coding represents the Chamfer distance (CD) error, with blue indicating perfect matches (CD Error = 0) and progressively warmer colors representing increasing error, culminating in red (CD Error > 25cm).  The figure likely demonstrates NeuroGauss4D-PCI\u2019s improved accuracy in point cloud interpolation, particularly for larger-scale scenes.", "section": "4 Experiments"}, {"figure_path": "LKdCkV31T7/figures/figures_16_2.jpg", "caption": "Figure 9: Point cloud interpolation in challenging autonomous driving scenarios. Blue/yellow: input clouds at T0,4/T8,12. Green: ground truth at T6,7. Pink: predictions at T6,7. The scenes feature local occlusions, sparse point clouds, repeating structures, and large displacements simulated by extended frame intervals. The overlap between green and pink points demonstrates our algorithm's accuracy in these complex scenarios, showcasing its robustness to data sparsity, occlusions, ambiguous temporal features from repeating structures, and large environmental displacements.", "description": "This figure shows the results of point cloud interpolation in challenging autonomous driving scenarios.  The input point clouds are shown in blue and yellow, the ground truth in green, and the model's predictions in pink. The scenarios include local occlusions, sparse data, repeating structures, and large displacements. The overlap between the ground truth and predictions demonstrates the robustness of the algorithm to these challenges.", "section": "4.3 Result Comparison on Point Cloud Scene Flow"}, {"figure_path": "LKdCkV31T7/figures/figures_17_1.jpg", "caption": "Figure 10: Robustness analysis of our point cloud interpolation method under varying noise conditions. Top row: Input point clouds PCt=0 (blue) and PCt=8 (yellow) with different noise ratios (0.4, 0.8) and standard deviations (0.4, 0.8). Bottom row: Comparison between ground truth PC-6 (green) and predicted PCpred (pink) point clouds at t = 6. Noise is added to input point clouds using a Gaussian distribution, where noise ratio determines the proportion of points affected, and noise STD defines the standard deviation of the noise. The alignment between green and pink points indicates prediction accuracy. Results demonstrate our method\u2019s resilience to increasing noise levels, maintaining reasonable performance even under severe noise conditions (noise ratio 0.8, STD 0.8).", "description": "This figure shows the robustness test of the proposed method under different noise levels.  The top row displays the input point clouds with varying levels of added Gaussian noise (different noise ratios and standard deviations). The bottom row compares the ground truth point cloud at a specific time point (t=6) with the point cloud predicted by the model under each noise condition. The alignment between the ground truth (green) and predicted (pink) points illustrates the accuracy of the method under different noise levels, highlighting its resilience to noise.", "section": "4 Experiments"}]