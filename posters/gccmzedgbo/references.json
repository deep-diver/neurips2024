{"references": [{"fullname_first_author": "A. Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-MM-DD", "reason": "This paper introduces the Transformer architecture, a crucial foundation for many modern vision models, including those used in this work."}, {"fullname_first_author": "A. Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-MM-DD", "reason": "This is a seminal work applying the Transformer architecture to image recognition, which significantly influenced subsequent vision transformer models discussed in the paper."}, {"fullname_first_author": "H. Touvron", "paper_title": "Training data-efficient image transformers & distillation through attention", "publication_date": "2021-MM-DD", "reason": "This paper details improvements in training efficiency and performance for vision transformers, a topic directly relevant to the paper's focus on optimizing the training process."}, {"fullname_first_author": "H. Touvron", "paper_title": "DeiT III: Revenge of the ViT", "publication_date": "2022-MM-DD", "reason": "This paper presents DeiT III, a state-of-the-art vision transformer model, whose training characteristics and techniques are analyzed and compared to the proposed method."}, {"fullname_first_author": "S. Ioffe", "paper_title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "publication_date": "2015-MM-DD", "reason": "Batch normalization is a widely used technique for improving training stability and efficiency in deep learning, thus its significance as a related work."}]}