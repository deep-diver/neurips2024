[{"figure_path": "LvAy07mCxU/tables/tables_18_1.jpg", "caption": "Table 1: Overview of the hyperparameters for DreamerV3.", "description": "This table presents the hyperparameters used for training the DreamerV3 model across three different environments: DeepMind Control Suite (DMC), ManiSkill2, and Miniworld.  It breaks down the hyperparameters into general settings (applicable across all environments) and world model specific settings.  The general settings include replay capacity, batch size, batch length, and other training parameters. World model settings encompass RSSM size, number of latents, and classes per latent.  The table highlights the variations in hyperparameter values across the three environments, reflecting the adaptations made to optimize performance in each specific simulation.", "section": "A.1 DreamerV3"}, {"figure_path": "LvAy07mCxU/tables/tables_19_1.jpg", "caption": "Table 2: Overview of used PVRs sorted alphabetically and according to the used backbone. In each row we describe the loss, dataset, model architecture and embedding size of a specific PVR (M corresponds to 1 million).", "description": "This table presents a comprehensive overview of the pre-trained visual representations (PVRs) used in the experiments.  Each row details a specific PVR, providing its name, the loss function used during training, the dataset it was trained on, its network architecture (backbone), and the dimensionality of its resulting embedding vector.  The table facilitates understanding of the different PVRs and their characteristics, enabling comparison and analysis of their performance in the context of the study.", "section": "A.3 PVRS"}, {"figure_path": "LvAy07mCxU/tables/tables_20_1.jpg", "caption": "Table 2: Overview of used PVRs sorted alphabetically and according to the used backbone. In each row we describe the loss, dataset, model architecture and embedding size of a specific PVR (M corresponds to 1 million).", "description": "This table provides a detailed overview of the pre-trained visual representations (PVRs) used in the experiments.  For each PVR, it lists the loss function used during training, the dataset on which it was trained, the backbone architecture of the model (e.g., ResNet-50, ViT-L), and the dimensionality of the resulting embedding vector. The table is organized alphabetically by PVR name and further sorted by backbone architecture.", "section": "A.3 PVRS"}, {"figure_path": "LvAy07mCxU/tables/tables_22_1.jpg", "caption": "Table 2: Overview of used PVRs sorted alphabetically and according to the used backbone. In each row we describe the loss, dataset, model architecture and embedding size of a specific PVR (M corresponds to 1 million).", "description": "This table provides a comprehensive overview of the pre-trained visual representations (PVRs) used in the study.  For each PVR, it lists the loss function used during training, the dataset it was trained on, its underlying architecture (backbone), and the dimensionality of its output embedding.", "section": "A.3 PVRS"}]