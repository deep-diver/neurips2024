[{"figure_path": "LvAy07mCxU/figures/figures_3_1.jpg", "caption": "Figure 1: Components of our PVR-based DreamerV3 (left) and TD-MPC2 (right) architectures. In DreamerV3, the output xt of the frozen pre-trained vision module g is given to the encoder enc(zt|xt) which maps its input to a discrete latent variable zt. In TD-MPC2 a stack xt\u22123:t of the last 3 PVR embeddings is given to the encoder enc(xt\u22123:t) which maps the inputs to fixed-dimensional simplices. The encoder of DreamerV3 additionally requires the recurrent state ht as input. The rest of both algorithms remains unchanged. Adapted from Hafner et al. [11] and Hansen et al. [46].", "description": "This figure shows the architectures of the PVR-based DreamerV3 and TD-MPC2 models.  Both models use pre-trained visual representations (PVRs) as input.  DreamerV3 uses a recurrent architecture with an encoder that maps the PVR output and recurrent state to a discrete latent variable, while TD-MPC2 uses a stack of the last three PVR embeddings as input to its encoder. The figure highlights the integration of the PVRs into the existing model architectures, showing how the pre-trained representations are incorporated into the overall model.", "section": "3 Experiments Setup"}, {"figure_path": "LvAy07mCxU/figures/figures_4_1.jpg", "caption": "Figure 2: Illustration of tasks ranging from DMC and ManiSkill2 to Miniworld with randomizations. Note that while DMC and Miniworld task images show the perspective of the agents, agents in ManiSkill2 tasks utilize the perspective of a wrist-mounted camera.", "description": "This figure shows example images from the three different simulated robotic control environments used in the paper: DeepMind Control Suite (DMC), ManiSkill2, and Miniworld.  It highlights the variety of tasks, illustrating the diverse challenges faced by the reinforcement learning agents.  The caption notes the differing camera perspectives used in each environment; DMC and Miniworld use the agent's perspective, while ManiSkill2 uses a wrist-mounted camera perspective.", "section": "3.3 Domains"}, {"figure_path": "LvAy07mCxU/figures/figures_5_1.jpg", "caption": "Figure 3: Normalized ID performance and data-efficiency comparison on DMC, ManiSkill2 and Miniworld environments between the different representations. Each line represents the mean over all runs with a given representation, the shaded area represents the corresponding standard deviation. Solid lines represent DreamerV3 runs, whereas dashed lines indicate TD-MPC2 experiments. Especially in the DMC experiments, representations trained from scratch outperform all PVRs also in terms of data-efficiency. Curves of each environment individually can be found in Appendix D.", "description": "This figure presents a comparison of the in-distribution performance and data efficiency of different visual representations (including pre-trained visual representations (PVRs) and representations learned from scratch) across three robotic control environments: DeepMind Control Suite (DMC), ManiSkill2, and Miniworld.  The results show that, especially on the DMC tasks, representations learned from scratch generally outperformed the PVRs in terms of both performance and data efficiency. The figure uses lines to show average performance and shaded areas to illustrate standard deviations; solid lines depict DreamerV3 and dashed lines depict TD-MPC2.  Individual environment-specific graphs are available in the appendix.", "section": "Results"}, {"figure_path": "LvAy07mCxU/figures/figures_6_1.jpg", "caption": "Figure 4: Average normalized performance on DMC, ManiSkill2 and Miniworld tasks in the OOD setting. The baseline representation learned from scratch outperforms all PVRs, even in the OOD settings. Thin black lines denote the standard error.", "description": "This figure shows the average normalized performance (with standard error bars) of different visual representation methods on three benchmark datasets (DMC, ManiSkill2, and Miniworld) in an out-of-distribution (OOD) setting.  The key finding is that the \"From Scratch\" method, where the visual representation is learned from scratch, consistently outperforms all pre-trained visual representations (PVRs) across all three datasets in terms of OOD performance. This suggests that pre-trained models are not as effective for model-based reinforcement learning (MBRL) in OOD scenarios as previously thought.", "section": "4.2 Generalization to OOD Settings"}, {"figure_path": "LvAy07mCxU/figures/figures_7_1.jpg", "caption": "Figure 4: Average normalized performance on DMC, ManiSkill2 and Miniworld tasks in the OOD setting. The baseline representation learned from scratch outperforms all PVRs, even in the OOD settings. Thin black lines denote the standard error.", "description": "This figure compares the average normalized performance of different visual representations on three distinct benchmark environments (DMC, ManiSkill2, and Miniworld) in out-of-distribution (OOD) settings.  The key finding is that the baseline model which learns representations from scratch consistently outperforms all pre-trained visual representations (PVRs), even when the evaluation is done on data that differs from the training data. The thin black lines represent the standard error, providing a measure of the variability in performance.", "section": "Results"}, {"figure_path": "LvAy07mCxU/figures/figures_8_1.jpg", "caption": "Figure 6: Average Accumulated Dynamics Prediction Errors on the Pendulum Swingup task for 200 trajectories. For DreamerV3 we average the forward and backward KL divergence between the prior and posterior distributions of the latent state zt. For TD-MPC2 the MSE between the predicted latent state zt of the dynamics model and the encoded latent state zt is plotted. Thin black lines denote the standard error.", "description": "This figure presents a comparison of the accumulated dynamics prediction errors for two model-based reinforcement learning algorithms, DreamerV3 and TD-MPC2, across different visual representations.  The x-axis represents the visual representation used, while the y-axis shows the accumulated prediction error over a horizon of 500 and 33 time steps, respectively.  DreamerV3's error is measured using KL divergence, while TD-MPC2's uses Mean Squared Error (MSE). The results show the relative accuracy of the learned dynamics models when using different visual representations within the context of model-based RL.", "section": "4 Results"}, {"figure_path": "LvAy07mCxU/figures/figures_8_2.jpg", "caption": "Figure 6: Average Accumulated Dynamics Prediction Errors on the Pendulum Swingup task for 200 trajectories. For DreamerV3 we average the forward and backward KL divergence between the prior and posterior distributions of the latent state zt. For TD-MPC2 the MSE between the predicted latent state zt of the dynamics model and the encoded latent state zt is plotted. Thin black lines denote the standard error.", "description": "This figure compares the dynamics prediction errors of various visual representations used in two different model-based reinforcement learning algorithms, DreamerV3 and TD-MPC2, on the Pendulum Swingup task. The x-axis represents different visual representations (PVRs), including those trained from scratch and several pre-trained models.  The y-axis represents the accumulated dynamics prediction error, showing the difference between the predicted and actual latent states of the environment models.  The plots for two time horizons (500 and 33 timesteps) are shown, indicating the error over longer and shorter prediction windows. The error metrics used are the average forward and backward Kullback-Leibler divergence for DreamerV3 and the Mean Squared Error (MSE) for TD-MPC2.  The results illustrate how the quality of the dynamics prediction varies across different visual representations, potentially influencing the overall performance of the model-based RL agents.", "section": "4 Results"}, {"figure_path": "LvAy07mCxU/figures/figures_9_1.jpg", "caption": "Figure 8: UMAP projections of DreamerV3 (top row) and TD-MPC2 (bottom row) encodings using different representations as input. The points are color coded by the real perceived reward. Each point represents a visited state in the Pendulum Swingup environment of DMC. The representations learned from scratch better disentangle low and high reward states whereas the embeddings of the PVRs are more entangled.", "description": "This figure visualizes the latent space representations generated by different models (DreamerV3 and TD-MPC2) using various visual representations, including one trained from scratch and several pre-trained visual representations (PVRs).  UMAP is used to reduce the dimensionality of the latent space for visualization. Each point represents a state visited by the agent during the Pendulum Swingup task. The color of each point represents the actual reward received in that state. The figure shows that models using representations trained from scratch produce more clearly separated clusters of high and low reward states than those using PVRs, suggesting that models trained from scratch produce latent spaces better organized according to the reward.", "section": "4 Results"}, {"figure_path": "LvAy07mCxU/figures/figures_22_1.jpg", "caption": "Figure 9: Performance comparison with representations using linear layers versus multilayer perceptrons. Top row shows normalized ID and OOD performance on DMC environments whereas the bottom row shows the performance on ManiSkill2 environments. Differences between MLPs and linear layers are negligible.", "description": "This figure compares the performance of using linear layers versus multilayer perceptrons (MLPs) as encoders in a model-based reinforcement learning (MBRL) setting.  The results show that performance is not significantly affected by the choice of encoder architecture, indicating that using the simpler linear layer is sufficient and doesn't negatively impact performance.", "section": "C Linear Layers Versus Multilayer Perceptrons"}, {"figure_path": "LvAy07mCxU/figures/figures_23_1.jpg", "caption": "Figure 10: Performance and data-efficiency comparison for each task of ManiSkill2, DMC and Miniworld between the different representations. The solid/dashed line shows the mean over multiple runs for DreamerV3/TD-MPC2. The shaded area represents the standard deviation of the respective representation.", "description": "This figure compares the performance and data efficiency of various visual representations (learned from scratch and pre-trained) in three different robotic control tasks environments (DMC, ManiSkill2, Miniworld). The graph plots the average return (reward) of different methods over the number of steps in the training process, highlighting the data efficiency of each approach.  The shaded regions represent the standard deviation, indicating variability across multiple training runs.  The figure provides insights into how effectively different visual representations learn and generalize in model-based reinforcement learning (MBRL) settings.", "section": "D Performance Curves (All Tasks)"}, {"figure_path": "LvAy07mCxU/figures/figures_23_2.jpg", "caption": "Figure 5: IQM return of the different categorizations. Each marker represents the interquartile-mean performance of an individual group. The x-axis shows the ID performance and the y-axis the OOD performance. Especially, ViT representations or representations trained on diverse data perform well in the OOD setting. Sequential data seem to help in ManiSkill2 and Miniworld but not in DMC. Categorization plots for each environment individually can be found in Appendix E.", "description": "This figure shows the performance of different categories of PVRs on in-distribution (ID) and out-of-distribution (OOD) tasks.  Each point represents the interquartile mean (IQM) performance of a group of PVRs sharing a common property (e.g., using Vision Transformers (ViT), trained on sequential data, etc.). The x-axis shows ID performance, and the y-axis shows OOD performance.  The figure indicates that ViT-based PVRs and those trained on diverse data tend to generalize better to OOD settings. The effect of sequential data is less clear, providing benefits in some environments but not others.", "section": "4.3 Properties of PVRs for Generalization"}, {"figure_path": "LvAy07mCxU/figures/figures_24_1.jpg", "caption": "Figure 12: Ablation of transformer blocks in VC-1. Using \u00bd of VC-1 results in similar performance compared to the full model. Transformer blocks near the final one seam to offer as much information as the final output. With only \u00bd of VC-1 the performance drops significantly. It seems that earlier representations do not offer enough information for the MBRL agent to perform better or similarly.", "description": "The figure shows the ablation study of transformer blocks within the VC-1 model on the Walker Walk task in the DeepMind Control Suite.  The results indicate that using only half of the transformer blocks in VC-1 achieves performance comparable to using the full model.  Interestingly, the blocks closer to the output seem to contain most of the crucial information as performance degrades significantly when using less than half of the blocks.  This suggests that the early layers of VC-1 might not contain sufficiently useful information for model-based reinforcement learning (MBRL).", "section": "F Transformer Block Ablations with VC1"}]