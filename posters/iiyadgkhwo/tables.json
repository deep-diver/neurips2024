[{"figure_path": "iiYadgKHwo/tables/tables_6_1.jpg", "caption": "Table 1: Comparison of distillation performance, (a) VDD achieves on-par performance with Consistency Distillation (CD) (b) VDD is able to possess versatile skills (indicated by high task entropy) while keeping high success rate. The best results for distillation are bolded, and the highest values except origin models are underlined. In most tasks VDD achieves both high success rate and entropy. Note: to better compare the distillation performance, we report the performance of origin diffusion model, therefore only seed 0 results of diffusion models are presented here.", "description": "This table compares the performance of Variational Diffusion Distillation (VDD) against other state-of-the-art distillation methods (Consistency Distillation and Consistency Trajectory Model) and Mixture-of-Experts (MoE) training methods (Expectation-Maximization and Information Maximizing Curriculum) across nine behavior learning tasks.  The metrics used are task success rate and task entropy.  The table highlights VDD's ability to achieve comparable performance to existing state-of-the-art while also exhibiting high versatility (high entropy).", "section": "5 Experiments"}, {"figure_path": "iiYadgKHwo/tables/tables_6_2.jpg", "caption": "Table 1: Comparison of distillation performance, (a) VDD achieves on-par performance with Consistency Distillation (CD) (b) VDD is able to possess versatile skills (indicated by high task entropy) while keeping high success rate. The best results for distillation are bolded, and the highest values except origin models are underlined. In most tasks VDD achieves both high success rate and entropy. Note: to better compare the distillation performance, we report the performance of origin diffusion model, therefore only seed 0 results of diffusion models are presented here.", "description": "This table compares the performance of Variational Diffusion Distillation (VDD) against other state-of-the-art distillation methods (Consistency Distillation, Consistency Trajectory Model) and Mixture-of-Experts (MoE) training methods (Expectation-Maximization, Information Maximizing Curriculum) across nine different robotic manipulation tasks.  The table shows task success rate and entropy for each method.  The results demonstrate that VDD achieves comparable or better performance than existing methods while maintaining or improving the diversity of learned behaviors.", "section": "5 Experiments"}, {"figure_path": "iiYadgKHwo/tables/tables_7_1.jpg", "caption": "Table 1: Comparison of distillation performance, (a) VDD achieves on-par performance with Consistency Distillation (CD) (b) VDD is able to possess versatile skills (indicated by high task entropy) while keeping high success rate. The best results for distillation are bolded, and the highest values except origin models are underlined. In most tasks VDD achieves both high success rate and entropy. Note: to better compare the distillation performance, we report the performance of origin diffusion model, therefore only seed 0 results of diffusion models are presented here.", "description": "This table compares the performance of Variational Diffusion Distillation (VDD) against other state-of-the-art distillation methods (Consistency Distillation and Consistency Trajectory Model) and Mixture of Experts training methods (Expectation-Maximization and Information Maximizing Curriculum) across nine different robot manipulation tasks.  The table shows success rates and entropy (measuring diversity of solutions) for each method.  It highlights VDD's ability to match or exceed the performance of other methods while also maintaining high solution diversity.", "section": "5 Experiments"}, {"figure_path": "iiYadgKHwo/tables/tables_7_2.jpg", "caption": "Table 3: Inference time in state-based pushing (left) and image-based stacking (right). The gray shaded area indicates the default setting for diffusion models.", "description": "This table compares the inference time of three different models:  Variance Exploding (VE) BESO, Variance Preserving (VP) DDPM, and the proposed Variational Diffusion Distillation (VDD) method.  Inference times are shown for different numbers of function evaluations (NFE), representing the number of steps in the diffusion process. The gray shaded area highlights the standard number of steps used in the original diffusion models.", "section": "5.4 Fast Inference with distilled MoE"}, {"figure_path": "iiYadgKHwo/tables/tables_19_1.jpg", "caption": "Table 1: Comparison of distillation performance, (a) VDD achieves on-par performance with Consistency Distillation (CD) (b) VDD is able to possess versatile skills (indicated by high task entropy) while keeping high success rate. The best results for distillation are bolded, and the highest values except origin models are underlined. In most tasks VDD achieves both high success rate and entropy. Note: to better compare the distillation performance, we report the performance of origin diffusion model, therefore only seed 0 results of diffusion models are presented here.", "description": "This table compares the performance of Variational Diffusion Distillation (VDD) with other state-of-the-art distillation methods and the original diffusion models.  The table is divided into two parts: (a) shows the task success rate or environment return (for the Kitchen task) and (b) shows task entropy.  Higher task entropy indicates greater versatility of the learned behavior.  The results demonstrate that VDD achieves comparable performance to state-of-the-art methods while achieving high task entropy in several tasks.", "section": "5 Experiments"}, {"figure_path": "iiYadgKHwo/tables/tables_19_2.jpg", "caption": "Table 1: Comparison of distillation performance, (a) VDD achieves on-par performance with Consistency Distillation (CD) (b) VDD is able to possess versatile skills (indicated by high task entropy) while keeping high success rate. The best results for distillation are bolded, and the highest values except origin models are underlined. In most tasks VDD achieves both high success rate and entropy. Note: to better compare the distillation performance, we report the performance of origin diffusion model, therefore only seed 0 results of diffusion models are presented here.", "description": "This table compares the performance of Variational Diffusion Distillation (VDD) against other state-of-the-art distillation methods and the original diffusion models on various tasks.  It shows success rates and entropy scores, highlighting that VDD achieves comparable performance while maintaining high versatility (entropy).", "section": "5 Experiments"}, {"figure_path": "iiYadgKHwo/tables/tables_20_1.jpg", "caption": "Table 1: Comparison of distillation performance, (a) VDD achieves on-par performance with Consistency Distillation (CD) (b) VDD is able to possess versatile skills (indicated by high task entropy) while keeping high success rate. The best results for distillation are bolded, and the highest values except origin models are underlined. In most tasks VDD achieves both high success rate and entropy. Note: to better compare the distillation performance, we report the performance of origin diffusion model, therefore only seed 0 results of diffusion models are presented here.", "description": "This table compares the performance of Variational Diffusion Distillation (VDD) against other state-of-the-art distillation methods and the original diffusion models across nine different tasks.  It shows success rates and task entropy, highlighting VDD's ability to achieve high performance and versatility while maintaining accuracy.", "section": "5 Experiments"}, {"figure_path": "iiYadgKHwo/tables/tables_20_2.jpg", "caption": "Table 1: Comparison of distillation performance, (a) VDD achieves on-par performance with Consistency Distillation (CD) (b) VDD is able to possess versatile skills (indicated by high task entropy) while keeping high success rate. The best results for distillation are bolded, and the highest values except origin models are underlined. In most tasks VDD achieves both high success rate and entropy. Note: to better compare the distillation performance, we report the performance of origin diffusion model, therefore only seed 0 results of diffusion models are presented here.", "description": "This table compares the performance of Variational Diffusion Distillation (VDD) with other state-of-the-art distillation methods (Consistency Distillation (CD) and Consistency Trajectory Model (CTM)) and Mixture of Experts (MoE) training methods (EM-GPT and IMC-GPT) across nine different behavior learning tasks.  It shows VDD achieves comparable success rate to CD, and in most cases outperforms other methods while also demonstrating higher task entropy (versatility) indicating diverse behavior solutions.  The table shows success rate and entropy for each method.", "section": "5 Experiments"}]