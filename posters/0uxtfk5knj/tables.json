[{"figure_path": "0uXtFk5KNJ/tables/tables_1_1.jpg", "caption": "Table 1: Algorithm feature summary. Here, M represents that the model to be trained has M billion number of parameters, r is the LoRA rank, m is the weight matrix dimension (here, we consider square weight matrices for simplicity), D is the number of transformer layers in LOMO or the number of partitioned blocks in BAdam. BAdam performs full parameter mixed precision update, while only requires memory that is comparable to LOMO and LORA.", "description": "This table summarizes the features of different optimization methods used for training large language models.  It compares Adam, LOMO, LoRA, and BAdam in terms of memory usage, whether they perform full parameter training, use momentum and second moment updates, update precision, and use gradient accumulation.  The memory usage is expressed in terms of the model size (M billion parameters) and other factors specific to each method (LoRA rank, weight matrix dimension, number of blocks/layers).", "section": "1 Introduction"}, {"figure_path": "0uXtFk5KNJ/tables/tables_6_1.jpg", "caption": "Table 1: Algorithm feature summary. Here, M represents that the model to be trained has M billion number of parameters, r is the LoRA rank, m is the weight matrix dimension (here, we consider square weight matrices for simplicity), D is the number of transformer layers in LOMO or the number of partitioned blocks in BAdam. BAdam performs full parameter mixed precision update, while only requires memory that is comparable to LOMO and LORA.", "description": "This table compares different methods for training large language models (LLMs).  It shows the memory requirements (considering both the model parameters and optimizer states), whether full parameter training is supported, if momentum and second moment are used, the update precision, and if gradient accumulation is used.  The table highlights that BAdam achieves comparable memory usage to other efficient methods while still performing full parameter training.", "section": "Main results"}, {"figure_path": "0uXtFk5KNJ/tables/tables_6_2.jpg", "caption": "Table 1: Algorithm feature summary. Here, M represents that the model to be trained has M billion number of parameters, r is the LoRA rank, m is the weight matrix dimension (here, we consider square weight matrices for simplicity), D is the number of transformer layers in LOMO or the number of partitioned blocks in BAdam. BAdam performs full parameter mixed precision update, while only requires memory that is comparable to LOMO and LORA.", "description": "This table summarizes the features of different optimization methods used in the paper, including Adam, LOMO, LoRA, and the proposed BAdam.  It compares the methods based on memory usage, whether they perform full parameter training, whether they use momentum and second moment updates, the precision of their updates (float16 or float32), and whether they use gradient accumulation. The table highlights that BAdam, despite performing full parameter training, achieves comparable memory efficiency to the other methods due to its block-coordinate descent approach.", "section": "1 Introduction"}, {"figure_path": "0uXtFk5KNJ/tables/tables_7_1.jpg", "caption": "Table 1: Algorithm feature summary. Here, M represents that the model to be trained has M billion number of parameters, r is the LoRA rank, m is the weight matrix dimension (here, we consider square weight matrices for simplicity), D is the number of transformer layers in LOMO or the number of partitioned blocks in BAdam. BAdam performs full parameter mixed precision update, while only requires memory that is comparable to LOMO and LORA.", "description": "This table compares several methods for training large language models, including Adam, LOMO, LoRA, and the proposed BAdam.  It highlights key differences in memory usage, whether they perform full parameter training or low-rank adaptation, and other features such as the use of momentum and the precision of updates. The table shows that BAdam achieves full parameter training with memory requirements comparable to more memory-efficient methods.", "section": "1 Introduction"}, {"figure_path": "0uXtFk5KNJ/tables/tables_8_1.jpg", "caption": "Table 1: Algorithm feature summary. Here, M represents that the model to be trained has M billion number of parameters, r is the LoRA rank, m is the weight matrix dimension (here, we consider square weight matrices for simplicity), D is the number of transformer layers in LOMO or the number of partitioned blocks in BAdam. BAdam performs full parameter mixed precision update, while only requires memory that is comparable to LOMO and LORA.", "description": "This table compares the features of different optimization methods for large language models, including Adam, LOMO, LoRA, and the proposed BAdam.  It shows the memory requirements, whether full parameter training is performed, the use of momentum and second moments, update precision, and gradient accumulation.  The table highlights that BAdam achieves full parameter training with memory efficiency comparable to LOMO and LoRA.", "section": "1 Introduction"}, {"figure_path": "0uXtFk5KNJ/tables/tables_17_1.jpg", "caption": "Table 1: Algorithm feature summary. Here, M represents that the model to be trained has M billion number of parameters, r is the LoRA rank, m is the weight matrix dimension (here, we consider square weight matrices for simplicity), D is the number of transformer layers in LOMO or the number of partitioned blocks in BAdam. BAdam performs full parameter mixed precision update, while only requires memory that is comparable to LOMO and LORA.", "description": "This table compares several methods, including Adam, LOMO, LoRA, and the proposed BAdam, in terms of memory usage, full parameter training capability, use of momentum and second moment, update precision, and gradient accumulation.  It highlights BAdam's memory efficiency, noting that it achieves full parameter training with memory comparable to more limited approaches like LoRA.", "section": "1 Introduction"}, {"figure_path": "0uXtFk5KNJ/tables/tables_18_1.jpg", "caption": "Table 1: Algorithm feature summary. Here, M represents that the model to be trained has M billion number of parameters, r is the LoRA rank, m is the weight matrix dimension (here, we consider square weight matrices for simplicity), D is the number of transformer layers in LOMO or the number of partitioned blocks in BAdam. BAdam performs full parameter mixed precision update, while only requires memory that is comparable to LOMO and LORA.", "description": "This table summarizes the features of various full parameter and parameter-efficient fine-tuning methods for large language models (LLMs). It compares Adam, LOMO, LoRA, and BAdam across several key aspects: memory usage, full parameter training capability, usage of momentum and second moments in the update rule, update precision (floating-point format), and gradient accumulation.  The table highlights that BAdam achieves comparable memory efficiency to LOMO and LoRA, while performing full parameter fine-tuning with mixed precision.", "section": "Main results"}, {"figure_path": "0uXtFk5KNJ/tables/tables_19_1.jpg", "caption": "Table 1: Algorithm feature summary. Here, M represents that the model to be trained has M billion number of parameters, r is the LoRA rank, m is the weight matrix dimension (here, we consider square weight matrices for simplicity), D is the number of transformer layers in LOMO or the number of partitioned blocks in BAdam. BAdam performs full parameter mixed precision update, while only requires memory that is comparable to LOMO and LORA.", "description": "This table summarizes the features of different optimization methods, including Adam, LOMO, LoRA, and BAdam. It compares the methods in terms of memory usage for full parameter training, whether they use momentum and second moment, update precision, and gradient accumulation.  The table highlights that BAdam, despite performing full parameter training, achieves memory efficiency comparable to LOMO and LoRA.", "section": "1 Introduction"}, {"figure_path": "0uXtFk5KNJ/tables/tables_19_2.jpg", "caption": "Table 1: Algorithm feature summary. Here, M represents that the model to be trained has M billion number of parameters, r is the LoRA rank, m is the weight matrix dimension (here, we consider square weight matrices for simplicity), D is the number of transformer layers in LOMO or the number of partitioned blocks in BAdam. BAdam performs full parameter mixed precision update, while only requires memory that is comparable to LOMO and LORA.", "description": "This table summarizes the key features of different optimization methods used for training large language models.  It compares Adam, LOMO, LoRA, and the proposed BAdam method across several key aspects including memory usage, whether full parameter training is performed, the use of momentum and second-moment updates, update precision (float32 vs. float16), and gradient accumulation. The table highlights BAdam's memory efficiency compared to Adam while achieving comparable performance to LoRA and full parameter training methods.", "section": "1 Introduction"}, {"figure_path": "0uXtFk5KNJ/tables/tables_20_1.jpg", "caption": "Table 1: Algorithm feature summary. Here, M represents that the model to be trained has M billion number of parameters, r is the LoRA rank, m is the weight matrix dimension (here, we consider square weight matrices for simplicity), D is the number of transformer layers in LOMO or the number of partitioned blocks in BAdam. BAdam performs full parameter mixed precision update, while only requires memory that is comparable to LOMO and LORA.", "description": "This table compares several optimization methods (Adam, LOMO, LoRA, and BAdam) based on their memory usage, whether they perform full parameter training, use momentum and second moments, update precision, and gradient accumulation.  It highlights that BAdam, despite performing full parameter training, has memory requirements similar to the more memory-efficient methods LOMO and LoRA.", "section": "1 Introduction"}, {"figure_path": "0uXtFk5KNJ/tables/tables_21_1.jpg", "caption": "Table 2: Actual memory costs of applying mixed precision training to finetune Llama 3-8B with gradient checkpointing using a single RTX3090. Note that LOMO only supports FP16 precision training. The maximum input sequence length is 728 and the batch size is 2.", "description": "This table shows the memory usage of different methods (Adam, LOMO, LoRA with different ranks, and BAdam) when training Llama 3-8B.  It breaks down the memory usage into the model parameters, gradients, and optimizer states. The table highlights that BAdam achieves a significant reduction in memory consumption compared to Adam, while maintaining comparable performance.", "section": "3.1 Memory Consumption and Wall-clock Running Time"}, {"figure_path": "0uXtFk5KNJ/tables/tables_21_2.jpg", "caption": "Table 3: Time spent per epoch on forward, backward, and update for finetuning Llama 3-8B using a single RTX3090. The single pass batch size is 2. The results are averaged over 3 epochs.", "description": "This table shows the time taken for each stage (forward pass, backward pass, and parameter update) during the training of the Llama 3-8B language model using different optimization methods (LOMO, LoRA, and BAdam). The single pass batch size is 2, and the results are averaged over three epochs of training. The results highlight the efficiency of BAdam, especially during the backward pass.", "section": "3.1 Memory Consumption and Wall-clock Running Time"}, {"figure_path": "0uXtFk5KNJ/tables/tables_21_3.jpg", "caption": "Table 1: Algorithm feature summary. Here, M represents that the model to be trained has M billion number of parameters, r is the LoRA rank, m is the weight matrix dimension (here, we consider square weight matrices for simplicity), D is the number of transformer layers in LOMO or the number of partitioned blocks in BAdam. BAdam performs full parameter mixed precision update, while only requires memory that is comparable to LOMO and LORA.", "description": "This table summarizes the features of different optimization methods for large language models, including Adam, LOMO, LoRA, and the proposed BAdam. It compares these methods based on memory usage, whether they perform full parameter training, the use of momentum and second moment, update precision, and gradient accumulation. The table highlights that BAdam achieves comparable memory efficiency to LOMO and LoRA while performing full parameter updates.", "section": "Main results"}]