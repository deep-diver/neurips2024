[{"figure_path": "lpXDZKiAnt/figures/figures_0_1.jpg", "caption": "Figure 1: Attack surface of harmful fine-tuning attack. Before fine-tuning, the model is aligned with alignment data with supervised fine-tuning (SFT). Fine-tuning on the aligned model breaks the alignment.", "description": "This figure illustrates the attack surface of harmful fine-tuning on large language models (LLMs).  The left side shows a pre-trained LLM, which is then aligned using supervised fine-tuning (SFT) on a safe, harmless dataset (represented by a happy face). This results in an aligned LLM, depicted with a graph whose nodes are safely interconnected.  However, when this aligned LLM is further fine-tuned using a dataset containing malicious instructions (represented by a devil face), the alignment is broken and the model produces harmful outputs (customized LLM). The graph shows a disruption of node interconnections, symbolizing the harmful effects.", "section": "1 Introduction"}, {"figure_path": "lpXDZKiAnt/figures/figures_3_1.jpg", "caption": "Figure 2: Statistic of SFT/non-aligned fine-tuned on SST2 mixed with different ratio of harmful data.", "description": "This figure presents a statistical analysis of the performance of models fine-tuned on the SST2 dataset with varying ratios of harmful data.  It comprises four sub-figures. (a) shows the harmful score and fine-tune accuracy; (b) illustrates the alignment loss and embedding drift for both SFT (Supervised Fine-Tuning) and Non-Aligned models.  The results highlight the impact of harmful data on model performance and alignment.  SFT models show greater resilience to harmful data compared to Non-Aligned models, but the effect worsens with increasing harmful ratios.  The embedding drift is a key indicator of alignment degradation.", "section": "3.2 Risk Analysis"}, {"figure_path": "lpXDZKiAnt/figures/figures_7_1.jpg", "caption": "Figure 2: Statistic of SFT/non-aligned fine-tuned on SST2 mixed with different ratio of harmful data.", "description": "This figure presents the results of an experiment comparing the performance of fine-tuned models with and without supervised fine-tuning (SFT) on the SST2 dataset, which is mixed with varying ratios of harmful data.  It shows the harmful score and fine-tuning accuracy, which reflect the model's robustness to harmful data.  Additionally, it displays the alignment loss and embedding drift, illustrating how the harmful data impacts the model's alignment with the original training data. This provides insights into the mechanism by which harmful data degrades the performance of large language models (LLMs).", "section": "3.2 Risk Analysis"}, {"figure_path": "lpXDZKiAnt/figures/figures_9_1.jpg", "caption": "Figure 4: T-SNE visualization of harmful embedding drift under different harmful ratios p. Each point represents the embedding of each alignment data.", "description": "This figure visualizes the embedding drift of SFT and Vaccine models under different harmful ratios using t-SNE.  It shows how the embeddings change (drift) as the ratio of harmful data increases during fine-tuning.  The left panel shows the SFT model's embeddings drifting significantly with higher harmful ratios, indicating a loss of alignment. The right panel shows the Vaccine model's embeddings exhibiting much less drift, even with high harmful ratios, demonstrating its resilience to harmful embedding drift and preservation of alignment.", "section": "5.6 Visualization"}, {"figure_path": "lpXDZKiAnt/figures/figures_9_2.jpg", "caption": "Figure 1: Attack surface of harmful fine-tuning attack. Before fine-tuning, the model is aligned with alignment data with supervised fine-tuning (SFT). Fine-tuning on the aligned model breaks the alignment.", "description": "This figure illustrates the vulnerability of fine-tuning-as-a-service.  A pre-trained LLM is first aligned using supervised fine-tuning (SFT) with a safe dataset. However, subsequent fine-tuning by users with even a small amount of harmful data can easily break this alignment, resulting in an LLM that produces unsafe or undesirable outputs.", "section": "1 Introduction"}, {"figure_path": "lpXDZKiAnt/figures/figures_18_1.jpg", "caption": "Figure 2: Statistic of SFT/non-aligned fine-tuned on SST2 mixed with different ratio of harmful data.", "description": "This figure presents the results of an experiment evaluating the impact of harmful data on fine-tuned models.  It compares the performance of models fine-tuned using supervised fine-tuning (SFT) with those that are not aligned (Non-Aligned) across varying percentages of harmful data in the dataset.  The charts show the harmful score (a measure of the model's tendency to produce harmful outputs), fine-tuning accuracy, alignment loss (a measure of how well the model retains its alignment after further fine-tuning), and embedding drift (a measure of changes in the model's hidden embeddings caused by harmful data).  This figure illustrates that the SFT approach significantly increases resilience to harmful data compared to Non-Aligned fine-tuning, and that increasing the proportion of harmful data negatively affects both alignment and harmful score, irrespective of the alignment method used.", "section": "3.2 Risk Analysis"}]