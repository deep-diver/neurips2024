{"references": [{"fullname_first_author": "Ouyang, L.", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper introduces a crucial reinforcement learning method for aligning LLMs, forming a foundational technique for safety and alignment research that is directly relevant to the presented work."}, {"fullname_first_author": "Qi, X.", "paper_title": "Fine-tuning aligned language models compromises safety, even when users do not intend to!", "publication_date": "2023-10-26", "reason": "This paper directly addresses the core security vulnerability of harmful fine-tuning, providing the primary threat model and motivation for the proposed Vaccine technique."}, {"fullname_first_author": "Zong, Y.", "paper_title": "Safety fine-tuning at (almost) no cost: A baseline for vision large language models", "publication_date": "2024-02-01", "reason": "This paper offers a baseline alignment technique against harmful fine-tuning, providing a crucial comparison point for evaluating the effectiveness of the proposed Vaccine approach."}, {"fullname_first_author": "Kirkpatrick, J.", "paper_title": "Overcoming catastrophic forgetting in neural networks", "publication_date": "2017-01-01", "reason": "This paper introduces the Elastic Weight Consolidation (EWC) technique for continual learning, providing a relevant method for mitigating catastrophic forgetting during fine-tuning, which is compared to the proposed solution."}, {"fullname_first_author": "Hu, E.J.", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2021-06-24", "reason": "This paper introduces the LoRA technique, a highly efficient parameter-efficient fine-tuning method used in the implementation and evaluation of the proposed alignment technique, making it a crucial contextual reference."}]}