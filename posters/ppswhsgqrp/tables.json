[{"figure_path": "pPSWHsgqRp/tables/tables_6_1.jpg", "caption": "Table 1: Comparing SMOOTHIE-GLOBAL to baseline methods on different ensembles across NLG datasets. Underlined values are the best performing unsupervised methods. Bold values are the best performing overall methods. We report rouge2 scores for CNN, XSum, WebNLG, and E2E, and accuracy for the rest. All metrics are scaled to 0-100.", "description": "This table compares the performance of SMOOTHIE-GLOBAL against a random baseline and a best-performing model selected using a small labeled validation set across seven different natural language generation (NLG) tasks, using two different ensembles of LLMs (3B and 7B parameter models).  The results show SMOOTHIE-GLOBAL's ability to identify high-performing LLMs without using labeled data, frequently matching or exceeding the performance of the model selected using labeled data.  Metrics are scaled to 0-100 for easy comparison.", "section": "Single-Task LLM Scoring"}, {"figure_path": "pPSWHsgqRp/tables/tables_7_1.jpg", "caption": "Table 2: Comparing SMOOTHIE-LOCAL to baseline methods on the 3B and 7B ensembles for multi-task distributions. DISTR-ACC and DISTR-ROUGE2 are measured with accuracy and rouge2 respectively. Bold values indicate the best performing method for each dataset and model size. Metrics are scaled to 0-100.", "description": "This table compares the performance of SMOOTHIE-LOCAL against several baseline methods (RANDOM, PAIRRM, LABELED-KNN, BEST-MODEL, SMOOTHIE-GLOBAL) on two multi-task datasets (DISTR-ACC, DISTR-ROUGE2) using two different ensembles of LLMs (3B and 7B).  The results show SMOOTHIE-LOCAL's superior performance, even surpassing supervised methods.", "section": "5.2 Multi-task Routing"}, {"figure_path": "pPSWHsgqRp/tables/tables_8_1.jpg", "caption": "Table 3: Comparing SMOOTHIE-GLOBAL and SMOOTHIE-LOCAL to baseline methods in the prompt-selection setting. Underlined values are the best performing unsupervised methods. Bold values are the best performing overall methods. We report rouge2 scores for CNN, XSum, WebNLG, and E2E, and accuracy for the rest. All metrics are scaled to 0-100.", "description": "This table presents the results of comparing SMOOTHIE-GLOBAL and SMOOTHIE-LOCAL against baseline methods for prompt selection on different tasks.  It demonstrates the performance improvement achieved by the proposed methods, especially in terms of accuracy and ROUGE-2 scores, compared to random selection and a labeled data-based baseline.", "section": "5.3 Prompt Selection"}, {"figure_path": "pPSWHsgqRp/tables/tables_18_1.jpg", "caption": "Table 1: Comparing SMOOTHIE-GLOBAL to baseline methods on different ensembles across NLG datasets. Underlined values are the best performing unsupervised methods. Bold values are the best performing overall methods. We report rouge2 scores for CNN, XSum, WebNLG, and E2E, and accuracy for the rest. All metrics are scaled to 0-100.", "description": "This table compares the performance of SMOOTHIE-GLOBAL against other methods (random selection and a method using a small labeled validation set) on several natural language generation tasks.  It shows that SMOOTHIE-GLOBAL performs competitively with or better than the methods using labeled data, even though it requires no labels for training.  Performance is measured using ROUGE-2 for summarization and data-to-text tasks and accuracy for other tasks, all scaled to 0-100 for easier comparison.", "section": "Single-Task LLM Scoring"}, {"figure_path": "pPSWHsgqRp/tables/tables_21_1.jpg", "caption": "Table 1: Comparing SMOOTHIE-GLOBAL to baseline methods on different ensembles across NLG datasets. Underlined values are the best performing unsupervised methods. Bold values are the best performing overall methods. We report rouge2 scores for CNN, XSum, WebNLG, and E2E, and accuracy for the rest. All metrics are scaled to 0-100.", "description": "This table compares the performance of SMOOTHIE-GLOBAL against unsupervised and supervised baselines on seven different natural language generation (NLG) tasks.  It shows the performance (rouge2 scores or accuracy) of three methods: SMOOTHIE-GLOBAL (an unsupervised method proposed in the paper), a random selection baseline (RANDOM), and a supervised baseline trained on a small validation set (BEST-ON-VAL). The results are reported for two different ensembles of LLMs (3B and 7B parameter models).  The table highlights the superior performance of SMOOTHIE-GLOBAL, especially compared to the unsupervised baseline, demonstrating its effectiveness in identifying high-performing LLMs without labeled data.", "section": "Single-Task LLM Scoring"}, {"figure_path": "pPSWHsgqRp/tables/tables_21_2.jpg", "caption": "Table 1: Comparing SMOOTHIE-GLOBAL to baseline methods on different ensembles across NLG datasets. Underlined values are the best performing unsupervised methods. Bold values are the best performing overall methods. We report rouge2 scores for CNN, XSum, WebNLG, and E2E, and accuracy for the rest. All metrics are scaled to 0-100.", "description": "This table compares the performance of SMOOTHIE-GLOBAL against two baseline methods (RANDOM and BEST-ON-VAL) across seven different natural language generation (NLG) tasks using two different ensembles of LLMs (3B and 7B parameter models).  It shows the accuracy or ROUGE-2 scores for each method, demonstrating the effectiveness of SMOOTHIE-GLOBAL in identifying high-performing LLMs, even without labeled data.  Underlined values represent the best performance among unsupervised methods, while bold values highlight the best overall performance.", "section": "Single-Task LLM Scoring"}, {"figure_path": "pPSWHsgqRp/tables/tables_21_3.jpg", "caption": "Table 1: Comparing SMOOTHIE-GLOBAL to baseline methods on different ensembles across NLG datasets. Underlined values are the best performing unsupervised methods. Bold values are the best performing overall methods. We report rouge2 scores for CNN, XSum, WebNLG, and E2E, and accuracy for the rest. All metrics are scaled to 0-100.", "description": "This table compares the performance of SMOOTHIE-GLOBAL against two baselines (RANDOM and BEST-ON-VAL) across seven different natural language generation (NLG) tasks.  For each task, two ensembles of LLMs (one with 3B parameter models and the other with 7B parameter models) are used.  The RANDOM baseline represents the average performance of the ensemble, while BEST-ON-VAL represents the performance of the best-performing model in the ensemble chosen using a small amount of labeled validation data.  The table reports performance using ROUGE-2 scores for summarization and data-to-text generation tasks, and accuracy scores for all other tasks.  All scores are normalized to a 0-100 scale.  The best unsupervised and overall results for each task and ensemble size are highlighted.", "section": "Single-Task LLM Scoring"}, {"figure_path": "pPSWHsgqRp/tables/tables_22_1.jpg", "caption": "Table 2: Comparing SMOOTHIE-LOCAL to baseline methods on the 3B and 7B ensembles for multi-task distributions. DISTR-ACC and DISTR-ROUGE2 are measured with accuracy and rouge2 respectively. Bold values indicate the best performing method for each dataset and model size. Metrics are scaled to 0-100.", "description": "This table compares the performance of SMOOTHIE-LOCAL against several baseline methods for multi-task routing on two different ensemble sizes (3B and 7B parameters).  The performance metrics used are accuracy (DISTR-ACC) and ROUGE2 score (DISTR-ROUGE2), representing different task types.  The table highlights the superior performance of SMOOTHIE-LOCAL, even compared to methods that utilize labeled data.", "section": "5.2 Multi-task Routing"}, {"figure_path": "pPSWHsgqRp/tables/tables_23_1.jpg", "caption": "Table 2: Comparing SMOOTHIE-LOCAL to baseline methods on the 3B and 7B ensembles for multi-task distributions. DISTR-ACC and DISTR-ROUGE2 are measured with accuracy and rouge2 respectively. Bold values indicate the best performing method for each dataset and model size. Metrics are scaled to 0-100.", "description": "This table compares the performance of SMOOTHIE-LOCAL against several baseline methods (RANDOM, PAIRRM, LABELED-KNN, and BEST-MODEL) on two multi-task datasets (DISTR-ACC and DISTR-ROUGE2) using two different ensembles of LLMs (3B and 7B).  The metrics used are accuracy (DISTR-ACC) and ROUGE2 (DISTR-ROUGE2), scaled to a 0-100 range.  Bold values highlight the best-performing method in each category.", "section": "5.2 Multi-task Routing"}, {"figure_path": "pPSWHsgqRp/tables/tables_24_1.jpg", "caption": "Table 10: Results for SMOOTHIE-GLOBAL and baselines on MixInstruct.", "description": "This table presents the results of the MixInstruct experiment, comparing the performance of SMOOTHIE-GLOBAL against a random baseline.  The metric used is the ChatGPT-Rank, a lower score indicating better performance.  SMOOTHIE-GLOBAL demonstrates a significant improvement over random selection.", "section": "5.1 Single-Task LLM Scoring"}, {"figure_path": "pPSWHsgqRp/tables/tables_25_1.jpg", "caption": "Table 11: Results for SMOOTHIE-GLOBAL and baselines on GSM8K. We report accuracy, with scores scaled to 0\u2013100.", "description": "This table presents the accuracy results of three different methods on the GSM8K dataset: RANDOM (randomly selecting a model), BEST-ON-VAL (selecting the best-performing model on a validation set), and SMOOTHIE-GLOBAL (the proposed method).  The accuracy scores are scaled to a range of 0-100 for easier comparison.  The results show the relative performance of SMOOTHIE-GLOBAL compared to baselines on a reasoning intensive task.", "section": "Single-Task LLM Scoring"}]