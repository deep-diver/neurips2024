[{"figure_path": "yxjWAJzUyV/tables/tables_6_1.jpg", "caption": "Table 1: Results on TL;DR Summarization. Results are averaged over three seed and the standard deviations across seeds are in parentheses. The best-performing method for each size and metric is highlighted in bold and the second best is underlined. REBEL outperforms all baselines on winrate.", "description": "This table presents the results of TL;DR summarization experiments, comparing REBEL's performance against several baseline reinforcement learning algorithms (DPO, Iterative DPO, PPO, REINFORCE, RLOO) across three different model sizes (1.4B, 2.8B, 6.9B parameters).  The metrics evaluated are winrate (higher is better, indicating better summarization quality as judged by GPT4), reward model score (RM score, higher is better), and KL divergence (lower is better, representing smaller deviation from the supervised fine-tuned policy). The best-performing algorithm for each metric and model size is highlighted in bold, and the second-best is underlined.", "section": "5 Experiments"}, {"figure_path": "yxjWAJzUyV/tables/tables_8_1.jpg", "caption": "Table 2: Results on General Chat. The best-performing method for each metric is highlighted in bold. Note that the APA result is directly obtained by evaluating the Starling-LM-7B-alpha model.", "description": "This table presents the results of the General Chat experiments, comparing the performance of REBEL against APA and a baseline model across several metrics: MT-Bench, AlpacaEval 2.0, and various benchmarks from the Open LLM Leaderboard.  Each metric assesses different aspects of the model's capabilities, such as reasoning, coding, and factual knowledge. The best performing model for each metric is highlighted in bold, showing REBEL's superior performance in most cases.", "section": "5.2 General Chat"}, {"figure_path": "yxjWAJzUyV/tables/tables_8_2.jpg", "caption": "Table 2: Results on General Chat. The best-performing method for each metric is highlighted in bold. Note that the APA result is directly obtained by evaluating the Starling-LM-7B-alpha model.", "description": "This table presents the results of the General Chat experiments, comparing the performance of REBEL against several baselines across various metrics.  The metrics used are AlpacaEval 2.0 (win rate and 5-shot win rate), MT-Bench, and Open LLM Leaderboard (MMLU, GSM8K, Arc, Winogrande, TruthfulQA, HellaSwag).  The best-performing model for each metric is highlighted in bold.  It's important to note that the APA results are taken directly from the evaluation of the Starling-LM-7B-alpha model, not from a direct comparison in the same experimental setup.", "section": "5.2 General Chat"}, {"figure_path": "yxjWAJzUyV/tables/tables_28_1.jpg", "caption": "Table 1: Results on TL;DR Summarization. Results are averaged over three seed and the standard deviations across seeds are in parentheses. The best-performing method for each size and metric is highlighted in bold and the second best is underlined. REBEL outperforms all baselines on winrate.", "description": "This table presents the results of TL;DR summarization experiments using different RL algorithms and model sizes.  The results are averaged over three random seeds to account for variability. The best-performing algorithm for each metric (winrate, reward model score, KL divergence) and model size is highlighted in bold, while the second-best is underlined. The table shows REBEL consistently outperforming the baselines in winrate, demonstrating its effectiveness in this summarization task.", "section": "5 Experiments"}, {"figure_path": "yxjWAJzUyV/tables/tables_29_1.jpg", "caption": "Table 1: Results on TL;DR Summarization. Results are averaged over three seed and the standard deviations across seeds are in parentheses. The best-performing method for each size and metric is highlighted in bold and the second best is underlined. REBEL outperforms all baselines on winrate.", "description": "This table presents the results of TL;DR summarization experiments using different reinforcement learning algorithms (REBEL, DPO, Iterative DPO, PPO, REINFORCE, RLOO) across three different model sizes (1.4B, 2.8B, and 6.9B parameters).  The results show the win rate (higher is better), Reward Model (RM) score (higher is better), and KL Divergence (lower is better). The best performing algorithm for each metric and model size is highlighted in bold, while the second-best is underlined.  The table demonstrates that REBEL achieves a better winrate than the other methods, suggesting superior performance in generating high-quality summaries.", "section": "5 Experiments"}, {"figure_path": "yxjWAJzUyV/tables/tables_30_1.jpg", "caption": "Table 1: Results on TL;DR Summarization. Results are averaged over three seed and the standard deviations across seeds are in parentheses. The best-performing method for each size and metric is highlighted in bold and the second best is underlined. REBEL outperforms all baselines on winrate.", "description": "This table presents the results of the TL;DR summarization experiment. Three different model sizes (1.4B, 2.8B, and 6.9B parameters) are used, and for each size, multiple reinforcement learning algorithms (SFT, DPO, Iterative DPO, PPO, REINFORCE, RLOO, and REBEL) are compared.  The table shows the win rate (higher is better), the reward model (RM) score (higher is better), and the KL divergence between the generated policy and the reference policy (lower is better).  The best performing algorithm in terms of winrate is REBEL, outperforming all baselines.", "section": "5 Experiments"}, {"figure_path": "yxjWAJzUyV/tables/tables_30_2.jpg", "caption": "Table 1: Results on TL;DR Summarization. Results are averaged over three seed and the standard deviations across seeds are in parentheses. The best-performing method for each size and metric is highlighted in bold and the second best is underlined. REBEL outperforms all baselines on winrate.", "description": "This table presents the results of the TL;DR summarization experiments, comparing REBEL's performance against several baseline reinforcement learning algorithms (DPO, Iterative DPO, PPO, REINFORCE, RLOO).  The results are averaged across three random seeds, with standard deviations reported. The table shows win rate, reward model (RM) score, and KL-divergence for three different model sizes (1.4B, 2.8B, and 6.9B parameters).  The best and second-best performing methods for each metric and model size are highlighted.", "section": "5 Experiments"}, {"figure_path": "yxjWAJzUyV/tables/tables_31_1.jpg", "caption": "Table 1: Results on TL;DR Summarization. Results are averaged over three seed and the standard deviations across seeds are in parentheses. The best-performing method for each size and metric is highlighted in bold and the second best is underlined. REBEL outperforms all baselines on winrate.", "description": "This table presents the results of the TL;DR summarization experiment.  Three different model sizes (1.4B, 2.8B, and 6.9B parameters) were used, each trained with several different RL algorithms (SFT, DPO, Iterative DPO, PPO, REINFORCE, RLOO, and REBEL). The table shows the winrate (percentage of times the model's summary was preferred to a human reference summary by GPT-4), the Reward Model (RM) score (a metric measuring the quality of generated summaries), and the KL divergence between the model's policy and a reference policy.  The best performing algorithm for each metric and model size is highlighted in bold, allowing for easy comparison.", "section": "5 Experiments"}, {"figure_path": "yxjWAJzUyV/tables/tables_31_2.jpg", "caption": "Table 1: Results on TL;DR Summarization. Results are averaged over three seed and the standard deviations across seeds are in parentheses. The best-performing method for each size and metric is highlighted in bold and the second best is underlined. REBEL outperforms all baselines on winrate.", "description": "This table presents the results of TL;DR summarization experiments using different RL algorithms (DPO, Iterative DPO, PPO, REINFORCE, RLOO, and REBEL) and model sizes (1.4B, 2.8B, and 6.9B).  The table compares the algorithms across three metrics: Winrate (higher is better, indicating better generation quality judged against human references), RM Score (higher is better, representing reward model score), and KL(\u03c0||\u03c0ref) (lower is better, measuring the KL divergence from a supervised finetuned policy).  The best performing algorithm for each metric and model size is highlighted in bold, with the second-best underlined.  The results demonstrate that REBEL generally outperforms the baselines, especially in terms of winrate.", "section": "5 Experiments"}, {"figure_path": "yxjWAJzUyV/tables/tables_32_1.jpg", "caption": "Table 1: Results on TL;DR Summarization. Results are averaged over three seed and the standard deviations across seeds are in parentheses. The best-performing method for each size and metric is highlighted in bold and the second best is underlined. REBEL outperforms all baselines on winrate.", "description": "This table presents the results of TL;DR summarization experiments, comparing REBEL against several baselines (SFT, DPO, Iterative DPO, PPO, REINFORCE, RLOO).  The results are averaged across three random seeds and show win rates, reward model (RM) scores, and KL divergence from a reference policy.  The best performing method for each metric and model size is highlighted.  REBEL demonstrates superior win rates compared to all baselines.", "section": "5.1 Summarization"}, {"figure_path": "yxjWAJzUyV/tables/tables_33_1.jpg", "caption": "Table 1: Results on TL;DR Summarization. Results are averaged over three seed and the standard deviations across seeds are in parentheses. The best-performing method for each size and metric is highlighted in bold and the second best is underlined. REBEL outperforms all baselines on winrate.", "description": "This table presents the results of TL;DR summarization experiments using different reinforcement learning algorithms, including REBEL, PPO, DPO, REINFORCE, and RLOO.  It compares their performance across various metrics, such as win rate (as assessed by GPT-4), reward model (RM) score, and KL-divergence from a supervised fine-tuned (SFT) policy. The results are shown for three model sizes (1.4B, 2.8B, and 6.9B parameters) and averaged across three random seeds to assess statistical significance.  The table highlights the best-performing algorithm for each metric and model size, showing REBEL's superior win rate while demonstrating competitiveness across other metrics.", "section": "5 Experiments"}, {"figure_path": "yxjWAJzUyV/tables/tables_38_1.jpg", "caption": "Table 1: Results on TL;DR Summarization. Results are averaged over three seed and the standard deviations across seeds are in parentheses. The best-performing method for each size and metric is highlighted in bold and the second best is underlined. REBEL outperforms all baselines on winrate.", "description": "This table presents the results of the TL;DR summarization task, comparing REBEL against several baseline methods (DPO, Iterative DPO, PPO, REINFORCE, RLOO).  Results are averaged across three random seeds and include win rate, reward model (RM) score, and KL divergence from a supervised fine-tuned (SFT) model.  The table shows REBEL's performance, especially in terms of win rate, across various model sizes (1.4B, 2.8B, and 6.9B parameters).", "section": "5 Experiments"}]