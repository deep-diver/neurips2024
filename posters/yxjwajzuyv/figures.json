[{"figure_path": "yxjWAJzUyV/figures/figures_1_1.jpg", "caption": "Figure 1: We present REBEL: a simple and scalable RL algorithm that performs policy optimization via iteratively regressing the difference in rewards in terms of the policy, allowing us to eliminate much of the complexity (e.g., value functions, clipping) of algorithms like PPO (Schulman et al., 2017). We apply REBEL to problems in both image generation and language modeling and find that despite its conceptual and implementation-level simplicity, REBEL is able to match or sometimes outperform the performance of PPO while out-performing purely offline techniques like DPO (Rafailov et al., 2023). REBEL also achieves strong performance on common benchmarks such as AlpacaEval when fine-tuning a Llama-3-8B model.", "description": "This figure illustrates the REBEL algorithm, highlighting its simplicity and scalability compared to PPO.  It shows how REBEL reduces policy optimization to regressing the difference in rewards between two policy choices.  The figure also presents results demonstrating REBEL's performance in image generation and language modeling, showcasing its competitive performance compared to existing methods like PPO and DPO on common benchmarks like AlpacaEval.", "section": "1 Introduction"}, {"figure_path": "yxjWAJzUyV/figures/figures_7_1.jpg", "caption": "Figure 1: We present REBEL: a simple and scalable RL algorithm that performs policy optimization via iteratively regressing the difference in rewards in terms of the policy, allowing us to eliminate much of the complexity (e.g. value functions, clipping) of algorithms like PPO (Schulman et al., 2017). We apply REBEL to problems in both image generation and language modeling and find that despite its conceptual and implementation-level simplicity, REBEL is able to match or sometimes outperform the performance of PPO while out-performing purely offline techniques like DPO (Rafailov et al., 2023). REBEL also achieves strong performance on common benchmarks such as AlpacaEval when fine-tuning a Llama-3-8B model.", "description": "This figure illustrates the REBEL algorithm, highlighting its simplicity and scalability.  It shows how REBEL reduces policy optimization to a regression problem, eliminating the need for complex components like value functions and clipping, which are commonly used in algorithms like PPO. The figure also showcases REBEL's application to both image generation and language modeling, demonstrating its competitive performance compared to PPO and DPO on standard benchmarks and AlpacaEval.", "section": "1 Introduction"}, {"figure_path": "yxjWAJzUyV/figures/figures_8_1.jpg", "caption": "Figure 3: Learning curves as a function of reward queries to the LAION aesthetic predictor. The colored areas represent 95% CIs.", "description": "This figure shows the learning curves for both REBEL and PPO algorithms on the image generation task, plotted against the number of reward queries made to the LAION aesthetic predictor.  The y-axis represents the LAION Aesthetic Score, a metric of image quality.  The x-axis represents the number of reward queries. The shaded areas around each line represent the 95% confidence intervals, indicating the uncertainty in the results. The figure demonstrates the performance of each algorithm over time, highlighting how the aesthetic score changes as more queries are made.", "section": "5.3 Image Generation"}, {"figure_path": "yxjWAJzUyV/figures/figures_9_1.jpg", "caption": "Figure 4: Generated images using PPO and REBEL during an intermediate checkpoint. At the same number of epochs, REBEL observes a higher reward under the reward model. This can further be seen by the more diverse background of images generated from REBEL with less training time.", "description": "This figure shows a comparison of image generation results using PPO and REBEL at an intermediate training checkpoint.  Both methods generated images of five different animals.  The numbers under each image represent the reward score for that specific image.  The caption highlights that REBEL achieved higher reward scores with less training time and also generated more diverse backgrounds than PPO.", "section": "5.3 Image Generation"}, {"figure_path": "yxjWAJzUyV/figures/figures_36_1.jpg", "caption": "Figure 4: Generated images using PPO and REBEL during an intermediate checkpoint. At the same number of epochs, REBEL observes a higher reward under the reward model. This can further be seen by the more diverse background of images generated from REBEL with less training time.", "description": "This figure shows a comparison of images generated by PPO and REBEL at a similar training stage.  It highlights that REBEL, despite using less training time, achieves higher rewards (as measured by a reward model) and generates images with more diverse backgrounds compared to PPO.", "section": "5.3 Image Generation"}, {"figure_path": "yxjWAJzUyV/figures/figures_37_1.jpg", "caption": "Figure 4: Generated images using PPO and REBEL during an intermediate checkpoint. At the same number of epochs, REBEL observes a higher reward under the reward model. This can further be seen by the more diverse background of images generated from REBEL with less training time.", "description": "This figure shows a comparison of images generated by the PPO and REBEL algorithms at an intermediate stage of training.  The caption highlights that REBEL achieves a higher reward based on the reward model, even with less training time, and that the images produced by REBEL exhibit greater diversity in backgrounds compared to those from PPO. The visual comparison serves as empirical evidence supporting the algorithm's efficacy.", "section": "5.3 Image Generation"}, {"figure_path": "yxjWAJzUyV/figures/figures_38_1.jpg", "caption": "Figure 5: Plot of Reward vs KL-Divergence for 2.8B REBEL and PPO for summarization. We evaluate the models across the entire test set every 100 steps for 2,000 steps. Left: each point represents the average reward score and KL-divergence for a specific time step; the eclipse represents the confidence interval with 2 standard deviations. Right: we divide the KL distribution at the 2,000-step into 10 bins with equal size and average the corresponding RM scores in each bin.", "description": "This figure shows the trade-off between reward model score and KL divergence for both REBEL and PPO algorithms during the summarization task using a 2.8B model. The left plot visualizes the average reward score and KL-divergence at each time step during training, while the right plot illustrates the relationship by dividing the KL-divergence distribution into bins and showing the average reward score for each bin.", "section": "K Trade-off between Reward Model Score and KL-divergence"}, {"figure_path": "yxjWAJzUyV/figures/figures_39_1.jpg", "caption": "Figure 6: REBEL's reward difference prediction error throughout training of our 6.9B parameter policy on the summarization task. The reward used for this task is unbounded with the range of values of the human labels in the validation set being [-6.81, 7.31]. We plot both the smoothed values with a moving average and the loss values at each iteration.", "description": "This figure shows the mean squared error (MSE) during the training of a 6.9B parameter policy using REBEL on a summarization task.  The y-axis represents the MSE, and the x-axis represents the training iterations. The reward used is unbounded, ranging from -6.81 to 7.31. The plot includes both smoothed values (using a moving average) and the raw loss values for each iteration, illustrating the model's learning process and the accuracy of its reward difference predictions over time.", "section": "L Regression Loss During Training"}, {"figure_path": "yxjWAJzUyV/figures/figures_39_2.jpg", "caption": "Figure 7: Breakdown of MT-Bench results over eight dimensions.", "description": "This radar chart visualizes the performance of three different models (REBEL-OpenChat-3.5, Starling-LM-7B-alpha, and OpenChat-3.5) across eight sub-tasks within the MT-Bench benchmark. Each axis represents a specific sub-task (Writing, Humanities, Roleplay, Reasoning, STEM, Math, Extraction, and Coding), and the distance from the center to the point on each axis represents the model's score on that specific sub-task.  The chart allows for a quick comparison of the models' strengths and weaknesses across different aspects of language understanding and generation.", "section": "M Breakdown of MT-Bench"}]