{"references": [{"fullname_first_author": "Pang Wei Koh", "paper_title": "Understanding black-box predictions via influence functions", "publication_date": "2017-00-00", "reason": "This paper introduces the influence function, a key concept on which the current paper's method is based."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-00-00", "reason": "This paper introduces in-context learning, the central topic of the current paper."}, {"fullname_first_author": "Kwangjun Ahn", "paper_title": "Transformers learn to implement preconditioned gradient descent for in-context learning", "publication_date": "2023-00-00", "reason": "This paper provides a theoretical framework for understanding how transformers perform in-context learning, which is crucial for the current paper's approach."}, {"fullname_first_author": "Johannes von Oswald", "paper_title": "Transformers learn in-context by gradient descent", "publication_date": "2023-00-00", "reason": "This paper supports the interpretation of transformers as implementing an internal optimization algorithm, a key assumption in the current paper."}, {"fullname_first_author": "Xiaoqiang Lin", "paper_title": "Prompt optimization with human feedback", "publication_date": "2024-00-00", "reason": "This paper explores prompt optimization, a related area that helps contextualize and enhance the value of the current work."}]}