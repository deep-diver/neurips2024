[{"figure_path": "4jRNkAH15k/figures/figures_4_1.jpg", "caption": "Figure 1: Illustration of computing DETAIL score for transformer-based ICL. Note that we use the same notation mp[.] before and after the random projection since the projection is optional.", "description": "This figure illustrates the process of calculating DETAIL scores for in-context learning (ICL) in transformer-based models.  It shows how the input demonstrations and query are processed through the transformer's layers to obtain internal representations (m). These representations are then projected (optionally) to a lower dimension (mp) for computational efficiency.  The core calculation involves computing both \"test influence\" (Itest) and \"self influence\" (Iself) scores using a kernel regression approach.  Itest measures the impact of each demonstration on the model's prediction of the query, while Iself assesses the relative importance of each demonstration within the overall prompt.  The figure highlights the key steps and components of the DETAIL method, providing a visual guide to its inner workings.", "section": "5 Empirical Evaluation"}, {"figure_path": "4jRNkAH15k/figures/figures_5_1.jpg", "caption": "Figure 2: (Left) Visualization of learning label mapping of MNIST digits in context. The left 9 images in each row are demonstrations while the right-most one is a query image. Below each image shows its mapped label (\"A\" to \"E\"). Above each ICL image is its Itest w.r.t. the query image with high values highlighted in green and low values highlighted in red. Above the query image is the prediction (pred) made by the pre-trained transformer which is in green if consistent with the ground truth (GT) and red otherwise. Top row shows that using all 9 demonstrations allows the transformer to learn the mapping in context as GT=pred=\"E\". Middle shows removing 5 demonstrations with the highest Itest results in most digit 0's removed, leading to a wrong prediction. Bottom shows removing 5 demonstrations with the lowest Itest results in 3 digit 0's remaining for the transformer to learn in context, leading to correct prediction. (Right) Average accuracy on 1013 ICL datasets repeated over 10 trials; \u03bb = 0.01; Lines and shades represent mean and standard error over 10 independent trials.", "description": "The left part of the figure visualizes how the DETAIL score attributes the prediction of a custom transformer to individual demonstrations in an MNIST digit classification task.  The right part shows the average accuracy of the model when demonstrations with high or low DETAIL scores are removed.", "section": "5.1 Evaluation on a Custom Transformer"}, {"figure_path": "4jRNkAH15k/figures/figures_5_2.jpg", "caption": "Figure 3: (1st and 2nd) Corrupting labels of demonstrations and (3rd and 4th) removing demonstrations with high/low DETAIL scores (Itest) on AG News. Perturbing demonstrations randomly result in an accuracy in the middle as expected. All experiments are repeated 10 trials. \u03bb = 1.0. Lines and shades represent the mean and standard error respectively.", "description": "This figure shows the results of experiments on the AG News dataset, demonstrating the effectiveness of DETAIL scores in identifying helpful and harmful demonstrations.  The left two plots show that corrupting (changing labels) demonstrations with low DETAIL scores (Itest) has less impact on accuracy than corrupting demonstrations with high scores. The right two plots show that removing demonstrations with low DETAIL scores leads to a smaller decrease in accuracy than removing high-scoring demonstrations. A control experiment with random removal is included.", "section": "5.2 Evaluation on Large Language Models"}, {"figure_path": "4jRNkAH15k/figures/figures_6_1.jpg", "caption": "Figure 4: (1st and 2nd) Fraction of noisy labels identified vs. number of demonstrations ranked by DETAIL (with d' = 1000) and LOO checked on Subj using Vicuna-7b and Llama-2-13b respectively. (3rd) Wall time comparison between DETAIL and LOO on all datasets. (4th) wall time in seconds (left y-axis) and AUCROC (right y-axis) vs. projection dimension on Subj using Vicuna-7b. All experiments are repeated 10 trials. \u03bb = 10\u22129. Lines and shades represent the mean and std. error.", "description": "This figure demonstrates the effectiveness of DETAIL in noisy demonstration detection. The first two subfigures show the fraction of noisy labels identified against the number of demonstrations checked using DETAIL and the Leave-One-Out (LOO) method for Subj dataset with Vicuna-7b and Llama-2-13b models. The third subfigure compares the wall time of DETAIL and LOO across multiple datasets. The final subfigure shows the trade-off between computational speed and AUC-ROC score of DETAIL by varying the dimension reduction using random projection on Subj dataset with Vicuna-7b model.", "section": "5 Empirical Evaluation"}, {"figure_path": "4jRNkAH15k/figures/figures_7_1.jpg", "caption": "Figure 5: Test accuracy (mean and std. error) vs. position of the demonstration with the corrupted label over 50 trials.", "description": "This figure displays the test accuracy results over 50 trials.  The x-axis represents the position of a single perturbed (corrupted label) demonstration within a sequence of demonstrations used for in-context learning. The y-axis shows the test accuracy.  Three datasets (Rotten Tomatoes, Subj, and SST-2) are shown, each with its own line and shaded error region.  The figure demonstrates how changing the position of a noisy demonstration impacts model accuracy.  The trend shows that placing a noisy demonstration towards the edges of the demonstration sequence often yields better accuracy than placing it near the middle.", "section": "5.2 Evaluation on Large Language Models"}, {"figure_path": "4jRNkAH15k/figures/figures_7_2.jpg", "caption": "Figure 3: (1st and 2nd) Corrupting labels of demonstrations and (3rd and 4th) removing demonstrations with high/low DETAIL scores (Itest) on AG News. Perturbing demonstrations randomly result in an accuracy in the middle as expected. All experiments are repeated 10 trials. \u03bb = 1.0. Lines and shades represent the mean and standard error respectively.", "description": "This figure shows the results of experiments on the AG News dataset, demonstrating how model accuracy changes when labels are corrupted or demonstrations are removed based on their DETAIL scores. The four subplots illustrate the effects of corrupting high-score, low-score, and randomly selected demonstrations, and removing high-score, low-score, and randomly selected demonstrations.  The results show that corrupting or removing demonstrations with high DETAIL scores (Itest) leads to a larger drop in accuracy than those with low DETAIL scores. This supports the effectiveness of the DETAIL method in identifying influential and less influential demonstrations.", "section": "Demonstration perturbation"}, {"figure_path": "4jRNkAH15k/figures/figures_18_1.jpg", "caption": "Figure 2: (Left) Visualization of learning label mapping of MNIST digits in context. The left 9 images in each row are demonstrations while the right-most one is a query image. Below each image shows its mapped label (\"A\" to \"E\"). Above each ICL image is its I<sub>test</sub> w.r.t. the query image with high values highlighted in green and low values highlighted in red. Above the query image is the prediction (pred) made by the pre-trained transformer which is in green if consistent with the ground truth (GT) and red otherwise. Top row shows that using all 9 demonstrations allows the transformer to learn the mapping in context as GT=pred=\"E\". Middle shows removing 5 demonstrations with the highest I<sub>test</sub> results in most digit 0's removed, leading to a wrong prediction. Bottom shows removing 5 demonstrations with the lowest I<sub>test</sub> results in 3 digit 0's remaining for the transformer to learn in context, leading to correct prediction. (Right) Average accuracy on 1013 ICL datasets repeated over 10 trials; \u03bb = 0.01; Lines and shades represent mean and standard error over 10 independent trials.", "description": "This figure visualizes the results of applying the DETAIL method to a custom transformer on the MNIST dataset.  The left side shows how removing demonstrations with high or low influence scores affects the model's prediction accuracy.  The right side shows the average accuracy across multiple experiments, demonstrating the impact of removing different demonstrations on the overall performance. The visualization helps to understand how DETAIL attributes the model's prediction to individual demonstrations.", "section": "5.1 Evaluation on a Custom Transformer"}, {"figure_path": "4jRNkAH15k/figures/figures_18_2.jpg", "caption": "Figure 2: (Left) Visualization of learning label mapping of MNIST digits in context. The left 9 images in each row are demonstrations while the right-most one is a query image. Below each image shows its mapped label (\"A\" to \"E\"). Above each ICL image is its Itest w.r.t. the query image with high values highlighted in green and low values highlighted in red. Above the query image is the prediction (pred) made by the pre-trained transformer which is in green if consistent with the ground truth (GT) and red otherwise. Top row shows that using all 9 demonstrations allows the transformer to learn the mapping in context as GT=pred=\"E\". Middle shows removing 5 demonstrations with the highest Itest results in most digit 0's removed, leading to a wrong prediction. Bottom shows removing 5 demonstrations with the lowest Itest results in 3 digit 0's remaining for the transformer to learn in context, leading to correct prediction. (Right) Average accuracy on 1013 ICL datasets repeated over 10 trials; \u03bb = 0.01; Lines and shades represent mean and standard error over 10 independent trials.", "description": "This figure visualizes how DETAIL scores (Itest) attribute the model's prediction to individual demonstrations in an in-context learning task using a custom transformer on the MNIST dataset. The left panel shows a qualitative visualization of Itest, highlighting how removing demonstrations with high Itest (green) leads to incorrect predictions, while removing those with low Itest (red) maintains correct predictions. The right panel quantitatively shows the average accuracy of the model when demonstrations are removed, confirming that removing high-Itest demonstrations causes a more significant drop in accuracy.", "section": "5.1 Evaluation on a Custom Transformer"}, {"figure_path": "4jRNkAH15k/figures/figures_19_1.jpg", "caption": "Figure 4: (1st and 2nd) Fraction of noisy labels identified vs. number of demonstrations ranked by DETAIL (with d' = 1000) and LOO checked on Subj using Vicuna-7b and Llama-2-13b respectively. (3rd) Wall time comparison between DETAIL and LOO on all datasets. (4th) wall time in seconds (left y-axis) and AUCROC (right y-axis) vs. projection dimension on Subj using Vicuna-7b. All experiments are repeated 10 trials. \u03bb = 10\u22129. Lines and shades represent the mean and std. error.", "description": "This figure presents a comparison of DETAIL and Leave-One-Out (LOO) methods for noisy demonstration detection.  The first two subfigures show the fraction of noisy labels identified as a function of the number of demonstrations checked, comparing DETAIL and LOO on the Subj dataset using two different LLMs: Vicuna-7b and Llama-2-13b.  The third subfigure displays the wall-clock time of each method on various datasets. Finally, the fourth subfigure shows how the wall-clock time and Area Under the ROC Curve (AUCROC) for DETAIL vary with the dimensionality reduction achieved using random projection on the Subj dataset. In all cases, error bars are included.", "section": "5.2 Evaluation on Large Language Models"}, {"figure_path": "4jRNkAH15k/figures/figures_20_1.jpg", "caption": "Figure 2: (Left) Visualization of learning label mapping of MNIST digits in context. The left 9 images in each row are demonstrations while the right-most one is a query image. Below each image shows its mapped label (\"A\" to \"E\"). Above each ICL image is its Itest w.r.t. the query image with high values highlighted in green and low values highlighted in red. Above the query image is the prediction (pred) made by the pre-trained transformer which is in green if consistent with the ground truth (GT) and red otherwise. Top row shows that using all 9 demonstrations allows the transformer to learn the mapping in context as GT=pred=\"E\". Middle shows removing 5 demonstrations with the highest Itest results in most digit 0's removed, leading to a wrong prediction. Bottom shows removing 5 demonstrations with the lowest Itest results in 3 digit 0's remaining for the transformer to learn in context, leading to correct prediction. (Right) Average accuracy on 1013 ICL datasets repeated over 10 trials; \u03bb = 0.01; Lines and shades represent mean and standard error over 10 independent trials.", "description": "The left part of the figure visualizes how DETAIL scores (Itest) attribute the model's prediction to each demonstration in a custom transformer trained on the MNIST dataset.  The right part shows that removing demonstrations with high Itest scores leads to a faster decrease in accuracy than removing those with low Itest scores, indicating that high-scoring demonstrations are more informative. ", "section": "5.1 Evaluation on a Custom Transformer"}, {"figure_path": "4jRNkAH15k/figures/figures_20_2.jpg", "caption": "Figure 1: Illustration of computing DETAIL score for transformer-based ICL. Note that we use the same notation mp[.] before and after the random projection since the projection is optional.", "description": "This figure illustrates the process of computing the DETAIL score for transformer-based in-context learning (ICL).  It shows how the influence of each demonstration on the model's prediction is calculated. The process involves mapping demonstrations to internal representations (using a random projection optionally), creating a kernel regression with these representations, and then computing influence scores using the Hessian and gradient of the loss function.  The figure visually demonstrates the computation of both test influence (Itest) and self-influence (Iself) scores, highlighting the key steps involved in the DETAIL method.", "section": "5 Empirical Evaluation"}, {"figure_path": "4jRNkAH15k/figures/figures_21_1.jpg", "caption": "Figure 2: (Left) Visualization of learning label mapping of MNIST digits in context. The left 9 images in each row are demonstrations while the right-most one is a query image. Below each image shows its mapped label (\"A\" to \"E\"). Above each ICL image is its Itest w.r.t. the query image with high values highlighted in green and low values highlighted in red. Above the query image is the prediction (pred) made by the pre-trained transformer which is in green if consistent with the ground truth (GT) and red otherwise. Top row shows that using all 9 demonstrations allows the transformer to learn the mapping in context as GT=pred=\"E\". Middle shows removing 5 demonstrations with the highest Itest results in most digit 0's removed, leading to a wrong prediction. Bottom shows removing 5 demonstrations with the lowest Itest results in 3 digit 0's remaining for the transformer to learn in context, leading to correct prediction. (Right) Average accuracy on 1013 ICL datasets repeated over 10 trials; \u03bb = 0.01; Lines and shades represent mean and standard error over 10 independent trials.", "description": "The left part of the figure visualizes how the DETAIL score attributes the model's prediction to individual demonstrations in an in-context learning task on the MNIST dataset.  It shows examples of how removing demonstrations with high or low DETAIL scores (Itest) impacts the model's prediction accuracy. The right part shows the average accuracy over multiple trials when removing demonstrations with varying Itest scores, demonstrating that higher Itest scores correspond to more helpful demonstrations.", "section": "5.1 Evaluation on a Custom Transformer"}, {"figure_path": "4jRNkAH15k/figures/figures_22_1.jpg", "caption": "Figure 1: Illustration of computing DETAIL score for transformer-based ICL. Note that we use the same notation mp[.] before and after the random projection since the projection is optional.", "description": "This figure illustrates the process of computing the DETAIL score for transformer-based in-context learning (ICL).  It shows how the influence function is applied to the internal kernel regression of the transformer model. The process begins with the input tokens and demonstrations, which are mapped to internal representations (m). These representations are used to compute the kernel regression weights (\u03b2), which in turn are used to calculate the influence of each demonstration on the model's prediction (Itest and Iself). The optional random projection step is included to reduce the dimensionality of the internal representations for computational efficiency. The figure clearly illustrates the flow of information and computations involved in obtaining the DETAIL scores.", "section": "5 Empirical Evaluation"}, {"figure_path": "4jRNkAH15k/figures/figures_23_1.jpg", "caption": "Figure 1: Illustration of computing DETAIL score for transformer-based ICL. Note that we use the same notation mp[.] before and after the random projection since the projection is optional.", "description": "This figure illustrates the process of calculating the DETAIL score for transformer-based in-context learning (ICL).  It depicts the key steps involved in the computation, highlighting the role of the internal kernel regression, the influence function, and the optional random projection for dimensionality reduction. The figure visually represents the flow of information from input demonstrations to the final DETAIL score, which is used to attribute the model's prediction to individual demonstrations.", "section": "5 Empirical Evaluation"}, {"figure_path": "4jRNkAH15k/figures/figures_23_2.jpg", "caption": "Figure 1: Illustration of computing DETAIL score for transformer-based ICL. Note that we use the same notation mp[.] before and after the random projection since the projection is optional.", "description": "This figure illustrates the process of computing the DETAIL score for transformer-based in-context learning (ICL). It shows how the influence function is applied to an internal kernel regression to attribute the model's prediction to each demonstration. The figure highlights the key steps involved in computing the influence function, including the mapping of ICL demonstrations to an internal representation and the computation of the Hessian and gradient. It also shows how random projection can be used to speed up the computation. The figure uses a consistent notation to represent the embedding before and after random projection, indicating that the projection is optional.", "section": "5 Empirical Evaluation"}, {"figure_path": "4jRNkAH15k/figures/figures_24_1.jpg", "caption": "Figure 2: (Left) Visualization of learning label mapping of MNIST digits in context. The left 9 images in each row are demonstrations while the right-most one is a query image. Below each image shows its mapped label (\"A\" to \"E\"). Above each ICL image is its Itest w.r.t. the query image with high values highlighted in green and low values highlighted in red. Above the query image is the prediction (pred) made by the pre-trained transformer which is in green if consistent with the ground truth (GT) and red otherwise. Top row shows that using all 9 demonstrations allows the transformer to learn the mapping in context as GT=pred=\"E\". Middle shows removing 5 demonstrations with the highest Itest results in most digit 0's removed, leading to a wrong prediction. Bottom shows removing 5 demonstrations with the lowest Itest results in 3 digit 0's remaining for the transformer to learn in context, leading to correct prediction. (Right) Average accuracy on 1013 ICL datasets repeated over 10 trials; \u03bb = 0.01; Lines and shades represent mean and standard error over 10 independent trials.", "description": "This figure visualizes the DETAIL scores' ability to attribute a model's prediction to individual demonstrations in an in-context learning setting using a custom transformer on the MNIST dataset.  The left panel shows a qualitative visualization of how removing demonstrations with high or low influence scores impacts the model's prediction accuracy. The right panel provides a quantitative evaluation of this effect across multiple datasets.", "section": "5.1 Evaluation on a Custom Transformer"}, {"figure_path": "4jRNkAH15k/figures/figures_24_2.jpg", "caption": "Figure 2: (Left) Visualization of learning label mapping of MNIST digits in context. The left 9 images in each row are demonstrations while the right-most one is a query image. Below each image shows its mapped label (\"A\" to \"E\"). Above each ICL image is its Itest w.r.t. the query image with high values highlighted in green and low values highlighted in red. Above the query image is the prediction (pred) made by the pre-trained transformer which is in green if consistent with the ground truth (GT) and red otherwise. Top row shows that using all 9 demonstrations allows the transformer to learn the mapping in context as GT=pred=\\\"E\\\". Middle shows removing 5 demonstrations with the highest Itest results in most digit 0's removed, leading to a wrong prediction. Bottom shows removing 5 demonstrations with the lowest Itest results in 3 digit 0's remaining for the transformer to learn in context, leading to correct prediction. (Right) Average accuracy on 1013 ICL datasets repeated over 10 trials; \u03bb = 0.01; Lines and shades represent mean and standard error over 10 independent trials.", "description": "The figure visualizes how the DETAIL scores attribute model prediction to each demonstration on a custom transformer using the MNIST dataset.  The left panel shows a qualitative visualization of the Itest attribution by highlighting demonstrations with high Itest (green) and low Itest (red). It demonstrates how removing helpful demonstrations (high Itest) leads to wrong predictions and removing unhelpful ones (low Itest) still allows for correct prediction. The right panel shows the average accuracy of the model on 1013 datasets after removing various numbers of demonstrations with high Itest, low Itest, or randomly.", "section": "5.1 Evaluation on a Custom Transformer"}]