{"references": [{"fullname_first_author": "Alexander Kirillov", "paper_title": "Segment Anything", "publication_date": "2023-04-02", "reason": "This paper introduces the Segment Anything Model (SAM), the core model that SlimSAM aims to compress, making it the most central reference."}, {"fullname_first_author": "Gongfan Fang", "paper_title": "Isomorphic pruning for vision models", "publication_date": "2024-07-04", "reason": "This paper introduces a novel pruning method used as a building block within SlimSAM, demonstrating its significance to the overall approach."}, {"fullname_first_author": "Geoffrey Hinton", "paper_title": "Distilling the knowledge in a neural network", "publication_date": "2015-03-02", "reason": "Knowledge distillation is a key technique used in SlimSAM for model refinement, with this paper providing the foundational concept."}, {"fullname_first_author": "Pavlo Molchanov", "paper_title": "Importance estimation for neural network pruning", "publication_date": "2019-00-00", "reason": "This paper presents the Taylor pruning method which SlimSAM builds upon and improves, making it foundational to SlimSAM's pruning strategy."}, {"fullname_first_author": "Song Han", "paper_title": "Learning both weights and connections for efficient neural network", "publication_date": "2015-00-00", "reason": "This paper introduces the concept of structural pruning, which is directly relevant and utilized by SlimSAM's alternate slimming strategy."}]}