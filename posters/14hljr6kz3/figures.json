[{"figure_path": "14hLJr6kZ3/figures/figures_1_1.jpg", "caption": "Figure 1: Baselines performance on Office-Home", "description": "This figure compares the performance of various domain adaptation methods on the Office-Home benchmark.  It shows that the proposed method (PGA) outperforms other methods, particularly in terms of the number of trainable parameters required to achieve a given level of accuracy.  The plot illustrates that PGA achieves competitive accuracy with significantly fewer parameters, highlighting its efficiency and effectiveness.", "section": "Experiments"}, {"figure_path": "14hLJr6kZ3/figures/figures_6_1.jpg", "caption": "Figure 2: Performance of ERM and PGA on the in-domain data (validation set) and out-of-distribution data (test set). Average results and shaded standard errors are obtained from 10 random seeds.", "description": "This figure compares the performance of three different methods: Empirical Risk Minimization (ERM), a method using only gradient alignment, and the proposed Prompt Gradient Alignment (PGA) method.  The left panel shows the in-domain performance (on a validation set) across training epochs, while the right panel shows the out-of-domain performance (on a test set).  The shaded areas represent standard errors calculated from ten independent runs. The figure demonstrates that PGA consistently outperforms both ERM and the gradient alignment-only method in both in-domain and out-of-domain settings.", "section": "5.1 Illustrative example"}, {"figure_path": "14hLJr6kZ3/figures/figures_9_1.jpg", "caption": "Figure 3: Evolution of the gradient similarity during training.", "description": "The figure shows the evolution of cosine similarity between gradients during the training process using different values of the hyperparameter pga.  When pga=0 (no gradient alignment), the similarity is initially low and fluctuates, indicating a lack of consensus between gradients.  As pga increases (pga=1 and pga=10), the cosine similarity initially increases, suggesting that the gradient alignment encourages consensus between objectives. However, in all cases, the similarity decreases as training progresses, which is likely due to the model converging towards a Pareto optimal solution where the gradients are in conflict.", "section": "4.3 Prompt gradient alignment for UDA"}, {"figure_path": "14hLJr6kZ3/figures/figures_23_1.jpg", "caption": "Figure 2: Performance of ERM and PGA on the in-domain data (validation set) and out-of-distribution data (test set). Average results and shaded standard errors are obtained from 10 random seeds.", "description": "This figure compares the in-domain and out-of-domain performance of Empirical Risk Minimization (ERM) and Prompt Gradient Alignment (PGA).  The left panel shows that ERM achieves high accuracy on in-domain data, but its performance drops significantly on out-of-domain data.  The right panel demonstrates that PGA maintains high accuracy on both in-domain and out-of-domain data, illustrating its better generalization capability.", "section": "5.1 Illustrative example"}, {"figure_path": "14hLJr6kZ3/figures/figures_24_1.jpg", "caption": "Figure 1: Baselines performance on Office-Home", "description": "The figure shows the performance of different baselines on the Office-Home dataset.  The x-axis represents the number of trainable parameters, and the y-axis represents the average accuracy across various tasks.  The plot visually compares the performance of the proposed method (PGA) against other state-of-the-art methods for unsupervised domain adaptation, highlighting its superior performance with fewer trainable parameters.", "section": "1 Introduction"}, {"figure_path": "14hLJr6kZ3/figures/figures_24_2.jpg", "caption": "Figure 1: Baselines performance on Office-Home", "description": "The figure shows the performance comparison of various baselines on the Office-Home dataset.  It visualizes the accuracy achieved by different domain adaptation methods, highlighting the superior performance of the proposed method (PGA) compared to existing techniques like DAPL, MPA, Simple Prompt, MFSAN, etc. The x-axis represents the number of trainable parameters and the y-axis represents the average accuracy across multiple Office-Home sub-datasets.", "section": "Experiments"}]