[{"type": "text", "text": "Unified Covariate Adjustment for Causal Inference ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yonghan $\\mathbf{J}\\mathbf{u}\\mathbf{n}\\mathbf{g}^{1}$ , Jin Tian2, and Elias Bareinboim3 ", "page_idx": 0}, {"type": "text", "text": "1Purdue University 2Mohamed bin Zayed University of Artificial Intelligence 3Columbia University 1jung222@purdue.edu, 2jin.tian@mbzuai.ac.ae, 3eb@cs.columbia.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Causal effect identification and estimation are fundamental tasks found throughout the data sciences. Although causal effect identification has been solved in theory, many existing estimators only address a subset of scenarios, known as the sequential back-door adjustment (SBD) (Pearl and Robins, 1995a) or g-formula (Robins, 1986). Recent efforts for developing general-purpose estimators with broader coverage, incorporating the front-door adjustment (FD) (Pearl, 2000) and others, are not scalable due to the high computational cost of summing over a highdimensional set of variables. In this paper, we introduce a novel approach that achieves broad coverage of causal estimands beyond the SBD, incorporating various sum-product functionals like the FD, while achieving scalability \u2013 estimated in polynomial time relative to the number of variables and samples in the problem. Specifically, we present the class of unified covariate adjustment (UCA) for which we develop a scalable and doubly robust estimator. In particular, we illustrate the expressiveness of UCA for a wide spectrum of causal estimands (e.g., SBD, FD, and others) in causal inference. We then develop an estimator that exhibits computational efficiency and double robustness. Experiments corroborate the scalability and robustness of the proposed framework. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Causal inference is a crucial aspect of scientific research, with broad applications ranging from social sciences to economics, and from biology to medicine. Two significant tasks in causal inference are causal effect identification and estimation. Causal effect identification concerns determining the conditions under which the causal effect can be inferred from a combination of available data distributions and a causal graph depicting the data-generating process. Causal effect estimation, on the other hand, develops an estimator for the identified causal effect expression using finite samples. ", "page_idx": 0}, {"type": "text", "text": "Causal effect identification theories have been well-established across various scenarios. These include cases where the input distribution is purely observational (Tian and Pearl, 2003; Shpitser and Pearl, 2006; Huang and Valtorta, 2006) (known as observational identification or obsID) or a combination of observational and interventional (Bareinboim and Pearl, 2012a; Lee et al., 2019) (referred to as generalized identification or gID); scenarios where the target query and input distributions originate from different populations (Bareinboim and Pearl, 2012b; Bareinboim et al., 2014; Bareinboim and Pearl, 2016; Correa et al., 2018; Lee et al., 2020) (known as recoverability or transportability); or cases where the target query is counterfactual (Rung 3) (Correa et al., 2021) (referred to as Ctf-ID) beyond interventional (Rung 2) of the Ladder of Causation (Pearl and Mackenzie, 2018; Bareinboim et al., 2020). In these situations, algorithmic solutions have been devised that take input distributions along with specified target queries and formulate identification functionals as arithmetic operations (sums/integration, products, ratios) on conditional distributions induced from input distributions. ", "page_idx": 0}, {"type": "text", "text": "Despite all the progress, existing estimators cover only a subset of all identification scenarios. Specifically, well-established estimators for the back-door (BD) adjustment (Pearl, 1995), represented as $\\textstyle\\sum_{z}\\mathbb{E}[{\\bar{Y}}\\mid x,z]P(z)$ , and sequential back-door adjustment (SBD) (Robins, 1986; Pearl and Robins, 19 95b) and off-policy evaluations (OPE) (Murphy, 2003), which is an SBD with policy interventions, are known for their robustness to the bias (Bang and Robins, 2005; Robins et al., 2009; van der Laan and Gruber, 2012; Murphy, 2003; Rotnitzky et al., 2017; Luedtke et al., 2017; Uehara et al., 2022; D\u00edaz et al., 2023). These estimators are also scalable; i.e., evaluable in polynomial time relative to the number of covariates $(|Z|)$ and capable in the presence of mixed discrete and continuous covariates. However, SBDs only address a fraction of the broader spectrum of identification scenarios. ", "page_idx": 1}, {"type": "text", "text": "Beyond SBD, recent efforts have expanded to developing estimators for the front-door (FD) adjustment $\\begin{array}{r}{\\sum_{z,x^{\\prime}}\\mathbb{E}[Y\\mid x^{\\prime},z]P(z\\mid x)\\bar{P}(x^{\\prime})}\\end{array}$ (Pearl, 1995). At first glance, this adjustment appears similar to SBD, as both involve the sum-product of conditional probabilities. However, FD involves treatments variables in dual roles \u2013 one being summed $\\overrightharpoon{x}^{\\prime}$ in $\\begin{array}{r}{\\dot{\\sum}_{x^{\\prime}}\\operatorname{\\mathbb{E}}[Y\\mid x^{\\prime},z]P(x^{\\prime}))}\\end{array}$ and the other being fixed ( $x$ in $P(z\\mid x),$ ). While FD estimators achieving d oubly robustness have been developed (Fulcher et al., 2019; Guo et al., 2023), they lack scalability due to the necessity of summing over the values of $Z$ (i.e., $\\textstyle\\sum_{z},$ , thereby limiting its practicality when $Z$ is high-dimensional or continuous. ", "page_idx": 1}, {"type": "text", "text": "Similar challenges arise in more general identification scenarios beyond SBD and FD. Recent efforts have focused on developing estimators for broad causal estimands, such as Tian\u2019s adjustment (Tian and Pearl, 2002a), which incorporates FD and other cases where causal effects are represented as sum-product functionals (Bhattacharya et al., 2022). These efforts also include work on covering any identification functional (Jung et al., 2021a; Xia et al., 2021, 2022; Bhattacharya et al., 2022; Jung et al., 2023a). While these estimators are designed to achieve a wide coverage of functionals, they lack scalability due to the necessity of summing over high-dimensional variables. ", "page_idx": 1}, {"type": "text", "text": "Thus far, we have assessed the pair (functional class, estimator) based on two criteria: (1) coverage of the functional class, and (2) scalability of the corresponding estimators. Scalable estimators achieving doubly robustness have been established predominantly for BD/SBD classes. While recent studies have developed estimators with a strong emphasis on coverage (e.g., any identification functional), less attention has been given to achieving scalability. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we establish a novel pair of a functional class and its corresponding estimation frameworks designed to ensure scalability while covering a broad spectrum of identification functionals. Our work aims to maximize coverage, enabling the effective development of scalable estimators with the doubly robust prop", "page_idx": 1}, {"type": "table", "img_path": "aX9z2eT6ul/tmp/512f801abb1f1c74a15de8b655122bf8dca301a95c67121cf26300f1d4be6f80.jpg", "table_caption": [], "table_footnote": ["Table 1: Scope. $\\checkmark$ denotes the addressed area (by UCA or prior works). \u2717denotes the unaddressed area. $\\blacktriangle$ denotes the partially addressed area. ? indicates areas where no known results are present. "], "page_idx": 1}, {"type": "text", "text": "erty. This functional class, termed unified covariate adjustment (UCA), integrates a sum-product of conditional distributions appearing in many causal inference scenarios such as BD/FD, Tian\u2019s adjustment, $S$ -admissibility in transportability/recoverability (Bareinboim and Pearl, 2016), effect-oftreatment-on-the-treated (ETT) (Heckman, 1992), and nested counterfactuals (Correa et al., 2021). The coverage of the proposed class is further demonstrated through the application to a novel estimand for the counterfactual directed effect (Ctf-DE) derived from fairness analysis (Plec\u02c7ko and Bareinboim, 2024). For the proposed UCA class, we develop a scalable and doubly robust estimator evaluable computationally efficiently relative to the number of samples. Table 1 visualizes the scope of our framework. The contributions of this paper are as follows: ", "page_idx": 1}, {"type": "text", "text": "1. We introduce unified covariate adjustment (UCA), a comprehensive framework that encompasses a broad class of sum-product causal estimands. This framework\u2019s expressiveness is demonstrated across various scenarios beyond SBD, including Tian\u2019s adjustment that incorporates FD and others as well novel counterfactual scenarios in fairness analysis. ", "page_idx": 1}, {"type": "text", "text": "2. We develop a corresponding estimator that is computationally efficient and doubly robust and provide its finite sample guarantee. We demonstrate scalability and robustness to bias both theoretically and empirically through simulations. ", "page_idx": 1}, {"type": "image", "img_path": "aX9z2eT6ul/tmp/9aecf9e3609159a0801630da21f741d6a730a2357d45bfdec971e4b103090b1a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: (a) Front-door in Example 1, (b) Verma in Example 2, (c) Napkin, (d) Standard fairness model in Example 3, and (e) Example graph from (Jung et al., 2021a, Fig. 1b) ", "page_idx": 2}, {"type": "text", "text": "Notations. We use $(\\mathbf{X},\\,X,\\,\\mathbf{x},\\,x)$ to denote a random vector, variable, and their realized values, respectively. For a function $f(\\mathbf{z}_{i})$ for $i\\;=\\;1,2,\\cdot\\cdot\\cdot$ ,, we use $\\begin{array}{r}{\\sum_{i}f(\\mathbf{z}_{i})\\;=\\;f(\\mathbf{z}_{1})\\,+\\,f(\\mathbf{z}_{2})\\cdot\\cdot\\cdot\\cdot.}\\end{array}$ . Also, for a function $f(\\mathbf{z})$ , we use $\\sum_{\\mathbf{z}}f(\\mathbf{z})$ to denote the summ ation/integration over a mixture of discrete/continuous random var iables $\\mathbf{Z}$ . For example, we write the back-door adjustment as $\\sum_{\\mathbf{z}}\\mathbb{E}_{P}[Y\\mid x,\\mathbf{z}]P(\\mathbf{z})$ when $\\mathbf{Z}$ is a mixture of discrete/continuous variables. Given an ordered set $\\mathbf{X}\\,=\\,\\{X_{1},\\cdots\\,,X_{n}\\}$ , we denote $\\mathbf{X}^{(i)}\\;:=\\;\\{X_{1},\\cdots,X_{i}\\}$ and $\\mathbf{X}^{\\geq i}\\;:=\\;\\{X_{i+1},\\cdot\\cdot\\cdot\\;,X_{m}\\}$ for $m=|\\mathbf{X}|$ . For a discrete $\\mathbf{X}$ , we use $\\mathbb{I}_{\\mathbf{x}}(\\mathbf{X})$ as a function such that $\\mathbb{1}_{\\mathbf{x}}(\\mathbf{X})=1$ if ${\\bf X}={\\bf x}$ ; $\\mathbb{1}_{\\mathbf{x}}(\\mathbf{X})=0$ otherwise. $P({\\bf V})$ denotes a distribution over $\\mathbf{V}$ and $P({\\bf v})$ as a probability at ${\\textbf{V}}={\\textbf{v}}$ , We use $\\mathbb{E}_{P}[f(\\mathbf{V})]$ and $\\mathbb{V}_{P}[f(\\mathbf{V})]$ to denote the mean and variance of $f(\\mathbf{V})$ relative to $P({\\bf V})$ . We use $\\|f\\|_{P}:=\\sqrt{\\mathbb{E}_{P}[\\{f(\\mathbf{V})\\}^{2}]}$ as L2-norm of $f$ with $P$ . If a function $\\widehat{f}$ is a consistent estimator of $f$ having a rate $r_{n}$ , we will use ${\\widehat{f}}-f=o_{P}(r_{n})$ . We will say $\\hat{f}$ is $L_{2}$ -consistent if $\\|\\hat{f}-f\\|_{P}=o_{P}(1)$ . We will use ${\\widehat{f}}-f=O_{P}(1)$ if ${\\widehat{f}}-f$ is bounded in probability. Also, ${\\widehat{f}}-f$ is said to be bounded in probability at rate $r_{n}$ if ${\\widehat{f}}-f=O_{P}(r_{n})$ . $[n]:=\\{1,\\cdot\\cdot\\cdot,n\\}$ is a collection of index. $D:=\\{\\mathbf{V}_{(i)}:$ $i\\in[n]\\}$ denotes a sample set, where $\\mathbf{V}_{(i)}$ denote the ith sample in $\\mathcal{D}$ . The empirical average of $f(\\mathbf{V})$ with samples $\\mathcal{D}$ is $\\begin{array}{r}{\\mathbb{E}_{\\mathcal{D}}[f(\\mathbf{V})]:=(1/|\\mathcal{D}|)\\sum_{i:\\mathbf{V}_{(i)}\\in\\mathcal{D}}f(\\mathbf{V}_{(i)})}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "Structural causal models. We use structural causal models (SCMs) (Pearl, 2000; Bareinboim et al., 2020) as our framework. An SCM $\\mathcal{M}$ is a quadruple $\\mathcal{M}\\,=\\,\\langle\\mathbf{U},\\mathbf{V},P(\\mathbf{U}),\\mathcal{F}\\rangle$ , where $\\mathbf{U}$ is a set of exogenous (latent) variables following a joint distribution $P({\\bf U})$ , and $\\mathbf{V}$ is a set of endogenous (observable) variables whose values are determined by functions $\\mathcal{F}=\\{f_{V_{i}}\\}_{V_{i}\\in\\mathbf{V}}$ such that $V_{i}\\gets f_{V_{i}}(\\mathbf{pa}_{i},\\mathbf{u}_{i})$ where $\\mathbf{PA}_{i}\\subseteq V$ and $\\mathbf{U}_{i}\\subseteq\\mathbf{U}$ . Each SCM $\\mathcal{M}$ induces a distribution $P({\\bf V})$ and a causal graph ${\\mathcal{G}}={\\mathcal{G}}({\\mathcal{M}})$ over $\\mathbf{V}$ in which directed edges exists from every variable in $\\mathbf{PA}_{i}$ to $V_{i}$ and dashed-bidirected arrows encode common latent variables. Performing an intervention fixing ${\\bf X}={\\bf x}$ is represented through the do-operator, $\\operatorname{do}(\\mathbf{X}=\\mathbf{x})$ , which encodes the operation of replacing the original equations of $X$ (i.e., $f_{X}(\\mathbf{pa}_{x},\\mathbf{u}_{x}))$ by the constant $x$ for all $X\\in\\mathbf{X}$ and induces an interventional distribution $P(\\mathbf{V}\\mid\\operatorname{do}(\\mathbf{x}))$ . For any $\\mathbf{Y}\\subseteq\\mathbf{V}$ , the potential response $\\mathbf{Y}_{\\mathbf{x}}(\\mathbf{u})$ is defined as the solution of $\\mathbf{Y}$ in the submodel $\\mathcal{M}_{\\bf x}$ given $\\mathbf{U}=\\mathbf{u}$ , which induces a counterfactual variable $\\mathbf{Y_{x}}$ . ", "page_idx": 2}, {"type": "text", "text": "Related work. Our work is an extension of existing sequential back-door adjustment (SBD) estimators (Mises, 1947; Bickel et al., 1993; Bang and Robins, 2005; Robins et al., 2009; van der Laan and Gruber, 2012; Rotnitzky et al., 2017; Luedtke et al., 2017; D\u00edaz et al., 2023) to a broader class of sum-product functionals, such as the front-door adjustment (FD) and Tian\u2019s adjustment (Tian and Pearl, 2002a) which generalizes FD and more, and nested counterfactuals, which will be detailed in later sections. Our work is aligned with recent works of Chernozhukov et al. (2022); Li and Luedtke (2023); Quintas-Martinez et al. (2024), which examined SBD derived from various joint distributions. Specifically, Li and Luedtke (2023) considered the SBD setting where conditional distributions are induced from different sources. In contrast, we study a broader class of sum-product functionals from multiple populations. Also, Quintas-Martinez et al. (2024) considered the Markovian model $\\textstyle\\prod_{i=1}^{n}P^{i}(\\bar{V}_{i}\\mid\\mathbf{\\dot{P}}\\mathbf{\\dot{A}}_{i})$ where each $P^{i}$ can be distinct. In contrast, we study a broader class of estimands that are not confined to conditioning on $\\mathbf{PA}_{i}$ . On the other hands, (Chernozhukov et al., 2022) considered the case where covariate distributions are allowed to be changed, and demonstrated that FD can be captured through this technique. Our work expands on these findings by covering a broader class, such as the Tian\u2019s adjustment and a nested counterfactual in fairness literature, and by providing a more formal theory that includes finite sample guarantees and asymptotic analysis. ", "page_idx": 2}, {"type": "text", "text": "2 Unified Covariate Adjustment ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A class of causal estimands termed unified covariate adjustment (UCA) is defined as follows: ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (Unified Covariate Adjustment (UCA)). Let $\\Psi[\\mathbf{P};\\pmb{\\sigma}]$ denote the following probability measure over an ordered set $\\mathbf{V}:=(\\mathbf{C}_{1},\\mathbf{R}_{1},\\cdots,\\mathbf{C}_{m},\\mathbf{R}_{m},\\bar{Y}:=\\mathbf{\\bar{C}}_{m+1}\\right)$ : $\\Psi[\\mathbf{P};\\pmb{\\sigma}]:=\\bar{P}^{m+1}(Y\\mid$ $\\begin{array}{r}{\\mathbf{S}_{m})\\prod_{i=1}^{m}P^{i}(\\mathbf{C}_{i}\\mid\\mathbf{S}_{i-1})\\sigma_{\\mathbf{R}_{i}}^{i}(\\mathbf{R}_{i}\\mid\\dot{\\mathbf{S}}_{i}\\mid\\mathbf{R}_{i})}\\end{array}$ , where ", "page_idx": 3}, {"type": "text", "text": "\u2022 ${\\mathbf{P}}:=\\{P^{i}({\\mathbf{V}}):i\\in[m\\!+\\!1]\\}$ is a set of distributions in the form of $P^{i}(\\mathbf{V})=Q^{i}(\\mathbf{V}\\mid\\mathbf{S}_{i-1}^{b}=\\mathbf{s}_{i-1}^{b})$ ), where $Q^{i}$ is a distribution, $\\mathbf{S}_{i-1}^{b}$ is $a$ (potentially empty) set. Each pairs $P^{i}(\\mathbf{V})$ and $P^{j}({\\bf V})$ can be the same $(P^{i}(\\mathbf{V})=P^{j}(\\mathbf{V}))$ or distinct $(P^{i}(\\mathbf{V})\\neq P^{j}(\\mathbf{V}))$ .   \n\u2022 For $i\\in[m+1],\\,{\\mathbf{S}}_{i-1}:=({\\mathbf{C}}^{(i-1)}\\cup{\\mathbf{R}}^{(i-1)})\\setminus{\\mathbf{S}}_{i-1}^{b}.$   \n\u2022 Each $\\mathbf{R}_{i}$ is controlled by a pre-specified / known probability measure $\\sigma_{\\mathbf{R}_{i}}^{i}:=\\sigma_{\\mathbf{R}_{i}}^{i}(\\mathbf{r}_{i}\\mid\\mathbf{s}_{i}\\mid\\mathbf{r}_{i})$ where $\\begin{array}{r}{\\sum_{\\mathbf{r}_{i}}\\sigma_{\\mathbf{R}_{i}}^{i}(\\mathbf{r}_{i}\\mid\\mathbf{s}_{i}\\setminus\\mathbf{r}_{i})=1}\\end{array}$ and $0\\leq\\sigma_{\\mathbf{R}_{i}}^{i}\\leq1$ almost surely (e.g., $\\sigma_{\\mathbf{R}_{i}}^{i}:=\\mathbb{1}_{\\mathbf{r}_{i}}(\\mathbf{R}_{i}))$ ). ", "page_idx": 3}, {"type": "text", "text": "Then, the expectation of $Y$ over $\\Psi[\\mathbf{P};\\pmb{\\sigma}]$ is called a Unified Covariate Adjustment $(U C A)$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\psi_{0}:=\\mathbb{E}_{\\Psi[\\mathbf{P};\\sigma]}[Y]=\\sum_{\\mathrm{clcr}}\\mathbb{E}_{P^{m+1}}[Y\\mid\\mathbf{s}_{m}]\\prod_{i=1}^{m}P^{i}(\\mathbf{c}_{i}\\mid\\mathbf{s}_{i-1})\\sigma_{\\mathbf{R}_{i}}^{i}(\\mathbf{r}_{i}\\mid\\mathbf{s}_{i}\\mid\\mathbf{r}_{i}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We will exemplify that UCA encompasses many well-known causal estimands, including the sequential back-door adjustment (SBD) (Robins, 1986; Pearl and Robins, 1995b), front-door adjustment (Pearl, 1995), Tian\u2019s adjustment (Tian and Pearl, 2002a), $S$ -admissibility in transportability/recoverability (Bareinboim and Pearl, 2016), effect-of-treatment-on-the-treated (ETT) (Heckman, 1992), nested counterfactuals (Correa et al., 2021), treatment-treatment interaction (Jung et al., 2023b), and off-policy evaluation (Murphy, 2003). This section particularly focuses on recently developed and lesser-known estimands, for which scalable estimators have been rarely explored. Appendix B provides additional examples, demonstrating how UCA can represent well-known estimands such as off-policy evaluation and $S$ -admissibility. ", "page_idx": 3}, {"type": "text", "text": "At first glance, UCA closely resembles the sequential back-door adjustment (SBD) (Robins, 1986; Pearl and Robins, 1995b). Indeed, UCA is reduced to SBD in the special case where $P^{i}=P(\\mathbf{V})$ for all $i=1,\\cdot\\cdot\\cdot,m+1$ and $\\sigma_{\\mathbf{R}_{i}}^{i}:=\\mathbb{1}_{\\mathbf{r}_{i}}(\\mathbf{R}_{i})$ ; i.e., $\\begin{array}{r}{\\psi_{0}=\\sum_{\\mathbf{c}}\\mathbb{E}_{P}[Y\\mid\\mathbf{c}^{(m)}\\cup\\mathbf{r}^{(m)})]\\prod_{i=1}^{m}P(\\mathbf{c}_{i}\\mid}\\end{array}$ $\\mathbf{c}^{(i-1)}\\cup\\mathbf{r}^{(i-1)})$ . However, UCA provides flexibility to represent target estimands beyond SBD by allowing $P^{i}$ to be any distribution that aligns with the target estimand, permitting arbitrary conditional distributions beyond the observational distribution $P$ . To demonstrate, consider the front-door adjustment (FD) scenario (Pearl, 1995) depicted in Fig. 1a: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}[Y\\mid\\operatorname{do}(x)]=\\sum_{c,z,x^{\\prime}}\\mathbb{E}[Y\\mid c,x^{\\prime},z]P(z\\mid c,x)P(c,x^{\\prime}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Even though FD cannot be expressed using SBD because the treatment variable $X$ is being fixed (in $P(z\\mid c,x))$ and summed (with $\\sum_{x^{\\prime}}$ ) simultaneously, it can be represented through UCA as follows: Example 1 (FD as UCA). FD can be written as the expectation of $Y$ over $P(Y\\mid Z,X,C)P(Z\\mid$ $x,C)\\bar{P}(X,C)$ . We set $\\mathbf{C}_{1}:=\\{X,C\\}$ , $\\mathbf{C}_{2}:=\\{Z\\}$ , $\\mathbf{R}={\\boldsymbol{\\emptyset}}$ , $P^{1}(\\mathbf{C}_{1})=P(X,\\dot{C})$ , $P^{2}(\\mathbf{C}_{2}\\mid\\mathbf{S}_{1})=$ $P(Z\\mid x,C)$ with $\\mathbf{S}_{1}^{b}=\\left\\{X\\right\\}$ , ${\\bf S}_{1}=\\{C\\}$ , and $P^{3}(Y\\mid\\mathbf{S}_{3})=P(Y\\mid Z,X,C)$ with $\\mathbf{S}_{2}=\\{Z,X,C\\}$ . ", "page_idx": 3}, {"type": "text", "text": "Next, consider Verma\u2019s equation (Verma and Pearl, 1990; Tian and Pearl, 2002b) with Fig. 1b: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}[Y\\mid\\operatorname{do}(x)]=\\sum_{b,a,x^{\\prime}}\\mathbb{E}[Y\\mid b,a,x]P(b\\mid a,x^{\\prime})P(a\\mid x)P(x^{\\prime}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $X$ is fixed to $x$ in $\\mathbb{E}[Y\\mid x,a,b]$ and $P(a\\mid x)$ while summed in $P(b\\mid a,x^{\\prime})$ and $P(x^{\\prime})$ . Similar to FD, due to the dual role of $X$ , the existing SBD framework is not suitable to express Verma\u2019s equation, which can be represented through UCA as follows: ", "page_idx": 3}, {"type": "text", "text": "Example 2 (Verma as UCA). Verma\u2019s equation is expressible as the expectation of $Y$ over $P(Y\\,|\\,$ $B,A,x)P(B\\mid A,X)P(A\\mid x)P(X)$ . We set $\\mathbf{C}_{1}=\\{X\\}$ , $\\mathbf{C}_{2}=\\{A\\}$ , ${\\bf C}_{3}=\\{B\\}$ , and $\\mathbf{R}=\\varnothing$ . We map $P^{1}(\\mathbf{C}_{1}):=P(X),\\,P^{2}(\\mathbf{C}_{2}\\mid\\mathbf{S}_{1})=P(A\\mid x)$ with $\\mathbf{S}_{1}=\\varnothing$ , $\\mathbf{S}_{1}^{b}=\\left\\{X\\right\\}$ , $P^{3}(\\mathbf{C}_{3}\\mid\\mathbf{S}_{2})=P(B\\mid$ $A,X)$ with $\\mathbf{S}_{2}=\\{A,X\\}$ , and $P^{4}(Y\\mid\\mathbf{S}_{3})=P(Y\\mid B,A,x)$ with $\\mathbf{S}_{3}=\\{B,A\\}$ , $\\mathbf{S}_{3}^{b}=\\{X\\}$ . ", "page_idx": 3}, {"type": "text", "text": "In both examples, the variable $\\mathbf{S}^{b}i=X$ is bifurcated, being fixed in some conditional distributions (e.g., $P(z\\mid x,c)$ in the front-door criterion (FD)) and summed over $\\sum x^{\\prime}$ in others (e.g., $P(y\\mid z,x^{\\prime},c)$ ", "page_idx": 3}, {"type": "text", "text": "Input: A graph $\\mathcal{G}$ and a set of topologically ordered variables $\\mathbf{V}:=(V_{1},\\cdots,V_{K},Y)$ . 1 Set $\\mathbf{C}_{1}:=(\\bar{V}_{1},\\cdot\\cdot\\cdot\\,,V_{k-1},V_{k}:=X,V_{k+1},\\cdot\\cdot\\cdot\\,,V_{k+i_{1}})$ as an ordered sequence, where $\\left(V_{1},\\cdot\\cdot\\cdot,V_{k-1}\\right)$ are predecessors of $X$ , and $\\left(V_{k+1},\\cdot\\cdot\\cdot,V_{k+i_{1}}\\right)$ are successors of $X$ within $\\mathbf{S}_{X}$ . 2 Set $P^{1}(\\mathbf{C}_{1}):=P(\\mathbf{C}_{1})$ , $i:=2$ and $\\mathbf{R}:=\\emptyset$ . 3 while $\\mathbf{V}\\setminus(\\{Y\\}\\cup\\mathbf{C}^{(i-1)})\\neq\\emptyset$ do 4 if $\\mathbf{C}_{i-1}\\subseteq\\mathbf{S}_{X}$ , set $\\mathbf{C}_{i}$ as the next sequence of vertices in $\\mathbf{V}\\setminus(\\{Y\\}\\cup\\mathbf{C}^{(i-1)})$ that are not in $\\mathbf{S}_{X}$ ; $\\mathbf{S}_{i-1}:=\\mathbf{C}^{(i-1)}\\setminus\\{X\\}$ ; and $P^{i}(\\mathbf{C}_{i}\\mid\\mathbf{S}_{i-1}):=P(\\mathbf{C}_{i}\\mid\\mathbf{S}_{i-1},x)$ with $\\mathbf{S}_{i-1}^{b}:=\\{X\\}$ . 5 else, set $\\mathbf{C}_{i}$ as the next sequence of vertices in $\\mathbf{V}\\setminus(\\{Y\\}\\cup\\mathbf{C}^{(i-1)})$ that are in $\\mathbf{\\mathsf{j}}_{X};\\mathbf{\\mathbf{S}}_{i-1}:=\\mathbf{C}^{(i-1)}$ ; and $P^{i}(\\mathbf{C}_{i}\\mid\\mathbf{S}_{i-1}):=P(\\mathbf{C}_{i}\\mid\\mathbf{S}_{i-1})$ with $\\mathbf{S}_{i-1}^{b}:=\\emptyset$ . 6 $i\\gets i+1$ . 7 end 8 Set $m\\gets i$ . If $Y\\in\\mathbf{S}_{X}$ , set $\\mathbf{S}_{m}:=\\mathbf{C}^{(m)}$ , $\\mathbf{S}_{m}^{b}=\\emptyset$ , and $P^{m+1}(Y\\mid\\mathbf{S}_{m})=P(Y\\mid\\mathbf{S}_{m})$ . Otherwise, set $\\mathbf{S}_{m}:=\\mathbf{C}^{(m)}\\setminus\\{X\\}$ , $\\mathbf{S}_{m}^{b}=\\{X\\}$ , and $P^{m+1}(Y\\mid\\mathbf{S}_{m})=P(Y\\mid\\mathbf{S}_{m},x)$ . 9 return $\\mathbb{E}_{\\Psi[\\mathbf{P}]}[Y]$ where $\\begin{array}{r}{\\Psi[\\mathbf{P}]:=P^{m+1}(Y\\mid\\mathbf{S}_{m})\\prod_{i=1}^{m}P^{i}(\\mathbf{C}_{i}\\mid\\mathbf{S}_{i-1}).}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "in FD). Both FD and Verma\u2019s equations are special cases of Tian\u2019s adjustment (Tian and Pearl, 2002a), which states that $\\mathbb{E}[Y\\mid\\operatorname{do}(x)]$ is identifiable under certain conditions. Specifically, when $X$ and its children $\\operatorname{ch}{\\mathcal{G}}(X)$ in the graph $\\mathcal{G}$ are not connected by bidirected edges, it can be expressed as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}[Y\\mid\\operatorname{do}(x)]=\\sum_{\\mathbf{v}\\setminus x y}\\sum_{x^{\\prime}}\\mathbb{E}_{P^{\\prime}}[Y\\mid\\mathbf{v}^{(K)}]\\prod_{i=1}^{K}P^{\\prime}(v_{i}\\mid\\mathbf{v}^{(i-1)}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{V}:=(V_{1},V_{2},\\cdots,V_{K},Y)$ is a topologically ordered set with $V_{k}\\,:=\\,X$ for some $k$ being the treatment variable $X$ , $P^{\\prime}(v_{i}\\mid\\mathbf{v}^{(i-1)}):=P(v_{i}\\mid\\mathbf{v}^{(k-1)},x,v_{k+1},\\cdot\\cdot\\cdot\\mid,v_{i-1})$ (i.e., $X$ is fixed to $x$ ) if $V_{i}\\not\\in{\\bf S}_{X}$ where $\\mathbf{S}_{X}$ is the set of vertices connected with $X$ through bidirected edges, and $P^{\\prime}(v_{i}\\mid\\mathbf{v}^{(i-1)}):=P(v_{i}\\mid\\mathbf{v}^{(k-1)},x^{\\prime},v_{k+1},\\cdot\\cdot\\cdot\\mid,v_{i-1})$ (i.e., $X$ is summed with $\\sum_{x^{\\prime}})$ if $V_{i}\\in\\mathbf{S}_{X}$ . In Tian\u2019s adjustment, $X$ is bifurcated into summed through $\\sum_{x^{\\prime}}$ and fixed to $X=x$ . We exhibit the expressiveness of UCA for Tian\u2019s adjustment: ", "page_idx": 4}, {"type": "text", "text": "Proposition 1. Tian\u2019s adjustment in Eq. (4) is UCA-expressible through Algo. 1. ", "page_idx": 4}, {"type": "text", "text": "Next, we exhibit the coverage of the UCA for a counterfactual quantity in the fairness literature. Specifically, we focus on the counterfactual directed effect (Ctf-DE) in the Standard fairness model (SFM) (Ple\u02c7cko and Bareinboim, 2024), as illustrated in Fig. 1d. This model includes several key components: the protected (discrete) attribute $(X)$ , such as race; the baseline covariates $(Z)$ , like age; the mediator variables $(W)$ affected by $X$ , for example, educational level; and the outcome variable $(Y)$ , such as salary. Consider a scenario where we investigate the the query, \u201cWhat would be the expected salary for someone who is Black, but hypothetically of Asian race and had been educated as $a$ White person typically would be?\u201d. The query is represented as Ctf-DE: $\\mathbb{E}[Y_{X=x_{0},W_{X=x_{1}}}\\mid X=x_{2}]$ , where $x_{0},x_{1}$ , and $x_{2}$ correspond to the races Asian, White, and Black, respectively. This query can be identified through the algorithm in (Correa et al., 2021) under the SFM in Fig. 1d: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}[Y_{X=x_{0},W_{X=x_{1}}}\\mid X=x_{2}]=\\sum_{w,z}\\mathbb{E}[Y\\mid X=x_{0},w,z]P(w\\mid X=x_{1},z)P(z\\mid X=x_{2}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This identification functional is UCA-expressible: ", "page_idx": 4}, {"type": "text", "text": "Example 3 (Ctf-DE as UCA). The Ctf-DE is expressible through the expectation of $Y$ over $P(Y\\,|\\,$ $X=x_{0},W,Z)P(W\\mid X,Z)P(Z\\mid X=x_{2})\\mathbb{1}_{x_{1}}(X)$ . Set $\\mathbf{R}_{1}:=\\{X\\},\\,\\sigma_{\\mathbf{R}_{1}}^{\\mathsf{1}}:=\\mathbb{1}_{x_{1}}(X),\\,P^{1}(\\mathbf{C}_{1})=$ $P(Z\\mid X=x_{2})$ with ${\\bf C}_{1}=\\{Z\\}$ and $\\mathbf{S}_{0}^{b}=\\left\\{X\\right\\}$ , $P^{2}(\\mathbf{C}_{2}\\mid\\mathbf{S}_{1})=P(W\\mid X,Z)$ with ${\\bf C}_{2}=\\{W\\}$ and $\\mathbf{S}_{1}=\\{X,Z\\},\\,P^{3}(Y\\mid\\mathbf{S}_{2})=P(Y\\mid X=x_{0},W,Z)$ with $\\mathbf{S}_{2}=\\{W,Z\\}$ and $\\mathbf{S}_{2}^{b}=\\{X\\}$ . ", "page_idx": 4}, {"type": "text", "text": "Despite the broad expressiveness of UCA, as illustrated in this section and ppendix B, not all causal estimand functionals are UCA-expressible. To witness, consider the \u2018napkin\u2019 estimand described in (Pearl and Mackenzie, 2018; Jung et al., 2021a) with $\\mathcal{G}$ in Fig. 1c, defined as $P(y\\mid\\operatorname{do}(x))=$ w P P( (y,xx|r|,r,ww))P P( (ww)) . Here, the functional for E[Y | do(x)] is represented not as the expectation of a product of conditional distributions, but rather as a quotient of sums of conditional distributions. The napkin estimand is not UCA-expressible. Intuitively, if a target functional is expressed as an expectation of a probability measure that is represented as a product of multiple conditional distributions, it can be captured through UCA. A formal criterion is the following: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Theorem 1 (Expressiveness). Suppose a functional $\\psi_{0}$ is expressed as the mean of the following measure, $\\begin{array}{r}{P^{m+1}\\bar{(}Y\\mid\\mathbf{S}_{m}^{\\prime})\\prod_{i=1}^{m}P^{\\hat{i}}\\bar{(}\\mathbf{C}_{i}\\mid\\check{\\mathbf{S}}_{i-1}^{\\prime})\\sigma_{\\mathbf{R}_{i}}^{i}(\\mathbf{R}_{i}\\mid\\mathbf{S}_{i}^{\\prime}\\mid\\mathbf{\\dot{R}}_{i}),}\\end{array}$ , where $\\mathbf{S}_{i}^{\\prime}=(\\mathbf{C}^{(\\bar{i})}\\cup\\bar{\\mathbf{R}}^{(i)})\\setminus\\mathbf{S}_{i}^{\\breve{b}}$ for each $i=1,\\hdots,m$ and $P^{j}({\\bf V})$ for $j=1,\\dots,m+1$ are distributions of the form $P^{j}({\\bf V})=$ $\\begin{array}{r}{Q^{j}(\\mathbf{V}\\mid\\mathbf{S}_{j-1}^{b}=\\mathbf{s}_{j-1}^{b})}\\end{array}$ ). Then, the functional $\\psi_{0}$ can be expressed through UCA in Eq. (1). ", "page_idx": 5}, {"type": "text", "text": "3 Scalable Estimator for Unified Covariate Adjustment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "So far, we discussed the coverage of UCA. In this section, we construct a scalable estimator for UCA that achieves doubly robustness property and provide its finite sample guarantee. We define the estimator with two sets of nuisance parameters $\\pmb{\\mu}$ and $\\pi,\\,\\mu$ is a collection of regression parameters, and $\\pi$ is a collection of ratio parameters. ", "page_idx": 5}, {"type": "text", "text": "We introduce sets to define the regression nuisances. Define $\\mathbf{B}_{i-1}:=\\mathbf{S}_{i}\\cap\\mathbf{C}_{j}\\cap\\mathbf{S}_{i-1}^{b}$ for some $j<i$ for $i=2,\\cdots,m$ as a bifurcated set, variables fixed to $\\mathbf{s}_{i-1}^{b}$ at $P^{i}(\\mathbf{C}_{i}\\mid\\mathbf{S}_{i-1})$ , while marginalized out over $P^{j}(\\mathbf{C}_{j}\\mid\\mathbf{S}_{j-1})$ (e.g., $X$ in FD). Set $\\mathbf{B}_{m}\\,=\\,\\emptyset$ . We use $\\mathbf{B}_{i-1}^{\\prime}$ to denote an independent copy of $\\mathbf{B}_{i-1}$ (variables following the same distribution as $\\mathbf{B}_{i-1}$ but independent of $\\mathbf{B}_{i-1}$ and ${\\mathbf V})$ . With $\\mathbf{B}_{i-1}$ and $\\mathbf{B}_{i-1}^{\\prime}$ , we define $\\mathbf{S}_{i}^{\\prime}:=\\left((\\mathbf{S}_{i}\\cup\\mathbf{B}_{i})\\setminus\\mathbf{B}_{i-1}\\right)\\cup\\mathbf{B}_{i-1}^{\\prime}$ and $\\check{\\mathbf{S}}_{i}:=\\,\\mathbf{S}_{i}^{\\prime}\\setminus\\mathbf{R}_{i}$ for $i=2,\\cdots,m$ . Define the regression nuisance parameters as follows: $\\mu_{0}^{m}(\\mathbf{S}_{m}):=\\mathbb{E}_{P^{m+1}}[Y\\mid\\mathbf{S}_{m}]$ and $\\begin{array}{r}{\\check{\\mu}_{0}^{m}(\\check{\\mathbf{S}}_{m}):=\\sum_{\\mathbf{r}_{m}}\\sigma_{\\mathbf{R}_{m}}^{m}(\\mathbf{r}_{m}\\mid\\mathbf{S}_{m}\\setminus\\mathbf{R}_{m})\\mu_{0}^{m}(\\mathbf{r}_{m},\\check{\\mathbf{S}}_{m})}\\end{array}$ . For $i=m-1,\\cdots\\,,1$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mu_{0}^{i}(\\mathbf{S}_{i},\\mathbf{B}_{i}^{\\prime}):=\\mathbb{E}_{P^{i+1}}\\big[\\check{\\mu}_{0}^{i+1}(\\check{\\mathbf{S}}_{i+1})\\mid\\mathbf{S}_{i},\\mathbf{B}_{i}^{\\prime}\\big],\\quad}\\\\ &{}&{\\check{\\mu}_{0}^{i}(\\check{\\mathbf{S}}_{i}):=\\displaystyle\\sum_{\\mathbf{r}_{i}}\\sigma_{\\mathbf{R}_{i}}^{i}(\\mathbf{r}_{i}\\mid\\mathbf{S}_{i}\\setminus\\mathbf{R}_{i})\\mu_{0}^{i}(\\mathbf{r}_{i},\\mathbf{S}_{i}^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Equipped with the regression nuisances, UCA can be computed as follows: ", "page_idx": 5}, {"type": "text", "text": "Proposition 2. UCA in Eq. (1) can be parameterized as $\\psi_{0}=\\mathbb{E}_{P^{1}}[\\check{\\mu}_{0}^{1}(\\check{\\mathbf{S}}_{1})]$ . ", "page_idx": 5}, {"type": "text", "text": "Whenever no variables are being summed and fixed simultaneously (i.e., $\\mathbf{B}_{i-1}\\,=\\,\\emptyset$ for all $i\\,=$ $2,\\cdot\\cdot\\cdot,m)$ in the UCA functional, as in Eq. (5) in Ctf-DE, the standard SBD adjustment or examples in Appendix B, we can estimate $\\pmb{\\mu}$ through nested regression methods with off-the-shelf regression models and compute UCA in Eq. (1) as $\\psi_{0}\\,=\\,\\mathbb{E}_{P^{1}}\\bar{[}\\breve{\\mu}_{0}^{1}(\\breve{\\mathbf{S}}_{1})\\big]$ . This approach aligns with existing SBD estimators (Bang and Robins, 2005; Robins et al., 2009; van der Laan and Gruber, 2012; Rotnitzky et al., 2017; Luedtke et al., 2017; D\u00edaz et al., 2023). For instance, in Ctf-DE in Example 3, $(W,\\dot{Z})\\;:=\\;\\mathbb{E}_{P}[Y\\;\\mid\\;W,Z,x_{0}],\\;\\ddot{\\mu}_{0}^{2}(W,Z)\\;=\\;\\mu_{0}^{2}(W,Z),\\;\\mu_{0}^{1}(X,Z)\\;:=\\;\\mathbb{E}_{P}[\\breve{\\mu}_{0}^{2}(W,Z)\\;\\mid\\;\\dot{X},Z],$ $\\check{\\mu}_{0}^{\\check{1}}(Z)=\\mu_{0}^{1}(x_{1},\\check{Z})$ , and $\\psi_{0}=\\mathbb{E}_{P}[\\check{\\mu}_{0}^{1}(Z)\\mid x_{2}]$ . These nuisances can be estimated efficiently with regression models run in polynomial time relative to the number of variables and samples (e.g., neural networks (LeCun et al., 2015) or XGBoost (Chen and Guestrin, 2016)). ", "page_idx": 5}, {"type": "text", "text": "Beyond the SBD framework, the regression nuisances are capable of representing functionals in the presence of variables being summed and fixed simultaneously (e.g., FD in Eq. (2) or Verma in Eq. (3)). As an example, consider FD in Eq. (2) with its UCA representation in Example 1. First, define $\\mu_{0}^{2}(Z,X,\\bar{C})\\,:=\\,\\mathbb{E}_{P}[Y\\mid Z,X,\\bar{C}]$ with $\\mathbf{S}_{2}\\,=\\,\\{Z,X,\\bar{C}\\}$ . Next, we have $\\mathbf{B}_{1}\\,=\\,\\mathbf{S}_{1}^{b}\\cap\\underline{{\\mathbf{C}}}_{1}\\,=\\,\\{X\\}$ and, $\\breve{\\bf S}_{2}\\,=\\,\\{Z,X^{\\prime},C\\}$ , where $X^{\\prime}$ is an independent copy of $X$ . Consequently, $\\check{\\mu}_{0}^{2}(Z,X^{\\prime},\\stackrel{\\cdot}{C}):=\\mu_{0}^{2}(Z,X^{\\prime},\\stackrel{\\cdot}{C})$ , where $(Z,X^{\\prime},C)$ is plugged into a function $\\mu_{0}^{2}$ . Next, define $\\mu_{0}^{1}(C,X^{\\prime}):=\\mathbb{E}_{P}[\\check{\\mu}_{0}^{2}(Z,X^{\\prime},C)\\mid x,C,X^{\\prime}].$ Finally, we have $\\tilde{\\mu}_{0}^{1}(C,X)=\\mu_{0}^{1}(C;\\overset{\\vee}{X})$ . The expectation, $\\begin{array}{r}{\\mathbb{E}_{P}[\\check{\\mu}_{0}^{1}(C,X)]=\\sum_{c,x^{\\prime}}P(c,x^{\\prime})\\check{\\mu}_{0}^{1}(c,x^{\\prime})}\\end{array}$ , correctly specifies FD in Eq. (2) as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{{}}&{{\\displaystyle\\sum_{c,x^{\\prime}}P(c,x^{\\prime})\\check{\\mu}_{0}^{1}(c,x^{\\prime})=\\displaystyle\\sum_{c,x^{\\prime}}P(c,x^{\\prime})\\mu_{0}^{1}(c,x^{\\prime})=\\displaystyle\\sum_{c,x^{\\prime}}P(c,x^{\\prime}){\\mathbb E}_{P}[\\check{\\mu}_{0}^{2}(Z,x^{\\prime},c)\\mid x,c,x^{\\prime}]}}\\\\ {{}}&{{\\displaystyle=^{*}\\displaystyle\\sum_{c,x^{\\prime},z}P(c,x^{\\prime})P(z\\mid x,c)\\mu_{0}^{2}(z,x^{\\prime},c)=\\displaystyle\\sum_{c,x^{\\prime},z}P(c,x^{\\prime})P(z\\mid x,c){\\mathbb E}_{P}[Y\\mid z,x^{\\prime},c],}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the equation $\\circeq^{*}$ holds since $X^{\\prime}$ is an independent copy of $X$ , so it\u2019s independent of $Z$ . ", "page_idx": 5}, {"type": "text", "text": "Empirically, generating $\\mathbf{B}_{i}^{\\prime}$ involves permuting copied samples of $\\mathbf{B}_{i}$ , an used in recent works in (Chernozhukov et al., 2022; Xu and Gretton, 2022). We name this approach empirical bifurcation: ", "page_idx": 5}, {"type": "text", "text": "1 (Sample splitting) For each $i\\in[m+1]$ , randomly split $\\mathcal{D}^{i}\\sim P^{i}$ into $L$ -folds. Let $\\mathcal{D}_{\\ell}^{i}$ denote the $\\ell$ -th partition, and define $\\mathcal{D}_{-\\ell}^{i}:=\\mathcal{D}^{i}\\setminus\\mathcal{D}_{\\ell}^{i}$ . We use $\\mathbf{W}(\\mathcal{D}_{\\ell}^{i})$ to refer to the samples of $\\forall\\mathbf{W}$ in $\\mathcal{D}_{\\ell}^{i}$ . ", "page_idx": 6}, {"type": "text", "text": "5 end   \n6 Return the DML-UCA estimator $\\hat{\\psi}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{\\psi}:=\\frac{1}{L}\\sum_{\\ell=1}^{L}\\sum_{i=1}^{m}\\mathbb{E}_{\\mathcal{D}_{\\ell}^{i+1}}[\\hat{\\pi}_{\\ell}^{i}(\\check{\\mu}_{\\ell}^{i+1}-\\hat{\\mu}_{\\ell}^{i})]+\\mathbb{E}_{\\mathcal{D}_{\\ell}^{1}}[\\check{\\mu}_{\\ell}^{1}].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Definition 2 (Empirical bifurcation). An empirical bifurcation for B following a distribution $P$ is the procedure of copying samples of $\\mathbf{B}\\sim P$ and randomly permuting to obtain new samples $\\mathbf{B^{\\prime}}$ . ", "page_idx": 6}, {"type": "text", "text": "In general, the regression nuisances can be estimated from data by employing empirical bifurcation and off-the-shelf regression models. ", "page_idx": 6}, {"type": "text", "text": "Next, we define the ratio nuisance parameters $\\pi$ . Define $\\pi_{0}^{m}(\\mathbf{S}_{m})$ as the solution functional satisfying $\\mathbb{E}_{P^{m+1}}[\\mu_{0}^{m}(\\mathbf{S}_{m})\\pi_{0}^{m}]=\\psi_{0}$ . Recursively, for $i=m-1,\\cdots\\,,1$ , define $\\pi_{0}^{i}(\\mathbf{S}_{i},\\mathbf{B}_{i}^{\\prime})$ as a functional satisfying the following equation, for any $\\mu^{i+1}\\in L_{2}(P^{i+2})$ . ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P^{i+2}}[\\pi_{0}^{i+1}(\\mathbf{S}_{i+1},\\mathbf{B}_{i+1}^{\\prime})\\mu^{i+1}(\\mathbf{S}_{i+1},\\mathbf{B}_{i+1}^{\\prime})]=\\mathbb{E}_{P^{i+1}}[\\pi_{0}^{i}(\\mathbf{S}_{i},\\mathbf{B}_{i}^{\\prime})\\mathbb{E}_{P^{i+1}}[\\tilde{\\mu}^{i+1}(\\tilde{\\mathbf{S}}_{i+1})\\mid\\mathbf{S}_{i},\\mathbf{B}_{i}^{\\prime}]],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the closed form solution is given as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\pi_{0}^{i}=\\frac{\\prod_{j=1}^{i}P^{j}(\\mathbf{C}_{j}\\mid\\mathbf{S}_{j})\\sigma_{\\mathbf{R}_{j}}^{j}(\\mathbf{R}_{j}\\mid\\mathbf{S}_{j}\\mid\\mathbf{R}_{j})}{P^{i+1}(\\mathbf{S}_{i},\\mathbf{B}_{i}^{\\prime})}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "For the example of FD, $\\begin{array}{r}{\\pi_{0}^{2}=\\frac{P(Z|x,C)}{P(Z|X,C)}}\\end{array}$ and $\\begin{array}{r}{\\pi_{0}^{1}=\\frac{P(x)}{P(x|C)}}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "Equipped with the ratio nuisances, UCA can be computed as follows: ", "page_idx": 6}, {"type": "text", "text": "Proposition 3. UCA in Eq. (1) can be parameterized as $\\psi_{0}=\\mathbb{E}_{P^{m+1}}[\\pi_{0}^{m}Y]$ . ", "page_idx": 6}, {"type": "text", "text": "Estimating the ratio nuisances may be challenging due to the distribution ratio of continuous/highdimensional variables. To address the challenge, we use Bayes\u2019 rule to transform the distribution ratio into a more tractable form. For example, in FD, if the treatment $X$ is a singleton binary, instead of estimating $\\begin{array}{r}{\\pi_{0}^{2}=\\frac{P(Z|x,C)}{P(Z|X,C)}}\\end{array}$ , an equivalent estimand $\\begin{array}{r}{\\pi_{0}^{2}=\\frac{P(x|Z,C)P(X|C)}{P(X|Z,C)P(x|C)}}\\end{array}$ can be estimated. This approach allows to use off-the-shelf probabilistic classification methods for estimating distribution ratios, allowing scalable computation. A detailed procedure for ratio estimation is in Appendix C.2. ", "page_idx": 6}, {"type": "text", "text": "Combining regression and ratio-nuisances, we present a double/debiased machine learning (DML) (Chernozhukov et al., 2018)-based estimator $\\hat{\\psi}$ for the UCA, titled \u2018DML-UCA\u2019, in Algo. 2. We provide detailed nuisance specification for various examples in Appendix A and B. ", "page_idx": 6}, {"type": "text", "text": "DML-UCA provides a scalable estimator for functionals expressible through UCA. When the target query is BD/SBD, DML-UCA aligns with existing doubly robust SBD estimators (Bang and Robins, 2005; Robins et al., 2009; van der Laan and Gruber, 2012; Rotnitzky et al., 2017; Luedtke et al., 2017; D\u00edaz et al., 2023). Beyond SBD, DML-UCA can be estimated in polynomial time relative to the number of variables and samples, ensuring its scalability: ", "page_idx": 6}, {"type": "text", "text": "Theorem 2 (Scalability). Algo. 2 runs in $O(K n_{\\operatorname*{max}}+T(m,n_{\\operatorname*{max}},K))$ , where $K$ is the number of distinct in P, $n_{\\operatorname*{max}}:=\\operatorname*{max}\\{|{\\mathcal{D}}^{k}|:k\\,\\in\\,[K]\\}$ , and $T(m,n_{\\mathrm{max}},K)$ is the time complexity for learning nuisances $\\hat{\\mu}_{\\ell}^{i}$ and $\\hat{\\pi}_{\\boldsymbol{\\ell}}^{i}$ . Specifically, $O(T(m,n_{\\mathrm{max}},K))=O(K\\times L\\times(T_{\\mu}+T_{\\pi}))$ , where $T_{\\mu}:=\\operatorname*{max}\\{T_{\\hat{\\mu}_{\\ell}^{i}}:i\\in[m],\\ell\\in[L]\\}$ , $T_{\\pi}:=\\operatorname*{max}\\{T_{\\hat{\\pi}_{\\ell}^{i}}:i\\in[m],\\ell\\in[L]\\}$ , and $T_{\\hat{\\mu}_{\\ell}^{i}}$ and $T_{\\hat{\\pi}_{\\ell}^{i}}$ denote the time complexity for learning and evaluating $\\hat{\\pi}_{\\boldsymbol{\\ell}}^{i}$ and $\\hat{\\mu}_{\\ell}^{i}$ , respectively. ", "page_idx": 6}, {"type": "table", "img_path": "aX9z2eT6ul/tmp/266dca1721037e2ee3d3cc14427ee1ca2853492a1fa0365561bae2967687dd15.jpg", "table_caption": [], "table_footnote": ["Table 2: Comparison of time complexities of existing estimators for estimands: $\\overline{{n_{\\mathrm{max}}:=\\operatorname*{max}\\{|\\mathcal{D}^{i}|\\}}}$ is the number of samples, $m$ is the number of variables, and $T(m,n_{\\mathrm{max}},K)$ (or $T(m,n)\\;:=\\;$ $T(m,n_{\\mathrm{max}}\\,=\\,n,K\\,=\\,1)$ ) is the time complexity for learning nuisance parameters for the target functional. The plug-in estimator for BD is one where $\\mathbb{E}_{P}[Y\\mid\\mathbf{x},\\mathbf{z}]$ and $P(\\mathbf{z})$ are estimated from data, and $\\sum_{\\mathbf{z}}\\mathbb{E}_{P}[Y\\mid\\bar{\\mathbf{x}},\\mathbf{z}]P(\\mathbf{z})$ is evaluated. Details are in Sec. C.4. "], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "An an example, for XGBoost (Chen and Guestrin, 2016), $T_{\\pi}\\,=\\,T_{\\mu}\\,=\\,{\\cal O}(\\mathrm{num}_{\\mathrm{tree}}\\,\\times\\,\\mathrm{depth}_{\\mathrm{tree}}\\,\\times$ $n_{\\mathrm{max}}\\log n_{\\mathrm{max}})$ , where $\\bf\\Pi^{\\mathrm{num}_{\\mathrm{tree}}}$ and $\\mathrm{\\depth_{\\mathrm{{tree}}}}$ are the number and depth of trees in XGBoost. ", "page_idx": 7}, {"type": "text", "text": "Table 2 summarizes the comparison of time complexities for existing estimators. As shown in the table, scalable estimators with polynomial time complexity have only been developed for BD/SBD estimands. Existing estimators beyond SBD often lack scalability. For instance, existing estimators for FD (Fulcher et al., 2019; Guo et al., 2023) or Tian\u2019s adjustment (Bhattacharya et al., 2022) face exponential time complexity in the dimension of mediators. In contrast, DML-UCA\u2019s polynomial time complexity positions it as a uniquely scalable solution within the UCA functional class, which includes FD and Tian\u2019s adjustment as special cases. For general obsID/gID estimands beyond the UCA class, scalable estimators have yet to be developed. ", "page_idx": 7}, {"type": "text", "text": "3.1 Error analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we show that DML-UCA exhibits doubly robustness, in addition to scalability. Since UCA is composed of multiple (possibly distinct) distributions, we provide a tool to distinguish them. ", "page_idx": 7}, {"type": "text", "text": "Definition 3 (Index set). The index sets $\\mathcal{T}_{1},\\cdot\\cdot\\cdot,\\mathcal{T}_{K}$ partition $\\{1,\\cdot\\cdot\\cdot,m+1\\}$ such that indices $i$ and $j$ are in the same set $\\mathcal{T}_{k}$ if and only if $P^{i}(\\mathbf{V})=P^{j}(\\mathbf{V})$ . ", "page_idx": 7}, {"type": "text", "text": "We will use $\\mathsf{P}^{k}$ for $k\\,=\\,1,\\cdot\\cdot\\cdot\\,,K$ to denote the distribution $P^{i}$ for $i\\in\\mathcal{T}_{k}$ . Then, the functional $\\Psi[\\mathbf{P};\\pmb{\\sigma}]$ in Eq. (1) can be written as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\Psi[{\\bf P};\\pmb{\\sigma}]=\\Psi[\\{{\\sf P}^{k}:k=1,\\cdots,K\\};\\pmb{\\sigma}].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Since multiple distributions are involved in UCA, deriving an influence function for each distribution $\\mathsf{P}^{k}$ becomes necessary. A standard influence function is typically defined for a single distribution $P$ , and thus, does not suffice for studying multi-distribution setting. To address the issue, we employ a partial influence function (PIF) (Pires and Branco, 2002), an influence function defined relative to each $\\mathsf{P}^{k}$ . A formal definition is in Appendix C. For UCA, PIFs are given as follows: ", "page_idx": 7}, {"type": "text", "text": "Theorem 3 (PIF for UCA). Assume that $\\mu_{0}^{i}<\\infty$ and $0<\\pi_{0}^{i}<\\infty$ almost surely for $i=1,\\cdot\\cdot\\cdot,m$ . Define $\\eta_{0}^{1}:=\\{\\mu_{0}^{1}\\}$ and $\\eta_{0}^{i}:=\\{\\pi_{0}^{i-1},\\mu_{0}^{i},{\\dot{\\mu}_{0}}^{i-1}\\}$ for $i=1,\\cdot\\cdot\\cdot,m+1,$ , and ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\varphi^{i}(\\check{\\mathbf{S}}_{i};\\eta_{0}^{i},\\psi_{0}):=\\left\\{\\!\\!\\!\\begin{array}{l l}{\\pi_{0}^{i-1}\\{\\check{\\mu}_{0}^{i}-\\mu_{0}^{i-1}\\}}&{i f i>1}\\\\ {\\check{\\mu}_{0}^{1}-\\psi_{0}}&{i f i=1.}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Let $\\mathbf{V}^{k}:=\\cup_{i\\in\\mathcal{Z}_{k}}\\breve{\\mathbf{S}}^{i}$ and $\\eta_{0}^{k}:=\\cup_{i\\in\\mathbb{Z}_{k}}\\eta_{0}^{i}$ . Then, the $k$ -th PIF for UCA is $\\phi_{0}^{k}:=\\phi^{k}(\\mathbf{V}^{k};\\eta_{0}^{k},\\psi_{0}):=$ $\\begin{array}{r}{\\sum_{i\\in\\mathcal{Z}_{k}}\\varphi^{i}(\\mathbf{S}^{i};\\eta_{0}^{i},\\psi_{0})}\\end{array}$ . ", "page_idx": 8}, {"type": "text", "text": "Equipped with PIFs, we provide a finite-sample guarantee for DML-UCA, extending Chernozhukov et al. (2023) which analyzed DML estimators for BDs. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4 (Finite sample guarantee). Suppose $\\mu_{0}^{i},\\hat{\\mu}_{\\ell}^{i}<\\infty$ and $0<\\pi_{0}^{i},\\hat{\\pi}_{\\ell}^{i}<\\infty$ almost surely for $i=1,\\cdot\\cdot\\cdot,m$ . Suppose the third moment of $\\phi_{0}^{k}$ for $k=1,\\cdots\\,,K$ exist. Let $\\phi_{0}^{k}:=\\phi^{k}(\\mathbf{V}^{k};\\eta_{0}^{k},\\psi_{0})$ and $\\hat{\\phi}_{\\ell}^{k}:=\\phi^{k}({\\mathbf{V}}^{k};\\hat{\\eta}_{\\ell}^{k},\\psi_{0})$ . Let $\\begin{array}{r}{R_{1}^{k}:=(1/L)\\sum_{\\ell=1}^{L}(\\mathbb{E}_{\\mathcal{D}_{\\ell}^{k}}[\\hat{\\phi}_{\\ell}^{k}]-\\mathbb{E}_{\\mathbb{P}^{k}}[\\hat{\\phi}_{\\ell}^{k}])}\\end{array}$ . Then, ", "page_idx": 8}, {"type": "text", "text": "1. The error $\\hat{\\psi}-\\psi_{0}$ is decomposed as follows: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\hat{\\psi}-\\psi_{0}=\\sum_{k=1}^{K}R_{1}^{k}+\\frac{1}{L}\\sum_{\\ell=1}^{L}\\sum_{i=1}^{m}\\mathbb{E}_{P^{i+1}}[(\\hat{\\pi}_{\\ell}^{i}-\\pi_{0}^{i})(\\mu_{0}^{i}-\\hat{\\mu}_{\\ell}^{i})].\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "2. Let $\\rho_{k,0}^{2}:=\\mathbb{V}_{\\mathbb{P}^{k}}[\\phi_{0}^{k}]$ . With probability $(W.P)$ greater than $1-\\epsilon,$ ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}R_{1}^{k}\\leq K\\sqrt{\\frac{2}{\\epsilon}}\\left(\\sqrt{\\sum_{k=1}^{K}\\frac{\\rho_{k,0}^{2}}{|\\mathcal{D}^{k}|}}+\\sqrt{\\sum_{\\ell=1}^{L}\\sum_{k=1}^{K}\\frac{\\|\\hat{\\phi}_{\\ell}^{k}-\\phi_{0}^{k}\\|_{\\mathsf{p}^{k}}^{2}}{|\\mathcal{D}_{\\ell}^{k}|}}\\right).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "3. Let $\\kappa_{k,0}^{3}:=\\mathbb{E}_{\\mathtt{P}^{k}}[|\\phi_{0}^{k}|^{3}]$ . Let NORMA $L(x)$ denote the standard normal CDF. W.P greater than $1-\\epsilon$ ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\bigg|\\mathbb{P}^{k}\\left(\\frac{\\sqrt{|\\mathcal{D}^{k}|}}{\\rho_{k,0}}R_{1}^{k}<x\\right)-{\\cal N}O R\\mathbb{M}L(x)\\bigg|\\leq\\frac{1}{\\sqrt{2\\pi}}\\sqrt{\\frac{1}{\\epsilon}\\sum_{\\ell=1}^{L}\\frac{\\|\\hat{\\phi}_{\\ell}^{k}-\\phi_{0}^{k}\\|_{\\mathbb{P}^{k}}^{2}}{|\\mathcal{D}_{\\ell}^{k}|}}+\\frac{0.4748\\kappa_{k,0}^{3}}{\\rho_{k,0}^{3}\\sqrt{|\\mathcal{D}^{k}|}},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "This is a novel finite sample guarantee of DML-based estimators for functionals beyond SBD. Finite sample analyses for functionals beyond SBD have been studied only for the non-doubly robust estimators (Bhattacharyya et al., 2022). For doubly robust estimators, only asymptotic analyses were provided for FD (Fulcher et al., 2019; Guo et al., 2023), Tian\u2019s adjustment (Bhattacharya et al., 2022), and obsID (Jung et al., 2021b). Thm. 4 elucidates that the error can be decomposed into two terms $R_{1}^{k}$ and $R_{2}^{\\ell}$ . The term $R_{1}^{k}$ closely approximates a standard normal distribution variable, and $R_{2}^{\\ell}$ , comprises the error of $(\\hat{\\pi}_{\\boldsymbol{\\ell}}^{i},\\hat{\\pi}_{\\boldsymbol{\\ell}}^{i-1})$ and $\\hat{\\mu}^{i}$ , exhibiting doubly-robustness behavior. Specifically, if the nuisance parameters $\\hat{\\mu}_{\\ell}^{i},\\,\\hat{\\pi}_{\\ell}^{i}$ , and $\\hat{\\pi}_{\\boldsymbol{\\ell}}^{i-1}$ converge at a rate of $n^{-1/4}$ (where $n$ represents the size of the smallest sample set), then DML-UCA converges at a faster rate of $n^{-1/2}$ . This point becomes evident in the corresponding asymptotic analysis: ", "page_idx": 8}, {"type": "text", "text": "Corollary 4 (Asymptotic error). Assume $\\mu_{0}^{i},\\hat{\\mu}_{0}^{i}<\\infty$ and $0<\\pi_{0}^{i}$ , $\\hat{\\pi}_{0}^{i}<\\infty$ almost surely. Suppose the map $\\hat{\\eta}_{\\ell}^{k}\\mapsto\\hat{\\phi}_{\\ell}^{k}$ is uniformly differentiable with respect to $\\hat{\\eta}_{\\ell}^{k}$ , and the derivative of $\\hat{\\phi}_{\\ell}^{k}\\ w.r.t.\\ \\hat{\\eta}_{\\ell}^{k}$ is bounded by some constants. Suppose $\\hat{\\mu}_{\\ell}^{i}$ and $\\hat{\\pi}_{\\boldsymbol{\\ell}}^{i}$ are $L_{2}$ -consistent. Then, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\hat{\\psi}-\\psi_{0}=\\sum_{k=1}^{K}R_{1}^{k}+\\frac{1}{L}\\sum_{\\ell=1}^{L}\\sum_{i=1}^{m}O_{P^{i+1}}\\left(\\|\\hat{\\mu}_{\\ell}^{i}-\\mu_{0}^{i}\\|(\\|\\hat{\\pi}_{\\ell}^{i}-\\pi_{0}^{i}\\|)\\right),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "and $\\sqrt{|\\mathcal{D}^{k}|}R_{1}^{k}$ converges in distribution to normal $(0,\\rho_{k,0}^{2})$ . ", "page_idx": 8}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we demonstrate the scalability and doubly robustness of the DML-UCA estimator, where nuisances are learned through XGBoost (Chen and Guestrin, 2016). We specify an SCM $\\mathcal{M}$ for FD (Fig. 1a), Verma (Fig. 1b), and the example graph in (Jung et al., 2021a) (Fig. 1e), and generate datasets $\\mathbb{D}^{\\check{k}}\\sim\\dot{\\mathbb{P}}^{k}$ from the SCM. The target estimand is denoted as $\\psi_{0}$ . Details are in Appendix F. Further simulations are provided in Appendix E. ", "page_idx": 8}, {"type": "text", "text": "Scalability. To demonstrate scalability of DML-UCA, we compare the running time with existing estimators of (Fulcher et al., 2019) (FD) and (Jung et al., 2021a) (Verma\u2019s equation and the estimand with Fig. 1e \u2014 $\\begin{array}{r}{\\mathbb{E}[Y\\mid\\operatorname{do}(x_{1},x_{2})]=\\sum_{x_{1}^{\\prime},r,z}\\mathbb{E}_{P}[Y\\mid r,x_{1}^{\\prime},x_{2},z]P(r\\mid x_{1},z)P(z,x_{1}^{\\prime})}\\end{array}$ \u2014 which we call \u2018Jung\u2019s equation\u2019). For each example, we increment the dimension of the summed variables, run 100 simulations, take the average of running times, and compare this average. We label this plot as \u2018run-time-plot\u2019, presented in the top side of Fig. 2. In the comparison with (Fulcher et al., 2019) for FD in Fig. 2a, we fix $|C|=2$ and increment $|Z|=\\{2,4,6,8,12,20,30,50,100\\}$ . When comparing with (Jung et al., 2021a), for Verma\u2019s equations in Figs. (2b), we fix $|A|\\,=\\,2$ and increment $|B|\\,=\\,\\{2,4,6,8,12,20,30,50,100\\}$ . For Jung\u2019s equation in Fig. 2c, we fix $|Z|\\,=\\,2$ and $|R|=\\{2,4,6,8,12,20,30,100\\}$ . The timeout for the run-time is set to 300 seconds. For all scenarios, the run-time of existing estimators increases rapidly over dimensions due to the summation operation while DML-UCA scales well for high-dimensional covariates. ", "page_idx": 8}, {"type": "image", "img_path": "aX9z2eT6ul/tmp/b7c36704790f91e612ea8ea4dddfb71b70bd8027fe0c08d150c7b397b369839c.jpg", "img_caption": ["Figure 2: Comparison of DML-UCA (\u2018DML\u2019) with existing estimators using $(\\mathbf{Top})$ running-timeplots ( $x$ -axis: the dimension of summed variables, $y$ -axis: running time); and (Bottom) AAE-plots $x$ -axis: the sample size, $y$ -axis: errors). DML-UCA is compared with (a,e) Fulcher et al. (2019) for FD; (b.f) (Jung et al., 2021a) for Verma\u2019s equation; and (c,f) Jung et al. (2021a) for Jung\u2019s equation. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Doubly robustness. To demonstrate doubly robustness, we compare the error of DML-UCA with existing estimators for FD of Fulcher et al. (2019) and for Verma\u2019s and Jung\u2019s equations of Jung et al. (2021a) We use $\\hat{\\psi}^{\\mathrm{{est}}}$ for est $\\in\\ \\{{\\mathrm{DML}},{\\mathrm{Fulcher}},{\\mathrm{Jung}}\\}$ to denote each estimator. We use the average absolute error (AAE), which is an average of the error of the estimated versus true causal effect of ${\\bf X}={\\bf x}$ : $\\begin{array}{r}{\\frac{1}{|\\operatorname{domain}(\\mathbf{X})|}\\sum_{\\mathbf{x}\\in\\operatorname{domain}(\\mathbf{X})}|\\hat{\\psi}^{\\mathrm{est}}(\\mathbf{x})-\\psi_{0}(\\mathbf{x})|}\\end{array}$ . To witness the fast convergence of DML-UCA, we enforce the convergence rate of nuisance estimates to be no faster than the decaying rate $n^{-1/4}$ by adding the noise term $\\epsilon\\sim\\mathrm{normal}(n^{-1/4},n^{-1/4})$ to nuisances, inspired by the experimental design in (Kennedy, 2023). We ran 100 simulations for each number of samples $n=\\{2500,5000,10000,20000\\}$ . We label the plot as \u2018AAE-plot\u2019, presented in the bottom side of Fig. 2. For each example, DML-UCA outperforms other estimators, exhibiting fast convergence. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduce a framework that encompasses a broad class of sum-product causal estimands, called UCA class, for which scalable estimators were previously unavailable. We demonstrate the expressiveness of the UCA class, which includes not only BD/SBD but also broader classes such as Tian\u2019s adjustment incorporating FD and Verma, and Ctf-DE, for which the existing SBD-based framework is not applicable. We develop an estimator for UCA called DML-UCA that can estimate the target estimand in polynomial time relative to the number of samples and variables, ensuring scalability. We provide finite-sample guarantees and corresponding asymptotic error analysis for DML-UCA, demonstrating its fast convergence. These scalability and fast convergence properties are empirically verified through simulations. Our results pave the way toward developing an estimation framework maximizing both coverage and scalability in Table 1. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank anonymous reviewers for constructive comments to improve the manuscript. This research is supported in part by the NSF, ONR, AFOSR, DoE, Amazon, JP Morgan, and The Alfred P. Sloan Foundation. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Bang, H. and Robins, J. M. (2005). Doubly robust estimation in missing data and causal inference models. Biometrics, 61(4):962\u2013973.   \nBareinboim, E., Correa, J. D., Ibeling, D., and Icard, T. (2020). On pearl\u2019s hierarchy and the foundations of causal inference. In Probabilistic and Causal Inference: The Works of Judea Pearl, Technical Report R-60. Causal Artificial Intelligence Laboratory, Columbia University.   \nBareinboim, E. and Pearl, J. (2012a). Causal inference by surrogate experiments: z-identifiability. In In Proceedings of the 28th Conference on Uncertainty in Artificial Intelligence, pages 113\u2013120. AUAI Press.   \nBareinboim, E. and Pearl, J. (2012b). Transportability of causal effects: Completeness results. In Proceedings of the 26th AAAI Conference on Artificial Intelligence, pages 698\u2013704.   \nBareinboim, E. and Pearl, J. (2016). Causal inference and the data-fusion problem. Proceedings of the National Academy of Sciences, 113(27):7345\u20137352.   \nBareinboim, E., Tian, J., and Pearl, J. (2014). Recovering from selection bias in causal and statistical inference. In Proceedings of the 28th AAAI Conference on Artificial Intelligence, pages 2410\u20132416.   \nBerry, A. C. (1941). The accuracy of the gaussian approximation to the sum of independent variates. Transactions of the american mathematical society, 49(1):122\u2013136.   \nBhattacharya, R., Nabi, R., and Shpitser, I. (2022). Semiparametric inference for causal effects in graphical models with hidden variables. Journal of Machine Learning Research, 23:1\u201376.   \nBhattacharyya, A., Gayen, S., Kandasamy, S., Raval, V., and Variyam, V. N. (2022). Efficient interventional distribution learning in the pac framework. In International Conference on Artificial Intelligence and Statistics, pages 7531\u20137549. PMLR.   \nBickel, P. J., Klaassen, C. A., Bickel, P. J., Ritov, Y., Klaassen, J., Wellner, J. A., and Ritov, Y. (1993). Efficient and adaptive estimation for semiparametric models, volume 4. Johns Hopkins University Press Baltimore.   \nChen, T. and Guestrin, C. (2016). Xgboost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 785\u2013794.   \nChernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., and Robins, J. (2018). Double/debiased machine learning for treatment and structural parameters: Double/debiased machine learning. The Econometrics Journal, 21(1).   \nChernozhukov, V., Newey, W., Singh, R., and Syrgkanis, V. (2022). Automatic debiased machine learning for dynamic treatment effects and general nested functionals. arXiv preprint arXiv:2203.13887.   \nChernozhukov, V., Newey, W. K., and Singh, R. (2023). A simple and general debiased machine learning theorem with finite-sample guarantees. Biometrika, 110(1):257\u2013264.   \nCorrea, J., Lee, S., and Bareinboim, E. (2021). Nested counterfactual identification from arbitrary surrogate experiments. Advances in Neural Information Processing Systems, 34.   \nCorrea, J. D., Tian, J., and Bareinboim, E. (2018). Generalized adjustment under confounding and selection biases. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence.   \nD\u00edaz, I., Williams, N., Hoffman, K. L., and Schenck, E. J. (2023). Nonparametric causal effects based on longitudinal modified treatment policies. Journal of the American Statistical Association, 118(542):846\u2013857.   \nEsseen, C.-G. (1942). On the liapunov limit error in the theory of probability. Ark. Mat. Astr. Fys., 28:1\u201319.   \nFulcher, I. R., Shpitser, I., Marealle, S., and Tchetgen Tchetgen, E. J. (2019). Robust inference on population indirect causal effects: the generalized front door criterion. Journal of the Royal Statistical Society: Series B (Statistical Methodology).   \nGuo, A., Benkeser, D., and Nabi, R. (2023). Targeted machine learning for average causal effect estimation using the front-door functional. arXiv preprint arXiv:2312.10234.   \nHeckman, J. J. (1992). Randomization and Social Policy Evaluation. In Manski, C. and Garfinkle, I., editors, Evaluations: Welfare and Training Programs, pages 201\u2013230. Harvard University Press, Cambridge, MA.   \nHuang, Y. and Valtorta, M. (2006). Pearl\u2019s calculus of intervention is complete. In Proceedings of the 22nd Conference on Uncertainty in Artificial Intelligence, pages 217\u2013224. AUAI Press.   \nJung, Y., D\u00edaz, I., Tian, J., and Bareinboim, E. (2023a). Estimating causal effects identifiable from combination of observations and experiments. In Proceedings of the 37th Neural Information Processing Systems.   \nJung, Y., Tian, J., and Bareinboim, E. (2021a). Estimating identifiable causal effects on markov equivalence class through double machine learning. In Proceedings of the 38th International Conference on Machine Learning.   \nJung, Y., Tian, J., and Bareinboim, E. (2021b). Estimating identifiable causal effects through double machine learning. In Proceedings of the 35th AAAI Conference on Artificial Intelligence.   \nJung, Y., Tian, J., and Bareinboim, E. (2023b). Estimating joint treatment effects by combining multiple experiments. In Proceedings of the 40th International Conference on Machine Learning.   \nKennedy, E. H. (2023). Towards optimal doubly robust estimation of heterogeneous causal effects. Electronic Journal of Statistics, 17(2):3008\u20133049.   \nKennedy, E. H., Balakrishnan, S., G\u2019Sell, M., et al. (2020). Sharp instruments for classifying compliers and generalizing causal effects. Annals of Statistics, 48(4):2008\u20132030.   \nLeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. nature, 521(7553):436\u2013444.   \nLee, S. and Bareinboim, E. (2020). Causal effect identifiability under partial-observability. In Proceedings of the 37th International Conference on Machine Learning.   \nLee, S., Correa, J., and Bareinboim, E. (2020). General transportability\u2013synthesizing observations and experiments from heterogeneous domains. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 10210\u201310217.   \nLee, S., Correa, J. D., and Bareinboim, E. (2019). General identifiability with arbitrary surrogate experiments. In Proceedings of the 35th Conference on Uncertainty in Artificial Intelligence. AUAI Press.   \nLi, S. and Luedtke, A. (2023). Efficient estimation under data fusion. Biometrika, 110(4):1041\u20131054.   \nLuedtke, A. R., Sofrygin, O., van der Laan, M. J., and Carone, M. (2017). Sequential double robustness in right-censored longitudinal models. arXiv preprint arXiv:1705.02459.   \nMises, R. v. (1947). On the asymptotic distribution of differentiable statistical functions. The annals of mathematical statistics, 18(3):309\u2013348.   \nMurphy, S. A. (2003). Optimal dynamic treatment regimes. Journal of the Royal Statistical Society Series B: Statistical Methodology, 65(2):331\u2013355. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Pearl, J. (1995). Causal diagrams for empirical research. Biometrika, 82(4):669\u2013710. ", "page_idx": 12}, {"type": "text", "text": "Pearl, J. (2000). Causality: Models, Reasoning, and Inference. Cambridge University Press, New York. 2nd edition, 2009.   \nPearl, J. and Mackenzie, D. (2018). The book of why: the new science of cause and effect. Basic Books.   \nPearl, J. and Robins, J. (1995a). Probabilistic evaluation of sequential plans from causal models with hidden variables. In Proceedings of the 11th Conference on Uncertainty in Artificial Intelligence, pages 444\u2013453. Morgan Kaufmann Publishers Inc.   \nPearl, J. and Robins, J. (1995b). Probabilistic evaluation of sequential plans from causal models with hidden variables. In Proceedings of the Eleventh conference on Uncertainty in artificial intelligence, pages 444\u2013453.   \nPires, A. M. and Branco, J. A. (2002). Partial influence functions. Journal of Multivariate Analysis, 83(2):451\u2013468.   \nPle\u02c7cko, D. and Bareinboim, E. (2024). Causal fairness analysis: A causal toolkit for fair machine learning. Foundations and Trends\u00ae in Machine Learning, 17(3):304\u2013589.   \nQuintas-Martinez, V., Bahadori, M. T., Santiago, E., Mu, J., Janzing, D., and Heckerman, D. (2024). Multiply-robust causal change attribution. arXiv preprint arXiv:2404.08839.   \nRobins, J. (1986). A new approach to causal inference in mortality studies with a sustained exposure period\u2014application to control of the healthy worker survivor effect. Mathematical modelling, 7(9-12):1393\u20131512.   \nRobins, J., Li, L., Tchetgen, E., and van der Vaart, A. W. (2009). Quadratic semiparametric von mises calculus. Metrika, 69:227\u2013247.   \nRobins, J. M. and Rotnitzky, A. (1995). Semiparametric efficiency in multivariate regression models with missing data. Journal of the American Statistical Association, 90(429):122\u2013129.   \nRosenbaum, P. R. and Rubin, D. B. (1983). The central role of the propensity score in observational studies for causal effects. Biometrika, 70(1):41\u201355.   \nRotnitzky, A., Robins, J., and Babino, L. (2017). On the multiply robust estimation of the mean of the g-functional. arXiv preprint arXiv:1705.08582.   \nRotnitzky, A., Robins, J. M., and Scharfstein, D. O. (1998). Semiparametric regression for repeated outcomes with nonignorable nonresponse. Journal of the american statistical association, 93(444):1321\u20131339.   \nShevtsova, I. (2014). On the absolute constants in the berry-esseen-type inequalities. In Doklady Mathematics, volume 89, pages 378\u2013381. Springer.   \nShpitser, I. and Pearl, J. (2006). Identification of joint interventional distributions in recursive semimarkovian causal models. In Proceedings of the 21st AAAI Conference on Artificial Intelligence, page 1219. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.   \nTian, J. and Pearl, J. (2002a). A general identification condition for causal effects. In Proceedings of the 18th National Conference on Artificial Intelligence, pages 567\u2013573.   \nTian, J. and Pearl, J. (2002b). On the testable implications of causal models with hidden variables. In Proceedings of the 18th conference on Uncertainty in artificial intelligence, pages 519\u2013527. Morgan Kaufmann Publishers Inc.   \nTian, J. and Pearl, J. (2003). On the identification of causal effects. Technical Report R-290-L, UCLA.   \nUehara, M., Shi, C., and Kallus, N. (2022). A review of off-policy evaluation in reinforcement learning. arXiv preprint arXiv:2212.06355.   \nvan der Laan, M. J. and Gruber, S. (2012). Targeted minimum loss based estimation of causal effects of multiple time point interventions. The international journal of biostatistics, 8(1).   \nVerma, T. and Pearl, J. (1990). Causal networks: Semantics and expressiveness. In Machine intelligence and pattern recognition, volume 9, pages 69\u201376. Elsevier.   \nXia, K., Lee, K.-Z., Bengio, Y., and Bareinboim, E. (2021). The causal-neural connection: Expressiveness, learnability, and inference. Advances in Neural Information Processing Systems, 34.   \nXia, K. M., Pan, Y., and Bareinboim, E. (2022). Neural causal models for counterfactual identification and estimation. In The Eleventh International Conference on Learning Representations.   \nXu, L. and Gretton, A. (2022). A neural mean embedding approach for back-door and front-door adjustment. In The Eleventh International Conference on Learning Representations. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Supplement to \u201cUnified Covariate Adjustment for Causal Inference\u201d ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1 Introduction ", "page_idx": 14}, {"type": "text", "text": "2 Unified Covariate Adjustment 3 ", "page_idx": 14}, {"type": "text", "text": "3 Scalable Estimator for Unified Covariate Adjustment 6 ", "page_idx": 14}, {"type": "text", "text": "3.1 Error analysis 8 ", "page_idx": 14}, {"type": "text", "text": "4 Experiments 9 ", "page_idx": 14}, {"type": "text", "text": "5 Conclusions 10 ", "page_idx": 14}, {"type": "text", "text": "A Nuisance Specification 17 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Front-door adjustment in Example 1 . . . 17   \nA.2 Verma\u2019s equation in Example 2 . . . 17   \nA.3 Counterfactual directed effect in Example 3 18   \nA.4 Example Estimand for Fig. 1e . . 19 ", "page_idx": 14}, {"type": "text", "text": "B More UCA Examples 20 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Effect of the treatment on the treated (ETT) 20   \nB.2 Transportability ( $S$ -admissibility) 20   \nB.3 Off-policy evaluation . . . . 21   \nB.4 Treatment-treatment interactions . 21 ", "page_idx": 14}, {"type": "text", "text": "C More Results 22 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Formal definition of Partial influence function (PIF) . . 22   \nC.2 Density Ratio Estimation . . 22   \nC.3 Analysis of non-UCA functionals . . 23   \nC.3.1 On Case 1 . . . 23   \nC.3.2 On Case 2 . . 24   \nC.4 Time Complexity . . . . 24   \nC.4.1 BD/SBD 25   \nC.4.2 Front-door adjustment (FD) 26   \nC.4.3 Tian\u2019s adjustment . . . 26   \nC.4.4 DML-UCA (BD, FD, and Tian\u2019s). . 27   \nC.4.5 DML-UCA (general). . . 27   \nC.4.6 DML-ID (obsID). . . . 27   \nC.4.7 DML-gID (gID). . . . 27   \nD Proofs 27   \nD.1 Proof of Proposition 1 27   \nD.2 Proof for Proposition 2 28   \nD.3 Proof for Proposition 3 28   \nD.4 Proof for Theorem 1 28   \nD.5 Proof for Theorem 2 28   \nD.6 Proof for Theorem 3 29   \nD.7 Proof for Theorem 4 29   \nD.7.1 Helper lemmas 30   \nD.7.2 Preliminary Results . 33   \nD.7.3 Proof of Theorem 4 - (1) . 33   \nD.7.4 Proof of Theorem 4 - (2) 33   \nD.7.5 Proof of Theorem 4 - (3) 34   \nD.8 Proof for Corollary 4 . . 35 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "E More Experiments 35 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "F Details in Experiments 36 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "F.1 FD (Fig. 1a) for Simulation in Fig. 2a 36   \nF.2 Verma (Fig. 1b) for Simulation in Fig. 2b . . . 37   \nF.3 Example estimand (Fig. 1e) for Simulation in Fig. 2c . . . 38   \nF.4 ETT in Sec. B for Simulation in Fig. E.4a . . . 40   \nF.5 Transportability in Sec. B for Simulation in Fig. E.4b . . . 41   \nF.6 FD with continuous mediators for Simulation in Fig. E.4c . . . . 42   \nF.7 Verma\u2019s equation with continuous mediators for Simulation in Fig. E.4d . . 43   \nF.8 Ctf-DE in Example 3 for Simulation in Fig. E.4e . . . 44 ", "page_idx": 15}, {"type": "text", "text": "A Nuisance Specification ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A.1 Front-door adjustment in Example 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We first note that the front-door adjustment is an expectation of the following product: ", "page_idx": 16}, {"type": "equation", "text": "$$\nP(C,X)P(Z\\mid x,C)P(Y\\mid C,X,Z),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which implies that ", "page_idx": 16}, {"type": "text", "text": "\u2022 ${\\bf C}_{1}=\\{C,X\\};{\\bf C}_{2}=\\{Z\\};{\\bf C}_{3}=\\{Y\\}$ (i.e., $m=2$ ). \u2022 ${\\bf S}_{1}=\\{C\\}$ and $\\mathbf{S}_{2}=\\{C,X,Z\\}$   \n$\\bullet\\,\\,\\mathbf{S}_{1}^{b}=\\{X\\}$   \n\u2022 $\\mathbf{B}_{1}=\\mathbf{S}_{2}\\cap\\mathbf{C}_{1}\\cap\\mathbf{S}_{1}^{b}=\\{X\\}.$   \n\u2022 $\\check{\\bf S}_{2}=\\{C,X^{\\prime},Z\\}$ and $\\check{\\bf S}_{1}=\\{C,X\\}$ .   \n\u2022 $P^{2}=P(\\cdot\\mid x)$ ", "page_idx": 16}, {"type": "text", "text": "The regression nuisances are the followings: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad\\mu_{0}^{2}(\\mathbf{S}_{2}):=\\mu_{0}^{2}(Z,X,C):=\\mathbb{E}_{P}[Y\\mid Z,X,C]}\\\\ &{\\quad\\quad\\tilde{\\mu}_{0}^{2}(\\check{\\mathbf{S}}_{2}):=\\mu_{0}^{2}(Z,X^{\\prime},C)}\\\\ &{\\quad\\quad\\mu_{0}^{1}(\\mathbf{S}_{1},\\mathbf{B}_{1}^{\\prime}):=\\mu_{0}^{1}(X^{\\prime},C):=\\mathbb{E}_{P}[\\mu_{0}^{2}(Z,X^{\\prime},C)\\mid x,C,X^{\\prime}]}\\\\ &{\\quad\\quad\\tilde{\\mu}_{0}^{1}(\\check{\\mathbf{S}}_{1})=\\mu_{0}^{1}(X,C).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The ratio nuisances are the followings: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\pi_{0}^{2}(Z,X,C)=\\displaystyle\\frac{P(Z\\mid x,C)}{P(Z\\mid X,C)},}}\\\\ {{\\pi_{0}^{1}(C)=\\displaystyle\\frac{P(x)}{P(x\\mid C)}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The representation for DML-UCA is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P}[\\pi_{0}^{2}(Z,X,C)\\{Y-\\mu_{0}^{2}(Z,X,C)\\}]}\\\\ &{\\mathrm{~}+\\mathbb{E}_{P}[\\pi_{0}^{1}(C)\\{\\mu_{0}^{2}(Z,X^{\\prime},C)-\\mu_{0}^{1}(X^{\\prime},C)\\}\\mid x]}\\\\ &{\\mathrm{~}+\\mathbb{E}_{P}[\\mu_{0}^{1}(X,C)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A.2 Verma\u2019s equation in Example 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We first note that the Verma\u2019s equation in Eq. (3) is an expectation of the following product: ", "page_idx": 16}, {"type": "equation", "text": "$$\nP(X)P(A\\mid x)P(B\\mid A,X)P(Y\\mid B,A,x),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which implies that ", "page_idx": 16}, {"type": "text", "text": "\u2022 ${\\bf C}_{1}=\\{X\\};{\\bf C}_{2}=\\{A\\};{\\bf C}_{3}=\\{B\\}$ ; and ${\\bf C}_{4}=\\{Y\\}$ (i.e., $m=3,$ ).   \n\u2022 $\\mathbf{S}_{1}=\\emptyset$ ; $\\mathbf{S}_{2}=\\{A,X\\}$ and $\\mathbf{S}_{3}=\\{B,A\\}$ .   \n\u2022 $\\mathbf{S}_{1}^{b}=\\left\\{X\\right\\}$ and $\\mathbf{S}_{3}^{b}=\\{X\\}$ .   \n\u2022 $\\mathbf{B}_{1}=\\mathbf{S}_{2}\\cap\\mathbf{C}_{1}\\cap\\mathbf{S}_{1}^{b}=\\{X\\}.$ \u2022 $\\check{\\bf S}_{3}=\\{B,A\\}$ ; $\\check{\\bf S}_{2}=\\{A,X^{\\prime}\\}$ and $\\check{\\bf S}_{1}=\\{X\\}$ .   \n\u2022 $P^{2}=P^{4}=P(\\cdot\\mid x)$ . ", "page_idx": 16}, {"type": "text", "text": "The regression nuisances are the followings: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{0}^{3}(\\mathbf{S}_{3}):=\\mu_{0}^{3}(B,A):=\\mathbb{E}_{P}[Y\\mid B,A,x]}\\\\ &{\\tilde{\\mu}_{0}^{3}(\\check{\\mathbf{S}}_{3}):=\\mu_{0}^{3}(B,A)=\\mathbb{E}_{P}[Y\\mid B,A,x]}\\\\ &{\\mu_{0}^{2}(\\mathbf{S}_{2}):=\\mu_{0}^{2}(A,X):=\\mathbb{E}_{P}[\\mu_{0}^{3}(B,A)\\mid A,X]}\\\\ &{\\tilde{\\mu}_{0}^{2}(\\check{\\mathbf{S}}_{2})=\\mu_{0}^{2}(A,X^{\\prime})}\\\\ &{\\mu_{0}^{1}(\\mathbf{S}_{1},\\mathbf{B}_{1}^{\\prime}):=\\mathbb{E}_{P}[\\mu_{0}^{2}(A,X^{\\prime})\\mid x,X^{\\prime}]}\\\\ &{\\tilde{\\mu}_{0}^{1}(\\check{\\mathbf{S}}_{1}):=\\mu_{0}^{1}(X).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The ratio nuisances are the followings: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi_{0}^{3}(B,A)=\\frac{\\sum_{x^{\\prime}}P(B\\mid A,x^{\\prime})P(x^{\\prime})}{P(B\\mid A,x)},}\\\\ &{\\pi_{0}^{2}(A,X)=\\frac{P(A\\mid x)}{P(A\\mid X)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The representation for DML-UCA is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P}[\\pi_{0}^{3}(B,A,X^{\\prime})\\{Y-\\mu_{0}^{3}(B,A,x)\\}\\mid x]}\\\\ &{\\mathrm{~}+\\mathbb{E}_{P}[\\pi_{0}^{2}(A,X)\\{\\mu_{0}^{3}(B,A,x)-\\mu_{0}^{2}(A,X)\\}]}\\\\ &{\\mathrm{~}+\\mathbb{E}_{P}[\\tilde{\\mu}_{0}^{2}(A,X^{\\prime})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A.3 Counterfactual directed effect in Example 3 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "From the fact that Ctf-DE in Eq. (5) is represented as the expectation of $Y$ over ${\\cal P}(Y\\mid X\\;=\\;$ $x_{0},W,Z)P(W\\mid X,Z)P(Z\\mid X=x_{2})\\mathbb{1}_{x_{1}}(X)$ . Set ", "page_idx": 17}, {"type": "text", "text": "\u2022 ${\\bf C}_{1}=\\{Z\\}$ ; $\\mathbf{C}_{2}=\\{W\\}$ ; and ${\\bf C}_{3}=\\{Y\\}$ $\\begin{array}{r l}&{\\bullet\\mathbf{\\Delta}\\mathbf{S}_{1}=\\{X,Z\\},\\mathbf{S}_{2}=\\{W,Z\\}}\\\\ &{\\bullet\\mathbf{\\Delta}\\mathbf{R}_{1}=\\{X\\}.}\\\\ &{\\bullet\\mathbf{\\Delta}\\mathbf{S}_{0}^{b}=\\{X_{2}\\};\\mathbf{S}_{1}^{b}=\\{X\\};\\mathbf{S}_{2}^{b}=\\{X\\}.}\\end{array}$ \u2022 $\\mathbf{B}_{i}=\\emptyset$ for all $i$ .   \n\u2022 \u02c7Si = Si \\ Ri for all i ", "page_idx": 17}, {"type": "text", "text": "The regression nuisances are the followings: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{0}^{2}(\\mathbf S_{2}):=\\mu_{0}^{2}(W,Z):=\\mathbb E_{P}[Y\\mid W,x_{0},Z]}\\\\ &{\\check{\\mu}_{0}^{2}(\\check{\\mathbf S}_{2}):=\\mu_{0}^{2}(W,Z)=\\mathbb E_{P}[Y\\mid W,x_{0},Z]}\\\\ &{\\mu_{0}^{1}(\\mathbf S_{1}):=\\mu_{0}^{1}(X,Z):=\\mathbb E_{P}[\\mu_{0}^{2}(W,Z)\\mid X,Z]}\\\\ &{\\check{\\mu}_{0}^{1}(\\check{\\mathbf S}_{1})=\\mu_{0}^{1}(x_{1},Z).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The ratio nuisances are the followings: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\pi_{0}^{2}(W,Z)=\\displaystyle\\frac{P(W\\mid x_{1},Z)P(Z\\mid x_{2})}{P(W,Z\\mid x_{0})},}\\\\ {\\pi_{0}^{1}(X,Z)=\\displaystyle\\frac{\\mathbb{1}_{x_{1}}(X)P(Z\\mid x_{2})}{P(X\\mid Z)P(Z)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The representation for DML-UCA is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P}[\\pi_{0}^{2}(W,Z)\\{Y-\\mu_{0}^{2}(W,X,Z)\\}\\mid X=x_{0}]}\\\\ &{\\,\\,+\\,\\mathbb{E}_{P}[\\pi_{0}^{1}(X,Z)\\{\\mu_{0}^{2}(W,x_{0},Z)-\\mu_{0}^{1}(X,Z)\\}]}\\\\ &{\\,\\,+\\,\\mathbb{E}_{P}[\\mu_{0}^{1}(x_{1},Z)\\mid x_{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "A.4 Example Estimand for Fig. 1e ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Given Fig. 1e, the causal effect is given as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}[Y\\mid\\operatorname{do}(x_{1},x_{2})]=\\sum_{r,z,x_{1}^{\\prime}}\\mathbb{E}_{P}[Y\\mid r,x_{2},z,x_{1}^{\\prime}]P(r\\mid x_{1},z)P(x_{1}^{\\prime},z),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which is the expectation of $Y$ over the probability measure ", "page_idx": 18}, {"type": "equation", "text": "$$\nP(Y\\mid R,X_{2},Z,X_{1})P(R\\mid x_{1},Z)P(X_{1},Z)\\mathbb{1}_{x_{2}}(X_{2}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Set ", "page_idx": 18}, {"type": "text", "text": "${\\bf\\Delta}:\\mathbf{C}_{1}=\\{X_{1},Z\\};\\mathbf{C}_{2}=\\{R\\};\\mathbf{C}_{3}=\\{Y\\}$   \n\u2022 $\\mathbf{R}_{1}=\\varnothing$ and ${\\bf R}_{2}=\\{X_{2}\\}$ with $\\sigma_{\\mathbf{R}_{2}}^{2}=\\mathbb{1}_{x_{2}}(X_{2})$ $\\mathbf{\\deltaS}_{1}=\\{Z\\},\\mathbf{S}_{2}=\\{R,X_{2},Z,X_{1}\\}.$   \n\u2022 $\\mathbf{S}_{1}^{b}=\\{X_{1}\\}$ .   \n\u2022 B1 = S2 \u2229C1 \u2229Sb1 = {X1}.   \n\u2022 \u02c7S2 = {R, X\u20321, Z}. \u02c7S1 = S1.   \n\u2022 P 2 = P(\u00b7 | x1). ", "page_idx": 18}, {"type": "text", "text": "The regression nuisances are the followings: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mu_{0}^{2}({\\bf S}_{2}):=\\mu_{0}^{2}(R,X_{2},Z,X_{1}):=\\mathbb{E}_{P}[Y\\mid R,X_{2},Z,X_{1}]}\\\\ &{}&{\\tilde{\\mu}_{0}^{2}(\\check{\\bf S}_{2}):=\\check{\\mu}_{0}^{2}(R,Z,X_{1}^{\\prime})=\\mathbb{E}_{P}[Y\\mid R,x_{2},Z,X_{1}^{\\prime}]\\qquad}\\\\ &{}&{\\mu_{0}^{1}({\\bf S}_{1},{\\bf B}_{1}^{\\prime}):=\\mu_{0}^{1}(Z,X_{1}^{\\prime}):=\\mathbb{E}_{P}[\\check{\\mu}_{0}^{2}(R,Z,X_{1}^{\\prime})\\mid x_{1},Z,X_{1}^{\\prime}]}\\\\ &{}&{\\tilde{\\mu}_{0}^{1}(\\check{\\bf S}_{1})=\\mu_{0}^{1}(Z,X_{1}).\\qquad\\qquad\\qquad\\qquad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The ratio nuisances are the followings: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pi_{0}^{2}(X_{2},R,X_{1},Z)=\\displaystyle\\frac{\\mathbb{1}_{x_{2}}(X_{2})}{P(X_{2}\\mid R,X_{1},Z)}\\displaystyle\\frac{P(R\\mid x_{1},Z)}{P(R\\mid X_{1},Z)},\\medskip}\\\\ {\\pi_{0}^{1}(Z)=\\displaystyle\\frac{P(Z)}{P(Z\\mid x)}=\\displaystyle\\frac{P(x)P(Z)}{P(x\\mid Z)P(Z)}=\\displaystyle\\frac{P(x)}{P(x\\mid Z)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The representation for DML-UCA is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P}[\\pi_{0}^{2}(X_{2},R,X_{1},Z)\\{Y-\\mu_{0}^{2}(R,X_{2},Z,X_{1})\\}]}\\\\ &{\\mathrm{~}+\\mathbb{E}_{P}[\\pi_{0}^{1}(Z,X^{\\prime})\\{\\tilde{\\mu}_{0}^{2}(R,Z,X_{1}^{\\prime})-\\mu_{0}^{1}(Z,X^{\\prime})\\}\\mid X_{1}=x_{1}]}\\\\ &{\\mathrm{~}+\\mathbb{E}_{P}[\\mu_{0}^{1}(Z,X)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "B More UCA Examples ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "B.1 Effect of the treatment on the treated (ETT) ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Let ${\\bf V}\\,=\\,\\{{\\bf Z},X,Y\\}$ be a set of variables where $\\mathbf{Z}$ is a covariate, $X$ is a treatment and $Y$ is an outcome. The target estimand is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}[Y(x)\\mid x^{\\prime}]=\\sum_{\\mathbf{z}}\\mathbb{E}_{P}[Y\\mid x,\\mathbf{z}]P(\\mathbf{z}\\mid x^{\\prime}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The ETT estimand can be written as an expectation of $Y$ over the probability measure ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Psi=P(Y\\mid X,\\mathbf{Z})P(\\mathbf{Z}\\mid x^{\\prime})\\mathbb{1}_{x}(X).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This factorization implies that $\\mathbf{C}_{1}:=\\{\\mathbf{Z}\\}$ , $\\mathbf{R}:=\\mathbf{V}\\setminus\\mathbf{C}_{1}\\cup\\{Y\\}=\\{X\\}$ , where ${\\bf R}_{1}=\\{X\\}$ , and $\\sigma_{\\mathbf{R}_{1}}^{1}:=\\mathbb{1}_{x}(X)$ . Also, ${\\bf S}_{1}=\\{X\\}\\cup{\\bf Z}$ . Finally, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{P^{1}({\\bf C}_{1})=P({\\bf Z}\\mid x^{\\prime})}}\\\\ {{P^{2}(Y\\mid{\\bf S}_{1})=P(Y\\mid X,{\\bf Z}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The regression nuisances are the followings: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad\\mu_{0}^{1}(\\mathbf{S}_{1}):=\\mu_{0}^{1}(X,\\mathbf{Z}):=\\mathbb{E}_{P}[Y\\mid X,\\mathbf{Z}]}\\\\ &{\\qquad\\qquad\\quad\\breve{\\mu}_{0}^{1}(\\mathbf{S}_{1}\\setminus\\mathbf{R}_{1}):=\\check{\\mu}_{0}^{1}(\\mathbf{Z}):=\\mu_{0}^{1}(x,\\mathbf{Z}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The ratio nuisances are the followings: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\pi_{0}^{1}(X,\\mathbf{Z})=\\frac{P(Z\\mid x^{\\prime})\\mathbb{1}_{x}(X)}{P(X,\\mathbf{Z})}=\\frac{P(x^{\\prime}\\mid\\mathbf{Z})P(\\mathbf{Z})}{P(x^{\\prime})}\\frac{\\mathbb{1}_{x}(X)}{P(X\\mid\\mathbf{Z})P(\\mathbf{Z})}=\\frac{P(x^{\\prime}\\mid\\mathbf{Z})}{P(X\\mid\\mathbf{Z})}\\frac{\\mathbb{1}_{x}(X)}{P(x^{\\prime})}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The representation for DML-UCA is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P}[\\pi_{0}^{1}(X,\\mathbf{Z})\\{Y-\\mu_{0}^{1}(X,\\mathbf{Z})\\}]+\\mathbb{E}_{P}[\\check{\\mu}_{0}^{1}(\\mathbf{Z})\\mid x^{\\prime}].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "B.2 Transportability ( $\\boldsymbol{S}$ -admissibility) ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Let ${\\bf V}\\,=\\,\\{{\\bf Z},X,Y\\}$ be a set of variables where $\\mathbf{Z}$ is a covariate, $X$ is a treatment and $Y$ is an outcome. Let $S$ denote the domain indicator such that $S=0$ means the target domain, and $S=1$ denotes the source. The $S$ -admissibility estimand appeared in transportability scenario is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}[Y\\mid\\operatorname{do}(x)]=\\sum_{\\mathbf{z}}\\mathbb{E}_{P}[Y\\mid x,\\mathbf{z},S=1]P(\\mathbf{z}\\mid S=0).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The estimand can be written as an expectation of $Y$ over the probability measure ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Psi=P(Y\\mid X,\\mathbf{Z},S=1)P(\\mathbf{Z}\\mid S=0)\\mathbb{1}_{x}(X).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "From this factorization, we have $\\mathbf{C}_{1}:=\\mathbf{Z}$ and $\\mathbf{R}_{1}:=X$ . Also, set $P^{1}(\\mathbf{C}_{1}):=P(\\mathbf{Z}\\mid S=0)$ with $\\mathbf{S}_{0}^{b}=S$ . Set $P^{2}(Y\\mid\\mathbf{S}_{1}):=P(Y\\mid X,\\mathbf{Z}\\mid S=1)$ with $\\mathbf{S}_{1}^{b}=S$ and $\\mathbf{S}_{1}:=\\{X\\}\\cup\\mathbf{Z}$ . ", "page_idx": 19}, {"type": "text", "text": "The regression nuisances are the followings: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{0}^{1}(\\mathbf{S}_{1}):=\\mu_{0}^{1}(X,\\mathbf{Z}):=\\mathbb{E}_{P}[Y\\mid X,\\mathbf{Z},S=1]}\\\\ &{\\check{\\mu}_{0}^{1}(\\check{\\mathbf{S}}_{1}):=\\check{\\mu}_{0}^{1}(\\mathbf{Z})=\\mu_{0}^{1}(x,\\mathbf{Z}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The ratio nuisances are the followings: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\pi_{0}^{1}(X,\\mathbf{Z})={\\frac{\\mathbb{1}_{x}(X)}{P(X\\mid\\mathbf{Z},S=1)}}{\\frac{P(\\mathbf{Z}\\mid S=0)}{P(\\mathbf{Z}\\mid S=1)}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The representation for DML-UCA is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P}[\\pi_{0}^{1}(X,\\mathbf{Z})\\{Y-\\mu_{0}^{1}(X,\\mathbf{Z})\\}\\mid S=1]+\\mathbb{E}_{P}[\\breve{\\mu}_{0}^{1}(\\mathbf{Z})\\mid S=0].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "B.3 Off-policy evaluation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Let ${\\bf V}\\,=\\,\\{{\\bf Z},X,Y\\}$ be a set of variables where $\\mathbf{Z}$ is a covariate, $X$ is a treatment and $Y$ is an outcome. Let $\\sigma^{*}(X\\mid Z)$ denote the behavioral policy that an agent observed; i.e., ", "page_idx": 20}, {"type": "equation", "text": "$$\n({\\bf Z},X,Y)\\sim P(Y\\mid X,{\\bf Z})\\sigma^{*}(X\\mid{\\bf Z})P({\\bf Z}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let $\\sigma(X\\mid\\mathbf{Z})$ denote a policy to be evaluated. Then, the effect of the policy $\\sigma^{*}$ is given as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}[Y\\mid\\sigma]:=\\sum_{x,\\mathbf{z}}\\mathbb{E}_{P}[Y\\mid x,\\mathbf{z}]\\sigma^{*}(x\\mid\\mathbf{z})P(\\mathbf{z}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The policy treatment effect in Eq. (B.4) can be represented as UCA as follow. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{C}_{1}:=\\mathbf{Z}}\\\\ {\\mathbf{R}_{1}:=\\{X\\}}\\\\ {\\sigma_{\\mathbf{R}_{1}}^{1}:=\\sigma^{*}(X\\mid Z)}\\\\ {\\mathbf{S}_{1}:=\\{X\\}\\cup\\mathbf{Z}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Set $P^{1}(\\mathbf{C}_{1})\\leftarrow P(\\mathbf{Z}),\\sigma_{\\mathbf{R}_{1}}^{1}(\\mathbf{R}_{1}\\mid\\mathbf{Z}_{1})\\leftarrow\\sigma(X\\mid\\mathbf{Z}).$ , and $P^{2}(Y\\mid\\mathbf{C}_{1},\\mathbf{R}_{1})\\leftarrow P(Y\\mid X,\\mathbf{Z})$ . Then, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Psi(\\mathbf{P};\\pmb{\\sigma}):=\\displaystyle\\sum_{\\mathbf{c},\\mathbf{R}}\\mathbb{E}_{P^{2}}[Y\\mid\\mathbf{c}_{1},\\mathbf{R}_{1}]\\sigma_{\\mathbf{R}_{1}}^{1}P^{1}(\\mathbf{c}_{1})}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{x,\\mathbf{z}}\\mathbb{E}_{P}[Y\\mid x,\\mathbf{z}]\\sigma^{*}(x\\mid\\mathbf{z})P(\\mathbf{z})}\\\\ &{\\qquad=\\mathbb{E}[Y\\mid\\sigma]}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The regression nuisances are the followings: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{0}^{1}(\\mathbf{C}^{(1)}\\cup\\mathbf{R}^{(1)}):=\\mu_{0}^{1}(X,\\mathbf{Z}):=\\mathbb{E}_{P}[Y\\mid X,\\mathbf{Z}]}\\\\ &{\\quad\\quad\\quad\\check{\\mu}_{0}^{1}(\\mathbf{C}^{(1)}):=\\check{\\mu}_{0}^{1}(\\mathbf{Z}):=\\displaystyle\\sum_{x}\\mu_{0}^{1}(x,\\mathbf{Z})\\sigma^{*}(x\\mid\\mathbf{Z}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The ratio nuisances are the followings: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\pi_{0}^{1}(X,\\mathbf{Z})=\\frac{\\sigma^{*}(X\\mid\\mathbf{Z})}{P(X\\mid\\mathbf{Z})}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The representation for DML-UCA is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P}[\\pi_{0}^{1}(X,\\mathbf{Z})\\{Y-\\mu_{0}^{1}(X,\\mathbf{Z})\\}]+\\mathbb{E}_{P}[\\check{\\mu}_{0}^{1}(\\mathbf{Z})].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "B.4 Treatment-treatment interactions ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Let ${\\bf V}\\,=\\,\\{{\\bf Z},X,Y\\}$ be a set of variables where $\\mathbf{Z}$ is a covariate, $X$ is a treatment and $Y$ is an outcome. The estimand for treatment-treatment interaction discussed in Jung et al. (2023b) is ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}[Y\\mid\\operatorname{do}(x_{1},x_{2})]=\\sum_{\\mathbf{z}}\\mathbb{E}[Y\\mid\\operatorname{do}(x_{2}),\\mathbf{z},x_{1}]P(\\mathbf{z}\\mid\\operatorname{do}(x_{1})),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which is an expectation of $Y$ over a product of probability measure ", "page_idx": 20}, {"type": "equation", "text": "$$\nP(Y\\mid\\mathbf{Z},\\operatorname{do}(x_{2}),X_{1})P(\\mathbf{Z}\\mid\\operatorname{do}(x_{1}))\\mathbb{1}_{x_{1}}(X_{1}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which satisfies an additivity. Therefore, $\\mathbb{E}[Y\\mid\\operatorname{do}(x_{1},x_{2})]$ is UCA-expressible. Such reduction can be done since the probability measure satisfies additivity w.r.t. all conditional distributions and the policy $\\mathbb{1}_{x_{1}}(X_{1})$ . Specifically, set ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{C}_{1}:=\\mathbf{Z}}\\\\ &{\\mathbf{R}_{1}:=\\{X_{1}\\}}\\\\ &{\\mathbf{S}_{1}:=\\{X_{1}\\}\\cup\\mathbf{Z}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Also, set ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c}{P^{1}(\\mathbf{C}_{1}):=P(\\mathbf{Z}\\mid\\operatorname{do}(x_{1}))}\\\\ {P^{2}(Y\\mid\\mathbf{C}_{1}\\cup\\mathbf{R}_{1}):=P(Y\\mid X_{1},\\mathbf{Z},\\operatorname{do}(x_{2}))}\\\\ {\\sigma_{\\mathbf{R}_{1}}^{1}:=\\mathbb{1}_{x_{1}}(X_{1}).}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The regression nuisances are the followings: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu_{0}^{1}(\\mathbf{C}^{(1)}\\cup\\mathbf{R}^{(1)}):=\\mu_{0}^{1}(X_{1},\\mathbf{Z}):=\\mathbb{E}_{P}[Y\\mid X_{1},\\mathbf{Z},\\mathrm{do}(x_{2})]}\\\\ {\\check{\\mu}_{0}^{1}(\\mathbf{C}^{(1)}):=\\mathbb{E}_{P}[Y\\mid x_{1},\\mathbf{Z},\\mathrm{do}(x_{2})].\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The ratio nuisances are the followings: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\pi_{0}^{1}(X,\\mathbf{Z})=\\frac{\\mathbb{1}_{x_{1}}(X_{1})P(\\mathbf{Z}\\mid\\operatorname{do}(x_{1}))}{P(X_{1}\\mid\\mathbf{Z},\\operatorname{do}(x_{2}))P(\\mathbf{Z}\\mid\\operatorname{do}(x_{2}))},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which can be estimated through the density estimation approach using the probabilistic classification method described in (D\u00edaz et al., 2023, Sec. 5.4). ", "page_idx": 21}, {"type": "text", "text": "The representation for DML-UCA is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P}[\\pi_{0}^{1}(X,\\mathbf{Z})\\{Y-\\mu_{0}^{1}(X,\\mathbf{Z})\\}\\mid\\operatorname{do}(x_{2})]+\\mathbb{E}_{P}[\\breve{\\mu}_{0}^{1}(\\mathbf{Z})\\mid\\operatorname{do}(x_{1})].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "C More Results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "C.1 Formal definition of Partial influence function (PIF) ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Definition C.1 (Partial influence function (PIF) (Pires and Branco, 2002)). Let $g(\\mathsf{P}^{1},\\cdots\\,,\\mathsf{P}^{K})$ denote a $K$ -multi-distribution functional. For the $k$ -th component, let $\\mathsf{P}_{t}^{k}\\;:=\\;\\mathsf{P}^{k}\\,+\\,t(\\mathsf{Q}^{k}\\,-\\,\\mathsf{P}^{k})$ for $t\\ \\in\\ [0,1]$ , where ${\\mathsf{Q}}^{k}$ is an arbitrary distribution absolutely continuous $w.r t.~\\mathsf{P}^{k}$ . The $k$ -th partial influence function is a function $\\phi^{k}(\\mathbf{V};\\eta^{i}(\\mathbb{P}^{k}),g_{0})$ such that $\\mathbb{E}_{\\mathtt{P}^{k}}[\\phi^{k}(\\mathbf{V};\\eta^{k}(\\mathbf{P}^{k}),g_{0})]=0,$ , $\\mathrm{\\bar{\\mathbb{V}}}_{\\mathbb{P}^{k}}[\\phi^{k}(\\mathrm{\\bar{\\mathbf{V}}};\\eta^{k}(\\mathrm{P}^{k}),g_{0})]<\\infty$ , and $\\begin{array}{r}{\\frac{\\partial}{\\partial t}g(\\mathrm{\\boldsymbol{P}}^{1},\\cdots,\\mathrm{\\boldsymbol{P}}_{t}^{k},\\cdot\\cdot\\cdot\\mathrm{\\boldmath~\\beta~},\\mathrm{\\boldsymbol{P}}^{K})\\big|_{t=0}=\\mathbb{E}_{\\mathbb{q}^{k}}[\\phi^{k}({\\bf V};\\eta^{k}(\\mathrm{\\bf{P}}^{k}),g_{0})]}\\end{array}$ . ", "page_idx": 21}, {"type": "text", "text": "C.2 Density Ratio Estimation ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Two available approaches for estimating the density ratio are the followings. The first approach is to apply the Bayes rule for rewriting the density ratio into more tractable form. For example, consider the problem of estimating $\\pi_{0}^{2}$ for FD, which is given as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\pi_{0}^{2}:=\\frac{P(Z\\mid x,C)}{P(Z\\mid X,C)}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Suppose $Z,C$ are high-dimensional random vectors, and $X$ is a binary singleton variable. Then, $P(X\\mid C)$ or $P(X\\mid Z,C)$ are tractable to estimate compared to $P(Z\\mid X,C)$ , since estimating $P(X\\mid\\cdot)$ can be done using off-the-shelf probabilistic classification method. Here, $\\pi_{0}^{2}$ can be written ", "page_idx": 21}, {"type": "text", "text": "as a tractable form as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi_{0}^{2}:=\\frac{P(Z\\mid x,C)}{P(Z\\mid X,C)}}\\\\ &{\\quad=\\frac{P(Z,X,C)}{P(X\\mid C)P(C)}\\frac{P(x\\mid C)P(C)}{P(Z,x,C)}}\\\\ &{\\quad=\\frac{P(C)}{P(C)}\\frac{P(Z,C)}{P(Z,C)}\\frac{P(x\\mid C)}{P(X\\mid C)}\\frac{P(X\\mid Z,C)}{P(x\\mid Z,C)}}\\\\ &{\\quad=\\frac{P(x\\mid C)}{P(X\\mid C)}\\frac{P(X\\mid Z,C)}{P(X\\mid Z,C)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The second approach is to recast the density ratio into the classification problem (D\u00edaz et al., 2023, Sec. 5.4). For example, consider the ratio nuisance appeared in Treatment-treatment interactions: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\pi_{0}^{1}(X,\\mathbf{Z})={\\frac{\\mathbb{1}_{x_{1}}(X_{1})P(\\mathbf{Z}\\mid\\operatorname{do}(x_{1}))}{P(X_{1}\\mid\\mathbf{Z},\\operatorname{do}(x_{2}))P(\\mathbf{Z}\\mid\\operatorname{do}(x_{2}))}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Here, P (Z|do(x2)) can be estimated as a following procedure. Let $D_{1}\\sim P(\\mathbf{Z}\\mid\\operatorname{do}(x_{1})$ and $\\mathcal{D}_{2}\\sim$ $P(\\mathbf{Z}\\mid\\operatorname{do}({\\dot{x}}_{2}))$ denote samples. Let $\\mathcal{D}_{0}:=\\mathcal{D}_{1}\\cup\\mathcal{D}_{2}$ . Let $\\lambda$ denote an indicator such that $\\lambda=0$ means samples are from $\\mathcal{D}_{1}$ and $\\lambda=1$ means they are from $\\mathcal{D}_{2}$ . Without loss of generality, $\\left|\\mathcal{D}_{1}\\right|=\\left|\\mathcal{D}_{2}\\right|$ . Then, ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\frac{P(\\mathbf{Z}\\mid\\operatorname{do}(x_{1}))}{P(\\mathbf{Z}\\mid\\operatorname{do}(x_{2}))}}={\\frac{P(\\mathbf{Z}\\mid\\lambda=0)}{P(\\mathbf{Z}\\mid\\lambda=1)}}={\\frac{P(\\lambda=1)}{P(\\lambda=0)}}{\\frac{P(\\lambda=0\\mid\\mathbf{Z})P(\\mathbf{Z})}{P(\\lambda=1\\mid\\mathbf{Z})P(\\mathbf{Z})}}={\\frac{P(\\lambda=0\\mid\\mathbf{Z})}{P(\\lambda=1\\mid\\mathbf{Z})}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then, instead of estimating the density ratio explicitly as PP  ((ZZ||ddoo((xx12)))), we can estimate the equivalent estimand P (\u03bb=0|Z) using any off-the-shelf probabilistic classification method. ", "page_idx": 22}, {"type": "text", "text": "C.3 Analysis of non-UCA functionals ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We consider two cases where a target estimand cannot be expressed through UCA: ", "page_idx": 22}, {"type": "text", "text": "1. Case 1. The target estimand is not in a form of the product (e.g., the target estimand is the quotient of sum-products of two conditional distributions ). ", "page_idx": 22}, {"type": "text", "text": "2. Case 2. For a target estimand that is represented as the expectation of $Y$ over the measure ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Psi^{\\prime}[\\mathbf{P};\\sigma]:=P^{m+1}(Y\\mid\\mathbf{S}_{m}^{\\prime})\\prod_{i=1}^{m}P^{i}(\\mathbf{C}_{i}\\mid\\mathbf{S}_{i-1}^{\\prime})\\sigma_{\\mathbf{R}_{i}}^{i}(\\mathbf{R}_{i}\\mid\\mathbf{S}_{i}^{\\prime}\\setminus\\mathbf{R}_{i}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $P^{i}(\\mathbf{V})=Q^{i}(\\mathbf{V}\\mid\\mathbf{S}_{i-1}^{b}=\\mathbf{s})$ for some distribution $Q^{i},\\exists i\\in\\{2,\\cdots,m+1\\}$ such that $\\mathbf{S}_{i-1}^{\\prime}\\neq(\\mathbf{C}^{(i-1)}\\cup\\mathbf{R}^{(i-1)})\\setminus\\mathbf{S}_{i-1}^{b}$ . ", "page_idx": 22}, {"type": "text", "text": "In this section, we will provide example functionals that cannot be expressed through UCA. ", "page_idx": 22}, {"type": "text", "text": "C.3.1 On Case 1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Consider Fig. 1c where the causal effect $P(y\\mid\\deg(x))$ is identifiable and given as ", "page_idx": 22}, {"type": "equation", "text": "$$\nP(y\\mid\\operatorname{do}(x))={\\frac{\\sum_{w}P(y,x\\mid r,w)P(w)}{\\sum_{w}P(x\\mid r,w)P(w)}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Here, the functional for $\\mathbb{E}[Y\\mid\\operatorname{do}(x)]$ is represented not as the expectation of a product of conditional distributions, but rather as a quotient of sums of conditional distributions. The napkin estimand is not UCA-expressible. ", "page_idx": 22}, {"type": "image", "img_path": "aX9z2eT6ul/tmp/668b81a52fba9249fb81fe3b8d078ca327421d5ff07d9262193ba4ee0458dbc5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure C.3: (a-c) Example for Case 2 (Generalized identification under partial observability (Lee and Bareinboim, 2020, Fig. 1)); and (d) Example for Case 3 ", "page_idx": 23}, {"type": "text", "text": "C.3.2 On Case 2 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Consider Figs. (C.3a-C.3c). A goal is to identify $P(y\\mid\\deg(x))$ from Fig. C.3c from two input distributions: (1) an interventional distribution $P(c,z\\mid\\deg(x))$ with Fig. C.3a, and (2) an observational distribution $P(r,z,y)$ with Fig. C.3b. This problem is entitled as the generalized identification under partial observability (Lee and Bareinboim, 2020). ", "page_idx": 23}, {"type": "text", "text": "Here, the causal effect is identifiable and given as (Lee and Bareinboim, 2020) ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}[Y\\mid\\operatorname{do}(x)]=\\sum_{c,r,z}\\mathbb{E}_{P}[Y\\mid r,z]P(z\\mid\\operatorname{do}(x),c)P(r)P(c\\mid\\operatorname{do}(x)).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This functional is an expectation of the probability measure $Y$ over $P(Y\\mid\\mathrm{~\\boldmath~\\cal~R,Z~})P(Z\\;\\;\\mid\\;\\;$ $\\mathrm{do}(x),C)P(R)P(C\\mid\\mathrm{do}(x))$ . Based on this probability measure, apply the following setting: ", "page_idx": 23}, {"type": "text", "text": "\u2022 ${\\bf C}_{1}=\\{C\\},{\\bf C}_{2}:=\\{R\\}$ and ${\\bf C}_{3}:=\\{Z\\}$ . \u2022 $\\mathbf{R}=\\emptyset$ . \u2022 $P^{1}(\\mathbf{C}_{1}):=P(C\\mid\\operatorname{do}(x)){\\mathrm{~with~}}\\mathbf{S}_{0}^{b}=\\varnothing.$ \u2022 $P^{2}(\\mathbf{C}_{2}\\mid\\mathbf{S}_{1}^{\\prime})=P(R)$ with $\\mathbf{S}_{1}^{b}=\\emptyset$ and $\\mathbf{S}_{1}^{\\prime}:=\\emptyset$ . \u2022 $P^{3}(\\mathbf{C}_{3}\\mid\\mathbf{S}_{2}^{\\prime})=P(Z\\mid\\operatorname{do}(x),C)$ with $\\mathbf{S}_{2}^{b}=\\emptyset$ and $\\mathbf{S}_{2}^{\\prime}:=\\{C\\}$ \u2022 $P^{4}(Y\\mid\\mathbf{S}_{3}^{\\prime})=P(Y\\mid R,Z)$ with $\\mathbf{S}_{2}^{b}=\\emptyset$ and $\\mathbf{S}_{3}^{\\prime}:=\\{R,Z\\}$ . ", "page_idx": 23}, {"type": "text", "text": "Here, $\\mathbf{S}_{1}^{\\prime}=\\emptyset\\neq(\\mathbf{C}^{(1)}\\cup\\mathbf{R}^{(1)})\\setminus\\mathbf{S}_{1}^{b}=\\{C\\}$ . Also, $\\mathbf{S}_{2}^{\\prime}=\\{C\\}\\neq(\\mathbf{C}^{(2)}\\cup\\mathbf{R}^{(2)})\\setminus\\mathbf{S}_{2}^{b}=\\{C,R\\}$ Finally, S\u20323 $=\\,\\{R,Z\\}\\,\\neq\\,({\\bf C}^{(3)}\\cup{\\bf R}^{(3)})\\setminus{\\bf S}_{3}^{b}\\,=\\,\\{C,R,Z\\}.$ . Therefore, Eq. (C.2) is not within UCA-class. ", "page_idx": 23}, {"type": "text", "text": "Now, we will witness that the target estimand cannot be correctly represented through the nested regression and empirical bifurcation. Applying the nested regression, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mu_{0}^{3}({\\bf S}_{3}^{\\prime})=\\mu_{0}^{3}(R,Z):=\\mathbb{E}_{P}[Y\\mid R,Z]}\\\\ &{}&{\\tilde{\\mu}_{0}^{3}(\\check{\\bf S}_{3}^{\\prime})=\\mu_{0}^{3}(R,Z)=\\mathbb{E}_{P}[Y\\mid R,Z].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mu_{0}^{2}(\\mathbf{S}_{2}^{\\prime})=\\mathbb{E}[\\mu_{0}^{3}(R,Z)\\mid C,\\mathrm{do}(x)]=\\sum_{r,z}\\mathbb{E}_{P}[Y\\mid r,z]P(r,z\\mid c,\\mathrm{do}(x)).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This doesn\u2019t correctly represent the target estimand in Eq. (C.2) because $P(r,z\\mid c,\\mathrm{do}(x))$ is not decomposed into $P(r)$ and $P(z\\mid r,c,\\mathrm{do}(x))$ . ", "page_idx": 23}, {"type": "text", "text": "C.4 Time Complexity ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we provide a detailed analysis on the time complexity in Table 2. Here, $n$ is the sample size. When there are multiple sample sets (i.e., $K>1$ ), we will use $n_{\\mathrm{max}}$ to denote the size of the largest sample set. $K$ is the number of sample sets in Eq. (11). $m$ is the number of variables in a causal graph. ", "page_idx": 23}, {"type": "text", "text": "C.4.1 BD/SBD ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Here, we focus on the back-door adjustment, since the SBD can be analyized similarly. The back-door (BD) adjustment estimand is given as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{\\mathbf{z}}\\mathbb{E}_{P}[Y\\mid\\mathbf{x},\\mathbf{z}]P(\\mathbf{z}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Plug-in. The plug-in estimator composed of two stage \u2013 learn the conditional probability table and and evaluate it for each samples. For a detailed description, we define some notations. Let ${\\mathcal{D}}:=\\{\\mathbf{V}_{(j)}:j=1,\\cdot\\cdot\\cdot\\mathbf{\\Omega},n\\}$ , where $\\mathbf{V}_{(j)}$ denote the $j$ \u2019th sample. For any $\\mathbf{W}\\subseteq\\mathbf{V}$ , we will use $\\mathcal{D}_{\\mathbf{w}}$ to denote the sub-sample of $\\mathcal{D}$ that $\\mathbf{W}$ is fixed to $\\mathbf{w}$ ; i.e., $\\mathcal{D}_{\\mathbf{w}}:=\\{\\mathbf{V}_{(j)}\\in\\mathcal{D}$ such that $\\mathbf{W}_{(j)}=\\mathbf{w}\\}$ We will use $\\mathcal{T}(\\mathcal{D}_{\\mathbf{w}})$ to denote the index set for $\\mathcal{D}_{\\mathbf{w}}$ . Finally, we will use $n_{\\mathbf{w}}:=|\\mathcal{D}_{\\mathbf{w}}|$ . ", "page_idx": 24}, {"type": "text", "text": "For the BD adjustment, the plug-in estimator is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{\\mathbf{z}}\\hat{\\mathbb{E}}[Y\\mid\\mathbf{x},\\mathbf{z}]\\hat{P}(\\mathbf{z}),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mathbb{E}}[Y\\mid\\mathbf{x},\\mathbf{z}]:=\\displaystyle\\frac{1}{n_{\\mathbf{x},\\mathbf{z}}}\\,\\sum_{j\\in\\mathbb{Z}(\\mathcal{D}_{\\mathbf{x},\\mathbf{z}})}Y_{(j)},}\\\\ &{\\qquad\\quad\\hat{P}(\\mathbf{z})=\\displaystyle\\frac{n_{\\mathbf{z}}}{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For the fixed $(\\mathbf{x},\\mathbf{z})$ , learning $\\hat{\\mathbb{E}}[Y\\mid\\mathbf{x},\\mathbf{z}]$ and ${\\hat{P}}(\\mathbf{z})$ take $O(n)$ . Such learning needs to be done for all possible realizations $(\\mathbf{x},\\mathbf{z})$ , where the cardinality of the realization is $O(2^{m})$ . As a result, the computational complexity is $O(n2^{m})$ . ", "page_idx": 24}, {"type": "text", "text": "IPW, OM, AIPW We first consider the inverse probability-weighting (IPW) estimator (Rosenbaum and Rubin, 1983). The IPW estimator is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\mathbb{1}_{\\mathbf{x}}(\\mathbf{X}_{(i)})}{\\hat{\\pi}(\\mathbf{X}_{(i)}\\mid\\mathbf{Z}_{(i)})}Y_{(i)},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where ${\\hat{\\pi}}(\\mathbf{X}\\mid\\mathbf{Z})$ is the evaluated function for $P(\\mathbf{X}\\mid\\mathbf{Z})$ . Learning the nuisance parameter $\\hat{\\pi}$ takes $T(n,m)$ and evaluating the IPW estimator takes $O(n)$ . As a result, the time complexity for the IPW estimator is $O(n+T(n,m))$ . ", "page_idx": 24}, {"type": "text", "text": "Next, we consider the outcome-model (OM) estimator (Robins, 1986). The OM estimator is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}\\hat{\\mu}(\\mathbf{X}_{(i)},\\mathbf{Z}_{(i)}),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\hat{\\mu}(\\mathbf{X},\\mathbf{Z})$ is the evaluated function for $\\mathbb{E}_{P}[Y\\mid\\mathbf{X},\\mathbf{Z}]$ . Learning the nuisance parameter $\\hat{\\pi}$ takes $T(n,m)$ and evaluating the OM estimator takes $O(n)$ . As a result, the time complexity for the OM estimator is $O(n+T(n,m))$ . ", "page_idx": 24}, {"type": "text", "text": "Finally, the AIPW estimator (Robins and Rotnitzky, 1995) is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\mathbb{1}_{\\mathbf{x}}(\\mathbf{X}_{(i)})}{\\hat{\\pi}(\\mathbf{X}_{(i)}\\mid\\mathbf{Z}_{(i)})}Y_{(i)}+\\frac{1}{n}\\sum_{i=1}^{n}\\hat{\\mu}(\\mathbf{X}_{(i)},\\mathbf{Z}_{(i)})-\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\mathbb{1}_{\\mathbf{x}}(\\mathbf{X}_{(i)})}{\\hat{\\pi}(\\mathbf{X}_{(i)}\\mid\\mathbf{Z}_{(i)})}\\hat{\\mu}(\\mathbf{X}_{(i)},\\mathbf{Z}_{(i)}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Learning the nuisance parameters $\\hat{\\pi}$ and $\\hat{\\mu}$ takes $T(n,m)$ and evaluating the OM/IPW estimator takes $O(n)$ . As a result, the time complexity for the OM estimator is $O(n+T(n,m))$ . ", "page_idx": 24}, {"type": "text", "text": "C.4.2 Front-door adjustment (FD) ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The front-door adjustment (Pearl, 2000) is ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{\\mathbf{z},\\mathbf{c}}P(\\mathbf{z}\\mid\\mathbf{x},\\mathbf{z})\\sum_{\\mathbf{z}}\\mathbb{E}_{P}[Y\\mid\\mathbf{z},\\mathbf{x}^{\\prime},\\mathbf{c}]P(\\mathbf{x}^{\\prime}\\mid\\mathbf{c})P(\\mathbf{c}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The FD estimators of (Fulcher et al., 2019; Guo et al., 2023) is ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{\\i}\\sum_{i=1}^{n}\\frac{\\hat{\\xi}({\\bf Z}_{(i)},x,{\\bf C}_{(i)})}{\\hat{\\xi}({\\bf Z}_{(i)},{\\bf X}_{(i)},{\\bf C}_{(i)})}\\{Y_{(i)}-\\hat{\\mu}({\\bf C}_{(i)},{\\bf X}_{(i)},{\\bf Z}_{(i)})\\}\\eqno(\\mathrm{~,~}{\\bf X}_{(i)},{\\bf Z}_{(i)})}\\\\ {\\displaystyle+\\,\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1_{{\\bf x}}({\\bf X}_{(i)})}{\\hat{\\pi}({\\bf X}_{(i)},{\\bf C}_{(i)})}\\left\\{\\sum_{\\bf x}\\hat{\\mu}({\\bf C}_{(i)},{\\bf x},{\\bf Z}_{(i)})\\hat{\\pi}({\\bf x},{\\bf C}_{(i)})-\\sum_{{\\bf x},{\\bf z}}\\hat{\\mu}({\\bf C}_{(i)},{\\bf x},{\\bf z})\\hat{\\xi}({\\bf z},{\\bf X}_{(i)},{\\bf C}_{(i)})\\hat{\\pi}({\\bf x},{\\bf C}_{(i)})\\right\\}\\sum_{{\\bf x},{\\bf z}}\\hat{\\mu}({\\bf C}_{(i)},{\\bf x},{\\bf Z}_{(i)})\\hat{\\xi}({\\bf z},{\\bf X}_{(i)},{\\bf C}_{(i)})\\=}\\\\ {\\displaystyle+\\,\\frac{1}{n}\\sum_{i=1}^{n}\\{\\sum_{\\bf z}\\hat{\\mu}({\\bf C}_{(i)},{\\bf X}_{(i)},{\\bf z})\\hat{\\xi}({\\bf z},{\\bf x},{\\bf C}_{(i)})\\},\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad({\\bf C}.{\\bf9})}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\hat{\\mu}(\\mathbf{C},\\mathbf{X},\\mathbf{Z})$ , $\\hat{\\xi}(\\mathbf{Z},\\mathbf{X},\\mathbf{C})$ and $\\hat{\\pi}(\\mathbf{X},\\mathbf{C})$ are the evaluated functions for $\\mathbb{E}_{P}[Y\\mid\\textbf{C},\\mathbf{X},\\mathbf{Z}]$ , $P(\\mathbf{Z}\\mid\\mathbf{X},\\mathbf{C})$ and $P(\\mathbf{X}\\mid\\mathbf{C})$ . Learning these nuisances takes $T(n,m)$ time. Equipped with $\\hat{\\mu},\\hat{\\xi},\\hat{\\pi}$ , evaluating the FD estimator takes $O(n2^{m})$ , since the evaluation over $n$ samples is repeated for $O(2^{m})$ realization of every $(\\mathbf{x},\\mathbf{z})$ . Therefore, the overall time complexity is $O(n2^{m}+T(n,m))$ . ", "page_idx": 25}, {"type": "text", "text": "C.4.3 Tian\u2019s adjustment ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The Tian\u2019s adjustment (Tian and Pearl, 2002a) is ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{\\mathbf{v}\\setminus x y}\\sum_{x^{\\prime}}\\mathbb{E}_{P^{\\prime}}[Y\\mid\\mathbf{v}^{(K)}]\\prod_{i=1}^{K}P^{\\prime}(v_{i}\\mid\\mathbf{v}^{(i-1)}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The estimator for Tian\u2019s adjstment proposed in (Bhattacharya et al., 2022) is $\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}\\varphi(\\mathbf{V}_{(i)};\\hat{\\boldsymbol{\\eta}})}\\end{array}$ , where $\\varphi(\\mathbf{V};\\eta_{0})$ is given as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\ell\\in\\mathbb{R}^{2+1}\\times\\mathbb{R}^{2}}{\\sum}\\left\\lbrace\\underset{j=1}{\\overset{1}{\\prod}}\\frac{\\mathbf{1}_{x}(X)}{P(V_{j}|\\,\\nabla(\\theta^{\\perp}))}\\sum_{\\ell^{\\prime}\\in\\mathbb{R}^{2+1}\\setminus V_{j}}\\underset{y\\in(\\mathbb{R}^{2+1}\\times X)\\cup\\mathbb{R}^{2+1}}{\\prod}P(v_{j}\\mid\\mathbf{v}^{(j-1)})\\vert_{\\infty=r^{\\prime}}\\vert\\nu_{j}\\varphi\\mathbf{s}_{x}\\right\\rbrace}\\\\ &{-\\underset{V_{\\ell}\\in\\mathbb{R}^{2+1}\\setminus V_{j}}{\\sum}\\left\\lbrace\\frac{1}{\\underset{j=1}{\\prod}}\\frac{\\mathbf{1}_{x}(X)}{P(V_{j}|\\,\\nabla(\\theta^{\\perp}))}\\sum_{\\ell^{\\prime}\\in\\mathbb{R}^{2+1}\\setminus V_{j}}\\underset{y\\in(\\mathbb{R}^{2+1}\\setminus V_{\\delta})\\cup\\mathbb{R}^{2+1}}{\\prod}P(v_{j}\\mid\\mathbf{v}^{(j-1)})\\vert_{\\infty=r^{\\prime}}\\vert\\nu_{j}\\varphi\\mathbf{s}_{x}\\right\\rbrace}\\\\ &{+\\underset{V_{\\ell}\\in V_{\\ell}\\in\\mathbb{R}^{2+1}\\setminus V_{s}}{\\sum}\\frac{\\prod_{\\ell\\in\\mathbb{R}^{(1-1)}}P(V_{j}|\\,\\nabla(\\theta^{-1}))\\vert_{\\infty=r}}{\\prod}\\left\\lbrace\\underset{v\\in\\mathbb{R}^{2+1}}{\\sum}\\frac{\\mathbf{1}}{\\nu_{\\ell}\\in\\mathbb{V}^{2+1}}\\right\\rbrace\\underset{v\\in\\mathbb{R}^{2+1}}{\\prod}P(v_{j}\\mid\\mathbf{v}^{(j-1)})\\vert_{\\infty=r^{\\prime}}\\vert\\nu_{\\ell}\\varphi\\mathbf{s}}\\\\ &{-\\underset{V_{\\ell}\\in V_{\\ell}\\in\\mathbb{R}^{2+1}\\setminus x}{\\sum}\\frac{\\prod_{\\ell\\in\\mathbb{R}^{(1-1)}}P(V_{j}|\\,\\nabla(\\theta^{-1}))\\vert_{\\infty=r}}{\\prod_{\\ell\\in\\mathbb{R}^{2+1}}\\prod_{\\ell}\\left\\lbrace\\nabla(\\theta^{\\perp})\\right\\rbrace}\\left\\lbrace\\underset{v\\in\\mathbb{R}^{2+1}}{\\sum}\\frac\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Learning these nuisances takes $T(n,m)$ time. Equipped with nuisances corresponding to $\\hat{P}(v_{i}\\mid$ $\\mathbf{v}^{(i-1)})$ , evaluating the estimator takes $O(n2^{m})$ , since the evaluation over $n$ samples is repeated for $O(2^{m})$ realization of every $\\mathbf{v}$ . Therefore, the overall time complexity is $O(n2^{m}+T(n,m))$ . ", "page_idx": 25}, {"type": "text", "text": "C.4.4 DML-UCA (BD, FD, and Tian\u2019s). ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "For BD, FD, and Tian\u2019s, the time complexity can be derived by specializing Thm. 2 with $K=1$ and $n_{\\mathrm{max}}=n$ , and $T(m,n):=K\\times L\\times(T_{\\mu}+T_{\\pi})$ . Then, the complexity in Thm. 2 reduces to $O(n+T(m,n))$ . ", "page_idx": 26}, {"type": "text", "text": "C.4.5 DML-UCA (general). ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Set $T(m,n_{\\mathrm{max}},K):=L\\times K\\times(T_{\\mu}+T_{\\pi})$ . Then, the complexity in Thm. 2 reduces to $O(K n_{\\mathrm{max}}+$ $T(m,n_{\\mathrm{max}},K))$ . ", "page_idx": 26}, {"type": "text", "text": "C.4.6 DML-ID (obsID). ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The DML-ID estimator of (Jung et al., 2021a) writes the identification functional of causal effect as am arithmetic function of multiple sequential back-door adjustments, where the arithmetic function is an arbitrary combination of marginalization, product, and division. Let $f(\\{A^{k}:k=1,\\cdots\\,,K\\})$ denote the DML-ID, where each $\\mathit{\\check{A}}^{k}$ is the sequential back-door adjustment, and $f$ denotes the arithmetic function. ", "page_idx": 26}, {"type": "text", "text": "To evaluate the DML-ID functional, the first step is to learn all nuisances composing each $A^{k}$ . This takes $O(T(m,n))$ time. The second step is to evaluate $f(\\{A^{k}:k=1,\\cdots\\,,K\\})$ ). Whenever $f$ contains a marginalization over some random vector, the time complexity for evaluating it is $O(n2^{m})$ In DML-ID, such marginalization can happen $O(2^{m})$ times in worst case. Therefore, evaluating $f(\\{A^{k}:k=1,\\cdots\\,,K\\})$ can take $O(n2^{m}\\times2^{m})=O(n2^{2m})$ . As a result, the total time complexity is $O(n2^{2m}+T(n,m))$ . ", "page_idx": 26}, {"type": "text", "text": "C.4.7 DML-gID (gID). ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The DML-gID estimator of (Jung et al., 2023a) writes the identification functional of causal effect as am arithmetic function of multiple generalized sequential back-door adjustments called $\\mathrm{g-mSBD}$ (Jung et al., 2023a), where the arithmetic function is an arbitrary combination of marginalization, product, and division. Let $f(\\{A^{j}\\,:\\,j\\,=\\,1,\\cdots\\,,J\\})$ ) denote the DML-ID, where each $A^{j}$ is the $\\mathrm{g-mSBD}$ , and $f$ denotes the arithmetic function. ", "page_idx": 26}, {"type": "text", "text": "To evaluate the DML-gID functional, the first step is to learn all nuisances composing each $A^{j}$ . This takes $O(T(m,n_{\\mathrm{max}},K))$ time. The second step is to evaluate $f(\\{A^{j}:j=1,\\cdot\\cdot\\cdot,J\\})$ . Whenever $f$ contains a marginalization over some random vector, the time complexity for evaluating it is $O(n_{\\mathrm{max}}2^{m})$ . In DML-gID, such marginalization can happen $O(2^{m}\\times K)$ times in worst case. Therefore, evaluating $f(\\{A^{j}:j=1,\\cdots\\,,J\\})$ can take $O(n_{\\mathrm{max}}2^{m}\\times2^{m}\\times K)=O(K n_{\\mathrm{max}}2^{2m})$ . As a result, the total time complexity is $O(\\dot{K}\\dot{n}_{\\operatorname*{max}}2^{2m}+\\dot{T}(m,n_{\\operatorname*{max}},K))$ . ", "page_idx": 26}, {"type": "text", "text": "D Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "D.1 Proof of Proposition 1 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The algorithm provides a product of probabilities in a form of ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\Psi[\\mathbf{P}]:=\\prod_{V_{i}\\in\\mathbf{S}_{X}}P(V_{i}\\mid\\mathbf{V}^{(i)})\\prod_{V_{j}\\notin\\mathbf{V}\\backslash\\mathbf{S}_{X}}P(V_{j}\\mid\\mathbf{V}^{(j)}\\setminus X,x).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then, by taking an expectation over $Y$ , it gives ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{\\mathbf{v}\\setminus x y}\\sum_{x^{\\prime}}\\mathbb{E}_{P^{\\prime}}[Y\\mid\\mathbf{v}^{(K)}]\\prod_{i=1}^{K}P^{\\prime}(v_{i}\\mid\\mathbf{v}^{(i-1)}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "D.2 Proof for Proposition 2 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "First, define ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{set}_{1}:=((\\mathbf{S}_{i+1}\\cup\\mathbf{B}_{i+1})\\setminus\\mathbf{R}_{i+1}\\setminus\\mathbf{B}_{i})}\\\\ &{\\qquad=(((\\mathbf{C}^{(i+1)}\\cup\\mathbf{R}^{(i)}\\cup\\mathbf{B}_{i+1})\\setminus\\mathbf{S}_{i+1}^{b})\\setminus\\mathbf{B}_{i})\\cup\\mathbf{B}_{i}^{\\prime},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbf{set}_{2}:=\\mathbf{S}_{i}\\cup\\mathbf{B}_{i}^{\\prime}=(\\mathbf{C}^{(i)}\\cup\\mathbf{R}^{(i)}\\setminus\\mathbf{S}_{i}^{b})\\cup\\mathbf{B}_{i}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then, we claim that $\\mathbf{set}_{1}\\setminus\\mathbf{set}_{2}=\\mathbf{C}_{i+1}$ . This holds when $\\mathbf{B}_{i+1}\\setminus\\mathbf{B}_{i}\\setminus\\mathbf{S}_{i}=\\emptyset$ . To prove this   \niswnui .nf t rHfaioxdeiwcdet ivvoeanrr,,i  afsoburl petsph oiissn $\\mathbf{B}_{i+1}\\setminus\\mathbf{S}_{i}\\neq\\emptyset$ .hu reTcnah, $\\mathbf{B}_{i+1}\\subseteq\\mathbf{S}_{i}^{b}$ .sl  ev Rsaehrcioaaulblll det  hbwaeti $\\mathbf{B}_{i+1}$ . $\\mathbf{S}_{i+1}^{b}$ $\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\$ $\\mathbf{B}_{i+1}\\subseteq\\mathbf{S}_{i}^{b}$ $P^{i+1}$ $\\mathbf{C}_{j}$ $\\mathbf{B}_{i}$   \nHowever, this is a contradiction of the definition of $\\mathbf{B}_{i+1}$ and $\\mathbf{B}_{i}$ . Therefore, $\\mathbf{B}_{i+1}\\setminus\\mathbf{B}_{i}\\setminus\\mathbf{S}_{i}=\\emptyset$   \nand $\\mathbf{set}_{1}\\setminus\\mathbf{set}_{2}=\\mathbf{C}_{i+1}$ . ", "page_idx": 27}, {"type": "text", "text": "Then, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{0}^{i}(\\mathbf{S}_{i},\\mathbf{B}_{i}^{\\prime})=\\mathbb{E}_{P^{i+1}}[\\tilde{\\mu}_{0}^{i+1}(\\tilde{\\mathbf{S}}_{i+1})\\mid\\mathbf{S}_{i},\\mathbf{B}_{i}^{\\prime}]}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{P^{i+1}}[\\tilde{\\mu}_{0}^{i+1}(((\\mathbf{S}_{i+1}\\cup\\mathbf{B}_{i+1})\\setminus\\mathbf{R}_{i+1}\\setminus\\mathbf{B}_{i})\\cup\\mathbf{B}_{i}^{\\prime})\\mid\\mathbf{S}_{i},\\mathbf{B}_{i}^{\\prime}]}\\\\ &{\\qquad\\qquad=\\underbrace{\\sum_{\\mathbf{c}_{i+1},\\mathbf{r}_{i+1}}P^{i+1}(\\mathbf{c}_{i+1}\\mid\\mathbf{S}_{i},\\mathbf{B}_{i}^{\\prime})\\sigma_{\\mathbf{R}_{i+1}}^{i+1}(\\mathbf{r}_{i+1})\\mu_{0}^{i+1}(\\mathbf{c}_{i+1},\\mathbf{r}_{i+1},\\mathbf{S}_{i},\\mathbf{B}_{i}^{\\prime})}_{\\mathbf{c}_{i+1},\\mathbf{r}_{i+1}}}\\\\ &{\\qquad\\qquad=\\underbrace{\\sum_{\\mathbf{c}_{i+1},\\mathbf{r}_{i+1}}P^{i+1}(\\mathbf{c}_{i+1}\\mid\\mathbf{S}_{i})\\sigma_{\\mathbf{R}_{i+1}}^{i+1}(\\mathbf{r}_{i+1})\\mu_{0}^{i+1}(\\mathbf{c}_{i+1},\\mathbf{r}_{i+1},\\mathbf{S}_{i},\\mathbf{B}_{i}^{\\prime})}_{\\mathbf{c}_{i+1},\\mathbf{r}_{i+1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By recursion, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P^{1}}[\\check{\\mu}_{0}^{1}(\\check{\\mathbf{S}}_{1})]=\\sum_{\\mathbf{c}^{(m)},\\mathbf{r}^{(m)}}\\prod_{j=1}^{m}P^{j}(\\mathbf{c}_{j}\\mid\\mathbf{s}_{j-1})\\sigma_{\\mathbf{R}_{j}}^{j}(\\mathbf{r}_{j}\\mid\\mathbf{s}_{j}\\mid\\mathbf{r}_{j})\\mathbb{E}_{P^{m+1}}[Y\\mid\\mathbf{s}_{m}].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "D.3 Proof for Proposition 3 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "By definition of $\\pi_{0}^{m}$ . ", "page_idx": 27}, {"type": "text", "text": "D.4 Proof for Theorem 1 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "If conditions in Theorem 1 met, the estimand reduces to the UCA by definition. ", "page_idx": 27}, {"type": "text", "text": "D.5 Proof for Theorem 2 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. The sample-splitting takes $O((m+1)n_{\\mathrm{max}})$ .   \n2. For the fixed $\\ell$ , learning $\\hat{\\mu}_{\\ell}^{i}$ for $i=m,\\cdots,1$ takes $O(T_{\\mu}\\times m)$ . Therefore, learning all regressionnuisances takes $O(T_{\\mu}\\times m\\times L)$ .   \n3. For the fixed $\\ell$ , learning $\\hat{\\pi}_{\\boldsymbol{\\ell}}^{i}$ for $i=1,\\cdot\\cdot\\cdot,m$ takes $O(T_{\\pi}\\times m)$ . Therefore, learning all rationuisances takes $O(T_{\\pi}\\times m\\times L)$ .   \n4. Evaluating the DML estimator in Eq. (8) takes $O((m+1)n_{\\mathrm{max}})$ . ", "page_idx": 27}, {"type": "text", "text": "In total, the time complexity is ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{O((m+1)n_{\\operatorname*{max}})+O(T_{\\mu}\\times m\\times L)+O(T_{\\pi}\\times m\\times L)+O((m+1)n_{\\operatorname*{max}})}\\\\ &{=O(m\\times\\{n_{\\operatorname*{max}}+L\\times(T_{\\mu}+T_{\\pi})\\})}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "D.6 Proof for Theorem 3 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Define $\\begin{array}{r}{\\Psi^{i}:=\\prod_{k=1}^{i}P^{k}(\\mathbf{C}_{k}\\mid\\mathbf{S}_{k-1})\\sigma^{k}(\\mathbf{R}_{k}\\mid\\mathbf{S}_{k}\\mid\\mathbf{R}_{k})}\\end{array}$ . Define ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\Psi_{t}^{i}:=P_{t}^{i}(\\mathbf{C}_{i}\\mid\\mathbf{S}_{i-1})\\sigma^{i}(\\mathbf{R}_{i}\\mid\\mathbf{S}_{i}\\setminus\\mathbf{R}_{i})\\prod_{k=1}^{i-1}P^{k}(\\mathbf{C}_{k}\\mid\\mathbf{S}_{k-1})\\sigma^{k}(\\mathbf{R}_{k}\\mid\\mathbf{S}_{k}\\setminus\\mathbf{R}_{k}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Define ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mu_{t}^{i}(\\mathbf{S}_{i}^{\\prime}):=\\mathbb{E}_{P_{t}^{i+1}}[\\check{\\mu}^{i+1}\\mid\\mathbf{S}_{i}^{\\prime}].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For any $P^{i}$ , we choose the following parametric submodel: ", "page_idx": 28}, {"type": "equation", "text": "$$\nP_{t}^{i}:=P^{i}+t(Q^{i}-P^{i}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For any $i=m,\\cdots,1$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\psi_{0}:=\\mathbb{E}_{\\Psi_{i}}[\\mu_{0}^{i}(\\mathbf{S}_{i}^{\\prime})].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Fix $i\\in\\{1,\\cdots,m\\}$ . Then, consider the differention with respect to $P_{t}^{i+1}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{\\partial}{\\partial t}\\mathbb{E}_{\\Psi_{0}^{i}}[\\mu_{t}^{i}(\\mathbf{S}_{i}^{\\prime})]=\\frac{\\partial}{\\partial t}\\mathbb{E}_{P^{i+1}}[\\mu_{t}^{i}(\\mathbf{S}_{i}^{\\prime})\\pi_{0}^{i}]}}\\\\ &{=\\frac{\\partial}{\\partial t}\\mathbb{E}_{P_{t}^{i+1}}[\\mu_{t}^{i}(\\mathbf{S}_{i}^{\\prime})\\pi_{0}^{i}]-\\frac{\\partial}{\\partial t}\\mathbb{E}_{P_{t}^{i+1}}[\\mu_{0}^{i}(\\mathbf{S}_{i}^{\\prime})\\pi_{0}^{i}]}\\\\ &{=\\frac{\\partial}{\\partial t}\\mathbb{E}_{P_{t}^{i+1}}[\\pi_{0}^{i}\\tilde{\\mu}^{i+1}(\\tilde{\\mathbf{S}}_{i+1})]-\\frac{\\partial}{\\partial t}\\mathbb{E}_{P_{t}^{i+1}}[\\pi_{0}^{i}\\mu_{0}^{i}(\\mathbf{S}_{i}^{\\prime})]}\\\\ &{=\\frac{\\partial}{\\partial t}\\mathbb{E}_{P_{t}^{i+1}}[\\pi_{0}^{i}\\left\\{\\tilde{\\mu}^{i+1}(\\tilde{\\mathbf{S}}_{i+1})-\\mu_{0}^{i}(\\mathbf{S}_{i}^{\\prime})\\right\\}]}\\\\ &{=\\mathbb{E}_{Q^{i+1}}[\\pi_{0}^{i}\\left\\{\\tilde{\\mu}^{i+1}(\\tilde{\\mathbf{S}}_{i+1})-\\mu_{0}^{i}(\\mathbf{S}_{i}^{\\prime})\\right\\}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Also, consider the differentiation with respect to $P^{1}$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial t}\\mathbb{E}_{P_{t}^{1}}[\\check{\\mu}_{0}^{1}(\\check{\\mathbf{S}}_{1})]=\\mathbb{E}_{Q^{1}}[\\check{\\mu}_{0}^{1}(\\check{\\mathbf{S}}_{1})-\\psi_{0}].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then, define $\\varphi^{i}$ as the differentiation with respect to $P^{i}$ as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\varphi^{i}({\\check{\\mathbf{S}}}_{i};\\eta_{0}^{i},\\psi_{0}):={\\left\\{\\!\\!\\!\\begin{array}{l l}{\\pi_{0}^{i-1}\\{\\check{\\mu}_{0}^{i}-\\mu_{0}^{i-1}\\}}&{{\\mathrm{~if~}}i>1}\\\\ {\\check{\\mu}_{0}^{1}-\\psi_{0}}&{{\\mathrm{~if~}}i=1.}\\end{array}\\!\\!\\right.}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial}{\\partial t}\\Psi(\\mathbf{P}^{1},\\cdots,\\mathbf{P}_{t}^{k},\\cdots,\\mathbf{P}^{K})\\bigg\\vert_{t=0}=\\displaystyle\\sum_{i\\in\\mathcal{I}_{k}}\\frac{\\partial P_{t}^{i}}{\\partial t}\\frac{\\partial}{\\partial P_{t}^{i}}\\Psi(P^{1},\\cdots,P_{t}^{i},\\cdots,P^{m+1};\\pmb{\\sigma})\\bigg\\vert_{t=0}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\sum_{i\\in\\mathcal{I}_{k}}\\mathbb{E}_{Q^{i}}[\\varphi^{i}(\\check{\\mathbf{S}}_{i};\\eta_{0}^{i},\\psi_{0})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which completes the proof. ", "page_idx": 28}, {"type": "text", "text": "D.7 Proof for Theorem 4 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Structure of the proof. Theorem 4 will be proven based on Lemma D.2, Lemma D.3, and Lemma D.4. Specifically, we proceed the proof as follows: ", "page_idx": 28}, {"type": "text", "text": "1. We will prove Lemma D.2, Lemma D.3, and Lemma D.4.   \n2. Berry-Essen\u2019s inequality (Berry, 1941) will be stated as a preliminary in Prop. D.1.   \n3. Theorem 4 will be proven based on the main lemmas and Berry-Essen\u2019s inequality. ", "page_idx": 28}, {"type": "text", "text": "D.7.1 Helper lemmas ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We first state and prove helper lemmas. ", "page_idx": 29}, {"type": "table", "img_path": "aX9z2eT6ul/tmp/7e40822b4db49e3ea21bb2223b5edb589fce42ee7b32481dd276f4702da16c65.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "Proof of Lemma D.1. By the total expectation law, it suffices to show that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\Psi(\\mathbf{P};\\pmb{\\sigma})=\\mathbb{E}_{P^{1}}[\\check{\\mu}_{0}^{1}(\\check{\\mathbf{S}}_{1})].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This holds from Prop. 2. ", "page_idx": 29}, {"type": "text", "text": "Lemma D.2 (Decomposition). Define the following ", "text_level": 1, "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\Phi(\\hat{\\pmb{\\mu}},\\hat{\\pi}):=\\sum_{i=1}^{m}\\mathbb{E}_{P^{i+1}}[\\hat{\\pi}^{i}\\{\\check{\\mu}^{i+1}-\\hat{\\mu}^{i}\\}]+\\mathbb{E}_{P^{1}}[\\check{\\mu}^{1}]}\\\\ &{\\displaystyle\\Phi(\\mu_{0},\\pi_{0}):=\\sum_{i=1}^{m}\\mathbb{E}_{P^{i+1}}[\\pi_{0}^{i}\\{\\check{\\mu}_{0}^{i+1}-\\mu_{0}^{i}\\}]+\\mathbb{E}_{P^{1}}[\\check{\\mu}_{0}^{1}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The following decomposition holds: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\Phi(\\hat{\\mu},\\hat{\\pi})-\\Phi(\\mu_{0},\\pi_{0})=\\sum_{r=1}^{m}\\mathbb{E}_{P^{r+1}}[\\hat{\\omega}^{(r-1)}\\{\\mu_{0}^{r}-\\hat{\\mu}^{r}\\}\\{\\hat{\\pi}^{r}-\\pi_{0}^{r}\\}].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof of Lemma D.2. First, ", "text_level": 1, "page_idx": 29}, {"type": "equation", "text": "$$\n\\Phi(\\hat{\\pmb{\\mu}},\\hat{\\pmb{\\pi}})-\\psi_{0}=\\Phi(\\hat{\\pmb{\\mu}},\\hat{\\pmb{\\pi}})-\\Phi(\\pmb{\\mu}_{0},\\pi_{0}).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Also, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathbb E}_{P^{m+1}}[\\hat{\\pi}^{m}\\{\\mu_{0}^{m}-\\hat{\\mu}^{m}\\}]+{\\mathbb E}_{P^{m+1}}[\\pi_{0}^{m}\\hat{\\mu}^{m}]-\\underbrace{{\\mathbb E}_{P^{m+1}}[\\pi_{0}^{m}\\mu_{0}^{m}]}_{:=\\psi_{0}}}\\\\ &{={\\mathbb E}_{P^{m+1}}[\\{\\pi_{0}^{m}-\\hat{\\pi}^{m}\\}\\{\\hat{\\mu}^{m}-\\mu_{0}^{m}\\}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "For $i=m-1,\\cdots\\,,1$ , define ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mu_{0}^{i}[\\check{\\mu}^{i+1}]:=\\mathbb{E}_{P^{i+1}}[\\check{\\mu}^{i+1}(\\check{\\mathbf{S}}_{i+1})\\mid\\mathbf{S}_{i},\\mathbf{B}_{i}^{\\prime}].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P^{i+1}}[\\hat{\\pi}^{i}\\{\\mu_{0}^{i}[\\check{\\mu}^{i+1}]-\\hat{\\mu}^{i}\\}]+\\mathbb{E}_{P^{i+1}}[\\pi_{0}^{i}\\hat{\\mu}^{i}]-\\mathbb{E}_{P^{i+1}}[\\pi_{0}^{i}\\mu_{0}^{i}[\\check{\\mu}^{i+1}]]}\\\\ &{\\;=\\mathbb{E}_{P^{i+1}}[\\{\\pi_{0}^{i}-\\hat{\\pi}^{i}\\}\\{\\hat{\\mu}^{i}-\\mu_{0}^{i}[\\check{\\mu}^{i+1}]\\}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Also, for any $\\mu^{i+1}$ and corresponding $\\check{\\mu}^{i+1}$ , and for all $i=m-1,\\cdots\\,,1$ , by the definition of the $\\pi_{0}^{i}$ nuisance, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P^{i+2}}[\\pi_{0}^{i+1}\\mu^{i+1}]=\\mathbb{E}_{P^{i+1}}[\\pi_{0}^{i}\\mu_{0}^{i}[\\breve{\\mu}^{i+1}]].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P^{m+1}}\\big[\\hat{\\pi}^{m}\\{\\mu_{0}^{m}-\\hat{\\mu}^{m}\\}\\big]+\\mathbb{E}_{P^{m+1}}\\big[\\pi_{0}^{m}\\hat{\\mu}^{m}\\big]-\\mathbb{E}_{P^{m+1}}\\big[\\pi_{0}^{m}\\mu_{0}^{m}\\big]}\\\\ &{\\big+\\displaystyle\\sum_{i=1}^{m-1}\\mathbb{E}_{P^{i+1}}\\big[\\hat{\\pi}^{i}\\{\\mu_{0}^{i}[\\tilde{\\mu}^{i+1}]-\\hat{\\mu}^{i}\\}\\big]+\\mathbb{E}_{P^{i+1}}\\big[\\pi_{0}^{i}\\hat{\\mu}^{i}\\big]-\\mathbb{E}_{P^{i+1}}\\big[\\pi_{0}^{i}\\mu_{0}^{i}\\big[\\tilde{\\mu}^{i+1}\\big]\\big]}\\\\ &{=\\displaystyle\\sum_{i=1}^{m}\\mathbb{E}_{P^{i+1}}\\big[\\hat{\\pi}^{i}\\{\\mu_{0}^{i}[\\tilde{\\mu}^{i+1}]-\\hat{\\mu}^{i}\\}\\big]+\\mathbb{E}_{P^{2}}\\big[\\pi_{0}^{1}\\hat{\\mu}^{1}\\big]-\\psi_{0}}\\\\ &{=\\displaystyle\\sum_{i=1}^{m}\\mathbb{E}_{P^{i+1}}\\big[\\{\\pi_{0}^{i}-\\hat{\\pi}^{i}\\}\\big\\{\\hat{\\mu}^{i}-\\mu_{0}^{i}[\\tilde{\\mu}^{i+1}]\\big\\}\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Note that $\\mathbb{E}_{P^{2}}[\\pi_{0}^{1}\\hat{\\mu}^{1}]=\\mathbb{E}_{P^{1}}[\\check{\\mu}^{1}]$ , since ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{P^{1}}[\\check{\\mu}^{1}(\\mathbf{S}_{1}\\cup\\mathbf{B}_{1})]}\\\\ &{=\\mathbb{E}_{P^{1}}[\\sigma_{\\mathbf{R}_{1}}^{1}(\\mathbf{R}_{1}\\mid\\mathbf{S}_{1}\\setminus\\mathbf{R}_{1})\\mu^{1}((\\mathbf{S}_{1}\\cup\\mathbf{B}_{1})\\setminus\\mathbf{R}_{1})]}\\\\ &{=\\mathbb{E}_{P^{2}}\\left[\\frac{P^{1}((\\mathbf{S}_{1}\\cup\\mathbf{B}_{1})\\setminus\\mathbf{R}_{1})}{P^{2}(\\mathbf{S}_{1}\\cup\\mathbf{B}_{1})}\\sigma_{\\mathbf{R}_{1}}^{1}(\\mathbf{R}_{1}\\mid\\mathbf{S}_{1}\\setminus\\mathbf{R}_{1})\\mu^{1}((\\mathbf{S}_{1}\\cup\\mathbf{B}_{1})\\setminus\\mathbf{R}_{1})\\right]}\\\\ &{=\\mathbb{E}_{P^{2}}\\left[\\frac{P^{1}(\\mathbf{C}_{1})}{P^{2}(\\mathbf{S}_{1}\\cup\\mathbf{B}_{1})}\\sigma_{\\mathbf{R}_{1}}^{1}(\\mathbf{R}_{1}\\mid\\mathbf{S}_{1}\\setminus\\mathbf{R}_{1})\\mu^{1}((\\mathbf{S}_{1}\\cup\\mathbf{B}_{1})\\setminus\\mathbf{R}_{1})\\right]}\\\\ &{=\\mathbb{E}_{P^{2}}[\\pi_{0}^{1}\\mu_{0}^{1}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\Phi(\\hat{\\mu},\\hat{\\pi})-\\Phi(\\mu_{0},\\pi_{0})}}\\\\ &{=\\displaystyle\\sum_{i=1}^{m}\\mathbb{E}_{P^{i+1}}[\\{\\pi_{0}^{i}-\\hat{\\pi}^{i}\\}\\{\\hat{\\mu}^{i}-\\mu_{0}^{i}[\\tilde{\\mu}^{i+1}]\\}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Lemma D.3 (Stochastic Equicontinuity). Let $\\mathcal{D}\\stackrel{i i d}{\\sim}P$ . Let $\\mathcal{D}=\\mathcal{D}_{0}\\cup\\mathcal{D}_{1}$ , where $n:=|\\mathcal{D}_{0}|$ . Let $\\hat{f}$ be a function estimated from $\\mathcal{D}_{1}$ . Then, in probability greater than $1-\\epsilon$ for any $\\epsilon\\in(0,1)$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{D}_{0}-P}\\left[\\left|\\hat{f}-f\\right|\\right]\\overset{w.p\\;1-\\epsilon}{<}\\frac{\\|\\hat{f}-f\\|_{P}}{\\sqrt{n\\epsilon}},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which implies that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{D}_{0}-P}[|\\hat{f}-f|]=O_{P}\\left(\\frac{\\|\\hat{f}-f\\|_{P}}{\\sqrt{n}}\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof of Lemma D.3. This proof is from (Kennedy et al., 2020, Lemma 2). Since $\\hat{f}$ is a function of $\\mathcal{D}_{1}$ , we will denote $\\hat{f}_{D_{1}}$ . Define a following random variable of interest: ", "page_idx": 30}, {"type": "equation", "text": "$$\nX:=\\mathbb{E}_{\\mathcal{D}_{0}-P}[\\hat{f}_{\\mathcal{D}_{1}}-f].\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then, the conditional expectation of $X$ given $\\mathcal{D}_{1}$ is zero, since ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}_{P}\\left[\\frac{1}{n}\\sum_{i=1}^{n}\\hat{f}_{\\mathcal{D}_{1}}(\\mathbf{V}_{i})~\\Bigg|~\\mathcal{D}_{1}\\right]=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{P}[\\hat{f}_{\\mathcal{D}_{1}}(\\mathbf{V}_{i})~|~\\mathcal{D}_{1}]=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{P}[\\hat{f}_{\\mathcal{D}_{1}}(\\mathbf{V})~|~\\mathcal{D}_{1}]=\\mathbb{E}_{P}[\\hat{f}_{\\mathcal{D}_{1}}(\\mathbf{V})~|~\\mathcal{D}_{1}]\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the third equality holds by the independence of $\\mathcal{D}_{0}$ and $\\mathcal{D}_{1}$ . Therefore, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathbb{E}}_{P}[X\\mid{\\mathcal{D}}_{1}]={\\mathbb{E}}_{P}[{\\mathbb{E}}_{\\mathcal{D}_{0}-P}[\\hat{f}_{\\mathcal{D}_{1}}-f]\\mid{\\mathcal{D}}_{1}]}\\\\ &{\\qquad\\qquad\\quad={\\mathbb{E}}_{P}[{\\mathbb{E}}_{\\mathcal{D}_{0}}[\\hat{f}_{\\mathcal{D}_{1}}-f]\\mid{\\mathcal{D}}_{1}]-{\\mathbb{E}}_{P}[{\\mathbb{E}}_{P}[\\hat{f}_{\\mathcal{D}_{1}}-f]\\mid{\\mathcal{D}}_{1}]}\\\\ &{\\qquad\\qquad\\quad={\\mathbb{E}}_{P}[{\\mathbb{E}}_{P}[\\hat{f}_{\\mathcal{D}_{1}}-f]\\mid{\\mathcal{D}}_{1}]-{\\mathbb{E}}_{P}[{\\mathbb{E}}_{P}[\\hat{f}_{\\mathcal{D}_{1}}-f]\\mid{\\mathcal{D}}_{1}]=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Also, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{V}_{P}[X\\mid\\mathcal{D}_{1}]=\\mathbb{V}_{P}[\\mathbb{E}_{\\mathcal{D}_{0}-P}[\\hat{f}_{\\mathcal{D}_{1}}-f]\\mid\\mathcal{D}_{1}]}\\\\ &{\\phantom{\\quad\\quad\\quad}=\\mathbb{V}_{P}[\\mathbb{E}_{\\mathcal{D}_{0}}[\\hat{f}_{\\mathcal{D}_{1}}-f]\\mid\\mathcal{D}_{1}]}\\\\ &{\\phantom{\\quad\\quad\\quad}=\\frac{1}{n}\\mathbb{V}_{P}[\\hat{f}_{\\mathcal{D}_{1}}-f\\mid\\mathcal{D}_{1}]}\\\\ &{\\phantom{\\quad\\quad\\quad}\\leq\\frac{1}{n}\\|\\hat{f}_{\\mathcal{D}_{1}}-f\\|_{P}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "By applying the (conditional-) Chevyshev\u2019s inequality, ", "page_idx": 31}, {"type": "equation", "text": "$$\nP(|X-\\mathbb{E}_{P}[X\\mid\\mathcal{D}_{1}]|\\ge t\\mid\\mathcal{D}_{1})\\le\\frac{1}{t^{2}}\\mathbb{V}_{P}[X\\mid\\mathcal{D}_{1}]\\le\\frac{1}{n t^{2}}\\|\\hat{f}_{\\mathcal{D}_{1}}-f\\|_{P}^{2}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(|X|\\geq t)=P(|X-\\mathbb{E}_{P}[X\\mid\\mathcal{D}_{1}]|\\geq t)}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{P(\\mathcal{D}_{1})}[P(|X-\\mathbb{E}_{P}[X\\mid\\mathcal{D}_{1}]|\\geq t\\mid\\mathcal{D}_{1})]}\\\\ &{\\qquad\\qquad\\leq\\frac{1}{n t^{2}}\\Vert\\hat{f}_{\\mathcal{D}_{1}}-f\\Vert_{P}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "In other words, $X<t$ in probability greater than $\\begin{array}{r}{1-\\frac{1}{n t^{2}}\\|\\hat{f}_{\\mathcal{D}_{1}}-f\\|_{P}^{2}}\\end{array}$ . If $\\begin{array}{r}{t=\\frac{\\|\\hat{f}_{\\mathcal{D}_{1}}-f\\|_{P}}{\\sqrt{n\\epsilon}}}\\end{array}$ , then $\\begin{array}{r}{X<\\frac{\\|\\hat{f}_{\\mathcal{D}_{1}}-f\\|_{P}}{\\sqrt{n\\epsilon}}}\\end{array}$ in the probability greater than $1-\\epsilon$ for any $\\epsilon\\in(0,1)$ . \u53e3 ", "page_idx": 31}, {"type": "text", "text": "Lemma D.4 (Combining concentration inequalities). Suppose $P(A_{k}\\,>\\,t)\\,\\le\\,b_{k}/t^{2}$ for $k\\,=$ $1,\\cdot\\cdot\\cdot,K$ . Then, ", "page_idx": 31}, {"type": "equation", "text": "$$\nP\\left(\\sum_{k=1}^{K}A_{k}\\leq t K\\right)\\geq1-{\\frac{1}{t^{2}}}\\sum_{k=1}^{K}b_{k}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. The event $\\textstyle\\sum_{k=1}^{K}A_{k}\\leq t K$ includes the case where $A_{k}<t$ for $k=1,\\cdots\\,,K$ . Therefore, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{P\\left(\\sum_{k=1}^{K}A_{k}\\le t K\\right)\\ge P\\left(A_{1}\\le t\\mathrm{~and~}\\cdots\\mathrm{~and~}A_{K}\\le t\\right)}}\\\\ &{=1-P\\left(A_{1}>t\\mathrm{~or~}\\cdots\\mathrm{~or~}A_{K}>t\\right)}\\\\ &{\\ge1-\\displaystyle\\sum_{k=1}^{K}P\\left(A_{k}>t\\right)}\\\\ &{\\ge1-\\displaystyle\\sum_{k=1}^{K}\\frac{b_{k}}{t^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "D.7.2 Preliminary Results ", "text_level": 1, "page_idx": 32}, {"type": "table", "img_path": "aX9z2eT6ul/tmp/47e9e15d38cdd0a47edc5977289ec28fa6dff6c5ffb9019d30460f53d0c14acb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "D.7.3 Proof of Theorem 4 - (1) ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "By Lemma D.2, we decompose the error as follow: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\hat{\\psi}-\\psi_{0}=\\sum_{k=1}^{K}\\mathbb{E}_{\\mathcal{D}^{k}-\\mathbb{P}^{k}}[\\phi_{0}^{k}]}\\\\ {\\displaystyle\\qquad+\\,\\frac{1}{L}\\sum_{\\ell=1}^{L}\\sum_{k=1}^{K}\\mathbb{E}_{\\mathcal{D}_{\\ell}^{k}-\\mathbb{P}^{k}}[\\hat{\\phi}_{\\ell}^{k}-\\phi_{0}^{k}]}\\\\ {\\displaystyle\\qquad+\\,\\frac{1}{L}\\sum_{\\ell=1}^{L}\\sum_{i=1}^{m}\\mathbb{E}_{P^{i+1}}[\\{\\mu_{0}^{i}-\\hat{\\mu}_{\\ell}^{i}\\}\\{\\hat{\\pi}_{\\ell}^{i}-\\pi_{0}^{i}\\}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Define ", "page_idx": 32}, {"type": "equation", "text": "$$\nR_{1}^{k}:=\\mathbb{E}_{\\mathcal{D}^{k}-\\mathbb{P}^{k}}[\\phi_{0}^{k}]+\\frac{1}{L}\\sum_{\\ell=1}^{L}\\mathbb{E}_{\\mathcal{D}_{\\ell}^{k}-\\mathbb{P}^{k}}[\\hat{\\phi}_{\\ell}^{k}-\\phi_{0}^{k}].\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then it completes the proof. ", "page_idx": 32}, {"type": "text", "text": "D.7.4 Proof of Theorem 4 - (2) ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We first study the term $\\mathbb{E}_{D^{k}-\\mathsf{P}^{k}}[\\phi_{0}^{k}]$ . By Chebyshev\u2019s inequality, ", "page_idx": 32}, {"type": "equation", "text": "$$\nP\\left(\\left|\\mathbb{E}_{\\mathcal{D}^{k}-\\mathbb{P}^{k}}[\\phi_{0}^{k}]\\right|>t\\frac{\\rho_{k,0}}{\\sqrt{\\left|\\mathcal{D}^{k}\\right|}}\\right)<\\frac{1}{t^{2}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Equivalently, ", "page_idx": 32}, {"type": "equation", "text": "$$\nP\\left(\\left|\\mathbb{E}_{\\mathcal{D}^{k}-\\mathbb{P}^{k}}[\\phi_{0}^{k}]\\right|>t\\right)<\\frac{1}{t^{2}}\\frac{\\rho_{k,0}^{2}}{|\\mathcal{D}^{k}|}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By Lemma D.4, ", "page_idx": 32}, {"type": "equation", "text": "$$\nP\\left(\\sum_{k=1}^{K}\\left|\\mathbb{E}_{\\mathcal{D}^{k}-\\mathbb{P}^{k}}[\\phi_{0}^{k}]\\right|\\leq t_{1}K\\right)>1-\\frac{1}{t_{1}^{2}}\\sum_{k=1}^{K}\\frac{\\rho_{k,0}^{2}}{|\\mathcal{D}^{k}|}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By Lemma D.3, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathsf{P}^{k}\\left(\\left|\\mathbb{E}_{\\mathcal{D}_{\\ell}^{k}-\\mathbb{P}^{k}}[\\widehat{\\phi}_{\\ell}^{k}-\\phi_{0}^{k}]\\right|>t_{2}\\right)\\leq\\frac{1}{t_{2}^{2}}\\frac{\\|\\widehat{\\phi}_{\\ell}^{k}-\\phi_{0}^{k}\\|_{\\mathbb{P}^{k}}^{2}}{|\\mathcal{D}_{\\ell}^{k}|}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By Lemma D.4, ", "page_idx": 32}, {"type": "equation", "text": "$$\nP\\left(\\frac{1}{L}\\sum_{\\ell=1}^{L}\\sum_{k=1}^{K}\\left|\\mathbb{E}_{\\mathcal{D}_{\\ell}^{k}-\\mathbb{P}^{k}}[\\hat{\\phi}_{\\ell}^{k}-\\phi_{0}^{k}]\\right|\\leq K t_{2}\\right)\\geq1-\\frac{1}{t_{2}^{2}}\\sum_{\\ell=1}^{L}\\sum_{k=1}^{K}\\frac{\\|\\hat{\\phi}_{\\ell}^{k}-\\phi_{0}^{k}\\|_{\\mathbb{P}^{k}}^{2}}{|\\mathcal{D}_{\\ell}^{k}|}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Choose $\\begin{array}{r l}{t_{1}:=\\sqrt{\\frac{2}{\\epsilon}\\sum_{k=1}^{K}\\frac{\\rho_{k,0}^{2}}{|\\mathcal{D}^{k}|}}}&{{}}\\end{array}$ and $\\begin{array}{r}{t_{2}\\;:=\\;\\sqrt{\\frac{2}{\\epsilon}\\sum_{\\ell=1}^{L}\\sum_{k=1}^{K}\\frac{\\|\\hat{\\phi}_{\\ell}^{k}-\\phi_{0}^{k}\\|_{\\mathrm{p}^{k}}^{2}}{|\\mathcal{D}_{\\ell}^{k}|}}}\\end{array}$ . Then, with a probability greater than $1-\\epsilon$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{k=1}^{K}R_{1}^{k}\\leq K\\left(\\sqrt{\\frac{2}{\\epsilon}\\displaystyle\\sum_{k=1}^{K}\\frac{\\rho_{k,0}^{2}}{|\\mathscr{D}^{k}|}}+\\sqrt{\\frac{2}{\\epsilon}\\displaystyle\\sum_{\\ell=1}^{L}\\sum_{k=1}^{K}\\frac{\\|\\hat{\\phi}_{\\ell}^{k}-\\phi_{0}^{k}\\|_{\\mathrm{p}^{k}}^{2}}{|\\mathscr{D}_{\\ell}^{k}|}}\\right)}}\\\\ &{}&{=K\\sqrt{\\frac{2}{\\epsilon}}\\left(\\sqrt{\\displaystyle\\sum_{k=1}^{K}\\frac{\\rho_{k,0}^{2}}{|\\mathscr{D}^{k}|}}+\\sqrt{\\displaystyle\\sum_{\\ell=1}^{L}\\sum_{k=1}^{K}\\frac{\\|\\hat{\\phi}_{\\ell}^{k}-\\phi_{0}^{k}\\|_{\\mathrm{p}^{k}}^{2}}{|\\mathscr{D}_{\\ell}^{k}|}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "D.7.5 Proof of Theorem 4 - (3) ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "By Lemma D.3, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathsf{P}^{k}\\left(\\left|\\mathbb{E}_{\\mathcal{D}_{\\ell}^{k}-\\mathsf{P}^{k}}[\\widehat{\\phi}_{\\ell}^{k}-\\phi_{0}^{k}]\\right|>t\\right)\\leq\\frac{1}{t^{2}}\\frac{\\|\\widehat{\\phi}_{\\ell}^{k}-\\phi_{0}^{k}\\|_{\\mathsf{P}^{k}}^{2}}{|\\mathcal{D}_{\\ell}^{k}|}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "By Lemma D.4, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{P}^{k}\\left(\\frac{1}{L}\\sum_{\\ell=1}^{L}\\left|\\mathbb{E}_{\\mathcal{D}_{\\ell}^{k}-\\mathbb{P}^{k}}[\\hat{\\phi}_{\\ell}^{k}-\\phi_{0}^{k}]\\right|\\leq t\\right)\\geq1-\\frac{1}{t^{2}}\\sum_{\\ell=1}^{L}\\frac{\\|\\hat{\\phi}_{\\ell}^{k}-\\phi_{0}^{k}\\|_{\\mathbb{P}^{k}}^{2}}{|\\mathcal{D}_{\\ell}^{k}|}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Equivalently, by choosing $\\begin{array}{r}{t=\\sqrt{\\frac{1}{\\epsilon}\\sum_{\\ell=1}^{L}\\frac{\\lVert\\hat{\\phi}_{\\ell}^{k}-\\phi_{0}^{k}\\rVert_{\\mathtt{p}^{k}}^{2}}{\\lvert\\mathcal{D}_{\\ell}^{k}\\rvert}}}\\end{array}$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{1}{L}\\sum_{\\ell=1}^{L}\\left|\\mathbb{E}_{\\mathcal{D}_{\\ell}^{k}-\\mathbb{P}^{k}}[\\hat{\\phi}_{\\ell}^{k}-\\phi_{0}^{k}]\\right|\\overset{\\mathrm{w.p.1}-\\epsilon}{\\leq}\\sqrt{\\frac{1}{\\epsilon}\\sum_{\\ell=1}^{L}\\frac{\\|\\hat{\\phi}_{\\ell}^{k}-\\phi_{0}^{k}\\|_{\\mathbb{P}^{k}}^{2}}{|\\mathcal{D}_{\\ell}^{k}|}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Define ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A^{k}:=\\mathbb{E}_{\\mathcal{D}^{k}-\\mathbb{P}^{k}}[\\phi_{0}^{k}]}\\\\ &{B^{k}:=\\cfrac{1}{L}\\cfrac{L}{\\ell=1}\\cfrac{\\mathbb{E}_{\\mathcal{D}_{\\ell}^{k}-\\mathbb{P}^{k}}[\\hat{\\phi}_{\\ell}^{k}-\\phi_{0}^{k}]}{|\\mathcal{D}_{\\ell}^{k}-\\mathbb{P}^{k}|}}\\\\ &{C^{k}:=\\cfrac{1}{L}\\cfrac{L}{\\ell=1}\\,\\Big|\\mathbb{E}_{\\mathcal{D}_{\\ell}^{k}-\\mathbb{P}^{k}}[\\hat{\\phi}_{\\ell}^{k}-\\phi_{0}^{k}]\\Big|}\\\\ &{\\Delta_{k}:=\\sqrt{\\cfrac{1}{\\epsilon}\\sum_{\\ell=1}^{L}\\cfrac{\\|\\hat{\\phi}_{\\ell}^{k}-\\phi_{0}^{k}\\|_{\\mathbb{P}^{k}}^{2}}{|\\mathcal{D}_{\\ell}^{k}|}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Here, ", "page_idx": 33}, {"type": "equation", "text": "$$\nR^{k}:=A^{k}+B^{k}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Then, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}^{k}\\left(R^{k}<x\\right)}\\\\ &{=\\mathbb{P}^{k}\\left(A_{k}+B_{k}<x\\right)}\\\\ &{=\\mathbb{P}^{k}\\left(A_{k}<x-B_{k}\\right)}\\\\ &{\\leq\\mathbb{P}^{k}\\left(A_{k}<x+C_{k}\\right)}\\\\ &{\\stackrel{\\mathrm{wp}\\,1-\\epsilon}{\\leq}\\mathbb{P}^{k}\\left(A_{k}<x+\\Delta_{k}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Then, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}^{k}\\left(A_{k}<x+\\Delta_{k}\\right)-\\Phi(x)\\big|}\\\\ &{=\\big|\\mathbb{P}^{k}\\left(A_{k}<x+\\Delta_{k}\\right)-\\Phi(x+\\Delta_{k})+\\Phi(x+\\Delta_{k})-\\Phi(x)\\big|}\\\\ &{\\leq\\big|\\mathbb{P}^{k}\\left(A_{k}<x+\\Delta_{k}\\right)-\\Phi(x+\\Delta_{k})\\big|+\\big|\\Phi(x+\\Delta_{k})-\\Phi(x)\\big|}\\\\ &{\\leq\\frac{0.4748\\kappa_{0}^{3}}{\\rho_{k,0}^{3}\\sqrt{|\\mathcal{D}^{k}|}}+\\big|\\Phi(x+\\Delta_{k})-\\Phi(x)\\big|}\\\\ &{=\\frac{0.4748\\kappa_{0}^{3}}{\\rho_{k,0}^{3}\\sqrt{|\\mathcal{D}^{k}|}}+\\big|\\Phi^{\\prime}(x^{\\prime})\\Delta_{k}\\big|}\\\\ &{\\leq\\frac{0.4748\\kappa_{0}^{3}}{\\rho_{k,0}^{3}\\sqrt{|\\mathcal{D}^{k}|}}+\\frac{1}{\\sqrt{2\\pi}}\\Delta_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "(Mean-value theorem) ", "page_idx": 34}, {"type": "text", "text": "This completes the proof. ", "page_idx": 34}, {"type": "text", "text": "D.8 Proof for Corollary 4 ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "By Cauchy-Schwartz\u2019 inequality, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{1}{L}\\sum_{\\ell=1}^{L}\\sum_{i=1}^{m}\\mathbb{E}_{P^{i+1}}[\\{\\mu_{0}^{i}-\\hat{\\mu}_{\\ell}^{i}\\}\\{\\hat{\\pi}_{\\ell}^{i}-\\pi_{0}^{i}\\}]\\le\\frac{1}{L}\\sum_{\\ell=1}^{L}\\sum_{i=1}^{m}O_{P^{i+1}}\\left(\\|\\mu_{0}^{i}-\\hat{\\mu}_{\\ell}^{i}\\|\\|\\pi_{0}^{i}-\\hat{\\pi}_{\\ell}^{i}\\|\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Given assumption, the upper bound in Eq. (15) converges at $O_{\\mathbb{P}^{k}}(1/\\sqrt{|\\mathcal{D}_{\\ell}^{k}|})$ . Therefore, we conclude that $R^{k}$ converges in distribution to normal $(0,\\rho_{k,0}^{2})$ . ", "page_idx": 34}, {"type": "text", "text": "E More Experiments ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In this section, we demonstrate the DML-UCA estimator through examples for the ETT, $S$ - admissibility, FD, Verma\u2019s equation, and Ctf-DE described in Sec. 2. For each example, the proposed estimator is constructed using a dataset $\\mathtt{D}^{k}$ following a distribution $\\mathsf{P}^{k}$ . Our goal is to provide empirical evidence of the fast convergence behavior of the proposed estimator compared to competing baseline estimators. We consider two standard baselines in the literature: the \u2018regression-based estimator (reg)\u2019 only uses the regression nuisance parameters $\\pmb{\\mu}$ , and the \u2018ratio-based estimator (ratio)\u2019 that only uses the ratio nuisance parameters $\\pi$ , while our DML-UCA estimator (\u2018dml\u2019) uses both. Details of the regression-based (\u2018reg\u2019) and the ratio-based (\u2018ratio\u2019) estimators are provided in Sec. A. Details of experimental setting is provided in Sec. F. In this experiments, we set all variables other than the treatment variable $X$ as continuous. ", "page_idx": 34}, {"type": "text", "text": "We compare DML-UCA estimator to the regression-based estimator (\u2018reg\u2019) and the ratio-based estimator (\u2018ratio\u2019). In particular, we use $\\hat{\\psi}^{\\mathrm{{est}}}$ for est $\\in\\ \\{\\mathrm{reg},\\mathrm{pw},\\mathrm{dml}\\}$ to denote the regressionbased, probability-weighting, and DML-UCA estimators. We assess the quality of the estimators by computing the average absolute error $\\mathrm{AAE^{est}}$ which is defined as follow. For the ETT and Ctf-DE, $\\mathrm{AAE}^{\\mathrm{est}}:=|\\hat{\\psi}^{\\mathrm{est}}-\\psi_{0}|$ , where $\\psi_{0}:=\\mathbb{E}[Y_{X=0}\\mid X=1]$ for the ETT and $\\psi_{0}:=\\mathbb{E}[Y_{X=0,W_{X=1}}\\mid X=$ 2] for the Ctf-DE. For the other examples, $\\begin{array}{r}{\\mathrm{AAE^{est}}:=\\frac{1}{\\mathrm{domqin}(X)}\\sum_{x\\in\\mathrm{domain}(X)}|\\hat{\\psi}^{\\mathrm{est}}(x)-\\psi_{0}(x)|}\\end{array}$ where $\\psi_{0}(x):=\\mathbb{E}[Y\\mid\\operatorname{do}(x)]$ , $\\hat{\\psi}^{\\mathrm{est}}(x)$ is an estimator for $\\psi_{0}(x)$ and $\\operatorname{dom}(X)$ is a cardinality of the domain of $X$ . Nuisance functions are estimated using XGBoost (Chen and Guestrin, 2016). We ran 100 simulations for each number of samples $n=\\{2500,5000,10000,20000\\}$ and drew the AAE plot. We evaluate the $\\mathrm{AAE^{est}}$ in the presence of the \u2018converging noise $\\epsilon^{\\ '}$ as in Sec. 4. ", "page_idx": 34}, {"type": "text", "text": "Statistical Robustness. The AAE plots for all scenarios are presented in Fig. E.4. For all examples, all the estimators (\u2018reg\u2019, \u2018pw\u2019, \u2018dml\u2019) converge as the sample size grows. Furthermore, the proposed DML-UCA estimator outperforms the other two estimators by achieving fast convergence. This result ", "page_idx": 34}, {"type": "image", "img_path": "aX9z2eT6ul/tmp/b128c41096d05a71ae9403627c93a563a0587c78c988ae53d498bdbefb812e4a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Figure E.4: (a) ETT in Sec. B, (b) Transportability ( $S$ -admissibility) in Sec. B, (c) Front-door in Example 1, (d) Verma in Example 2, (e) Ctf-DE in Example 3. ", "page_idx": 35}, {"type": "text", "text": "corroborates the robustness property in Thm. 4, which implies that DML-UCA converges faster than the other counterparts. ", "page_idx": 35}, {"type": "text", "text": "F Details in Experiments ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "As described in Sec. 4, we used the XGBoost (Chen and Guestrin, 2016) as a model for estimating nuisances. We implemented the model using Python. In modeling nuisance using the XGBoost, we used the command xgboost.XGBClassifier(eval_metri $=\\Rightarrow\\log[0\\,{\\bf s}\\,{\\bf s}^{\\,\\prime}\\,)^{1}$ to use the XGBoost. We tuned the parameters for each examples to empirically guarantee the convergence of the regression and ratio nuisances. For each examples, the same parameters are used globally for implementing DML-UCA, regression-based estimator, ratio-based estimator, or other competing estimators (Fulcher et al., 2019; Jung et al., 2021a). ", "page_idx": 35}, {"type": "text", "text": "Now, we present the structural causal models (SCMs) utilized for generating the dataset. Furthermore, we include a segment of the code employed to generate the dataset. ", "page_idx": 35}, {"type": "text", "text": "F.1 FD (Fig. 1a) for Simulation in Fig. 2a ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "We define the following structural causal models: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U\\sim\\mathrm{normal}(0.5,0,5),}\\\\ &{U_{Z_{i}}\\sim\\mathrm{normal}(0,1),\\mathrm{~for~}i=1,\\cdots,d_{Z}}\\\\ &{\\,\\,\\,C_{i}:=f_{C_{i}}(U),\\mathrm{~where~}\\mathbf{C}:=\\{C_{i}:i=1,\\cdots,d_{C}\\}}\\\\ &{\\,\\,\\,X:=f_{X}(\\mathbf{C},U),}\\\\ &{Z_{i}:=f_{Z_{i}}(\\mathbf{C},X),\\mathrm{~where~}\\mathbf{Z}:=\\{Z_{i}:i=1,\\cdots,d_{Z}\\}}\\\\ &{\\,\\,\\,Y:=f_{Y}(\\mathbf{C},Z,U),}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\;f_{C_{i}}(U):=\\left\\lfloor\\frac{1}{1+\\exp(0.25U_{Z}+2U-1)}\\right\\rfloor,}\\\\ &{\\quad\\;f_{X}(\\mathbf{C},U):=\\mathtt{B i n a r y}\\left(\\frac{1}{1+\\exp(2\\mathbf{C}^{\\top}\\mathbf{1}-1+U)}\\right),}\\\\ &{\\quad\\;f_{Z_{i}}(\\mathbf{C},X):=\\mathtt{B i n a r y}\\left(\\frac{1}{1+\\exp(2X-1+0.5\\mathbf{C}^{\\top}\\mathbf{1}+U_{Z_{i}})}\\right)}\\\\ &{\\quad\\;f_{Y}(\\mathbf{C},Z,U):=\\frac{1}{1+\\exp((1/d_{C})\\mathbf{C}^{\\top}\\mathbf{1}+(1/d_{Z})(2\\mathbf{Z}^{\\top}\\mathbf{1}-1)+2U)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The parameterization for XGBoost used in $\\pmb{\\mu}$ called (mu_params) and $\\pi$ called (pi_params) is the following: ", "page_idx": 35}, {"type": "text", "text": "mu_params = { 'booster': 'gbtree', 'eta': 0.3, 'gamma': 0, 'max_depth': 10, 'min_child_weight': 1, 'subsample': 1.0, 'colsample_bytree': 1, 'lambda': 0.0, 'alpha': 0.0, 'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'n_jobs': 4 } ", "page_idx": 36}, {"type": "text", "text": "pi_params $\\mathbf{\\Sigma}=\\{\\begin{array}{r l}\\end{array}\\}$ 'booster': 'gbtree', 'eta': 0.3, 'gamma': 0, 'max_depth': 10, 'min_child_weight': 1, 'subsample': 0.0, 'colsample_bytree': 1, 'objective': 'binary:logistic', 'eval_metric': 'logloss', 'reg_lambda': 0.0, 'reg_alpha': 0.0, 'nthread': 4 } ", "page_idx": 36}, {"type": "text", "text": "F.2 Verma (Fig. 1b) for Simulation in Fig. 2b ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "We define the following structural causal models: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U_{X B}\\sim\\mathrm{normal}(1,0,5),}\\\\ &{U_{A Y}\\sim\\mathrm{normal}(-1,0,5),}\\\\ &{U_{A}\\sim\\mathrm{normal}(0,1)}\\\\ &{U_{B}\\sim\\mathrm{normal}(0,1)}\\\\ &{\\,\\,\\,\\,X:=f_{X}(U_{X}B)}\\\\ &{\\,\\,\\,\\,A_{i}:=f_{A_{i}}(X,U_{A Y}),\\,\\,\\mathrm{for}\\,\\,i=1,\\cdots,d_{A}}\\\\ &{B_{i}:=f_{B_{i}}(X,U_{X B}),\\,\\,\\mathrm{for}\\,\\,i=1,\\cdots,d_{B}}\\\\ &{\\,\\,\\,\\,Y:=f_{Y}(\\mathbf{B},U_{A Y}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{X}(U_{X}B):=\\mathtt{B i n a r y}\\left(\\frac{1}{1+\\exp(2U_{X B}-1)}\\right),}\\\\ &{f_{A_{i}}(X,U_{A Y}):=\\mathtt{B i n a r y}\\left(\\frac{1}{1+\\exp(2X-1+U_{A}+U_{A Y})}\\right)}\\\\ &{f_{B_{i}}(X,U_{X B}):=\\mathtt{B i n a r y}\\left(\\frac{1}{1+\\exp(2\\mathbf{A}\\tau\\mathbf{1}-1+U_{B}+0.5U_{X B})}\\right)}\\\\ &{f_{Y}(\\mathbf{B},U_{A Y}):=\\frac{1}{1+\\exp(2\\mathbf{B}\\tau\\mathbf{1}-1+0.5U_{A Y})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The parameterization for XGBoost used in $\\pmb{\\mu}$ called (mu_params) and $\\pi$ called (pi_params) is the following: ", "page_idx": 37}, {"type": "text", "text": "mu_params = { 'booster': 'gbtree', 'eta': 0.35, 'gamma': 0, 'max_depth': 6, 'min_child_weight': 1, 'subsample': 1.0, 'colsample_bytree': 1, 'lambda': 0.0, 'alpha': 0.0, 'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'n_jobs': 4 # Assuming you have 4 cores } ", "page_idx": 37}, {"type": "text", "text": "pi_params $\\mathbf{\\Sigma}=\\{\\begin{array}{r l}\\end{array}\\}$ 'booster': 'gbtree', 'eta': 0.1, 'gamma': 0, 'max_depth': 10, 'min_child_weight': 1, 'subsample': 0.0, 'colsample_bytree': 1, 'objective': 'binary:logistic', # Change as per your objective 'eval_metric': 'logloss', # Change as per your needs 'reg_lambda': 0.0, 'reg_alpha': 0.0, 'nthread': 4   \n} ", "page_idx": 37}, {"type": "text", "text": "F.3 Example estimand (Fig. 1e) for Simulation in Fig. 2c ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "We define the following structural causal models: ", "page_idx": 37}, {"type": "text", "text": "$\\begin{array}{r l}&{U_{X_{1},Z}\\sim\\mathrm{normal}(1,0,5),}\\\\ &{U_{X_{1},Y}\\sim\\mathrm{normal}(-1,0,5),}\\\\ &{U_{Z,Y}\\sim\\mathrm{normal}(0.5,0.5)}\\\\ &{U_{R}\\sim\\mathrm{normal}(0,0.5)}\\\\ &{U_{Z}\\sim\\mathrm{normal}(0,0.5)}\\\\ &{U_{X_{2}}\\sim\\mathrm{normal}(0,0.5)}\\\\ &{X_{1}:=f_{X_{1}}(U_{X_{1},Z},U_{X_{1},Y})}\\\\ &{Z_{i}:=f_{Z_{i}}(X_{1},U_{X_{1},Z},U_{Z_{i}})}\\\\ &{R_{i}:=f_{R_{i}}(X_{1}),~\\mathrm{for~}i=1,}\\\\ &{Y:=f_{Y}(\\mathbf{B},U_{X}),}\\end{array}$ , for $i=1,\\cdots,d_{Z}$ ", "page_idx": 37}, {"type": "text", "text": "where ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{X_{1}}(U_{X_{1},Z},U_{X_{1},Y}):=\\mathrm{Binary}\\left(\\frac{1}{1+\\exp(2U_{X_{1},Z}-U_{X_{1},Y}-1)}\\right),}\\\\ &{\\qquad\\qquad\\qquad f_{R_{i}}(X_{1}):=\\mathrm{Binary}\\left(\\frac{1}{1+\\exp(2X_{1}-1+U_{R})}\\right)}\\\\ &{f_{Z_{i}}(X_{1},U_{X_{1},Z},U_{Z,Y}):=\\mathrm{Binary}\\left(\\frac{1}{1+\\exp(4X_{1}-1+U_{Z}+U_{X_{1},Z}+U_{Z,Y})}\\right)}\\\\ &{\\qquad\\qquad\\qquad f_{X_{2}}(\\mathbf{Z},X_{1}):=\\mathrm{Binary}\\left(\\frac{1}{1+\\exp((2X_{1}-1)\\mathbf{Z}\\mathbf{7}\\mathbf{1}-U_{X_{2}})}\\right),}\\\\ &{f_{Y}(\\mathbf{R},X_{2},U_{X_{1},Y},U_{Z,Y}):=\\frac{1}{1+\\exp((1/d R)\\mathbf{R}\\mathbf{\\mathbf{7}}\\mathbf{1}+2X_{2}-1+2(U_{X_{1},Y}+U_{Z,Y}))}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "The parameterization for XGBoost used in $\\pmb{\\mu}$ called (mu_params) and $\\pi$ called (pi_params) is the following: ", "page_idx": 38}, {"type": "text", "text": "mu_params = {   \n'booster': 'gbtree',   \n'eta': 0.3,   \n'gamma': 0,   \n'max_depth': 8,   \n'min_child_weight': 1,   \n'subsample': 0.8,   \n'colsample_bytree': 0.8,   \n'lambda': 0.0,   \n'alpha': 0.0,   \n'objective': 'reg:squarederror',   \n'eval_metric': 'rmse',   \n'n_jobs': 4 # Assuming you have 4 cores   \npi_params $\\mathbf{\\Sigma}=\\{\\begin{array}{r l}\\end{array}\\}$ 'booster': 'gbtree', 'eta': 0.1, 'gamma': 0, 'max_depth': 10, 'min_child_weight': 1, 'subsample': 0.75, 'colsample_bytree': 0.75, 'objective': 'binary:logistic', # Change as per your objective 'eval_metric': 'logloss', # Change as per your needs 'reg_lambda': 0.0, 'reg_alpha': 0.0, 'nthread': 4   \n} ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "F.4 ETT in Sec. B for Simulation in Fig. E.4a ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "We define the following structural causal models: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U_{X}\\sim\\mathrm{normal}(0,1)}\\\\ &{U_{Y}\\sim0.5\\,t e x t t t h o r m a l(0,1)}\\\\ &{\\textbf{Z}\\sim0.25\\mathrm{normal}(0,1,d Z),}\\\\ &{X:=f_{X}(\\mathbf{Z})}\\\\ &{Y:=f_{Y}(X,\\mathbf{Z})}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{X}(\\mathbf{Z}):=\\mathtt{B i n a r y}\\left(\\frac{1}{1+\\exp(2\\mathbf{Z}\\boldsymbol{\\mathsf{T}}\\mathbf{1}-1+U_{X})}\\right)}\\\\ &{f_{Y}(\\mathbf{Z},X):=\\frac{1}{1+\\exp(\\mathbf{Z}^{\\top}\\mathbf{1}(2X-1)+U_{Y})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "The parameterization for XGBoost used in $\\pmb{\\mu}$ called (mu_params) and $\\pi$ called (pi_params) is the following: ", "page_idx": 39}, {"type": "text", "text": "mu_params = { 'booster': 'gbtree', 'eta': 0.5, 'gamma': 0, 'max_depth': 15, 'min_child_weight': 1, 'subsample': 0.8, 'colsample_bytree': 1, 'lambda': 0, 'alpha': 0, 'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'n_jobs': 4 # Assuming you have 4 cores } ", "page_idx": 39}, {"type": "text", "text": "pi_params $\\mathbf{\\Sigma}=\\{\\begin{array}{r l}\\end{array}\\}$ 'booster': 'gbtree', 'eta': 0.3, 'gamma': 0, 'max_depth': 10, 'min_child_weight': 1, 'subsample': 1, 'colsample_bytree': 1, 'objective': 'binary:logistic', # Change as per your objective 'eval_metric': 'logloss', # Change as per your needs 'reg_lambda': 1, 'reg_alpha': 0, 'nthread': 4   \n} ", "page_idx": 39}, {"type": "text", "text": "F.5 Transportability in Sec. B for Simulation in Fig. E.4b ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "We define the following structural causal models: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U_{X}\\sim\\mathrm{norma1}(0,1)}\\\\ &{U_{Y}\\sim0.5\\:t e x t t t n o r m a l(0,1)}\\\\ &{\\;\\;\\;\\mathbf{Z}\\sim0.25\\mathrm{norma1}(0,0.5,d Z)+S\\mathrm{norma1}(0.1,0.5,d Z)}\\\\ &{\\;\\;\\;X:=f_{X}(\\mathbf{Z})}\\\\ &{Y:=f_{Y}(X,\\mathbf{Z})}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\,\\,f_{X}(\\mathbf{Z}):=\\operatorname{Binary}\\left(\\frac{1}{1+\\exp((1/d Z)(2\\mathbf{Z}^{\\intercal}\\mathbf{1}-1)+U_{X})}\\right)}\\\\ &{\\,\\,f_{Y}(\\mathbf{Z},X):=\\frac{1}{1+\\exp(\\mathbf{Z}^{\\intercal}\\mathbf{1}(2X-1)+U_{Y})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "The parameterization for XGBoost used in $\\pmb{\\mu}$ called (mu_params) and $\\pi$ called (pi_params) is the following: ", "page_idx": 40}, {"type": "text", "text": "mu_params = { 'booster': 'gbtree', 'eta': 0.3, 'gamma': 0, 'max_depth': 15, 'min_child_weight': 1, 'subsample': 0.8, 'colsample_bytree': 1, 'lambda': 0, 'alpha': 0, 'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'n_jobs': 4 # Assuming you have 4 cores } ", "page_idx": 40}, {"type": "text", "text": "pi_params $\\mathbf{\\Sigma}=\\{\\begin{array}{r l}\\end{array}\\}$ 'booster': 'gbtree', 'eta': 0.1, 'gamma': 0, 'max_depth': 10, 'min_child_weight': 1, 'subsample': 1, 'colsample_bytree': 1, 'objective': 'binary:logistic', # Change as per your objective 'eval_metric': 'logloss', # Change as per your needs 'reg_lambda': 1, 'reg_alpha': 0, 'nthread': 4   \n} ", "page_idx": 40}, {"type": "text", "text": "F.6 FD with continuous mediators for Simulation in Fig. E.4c ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "We define the following structural causal models: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\bf~J}_{\\mathbf{C}}\\sim\\mathrm{normal}(0,1,d_{C})}\\\\ &{\\mathrm{\\bf~U}\\sim\\mathrm{normal}(0,1)}\\\\ &{\\mathrm{\\bf~C}:=f_{\\mathbf{C}}(U)}\\\\ &{X:=f_{X}(U,\\mathbf{C})}\\\\ &{Z:=f_{Z}(X,\\mathbf{C})}\\\\ &{Y:=f_{Y}(U,Z,\\mathbf{C})}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\~~~~f_{\\mathbf{C}}(U):=0.25\\mathbf{U_{C}}+2U-1}\\\\ &{f_{X}(U,\\mathbf{C}):=\\mathtt{B i n a r y}\\left(\\frac{1}{1+\\exp\\left(\\left(2\\mathbf{C}\\boldsymbol{\\mathsf{T}}\\boldsymbol{\\mathsf{1}}-1\\right)+U\\right)}\\right)}\\\\ &{f_{Z}(X,\\mathbf{C}):=\\frac{1}{1+\\exp\\left(0.1\\mathbf{C}\\boldsymbol{\\mathsf{T}}\\boldsymbol{\\mathsf{1}}(2X-1)+X\\right)}}\\\\ &{f_{Y}(\\mathbf{Z},X):=\\frac{1}{1+\\exp\\left(\\mathbf{C}\\boldsymbol{\\mathsf{T}}\\boldsymbol{\\mathsf{1}}+\\left(2Z-1\\right)+U\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "The parameterization for XGBoost used in $\\pmb{\\mu}$ called (mu_params) and $\\pi$ called (pi_params) is the following: ", "page_idx": 41}, {"type": "text", "text": "mu_params $\\mathbf{\\Sigma}=\\{\\begin{array}{r l}\\end{array}\\}$ 'booster': 'gbtree', 'eta': 0.01, 'gamma': 0, 'max_depth': 10, 'min_child_weight': 1, 'subsample': 1.0, 'colsample_bytree': 1, 'lambda': 0.0, 'alpha': 0.0, 'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'n_jobs': 4 ", "page_idx": 41}, {"type": "text", "text": "pi_params = { 'booster': 'gbtree', 'eta': 0.3, 'gamma': 0, 'max_depth': 20, 'min_child_weight': 1, 'subsample': 0.0, 'colsample_bytree': 1, 'objective': 'binary:logistic', 'eval_metric': 'logloss', 'reg_lambda': 0.0, 'reg_alpha': 0.0, 'nthread': 4   \n} ", "page_idx": 41}, {"type": "text", "text": "F.7 Verma\u2019s equation with continuous mediators for Simulation in Fig. E.4d ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "We define the following structural causal models: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U_{X B}\\sim\\tt n o r m a l(1,0,5),}\\\\ &{U_{A Y}\\sim\\tt n o r m a l(-1,0,5),}\\\\ &{\\quad X:=f_{X}(U_{X}B)}\\\\ &{\\quad A:=f_{A}(X,U_{A Y})}\\\\ &{\\quad B:=f_{B}(X,U_{X B})}\\\\ &{\\quad Y:=f_{Y}(B,U_{A Y}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{X}(U_{X}B):=\\mathrm{Binary}\\left(\\frac{1}{1+\\exp(2U_{X B}-1)}\\right),}\\\\ &{f_{A}(X,U_{A Y}):=\\mathrm{Binary}\\left(\\frac{1}{1+\\exp(2X-1+0.5U_{A}Y)}\\right)}\\\\ &{f_{B}(X,U_{X B}):=\\mathrm{Binary}\\left(\\frac{1}{1+\\exp(2A-1+0.5U_{X B})}\\right)}\\\\ &{f_{Y}(B,U_{A Y}):=\\frac{1}{1+\\exp(2B-1+0.5U_{A}Y)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "The parameterization for XGBoost used in $\\pmb{\\mu}$ called (mu_params) and $\\pi$ called (pi_params) is the following: ", "page_idx": 42}, {"type": "text", "text": "mu_params $\\mathbf{\\Sigma}=\\{\\begin{array}{r l}\\end{array}\\}$ 'booster': 'gbtree', 'eta': 0.35, 'gamma': 0, 'max_depth': 6, 'min_child_weight': 1, 'subsample': 1.0, 'colsample_bytree': 1, 'lambda': 0.0, 'alpha': 0.0, 'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'n_jobs': 4 # Assuming you have 4 cores   \n}   \npi_params $\\mathbf{\\Sigma}=\\{\\begin{array}{r l}\\end{array}\\}$ 'booster': 'gbtree', 'eta': 0.1, 'gamma': 0, 'max_depth': 10, 'min_child_weight': 1, 'subsample': 0.0, 'colsample_bytree': 1, 'objective': 'binary:logistic', # Change as per your objective   \n'eval_metric': 'logloss', # Change as per your needs 'reg_lambda': 0.0, 'reg_alpha': 0.0, 'nthread': 4} ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "F.8 Ctf-DE in Example 3 for Simulation in Fig. E.4e ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "We define the following structural causal models: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U\\sim\\mathrm{normal}(0,2),}\\\\ &{X:=f_{X}(U)}\\\\ &{Z:=f_{Z}(U)}\\\\ &{W:=f_{W}(X,Z)}\\\\ &{Y:=f_{Y}(X,Z,W),}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{X}(U)=\\left\\{\\begin{array}{l l}{\\displaystyle0\\mathrm{~if~}\\frac{1}{1+\\exp(2U_{X B}-1)}<0.5}\\\\ {\\displaystyle1\\mathrm{~if~}\\le0.5\\frac{1}{1+\\exp(2U_{X B}-1)}<0.8}\\\\ {\\displaystyle2\\mathrm{~if~}\\le0.8\\frac{1}{1+\\exp(2U_{X B}-1)}.}\\end{array}\\right.}\\\\ &{\\quad\\quad f_{Z}(U)=\\frac{1}{1+\\exp(-U+1)}}\\\\ &{\\quad\\quad f_{W}(X,Z):=\\frac{1}{1+\\exp(X-1+Z)}}\\\\ &{\\quad\\quad f_{Y}(Z,X,W):=\\frac{1}{1+\\exp(3X-1+0.17+0.1W+W(X-1))}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "The parameterization for XGBoost used in $\\pmb{\\mu}$ called (mu_params) and $\\pi$ called (pi_params) is the following: ", "page_idx": 43}, {"type": "text", "text": "mu_params $\\mathbf{\\Sigma}=\\{\\begin{array}{r l}\\end{array}\\}$ 'booster': 'gbtree', 'eta': 0.3, # vab 'gamma': 0.0, 'max_depth': 6, #vb (same as va) 'min_child_weight': 1, 'subsample': 1.0, 'colsample_bytree': 1, 'lambda': 0.0, 'alpha': 0.0, 'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'n_jobs': 4 # Assuming you have 4 cores   \n}   \npi_params $\\mathbf{\\Sigma}=\\{\\begin{array}{r l}\\end{array}\\}$ 'booster': 'gbtree', 'eta': 0.05, 'gamma': 0, 'max_depth': 10, 'min_child_weight': 1, 'subsample': 1.0, 'colsample_bytree': 1, 'objective': 'multi:softprob', # Change as per your objective 'num_class': 3, 'eval_metric': 'mlogloss', # Change as per your needs 'reg_lambda': 0.0, 'reg_alpha': 0.0, 'nthread': 4   \n} ", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 45}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 45}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 45}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 45}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 45}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 45}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 45}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We clearly state our main claim in the abstract. The contributions are described in the introduction and are clearly reflected throughout the entire paper. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 45}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: We explicitly state the assumptions that theories are depending on. This assumption is serving as limitations of the work. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 46}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: Detailed proofs are provided in the supplemental material. Additionally, we have provided the full set of assumptions for each theorems. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 46}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: We provide the detailed recipe for running the simulation and the guideline for implementing theories. Also, we provide a detailed method on how the synthetic data are generated. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 47}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: Every detail in the simulations is sufficiently provided to reproduce the results. Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 47}, {"type": "text", "text": "", "page_idx": 48}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: We provide the details of simulations enough to reproduce the results. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 48}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: We provide the details of simulations enough to reproduce the results. Also, our simulation includes confidence intervals. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 48}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: We provide the details of simulations enough to reproduce the results. Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 49}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: We conform the NeurIPS Code of Ethics. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 49}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: Our contribution is to make a new causal effect estimator, focusing on theoretic contribution. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ", "page_idx": 49}, {"type": "text", "text": "\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 50}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 50}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 50}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets. ", "page_idx": 51}, {"type": "text", "text": "\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 51}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 51}, {"type": "text", "text": "", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 51}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: [NA] Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 51}]