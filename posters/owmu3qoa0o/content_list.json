[{"type": "text", "text": "Sparse maximal update parameterization: A holistic approach to sparse training dynamics ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Nolan Dey Shane Bergsma Joel Hestness Cerebras Systems {nolan,joel}@cerebras.net ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Several challenges make it difficult for sparse neural networks to compete with dense models. First, setting a large fraction of weights to zero impairs forward and gradient signal propagation. Second, sparse studies often need to test multiple sparsity levels, while also introducing new hyperparameters (HPs), leading to prohibitive tuning costs. Indeed, the standard practice is to re-use the learning HPs originally crafted for dense models. Unfortunately, we show sparse and dense networks do not share the same optimal HPs. Without stable dynamics and effective training recipes, it is costly to test sparsity at scale, which is key to surpassing dense networks and making the business case for sparsity acceleration in hardware. A holistic approach is needed to tackle these challenges and we propose sparse maximal update parameterization $(\\mathrm{S}\\upmu\\mathrm{Par})$ as one such approach. For random unstructured static sparsity, $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ ensures activations, gradients, and weight updates all scale independently of sparsity level. Further, by reparameterizing the HPs, $\\mathsf{S}\\upmu\\mathrm{Par}$ enables the same HP values to be optimal as we vary both sparsity level and model width. HPs can be tuned on small dense networks and transferred to large sparse models, greatly reducing tuning costs. On largescale language modeling, $\\mathrm{S}\\upmu\\mathrm{Par}$ shows increasing improvements over standard parameterization as sparsity increases, leading up to $11.9\\%$ relative loss improvement at $99.2\\%$ sparsity. A minimal implementation of $\\mathrm{S\\upmuPar}$ is available at https://github.com/EleutherAI/nanoGPT-mup/tree/supar. ", "page_idx": 0}, {"type": "text", "text": "1 Intro ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sparsity has emerged as a key technique to mitigate the increasing computational costs of training and inference in deep neural networks. This work focuses on weight sparsity, whereby a significant fraction of model weights are kept at zero. It has long been known that dense neural networks can be heavily pruned after training [30]. With the goal of reducing costs during training, recent work has explored static weight sparsity from initialization. In this work we focus on random unstructured static sparsity, which has re-emerged as a surprisingly effective strategy [33, 58]. This type of sparsity can be accelerated by CPUs, Cerebras, Graphcore, and SambaNova. Furthermore, GPUs and TPUs support 2:4 block structured sparsity which is quite similar to $50\\%$ unstructured sparsity. ", "page_idx": 0}, {"type": "text", "text": "Unfortunately, several challenges have hindered progress in weight-sparse neural networks. First, sparsity impairs signal propagation during training [31, 11, 1]. Second, with today\u2019s techniques, sparse training is costly. Sparse techniques typically introduce extra hyperparameters (HPs), e.g., number of pruning iterations at initialization [60, 7, 56], and it is common to train models across different sparsity levels. Since tuning should be performed at each level and the search space grows exponentially with the number of HPs, the tuning costs essentially \u201cdefeat the purpose\u201d of sparsity, i.e., to reduce computation [60]. Finally, today there is only a nascent ecosystem of hardware acceleration for unstructured sparsity, so most researchers get little sparsity benefit when tuning. ", "page_idx": 0}, {"type": "image", "img_path": "OWmu3QOa0O/tmp/fcc501327c1aab4b9667f10c7b516c28bbc568b06875068e59597d0dbb46026a.jpg", "img_caption": ["Figure $1\\colon\\mathrm{S}\\mu\\mathrm{Par}$ (Our work) allows stable optimum HPs for any sparsity level, unlike standard practice. "], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "OWmu3QOa0O/tmp/cc4c82380c1e666de360a272677f2cd317b01e8c30d02faf23aff97a7e7933e4.jpg", "img_caption": ["Figure 2: $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ enables sparse training at scale, helping to surpass dense and motivate sparsity in hardware. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "These costs have led to the standard practice of simply re-using HPs that were previously optimized for the baseline dense models (Section 2). One might hope that sparse models thrive with the same learning rates and other HPs as their dense counterparts. Unfortunately, they do not: optimal HPs systematically vary with sparsity level (Figure 1, left). With impaired training dynamics, prohibitive tuning cost, and lacking the established training recipes enjoyed by dense models, it is often inefficient to train sparse networks at scale (Figure 2). ", "page_idx": 1}, {"type": "text", "text": "To remedy this situation, we propose sparse maximal update parameterization $\\mathrm{\\DeltaS\\muPar}$ , pronounced \u201csoo-pahr\u201d), a novel, holistic approach to stabilize sparse training dynamics. $\\mathsf{S}\\upmu\\mathrm{Par}$ fulflils the Feature Learning Desiderata (Section 3) by parameterizing weight initialization and learning rates with respect to change in width and sparsity level. As a generalization of maximal update parameterization $(\\upmu\\mathrm{P})$ [64, 63], $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ enjoys well-controlled activation, gradient, and weight update scales in expectation, avoiding exploding or vanishing signal when changing both sparsity and model width. ", "page_idx": 1}, {"type": "text", "text": "By reparameterizing HPs in this way, $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ enables the same HP values to be optimal as sparsity varies (Figure 1, right). We therefore enjoy $\\upmu\\mathrm{T}1$ ransfer: we can tune small proxy models and transfer optimal HPs directly to models at scale. In fact, we discovered our $\\upmu\\mathrm{P}$ HPs, tuned for dense models in prior work (and equivalent to $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ with sparsity ${\\it\\Delta}=0\\%$ ), correspond to the optimal learning rate and initial weight variance for all sparse models tuned in this paper! As sparsity increases, our formulation shows the standard parameterization (SP) and $\\upmu\\mathrm{P}$ suffer from vanishing signal, further clarifying prior observations of gradient flow issues in sparse networks. The improvements enabled by $\\mathrm{S}\\upmu\\mathrm{Par}$ set the Pareto-frontier best loss across sparsity levels. Figure 3 previews this improvement for large language models trained from compute-optimal configurations [23]. Here, $\\mathrm{S\\upmuPar}$ benefits grow with increasing sparsity, to $11.9\\%$ better loss than SP and $1.9\\%$ better loss than $\\upmu\\mathrm{P}$ at $99.2\\%$ random unstructured sparsity. See Section 4.3 for details on this experiment. ", "page_idx": 1}, {"type": "image", "img_path": "OWmu3QOa0O/tmp/08eaa8141dc75f392e6482a9197564d0a9f02457fcc9fe9fa8a1306a1c2f4d4f.jpg", "img_caption": ["Figure 3: For LLMs, $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ forms the Pareto frontier loss across sparsity levels, with no HP tuning required. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Sparse training landscape Sparse training can be divided into static sparsity, where the connectivity is fixed (our focus) and dynamic sparsity, where the sparsity mask can evolve [22]. We use unstructured sparsity, though our approach generalizes to structured approaches where a particular sparsity pattern increases efficiency on specific hardware [67, 26, 38, 14, 29, 1]. Unstructured connectivity may be based on both random pruning [40, 18, 57, 33, 58] and various pruning-at-initialization criteria [32, 60, 61, 56, 7]. Liu et al. [33] found that as models scale, the relative performance of randomly pruned networks grow. Furthermore, Frantar et al. [15] found the optimal level of sparsity increases with the amount of training data. Together, these findings suggest that as neural networks continue to get wider and deeper, and trained on more and more data, very sparse randomly-pruned networks may emerge as an attractive option. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Improving sparse training dynamics Many prior works identify various sparse training dynamics issues. In particular, prior works note sparsity impacts weight initialization [35, 31, 49, 11], activation variance [29], gradient flow [61, 37, 57, 11, 1], and step sizes during weight updates [15]. These prior works each only address a subset of these issues in targeted ways, often showing benefits to sparse model training loss. We advocate for a holistic approach, and discuss the relationship between these prior works and our approach in Section 5 after describing and evaluating $\\mathsf{S}\\upmu\\mathrm{Par}$ . ", "page_idx": 2}, {"type": "text", "text": "Sparse sensitivity to HPs Due to the costs of training with fixed weight sparsity, re-using dense HPs is standard practice. Such re-use is typically indicated in appendices or supplemental materials, e.g., [40, 32, 35, 31, 16, 60, 61, 56, 13, 7, 18, 57, 33, 58]. Also, dynamic sparsity approaches often compare to fixed sparsity; these baselines are likewise reported to re-use the dense HPs [2, 41, 10, 34, 11, 59]. However, some prior work has suggested such training is sensitive to HPs, e.g., learning rates [35, 57], learning rate schedules [16], or training length [28], although systematic tuning was not performed. For dynamic sparse training (DST), it is also conventional to re-use dense HPs, whether in dense-to-sparse [37, 15] or sparse-to-sparse (evolving mask) training [2, 8, 34, 11, 59]. As with fixed sparsity, work here has also suggested sensitivity to HPs, e.g., to dropout and label smoothing [16]. DST may also benefti from extra training steps [10] or smaller batch sizes [34], although in DST this may mainly be due to a greater number of opportunities for connectivity exploration [34]. ", "page_idx": 2}, {"type": "text", "text": "3 Sparse maximal update parameterization $(\\mathbf{S}\\mathbf{\\upmu}\\mathbf{P}\\mathbf{ar})$ ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We now provide background, motivation, and derivation for $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ , first introducing notation (Section 3.1) and then defining Feature Learning Desiderata (Section 3.2) with a brief overview of $\\upmu\\mathrm{P}$ (Section 3.3). Finally we motivate $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ and provide an overview of the parameterization (Section 3.4). ", "page_idx": 2}, {"type": "text", "text": "3.1 Notation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The operations for a single sparse training step are illustrated in Figure 4. The definition and dimensions are: batch size $B$ , learning rate $\\eta$ , loss function $\\mathcal{L}$ , forward pass function $\\mathcal{F}$ , input dimension $d_{\\mathrm{in}}$ , input activations $\\mathbf{X}\\in\\mathbb{R}^{B\\times d_{\\mathrm{in}}}$ , input activation gradient $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{X}}=\\mathbf{\\bar{V}}_{\\mathbf{X}}\\mathcal{L}\\in\\mathbb{R}^{B\\times d_{\\mathrm{in}}}}\\end{array}$ , output dimension $d_{\\mathrm{out}}$ , output activations $\\mathbf{Y}\\in\\mathbb{R}^{B\\times d_{\\mathrm{out}}}$ , output activation gradient $\\begin{array}{r}{\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{Y}}=\\nabla_{\\mathbf{Y}}\\mathcal{L}\\in\\mathbb{R}^{B\\times d_{\\mathrm{out}}}}\\end{array}$ , weights $\\mathbf{W}\\in\\mathbb{R}^{d_{\\mathrm{in}}\\times d_{\\mathrm{out}}}$ , initialization variance $\\sigma_{W}$ for weights $\\mathbf{W}$ , weight update $\\Delta\\mathbf{W}\\in\\mathbb{R}^{d_{\\mathrm{in}}\\times d_{\\mathrm{out}}}$ , and $\\mathrm{\\Delta}\\setminus\\nabla\\in\\mathbb{R}^{B\\times d_{\\mathrm{out}}}$ is the effect of the weight update on output activations: $\\mathbf{\\bar{\\Delta}}\\mathbf{Y}=\\mathbf{X}(\\Delta\\mathbf{W}\\odot\\mathbf{M})$ . Unless otherwise specified, $\\mathbf{M}\\in\\{0,1\\}^{d_{\\mathrm{in}}\\times d_{\\mathrm{out}}}$ is an unstructured random static mask with sparsity $s$ and density $\\rho=1-s$ . When changing model scale or sparsity, we refer to a width multiplier $\\begin{array}{r}{m_{d}=\\frac{d_{\\mathrm{in}}}{d_{\\mathrm{in,\\,base}}}=\\frac{d_{\\mathrm{out}}}{d_{\\mathrm{out,\\,base}}}}\\end{array}$ ddout  and density multiplier m\u03c1 = ", "page_idx": 2}, {"type": "image", "img_path": "OWmu3QOa0O/tmp/babaa8139aa524db3ef6829a503ffbbed45da33d5116eee859f2d01d8ebf1c87.jpg", "img_caption": ["Figure 4: The three operations associated with training a layer with weights that perform the function $\\mathcal{F}$ : Forward activation calculation, backward gradient propagation, and the weight update. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "If we apply sparsity to a linear layer (i.e., $\\mathcal{F}$ is a fully-connected layer), our aim is to control: ", "page_idx": 2}, {"type": "text", "text": "1. Forward pass: $\\mathbf{Y}={\\mathcal{F}}(\\mathbf{X},\\mathbf{W}\\odot\\mathbf{M})=\\mathbf{X}(\\mathbf{W}\\odot\\mathbf{M}).$ ", "page_idx": 2}, {"type": "text", "text": "2. Backward pass: $\\nabla_{\\mathbf{X}}\\mathcal{L}=(\\nabla_{\\mathbf{Y}}\\mathcal{L})\\cdot(\\mathbf{W}\\odot\\mathbf{M})^{\\top}$ .   \n3. Effect of weight update \u2206W on Y: $\\Delta\\mathbf{Y}=\\mathbf{X}(\\Delta\\mathbf{W}\\odot\\mathbf{M})^{1}$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 Feature learning: Defining the goal of $\\upmu\\mathrm{P}$ and $\\bf{S}\\mu\\bf{P}a r$ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Prior works [64, 63, 65] introduce the Feature Learning Desiderata (FLD) to ensure stable training dynamics as width is varied. Building on prior works, we include gradients $\\nabla_{\\mathbf{X}}\\mathcal{L}$ in the desiderata. ", "page_idx": 3}, {"type": "text", "text": "Fea\u221ature Learning Deside\u221arata (FLD): For lay\u221aer $l$ and token $i$ , we desire that $\\overline{{{\\|\\mathbf{Y}_{i}^{l}\\|_{2}\\;\\;=}}}$ $\\begin{array}{r}{\\big|\\,\\Theta(\\sqrt{d_{\\mathrm{out}}}),\\|\\nabla_{\\mathbf{X}}\\mathcal{L}_{i}^{l}\\|_{2}=\\Theta(\\sqrt{d_{\\mathrm{in}}}),\\|\\Delta\\mathbf{Y}_{i}^{l}\\|_{2}=\\Theta(\\sqrt{d_{\\mathrm{out}}}),\\forall i,\\forall l.}\\end{array}$ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Recall that if all the entries of some vector ${\\bf v}\\in\\mathbb{R}^{n}$ are some constant $c$ , then $\\|\\mathbf{v}\\|_{2}=\\Theta({\\sqrt{n}})$ with respect to width $n$ . Therefore we can satisfy the FLD by ensuring the typical element size of $\\mathbf{Y}$ , $\\nabla_{\\mathbf{X}}\\mathcal{L}$ , and $\\Delta\\mathbf Y$ is $\\Theta(1)$ with respect to some variable(s) we would like to scale. Variables to scale include width [64, 63, 65], depth [66, 4], and sparsity (this work). The FLD prescribes a holistic signal propagation approach of controlling each of the three operations in a training step, not a subset2. ", "page_idx": 3}, {"type": "text", "text": "3.3 Maximal update parameterization $(\\upmu\\mathrm{P})$ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Here we provide a brief overview of maximal update parameterization $(\\upmu\\mathrm{P})$ [64, 63, 65]. With the standard parameterization (SP), Yang and Hu [64] show the scale of activations throughout training increases as model width increases, motivating the development of $\\upmu\\mathrm{P}$ . $\\upmu\\mathrm{P}$ [64, 63] is defined as the unique parameterization that satisfies the FLD by ensuring the typical element size of Y, $\\nabla_{\\mathbf{X}}\\mathcal{L}$ , and $\\Delta\\mathbf Y$ is $\\Theta(1)$ with respect to change in width $m_{d}$ . The FLD can also be satisfied by controlling the spectral norm of weights [65]. $\\upmu\\mathrm{P}$ enables $\\upmu\\mathrm{T}$ ransfer: the optimum learning rate, initialization weight variance, scalar multipliers, and learning rate schedule all remain consistent as width is increased for $\\upmu\\mathrm{P}$ models [63]. $\\upmu^{\\prime}$ Transfer can be leveraged to take a tune small, train large approach where hyperparameters are extensively tuned for a small model then transferred, enabling reduced tuning budgets and superior tuning for large models compared to standard practice. ", "page_idx": 3}, {"type": "text", "text": "3.4 Sparse maximal update parameterization $(\\mathbf{S}\\upmu\\mathbf{P}\\mathbf{ar})$ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Yang et al. [63] show activation magnitudes explode with increasing model width. In Figure 5 we show sparsity has the opposite effect: increasing sparsity causes shrinking activation magnitudes. ", "page_idx": 3}, {"type": "image", "img_path": "OWmu3QOa0O/tmp/a41858ec33d640f0bca03e9720f8734fbb366f5ae8f78796352712393c601bbc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 5: Mean absolute value activations for attention and feed forward blocks after training step $t$ (10 seeds). In SP and $\\upmu\\mathrm{P}$ models, decreasing density causes activations to vanish (note axes on log-scale). In $\\mathsf{S}\\upmu\\mathrm{Par}$ models, density has little effect on activation scales and there is no vanishing. ", "page_idx": 3}, {"type": "text", "text": "$\\mathrm{{S}\\mu\\mathrm{{Par}}}$ is defined as the unique parameterization that satisfies the FLD by ensuring the typical element size of $\\mathbf{Y}$ , $\\nabla_{\\mathbf{X}}\\mathcal{L}$ , and $\\Delta\\mathbf Y$ is $\\Theta(1)$ with respect to change in width $m_{d}$ and change in density $m_{\\rho}$ . $\\mathsf{S}\\upmu\\mathrm{Par}$ enables stable activation scales across sparsity levels (Figure 5, right). In this section, we walk through the changes required to control each of the three operations in a sparse training step, providing an overview of the $\\mathrm{S\\upmuPar}$ derivation. We focus on the AdamW [36] optimizer used in our experiments. For a more detailed derivation, including both SGD and Adam, see Appendix D. ", "page_idx": 4}, {"type": "text", "text": "Forward pass at initialization To ensure the typical element size of $\\mathbf{Y}$ is $\\Theta(1)$ with respect to change in width $m_{d_{\\mathrm{in}}}$ and change in density $m_{\\rho}$ , we can control the mean and variance of $\\mathbf{Y}_{i j}$ . Since at initialization $\\mathbb{E}[\\ddot{\\mathbf{W}}]=0,\\mathbb{E}[\\bar{\\mathbf{Y}}]=0$ , and $\\mathbf{W}\\perp\\mathbf{Y}$ , the mean is controlled. The variance of $\\mathbf{Y}_{i j}$ can be written as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname{Var}(\\mathbf{Y}_{i j})=m_{d_{\\mathrm{in}}}d_{\\mathrm{in,base}}m_{\\rho}\\rho_{\\mathrm{base}}\\sigma_{W}^{2}(\\operatorname{Var}(\\mathbf{X})+\\mathbb{E}[\\mathbf{X}]^{2})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To ensure $\\operatorname{Var}(\\mathbf{Y}_{i j})$ scales independent of $m_{d_{\\mathrm{in}}}$ and $m_{\\rho}$ , we choose $\\begin{array}{r}{\\sigma_{\\mathbf{W}}^{2}=\\frac{\\sigma_{\\mathbf{W},b a s e}^{2}}{m_{d_{\\mathrm{in}}}m_{\\rho}}}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "Backward gradient pass at initialization To ensure the typical element size of $\\nabla_{\\mathbf{X}}\\mathcal{L}$ is $\\Theta(1)$ with respect to change in width $m_{d_{\\mathrm{out}}}$ and change in density $m_{\\rho}$ , we can control the mean and variance of $\\nabla_{\\mathbf{X}}\\mathcal{L}$ . Since at initialization $\\mathbb{E}[\\mathbf{W}]=0$ , $\\mathbb{E}[\\nabla_{\\mathbf{X}}\\mathcal{L}]=0$ and the mean is controlled3. The variance of $\\nabla_{\\mathbf{X}}\\mathcal{L}_{i j}$ can be written as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{Var}(\\nabla_{\\mathbf{X}}\\mathcal{L}_{i j})=m_{d_{\\mathrm{out}}}d_{\\mathrm{out,base}}m_{\\rho}\\rho_{\\mathrm{base}}\\sigma_{\\mathbf{W}}^{2}\\mathrm{Var}(\\nabla_{\\mathbf{Y}}\\mathcal{L})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To ensure Var(\u2207XLij) scales independent of mdout and m\u03c1, we choose \u03c32W = \u03c3mWd,bamse\u03c1 . Typically $m_{d_{\\mathrm{out}}}=m_{d_{\\mathrm{in}}}$ , allowing the same $\\sigma_{\\mathbf{W}}^{2}$ to control both forward and backward scales. ", "page_idx": 4}, {"type": "text", "text": "Effect of Adam weight update $\\Delta\\mathbf{W}$ on $\\mathbf{Y}$ To ensure the typical element size of $\\Delta\\mathbf Y$ is $\\Theta(1)$ with respect to change in width $m_{d_{\\mathrm{out}}}$ and change in density $m_{\\rho}$ . By the law of large numbers, the expected size of each element can be written as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\Delta\\mathbf{Y}_{i j}]\\rightarrow\\eta m_{d_{\\mathrm{in}}}d_{\\mathrm{in},\\mathrm{base}}m_{\\rho}\\rho_{\\mathrm{base}}\\mathbb{E}\\left[\\mathbf{X}_{i k}\\left(\\frac{\\sum_{t}^{T}\\gamma_{t}\\sum_{b}^{B}\\mathbf{X}_{b k}^{t}\\nabla_{\\mathbf{Y}}\\mathcal{L}_{b j}^{t}}{\\sqrt{\\sum_{t}^{T}\\omega_{t}\\sum_{b}^{B}(\\mathbf{X}_{b k}^{t}\\nabla_{\\mathbf{Y}}\\mathcal{L}_{b j}^{t})^{2}}}\\right)\\right],\\;\\mathrm{as}\\;(d_{\\mathrm{in}}\\rho)\\rightarrow\\infty\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To ensure $\\Delta\\mathbf{Y}_{i j}$ and $\\|\\Delta\\mathbf{Y}\\|_{F}$ are scale invariant to $m_{d_{\\mathrm{in}}},m_{\\rho}$ , we choose $\\begin{array}{r}{\\eta=\\frac{\\eta_{\\mathrm{base}}}{m_{d_{\\mathrm{in}}}m_{\\rho}}}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "Implementation summary Table 1 summarizes the differences between SP, $\\upmu\\mathrm{P}$ , and $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ . Since we only sparsify hidden weights, $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ matches $\\upmu\\mathrm{P}$ for input, output, bias, layer-norm, and attention logits. Also note width multipliers $m_{d}$ and density multipliers $m_{\\rho}$ are usually the same for all layers, allowing simplified notation. This correction is equivalent to $\\upmu\\mathrm{P}$ [63] when $\\rho=1$ and $m_{\\rho}=1$ . The correction to hidden weight initialization we derive is similar to the sparsity-aware initialization in prior work [35, 49, 11]. $\\mathsf{S}\\upmu\\mathrm{Par}$ should also easily extend to 2:4 sparsity patterns because, in expectation, the rows and columns of $M^{l}$ should have equal density. A minimal implementation of $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ is available at https://github.com/EleutherAI/nanoGPT-mup/tree/supar. ", "page_idx": 4}, {"type": "text", "text": "4 $\\bf{S}\\mu\\bf{P}a r$ Training Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Here, we present empirical results showing the effectiveness of $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ over SP and $\\upmu\\mathrm{P}$ when training sparse models. When using SP or $\\upmu\\mathrm{P}$ , optimal HPs drift as we change the sparsity level, possibly leading to inconclusive or even reversed findings. $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ has stable optimal HPs across both model width and sparsity level, and we show it improves over SP and $\\upmu\\mathrm{P}$ across different scaling approaches. Taken together, we see that $\\mathsf{S}\\upmu\\mathrm{Par}$ sets the Pareto frontier best loss across all sparsities and widths, including when we scale to a large dense model with width equal to GPT-3 XL [5]. Optimal dense $\\upmu\\mathrm{P}$ HPs\u2014when adjusted using S\u00b5Par\u2014are also optimal HPs for all sparse models that we test here. All tests in this section use GPT-like transformer language models [48, 9], trained on the SlimPajama dataset [54] with a 2048 token context length. We apply random unstructured static sparsity to all projection weights in attention and feedforward blocks while keeping embedding, layer normalization, and bias parameters dense. We refer the reader to Appendix E for full methodology of all experiments. ", "page_idx": 4}, {"type": "table", "img_path": "OWmu3QOa0O/tmp/c33af8ebcc1e5b66226e84705a7633de8e0e02b367e1963011a27a35635d210f.jpg", "table_caption": ["Table 1: Summary of $S\\mathrm{P},\\upmu\\mathrm{P},$ and $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ implementations. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "4.1 Sparse hyperparameter transfer ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We first show sparsifying a dense model using either SP or $\\upmu\\mathrm{P}$ leads to significant drift in optimal HPs as the sparsity level changes. Figure 6 shows train loss for SP, $\\upmu\\mathrm{P}$ , and $\\mathrm{S}\\upmu\\mathrm{Par}$ models when trained with varying sparsity levels and sweeping across different peak learning rates. For the SP configuration, as sparsity increases, the optimal learning rate increases in a somewhat unpredictable way. $\\upmu\\mathrm{P}$ experiences similar shift in optimal learning rate, though shifts are even more abrupt. For $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ , the optimal learning rate is consistently near $\\!{\\overline{{2}}}^{-6}$ across all sparsity levels. ", "page_idx": 5}, {"type": "image", "img_path": "OWmu3QOa0O/tmp/fe91c80b7578b4f66bfddbaf73db6102e66abfd6aff6bf89cc8be71b4eb07f58.jpg", "img_caption": ["Figure 6: $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ ensures stable optimal learning rate for any sparsity $s$ , unlike SP and $\\upmu\\mathrm{P}$ (3 seeds). ", "Finding 2: With SP and \u00b5P, dense and sparse networks do not share the same optimal HPs. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "We also sweep base weight initialization values and find even more chaotic behaviors for SP and $\\upmu\\mathrm{P}$ with different sparsity levels (Figure 7, left and center, respectively)4. $\\upmu\\mathrm{P}$ even shows discontinuities in optimal initialization values at different sparsity levels. We attribute this discontinuity to widely varying expected activation scales between embedding and transformer decoder layers, where embedding activation scales will tend to dominate for high sparsity levels. $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ shows consistent optimal initialization (right plot). Figures 6 and 7 demonstrate our second finding. ", "page_idx": 5}, {"type": "text", "text": "Figure 8 summarizes our HP transfer tests, showing loss for each parameterization across all sparsities. Even when selecting the best learning rate at each sparsity level for SP and $\\upmu\\mathrm{P}$ , $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ (largely) forms the Pareto frontier with an average gap of $0.8\\%$ better than SP and $2.1\\%$ better than $\\upmu\\mathrm{P}$ . ", "page_idx": 5}, {"type": "image", "img_path": "OWmu3QOa0O/tmp/f8b405316c89e396d0a2ec03e66341e7871e8c068dc4380bf1964d6ec11b0ac1.jpg", "img_caption": ["Figure 7: Across sparsity $s$ , SP and $\\upmu\\mathrm{P}$ show unstable optimal initialization. $\\mathsf{S}\\upmu\\mathrm{Par}$ is stable (3 seeds). "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "OWmu3QOa0O/tmp/29dfe04345b2a29f7b48d1d0c686f5a6b86a133c8525170345e88bb07e00a340.jpg", "img_caption": ["Figure 9: $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ ensures stable optimal learning rate in Iso-Parameter sparse $^+$ wide scaling (3 seeds). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.2 Studying $\\mathbf{S}\\mathbf{\\upmu}\\mathbf{P}\\mathbf{ar}$ indicates how some sparse scaling techniques appear to work ", "page_idx": 6}, {"type": "text", "text": "So far, we see $\\mathsf{S}\\upmu\\mathrm{Par}$ can transfer optimal HPs across sparsity levels, but we have also designed it to transfer HPs across different model widths (hidden sizes), similar to $\\upmu\\mathrm{P}$ . Here, we further demonstrate that $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ transfers optimal HPs across width. More generally, sparse scaling that keeps a fixed number of non-zero weights per neuron allows SP and $\\upmu\\mathrm{P}$ to also transfer HPs. ", "page_idx": 6}, {"type": "text", "text": "Figure 9 shows learning rate transfer tests when changing both the model\u2019s hidden size, $d_{\\mathrm{model}}$ , and sparsity level in a common scaling approach called Iso-Parameter scaling [18, 10, 59]. Iso-Parameter scaling keeps the model\u2019s number of non-zero parameters approximately the same, as width and sparsity are varied5. Here, we see the common result that SP models starting from dense HPs do tend to significantly improve as we increase width and sparsity. Note, though, the optimal learning rate for each sparsity level still shifts. When we correct dense HPs using $\\upmu\\mathrm{P}$ or $\\mathsf{S}\\upmu\\mathrm{Par}$ , the dense baseline significantly improves, but only $\\mathsf{S}\\upmu\\mathrm{Par}$ shows consistent loss improvement and stable HPs. ", "page_idx": 6}, {"type": "image", "img_path": "OWmu3QOa0O/tmp/ea471da43aa168acb877ff434ad1e85aff538faf69d21517d22b3a5d70bdce4e.jpg", "img_caption": ["Figure 8: Summarizing loss results from Figure 6 with the optimal learning rate for each parameterization and sparsity. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Based on the $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ formulation: When the number of non-zero weights per neuron (WPN) in the network is the same, $\\upmu\\mathrm{P}$ and $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ become synonymous, because initialization and learning rate adjustment factors will be constant (i.e., $d_{\\mathrm{model}}\\cdot\\rho=\\mathbf{WPN}=O(1);$ . Optimized SP HPs will also tend to work well. We define this new scaling setting, which we call Iso-WPN, to verify this hypothesis. In Figure 11, we test SP HPs with Iso-WPN scaling and see the optimal learning rate stays consistently between $2^{-7}$ and $2^{-6}$ with roughly aligned curves (we omit similar $\\upmu\\mathrm{P}$ and $\\mathsf{S}\\upmu\\mathrm{Par}$ plots for space, because their corrections are the same). The conclusion is that when scaling SP models in an Iso-WPN sparse setting, HPs should maintain similar training dynamics. More generally, as WPN decreases (e.g., by increasing sparsity), the optimal learning rate will tend to increase proportionally, and vice versa6. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "OWmu3QOa0O/tmp/9d88d624aa42ecdbadc25a6713cfd8632707690bea358e3e6f1fb604b70fafe8.jpg", "img_caption": ["Figure 10: Losses at the end of training when Iso-Parameter scaling. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "OWmu3QOa0O/tmp/3f3f3bf3a281aeb6489be4e388ea3378676db24057222c00586609e7ab9531ff.jpg", "img_caption": ["Figure 11: The SP optimized LR is relatively stable with iso-WPN scaling (3 seeds). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figures 5, 6, 7, and 9 show $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ is the only parameterization that ensures stable activation scales and stable optimal HPs across model widths and sparsities, satisfying the FLD. ", "page_idx": 7}, {"type": "text", "text": "Finding 3: S\u00b5Par enables stable activation and stable optimal HPs for any width and sparsity. ", "page_idx": 7}, {"type": "text", "text": "4.3 $\\mathbf{S}\\mathbf{\\upmu}\\mathbf{P}\\mathbf{ar}$ scaling to large language model pretraining ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conclude this section reflecting on the demonstration of $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ improvements in a large-scale language model. We train 610M parameter models starting from a Chinchilla [23] compute-optimal training configuration with 20 tokens per parameter from the SlimPajama dataset. This larger model\u2014 with hidden size 2048, 10 layers, and attention head size 64\u2014permits sweeping over a larger range of sparsity levels, so we test up to $99.2\\%$ sparsity (density $2^{-7}$ ). ", "page_idx": 7}, {"type": "text", "text": "Figure 3 shows validation loss for each parameterization as we sweep sparsity levels. Additionally, in Table 2, we evaluate the models from Figure 3 on five downstream tasks: ARC-easy, lambada, RACE, PIQA, and BoolQ, which collectively test for common sense reasoning, world knowledge, and reading comprehension. As sparsity increases, results across pretraining loss and average downstream task accuracy consistently show SP and $\\upmu\\mathrm{P}$ fall farther behind $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ . Since these models are trained with a large number of tokens, we attribute the widening loss gap mostly to increasingly under-tuned learning rates for SP and $\\upmu\\mathrm{P}$ as sparsity increases\u2013the weight updates lose gradient information throughout training. Figure 8 shows retuning SP and $\\upmu\\mathrm{P}$ could recover some of the gap to $\\mathsf{S}\\upmu\\mathrm{Par}$ , but that could be costly: These runs take 3-6 hours on a Cerebras CS-3 system (or $>9$ days on an NVIDIA A100 GPU). ", "page_idx": 7}, {"type": "text", "text": "Finally, returning to the Iso-Parameter scaling setting, Figure 10 shows losses for 111M parameter models trained on 1B tokens and scaled up while using dense optimal HPs. The SP and $\\upmu\\mathrm{P}$ models experience detuning as sparsity increases, allowing $\\mathsf{S}\\upmu\\mathrm{Par}$ to achieve superior losses7. ", "page_idx": 7}, {"type": "text", "text": "Finding 4: Sparse networks trained with S\u00b5Par improve over SP and \u00b5P due to improved tuning. ", "page_idx": 7}, {"type": "text", "text": "4.4 Dynamic sparsity hyperparameter transfer ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In Figure 12 we test the transfer of optimal learning rate across sparsity levels for two popular dynamic sparse training methods: Rigging the Lottery (RigL) $[10]^{8}$ and Gradual Magnitude Pruning (GMP) $[68]^{9}$ . We show that none of SP, $\\upmu\\mathrm{P}$ , or $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ achieve transfer of optimal learning rate across sparsity levels. For SP and $\\upmu\\mathrm{P}$ we see that higher sparsity levels have higher optimal learning rates. ", "page_idx": 7}, {"type": "text", "text": "This is because sparsity reduces activation and gradient scales such that a larger learning rate is needed to counteract this. $\\mathsf{S}\\upmu\\mathrm{Par}$ sees the opposite trend where higher sparsity levels have lower optimal learning rates, indicating that $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ is \u201covercorrecting\u201d. ", "page_idx": 8}, {"type": "text", "text": "Dynamic sparse methods can make updates to the weight mask such that the distribution of unmasked/non-zero weights changes to something non-Gaussian, which prevents $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ from being correct in expectation. Compared to random pruning, a mask obtained from magnitude pruning will better preserve the size of activations and gradients seen in the dense network. Since $\\mathsf{S}\\upmu\\mathrm{Par}$ assumes weights are drawn from a Gaussian distribution, $\\mathsf{S}\\upmu\\mathrm{Par}$ ends up \u201covercorrecting\u201d the initialization and learning rate. In future work it would be impactful to develop a parameterization which generalizes $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ to work for an arbitrary sparse training algorithm. ", "page_idx": 8}, {"type": "image", "img_path": "OWmu3QOa0O/tmp/4f003185594ade5fb84177454bc9b60fe1f6f19d9f87b71d046376a6271f40b9.jpg", "img_caption": ["Figure 12: For dynamic sparse training methods RigL and GMP, none of SP, $\\upmu\\mathrm{P}.$ , or $\\mathrm{S\\upmuPar}$ achieve stable optimal learning rate across sparsity (3 seeds). Missing points indicate diverged training runs. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To improve sparse training, prior works make targeted corrections which arise from observations that sparsity can cause degraded activation, gradient, and/or weight update signal propagation. We review these observations and corrections to advocate for holistic control of sparse training dynamics. ", "page_idx": 8}, {"type": "text", "text": "Sparsifying Can Cause Vanishing Activations Evci et al. [11] note that by initializing weights using dense methods (e.g., [17, 21]), the \u201cvast majority\u201d of sparse networks have vanishing activations. Lasby et al. [29, App. A] analyze activation variance as a guide for selecting structured sparsity. The FLD suggest activation norms be measured and controlled with respect to sparsity, so activation variance can be considered a proxy to whether sparsity might negatively impact training dynamics. Evci et al. [11] ultimately initialize variances via neuron-specific sparse connectivity, while Liu et al. [35] and Ramanujan et al. [49] propose scaling weight variances proportional to layer sparsity. These corrections, however, only target controlling activations but not weight updates. ", "page_idx": 8}, {"type": "text", "text": "Gradient Flow Partially Measures the Weight Update \u00b5Desideratum Sparsity also impairs gradient flow\u2014the magnitude of the gradient to the weights\u2014during training [11, 1]. Since gradient flow is measured using the norm of the weight gradients, it measures a piece of the weight update. Unfortunately, gradient flow does not directly measure the effect of the weight update step, which can also involve adjustments for things like optimizer state (e.g., momentum and velocity), the learning rate, and weight decay. Prior works propose techniques to improve gradient flow during sparse training and pruning by adjusting individual hyperparameters or adding normalization [61, 37, 11, 1]. However, these techniques might overlook the effects of the optimizer and learning rates in weight updates. Notably, Tessera et al. [57] do consider some of these effects, but their proposed techniques maintain gradient flow only in the Iso-Parameter scaling setting rather than arbitrary sparsification. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Frantar et al. [15, App. A.1] also endeavor to control weight updates, where they observe diminished step sizes when optimizing sparse networks with Adafactor [52]. They correct this by computing Adafactor\u2019s root-mean-square scaling adjustments over unpruned weights and updates. However, such normalization does not prevent activations from scaling with model width [63, 65]. In this sense, sparsity-aware fixes to Adafactor can improve dynamics, but will not address instability holistically. In Figure 14 we show the $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ LR correction alone is not even sufficient to achieve stable optimal $\\eta$ . ", "page_idx": 9}, {"type": "text", "text": "Weight Initialization Only Controls Dynamics at Initialization We noted works above that adjust sparse weight initializations [11, 35, 49]. Additionally, Lee et al. [31] explore orthogonal weight initialization [46], both before pruning (to ensure SNIP [32] pruning scores are on a similar scale across layers) and after (to improve trainability of the sparse network). While adjusting weights can improve sparse training dynamics at initialization, such adjustments are insufficient to stabilize signals after multiple steps of training, in the same way that standard weight initializations fail to stabilize training of dense networks. In Figure 13 we show the $\\mathrm{S}\\upmu\\mathrm{Par}$ init. alone is not even sufficient to achieve stable optimal $\\sigma_{W}$ . ", "page_idx": 9}, {"type": "text", "text": "6 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "As Section 4.4 shows, $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ requires further extension for dynamic sparse training due to unpredictable changes in weight distributions. The same applies to methods which prune at initialization or after pretraining in a non-random fashion. Iterative magnitude pruning (IMP) is an interesting case since it involves rewinding weights back to their initial values while maintaining the same mask [12]. If the IMP mask at initialization still allows the non-zero weights to have a Gaussian distribution, then $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ would apply to this case. Therefore, it\u2019s possible $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ could prevent \u201cHP detuning\u201d in later IMP iterations, and potentially improve IMP losses, though we do not explore this. $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ would also work for random structured pruning of entire neurons at initialization because this case simply reduces to training with a narrower dense model. ", "page_idx": 9}, {"type": "text", "text": "For weight sparsity more generally, the most pressing limitation is the lack of hardware acceleration [38]. While new software [50, 29, 43] continues to better leverage existing hardware, the growth of software and hardware co-design is also encouraging [59, 6], as effective sparsity techniques can be specifically optimized in deep learning hardware. But to effectively plan hardware, we need to train and test sparse prototypes at next-level sizes, at scales where the optimum sparsity level may be higher than in current networks [15]. Performing such scaling law-style studies requires incredible resources even for dense models with well-established training recipes [27, 23]. As $\\mathsf{S}\\upmu\\mathrm{Par}$ reduces training and tuning costs, it can help unlock these studies and guide future hardware design. ", "page_idx": 9}, {"type": "text", "text": "Finally, the scaling factors for the weight update need to be derived for each optimizer, which might limit the usability of $\\mathsf{S}\\upmu\\mathrm{Par}$ in practice. For a discussion of the broader impacts of $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ , see Appendix A. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Nobody said training with sparsity was easy. We showed that with the standard parameterization and $\\upmu\\mathrm{P}$ , increasing sparsity level directly correlates with vanishing activations. Impaired training dynamics prevent sparse models from sharing the same optimal hyperparameters, suggesting prior results based on re-use of dense HPs merit re-examination. In contrast, by holistically controlling the training process, $\\mathrm{S\\upmuPar}$ prevents vanishing activations and enables HP transfer (across both width and sparsity). LLMs trained with $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ improve over $\\upmu\\mathrm{P}$ and the standard parameterization. As such, we hope $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ makes things a little easier for sparsity research going forward. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We would like to thank Gavia Gray, who provided helpful feedback on the manuscript, and Gurpreet Gosal, who tuned the $\\upmu$ Transferred hyperparameters seen throughout the document. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Abhimanyu Rajeshkumar Bambhaniya, Amir Yazdanbakhsh, Suvinay Subramanian, ShengChun Kao, Shivani Agrawal, Utku Evci, and Tushar Krishna. 2024. Progressive Gradient Flow for Robust N:M Sparsity Training in Transformers. arXiv preprint arXiv:2402.04744 (2024).   \n[2] Guillaume Bellec, David Kappel, Wolfgang Maass, and Robert Legenstein. 2017. Deep rewiring: Training very sparse deep networks. arXiv preprint arXiv:1711.05136 (2017).   \n[3] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big?. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. 610\u2013623.   \n[4] Blake Bordelon, Lorenzo Noci, Mufan Bill Li, Boris Hanin, and Cengiz Pehlevan. 2024. Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit. In The Twelfth International Conference on Learning Representations. https://openreview. net/forum?id=KZJehvRKGD   \n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in Neural Information Processing Systems 33 (2020), 1877\u20131901.   \n[6] Cerebras Systems. 2024. Train a Model with Weight Sparsity. Cerebras Systems Documentation. https://docs.cerebras.net/en/2.1.1/wsc/how_to_guides/sparsity.html Version 2.1.1.   \n[7] Pau de Jorge, Amartya Sanyal, Harkirat S Behl, Philip HS Torr, Gregory Rogez, and Puneet K Dokania. 2020. Progressive skeletonization: Trimming more fat from a network at initialization. arXiv preprint arXiv:2006.09081 (2020).   \n[8] Tim Dettmers and Luke Zettlemoyer. 2019. Sparse networks from scratch: Faster training without losing performance. arXiv preprint arXiv:1907.04840 (2019).   \n[9] Nolan Dey, Daria Soboleva, Faisal Al-Khateeb, Bowen Yang, Ribhu Pathria, Hemant Khachane, Shaheer Muhammad, Zhiming Chen, Robert Myers, Jacob Robert Steeves, Natalia Vassilieva, Marvin Tom, and Joel Hestness. 2023. BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model. arXiv:2309.11568 [cs.AI]   \n[10] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. 2020. Rigging the lottery: Making all tickets winners. In International conference on machine learning. PMLR, 2943\u20132952.   \n[11] Utku Evci, Yani Ioannou, Cem Keskin, and Yann Dauphin. 2022. Gradient flow in sparse neural networks and how lottery tickets win. In Proceedings of the AAAI conference on artificial intelligence, Vol. 36. 6577\u20136586.   \n[12] Jonathan Frankle and Michael Carbin. 2018. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635 (2018).   \n[13] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. 2020. Pruning neural networks at initialization: Why are we missing the mark? arXiv preprint arXiv:2009.08576 (2020).   \n[14] Elias Frantar and Dan Alistarh. 2023. SparseGPT: Massive language models can be accurately pruned in one-shot. In International Conference on Machine Learning. 10323\u201310337.   \n[15] Elias Frantar, Carlos Riquelme, Neil Houlsby, Dan Alistarh, and Utku Evci. 2023. Scaling laws for sparsely-connected foundation models. arXiv preprint arXiv:2309.08520 (2023).   \n[16] Trevor Gale, Erich Elsen, and Sara Hooker. 2019. The state of sparsity in deep neural networks. arXiv preprint arXiv:1902.09574 (2019).   \n[17] Xavier Glorot and Yoshua Bengio. 2010. Understanding the Difficulty of Training Deep Feedforward Neural Networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (PMLR).   \n[18] Anna Golubeva, Behnam Neyshabur, and Guy Gur-Ari. 2020. Are wider nets better given the same number of parameters? arXiv preprint arXiv:2010.14495 (2020).   \n[19] Hila Gonen and Yoav Goldberg. 2019. Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them. arXiv preprint arXiv:1903.03862 (2019).   \n[20] Yiwen Guo, Chao Zhang, Changshui Zhang, and Yurong Chen. 2018. Sparse dnns with improved adversarial robustness. Advances in neural information processing systems 31 (2018).   \n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification. In Proceedings of the IEEE international conference on computer vision. 1026\u20131034.   \n[22] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. 2021. Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. Journal of Machine Learning Research 22, 241 (2021), 1\u2013124.   \n[23] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack William Rae, and Laurent Sifre. 2022. An Empirical Analysis of Compute-optimal Large Language Model Training. In The Conference on Neural Information Processing Systems (NeurIPS).   \n[24] Sara Hooker, Aaron Courville, Gregory Clark, Yann Dauphin, and Andrea Frome. 2019. What do compressed deep neural networks forget? arXiv preprint arXiv:1911.05248 (2019).   \n[25] Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio, and Emily Denton. 2020. Characterising bias in compressed models. arXiv preprint arXiv:2010.03058 (2020).   \n[26] Hyeong-Ju Kang. 2019. Accelerator-aware pruning for convolutional neural networks. IEEE Transactions on Circuits and Systems for Video Technology 30, 7 (2019), 2093\u20132103.   \n[27] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling Laws for Neural Language Models. arXiv:2001.08361 [cs.LG] https://arxiv.org/abs/2001.08361   \n[28] Denis Kuznedelev, Eldar Kurtic, Eugenia Iofinova, Elias Frantar, Alexandra Peste, and Dan Alistarh. 2023. Accurate neural network pruning requires rethinking sparse optimization. arXiv preprint arXiv:2308.02060 (2023).   \n[29] Mike Lasby, Anna Golubeva, Utku Evci, Mihai Nica, and Yani Ioannou. 2023. Dynamic Sparse Training with Structured Sparsity. arXiv preprint arXiv:2305.02299 (2023).   \n[30] Yann LeCun, John Denker, and Sara Solla. 1989. Optimal brain damage. Advances in Neural Information Processing Systems 2 (1989).   \n[31] Namhoon Lee, Thalaiyasingam Ajanthan, Stephen Gould, and Philip HS Torr. 2019. A signal propagation perspective for pruning neural networks at initialization. arXiv preprint arXiv:1906.06307 (2019).   \n[32] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. 2018. SNIP: Single-shot network pruning based on connection sensitivity. arXiv preprint arXiv:1810.02340 (2018).   \n[33] Shiwei Liu, Tianlong Chen, Xiaohan Chen, Li Shen, Decebal Constantin Mocanu, and Mykola Wang, Zhangyang an d Pechenizkiy. 2022. The unreasonable effectiveness of random pruning: Return of the most naive baseline for sparse training. arXiv preprint arXiv:2202.02643 (2022).   \n[34] Shiwei Liu, Lu Yin, Decebal Constantin Mocanu, and Mykola Pechenizkiy. 2021. Do we actually need dense over-parameterization? In-time over-parameterization in sparse training. In International Conference on Machine Learning. PMLR, 6989\u20137000.   \n[35] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. 2018. Rethinking the value of network pruning. arXiv preprint arXiv:1810.05270 (2018).   \n[36] Ilya Loshchilov and Frank Hutter. 2017. Decoupled Weight Decay Regularization. In International Conference on Learning Representations.   \n[37] Ekdeep Singh Lubana and Robert P Dick. 2020. A gradient flow framework for analyzing network pruning. arXiv preprint arXiv:2009.11839 (2020).   \n[38] Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu, and Paulius Micikevicius. 2021. Accelerating sparse deep neural networks. arXiv preprint arXiv:2104.08378 (2021).   \n[39] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019. Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency. 220\u2013229.   \n[40] Decebal Constantin Mocanu, Elena Mocanu, Phuong H Nguyen, Madeleine Gibescu, and Antonio Liotta. 2016. A topological insight into restricted Boltzmann machines. Machine Learning 104 (2016), 243\u2013270.   \n[41] Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu, and Antonio Liotta. 2018. Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science. Nature communications 9, 1 (2018), 2383.   \n[42] Moin Nadeem, Anna Bethke, and Siva Reddy. 2020. StereoSet: Measuring stereotypical bias in pretrained language models. arXiv preprint arXiv:2004.09456 (2020).   \n[43] Neural Magic. 2024. DeepSparse. GitHub repository. https://github.com/neuralmagic/ deepsparse   \n[44] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training Language Models to Follow Instructions With Human Feedback. arXiv:2203.02155 [cs.CL] https://arxiv.org/abs/2203.02155   \n[45] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350 (2021).   \n[46] Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. 2017. Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. Advances in neural information processing systems 30 (2017).   \n[47] Ofir Press, Noah Smith, and Mike Lewis. 2022. Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation. In International Conference on Learning Representations.   \n[48] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners. https://d4mucfpksywv. cloudfront.net/better-language-models/language-models.pdf   \n[49] Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, and Mohammad Rastegari. 2020. What\u2019s hidden in a randomly weighted neural network?. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 11893\u201311902.   \n[50] Erik Schultheis and Rohit Babbar. 2023. Towards Memory-Efficient Training for Extremely Large Output Spaces \u2013 Learning with 670k Labels on a Single Commodity GPU. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, 689\u2013704.   \n[51] Noam Shazeer. 2020. GLU Variants Improve Transformer. arXiv:2002.05202 [cs.LG] https: //arxiv.org/abs/2002.05202   \n[52] Noam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning. PMLR, 4596\u20134604.   \n[53] Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. 2019. The woman worked as a babysitter: On biases in language generation. arXiv preprint arXiv:1909.01326 (2019).   \n[54] Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, and Nolan Dey. 2023. SlimPajama: A 627B token cleaned and deduplicated version of RedPajama. https://www.cerebras.net/blog/ slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama. https://huggingface.co/datasets/cerebras/SlimPajama-627B   \n[55] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019).   \n[56] Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli. 2020. Pruning neural networks without any data by iteratively conserving synaptic flow. Advances in neural information processing systems 33 (2020), 6377\u20136389.   \n[57] Kale-ab Tessera, Sara Hooker, and Benjamin Rosman. 2021. Keep the gradients flowing: Using gradient flow to study sparse network optimization. arXiv preprint arXiv:2102.01670 (2021).   \n[58] Vithursan Thangarasa, Abhay Gupta, William Marshall, Tianda Li, Kevin Leong, Dennis DeCoste, Sean Lie, and Shreyas Saxena. 2023. SPDF: Sparse pre-training and dense fine-tuning for large language models. In Uncertainty in Artificial Intelligence. 2134\u20132146.   \n[59] Vithursan Thangarasa, Shreyas Saxena, Abhay Gupta, and Sean Lie. 2023. Sparse-IFT: Sparse Iso-FLOP transformations for maximizing training efficiency. arXiv preprint arXiv:2303.11525 (2023).   \n[60] Stijn Verdenius, Maarten Stol, and Patrick Forr\u00e9. 2020. Pruning via iterative ranking of sensitivity statistics. arXiv preprint arXiv:2006.00896 (2020).   \n[61] Chaoqi Wang, Guodong Zhang, and Roger Grosse. 2020. Picking winning tickets before training by preserving gradient flow. arXiv preprint arXiv:2002.07376 (2020).   \n[62] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. 2023. Jailbroken: How does LLM safety training fail? Advances in Neural Information Processing Systems 36 (2023).   \n[63] Greg Yang, Edward Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. 2021. Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer. In Advances in Neural Information Processing Systems.   \n[64] Greg Yang and Edward J Hu. 2020. Feature learning in infinite-width neural networks. arXiv preprint arXiv:2011.14522 (2020).   \n[65] Greg Yang, James B Simon, and Jeremy Bernstein. 2023. A spectral condition for feature learning. arXiv preprint arXiv:2310.17813 (2023).   \n[66] Greg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou. 2023. Feature Learning in Infinite Depth Neural Networks. In International Conference on Learning Representations.   \n[67] Zhuliang Yao, Shijie Cao, Wencong Xiao, Chen Zhang, and Lanshun Nie. 2019. Balanced sparsity for efficient DNN inference on GPU. In Proceedings of the AAAI conference on artificial intelligence, Vol. 33. 5676\u20135683.   \n[68] Michael Zhu and Suyog Gupta. 2017. To prune, or not to prune: exploring the efficacy of pruning for model compression. arXiv preprint arXiv:1710.01878 (2017). ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Broader impacts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Sparsity is recognized to reduce carbon emissions [45] and offset well-known environmental and financial costs of large model training [3]. For example, unstructured sparsity can be accelerated by the Cerebras Wafer-Scale Engine10 and 2:4 block sparsity can be accelerated by NVIDIA Ampere GPUs11. There is growing recognition that HP tuning is a key contributor to these costs. HP tuning is costly, possibly undermining equity in AI research due to financial resources [55]. During model retraining, sensitivity to HPs also leads to downstream costs [55]. $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ can reduce these costs and sensitivities and thus improve equity. ", "page_idx": 14}, {"type": "text", "text": "Sparsity also has potential drawbacks. Hooker et al. [24] showed that even when top-line performance metrics are comparable, pruned networks may perform worse on specific subsets of the data (including on underrepresented groups [25]), may amplify sensitivity to adversarial examples, and may be more sensitive to distribution shift. These sensitivities may depend on the degree of sparsity [20]. It remains an open question whether such drawbacks occur only with pruning or when training with sparsity from scratch (as in $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ ) [22], and how such sensitivity may impact susceptibility to misuse [62]. We require sparsity-specific methods to detect [53, 42] and mitigate [19, 44] harm. Moreover, since many large models are later pruned for deployment, we recommend testing and documenting in the model card [39] any adverse affects of sparsification at the time of model release. ", "page_idx": 14}, {"type": "text", "text": "B Downstream task comparison of parameterizations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In Table 2, we evaluate the models from Figure 3 on five downstream tasks: ARC-easy, lambada, RACE, PIQA, and BoolQ, which collectively test for common sense reasoning, world knowledge, and reading comprehension. We also specifically chose tasks that are easy enough for even extremely sparse models to significantly outperform random chance. ", "page_idx": 14}, {"type": "table", "img_path": "OWmu3QOa0O/tmp/2bcb0915586b3324ae3730905f9a8860109b2265a711984ac981263c65508783.jpg", "table_caption": [], "table_footnote": ["Table 2: Downstream evaluation accuracy; higher is better: $\\overline{{\\mathrm{\\DeltaS\\upmuPar}}}$ performs best or within 0.01 of best across all sparsity levels and tasks, except boolq at $50\\%$ and $75\\%$ sparsity. Even at $99\\%$ sparsity, $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ models maintain $40\\%+$ average accuracy, whereas the SP model drops to $34\\%$ , close to the $30\\%$ accuracy of the random baseline. "], "page_idx": 14}, {"type": "text", "text": "C Individual ablations of $\\mathbf{S}\\mathbf{\\upmu}\\mathbf{P}\\mathbf{a}\\mathbf{r}$ initialization and learning rate corrections ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In Figures 13 and 14, we individually ablate the effect of the $\\mathsf{S}\\upmu\\mathrm{Par}$ initialization and the $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ learning rate. We show that using only the $\\mathrm{S\\upmuPar}$ initialization in conjunction with $\\upmu\\mathrm{P}$ $\\mathrm{P\\left(\\upmu\\mathrm{P}+\\mathrm{S}\\upmu\\mathrm{Par}\\right.}$ initialization only) does not allow for transfer of optimal initialization standard deviation or optimal learning rate across sparsity levels. We also show that using only the $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ learning rate in conjunction with $\\upmu\\mathrm{P}$ does not achieve transfer either. Therefore, both the $\\mathsf{S}\\upmu\\mathrm{Par}$ initialization and learning rate corrections are required to achieve optimal hyperparameter transfer across sparsity levels. ", "page_idx": 14}, {"type": "image", "img_path": "OWmu3QOa0O/tmp/57f6ec68a72a42da87d3a9c6609c776b864ae27f388a2ec377b4ee3f0978bce9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 13: $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ ensures stable optimal weight initialization standard deviation, unlike SP, $\\upmu\\mathrm{P}$ , $\\upmu\\mathrm{P}+$ $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ init only, and $\\upmu\\mathrm{P}+\\mathrm{S}\\upmu\\mathrm{Par}$ LR only. ", "page_idx": 15}, {"type": "image", "img_path": "OWmu3QOa0O/tmp/4ba9d62a0ce718783dcb0a8eaa276ea9cc9a156f63ecd26970bedafffa741ad4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 14: $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ ensures stable optimal learning rate (Bottom), unlike SP, $\\upmu\\mathrm{P}$ , $\\upmu\\mathrm{P}+\\mathrm{S}\\upmu\\mathrm{Par}$ init only, and $\\upmu\\mathrm{P}+\\mathrm{S}\\upmu\\mathrm{Par}$ LR only. ", "page_idx": 15}, {"type": "text", "text": "D $\\bf{S}\\mu\\bf{P}a r$ detailed derivation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "D.1 Forward pass at initialization ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The first stage where we would like to control training dynamics is in the layer\u2019s forward function. For a random unstructured sparsity mask $\\mathbf{M}$ , since each column of $\\mathbf{M}$ has $d_{\\mathrm{in}}\\rho$ non-zero elements in expectation, we can rewrite the forward pass as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{Y}_{i j}=\\left[\\mathbf{X}(\\mathbf{W}\\odot\\mathbf{M})\\right]_{i j}=\\sum_{q=1}^{d_{\\mathrm{in}}}\\mathbf{X}_{i q}(\\mathbf{W}_{q j}\\cdot\\mathbf{M}_{q j})=\\sum_{k:\\mathbf{M}_{k j}=1}^{d_{\\mathrm{in}}\\rho}\\mathbf{X}_{i k}\\mathbf{W}_{k j}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To satisfy the FLD, we desire the typical element size of $\\mathbf{Y}$ is $\\Theta(1)$ with respect to change in width $m_{d_{\\mathrm{in}}}$ and change in density $m_{\\rho}$ . To achieve this we can ensure the mean and variance of $\\mathbf{Y}_{i j}$ are invariant to $m_{d_{\\mathrm{in}}}$ and $m_{\\rho}$ . ", "page_idx": 15}, {"type": "text", "text": "Mean: As expectation is linear and $\\mathbf{X}$ and $\\mathbf{W}$ are independent at initialization: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{Y}_{i j}]=\\mathbb{E}\\left[\\sum_{k:\\mathbf{M}_{k j}=1}^{d_{\\mathrm{in}}\\rho}\\mathbf{Y}_{i k}\\mathbf{W}_{k j}\\right]=\\sum_{k:\\mathbf{M}_{k j}=1}^{d_{\\mathrm{in}}\\rho}\\mathbb{E}[\\mathbf{X}_{i k}\\mathbf{W}_{k j}]=\\sum_{k:\\mathbf{M}_{k j}=1}^{d_{\\mathrm{in}}\\rho}\\mathbb{E}[\\mathbf{X}_{i k}]\\mathbb{E}[\\mathbf{W}_{k j}]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, since at initialization $\\mathbb{E}[\\mathbf{W}_{i j}]=0,\\mathbb{E}[\\mathbf{Y}_{i j}]=0$ and the mean is controlled. ", "page_idx": 15}, {"type": "text", "text": "Variance: As expectation is linear and each weight element is IID: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname{Var}(\\mathbf{Y}_{i j})=\\operatorname{Var}\\left(\\sum_{k:\\mathbf{M}_{k j}=1}^{d_{\\mathrm{in}}\\rho}\\mathbf{X}_{i k}\\mathbf{W}_{k j}\\right)=\\sum_{k:\\mathbf{M}_{k j}=1}^{d_{\\mathrm{in}}\\rho}\\operatorname{Var}(\\mathbf{X}_{i k}\\mathbf{W}_{k j})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, since $\\mathbf{X}$ and $\\mathbf{W}$ are independent at initialization: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{Var}(\\mathbf{Y}_{i j})=\\sum_{k:\\mathbf{M}_{k j}=1}^{d_{\\mathrm{in}}\\rho}(\\mathrm{Var}(\\mathbf{X}_{i k})+\\mathbb{E}[\\mathbf{X}_{i k}]^{2})(\\mathrm{Var}(\\mathbf{W}_{k j})+\\mathbb{E}[\\mathbf{W}_{k j}]^{2})-(\\mathbb{E}[\\mathbf{X}_{i k}]\\mathbb{E}[\\mathbf{W}_{k j}])^{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Finally, since at initialization $\\mathbb{E}[\\mathbf{W}_{k j}]=0$ and redefining $\\operatorname{Var}(\\mathbf{W}_{k j})=\\sigma_{\\mathbf{W}}^{2}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{Var}(\\mathbf{Y}_{i j})=\\sum_{k:\\mathbf{M}_{k j}=1}^{d_{\\mathrm{in}}\\rho}(\\mathrm{Var}(\\mathbf{X}_{i k})+\\mathbb{E}[\\mathbf{X}_{i k}]^{2})\\mathrm{Var}(\\mathbf{W}_{k j})=d_{\\mathrm{in}}\\rho\\sigma_{\\mathbf{W}}^{2}(\\mathrm{Var}(\\mathbf{X})+\\mathbb{E}[\\mathbf{X}]^{2})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Rewriting in terms of multipliers for the width mdin = dind, ibnase and the change in density $\\begin{array}{r}{m_{\\rho}=\\frac{\\rho}{\\rho_{\\mathrm{basc}}}}\\end{array}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname{Var}(\\mathbf{Y}_{i j})=m_{d_{\\mathrm{in}}}d_{\\mathrm{in},\\,\\mathrm{base}}m_{\\rho}\\rho_{\\mathrm{base}}\\sigma_{\\mathbf{W}}^{2}(\\operatorname{Var}(\\mathbf{X})+\\mathbb{E}[\\mathbf{X}]^{2})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Solution: To satisfy the FLD and ensure $\\operatorname{Var}(\\mathbf{Y}_{i j})$ scales independently of $m_{d_{\\mathrm{in}}}$ and $m_{\\rho}$ , we choose to set \u03c32W = $\\begin{array}{r}{\\sigma_{\\mathbf{W}}^{2}=\\frac{\\sigma_{\\mathbf{W},b a s e}^{2}}{m_{d_{\\mathrm{in}}}m_{\\rho}}}\\end{array}$ \u03c3m2Wdi,nbams\u03c1e . This ensures typical entry size of Y is invariant to changes in width mdin and density m\u03c1. ", "page_idx": 16}, {"type": "text", "text": "Note that this correction is equivalent to $\\upmu\\mathrm{P}$ [63] when $m_{\\rho}=1$ . Further, the sparsity factor in the denominator matches the correction for sparsity-aware initialization from Evci et al. [11]. ", "page_idx": 16}, {"type": "text", "text": "D.2 Backward gradient pass at initialization ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The next stage we would like to control training dynamics is in the layer\u2019s backward pass. For a random unstructured sparsity mask $\\mathbf{M}$ , since each row of $\\mathbf{M}$ has $d_{\\mathrm{out}}\\rho$ non-zero elements in expectation, we can rewrite the backward pass as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{X}}\\mathcal{L}_{i j}=\\left[\\nabla_{\\mathbf{Y}}\\mathcal{L}(\\mathbf{W}\\odot\\mathbf{M})^{\\top}\\right]_{i j}=\\sum_{q}^{d_{\\mathrm{out}}}\\nabla_{\\mathbf{Y}}\\mathcal{L}_{i q}(\\mathbf{W}_{j q}\\cdot\\mathbf{M}_{j q})=\\sum_{k:\\mathbf{M}_{j k}=1}^{d_{\\mathrm{out}}}\\nabla_{\\mathbf{Y}}\\mathcal{L}_{i k}\\mathbf{W}_{j k}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To satisfy the FLD, we desire the typical element size of $\\nabla_{\\mathbf{X}}\\mathcal{L}$ is $\\Theta(1)$ with respect to change in width $m_{d_{\\mathrm{out}}}$ and change in density $m_{\\rho}$ . To achieve this, we can ensure the mean and variance of $\\nabla_{\\mathbf{X}}\\mathcal{L}$ are invariant to $m_{d_{\\mathrm{out}}}$ and $m_{\\rho}$ . ", "page_idx": 16}, {"type": "text", "text": "Mean: Although the gradients $\\nabla\\mathbf{v}\\mathcal{L}$ will have some correlation with weights W even at initialization, we assume for simplicity that they are fully independent. Future work could investigate this assumption more deeply. As expectation is linear: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\xi[\\nabla_{\\mathbf{X}}\\mathcal{L}_{i j}]=\\mathbb{E}\\left[\\sum_{k:\\mathbf{M}_{j k}=1}^{d_{\\mathrm{ou}}\\rho}\\nabla_{\\mathbf{Y}}\\mathcal{L}_{i k}\\mathbf{W}_{j k}\\right]=\\sum_{k:\\mathbf{M}_{j k}=1}^{d_{\\mathrm{ou}}\\rho}\\mathbb{E}[\\nabla_{\\mathbf{Y}}\\mathcal{L}_{i k}\\mathbf{W}_{j k}]=\\sum_{k:\\mathbf{M}_{j k}=1}^{d_{\\mathrm{ou}}\\rho}\\mathbb{E}[\\nabla_{\\mathbf{Y}}\\mathcal{L}_{i k}]\\mathbb{E}[\\mathbf{W}_{j k}]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, since at initialization $\\mathbb{E}[\\mathbf{W}_{j k}]=0,\\mathbb{E}[\\nabla_{\\mathbf{X}}\\mathcal{L}_{i j}]=0$ , the mean is controlled. ", "page_idx": 16}, {"type": "text", "text": "Variance: As expectation is linear and each weight element is IID: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{Var}(\\nabla_{\\mathbf{X}}\\mathcal{L}_{i j})=\\mathrm{Var}\\left(\\sum_{k:\\mathbf{M}_{j k}=1}^{d_{\\mathrm{out}}\\rho}\\nabla_{\\mathbf{Y}}\\mathcal{L}_{i k}\\mathbf{W}_{j k}\\right)=\\sum_{k:\\mathbf{M}_{j k}=1}^{d_{\\mathrm{out}}\\rho}\\mathrm{Var}(\\nabla_{\\mathbf{Y}}\\mathcal{L}_{i k}\\mathbf{W}_{j k})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "From the backward pass mean derivation, we know $\\mathbb{E}[\\nabla_{\\mathbf{Y}}\\mathcal{L}_{i j}]=0$ . Then, similar to the forward pass variance derivation, we can simplify using the facts that at initialization, $\\nabla_{\\mathbf{Y}}\\mathcal{L}$ and $\\mathbf{W}$ are (roughly) independent and $\\mathbb{E}[\\mathbf{W}]=0$ . Similarly we can also define $\\operatorname{Var}(\\mathbf{W}_{k j}^{l})=\\sigma_{\\mathbf{W}}^{2}$ and rewrite in terms of width multiplier mdout = doduto,ubtase and changes in density $\\begin{array}{r}{m_{\\rho}=\\frac{\\rho}{\\rho_{\\mathrm{basc}}}}\\end{array}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{Var}(\\nabla_{\\mathbf{X}}\\mathcal{L}_{i j})=m_{d_{\\mathrm{out}}}d_{\\mathrm{out,base}}m_{\\rho}\\rho_{\\mathrm{base}}\\sigma_{\\mathbf{W}}^{2}\\mathrm{Var}(\\nabla_{\\mathbf{Y}}\\mathcal{L})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Solution: To satisfy the FLD and ensure $\\mathrm{Var}(\\nabla_{\\mathbf{x}}\\mathcal{L}_{i j})$ scales independently of $m_{d_{\\mathrm{out}}}$ and $m_{\\rho}$ , we choose to set \u03c32W $\\begin{array}{r}{\\sigma_{\\mathbf{W}}^{2}=\\frac{\\sigma_{\\mathbf{W},\\mathrm{base}}^{2}}{m_{d_{000}}m_{\\rho}}}\\end{array}$ m\u03c3dW,bamse\u03c1 . This ensures the typical entry size of \u2207XL is invariant to changes in width $m_{d_{\\mathrm{out}}}$ and density $m_{\\rho}$ . Typically, we scale model width such that $m_{d_{\\mathrm{out}}}\\,=\\,m_{d_{\\mathrm{in}}}$ . This equal scaling allows the same initialization variance to correct both forward activation and backward gradient scales, making them independent of width. Further, since we assume random sparsity across layer\u2019s weights, the sparsity initialization correction factor, $m_{\\rho}$ , is the same for both the forward activations and backward gradients. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "D.3 Effect of weight update $\\Delta\\mathbf{W}$ on Y ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To satisfy the FLD, we desire the typical element size of the weight update $\\Delta\\mathbf Y$ is $\\Theta(1)$ with respect to change in width $m_{d_{\\mathrm{in}}}$ and change in density $m_{\\rho}$ . To achieve this we examine the expected size of each element. Here, we use $\\eta$ to be the learning rate for this layer. For a random unstructured sparsity mask $\\mathbf{M}$ , since each column of M has $d_{\\mathrm{in}}\\rho$ non-zero elements in expectation: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Delta\\mathbf{Y}_{i j}=[\\eta\\mathbf{X}(\\Delta\\mathbf{W}\\odot\\mathbf{M})]_{i j}=\\eta\\sum_{q=1}^{d_{\\mathrm{in}}}\\mathbf{X}_{i q}(\\Delta\\mathbf{W}_{q j}\\cdot\\mathbf{M}_{q j})=\\eta\\sum_{k:\\mathbf{M}_{k j}=1}^{d_{\\mathrm{in}}\\rho}\\mathbf{X}_{i k}\\Delta\\mathbf{W}_{k j}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Mean: As expectation in linear: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\Delta\\mathbf{Y}_{i j}]=\\mathbb{E}\\left[\\eta\\sum_{k:\\mathbf{M}_{k j}=1}^{d_{\\mathrm{in}}\\rho}\\mathbf{X}_{i k}\\Delta\\mathbf{W}_{k j}\\right]=\\eta\\sum_{k:\\mathbf{M}_{k j}=1}^{d_{\\mathrm{in}}\\rho}\\mathbb{E}[\\mathbf{X}_{i k}\\Delta\\mathbf{W}_{k j}]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $\\Delta\\mathbf{W}$ was derived from $\\mathbf{X}$ , there is covariance between these variables and $\\mathbb{E}[\\mathbf{X}_{i k}\\Delta\\mathbf{W}_{k j}]$ is non-zero. By the Law of Large Numbers: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\Delta\\mathbf{Y}_{i j}]\\rightarrow\\eta d_{\\mathrm{in}}\\rho\\mathbb{E}\\left[\\mathbf{X}_{i k}\\Delta\\mathbf{W}\\right],\\;\\mathrm{as}\\left(d_{\\mathrm{in}}\\rho\\right)\\rightarrow\\infty\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Rewriting in terms of width and density multipliers: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\Delta\\mathbf{Y}_{i j}]\\rightarrow\\eta m_{d_{\\mathrm{in}}}d_{\\mathrm{in,base}}m_{\\rho}\\rho_{\\mathrm{base}}\\mathbb{E}\\left[\\mathbf{X}_{i k}\\Delta\\mathbf{W}\\right],\\;\\mathrm{as}\\;(d_{\\mathrm{in}}\\rho)\\rightarrow\\infty\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Equation 17 will be used as intermediate result in the following sections. ", "page_idx": 17}, {"type": "text", "text": "D.3.1 Effect SGD weight update \u2206W on Y ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Following the formulation in [63], stochastic gradient descent (SGD) weight updates take the form: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Delta{\\bf W}_{k j}^{l}=\\left[\\frac{({\\bf X})^{\\top}(\\nabla_{\\bf Y}\\mathcal{L})}{d_{\\mathrm{in}}}\\right]_{k j}=\\frac{1}{d_{\\mathrm{in}}}\\sum_{b=1}^{B}{\\bf X}_{b k}(\\nabla_{\\bf Y}\\mathcal{L})_{b j}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "so we can rewrite Equation 17 as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\Delta\\mathbf{Y}_{i j}]\\rightarrow\\eta m_{\\rho}\\rho_{\\mathrm{base}}\\mathbb{E}\\left[\\mathbf{X}_{i k}\\sum_{b=1}^{B}\\mathbf{X}_{b k}(\\nabla_{\\mathbf{Y}}\\mathcal{L})_{b j}\\right],\\;\\mathrm{as}\\;(d_{\\mathrm{in}}\\rho)\\rightarrow\\infty\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Solution: For SGD updates, to satisfy the FLD and ensure $\\mathbb{E}[\\Delta\\mathbf{Y}_{i j}]$ and the typical entry size of $\\Delta\\mathbf Y$ are scale invariant to $m_{d}$ and $m_{\\rho}$ , we choose $\\eta=\\eta_{\\mathrm{base}}/m_{\\rho}$ . Note this correction is equivalent to $\\upmu\\mathrm{P}$ [63] when $\\rho=1,m_{\\rho}=1$ . ", "page_idx": 17}, {"type": "text", "text": "D.3.2 Effect of Adam weight update $\\Delta\\mathbf{W}$ on $\\mathbf{Y}$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Following the formulation in Yang et al. [63], Adam weight updates take the form: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Delta{\\mathbf{W}}_{k j}=\\frac{\\sum_{t}^{T}\\gamma_{t}\\sum_{b}^{B}{\\mathbf{X}}_{b k}^{l,t}(\\nabla_{\\mathbf{Y}}\\mathcal{L})_{b j}^{t}}{\\sqrt{\\sum_{t}^{T}\\omega_{t}\\sum_{b}^{B}(\\mathbf{X}_{b k}^{t}(\\nabla_{\\mathbf{Y}}\\mathcal{L})_{b j}^{t})^{2}}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $T$ is the current training step and $\\gamma_{t},\\omega_{t}$ are the moving average weights at each training step. Here, we can just consider the weight update associated with an unpruned weight, since a pruned ", "page_idx": 17}, {"type": "text", "text": "weight will have value and update 0 (i.e., pruned weights trivially satisfy that their effect on forward activations cannot depend on width or sparsity). We can rewrite Equation 17 as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\Delta\\mathbf{Y}_{i j}]\\rightarrow\\eta m_{d_{\\mathrm{in}}}d_{\\mathrm{in},\\mathrm{base}}m_{\\rho}\\rho_{\\mathrm{base}}\\mathbb{E}\\left[\\mathbf{X}_{i k}\\left(\\frac{\\sum_{t}^{T}\\gamma_{t}\\sum_{b}^{B}\\mathbf{X}_{b k}^{t}\\nabla_{\\mathbf{Y}}\\mathcal{L}_{b j}^{t}}{\\sqrt{\\sum_{t}^{T}\\omega_{t}\\sum_{b}^{B}(\\mathbf{X}_{b k}^{t}\\nabla_{\\mathbf{Y}}\\mathcal{L}_{b j}^{t})^{2}}}\\right)\\right],\\;\\mathrm{as}\\;(d_{\\mathrm{in}}\\rho)\\rightarrow\\infty\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Solution: For Adam updates, to satisfy the FLD and ensure $\\mathbb{E}[\\Delta\\mathbf{Y}_{i j}]$ and the typical entry size of $\\Delta\\mathbf Y$ are scale invariant to $m_{d_{\\mathrm{in}}}$ and $m_{\\rho}$ , we choose $\\begin{array}{r}{\\eta=\\frac{\\eta_{\\mathrm{base}}}{m_{d_{\\mathrm{in}}}m_{\\rho}}}\\end{array}$ . Note that this correction is equivalent to $\\upmu\\mathrm{P}$ [63] when $\\rho=1,m_{\\rho}=1$ . ", "page_idx": 18}, {"type": "text", "text": "D.4 Additional notes about derivation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We make a few supplementary notes about the above derivation: ", "page_idx": 18}, {"type": "text", "text": "\u2022 Throughout our derivation, we use $\\rho$ to refer to the density level. Note that since this derivation is local to a single layer in the model, the density (or sparsity) level can also be parameterized independently for each layer. If a sparsity technique will use layer-wise independent sparsity levels, appropriate corrections should be made for each layer.   \n\u2022 Similar to the $\\rho$ notation, we use $\\eta$ to denote the learning rate, but this learning rate can be layer-specific depending on sparsity level. Appropriate corrections must be made if using layer-wise independent sparsities.   \n\u2022 The use of the Law of Large Numbers in portions of the above derivation indicate that $\\mathsf{S}\\upmu\\mathrm{Par}$ is expected to provide stable training dynamics as the number of non-zero weights per neuron (WPN) tends to infinity. However, in sparse settings, the WPN can tend to be small. If WPN is small, training dynamics may be affected, and this might be a direction for future work.   \n\u2022 In this work, we only consider sparsifying linear projection layers. As a result, $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ matches $\\upmu\\mathrm{P}$ for other layers like input, output, bias, layer-norm, and attention logits. Depending on the sparsification technique, these other layers might need to be reviewed for their effects on training dynamics. ", "page_idx": 18}, {"type": "text", "text": "E Experimental details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "$\\bf{S}\\mu\\bf{P}a r$ base hyperparameter tuning To find the optimized set of hyperparameters for $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ , we actually tune $\\upmu\\mathrm{P}$ HPs on a dense proxy model. By formulation of $\\mathrm{{S}\\mu\\mathrm{{Par}}}$ , these HPs transfer optimally to all the sparse models trained for this work. This dense proxy model is a GPT-2 model, but with small changes: ALiBi position embeddings [47] and SwiGLU nonlinearity [51]. We configure it with width: $d_{\\mathrm{model}}=d_{\\mathrm{model,base}}=256$ , number of layers: $n_{\\mathrm{layers}}=24$ , and head size: $d_{\\mathrm{head}}=64$ , resulting in 39M parameters. We trained this proxy model on 800M tokens with a batch size of 256 sequences and sequence length 2048 tokens. We randomly sampled 350 configurations of base learning rates, base initialization standard deviation, and embedding and output logits scaling factors. From this sweep we obtained the tuned hyperparameters listed in Table 3. ", "page_idx": 18}, {"type": "table", "img_path": "OWmu3QOa0O/tmp/00013842664b0970db3e4725e3a51ab86fc681f5ae11f79bde7170fdb088e303.jpg", "table_caption": ["Table 3: Tuned hyperparameters for our dense proxy model. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Experimental details for all figures In Table 4, we provide extensive details on hyperparameters, model size, and training schedule for all experiments in this paper. All models in this paper were trained on the SlimPajama dataset [54], a cleaned and deduplicated version of the RedPajama dataset. ", "page_idx": 18}, {"type": "table", "img_path": "OWmu3QOa0O/tmp/b3ffece7a001998c027026c2ba00baaa69fea7124bdf2ed77e1e2fffaebdb619.jpg", "table_caption": [""], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: Sections 6 and A contain a discussion of limitations and broader impact. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: In Section 3 we provide a short proof sketch to provide intuition behind both $\\mathsf{S}\\upmu\\mathrm{Par}$ and $\\upmu\\mathrm{P}$ . Then in Section D we provide a detailed derivation for $\\mathrm{S\\upmuPar}$ (which also reduces to $\\upmu\\mathrm{P}$ when $\\rho=1$ ). ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: To ensure reproducibility, in Section E we provide extensive details on all experiments contained in the paper. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: All tests in this paper use GPT-like transformer language models [48, 9], trained on the SlimPajama dataset [54], which are both open. Section E contains sufficient detail to faithfully reproduce the main experimental results. A minimal implementation of $\\mathsf{S}\\upmu\\mathrm{Par}$ is available at https://github.com/EleutherAI/nanoGPT-mup/tree/supar. We also provide a simple breakdown of the code changes required in Table 1. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: To ensure reproducibility, in Section E we provide extensive details on all experiments contained in the paper. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: In most of our experiments, we sample multiple seeds and plot the mean loss as a solid line, as well as the standard error of the mean as a shaded area of the same color. We disclose the number of seeds used for each experiment in the corresponding figure captions. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: In Section E we provide extensive experimental details which can be used to easily derive compute requirements based on a reader\u2019s specific hardware setting. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We do not conduct any research with human participants. Also, we consider the broader impacts and limitations of our work in Sections A and 6. Finally, we provide extensive experimental details in Section E to improve reproducibility. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: See Section A. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper presents a new neural network parameterization $\\mathrm{S\\upmuPar}$ and does not release any models or datasets. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We cite all datasets and architectures in accordance with their licenses. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: We do not introduce new assets in this paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: We do not conduct any research involving crowdsourcing or human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: We do not conduct any research involving human subjects. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 25}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]