{"references": [{"fullname_first_author": "Yossi Arjevani", "paper_title": "Lower bounds for non-convex stochastic optimization", "publication_date": "2022-01-01", "reason": "This paper provides lower bounds for non-convex stochastic optimization, which is the theoretical foundation for analyzing the convergence behavior of the proposed HyperPrism framework."}, {"fullname_first_author": "S\u00e9bastien Bubeck", "paper_title": "Convex optimization: Algorithms and complexity", "publication_date": "2015-01-01", "reason": "This paper provides a comprehensive overview of convex optimization algorithms and their complexity, which are fundamental to understanding the design and analysis of distributed mirror descent algorithms used in HyperPrism."}, {"fullname_first_author": "Liam Collins", "paper_title": "FedAvg with fine tuning: Local updates lead to representation learning", "publication_date": "2022-01-01", "reason": "This paper explores the use of FedAvg with fine-tuning for federated learning, addressing the challenges of data heterogeneity and non-IID data distributions, which are directly relevant to the context of HyperPrism."}, {"fullname_first_author": "Dmitry Kovalev", "paper_title": "ADOM: Accelerated decentralized optimization method for time-varying networks", "publication_date": "2021-01-01", "reason": "This paper introduces ADOM, an accelerated decentralized optimization method specifically designed for time-varying networks, addressing the challenges of dynamic communication links that HyperPrism also aims to handle."}, {"fullname_first_author": "Tian Li", "paper_title": "Federated optimization in heterogeneous networks", "publication_date": "2020-01-01", "reason": "This paper addresses the challenges of federated optimization in heterogeneous networks with non-IID data, providing valuable insights into the design and analysis of the proposed HyperPrism framework for tackling similar issues in distributed machine learning."}]}