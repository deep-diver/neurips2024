[{"figure_path": "93HCE8vTye/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Representational Collapse (Theorem 4.2). From top to bottom, we have a series of sequences given to Transformer architectures, each comprising repeated 1 tokens with a single 0 token at the end. The color and proximity of the curved lines illustrate how these representations converge as sequence length increases. (b) Over-squashing (Theorem 5.1). Due to the architecture of decoder-only Transformers, tokens that are earlier in their input sequence will have significantly more paths through which their data can reach the representation used for next-token prediction, leading to 'over-squashing'. This effect is depicted here for an early token (blue) and later token (red) in a five-token sequence.", "description": "This figure illustrates two key concepts discussed in the paper: representational collapse and over-squashing. (a) shows how the representations of sequences with increasing numbers of repeated '1' tokens, followed by a single '0', converge in the final layer of a Transformer model.  This convergence is problematic because it means that the model cannot distinguish between these sequences, resulting in errors when performing tasks like counting. (b) depicts over-squashing, where earlier tokens in a sequence have more influence on the final token's representation because of the unidirectional nature of the Transformer architecture. The figure highlights how this uneven distribution of influence can affect the model's performance.", "section": "Motivating Examples"}, {"figure_path": "93HCE8vTye/figures/figures_3_1.jpg", "caption": "Figure 2: Results on simple copying tasks. (a). Gemini was prompted to predict the last token (diamond) of a sequence \u20181...10\u2019 or the first token (square) of a sequence \u201801...1\u2019. (b). Same as (a) but with hints (see 3.2 for details) (c). Same as (a) but the sequences have interleaved 0s and 1s. See C.1 for extra details", "description": "The figure shows the results of three different copying tasks performed using the Gemini language model.  The first task involves copying either the first or last token from a sequence of 1s and 0s of varying lengths. The second task is similar to the first, but includes hints to aid the model. The third task uses sequences with interleaved 1s and 0s. The results highlight the challenges faced by the model in these tasks, particularly with longer sequences and when attempting to copy the last token.", "section": "3 Motivating Examples"}, {"figure_path": "93HCE8vTye/figures/figures_4_1.jpg", "caption": "Figure 3: Gemini 1.5 being prompted to sum 1 + \u2026 + 1 (Column 1), Count the number of ones in a sequence of 1s (Column 2), Count the number of ones in a sequence of ones and zeroes (the sequence is a Bernoulli sequence with probability of sampling a one being 0.7) (Column 3), and to counter the number of times a word appears in a sentence (Column 4).", "description": "The figure displays the results of four counting tasks performed by the Gemini 1.5 language model.  Each task involves counting something different, ranging from summing consecutive ones to counting word occurrences in sentences. The x-axis represents the length of the sequence or sentence, while the y-axis shows the absolute error. The figure shows how the model's accuracy decreases as the sequence length increases in all four tasks. The different colored bars in each column represent different prompting strategies.", "section": "3 Motivating Examples"}, {"figure_path": "93HCE8vTye/figures/figures_5_1.jpg", "caption": "Figure 7: Frequency of different outputs for Gemma 7B", "description": "This figure shows the frequency of different outputs generated by the Gemma 7B language model for three counting tasks: summing a sequence of 1s, counting ones in a sequence of 1s, and counting ones in a sequence of 1s and 0s. The results highlight the model's tendency to produce incorrect answers, especially for longer sequences, and demonstrate the phenomenon of representational collapse discussed in the paper.", "section": "C.2 Counting with Gemma"}, {"figure_path": "93HCE8vTye/figures/figures_7_1.jpg", "caption": "Figure 5: Representational collapse for counting (a, b) and copying (c, d) tasks. representations collapse for the sequence of ones (c), adding commas every third digit (d) helps to keep the representations well-separated.", "description": "This figure shows the results of experiments that measure the representational collapse phenomenon in a transformer model for counting and copying tasks.  The plots show the mean representational difference (with error bars) between the final token representations of pairs of similar sequences, as a function of sequence length. The pairs of sequences differ only in the last token (or one of the last tokens). For counting, two types of experiments are conducted: counting 1s in a sequence of 1s, and counting 1s in sequences of randomly sampled 0s and 1s. The copying tasks involve copying a sequence of 1s, and copying a sequence of 1s with commas inserted every third digit. The results show that for longer sequences, the representational difference decreases significantly, approaching machine precision. The addition of commas in the copying task mitigates this representational collapse.", "section": "4 Representational Collapse"}, {"figure_path": "93HCE8vTye/figures/figures_21_1.jpg", "caption": "Figure 3: Gemini 1.5 being prompted to sum 1 + \u2026 + 1 (Column 1), Count the number of ones in a sequence of 1s (Column 2), Count the number of ones in a sequence of ones and zeroes (the sequence is a Bernoulli sequence with probability of sampling a one being 0.7) (Column 3), and to counter the number of times a word appears in a sentence (Column 4).", "description": "This figure displays the results of experiments conducted using Gemini 1.5, a large language model (LLM), on four different counting tasks. The tasks involved adding a sequence of ones, counting the number of ones in sequences of ones, counting the number of ones in a mixed sequence of ones and zeros (Bernoulli sequence with p=0.7), and counting the occurrences of a specific word in a sentence.  The x-axis represents sequence length, while the y-axis represents the absolute error in the predictions. The figure shows the results for three different prompting strategies: no Chain-of-Thought (CoT), CoT zero-shot, and CoT few-shot. The results highlight the limitations of LLMs in handling simple counting tasks, particularly as sequence length increases.", "section": "3 Motivating Examples"}, {"figure_path": "93HCE8vTye/figures/figures_21_2.jpg", "caption": "Figure 7: Frequency of different outputs for Gemma 7B", "description": "This figure shows the frequency distribution of different outputs generated by the Gemma 7B language model for three counting tasks: summing 1+...+1, counting ones in a sequence of 1s, and counting ones in a sequence of ones and zeros.  The figure visually demonstrates the model's tendency to produce inaccurate counts, especially as sequence length increases, highlighting a key failure mode discussed in the paper.", "section": "C.2 Counting with Gemma"}, {"figure_path": "93HCE8vTye/figures/figures_22_1.jpg", "caption": "Figure 8: Convergence behaviour with a synthetic Transformer experiment. We sample the key, query, and values from a Gaussian distribution and apply the traditional sinusoidal PEs from [30]. We apply a logarithmic scale on the y-axis.", "description": "This figure shows the results of a synthetic experiment to demonstrate representational collapse using sinusoidal positional encodings.  Key, query, and value vectors were sampled from a Gaussian distribution, and the standard sinusoidal positional embeddings from the original Transformer paper were applied.  The plot shows the mean representational difference between the final token representations of sequences of length n and n+1 (where the n+1 sequence is identical to the n sequence except for a repeated final token), as the sequence length n increases. The y-axis uses a logarithmic scale.", "section": "C.3 Synthetic Experiments on Representational Collapse"}, {"figure_path": "93HCE8vTye/figures/figures_22_2.jpg", "caption": "Figure 9: Total variation decay of softmax distributions with growing sequence length. We sample n elements uniformly from [0, 1] and then create a related sequence by taking its first k = 200 and adding to these elements noise sampled uniformly from [0,0.1]. We measure the total variation between their softmax distributions. It is clear how the total variation decays with length, in accordance with Lemma B.2. Error bars show minimum and maximum over 5 seeds.", "description": "This figure shows the decay of total variation between softmax distributions of two sequences as the sequence length increases. One sequence is sampled uniformly from [0,1], and the other sequence is created by adding noise to the first 200 elements of the first sequence. The plot demonstrates that the total variation decreases as the sequence length increases, supporting Lemma B.2 in the paper which shows that the total variation between two softmax distributions tends to zero as the length of the sequences increases.", "section": "C.3 Synthetic Experiments on Representational Collapse"}, {"figure_path": "93HCE8vTye/figures/figures_23_1.jpg", "caption": "Figure 10: We sample n queries, keys, and values independently from a standard Gaussian, applying different positional encodings. We then construct sequences of length n + 1, by repeating the n-th token. We report the L\u2081 distance between the last tokens of the sequences of length n and n + 1 after one decoder-only Transformer layer. We set the hidden dimension to 64, use a single attention head, and normalise appropriately to simulate the effects of LayerNorm. The y-axis is shown in log-scale.", "description": "This figure shows the results of an experiment designed to test the impact of different positional encodings on the representational collapse phenomenon in Transformer models.  The experiment uses a simplified Transformer architecture with a single attention head and layer normalization.  The x-axis represents the sequence length, and the y-axis represents the L\u2081 distance between the final token representations of two sequences: one of length n, and the other of length n+1 (where the last token is repeated). The different lines represent the results obtained with different positional encodings (Sinusoidal Absolute Encoding, ROPE, Alibi, No Positional Encoding), as well as the effect of bf16 precision.", "section": "C.4 Effect of Positional Encodings"}, {"figure_path": "93HCE8vTye/figures/figures_24_1.jpg", "caption": "Figure 1: (a) Representational Collapse (Theorem 4.2). From top to bottom, we have a series of sequences given to Transformer architectures, each comprising repeated 1 tokens with a single 0 token at the end. The color and proximity of the curved lines illustrate how these representations converge as sequence length increases. (b) Over-squashing (Theorem 5.1). Due to the architecture of decoder-only Transformers, tokens that are earlier in their input sequence will have significantly more paths through which their data can reach the representation used for next-token prediction, leading to 'over-squashing'. This effect is depicted here for an early token (blue) and later token (red) in a five-token sequence.", "description": "This figure illustrates two phenomena in decoder-only Transformers: representational collapse and over-squashing.  (a) shows how the final representation of sequences of repeated '1's ending in a '0' converges as the sequence length increases, demonstrating representational collapse.  The different colored lines represent distinct sequences becoming indistinguishably close in representation. (b) depicts over-squashing, where tokens earlier in the sequence have a disproportionately large influence on the final token's representation due to the unidirectional information flow in decoder-only architectures.", "section": "Representational Collapse"}, {"figure_path": "93HCE8vTye/figures/figures_24_2.jpg", "caption": "Figure 12: Representational collapse in Gemma for the prompt: \"What is the last digit of the following sequence? Please answer exactly as \u2018The answer to your question is: <ANSWER>'\". Here is the sequence: {seq} and (Type 2)", "description": "This figure shows the representational collapse phenomenon in the Gemma language model using two different prompt types. Type 1 requests the last digit directly, while Type 2 requests it indirectly.  The y-axis represents the representational difference between sequences of different lengths but with similar structures. The x-axis shows the sequence length. As the length increases, the representational difference decreases, demonstrating representational collapse. The horizontal dashed line indicates the bf16 precision limit.", "section": "C.5 Ablation on Prompt Structure"}]