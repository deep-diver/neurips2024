[{"type": "text", "text": "Randomized Exploration for Reinforcement Learning with Multinomial Logistic Function Approximation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Wooseong Cho\u2217 Seoul National University Seoul, South Korea wooseong_cho@snu.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Taehyun Hwang\u2217   \nSeoul National University   \nSeoul, South Korea   \nth.hwang@snu.ac.kr   \nJoongkyu Lee   \nSeoul National University   \nSeoul, South Korea   \njklee0717@snu.ac.kr   \nMin-hwan Oh\u2020   \nSeoul National University   \nSeoul, South Korea   \nminoh@snu.ac.kr ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study reinforcement learning with multinomial logistic (MNL) function approximation where the underlying transition probability kernel of the Markov decision processes (MDPs) is parametrized by an unknown transition core with features of state and action. For the finite horizon episodic setting with inhomogeneous state transitions, we propose provably efficient algorithms with randomized exploration having frequentist regret guarantees. For our first algorithm, RRL-MNL, we adapt optimistic sampling to ensure the optimism of the estimated value function with sufficient frequency. We establish that RRL-MNL achieves a $\\widetilde{\\mathcal{O}}(\\kappa^{-1}d^{\\frac{3}{2}}H^{\\frac{3}{2}}\\sqrt{T})$ frequentist regret bound with constant-time computational cost per episode. Here, $d$ is the dimension of the transition core, $H$ is the horizon length, $T$ is the total number of steps, and $\\kappa$ is a problem-dependent constant. Despite the simplicity and practicality of RRL-MNL, its regret bound scales with $\\kappa^{-1}$ , which is potentially large in the worst case. To improve the dependence on $\\kappa^{-1}$ , we propose ORRL-MNL, which estimates the value function using the local gradient information of\u221a the MNL transition model. We show that its frequentist regret bound is $\\widetilde{\\mathcal{O}}(d^{\\frac{3}{2}}H^{\\frac{3}{2}}\\sqrt{T}\\!+\\!\\kappa^{-1}d^{2}H^{2})$ To the best of our knowledge, these are the first randomized RL algorithms for the MNL transition model that achieve statistical guarantees with constant-time computational cost per episode. Numerical experiments demonstrate the superior performance of the proposed algorithms. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement learning (RL) is a sequential decision-making problem in which an agent tries to maximize its expected cumulative reward by interacting with an unknown environment over time. Despite significant empirical progress in RL algorithms for various applications [47, 52, 65, 66, 25], the theoretical understanding of RL algorithms had long been limited to tabular methods [40, 56, 10, 77, 79], which explicitly enumerate the entire state and action spaces and learn the value (or the policy) for each state and action. Recently, there has been an increasing body of research in RL with function approximation to extend beyond the tabular problem setting. In particular, linear function approximation has served as a foundational model [43, 73, 22, 9, 37]. On the other hand, the linear transition model assumption poses significant constraints: 1) the output of the function must be within $[0,1]$ , and 2) the sum of the probabilities for all possible next states must be exactly 1. These constraints make it challenging to apply RL with linear function approximation to real-world applications [35]. To overcome such challenges, there has been literature on RL with general function approximation [21, 28, 37, 44, 4, 18]. Despite the guarantee of sample efficiency achieved by their algorithms, this accomplishment might be impeded by computational intractability or the necessity to rely on stronger assumptions. As a result, the resulting methods may not be as general or practical. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "On the other hand, Hwang and Oh [35] introduce specific non-linear parametric MDPs called MNLMDPs (Assumption 1) where the transition probability of MDPs is given by an MNL model. They consider an upper confidence bound (UCB) approach to balance exploration and exploitation. Since it is costly or even intractable to compute UCB explicitly, randomized exploration methods such as Thompson Sampling (TS) are widely studied in RL with linear function approximation as well as tabular MDPs. This is because, in various decision-making problems ranging from multi-armed bandits to RL, randomized exploration algorithms have been shown to perform better than UCB methods in empirical evaluations [16, 57, 64, 49]. Furthermore, randomized exploration can be easily integrated with linear function approximation. This is because the value function in linear MDPs can be linearly parameterized, allowing perturbations of the estimator to directly control the perturbations of the value function. However, although there has been some literature aiming to propose randomized algorithms for general function classes [37, 4, 5, 75], these methods do not discuss how to define the posterior distribution supported by the given function class and how to draw the optimistic sample from the posterior [4, 5, 75], or they require stronger assumptions on stochastic optimism [37], which is one of the most challenging elements in frequentist regret analysis. Thus, the design of a tractable randomized exploration RL algorithm and the feasibility of frequentist regret analysis for randomized exploration remain open challenges. Hence, the following question arises: ", "page_idx": 1}, {"type": "text", "text": "Can we design a provably efficient and tractable randomized algorithm for RL with MNL function approximation? ", "page_idx": 1}, {"type": "text", "text": "We answer the\u221a above question by proposing the first randomized algorithm, RRL-MNL, achieving $\\widetilde{\\mathcal{O}}(\\kappa^{-1}d^{\\frac{3}{2}}H^{\\frac{3}{2}}\\sqrt{T})$ frequentist regret with constant-time computational cost per episode. RRL-MNL is not only the first algorithm with randomized exploration for MNL-MDPs, but also, to the best of our knowledge, it provides the first frequentist regret analysis for a non-linear model-based algorithm with randomized exploration without assuming stochastic optimism [37]. ", "page_idx": 1}, {"type": "text", "text": "While RRL-MNL is statistically efficient, the current method used to analyze the regret of MNL function approximation introduces a problem-dependent constant $\\kappa$ (Assumption 4), which reflects the level of non-linearity of the MNL transition model. This constant $\\kappa$ originates from the use of generalized linear models (GLMs) for contextual bandit settings [26, 51, 45] and MNL bandit settings [54, 17, 55]. The magnitude of the constant $\\kappa$ can be exponentially small with respect to the size of the decision set, hence the regret bound scaling with $\\kappa^{-1}$ could be prohibitively large in the worst case [23]. Worse yet, the situation is even more challenging in RL, as in the worst case, $\\kappa^{-1}$ can be much larger than in the case of bandits. To overcome the prohibitive dependence on $\\kappa$ , algorithms based on new Bernstein-like inequalities and the self-concordant-like property of the log-loss have been proposed for logistic bandits [23, 3, 24] and for MNL bandits [61, 6, 50]. As an extension of these works, the following fundamental question remains open: ", "page_idx": 1}, {"type": "text", "text": "Is it possible for RL algorithms with MNL function approximation to have a sharper dependence on the problem-dependent constant $\\kappa$ ? ", "page_idx": 1}, {"type": "text", "text": "For the above question, we propose the s\u221aecond randomized algorithm referred to as ORRL-MNL, which establishes a regret bound of $\\bar{\\tilde{\\mathcal{O}}}(d^{\\frac{3}{2}}H^{\\frac{3}{2}}\\sqrt{T}+\\kappa^{-1}d^{2}H^{2})$ with constant-time computational cost per episode. We summarize our main contributions as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose computationally tractable randomized algorithms for RL with MNL function approximation: RRL-MNL and ORRL-MNL. To the best of our knowledge, these are the first randomized model-based RL algorithms with MNL function approximation that achieve the frequentist regret bounds with constant-time computational cost per episode. \u2022 We establish that RRL-MNL enjoys $\\widetilde{\\mathcal{O}}(\\kappa^{-1}d^{\\frac{3}{2}}H^{\\frac{3}{2}}\\sqrt{T})$ frequentist regret bound with constanttime computational cost per episo de, where $d$ is the dimension of the transition core, $H$ is horizon length, $T$ is the total number of rounds, and $\\kappa$ is a problem-dependent constant. We derive the stochastic optimism of RRL-MNL, and to our knowledge, this is the first frequentist regret analysis for a non-linear model-based algorithm with randomized exploration without assuming stochastic optimism. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 To achieve a regret bound with improved dependence on $\\kappa$ , we introduce ORRL-MNL, which constructs the optimistic randomized value functions by taking into account the effects of the local gradient information for the MN\u221aL transition model at each reachable state. We prove that ORRL-MNL enjoys an $\\widetilde{\\mathcal{O}}(d^{\\frac{3}{2}}H^{\\frac{3}{2}}\\sqrt{T}+\\kappa^{-1}d^{2}H^{2})$ regret with constant-time computational cost per episode, significantly improving the regret of RRL-MNL without requiring prior knowledge of $\\kappa$ .   \n\u2022 We evaluate our algorithms on tabular MDPs and demonstrate the superior performance of our proposed algorithms compared to the existing state-of-the-art MNL-MDP algorithm [35]. The experiments provide evidence that our proposed algorithms are both computationally and statistically efficient. ", "page_idx": 2}, {"type": "text", "text": "Related works on RL with function approximation and MNL contextual bandits are provided in Appendix A. ", "page_idx": 2}, {"type": "text", "text": "2 Problem Setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider the episodic Markov decision processes (MDPs) denoted by $\\mathcal{M}(\\boldsymbol{S},\\mathcal{A},H,\\{P\\}_{h=1}^{H},r)$ , where $\\boldsymbol{S}$ is the state space, $\\boldsymbol{\\mathcal{A}}$ is the action space, $H$ is the horizon length of each episode, $\\{P\\}_{h=1}^{H}$ is the collection of probability distributions, and $r$ is the reward function. Every episodes start from the initial state $s_{1}$ and for every step $h\\in[H]:=\\{1,...,H\\}$ in an episode, the learning agent interacts with the environment represented as $\\mathcal{M}$ . The agent observes the state $s_{h}\\,\\in\\,{\\cal S}$ , chooses an action $a_{h}\\ \\in\\ A$ , receives a reward $r(s_{h},a_{h})\\,\\in\\,[0,1]$ and the next state $s_{h+1}$ is given by the transition probability distribution $P_{h}(\\cdot|s_{h},a_{h})$ . Then this process is repeated throughout the episode. A policy $\\pi:S\\times[H]\\to A$ is a function that determines the action of the agent at state $s_{h}$ , i.e., $a_{h}=\\pi(s_{h},h):=\\pi_{h}(s_{h})$ . ", "page_idx": 2}, {"type": "text", "text": "We define the value function of the policy $\\pi$ , denoted by $V_{h}^{\\pi}(s)$ , as the expected sum of rewards under the policy $\\pi$ until the end of the episode starting from $s_{h}~=~s$ , i.e., $V_{h}^{\\pi}(s)\\;=\\;$ $\\mathbb{E}_{\\pi}\\left[\\sum_{h^{\\prime}=h}^{H}r(s_{h^{\\prime}},\\pi_{h^{\\prime}}(s_{h^{\\prime}}))\\mid s_{h}=s\\right].$ Similarly, we define the action-value function $Q_{h}^{\\pi}(s,a)\\;=\\;$ $r(s,a)+\\mathbb{E}_{s^{\\prime}\\sim P_{h}(\\cdot|s,a)}\\left[V_{h+1}^{\\pi}(s^{\\prime})\\right]\\!.$ . We define an optimal policy $\\pi^{*}$ to be a policy that achieves the highest possible value at every $(s,h)\\,\\in\\,S\\times[H]$ . We denote the optimal value function by $V_{h}^{*}(s)=V_{h}^{\\pi^{*}}(s)$ and the optimal action-value function by $Q_{h}^{*}(s,a)=Q_{h}^{\\pi^{*}}(s,a)$ . To simplify, we introduce the notation $P_{h}V_{h+1}(s,a)=\\mathbb{E}_{s^{\\prime}\\sim P_{h}(\\cdot|s,a)}[V_{h+1}(s^{\\prime})]$ . Recall that the Bellman equations are, ", "page_idx": 2}, {"type": "equation", "text": "$$\nQ_{h}^{\\pi}(s,a)=r(s,a)+P_{h}V_{h+1}^{\\pi}(s,a)\\,,\\quad Q_{h}^{*}(s,a)=r(s,a)+P_{h}V_{h+1}^{*}(s,a)\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $V_{H+1}^{\\pi}(s)=V_{H+1}^{*}(s)=0$ and $V_{h}^{*}(s)=\\operatorname*{max}_{a\\in\\mathcal{A}}Q_{h}^{*}(s,a)$ for all $s\\in S$ . ", "page_idx": 2}, {"type": "text", "text": "The goal of the agent is to maximize the sum of rewards for ${\\bf K}$ episodes. In other words, the goal is to minimize the cumulative regret of the policy $\\pi$ over $\\mathbf{K}$ episodes where $\\pi=\\{\\pi^{k}\\}_{k=1}^{K}$ is a collection of policies $\\pi^{k}$ at $\\boldsymbol{\\mathrm{k}}$ -th episode. The regret is defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{Regret}_{\\pi}(K):=\\sum_{k=1}^{K}(V_{1}^{*}-V_{1}^{\\pi^{k}})(s_{1}^{k})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $s_{1}^{k}$ is the initial state at the $k$ -th episode. ", "page_idx": 2}, {"type": "text", "text": "2.1 Multinomial Logistic Markov Decision Processes (MNL-MDPs) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Even though a lot of provable RL algorithms for linear MDPs are proposed, there is a simple but fundamental problem with the linear transition model assumption on the linear MDPs. In other words, the output of a linear function approximating the transition model must be in [0, 1] and the probability of all possible following states must sum to 1 exactly. Such restrictive assumption can affect the regret performances of algorithm suggested under the linearity assumption. To resolve these challenges, Hwang and Oh [35] propose a setting of a multinomial logistic Markov decision processes (MNL-MDPs), where the state transition model is given by a multinomial logistic model. We introduce the formal definition for MNL-MDP as follows: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Assumption 1 (MNL-MDPs [35]). An MDP $\\mathcal{M}(\\mathcal{S},\\mathcal{A},H,\\{P_{h}\\}_{h=1}^{H},r)$ is an MNL-MDP with a feature map $\\varphi:S\\times A\\times S\\rightarrow\\mathbb{R}^{d}$ , if for each $h\\in[H]$ , there exists $\\pmb{\\theta}_{h}^{*}\\in\\mathbb{R}^{d}$ , such that for any $(s,a)\\in S\\times A$ and $s^{\\prime}\\in\\mathcal{S}_{s,a}:=\\{s^{\\prime}\\in\\mathcal{S}:\\mathbb{P}(s^{\\prime}\\mid s,a)\\neq0\\}$ , the state transition kernel of $s^{\\prime}$ when an action a is taken at a state $s$ is given by, ", "page_idx": 3}, {"type": "equation", "text": "$$\nP_{h}(s^{\\prime}\\mid s,a)=\\frac{\\exp(\\varphi(s,a,s^{\\prime})^{\\top}\\theta_{h}^{*})}{\\sum_{\\widetilde{s}\\in S_{s,a}}\\exp(\\varphi(s,a,\\widetilde{s})^{\\top}\\theta_{h}^{*})}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We call each unknown vector $\\pmb{\\theta}_{h}^{*}$ transition core. Furthermore, we denote the maximum cardinality of the set of reachable states as $\\boldsymbol{\\mathcal{U}}$ , i.e., $\\mathcal{U}:=\\operatorname*{max}_{s,a}\\left|S_{s,a}\\right|$ . ", "page_idx": 3}, {"type": "text", "text": "Remark 1. While Hwang and Oh [35] assume a homogeneous transition kernel, we assume an inhomogeneous transition kernel, in which the probability varies depending on the current time step h even for the same state transition, which is a more general setting. Also, for notational simplicity, we denote the true transition kernel $P_{h}$ as $P_{\\theta_{h}^{*}}$ , and the estimated transition kernel by $\\pmb{\\theta}$ as $P_{\\theta}$ . ", "page_idx": 3}, {"type": "text", "text": "2.2 Assumptions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We introduce some standard regularity assumptions. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2 (Boundedness). We assume $\\|\\varphi(s,a,s^{\\prime})\\|_{2}\\leq L_{\\varphi}$ for all $(s,a,s^{\\prime})\\in\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S}_{s,a},$ , and $\\lVert\\pmb{\\theta}_{h}^{\\mp}\\rVert_{2}\\leq L_{\\pmb{\\theta}}$ for all $h\\in[H]$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 3 (Known reward). We assume that the reward function $r$ is known to the agent. ", "page_idx": 3}, {"type": "text", "text": "Assumption 4 (Problem-dependent constant). Let $\\mathcal{B}_{d}(L_{\\pmb\\theta}):=\\{\\pmb\\theta\\in\\mathbb{R}^{d}:\\|\\pmb\\theta\\|_{2}\\leq L_{\\pmb\\theta}\\}$ . There exists $\\kappa>0$ such that for any $(s,{\\bar{a}})\\in S\\times A$ and $s^{\\prime},\\widetilde{s}\\in\\mathcal{S}_{s,a}$ with $s^{\\prime}\\neq\\widetilde s$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\theta\\in\\mathcal{B}_{d}(L_{\\theta})}P_{\\theta}(s^{\\prime}\\mid s,a)P_{\\theta}(\\widetilde{s}\\mid s,a)\\ge\\kappa\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Discussion of assumptions Assumption 2 is common in the literature on RL with function approximation [43, 72, 73, 37, 35] to make the regret bounds scale-free. Assumption 3 is used to focus on the main challenge of model-based RL that learning about $P$ of the environment is more difficult than learning $r$ . In the model-based RL literature [71, 9, 72, 81, 35], the known reward $r$ assumption is widely used. Assumption 4 is typical in generalized linear contextual bandit [26, 51, 23, 3, 24] and MNL contextual bandit literature [54, 8, 55, 61, 6, 76, 50] to guarantee non-singular Fisher information matrix. ", "page_idx": 3}, {"type": "text", "text": "3 Randomized Algorithm for MNL-MDPs having constant-time computational cost ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Previous work for MNL-MDPs [35] proposed a UCB-based exploration algorithm. Constructing a UCB-based optimistic value function is not only computationally intractable but also tends to overly optimistically estimate the true optimal value function. Additionally, their algorithm incurs increasing computation costs as episodes progress, as it requires all samples from the previous episode to estimate the transition core. In this section, we present a novel model-based RL algorithm that incorporates randomized exploration and online parameter estimation for MNL-MDPs. ", "page_idx": 3}, {"type": "text", "text": "3.1 Algorithm: RRL-MNL ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Online transition core estimation While Hwang and Oh [35] estimate the transition core using maximum likelihood estimation over all samples from previous episodes, we employ an efficient online parameter estimation method by exploiting the particular structure of the MNL transition model. The key insight is that the negative log-likelihood function for the MNL model in each episode is strongly convex over a bounded domain. This property allows us to utilize a variation of the online Newton step [30, 31], which inspired online algorithms for logistic bandits [74] and MNL contextual bandits [55]. Specifically, for $({\\bar{k}},h)\\in[K]\\times[H]$ , we define the response variable $y_{h}^{k}=$ ", "page_idx": 3}, {"type": "text", "text": "1: Inputs: Episodic MDP $\\mathcal{M}$ , Feature map $\\varphi:S\\times A\\times S\\,\\rightarrow\\,\\mathbb{R}^{d}$ , Number of episodes $K$ , Regularization parameter $\\lambda$ , Exploration variance $\\{\\sigma_{k}\\}_{k=1}^{K}$ , Sample size $M$ , Problem-dependent constant $\\kappa$   \n2: Initialize: $\\theta_{h}^{1}=\\mathbf{0}_{d}$ , $\\mathbf{A}_{1,h}=\\lambda\\mathbf{I}_{d}$ for $h\\in[H]$   \n34:: forO ebpsiesrovde $k=1,2,\\cdots\\,,K$ dd. onoise vector for and $s_{1}^{k}$ $\\boldsymbol{\\xi}_{\\boldsymbol{k},h}^{(m)}\\sim\\mathcal{N}(\\mathbf{0}_{d},\\sigma_{\\boldsymbol{k}}^{2}\\mathbf{A}_{\\boldsymbol{k},h}^{-1})$ $m\\in[M]$ $h\\in[H]$   \n5: Set $\\left\\{Q_{h}^{k}(\\cdot,\\cdot)\\right\\}_{h\\in[H]}$ as described in (4)   \n76:: forS heloercitz $a_{h}^{k}=\\operatorname{argmax}_{a\\in A}Q_{h}^{k}(s_{h}^{k},a)$ $h=1,2,\\cdots\\,,H$ and observe $s_{h+1}^{k}$   \n8: Update $\\begin{array}{r}{{\\bf A}_{k+1,h}={\\bf A}_{k,h}+\\frac{\\kappa}{2}\\sum_{s^{\\prime}\\in{\\cal S}_{k,h}}\\!\\varphi(s_{h}^{k},a_{h}^{k},s^{\\prime})\\varphi(s_{h}^{k},a_{h}^{k},s^{\\prime})^{\\top}}\\end{array}$ and $\\pmb{\\theta}_{h}^{k+1}$ as in (2)   \n9: end for   \n10: end for ", "page_idx": 4}, {"type": "text", "text": "$\\left[y_{h}^{k}(s^{\\prime})\\right]_{s^{\\prime}\\in S_{k,h}}$ such that $y_{h}^{k}(s^{\\prime})=\\mathbb{1}(s_{h+1}^{k}=s^{\\prime})$ for $s^{\\prime}\\in S_{k,h}:=S_{s_{h}^{k},a_{h}^{k}}$ . Then, $y_{h}^{k}$ is sampled from the following multinomial distribution: $y_{h}^{k}\\sim\\mathrm{multinomial}(1,\\big[P_{\\pmb{\\theta}_{h}^{*}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\big]_{s^{\\prime}\\in S_{k,h}})$ , where 1 represents that $y_{h}^{k}$ is a single-trial sample. We define the per-episode loss $\\ell_{k,h}(\\pmb\\theta)$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\ell_{k,h}(\\pmb\\theta):=-\\sum_{s^{\\prime}\\in S_{k,h}}y_{h}^{k}(s^{\\prime})\\log P_{\\pmb\\theta}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then, the estimated transition core for $\\pmb{\\theta}_{h}^{*}$ is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{\\theta}_{h}^{k}=\\operatorname*{argmin}_{\\pmb{\\theta}\\in B_{d}(L_{\\theta})}\\frac{1}{2}\\|\\pmb{\\theta}-\\pmb{\\theta}_{h}^{k-1}\\|_{\\mathbf{A}_{k,h}}^{2}+(\\pmb{\\theta}-\\pmb{\\theta}_{h}^{k-1})^{\\top}\\nabla\\ell_{k-1,h}(\\pmb{\\theta}_{h}^{k-1})\\,,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\theta_{h}^{1}$ can be initialized as any point in $B_{d}(L_{\\theta})$ and ${\\bf A}_{k,h}$ is the Gram matrix defined by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{A}_{k,h}:=\\lambda\\mathbf{I}_{d}+\\frac{\\kappa}{2}\\sum_{i=1}^{k-1}\\sum_{s^{\\prime}\\in S_{i,h}}\\varphi(s_{h}^{i},a_{h}^{i},s^{\\prime})\\varphi(s_{h}^{i},a_{h}^{i},s^{\\prime})^{\\top}\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Stochastically optimistic value function First of all, we introduce the key challenges of regret analysis for randomized algorithms, explain how previous works have overcome these challenges, and then describe why the techniques from previous works cannot be applied to MNL-MDPs. Ensuring that the estimated value function is optimistic with sufficient frequency is a crucial challenge in analyzing the frequentist regret of randomized algorithms. A common way to promote sufficient exploration in randomized algorithms is by perturbing the estimated value function or by performing posterior sampling in the transition model class. Frequentist regret analysis of randomized exploration in an RL setting has been conducted for tabular [59, 7, 62, 60, 67], linear MDPs [73, 37], and general function classes [37, 4, 5, 75]. In the case of linear MDPs [73, 37], since the property that the action-value function is linear in the feature map allows perturbing the estimated parameter directly to control the perturbation of the estimated value function. Also, even though Ishfaq et al. [37] presented a randomized algorithm for the general function class using eluder dimension, they assume stochastic optimism (anti-concentration), which is in fact one of the most challenging aspects of frequentist analysis. Other posterior sampling algorithms in RL for the general function class such as [4, 5, 75], except for very limited examples, do not discuss how to define the posterior distribution supported by the given function class and how to draw the optimistic sample from the posterior. That is why even after there exists a so-called general function class-based result, it is often the case that results in specific parametric models are still needed. ", "page_idx": 4}, {"type": "text", "text": "Note that in episodic RL, the perturbed estimated value functions are propagated back through horizontal steps, requiring careful adjustment of the perturbation scheme to maintain a sufficient probability of optimism without decaying too quickly with the horizon. For example, if the probability of the estimated value function being optimistic at horizon $h$ is denoted as $p$ , this would result in the probability that the estimated value function in the initial state is optimistic being on the order of $p^{H}$ , implying that the regret can increase exponentially with the length of the horizon $H$ . ", "page_idx": 4}, {"type": "text", "text": "Additionally, the non-linearity and substitution effect of the next state transition in the MNL-MDPs make applying the existing TS techniques infeasible to guarantee optimism in MNL-MDPs with sufficient frequency. Instead, we design the stochastically optimistic value function by exploiting the structure of the MNL transition model. In other words, the prediction error of MNL transition model (Definition 1) can be bounded by the weighted norm of the dominant feature $\\hat{\\varphi}$ (Lemma 4). Based on such dominant feature, we perturb the estimated value function by injecting Gaussian noise whose variance is proportional to the inverse of the Gram matrix to encourage the perturbation with higher variance in less explored directions. To guarantee the optimism with fixed probability, we adapt optimistic sampling technique [7, 54, 37, 36]. For each $m\\in[M]$ , sample i.i.d. Gaussian noise vector $\\boldsymbol{\\xi}_{k,h}^{(m)}\\sim\\mathcal{N}(\\mathbf{0}_{d},\\sigma_{k}^{2}\\mathbf{A}_{k,h}^{-1})$ where $\\sigma_{k}$ is an exploration parameter, and add the most optimistic inner product value $\\scriptstyle\\operatorname*{max}_{m\\in[M]}{\\hat{\\varphi}}_{k,h}\\left(s,a\\right)^{\\top}\\xi_{k,h}^{(m)}$ to the estimated value function. To summarize for any $(s,a)\\in S\\times A$ , set $Q_{H+1}^{k}(s,a)=0$ and for $h\\in[H]$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\nQ_{h}^{k}(s,a)=\\operatorname*{min}\\left\\{r(s,a)+\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\theta_{h}^{k}}(s^{\\prime}\\mid s,a)V_{h+1}^{k}(s^{\\prime})+\\operatorname*{max}_{m\\in[M]}\\hat{\\varphi}_{k,h}(s,a)^{\\top}\\xi_{k,h}^{(m)},H\\right\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r l r}{V_{h}^{k}(s)}&{{}=}&{-\\mathrm{\\nabla}\\operatorname*{max}_{a^{\\prime}}Q_{h}^{k}(s,a^{\\prime})}\\end{array}$ and $\\begin{array}{r l r}{\\hat{\\varphi}_{k,h}(s,a)}&{{}:=}&{\\varphi(s,a,\\hat{s})}\\end{array}$ for $\\begin{array}{r l}{\\hat{s}}&{{}=}\\end{array}$ a $\\mathrm{rgmax}_{s^{\\prime}\\in S_{s,a}}\\left\\|\\varphi(s,a,s^{\\prime})\\right\\|_{\\mathbf{A}_{k,h}^{-1}}$ . Based on these stochastically optimistic value function, the agent plays a greedy action $a_{h}^{k}=\\mathrm{argmax}_{a^{\\prime}}\\,Q_{h}^{k}(s_{h}^{k},a^{\\prime})$ . We layout the procedure in Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "Remark 2. Note that RRL-MNL only requires constant-time computational cost and storage cost per episode, as it does not require storing all samples from previous episodes, and the Gram matrix ${\\bf A}_{k,h}$ can be updated incrementally. ", "page_idx": 5}, {"type": "text", "text": "3.2 Regret bound of RRL-MNL ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We present the regret upper bound of RRL-MNL. The complete proof is deferred to Appendix C. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 (Regret Bound of RRL-MNL). Suppose that Assumption 1- 4 h\u221aold. For any 0 < \u03b4 < \u03a6(2\u22121), if we set the input parameters in Algorithm $^{\\,l}$ as $\\lambda=L_{\\varphi}^{2},\\sigma_{k}=\\widetilde{\\mathcal{O}}(H\\sqrt{d})$ and $\\begin{array}{r}{M=\\lceil1-\\frac{\\log H}{\\log\\Phi(1)}\\rceil}\\end{array}$ where $\\Phi$ is the normal CDF, then with probability at least $1-\\delta$ , the cumulative regret of the RRL-MNL policy $\\pi$ is upper-bounded as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nR e g r e t_{\\pi}(K)=\\widetilde{\\mathcal{O}}\\left(\\kappa^{-1}d^{\\frac{3}{2}}H^{\\frac{3}{2}}\\sqrt{T}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $T=K H$ is the total number of steps. ", "page_idx": 5}, {"type": "text", "text": "Discussion of Theorem 1 To our best knowledge, this is the first result to provide a frequentist regret bound for the MNL-MDPs. Among the previous RL algorithms using function approximation, the most comparable techniques to our method are model-free algorithms with randomized exploration [73, 37]. To guarantee stochastic optimism, Zanette et al. [73] established a lower bound on the difference between the estimated value and the optimal value by the summation of linear terms with respect to the average feature (Lemma F.1 in [73]). This property is achievable due to the linear expression of the value function in linear MDPs. Instead, we established a lower bound on the difference between value functions by the summation of the Bellman errors (Definition 1) along the sample path obtained through the optimal policy (Lemma 7). Hence, our analysis significantly differs from that of Zanette et al. [73] since the value function in MNL-MDPs is no longer linearly parametrized, and there is no closed-form expression for it. ", "page_idx": 5}, {"type": "text", "text": "Compared to [37], they also used an optimistic sampling technique; however, our theoretical sampling size $M\\,=\\,{\\mathcal{O}}(\\log H)$ is much tighter than that of [37], i.e., $O(d)$ for the linear function class, ${\\mathcal{O}}(\\log(T|S||A|))$ for the general function class. While Ishfaq et al. [37] extend the results of the linear function class to general function class under the assumption of stochastic optimism (Assumption C in [37]), we provide the frequentist regret analysis for a non-linear model-based algorithm with randomized exploration without assuming stochastic optimism. ", "page_idx": 5}, {"type": "text", "text": "Compared to the optimistic exploration algorithm for MNL-MDPs [35], our randomized exploration requires a more involved proof technique to ensure that the perturbation of the estimated value function has enough variance to maintain optimism with sufficient frequency (Lemma 6). As a result, the established regret of RRL-MNL differs by a factor of $\\sqrt{d}$ , which aligns with the difference in the existing bounds of linear bandits between a TS-based algorithm [2] and a UCB-based algorithm [1]. Additionally, we achieve statistical efficiency for the inhomogeneous transition model, which is a more general setting than that of Hwang and Oh [35]. Our computation cost per episode is $\\mathcal{O}(1)$ while the computation cost per episode of Hwang and Oh [35] is $\\mathcal{O}(K)$ . ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Proof Sketch of Theorem 1 We provide the proof sketch of Theorem 1. By decomposing the regret into the estimation part and the pessimism part, we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}(V_{1}^{*}-V_{1}^{\\pi_{k}})(s_{1}^{k})=\\sum_{k=1}^{K}\\Big(\\underbrace{V_{1}^{*}-V_{1}^{k}}_{\\mathrm{Pessimism}}+\\underbrace{V_{1}^{k}-V_{1}^{\\pi_{k}}}_{\\mathrm{Estimation}}\\Big)(s_{1}^{k})\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We bound these two parts separately. For the estimation part, for each $k\\in[K],h\\in[H]$ , we first show that the online estimated transition core $\\theta_{h}^{k}$ (2) concentrates around the unknown transition core parameter $\\pmb{\\theta}_{h}^{*}$ with high probability (Lemma 1). Then, we show that the prediction error induced by the estimated transition core can be bounded by the weighted norm of the dominant feature $\\hat{\\varphi}$ , multiplied by the confidence radius of the estimated transition core (Lemma 4). The bounded prediction error, together with the concentration of Gaussian noise, implies the desired bound on the estimation part (Lemma 10). For the pessimism part, we first show that the stochastically optimistic value function $V_{1}^{k}$ is optimistic than the true optimal value function $V_{1}^{*}$ with sufficient frequency (Lemma 6). In the next step, we show that the pessimism part is upper bounded by a bound of the estimation part times the inverse probability of being optimistic (Lemma 11). Combining all the results, we can conclude the proof. Refer to Appendix C for detailed proofs. ", "page_idx": 6}, {"type": "text", "text": "4 Statistically Improved Algorithm for MNL-MDPs ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Although RRL-MNL is provably efficient and achieves constant-time computational cost per episode, the current analysis makes its regret bound scale with $\\kappa^{-1}$ . Recall that the problem-dependent constant $\\kappa$ introduced in Assumption 4 indicates the curvature of the MNL function, i.e., how difficult it is to learn the true transition core parameter. It is required to ensure the non-singular Fisher information matrix, hence is typically used in GLM or MNL bandit algorithms that use the maximum likelihood estimator. As introduced in Faury et al. [23], $\\kappa^{-1}$ can be exponentially large in the worst case. The appearance of $\\kappa$ in existing bounds originates in the connection between the difference of estimators and the difference of gradients of negative log-likelihood, usually denoted as $\\mathbf{G}$ in Filippi et al. [26]. Without considering local information at all, using a loose lower bound for $\\mathbf{G}$ incurs $\\kappa^{-1}$ in regret bound (see Section 4.1 in Agrawal et al. [6]). Recently, improved dependence on $\\kappa$ has been achieved in bandit literature [23, 3, 61, 6, 76, 50] through the use of generalization of the Bernstein-like tail inequality [23] and the self-concordant-like property of the log loss [11]. However, a direct adaptation of the MNL bandit technique would result in sub-optimal dependence on the assortment size in MNL bandit, which corresponds to the size of the set of reachable states, such as $\\boldsymbol{\\mathcal{U}}$ . In this section, we introduce a new randomized algorithm for MNL-MDPs, equipped with a tight online parameter estimation and feature centralization technique that achieves a regret bound with improved dependence on $\\kappa$ and $\\boldsymbol{\\mathcal{U}}$ . ", "page_idx": 6}, {"type": "text", "text": "4.1 Algorithms: ORRL-MNL ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Tight online transition core estimation Zhang and Sugiyama [76] presented a jointly efficient UCB-based MNL contextual bandit algorithm using online mirror descent algorithm. Adapting the update rule from [76], the estimated transition core run by the online mirror descent is given by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widetilde{\\pmb{\\theta}}_{h}^{k+1}=\\operatorname*{argmin}_{\\pmb{\\theta}\\in\\mathcal{B}_{d}(L_{\\pmb{\\theta}})}\\frac{1}{2\\eta}\\|\\pmb{\\theta}-\\widetilde{\\pmb{\\theta}}_{h}^{k}\\|_{\\widetilde{\\mathbf{B}}_{k,h}}^{2}+\\pmb{\\theta}^{\\top}\\nabla\\ell_{k,h}(\\widetilde{\\pmb{\\theta}}_{h}^{k})\\,,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\widetilde{\\pmb{\\theta}}_{h}^{1}$ can be initialized as any point in $B_{d}(L_{\\theta})$ , $\\eta$ is a step size, and $\\widetilde{\\mathbf{B}}_{k,h}$ is defined as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf{B}}_{k,h}:=\\mathbf{B}_{k,h}+\\eta\\nabla^{2}\\ell_{k,h}(\\widetilde{\\pmb{\\theta}}_{h}^{k})\\,,\\quad\\mathbf{B}_{k,h}:=\\lambda\\mathbf{I}_{d}+\\sum_{i=1}^{k-1}\\nabla^{2}\\ell_{i,h}(\\widetilde{\\pmb{\\theta}}_{h}^{i+1})\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "1: Inputs: Episodic MDP $\\mathcal{M}$ , Feature map $\\varphi:S\\times A\\times S\\,\\rightarrow\\,\\mathbb{R}^{d}$ , Number of episodes $K$ ,   \nRegularization parameter $\\lambda$ , Exploration variance $\\{\\sigma_{k}\\}_{k=1}^{K}$ , Confidence radius $\\{\\beta_{k}\\}_{k=1}^{K}$ , Sample   \nsize $M$ , Step size $\\eta$   \n2: Initialize: $\\widetilde{\\pmb{\\theta}}_{h}^{1}=\\mathbf{0}_{d}$ , $\\mathbf{B}_{1,h}=\\lambda\\mathbf{I}_{d}$ for all $h\\in[H]$   \n3: for episode $k=1,2,\\cdots\\,,K$ do   \n4: Observe $s_{1}^{k}$ and sample i.i.d. noise vector $\\boldsymbol{\\xi}_{k,h}^{(m)}\\sim\\mathcal{N}(\\mathbf{0}_{d},\\sigma_{k}^{2}\\mathbf{B}_{k,h}^{-1})$ for $m\\in[M]$ and $h\\in[H]$   \n5: Set $\\left\\{\\widetilde{Q}_{h}^{k}(\\cdot,\\cdot)\\right\\}_{h\\in[H]}$ as described in (7)   \n6: for horizon $h=1,2,\\cdots\\,,H$ do   \n7: Select $a_{h}^{k}=\\operatorname{argmax}_{a\\in A}\\widetilde{Q}_{h}^{k}(s_{h}^{k},a)$ and observe $s_{h+1}^{k}$   \n8: Update $\\widetilde{\\mathbf{B}}_{k,h}=\\mathbf{B}_{k,h}+\\eta\\nabla^{2}\\ell_{k,h}(\\widetilde{\\pmb{\\theta}}_{h}^{k})$ and $\\widetilde{\\pmb{\\theta}}_{h}^{k+1}$ as in (5)   \n9: Update $\\mathbf{B}_{k+1,h}=\\mathbf{B}_{k,h}+\\nabla^{2}\\ell_{k,h}(\\widetilde{\\pmb{\\theta}}_{h}^{k+1})$   \n10: end for   \n11: end for ", "page_idx": 7}, {"type": "text", "text": "Note that the MNL model in Zhang and Sugiyama [76] operates in a multiple-parameter setting, where there are multiple unknown choice parameters and one given context feature. In contrast, our MNL model operates in a single-parameter setting, where there is one unknown transition core and features for up to $\\boldsymbol{\\mathcal{U}}$ reachable states. This difference results in variations in applying the self-concordant-like property of the log-loss for the MNL model. For instance, Z\u221ahang and Sugiyama [76] utilized the fact that the log-loss for the multiple parameter MNL model is $\\sqrt{6}$ -self-concordantlike (Lemma 2 in Zhang and Sugiyama [76]). On the other hand, Lee and Oh [50] revisit the self-\u221aconcordant-like property and demonstrate that the log-loss of the single-parameter MNL model is $3{\\sqrt{2}}.$ -self-concordant-like (Proposition B.1 in Lee and Oh [50]). This results in a concentration bound that is independent of $\\kappa$ and $\\boldsymbol{\\mathcal{U}}$ , introduced in Lemma 12. ", "page_idx": 7}, {"type": "text", "text": "Remark 3. Note that the online estimated parameters $\\theta_{h}^{k}$ (2) and $\\widetilde{\\pmb{\\theta}}_{h}^{k}$ (5) do not aim to minimize the sum of negative log-likelihoods, $\\sum_{k^{\\prime}=1}^{k}\\ell_{k^{\\prime},h}(\\pmb{\\theta})$ . Instead, w\u2217e s how that the online estimated parameter concentrates around the  unknown transition core $\\pmb{\\theta}_{h}^{*}$ with high probability (Lemma & 12). This online update approach allows us to estimate the transition core with constant-time computational cost per episode, as the agent does not need to store all samples from previous episodes. ", "page_idx": 7}, {"type": "text", "text": "Optimistic randomized value function To achieve improved dependence on $\\kappa$ , a crucial point is to utilize the local gradient information of MNL transition probabilities for each reachable state when constructing the Gram matrix. In MNL bandit problems [61, 76], this can be accomplished by substituting the Hessian of the negative log-likelihood with the Gram matrix using global gradient information $\\kappa$ . However, there are fundamental differences between the settings in Perivier and Goyal [61], Zhang and Sugiyama [76] and ours. Perivier and Goyal [61] address the case where the reward for each product is uniform (i.e., all products have a reward of 1), and the reward for not selecting a product from the given assortment (also known as the outside option) is 0. On the other hand, Zhang and Sugiyama [76] deal with non-uniform rewards where the reward for each product may vary; however, the rewards for individual products are known a priori to the agent. In contrast, in MNL-MDPs, the value for each reachable state may vary (non-uniform) and is not known beforehand. Due to these differences, the analysis techniques in MNL bandits [61, 76] cannot be directly applied to our setting. Instead, we adapt the feature centralization technique [50]. Then, the Hessian of the per-round loss $\\ell_{k,h}(\\pmb\\theta)$ is expressed in terms of the centralized feature as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\nabla^{2}\\ell_{k,h}(\\pmb\\theta)=\\sum_{s^{\\prime}\\in S_{k,h}}P_{\\pmb\\theta}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\bar{\\varphi}(s_{h}^{k},a_{h}^{k},s^{\\prime};\\pmb\\theta)\\bar{\\varphi}(s_{h}^{k},a_{h}^{k},s^{\\prime};\\pmb\\theta)^{\\top}\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\bar{\\varphi}(s,a,s^{\\prime};\\theta):=\\varphi(s,a,s^{\\prime})-\\mathbb{E}_{\\widetilde{s}\\sim P_{\\theta}(\\cdot|s,a)}[\\varphi(s,a,\\widetilde{s})]$ is the centralized feature by $\\pmb{\\theta}$ . For more details, please refer to Appendix D.2. ", "page_idx": 7}, {"type": "text", "text": "Now we introduce the optimistic randomized value function $\\widetilde{Q}_{h}^{k}(\\cdot,\\cdot)$ for ORRL-MNL. The key point is that when perturbing the estimated value function, we use the centralized feature by the estimated ", "page_idx": 7}, {"type": "text", "text": "transition parameter $\\widetilde{\\pmb{\\theta}}_{h}^{k}$ . For any $(s,a)\\in S\\times A$ , set $\\widetilde{Q}_{H+1}^{k}(s,a)=0$ and for each $h\\in[H]$ , ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\widetilde{Q}_{h}^{k}(s,a):=\\operatorname*{min}\\left\\{r(s,a)+\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\widetilde{\\theta}_{h}^{k}}(s^{\\prime}\\mid s,a)\\widetilde{V}_{h+1}^{k}(s^{\\prime})+\\nu_{k,h}^{\\mathrm{rand}}(s,a)\\,,H\\right\\},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\widetilde{V}_{h}^{k}(s):=\\operatorname*{max}_{a\\in\\mathcal{A}}\\widetilde{Q}_{h}^{k}(s,a)$ and $\\nu_{k,h}^{\\mathrm{rand}}(s,a)$ is the randomized bonus term defined by ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\nu_{k,h}^{\\mathrm{rand}}(s,a):=\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\tilde{\\theta}_{h}^{k}}(s^{\\prime}\\mid s,a)\\bar{\\varphi}(s,a,s^{\\prime};\\widetilde{\\theta}_{h}^{k})^{\\top}\\xi_{k,h}^{s^{\\prime}}+3H\\beta_{k}^{2}\\operatorname*{max}_{s^{\\prime}\\in S_{s,a}}\\|\\varphi(s,a,s^{\\prime})\\|_{\\mathbf{B}_{k,h}^{-1}}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Here we sample i.i.d. Gaussian noise $\\boldsymbol{\\xi}_{k,h}^{(m)}\\sim\\mathcal{N}(\\mathbf{0}_{d},\\sigma_{k}^{2}\\mathbf{B}_{k,h}^{-1})$ for each $m\\in[M]$ and set $\\xi_{k,h}^{s^{\\prime}}:=$ \u03bekm,(hs\u2032)where m(s\u2032) := argmaxm\u2208[M] \u03c6\u00af(s, a, s\u2032; \u03b8 kh)\u22a4\u03bekm,h is the most optimistic sampling index for a reachable state $s^{\\prime}$ . Based on these optimistic randomized value function, at each episode the agent plays a greedy action with respect to $\\widetilde{Q}_{h}^{k}$ as summarized in Algorithm 2. ", "page_idx": 8}, {"type": "text", "text": "Remark 4. Note that the second term in the randomized bonus always has a positive value, but it rapidly decreases as episode proceeds. While due to the randomness of $\\xi,$ , the randomized bonus $\\nu_{k,h}^{\\mathrm{rand}}$ itself cannot be guaranteed to always have a positive value. Consequently, the constructed value function $\\widetilde{Q}_{h}^{k}(\\cdot,\\cdot)$ can be optimistic or pessimistic. However, as shown in Lemma 18, optimistic sampling technique ensures that the optimistic randomized value function $\\widetilde{Q}_{h}^{k}$ has at least a constant probability of being optimistic than the true optimal value function. ", "page_idx": 8}, {"type": "text", "text": "Remark 5. As with RRL-MNL, since the transition core is estimated in an online manner and the Gram matrices with local gradient information $\\mathbf{B}_{k,h}$ and $\\widetilde{\\mathbf{B}}_{k,h}$ are updated incrementally, ORRL-MNL also requires constant-time computational cost and storage cost per-episode. Although ORRL-MNL requires an additional $\\mathcal O(\\mathcal{U})$ computation cost for feature centralization, the computation complexity order is the same as that of RRL-MNL because it also needs to go over reachable states to calculate the dominant feature $\\hat{\\varphi}$ , which also incurs a $\\mathcal{O}(\\mathcal{U})$ computation cost. On the other hand, ORRL-MNL does not require prior knowledge of $\\kappa$ and achieves a regret with a better dependence on $\\kappa$ . ", "page_idx": 8}, {"type": "text", "text": "4.2 Regret Bound of ORRL-MNL ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We present the regret upper bound of ORRL-MNL. The complete proof is deferred to Appendix D. ", "page_idx": 8}, {"type": "text", "text": "Theorem 2 (Regret Bound of ORRL-MNL). Suppose that Assumption 1- 4 hold. For any $0~<$ $\\delta\\ <\\ \\ \\frac{\\Phi(-1)}{2}$ , if we set the input parameters in Algorithm 2 as $\\lambda~=~\\mathcal{O}(L_{\\varphi}^{2}d\\log\\mathcal{U}),\\beta_{k}~=$ $\\mathcal{O}(\\sqrt{d}\\log\\mathcal{U}\\log(k H)),\\sigma_{k}=H\\beta_{k}$ , $\\begin{array}{r}{M=\\lceil1-\\frac{\\log(H\\mathcal{U})}{\\log\\Phi(1)}\\rceil}\\end{array}$ , an $\\eta=\\mathcal{O}(\\log\\mathcal{U})$ , then with probability at least $1-\\delta$ , the cumulative regret of the ORRL-MNL policy $\\pi$ is upper-bounded as follows: ", "page_idx": 8}, {"type": "equation", "text": "$$\nR e g r e t_{\\pi}(K)=\\widetilde{\\mathcal{O}}\\left(d^{3/2}H^{3/2}\\sqrt{T}+\\kappa^{-1}d^{2}H^{2}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $T=K H$ is the total number of time steps. ", "page_idx": 8}, {"type": "text", "text": "Dicussion of Theorem 2 Theorem 2 establishes that the leading term in the regret bound does not suffer from the problem-dependent constant $\\kappa^{-1}$ and the second term of the regret bound is independent of the size of set of reachable states. To the extent of our knowledge, this is the first algorithm that provides a frequentist regret guarantee with improved dependence on $\\kappa^{-1}$ in MNLMDPs. Compared to RRL-MNL, the technical challenge lies in ensuring the stochastic optimism of the estimated value for ORRL-MNL. Note that the prediction error (Definition 1) for ORRL-MNL is characterized by two components: one related to the gradient information of the MNL transition model at each reachable state, and the other related to the dominant feature with respect to the Gram matrix $\\mathbf{B}_{k,h}$ (Lemma 16). Hence, the probability of the Bellman error at each horizon, when following the optimal policy, being negative can depend on the size of the reachable states. This implies that the probability of stochastic optimism can be exponentially small, not only in the horizon $H$ but also in the size of the reachable states $\\boldsymbol{\\mathcal{U}}$ . However, as shown in Lemma 18, this challenge has been overcome by using a sample size $M$ that logarithmically increases with $\\boldsymbol{\\mathcal{U}}$ , effectively addressing the issue. ", "page_idx": 8}, {"type": "image", "img_path": "7tRtH0AoBl/tmp/d5942b8d6ccd6c6e849b0d28a42ceaeb6d0f56790cce477ad7a214d4897d4490.jpg", "img_caption": ["Figure 1: Riverswim experiment results "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Proof Sketch of Theorem 2 The overall proof pipeline for Theorem 2 is similar to that of Theorem 1. The main differences lie in the concentration of the estimated transition core (Lemma D.2), the bound on the prediction error (Lemma D.2), and the stochastic optimism (Lemma 18). Please refer to Appendix D for detailed proofs. ", "page_idx": 9}, {"type": "text", "text": "Optimistic exploration extension In general, since TS-based randomized exploration requires a more rigorous proof technique than UCB-based algorithms, our technical ingredients enable the use of optimistic exploration in a straightforward manner. We introduce UCRL-MNL $^+$ (Algorithm 3) in the Appendix E, an optimism-based algorithm for MNL-MDPs. It is both computationally and statistically efficient compared to UCRL-MNL [35], achieving the tightest regret bound for MNLMDPs. ", "page_idx": 9}, {"type": "text", "text": "Corollary 1. UCRL-MNL $^+$ (Algorithm 3) has $\\tilde{\\mathcal{O}}(d H^{3/2}\\sqrt{T}+\\kappa^{-1}d^{2}H^{2})$ regret with high probability. ", "page_idx": 9}, {"type": "text", "text": "5 Numerical Experiments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We perform a numerical evaluation on a variant of RiverSwim [58] to demonstrate practicality of our proposed algorithms. We compare our algorithms (RRL-MNL, ORRL-MNL, UCRL-MNL+) with the state-of-the-art UCRL-MNL [35] for MNL-MDPs. For each configuration, we report the averaged results over 10 independent runs. Figure 1a and 1b show the episodic return of each algorithm, which is the sum of all the rewards obtained in one episode. First, our proposed algorithms (RRL-MNL, ORRL-MNL, UCRL-MNL $^+$ ) outperform UCRL-MNL [35] for both cases of $|\\bar{S}|=4,8$ . Second, ORRL-MNL and UCRL-MNL $^+$ reach the optimal values quickly compared to the other algorithms, demonstrating improved statistical efficiency. Figure 1c illustrates the comparison in running time of the algorithms for the first 1,000 episodes. Our proposed algorithms are at least 50 times faster than UCRL-MNL. These differences become more pronounced as the episodes progress because our algorithms have a constant computation cost, whereas the computation cost of UCRL-MNL increases over time. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose randomized algorithms with provable efficiency and constant-time computational cost for MNL-MDPs. For the first algorithm, RRL-MNL, we use an optimistic sampling technique to ensure the stochastic optimism of the estimated value functions and provide the frequentist regret analysis. This is the first frequentist regret analysis for a non-linear model-based algorithm with randomized exploration without assuming stochastic optimism. To achieve a statistically improved regret bound, we propose ORRL-MNL by constructing the optimistic randomized value function using the effects of the local gradient of the MNL transition model equipped with the centralized feature. As a result, we achieve a frequentist regret guarantee with improved dependence on $\\kappa$ in RL with the MNL transition model, which is a significant contribution. The effectiveness and practicality of our methods are supported by numerical experiments. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We sincerely thank the anonymous reviewers for their constructive feedback. This work was supported by the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (No. 2022R1C1C1006859, 2022R1A4A1030579, and RS-2023-00222663) and by AI-Bio Research Grant through Seoul National University. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Yasin Abbasi-Yadkori, D\u00e1vid P\u00e1l, and Csaba Szepesv\u00e1ri. Improved algorithms for linear stochastic bandits. Advances in neural information processing systems, 24:2312\u20132320, 2011.   \n[2] Marc Abeille and Alessandro Lazaric. Linear Thompson Sampling Revisited. In Aarti Singh and Jerry Zhu, editors, Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pages 176\u2013184. PMLR, PMLR, 20\u201322 Apr 2017.   \n[3] Marc Abeille, Louis Faury, and Cl\u00e9ment Calauz\u00e8nes. Instance-wise minimax-optimal algorithms for logistic bandits. In International Conference on Artificial Intelligence and Statistics, pages 3691\u20133699. PMLR, 2021.   \n[4] Alekh Agarwal and Tong Zhang. Model-based rl with optimistic posterior sampling: Structural conditions and sample complexity. Advances in Neural Information Processing Systems, 35: 35284\u201335297, 2022.   \n[5] Alekh Agarwal and Tong Zhang. Non-linear reinforcement learning in large action spaces: Structural conditions and sample-efficiency of posterior sampling. In Conference on Learning Theory, pages 2776\u20132814. PMLR, 2022.   \n[6] Priyank Agrawal, Theja Tulabandhula, and Vashist Avadhanula. A tractable online learning algorithm for the multinomial logit contextual bandit. European Journal of Operational Research, 2023.   \n[7] Shipra Agrawal and Randy Jia. Posterior sampling for reinforcement learning: worst-case regret bounds. In Advances in Neural Information Processing Systems, pages 1184\u20131194, 2017.   \n[8] Sanae Amani and Christos Thrampoulidis. Ucb-based algorithms for multinomial logistic regression bandits. Advances in Neural Information Processing Systems, 34:2913\u20132924, 2021.   \n[9] Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement learning with value-targeted regression. In International Conference on Machine Learning, pages 463\u2013474. PMLR, 2020.   \n[10] Mohammad Gheshlaghi Azar, Ian Osband, and R\u00e9mi Munos. Minimax regret bounds for reinforcement learning. In International Conference on Machine Learning, pages 263\u2013272. PMLR, 2017.   \n[11] Francis Bach. Self-concordant analysis for logistic regression. Electronic Journal of Statistics, 4(2):384 \u2013 414, 2010.   \n[12] Peter L Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. The Annals of Statistics, 2005.   \n[13] Steven J Bradtke and Andrew G Barto. Linear least-squares algorithms for temporal difference learning. Machine learning, 22(1):33\u201357, 1996.   \n[14] Qi Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang. Provably efficient exploration in policy optimization. In International Conference on Machine Learning, pages 1283\u20131294. PMLR, 2020.   \n[15] Nicolo Campolongo and Francesco Orabona. Temporal variability in implicit online learning. Advances in neural information processing systems, 33:12377\u201312387, 2020.   \n[16] Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. Advances in neural information processing systems, 24, 2011.   \n[17] Xi Chen, Yining Wang, and Yuan Zhou. Dynamic assortment optimization with changing contextual information. Journal of machine learning research, 2020.   \n[18] Zixiang Chen, Chris Junchi Li, Huizhuo Yuan, Quanquan Gu, and Michael Jordan. A general framework for sample-efficient function approximation in reinforcement learning. In The Eleventh International Conference on Learning Representations, 2023.   \n[19] Christoph Dann, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. On oracle-efficient pac rl with rich observations. In Advances in Neural Information Processing Systems, volume 31, 2018.   \n[20] Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford. Provably efficient rl with rich observations via latent state decoding. In International Conference on Machine Learning, pages 1665\u20131674. PMLR, 2019.   \n[21] Simon Du, Sham Kakade, Jason Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and Ruosong Wang. Bilinear classes: A structural framework for provable generalization in rl. In International Conference on Machine Learning, pages 2826\u20132836. PMLR, 2021.   \n[22] Simon S. Du, Sham M. Kakade, Ruosong Wang, and Lin F. Yang. Is a good representation sufficient for sample efficient reinforcement learning? In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020.   \n[23] Louis Faury, Marc Abeille, Cl\u00e9ment Calauz\u00e8nes, and Olivier Fercoq. Improved optimistic algorithms for logistic bandits. In International Conference on Machine Learning, pages 3052\u20133060. PMLR, 2020.   \n[24] Louis Faury, Marc Abeille, Kwang-Sung Jun, and Cl\u00e9ment Calauz\u00e8nes. Jointly efficient and optimal algorithms for logistic bandits. In International Conference on Artificial Intelligence and Statistics, pages 546\u2013580. PMLR, 2022.   \n[25] Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Francisco J R Ruiz, Julian Schrittwieser, Grzegorz Swirszcz, et al. Discovering faster matrix multiplication algorithms with reinforcement learning. Nature, 610(7930):47\u201353, 2022.   \n[26] Sarah Filippi, Olivier Capp\u00e9, Aur\u00e9lien Garivier, and Csaba Szepesv\u00e1ri. Parametric bandits: The generalized linear case. In Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1, NIPS\u201910, page 586\u2013594, Red Hook, NY, USA, 2010. Curran Associates Inc.   \n[27] Dylan J Foster, Satyen Kale, Haipeng Luo, Mehryar Mohri, and Karthik Sridharan. Logistic regression: The importance of being improper. In Conference On Learning Theory, pages 167\u2013208. PMLR, 2018.   \n[28] Dylan J Foster, Sham M Kakade, Jian Qian, and Alexander Rakhlin. The statistical complexity of interactive decision making. arXiv preprint arXiv:2112.13487, 2021.   \n[29] David A Freedman. On tail probabilities for martingales. the Annals of Probability, pages 100\u2013118, 1975.   \n[30] Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex optimization. Machine Learning, 69(2):169\u2013192, 2007.   \n[31] Elad Hazan, Tomer Koren, and Kfir Y Levy. Logistic regression: Tight bounds for stochastic and online optimization. In Conference on Learning Theory, pages 197\u2013209. PMLR, 2014.   \n[32] Elad Hazan et al. Introduction to online convex optimization. Foundations and Trends\u00ae in Optimization, 2(3-4):157\u2013325, 2016.   \n[33] Jiafan He, Dongruo Zhou, and Quanquan Gu. Logarithmic regret for reinforcement learning with linear function approximation. In International Conference on Machine Learning, pages 4171\u20134180. PMLR, 2021.   \n[34] Jiafan He, Heyang Zhao, Dongruo Zhou, and Quanquan Gu. Nearly minimax optimal reinforcement learning for linear markov decision processes. In International Conference on Machine Learning, pages 12790\u201312822. PMLR, 2023.   \n[35] Taehyun Hwang and Min-hwan Oh. Model-based reinforcement learning with multinomial logistic function approximation. In Proceedings of the AAAI conference on artificial intelligence, pages 7971\u20137979, 2023.   \n[36] Taehyun Hwang, Kyuwook Chai, and Min-Hwan Oh. Combinatorial neural bandits. In Proceedings of the 40th International Conference on Machine Learning. PMLR, 2023.   \n[37] Haque Ishfaq, Qiwen Cui, Viet Nguyen, Alex Ayoub, Zhuoran Yang, Zhaoran Wang, Doina Precup, and Lin Yang. Randomized exploration in reinforcement learning with general value function approximation. In International Conference on Machine Learning, volume 139, pages 4607\u20134616. PMLR, PMLR, 2021.   \n[38] Haque Ishfaq, Qingfeng Lan, Pan Xu, A. Rupam Mahmood, Doina Precup, Anima Anandkumar, and Kamyar Azizzadenesheli. Provable and practical: Efficient exploration in reinforcement learning via langevin monte carlo. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=nfIAEJFiBZ.   \n[39] Haque Ishfaq, Yixin Tan, Yu Yang, Qingfeng Lan, Jianfeng Lu, A Rupam Mahmood, Doina Precup, and Pan Xu. More efficient randomized exploration for reinforcement learning via approximate sampling. Reinforcement Learning Journal, 3(1), 2024.   \n[40] Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning. Journal of Machine Learning Research, 11(4), 2010.   \n[41] Zeyu Jia, Lin Yang, Csaba Szepesvari, and Mengdi Wang. Model-based reinforcement learning with value-targeted regression. In Learning for Dynamics and Control, pages 666\u2013686. PMLR, 2020.   \n[42] Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contextual decision processes with low bellman rank are pac-learnable. In International Conference on Machine Learning, pages 1704\u20131713. PMLR, 2017.   \n[43] Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In Conference on Learning Theory, pages 2137\u2013 2143. PMLR, 2020.   \n[44] Chi Jin, Qinghua Liu, and Sobhan Miryoosef.i Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms. Advances in neural information processing systems, 34:13406\u201313418, 2021.   \n[45] Kwang-Sung Jun, Aniruddha Bhargava, Robert Nowak, and Rebecca Willett. Scalable generalized linear bandits: Online computation and hashing. Advances in Neural Information Processing Systems, 30, 2017.   \n[46] Yeoneung Kim, Insoon Yang, and Kwang-Sung Jun. Improved regret analysis for varianceadaptive linear bandits and horizon-free linear mixture mdps. Advances in Neural Information Processing Systems, 35:1060\u20131072, 2022.   \n[47] Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The International Journal of Robotics Research, 32(11):1238\u20131274, 2013.   \n[48] Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Pac reinforcement learning with rich observations. Advances in Neural Information Processing Systems, 29:1840\u20131848, 2016.   \n[49] Branislav Kveton, Csaba Szepesv\u00e1ri, Mohammad Ghavamzadeh, and Craig Boutilier. Perturbedhistory exploration in stochastic linear bandits. In Uncertainty in Artificial Intelligence, pages 530\u2013540. PMLR, 2020.   \n[50] Joongkyu Lee and Min-hwan Oh. Nearly minimax optimal regret for multinomial logistic bandit. arXiv preprint arXiv:2405.09831, 2024.   \n[51] Lihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear contextual bandits. In International Conference on Machine Learning, pages 2071\u20132080. PMLR, 2017.   \n[52] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529\u2013533, 2015.   \n[53] Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh. Sample complexity of reinforcement learning using linearly combined model ensembles. In International Conference on Artificial Intelligence and Statistics, pages 2010\u20132020. PMLR, 2020.   \n[54] Min-hwan Oh and Garud Iyengar. Thompson sampling for multinomial logit contextual bandits. Advances in Neural Information Processing Systems, 32:3151\u20133161, 2019.   \n[55] Min-hwan Oh and Garud Iyengar. Multinomial logit contextual bandits: Provable optimality and practicality. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 9205\u20139213, 2021.   \n[56] Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the eluder dimension. In Advances in Neural Information Processing Systems, pages 1466\u20131474, 2014.   \n[57] Ian Osband and Benjamin Van Roy. Why is posterior sampling better than optimism for reinforcement learning? In International conference on machine learning, pages 2701\u20132710. PMLR, 2017.   \n[58] Ian Osband, Daniel Russo, and Benjamin Van Roy. (more) efficient reinforcement learning via posterior sampling. Advances in Neural Information Processing Systems, 26, 2013.   \n[59] Ian Osband, Benjamin Van Roy, and Zheng Wen. Generalization and exploration via randomized value functions. In International Conference on Machine Learning, pages 2377\u20132386. PMLR, 2016.   \n[60] Aldo Pacchiano, Philip Ball, Jack Parker-Holder, Krzysztof Choromanski, and Stephen Roberts. Towards tractable optimism in model-based reinforcement learning. In Uncertainty in Artificial Intelligence, pages 1413\u20131423. PMLR, 2021.   \n[61] Noemie Perivier and Vineet Goyal. Dynamic pricing and assortment under a contextual mnl demand. Advances in Neural Information Processing Systems, 35:3461\u20133474, 2022.   \n[62] Daniel Russo. Worst-case regret bounds for exploration via randomized value functions. Advances in Neural Information Processing Systems, 32, 2019.   \n[63] Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic exploration. In Advances in Neural Information Processing Systems, pages 2256\u20132264, 2013.   \n[64] Daniel J Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, Zheng Wen, et al. A tutorial on thompson sampling. Foundations and Trends\u00ae in Machine Learning, 11(1):1\u201396, 2018.   \n[65] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. nature, 550(7676):354\u2013359, 2017.   \n[66] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140\u20131144, 2018.   \n[67] Daniil Tiapkin, Denis Belomestny, Daniele Calandriello, Eric Moulines, Remi Munos, Alexey Naumov, Mark Rowland, Michal Valko, and Pierre M\u00e9nard. Optimistic posterior sampling for reinforcement learning with few samples and tight guarantees. Advances in Neural Information Processing Systems, 35:10737\u201310751, 2022.   \n[68] Ruosong Wang, Russ R Salakhutdinov, and Lin Yang. Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension. Advances in Neural Information Processing Systems, 33, 2020.   \n[69] Yining Wang, Ruosong Wang, Simon Shaolei Du, and Akshay Krishnamurthy. Optimism in reinforcement learning with generalized linear function approximation. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021.   \n[70] Gell\u00e9rt Weisz, Philip Amortila, and Csaba Szepesv\u00e1ri. Exponential lower bounds for planning in mdps with linearly-realizable optimal action-value functions. In Algorithmic Learning Theory, pages 1237\u20131264. PMLR, 2021.   \n[71] Lin Yang and Mengdi Wang. Sample-optimal parametric q-learning using linearly additive features. In International Conference on Machine Learning, pages 6995\u20137004. PMLR, 2019.   \n[72] Lin Yang and Mengdi Wang. Reinforcement learning in feature space: Matrix bandit, kernels, and regret bound. In International Conference on Machine Learning, pages 10746\u201310756. PMLR, 2020.   \n[73] Andrea Zanette, David Brandfonbrener, Emma Brunskill, Matteo Pirotta, and Alessandro Lazaric. Frequentist regret bounds for randomized least-squares value iteration. In International Conference on Artificial Intelligence and Statistics, pages 1954\u20131964. PMLR, 2020.   \n[74] Lijun Zhang, Tianbao Yang, Rong Jin, Yichi Xiao, and Zhi-Hua Zhou. Online stochastic linear optimization under one-bit feedback. In International Conference on Machine Learning, pages 392\u2013401. PMLR, 2016.   \n[75] Tong Zhang. Feel-good thompson sampling for contextual bandits and reinforcement learning. SIAM Journal on Mathematics of Data Science, 4(2):834\u2013857, 2022.   \n[76] Yu-Jie Zhang and Masashi Sugiyama. Online (multinomial) logistic bandit: Improved regret and constant computation cost. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[77] Zihan Zhang, Yuan Zhou, and Xiangyang Ji. Almost optimal model-free reinforcement learningvia reference-advantage decomposition. In Advances in Neural Information Processing Systems, volume 33, pages 15198\u201315207, 2020.   \n[78] Zihan Zhang, Jiaqi Yang, Xiangyang Ji, and Simon S Du. Improved variance-aware confidence sets for linear bandits and linear mixture mdp. Advances in Neural Information Processing Systems, 34:4342\u20134355, 2021.   \n[79] Zihan Zhang, Yuan Zhou, and Xiangyang Ji. Model-free reinforcement learning: from clipped pseudo-regret to sample complexity. In International Conference on Machine Learning, pages 12653\u201312662. PMLR, 2021.   \n[80] Dongruo Zhou and Quanquan Gu. Computationally efficient horizon-free reinforcement learning for linear mixture mdps. Advances in neural information processing systems, 35:36337\u201336349, 2022.   \n[81] Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. Nearly minimax optimal reinforcement learning for linear mixture markov decision processes. In Conference on Learning Theory, pages 4532\u20134576. PMLR, 2021.   \n[82] Dongruo Zhou, Jiafan He, and Quanquan Gu. Provably efficient reinforcement learning for discounted mdps with feature mapping. In International Conference on Machine Learning, pages 12793\u201312802. PMLR, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Contents of Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "B Notations & Definitions 18 ", "page_idx": 15}, {"type": "text", "text": "C Detailed Regret Analysis for RRL-MNL (Theorem 1) 22 ", "page_idx": 15}, {"type": "text", "text": "C.1 Concentration of Estimated Transition Core $\\theta_{h}^{k}$ 23   \nC.2 Bound on Prediction Error 32   \nC.3 Good Events with High Probability . . . . . 33   \nC.4 Stochastic Optimism . . . 33   \nC.5 Bound on Estimation Part . . 36   \nC.6 Bound on Pessimism Part . . . 38   \nC.7 Regret Bound of RRL-MNL 41 ", "page_idx": 15}, {"type": "text", "text": "D Detailed Regret Analysis for ORRL-MNL (Theorem 2) 42 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "D.1 Concentration of Estimated Transition Core $\\widetilde{\\pmb{\\theta}}_{h}^{k}$ 42   \nD.2 Bound on Prediction Error . 50   \nD.3 Good Events with High Probability . . . 55   \nD.4 Stochastic Optimism . . 55   \nD.5 Bound on Estimation Part . . . . 57   \nD.6 Bound on Pessimism Part . . . 66   \nD.7 Regret Bound of ORRL-MNL . 67   \nE Optimistic Exploration Extension 67   \nE.1 Optimism . 69 ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "F Experiment Details 70 ", "page_idx": 15}, {"type": "text", "text": "G Auxiliary Lemmas 70 ", "page_idx": 15}, {"type": "text", "text": "H Limitations ", "page_idx": 15}, {"type": "text", "text": "A Related Work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "RL with linear function approximation There has been a growing interest in studies that extend beyond tabular MDPs and focus on function approximation methods with provable guarantees [42, 71, 43, 73, 53, 22, 14, 9, 68, 70, 33, 81, 82, 37, 35, 38]. In particular, for minimizing regret in linear MDPs, Jin et al. [43] propose an optimistic variant of the Least-Squares Value Iteration (LSVI) algorithm [13, 59] under the assumption that the transition model and reward function of the MDPs are linear function of a $d$ -dimensional feature mapping and they guarantee $\\widetilde{\\mathcal{O}}(d^{{\\frac{3}{2}}}H^{{\\frac{3}{2}}}\\sqrt{T})$ regret. Zanette et al. [73] propose a randomized LSVI algorithm that incorporates exploration by perturbing\u221a the least-square approximation of the action-value function, and this algorithm guarantees $\\tilde{\\mathcal{O}}(d^{2}H^{2}\\bar{\\sqrt{T}})$ regret. Ishfaq et al. [37] propose a variant of the randomized LSVI algorithm that combines optimism a\u221and TS by perturbing the training data with i.i.d. scalar noise, achieving a regret bound of $\\tilde{\\mathcal{\\tilde{O}}}(d^{\\frac{3}{2}}H^{\\frac{3}{2}}\\sqrt{T})$ . Similarly, Ishfaq et al. [38] introduce a randomized RL algorithm that employs Langevin Monte Carlo (LMC) to approximate the posterior distribution of the action-value function, also ensuring a regret bound of $\\widetilde{\\mathcal{O}}(d^{{\\frac{3}{2}}}H^{{\\frac{3}{2}}}\\sqrt{T})$ . Also, there have been studies on modelbased methods with function approximati on in linear MDPs, such as Yang and Wang [72], which assume that the transition probability kernel is a bilinear mo\u221adel parametrized by a matrix and propose a UCB-based algorithm with an upper bound of $\\widetilde{\\mathcal{O}}(d^{\\frac{3}{2}}H^{2}\\sqrt{T})$ for regret. He et al. [34] propose an algorithm achieving nearly minimax optimal regret $\\widetilde{\\mathcal{O}}(d H\\sqrt{T})$ . Jia et al. [41] consider a specific type of MDPs called linear mixture MDPs in which the transition probability kernel is a linear combination of different basis kernels. This model encompasses various types of MDPs studied previously in Modi et al. [53], Yang and Wang [72]. For this model, Jia et al. [41] propose a UCB-based RL algorithm with value-targeted model parameter estimation that guarantees an upper bound of $\\widetilde{\\mathcal{O}}(d H^{{\\frac{3}{2}}}\\sqrt{T})$ for regret. The same linear mixture MDPs have been used in other studies such as Ayoub et al. [9], Zhou et al. [81, 82]. Specifically, in Zhou et al. [81], a variant of the method proposed by Jia et al. [41] is suggested and proved that the \u221aalgorithm guarantees an upper bound of $\\tilde{\\mathcal{O}}(d H\\sqrt{T})$ regret with a matching lower bound of $\\Omega(d H{\\sqrt{T}})$ for linear mixture MDPs. More recently, there are also works achieving horizon-free regret bounds for linear mixture MDPs [78, 46, 80]. ", "page_idx": 15}, {"type": "table", "img_path": "7tRtH0AoBl/tmp/35298574dbc4e87177859232ba50faaba3f9e4db9006ebf5ce60585415bfaf38.jpg", "table_caption": ["Table 1: This table compares the problem settings, online update, performance of the this paper with those of other methods in provable RL with function approximation. For computation cost, we only keep the dependence on the number of episode $K$ . "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "RL with non-linear function approximation Studies have been conducted on extending function approximation beyond linear models. Ayoub et al. [9], Wang et al. [68], Ishfaq et al. [37] provide upper bound for regret based on eluder dimension [63]. Also, there has been an effort to develop sample-efficient methods with more \u201cgeneral\u201d function approximation [48, 42, 19\u201321, 28, 37, 44, 4, 5, 75, 18, 39] However, these attempts may have been hindered by the difficulty of solving computationally intractable problems [48, 42, 19, 21, 28, 44, 18], the necessity of relying on stronger assumptions [20, 37], or the lack of discussion on how to define the posterior distribution supported by a given function class and how to draw the optimistic sample from the posterior [4, 5, 75]. That is why even after there exists a so-called \u201cgeneral function class\u201d-based result, it is often the case that the results in specific parametric models are still needed. Despite the large number of studies on RL with linear function approximation, there is limited research on extending beyond linear models to other parametric models. Wang et al. [69] use generalized linear function approximation, where the Bellman backup of any value function is assumed to be a generalized linear function of feature mapping. Hwang and Oh [35] discuss the limitations of linear function approximation and propose a UCB-based algorithm for MNL transition model in feature space achieving $\\widetilde{\\mathcal{O}}(d H^{{\\frac{3}{2}}}\\sqrt{T})$ . Ishfaq et al. [39] present TS-based RL algorithms that utilize approximate samplers, su ch as LMC or Underdamped LMC, to enhance the implementation and computational tractability of TS for RL with general function classes. ", "page_idx": 16}, {"type": "text", "text": "Contextual bandits Faury et al. [23] first provide a UCB-based algorithm with $\\kappa$ -independent regret for binary logistic bandit and Abeille et al. [3] present UCB & TS based algorithms achieving nearly minimax optimal regret for the same setting. Faury et al. [24] propose a jointly efficient UCB-based algorithm that achieve $\\kappa$ -independent regret bound with ${\\mathcal{O}}(\\log t)$ computation cost. In the context of MNL model, Oh and Iyengar [54] employ TS approach, while Oh and Iyengar [55] incorporate a combination of UC\u221aB exploration and online parameter updates for MNL bandits. Both of the methods have $\\mathcal{O}(\\kappa^{-1}\\sqrt{T})$ regret. Amani and Thrampoulidis [8] propose an optimistic algorithm with better dependence on $\\kappa$ . Agrawal et al. [6] design a UCB-based algorithm with $\\mathcal{O}(\\sqrt{T})$ regret bound without $\\kappa$ in its leading term, and Perivier and Goyal [61] establish $\\mathcal{O}(\\sqrt{T/\\kappa_{*}})$ regret for the uniform reward setting. Zhang and Sugiyama [76] develop jointly efficient UCB-based algorithm for non-uniform MNL bandit problem. Lee and Oh [50] propose nearly minimax optimal MNL bandit algorithm for both uniform and non-uniform reward structures. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "B Notations & Definitions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we formally summarize some definitions and notations used to analyze the proposed algorithm. ", "page_idx": 17}, {"type": "text", "text": "Inhomogeneous MNL transition model ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For $h\\in[H]$ , the probability of state transition to $s^{\\prime}\\in\\mathcal{S}_{s,a}$ when an action $a$ is taken at a state $s$ is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\nP_{h}(s^{\\prime}\\mid s,a):=P_{\\theta_{h}^{*}}(s^{\\prime}\\mid s,a)=\\frac{\\exp(\\varphi(s,a,s^{\\prime})^{\\top}\\theta_{h}^{*})}{\\sum_{\\tilde{s}\\in\\mathcal{S}_{s,a}}\\exp(\\varphi(s,a,\\tilde{s})^{\\top}\\theta_{h}^{*})}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The estimated transition probability parameterized by $\\pmb{\\theta}$ is denoted as ", "page_idx": 17}, {"type": "equation", "text": "$$\nP_{\\pmb\\theta}(s^{\\prime}\\mid s,a):=\\frac{\\exp(\\varphi(s,a,s^{\\prime})^{\\top}\\pmb\\theta)}{\\sum_{\\widetilde{s}\\in{\\cal S}_{s,a}}\\exp(\\varphi(s,a,\\widetilde{s})^{\\top}\\pmb\\theta)}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Feature vector ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We abbreviate the feature vector as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\varphi_{s,a,s^{\\prime}}:=\\varphi(s,a,s^{\\prime})\\ \\mathrm{~for~}(s,a,s^{\\prime})\\in\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S}_{s,a}\\,,}\\\\ &{\\varphi_{k,h,s^{\\prime}}:=\\varphi(s_{h}^{k},a_{h}^{k},s^{\\prime})\\ \\mathrm{~for~}(k,h)\\in[K]\\times[H]\\mathrm{~and~}s^{\\prime}\\in\\mathcal{S}_{k,h}:=\\mathcal{S}_{s_{h}^{k}}}\\\\ &{\\hat{\\varphi}_{k,h}(s,a):=\\varphi(s,a,\\hat{s})\\ \\mathrm{~for~}\\hat{s}:=\\underset{s^{\\prime}\\in\\mathcal{S}_{s,a}}{\\mathrm{argmax}}\\,\\|\\varphi(s,a,s^{\\prime})\\|_{\\mathbf{A}_{k,h}^{-1}}\\,,}\\\\ &{\\bar{\\varphi}_{s,a,s^{\\prime}}(\\theta):=\\bar{\\varphi}(s,a,s^{\\prime};\\theta)=\\varphi(s,a,s^{\\prime})-\\mathbb{E}_{\\tilde{s}\\sim P_{\\theta}(\\cdot\\vert s,a)}[\\varphi(s,a,\\tilde{s})]\\,,}\\\\ &{\\bar{\\varphi}_{k,h,s^{\\prime}}(\\theta):=\\bar{\\varphi}(s_{h}^{k},a_{h}^{k},s^{\\prime};\\theta)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Response variable $\\pmb{\\&}$ per-episode loss ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The response variable $y_{h}^{k}$ is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\ny_{h}^{k}:=[y_{h}^{k}(s^{\\prime})]_{s^{\\prime}\\in S_{k,h}}\\;\\mathrm{~where~}y_{h}^{k}(s^{\\prime}):=\\mathbb{1}(s_{h+1}^{k}=s^{\\prime})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The per-episode loss $\\ell_{k,h}(\\pmb\\theta)$ is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\ell_{k,h}(\\pmb{\\theta}):=-\\sum_{s^{\\prime}\\in S_{k,h}}y_{h}^{k}(s^{\\prime})\\log P_{\\pmb{\\theta}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\,,}\\\\ {\\displaystyle\\mathbf{G}_{k,h}(\\pmb{\\theta}):=\\nabla\\ell_{k,h}(\\pmb{\\theta})=\\sum_{s^{\\prime}\\in S_{k,h}}(P_{\\pmb{\\theta}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})-y_{h}^{k}(s^{\\prime}))\\varphi_{k,h,s^{\\prime}}\\,,}\\\\ {\\displaystyle\\mathbf{H}_{k,h}(\\pmb{\\theta}):=\\nabla^{2}\\ell_{k,h}(\\pmb{\\theta})}\\\\ {\\displaystyle=\\sum_{s^{\\prime}\\in S_{k,h}}P_{\\pmb{\\theta}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\varphi_{k,h,s^{\\prime}}\\varphi_{k,h,s^{\\prime}}^{\\top}-\\sum_{s^{\\prime}\\in S_{k,h}}\\sum_{\\widetilde{s}\\in S_{k,h}}P_{\\pmb{\\theta}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})P_{\\pmb{\\theta}}(\\widetilde{s}\\mid s_{h}^{k},a_{h}^{k})\\varphi_{k,h,s^{\\prime}}\\varphi_{k,h,s^{\\prime}}^{\\top}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Regularity constants ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "$H$ : Horizon length   \n$K$ : Episode number   \n$T=K H$ : Total number of interactions   \n$L_{\\varphi}:\\ell_{2}$ -norm upper bound of $\\varphi(s,a,s)$ , i.e., $\\|\\varphi(s,a,s^{\\prime})\\|_{2}\\leq L_{\\varphi}$ ,   \n$L_{\\theta}:\\ell_{2}$ -norm upper bound of $\\pmb{\\theta}_{h}^{*}$ , i.e., $\\lvert|\\pmb{\\theta}_{h}^{*}\\rvert\\rvert_{2}\\leq L_{\\pmb{\\theta}}$ ,   \n$\\kappa:$ Problem-dependent constant such that $\\operatorname*{inf}_{\\theta\\in{\\mathscr{B}_{d}}(L_{\\theta})}P_{\\theta}(s^{\\prime}\\mid s,a)P_{\\theta}(\\widetilde{s}\\mid s,a)\\geq\\kappa\\,,$ $\\boldsymbol{u}:$ Maximum cardinality of the set of reachable states, i.e., $\\mathcal{U}:=\\operatorname*{max}_{s,a}|S_{s,a}|$ . ", "page_idx": 18}, {"type": "text", "text": "Estimated transition core ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The estimated transition core for RRL-MNL is given by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\pmb{\\theta}_{h}^{k}=\\operatorname*{argmin}_{\\pmb{\\theta}\\in B_{d}(L_{\\theta})}\\frac{1}{2}\\|\\pmb{\\theta}-\\pmb{\\theta}_{h}^{k-1}\\|_{\\mathbf{A}_{k,h}}^{2}+(\\pmb{\\theta}-\\pmb{\\theta}_{h}^{k-1})^{\\top}\\nabla\\ell_{k-1,h}(\\pmb{\\theta}_{h}^{k-1})\\,,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and the estimated transition core for ORRL-MNL is given by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widetilde{\\pmb{\\theta}}_{h}^{k+1}=\\underset{\\pmb{\\theta}\\in\\mathcal{B}_{d}(L_{\\theta})}{\\mathrm{argmin}}\\,\\frac{1}{2\\eta}\\left\\|\\pmb{\\theta}-\\widetilde{\\pmb{\\theta}}_{h}^{k}\\right\\|_{\\widetilde{\\mathbf{B}}_{k,h}}^{2}+\\pmb{\\theta}^{\\top}\\nabla\\ell_{k,h}(\\widetilde{\\pmb{\\theta}}_{h}^{k})\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Gram matrices ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The Gram matrix with global gradient information $\\kappa$ is given by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{A}_{k,h}:=\\lambda\\mathbf{I}_{d}+\\frac{\\kappa}{2}\\sum_{i=1}^{k-1}\\sum_{s^{\\prime}\\in S_{i,h}}\\varphi(s_{h}^{i},a_{h}^{i},s^{\\prime})\\varphi(s_{h}^{i},a_{h}^{i},s^{\\prime})^{\\top}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The Gram matrices with local gradient information are given by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf{B}}_{k,h}:=\\mathbf{B}_{k,h}+\\eta\\nabla^{2}\\ell_{k,h}(\\widetilde{\\pmb{\\theta}}_{h}^{k})\\,\\mathrm{~and~}\\,\\mathbf{B}_{k,h}:=\\lambda\\mathbf{I}_{d}+\\sum_{i=1}^{k-1}\\nabla^{2}\\ell_{i,h}(\\widetilde{\\pmb{\\theta}}_{h}^{i+1})\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Confidence radius ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For some absolute constants $C_{\\beta},C_{\\xi}>0$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\alpha_{k}:=\\alpha_{k}(\\delta)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\sqrt{\\frac{8d}{\\kappa}}\\log\\left(1+\\frac{k\\mathcal{U}L_{\\varphi}^{2}}{d\\lambda}\\right)+\\left(\\frac{32L_{\\varphi}L_{\\theta}}{3}+\\frac{16}{\\kappa}\\right)\\log\\frac{\\left(1+\\lceil2\\log_{2}k\\mathcal{U}L_{\\varphi}L_{\\theta}\\rceil\\right)k^{2}}{\\delta}+2\\sqrt{2}+2\\lambda L_{\\theta}^{2}}\\\\ &{=\\tilde{\\mathcal{O}}(\\kappa^{-1/2}d^{1/2})\\,,}\\\\ &{\\beta_{k}:=\\beta_{k}(\\delta)=C_{\\beta}\\sqrt{\\log(\\lambda\\log(\\mathcal{U}k)+\\log(\\mathcal{U}k)\\log\\left(\\frac{H\\sqrt{1+2k}}{\\delta}\\right)+d\\log\\left(1+\\frac{k}{d\\lambda}\\right)\\right)+\\lambda L_{\\theta}}}\\\\ &{\\quad=\\mathcal{O}(\\sqrt{d}\\log{\\mathcal{U}\\log(k H)})\\,,}\\\\ &{\\gamma_{k}:=\\gamma_{k}(\\delta)=C_{\\xi}\\sigma_{k}\\sqrt{d\\log(M d/\\delta)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Filtration ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For an arbitrary set $X$ , we denote the $\\Sigma$ -algebra generated by $X$ as $\\Sigma(X)$ . Then we define the following filtrations ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal F_{k}:=\\Sigma\\left(\\left\\{s_{j}^{i},a_{j}^{i},r(s_{j}^{i},a_{j}^{i})\\mid i<k,j\\leq H\\right\\}\\cup\\left\\{\\xi_{i,j}^{(m)}\\mid i<k,j\\leq H,1\\leq m\\leq M\\right\\}\\right)\\,,}\\\\ &{\\mathcal F_{k,h}:=\\Sigma\\left(\\mathcal F_{k}\\cup\\left\\{s_{j}^{k},a_{j}^{k},r(s_{j}^{k},a_{j}^{k})\\mid j\\leq h\\right\\}\\cup\\left\\{\\xi_{k,j}^{(m)}\\mid j\\geq h,1\\leq m\\leq M\\right\\}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Pseudo-noise ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For RRL-MNL, the pseudo-noise is sampled as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\boldsymbol{\\xi}_{k,h}^{(m)}\\sim\\mathcal{N}(\\mathbf{0}_{d},\\sigma_{k}^{2}\\mathbf{A}_{k,h}^{-1})\\,,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and for ORRL-MNL, the pseudo-noise is sampled as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\boldsymbol{\\xi}_{k,h}^{(m)}\\sim\\mathcal{N}(\\mathbf{0}_{d},\\sigma_{k}^{2}\\mathbf{B}_{k,h}^{-1})\\,,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for $M$ times independently. ", "page_idx": 19}, {"type": "text", "text": "Estimated value functions ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The stochastically optimistic value function for RRL-MNL is defined as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle2_{H+1}^{k}(s,a)=0\\,,}\\\\ {\\displaystyle2_{h}^{k}(s,a)=\\operatorname*{min}\\left\\{r(s,a)+\\sum_{s^{\\prime}\\in{\\mathcal S}_{s,a}}P_{\\theta_{h}^{k}}(s^{\\prime}\\mid s,a)V_{h+1}^{k}(s^{\\prime})+\\operatorname*{max}_{m\\in[M]}\\hat{\\varphi}_{k,h}(s,a)^{\\top}\\xi_{k,h}^{(m)},H\\right\\}\\mathrm{for}\\ h\\in[H]}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The optimistic randomized value function for ORRL-MNL is defined as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\widetilde{Q}_{H+1}^{k}(s,a)=0\\,,}\\\\ {\\displaystyle\\widetilde{Q}_{h}^{k}(s,a):=\\operatorname*{min}\\left\\{r(s,a)+\\sum_{s^{\\prime}\\in{\\mathcal S}_{s,a}}P_{\\widetilde{\\theta}_{h}^{k}}(s^{\\prime}\\mid s,a)\\widetilde{V}_{h+1}^{k}(s^{\\prime})+\\nu_{k,h}^{\\mathrm{rand}}(s,a)\\,,H\\right\\}\\,\\mathrm{for}\\,\\,h\\in[H]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\nu_{k,h}^{\\mathrm{rand}}(s,a):=\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\tilde{\\theta}_{h}^{k}}(s^{\\prime}\\mid s,a)\\bar{\\varphi}(s,a,s^{\\prime};\\widetilde{\\theta}_{h}^{k})^{\\top}\\xi_{k,h}^{s^{\\prime}}+3H\\beta_{k}^{2}\\operatorname*{max}_{s^{\\prime}\\in S_{s,a}}\\|\\varphi(s,a,s^{\\prime})\\|_{\\mathbf{B}_{k,h}^{-1}}^{2}\\,,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Prediction error & Bellman error ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Definition 1 (Prediction error $\\&$ Bellman error). For any $(s,a)\\in S\\times A$ and $(k,h)\\in[K]\\times[H]$ , we define the prediction error about $\\theta_{h}^{k}$ as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Delta_{h}^{k}(s,a):=\\sum_{s^{\\prime}\\in S_{s,a}}\\left(P_{\\theta_{h}^{k}}(s^{\\prime}\\mid s,a)-P_{\\theta_{h}^{*}}(s^{\\prime}|s,a)\\right)V_{h+1}^{k}(s^{\\prime})\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Also we define the Bellman error as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\iota_{h}^{k}(s,a):=r(s,a)+P_{h}V_{h+1}^{k}(s,a)-Q_{h}^{k}(s,a)\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Good events ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For any $\\delta\\in(0,1)$ , we define the following good events: ", "page_idx": 19}, {"type": "text", "text": "For RRL-MNL, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{G}_{k,h}^{\\Delta}(\\delta):=\\left\\{\\left|\\Delta_{h}^{k}(s,a)\\right|\\leq H\\alpha_{k}(\\delta)\\|\\hat{\\varphi}_{k,h}(s,a)\\|_{\\mathbf{A}_{k,h}^{-1}}\\right\\}\\,,}\\\\ &{\\mathcal{G}_{k,h}^{\\xi}(\\delta):=\\left\\{\\displaystyle\\operatorname*{max}_{m\\in[M]}\\|\\xi_{k,h}^{(m)}\\|_{\\mathbf{A}_{k,h}}\\leq\\gamma_{k}(\\delta)\\right\\}\\,,}\\\\ &{\\mathcal{G}_{k,h}(\\delta):=\\left\\{\\mathcal{G}_{k,h}^{\\Delta}(\\delta)\\cap\\mathcal{G}_{k,h}^{\\xi}(\\delta)\\right\\}\\,,}\\\\ &{\\mathcal{G}_{k}(\\delta):=\\displaystyle\\prod_{h\\in[H]}\\mathcal{G}_{k,h}(\\delta)\\,,}\\\\ &{\\mathcal{G}(K,\\delta):=\\displaystyle\\prod_{k\\leq K}\\mathcal{G}_{k}(\\delta)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For ORRL-MNL, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Phi_{k,h}^{\\Delta}(\\delta):=\\left\\{\\left|\\Delta_{h}^{k}(s,a)\\right|\\leq H\\beta_{k}(\\delta)\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\bar{\\theta}_{h}}(\\varepsilon^{\\prime}\\mid s,a)\\left\\|\\bar{\\varphi}_{s,a,s^{\\prime}}(\\bar{\\theta}_{h}^{k})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}\\right.}\\\\ &{\\qquad\\qquad\\left.+3H\\beta_{k}(\\delta)^{2}\\displaystyle\\operatorname*{max}_{s^{\\prime}\\in S_{s,a}}\\left\\|\\varphi_{s,a,s^{\\prime}}\\right\\|_{\\mathbf{B}_{k,h}^{-1}}\\right\\},}\\\\ &{\\Phi_{k,h}^{\\Delta}(\\delta):=\\left\\{\\displaystyle\\operatorname*{max}_{m\\in[M]}\\{\\xi_{k,h}^{(\\alpha)}\\|_{\\mathbf{B}_{k,h}}\\leq\\gamma_{k}(\\delta)\\}\\right\\},}\\\\ &{\\Phi_{k,h}(\\delta):=\\left\\{\\Phi_{k,h}^{\\Delta}(\\delta)\\cap\\mathcal{E}_{k,h}^{\\delta}(\\delta)\\right\\},}\\\\ &{\\Phi_{k}(\\delta):=\\displaystyle\\bigcap_{k\\in[M]}\\Phi_{k,h}(\\delta)\\,,}\\\\ &{\\Phi(K,\\delta):=\\displaystyle\\bigcap_{k\\in[K]}\\Phi_{k}(\\delta)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Derivative of MNL transition model ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proposition 1 (Derivative of MNL transition model). The gradient and Hessian of $P_{\\theta}(\\cdot\\mid\\cdot,\\cdot)$ can be calculated as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\nabla P_{\\theta}(s^{\\prime}\\mid s,a)=P_{\\theta}(s^{\\prime}\\mid s,a)\\left(\\varphi_{s,a,s^{\\prime}}-\\displaystyle\\sum_{s^{\\prime\\prime}\\in S_{s,a}}P_{\\theta}(s^{\\prime\\prime}\\mid s,a)\\varphi_{s,a,s^{\\prime\\prime}}\\right)}}\\\\ {{\\displaystyle=P_{\\theta}(s^{\\prime}\\mid s,a)\\bar{\\varphi}_{s,a,s^{\\prime}}(\\theta)\\,,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla^{2}P_{\\theta}(s^{\\prime}\\mid s,a)}\\\\ &{\\ =P_{\\theta}(s^{\\prime}\\mid s,a)\\varphi_{s,a,s^{\\prime}}\\varphi_{s,a,s^{\\prime}}^{\\top}}\\\\ &{\\quad\\displaystyle-\\,P_{\\theta}(s^{\\prime}\\mid s,a)\\sum_{s^{\\prime\\prime}\\in S_{s,a}}P_{\\theta}(s^{\\prime\\prime}\\mid s,a)\\left(\\varphi_{s,a,s^{\\prime}}\\varphi_{s,a,s^{\\prime\\prime}}^{\\top}+\\varphi_{s,a,s^{\\prime\\prime}}\\varphi_{s,a,s^{\\prime}}^{\\top}+\\varphi_{s,a,s^{\\prime\\prime}}\\varphi_{s,a,s^{\\prime\\prime}}^{\\top}\\right)}\\\\ &{\\quad\\displaystyle+\\,2P_{\\theta}(s^{\\prime}\\mid s,a)\\left(\\sum_{s^{\\prime\\prime}\\in S_{s,a}}P_{\\theta}(s^{\\prime\\prime}\\mid s,a)\\varphi_{s,a,s^{\\prime\\prime}}\\right)\\left(\\sum_{s^{\\prime\\prime}\\in S_{s,a}}P_{\\theta}(s^{\\prime\\prime}\\mid s,a)\\varphi_{s,a,s^{\\prime\\prime}}\\right)^{\\top}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof of Proposition $^{\\,l}$ . Let $\\pmb{\\theta}=(\\theta_{1},\\ldots,\\theta_{d})$ and $[\\varphi_{s,a,s^{\\prime}}]_{i}$ be the $i$ -th component of $\\varphi_{s,a,s^{\\prime}}$ . Then, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial}{\\partial\\theta_{j}}P_{\\theta}(s^{\\prime}\\mid s,a)}\\\\ &{=\\frac{\\exp\\left(\\varphi_{s,a,s^{\\prime}}^{\\top}\\theta\\right)[\\varphi_{s,a,s^{\\prime}}]_{j}}{\\sum_{s^{\\prime\\prime}\\in S_{s,a}}\\exp\\left(\\varphi_{s,a,s^{\\prime\\prime}}^{\\top}\\theta\\right)}-\\frac{\\exp\\left(\\varphi_{s,a,s^{\\prime}}^{\\top}\\theta\\right)\\sum_{s^{\\prime\\prime}\\in S_{s,a}}\\exp\\left(\\varphi_{s,a,s^{\\prime\\prime}}^{\\top}\\theta\\right)[\\varphi_{s,a,s^{\\prime\\prime}}]_{j}}{\\left(\\sum_{s^{\\prime\\prime}\\in S_{s,a}}\\exp\\left(\\varphi_{s,a,s^{\\prime\\prime}}^{\\top}\\theta\\right)\\right)^{2}}}\\\\ &{=P_{\\theta}(s^{\\prime}\\mid s,a)\\left([\\varphi_{s,a,s^{\\prime}}]_{j}-\\displaystyle\\sum_{s^{\\prime\\prime}\\in S_{s,a}}P_{\\theta}(s^{\\prime\\prime}\\mid s,a)[\\varphi_{s,a,s^{\\prime\\prime}}]_{j}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, the gradient of $P_{\\theta}(s^{\\prime}\\mid s,a)$ is given by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla P_{\\theta}(s^{\\prime}\\mid s,a)=P_{\\theta}(s^{\\prime}\\mid s,a)\\varphi_{s,a,s^{\\prime}}-P_{\\theta}(s^{\\prime}\\mid s,a)\\displaystyle\\sum_{s^{\\prime\\prime}\\in S_{s,a}}P_{\\theta}(s^{\\prime\\prime}\\mid s,a)\\varphi_{s,a,s^{\\prime\\prime}}}\\\\ &{\\qquad\\qquad=P_{\\theta}(s^{\\prime}\\mid s,a)\\left(\\varphi_{s,a,s^{\\prime}}-\\displaystyle\\sum_{s^{\\prime\\prime}\\in S_{s,a}}P_{\\theta}(s^{\\prime\\prime}\\mid s,a)\\varphi_{s,a,s^{\\prime\\prime}}\\right)}\\\\ &{\\qquad\\qquad=P_{\\theta}(s^{\\prime}\\mid s,a)\\bar{\\varphi}_{s,a,s^{\\prime}}(\\theta)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "On the other hand, the second derivative $\\begin{array}{r}{\\frac{\\partial}{\\partial\\theta_{i}\\partial\\theta_{j}}P_{\\pmb{\\theta}}(s^{\\prime}\\mid s,a)}\\end{array}$ can be obtained as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{e}^{-\\mathrm{i}k\\theta_{1}^{\\prime}}\\mathrm{E}_{[1,6]}^{[1]}\\left(\\left|\\Phi_{q,i-1}\\right|\\right)\\left(\\left|\\Phi_{q,i-1}\\right|-\\sum_{s=0}^{\\infty}\\left|\\Phi_{q}\\left(\\varepsilon^{\\prime},s\\right)\\right|\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathrm{i}\\left(\\left|\\Phi_{q,i-1}\\right|\\right)=\\frac{\\varepsilon^{\\prime}(s)}{\\varepsilon^{\\prime}(s)}\\left(1-\\varepsilon\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\left(\\Phi_{q,i-1}\\right)=-\\frac{\\varepsilon^{\\prime}(s)}{\\varepsilon^{\\prime}(s)}\\left(1-\\varepsilon\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ +\\left(\\frac{1}{\\varepsilon^{\\prime}(s)}\\rho_{i,i-1}^{[1]}\\left(\\|\\Phi_{q,i-1}\\|\\right)\\left(\\|\\Phi_{q,i-1}\\|\\right)\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ +\\left(b_{q,i}^{[1]}\\left(\\left|\\Phi_{q,i-1}\\right|\\right)\\left(\\|\\Phi_{q,i-1}\\|\\right)\\left(\\|\\Phi_{q,i-1}\\|-\\sum_{s=0}^{\\infty}\\left|\\Phi_{q,i-1}^{[2]}\\left(\\|\\Phi_{q,i-1}\\|\\right)\\right)\\left|\\Phi_{q,i-1}\\right|\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ -\\sum_{s=0}^{\\infty}b_{q}\\left(1-\\varepsilon\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ -\\sum_{s=1}^{\\infty}b_{q}\\left(1-\\varepsilon\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ +\\left(\\sum_{s=0}^{\\infty}b_{q}(1-\\varepsilon)\\left(\\left|\\Phi_{q,i-1}\\|\\right)\\left(\\|\\Phi_{q,i-1}\\|+\\left|\\Phi_{q,i-1}\\|\\right)\\left(\\|\\Phi_{q,i-1}\\|\\right)\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ +\\left(\\sum_{s=1}^{\\infty}b_{q}(1-\\varepsilon)\\left(\\left|\\Phi_{q,i-1}\\|\\right)\\left(\\|\\Phi_{q,i-1}\\|\\right)\\right)\\left(\\int_{\\varepsilon^{\\prime}(s)}\\left|\\Phi_{q,i-1}\\|\\left(\\|\\Phi_{q,i-1}\\|\\right)\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ -\\sum_{s=1}^{\\infty}b_{q}(1-\\varepsilon)\\left\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, we get the desired result as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla^{2}P_{\\theta}(s^{\\prime}\\mid s,a)}\\\\ &{\\ =P_{\\theta}(s^{\\prime}\\mid s,a)\\varphi_{s,a,s^{\\prime}}\\varphi_{s,a,s^{\\prime}}^{\\top}}\\\\ &{\\quad\\displaystyle-\\;P_{\\theta}(s^{\\prime}\\mid s,a)\\sum_{s^{\\prime\\prime}\\in S_{s,a}}P_{\\theta}(s^{\\prime\\prime}\\mid s,a)\\left(\\varphi_{s,a,s^{\\prime}}\\varphi_{s,a,s^{\\prime\\prime}}^{\\top}+\\varphi_{s,a,s^{\\prime\\prime}}\\varphi_{s,a,s^{\\prime}}^{\\top}+\\varphi_{s,a,s^{\\prime\\prime}}\\varphi_{s,a,s^{\\prime\\prime}}^{\\top}\\right)}\\\\ &{\\quad\\displaystyle\\ +\\;2P_{\\theta}(s^{\\prime}\\mid s,a)\\left(\\sum_{s^{\\prime\\prime}\\in S_{s,a}}P_{\\theta}(s^{\\prime\\prime}\\mid s,a)\\varphi_{s,a,s^{\\prime\\prime}}\\right)\\left(\\sum_{s^{\\prime\\prime}\\in S_{s,a}}P_{\\theta}(s^{\\prime\\prime}\\mid s,a)\\varphi_{s,a,s^{\\prime\\prime}}\\right)^{\\top}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "C Detailed Regret Analysis for RRL-MNL (Theorem 1) ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we provide the complete proof of Theorem 1. First, we introduce all the technical lemmas needed to prove Theorem 1 along with their proofs. At the end of this section, we present the proof of Theorem 1. ", "page_idx": 21}, {"type": "text", "text": "C.1 Concentration of Estimated Transition Core $\\theta_{h}^{k}$ ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we provide the concentration inequality for the estimated transition core run by the approximate online Newton step. The proof is similar to that given by Oh and Iyengar [55]. For completeness, we provide the detailed proof. ", "page_idx": 22}, {"type": "text", "text": "Lemma 1 (Concentration of online estimated transition core). For each $h\\in[H],\\,i f\\,\\lambda\\geq L_{\\varphi}^{2},$ , then we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\forall k\\geq1,\\|\\pmb\\theta_{h}^{k}-\\pmb\\theta_{h}^{*}\\|_{\\mathbf A_{k,h}}\\leq\\alpha_{k}(\\delta)\\right)\\geq1-\\delta\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\alpha_{k}(\\delta)$ is given by ", "page_idx": 22}, {"type": "equation", "text": "$$\n:=\\sqrt{\\frac{8d}{\\kappa}\\log\\left(1+\\frac{k\\mathcal{U}L_{\\varphi}^{2}}{d\\lambda}\\right)+\\left(\\frac{32L_{\\varphi}L_{\\theta}}{3}+\\frac{16}{\\kappa}\\right)\\log\\frac{\\left(1+\\lceil2\\log_{2}k\\mathcal{U}L_{\\varphi}L_{\\theta}\\right)\\rceil k^{2}}{\\delta}+2\\sqrt{2}+2\\lambda L_{\\theta}^{2}}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof of lemma $^{\\,I}$ . Recall that the per-round loss $\\ell_{k,h}(\\pmb\\theta)$ and its gradient ${\\bf G}_{k,h}(\\pmb{\\theta})$ is defined as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\ell_{k,h}(\\pmb\\theta):=-\\sum_{s^{\\prime}\\in\\mathcal{S}_{k,h}}y_{h}^{k}(s^{\\prime})\\log P_{\\pmb\\theta}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\,,\\quad\\mathbf{G}_{k,h}(\\pmb\\theta):=\\nabla_{\\pmb\\theta}\\ell_{k,h}(\\pmb\\theta)\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For the analysis, we define the conditional expectations of $\\ell_{k,h}(\\pmb\\theta)$ & ${\\bf G}_{k,h}(\\pmb{\\theta})$ as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\bar{\\ell}_{k,h}(\\pmb{\\theta}):=\\mathbb{E}_{y_{h}^{k}}\\left[\\ell_{k,h}(\\pmb{\\theta})\\mid\\mathcal{F}_{k,h}\\right]\\,,\\quad\\bar{\\mathbf{G}}_{k,h}(\\pmb{\\theta}):=\\mathbb{E}_{y_{h}^{k}}[\\mathbf{G}_{k,h}(\\pmb{\\theta})\\mid\\mathcal{F}_{k,h}]\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By Taylor expansion with $\\bar{\\pmb{\\theta}}=\\nu\\pmb{\\theta}_{h}^{k}+(1-\\nu)\\pmb{\\theta}_{h}^{*}$ for some $\\nu\\in(0,1)$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\ell_{k,h}(\\pmb{\\theta}_{h}^{*})=\\ell_{k,h}(\\pmb{\\theta}_{h}^{k})+\\mathbf{G}_{k,h}(\\pmb{\\theta}_{h}^{k})^{\\top}(\\pmb{\\theta}_{h}^{*}-\\pmb{\\theta}_{h}^{k})+\\frac{1}{2}(\\pmb{\\theta}_{h}^{*}-\\pmb{\\theta}_{h}^{k})^{\\top}\\mathbf{H}_{k,h}(\\bar{\\pmb{\\theta}})(\\pmb{\\theta}_{h}^{*}-\\pmb{\\theta}_{h}^{k})\\,,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where ${\\bf H}_{k,h}(\\pmb{\\theta})$ is the Hessian of the per-round loss evaluated at $\\pmb{\\theta}$ , i.e., ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf H}_{k,h}(\\pmb\\theta):=\\nabla^{2}\\ell_{k,h}(\\pmb\\theta)}\\ ~}\\\\ {{\\displaystyle~~~~~~~~~=\\sum_{s^{\\prime}\\in\\mathcal{S}_{k,h}}P_{\\pmb\\theta}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\varphi_{k,h,s^{\\prime}}\\varphi_{k,h,s^{\\prime}}^{\\top}}}\\\\ {{\\displaystyle~~~~~~~~~~~~-\\sum_{s^{\\prime},\\tilde{s}\\in\\mathcal{S}_{k,h}}P_{\\pmb\\theta}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})P_{\\pmb\\theta}(\\tilde{s}\\mid s_{h}^{k},a_{h}^{k})\\varphi_{k,h,s^{\\prime}}\\varphi_{k,h,\\tilde{s}}^{\\top}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that for $\\bar{\\pmb{\\theta}}=\\nu\\pmb{\\theta}_{h}^{k}+(1-\\nu)\\pmb{\\theta}_{h}^{*}$ with $\\nu\\in(0,1)$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathrm{d}\\left(\\theta\\right)=\\sum_{s\\in\\mathcal{S}_{\\mathbb{R}_{n}}}P_{0}\\Bigg(\\mathrm{g}^{t}\\mid x_{t}^{\\theta}\\bigg|_{s}^{\\theta}u_{t}^{t}\\bigg)g_{t,\\lambda_{x},\\theta_{0}^{t}}\\mathrm{d}{\\mathcal{E}}_{\\lambda,\\theta_{0}}^{\\theta}}\\ \\ \\ }}\\\\ &{\\ \\ \\ \\ -\\sum_{t\\in\\mathcal{S}_{\\mathbb{R}_{n}}}\\sum_{\\alpha=\\pm1}^{t}P_{0}\\Bigg(\\mathrm{g}^{t}\\mid x_{t}^{\\theta}\\bigg|_{s}^{\\theta}u_{t}^{t}\\bigg)P_{0}\\Bigg(\\mathrm{g}^{t}\\mid x_{t}^{\\theta}\\bigg|_{s}^{\\theta}u_{t}^{t}\\bigg)g_{t,\\lambda_{x},\\theta_{0}^{t}}\\sqrt{g_{t,\\lambda_{x},\\theta_{0}^{t}}^{t}}}\\\\ &{=\\sum_{s\\in\\mathcal{S}_{\\mathbb{R}_{n}}}P_{0}\\Bigg(\\mathrm{g}^{t}\\mid x_{t}^{\\theta}u_{t}^{\\theta}\\bigg)g_{t,\\lambda_{x},\\theta_{0}^{t}}\\mathrm{d}{\\mathcal{E}}_{\\lambda,\\theta_{0}}^{\\theta}}\\\\ &{\\ \\ \\ \\ \\ -\\sum_{t\\in\\mathcal{S}_{\\mathbb{R}_{n}}}\\sum_{\\alpha=\\pm1}^{t}P_{0}\\Bigg(\\mathrm{g}^{t}\\mid x_{t}^{\\theta}u_{t}^{\\theta}\\bigg)P_{0}\\Bigg(\\mathrm{g}^{t}\\mid x_{t}^{\\theta}u_{t}^{\\theta}\\bigg)\\Bigg(g_{t,\\lambda_{x},\\theta_{0}^{t}}\\sqrt{g_{t,\\lambda_{x},\\theta_{0}^{t}}^{t}}+\\varphi_{t,\\lambda_{x},\\theta_{0}^{t}}\\sqrt{g_{t,\\lambda_{x},\\theta_{0}^{t}}^{t}}\\Bigg)}\\\\ &{\\ \\ \\ \\ \\ -\\sum_{t\\in\\mathcal{S}_{\\mathbb{R}_{n}}}P_{0}\\Bigg(\\mathrm{g}^{t}\\mid x_{t}^{\\theta}u_{t}^{\\theta}\\bigg)g_{t,\\lambda_{x},\\theta_{0}^{t}}\\mathrm{d}{\\mathcal{E}}_{\\lambda,\\theta_{0}}^{\\theta}}\\\\ &{\\ \\ \\ \\ \\ -\\frac{1}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the inequality utilizes the fact that $\\mathbf{x}\\mathbf{x}^{\\top}+\\mathbf{y}\\mathbf{y}^{\\top}\\succeq\\mathbf{x}\\mathbf{y}^{\\top}+\\mathbf{y}\\mathbf{x}^{\\top}$ for any $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{d}$ . Therefore, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{H}_{k,\\delta}(\\theta)\\ \\underset{\\geq\\ r\\leq\\ r_{k}}{\\sum}\\ \\mathcal{P}_{p}(s^{\\prime}\\mid x_{\\delta},\\theta_{k}^{\\prime})|\\mathcal{P}_{k,\\delta}(x^{\\prime}\\mid x_{\\delta},x^{\\prime}),}&{}\\\\ &{\\quad-\\ \\sum_{i\\geq1,\\ \\delta}\\sum_{p\\in\\mathcal{Q}(\\delta)_{k}}\\Big(\\int_{x_{\\delta}^{\\prime}\\mid x_{\\delta},\\theta_{k}^{\\prime})}^{\\ r}\\Big(\\sum_{i,j=1}^{D}\\Big)\\rho_{\\theta_{k}}\\Big(\\sum_{i,j=1}^{D}\\Big)\\varphi_{\\theta_{k},\\gamma}\\varphi_{\\theta_{k},\\gamma}^{\\top}}\\\\ &{\\quad-\\ \\sum_{i\\geq1,\\ \\delta}\\Big(\\mathrm{P}_{i}(s^{\\prime}\\mid x_{\\delta},\\theta_{k}^{\\prime})\\Big)\\varphi_{\\theta_{k},\\gamma}\\varphi_{\\theta_{k},\\gamma}^{\\top}}\\\\ &{\\quad-\\ \\sum_{p\\in\\mathcal{Q}(\\delta)_{k}}\\sum_{i=1,\\ \\delta}\\mathbb{P}_{p}(s^{\\prime}\\mid x_{\\delta},\\theta_{k}^{\\prime})P_{k,\\theta}\\ \\varphi_{k}^{\\top}\\Big(\\Big|\\ x_{\\delta}^{\\prime}\\ b_{\\delta}^{\\prime}\\Big)\\varphi_{k,\\gamma}\\varphi_{\\theta_{k},\\gamma}^{\\top}}\\\\ &{\\quad-\\ \\sum_{i\\geq1,\\ \\delta}\\int_{x_{\\delta}^{\\prime}\\mid x_{\\delta},\\theta_{k}^{\\prime}}^{\\ r}\\Big(\\sum_{i,j=1,\\delta}^{D}P_{k}(\\xi)\\Big)\\varphi_{k}\\left(x_{\\delta}^{\\prime}\\mid x_{\\delta},\\theta_{k}^{\\prime}\\right)\\varphi_{k,\\gamma}\\varphi_{\\theta_{k},\\gamma}^{\\top}}\\\\ &{=\\ \\sum_{i\\geq1,\\ \\delta}P_{p}(\\varepsilon^{\\prime}\\mid x_{\\delta},\\theta_{k}^{\\prime})\\left(1-\\ \\sum_{i\\geq1}P_{p}(\\varepsilon)\\Big|x_{\\delta}^{\\prime}\\ b_{\\delta}^{\\prime}\\right)\\varphi_{k,\\gamma}\\varphi_{\\theta_{k},\\gamma}^{\\top}}\\\\ &{\\quad-\\ \\sum_{i\\geq1,\\ \\delta}P_{p}(\\varepsilon^{\\prime}\\mid x_{\\delta},\\theta_{\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\dot{s}_{k,h}$ is the state satisfying $\\pmb{\\varphi}(s_{h}^{k},a_{h}^{k},\\dot{s}_{k,h})\\;=\\;\\mathbf{0}_{d}$ and the last inequality comes from the Assumption 4. ", "page_idx": 23}, {"type": "text", "text": "Using the lower bound of the Hessian of the per-round loss evaluated at $\\bar{\\pmb\\theta}$ , from (10) we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{\\Phi}_{k,h}(\\theta_{h}^{*})\\geq\\ell_{k,h}(\\theta_{h}^{k})+\\mathbf{G}_{k,h}(\\theta_{h}^{k})^{\\top}(\\theta_{h}^{*}-\\theta_{h}^{k})+\\frac{\\kappa}{2}(\\theta_{h}^{*}-\\theta_{h}^{k})^{\\top}\\left(\\sum_{s^{\\prime}\\in\\mathcal{S}_{k,h}}\\varphi_{k,h,s^{\\prime}}\\varphi_{k,h,s^{\\prime}}^{\\top}\\right)(\\theta_{h}^{*}-\\theta_{h}^{k})\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By rearranging, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\ell_{k,h}(\\boldsymbol{\\theta}_{h}^{k})\\leq\\ell_{k,h}(\\boldsymbol{\\theta}_{h}^{*})+\\mathbf{G}_{k,h}(\\boldsymbol{\\theta}_{h}^{k})^{\\top}(\\boldsymbol{\\theta}_{h}^{k}-\\boldsymbol{\\theta}_{h}^{*})-\\frac{\\kappa}{2}(\\boldsymbol{\\theta}_{h}^{*}-\\boldsymbol{\\theta}_{h}^{k})^{\\top}\\mathbf{W}_{k,h}(\\boldsymbol{\\theta}_{h}^{*}-\\boldsymbol{\\theta}_{h}^{k})\\,,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we denote $\\begin{array}{r}{\\mathbf{W}_{k,h}:=\\sum_{s^{\\prime}\\in S_{k,h}}\\varphi_{k,h,s^{\\prime}}\\varphi_{k,h,s^{\\prime}}^{\\top}}\\end{array}$ . By taking expectation over $y_{h}^{k}$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\bar{\\ell}_{k,h}(\\boldsymbol{\\theta}_{h}^{k})\\leq\\bar{\\ell}_{k,h}(\\boldsymbol{\\theta}_{h}^{*})+\\bar{\\mathbf{G}}_{k,h}(\\boldsymbol{\\theta}_{h}^{k})^{\\top}(\\boldsymbol{\\theta}_{h}^{k}-\\boldsymbol{\\theta}_{h}^{*})-\\frac{\\kappa}{2}(\\boldsymbol{\\theta}_{h}^{*}-\\boldsymbol{\\theta}_{h}^{k})^{\\top}\\mathbf{W}_{k,h}(\\boldsymbol{\\theta}_{h}^{*}-\\boldsymbol{\\theta}_{h}^{k})\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "On the other hand, for any $\\pmb{\\theta}\\in\\mathbb{R}^{d}$ , since we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell_{k,h}(\\boldsymbol{\\theta})-\\ell_{k,h}(\\boldsymbol{\\theta}_{h}^{\\star})}\\\\ &{\\ =\\displaystyle-\\sum_{s^{\\prime}\\in S_{k,h}}P_{\\theta_{h}^{\\star}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\log P_{\\theta}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})+\\displaystyle\\sum_{s^{\\prime}\\in S_{k,h}}P_{\\theta_{h}^{\\star}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\log P_{\\theta_{h}^{\\star}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})}\\\\ &{\\ =\\displaystyle\\sum_{s^{\\prime}\\in S_{k,h}}P_{\\theta_{h}^{\\star}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\left(\\log P_{\\theta_{h}^{\\star}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})-\\log P_{\\theta}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\right)}\\\\ &{\\ =\\displaystyle\\sum_{s^{\\prime}\\in S_{k,h}}P_{\\theta_{h}^{\\star}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\log\\frac{P_{\\theta_{h}^{\\star}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})}{P_{\\theta}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})}}\\\\ &{\\ =D_{\\mathrm{KL}}(P_{\\theta_{h}^{\\star}}\\parallel P_{\\theta})}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $D_{\\mathrm{KL}}(P\\parallel Q)$ is the Kullback-Leibler divergence of $P$ from $Q$ , from (12) we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0\\leq\\bar{\\ell}_{k,h}(\\boldsymbol{\\theta}_{h}^{k})-\\bar{\\ell}_{k,h}(\\boldsymbol{\\theta}_{h}^{*})}\\\\ &{\\quad\\leq\\bar{\\mathbf{G}}_{k,h}(\\boldsymbol{\\theta}_{h}^{k})^{\\top}(\\boldsymbol{\\theta}_{h}^{k}-\\boldsymbol{\\theta}_{h}^{*})-\\frac{\\kappa}{2}\\|\\boldsymbol{\\theta}_{h}^{*}-\\boldsymbol{\\theta}_{h}^{k}\\|_{\\mathbf{W}_{k,h}}^{2}}\\\\ &{\\quad=\\mathbf{G}_{k,h}(\\boldsymbol{\\theta}_{h}^{k})^{\\top}(\\boldsymbol{\\theta}_{h}^{k}-\\boldsymbol{\\theta}_{h}^{*})-\\frac{\\kappa}{2}\\|\\boldsymbol{\\theta}_{h}^{*}-\\boldsymbol{\\theta}_{h}^{k}\\|_{\\mathbf{W}_{k,h}}^{2}+\\left(\\bar{\\mathbf{G}}_{k,h}(\\boldsymbol{\\theta}_{h}^{k})-\\mathbf{G}_{k,h}(\\boldsymbol{\\theta}_{h}^{k})\\right)^{\\top}(\\boldsymbol{\\theta}_{h}^{k}-\\boldsymbol{\\theta}_{h}^{*})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "To get an upper bound of $\\mathbf{G}_{k,h}(\\pmb{\\theta}_{h}^{k})^{\\top}(\\pmb{\\theta}_{h}^{k}-\\pmb{\\theta}_{h}^{*})$ , recall that the estimated transition core is given by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\pmb{\\theta}_{h}^{k+1}=\\operatorname*{argmin}_{\\pmb{\\theta}\\in\\mathcal{B}_{d}(L_{\\theta})}\\frac{1}{2}\\|\\pmb{\\theta}-\\pmb{\\theta}_{h}^{k}\\|_{\\mathbf{A}_{k+1,h}}^{2}+(\\pmb{\\theta}-\\pmb{\\theta}_{h}^{k})^{\\top}\\mathbf{G}_{k,h}(\\pmb{\\theta}_{h}^{k})\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since the objective function in (14) is convex, by the first-order optimality condition for any $\\pmb\\theta\\in$ $B_{d}(L_{\\theta})$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\mathbf{G}_{k,h}(\\pmb{\\theta}_{h}^{k})+\\mathbf{A}_{k+1,h}(\\pmb{\\theta}_{h}^{k+1}-\\pmb{\\theta}_{h}^{k})\\right)^{\\top}(\\pmb{\\theta}-\\pmb{\\theta}_{h}^{k+1})\\geq0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which gives ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\theta^{\\top}{\\mathbf A}_{k+1,h}(\\theta_{h}^{k+1}-\\theta_{h}^{k})\\geq(\\theta_{h}^{k+1})^{\\top}{\\mathbf A}_{k+1,h}(\\theta_{h}^{k+1}-\\theta_{h}^{k})-{\\mathbf G}_{k,h}(\\theta_{h}^{k})^{\\top}(\\theta-\\theta_{h}^{k+1})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\|\\theta_{h}^{k}-\\theta_{h}^{*}\\|_{\\mathsf{A}_{k+1,h}}^{2}-\\|\\theta_{h}^{k+1}-\\theta_{h}^{*}\\|_{\\mathsf{A}_{h+1,h}}^{2}}\\\\ &{=(\\theta_{h}^{k})^{\\top}\\mathbf{A}_{k+1,h}\\theta_{h}^{k}-(\\theta_{h}^{k+1})^{\\top}\\mathbf{A}_{k+1,h}\\theta_{h}^{k+1}+2(\\theta_{h}^{*})^{\\top}\\mathbf{A}_{k+1,h}(\\theta_{h}^{k+1}-\\theta_{h}^{k})}\\\\ &{\\ge(\\theta_{h}^{k})^{\\top}\\mathbf{A}_{k+1,h}\\theta_{h}^{k}-(\\theta_{h}^{k+1})^{\\top}\\mathbf{A}_{k+1,h}\\theta_{h}^{k+1}+2(\\theta_{h}^{k+1})^{\\top}\\mathbf{A}_{k+1,h}(\\theta_{h}^{k+1}-\\theta_{h}^{k})}\\\\ &{~~-2\\mathbf{G}_{k,h}(\\theta_{h}^{k})^{\\top}(\\theta_{h}^{*}-\\theta_{h}^{k+1})}\\\\ &{=(\\theta_{h}^{k})^{\\top}\\mathbf{A}_{k+1,h}\\theta_{h}^{k}+(\\theta_{h}^{k+1})^{\\top}\\mathbf{A}_{k+1,h}\\theta_{h}^{k+1}-2(\\theta_{h}^{k+1})^{\\top}\\mathbf{A}_{k+1,h}\\theta_{h}^{k}-2\\mathbf{G}_{k,h}(\\theta_{h}^{k})^{\\top}(\\theta_{h}^{*}-\\theta_{h}^{k+1})}\\\\ &{=\\|\\theta_{h}^{k}-\\theta_{h}^{k+1}\\|_{\\mathsf{A}_{k+1,h}}^{2}-2\\mathbf{G}_{k,h}(\\theta_{h}^{k})^{\\top}(\\theta_{h}^{*}-\\theta_{h}^{k+1})}\\\\ &{=\\|\\theta_{h}^{k}-\\theta_{h}^{k+1}\\|_{\\mathsf{A}_{k+1,h}}^{2}+2\\mathbf{G}_{k,h}(\\theta_{h}^{k})^{\\top}(\\theta_{h}^{k+1}-\\theta_{h}^{k})+2\\mathbf{G}_\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last inequality follows by the fact that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\theta_{h}^{k}-\\theta_{h}^{k+1}\\|_{\\mathbf{A}_{k+1,h}}^{2}+2\\mathbf{G}_{k,h}(\\theta_{h}^{k})^{\\top}(\\theta_{h}^{k+1}-\\theta_{h}^{k})\\geq\\underset{\\theta\\in B_{d}(L_{\\theta})}{\\operatorname*{min}}\\left\\{\\|\\theta\\|_{\\mathbf{A}_{k+1,h}}^{2}+2\\mathbf{G}_{k,h}(\\theta_{h}^{k})^{\\top}\\theta\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=-\\|\\mathbf{G}_{k,h}(\\theta_{h}^{k})\\|_{\\mathbf{A}_{k+1,h}^{-1}}^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, from (16) we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{G}_{k,h}(\\theta_{h}^{k})^{\\top}(\\theta_{h}^{k}-\\theta_{h}^{*})\\leq\\frac{1}{2}\\|\\mathbf{G}_{k,h}(\\theta_{h}^{k})\\|_{\\mathbf{A}_{k+1,h}^{-1}}^{2}+\\frac{1}{2}\\|\\theta_{h}^{k}-\\theta_{h}^{*}\\|_{\\mathbf{A}_{k+1,h}}^{2}-\\frac{1}{2}\\|\\theta_{h}^{k+1}-\\theta_{h}^{*}\\|_{\\mathbf{A}_{k+1,h}}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By substituting (17) into (13), we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0\\leq\\displaystyle\\frac{1}{2}\\|\\mathbf{G}_{k,h}(\\pmb{\\theta}_{h}^{k})\\|_{\\mathbf{A}_{k+1,h}^{-1}}^{2}+\\frac{1}{2}\\|\\pmb{\\theta}_{h}^{k}-\\pmb{\\theta}_{h}^{*}\\|_{\\mathbf{A}_{k+1,h}}^{2}-\\frac{1}{2}\\|\\pmb{\\theta}_{h}^{k+1}-\\pmb{\\theta}_{h}^{*}\\|_{\\mathbf{A}_{k+1,h}}^{2}}\\\\ &{\\quad-\\displaystyle\\frac{\\kappa}{2}\\|\\pmb{\\theta}_{h}^{*}-\\pmb{\\theta}_{h}^{k}\\|_{\\mathbf{W}_{k,h}}+\\left(\\bar{\\mathbf{G}}_{k,h}(\\pmb{\\theta}_{h}^{k})-\\mathbf{G}_{k,h}(\\pmb{\\theta}_{h}^{k})\\right)^{\\top}(\\pmb{\\theta}_{h}^{k}-\\pmb{\\theta}_{h}^{*})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that since we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\mathbf{E}_{[\\mu,\\nu]}(\\mathbf{E}_{[\\mu,\\nu]}^{[\\nu]},\\mathbf{E}_{[\\mu,\\nu]}^{[\\nu]}),\\mathbf{a}_{[\\mu,\\nu]}^{[\\nu]})}\\\\ &{=\\displaystyle\\sum_{s\\geq\\nu_{\\infty}}\\Biggl(P_{\\mu(\\nu)}\\left(\\varepsilon^{[\\nu]},s,\\omega_{\\nu}^{[\\nu]}\\right)-\\frac{\\nu_{\\infty}(\\nu)}{\\nu}\\Biggr)\\left(P_{\\mu(\\nu)}\\left(\\varepsilon^{[\\nu]},s,\\omega_{\\nu}^{[\\nu]}\\right)-\\frac{\\nu_{\\infty}(\\nu)}{\\nu}\\right)\\nabla_{\\nu}\\int_{\\mu(\\nu)}^{\\nu}\\varepsilon^{[\\nu]}\\nabla_{\\nu}\\int_{\\nu}\\nabla_{\\nu}\\tilde{F}_{\\mu(\\nu)}\\Biggr)}\\\\ &{=\\displaystyle\\sum_{s\\geq\\nu_{\\infty}}\\sum_{\\alpha=1}^{\\infty}\\left(P_{\\mu(\\nu)}\\left(\\varepsilon^{[\\nu]},s,\\omega_{\\nu}^{[\\nu]}\\right)-\\frac{\\nu_{\\infty}(\\nu)}{\\nu}\\right)\\left(P_{\\mu(\\nu)}\\left(\\varepsilon^{[\\nu]},s,\\omega_{\\nu}^{[\\nu]}\\right)-\\frac{\\nu_{\\infty}(\\nu)}{\\nu}\\right)}\\\\ &{\\qquad-\\frac{\\nu_{\\infty}(\\nu)}{\\nu}\\nabla_{\\nu}\\int_{\\mu(\\nu)}^{\\nu}\\varepsilon^{[\\nu]}\\nabla_{\\nu}\\tilde{F}_{\\mu(\\nu)}\\Biggr)\\times}\\\\ &{\\leq\\displaystyle\\frac{1}{2}\\sum_{s\\geq\\nu_{\\infty}}\\sum_{\\alpha=1}^{\\infty}\\left(P_{\\mu(\\nu)}\\left(\\varepsilon^{[\\nu]},s,\\omega_{\\nu}^{[\\nu]}\\right)-\\nabla_{\\nu}(\\varepsilon^{[\\nu]})\\right)^{2}\\nabla_{\\nu}\\int_{\\mu(\\nu)}^{\\nu}\\varepsilon_{\\alpha}\\nabla_{\\nu}\\int_{\\nu}\\nabla_{\\nu}\\tilde{F}_{\\mu(\\nu)}\\Biggr)\\times}\\\\ &{\\qquad\\qquad\\qquad+\\left(P_{\\mu(\\nu)}\\left(\\varepsilon^{[\\nu]},s,\\omega_{\\nu}^{[\\nu]}\\right)-\\nabla_{\\nu}(\\nu)\\right)^{2}\\nabla_{\\nu}\\int_{\\mu(\\nu)}^{\\nu}\\varepsilon_{\\alpha}\\nabla_{\\nu}\\int_{\\nu}\\nabla_{\\nu}\\tilde\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the first inequality utilizes the inequality $\\mathbf{x}^{\\top}\\mathbf{A}\\mathbf{y}+\\mathbf{y}^{\\top}\\mathbf{A}\\mathbf{x}\\leq\\mathbf{x}^{\\top}\\mathbf{A}\\mathbf{x}+\\mathbf{y}^{\\top}$ Ay for any positivesemidefinite matrix $\\mathbf{A}$ , and the last inequality holds since $0\\leq P_{\\theta_{h}^{k}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\leq1$ and $\\sum_{s^{\\prime}}\\bar{P}_{\\theta_{h}^{k}}(s^{\\prime}\\mid$ $s_{h}^{k},a_{h}^{k})=1.$ . ", "page_idx": 25}, {"type": "text", "text": "Combining the results of (18) and (19), we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{{0}\\:\\underset{s^{\\prime}\\in\\mathcal{S}_{h,h}}{\\operatorname*{max}}\\:||\\varphi_{k,h,s^{\\prime}}||_{\\mathbf{A}_{k+1,h}^{\\prime}}^{2}+\\frac{1}{2}||\\theta_{h}^{k}-\\theta_{h}^{*}||_{\\mathbf{A}_{k+1,h}}^{2}-\\frac{1}{2}||\\theta_{h}^{k+1}-\\theta_{h}^{*}||_{\\mathbf{A}_{k+1,h}}^{2}}\\\\ &{\\quad-\\frac{\\kappa}{2}||\\theta_{h}^{*}-\\theta_{h}^{k}||_{\\mathbf{W}_{k,h}}+\\Big(\\overline{{\\mathbf{G}}}_{k,h}(\\theta_{h}^{k})-\\mathbf{G}_{k,h}(\\theta_{h}^{k})\\Big)^{\\top}\\,(\\theta_{h}^{k}-\\theta_{h}^{*})}\\\\ &{=\\underset{s^{\\prime}\\in\\mathcal{S}_{h,h}}{\\operatorname*{max}}\\:||\\varphi_{k,h,s^{\\prime}}||_{\\mathbf{A}_{k+1,h}^{\\prime}}^{2}+\\frac{1}{2}||\\theta_{h}^{k}-\\theta_{h}^{*}||_{\\mathbf{A}_{h,h}}^{2}+\\frac{\\kappa}{4}||\\theta_{h}^{k}-\\theta_{h}^{*}||_{\\mathbf{W}_{k,h}}^{2}-\\frac{1}{2}||\\theta_{h}^{k+1}-\\theta_{h}^{*}||_{\\mathbf{A}_{k+1,h}}^{2}}\\\\ &{\\quad-\\frac{\\kappa}{2}||\\theta_{h}^{*}-\\theta_{h}^{k}||_{\\mathbf{W}_{k,h}}+\\Big(\\overline{{\\mathbf{G}}}_{k,h}(\\theta_{h}^{k})-\\mathbf{G}_{k,h}(\\theta_{h}^{k})\\Big)^{\\top}\\,(\\theta_{h}^{k}-\\theta_{h}^{*})}\\\\ &{=\\underset{s^{\\prime}\\in\\mathcal{S}_{h,h}}{\\operatorname*{max}}\\:||\\varphi_{k,h,s^{\\prime}}||_{\\mathbf{A}_{k+1,h}^{\\prime}}^{2}+\\frac{1}{2}||\\theta_{h}^{k}-\\theta_{h}^{*}||_{\\mathbf{A}_{h,h}}^{ \n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where for the first equality we use $\\mathbf{A}_{k+1,h}=\\mathbf{A}_{k,h}+\\textstyle\\frac{\\kappa}{2}\\mathbf{W}_{k,h}$ . By rearranging the terms, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\|\\theta_{h}^{k+1}-\\theta_{h}^{*}\\|_{\\mathbf{A}_{k+1,h}}^{2}\\leq\\|\\theta_{h}^{k}-\\theta_{h}^{*}\\|_{\\mathbf{A}_{k,h}}^{2}+2\\displaystyle\\operatorname*{max}_{s^{\\prime}\\in S_{k,h}}\\|\\varphi_{k,h,s^{\\prime}}\\|_{\\mathbf{A}_{k+1,h}^{-1}}^{2}-\\frac{\\kappa}{2}\\|\\theta_{h}^{k}-\\theta_{h}^{*}\\|_{\\mathbf{W}_{k,h}}^{2}}&{}\\\\ {\\displaystyle+\\,2\\left(\\bar{\\mathbf{G}}_{k,h}(\\theta_{h}^{k})-\\mathbf{G}_{k,h}(\\theta_{h}^{k})\\right)^{\\top}(\\theta_{h}^{k}-\\theta_{h}^{*})\\,.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then summing over $k$ gives ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\|\\theta_{h}^{k+1}-\\theta_{h}^{*}\\|_{\\mathbf{A}_{k+1,h}}^{2}\\leq\\|\\theta_{1,h}-\\theta_{h}^{*}\\|_{\\mathbf{A}_{1,h}}^{2}+2\\displaystyle\\sum_{i=1}^{k}\\operatorname*{max}_{s^{\\prime}\\in S_{i,h}}\\|\\varphi_{i,h,s^{\\prime}}\\|_{\\mathbf{A}_{i+1,h}^{2}}^{2}-\\displaystyle\\frac{\\kappa}{2}\\sum_{i=1}^{k}\\|\\theta_{h}^{i}-\\theta_{h}^{*}\\|_{\\mathbf{W}_{i,h}}^{2}}\\\\ {\\displaystyle\\quad+\\,2\\sum_{i=1}^{k}\\big(\\bar{\\mathbf{G}}_{i,h}(\\theta_{h}^{i})-\\mathbf{G}_{i,h}(\\theta_{h}^{i})\\big)^{\\top}\\,(\\theta_{h}^{i}-\\theta_{h}^{*})}\\\\ {\\displaystyle\\leq2\\lambda L_{\\theta}^{2}+2\\displaystyle\\sum_{i=1}^{k}\\operatorname*{max}_{s^{\\prime}\\in S_{i,h}}\\|\\varphi_{i,h,s^{\\prime}}\\|_{\\mathbf{A}_{i+1,h}^{-1}}^{2}-\\displaystyle\\frac{\\kappa}{2}\\displaystyle\\sum_{i=1}^{k}\\|\\theta_{h}^{i}-\\theta_{h}^{*}\\|_{\\mathbf{W}_{i,h}}^{2}}\\\\ {\\displaystyle\\quad+\\,2\\sum_{i=1}^{k}\\big(\\bar{\\mathbf{G}}_{i,h}(\\theta_{h}^{i})-\\mathbf{G}_{i,h}(\\theta_{h}^{i})\\big)^{\\top}\\,(\\theta_{h}^{i}-\\theta_{h}^{*})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For the final step, note that $\\left(\\bar{\\mathbf{G}}_{i,h}(\\pmb{\\theta}_{h}^{i})-\\mathbf{G}_{i,h}(\\pmb{\\theta}_{h}^{i})\\right)^{\\top}(\\pmb{\\theta}_{h}^{i}-\\pmb{\\theta}_{h}^{*})$ is a martingale difference sequence. To bound this term, we invoke the following lemmas: ", "page_idx": 26}, {"type": "text", "text": "Lemma 2. For $\\delta\\in(0,1)$ and $(k,h)\\in[K]\\times[H]$ , with a probability at least $1-\\delta$ we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{k}\\left(\\bar{\\mathbf{G}}_{i,h}(\\pmb{\\theta}_{h}^{i})-\\mathbf{G}_{i,h}(\\pmb{\\theta}_{h}^{i})\\right)^{\\top}(\\pmb{\\theta}_{h}^{i}-\\pmb{\\theta}^{*})}\\\\ &{\\displaystyle\\leq\\frac{\\kappa}{4}\\sum_{i=1}^{k}\\|\\pmb{\\theta}_{h}^{i}-\\pmb{\\theta}_{h}^{*}\\|_{\\mathbf{W}_{i,h}}^{2}+\\left(\\frac{16L_{\\varphi}L_{\\theta}}{3}+\\frac{8}{\\kappa}\\right)\\log\\frac{\\left(1+\\lceil2\\log_{2}k\\mathcal{U}L_{\\varphi}L_{\\theta}\\rceil\\right)k^{2}}{\\delta}+\\sqrt{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Lemma 3 (Generalized elliptical potential). Let $S_{t}:=\\{\\mathbf{x}_{t,1},\\dots,\\mathbf{x}_{t,K}\\}\\subset\\mathbb{R}^{d}$ . For any $1\\leq t\\leq T$ and $i\\in[K]$ , suppose $\\|\\mathbf{x}_{t,i}\\|_{2}\\leq L$ . Let $\\begin{array}{r}{\\mathbf{V}_{t}:=\\lambda\\mathbf{I}_{d}+\\sum_{\\tau=1}^{t-1}\\sum_{i\\in S_{\\tau}}\\mathbf{x}_{\\tau,i}\\mathbf{x}_{\\tau,i}^{\\top}}\\end{array}$ for some $\\lambda>0$ . If $\\lambda\\geq L^{2}$ , then we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\operatorname*{max}_{i\\in[K]}\\|\\mathbf{x}_{t,i}\\|_{\\mathbf{V}_{t}^{-1}}^{2}\\leq2d\\log\\left(1+\\frac{T K L}{d\\lambda}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By Lemma 2, with probability at least $1-\\delta$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\theta_{h}^{k+1}-\\theta_{h}^{*}\\|_{\\mathbb{A}_{k+1,h}}^{2}}\\\\ &{\\leq2\\lambda L_{\\theta}^{2}+2\\displaystyle\\sum_{i=1}^{k}\\operatorname*{max}_{s^{\\prime}\\in S_{i,h}}\\|\\varphi_{i,h,s^{\\prime}}\\|_{\\mathbb{A}_{i+1,h}^{-1}}^{2}}\\\\ &{\\quad\\,+\\left(\\frac{32L_{\\varphi}L_{\\theta}}{3}+\\frac{16}{\\kappa}\\right)\\log\\frac{(1+\\lceil2\\log_{2}k\\mathcal{U}L_{\\varphi}L_{\\theta}\\rceil)\\,k^{2}}{\\delta}+2\\sqrt{2}}\\\\ &{\\le2\\lambda L_{\\theta}^{2}+\\frac{8}{\\kappa}d\\log\\left(1+\\frac{k\\mathcal{U}L_{\\varphi}^{2}}{d\\lambda}\\right)+\\left(\\frac{32L_{\\varphi}L_{\\theta}}{3}+\\frac{16}{\\kappa}\\right)\\log\\frac{\\left(1+\\lceil2\\log_{2}k\\mathcal{U}L_{\\varphi}L_{\\theta}\\rceil\\right)\\,k^{2}}{\\delta}+2\\sqrt{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the second inequality comes from Lemma 3. Note that the Gram matrix ${\\bf A}_{k,h}$ in Algorithm 1 $\\mathbf{V}$ in Lemma 3 are different by the factor of $\\scriptstyle{\\frac{\\kappa}{2}}$ , which results in additional $\\frac{2}{\\kappa}$ factor for the bound of  ik=1 maxs\u2032\u2208Si,h \u2225\u03c6i,h,s\u2032\u22252Ai\u2212+11,h. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "In the following, we provide all the proofs of the lemmas used to prove Lemma 1. ", "page_idx": 26}, {"type": "text", "text": "C.1.1 Proof of Lemma 2 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Proof of Lemma 2. Note that $\\left(\\bar{\\mathbf{G}}_{i,h}(\\pmb{\\theta}_{h}^{i})-\\mathbf{G}_{i,h}(\\pmb{\\theta}_{h}^{i})\\right)^{\\top}(\\pmb{\\theta}_{h}^{i}-\\pmb{\\theta}_{h}^{*})$ is a martingale difference sequence, i.e., ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left(\\bar{\\mathbf{G}}_{i,h}(\\pmb{\\theta}_{h}^{i})-{\\mathbf{G}}_{i,h}(\\pmb{\\theta}_{h}^{i})\\right)^{\\top}(\\pmb{\\theta}_{h}^{i}-\\pmb{\\theta}_{h}^{*})\\mid\\mathcal{F}_{i,h}\\right]}\\\\ &{=\\left(\\bar{\\mathbf{G}}_{i,h}(\\pmb{\\theta}_{h}^{i})-\\mathbb{E}\\left[\\mathbf{G}_{i,h}(\\pmb{\\theta}_{h}^{i})\\mid\\mathcal{F}_{i,h}\\right]\\right)^{\\top}(\\pmb{\\theta}_{h}^{i}-\\pmb{\\theta}_{h}^{*})}\\\\ &{=0\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "On the other hand, for any $\\pmb{\\theta}\\in\\mathbb{R}^{d}$ , since we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{G}_{i,h}(\\theta)\\|_{2}=\\displaystyle\\left\\|\\sum_{s^{\\prime}\\in\\mathcal{S}_{i,h}}\\left(P_{\\theta}(s^{\\prime}\\mid s_{h}^{i},a_{h}^{i})-y_{h}^{i}(s^{\\prime})\\right)\\varphi_{i,h,s^{\\prime}}\\right\\|_{2}}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}_{i,h}}\\left|P_{\\theta}(s^{\\prime}\\mid s_{h}^{i},a_{h}^{i})-y_{h}^{i}(s^{\\prime})\\right|\\|\\varphi_{i,h,s^{\\prime}}\\|_{2}}\\\\ &{\\qquad\\qquad\\leq L\\varphi\\left(\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}_{i,h}}P_{\\theta}(s^{\\prime}\\mid s_{h}^{i},a_{h}^{i})+\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}_{i,h}}y_{h}^{i}(s^{\\prime})\\right)}\\\\ &{\\qquad=2L\\varphi,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "then, it follows by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left(\\bar{\\mathbf{G}}_{i,h}(\\pmb{\\theta}_{h}^{i})-\\mathbf{G}_{i,h}(\\pmb{\\theta}_{h}^{i})\\right)^{\\top}(\\pmb{\\theta}_{h}^{i}-\\pmb{\\theta}_{h}^{*})\\right|}\\\\ &{\\leq\\left|\\left(\\bar{\\mathbf{G}}_{i,h}(\\pmb{\\theta}_{h}^{i})\\right)^{\\top}(\\pmb{\\theta}_{h}^{i}-\\pmb{\\theta}_{h}^{*})\\right|+\\left|\\left(\\mathbf{G}_{i,h}(\\pmb{\\theta}_{h}^{i})\\right)^{\\top}(\\pmb{\\theta}_{h}^{i}-\\pmb{\\theta}_{h}^{*})\\right|}\\\\ &{\\leq\\|\\bar{\\mathbf{G}}_{i,h}(\\pmb{\\theta}_{h}^{i})\\|_{2}\\|\\pmb{\\theta}_{h}^{i}-\\pmb{\\theta}_{h}^{*}\\|_{2}+\\|\\mathbf{G}_{i,h}(\\pmb{\\theta}_{h}^{i})\\|_{2}\\|\\pmb{\\theta}_{h}^{i}-\\pmb{\\theta}_{h}^{*}\\|_{2}}\\\\ &{\\leq4L_{\\varphi}\\|\\pmb{\\theta}_{h}^{i}-\\pmb{\\theta}_{h}^{*}\\|_{2}}\\\\ &{\\leq8L_{\\varphi}L_{\\theta}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the last inequality follows by $\\lVert\\pmb{\\theta}_{h_{-}}^{i}-\\pmb{\\theta}_{h}^{*}\\rVert_{2}\\leq\\lVert\\pmb{\\theta}_{h}^{i}\\rVert_{2}+\\lVert\\pmb{\\theta}_{h}^{*}\\rVert_{2}\\leq2L_{\\pmb{\\theta}}$ . Hence, if we denote $\\begin{array}{r}{M_{k,h}:=\\sum_{i=1}^{k}\\left(\\bar{\\mathbf{G}}_{i,h}(\\pmb{\\theta}_{h}^{i})-\\mathbf{G}_{i,h}(\\pmb{\\theta}_{h}^{i})\\right)^{\\top}(\\pmb{\\theta}_{h}^{i}-\\pmb{\\theta}_{h}^{*})}\\end{array}$ , then $M_{k,h}$ is a martingale. Note that we also ", "page_idx": 27}, {"type": "text", "text": "have ", "page_idx": 28}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\frac{\\displaystyle\\sum_{k=1}^{k}\\mathbb{E}_{\\sigma_{k}}\\Bigg[\\Big(\\mathbf{E}_{t,\\alpha}\\big(\\mathbf{\\uprho}_{k}^{(1)}\\big)^{\\top}\\left(\\mathbf{f}_{U_{k}}^{t}-\\mathbf{g}_{t}\\right)\\Big)^{\\top}\\Bigg]-\\mathbb{E}_{\\sigma_{k}}\\Bigg[\\Big(\\big(\\mathbf{E}_{t,\\alpha}\\big(\\mathbf{\\uprho}_{k}^{(1)}\\big)^{\\top}\\big(\\mathbf{f}_{U_{k}}^{t}-\\mathbf{g}_{t}\\big)\\Big)^{\\top}\\Bigg]}\\\\ &{\\le\\frac{\\displaystyle\\sum_{k=1}^{k}\\mathbb{E}_{\\sigma_{k}}\\Bigg[\\Big(\\mathbf{E}_{t,\\alpha}\\big(\\mathbf{\\uprho}_{k}^{(1)}\\big)^{\\top}\\big(\\mathbf{f}_{U_{k}}^{t}-\\mathbf{g}_{t}\\big)\\Big)^{\\top}\\Bigg]}\\\\ &{=\\frac{\\displaystyle\\sum_{k=1}^{k}\\mathbb{E}_{\\sigma_{k}}}{\\displaystyle\\sum_{k=1}^{k}\\mathbb{E}_{\\sigma_{k}}\\Bigg[\\Bigg(\\sum_{n=0}^{k}\\int_{\\mathbb{R}_{k}}\\Big(\\mathbf{f}_{u}^{t}\\big(\\mathbf{\\uprho}_{k}^{(1)}\\big)_{i}\\mathbf{f}_{U_{k}}^{t}\\Big(\\mathbf{f}_{U_{k}}^{t}-\\mathbf{g}_{t}\\big)\\Big)^{\\top}\\Bigg]}\\\\ &{\\le\\frac{\\displaystyle\\sum_{k=1}^{k}\\mathbb{E}_{\\sigma_{k}}}{\\displaystyle\\sum_{k=1}^{k}\\mathbb{E}_{\\sigma_{k}}\\Bigg[\\Bigg(\\sum_{n=0}^{k}\\int_{\\mathbb{R}_{k}}\\Big(\\mathbf{f}_{u}^{t}\\big(\\mathbf{f}_{u}^{t}\\big(\\mathbf{f}_{u}^{t}\\big)\\big(\\mathbf{f}_{u}^{t}\\big)\\Big)^{\\top}\\Bigg)\\Bigg(\\sum_{n=0,k}\\Big(\\mathbf{f}_{u}^{t}\\big(\\mathbf{f}_{u}^{t}\\big(\\mathbf{f}_{u}^{t}-\\mathbf{g}_{t}\\big)\\Big)^{\\top}\\Bigg)\\Bigg]}\\\\ &{=\\frac{\\displaystyle\\sum_{k=1}^{k}\\mathbb{E}_{\\sigma_{k}}}{\\displaystyle\\sum_{k=1}^{k}\\mathbb{E}_{ \n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where (21) holds by the Cauchy\u2013Schwarz inequality, (22) holds because ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s^{\\prime}\\in S_{i,h}}\\left(P_{\\theta_{h}^{i}}(s^{\\prime}\\mid s_{h}^{i},a_{h}^{i})-y_{h}^{i}(s^{\\prime})\\right)^{2}}\\\\ &{=\\displaystyle\\sum_{s^{\\prime}\\in S_{i,h}}\\left\\{P_{\\theta_{h}^{i}}(s^{\\prime}\\mid s_{h}^{i},a_{h}^{i})\\right\\}^{2}-2P_{\\theta_{h}^{i}}(s^{\\prime}\\mid s_{h}^{i},a_{h}^{i})y_{h}^{i}(s^{\\prime})+\\left\\{y_{h}^{i}(s^{\\prime})\\right\\}^{2}}\\\\ &{\\leq2\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "However, if we denote $\\begin{array}{r}{B_{k,h}:=2\\sum_{i=1}^{k}\\|\\pmb{\\theta}_{h}^{i}-\\pmb{\\theta}_{h}^{*}\\|_{\\mathbf{W}_{i,h}}^{2}}\\end{array}$ , since $B_{k,h}$ is itself a random variable, to apply Freedman\u2019s inequality to $M_{k,h}$ , we consider two cases depending on the values of $B_{k,h}$ . ", "page_idx": 28}, {"type": "text", "text": "Case 1 : $\\begin{array}{r}{B_{k,h}\\leq\\frac{4}{k\\mathcal{U}}}\\end{array}$ ", "page_idx": 28}, {"type": "text", "text": "Suppose that $\\begin{array}{r}{B_{k,h}=2\\sum_{i=1}^{k}\\|\\pmb{\\theta}_{h}^{i}-\\pmb{\\theta}_{h}^{*}\\|_{\\mathbf{W}_{i,h}}^{2}\\le\\frac{4}{k\\mathcal{U}}}\\end{array}$ . Then we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M_{k,h}=\\displaystyle\\sum_{i=1}^{k}\\left(\\mathbb{G}_{k,h}(\\theta_{h}^{i})-\\mathbb{G}_{k,h}(\\theta_{h}^{i})\\right)^{\\top}(\\theta_{h}^{i}-\\theta_{h}^{i})}\\\\ &{\\quad=\\displaystyle\\sum_{i=1}^{k}\\sum_{\\sigma\\in\\mathcal{S}_{h,h}}\\left(\\hat{\\psi}_{k}^{\\uparrow}(\\varepsilon)-\\mathbb{E}_{\\{h_{k}^{\\varepsilon}(\\varepsilon)^{\\top}\\}}(\\theta_{h}^{\\uparrow})\\right)\\varphi_{\\sigma,h}^{\\top}(\\theta_{h}^{\\downarrow}-\\theta_{h}^{\\downarrow})}\\\\ &{\\quad=\\displaystyle\\sum_{i=1}^{k}\\sum_{\\sigma\\in\\mathcal{S}_{h,h}}\\left(\\hat{\\psi}_{k}^{\\uparrow}(\\varepsilon)-\\mathbb{P}_{\\varepsilon\\in\\mathcal{S}_{h}}\\left(\\varepsilon^{\\uparrow}\\left\\{x_{h}^{\\varepsilon},\\,\\hat{\\psi}_{h}^{\\downarrow}(\\varepsilon)\\right\\}\\right)\\varphi_{\\sigma,h}^{\\top}(\\theta_{h}^{\\downarrow}-\\theta_{h}^{\\uparrow})\\right.}\\\\ &{\\quad\\le\\displaystyle\\sum_{i=1}^{k}\\sum_{\\sigma\\in\\mathcal{S}_{h,h}}\\left[\\varphi_{h,h}^{\\top}(\\varepsilon)-\\theta_{h}^{\\uparrow}\\right]}\\\\ &{\\quad\\le\\displaystyle\\sqrt{\\operatorname*{sup}_{i=1}^{k}\\sum_{\\sigma\\in\\mathcal{S}_{h,h}}\\left(\\varphi_{h,h}^{\\top}(\\varepsilon)-\\theta_{h}^{\\uparrow}\\right)}}\\\\ &{\\quad=\\displaystyle\\sqrt{\\operatorname*{sup}_{i=1}^{k}\\sum_{\\sigma\\in\\mathcal{S}_{h,h}}\\left(\\varphi_{h,h}^{\\top}(\\varepsilon)-\\theta_{h}^{\\leftrightarrow}\\right)^{2}}}\\\\ &{\\quad=\\displaystyle\\sqrt{\\operatorname*{sup}_{i=1}^{k}\\sum_{\\sigma\\in\\mathcal{S}_{h,h}}\\left(\\varphi_{h,h}^{\\top}(\\varepsilon)-\\theta_{h}^{\\leftrightarrow}\\right)^{2}}}\\\\ &{\\quad<\\sqrt{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Case 2 : Bk,h >k4U ", "page_idx": 29}, {"type": "text", "text": "Suppose that $\\begin{array}{r}{B_{k,h}=2\\sum_{i=1}^{k}\\|\\pmb{\\theta}_{h}^{i}-\\pmb{\\theta}_{h}^{*}\\|_{\\mathbf{W}_{i,h}}^{2}>\\frac{4}{k\\mathcal{U}}}\\end{array}$ . Then, we have both a lower and upper bound for $B_{k,h}$ as follows: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{4}{k\\mathcal{U}}<B_{k,h}\\leq2\\sum_{i=1}^{k}\\sum_{s^{\\prime}\\in S_{i,h}}\\|\\varphi_{i,h,s^{\\prime}}\\|_{2}^{2}\\|\\theta_{h}^{i}-\\theta_{h}^{*}\\|_{2}^{2}\\leq8k\\mathcal{U}L_{\\varphi}^{2}L_{\\theta}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then by the peeling process from Bartlett et al. [12], for any $\\eta_{k}>0$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(M_{k,h}\\ge2\\sqrt{\\eta_{k}B_{k,h}}+\\frac{16\\eta_{k}L_{\\varphi}L_{\\theta}}{3}\\right)}\\\\ &{=\\mathbb{P}\\left(M_{k,h}\\ge2\\sqrt{\\eta_{k}B_{k,h}}+\\frac{16\\eta_{k}L_{\\varphi}L_{\\theta}}{3},\\frac{4}{k\\mathcal{U}}<B_{k,h}\\le8k\\mathcal{U}L_{\\varphi}^{2}L_{\\theta}^{2}\\right)}\\\\ &{=\\mathbb{P}\\left(M_{k,h}\\ge2\\sqrt{\\eta_{k}B_{k,h}}+\\frac{16\\eta_{k}L_{\\varphi}L_{\\theta}}{3},\\frac{4}{k\\mathcal{U}}<B_{k,h}\\le8k\\mathcal{U}L_{\\varphi}^{2}L_{\\theta}^{2},\\Sigma_{k,h}\\le B_{k,h}\\right)}\\\\ &{\\le\\displaystyle{\\sum_{j=1}^{m}\\mathbb{P}\\left(M_{k,h}\\ge2\\sqrt{\\eta_{k}B_{k,h}}+\\frac{16\\eta_{k}L_{\\varphi}L_{\\theta}}{3},\\frac{4\\cdot2^{j-1}}{k\\mathcal{U}}<B_{k,h}\\le\\frac{4\\cdot2^{j}}{k\\mathcal{U}},\\Sigma_{k,h}\\le B_{k,h}\\right)}}\\\\ &{\\le\\displaystyle{\\sum_{j=1}^{m}\\mathbb{P}\\left(M_{k,h}\\ge\\sqrt{\\eta_{k}\\frac{8\\cdot2^{j}}{k\\mathcal{U}}}+\\frac{16\\eta_{k}L_{\\varphi}L_{\\theta}}{j},\\Sigma_{k,h}\\le\\frac{4\\cdot2^{j}}{k\\mathcal{U}}\\right)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $m=1+\\lceil2\\log_{2}k\\mathcal{U}L_{\\varphi}L_{\\theta}\\rceil$ . For $I_{j}$ , note that from (20) we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\left(\\bar{\\mathbf{G}}_{i,h}(\\pmb{\\theta}_{h}^{i})-\\mathbf{G}_{i,h}(\\pmb{\\theta}_{h}^{i})\\right)^{\\top}(\\pmb{\\theta}_{h}^{i}-\\pmb{\\theta}_{h}^{*})\\right|\\leq8L_{\\varphi}L_{\\pmb{\\theta}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By Freedman\u2019s inequality (Lemma 29), we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{P}\\left(M_{k,\\lambda}\\geq\\sqrt{n}\\frac{\\mathfrak{L}^{\\infty}\\cdot\\mathcal{B}^{\\prime}}{\\hbar\\ell}+\\frac{\\log_{\\lambda}L_{\\infty}\\varrho_{\\lambda,\\lambda}}{3},\\chi_{k,\\lambda}\\leq\\frac{4\\cdot\\mathcal{B}^{\\prime}}{\\hbar\\ell}\\right)}\\\\ &{\\leq\\exp\\left(\\frac{-\\left(\\sqrt{n}\\frac{\\mathfrak{L}^{\\infty}\\mathcal{B}^{\\prime}}{\\hbar\\ell}+\\frac{\\log_{\\lambda}L_{\\infty}\\varrho_{\\lambda,\\ell}}{3}\\right)^{2}}{\\frac{\\mathcal{B}^{2}\\cdot\\mathcal{B}^{\\prime}}{\\hbar\\ell}+\\frac{3}{2}\\cdot\\mathcal{B}_{\\varkappa,\\ell}L_{\\infty}\\left(\\sqrt{n}\\frac{\\mathfrak{R}^{\\infty}\\mathcal{B}^{\\prime}}{\\hbar\\ell}+\\frac{4\\pi\\mathfrak{B}_{\\varkappa,\\ell}L_{\\infty}\\mathcal{B}^{\\prime}}{3}\\right)}\\right)}\\\\ &{=\\exp\\left(\\frac{-\\mathfrak{R}\\left(\\sqrt{\\frac{\\mathfrak{R}^{\\infty}\\mathcal{B}^{\\prime}}{\\hbar\\ell}}+\\frac{\\log_{\\sqrt{\\mathfrak{B}^{\\prime}}}L_{\\infty}\\mathcal{B}^{\\prime}}{3}\\right)^{2}}{\\frac{\\sqrt{\\mathfrak{R}^{\\infty}\\mathcal{B}^{\\prime}}}{\\hbar\\ell}+\\frac{\\log_{\\lambda}L_{\\infty}\\mathcal{B}^{\\prime}}{3}\\sqrt{\\hbar\\ell}\\frac{\\mathfrak{B}^{\\prime}\\cdot\\mathcal{B}^{\\prime}}{\\hbar\\ell}+\\frac{\\log_{\\lambda}L_{\\infty}\\mathcal{B}^{\\prime}\\frac{\\mathcal{B}^{\\prime}}{2}}{3}}\\right)}\\\\ &{\\leq\\exp\\left(\\frac{-\\mathfrak{R}\\left(\\sqrt{\\frac{\\mathfrak{R}^{\\infty}\\mathcal{B}^{\\prime}}{\\hbar\\ell}}+\\frac{\\log_{\\sqrt{\\mathfrak{B}^{\\prime}}}L_{\\infty}\\mathcal{B}^{\\prime}}{3}\\right)^{2}}{\\frac{\\sqrt{\\mathfrak{R}^{\\infty}\\mathcal{B}^{\\prime}}}{\\hbar\\ell}+\\frac{32\\pi\\sqrt{\\mathfrak{B}^{\\prime}}\\mathcal{B}^{\\prime}}{3}\\sqrt{\\hbar\\ell}}\\right)}\\\\ &{=\\exp\\left(-\\frac{\\mathfrak{R}\\left(\\sqrt{\\frac{\\mathfrak{R}^{\\infty}\\mathcal{B}^{\\prime}}{\\hbar\\ell}}+\\frac{\\log_{\\sqrt{\\mathfrak{B}^{\\\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By substituting Eq. (24) into Eq. (23), we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(M_{k,h}\\ge2\\sqrt{\\eta_{k}B_{k,h}}+\\frac{16\\eta_{k}L_{\\varphi}L_{\\theta}}{3}\\right)\\le m\\exp(-\\eta_{k})\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then, combining with the result of Case 1 & 2, letting $\\begin{array}{r}{\\eta_{k}=\\log\\frac{m}{\\delta/k^{2}}=\\log\\frac{(1+\\lceil2\\log_{2}k\\mathcal{U}L_{\\varphi}L_{\\theta}\\rceil)k^{2}}{\\delta}}\\end{array}$ and taking union bound over , with probability at least $1-\\delta$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\nM_{k,h}\\leq2\\sqrt{2\\eta_{k}\\sum_{i=1}^{k}\\|\\pmb\\theta_{h}^{i}-\\pmb\\theta_{h}^{*}\\|_{\\mathbf W_{i,h}}^{2}}+\\frac{16\\eta_{k}L_{\\varphi}L_{\\pmb\\theta}}{3}+\\sqrt{2}\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By applying $2{\\sqrt{a b}}\\leq a+b$ to the first term on the right hand side, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n2\\sqrt{2\\eta_{k}\\sum_{i=1}^{k}\\|\\pmb{\\theta}_{h}^{i}-\\pmb{\\theta}_{h}^{*}\\|_{\\mathbf{W}_{i,h}}^{2}}\\leq\\frac{8\\eta_{k}}{\\kappa}+\\frac{\\kappa}{4}\\sum_{i=1}^{k}\\|\\pmb{\\theta}_{h}^{i}-\\pmb{\\theta}_{h}^{*}\\|_{\\mathbf{W}_{i,h}}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Combining the results of Eq. (25) & Eq. (26), we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle M_{k,h}=\\sum_{i=1}^{k}\\big(\\bar{\\mathbf{G}}_{i,h}(\\theta_{h}^{i})-\\mathbf{G}_{i,h}(\\theta_{h}^{i})\\big)^{\\top}\\,(\\theta_{h}^{i}-\\theta_{h}^{*})}}\\\\ {{\\displaystyle\\qquad\\leq\\frac{\\kappa}{4}\\sum_{i=1}^{k}\\|\\theta_{h}^{i}-\\theta_{h}^{*}\\|_{\\mathbf{W}_{i,h}}^{2}+\\bigg(\\frac{16L_{\\varphi}L_{\\theta}}{3}+\\frac{8}{\\kappa}\\bigg)\\log\\frac{\\big(1+\\lceil2\\log_{2}k\\mathcal{U}L_{\\varphi}L_{\\theta}\\big)\\big)\\,k^{2}}{\\delta}+\\sqrt{2}\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "C.1.2 Proof of Lemma 3 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Proof of Lemma 3. By definition of $\\mathbf{V}_{t}$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{det}(\\mathbf{V}_{t+1})=\\operatorname*{det}\\left(\\mathbf{V}_{t}+\\displaystyle\\sum_{i\\in S_{t}}\\mathbf{x}_{t,i}\\mathbf{x}_{t,i}^{\\top}\\right)}\\\\ &{=\\operatorname*{det}(\\mathbf{V}_{t})\\operatorname*{det}\\left(\\mathbf{I}_{d}+\\displaystyle\\sum_{i\\in S_{t}}\\mathbf{V}_{t}^{-\\frac{1}{2}}\\mathbf{x}_{t,i}\\mathbf{x}_{t,i}^{\\top}\\mathbf{V}_{t}^{-\\frac{1}{2}}\\right)}\\\\ &{=\\operatorname*{det}(\\mathbf{V}_{t})\\left(1+\\displaystyle\\sum_{i\\in S_{t}}\\lVert\\mathbf{x}_{t,i}\\rVert_{\\mathbf{V}_{t}^{-1}}^{2}\\right)}\\\\ &{=\\operatorname*{det}(\\mathbf{M}_{d})\\displaystyle\\prod_{r=1}^{t}\\left(1+\\displaystyle\\sum_{i\\in S_{r}}\\lVert\\mathbf{x}_{\\tau,i}\\rVert_{\\mathbf{V}_{t}^{-1}}^{2}\\right)}\\\\ &{\\ge\\operatorname*{det}(\\mathbf{M}_{d})\\displaystyle\\prod_{r=1}^{t}\\left(1+\\displaystyle\\operatorname*{max}_{i\\in S_{r}}\\lVert\\mathbf{x}_{\\tau,i}\\rVert_{\\mathbf{V}_{t}^{-1}}^{2}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Since $\\lambda\\geq L^{2}$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in S_{\\tau}}\\|\\mathbf{x}_{\\tau,i}\\|_{\\mathbf{V}_{\\tau}^{-1}}^{2}\\leq\\frac{L^{2}}{\\lambda}\\leq1\\,.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Since for any $z\\in[0,1]$ , it follows that $z\\le2\\log(1+z)$ . Hence, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\operatorname*{max}_{i\\in S_{t}}\\|\\mathbf{x}_{t,i}\\|_{\\mathbf{V}_{t}^{-1}}^{2}\\leq2\\displaystyle\\sum_{t=1}^{T}\\log\\left(1+\\operatorname*{max}_{i\\in S_{t}}\\|\\mathbf{x}_{t,i}\\|_{\\mathbf{V}_{t}^{-1}}^{2}\\right)}\\\\ &{\\displaystyle=2\\log\\prod_{t=1}^{T}\\left(1+\\operatorname*{max}_{i\\in S_{t}}\\|\\mathbf{x}_{t,i}\\|_{\\mathbf{V}_{t}^{-1}}^{2}\\right)}\\\\ &{\\displaystyle\\leq2\\log\\frac{\\operatorname*{det}(\\mathbf{V}_{T+1})}{\\operatorname*{det}(\\lambda\\mathbf{A})}}\\\\ &{\\displaystyle\\leq2d\\log\\left(1+\\frac{T K L^{2}}{d\\lambda}\\right)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the second inequality comes from Eq. (27) and the last inequality follows by the determinanttrace inequality (Lemma 28). \u53e3 ", "page_idx": 31}, {"type": "text", "text": "C.2 Bound on Prediction Error ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section, we provide the bound on the prediction error induced by estimated transition core $\\theta_{h}^{k}$ . Lemma 4 (Bound on Prediction Error). For any $\\delta\\in(0,1)$ , suppose that Lemma 1 holds. Then for any $(s,a)\\in S\\times A_{\\cdot}$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\Delta_{h}^{k}(s,a)|\\leq H\\alpha_{k}(\\delta)\\|\\hat{\\varphi}_{k,h}(s,a)\\|_{\\mathbf{A}_{k,h}^{-1}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof of Lemma 4. Recall that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{h}^{k}(s,a)=\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}\\left(P_{\\theta_{h}^{k}}(s^{\\prime}\\mid s,a)-P_{\\theta_{h}^{*}}(s^{\\prime}\\mid s,a)\\right)V_{h+1}^{k}(s^{\\prime})}\\\\ &{\\qquad=\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}\\frac{\\exp(\\varphi_{s,a,s^{\\prime}}^{\\top}\\theta_{h}^{k})V_{h+1}^{k}(s^{\\prime})}{\\sum_{\\tilde{s}\\in S_{s,a}}\\exp(\\varphi_{s,a,\\tilde{s}}^{\\top}\\theta_{h}^{k})}-\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}\\frac{\\exp(\\varphi_{s,a,s^{\\prime}}^{\\top}\\theta_{h}^{*})V_{h+1}^{k}(s^{\\prime})}{\\sum_{\\tilde{s}\\in S_{s,a}}\\exp(\\varphi_{s,a,\\tilde{s}}^{\\top}\\theta_{h}^{*})}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then by the mean value theorem, there exists $\\bar{\\pmb{\\theta}}=\\rho\\pmb{\\theta}_{h}^{k}+(1-\\rho)\\pmb{\\theta}_{h}^{*}$ for some $\\rho\\in[0,1]$ satisfying that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\lambda}_{h}^{k}(s,\\alpha)=\\frac{\\displaystyle(\\sum_{x^{\\prime}\\in\\mathcal{S}_{+}}\\mathrm{exp}(\\varphi_{s,a}^{\\top},s_{i},\\theta)V_{h+1}^{k}(x^{\\prime})\\varphi_{s,a,e}^{\\top}(\\theta_{h}^{k}-\\theta_{h}^{*}))\\Big(\\sum_{\\bar{x}\\in\\mathcal{S}_{+}}\\mathrm{exp}(\\varphi_{s,a}^{\\top},\\theta)\\Big)}{\\displaystyle(\\sum_{\\bar{x}\\in\\mathcal{S}_{+}}\\mathrm{exp}(\\varphi_{s,a}^{\\top},\\theta))^{2}}}\\\\ &{\\quad-\\frac{\\displaystyle(\\sum_{x^{\\prime}\\in\\mathcal{S}_{+}}\\mathrm{exp}(\\varphi_{s,a}^{\\top},s_{i},\\theta)V_{h+1}^{k}(x^{\\prime}))\\left(\\sum_{\\bar{x}\\in\\mathcal{S}_{+}}\\mathrm{exp}(\\varphi_{s,a}^{\\top},\\theta)\\varphi_{s,a,\\bar{x}}^{\\top}(\\theta_{h}^{k}-\\theta_{h}^{*})\\right)}{\\displaystyle(\\sum_{\\bar{x}\\in\\mathcal{S}_{-}}\\mathrm{exp}(\\varphi_{s,a}^{\\top},\\theta))^{2}}}\\\\ &{\\quad=\\displaystyle\\sum_{s\\in\\mathcal{S}_{+}}P_{0}(s^{\\prime}\\mid s,a)V_{h+1}^{k}(s^{\\prime})\\varphi_{s,a,e}^{\\top}(\\theta_{h}^{k}-\\theta_{h}^{*})}\\\\ &{\\quad\\quad-\\left(\\frac{\\sum_{x^{\\prime}\\in\\mathcal{S}_{+}}\\mathrm{exp}(\\varphi_{s,a}^{\\top},\\theta)V_{h+1}^{k}(x^{\\prime})}{\\displaystyle\\sum_{\\bar{x}\\in\\mathcal{S}_{+}}\\mathrm{exp}(\\varphi_{s,a}^{\\top},\\theta)}\\right)_{s^{\\prime}\\in\\mathcal{S}_{+}}P_{0}(s^{\\prime}\\mid s,a)\\varphi_{s,a,e}^{\\top}(\\theta_{h}^{k}-\\theta_{h}^{*})}\\\\ &{\\quad=\\displaystyle\\sum_{s\\in\\mathcal{S}_{+}}\\left(V_{h+1}^{k}(s^{\\prime})-\\frac{\\sum_{x^{\\prime}\\in\\mathcal{S}_{+}}\\mathrm{exp}(\\varphi_{s,a,e}^{\\top \n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Since $V_{h}^{k}(s^{\\prime})\\leq H$ for all $s^{\\prime}\\in\\mathcal{S},k\\in[K]$ , and $h\\in[H]$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{h}^{k}(s,a)\\leq H\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\bar{\\theta}}(s^{\\prime}\\mid s,a)\\varphi_{s,a,s^{\\prime}}^{\\top}(\\theta_{h}^{k}-\\theta_{h}^{*})}\\\\ &{\\qquad\\qquad\\leq H\\displaystyle\\operatorname*{max}_{s^{\\prime}\\in S_{s,a}}|\\varphi_{s,a,s^{\\prime}}^{\\top}(\\theta_{h}^{k}-\\theta_{h}^{*})|}\\\\ &{\\qquad\\qquad\\leq H\\displaystyle\\operatorname*{max}_{s^{\\prime}\\in S_{s,a}}\\|\\varphi_{s,a,s^{\\prime}}\\|_{\\mathbf{A}_{k,h}^{-1}}\\|\\theta_{h}^{k}-\\theta_{h}^{*}\\|_{\\mathbf{A}_{k,h}}}\\\\ &{\\qquad\\qquad\\leq H\\alpha_{k}(\\delta)\\|\\hat{\\varphi}_{k,h}(s,a)\\|_{\\mathbf{A}_{k,h}^{-1}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the second inequality comes from the fact that $P_{\\bar{\\theta}}(s^{\\prime}\\ \\ \\ |\\ \\ s,a)\\ \\leq\\ 1$ is a multinomial probability, the third inequality holds due to the Cauchy-Schwarz inequality, and the last inequality follows from Lemma 1 and the definition of $\\hat{\\varphi}_{k,h}$ , i.e., $\\hat{\\varphi}_{k,h}(s,a)\\;:=\\;\\varphi(s,a,\\hat{s})$ for $\\hat{s}=\\mathrm{argmax}_{s^{\\prime}\\in S_{s,a}}\\,\\|\\varphi(s,a,s^{\\prime})\\|_{\\mathbf{A}_{k,h}^{-1}}$ . \u53e3 ", "page_idx": 32}, {"type": "text", "text": "C.3 Good Events with High Probability ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Lemma 5 (Good event probability). For any $K\\in\\mathbb N$ and $\\delta\\in(0,1)$ , the good event $\\mathcal{G}(K,\\delta^{\\prime})$ holds with probability at least $1-\\delta$ where $\\delta^{\\prime}=\\delta/(2K H)$ . ", "page_idx": 32}, {"type": "text", "text": "Proof of Lemma 5. For any $\\delta^{\\prime}\\in(0,1)$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathcal{G}(K,\\delta^{\\prime})=\\bigcap_{k\\leq K}\\bigcap_{h\\leq H}\\mathcal{G}_{k,h}(\\delta^{\\prime})=\\bigcap_{k\\leq K}\\bigcap_{h\\leq H}\\left\\{\\mathcal{G}_{k,h}^{\\Delta}(\\delta^{\\prime})\\cap\\mathcal{G}_{k,h}^{\\xi}(\\delta^{\\prime})\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "On the other hand, for any $(k,h)\\in[K]\\times[H]$ , by Lemma 30, $\\mathcal{G}_{k,h}^{\\xi}(\\delta^{\\prime})$ holds with probability at least $1-\\delta^{\\prime}$ . Then, for $\\delta^{\\prime}=\\delta/(2K H)$ by taking union bound, we have the desired result as follows: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(\\mathcal{G}(K,\\delta^{\\prime}))\\geq(1-\\delta^{\\prime})^{2K H}\\geq1-2K H\\delta^{\\prime}=1-\\delta\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "C.4 Stochastic Optimism ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Lemma 6 (Stochastic optimism). For any $\\delta$ with $0<\\delta<\\Phi(-1)/2,$ , let $\\sigma_{k}=H\\alpha_{k}(\\delta)=\\widetilde{\\mathcal{O}}(H\\sqrt{d})$ . If we take multiple sample size $\\begin{array}{r}{M=\\lceil1-\\frac{\\log H}{\\log\\Phi(1)}\\rceil}\\end{array}$ lolgo g\u03a6 (H1)\u2309, then for any k \u2208[K], we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left((V_{1}^{k}-V_{1}^{*})(s_{1}^{k})\\geq0\\mid s_{1}^{k},\\mathcal{F}_{k}\\right)\\geq\\Phi(-1)/2\\,.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof of lemma $\\theta,$ . Before presenting the proof, we introduce the following lemmas. ", "page_idx": 33}, {"type": "text", "text": "Lemma 7. For any $k\\in[K]$ , it holds ", "page_idx": 33}, {"type": "equation", "text": "$$\nV_{1}^{k}(s_{1}^{k})-V_{1}^{*}(s_{1}^{k})\\geq\\mathbb{E}_{\\pi^{*}}\\left[\\sum_{h=1}^{H}-\\iota_{h}^{k}(x_{h},a_{h})\\mid x_{1}=s_{1}^{k}\\right]\\,,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\iota_{h}^{k}(s,a):=r(s,a)+P_{h}V_{h+1}^{k}(s,a)-Q_{h}^{k}(s,a).$ ", "page_idx": 33}, {"type": "text", "text": "Lemma 8. Let $\\delta\\in(0,1)$ be given. For any $(k,h)\\in[K]\\times[H],$ , let $\\sigma_{k}=H\\alpha_{k}(\\delta)$ . If we define the event $\\mathcal{G}_{k,h}^{\\Delta}(\\delta)$ as ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{G}_{k,h}^{\\Delta}(\\delta):=\\left\\{\\Delta_{h}^{k}(s,a)\\leq H\\alpha_{k}(\\delta)\\|\\hat{\\varphi}_{k,h}(s,a)\\|_{\\mathbf{A}_{k,h}^{-1}}\\right\\}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "then conditioned on $\\mathcal{G}_{k,h}^{\\Delta}(\\delta)$ , for any $(s,a)\\in S\\times A_{\\cdot}$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(-\\iota_{h}^{k}(s,a)\\geq0\\ |\\ \\mathcal{G}_{k,h}^{\\Delta}(\\delta)\\right)\\geq1-\\Phi(1)^{M}\\,.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Lemma 9. Let $\\delta\\,\\in\\,(0,1)$ be given. For any $(h,k)\\,\\in\\,[H]\\,\\times\\,[K],$ , let $\\sigma_{k}\\,=\\,H\\alpha_{k}(\\delta)$ . If we take multiple sample size $\\begin{array}{r}{M=\\lceil1-\\frac{\\log H}{\\log\\Phi(1)}\\rceil}\\end{array}$ , then conditioned on the event $\\begin{array}{r}{\\mathcal{G}_{k}^{\\Delta}(\\delta):=\\bigcap_{h\\in[H]}\\mathcal{G}_{k,h}^{\\Delta}(\\delta)}\\end{array}$ we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(-\\iota_{h}^{k}(s_{h},a_{h})\\geq0,\\forall h\\in[H]\\ |\\ \\mathcal{G}_{k}^{\\Delta}(\\delta)\\right)\\geq\\Phi(-1)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Now, we define the event of the estimated value function being optimistic at the start of the $k$ -th episode as ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathcal{X}_{k}:=\\left\\{(V_{1}^{k}-V_{1}^{*})(s_{1}^{k})\\geq0\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Then for the event $\\mathcal{G}_{k}(\\delta)=:\\mathcal{G}_{k}$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\mathcal{X}_{k})=1-\\mathbb{P}(\\mathcal{X}_{k}^{\\mathtt{c}})}\\\\ &{\\qquad\\quad=1-\\mathbb{P}(\\mathcal{X}_{k}^{\\mathtt{c}}\\cap\\mathcal{G}_{k})-\\mathbb{P}(\\mathcal{X}_{k}^{\\mathtt{c}}\\cap\\mathcal{G}_{k}^{\\mathtt{c}})}\\\\ &{\\qquad\\quad\\ge1-\\mathbb{P}(\\mathcal{X}_{k}^{\\mathtt{c}}\\cap\\mathcal{G}_{k})-\\mathbb{P}(\\mathcal{G}_{k}^{\\mathtt{c}})}\\\\ &{\\qquad\\quad\\ge1-\\mathbb{P}(\\mathcal{X}_{k}^{\\mathtt{c}}\\cap\\mathcal{G}_{k})-\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the last inequality comes from lemma 5. ", "page_idx": 33}, {"type": "text", "text": "On the other hand, by Lemma 7, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{V_{1}^{k}(s_{1}^{k})-V_{1}^{*}(s_{1}^{k})\\geq\\mathbb{E}_{\\pi^{*}}\\left[\\displaystyle\\sum_{h=1}^{H}-\\iota_{h}^{k}(x_{h},a_{h})\\mid x_{1}=s_{1}^{k}\\right]}\\\\ &{}&{=\\displaystyle\\sum_{h=1}^{H}\\mathbb{E}_{\\pi^{*}}\\left[-\\iota_{h}^{k}(x_{h},a_{h})\\mid x_{1}=s_{1}^{k}\\right]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "If we define an event ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathcal{V}_{k}=\\left\\{\\sum_{h=1}^{H}\\mathbb{E}_{\\pi^{*}}\\left[-\\iota_{h}^{k}(x_{h},a_{h})\\mid x_{1}=s_{1}^{k}\\right]\\geq0\\right\\}\\,,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "then, by Lemma 9, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\mathcal{Y}_{k}\\ |\\ \\mathcal{G}_{k})\\geq\\Phi(-1)\\iff\\mathbb{P}(\\mathcal{Y}_{k}^{c}\\ |\\ \\mathcal{G}_{k})\\leq1-\\Phi(-1)}\\\\ &{\\qquad\\qquad\\implies\\mathbb{P}(\\mathcal{Y}_{k}^{c}\\cap\\mathcal{G}_{k})\\leq(1-\\Phi(-1))\\,\\mathbb{P}(\\mathcal{G}_{k})\\leq1-\\Phi(-1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Note that since $\\mathcal{X}_{k}^{\\mathtt{c}}\\cap\\mathcal{G}_{k}\\subset\\mathcal{V}_{k}^{\\mathtt{c}}\\cap\\mathcal{G}_{k}$ , we can conclude that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\mathcal{X}_{k})\\geq1-\\mathbb{P}(\\mathcal{X}_{k}^{\\mathfrak{c}}\\cap\\mathcal{G}_{k})-\\delta}\\\\ &{\\geq1-\\mathbb{P}(\\mathcal{Y}_{k}^{\\mathfrak{c}}\\cap\\mathcal{G}_{k})-\\delta}\\\\ &{\\geq1-(1-\\Phi(-1))-\\delta}\\\\ &{=\\Phi(-1)-\\delta}\\\\ &{\\geq\\Phi(-1)/2}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the last inequality comes from the choice of $\\delta$ . ", "page_idx": 33}, {"type": "text", "text": "In the following, we provide all the proofs of the lemmas used to prove Lemma 6. ", "page_idx": 33}, {"type": "text", "text": "C.4.1 Proof of Lemma 7 ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Proof of lemma 7. In this proof, we use $\\boldsymbol{x}_{h}^{k}$ as the states sampled under the $\\pi^{*}$ to distinguish with $s_{h}^{k}$ Since we have, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{1}^{*}(s_{1}^{k})-V_{1}^{*}(s_{1}^{k})}\\\\ &{\\geq Q_{1}^{*}(s_{1}^{k},\\pi^{*}(s_{1}^{k}))-Q_{1}^{*}(s_{1}^{k},\\pi^{*}(s_{1}^{k}))}\\\\ &{=r(s_{1}^{k},\\pi^{*}(s_{1}^{k}))+P_{1}V_{2}^{*}(s_{1}^{k},\\pi^{*}(s_{1}^{k}))-t_{1}^{k}(s_{1}^{k},\\pi^{*}(s_{1}^{k}))-\\left(r(s_{1}^{k},\\pi^{*}(s_{1}^{k}))+P_{1}V_{2}^{*}(s_{1}^{k},\\pi^{*}(s_{1}^{k}))\\right)}\\\\ &{=P_{1}(V_{2}^{*}-V_{2}^{*})(s_{1}^{k},\\pi^{*}(s_{1}^{k}))-t_{1}^{k}(s_{1}^{k},\\pi^{*}(s_{1}^{k}))}\\\\ &{=R_{2}\\nu_{1}^{*},\\cdot\\alpha(s_{1}^{k})\\left[(V_{2}^{*}-V_{2}^{*})(x)\\right]-t_{1}^{k}(s_{1}^{k},\\pi^{*}(s_{1}^{k}))}\\\\ &{\\geq\\mathbb{E}_{\\alpha_{2}^{*}\\nu_{1}^{*},\\pi^{*}(s_{1}^{k})}\\left[(Q_{2}^{*}-Q_{2}^{*})(x_{2}^{k},\\pi^{*}(x_{2}^{k}))\\right]-t_{1}^{k}(s_{1}^{k},\\pi^{*}(s_{1}^{k}))}\\\\ &{=\\mathbb{E}_{\\alpha_{2}^{*}\\nu_{1}^{*},\\pi^{*}(s_{1}^{k})}\\left[\\mathbb{E}_{\\alpha_{1}^{*},\\pi^{*}(s_{1}^{k})}\\left[(V_{3}^{*}-V_{3}^{*})(x)\\right]-t_{2}^{k}(x_{2}^{k},\\pi^{*}(x_{2}^{k}) \n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "then by applying this argument recursively, we finally have ", "page_idx": 34}, {"type": "equation", "text": "$$\nV_{1}^{k}(s_{1}^{k})-V_{1}^{*}(s_{1}^{k})\\geq\\mathbb{E}_{\\pi^{*}}\\left[\\sum_{h=1}^{H}-\\iota_{h}^{k}(x_{h},a_{h})\\mid x_{1}=s_{1}^{k}\\right]\\,.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "C.4.2 Proof of Lemma 8 ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Proof of Lemma 8. Since we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\iota_{h}^{k}(s,a)=Q_{h}^{k}(s,a)-\\left(r(s,a)+P_{h}V_{h+1}^{k}(s,a)\\right)}\\\\ &{\\qquad\\qquad=\\operatorname*{min}\\left\\{r(s,a)+\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}_{s,a}}P_{\\theta_{h}^{k}}(s^{\\prime}\\mid s,a)V_{h+1}^{k}(s^{\\prime})+\\displaystyle\\operatorname*{max}_{m\\in[M]}\\hat{\\varphi}_{k,h}(s,a)^{\\top}\\xi_{k,h}^{(m)},H\\right\\}}\\\\ &{\\qquad\\qquad-\\left(r(s,a)+P_{h}V_{h+1}^{k}(s,a)\\right)}\\\\ &{\\qquad\\qquad\\geq\\operatorname*{min}\\left\\{\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}_{s,a}}P_{\\theta_{h}^{k}}(s^{\\prime}\\mid s,a)V_{h+1}^{k}(s^{\\prime})+\\displaystyle\\operatorname*{max}_{m\\in[M]}\\hat{\\varphi}_{k,h}(s,a)^{\\top}\\xi_{k,h}^{(m)}-P_{h}V_{h+1}^{k}(s,a),0\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "it is enough to show that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\theta_{h}^{k}}(s^{\\prime}\\mid s,a)V_{h+1}^{k}(s^{\\prime})+\\operatorname*{max}_{m\\in[M]}\\hat{\\varphi}_{k,h}(s,a)^{\\top}\\xi_{k,h}^{(m)}-P_{h}V_{h+1}^{k}(s,a)\\geq0\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "at least with constant probability. ", "page_idx": 34}, {"type": "text", "text": "On the other hand, under the event $\\mathcal{G}_{k,h}(\\delta)$ , by Lemma 4 we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\theta_{h}^{k}}(s^{\\prime}\\mid s,a)V_{h+1}^{k}(s^{\\prime})+\\operatorname*{max}_{m\\in[M]}\\hat{\\varphi}_{k,h}(s,a)^{\\top}\\xi_{k,h}^{(m)}-P_{h}V_{h+1}^{k}(s,a)}\\\\ &{\\geq\\displaystyle\\operatorname*{max}_{m\\in[M]}\\hat{\\varphi}_{k,h}(s,a)^{\\top}\\xi_{k,h}^{(m)}-H\\alpha_{k}(\\delta)\\|\\hat{\\varphi}_{k,h}(s,a)\\|_{\\mathbf A_{k,h}^{-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Now, for \u2200m \u2208[M], since \u03be(k,mh) \u223cN(0d, \u03c3k2Ak\u2212,1h), we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\hat{\\varphi}_{k,h}(s,a)^{\\top}\\xi_{k,h}^{(m)}\\sim{\\cal N}(0,\\sigma_{k}^{2}\\|\\hat{\\varphi}_{k,h}(s,a)\\|_{{\\bf A}_{k,h}^{-1}}^{2})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "which means, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\hat{\\varphi}_{k,h}(s,a)^{\\top}\\xi_{k,h}^{(m)}\\geq H\\alpha_{k}(\\delta)\\|\\hat{\\varphi}_{k,h}(s,a)\\|_{\\mathbf{A}_{k,h}^{-1}}\\right)\\geq\\Phi(-1)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "by setting $\\sigma_{k}=H\\alpha_{k}(\\delta)$ . Then, finally we have the desired results as follows: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(-\\iota_{h}^{k}(s,a)\\geq0\\ |\\ \\mathcal{G}_{k,h}^{\\Delta}(\\delta)\\right)}\\\\ &{\\geq\\mathbb{P}\\left(\\operatorname*{max}_{m\\in[M]}\\hat{\\varphi}_{k,h}(s,a)^{\\top}\\xi_{k,h}^{(m)}\\geq H\\alpha_{k}(\\delta)\\|\\hat{\\varphi}_{k,h}(s,a)\\|_{\\mathbf{A}_{k,h}^{-1}}\\ |\\ \\mathcal{G}_{k,h}^{\\Delta}(\\delta)\\right)}\\\\ &{=1-\\mathbb{P}\\left(\\hat{\\varphi}_{k,h}(s,a)^{\\top}\\xi_{k,h}^{(m)}<H\\alpha_{k}(\\delta)\\|\\hat{\\varphi}_{k,h}(s,a)\\|_{\\mathbf{A}_{k,h}^{-1}},\\forall m\\in[M]\\ |\\ \\mathcal{G}_{k,h}^{\\Delta}(\\delta)\\right)}\\\\ &{\\geq1-(1-\\Phi(-1))^{M}}\\\\ &{=1-\\Phi(1)^{M}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "C.4.3 Proof of Lemma 9 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Proof of Lemma 9. For each $h\\in[H]$ and $k\\in[K]$ , define an event $\\mathcal{E}_{h}^{k}:=\\{-\\iota_{h}^{k}(s_{h},a_{h})\\geq0\\}$ Then it holds ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(-\\iota_{h}^{k}(s_{h},a_{h})\\geq0,\\forall h\\in[H]\\ |\\ \\mathcal{G}_{k}^{\\Delta}(\\delta)\\right)=\\mathbb{P}\\left(\\prod_{h=1}^{H}\\mathcal{G}_{k}^{k}\\ |\\ \\mathcal{G}_{k}^{\\Delta}(\\delta)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=1-\\mathbb{P}\\left(\\bigcup_{h=1}^{H}\\mathcal{G}_{k}^{k}(\\delta)^{\\epsilon}\\ |\\ \\mathcal{G}_{k}^{\\Delta}(\\delta)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\geq1-\\sum_{h=1}^{H}\\mathbb{P}\\left((\\mathcal{E}_{h}^{k})^{\\epsilon}\\ |\\ \\mathcal{G}_{k,h}^{\\Delta}(\\delta)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\geq1-H\\Phi(1)^{M}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\geq\\Phi(-1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where the first inequality uses the union bound, the second inequality comes from the Lemma 8 and the last inequality holds due to the choice of $\\begin{array}{r}{M=\\lceil1-\\frac{\\log H}{\\log\\Phi(1)}\\rceil}\\end{array}$ . \u53e3 ", "page_idx": 35}, {"type": "text", "text": "C.5 Bound on Estimation Part ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "We decompose the regret into the estimation part and the pessimism part as follows: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}(V_{1}^{*}-V_{1}^{\\pi^{k}})(s_{1}^{k})=\\sum_{k=1}^{K}\\Big(\\underbrace{V_{1}^{*}-V_{1}^{k}}_{\\mathrm{Pessimism}}+\\underbrace{V_{1}^{k}-V_{1}^{\\pi^{k}}}_{\\mathrm{Estimation}}\\Big)(s_{1}^{k})\\,,\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and we bound these two parts in the following sections, respectively. ", "page_idx": 35}, {"type": "text", "text": "Lemma 10 (Bound on estimation part). For any $\\delta\\in(0,1)$ , $i f\\lambda\\ge L_{\\varphi}^{2}$ , then with probability at least $1-\\delta/2$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}(V_{1}^{k}-V_{1}^{\\pi^{k}})(s_{1}^{k})=\\widetilde{\\mathcal{O}}\\left(\\kappa^{-1}d^{\\frac{3}{2}}H^{\\frac{3}{2}}\\sqrt{T}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof of lemma $I O$ . For any given $k\\in[K]$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(V_{1}^{k}-V_{1}^{\\pi^{k}})(s_{1}^{k})=(Q_{1}^{k}-Q_{1}^{\\pi^{k}})(s_{1}^{k},a_{1}^{k})+\\iota_{1}^{k}(s_{1}^{k},a_{1}^{k})-\\iota_{1}^{k}(s_{1}^{k},a_{1}^{k})}\\\\ &{\\phantom{(V_{1}^{k}-V_{1}^{\\pi^{k}})(s_{1}^{k},a_{1}^{k})}=(Q_{1}^{k}-Q_{1}^{\\pi^{k}})(s_{1}^{k},a_{1}^{k})+P_{1}(V_{2}^{k}-V_{2}^{\\pi^{k}})(s_{1}^{k},a_{1}^{k})}\\\\ &{\\phantom{(V_{1}^{k}-V_{1}^{\\pi^{k}})(s_{1}^{k},a_{1}^{k})}+(Q_{1}^{\\pi^{k}}-Q_{1}^{k})(s_{1}^{k},a_{1}^{k})}\\\\ &{\\phantom{(V_{1}^{k}-V_{2}^{\\pi^{k}})(s_{1}^{k},a_{1}^{k})}=\\underbrace{\\frac{P_{1}(V_{2}^{k}-V_{2}^{\\pi^{k}})(s_{2}^{k})}{\\tilde{\\zeta}_{1}^{k}}\\cdot(V_{2}^{k}-V_{2}^{\\pi^{k}})(s_{2}^{k})}_{\\tilde{\\zeta}_{1}^{k}}+(V_{2}^{k}-V_{2}^{\\pi^{k}})(s_{2}^{k})-\\iota_{1}^{k}(s_{1}^{k},a_{1}^{k})}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where the second equality holds due to the variant of $\\iota_{h}^{k}(s_{h}^{k},a_{h}^{k})$ as follows: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\iota_{h}^{k}(s_{h}^{k},a_{h}^{k})=r(s_{h}^{k},a_{h}^{k})+P_{h}V_{h+1}^{k}(s_{h}^{k},a_{h}^{k})-Q_{h}^{k}(s_{h}^{k},a_{h}^{k})+Q_{h}^{\\pi^{k}}(s_{h}^{k},a_{h}^{k})-Q_{h}^{\\pi^{k}}(s_{h}^{k},a_{h}^{k})}\\\\ &{\\quad\\quad\\quad=r(s_{h}^{k},a_{h}^{k})+P_{h}V_{h+1}^{k}(s_{h}^{k},a_{h}^{k})-Q_{h}^{k}(s_{h}^{k},a_{h}^{k})}\\\\ &{\\quad\\quad\\quad\\quad\\quad+Q_{h}^{\\pi^{k}}(s_{h}^{k},a_{h}^{k})-\\Big(r(s_{h}^{k},a_{h}^{k})+P_{h}V_{h+1}^{\\pi^{k}}(s_{h}^{k},a_{h}^{k})\\Big)}\\\\ &{\\quad\\quad\\quad=P_{h}(V_{h+1}^{k}-V_{h+1}^{\\pi^{k}})(s_{h}^{k},a_{h}^{k})+(Q_{h}^{\\pi^{k}}-Q_{h}^{k})(s_{h}^{k},a_{h}^{k})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Then, by applying this argument recursively for whole horizon, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n(V_{1}^{k}-V_{1}^{\\pi^{k}})(s_{1}^{k})=\\sum_{h=1}^{H}-\\iota_{h}^{k}(s_{h}^{k},a_{h}^{k})+\\sum_{h=1}^{H}\\dot{\\zeta}_{h}^{k}\\,,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $\\dot{\\zeta}_{h}^{k}:=P_{h}(V_{h+1}^{k}-V_{h+1}^{\\pi^{k}})(s_{h}^{k},a_{h}^{k})-(V_{h+1}^{k}-V_{h+1}^{\\pi^{k}})(s_{h+1}^{k}).$ ", "page_idx": 36}, {"type": "text", "text": "Let $\\delta^{\\prime}=\\delta/(8K H)$ . By Lemma 5, the good event $\\mathcal{G}(K,\\delta^{\\prime})$ holds with probability at least $1-\\delta/4$ . Then under the event $\\mathcal{G}(K,\\delta^{\\prime})$ , for any $h\\in[H]$ we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-i\\binom{k}{n}\\binom{k}{n},}\\\\ &{=Q_{k}^{k}\\binom{n}{k},\\qquad\\bigl(r(s_{k}^{k},a_{k}^{k})+P_{1}V_{k+1}^{k}(s_{k}^{k},a_{k}^{k})\\bigr)}\\\\ &{=\\operatorname*{min}\\left\\{r\\binom{n}{k},a_{k}\\overset{k}{\\sim}\\sum_{\\ell\\in\\mathcal{K}_{k}}P_{k}\\binom{n}{\\ell}\\binom{k^{\\prime}}{n},b_{k}^{\\ell}{\\cal V}_{k+1}^{k}(s^{\\prime})+\\operatorname*{max}_{\\ell\\in[\\mathcal{M}]}\\hat{\\varphi}_{k,\\,h}\\binom{n}{k}\\top_{\\ell}\\binom{m}{k},H\\right\\}}\\\\ &{\\qquad-\\left(r(s_{k}^{k},a_{k}^{k})+P_{1}V_{k+1}^{k}(s_{k}^{k},a_{k}^{k})\\right)}\\\\ &{\\overset{\\leq}\\ldots\\sqrt{\\binom{n}{k}\\binom{k^{\\prime}}{k}\\binom{n}{k}\\binom{k^{\\prime}}{n}{k}\\binom{k^{\\prime}}{n}+\\frac{m a x}{n\\in[n]}\\hat{\\varphi}_{k,\\,h}\\binom{n}{k}\\binom{n}{k}\\binom{n}{k}-P_{k}V_{k+1}^{k}(s_{k}^{k},a_{k}^{k})}}\\\\ &{\\overset\\leq\\left\\lvert\\sqrt{\\sum_{\\ell\\in\\mathcal{K}_{k}}\\hat{\\varphi}_{k}^{(k^{\\prime}}|s_{k}^{k},a_{k}^{k})\\overset{k^{\\prime}\\to}{\\prod_{\\ell=1}^{k}(s^{\\prime})}-P_{1}V_{k+1}^{k}(s_{k}^{k},a_{k}^{k})\\right\\rvert+\\operatorname*{max}_{\\ell\\in[\\mathcal{M}]}\\left\\lvert\\hat{\\varphi}_{k,\\,h}\\binom{n}{k}\\right\\rvert\\prod_{k^{\\prime}}\\hat{\\varphi}_{k,\\,h}\\^{k^{\\prime}}\\prod_{\\ell=1}^{k}\\hat{\\varphi}_{k,\\,h}\\right\\rvert}\\\\ &{\\overset{\\leq}\\left\\lvert\\langle\\Delta_{k}^{k}(s_{k}^{k},a_{k}^{k})\\Big\\rvert+\\operatorname*{max}_{\\ell\\in[\\mathcal{M}]}\\Big\\lvert\\hat{\\varphi}_{k,\\,h}\\binom{n}{k}\\rVert_{\\ell \n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where (30) comes from the Cauchy-Schwarz inequality and (31) holds due the the Lemma 4 & 30. Then, with probability at least $1-\\delta/4$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{h=1}^{H}-\\iota_{h}^{k}(s_{h}^{k},a_{h}^{k})\\leq\\sum_{h=1}^{H}\\left(H\\alpha_{k}(\\delta^{\\prime})+\\gamma_{k}(\\delta^{\\prime})\\right)\\|\\hat{\\varphi}_{k,h}(s_{h}^{k},a_{h}^{k})\\|_{\\mathbf{A}_{k,h}^{-1}}\\,.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "On the other hand, for $\\dot{\\zeta}_{h}^{k}$ , we have $|\\dot{\\zeta}_{h}^{k}|\\leq2H$ and $\\mathbb{E}[\\dot{\\zeta}_{h}^{k}\\mid\\mathcal{F}_{k,h}]=0$ , which means $\\{\\dot{\\zeta}_{h}^{k}\\mid\\mathcal{F}_{k,h}\\}_{k,h}$ is a martingale difference sequence for any $k\\in[K]$ and $h\\in[H]$ . Hence, by applying the AzumaHoeffding inequality with probability at least $1-\\delta/4$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\dot{\\zeta}_{h}^{k}\\leq2H\\sqrt{2K H\\log(4/\\delta)}\\,.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Combining the results of (32) and (33), with probability at least $1-\\delta/2$ , we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|V_{1}^{k}-V_{1}^{k}|^{\\alpha}|)_{1}^{k}}\\\\ &{\\le2H\\sqrt{2\\operatorname{IPE}(4|\\beta)}+\\displaystyle\\sum_{k=1}^{K}\\sum_{l=1}^{n}(I t a_{k}(\\beta^{\\prime})+\\gamma_{k}(\\beta^{\\prime}))\\big\\|\\hat{\\varphi}_{k,l}(s_{k}^{k},a_{l}^{k})\\big\\|_{\\mathbf{k}_{1}^{\\perp}}}\\\\ &{\\le2H\\sqrt{2\\operatorname{IPE}(4|\\beta)}+(H\\alpha_{k}(\\beta^{\\prime})+\\gamma_{k}(\\beta^{\\prime}))\\displaystyle\\sum_{k=1}^{K}\\sum_{l=1}^{H}\\Big\\|\\hat{\\varphi}_{k,l}(s_{k}^{k},a_{l}^{k})\\Big\\|_{\\mathbf{k}_{1}^{\\perp}}}\\\\ &{\\le2H\\sqrt{2\\operatorname{IPE}(4|\\beta)}+(H\\alpha_{k}(\\beta^{\\prime})+\\gamma_{k}(\\beta^{\\prime}))\\displaystyle\\sum_{k=1}^{K}\\sqrt{\\sum_{k=1}^{K}\\|\\hat{\\varphi}_{k,l}(s_{k}^{k},a_{l}^{k})\\|_{\\mathbf{k}_{1}^{\\perp}}}}\\\\ &{\\le2H\\sqrt{2\\operatorname{IPE}(4|\\beta)}+(H\\alpha_{k}(\\beta^{\\prime})+\\gamma_{k}(\\beta^{\\prime}))\\displaystyle\\sum_{k=1}^{M}\\sqrt{\\sum_{k=1}^{K}\\|\\hat{\\varphi}_{k,l}(s_{k}^{k},a_{l}^{k})\\|_{\\mathbf{k}_{1}^{\\perp}}}}\\\\ &{=2H\\sqrt{2H\\operatorname{IPE}(4|\\beta)}+(H\\alpha_{k}(\\beta^{\\prime})+\\gamma_{k}(\\beta^{\\prime}))\\displaystyle\\sum_{k=1}^{M}\\sqrt{4\\kappa^{-1}H\\operatorname{Iog}\\Big(1+\\frac{K U L_{2}^{2}}{d\\lambda}\\Big)}}\\\\ &{=2H\\sqrt{2H\\operatorname{IPE}(4|\\beta)}+(H\\alpha_{k}(\\beta^{\\prime})+\\gamma_{k}(\\beta^{\\prime}))\\displaystyle\\sqrt{4\\kappa^{-1}T H\\operatorname{Iog}\\Big(1+\\frac{K U L_{2}^{2}}{d\\lambda}\\Big)},}\\\\ &{=\\tilde{O}\\left(\\kappa^{-1}d_{k}^{1}\\mathbb\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where (34) follows from the fact that both $\\alpha_{k}(\\delta)$ and $\\gamma_{k}(\\delta)$ are increasing in $k$ , (35) comes from Cauchy-Schwarz inequality and (36) holds by the generalized elliptical potential lemma (Lemma 3). ", "page_idx": 37}, {"type": "text", "text": "C.6 Bound on Pessimism Part ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Lemma 11 (Bound on pessimism). For any $\\delta$ with $0<\\delta<\\Phi(-1)/2,$ , let $\\sigma_{k}=H\\alpha_{k}(\\delta)$ . If $\\lambda\\geq L_{\\varphi}^{2}$ and we take multiple sample size $\\begin{array}{r}{M=\\lceil1-\\frac{\\log H}{\\log\\Phi(1)}\\rceil}\\end{array}$ , then with probability at least $1-\\delta/2$ , we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}(V_{1}^{*}-V_{1}^{k})(s_{1}^{k})=\\widetilde{\\mathcal{O}}\\left(\\kappa^{-1}d^{\\frac{3}{2}}H^{\\frac{3}{2}}\\sqrt{T}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof of lemma $_{l l}$ . Similar to the techniques used in [73], we show that the difference between the optimal value function $V_{1}^{*}$ and the estimated value function $V_{1}^{k}$ can be controlled by constructing an upper bound on $V_{1}^{*}$ and a lower bound on $V_{1}^{k}$ . In this proof, we consider three kinds of pseudo-noises, $\\xi,\\bar{\\xi}$ and $\\pmb{\\xi}$ that we define later in the proof. Also, for $\\delta^{\\prime}=\\delta/10$ , we denote $\\mathcal{G}(K,\\delta^{\\prime}),\\bar{\\mathcal{G}}(K,\\delta^{\\prime})$ and $\\mathcal{G}(K,\\delta^{\\prime})$ as the good events induced by $\\xi,\\bar{\\xi}$ and $\\underline{{\\boldsymbol{\\xi}}}$ respectively. From now on, we denote $G(K,\\delta^{\\prime})$ by the event $\\mathcal{G}(K,\\delta^{\\prime})\\cap\\bar{\\mathcal{G}}(K,\\delta^{\\prime})\\cap\\underline{{\\mathcal{G}}}(K,\\delta^{\\prime})$ . Then, by Lemma 5, the event $G(K,\\delta^{\\prime})$ holds with high probability at least $1-3\\delta/10$ . ", "page_idx": 37}, {"type": "text", "text": "First, we construct the lower bound of $V_{1}^{k}$ . For any given $k\\in[K]$ , let $\\widetilde{\\xi}:=\\{\\widetilde{\\xi}_{\\boldsymbol{k},\\boldsymbol{h}}^{(m)}\\}_{m\\in[M]}\\subset\\mathbb{R}^{d}$ be a set of vectors for $h\\in[H]$ and $V_{h}^{k}(\\cdot\\,;\\tilde{\\pmb\\xi})$ be the value function obtained by the Algorithm 1 with non-random $\\widetilde{\\xi}_{k,h}^{(m)}$ in place of $\\pmb{\\xi}_{k,h}^{(m)}$ . Then consider the following minimization problem: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\{\\widetilde{\\xi}_{k,h}^{(m)}\\}_{h\\in[H],m\\in[M]}}{\\operatorname*{min}}}&{V_{1}^{k}(s_{1}^{k};\\widetilde{\\pmb{\\xi}})}\\\\ {\\mathrm{s.t.}}&{\\underset{m\\in[M]}{\\operatorname*{max}}\\,||\\widetilde{\\xi}_{k,h}^{(m)}||_{{\\pmb{\\mathrm{A}}}_{k,h}}\\leq\\gamma_{k}(\\delta),\\quad\\forall h\\in[H]}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "And we denote $\\underline{{\\pmb{\\xi}}}\\;:=\\;\\{\\underline{{\\pmb{\\xi}}}_{k,h}^{(m)}\\}_{h\\in[H],m\\in[M]}$ by a minimizer and $\\underline{{V}}_{1}^{k}(s_{1}^{k})$ by the minimum of the above minimization problem, i.e., $\\underline{{{V}}}_{h}^{k}(\\cdot)\\;:=\\;V_{h}^{k}(\\cdot\\,;\\underline{{{\\xi}}})$ . Then, under the event $\\mathcal{G}(K,\\delta^{\\prime})$ , since $\\{\\xi_{k,h}^{(m)}\\}_{h\\in[H],m\\in[M]}$ is also a feasible solution of the above optimization problem, and since $V_{h}^{k}=$ $V_{h}^{k}(:\\mathfrak{X})$ , thus we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\underline{{{V}}}_{1}^{k}(s_{1}^{k})\\leq V_{1}^{k}(s_{1}^{k})\\,.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Second, to find an upper bound for $V^{*}$ , considering i.i.d copies $\\{\\bar{\\xi}_{k,h}^{(m)}\\}_{h\\in[H],m\\in[M]}$ of $\\{\\xi_{k,h}^{(m)}\\}_{h\\in[H],m\\in[M]}$ and run Algorithm 1 to get a corresponding value function $\\bar{V}_{h}^{k}$ and $\\bar{Q}_{h}^{k}$ for all $h\\in[H]$ . Define the event that $\\bar{V}_{1}^{k}(s_{1}^{k})$ is optimistic in the $k$ -th episode as ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\bar{\\mathcal{X}}_{k}=\\{(\\bar{V}_{1}^{k}-V_{1}^{*})(s_{1}^{k})\\geq0\\}\\,.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Then by Lemma 6, for given $\\delta$ , we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\bar{\\mathcal{X}}_{k}\\mid s_{1}^{k},\\mathcal{F}_{k})\\ge\\Phi(-1)/2\\,.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Then by the definition of optimism, under the event $\\mathcal{G}(K,\\delta^{\\prime})$ , we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ (V_{1}^{*}-V_{1}^{k})(s_{1}^{k})\\leq\\mathbb{E}_{\\bar{\\xi}|\\bar{\\mathcal{X}}_{k}}\\left[(\\bar{V}_{1}^{k}-V_{1}^{k})(s_{1}^{k})\\right]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\leq\\mathbb{E}_{\\bar{\\xi}|\\bar{\\mathcal{X}}_{k}}\\left[(\\bar{V}_{1}^{k}-\\underline{{V}}_{1}^{k})(s_{1}^{k})\\right]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where the expectations are over the $\\bar{\\pmb\\xi}$ \u2019s conditioned on the event $\\bar{\\mathcal{X}}_{k}$ and the second inequality comes from (37). On the other hand, under the event $\\bar{\\mathcal{G}}(K,\\delta^{\\prime})$ by the law of the total expectation, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\bar{\\xi}}\\left[(\\bar{V}_{1}^{k}-\\underline{{V}}_{1}^{k})(s_{1}^{k})\\right]=\\mathbb{E}_{\\bar{\\xi}|\\bar{x}_{k}}\\left[(\\bar{V}_{1}^{k}-\\underline{{V}}_{1}^{k})(s_{1}^{k})\\right]\\mathbb{P}(\\bar{X}_{k})+\\mathbb{E}_{\\bar{\\xi}|\\bar{x}_{k}^{\\ast}}\\left[(\\bar{V}_{1}^{k}-\\underline{{V}}_{1}^{k})(s_{1}^{k})\\right]\\mathbb{P}(\\bar{X}_{k}^{\\mathrm{c}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq\\mathbb{E}_{\\bar{\\xi}|\\bar{x}_{k}}\\left[(\\bar{V}_{1}^{k}-\\underline{{V}}_{1}^{k})(s_{1}^{k})\\right]\\mathbb{P}(\\bar{X}_{k})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where (39) comes from the fact that $\\{\\bar{\\xi}_{k,h}^{(m)}\\}_{h\\in[H],m\\in[M]}$ is also a feasible solution of the above optimization problem under the event $\\bar{\\mathcal{G}}(K,\\delta^{\\prime})$ , i.e., $\\dot{V}_{1}^{k}(s_{1}^{k})\\geq\\underbar{V}_{1}^{k}(s_{1}^{k})$ . Then, by combining the results of (39) and (38), under the event $G(K,\\delta^{\\prime})$ , we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(V_{1}^{*}-V_{1}^{k})(s_{1}^{k})\\le\\mathbb{E}_{\\bar{\\xi}|\\bar{x}_{k}}\\left[(\\bar{V}_{1}^{k}-\\underline{{V}}_{1}^{k})(s_{1}^{k})\\right]}\\\\ &{\\phantom{(V_{1}^{k}-V_{1}^{k})(s_{1}^{k})}\\le\\mathbb{E}_{\\bar{\\xi}}\\left[(\\bar{V}_{1}^{k}-\\underline{{V}}_{1}^{k})(s_{1}^{k})\\right]/\\mathbb{P}(\\bar{X}_{k})}\\\\ &{\\phantom{(V_{1}^{k}-V_{1}^{k})(s_{1}^{k})}\\le\\frac{2}{\\Phi(-1)}\\mathbb{E}_{\\bar{\\xi}}\\left[(\\bar{V}_{1}^{k}-V_{1}^{k}+V_{1}^{k}-\\underline{{V}}_{1}^{k})(s_{1}^{k})\\right]}\\\\ &{\\phantom{(V_{1}^{k}-V_{1}^{k})(s_{1}^{k})}=\\frac{2}{\\Phi(-1)}\\left((V_{1}^{k}-\\underline{{V}}_{1}^{k})(s_{1}^{k})\\right)+\\ddot{\\zeta}_{k}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where we denote ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\ddot{\\zeta}_{k}:=\\frac{2}{\\Phi(-1)}\\left(\\mathbb{E}_{\\bar{\\xi}}\\left[\\bar{V}_{1}^{k}(s_{1}^{k})\\right]-V_{1}^{k}(s_{1}^{k})\\right)\\,.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "$\\bar{\\pmb\\xi}$ si sa  thmea rit.ii.ndg acloe pdyi ffoef $\\xi$ ,n cteh e $\\bar{V}_{k,1}$ $V_{k,1}$ .e pTehne bhyi cahp pmleyianngs $\\{\\ddot{\\zeta}_{k}~\\vert~\\mathcal{F}_{k-1}\\}_{k=1}^{K}$ $|\\ddot{\\zeta}_{k}|\\,\\leq\\,\\frac{2H}{\\Phi(-1)}$ $G(K,\\delta^{\\prime})$ $1-\\delta^{\\prime}$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\ddot{\\zeta}_{k}\\leq\\frac{2H}{\\Phi(-1)}\\sqrt{2K\\log(1/\\delta^{\\prime})}\\,.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "On the other hand, by dividing the first term in (40) into two terms we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n(V_{1}^{k}-\\underline{{V_{1}^{k}}})(s_{1}^{k})=\\underbrace{(V_{1}^{k}-V_{1}^{\\pi^{k}})(s_{1}^{k})}_{I_{1}}+\\underbrace{(V_{1}^{\\pi^{k}}-\\underline{{V_{1}^{k}}})(s_{1}^{k})}_{I_{2}}\\;.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "For $I_{1}$ , note that since it is related to the estimation error, under the event $G(K,\\delta^{\\prime})$ we can bound the sum of $I_{1}$ for the total episode number using Lemma 10 as follows: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=1}^{K}(V_{1}^{k}-V_{1}^{\\pi^{k}})(s_{1}^{k})\\leq(H\\alpha_{K}(\\delta^{\\prime})+\\gamma_{K}(\\delta^{\\prime}))\\,\\sqrt{4\\kappa^{-1}T H d\\log\\left(1+\\frac{K\\mathcal{U}L_{\\varphi}^{2}}{d\\lambda}\\right)}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\,2H\\sqrt{2T\\log(1/\\delta^{\\prime})}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "For $I_{2}$ , since we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{2}=Q_{1}^{\\pi^{k}}(s_{1}^{k},a_{1}^{k})-\\underline{{V}}_{1}^{k}(s_{1}^{k})}\\\\ &{\\quad\\leq Q_{1}^{\\pi^{k}}(s_{1}^{k},a_{1}^{k})-\\underline{{Q}}_{1}^{k}(s_{1}^{k},a_{1}^{k})}\\\\ &{\\quad=Q_{1}^{\\pi^{k}}(s_{1}^{k},a_{1}^{k})-\\underline{{Q}}_{1}^{k}(s_{1}^{k},a_{1}^{k})-\\underline{{\\iota}}_{1}^{k}(s_{1}^{k},a_{1}^{k})+\\underline{{\\iota}}_{1}^{k}(s_{1}^{k},a_{1}^{k})}\\\\ &{\\quad=P_{1}(V_{2}^{\\pi^{k}}-\\underline{{V}}_{2}^{k})(s_{1}^{k},a_{1}^{k})+\\underline{{\\iota}}_{1}^{k}(s_{1}^{k},a_{1}^{k})}\\\\ &{\\quad=\\underbrace{P_{1}(V_{2}^{\\pi^{k}}-\\underline{{V}}_{2}^{k})(s_{1}^{k},a_{1}^{k})}_{\\overset{\\ddots{k}}{\\vdots}_{1}}-(V_{2}^{\\pi^{k}}-\\underline{{V}}_{2}^{k})(s_{2}^{k})}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where (43) comes from $a_{1}^{k}\\,=\\,\\mathrm{argmax}_{a}\\,Q_{1}^{k}(s_{1}^{k},a)$ and (44) holds by the following definition of $\\underline{{\\iota}}_{h}^{k}(s_{h}^{k},a_{h}^{k})$ : ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underline{{\\iota}}_{h}^{k}(s_{h}^{k},a_{h}^{k}):=r(s_{h}^{k},a_{h}^{k})+P_{h}\\underline{{V}}_{h+1}^{k}(s_{h}^{k},a_{h}^{k})-\\underline{{Q}}_{h}^{k}(s_{h}^{k},a_{h}^{k})}\\\\ &{\\qquad\\qquad=r(s_{h}^{k},a_{h}^{k})+P_{h}\\underline{{V}}_{h+1}^{k}(s_{h}^{k},a_{h}^{k})-\\underline{{Q}}_{h}^{k}(s_{h}^{k},a_{h}^{k})+Q_{h}^{\\pi^{k}}(s_{h}^{k},a_{h}^{k})-Q_{h}^{\\pi^{k}}(s_{h}^{k},a_{h}^{k})}\\\\ &{\\qquad\\qquad=P_{h}(\\underline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{\\pi^{k}})(s_{h}^{k},a_{h}^{k})+(Q_{h}^{\\pi^{k}}-\\underline{{Q}}_{h}^{k})(s_{h}^{k},a_{h}^{k})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Then by applying the same argument recursively for the whole horizon, we have ", "page_idx": 39}, {"type": "equation", "text": "$$\nI_{2}\\leq\\sum_{h=1}^{H}\\underline{{{\\iota}}}_{h}^{k}(s_{h}^{k},a_{h}^{k})+\\sum_{h=1}^{H}\\stackrel{\\cdots}{\\zeta}_{h}^{k}\\,,\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where we denote ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\ddagger_{h}^{k}:=P_{h}(V_{h+1}^{\\pi^{k}}-\\underline{{V}}_{h+1}^{k})(s_{h}^{k},a_{h}^{k})-(V_{h+1}^{\\pi^{k}}-\\underline{{V}}_{h+1}^{k})(s_{h+1}^{k})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Note that \u03b6kh | Fk,h k,h i s a martingale difference sequence with $|\\stackrel{{\\bf\\cdots}}{\\zeta}_{h}^{k}|\\leq2H$ . Then, under the event $G(K,\\delta^{\\prime})$ by applying the Azuma-Hoeffding inequality with probability at least $1-\\delta^{\\prime}$ , we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\stackrel{\\cdot\\cdot\\cdot k}{\\zeta}_{h}^{k}\\leq2H\\sqrt{2T\\log(1/\\delta^{\\prime})}\\,.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "To bound $\\begin{array}{r}{\\sum_{h=1}^{H}\\underline{{\\iota}}_{h}^{k}(s_{h}^{k},a_{h}^{k})}\\end{array}$ , we divide the whole horizon index set into two groups as follows: $H^{+}$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad=\\left\\{j\\in[H]:r(s_{j}^{k},a_{j}^{k})+\\displaystyle\\sum_{s^{\\prime}\\in{\\mathcal{S}}_{k,j}}P_{\\theta_{h}^{k}}(s^{\\prime}\\mid s_{j}^{k},a_{j}^{k})\\underline{{V}}_{j+1}^{k}(s^{\\prime})+\\displaystyle\\operatorname*{max}_{m\\in[M]}\\hat{\\varphi}_{k,j}(s_{j}^{k},a_{j}^{k})^{\\top}\\underline{{\\xi}}_{k,j}^{(m)}>H\\right\\}}\\\\ &{H^{-}=[H]\\backslash H^{+}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Then, for $j\\in H^{+}$ since $\\underline{{Q}}_{j}^{k}(s_{j}^{k},a_{j}^{k})=H-j+1,\\underline{{V}}_{j+1}^{k}\\le H-j$ and $r(s_{j}^{k},a_{j}^{k})\\le1$ , we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\iota_{j}^{k}(s_{j}^{k},a_{j}^{k})=r(s_{j}^{k},a_{j}^{k})+P_{j}\\underline{{V}}_{j+1}^{k}(s_{j}^{k},a_{j}^{k})-(H-j+1)\\leq0\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "On the other hand, for $j\\in H^{-}$ , under the event $G(K,\\delta^{\\prime})$ we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{r l}&{\\underline{{\\iota}}_{j}^{k}(s_{j}^{k},a_{j}^{k})=P_{j}\\underline{{V}}_{j+1}^{k}(s_{j}^{k},a_{j}^{k})-\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}_{k,j}}P_{\\theta_{h}}(s^{\\prime}\\mid s_{j}^{k},a_{j}^{k})\\underline{{V}}_{j+1}^{k}(s^{\\prime})-\\operatorname*{max}_{m\\in[M]}\\hat{\\varphi}_{k,j}(s_{j}^{k},a_{j}^{k})^{\\top}\\underline{{\\xi}}_{k,j}^{(m)}}\\\\ &{\\qquad\\qquad\\leq\\left|P_{j}\\underline{{V}}_{j+1}^{k}(s_{j}^{k},a_{j}^{k})-\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}_{k,j}}P_{\\theta_{h}}(s^{\\prime}\\mid s_{j}^{k},a_{j}^{k})\\underline{{V}}_{j+1}^{k}(s^{\\prime})\\right|+\\left|\\operatorname*{max}_{m\\in[M]}\\hat{\\varphi}_{k,j}(s_{j}^{k},a_{j}^{k})^{\\top}\\underline{{\\xi}}_{k,j}^{(m)}\\right|}\\\\ &{\\qquad\\leq H\\alpha_{k}(\\delta^{\\prime})\\|\\hat{\\varphi}_{k,j}(s_{j}^{k},a_{j}^{k})\\|_{\\mathbf{A}_{k,j}^{-1}}+\\displaystyle\\operatorname*{max}_{m\\in[M]}\\left|\\hat{\\varphi}_{k,j}(s_{j}^{k},a_{j}^{k})^{\\top}\\underline{{\\xi}}_{k,j}^{(m)}\\right|}\\end{array}\\qquad\\qquad~(4;2)}\\\\ &{\\leq H\\alpha_{k}(\\delta^{\\prime})\\|\\hat{\\varphi}_{k,j}(s_{j}^{k},a_{j}^{k})\\|_{\\mathbf{A}_{k,j}^{-1}}+\\displaystyle\\operatorname*{max}_{m\\in[M]}\\left\\|\\hat{\\varphi}_{k,j}(s_{j}^{k},a_{j}^{k})\\right\\|_{\\mathbf{A}_{k,j}^{-1}}\\|\\xi_{k,j}^{(m)}\\|_{\\mathbf{A}_{k,j}}}\\\\ &{\\leq H\\alpha_{k}(\\delta^{\\prime})\\|\\hat{\\varphi}_{k,j}(s_{j\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where (47) holds by Lemma 4. ", "page_idx": 40}, {"type": "text", "text": "By combining the result of (46) and (48), we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle I_{2}\\leq\\sum_{j\\in H^{-}}\\big(H\\alpha_{k}(\\delta^{\\prime})+\\gamma_{k}(\\delta^{\\prime})\\big)\\,\\|\\hat{\\varphi}_{k,j}(s_{j}^{k},a_{j}^{k})\\|_{{\\mathbf A}_{k,j}^{-1}}+\\sum_{h=1}^{H}\\,\\ddot{\\zeta}_{h}^{k}\\,.}\\\\ {\\displaystyle\\leq\\sum_{h=1}^{H}\\big(H\\alpha_{k}(\\delta^{\\prime})+\\gamma_{k}(\\delta^{\\prime})\\big)\\,\\|\\hat{\\varphi}_{k,h}(s_{h}^{k},a_{h}^{k})\\|_{{\\mathbf A}_{k,h}^{-1}}+\\displaystyle\\sum_{h=1}^{H}\\,\\ddot{\\zeta}_{h}^{k}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Then summing $I_{2}$ over the total number of episodes, under the event $G(K,\\delta^{\\prime})$ , we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=1}^{K}(V_{1}^{\\pi^{k}}-\\underline{{V}}_{1}^{k})(s_{1}^{k})\\leq\\displaystyle\\sum_{k=1}^{K}\\sum_{h=1}^{H}(H\\alpha_{k}(\\delta^{\\prime})+\\gamma_{k}(\\delta^{\\prime}))\\,\\|\\hat{\\varphi}_{k,h}(s_{h}^{k},a_{h}^{k})\\|_{\\mathbf{A}_{k,h}^{-1}}+\\displaystyle\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\widetilde{\\zeta}_{h}^{\\cdot k}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\leq\\,(H\\alpha_{K}(\\delta^{\\prime})+\\gamma_{K}(\\delta^{\\prime}))\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\|\\hat{\\varphi}_{k,h}(s_{h}^{k},a_{h}^{k})\\|_{\\mathbf{A}_{k,h}^{-1}}+\\displaystyle\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\widetilde{\\zeta}_{h}^{\\cdot k}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\leq\\,(H\\alpha_{K}(\\delta^{\\prime})+\\gamma_{K}(\\delta^{\\prime}))\\,\\sqrt{4\\kappa^{-1}T H d\\log\\left(1+\\frac{K\\mathcal{U}L_{\\varphi}^{2}}{d\\lambda}\\right)}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\,(1\\times1)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where the last inequality holds due to the Lemma 3 and (45). ", "page_idx": 40}, {"type": "text", "text": "Finally, by summing (40) over $k$ and plugging the results of (42), (50) and (41) then, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=1}^{K}(V_{1}^{*}-V_{1}^{k})(s_{1}^{k})}\\\\ &{\\le\\displaystyle\\frac{4}{\\Phi(-1)}\\left[(H\\alpha_{K}(\\delta^{\\prime})+\\gamma_{K}(\\delta^{\\prime}))\\,\\sqrt{4\\kappa^{-1}T H d\\log\\left(1+\\frac{K U L_{\\varphi}^{2}}{d\\lambda}\\right)}+2H\\sqrt{2T\\log(1/\\delta^{\\prime})}\\right]}\\\\ &{\\displaystyle\\quad+\\,\\frac{2H}{\\Phi(-1)}\\sqrt{2K\\log(1/\\delta^{\\prime})}}\\\\ &{\\le\\tilde{\\mathcal{O}}\\left(\\kappa^{-1}d^{3/2}H^{3/2}\\sqrt{T}+H\\sqrt{T}+H\\sqrt{K}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "To conclude the proof, by setting $\\delta^{\\prime}=\\delta/10$ and we take a union bound over the two applications of Azuma-Hoeffding $(\\ddot{\\zeta}_{k},\\stackrel{...}{\\zeta}_{h}^{...})$ and the event $G(K,\\delta^{\\prime})$ , we get the desired result with probability at least $1-\\delta/2$ . \u53e3 ", "page_idx": 40}, {"type": "text", "text": "C.7 Regret Bound of RRL-MNL ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Proof of Theorem $^{\\,l}$ . We can decompose the regret with estimation part and pessimism part as follows: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\bf{R e g r e t}}_{\\pi}(K)=\\displaystyle\\sum_{k=1}^{K}(V_{1}^{*}-V_{1}^{\\pi^{k}})(s_{1}^{k})}\\\\ {\\quad\\quad\\quad\\quad=\\displaystyle\\sum_{k=1}^{K}(V_{1}^{*}-V_{1}^{k})(s_{1}^{k})+\\displaystyle\\sum_{k=1}^{K}(V_{1}^{k}-V_{1}^{\\pi^{k}})(s_{1}^{k})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Since both Lemma 10 and Lemma 11 holds with probability at least $1-\\delta/2$ respectively, by taking the union bound the following holds with probability at least $1-\\delta$ : ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{}}&{{}}&{{\\mathbf{Regret}_{\\pi}(K)=\\widetilde{{\\mathcal O}}\\left(\\kappa^{-1}d^{\\frac{3}{2}}H^{\\frac{3}{2}}\\sqrt{T}+H\\sqrt{T}+H\\sqrt{K}\\right)+\\widetilde{{\\mathcal O}}\\left(\\kappa^{-1}d^{\\frac{3}{2}}H^{\\frac{3}{2}}\\sqrt{T}+H\\sqrt{T}\\right)}}\\\\ {{}}&{{}}&{{}}\\\\ {{}}&{{}}&{{=\\widetilde{{\\mathcal O}}\\left(\\kappa^{-1}d^{\\frac{3}{2}}H^{\\frac{3}{2}}\\sqrt{T}\\right)\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "D Detailed Regret Analysis for ORRL-MNL (Theorem 2) ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "D.1 Concentration of Estimated Transition Core $\\widetilde{\\pmb{\\theta}}_{h}^{k}$ ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "In this section, we provide the detailed proof of Lemma 12, which demonstrates the concentration result for $\\widetilde{\\pmb{\\theta}}_{h}^{k}$ independently of $\\kappa$ and $\\boldsymbol{\\mathcal{U}}$ . Note that we adapt the proof provided by Zhang and Sugiyama [76] in the MNL contextual bandit setting to MNL-MDPs and improve the result, making it independent of $\\boldsymbol{\\mathcal{U}}$ . We provide the lemmas for the concentration of the online transition core for completeness, noting that there are slight differences compared to their work, which stem from the different problem setting. ", "page_idx": 41}, {"type": "text", "text": "Lemma 12 (Concentration of online estimated transition core). Let $\\eta\\;=\\;\\mathcal{O}(\\log\\mathcal{U})$ and $\\lambda\\,=$ ${\\mathcal{O}}(d\\log{\\mathcal{U}})$ . Then, for any $\\delta\\in(0,1]$ and for any $h\\in[H]$ , we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\forall k\\geq1,\\left\\|\\widetilde{\\pmb{\\theta}}_{h}^{k}-\\pmb{\\theta}_{h}^{*}\\right\\|_{\\mathbf{B}_{k,h}}\\leq\\beta_{k}(\\delta)\\right)\\geq1-\\delta\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $\\beta_{k}(\\delta)=\\mathcal{O}(\\sqrt{d}\\log\\mathcal{U}\\log(k H))$ . ", "page_idx": 41}, {"type": "text", "text": "Proof of Lemma $^{12}$ . Recall that the transition core updated by the online mirror descent is represented by ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\widetilde{\\pmb{\\theta}}_{h}^{k+1}=\\underset{\\pmb{\\theta}\\in\\mathcal{B}(L_{\\pmb{\\theta}})}{\\mathrm{argmin}}\\,\\widetilde{\\ell}_{k,h}(\\pmb{\\theta})+\\frac{1}{2\\eta}\\left\\|\\pmb{\\theta}-\\widetilde{\\pmb{\\theta}}_{h}^{k}\\right\\|_{\\mathbf{B}_{k,h}}^{2},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $\\begin{array}{r l}&{\\widetilde{\\ell}_{k,h}(\\pmb{\\theta})\\,=\\,\\ell_{k,h}(\\widetilde{\\pmb{\\theta}}_{h}^{k})+(\\pmb{\\theta}-\\widetilde{\\pmb{\\theta}}_{h}^{k})^{\\top}\\nabla\\ell_{k,h}(\\widetilde{\\pmb{\\theta}}_{h}^{k})+\\frac{1}{2}\\left\\|\\pmb{\\theta}-\\widetilde{\\pmb{\\theta}}_{h}^{k}\\right\\|_{\\nabla^{2}\\ell_{k,h}(\\widetilde{\\pmb{\\theta}}_{h}^{k})}}\\end{array}$ . We introduce the following lemma providing that the estimation error of the online estimator $\\widetilde{\\pmb{\\theta}}_{h}^{k}$ can be bounded by the regret. ", "page_idx": 41}, {"type": "text", "text": "Lemma 13 (Lemma 12 in [76]). Let $\\alpha=\\log\\mathcal{U}+2(1+L_{\\theta}L_{\\varphi})$ and $\\lambda>0$ . If we set the step size $\\eta=\\alpha/2$ , then we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\widetilde{\\pmb{\\theta}}_{h}^{k}-\\pmb{\\theta}_{h}^{*}\\right\\|_{\\mathbf{B}_{k,h}}^{2}\\le\\alpha\\displaystyle\\sum_{i=1}^{k}\\left(\\ell_{i,h}(\\pmb{\\theta}_{h}^{*})-\\ell_{i,h}(\\widetilde{\\pmb{\\theta}}_{h}^{i+1})\\right)+\\lambda L_{\\pmb{\\theta}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,3\\sqrt{2}L_{\\varphi}^{3}\\alpha\\displaystyle\\sum_{i=1}^{k}\\left\\|\\widetilde{\\pmb{\\theta}}_{h}^{i+1}-\\widetilde{\\pmb{\\theta}}_{h}^{i}\\right\\|_{2}^{2}-\\displaystyle\\sum_{i=1}^{k}\\left\\|\\widetilde{\\pmb{\\theta}}_{h}^{i+1}-\\widetilde{\\pmb{\\theta}}_{h}^{i}\\right\\|_{\\mathbf{B}_{i,h}}^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Now, we bound the first term of (51). To simplify the presentation, for all $(k,h)\\in[K]\\times[H]$ , we define the softmax function $\\pmb{\\sigma}_{k,h}:\\mathbb{R}^{|S_{k,h}|}\\rightarrow[0,1]^{|S_{k,h}|}$ as follows: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{[\\pmb{\\sigma}_{k,h}(\\mathbf{z})]_{s^{\\prime}}=\\frac{\\exp([\\mathbf{z}]_{s^{\\prime}})}{\\sum_{s^{\\prime\\prime}\\in{\\cal S}_{k,h}}\\exp([\\mathbf{z}]_{s^{\\prime\\prime}})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $[\\cdot]_{s^{\\prime}}$ denote the element corresponding to $s^{\\prime}\\,\\in\\,\\mathcal{S}$ of the input vector. We also define the pseudo-inverse of the softmax function $\\sigma_{k,h}$ via $[\\pmb{\\sigma}_{k,h}^{+}(\\mathbf{p})]_{s^{\\prime}}=\\log([\\mathbf{p}]_{s^{\\prime}})$ which has the property that for all $\\mathbf{p}\\in\\Delta_{|S_{k,h}|}$ , we have $\\pmb{\\sigma}_{k,h}(\\pmb{\\sigma}_{k,h}^{+}(\\mathbf{p}))=\\mathbf{p}$ and $\\begin{array}{r}{\\sum_{s\\in S_{k,h}}\\exp\\left([\\pmb{\\sigma}_{k,h}^{+}(\\mathbf{p})]_{s}\\right)=1.}\\end{array}$ ", "page_idx": 41}, {"type": "text", "text": "We denote $\\Phi_{k,h}~=~[\\varphi_{k,h,s^{\\prime}}]_{s^{\\prime}\\in S_{k,h}}~\\in~\\mathbb{R}^{d\\times|S_{k,h}|}$ for simplicity. Then, the transition model can also be written as $\\begin{array}{l l l l}{P_{\\pmb\\theta}(s^{\\prime}}&{|}&{s_{h}^{k},a_{h}^{k})}&{=}&{[\\pmb\\sigma_{k,h}(\\pmb\\Phi_{k,h}^{\\top}\\pmb\\theta_{h}^{*})]_{s^{\\prime}}}\\end{array}$ . We further define $\\widetilde{\\mathbf{z}}_{i,h}\\;\\;=\\;\\;$ $\\pmb{\\sigma}_{i,h}^{+}\\left(\\mathbb{E}_{\\pmb{\\theta}\\sim\\mathcal{N}\\left(\\widetilde{\\pmb{\\theta}}_{h}^{i},c\\mathbf{B}_{i,h}^{-1}\\right)}[\\pmb{\\sigma}_{i,h}(\\Phi_{i,h}^{\\top}\\pmb{\\theta})]\\right)$ for our analysis. Then, we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{k}\\left(\\ell_{i,h}(\\pmb{\\theta}_{h}^{*})-\\ell_{i,h}(\\widetilde{\\pmb{\\theta}}_{h}^{i+1})\\right)=\\sum_{i=1}^{k}\\left(\\ell_{i,h}(\\pmb{\\theta}_{h}^{*})-\\ell(\\widetilde{\\mathbf{z}}_{i,h},y_{h}^{i})\\right)+\\sum_{i=1}^{k}\\left(\\ell(\\widetilde{\\mathbf{z}}_{i,h},y_{h}^{i})-\\ell_{i,h}(\\widetilde{\\pmb{\\theta}}_{h}^{i+1})\\right).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We can bound the first term of (52) by the following lemma. ", "page_idx": 41}, {"type": "text", "text": "Lemma 14. Let $\\delta\\in(0,1]$ . Then, for all $(k,h)\\in[K]\\times[H],$ , with probability at least $1-\\delta$ , we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{k}\\big(\\ell_{i,h}(\\pmb{\\theta}_{h}^{*})-\\ell(\\widetilde{\\mathbf{z}}_{i,h},y_{h}^{i})\\big)\\leq\\Gamma_{k}^{A}(\\delta),\n$$", "text_format": "latex", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Gamma_{k}^{A}(\\delta)=\\frac{5}{4}(3\\log(\\mathcal{U}k)+L_{\\varphi}L_{\\theta})\\lambda+4(3\\log(\\mathcal{U}k)+L_{\\varphi}L_{\\theta})\\log\\left(\\frac{H\\sqrt{1+2k}}{\\delta}\\right)+2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Furthermore, we can bound the second term of (52) by the following lemma. ", "page_idx": 42}, {"type": "text", "text": "Lemma 15. Let $\\lambda\\geq72L_{\\varphi}^{2}c d$ . Then, for any $c>0$ and all $(k,h)\\in[K]\\times[H],$ , we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{k}\\Big(\\ell(\\widetilde{\\mathbf{z}}_{i,h},y_{h}^{i})-\\ell_{i,h}(\\widetilde{\\pmb{\\theta}}_{h}^{i+1})\\Big)\\leq\\frac{1}{2c}\\sum_{i=1}^{k}\\Big\\|\\widetilde{\\pmb{\\theta}}_{h}^{i+1}-\\widetilde{\\pmb{\\theta}}_{h}^{i}\\Big\\|_{\\mathbf{B}_{i,h}}^{2}+\\Gamma_{k}^{B}(\\delta).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $\\begin{array}{r}{\\Gamma_{k}^{B}(\\delta)=\\sqrt{6}c d\\log\\left(1+\\frac{2k L_{\\varphi}^{2}}{d\\lambda}\\right)}\\end{array}$ ", "page_idx": 42}, {"type": "text", "text": "Combining L\u221aemma 13, Lemma 14, and Lemma 15, and by setting $\\eta\\;=\\;\\alpha/2,c\\;=\\;2\\alpha/3$ and $\\lambda\\geq\\operatorname*{max}\\{12\\sqrt{2}L_{\\varphi}^{3}\\alpha,48L_{\\varphi}^{2}d\\alpha\\}$ , we derive that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left\\|\\widetilde{\\theta}_{h}^{k+1}-\\theta_{h}^{*}\\right\\|_{\\mathrm{B}_{k,h}}^{2}}\\\\ &{\\displaystyle\\leq\\alpha\\Gamma_{k}^{A}(\\delta)+\\alpha\\Gamma_{k}^{B}(\\delta)+\\lambda L_{\\theta}^{2}+3\\sqrt{2}L_{\\varphi}^{3}\\alpha\\sum_{i=1}^{k}\\left\\|\\widetilde{\\theta}_{h}^{i+1}-\\widetilde{\\theta}_{h}^{i}\\right\\|_{2}^{2}+\\left(\\frac{\\alpha}{2c}-1\\right)\\sum_{i=1}^{k}\\left\\|\\widetilde{\\theta}_{h}^{i+1}-\\widetilde{\\theta}_{h}^{i}\\right\\|_{\\mathrm{B}_{i,h}}^{2}}\\\\ &{\\displaystyle\\leq\\alpha\\Gamma_{k}^{A}(\\delta)+\\alpha\\Gamma_{k}^{B}(\\delta)+\\lambda L_{\\theta}^{2}}\\\\ &{\\displaystyle\\leq C\\log{\\mathcal{U}\\left(\\lambda\\log(\\mathcal{U}k)+\\log(\\mathcal{U}k)\\log\\left(\\frac{H\\sqrt{1+2k}}{\\delta}\\right)+d\\log\\left(1+\\frac{k}{d\\lambda}\\right)\\right)}+\\lambda L_{\\theta}^{2}}\\\\ &{\\displaystyle\\qquad\\triangleq\\frac{-\\alpha\\cdot2}{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $C>0$ is an absolute constant. In the above, we choose $\\lambda={\\cal O}(d\\log{\\mathcal{U}})$ , $\\alpha=\\mathcal{O}(\\log\\mathcal{U})$ . The second inequality of (53) is derived from the fact that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{3\\sqrt{2}L_{\\varphi}^{3}\\alpha\\displaystyle\\sum_{i=1}^{k}\\left\\|\\widetilde{\\theta}_{h}^{i+1}-\\widetilde{\\theta}_{h}^{i}\\right\\|_{2}^{2}+\\left(\\frac{\\alpha}{2c}-1\\right)\\displaystyle\\sum_{i=1}^{k}\\left\\|\\widetilde{\\theta}_{h}^{i+1}-\\widetilde{\\theta}_{h}^{i}\\right\\|_{\\mathbf{B}_{i,h}}^{2}}\\\\ &{\\ =3\\sqrt{2}L_{\\varphi}^{3}\\alpha\\displaystyle\\sum_{i=1}^{k}\\left\\|\\widetilde{\\theta}_{h}^{i+1}-\\widetilde{\\theta}_{h}^{i}\\right\\|_{2}^{2}-\\frac{1}{4}\\displaystyle\\sum_{i=1}^{k}\\left\\|\\widetilde{\\theta}_{h}^{i+1}-\\widetilde{\\theta}_{h}^{i}\\right\\|_{\\mathbf{B}_{i,h}}^{2}}\\\\ &{\\ \\ \\leq3\\sqrt{2}L_{\\varphi}^{3}\\alpha\\displaystyle\\sum_{i=1}^{k}\\left\\|\\widetilde{\\theta}_{h}^{i+1}-\\widetilde{\\theta}_{h}^{i}\\right\\|_{2}^{2}-\\frac{\\lambda}{4}\\displaystyle\\sum_{i=1}^{k}\\left\\|\\widetilde{\\theta}_{h}^{i+1}-\\widetilde{\\theta}_{h}^{i}\\right\\|_{2}^{2}}\\\\ &{\\ \\ \\leq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "The first inequality holds from $\\mathbf{B}_{i,h}\\succeq\\lambda\\mathbf{I}_{d}$ , and the second inequality is obvious from our setting of $\\lambda$ . Therefore, we can conclude that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\widetilde{\\pmb{\\theta}}_{h}^{k}-\\pmb{\\theta}_{h}^{*}\\right\\|_{\\mathbf{B}_{k,h}}\\le\\beta_{k}(\\delta)=\\mathcal{O}(\\sqrt{d}\\log\\mathcal{U}\\log(k H))\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "In the following section, we provide the proofs of the lemmas used in Lemma 12. ", "page_idx": 42}, {"type": "text", "text": "D.1.1 Proof of Lemma 13 ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Proof of Lemma 13. Let \u2113 i,h(\u03b8) = \u2113i,h(\u03b8 ih) + \u2207\u2113i,h(\u03b8 ih)\u22a4 \u03b8 \u2212\u03b8 ih + 12   \u03b8 \u2212\u03b8 ih   2\u22072\u2113i,h(\u03b8i) a second-order Taylor expansion of $\\ell_{i,h}(\\pmb\\theta)$ at $\\widetilde{\\pmb{\\theta}}_{h}^{i}$ . Since we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\widetilde{\\theta}_{h}^{k+1}=\\operatorname*{argmin}_{\\theta\\in B_{d}(L_{\\theta})}\\frac{1}{2\\eta}\\left\\Vert\\theta-\\widetilde{\\theta}_{h}^{k}\\right\\Vert_{\\widetilde{\\mathbf{B}}_{k,h}}^{2}+\\nabla\\ell_{k,h}(\\widetilde{\\theta}_{h}^{k})^{\\top}\\theta=\\operatorname*{argmin}_{\\theta\\in B(\\mathbf{0}_{d},L_{\\theta})}\\widetilde{\\ell}_{k,h}(\\theta)+\\frac{1}{2\\eta}\\left\\Vert\\theta-\\widetilde{\\theta}_{h}^{k}\\right\\Vert_{\\mathbf{B}_{k,h}}^{2},\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "by Lemma 31, if we define $\\begin{array}{r}{\\psi(\\pmb{\\theta})=\\frac{1}{2}\\|\\pmb{\\theta}\\|_{\\mathbf{B}_{i,h}}^{2}}\\end{array}$ we obtain ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\nabla\\widetilde{\\ell}_{i,h}(\\widetilde{\\boldsymbol{\\theta}}_{h}^{i+1})^{\\top}(\\widetilde{\\boldsymbol{\\theta}}_{h}^{i+1}-\\boldsymbol{\\theta}_{h}^{*})\\le\\frac{1}{2\\eta}\\left(\\left\\Vert\\widetilde{\\boldsymbol{\\theta}}_{h}^{i}-\\boldsymbol{\\theta}_{h}^{*}\\right\\Vert_{\\mathbf{B}_{i,h}}^{2}-\\left\\Vert\\widetilde{\\boldsymbol{\\theta}}_{h}^{i+1}-\\boldsymbol{\\theta}_{h}^{*}\\right\\Vert_{\\mathbf{B}_{i,h}}^{2}-\\left\\Vert\\widetilde{\\boldsymbol{\\theta}}_{h}^{i+1}-\\widetilde{\\boldsymbol{\\theta}}_{h}^{i}\\right\\Vert_{\\mathbf{B}_{i,h}}\\right).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "By applying Lemma 33, we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\ell_{i,h}(\\widetilde{\\pmb{\\theta}}_{h}^{i+1})-\\ell_{i,h}(\\pmb{\\theta}_{h}^{*})\\leq\\left\\langle\\nabla\\ell_{i,h}(\\widetilde{\\pmb{\\theta}}_{h}^{i+1}),\\widetilde{\\pmb{\\theta}}_{h}^{i+1}-\\pmb{\\theta}_{h}^{*}\\right\\rangle-\\frac{1}{\\alpha_{i,h}}\\left\\|\\widetilde{\\pmb{\\theta}}_{h}^{i+1}-\\pmb{\\theta}_{h}^{*}\\right\\|_{\\nabla^{2}\\ell_{i,h}(\\widetilde{\\pmb{\\theta}}_{h}^{i+1})},\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where $\\alpha_{i,h}=\\log\\vert{\\cal S}_{i,h}\\vert+2(1+L_{\\varphi}L_{\\theta})$ . ", "page_idx": 43}, {"type": "text", "text": "By setting $\\eta=\\alpha_{i,h}/2$ and merging equations (54) and (55), we arrive at ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell_{i,h}(\\widetilde{\\pmb{\\theta}}_{h}^{i+1})-\\ell_{i,h}(\\pmb{\\theta}_{h}^{*})\\leq\\left\\langle\\nabla\\ell_{i,h}(\\widetilde{\\pmb{\\theta}}_{h}^{i+1})-\\nabla\\widetilde{\\ell}_{i,h}(\\widetilde{\\pmb{\\theta}}_{h}^{i+1}),\\widetilde{\\pmb{\\theta}}_{h}^{i+1}-\\pmb{\\theta}_{h}^{*}\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left.\\frac{1}{\\alpha_{i,h}}\\left(\\left\\|\\widetilde{\\pmb{\\theta}}_{h}^{i}-\\pmb{\\theta}_{h}^{*}\\right\\|_{\\mathbf{B}_{i,h}}^{2}-\\left\\|\\widetilde{\\pmb{\\theta}}_{h}^{i+1}-\\pmb{\\theta}_{h}^{*}\\right\\|_{\\mathbf{B}_{i+1,h}}^{2}-\\left\\|\\widetilde{\\pmb{\\theta}}_{h}^{i+1}-\\widetilde{\\pmb{\\theta}}_{h}^{i}\\right\\|_{\\mathbf{B}_{i,h}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Meanwhile, we obtain ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\nabla\\widetilde{\\ell}_{i,h}(\\pmb{\\theta})=\\nabla\\ell_{i,h}(\\widetilde{\\pmb{\\theta}}_{h}^{i})+\\nabla^{2}\\ell_{i,h}(\\widetilde{\\pmb{\\theta}}_{h}^{i})(\\pmb{\\theta}-\\widetilde{\\pmb{\\theta}}_{h}^{i})\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "by taking the gradient over both sides of the Taylor approximation of $\\ell_{i,h}(\\pmb\\theta)$ . Using (57), we proceed to bound the first term of (56) as follows: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle\\nabla\\ell_{i,h}(\\tilde{\\theta}_{h}^{i+1})-\\nabla\\tilde{\\ell}_{i,h}(\\tilde{\\theta}_{h}^{i+1}),\\tilde{\\theta}_{h}^{i+1}-\\theta_{h}^{*}\\right\\rangle}\\\\ &{\\quad=\\left\\langle\\nabla\\ell_{i,h}(\\tilde{\\theta}_{h}^{i+1})-\\nabla\\ell_{i,h}(\\tilde{\\theta}_{h}^{i})-\\nabla^{2}\\ell_{i,h}(\\tilde{\\theta}_{h}^{i})(\\tilde{\\theta}_{h}^{i+1}-\\tilde{\\theta}_{h}^{i}),\\tilde{\\theta}_{h}^{i+1}-\\theta^{*}\\right\\rangle}\\\\ &{\\quad=\\left\\langle D^{3}\\ell_{i,h}(\\tilde{\\theta}_{h}^{i+1})\\left[\\tilde{\\theta}_{h}^{i+1}-\\tilde{\\theta}_{h}^{i}\\right]\\left(\\tilde{\\theta}_{h}^{i+1}-\\tilde{\\theta}_{h}^{i}\\right),\\tilde{\\theta}_{h}^{i+1}-\\theta^{*}\\right\\rangle}\\\\ &{\\quad\\leq3\\sqrt{2}L_{\\varphi}\\left\\lVert\\tilde{\\theta}_{h}^{i+1}-\\theta_{h}^{*}\\right\\rVert_{2}\\left\\lVert\\tilde{\\theta}_{h}^{i+1}-\\tilde{\\theta}_{h}^{i}\\right\\rVert_{\\nabla^{2}\\ell_{i,h}(\\tilde{\\theta}_{h}^{i+1})}^{2}}\\\\ &{\\quad\\leq3\\sqrt{2}L_{\\varphi}\\left\\lVert\\tilde{\\theta}_{h}^{i+1}-\\tilde{\\theta}_{h}^{i}\\right\\rVert_{\\nabla^{2}\\ell_{i,h}(\\tilde{\\theta}_{h}^{i+1})}^{2}}\\\\ &{\\quad\\leq3\\sqrt{2}L_{\\varphi}\\left\\lVert\\tilde{\\theta}_{h}^{i+1}-\\tilde{\\theta}_{h}^{i}\\right\\rVert_{\\nabla^{2}\\ell_{i,h}(\\tilde{\\theta}_{h}^{i+1})}^{2}}\\\\ &{\\quad\\leq3\\sqrt{2}L_{\\varphi}^{3}\\left\\lVert\\tilde{\\theta}_{h}^{i+1}-\\tilde{\\theta}_{h}^{i}\\right\\rVert_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where $\\bar{\\pmb{\\theta}}_{h}^{i+1}$ is a convex combination of $\\widetilde{\\pmb{\\theta}}_{h}^{i}$ and $\\widetilde{\\pmb{\\theta}}_{h}^{i+1}$ . The second equality arises from the Taylor expansion, the first inequality is due to the self-concordant property, and the final inequality is justified by the following: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla^{2}\\ell_{i,h}(\\bar{\\theta}_{h}^{i+1})}\\\\ &{=\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}_{i,h}}P_{\\bar{\\theta}_{h}^{i+1}}(s^{\\prime}\\mid s_{h}^{i},a_{h}^{i})\\varphi_{i,h,s^{\\prime}}\\varphi_{i,h,s^{\\prime}}^{\\top}}\\\\ &{\\quad-\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}_{i,h}}\\sum_{s^{\\prime\\prime}\\in\\mathcal{S}_{i,h}}P_{\\bar{\\theta}_{h}^{i+1}}(s^{\\prime}\\mid s_{h}^{i},a_{h}^{i})P_{\\bar{\\theta}_{h}^{i+1}}(s^{\\prime\\prime}\\mid s_{h}^{i},a_{h}^{i})\\varphi_{i,h,s^{\\prime}}\\varphi_{i,h,s^{\\prime\\prime}}^{\\top}}\\\\ &{\\preceq\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}_{i,h}}P_{\\bar{\\theta}_{h}^{i+1}}(s^{\\prime}\\mid s_{h}^{i},a_{h}^{i})\\varphi_{i,h,s^{\\prime}}\\varphi_{i,h,s^{\\prime}}^{\\top}}\\\\ &{\\preceq L_{\\sigma}^{\\top}\\mathbf{I}_{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "By summing over $i$ and reorganizing the terms, we arrive at the final result as follows: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\widetilde{\\theta}_{h}^{k+1}-\\theta_{h}^{*}\\right\\|_{\\mathbf{B}_{k+1,h}}^{2}}\\\\ &{\\le\\displaystyle\\sum_{i=1}^{k}\\alpha_{i,h}\\left(\\ell_{i,h}(\\theta_{h}^{*})-\\ell_{i,h}(\\widetilde{\\theta}_{h}^{i+1})\\right)+\\left\\|\\widetilde{\\theta}_{h}^{1}-\\theta_{h}^{*}\\right\\|_{\\mathbf{B}_{1,h}}^{2}}\\\\ &{\\quad+\\displaystyle3\\sqrt{2}L_{\\varphi}^{3}\\displaystyle\\sum_{i=1}^{k}\\alpha_{i,h}\\left\\|\\widetilde{\\theta}_{h}^{i+1}-\\widetilde{\\theta}_{h}^{i}\\right\\|_{2}^{2}-\\displaystyle\\sum_{i=1}^{k}\\left\\|\\widetilde{\\theta}_{h}^{i+1}-\\widetilde{\\theta}_{h}^{i}\\right\\|_{\\mathbf{B}_{i,h}}^{2}}\\\\ &{\\le\\displaystyle\\alpha\\sum_{i=1}^{k}\\left(\\ell_{i,h}(\\theta_{h}^{*})-\\ell_{i,h}(\\widetilde{\\theta}_{h}^{i+1})\\right)+\\lambda L_{\\theta}^{2}+3\\sqrt{2}L_{\\varphi}^{3}\\alpha_{i=1}^{\\sum{k}}\\left\\|\\widetilde{\\theta}_{h}^{i+1}-\\widetilde{\\theta}_{h}^{i}\\right\\|_{2}^{2}-\\displaystyle\\sum_{i=1}^{k}\\left\\|\\widetilde{\\theta}_{h}^{i+1}-\\widetilde{\\theta}_{h}^{i}\\right\\|_{\\mathbf{B}_{i,h}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where the first inequality holds by Assumption 2 and the last inequality holds since $\\alpha=\\log\\mathcal{U}+$ $2(1+L_{\\varphi}L_{\\theta})\\geq\\alpha_{i,h}^{-}$ for all $i\\in[k]$ . ", "page_idx": 44}, {"type": "text", "text": "D.1.2 Proof of Lemma 14 ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Proof of Lemma $^{l4}$ . The norm of $\\widetilde{\\mathbf z}_{i,h}\\;=\\;\\pmb{\\sigma}_{i,h}^{+}\\left(\\mathbb{E}_{\\pmb{\\theta}\\sim\\mathcal{N}\\left(\\widetilde{\\pmb{\\theta}}_{h}^{i},c\\mathbf{B}_{i,h}^{-1}\\right)}[\\pmb{\\sigma}_{i,h}(\\Phi_{i,h}^{\\top}\\pmb{\\theta})]\\right)$ is generally unbounded [27]. In this proof, we utilize the smoothed version of $\\widetilde{\\mathbf{z}}_{i,h}$ , defined as follows: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf{z}}_{i,h}^{u}=\\pmb{\\sigma}_{i,h}^{+}\\left(\\operatorname{smooth}_{i,h}^{u}\\mathbb{E}_{\\pmb{\\theta}\\sim\\mathcal{N}\\left(\\widetilde{\\pmb{\\theta}}_{h}^{i},c\\mathbf{B}_{i,h}^{-1}\\right)}[\\pmb{\\sigma}_{i,h}(\\Phi_{i,h}^{\\top}\\pmb{\\theta})]\\right)\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where the smooth function smo $\\mathrm{)th}_{i,h}^{u}({\\bf p})=(1-u){\\bf p}+(u/\\mathscr{U}){\\bf1}$ with $u\\in[0,1/2]$ , and $\\mathbf{1}\\in\\mathbb{R}^{|S_{i,h}|}$ is an all-one vector. ", "page_idx": 44}, {"type": "text", "text": "Exploiting the property of ${\\pmb\\sigma}_{i,h}^{+}$ such that $\\pmb{\\sigma}_{i,h}(\\pmb{\\sigma}_{i,h}^{+}(\\mathbf{p}))=\\mathbf{p}$ for any $\\mathbf{p}\\in\\Delta_{|S_{i,h}|}$ , it is straightforward to show that $\\widetilde{\\mathbf z}_{i,h}^{u}=\\pmb{\\sigma}_{i,h}^{+}(\\mathrm{smooth}_{i,h}^{u}(\\pmb{\\sigma}_{i,h}(\\widetilde{\\mathbf z}_{i,h})))$ . Then, by Lemma 34, we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{k}\\ell(\\widetilde{\\mathbf z}_{i,h}^{u},y_{h}^{i})-\\sum_{i=1}^{k}\\ell(\\widetilde{\\mathbf z}_{i,h},y_{h}^{i})\\leq2u k,\\quad\\mathrm{and}\\quad\\|\\widetilde{\\mathbf z}_{i,h}^{u}\\|_{\\infty}\\leq\\log(\\mathcal{U}/u).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Given the definition of $\\ell_{i,h}$ , we know that $\\ell(\\mathbf{z}_{i,h}^{*},y_{h}^{i})=\\ell_{i,h}(\\pmb{\\theta}_{h}^{*})$ , where $\\mathbf{z}_{i,h}^{*}=\\Phi_{i,h}^{\\top}\\pmb{\\theta}_{h}^{*}$ . We can bound the gap between the loss of $\\pmb{\\theta}_{h}^{*}$ and $\\widetilde{\\mathbf z}_{i,h}^{u}$ as follows: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{i=1}^{k}\\left(\\ell_{i,h}(\\theta_{h}^{*})-\\ell(\\widetilde{\\mathbf{z}}_{i,h}^{u},y_{h}^{i})\\right)}\\\\ {\\displaystyle=\\sum_{i=1}^{k}\\left(\\ell(\\mathbf{z}_{i,h}^{*},y_{h}^{i})-\\ell(\\widetilde{\\mathbf{z}}_{i,h}^{u},y_{h}^{i})\\right)}\\\\ {\\displaystyle\\leq\\sum_{i=1}^{k}\\langle\\nabla_{z}\\ell(\\mathbf{z}_{i,h}^{*},y_{h}^{i}),\\mathbf{z}_{i,h}^{*}-\\widetilde{\\mathbf{z}}_{i,h}^{u}\\rangle-\\displaystyle\\sum_{i=1}^{k}\\frac{1}{M_{i,h}}\\|\\mathbf{z}_{i,h}^{*}-\\widetilde{\\mathbf{z}}_{i,h}^{u}\\|_{\\nabla_{z}^{2}\\ell(\\mathbf{z}_{i,h}^{*},y_{h}^{i})}^{2}}\\\\ {\\displaystyle=\\sum_{i=1}^{k}\\langle\\nabla_{z}\\ell(\\mathbf{z}_{i,h}^{*},y_{h}^{i}),\\mathbf{z}_{i,h}^{*}-\\widetilde{\\mathbf{z}}_{i,h}^{u}\\rangle-\\displaystyle\\sum_{i=1}^{k}\\frac{1}{M_{i,h}}\\|\\mathbf{z}_{i,h}^{*}-\\widetilde{\\mathbf{z}}_{i,h}^{u}\\|_{\\nabla\\sigma_{i,h}(\\mathbf{z}_{i,h}^{*})}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where $M_{i,h}=\\log(|S_{i,h}|)+2\\log(\\mathcal{U}/u)$ , and the second equality holds by a direct calculation of the first order and Hessian of the logistic loss. ", "page_idx": 44}, {"type": "text", "text": "Now, we first bound the first term of the right-hand side. Let $\\mathbf{d}_{i,h}=(\\mathbf{z}_{i,h}^{*}-\\widetilde{\\mathbf{z}}_{i,h}^{u})/(M+L_{\\varphi}L_{\\theta})$ , where $M\\;=\\;\\log\\mathcal{U}\\,+\\,2\\log(\\mathcal{U}/u)$ . Then, one can check that $\\|\\mathbf{d}_{i,h}\\|_{\\infty}\\,\\leq\\,1$ since $\\|\\mathbf{z}_{i,h}^{*}\\|_{\\infty}\\ \\leq$ $\\begin{array}{r}{\\operatorname*{max}_{s^{\\prime}\\in S_{i,h}}\\|\\varphi_{i,h,s^{\\prime}}\\|_{2}\\|\\theta_{h}^{*}\\|_{2}\\,\\le\\,L_{\\varphi}L_{\\theta}}\\end{array}$ and $\\|\\widetilde{\\mathbf z}_{i,h}^{u}\\|_{\\infty}\\,\\leq\\,\\log(\\mathcal{U}/u)$ . Moreover, since ${\\bf z}_{i,h}^{*}$ and $\\widetilde{\\mathbf z}_{i,h}^{u}$ are independent of $y_{h}^{i}$ , $\\mathbf{d}_{i,h}$ is $\\mathcal{F}_{i,h}$ -measurable. Since $\\mathbb{E}[(\\pmb{\\sigma}_{i,h}(\\mathbf{z}_{i,h}^{*})-y_{h}^{i})(\\pmb{\\sigma}_{i,h}(\\mathbf{z}_{i,h}^{*})-y_{h}^{i})^{\\top}\\mid$ $\\mathcal{F}_{i,h}\\big]=\\nabla\\pmb{\\sigma}_{i,h}\\big(\\mathbf{z}_{i,h}^{*}\\big)$ and $\\|\\pmb{\\sigma}_{i,h}(\\mathbf{z}_{i,h}^{*})-y_{h}^{i}\\|_{1}\\leq2$ , we can apply Lemma 32. For any $k$ and $\\delta\\in(0,1]$ , ", "page_idx": 44}, {"type": "text", "text": "with probability at least $1-\\delta/H$ , we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{k}\\biggl\\langle\\nabla_{z}\\ell\\left(z_{i,t}^{*},y_{i}^{*}\\right),z_{i,t}^{*}-\\bar{z}_{i,t}^{*}\\biggr\\rangle}\\\\ &{=\\displaystyle(M+L_{e}L_{e}L_{g})\\displaystyle\\sum_{i=1}^{k}\\biggl\\langle\\nabla_{z}\\ell\\left(z_{i,t}^{*},y_{i}^{*}\\right),\\mathbf{d}_{i,t}\\biggr\\rangle}\\\\ &{\\leq\\displaystyle(M+L_{e}L_{e}L_{g})\\sqrt{\\lambda+\\sum_{i=1}^{k}\\|\\mathbf{d}_{i,t}\\|_{\\nabla\\sigma_{i,t}(\\kappa_{i}^{*})}}}\\\\ &{\\displaystyle\\qquad\\cdot\\sqrt{\\frac{\\sqrt{\\lambda}}{4}+\\frac{4}{\\sqrt{\\lambda}}\\log\\left(\\frac{H\\sqrt{1+\\frac{1}{\\lambda}\\sum_{i=1}^{k}\\|\\mathbf{d}_{i,t}\\|_{\\nabla\\sigma_{i,t}(\\kappa_{i}^{*})}}}{\\delta}\\right)}}\\\\ &{\\leq\\displaystyle(M+L_{e}L_{e}L_{g})\\sqrt{\\lambda+\\sum_{i=1}^{k}\\|\\mathbf{d}_{i,t}\\|_{\\nabla\\sigma_{i,t}(\\kappa_{i}^{*})}}\\sqrt{\\frac{\\sqrt{\\lambda}}{4}+4\\log\\left(\\frac{H}{\\lambda}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where the second inequality holds since $\\|\\mathbf{d}_{i,h}\\|_{\\nabla\\sigma_{i,h}(\\mathbf{z}_{i,h}^{*})}^{2}=\\mathbf{d}_{i,h}^{\\top}\\nabla\\pmb{\\sigma}_{i,h}(\\mathbf{z}_{i,h}^{*})\\mathbf{d}_{i,h}\\le2$ and $\\lambda\\geq1$ Plugging (60) into (59) and rearranging the term, we get ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\int_{\\Omega}}{\\int_{\\Omega}}(\\tau_{1}\\omega^{\\prime}-\\tau(x_{2},x_{3}))\\,d x}\\\\ &{\\leq(M+L_{p}L_{p})\\Biggl\\{\\lambda+\\frac{\\int_{\\Omega}^{1}\\Big|d x_{4}\\Big|\\big|Q_{p,x_{4},\\epsilon(x,y_{3})}\\sqrt{\\frac{\\sqrt{\\Delta}}{4}}+\\mathrm{i}\\log\\Big(H\\frac{\\sqrt{1+2\\delta}}{\\delta}\\Big)\\Big|}{\\beta}}\\\\ &{\\quad-\\frac{\\sum}{\\delta+1}\\frac{1}{L_{p}(x_{2})}\\|u_{x}\\|_{L^{2}(\\Omega_{p,x_{4},\\epsilon(x,y_{3})})}}\\\\ &{\\leq(M+L_{p,L_{p}}L_{p})\\Biggr\\}\\lambda+\\frac{\\sum}{\\int_{\\Omega}^{1}\\Big|d x_{4}\\|\\big|Q_{p,x_{4},\\epsilon(x,y_{3})}\\sqrt{\\frac{\\sqrt{\\Delta}}{4}}+\\mathrm{i}\\log\\Big(\\frac{H\\sqrt{1+2\\delta}}{\\delta}\\Big)}}\\\\ &{\\quad-(M+L_{p,L_{p}}L_{p})\\underbrace{\\sum_{i=1}^{N}\\|d x_{4}\\|_{L^{2}(\\Omega_{p,x_{4},\\epsilon(x,y_{3})})}}_{=1}}\\\\ &{\\leq(M+L_{p,L_{p}}L_{p})\\geq\\bigg\\{(\\lambda+\\sum_{i=1}^{N}\\|d x_{4}\\|_{L^{2}(\\Omega_{p,x_{4},\\epsilon(x,y_{3})})}+(M+L_{p,L_{p}}L_{p})\\left(\\frac{\\sqrt{\\Delta}}{4}+\\mathrm{i}\\log\\Big(\\frac{H\\sqrt{1+2\\delta}}{\\delta}\\Big)\\right)\\bigg\\}}\\\\ &{\\quad-(M+L_{p}L_{p})\\sum_{i=1}^{N}\\|d x_{4}\\|_{L^{2}(\\Omega_{p,x_{4},\\epsilon(x,y_{3})})}}\\\\ &{\\leq\\frac{5}{6}(M+L_{p,L_{p}}L_{p})+(M+L_{p,L_{p}}L_{p})\\ln\\Big(\\frac{(M+T)^{2}(\\Omega_{p,x})}{\\delta}\\Big)\\Biggr\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Finally, combining (58) and (61), by setting $u=1/k$ , we derive that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{i=1}^{k}\\big(\\ell_{i,h}(\\theta_{h}^{*})-\\ell(\\widetilde{\\mathbf{z}}_{i,h},y_{h}^{i})\\big)}\\quad}&{}\\\\ &{\\leq\\frac{5}{4}(M+L_{\\varphi}L_{\\theta})\\lambda+4\\big(M+L_{\\varphi}L_{\\theta}\\big)\\log\\bigg(\\frac{H\\sqrt{1+2k}}{\\delta}\\bigg)+2u k}\\\\ &{\\leq\\frac{5}{4}(3\\log(\\mathcal{U}k)+L_{\\varphi}L_{\\theta})\\lambda+4\\big(3\\log(\\mathcal{U}k)+L_{\\varphi}L_{\\theta}\\big)\\log\\bigg(\\frac{H\\sqrt{1+2k}}{\\delta}\\bigg)+2}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where the last inequality holds by the definition of $M=\\log\\mathcal{U}+2\\log(\\mathcal{U}/u)$ . Taking the union bound over $h\\in[H]$ , we conclude the proof. \u53e3 ", "page_idx": 45}, {"type": "text", "text": "D.1.3 Proof of Lemma 15 ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Proof of Lemma 15. We start the proof from the observation of Proposition 2 in Foster et al. [27], stating that $\\widetilde{\\mathbf{z}}_{i,h}$ represents the mixed prediction, which adheres to the following property: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\langle\\widetilde{\\mathbf z}_{i,h},y_{h}^{i}\\rangle\\leq-\\log\\left(\\mathbb{E}_{\\theta\\sim\\mathcal{N}\\left(\\widetilde{\\theta}_{h}^{i},\\mathbf{c}_{i,h}^{-1}\\right)}\\left[\\exp\\left(-\\ell_{i,h}(\\theta)\\right)\\right]\\right)=-\\log\\left(\\frac{1}{Z_{i,h}}\\int_{\\mathbb{R}^{d}}\\exp\\left(-L_{i,h}(\\theta)\\right)\\mathrm{d}\\theta\\right),\n$$", "text_format": "latex", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L_{i,h}(\\pmb\\theta):=\\ell_{i,h}(\\pmb\\theta)+\\frac{1}{2c}\\left\\|\\pmb\\theta-\\widetilde\\theta_{h}^{i}\\right\\|_{\\mathbf{B}_{i,h}}^{2}\\mathrm{and}\\;Z_{i,h}:=\\sqrt{(2\\pi)^{d}c|\\mathbf{B}_{i,h}^{-1}|}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Consider the quadratic approximation ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\widetilde{L}_{i,h}(\\pmb{\\theta})=L_{i,h}(\\widetilde{\\pmb{\\theta}}_{h}^{i+1})+\\left\\langle\\nabla L_{i,h}(\\widetilde{\\pmb{\\theta}}_{h}^{i+1}),\\pmb{\\theta}-\\widetilde{\\pmb{\\theta}}_{h}^{i+1}\\right\\rangle+\\frac{1}{2c}\\left\\|\\pmb{\\theta}-\\widetilde{\\pmb{\\theta}}_{h}^{i+1}\\right\\|_{\\mathbf{B}_{i,h}}^{2}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Using the property that $\\ell_{i,h}$ is $3\\sqrt{2}L_{\\varphi}$ -self-concordant-like function as asserted by Proposition B.1 in [50], and applying Lemma 35, we obtain ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{i,h}(\\pmb\\theta)\\leq\\widetilde{L}_{i,h}(\\pmb\\theta)+\\exp\\left(18L_{\\varphi}^{2}\\left\\|\\pmb\\theta-\\widetilde{\\pmb\\theta}_{h}^{i+1}\\right\\|_{2}^{2}\\right)\\left\\|\\pmb\\theta-\\widetilde{\\pmb\\theta}_{h}^{i+1}\\right\\|_{\\nabla\\ell_{i,h}(\\widetilde{\\pmb\\theta}_{h}^{i+1})}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Also, we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{Z_{i,h}}\\int_{\\mathbb{R}^{d}}\\exp(-L_{i,h}(\\pmb\\theta))\\,\\mathrm{d}\\theta}\\\\ &{\\displaystyle\\geq\\frac{1}{Z_{i,h}}\\int_{\\mathbb{R}^{d}}\\exp\\left(-\\widetilde{L}_{i,h}(\\pmb\\theta)-\\exp\\left(18L_{\\varphi}^{2}\\left\\|\\pmb\\theta-\\widetilde{\\theta}_{h}^{i+1}\\right\\|_{2}^{2}\\right)\\left\\|\\pmb\\theta-\\widetilde{\\theta}_{h}^{i+1}\\right\\|_{\\nabla\\ell_{i,h}(\\widetilde{\\theta}_{h}^{i+1})}^{2}\\right)\\mathrm{d}\\pmb\\theta}\\\\ &{\\displaystyle=\\frac{\\exp\\left(-L_{i,h}(\\widetilde{\\theta}_{h}^{i+1})\\right)}{Z_{i,h}}\\int_{\\mathbb{R}^{d}}\\widetilde{f}_{i+1,h}(\\pmb\\theta)\\cdot\\exp\\left(-\\left\\langle\\nabla L_{i,h}(\\widetilde{\\theta}_{h}^{i+1}),\\pmb\\theta-\\widetilde{\\theta}_{h}^{i+1}\\right\\rangle\\right)\\mathrm{d}\\pmb\\theta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where we define the function $\\widetilde{f}_{i,h}:B(\\mathbf{0}_{d},1)\\rightarrow\\mathbb{R}$ as ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\widetilde{f}_{i+1,h}(\\pmb\\theta)=\\exp\\left(-\\frac{1}{2c}\\left\\|\\pmb\\theta-\\widetilde{\\theta}_{h}^{i+1}\\right\\|_{\\mathbf{B}_{i,h}}^{2}-\\exp\\left(18L_{\\varphi}^{2}\\left\\|\\pmb\\theta-\\widetilde{\\theta}_{h}^{i+1}\\right\\|_{2}^{2}\\right)\\left\\|\\pmb\\theta-\\widetilde{\\theta}_{h}^{i+1}\\right\\|_{\\nabla^{2}\\ell_{i,h}(\\widetilde{\\theta}_{h}^{i+1})}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "We denote $\\begin{array}{r}{\\widetilde{Z}_{i+1,h}=\\int_{\\mathbb{R}^{d}}\\widetilde{f}_{i+1,h}(\\pmb{\\theta})\\,\\mathrm{d}\\pmb{\\theta}\\leq+\\infty}\\end{array}$ and define $\\widetilde{\\Theta}_{i+1,h}$ as the distribution whose density function is $\\widetilde{f}_{i+1,h}(\\pmb{\\theta})/\\widetilde{Z}_{i+1,h}$ . Then, we can rewrite (63) as follows: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{Z_{i,h}}\\displaystyle\\int_{\\mathbb{R}^{d}}\\exp(-L_{i,h}(\\theta))\\,{\\mathrm{d}}\\theta}\\\\ &{\\geq\\frac{\\exp\\left(-L_{i,h}(\\widetilde{\\theta}_{h}^{i+1})\\right)\\widetilde{Z}_{i+1,h}}{Z_{i,h}}\\mathbb{E}_{\\theta\\sim\\widetilde{\\Theta}_{i+1,h}}\\left[\\exp\\left(-\\left\\langle\\nabla L_{i,h}(\\widetilde{\\theta}_{h}^{i+1}),\\theta-\\widetilde{\\theta}_{h}^{i+1}\\right\\rangle\\right)\\right]}\\\\ &{\\ge\\frac{\\exp\\left(-L_{i,h}(\\widetilde{\\theta}_{h}^{i+1})\\right)\\widetilde{Z}_{i+1,h}}{Z_{i,h}}\\exp\\left(-\\mathbb{E}_{\\theta\\sim\\widetilde{\\Theta}_{i+1,h}}\\left[\\left\\langle\\nabla L_{i,h}(\\widetilde{\\theta}_{h}^{i+1}),\\theta-\\widetilde{\\theta}_{h}^{i+1}\\right\\rangle\\right]\\right)}\\\\ &{=\\frac{\\exp\\left(-L_{i,h}(\\widetilde{\\theta}_{h}^{i+1})\\right)\\widetilde{Z}_{i+1,h}}{Z_{i,h}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where the second inequality is by Jensen\u2019s inequality and the last inequality holds because $\\widetilde{\\Theta}_{i+1,h}$ is symmetric around $\\widetilde{\\theta}_{h}^{i+1}$ ih+1and thus E\u03b8\u223c\u0398 $\\mathbb{E}_{\\pmb{\\theta}\\sim\\widetilde{\\Theta}_{i+1,h}}\\left[\\left\\langle\\nabla L_{i,h}(\\widetilde{\\pmb{\\theta}}_{h}^{i+1}),\\pmb{\\theta}-\\widetilde{\\pmb{\\theta}}_{h}^{i+1}\\right\\rangle\\right]=0.$ ", "page_idx": 46}, {"type": "text", "text": "Combining (62) and (64), we get ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\ell_{i,h}(\\widetilde{\\mathbf{z}})\\leq L_{i,h}(\\widetilde{\\pmb{\\theta}}_{h}^{i+1})+\\log Z_{i,h}-\\log\\widetilde{Z}_{i+1,h}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Moreover, we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\log\\tilde{Z}_{i+1,h}}\\\\ &{=-\\log\\left(\\int_{\\mathbb{R}^{d}}\\exp\\left(-\\frac{1}{2c}\\left\\|\\theta-\\widetilde{\\theta}_{h}^{i+1}\\right\\|_{\\mathrm{B}_{i,h}}^{2}\\right.\\right.}\\\\ &{\\qquad\\qquad\\left.\\left.-\\exp\\left(18L_{\\varphi}^{2}\\left\\|\\theta-\\widetilde{\\theta}_{h}^{i+1}\\right\\|_{\\mathrm{Z}}^{2}\\right)\\left\\|\\theta-\\widetilde{\\theta}_{h}^{i+1}\\right\\|_{\\mathrm{V}^{2}\\ell_{i,h}(\\widetilde{\\theta}_{h}^{i+1})}^{2}\\right)\\mathrm{d}\\theta\\right)}\\\\ &{=-\\log\\left(\\widehat{Z}_{i+1,h}\\cdot\\mathbb{E}_{\\theta\\sim\\widehat{\\Theta}_{i+1,h}}\\left[\\exp\\left(-\\exp\\left(18L_{\\varphi}^{2}\\left\\|\\theta-\\widetilde{\\theta}_{h}^{i+1}\\right\\|_{\\mathrm{Z}}^{2}\\right)\\left\\|\\theta-\\widetilde{\\theta}_{h}^{i+1}\\right\\|_{\\mathrm{V}^{2}\\ell_{i,h}(\\widetilde{\\theta}_{h}^{i+1})}^{2}\\right)\\right]\\right)}\\\\ &{\\le-\\log\\widehat{Z}_{i+1,h}+\\mathbb{E}_{\\theta\\sim\\widehat{\\Theta}_{i+1,h}}\\left[\\exp\\left(18L_{\\varphi}^{2}\\left\\|\\theta-\\widetilde{\\theta}_{h}^{i+1}\\right\\|_{\\mathrm{Z}}^{2}\\right)\\left\\|\\theta-\\widetilde{\\theta}_{h}^{i+1}\\right\\|_{\\mathrm{V}^{2}\\ell_{i,h}(\\widetilde{\\theta}_{h}^{i+1})}^{2}\\right]}\\\\ &{=-\\log Z_{i,h}+\\mathbb{E}_{\\theta\\sim\\widehat{\\Theta}_{i+1,h}}\\left[\\exp\\left(18L_{\\varphi}^{2}\\left\\|\\theta-\\widetilde{\\theta}_{h}^{i+1}\\right\\|_{\\mathrm{Z}}^{2}\\right)\\left\\|\\theta-\\widetilde{\\theta}_{h}^{i+1}\\right\\|_{\\mathrm{V}^{2}\\ell_{i,h}(\\widetilde{\\theta}_{h}^{i+1})}^{2}\\right],\\qquad\\quad\\mathrm{(66)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where $\\widehat{\\Theta}_{i+1,h}=\\mathcal{N}(\\widetilde{\\pmb{\\theta}}_{h}^{i+1},c\\mathbf{B}_{i,h}^{-1})$ and $\\widehat{Z}_{i+1,h}=\\int_{\\mathbb{R}^{d}}\\exp\\left(-\\frac{1}{2c}\\left\\|\\pmb{\\theta}-\\widetilde{\\pmb{\\theta}}_{h}^{i+1}\\right\\|_{\\mathbf{B}_{i,h}}^{2}\\right)\\mathrm{d}\\pmb{\\theta},$ , and the last inequality holds because $\\widehat{Z}_{i+1,h}$ and $Z_{i,h}$ are identical normalizing factors. Integrating (65) and (66) and summing over $k$ , yie lds ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{k}\\ell(\\widetilde\\mathbf{z}_{i,h},y_{h}^{i})}\\\\ &{\\displaystyle=\\sum_{i=1}^{k}L_{i,h}(\\widetilde\\pmb{\\theta}_{h}^{i+1})+\\sum_{i=1}^{k}\\mathbb{E}_{\\pmb{\\theta}\\sim\\hat{\\Theta}_{i+1,h}}\\left[\\exp\\left(18L_{\\varphi}^{2}\\left\\|\\pmb{\\theta}-\\widetilde{\\pmb{\\theta}}_{h}^{i+1}\\right\\|_{2}^{2}\\right)\\left\\|\\pmb{\\theta}-\\widetilde{\\pmb{\\theta}}_{h}^{i+1}\\right\\|_{\\nabla^{2}\\ell_{i,h}(\\widetilde{\\pmb{\\theta}}_{h}^{i+1})}^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Moreover, we can further bound the second term on the right-hand side of (66). By Cauchy-Schwarz inequality, we get ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\theta\\sim\\widehat{\\Theta}_{i+1,h}}\\left[\\exp\\left(18L_{\\varphi}^{2}\\left\\Vert\\theta-\\widetilde{\\theta}_{h}^{i+1}\\right\\Vert_{2}^{2}\\right)\\left\\Vert\\theta-\\widetilde{\\theta}_{h}^{i+1}\\right\\Vert_{\\nabla^{2}\\ell_{i,h}(\\widehat{\\theta}_{h}^{i+1})}^{2}\\right]}\\\\ &{\\le\\sqrt{\\mathbb{E}_{\\theta\\sim\\widehat{\\Theta}_{i+1,h}}\\left[\\exp\\left(36L_{\\varphi}^{2}\\left\\Vert\\theta-\\widetilde{\\theta}_{h}^{i+1}\\right\\Vert_{2}^{2}\\right)\\right]}\\underbrace{\\sqrt{\\mathbb{E}_{\\theta\\sim\\widehat{\\Theta}_{i+1,h}}\\left[\\left\\Vert\\theta-\\widetilde{\\theta}_{h}^{i+1}\\right\\Vert_{\\nabla^{2}\\ell_{i,h}(\\widetilde{\\theta}_{h}^{i+1})}^{4}\\right]}}_{(\\mathrm{II})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Since $\\widehat{\\Theta}_{i+1,h}=\\mathcal{N}\\left(\\widetilde{\\pmb{\\theta}}_{h}^{i+1},c\\mathbf{B}_{i,h}^{-1}\\right),\\pmb{\\theta}-\\widetilde{\\pmb{\\theta}}_{h}^{i+1}$ follows the same distribution as ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{d}\\sqrt{c\\lambda_{j}\\left(\\mathbf B_{i,h}^{-1}\\right)}X_{j}\\mathbf e_{j},\\quad\\mathrm{where}\\;X_{j}\\stackrel{i.i.d.}{\\sim}\\mathcal N(0,1),\\forall j\\in[d],\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where $\\lambda_{j}\\left(\\mathbf{B}_{i,h}^{-1}\\right)$ denotes the $j$ -th largest eigenvalue of $\\mathbf{B}_{i,h}^{-1}$ and $\\{\\mathbf{e}_{1},\\ldots,\\mathbf{e}_{d}\\}$ are orthogonal basis of $\\mathbb{R}^{d}$ . Furthermore, since we know that $\\mathbf{B}_{i,h}^{-1}\\leq\\lambda^{-1}\\mathbf{I}_{d}$ , we can bound the term (I) by ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{(I)}\\leq\\sqrt{\\mathbb{E}_{X_{j}}\\left[\\displaystyle\\prod_{j=1}^{d}\\exp\\left(36L_{\\varphi}^{2}c\\lambda^{-1}X_{j}^{2}\\right)\\right]}=\\sqrt{\\displaystyle\\prod_{j=1}^{d}\\mathbb{E}_{X_{j}}\\left[\\exp\\left(36L_{\\varphi}^{2}c\\lambda^{-1}X_{j}^{2}\\right)\\right]}}\\\\ &{\\quad\\leq\\left(\\mathbb{E}_{W\\sim\\chi^{2}}\\left[\\exp\\left(36L_{\\varphi}^{2}c\\lambda^{-1}W\\right)\\right]\\right)^{\\frac{d}{2}}\\leq\\mathbb{E}_{W\\sim\\chi^{2}}\\left[\\exp\\left(18L_{\\varphi}^{2}c\\lambda^{-1}W d\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where $\\chi^{2}$ is the chi-square distribution and the last inequality holds due to Jensen\u2019s inequality. By choosing $\\lambda\\geq72L_{\\varphi}^{2}c\\dot{d}$ , we arrive that ", "page_idx": 47}, {"type": "equation", "text": "$$\n{\\mathrm{(I)}}\\leq\\mathbb{E}_{W\\sim\\chi^{2}}\\left[\\exp\\left({\\frac{W}{4}}\\right)\\right]\\leq{\\sqrt{2}},\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where the last inequality hold\u221as because the moment-generating function for $\\chi^{2}$ -distribution is bounded by $\\mathbb{E}_{W\\sim\\chi^{2}}[\\exp(t\\dot{W})]\\stackrel{\\cdot}{\\leq}1/\\sqrt{1-2t}$ for all $t\\leq1/2$ . Now, we bound the term (II). ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{(II)}=\\sqrt{\\mathbb{E}_{\\theta\\sim\\hat{\\Theta}_{i+1,h}}\\left[\\left\\Vert\\theta-\\widetilde{\\theta}_{h}^{i+1}\\right\\Vert_{\\nabla^{2}\\ell_{i,h}(\\widetilde{\\theta}_{h}^{i+1})}^{4}\\right]}=\\sqrt{\\mathbb{E}_{\\theta\\sim N\\left(0,c\\mathbf{B}_{i,h}^{-1}\\right)}\\left[\\Vert\\theta\\Vert_{\\nabla^{2}\\ell_{i,h}(\\widetilde{\\theta}_{h}^{i+1})}^{4}\\right]}}\\\\ &{\\quad=\\sqrt{\\mathbb{E}_{\\theta\\sim\\mathcal{N}\\left(0,c\\widehat{\\mathbf{B}}_{i,h}^{-1}\\right)}\\left[\\Vert\\theta\\Vert_{2}^{4}\\right]},}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where $\\bar{\\mathbf{B}}_{i,h}=\\left(\\nabla^{2}\\ell_{i,h}(\\widetilde{\\pmb{\\theta}}_{h}^{i+1})\\right)^{-1/2}\\mathbf{B}_{i,h}\\left(\\nabla^{2}\\ell_{i,h}(\\widetilde{\\pmb{\\theta}}_{h}^{i+1})\\right)^{-1/2}$ . Let $\\bar{\\lambda}_{j}:=\\lambda_{j}\\left(c\\bar{\\mathbf{B}}_{i,h}^{-1}\\right)$ be the $j$ -th largest eigenvalue of the matrix. Then, a similar analysis as (67) gives that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{(II)}=\\sqrt{\\left|\\mathbb{E}_{X_{j}\\sim\\mathcal{N}(0,1)}\\left[\\left\\|\\displaystyle\\sum_{j=1}^{d}\\sqrt{\\bar{\\lambda}_{j}}X_{j}\\mathbf{e}_{j}\\right\\|_{2}^{4}\\right]^{4}\\right|}=\\sqrt{\\mathbb{E}_{X_{j}\\sim\\mathcal{N}(0,1)}\\left[\\left(\\displaystyle\\sum_{j=1}^{d}\\bar{\\lambda}_{j}X_{j}^{2}\\right)^{2}\\right]}}\\\\ &{\\mathrm{~\\~\\}=\\sqrt{\\displaystyle\\sum_{j=1}^{d}\\sum_{j^{\\prime}=1}^{d}\\bar{\\lambda}_{j}\\bar{\\lambda}_{j^{\\prime}}\\mathbb{E}_{X_{j},X_{j^{\\prime}}\\sim\\mathcal{N}(0,1)}[X_{j}^{2}X_{j^{\\prime}}^{2}]}\\leq\\sqrt{3\\displaystyle\\sum_{j=1}^{d}\\sum_{j^{\\prime}=1}^{d}\\bar{\\lambda}_{j}\\bar{\\lambda}_{j^{\\prime}}}=\\sqrt{3}c\\,\\mathrm{tr}\\left(\\bar{\\mathbf{B}}_{i,h}^{-1}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where the last inequality holds due to $\\mathbb{E}_{X_{j},X_{j^{\\prime}}\\sim\\mathcal{N}(0,1)}[X_{j}^{2}X_{j^{\\prime}}^{2}]\\le3$ when considering the case where $j=j^{\\prime}$ and the last equality is derived from the fact that $\\begin{array}{r}{\\left(\\sum_{j=1}^{d}\\bar{\\lambda}_{j}\\right)^{2}=\\mathrm{tr}\\left(c\\bar{\\mathbf{B}}_{i,h}^{-1}\\right)}\\end{array}$ . Here, we denote $\\operatorname{tr}(A)$ as the trace of the matrix $A$ . ", "page_idx": 48}, {"type": "text", "text": "We define matrix $\\begin{array}{r}{\\mathbf{R}_{i+1,h}:=\\lambda\\mathbf{I}_{d}/2+\\sum_{\\tau=1}^{i}\\nabla^{2}\\ell_{\\tau,h}(\\pmb{\\theta}_{\\tau+1,h})}\\end{array}$ . Under the condition $\\lambda\\ge2L_{\\varphi}^{2}$ , we have $\\begin{array}{r}{\\nabla^{2}\\ell_{i,h}(\\pmb{\\theta}_{i+1,h})\\preceq L_{\\varphi}^{2}{\\bf I}_{d}\\leq\\frac{\\lambda}{2}{\\bf I}_{d}}\\end{array}$ . Then, we have $\\mathbf{B}_{i,h}\\succeq\\mathbf{R}_{i+1,h}$ . Therefore, we can bound the trace by ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{tr}\\left(\\bar{\\mathbf{B}}_{i,h}^{-1}\\right)=\\mathrm{tr}\\left({\\mathbf{B}}_{i,h}^{-1}\\nabla^{2}\\ell_{i,h}(\\pmb{\\theta}_{i+1,h})\\right)\\le\\mathrm{tr}\\left({\\mathbf{R}}_{i+1,h}^{-1}\\nabla^{2}\\ell_{i,h}(\\pmb{\\theta}_{i+1,h})\\right)}\\\\ &{\\qquad\\qquad=\\mathrm{tr}\\left({\\mathbf{R}}_{i+1,h}^{-1}({\\mathbf{R}}_{i+1,h}-{\\mathbf{R}}_{i,h})\\right)\\le\\log\\frac{\\mathrm{det}({\\mathbf{R}}_{i+1,h})}{\\mathrm{det}({\\mathbf{R}}_{i,h})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where the last inequality holds due to Lemma 4.7 of Hazan et al. [32]. Therefore we can bound the term (II) as ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\operatorname{(II)}\\leq\\sqrt{3}c\\log\\frac{\\operatorname*{det}(\\mathbf{R}_{i+1,h})}{\\operatorname*{det}(\\mathbf{R}_{i,h})}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Combining (68) and (69), we get ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta\\sim\\widehat{\\Theta}_{i+1,h}}\\left[\\exp\\left(6\\left\\|\\theta-\\widetilde{\\theta}_{h}^{i+1}\\right\\|_{2}^{2}\\right)\\left\\|\\theta-\\widetilde{\\theta}_{h}^{i+1}\\right\\|_{\\nabla^{2}\\ell_{i,h}(\\widetilde{\\theta}_{h}^{i+1})}^{2}\\right]\\le\\sqrt{6}c\\log\\frac{\\operatorname*{det}(\\mathbf{R}_{i+1,h})}{\\operatorname*{det}(\\mathbf{R}_{i,h})}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Plugging (66) and (70) into (65), and taking summation over $k$ , we derive that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{i=1}^{k}\\ell(\\widetilde\\mathbf{z}_{i,h},y_{h}^{i})\\leq\\displaystyle\\sum_{i=1}^{k}L_{i,h}(\\widetilde{\\pmb{\\theta}}_{h}^{i+1})+\\sqrt{6}c\\displaystyle\\sum_{i=1}^{k}\\log\\frac{\\mathrm{det}(\\mathbf{R}_{i+1,h})}{\\mathrm{det}(\\mathbf{R}_{i,h})}}&{}\\\\ {\\displaystyle=\\sum_{i=1}^{k}\\left(\\ell_{i,h}(\\widetilde{\\pmb{\\theta}}_{h}^{i+1})+\\frac{1}{2c}\\left\\|\\widetilde\\theta_{h}^{i+1}-\\widetilde\\theta_{h}^{i}\\right\\|_{{\\mathbf{B}}_{i,h}}^{2}\\right)+\\sqrt{6}c\\displaystyle\\sum_{i=1}^{k}\\log\\frac{\\mathrm{det}(\\mathbf{R}_{i+1,h})}{\\mathrm{det}(\\mathbf{R}_{i,h})}}&{}\\\\ {\\displaystyle}&{\\leq\\displaystyle\\sum_{i=1}^{k}\\left(\\ell_{i,h}(\\widetilde\\theta_{h}^{i+1})+\\frac{1}{2c}\\left\\|\\widetilde\\theta_{h}^{i+1}-\\widetilde\\theta_{h}^{i}\\right\\|_{{\\mathbf{B}}_{i,h}}^{2}\\right)+\\sqrt{6}c d\\log\\left(1+\\frac{2k L_{\\varphi}^{2}}{d\\lambda}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where the last inequality holds because ik=1 log dedte(t(RRi+i,1h,)h) $\\begin{array}{r}{\\sum_{i=1}^{k}\\log\\frac{\\operatorname*{det}({\\mathbf{R}}_{i+1,h})}{\\operatorname*{det}({\\mathbf{R}}_{i,h})}=\\log(\\operatorname*{det}({\\mathbf{R}}_{k+1,h})/\\operatorname*{det}(\\lambda/2{\\mathbf{I}}_{d}))\\leq}\\end{array}$ $\\begin{array}{r}{d\\log\\left(1+\\frac{2k L_{\\varphi}^{2}}{d\\lambda}\\right)}\\end{array}$ . By rearranging the terms, we conclude the proof. \u53e3 ", "page_idx": 48}, {"type": "text", "text": "D.2 Bound on Prediction Error ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "In this section, we present the bound on the prediction error of parameters updated by ORRL-MNL. First, we compare the problem setting of MNL contextual bandits with ours and introduce the challenges of applying their analysis to our setting. ", "page_idx": 49}, {"type": "text", "text": "MNL dynamic assortment optimization (single-parameter $\\&$ uniform reward) [61] Perivier and Goyal [61] consider an assortment selection problem where the user choice is given by a MNL choice model with the single-parameter. At each time $t$ , the agent observes context features $\\{\\mathbf{x}_{t,i}\\}_{i=1}^{M}\\subset\\mathbb{R}^{d}$ . Then the agent decides on the set $S_{t}\\,\\subset\\,[M]$ to offer to a user, with $|S_{t}|\\,\\le\\,N$ Without loss of generality, we may assume $|S_{t}|=N$ . Then the user purchases one single product $j\\in S_{t}\\cup\\{0\\}$ and the probability of each product $j$ is purchased by a user follows the MNL model parametrized by a unknown fixed parameter $\\pmb{\\theta}^{*}\\in\\mathbb{R}^{d}$ , ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q_{t,j}(S_{t},\\pmb{\\theta}^{*}):=\\left\\{\\frac{\\exp(\\mathbf{x}_{t,j}^{\\top}\\pmb{\\theta}^{*})}{1+\\sum_{k\\in S_{t}}\\exp(\\mathbf{x}_{k}^{\\top}\\pmb{\\theta}^{*})}\\qquad\\mathrm{if~}j\\in S_{t}\\right.}\\\\ {\\frac{1}{1+\\sum_{k\\in S_{t}}\\exp(\\mathbf{x}_{k}^{\\top}\\pmb{\\theta}^{*})}\\qquad\\mathrm{if~}j=0\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Then the difference between the revenue induced by $\\pmb{\\theta}^{*}$ and that by an estimator $\\pmb{\\theta}$ in Perivier and Goyal [61] is expressed as follows: ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\sum_{j\\in S_{t}}q_{t,j}(S_{t},\\pmb{\\theta}^{*})-\\sum_{j\\in S_{t}}q_{t,j}(S_{t},\\pmb{\\theta})\\,.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "If we define Q : RN \u2192R, such that for all u = (u1, . . . , uN) \u2208RN, Q(u) :=  iN=11+ ejNx=p1( euix)p(uj) and let $\\mathbf{v}^{*}=(\\mathbf{x}_{t,i_{1}}^{\\top}\\pmb{\\theta}^{*},\\dots,\\mathbf{x}_{t,i_{N}}^{\\top}\\pmb{\\theta}^{*})$ and $\\mathbf{v}=(\\mathbf{x}_{t,i_{1}}^{\\top}\\pmb{\\theta},\\dots,\\mathbf{x}_{t,i_{N}}^{\\top}\\pmb{\\theta})$ , then Eq. (71) can be expressed as follows: ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{j\\in S_{t}}q_{t,j}(S_{t},\\theta^{*})-\\displaystyle\\sum_{j\\in S_{t}}q_{t,j}(S_{t},\\theta)=Q(\\mathbf{v^{*}})-Q(\\mathbf{v})}&{}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad=\\nabla Q(\\mathbf{v^{*}})^{\\top}(\\mathbf{v^{*}}-\\mathbf{v})+\\frac{1}{2}(\\mathbf{v^{*}}-\\mathbf{v})^{\\top}\\nabla^{2}Q(\\bar{\\mathbf{v}})(\\mathbf{v^{*}}-\\mathbf{v})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where $\\bar{\\bf v}$ is a convex combination of $\\mathbf{v}^{*}$ and $\\mathbf{v}$ . For the first term in Eq. (72), we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla Q(\\mathbf{v}^{*})^{\\top}(\\mathbf{v}^{*}-\\mathbf{v})}\\\\ &{=\\displaystyle\\frac{\\sum_{i\\in S_{t}}\\exp(\\mathbf{x}_{i}^{\\top}\\theta^{*})(v_{j}-v_{j}^{*})}{1+\\sum_{j\\in S_{t}}\\exp(\\mathbf{x}_{i,j}^{\\top}\\theta^{*})}-\\frac{\\sum_{i\\in S_{t}}\\exp(\\mathbf{x}_{i,j}^{\\top}\\theta^{*})\\sum_{i\\in S_{t}}\\exp(\\mathbf{x}_{i,i}^{\\top}\\theta^{*})(v_{j}-v_{j}^{*})}{\\left(1+\\sum_{j\\in S_{t}}\\exp(\\mathbf{x}_{i,j}^{\\top}\\theta^{*})\\right)^{2}}}\\\\ &{=\\displaystyle\\sum_{j\\in S_{t}}q_{i,j}(S_{t},\\theta^{*})\\mathbf{x}_{i,j}^{\\top}(\\theta^{*}-\\theta)-\\displaystyle\\sum_{j\\in S_{t}}\\sum_{i\\in S_{t}}q_{i,j}(S_{t},\\theta^{*})q_{i,j}(S_{t},\\theta^{*})\\mathbf{x}_{i,i}^{\\top}(\\theta^{*}-\\theta)}\\\\ &{=\\displaystyle\\sum_{j\\in S_{t}}q_{i,j}(S_{t},\\theta^{*})\\left(1-\\sum_{i\\in S_{t}}q_{i,i}(S_{t},\\theta^{*})\\right)\\mathbf{x}_{i,j}^{\\top}(\\theta^{*}-\\theta)}\\\\ &{=\\displaystyle\\sum_{j\\in S_{t}}q_{i,j}(S_{t},\\theta^{*})q_{i,0}(S_{t},\\theta^{*})\\mathbf{x}_{i,j}^{\\top}(\\theta^{*}-\\theta)}\\\\ &{\\le\\displaystyle\\sum_{j\\in S_{t}}q_{i,j}(S_{t},\\theta^{*})q_{i,0}(S_{t},\\theta^{*})\\|\\mathbf{x}_{i,j}\\|_{\\mathbf{H}_{t}^{-1}(\\theta^{*})}\\|\\theta^{*}-\\theta\\|_{\\mathbf{H}_{t}(\\theta^{*})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where $\\mathbf{H}_{t}(\\pmb{\\theta})$ is the Gram matrix used in [61] defined by ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathbf{H}_{t}(\\theta^{*}):=\\sum_{\\tau=1}^{t-1}\\sum_{j\\in S_{\\tau}}q_{\\tau,j}(S_{\\tau},\\theta^{*})\\mathbf{x}_{\\tau,j}\\mathbf{x}_{\\tau,j}^{\\top}-\\sum_{j\\in S_{\\tau}}\\sum_{i\\in S_{\\tau}}q_{\\tau,j}(S_{\\tau},\\theta^{*})q_{\\tau,i}(S_{\\tau},\\theta^{*})\\mathbf{x}_{\\tau,j}\\mathbf{x}_{\\tau,i}^{\\top}\\,.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Note that the term $\\lVert\\theta^{*}\\ -\\ \\theta\\rVert_{\\mathbf{H}_{t}(\\theta^{*})}$ can be bounded by the concentration result of the estimated parameter. On the other hand, to apply the elliptical potential lemma to the term ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{H}_{t}(\\theta^{*})}\\\\ &{=\\mathbf{H}_{t-1}(\\theta^{*})+\\displaystyle\\sum_{j\\in S_{t}}q_{t,j}(S_{t},\\theta^{*})\\mathbf{x}_{t,j}\\mathbf{x}_{t,j}^{\\top}-\\displaystyle\\frac{1}{2}\\sum_{i,j\\in S_{t}}q_{t,j}(S_{t},\\theta^{*})q_{t,i}(S_{t},\\theta^{*})\\left(\\mathbf{x}_{t,j}\\mathbf{x}_{t,i}^{\\top}+\\mathbf{x}_{t,i}\\mathbf{x}_{t,j}^{\\top}\\right)}\\\\ &{\\le\\mathbf{H}_{t-1}(\\theta^{*})+\\displaystyle\\sum_{j\\in S_{t}}q_{t,j}(S_{t},\\theta^{*})\\mathbf{x}_{t,j}\\mathbf{x}_{t,j}^{\\top}-\\displaystyle\\frac{1}{2}\\sum_{i,j\\in S_{t}}q_{t,j}(S_{t},\\theta^{*})q_{t,i}(S_{t},\\theta^{*})\\left(\\mathbf{x}_{t,j}\\mathbf{x}_{t,j}^{\\top}+\\mathbf{x}_{t,i}\\mathbf{x}_{t,i}^{\\top}\\right)}\\\\ &{=\\mathbf{H}_{t-1}(\\theta^{*})+\\displaystyle\\sum_{j\\in S_{t}}q_{t,j}(S_{t},\\theta^{*})\\left(1-\\displaystyle\\sum_{i\\in S_{t}}q_{t,i}(S_{t},\\theta^{*})\\right)\\mathbf{x}_{t,j}\\mathbf{x}_{t,j}^{\\top}}\\\\ &{=\\mathbf{H}_{t-1}(\\theta^{*})+\\displaystyle\\sum_{j\\in S_{t}}q_{t,j}(S_{t},\\theta^{*})q_{t,0}(S_{t},\\theta^{*})\\mathbf{x}_{t,j}\\mathbf{x}_{t,j}^{\\top}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Now since the coefficient $q_{t,j}(S_{t},\\pmb{\\theta}^{*})q_{t,0}(S_{t},\\pmb{\\theta}^{*})$ of $\\lVert\\mathbf{x}\\rVert_{\\mathbf{H}_{t}^{-1}(\\pmb{\\theta}^{*})}$ in Eq. (73) aligns with the coefficients of the lower bound of $\\mathbf{H}_{t}(\\pmb{\\theta}^{*})$ in Eq. (74), the elliptical potential lemma can be applied. Note that such a lower bound in Eq. (74) holds since Perivier and Goyal [61] deals with the uniform reward, i.e., $\\begin{array}{r}{1-\\sum_{i\\in S_{t}}q_{t,i}\\bigl(S_{t},\\pmb{\\theta}^{*}\\bigr)\\stackrel{\\_}{=}q_{t,0}\\bigl(S_{t},\\pmb{\\theta}^{*}\\bigr)}\\end{array}$ . ", "page_idx": 50}, {"type": "text", "text": "Mulitinomial logistic bandit problem [76] Zhang and Sugiyama [76] address the multipleparameter MNL contextual bandit problem where at each time step $t$ the agent selects an action $\\dot{\\mathbf{x}}_{t}\\,\\in\\,\\mathbb{R}^{d}$ and receives response feedback $y_{t}\\;\\in\\;\\{0\\}\\cup[N]$ with $N+1$ possible outcomes. Each outcome $i\\in[N]$ is associated with a ground-truth parameter $\\pmb{\\theta}_{i}^{*}\\in\\mathbb{R}^{d}$ , and the probability of the outcome $\\mathbb{P}(y_{t}=\\dot{\\boldsymbol{\\imath}}\\mid\\mathbf{x}_{t})$ follows the MNL model, ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\mathbb P(y_{t}=i\\mid\\mathbf x_{t})=\\frac{\\exp(\\mathbf x_{t}^{\\top}\\theta_{i}^{*})}{1+\\sum_{j=1}^{N}\\exp(\\mathbf x_{t}^{\\top}\\theta_{j}^{*})}\\,,\\quad\\mathbb P(y_{t}=0\\mid\\mathbf x_{t})=1-\\sum_{j=1}^{N}\\mathbb P(y_{t}=j\\mid\\mathbf x_{t})\\,.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "In this model, there are $N$ unknown choice parameter $\\Theta^{\\ast}:=[\\pmb{\\theta}_{1}^{\\ast},\\dots,\\pmb{\\theta}_{N}^{\\ast}]\\in\\mathbb{R}^{d\\times N}$ and the agent chooses one context feature $\\mathbf{x}_{t}$ , that is why we call multiple-parameter MNL model. Then, the expected revenue of an action $\\mathbf{x}_{t}$ in [76] is given by ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{N}\\frac{\\exp(\\mathbf{x}_{i}^{\\top}\\pmb{\\theta}_{i}^{*})\\rho_{i}}{1+\\sum_{j=1}^{N}\\exp(\\mathbf{x}_{t}^{\\top}\\pmb{\\theta}_{j}^{*})}:=\\rho^{\\top}\\pmb{\\sigma}(\\pmb{\\Theta}^{*}\\mathbf{x}_{t})\\,,\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where we define the softmax function $\\pmb{\\sigma}:\\mathbb{R}^{N}\\rightarrow[0,1]^{N}$ by ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\sigma(\\mathbf{z})]_{k}=\\frac{\\exp([\\mathbf{z}]_{i})}{1+\\sum_{j=1}^{N}\\exp([\\mathbf{z}]_{j})}\\quad\\forall k\\in[N]\\quad\\mathrm{and}\\quad[\\sigma(\\mathbf{z})]_{0}=\\frac{1}{1+\\sum_{j=1}^{N}\\exp([\\mathbf{z}]_{j})}\\quad\\forall k\\in[N]\\,,\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "and $\\pmb{\\rho}:=[\\rho_{1},\\ldots,\\rho_{N}]\\in\\mathbb{R}_{+}^{N+1}$ represents the reward for each outcome $j\\in[N]$ with $\\rho_{0}=0$ . Then, the difference between the revenue induced by $\\Theta^{*}$ and that by an estimator $\\hat{\\Theta}$ in [76] is expressed by ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rho^{\\top}\\left(\\sigma(\\Theta^{*}\\mathbf{x}_{t})-\\sigma(\\hat{\\Theta}\\mathbf{x}_{t})\\right)}\\\\ &{=\\displaystyle\\sum_{k=1}^{N}\\rho_{k}\\left([\\sigma(\\Theta^{*}\\mathbf{x}_{t})]_{k}-[\\sigma(\\hat{\\Theta}\\mathbf{x}_{t})]_{k}\\right)}\\\\ &{=\\displaystyle\\sum_{k=1}^{N}\\rho_{k}\\left(\\nabla[\\sigma(\\hat{\\Theta}\\mathbf{x}_{t})]_{k}\\right)^{\\top}(\\Theta^{*}-\\hat{\\Theta})\\mathbf{x}_{t}+\\displaystyle\\sum_{k=1}^{N}\\rho_{k}\\|(\\Theta^{*}-\\Theta)\\mathbf{x}_{t}\\|\\mathbf{z}_{k}\\mathrm{~,~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where $\\begin{array}{r}{\\Xi_{k}=\\int_{0}^{1}(1-\\nu)\\nabla^{2}[\\sigma(\\hat{\\Theta}\\mathbf{x}_{t}+\\nu(\\Theta^{*}-\\hat{\\Theta})\\mathbf{x}_{t})]_{k}d\\nu}\\end{array}$ . Then for the first term in Eq. (75), we have ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{k=1}^{N}\\rho_{k}\\left(\\nabla[\\sigma(\\hat{\\mathbf{S}}\\mathbf{x}_{t})]_{k}\\right)^{\\top}(\\Theta^{*}-\\hat{\\Theta})\\mathbf{x}_{t}}\\\\ {\\displaystyle\\leq\\Big|\\rho^{\\top}\\nabla\\sigma(\\hat{\\Theta}\\mathbf{x}_{t})(\\Theta^{*}-\\hat{\\Theta})\\mathbf{x}_{t}\\Big|}\\\\ {\\displaystyle=\\Big|\\rho^{\\top}\\nabla\\sigma(\\hat{\\Theta}\\mathbf{x}_{t})(\\mathbf{I}_{N}\\otimes\\mathbf{x}_{t}^{\\top})(\\mathrm{vec}(\\Theta^{*})-\\mathrm{vec}(\\hat{\\Theta}))\\Big|}\\\\ {\\displaystyle\\leq\\|\\mathrm{vec}(\\Theta^{*})-\\mathrm{vec}(\\hat{\\Theta})\\|_{\\mathbf{H}_{t}}\\|\\mathbf{H}_{t}^{-\\frac{1}{2}}(\\mathbf{I}_{N}\\otimes\\mathbf{x}_{t}^{\\top})\\nabla\\sigma(\\hat{\\Theta}\\mathbf{x}_{t})\\rho\\|_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where $\\mathbf{H}_{t}$ is the Gram matrix used in [76] defined by ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathbf{H}_{t}:=\\lambda\\mathbf{I}_{N}+\\sum_{s=1}^{t-1}\\nabla\\pmb{\\sigma}(\\hat{\\Theta}_{s+1}\\mathbf{x}_{s})\\otimes\\mathbf{x}_{s}\\mathbf{x}_{s}^{\\top}\\,.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Note that the term $\\lVert\\mathrm{vec}(\\Theta^{*})-\\mathrm{vec}(\\hat{\\Theta})\\rVert_{\\mathbf{H}_{t}}$ in Eq. (76) can be bounded by the concentration result of the estimated parameter, and the term $\\|\\mathbf{H}_{t}^{-\\frac{1}{2}}(\\mathbf{I}_{N}\\otimes\\mathbf{x}_{t}^{\\top})\\nabla\\pmb{\\sigma}(\\hat{\\Theta}\\mathbf{x}_{t})\\pmb{\\rho}\\|_{2}$ also can be bounded as follows: ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbf{H}_{t}^{-\\frac{1}{2}}(\\mathbf{I}_{N}\\otimes\\mathbf{x}_{t}^{\\top})\\nabla\\sigma(\\hat{\\Theta}\\mathbf{x}_{t})\\rho\\|_{2}\\leq\\|\\rho\\|_{2}\\|\\mathbf{H}_{t}^{-\\frac{1}{2}}(\\mathbf{I}_{N}\\otimes\\mathbf{x}_{t}^{\\top})\\nabla\\sigma(\\hat{\\Theta}\\mathbf{x}_{t})\\|_{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Here Zhang and Sugiyama [76] bound the term $\\|\\mathbf{H}_{t}^{-\\frac{1}{2}}(\\mathbf{I}_{N}\\otimes\\mathbf{x}_{t}^{\\top})\\nabla\\sigma(\\hat{\\Theta}\\mathbf{x}_{t})\\|_{2}$ using a matrix version of elliptical lemma. However, they assume $\\|\\pmb{\\rho}\\|_{2}\\leq R$ (Assumption 2 in [76]). ", "page_idx": 51}, {"type": "text", "text": "Now, regarding the prediction error in our setting, the estimated values $(\\widetilde V_{h+1}^{k}(\\cdot))$ for each reachable state are typically distinct, and we do not assume a constant upper bound on the $\\ell_{2}$ -norm of the estimated value vector for all reachable states. Instead, we can bound the $\\ell_{2}$ -norm of the estimated value vector for all reachable states as follows: ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\|\\widetilde{\\mathbf{V}}_{h+1}^{k}(s,a)\\|_{2}\\leq\\operatorname*{max}_{s^{\\prime}\\in{\\ensuremath{S}_{s,a}}}\\left|\\widetilde{V}_{h+1}^{k}(s^{\\prime})\\right|\\sqrt{|{\\ensuremath{S}_{s,a}}|}\\leq H\\sqrt{\\mathscr{U}}\\,,\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where $\\widetilde{\\mathbf{V}}_{h+1}^{k}(s,a):=\\left[\\widetilde{V}_{h+1}^{k}(s^{\\prime})\\right]_{s^{\\prime}\\in S_{s,a}}\\in\\mathbb{R}^{|S_{s,a}|},$ . However, such a bound leads to a looser regret by a factor of $\\sqrt{\\mathcal{U}}$ . To address, we adapt the feature centralization technique [50] to bound the prediction error independently of $\\boldsymbol{\\mathcal{U}}$ , without making any additional assumptions. The key point is that the Hessian of per-round loss $\\ell_{k,h}(\\pmb\\theta)$ is expressed in terms of the centralized feature as follows: ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\nabla^{2}\\ell_{k,h}(\\pmb\\theta)=\\sum_{s^{\\prime}\\in S_{k,h}}P_{\\pmb\\theta}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\bar{\\varphi}(s_{h}^{k},a_{h}^{k},s^{\\prime};\\pmb\\theta)\\bar{\\varphi}(s_{h}^{k},a_{h}^{k},s^{\\prime};\\pmb\\theta)^{\\top}\\,.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where $\\bar{\\varphi}(s,a,s^{\\prime};\\theta):=\\varphi(s,a,s^{\\prime})-\\mathbb{E}_{\\widetilde{s}\\sim P_{\\theta}(\\cdot|s,a)}[\\varphi(s,a,\\widetilde{s})]$ is the centralized feature by $\\pmb{\\theta}$ . Now, we provide the bound on prediction error of the estimated parameter updated by ORRL-MNL. ", "page_idx": 51}, {"type": "text", "text": "Lemma 16 (Bound on the prediction error). For any $\\delta\\in(0,1)$ , suppose that Lemma $^{12}$ holds. Let us denote the prediction error about $\\widetilde{\\pmb{\\theta}}_{h}^{k}$ by ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\Delta_{h}^{k}(s,a):=\\sum_{s^{\\prime}\\in S_{s,a}}\\left(P_{\\widetilde{\\theta}_{h}^{k}}(s^{\\prime}\\mid s,a)-P_{\\theta_{h}^{*}}(s^{\\prime}\\mid s,a)\\right)\\widetilde{V}_{h+1}^{k}(s^{\\prime})\\,.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Then, for any $(s,a)\\in S\\times A,$ , we have ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\left|\\Delta_{h}^{k}(s,a)\\right|\\leq H\\beta_{k}(\\delta)\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\widetilde{\\theta}_{h}^{k}}(s^{\\prime}\\mid s,a)\\left\\Vert\\overline{{\\varphi}}_{s,a,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k})\\right\\Vert_{\\mathbf{B}_{k,h}^{-1}}+3H\\beta_{k}(\\delta)^{2}\\operatorname*{max}_{s^{\\prime}\\in S_{s,a}}{\\Vert\\varphi_{s,a,s^{\\prime}}\\Vert_{\\mathbf{B}_{k,h}^{-1}}^{2}}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Proof of Lemma $I6$ . Let us define $\\begin{array}{r}{F(\\pmb\\theta):=\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\pmb\\theta}(s^{\\prime}\\mid s,a)\\widetilde{V}_{h+1}^{k}(s^{\\prime})}\\end{array}$ . Then, by Taylor expansion we have ", "page_idx": 51}, {"type": "equation", "text": "$$\nF(\\pmb{\\theta}_{h}^{*})=F(\\widetilde{\\pmb{\\theta}}_{h}^{k})+\\nabla F(\\widetilde{\\pmb{\\theta}}_{h}^{k})^{\\top}(\\pmb{\\theta}_{h}^{*}-\\widetilde{\\pmb{\\theta}}_{h}^{k})+\\frac{1}{2}(\\pmb{\\theta}_{h}^{*}-\\widetilde{\\pmb{\\theta}}_{h}^{k})^{\\top}\\nabla^{2}F(\\widetilde{\\pmb{\\theta}})(\\pmb{\\theta}_{h}^{*}-\\widetilde{\\pmb{\\theta}}_{h}^{k})\\,,\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where $\\bar{\\pmb{\\theta}}=(1-v)\\pmb{\\theta}_{h}^{*}+v\\widetilde{\\pmb{\\theta}}_{h}^{k}$ for some $v\\in(0,1)$ . By Proposition 1, we have ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla F(\\pmb\\theta)=\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}\\nabla P_{\\theta}(s^{\\prime}\\mid s,a)\\widetilde V_{h+1}^{k}(s^{\\prime})}\\\\ &{\\quad\\quad=\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\theta}(s^{\\prime}\\mid s,a)\\left(\\varphi_{s,a,s^{\\prime}}-\\displaystyle\\sum_{\\widetilde{s}\\in S_{s,a}}P_{\\theta}(\\widetilde s\\mid s,a)\\varphi_{s,a,\\widetilde{s}}\\right)\\widetilde V_{h+1}^{k}(s^{\\prime})}\\\\ &{\\quad\\quad=\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\theta}(s^{\\prime}\\mid s,a)\\bar{\\varphi}_{s,a,s^{\\prime}}(\\pmb\\theta)\\widetilde V_{h+1}^{k}(s^{\\prime})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "and ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla^{2}F(\\theta)=\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}_{n,s}}\\nabla^{2}P_{\\theta}(s^{\\prime}\\mid s,a)\\tilde{V}_{h+1}^{k}(s^{\\prime})}\\\\ &{=\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}_{n,s}}P_{\\theta}(s^{\\prime}\\mid s,a)\\tilde{V}_{h+1}^{k}(s^{\\prime})\\varphi_{s,a,s^{\\prime}}\\varphi_{s,a^{\\prime}}^{\\top}}\\\\ &{\\quad-\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}_{n,s}}P_{\\theta}(s^{\\prime}\\mid s,a)\\tilde{V}_{h+1}^{k}(s^{\\prime})}\\\\ &{\\quad\\quad\\cdot\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}_{n,s}}P_{\\theta}(s^{\\prime}\\mid s,a)\\left(\\varphi_{s,a^{\\prime}}\\varphi_{s,a^{\\prime}}^{\\top}+\\varphi_{s,a,s^{\\prime\\prime}}\\varphi_{s,a^{\\prime}}^{\\top}+\\varphi_{s,a,s^{\\prime\\prime}}\\varphi_{s,a^{\\prime\\prime}}^{\\top}\\right)}\\\\ &{\\quad\\quad+2\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}_{n,s}}P_{\\theta}(s^{\\prime}\\mid s,a)\\tilde{V}_{h+1}^{k}(s^{\\prime})}\\\\ &{\\quad\\quad\\quad\\cdot\\left(\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}_{n,s}}P_{\\theta}(s^{\\prime\\prime}\\mid s,a)\\varphi_{s,a^{\\prime\\prime}}\\right)\\left(\\displaystyle\\sum_{s^{\\prime\\prime}\\in\\mathcal{S}_{n,s}}P_{\\theta}(s^{\\prime\\prime}\\mid s,a)\\varphi_{s,a^{\\prime\\prime}}\\right)^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Then, the prediction error can be bounded as follows: ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\Delta_{h}^{k}(s,a)|=|F(\\pmb{\\theta}_{h}^{*})-F(\\widetilde{\\pmb{\\theta}}_{h}^{k})|}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\left|\\nabla F(\\widetilde{\\pmb{\\theta}}_{h}^{k})^{\\top}(\\widetilde{\\pmb{\\theta}}_{h}^{k}-\\pmb{\\theta}_{h}^{*})\\right|+\\frac{1}{2}\\left|(\\widetilde{\\pmb{\\theta}}_{h}^{k}-\\pmb{\\theta}_{h}^{*})^{\\top}\\nabla^{2}F(\\widetilde{\\pmb{\\theta}})(\\widetilde{\\pmb{\\theta}}_{h}^{k}-\\pmb{\\theta}_{h}^{*})\\right|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "For the first term in Eq. (77), ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\nabla F(\\widetilde{\\theta}_{h}^{k})^{\\top}(\\widetilde{\\theta}_{h}^{k}-\\theta_{h}^{*})\\right|=\\displaystyle\\left|\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\widetilde{\\theta}_{h}^{k}}(s^{\\prime}\\mid s,a)\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k})^{\\top}(\\widetilde{\\theta}_{h}^{k}-\\theta_{h}^{*})\\widetilde{V}_{h+1}^{k}(s^{\\prime})\\right|}\\\\ &{\\qquad\\qquad\\qquad\\leq H\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\widetilde{\\theta}_{h}^{k}}(s^{\\prime}\\mid s,a)\\left\\|\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}\\left\\|\\widetilde{\\theta}_{h}^{k}-\\theta_{h}^{*}\\right\\|_{\\mathbf{B}_{k,h}}}\\\\ &{\\qquad\\qquad\\leq H\\beta_{k}(\\delta)\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\widetilde{\\theta}_{h}^{k}}(s^{\\prime}\\mid s,a)\\left\\|\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where in the first inequality we use $\\widetilde{V}_{h+1}^{k}(s^{\\prime})\\leq H$ and Cauchy-Scharwz inequality, and the second inequality follows by the concentration result of Lemma 12. ", "page_idx": 52}, {"type": "text", "text": "For the second term in Eq. (77), since $0\\leq\\widetilde{V}_{h+1}^{k}(s^{\\prime})\\leq H$ , ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\begin{array}{l}{\\rho_{1}}\\\\ {,~}\\\\ \\\\ {~}\\\\ {~+\\eta_{c-1}^{\\prime\\prime}\\rho_{1}^{\\prime\\prime}(\\hat{\\tau}_{1}\\omega_{1}\\left(\\rho_{1}^{\\prime\\prime}-\\theta_{c}\\right)^{\\top}\\varphi_{c-\\alpha,\\alpha})}\\end{array}\\right]}\\\\ &{\\quad\\times\\quad\\sum_{k=1}^{{N}}\\beta_{k}(\\sigma^{\\top}|\\gamma_{k+1,k}|\\theta_{c})}\\\\ &{\\quad\\times\\left(\\frac{1}{N}\\rho_{1}^{\\top}\\left(\\sigma^{\\top}|\\gamma_{k+1,k}|\\right)\\left(\\frac{1}{N}\\rho_{1}^{\\top}-\\sigma_{c}\\right)^{\\top}\\left(\\sigma_{c-\\alpha,\\alpha}\\gamma_{k-1,k}^{\\top}+\\varphi_{c-\\alpha,\\alpha}\\gamma_{k-1,k}^{\\top}\\right)|\\tilde{\\Psi}_{t}-\\rho_{1}|\\right)}\\\\ &{\\quad\\times\\quad\\sum_{k=1}^{{N}}\\beta_{k}(\\sigma^{\\top}|\\gamma_{k+1,k}|\\rho_{c-\\alpha,\\alpha}\\gamma_{k-1,k}^{\\top}\\rho_{1}^{\\top}(\\hat{\\tau}_{1}\\omega_{1}\\left(\\rho_{1}^{\\prime}-\\theta_{c}\\right)^{\\top}\\varphi_{c-\\alpha,\\alpha})^{\\top}}\\\\ &{\\quad+\\eta_{c-\\alpha,\\alpha}\\gamma_{k}(\\rho^{\\top}|\\gamma_{k+1,k}|\\rho_{c-\\alpha,\\alpha}\\gamma_{k-1,k}^{\\top}\\rho_{1}^{\\top}(\\hat{\\tau}_{1}\\omega_{1}\\left(\\rho_{1}^{\\prime}-\\theta_{c}\\right)^{\\top}\\varphi_{c-\\alpha,\\alpha})^{\\top})}\\\\ &{\\quad\\times\\left(\\frac{1}{N}\\rho_{1}^{\\top}\\left(\\sigma^{\\top}|\\gamma_{k}|\\rho_{c-\\alpha,\\alpha}\\gamma_{k-1,k}^{\\top}\\rho_{1}^{\\top}\\right)\\left(\\frac{1}{N}\\rho_{1}^{\\top}-\\sigma_{c}\\right)^{\\top}\\right)}\\\\ &{\\quad\\le\\eta_{c-1}^{\\top}\\rho_{1}(\\sigma^{\\top}|\\gamma_{1}\\omega_{1}\\left(\\rho_{1}^{\\top}-\\sigma_{c}\\right)^{\\top}|\\beta_{c-\\alpha,\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "where for the second inequality we use Cauchy-Schwarz inequality, $\\mathbf{x}\\mathbf{x}^{\\top}+\\mathbf{y}\\mathbf{y}^{\\top}\\succeq\\mathbf{x}\\mathbf{y}^{\\top}+\\mathbf{y}\\mathbf{x}^{\\top}$ for any $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{d}$ , and triangle inequality. Note that ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\theta}(s^{\\prime}\\mid s,a)}\\\\ &{\\quad\\cdot\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\theta}(s^{\\prime\\prime}\\mid s,a)\\left[(\\tilde{\\theta}_{h}^{k}-\\theta_{h}^{*})^{\\top}\\left(\\varphi_{s,a,s^{\\prime}}\\varphi_{s,a,s^{\\prime}}^{\\top}+\\varphi_{s,a,s^{\\prime\\prime}}\\varphi_{s,a,s^{\\prime\\prime}}^{\\top}\\right)(\\tilde{\\theta}_{h}^{k}-\\theta_{h}^{*})\\right]}\\\\ &{=H\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\theta}(s^{\\prime}\\mid s,a)\\left((\\tilde{\\theta}_{h}^{k}-\\theta_{h}^{*})^{\\top}\\varphi_{s,a,s^{\\prime}}\\right)^{2}}\\\\ &{\\quad+H\\displaystyle\\sum_{s^{\\prime\\prime}\\in S_{s,a}}P_{\\theta}(s^{\\prime\\prime}\\mid s,a)\\left((\\tilde{\\theta}_{h}^{k}-\\theta_{h}^{*})^{\\top}\\varphi_{s,a,s^{\\prime\\prime}}\\right)^{2}}\\\\ &{\\leq2H\\displaystyle\\sum_{s^{\\prime\\prime}\\in S_{s,a}}P_{\\theta}(s^{\\prime}\\mid s,a)\\lVert\\varphi_{s,a,s^{\\prime}}\\rVert_{\\mathbf{B}_{h,h}^{-1}}^{2}\\left\\lVert\\tilde{\\theta}_{h}^{k}-\\theta_{h}^{*}\\right\\rVert_{\\mathbf{B}_{h,h}^{\\lambda}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "By substituting Eq. (80) into Eq. (79) we have ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bigl(\\widetilde{\\theta}_{h}^{k}-\\theta_{h}^{*}\\bigr)^{\\top}\\nabla^{2}F(\\widetilde{\\theta})(\\widetilde{\\theta}_{h}^{k}-\\theta_{h}^{*})\\Bigr|}\\\\ &{\\leq4H\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}_{s,n}}P_{\\theta}(s^{\\prime}\\mid s,a)\\|\\varphi_{s,a,s^{\\prime}}\\|_{\\mathbf{B}_{h,h}^{-1}}^{2}\\,\\Bigl\\|\\widetilde{\\theta}_{h}^{k}-\\theta_{h}^{*}\\Bigr\\|_{\\mathbf{B}_{h,h}}^{2}}\\\\ &{\\qquad+2H\\displaystyle\\langle\\sum_{s^{\\prime\\prime}\\in\\mathcal{S}_{s,a}}P_{\\theta}(s^{\\prime\\prime}\\mid s,a)\\|\\varphi_{s,a,s^{\\prime\\prime}}\\|_{\\mathbf{B}_{h,h}^{-1}}\\,\\Bigl\\|\\widetilde{\\theta}_{h}^{k}-\\theta_{h}^{*}\\Bigr\\|_{\\mathbf{B}_{h,h}}\\Bigr\\rangle^{2}}\\\\ &{\\leq4H\\beta_{k}^{2}\\operatorname*{max}_{s^{\\prime}\\in\\mathcal{S}_{s,a}}\\,\\bigl\\|\\varphi_{s,a,s^{\\prime}}\\|_{\\mathbf{B}_{h,h}^{-1}}^{2}+2H\\left(\\beta_{k}\\operatorname*{max}_{s^{\\prime}\\in\\mathcal{S}_{s,a}}\\,\\|\\varphi_{s,a,s^{\\prime}}\\|_{\\mathbf{B}_{h,h}^{-1}}\\right)^{2}}\\\\ &{\\leq6H\\beta_{k}^{2}\\operatorname*{max}_{s^{\\prime}\\in\\mathcal{S}_{s,a}}\\,\\bigl\\|\\varphi_{s,a,s^{\\prime}}\\|_{\\mathbf{B}_{h,h}^{-1}}^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where for the second inequality follows by Lemma 12 and $\\textstyle\\sum_{s^{\\prime}\\in{\\cal S}_{s,a}}P_{\\bar{\\theta}}(s^{\\prime}\\mid s,a)=1$ . Combining the results of Eq. (78) and Eq. (81) and , we conclude the proof. \u53e3 ", "page_idx": 54}, {"type": "text", "text": "D.3 Good Events with High Probability ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "In this section, we introduce the good events used to prove Theorem 2 and show that the good events happen with high probability. ", "page_idx": 54}, {"type": "text", "text": "Lemma 17 (Good event probability). For any $K\\in\\mathbb N$ and $\\delta\\in(0,1)$ , the good event $\\mathfrak{G}(K,\\delta^{\\prime})$ holds with probability at least $1-\\delta$ where $\\delta^{\\prime}=\\delta/(2K H)$ . ", "page_idx": 54}, {"type": "text", "text": "Proof of Lemma $I7.$ . For any $\\delta^{\\prime}\\in(0,1)$ , we have ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\mathfrak{G}(K,\\delta^{\\prime})=\\bigcap_{k\\leq K}\\bigcap_{h\\leq H}\\mathfrak{G}_{k,h}(\\delta^{\\prime})=\\bigcap_{k\\leq K}\\bigcap_{h\\leq H}\\left\\{\\mathfrak{G}_{k,h}^{\\Delta}(\\delta^{\\prime})\\cap\\mathfrak{G}_{k,h}^{\\xi}(\\delta^{\\prime})\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "On the other hand, for any $(k,h)\\in[K]\\times[H]$ , by Lemma $30\\,\\mathfrak{G}_{k,h}^{\\xi}(\\delta^{\\prime})$ holds with probability at least $1-\\delta^{\\prime}$ . Then, for $\\delta^{\\prime}=\\delta/(2K H)$ by taking union bound, we have the desired result as follows: ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathfrak{G}(K,\\delta^{\\prime}))\\geq(1-\\delta^{\\prime})^{2K H}\\geq1-2K H\\delta^{\\prime}=1-\\delta\\,.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "D.4 Stochastic Optimism ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Lmeulmtipmlae  s1a8 m(pSlteo csihzaes $\\begin{array}{r}{M=\\lceil1-\\frac{\\log(H\\mathcal{U})}{\\log\\Phi(1)}\\rceil}\\end{array}$ ,n ty $\\delta$ nw fiothr $0<\\delta<\\Phi(-1)/2$ $k\\in[K]$ a, vleet $\\sigma_{k}=H\\beta_{k}(\\delta)$ . If we take ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left((\\widetilde{V}_{1}^{k}-V_{1}^{*})(s_{1}^{k})\\geq0\\mid s_{1}^{k},\\mathcal{F}_{k}\\right)\\geq\\Phi(-1)/2\\,.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Proof of Lemma 18. First, we introduce the following lemmas. ", "page_idx": 54}, {"type": "text", "text": "Lemma 19. Let $\\delta\\in(0,1)$ be given. For any $(k,h)\\in[K]\\times[H],$ , let $\\sigma_{k}=H\\beta_{k}(\\delta)$ . If we define the event $\\mathfrak{G}_{k,h}^{\\Delta}(\\delta)$ as ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{G}_{k,h}^{\\Delta}(\\delta):=\\bigg\\{|\\Delta_{h}^{k}(s,a)|\\leq H\\beta_{k}(\\delta)\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\widetilde{\\theta}_{h}^{k}}(s^{\\prime}\\mid s,a)\\left\\|\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}}\\\\ &{\\qquad\\qquad+\\left.3H\\beta_{k}(\\delta)^{2}\\displaystyle\\operatorname*{max}_{s^{\\prime}\\in S_{s,a}}\\|\\varphi_{s,a,s^{\\prime}}\\|_{\\mathbf{B}_{k,h}^{-1}}^{2}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "then conditioned on $\\mathfrak{G}_{k,h}^{\\Delta}(\\delta)$ , for any $(s,a)\\in S\\times A$ , we have ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(-\\iota_{h}^{k}(s,a)\\geq0\\ |\\ \\mathfrak{G}_{k,h}^{\\Delta}(\\delta)\\right)\\geq1-\\Phi(1)^{M}\\,.\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Lemma 20. Let $\\delta\\,\\in\\,(0,1)$ be given. For any $(h,k)\\,\\in\\,[H]\\,\\times\\,[K]$ , let $\\sigma_{k}\\,=\\,H\\beta_{k}(\\delta)$ . If we take multiple sample size $\\begin{array}{r}{M=\\lceil1-\\frac{\\log(H\\mathcal{U})}{\\log\\Phi(1)}\\rceil}\\end{array}$ , then conditioned on the event $\\mathfrak{G}_{k}^{\\Delta}(\\delta):=\\cap_{h\\in[H]}\\mathfrak{G}_{k,h}^{\\Delta}(\\delta)$ ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(-\\iota_{h}^{k}(s_{h},a_{h})\\geq0,\\forall h\\in[H]\\ |\\ \\mathfrak{G}_{k}^{\\Delta}(\\delta)\\right)\\geq\\Phi(-1)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Based on the result of Lemma 20, using the same argument as in Lemma 6 we obtain the desired result. \u53e3 ", "page_idx": 55}, {"type": "text", "text": "In the following section, we provide the proofs of the lemmas used in Lemma 18. ", "page_idx": 55}, {"type": "text", "text": "D.4.1 Proof of Lemma 19 ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Proof of Lemma 19. Recall the definition of Bellman error (Definition 1), we have ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\iota_{h}^{k}(s,a)}\\\\ &{=\\widetilde{Q}_{h}^{k}(s,a)-\\Big(r(s,a)+P_{h}\\widetilde{V}_{h+1}^{k}(s,a)\\Big)}\\\\ &{=\\operatorname*{min}\\Bigg\\{r(s,a)+\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\widetilde{\\theta}_{h}^{k}}(s^{\\prime}\\mid s,a)\\widetilde{V}_{h+1}^{k}(s^{\\prime})+\\nu_{k,h}^{\\mathrm{rand}}(s,a)\\Bigg\\}-\\Big(r(s,a)+P_{h}\\widetilde{V}_{h+1}^{k}(s,a)\\Big)}\\\\ &{\\geq\\operatorname*{min}\\Bigg\\{\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\widetilde{\\theta}_{h}^{k}}(s^{\\prime}\\mid s,a)\\widetilde{V}_{h+1}^{k}(s^{\\prime})-P_{h}\\widetilde{V}_{h+1}^{k}(s,a)+\\nu_{k,h}^{\\mathrm{rand}}(s,a),0\\Bigg\\}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Then, it is enough to show that ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\tilde{\\theta}_{h}^{k}}(s^{\\prime}\\mid s,a)\\widetilde{V}_{h+1}^{k}(s^{\\prime})-P_{h}\\widetilde{V}_{h+1}^{k}(s,a)+\\nu_{k,h}^{\\mathrm{rand}}(s,a)\\geq0\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "at least with constant probability. On the other hand, under the event $\\mathfrak{G}_{k,h}^{\\Delta}(\\delta)$ , by Lemma 16 we have ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\hat{\\boldsymbol{\\kappa}}_{h}^{k}}(s^{\\prime}\\mid s,a)\\tilde{V}_{h+1}^{k}(s^{\\prime})-P_{h}\\tilde{V}_{h+1}^{k}(s,a)+\\nu_{k,h}^{\\mathrm{rand}}(s,a)}\\\\ &{=\\Delta_{h}^{k}(s,a)+\\nu_{k,h}^{\\mathrm{rand}}(s,a)}\\\\ &{\\geq-H\\beta_{k}(\\delta)\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\hat{\\boldsymbol{\\kappa}}_{h}^{k}}(s^{\\prime}\\mid s,a)\\left\\|\\overline{{\\varphi}}_{s,a,s^{\\prime}}(\\tilde{\\theta}_{h}^{k})\\right\\|_{\\mathbf{B}_{h,h}^{-1}}}\\\\ &{\\quad-\\displaystyle\\left.3H\\beta_{k}(\\delta)^{2}\\operatorname*{max}_{s^{\\prime}\\in S_{s,a}}\\left\\|\\varphi_{s,a,s^{\\prime}}\\right\\|_{\\mathbf{B}_{h,h}^{-1}}^{2}+\\nu_{k,h}^{\\mathrm{rand}}(s,a)\\right.}\\\\ &{=\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\hat{\\boldsymbol{\\kappa}}_{h}}(s^{\\prime}\\mid s,a)\\overline{{\\varphi}}_{s,a,s^{\\prime}}(\\tilde{\\theta}_{h}^{k})^{\\top}\\xi_{k,h}^{s^{\\prime}}-H\\beta_{k}(\\delta)\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\hat{\\boldsymbol{\\kappa}}_{h}}(s^{\\prime}\\mid s,a)\\left\\|\\overline{{\\varphi}}_{s,a,s^{\\prime}}(\\tilde{\\theta}_{h}^{k})\\right\\|_{\\mathbf{B}_{h,h}^{-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Note that since $\\pmb{\\xi}_{k,h}^{(m)}\\sim\\mathcal{N}(\\mathbf{0},\\sigma_{k}^{2}\\mathbf{B}_{k,h}^{-1})$ , it follows that ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\pmb{\\theta}}_{h}^{k})^{\\top}\\pmb{\\xi}_{k,h}^{(m)}\\sim\\mathcal{N}\\left(0,\\sigma_{k}^{2}\\left\\lVert\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\pmb{\\theta}}_{h}^{k})\\right\\rVert_{{\\pmb{\\mathbf{B}}}_{k,h}^{-1}}^{2}\\right)\\,,\\quad\\forall m\\in[M]\\,.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Therefore, by setting $\\sigma_{k}=H\\beta_{k}(\\delta)$ , we have for $m\\in[M]$ and $s^{\\prime}\\in\\mathcal{S}_{s,a}$ , ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\pmb{\\theta}}_{h}^{k})^{\\top}\\pmb{\\xi}_{k,h}^{(m)}\\geq H\\beta_{k}(\\delta)\\left\\|\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\pmb{\\theta}}_{h}^{k})\\right\\|_{{\\bf B}_{k,h}^{-1}}\\right)=\\Phi(-1)\\,.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Recall that $\\pmb{\\xi}_{k,h}^{s^{\\prime}}:=\\pmb{\\xi}_{k,h}^{m(s^{\\prime})}$ where $\\begin{array}{r}{m(\\boldsymbol{s}^{\\prime}):=\\mathrm{argmax}_{m\\in[M]}\\,\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\pmb{\\theta}}_{h}^{k})^{\\top}\\pmb{\\xi}_{k,h}^{(m)}}\\end{array}$ . Then, we can deduce ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\boldsymbol{\\theta}}_{h}^{k})^{\\top}\\boldsymbol{\\xi}_{k,h}^{s^{\\prime}}\\geq H\\beta_{k}(\\delta)\\left\\lVert\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\boldsymbol{\\theta}}_{h}^{k})\\right\\rVert_{\\mathbf{B}_{k,h}^{-1}}\\right)}\\\\ &{=\\mathbb{P}\\left(\\underset{m\\in[M]}{\\operatorname*{max}}\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\boldsymbol{\\theta}}_{h}^{k})^{\\top}\\boldsymbol{\\xi}_{k,h}^{(m)}\\geq H\\beta_{k}(\\delta)\\left\\lVert\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\boldsymbol{\\theta}}_{h}^{k})\\right\\rVert_{\\mathbf{B}_{k,h}^{-1}}\\right)}\\\\ &{=1-\\mathbb{P}\\left(\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\boldsymbol{\\theta}}_{h}^{k})^{\\top}\\boldsymbol{\\xi}_{k,h}^{(m)}<H\\beta_{k}(\\delta)\\left\\lVert\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\boldsymbol{\\theta}}_{h}^{k})\\right\\rVert_{\\mathbf{B}_{k,h}^{-1}},\\forall m\\in[M]\\right)}\\\\ &{\\geq1-(1-\\Phi(-1))^{M}}\\\\ &{=1-\\Phi(1)^{M}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Consequently, we arrive at the conclusion as follows: ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(-\\boldsymbol{\\lambda}_{k}^{\\otimes}(s,\\boldsymbol{\\lambda}_{k})\\geq0)\\otimes|\\Phi_{\\hat{\\mathcal{A}}_{k}}^{\\vec{\\mathbf{A}}_{k}}(\\boldsymbol{\\lambda})|}\\\\ &{\\geq\\mathbb{P}\\left(\\frac{P_{k}}{\\sigma_{k,n}^{2}}\\sum_{\\boldsymbol{\\ell}_{k}=0}^{P_{k}}\\langle\\boldsymbol{\\lambda}_{k}^{\\vec{\\mathbf{A}}_{k}}|\\hat{\\mathcal{H}}_{k+1}^{\\vec{\\mathbf{A}}_{k}}(\\boldsymbol{s})-P_{k}\\hat{\\mathcal{H}}_{k+1}^{\\vec{\\mathbf{A}}_{k}}(\\boldsymbol{s},\\boldsymbol{\\lambda})+\\nu_{k,n}^{\\vec{\\mathbf{A}}_{k}}\\langle\\boldsymbol{s},\\boldsymbol{\\lambda}_{k}\\rangle\\geq0\\ |\\ \\mathcal{S}_{k,n}^{\\vec{\\mathbf{A}}_{k}}(\\boldsymbol{\\delta})\\right)}\\\\ &{\\geq\\mathbb{P}\\left(\\sum_{\\boldsymbol{\\nu}=\\hat{\\mathcal{U}}_{n,k}}P_{k}\\phi_{\\boldsymbol{\\nu},\\boldsymbol{\\lambda}_{k}}^{\\vec{\\mathbf{A}}_{k}}(\\boldsymbol{\\nu}^{\\prime}\\,\\boldsymbol{\\lambda}_{k})\\phi_{\\boldsymbol{\\nu},\\boldsymbol{\\lambda}_{k}}(\\boldsymbol{\\delta})\\right)^{\\top}\\xi_{k,n}^{\\prime},}\\\\ &{\\qquad\\qquad\\geq H\\beta\\lambda_{k}(\\delta)\\sum_{\\ell=0}^{P_{k}}P_{k}\\phi_{\\ell}^{(\\ell,\\ell,\\ell)}|\\left\\|\\phi_{k,n}(\\vec{\\nu}^{\\prime}\\,\\boldsymbol{\\lambda}_{k})\\right\\|_{\\mathbb{R}_{k,1}^{n}}\\ \\big|\\Theta_{\\boldsymbol{\\lambda}_{k},\\boldsymbol{\\lambda}_{k}}^{\\vec{\\mathbf{A}}_{k}}(\\boldsymbol{\\delta})\\Big)}\\\\ &{\\geq\\mathbb{P}\\left(\\phi_{k,n}\\langle\\hat{\\mathcal{U}}_{n,k}^{\\mathcal{T}}\\xi_{k,n}^{\\mathcal{T}}\\,\\mathcal{U}_{k}(\\boldsymbol{\\delta})\\Big\\|\\phi_{k,n}(\\hat{\\mathcal{U}}_{n,k})\\Big\\|_{\\mathbb{R}_{k,1}^{n}},\\mathcal{V}^{\\vec{U}}\\xi_{k}^{\\mathcal{U}}\\,\\mathcal{S}_{k,n}\\big)\\ \\Theta_{k,n}^{\\vec{\\\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where (84) comes from the fact that $\\operatorname*{max}_{s,a}|S_{s,a}|\\,=\\mathcal{U}$ and the union bound, and (85) follows by (82). \u53e3 ", "page_idx": 56}, {"type": "text", "text": "D.4.2 Proof of Lemma 20 ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Proof of Lemma 20. It holds ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}\\left(-\\iota_{h}^{k}(s_{h},a_{h})\\geq0,\\forall h\\in[H]\\right)=1-\\mathbb{P}\\left(\\exists h\\in[H]\\mathrm{~s.t.~}-\\iota_{h}^{k}(s_{h},a_{h})<0\\right)}&{}\\\\ {\\geq1-H\\mathbb{P}\\left(-\\iota_{h}^{k}(s_{h},a_{h})<0\\right)}&{}\\\\ {\\geq1-H\\mathcal{U}\\Phi(1)^{M}}&{}\\\\ {\\geq\\Phi(-1)}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where the first inequality uses the Bernoulli\u2019s inequality, the second inequality follows by Lemma 19, and the last inequality holds due to the choice of $\\begin{array}{r}{M=\\lceil1-\\frac{\\log(\\mathcal{U}H)}{\\log\\Phi(1)}\\rceil}\\end{array}$ log \u03a6(1) \u2309. \u53e3 ", "page_idx": 56}, {"type": "text", "text": "D.5 Bound on Estimation Part ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "In this section, we provide the upper bound on the estimation part of the regret: $\\begin{array}{r}{\\sum_{k=1}^{K}(\\widetilde{V}_{1}^{k}-V_{1}^{*})(s_{1}^{k})}\\end{array}$ Lemma 21 (Bound on estimation). For any $\\delta\\in(0,1)$ , $i f\\lambda=\\mathcal{O}(L_{\\varphi}^{2}d\\log\\mathcal{U}).$ , then with probability at least $1-\\delta/2$ , we have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}(\\widetilde{V}_{1}^{k}-V_{1}^{\\pi^{k}})(s_{1}^{k})=\\widetilde{\\mathcal{O}}\\left(d^{3/2}H^{3/2}\\sqrt{T}+\\kappa^{-1}d^{2}H^{2}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Proof of Lemma 21. With the same argument in Lemma 10, we have ", "page_idx": 57}, {"type": "equation", "text": "$$\n(\\widetilde V_{1}^{k}-V_{1}^{\\pi^{k}})(s_{1}^{k})=\\sum_{h=1}^{H}-\\iota_{h}^{k}(s_{h}^{k},a_{h}^{k})+\\sum_{h=1}^{H}\\dot{\\zeta}_{h}^{k}\\,,\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "where $\\begin{array}{r}{\\dot{\\zeta}_{h}^{k}:=P_{h}(\\widetilde{V}_{h+1}^{k}-V_{h+1}^{\\pi^{k}})(s_{h}^{k},a_{h}^{k})-(\\widetilde{V}_{h+1}^{k}-V_{h+1}^{\\pi^{k}})(s_{h+1}^{k}).}\\end{array}$ . Note that ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\iota_{h}^{k}(s_{h}^{k},a_{h}^{k})=\\widetilde Q_{h}^{k}(s_{h}^{k},a_{h}^{k})-\\Big(r(s_{h}^{k},a_{h}^{k})+P_{h}\\widetilde V_{h+1}^{k}(s_{h}^{k},a_{h}^{k})\\Big)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal S_{k,h}}P_{\\widetilde\\theta_{h}^{k}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\widetilde V_{h+1}^{k}(s^{\\prime})-P_{h}\\widetilde V_{h+1}^{k}(s_{h}^{k},a_{h}^{k})+\\nu_{k,h}^{\\mathrm{rand}}(s_{h}^{k},a_{h}^{k})}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\left|\\Delta_{h}^{k}(s_{h}^{k},a_{h}^{k})\\right|+\\nu_{k,h}^{\\mathrm{rand}}(s_{h}^{k},a_{h}^{k})}\\\\ &{\\quad\\quad\\quad\\quad\\leq H\\beta_{k}\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal S_{k,h}}P_{\\widetilde\\theta_{h}^{k}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\left\\|\\bar\\varphi_{k,h,s^{\\prime}}(\\widetilde\\theta_{h}^{k})\\right\\|_{\\mathbf B_{k,h}^{-1}}+3H\\beta_{k}^{2}\\displaystyle\\operatorname*{max}_{s^{\\prime}\\in\\mathcal S_{k,h}}\\|\\varphi_{k,h,s^{\\prime}}\\|_{\\mathbf B_{k,h}^{-1}}^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(s_{h,h}^{\\mathrm{rand}}(s_{h}^{k},a_{h}^{k}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "where the last inequality follows by Lemma 16. Now we introduce the following lemma. ", "page_idx": 57}, {"type": "text", "text": "Lemma 22. For any $(k,h)\\in[K]\\times[H]$ and $(s,a)\\in S\\times A,$ , it holds ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\tilde{\\theta}_{h}^{k}}(s^{\\prime}\\mid s,a)\\left\\|\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}}\\\\ &{\\displaystyle\\leq\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\tilde{\\theta}_{h}^{k+1}}(s^{\\prime}\\mid s,a)\\left\\|\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}+\\frac{16\\eta L_{\\varphi}}{\\sqrt{\\lambda}}\\operatorname*{max}_{s^{\\prime}\\in S_{s,a}}\\left\\|\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "By plugging the result of Lemma 22 into Eq. (87), we have ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-t_{h}^{k}(s_{h}^{k},a_{k}^{k})}\\\\ &{\\leq H\\beta_{h}\\underset{s^{\\prime}\\in\\mathcal{S}_{h,h}}{\\sum}P_{\\hat{\\sigma}_{h}^{k+1}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\left\\|\\bar{\\varphi}_{k,h,s^{\\prime}}(\\tilde{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}}\\\\ &{\\quad+H\\beta_{h}\\frac{16H\\sigma}{\\sqrt{\\lambda}}\\underset{s^{\\prime}\\in\\mathcal{S}_{h,h}}{\\sum}\\left\\|\\varphi_{k,h,s^{\\prime}}(\\tilde{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}^{2}+3H\\beta_{h}^{2}\\underset{s^{\\prime}\\in\\mathcal{S}_{h,h}}{\\operatorname*{max}}\\;\\|\\varphi_{k,h,s^{\\prime}}\\|_{\\mathbf{B}_{k,h}^{-1}}^{2}+\\nu_{k,h}^{\\operatorname{rand}}(s_{h}^{k},a_{h}^{k})}\\\\ &{\\leq H\\beta_{h}\\underset{s^{\\prime}\\in\\mathcal{S}_{h,h}}{\\sum}P_{\\hat{\\sigma}_{h}^{k+1}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\left\\|\\bar{\\varphi}_{k,h,s^{\\prime}}(\\tilde{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}}\\\\ &{\\quad+\\underset{s^{\\prime}\\in\\mathcal{S}_{h,h}}{\\sum}P_{\\hat{\\sigma}_{h}^{k}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\bar{\\varphi}_{k,h,s^{\\prime}}(\\tilde{\\theta}_{h}^{k})^{\\top}\\xi_{k,h}^{\\ast}}\\\\ &{\\quad+H\\beta_{h}\\frac{16H\\sigma L_{\\varphi}}{\\sqrt{\\lambda}}\\underset{s^{\\prime}\\in\\mathcal{S}_{h,h}}{\\operatorname*{max}}\\;\\Big\\|\\bar{\\varphi}_{k,h,s^{\\prime}}(\\tilde{\\theta}_{h}^{k+1})\\Big\\|_{\\mathbf{B}_{h,h}^{-1}}^{2}+6H\\beta_{h}^{2}\\underset{s^{\\prime}\\in\\mathcal{S}_{h \n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "By letting us denote ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\Upsilon_{h}^{k}(s,a):=H\\beta_{k}\\frac{16\\eta L_{\\varphi}}{\\sqrt{\\lambda}}\\operatorname*{max}_{s^{\\prime}\\in\\mathcal{S}_{s,a}}\\left\\Vert\\overline{{\\varphi}}_{s,a,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\right\\Vert_{\\mathbf{B}_{k,h}^{-1}}^{2}+6H\\beta_{k}^{2}\\operatorname*{max}_{s^{\\prime}\\in\\mathcal{S}_{s,a}}\\Vert\\varphi_{s,a,s^{\\prime}}\\Vert_{\\mathbf{B}_{k,h}^{-1}}^{2},\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "and summing over all episodes, we have ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=1}^{K}(\\widetilde{V}_{1}^{k}-V_{1}^{\\pi^{k}})(s_{1}^{k})=\\displaystyle\\sum_{k=1}^{K}\\sum_{h=1}^{H}-{}_{h}^{k}(s_{h}^{k},a_{h}^{k})+\\displaystyle\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\dot{\\zeta}_{h}^{k}}\\\\ &{\\quad\\qquad\\qquad\\qquad\\qquad\\leq H\\beta_{K}\\displaystyle\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\sum_{s^{\\prime}\\in\\mathcal{S}_{k,h}}P_{\\widetilde{\\theta}_{h}^{k+1}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\left\\|\\Bar{\\varphi}_{k,h,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "equation", "text": "$$\n+\\underbrace{\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\Upsilon_{h}^{k}(s_{h}^{k},a_{h}^{k})}_{\\mathrm{(iii)}}+\\underbrace{\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\dot{\\zeta}_{h}^{k}}_{\\mathrm{(iv)}}\\ .\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Note that $\\begin{array}{r}{\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\sum_{s^{\\prime}\\in S_{k,h}}}\\end{array}$ is hereafter abbreviated as $\\sum_{k,h,s^{\\prime}}$ ", "page_idx": 58}, {"type": "text", "text": "For term (i), we have ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{k,h,s^{\\prime}}{\\sum}P_{\\widehat{\\theta}_{h}^{k+1}}\\big(s^{\\prime}\\ |\\ s_{h}^{k},a_{h}^{k}\\big)\\left\\|\\bar{\\varphi}_{k,h,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}}\\\\ &{\\leq\\sqrt{\\underset{k,h,s^{\\prime}}{\\sum}P_{\\widehat{\\theta}_{h}^{k+1}}\\big(s^{\\prime}\\ |\\ s_{h}^{k},a_{h}^{k}\\big)}\\sqrt{\\underset{k,h,s^{\\prime}}{\\sum}P_{\\widehat{\\theta}_{h}^{k+1}}\\big(s^{\\prime}\\ |\\ s_{h}^{k},a_{h}^{k}\\big)\\left\\|\\bar{\\varphi}_{k,h,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}^{2}}}\\\\ &{=\\sqrt{T}\\sqrt{\\underset{h=1}{\\overset{H}{\\sum}}\\sum_{k=1}^{K}\\underset{s^{\\prime}\\in S_{k,h}}{\\sum}P_{\\widehat{\\theta}_{h}^{k+1}}\\big(s^{\\prime}\\ |\\ s_{h}^{k},a_{h}^{k}\\big)\\left\\|\\bar{\\varphi}_{k,h,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}^{2}}}\\\\ &{\\leq\\sqrt{T}\\sqrt{2H d\\log\\left(1+\\frac{K M L_{\\varphi}^{2}}{d\\lambda}\\right)}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "where the last inequality follows by the following lemma: ", "page_idx": 58}, {"type": "text", "text": "Lemma 23. For each $h\\in[H],\\,i f\\,\\lambda\\geq L_{\\varphi}^{2}$ , then we have ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\sum_{s^{\\prime}\\in S_{k,h}}P_{\\widetilde{\\theta}_{h}^{k+1}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\left\\|\\bar{\\varphi}_{k,h,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}^{2}\\le2d\\log\\left(1+\\frac{K\\mathcal{U}L_{\\varphi}^{2}}{d\\lambda}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Then, term (i) can be bounded as follows: ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left(\\mathrm{i}\\right)=H\\beta_{K}\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\sum_{s^{\\prime}\\in S_{k,h}}P_{\\tilde{\\theta}_{h}^{k+1}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\left\\Vert\\bar{\\varphi}_{k,h,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\right\\Vert_{\\mathbf{B}_{k,h}^{-1}}}\\\\ {\\displaystyle\\qquad\\leq H\\beta_{K}\\sqrt{T}\\sqrt{2H d\\log\\left(1+\\frac{K\\mathcal{U}L_{\\varphi}^{2}}{d\\lambda}\\right)}}\\\\ {\\displaystyle=\\widetilde{O}(d H^{3/2}\\sqrt{T})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "For term (ii), we introduce the following lemma: ", "page_idx": 58}, {"type": "text", "text": "Lemma 24. Let $\\delta\\in(0,1)$ be given. For any $(k,h)\\in[K]\\times[H]$ and $(s,a)\\in S\\times A$ , with probability at least $1-\\delta$ , it holds ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\widetilde{\\theta}_{h}^{k}}(s^{\\prime}\\mid s,a)\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k})^{\\top}\\xi_{k,h}^{s^{\\prime}}}\\\\ &{\\le\\displaystyle\\gamma_{k}(\\delta)\\biggr(\\displaystyle\\sum_{s^{\\prime}\\in S_{k,h}}P_{\\widetilde{\\theta}_{h}^{k+1}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\left\\|\\bar{\\varphi}_{k,h,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}}\\\\ &{\\displaystyle\\qquad\\qquad+\\left.\\frac{16\\eta L\\varphi}{\\sqrt{\\lambda}}\\displaystyle\\operatorname*{max}_{s^{\\prime}\\in S_{k,h}}\\left\\|\\bar{\\varphi}_{k,h,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}^{2}\\biggr)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "where $\\gamma_{k}(\\delta):=C_{\\xi}\\sigma_{k}\\sqrt{d\\log(M d/\\delta)}$ for an absolute constant $C_{\\xi}>0$ . ", "page_idx": 59}, {"type": "text", "text": "By Lemma 24, we have ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k,b,s^{\\prime}}P_{\\tilde{\\theta}_{h}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\bar{\\varphi}_{k,h,s^{\\prime}}(\\tilde{\\theta}_{h}^{k})^{\\top}\\xi_{k,h}^{s^{\\prime}}}\\\\ &{\\le\\gamma_{K}(\\delta)\\biggr(\\displaystyle\\sum_{k,h,s^{\\prime}}P_{\\tilde{\\theta}_{h}^{k+1}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\left\\|\\bar{\\varphi}_{k,h,s^{\\prime}}(\\tilde{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}}\\\\ &{\\qquad\\qquad+\\displaystyle\\frac{16\\eta L_{\\varphi}}{\\sqrt{\\lambda}}\\displaystyle\\sum_{k=1}^{K}\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}_{k,h}}^{H}\\left\\|\\bar{\\varphi}_{k,h,s^{\\prime}}(\\tilde{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}\\biggr)}\\\\ &{\\leq\\gamma_{K}(\\delta)\\biggr(\\sqrt{T}\\sqrt{2H d\\log\\left(1+\\displaystyle\\frac{K\\mathcal{U}L_{\\varphi}^{2}}{d\\lambda}\\right)}+\\displaystyle\\frac{16\\eta L_{\\varphi}}{\\sqrt{\\lambda}}\\displaystyle\\sum_{k=1}^{K}\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}_{k,h}}^{H}\\operatorname*{max}_{s^{\\prime}\\in\\mathcal{S}_{h}}\\left\\|\\bar{\\varphi}_{k,h,s^{\\prime}}(\\tilde{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}^{2}\\biggr)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "where the last inequality follows by Eq. (90). Note that ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\displaystyle\\sum_{t=1}^{N}\\sum_{t=1}^{N}\\sum_{t=1}^{N}\\sum_{t=1}^{N}\\left|\\phi_{t,h,t}\\right|\\left|\\phi_{t,h,t}\\right|\\left|\\phi_{t,h,t}^{-1}\\right|\\right|\\right|_{h,t}^{2}}\\\\ &{\\le\\displaystyle\\sum_{t=1}^{N}\\sum_{t=1}^{N}\\sum_{t=1}^{N}\\left|\\phi_{t,h,t}\\right|\\left|\\phi_{t,h,t}\\right|\\left|\\phi_{t+1}^{-1}\\right|\\displaystyle\\sum_{s=1}^{N}\\sum_{t=1}^{N}}\\\\ &{=\\displaystyle\\sum_{t=1}^{N}\\sum_{t=1}^{N}\\sum_{t=1}^{N}\\sum_{t=1}^{N}\\left|\\phi_{s,t}\\right|\\left|\\phi_{s,t}-\\sum_{t=1}^{N}\\sum_{t=1}^{N}\\sum_{t=1}^{t}\\left|\\phi_{t}+(\\lambda)\\right|\\sum_{s=1}^{N}\\sum_{t=1}^{N}\\phi_{t,h,t}\\right|\\left|\\phi_{t,h,t}\\right|\\left|\\right|_{h,t}^{2}}\\\\ &{\\leq\\displaystyle\\sum_{t=1}^{N}\\sum_{t=1}^{N}\\sum_{t=1}^{N}\\sum_{t=1}^{N}\\left(2|\\phi_{t,h,t}|\\sum_{s=1}^{N}+2\\left|\\sum_{t=1}^{N}\\sum_{t=1}^{N}\\sum_{t=1}^{N}\\phi_{t,h,t}^{-1}\\right|(\\lambda)\\right|\\phi_{t,h,t}\\right|\\left|\\phi_{t,h,t}\\right|\\right)\\Bigg|_{h,t}\\leq\\displaystyle\\sum_{t=1}^{N}\\sum_{t=1}^{N}\\sum_{t=1}^{N}\\sum_{t=1}^{N}\\sum_{t=1}^{N}\\phi_{t,h,t}^{-1}}\\\\ &{\\leq2\\displaystyle\\sum_{t=1}^{N}\\sum_{t=1}^{N}\\sum_{t=1}^{N}\\phi_{t,h,t}^{-1}\\left|\\phi_{s,t}\\right|\\left|\\phi_{s,t}\\right|\\displaystyle\\sum_{s=1}^{N}+2\\sum_{t=1}^{N}\\sum_{t=1}^{N}\\sum_{t=1}^{N}\\phi_{t,s}^{-1}+(\\lambda)\\left|\\phi_{s,t}^{-1}\\right|\\left|\\phi_{s,t}\\right\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "where the first inequality holds since $\\mathbf{B}_{k,h}^{-1}\\preceq\\mathbf{A}_{k,h}^{-1}$ , the second inequality follows from $(x+y)^{2}\\leq$ $2x^{2}+2y^{2}$ , and the third inequality uses the triangle inequality, and the fourth inequality uses $\\textstyle\\sum_{\\widetilde{s}\\in{\\cal S}_{k,h}}\\!\\stackrel{\\cdot}{P}_{\\widetilde{\\theta}_{h}^{k+1}}(\\widetilde{s}\\ |\\ \\,s_{h}^{k},a_{h}^{k})\\;\\stackrel{\\cdot}{=}\\;1$ , and the last inequality follows by Lemma 3. By substituting ", "page_idx": 59}, {"type": "text", "text": "Eq. (93) into Eq. (92), we have ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathrm{(ii)}\\le\\gamma_{K}(\\delta)\\biggr(\\sqrt{T}\\sqrt{2H d\\log\\left(1+K\\mathcal{U L}_{\\varphi}^{2}/(d\\lambda)\\right)}+\\frac{256\\eta L\\varphi}{\\sqrt{\\lambda}}\\kappa^{-1}d H\\log\\left(1+K\\mathcal{U L}_{\\varphi}^{2}/(d\\lambda)\\right)\\biggr)}\\\\ &{\\qquad=\\widetilde{\\mathcal{O}}(d^{3/2}H^{3/2}\\sqrt{T}+\\kappa^{-1}d^{3/2}H^{2})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "For term (iii), ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{k=1}^{K}\\displaystyle\\sum_{h=1}^{H}\\Upsilon_{h}^{k}(s_{h}^{k},a_{h}^{k})}\\\\ {\\displaystyle}&{=\\displaystyle\\sum_{k=1}^{K}\\displaystyle\\sum_{h=1}^{H}\\left(H\\beta_{k}\\frac{16\\eta L\\varphi}{\\sqrt{\\lambda}}\\operatorname*{max}_{s^{\\prime}\\in\\mathcal{S}_{k,h}}\\left\\|\\bar{\\varphi}_{k,h,s^{\\prime}}(\\tilde{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}^{2}+6H\\beta_{k}^{2}\\operatorname*{max}_{s^{\\prime}\\in\\mathcal{S}_{k,h}}\\|\\varphi_{k,h,s^{\\prime}}\\|_{\\mathbf{B}_{k,h}^{-1}}^{2}\\right)}\\\\ {\\displaystyle}&{\\leq H\\beta_{K}\\frac{16\\eta L\\varphi}{\\sqrt{\\lambda}}\\displaystyle\\sum_{k=1}^{K}\\displaystyle\\sum_{h=1}^{H}\\operatorname*{max}_{s^{\\prime}\\in\\mathcal{S}_{k,h}}\\left\\|\\bar{\\varphi}_{k,h,s^{\\prime}}(\\tilde{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}^{2}+6H\\beta_{K}^{2}\\displaystyle\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\operatorname*{max}_{s^{\\prime}\\in\\mathcal{S}_{k,h}}\\|\\varphi_{k,h,s^{\\prime}}\\|_{\\mathbf{A}_{k,h}^{-1}}^{2}}\\\\ {\\displaystyle}&{\\leq\\beta\\kappa\\frac{256\\eta L\\varphi}{\\sqrt{\\lambda}}\\kappa^{-1}d H^{2}\\log\\left(1+K\\mathcal{U}L_{\\varphi}^{2}/(d\\lambda)\\right)+24\\kappa^{-1}d H^{2}\\beta_{K}^{2}\\log\\left(1+K\\mathcal{U}L_{\\varphi}^{2}/(d\\lambda)\\right)}\\\\ &{=\\tilde{O}(\\kappa^{-1}d^{2}H^{2})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "where for the second inequality we use the same argument used to derive Eq. (93) and Lemma 3. ", "page_idx": 60}, {"type": "text", "text": "For term (iv), since we have $|\\dot{\\zeta}_{h}^{k}|\\leq2H$ and $\\mathbb{E}[\\dot{\\zeta}_{h}^{k}\\ |\\ \\mathcal{F}_{k,h}]=0$ , which means $\\{\\dot{\\zeta}_{h}^{k}\\ |\\ \\mathcal{F}_{k,h}\\}_{k,h}$ is a martingale difference sequence for any $k\\,\\in\\,[K]$ and $h\\in[H]$ . Hence, by applying the AzumaHoeffding inequality with probability at least $1-\\delta/4$ , we have ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\dot{\\zeta}_{h}^{k}\\leq2H\\sqrt{2K H\\log(4/\\delta)}\\,.\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Combining all results of Eq. (91), (94), (95), and (96), we have the desired result. ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{}}&{{}}&{{\\displaystyle\\sum_{k=1}^{K}(\\widetilde{V}_{1}^{k}-V_{1}^{\\pi^{k}})(s_{1}^{k})=\\widetilde{\\mathcal{O}}(d H^{3/2}\\sqrt{T}+d^{3/2}H^{3/2}\\sqrt{T}+\\kappa^{-1}d^{3/2}H^{2}+\\kappa^{-1}d^{2}H^{2}+H\\sqrt{T})}}\\\\ {{}}&{{}}&{{}}\\\\ {{}}&{{}}&{{=\\widetilde{\\mathcal{O}}(d^{3/2}H^{3/2}\\sqrt{T}+\\kappa^{-1}d^{2}H^{2})\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "In the following, we provide the proof of the lemmas used in Lemma 21. ", "page_idx": 60}, {"type": "text", "text": "D.5.1 Proof of Lemma 22 ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Proof of Lemma 22. Note that ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}_{n_{+}}}P_{\\Phi_{k}}(s^{\\prime}\\mid s,a)\\left\\|\\widehat{\\varphi}_{s,a^{\\prime}}(\\widehat{\\theta}_{h}^{k})\\right\\|_{\\mathbf{B}_{\\lambda}^{-1}}}\\\\ &{\\leq\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}_{n_{+}}}P_{\\Phi_{k}}(s^{\\prime}\\mid s,a)\\left\\|\\widehat{\\varphi}_{s,a^{\\prime}}(\\widehat{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{\\lambda}^{-1}}}\\\\ &{\\quad+\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}_{n_{+}}}P_{\\Phi_{k}}(s^{\\prime}\\mid s,a)\\left\\|\\widehat{\\varphi}_{s,a^{\\prime}}(\\widehat{\\theta}_{h}^{k})-\\widehat{\\varphi}_{s,a^{\\prime}}(\\widehat{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{\\lambda}^{-1}}}\\\\ &{\\leq\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}_{n_{+}}}P_{\\Phi_{k}+1}(s^{\\prime}\\mid s,a)\\left\\|\\widehat{\\varphi}_{s,a^{\\prime}}(\\widehat{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{\\lambda}^{-1}}}\\\\ &{\\quad+\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}_{n_{+}}}\\left(P_{\\widehat{\\theta}_{h}^{k}}(s^{\\prime}\\mid s,a)-P_{\\widehat{\\theta}_{h}^{k+1}}(s^{\\prime}\\mid s,a)\\right)\\left\\|\\widehat{\\varphi}_{s,a^{\\prime}}(\\widehat{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{\\lambda}^{-1}}}\\\\ &{\\quad+\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}_{n_{+}}}P_{\\Phi_{k}}(s^{\\prime}\\mid s,a)\\left\\|\\widehat{\\varphi}_{s,a^{\\prime}}(\\widehat{\\theta}_{h}^{k})-\\widehat{\\varphi}_{s,a^{\\prime}}(\\widehat{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{\\lambda}^{-1}}}\\\\ &{\\quad+\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}_{n_{+}}}P_{\\Phi_{k}}(s^{\\prime}\\mid s,a)\\left\\|\\widehat{\\varphi}_{s \n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "where the first inequality holds by triangle inequality. ", "page_idx": 61}, {"type": "text", "text": "For (i), we have ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{(i)}=}&{\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}\\nabla P_{\\vartheta_{h}^{k}}(s^{\\prime}\\mid s,a)^{\\top}(\\widetilde{\\theta}_{h}^{k}-\\widetilde{\\theta}_{h}^{k+1})\\left\\|\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}}\\\\ &{\\quad\\leq\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}\\|\\nabla P_{\\vartheta_{h}^{k}}(s^{\\prime}\\mid s,a)\\|_{\\mathbf{B}_{k,h}^{-1}}\\left\\|\\widetilde{\\theta}_{h}^{k}-\\widetilde{\\theta}_{h}^{k+1}\\right\\|_{\\mathbf{B}_{k,h}}\\left\\|\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "where in the equality we apply the mean value theorem with $\\pmb{\\vartheta}_{h}^{k}=v\\widetilde{\\pmb{\\theta}}_{h}^{k}+(1-v)\\widetilde{\\pmb{\\theta}}_{h}^{k+1}$ for some $v\\in[0,1]$ , and the inequality follows by Cauchy-Schwarz inequality. M eanwhile, since we have ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{r l}&{P_{\\theta_{1}|}(x^{t}|\\cdot|\\mathbf{x},\\omega)\\biggl(\\phi_{1,x}\\!+\\!\\sum_{s^{\\prime}=0}^{\\infty}P_{\\phi_{1}}(x^{t}|\\cdot|\\mathbf{x},a)\\phi_{\\theta_{2},x}\\!-\\!\\phi_{1}^{(s+1)}\\!\\biggr)}\\\\ &{=P_{\\theta_{1}}(x^{t}|s,a)\\biggl(\\phi_{1,x}\\!-\\!\\sum_{s^{\\prime}=0}^{\\infty}P_{\\phi_{1}}(x^{t}|s,a)\\phi_{\\theta_{2},x}\\!+\\!\\sum_{s^{\\prime}=1}^{\\infty}P_{\\phi_{2}}(x^{t}|s,a)\\phi_{\\theta_{3},x}\\!-\\!\\sum_{s^{\\prime}=1}^{\\infty}P_{\\phi_{1}}(x^{t}|s,a)\\phi_{\\theta_{3},x}\\!-\\!\\sum_{s^{\\prime}=1}^{\\infty}P_{\\phi_{1}}(x^{t}|s,a)\\phi_{\\theta_{3},x}\\!-\\!P_{\\phi_{1}}(x^{t}|\\cdot|\\mathbf{x},a)\\biggr)}\\\\ &{\\quad-\\sum_{s^{\\prime}=0}^{\\infty}P_{\\phi_{1}}(x^{t}|s,a)\\biggl[\\phi_{1,x}\\!-\\!\\sum_{s^{\\prime}=1}^{\\infty}P_{\\phi_{1}}(x^{t}|s,a)\\biggl]\\!-\\!P_{\\phi_{1}}(x^{t}|s,a)\\phi_{\\theta_{2},x}\\!\\biggr]\\Biggr)}\\\\ &{=P_{\\phi_{1}}(x^{t}|\\cdot|s,a)\\phi_{1,x}\\!-\\!P_{\\phi_{1}}(x^{t}|\\cdot|s,a)\\sum_{s^{\\prime}=1}^{\\infty}P_{\\phi_{1}}(x^{t}|s,a)\\phi_{\\theta_{2},x}\\!+\\!P_{\\phi_{1}}(x^{t}|\\cdot|s,a)\\phi_{\\theta_{3},x}\\!-\\!P_{\\phi_{1}}(x^{t}|\\cdot|s,a)\\psi_{3,x}\\!-\\!P_{\\phi_{1}}(x^{t}|\\cdot|s,a)\\psi_{3,x}\\!-\\!P_{\\phi_{1}}(x^{t}|\\cdot|s,a)\\psi_{3,x}\\!-\\!P_{\\phi_{1}}(x \n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "by substituting (98) into (97) we have ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{s^{\\prime}\\in{\\mathcal E}_{s,a}}\\left\\{\\left\\|P_{\\Phi_{h}^{k}}(s^{\\prime}\\mid s,a)\\widehat{\\varphi}_{s,a^{\\prime}}(\\tilde{\\theta}_{h}^{k+1})\\right.\\right.}}\\\\ &{=\\left.\\left.-P_{\\Phi_{h}^{k}}(s^{\\prime}\\mid s,a)\\sum_{s^{\\prime}\\in{\\mathcal E}_{s,a}}P_{\\Phi_{h}^{k}}(s^{\\prime\\prime}\\mid s,a)\\widehat{\\varphi}_{s,a,s^{\\prime\\prime}}(\\tilde{\\theta}_{h}^{k+1})\\right\\|_{{\\mathbb R}_{s,h}^{-1}}\\right.}\\\\ &{\\qquad\\qquad\\left.-\\left.\\left\\|\\tilde{\\theta}_{h}^{k}-\\tilde{\\theta}_{h}^{k+1}\\right\\|_{{\\mathbb R}_{h,h}^{k}}\\right\\|\\widehat{\\varphi}_{s,a,s^{\\prime}}(\\tilde{\\theta}_{h}^{k+1})\\right\\|_{{\\mathbb R}_{h,h}^{-1}}\\right\\}}\\\\ &{\\leq\\sum_{s^{\\prime}\\in{\\mathcal E}_{s,a}}P_{\\Phi_{h}^{k}}(s^{\\prime}\\mid s,a)\\left\\|\\widehat{\\varphi}_{s,a,s^{\\prime}}(\\tilde{\\theta}_{h}^{k+1})\\right\\|_{{\\mathbb R}_{h,h}^{-1}}\\left\\|\\tilde{\\theta}_{h}^{k}-\\tilde{\\theta}_{h}^{k+1}\\right\\|_{{\\mathbb R}_{h,h}}}\\\\ &{\\qquad+\\left.\\left(\\sum_{s^{\\prime}\\in{\\mathcal E}_{s,a}}P_{\\Phi_{h}^{k}}(s^{\\prime}\\mid s,a)\\left\\|\\widehat{\\varphi}_{s,a,s^{\\prime}}(\\tilde{\\theta}_{h}^{k+1})\\right\\|_{{\\mathbb R}_{h,h}^{-1}}\\right)^{2}\\left\\|\\tilde{\\theta}_{h}^{k}-\\tilde{\\theta}_{h}^{k+1}\\right\\|_{{\\mathbb R}_{h,h}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Note that by Jensen\u2019s inequality, we have ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\bigg(\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\vartheta_{h}^{k}}(s^{\\prime}\\,|\\,s,a)\\left\\lVert\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\right\\rVert_{\\mathbf{B}_{k,h}^{-1}}\\bigg)^{2}=\\bigg(\\mathbb{E}_{s^{\\prime}\\sim P_{\\vartheta_{h}^{k}}(\\cdot\\,|\\,s,a)}\\left[\\left\\lVert\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\right\\rVert_{\\mathbf{B}_{k,h}^{-1}}\\right]\\bigg)^{2}}\\\\ &{\\leq\\mathbb{E}_{s^{\\prime}\\sim P_{\\vartheta_{h}^{k}}(\\cdot\\,|\\,s,a)}\\left[\\left\\lVert\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\right\\rVert_{\\mathbf{B}_{k,h}^{-1}}^{2}\\right]}\\\\ &{=\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\vartheta_{h}^{k}}(s^{\\prime}\\,|\\,s,a)\\left\\lVert\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\right\\rVert_{\\mathbf{B}_{k,h}^{-1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Also, we introduce the following lemma: ", "page_idx": 62}, {"type": "text", "text": "Lemma 25. For any $k\\in[K]$ and $h\\in[H].$ , the following holds: ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\left\\|\\widetilde{\\pmb{\\theta}}_{h}^{k+1}-\\widetilde{\\pmb{\\theta}}_{h}^{k}\\right\\|_{{\\bf B}_{k,h}}\\leq\\frac{4\\eta L_{\\varphi}}{\\sqrt{\\lambda}}\\,.\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Then, substituting (100) into (99), we have ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{(i)}\\leq2\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\vartheta_{h}^{k}}(s^{\\prime}\\mid s,a)\\left\\|\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}^{2}\\left\\|\\widetilde{\\theta}_{h}^{k}-\\widetilde{\\theta}_{h}^{k+1}\\right\\|_{\\mathbf{B}_{k,h}}}\\\\ &{\\quad\\leq\\displaystyle\\frac{8\\eta L_{\\varphi}}{\\sqrt{\\lambda}}\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\vartheta_{h}^{k}}(s^{\\prime}\\mid s,a)\\left\\|\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}^{2}}\\\\ &{\\quad\\leq\\displaystyle\\frac{8\\eta L_{\\varphi}}{\\sqrt{\\lambda}}\\displaystyle\\operatorname*{max}_{s^{\\prime}\\in S_{s,a}}\\left\\|\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "where the second inequality comes from Lemma 25, and the last inequality holds due to $\\begin{array}{r}{\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\vartheta_{h}^{k}}(s^{\\prime}\\mid s,a)\\stackrel{.}{=}1}\\end{array}$ . ", "page_idx": 62}, {"type": "text", "text": "For (ii), we have ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\overline{{h}})=\\displaystyle{\\sum_{s\\in\\mathcal{E}_{s}}P_{\\alpha_{k}}\\big(\\mathcal{E}^{\\tau}\\big|\\mathcal{S}^{\\tau},\\boldsymbol{u}\\big)}\\left\\|\\mathcal{P}_{\\alpha_{k},\\varepsilon}\\big(\\widehat{\\theta}_{h}^{h}\\big)-\\mathcal{P}_{\\alpha_{k},\\varepsilon}\\big(\\widehat{\\theta}_{h}^{h+1}\\big)\\right\\|_{\\mathbf{B}_{h}^{-1}}}\\\\ &{\\quad=\\displaystyle{\\sum_{s\\in\\mathcal{E}_{s,h}}P_{\\alpha_{k}}\\big(\\mathcal{E}^{\\tau}\\big|\\mathcal{S}^{\\tau},\\alpha_{k}\\big)}\\left\\|\\mathcal{E}_{\\xi_{h}\\sim\\mathcal{E}_{h}^{\\tau}(\\cdot\\vert\\times\\delta_{h})}\\left[\\mathcal{P}_{\\alpha_{k},\\varepsilon}\\big]-\\mathbb{E}_{\\xi_{h}\\sim\\mathcal{E}_{h}^{\\tau}(\\cdot\\vert\\times\\delta_{h})}\\left[\\mathcal{P}_{\\alpha_{k},\\varepsilon}\\right]\\right\\|_{\\mathbf{B}_{h}^{-1}}}\\\\ &{\\quad=\\displaystyle{\\left\\|\\sum_{s\\in\\mathcal{E}_{s,h}}\\left(P_{\\alpha_{k}}(\\widehat{s}\\,\\vert\\,s,\\alpha)-P_{\\alpha_{k}+1}^{\\tau}(\\widehat{s}\\,\\vert\\,s,\\alpha)\\right)\\varphi_{s,h}\\right\\|_{\\mathbf{B}_{h}^{-1}}}}\\\\ &{\\quad=\\displaystyle{\\left\\|\\sum_{s\\in\\mathcal{E}_{s,h}}\\left(P_{\\alpha_{k}}(\\widehat{s}\\,\\vert\\,s,\\alpha)-P_{\\alpha_{k}+1}^{\\tau}(\\widehat{s}\\,\\vert\\,s,\\alpha)\\right)\\left(\\varphi_{s,h}\\mathcal{E}^{\\tau}-\\mathbb{E}_{\\xi_{h}^{\\tau}\\sim P_{\\alpha_{k}+1}^{\\tau}(\\cdot\\vert\\,s,\\alpha)}\\left[\\varphi_{s,h},\\alpha_{k}\\right]\\right)\\right\\|_{\\mathbf{B}_{h}^{-1}}}}\\\\ &{\\quad=\\displaystyle{\\left\\|\\sum_{s\\in\\mathcal{E}_{s,h}}\\left(P_{\\alpha_{k}}(\\widehat{s}\\,\\vert\\,s,\\alpha)-P_{\\beta_{k}+1}^{\\tau}(\\widehat{s}\\,\\vert\\,s\\,)\\right)\\varphi_{s,s,h}(\\widehat{\\theta}_{h}^{h+1}\\left\\vert\\,s\\right)\\right\\|_{\\mathbf{B}_{h \n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "where the last inequality is obtained through the same argument as used to bound (i). Combining the results of Eq. (101) and Eq. (102), we have ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\widetilde{\\theta}_{h}^{k}}(s^{\\prime}\\mid s,a)\\left\\|\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}}}\\\\ &{\\leq\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\widetilde{\\theta}_{h}^{k+1}}(s^{\\prime}\\mid s,a)\\left\\|\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}+\\frac{16\\eta L_{\\varphi}}{\\sqrt{\\lambda}}\\operatorname*{max}_{s^{\\prime}\\in S_{s,a}}\\left\\|\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}^{2}}\\\\ &{\\leq\\displaystyle\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\widetilde{\\theta}_{h}^{k+1}}(s^{\\prime}\\mid s,a)\\left\\|\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}+\\frac{16\\eta L_{\\varphi}}{\\sqrt{\\lambda}}\\operatorname*{max}_{s^{\\prime}\\in S_{s,a}}\\left\\|\\bar{\\varphi}_{s,a,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "D.5.2 Proof of Lemma 23 ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Proof of Lemma 23. Note that ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\bf B}_{k+1,h}={\\bf B}_{k,h}+\\sum_{s^{\\prime}\\in{\\cal S}_{k,h}}P_{\\tilde{\\theta}_{h}^{k+1}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\bar{\\varphi}_{k,h,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\bar{\\varphi}_{k,h,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})^{\\top}}\\\\ &{\\quad\\quad\\quad={\\bf B}_{k,h}+\\sum_{s^{\\prime}\\in{\\cal S}_{k,h}}\\widetilde{\\varphi}_{k,h,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\widetilde{\\varphi}_{k,h,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})^{\\top}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "where we define $\\widetilde{\\varphi}_{k,h,s^{\\prime}}(\\widetilde{\\pmb{\\theta}}_{h}^{k+1}):=\\sqrt{P_{\\widetilde{\\pmb{\\theta}}_{h}^{k+1}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})}\\bar{\\varphi}_{k,h,s^{\\prime}}(\\widetilde{\\pmb{\\theta}}_{h}^{k+1})$ . Then, we have ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{det}({\\bf B}_{k+1,h})=\\operatorname*{det}({\\bf B}_{k,h})\\operatorname*{det}\\left({\\bf I}_{d}+{\\bf B}_{k,h}^{-\\frac{1}{2}}\\sum_{s^{\\prime}\\in{\\cal S}_{k,h}}\\widetilde{\\varphi}_{k,h,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\widetilde{\\varphi}_{k,h,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})^{\\top}{\\bf B}_{k,h}^{-\\frac{1}{2}}\\right)}\\\\ &{\\quad\\quad\\quad=\\operatorname*{det}({\\bf B}_{k,h})\\left(1+\\displaystyle\\sum_{s^{\\prime}\\in{\\cal S}_{k,h}}\\Big\\lVert\\widetilde{\\varphi}_{k,h,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\Big\\rVert_{{\\bf B}_{k,h}^{-1}}^{2}\\right)}\\\\ &{\\quad\\quad\\quad=\\operatorname*{det}(\\lambda{\\cal M})\\displaystyle\\prod_{k=1}^{K}\\left(1+\\displaystyle\\sum_{s^{\\prime}\\in{\\cal S}_{k,h}}\\Big\\lVert\\widetilde{\\varphi}_{k,h,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\Big\\rVert_{{\\bf B}_{k,h}^{-1}}^{2}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "Taking the logarithm on both sides yields ", "page_idx": 63}, {"type": "equation", "text": "$$\n\\log\\frac{\\mathrm{det}(\\mathbf{B}_{k+1,h})}{\\mathrm{det}(\\lambda\\mathbf{I}_{d})}=\\sum_{k=1}^{K}\\log\\left(1+\\sum_{s^{\\prime}\\in{\\cal S}_{k,h}}\\left\\|\\widetilde{\\varphi}_{k,h,s^{\\prime}}(\\widetilde{\\pmb{\\theta}}_{h}^{k+1})\\right\\|_{{\\bf B}_{k,h}^{-1}}^{2}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 63}, {"type": "text", "text": "On the other hand, since $\\lambda\\geq L_{\\varphi}^{2}$ , ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{s^{\\prime}\\in S_{k,h}}\\left\\|\\widetilde{\\varphi}_{k,h,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}^{2}\\leq\\displaystyle\\sum_{s^{\\prime}\\in S_{k,h}}\\frac{1}{\\lambda}\\left\\|\\widetilde{\\varphi}_{k,h,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\right\\|_{2}^{2}}\\\\ {\\displaystyle=\\displaystyle\\sum_{s^{\\prime}\\in S_{k,h}}\\frac{1}{\\lambda}P_{\\widetilde{\\theta}_{h}^{k+1}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\left\\|\\overline{{\\varphi}}_{k,h,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\right\\|_{2}^{2}}\\\\ {\\displaystyle}&{\\leq\\displaystyle\\frac{L_{\\varphi}^{2}}{\\lambda}\\sum_{s^{\\prime}\\in S_{k,h}}P_{\\widetilde{\\theta}_{h}^{k+1}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})}\\\\ {\\displaystyle}&{\\leq1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "where the last inequality uses $\\textstyle\\sum_{s^{\\prime}\\in{\\cal S}_{k,h}}P_{\\widetilde{\\pmb{\\theta}}_{h}^{k+1}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})=1$ . From the fact that $z\\leq2\\log(1+z)$ for any $z\\in[0,1]$ , it follows that ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\log\\left(1+\\sum_{s^{\\prime}\\in S_{k,h}}\\left\\lVert\\widetilde{\\varphi}_{k,h,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\right\\rVert_{\\mathbf{B}_{k,h}^{-1}}^{2}\\right)\\ge\\sum_{k=1}^{K}\\frac{1}{2}\\sum_{s^{\\prime}\\in S_{k,h}}\\left\\lVert\\widetilde{\\varphi}_{k,h,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\right\\rVert_{\\mathbf{B}_{k,h}^{-1}}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "Finally, we obtain ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{k=1}^{K}\\displaystyle\\sum_{s^{\\prime}\\in S_{k,h}}\\left\\|\\widetilde{\\varphi}_{k,h,s^{\\prime}}(\\widetilde{\\pmb{\\theta}}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}^{2}\\leq2\\displaystyle\\sum_{k=1}^{K}\\log\\left(1+\\displaystyle\\sum_{s^{\\prime}\\in S_{k,h}}\\left\\|\\widetilde{\\varphi}_{k,h,s^{\\prime}}(\\widetilde{\\pmb{\\theta}}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}^{2}\\right)}\\\\ {\\displaystyle=2\\log\\displaystyle\\frac{\\operatorname*{det}(\\mathbf{B}_{K+1,h})}{\\operatorname*{det}(\\lambda\\mathbf{I}_{d})}}\\\\ {\\displaystyle}&{\\leq2d\\log\\left(1+\\displaystyle\\frac{K\\mathcal{U}L_{\\varphi}^{2}}{d\\lambda}\\right)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "where the last inequality follows by the determinant-trace inequality (Lemma 28). ", "page_idx": 64}, {"type": "text", "text": "D.5.3 Proof of Lemma 24 ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Proof of Lemma 24. Since \u03be(k,mh) $\\pmb{\\xi}_{k,h}^{(m)}\\sim\\mathcal{N}(\\mathbf{0},\\sigma_{k}^{2}\\mathbf{B}_{k,h}^{-1})$ , by Lemma 30 for each $m\\in[M]$ , we have ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\|\\pmb{\\xi}_{k,h}^{(m)}\\|_{\\mathbf{B}_{k,h}}\\leq C_{\\pmb{\\xi}}\\sigma_{k}\\sqrt{d\\log(M d/\\delta)}\\,.\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "Following the result of Lemma 22, we have ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{s^{\\prime}\\in S_{k,h}}P_{\\tilde{\\theta}_{h}^{k}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\left\\|\\bar{\\varphi}_{k,h,s^{\\prime}}(\\tilde{\\theta}_{h}^{k})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}\\leq\\displaystyle\\sum_{s^{\\prime}\\in S_{k,h}}P_{\\tilde{\\theta}_{h}^{k+1}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\left\\|\\bar{\\varphi}_{k,h,s^{\\prime}}(\\tilde{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}}&{}\\\\ {\\displaystyle}&{+\\left.\\frac{16\\eta L_{\\varphi}}{\\sqrt{\\lambda}}\\operatorname*{max}_{s^{\\prime}\\in S_{k,h}}\\left\\|\\bar{\\varphi}_{k,h,s^{\\prime}}(\\tilde{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}^{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "Then, we obtain ", "page_idx": 64}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{s^{\\prime}\\in S_{h,h}}{\\sum_{\\substack{c\\in S_{h,h}}}}P_{\\tilde{\\theta}_{h}^{k}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\bar{\\varphi}_{k,h,s^{\\prime}}(\\tilde{\\theta}_{h}^{k})^{\\top}\\xi_{k,h}^{\\prime}}\\\\ &{\\leq\\underset{s^{\\prime}\\in S_{h,h}}{\\sum_{\\substack{c\\in S_{h,h}}}}P_{\\tilde{\\theta}_{h}^{k}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\Big\\lVert\\bar{\\varphi}_{k,h,s^{\\prime}}(\\tilde{\\theta}_{h}^{k})\\Big\\rVert_{\\mathbf{B}_{k,h}^{-1}}\\lVert\\xi_{k,h}^{\\prime}\\rVert_{\\mathbf{B}_{k,h}}}\\\\ &{\\leq C_{\\xi}\\sigma_{k}\\sqrt{d\\log(M d/\\delta)}\\underset{s^{\\prime}\\in S_{h,h}}{\\sum_{\\substack{c\\in S_{h,h}}}}P_{\\tilde{\\theta}_{h}^{k}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\lVert\\bar{\\varphi}_{k,h,s^{\\prime}}(\\tilde{\\theta}_{h}^{k})\\rVert_{\\mathbf{B}_{k,h}^{-1}}}\\\\ &{\\leq\\gamma_{k}(\\delta)\\Bigg(\\underset{s^{\\prime}\\in S_{h,h}}{\\sum_{\\substack{c\\in S_{h,h}}}}P_{\\tilde{\\theta}_{h}^{k+1}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\left\\lVert\\bar{\\varphi}_{k,h,s^{\\prime}}(\\tilde{\\theta}_{h}^{k+1})\\right\\rVert_{\\mathbf{B}_{k,h}^{-1}}}\\\\ &{\\qquad\\qquad+\\frac{16\\eta L_{\\varphi}}{\\sqrt{\\lambda}}\\underset{s^{\\prime}\\in S_{h,h}}{\\operatorname*{max}}\\left\\lVert\\bar{\\varphi}_{k,h,s^{\\prime}}(\\tilde{\\theta}_{h}^{k+1})\\right\\rVert_{\\mathbf{B}_{k,h}^{-1}}^{2}\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 64}, {"type": "text", "text": "D.5.4 Proof of Lemma 25 ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Proof of Lemma 25. We provide a proof for Lemma 25 since it is slight modification of Lemma 20 of [76]. From the definition, we know that ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\left(\\widetilde{\\pmb{\\theta}}_{h}^{k+1}\\right)^{\\top}\\nabla\\ell_{k,h}(\\widetilde{\\pmb{\\theta}}_{h}^{k})+\\frac{1}{2\\eta}\\left\\|\\widetilde{\\pmb{\\theta}}_{h}^{k+1}-\\widetilde{\\pmb{\\theta}}_{h}^{k}\\right\\|_{\\widetilde{\\mathbf{B}}_{k,h}}^{2}\\le\\left(\\widetilde{\\pmb{\\theta}}_{h}^{k}\\right)^{\\top}\\nabla\\ell_{k,h}(\\widetilde{\\pmb{\\theta}}_{h}^{k})\\,.\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "By rearranging the terms, the following holds: ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{1}{2\\eta}\\left\\|\\widetilde{\\pmb{\\theta}}_{h}^{k+1}-\\widetilde{\\pmb{\\theta}}_{h}^{k}\\right\\|_{\\widetilde{\\mathbf{B}}_{k,h}}^{2}\\leq\\left(\\widetilde{\\pmb{\\theta}}_{h}^{k}-\\widetilde{\\pmb{\\theta}}_{h}^{k+1}\\right)^{\\top}\\nabla\\ell_{k,h}(\\widetilde{\\pmb{\\theta}}_{h}^{k})}&{}\\\\ {\\leq\\left\\|\\widetilde{\\pmb{\\theta}}_{h}^{k}-\\widetilde{\\pmb{\\theta}}_{h}^{k+1}\\right\\|_{\\widetilde{\\mathbf{B}}_{k,h}}\\left\\|\\nabla\\ell_{k,h}(\\widetilde{\\pmb{\\theta}}_{h}^{k})\\right\\|_{\\widetilde{\\mathbf{B}}_{k,h}^{-1}}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Thus, we get ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\left\\|\\widetilde{\\pmb{\\theta}}_{h}^{k+1}-\\widetilde{\\pmb{\\theta}}_{h}^{k}\\right\\|_{\\widetilde{\\mathbf{B}}_{k,h}}\\leq2\\eta\\left\\|\\nabla\\ell_{k,h}(\\widetilde{\\pmb{\\theta}}_{h}^{k})\\right\\|_{\\widetilde{\\mathbf{B}}_{k,h}^{-1}}\\,.\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Since $\\mathbf{B}_{k,h}\\preceq\\widetilde{\\mathbf{B}}_{k,h}$ and $\\widetilde{\\mathbf{B}}_{k,h}^{-1}\\preceq\\lambda^{-1}\\mathbf{I}_{d}$ , we obtain ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\left\\|\\widetilde{\\theta}_{h}^{k+1}-\\widetilde{\\theta}_{h}^{k}\\right\\|_{\\mathbf{B}_{k,h}}\\le\\left\\|\\widetilde{\\theta}_{h}^{k+1}-\\widetilde{\\theta}_{h}^{k}\\right\\|_{\\widetilde{\\mathbf{B}}_{k,h}}\\le2\\eta\\left\\|\\nabla\\ell_{k,h}(\\widetilde{\\theta}_{h}^{k})\\right\\|_{\\widetilde{\\mathbf{B}}_{k,h}^{-1}}\\le\\frac{2\\eta}{\\sqrt{\\lambda}}\\left\\|\\nabla\\ell_{k,h}(\\widetilde{\\theta}_{h}^{k})\\right\\|_{2}\\le\\frac{4\\eta L_{\\varphi}}{\\sqrt{\\lambda}}\\,.\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "For the last inequality of (103), we provide the upper bound of $l_{2}$ -norm of $\\nabla\\ell_{k,h}(\\pmb\\theta)$ . Since ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\ell_{k,h}(\\pmb\\theta)=-\\sum_{s^{\\prime}\\in S_{k,h}}y_{h}^{k}(s^{\\prime})\\log P_{\\pmb\\theta}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\,,\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "the gradient of the loss function is given by ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla\\ell_{k,h}(\\theta)=-\\displaystyle\\sum_{s^{\\prime}\\in S_{k,h}}y_{h}^{k}(s^{\\prime})\\left(\\varphi_{s,a,s^{\\prime}}-\\displaystyle\\sum_{s^{\\prime\\prime}\\in S_{k,h}}P_{\\theta}(s^{\\prime\\prime}\\mid s_{h}^{k},a_{h}^{k})\\varphi_{s,a,s^{\\prime\\prime}}\\right)}\\\\ &{\\qquad=\\displaystyle\\sum_{s^{\\prime}\\in S_{k,h}}y_{h}^{k}(s^{\\prime})\\sum_{s^{\\prime\\prime}\\in S_{k,h}}P_{\\theta}(s^{\\prime\\prime}\\mid s_{h}^{k},a_{h}^{k})\\varphi_{s,a,s^{\\prime\\prime}}-\\displaystyle\\sum_{s^{\\prime}\\in S_{k,h}}y_{h}^{k}(s^{\\prime})\\varphi_{s,a,s^{\\prime}}}\\\\ &{\\qquad=\\displaystyle\\sum_{s^{\\prime\\prime}\\in S_{k,h}}P_{\\theta}(s^{\\prime\\prime}\\mid s_{h}^{k},a_{h}^{k})\\varphi_{s,a,s^{\\prime\\prime}}-\\displaystyle\\sum_{s^{\\prime}\\in S_{k,h}}y_{h}^{k}(s^{\\prime})\\varphi_{s,a,s^{\\prime}}}\\\\ &{\\qquad=\\displaystyle\\sum_{s^{\\prime\\prime}\\in S_{k,h}}\\left(P_{\\theta}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})-y_{h}^{k}(s^{\\prime})\\right)\\varphi_{s,a,s^{\\prime}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Therefore, we have ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla\\ell_{k,h}(\\pmb\\theta)\\|_{2}=\\left\\|\\displaystyle\\sum_{s^{\\prime}\\in S_{k,h}}\\big(P_{\\pmb\\theta}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})-y_{h}^{k}(s^{\\prime})\\big)\\,\\varphi_{s,a,s^{\\prime}}\\right\\|_{2}}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\sum_{s^{\\prime}\\in S_{k,h}}\\big|P_{\\pmb\\theta}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})-y_{h}^{k}(s^{\\prime})\\big|\\,\\|\\varphi_{s,a,s^{\\prime}}\\|_{2}}\\\\ &{\\qquad\\qquad\\leq2L_{\\varphi}}\\end{array}\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "and this concludes the proof. ", "page_idx": 65}, {"type": "text", "text": "D.6 Bound on Pessimism Part ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "In this section, we provide the upper bound on the pessimism part of the regret: $\\begin{array}{r}{\\sum_{k=1}^{K}(V_{1}^{*}-\\widetilde{V}_{1}^{k})(s_{1}^{k})}\\end{array}$ . Lemma 26 (Bound on pessimism). For any $\\delta$ with $0\\,<\\,\\delta\\,<\\,\\Phi(-1)/2$ , let $\\sigma_{k}\\,=\\,H\\beta_{k}$ . If $\\lambda=$ $\\mathcal{O}(L_{\\varphi}^{2}d\\log\\mathcal{U})$ and we take multiple sample size $\\begin{array}{r}{M=\\lceil1-\\frac{\\log(H\\mathcal{U})}{\\log\\Phi(1)}\\rceil}\\end{array}$ \u2212lloogg (\u03a6H(U1)) \u2309, then with probability at least $1-\\delta/2$ , we have ", "page_idx": 65}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}(V_{1}^{*}-V_{1}^{k})(s_{1}^{k})=\\widetilde{\\mathcal{O}}\\left(d^{3/2}H^{3/2}\\sqrt{T}+\\kappa^{-1}d^{2}H^{2}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 65}, {"type": "text", "text": "Proof of Lemma 26. As seen in Lemma 18, by using multiple sampling technique we show that the optimistic randomized value functionV of ORRL-MNL is optimistic than the true optimal value with constant probability Hence, with the same argument used in Lemma 11, we can show that the pessimism term of ORRL-MNL is upper bounded by a bound of the estimation term times the inverse probability of being optimistic, i.e., ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\left(V_{1}^{*}-V_{1}^{k}\\right)(s_{1}^{k})\\leq\\widetilde{\\mathcal{O}}\\left(\\frac{1}{\\Phi(-1)}\\sum_{k=1}^{K}\\left(V_{1}^{k}-V_{1}^{\\pi^{k}}\\right)(s_{1}^{k})\\right)\\,.\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "D.7 Regret Bound of ORRL-MNL ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Proof of Theorem 2. Since both Lemma 21 and Lemma 26 holds with probability at least $1-\\delta/2$ respectively, by taking the union bound we conclude the proof. ", "page_idx": 66}, {"type": "text", "text": "E Optimistic Exploration Extension ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "In this section, we introduce UCRL- $\\tt M N L+$ (Algorithm 3), which is both computationally and statistically efficient for MNL-MDPs with UCB-based exploration. The main difference compared to ORRL-MNL is that UCRL-MNL $^+$ constructs an optimistic value function that is greater than the optimal value function with high probability. At each episode $k\\in[K]$ , with the estimated transition core parameter $\\widetilde{\\pmb{\\theta}}_{h}^{k}$ (5), for $(s,a)\\in S\\times A$ , set $\\hat{Q}_{H+1}^{k}(s,a)=0$ . For each $h\\in[H]$ , ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\hat{Q}_{h}^{k}(s,a):=r(s,a)+\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\widetilde{\\theta}_{h}^{k}}(s^{\\prime}\\mid s,a)\\hat{V}_{h+1}^{k}(s^{\\prime})+\\nu_{k,h}^{\\mathrm{opt}}(s,a)\\,,\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "where $\\hat{V}_{h}^{k}(s):=\\operatorname*{min}\\{\\operatorname*{max}_{a\\in\\mathcal{A}}\\hat{Q}_{h}^{k}(s,a),H\\}$ and $\\nu_{k,h}^{\\mathrm{opt}}(s,a)$ is the optimistic bonus term defined by ", "page_idx": 66}, {"type": "equation", "text": "$$\n\\nu_{k,h}^{\\mathrm{opt}}(s,a):=H\\beta_{k}\\sum_{s^{\\prime}\\in S_{s,a}}P_{\\tilde{\\theta}_{h}^{k}}(s^{\\prime}\\mid s,a)\\|\\bar{\\varphi}(s,a,s^{\\prime};\\tilde{\\theta}_{h}^{k})\\|_{\\mathbf{B}_{k,h}^{-1}}+3H\\beta_{k}^{2}\\operatorname*{max}_{s^{\\prime}\\in S_{s,a}}\\|\\varphi(s,a,s^{\\prime})\\|_{\\mathbf{B}_{k,h}^{-1}}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 66}, {"type": "text", "text": "Based on these optimistic value function $\\hat{Q}_{h}^{k}$ , at each episode the agent plays a greedy action with respect to $\\hat{Q}_{h}^{k}$ as summarized in Algorithm 3. ", "page_idx": 66}, {"type": "text", "text": "Algorithm 3 UCRL-MNL+ (Upper Confidence RL for MNL-MDPs) ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "1: Inputs: Episodic MDP $\\mathcal{M}$ , Feature map $\\varphi:S\\times A\\times S\\,\\rightarrow\\,\\mathbb{R}^{d}$ , Number of episodes $K$ ,   \nRegularization parameter $\\lambda$ , Confidence radius $\\{\\beta_{k}\\}_{k=1}^{K}$ , Step size $\\eta$   \n2: Initialize: $\\widetilde{\\pmb{\\theta}}_{h}^{1}=\\mathbf{0}_{d}$ , $\\mathbf{B}_{1,h}=\\lambda\\mathbf{I}_{d}$ for all $h\\in[H]$   \n3: for episode $k=1,2,\\cdots\\,,K$ do   \n4: Observe $s_{1}^{k}$ and set $\\left\\{\\hat{Q}_{h}^{k}(\\cdot,\\cdot)\\right\\}_{h\\in[H]}$ as described in (104)   \n5: for horizon $h=1,2,\\cdots\\,,H$ do   \n6: Select $a_{h}^{k}=\\operatorname{argmax}_{a\\in A}\\hat{Q}_{h}^{k}(s_{h}^{k},a)$ and observe $s_{h+1}^{k}$   \n7: Update $\\widetilde{\\mathbf{B}}_{k,h}=\\mathbf{B}_{k,h}+\\eta\\nabla^{2}\\ell_{k,h}(\\widetilde{\\pmb{\\theta}}_{h}^{k})$ and $\\widetilde{\\pmb{\\theta}}_{h}^{k+1}$ as in (5)   \n8: Update $\\mathbf{B}_{k+1,h}=\\mathbf{B}_{k,h}+\\nabla^{2}\\ell_{k,h}(\\widetilde{\\pmb{\\theta}}_{h}^{k+1})$   \n9: end for   \n10: end for ", "page_idx": 66}, {"type": "text", "text": "The main difference in regret analysis lies in ensuring the optimism of the estimated value function $\\hat{Q}_{h}^{k}$ (Lemma 27). In the following statement (formal statement of Corollary 1), we provide a regret guarantee for UCRL-MNL $^+$ , which enjoys the tightest regret bound for MNL-MDPs. ", "page_idx": 66}, {"type": "text", "text": "Theorem 3 (Regret Bound of UCRL-MNL $^+$ ). Suppose that Assumption 1- 4 hold. \u221aFor any $\\delta\\in(0,1)$ , if we set the input parameters in Algorithm 3 as $\\lambda=\\mathcal{O}(L_{\\varphi}^{2}d\\mathrm{log}\\mathcal{U}),\\beta_{k}=\\mathcal{O}(\\sqrt{d}\\log\\mathcal{U}\\log(k H))$ ", "page_idx": 66}, {"type": "text", "text": "$\\eta=\\mathcal{O}(\\log\\mathcal{U})$ , then with probability at least $1-\\delta$ , the cumulative regret of the UCRL-MNL $^+$ policy $\\pi$ is upper-bounded by ", "page_idx": 67}, {"type": "equation", "text": "$$\nR e g r e t_{\\pi}(K)=\\widetilde{\\mathcal{O}}\\left(d H^{3/2}\\sqrt{T}+\\kappa^{-1}d^{2}H^{2}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "where $T=K H$ is the total number of time steps. ", "page_idx": 67}, {"type": "text", "text": "Proof of Theorem 3. By Lemma 17, suppose that the good event $\\mathfrak{G}(K,\\delta^{\\prime})$ holds with probability at least $1-\\delta$ . Then, we show that the optimistic value function $\\hat{Q}_{h}^{k}$ is deterministically greater than the true optimal value function as follows: ", "page_idx": 67}, {"type": "text", "text": "Lemma 27 (Optimism). Suppose that the event $\\mathfrak{G}_{k,h}^{\\Delta}(\\delta)$ holds for all $k\\in[K]$ and $h\\in[H]$ . Then for any $(s,a)\\in S\\times A_{\\cdot}$ , we have ", "page_idx": 67}, {"type": "equation", "text": "$$\nQ_{h}^{*}(s,a)\\leq\\hat{Q}_{h}^{k}(s,a)\\,.\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "Conditioned on $\\mathfrak{G}(K,\\delta^{\\prime})$ , by Lemma 27 we have ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(V_{1}^{*}-V_{1}^{\\pi^{k}})(s_{1}^{k})=Q_{1}^{*}(s_{1}^{k},\\pi^{*}(s_{1}^{k}))-Q_{1}^{\\pi^{k}}(s_{1}^{k},a_{1}^{k})}\\\\ &{\\qquad\\qquad\\qquad\\leq\\hat{Q}_{1}^{k}(s_{1}^{k},\\pi^{*}(s_{1}^{k}))-Q_{1}^{\\pi^{k}}(s_{1}^{k},a_{1}^{k})}\\\\ &{\\qquad\\qquad\\qquad\\leq\\hat{Q}_{1}^{k}(s_{1}^{k},a_{1}^{k})-Q_{1}^{\\pi^{k}}(s_{1}^{k},a_{1}^{k})=\\nu_{k,1}^{\\mathrm{opt}}(s_{1}^{k},a_{1}^{k})+P_{1}(\\hat{V}_{2}^{k}-V_{2}^{\\pi^{k}})(s_{1}^{k},a_{1}^{k})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "Note that ", "page_idx": 67}, {"type": "equation", "text": "$$\nP_{1}(\\hat{V}_{2}^{k}-V_{2}^{\\pi^{k}})(s_{1}^{k},a_{1}^{k})=\\mathbb{E}_{\\tilde{s}|s_{1}^{k},a_{1}^{k}}\\left[(\\hat{V}_{2}^{k}-V_{2}^{\\pi^{k}})(\\tilde{s})\\right]=(\\hat{V}_{2}^{k}-V_{2}^{\\pi^{k}})(s_{2}^{k})+\\dot{\\zeta}_{1}^{k}\\,,\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "where we denote $\\zeta_{h}^{k}:=(\\hat{V}_{h+1}^{k}-V_{h+1}^{\\pi^{k}})(s_{h+1}^{k})-{\\mathbb{E}}_{\\widetilde{s}\\mid s_{h}^{k},a_{h}^{k}}\\left[(\\hat{V}_{h+1}^{k}-V_{h+1}^{\\pi^{k}})(\\widetilde{s})\\right]\\!.$ Then, with the same argument, we have ", "page_idx": 67}, {"type": "equation", "text": "$$\n(V_{1}^{*}-V_{1}^{\\pi^{k}})(s_{1}^{k})\\leq\\sum_{h=1}^{H}\\nu_{k,h}^{\\mathrm{opt}}(s_{h}^{k},a_{h}^{k})+\\sum_{h=1}^{H}\\dot{\\zeta}_{h}^{k}\\,.\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "By summing over all episodes, we have ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\mathop{\\mathbf{Regret}}_{\\pi}(K)\\le\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\nu_{k,h}^{\\mathrm{opt}}(s_{h}^{k},a_{h}^{k})+\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\dot{\\zeta}_{h}^{k}\\,.\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "On the other hand, note that ", "page_idx": 67}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=1}^{K}\\displaystyle\\sum_{h=1}^{H}\\nu_{k,n}^{\\alpha_{h}}(s_{k}^{k},a_{h}^{k})}\\\\ &{=\\displaystyle\\sum_{k=1}^{K}\\displaystyle\\sum_{h=1}^{H}H\\beta_{k}\\sum_{\\nu\\in\\mathcal{S}_{h,k}}P_{\\partial_{k}}\\big(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k}\\big)\\|\\varphi_{k,h,\\nu}(\\tilde{\\theta}_{h}^{k})\\|_{\\mathbf{B}_{k,h}^{-1}}+\\displaystyle\\sum_{k=1}^{K}\\sum_{h=1}^{H}3H\\beta_{k,\\nu}^{2}\\operatorname*{max}_{\\nu\\in\\mathcal{S}_{h,k}}\\|\\varphi_{k,h,\\nu}\\|_{\\mathbf{B}_{k,h}^{-1}}^{2}}\\\\ &{\\leq H\\beta_{K}\\displaystyle\\sum_{k=1}^{K}\\displaystyle\\sum_{\\nu=\\frac{1}{K}}^{H}\\sum_{\\nu\\in\\mathcal{S}_{h,k}}P_{\\partial_{k}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\|\\varphi_{k,h,\\nu}(\\tilde{\\theta}_{h}^{k})\\|_{\\mathbf{B}_{k,h}^{-1}}}\\\\ &{\\quad+\\displaystyle\\left.3H\\beta_{K}^{2}\\displaystyle\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\operatorname*{max}_{\\nu\\in\\mathcal{S}_{h,k}}\\|\\varphi_{k,h,\\nu}\\|_{\\mathbf{B}_{k,h}^{-1}}\\right.}\\\\ &{\\leq H\\beta_{K}\\displaystyle\\sum_{k=1}^{K}\\displaystyle\\sum_{h=1}^{H}\\sum_{\\nu\\in\\mathcal{S}_{h,k}}P_{\\partial_{k}^{k+1}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\left\\|\\varphi_{k,h,\\nu}(\\tilde{\\theta}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 67}, {"type": "equation", "text": "$$\n+\\frac{16\\eta L_{\\varphi}}{\\sqrt{\\lambda}}H\\beta_{K}\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\operatorname*{max}_{s^{\\prime}\\in\\mathcal{S}_{k,h}}\\left\\Vert\\bar{\\varphi}_{k,h,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\right\\Vert_{\\mathbf{B}_{k,h}^{-1}}^{2}+3H\\beta_{K}^{2}\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\operatorname*{max}_{s^{\\prime}\\in\\mathcal{S}_{k,h}}\\|\\varphi_{k,h,s^{\\prime}}\\|_{\\mathbf{B}_{k,h}^{-1}}^{2}\\,,\n$$", "text_format": "latex", "page_idx": 67}, {"type": "text", "text": "where the last inequality follows by Lemma 22. ", "page_idx": 68}, {"type": "text", "text": "Term (i) can be bounded as in Eq. (91): ", "page_idx": 68}, {"type": "equation", "text": "$$\nH\\beta_{K}\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\sum_{s^{\\prime}\\in{\\cal S}_{k,h}}P_{\\tilde{\\theta}_{h}^{k+1}}(s^{\\prime}\\mid s_{h}^{k},a_{h}^{k})\\left\\|\\bar{\\varphi}_{k,h,s^{\\prime}}(\\widetilde{\\theta}_{h}^{k+1})\\right\\|_{{\\bf B}_{k,h}^{-1}}=\\widetilde{\\mathcal{O}}(d H^{3/2}\\sqrt{T})\\,.\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "For term (ii), recall that as in Eq. (93) we have ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\operatorname*{max}_{s^{\\prime}\\in\\mathcal{S}_{k,h}}\\left\\|\\bar{\\varphi}_{k,h,s^{\\prime}}(\\widetilde{\\pmb{\\theta}}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}^{2}\\le16\\kappa^{-1}d H\\log\\left(1+\\frac{K\\mathcal{U}L_{\\varphi}^{2}}{d\\lambda}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "Then, we have ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\frac{16\\eta L_{\\varphi}}{\\sqrt{\\lambda}}H\\beta_{K}\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\operatorname*{max}_{s^{\\prime}\\in\\mathcal{S}_{k,h}}\\left\\|\\bar{\\varphi}_{k,h,s^{\\prime}}(\\widetilde{\\boldsymbol{\\theta}}_{h}^{k+1})\\right\\|_{\\mathbf{B}_{k,h}^{-1}}^{2}=\\widetilde{\\mathcal{O}}(\\kappa^{-1}d H^{2})\\,.\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "For term (iii), since we have ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{{3}H\\beta_{K}^{2}\\displaystyle\\sum_{k=1}^{K}\\displaystyle\\sum_{h=1}^{H}\\displaystyle\\operatorname*{max}_{s^{\\prime}\\in S_{k,h}}\\|\\varphi_{k,h,s^{\\prime}}\\|_{\\mathbf{B}_{k,h}^{-1}}^{2}\\le3H\\beta_{K}^{2}\\displaystyle\\sum_{k=1}^{K}\\displaystyle\\sum_{h=1}^{H}\\displaystyle\\operatorname*{max}_{s^{\\prime}\\in S_{k,h}}\\|\\varphi_{k,h,s^{\\prime}}\\|_{\\mathbf{A}_{k,h}^{-1}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq12\\kappa^{-1}d H^{2}\\beta_{K}^{2}\\log\\left(1+K\\mathcal{U}L_{\\varphi}^{2}/(d\\lambda)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\widetilde{\\mathcal{O}}(\\kappa^{-1}d^{2}H^{2})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "Combining the results of Eq. (106), (107), and (108), we have ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\nu_{k,h}^{\\mathrm{opt}}(s_{h}^{k},a_{h}^{k})=\\widetilde{\\mathcal{O}}(d H^{3/2}\\sqrt{T}+\\kappa^{-1}d^{2}H^{2})\\,.\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "Finally, by Azuma-Hoeffiding inequality as in Eq. (96) we have ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\dot{\\zeta}_{h}^{k}=\\widetilde{\\mathcal{O}}(H\\sqrt{T})\\,.\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 68}, {"type": "text", "text": "In the following, we provide the proof of Lemma 27. ", "page_idx": 68}, {"type": "text", "text": "E.1 Optimism ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "Proof of Lemma 27. We prove this by backwards induction on $h$ . For the base case $h=H$ , since $V_{H+1}^{*}(s)=\\hat{V}_{H+1}^{k}(s)=0$ for all $s\\in S$ , we have ", "page_idx": 68}, {"type": "equation", "text": "$$\n\\hat{Q}_{H}^{k}(s,a)=r(s,a)=Q_{H}^{*}(s,a)\\,.\n$$", "text_format": "latex", "page_idx": 68}, {"type": "text", "text": "Suppose that the statement holds for $h\\!+\\!1$ where $h\\in[H\\!-\\!1]$ . Then, for $h$ and for any $(s,a)\\in S\\times A$ , ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{Q}_{k}^{k}(s,\\alpha)}\\\\ &{\\!=\\!r\\left(s,\\alpha\\right)\\!+\\!\\displaystyle\\sum_{s^{\\prime}\\in{\\mathcal E}_{s,\\alpha}}P_{\\theta_{k}^{k}}(s^{\\prime}\\mid s,\\alpha)\\hat{V}_{k+1}^{k}(s^{\\prime})+\\nu_{k,h}^{o p t}(s,\\alpha)}\\\\ &{\\!\\geq\\!r\\left(s,\\alpha\\right)\\!+\\!\\displaystyle\\sum_{s^{\\prime}\\in{\\mathcal E}_{s,\\alpha}}P_{\\theta_{k}^{k}}(s^{\\prime}\\mid s,\\alpha)V_{h+1}^{*}(s^{\\prime})+\\nu_{k,h}^{o p t}(s,\\alpha)}\\\\ &{\\!=\\!r(s,\\alpha)\\!+\\!\\displaystyle\\sum_{s^{\\prime}\\in{\\mathcal E}_{s,\\alpha}}P_{\\theta_{k}}(s^{\\prime}\\mid s,\\alpha)V_{h+1}^{*}(s^{\\prime})}\\\\ &{\\quad+\\sum_{s^{\\prime}\\in{\\mathcal E}_{s,\\alpha}}\\left(P_{\\theta_{k}}(s^{\\prime}\\mid s,\\alpha)-P_{\\theta_{k}}(s^{\\prime}\\mid s,\\alpha)\\right)V_{h+1}^{*}(s^{\\prime})+\\nu_{k,h}^{o p t}(s,\\alpha)}\\\\ &{\\quad\\times\\frac{r}{s(s,\\alpha)_{s}}\\!+\\!\\displaystyle\\sum_{s^{\\prime}\\in{\\mathcal E}_{s,\\alpha}}P_{\\theta_{k}}(s^{\\prime}\\mid s,\\alpha)V_{h+1}^{*}(s^{\\prime})}\\\\ &{\\geq r(s,\\alpha)\\!+\\!\\sum_{s^{\\prime}\\in{\\mathcal E}_{s,\\alpha}}P_{\\theta_{k}}(s^{\\prime}\\mid s,\\alpha)V_{h+1}^{*}(s^{\\prime})}\\\\ &{=\\!g_{s^{\\prime}}^{*}(s,\\alpha),}\\end{array}\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "where the first inequality follows from the induction hypothesis and the second inequality holds by Lemma 16. ", "page_idx": 69}, {"type": "text", "text": "F Experiment Details ", "text_level": 1, "page_idx": 69}, {"type": "image", "img_path": "7tRtH0AoBl/tmp/acf2ff4aebc644e50663c53952e45a2c15382bf6eca89232c2f8131533e0acb0.jpg", "img_caption": ["Figure 2: The \u201cRiverSwim\u201d environment with $n$ states [58] "], "img_footnote": [], "page_idx": 69}, {"type": "text", "text": "The RiverSwim environment (Figure 2) consists of $n$ states that are arranged in a chain. The agent starts in the leftmost state with a relatively small reward of 0.005 and aims to reach the rightmost state, which has a relatively large reward of 1. Choosing to swim to the left moves the agent deterministically to the left, while swimming to the right has a probability of transitioning the agent toward the right state, but also a high chance of remaining in the current state or even moving left due to the strong current of river. Therefore, efficient exploration is crucial in order to learn the optimal policy for this environment. ", "page_idx": 69}, {"type": "text", "text": "We fine-tuned the hyperparameters for each algorithm within specific ranges. Figures 1a and 1b show the episodic returns in the RiverSwim environment over 10 independent runs with $|S|=4,H=12$ , and $K=10,000$ and $|S|=8,H=24$ , and $K=10,000$ , respectively. The shaded areas represent the standard deviations (1-sigma error). Figure 1c compares the running time of the algorithms over the first 1,000 episodes. All experiments were conducted on a Xeon(R) Gold 6226R CPU $\\textcircled{a}2.90\\mathrm{GHz}$ (16 cores). ", "page_idx": 69}, {"type": "text", "text": "G Auxiliary Lemmas ", "text_level": 1, "page_idx": 69}, {"type": "text", "text": "Lemma 28 (Determinant-trace inequality [1]). Suppose $\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{t}\\in\\mathbb{R}^{d}$ and for any $1\\leq\\tau\\leq t,$ , $\\|\\mathbf{x}_{\\tau}\\|_{2}\\leq L$ . Let $\\begin{array}{r}{\\mathbf{V}_{t}=\\lambda\\mathbf{I}_{d}+\\sum_{\\tau=1}^{t}\\mathbf{\\bar{x}}_{\\tau}\\mathbf{x}_{\\tau}^{\\top}}\\end{array}$ for some $\\lambda>0$ . Then, ", "page_idx": 69}, {"type": "equation", "text": "$$\n\\operatorname*{det}(\\mathbf{V}_{t})\\leq(\\lambda+t L^{2}/d)^{d}\\,.\n$$", "text_format": "latex", "page_idx": 69}, {"type": "text", "text": "Lemma 29 (Freedman\u2019s inequality [29]). Consider a real-valued martingale $\\{Y_{k}:k=0,1,2,..\\,.\\}$ with difference sequence $\\{X_{k}:k=0,1,2,3,.\\,.\\,.\\}$ . Assume that the difference sequence is uniformly ", "page_idx": 69}, {"type": "text", "text": "bounded, $X_{k}\\,\\leq\\,R$ almost surely for $k\\,=\\,1,2,3,...$ . Define the predictable quadratic variation process of the martingale: ", "page_idx": 70}, {"type": "equation", "text": "$$\nW_{k}:=\\sum_{j=1}^{k}\\mathbb{E}_{j-1}[X_{j}^{2}]\\quad f o r\\,k=1,2,3,\\dots.\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "Then, for all $t\\geq0$ and $\\sigma^{2}>0$ , ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\exists k\\ge0:Y_{k}\\ge t\\ a n d\\ W_{k}\\le\\sigma^{2}\\right)\\le\\exp\\left(-\\frac{-t^{2}/2}{\\sigma^{2}+R t/3}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "Lemma 30 (Gaussian noise concentration (Lemma D.2 in [37])). Let $\\pmb{\\xi}^{(1)},\\pmb{\\xi}^{(2)},\\dots,\\pmb{\\xi}^{(M)}$ be $M$ independent $d$ -dimensional multivariate normal distributed vector with mean ${\\bf0}_{d}$ and covariance $\\sigma^{2}\\mathbf{A}^{-1}$ for some $\\sigma>0$ and a positive definite matrix ${\\bf A}^{-1}$ , i.e., $\\pmb{\\xi}^{(m)}\\sim\\mathcal{N}(\\mathbf{0}_{d},\\sigma^{2}\\mathbf{A}^{-1})\\,f o r\\,m\\in[M]$ . Then for any $\\delta\\in(0,1)$ , with probability at least $1-\\delta$ , we have ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{m\\in[M]}\\|\\pmb{\\xi}^{(m)}\\|_{\\mathbf{A}}\\leq C_{\\pmb{\\xi}}\\sigma\\sqrt{d\\log(M d/\\delta)}:=\\gamma(\\delta)\\,,\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "where $C_{\\xi}$ is an absolute constant. ", "page_idx": 70}, {"type": "text", "text": "Lemma 31 (Proposition 4.1 of 15). Let the $w_{t+1}$ be the solution of the update rule ", "page_idx": 70}, {"type": "equation", "text": "$$\nw_{t+1}=\\arg\\operatorname*{min}_{w\\in\\mathcal{V}}\\eta\\ell_{t}(w)+D_{\\psi}(w,w_{t}),\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "where $\\mathcal{V}\\subseteq\\mathcal{W}\\subseteq\\mathbb{R}^{d}$ is a non-empty convex set and $D_{\\psi}(w_{1},w_{2})=\\psi(w_{1})\\!-\\!\\psi(w_{2})\\!-\\!\\langle\\nabla\\psi(w_{2}),w_{1}-$ $\\left.w_{2}\\right\\rangle$ is the Bregman Divergence w.r.t. a strictly convex and continuously differentiable function $\\psi:\\mathcal{W}\\to\\mathbb{R}$ . Further supposing $\\psi(w)$ is 1-strongly convex w.r.t. a certain norm $\\|\\cdot\\|$ in $\\mathcal{W}$ , then there exists a $g_{t}\\in\\partial\\ell_{t}(w_{t+1})$ such that ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\langle\\eta_{t}g_{t}^{\\prime},w_{t+1}-u\\rangle\\leq\\langle\\nabla\\psi(w_{t})-\\nabla\\psi(w_{t+1}),w_{t+1}-u\\rangle\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "for any $u\\in\\mathcal{W}$ . ", "page_idx": 70}, {"type": "text", "text": "Lemma 32. Let $\\{\\mathcal{F}_{t}\\}_{t=1}^{\\infty}$ be a flitration. Let $\\{{\\bf z}_{t}\\}_{t=1}^{\\infty}$ be a stochastic process in $B_{2}(\\mathcal{U})=\\{\\mathbf{z}\\in\\mathbb{R}^{\\mathcal{U}}\\mid$ $\\|\\mathbf{z}\\|_{\\infty}\\leq1\\}$ such that $\\mathbf{z}_{t}$ is $\\mathcal{F}_{t}$ measurable. Let $\\{\\varepsilon_{t}\\}_{t=1}^{\\infty}$ be a martingale difference sequence such that $\\varepsilon_{t}\\in\\mathbb{R}^{\\mathcal{U}}$ is $\\mathcal{F}_{t+1}$ measurable. Furthermore, assume that conditional on $\\mathcal{F}_{t}$ , we have $\\|\\varepsilon_{t}\\|_{1}\\leq2$ almost surely, and denote by $\\Sigma_{t}=\\mathbb{E}[\\varepsilon_{t}\\varepsilon_{t}^{\\top}|\\mathcal{F}_{t}]$ . Let $\\lambda>0$ and for any $t\\geq1$ define ", "page_idx": 70}, {"type": "equation", "text": "$$\nU_{t}=\\sum_{i=1}^{t-1}\\langle\\varepsilon_{i},\\mathbf{z}_{i}\\rangle\\quad a n d\\quad\\mathbf{B}_{t}=\\lambda+\\sum_{i=1}^{t-1}\\|\\mathbf{z}_{i}\\|_{\\Sigma_{i}}^{2},\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "Then, for any $\\delta\\in(0,1]$ , we have ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\exists t\\geq1,U_{t}\\geq\\sqrt{\\mathbf{B}_{t}}\\left(\\frac{\\sqrt{\\lambda}}{4}+\\frac{4}{\\sqrt{\\lambda}}\\log\\left(\\sqrt{\\frac{\\mathbf{B}_{t}}{\\lambda}}\\right)+\\frac{4}{\\sqrt{\\lambda}}\\log\\left(\\frac{2}{\\delta}\\right)\\right)\\right]\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "Lemma 33 (Lemma 1 of 76). Let $\\begin{array}{r}{\\ell(\\mathbf{z},y)\\;=\\;\\sum_{k=0}^{K}\\mathbf{1}\\{y\\;=\\;k\\}\\,\\cdot\\log\\Big(\\frac{1}{[\\sigma(\\mathbf{z})]_{k}}\\Big),\\;\\mathbf{a}\\;\\in\\;[-C,C]^{K},}\\end{array}$ $y\\in\\{0\\}\\cup[K]$ and $\\mathbf{b}\\in\\mathbb{R}^{K}$ where $C>0,$ . Then, we have ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\ell(\\mathbf{a},y)\\geq\\ell(\\mathbf{b},y)+\\nabla\\ell(\\mathbf{b},y)^{\\top}(\\mathbf{a}-\\mathbf{b})+{\\frac{1}{\\log(K+1)+2(C+1)}}(\\mathbf{a}-\\mathbf{b})^{\\top}\\nabla^{2}\\ell(\\mathbf{b},y)(\\mathbf{a}-\\mathbf{b}).\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "Lemma 34 (Lemma 17 of 76). Let \u2113(z, y) = kK=0 1{y = k} \u00b7 log [\u03c3(1z)]k and $\\textbf{z}\\in\\mathbb{R}^{K}$ be a $K$ -dimensional vector. Define $\\mathbf{z}^{\\mu}\\,\\triangleq\\,\\sigma^{+}\\,(\\mathrm{smooth}_{\\mu}(\\sigma(\\mathbf{z})))$ , where $\\mathrm{smooth}_{\\mu}({\\bf p})\\,=\\,(1-\\mu){\\bf p}+{}$ $\\mu\\mathbf{1}/(K+1)$ . Then, for $\\mu\\in[0,1/2]$ , we have ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\ell(\\mathbf{z}^{\\mu},y)-\\ell(\\mathbf{z},y)\\leq2\\mu\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "for any $y\\in\\{0\\}\\cup[K]$ . We also have $\\|\\mathbf{z}^{\\mu}\\|_{\\infty}\\leq\\log(K/\\mu)$ . ", "page_idx": 70}, {"type": "text", "text": "Lemma 35 (Lemma 18 of 76). Let $\\begin{array}{r}{L_{i,h}(\\pmb\\theta):=\\ell_{i,h}(\\pmb\\theta)+\\frac{1}{2c}\\|\\pmb\\theta-\\pmb\\theta_{h}^{i}\\|_{\\mathbf B_{i,h}}^{2}}\\end{array}$ . Assume that $\\ell_{i,h}$ is a $\\sqrt{N}$ -self-concordant-like function. Then, for any $\\theta,\\pmb{\\theta}_{h}^{i}\\in B(\\mathbf{0}_{d},1)$ , the quadratic approximation ", "page_idx": 70}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{L}_{i,h}(\\pmb{\\theta})=L_{i,h}(\\widetilde{\\pmb{\\theta}}_{h}^{i+1})+\\langle\\nabla L_{i,h}(\\widetilde{\\pmb{\\theta}}_{h}^{i+1}),\\pmb{\\theta}-\\widetilde{\\pmb{\\theta}}_{h}^{i+1}\\rangle+\\frac{1}{2c}\\left\\|\\pmb{\\theta}-\\widetilde{\\pmb{\\theta}}_{h}^{i+1}\\right\\|_{\\mathbf{B}_{i,h}}^{2}s a t i s f i e s}\\\\ &{\\qquad\\qquad\\qquad L_{i,h}(\\pmb{\\theta})\\leq\\widetilde{L}_{i,h}(\\pmb{\\theta})+\\exp\\left(N\\left\\|\\pmb{\\theta}-\\widetilde{\\pmb{\\theta}}_{h}^{i+1}\\right\\|_{2}^{2}\\right)\\left\\|\\pmb{\\theta}-\\widetilde{\\pmb{\\theta}}_{h}^{i+1}\\right\\|_{\\nabla\\ell_{i,h}(\\widetilde{\\pmb{\\theta}}_{h}^{i+1})}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 70}, {"type": "text", "text": "H Limitations ", "text_level": 1, "page_idx": 71}, {"type": "text", "text": "We make an assumption about the transition model of MDPs by using the MNL model, which is a specific parametric model. This assumption implies that we assume the realizability of the MNL model. It\u2019s worth noting that the realizability assumption has also been commonly made in previous literature on provable reinforcement learning with function approximation, including works such as [72, 43, 73, 53, 22, 14, 9, 68, 70, 33, 81, 82, 37, 35]. However, we hope that this condition can be relaxed in the future work. ", "page_idx": 71}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 72}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 72}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 72}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 72}, {"type": "text", "text": "Justification: The main claims made in the abstract is to propose provably efficient randomized algorithms for MNL-MDPs. In Section 1 (Introduction), we provide the motivation and main contributions of this paper. ", "page_idx": 72}, {"type": "text", "text": "Guidelines: ", "page_idx": 72}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 72}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 72}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 72}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 72}, {"type": "text", "text": "Justification: We discuss the limitation of this work in Appendix H ", "page_idx": 72}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 72}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 72}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 72}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 72}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 73}, {"type": "text", "text": "Justification: We provide the full set of assumptions in Section 2.2 and a complete proof of main results in Appendix C and D. ", "page_idx": 73}, {"type": "text", "text": "Guidelines: ", "page_idx": 73}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 73}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 73}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 73}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 73}, {"type": "text", "text": "Justification: We provide numerical experiments that support our main claims in Section 5 and the detailed information of experiments in Appendix F. ", "page_idx": 73}, {"type": "text", "text": "Guidelines: ", "page_idx": 73}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 73}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 73}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 74}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 74}, {"type": "text", "text": "Justification: We have attached the data and code with sufficient instructions to reproduce the main experimental results in the supplementary material. ", "page_idx": 74}, {"type": "text", "text": "Guidelines: ", "page_idx": 74}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 74}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 74}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 74}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 74}, {"type": "text", "text": "Justification: We provide the detailed explanation for the experimental setting in Appendix F. Guidelines: ", "page_idx": 74}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 74}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 74}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 74}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 74}, {"type": "text", "text": "Justification: We report error bars (standard deviation) in our numerical experiment results shown in Section 5. ", "page_idx": 74}, {"type": "text", "text": "Guidelines: ", "page_idx": 74}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 74}, {"type": "text", "text": "", "page_idx": 75}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 75}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 75}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 75}, {"type": "text", "text": "Justification: We provide the detailed information on the computer resources used to conduct numerical experiments in Appendix F. ", "page_idx": 75}, {"type": "text", "text": "Guidelines: ", "page_idx": 75}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 75}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 75}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 75}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 75}, {"type": "text", "text": "Justification: The research conducted in this paper adheres to the NeurIPS Code of Ethics in all aspects. ", "page_idx": 75}, {"type": "text", "text": "Guidelines: ", "page_idx": 75}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 75}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 75}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 75}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 75}, {"type": "text", "text": "Justification: There is no negative societal impacts of the work performed because this research focuses on theoretical aspects. ", "page_idx": 75}, {"type": "text", "text": "Guidelines: ", "page_idx": 75}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 75}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 76}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 76}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 76}, {"type": "text", "text": "Justification: The research conducted in this paper does not pose any such risks. Guidelines: ", "page_idx": 76}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 76}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 76}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Justification: This paper does not use any external assets such as code, data, or models. Guidelines: ", "page_idx": 76}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 76}, {"type": "text", "text": "", "page_idx": 77}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 77}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 77}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 77}, {"type": "text", "text": "Guidelines: ", "page_idx": 77}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 77}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 77}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 77}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 77}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 77}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 77}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 77}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 77}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 77}]