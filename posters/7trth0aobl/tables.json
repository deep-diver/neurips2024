[{"figure_path": "7tRtH0AoBl/tables/tables_16_1.jpg", "caption": "Table 1: This table compares the problem settings, online update, performance of the this paper with those of other methods in provable RL with function approximation. For computation cost, we only keep the dependence on the number of episode K.", "description": "This table compares different reinforcement learning algorithms in terms of their model-based approach, transition model used, reward setting, computational cost per episode, and regret bound.  It highlights the differences and improvements of the proposed algorithms (RRL-MNL, ORRL-MNL, UCRL-MNL+) compared to existing methods. The table shows that the proposed algorithms achieve constant-time computational cost while maintaining favorable regret bounds.", "section": "A Related Work"}]