[{"type": "text", "text": "Optimal Algorithms for Augmented Testing of Discrete Distributions ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Maryam Aliakbarpour Piotr Indyk Ronitt Rubinfeld Rice University MIT MIT Houston, TX Cambridge, MA Cambridge, MA maryama@rice.edu indyk@mit.edu ronitt@csail.mit.edu ", "page_idx": 0}, {"type": "text", "text": "Sandeep Silwal   \nUW-Madison   \nMadison, WI   \nsilwal@cs.wisc.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider the problem of hypothesis testing for discrete distributions. In the standard model, where we have sample access to an underlying distribution $p$ , extensive research has established optimal bounds for uniformity testing, identity testing (goodness of fti), and closeness testing (equivalence or two-sample testing). We explore these problems in a setting where a predicted data distribution, possibly derived from historical data or predictive machine learning models, is available. We demonstrate that such a predictor can indeed reduce the number of samples required for all three property testing tasks. The reduction in sample complexity depends directly on the predictor\u2019s quality, measured by its total variation distance from $p$ . A key advantage of our algorithms is their adaptability to the precision of the prediction. Specifically, our algorithms can self-adjust their sample complexity based on the accuracy of the available prediction, operating without any prior knowledge of the estimation\u2019s accuracy (i.e. they are consistent). Additionally, we never use more samples than the standard approaches require, even if the predictions provide no meaningful information (i.e. they are also robust). We provide lower bounds to indicate that the improvements in sample complexity achieved by our algorithms are information-theoretically optimal. Furthermore, experimental results show that the performance of our algorithms on real data significantly exceeds our worst-case guarantees for sample complexity, demonstrating the practicality of our approach. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Property testing of distributions is a fundamental task that lies at the heart of many scientific endeavors: Given sample access to an underlying unknown distribution $p$ , the goal is to infer whether $p$ has a certain property or it is $\\epsilon_{}$ -far from any distribution that has the property (in some reasonable notion of distance, such as total variation distance) with as few samples as possible. Over the past century [66], this problem has been extensively explored in statistics, machine learning, and theoretical computer science. Indeed, distribution testing (also called hypothesis testing) is now a major pillar of modern learning theory and algorithmic statistics, with applications in learning mixtures of distributions such as Gaussians, Poisson Binomial Distributions and robust learning [40, 45, 36, 75, 34, 45]. The framework has also been extensively studied under privacy [56, 14, 27, 18, 26, 3, 72, 50] and low-memory constraints [8]. ", "page_idx": 0}, {"type": "text", "text": "One of the most natural and well-studied questions in this framework is: given sample access to an unknown distribution $p$ , can we determine whether $p$ is equal to another distribution $q$ , or $\\epsilon$ -far from it (e.g. in total variation distance)? This problem has been studied under various assumptions about how we access $q$ : It is called uniformity testing when $q$ is a uniform distribution, identity testing (or \u201cgoodness-of-fit\u201d) when a description of $q$ is known, and closeness testing (\u201ctwo-sample testing\u201d or \u201cequivalence testing\u201d) when we only have sample access to $q$ . The primary goal in solving these tasks is to design algorithms that use as few samples as possible. Optimal sample co\u221amplexity bounds have been established for discrete distributions $p$ and $q$ over a domain of size $n$ : $\\Theta(\\bar{\\sqrt{n}}/\\epsilon^{2})$ samples for uniformity testing [52, 15, 69, 38] and identity testing [76, 5, 39], and $\\Theta\\left(n^{2/3}/\\epsilon^{4/3}+\\sqrt{n}/\\epsilon^{2}\\right)$ samples for closeness testing [16, 30, 38, 39]. Other related versions of uniformity, identity, and closeness testing are presented in [63, 6, 19, 51, 46, 74, 12, 13, 68]. ", "page_idx": 1}, {"type": "text", "text": "Given that the aforementioned results are tight and cannot be improved, any further progress requires equipping the algorithm with additional functionality. A natural approach is to leverage the fact that in numerous applications, the underlying distribution is not completely unknown; some prediction of the underlying distribution may be available or can be learned via a predictive machine learning model. For example, if distributions evolve slowly over time, earlier iterations can serve as approximations for later ones, e.g., network traffic data and search engine queries. Such estimations can often be learned from \u201colder\u201d data by using it to train a predictor or regressor. In linguistics and text processing tasks that involve distributions over words, the length of a word can approximate its frequency, since longer words are known to be less frequent. Another example is when data is available at different \u201cscales.\u201d For instance,demographic data on loan defaults at the national level could be informative for data from specific areas. ", "page_idx": 1}, {"type": "text", "text": "One challenge to using such information is that it rarely comes with a guarantee of precision. This fact leads to the information being deemed unreliable, as it may poorly predict the underlying distribution. For example, while the national loan default rate might be close to that of a typical area code, it could differ significantly in affluent areas. Thus a natural algorithmic question that arises is how to design algorithms that can exploit predictive information as much as possible without any prior assumptions about its accuracy. The goal is then to design an algorithm that solves the problem with as few samples as possible, given the quality of the prediction. ", "page_idx": 1}, {"type": "text", "text": "In this work, we study the fundamental problems of uniformity, identity, and closeness testing in the setting where a prediction of the underlying distribution is provided. This prediction can be formalized by assuming that the distribution testing algorithm has access to a predicted distribution $\\hat{p}$ of $p^{1}$ . This is in addition to having sample access to $p$ as in the standard model (without access to prediction). Our algorithms achieve the optimal reduction in the number of samples used compared to the standard case, where the improvement depends on the quality of the predictor $\\hat{p}$ in terms of its total variation distance from $p$ . Our algorithms can also self-adjust their sample complexity to the accuracy of $\\hat{p}$ , minimizing sample complexity wherever feasible, without prior knowledge of $\\hat{p}$ \u2019s accuracy. Our approach ensures that the algorithm is resilient to inaccuracies in predictions and does not exceed the optimal sampling bound in the standard model, even when $\\hat{p}$ significantly deviates from the actual $p$ . Furthermore, our matching lower bounds demonstrate the optimality of our algorithms. Experimental results additionally confirm the practicality of our algorithms. ", "page_idx": 1}, {"type": "text", "text": "Measuring accuracy of predictor. We use the total variation distance between the prediction $\\hat{p}$ and the unknown distribution $p$ as our measure of predictor accuracy. Previous work often assumed a strong element-wise guarantees, where $\\hat{p}_{i}$ is within a constant multiplicative factor of $p_{i}$ for all domain elements $i$ , a constraint that becomes limiting especially for small $p_{i}$ (see Section 1.3). This paper is the first to study a notion of average error between $p$ and $\\hat{p}$ , measuring via the TV distance. This metric was chosen for its prevalent use in statistical inference and its intuitive interpretation. ", "page_idx": 1}, {"type": "text", "text": "1.1 Our approach and problem formulation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Our approach to solving a distribution testing problem (uniformity, identity, or closeness testing) consists of two components: search and test. At a high level, search aims to guess $\\lVert\\boldsymbol{p}-\\hat{\\boldsymbol{p}}\\rVert_{\\mathrm{TV}}$ , and test performs the actual distribution testing using the guess of the accuracy provided by search as a suggested accuracy level. ", "page_idx": 1}, {"type": "text", "text": "More precisely, our augmented test component aims to evaluate whether $p=q$ , while receiving $\\hat{p}$ and a suggested accuracy level $\\alpha$ (which may or may not reflect the true distance between $p$ and $\\hat{p}$ ). In addition to two possible outcomes of accept and reject in the standard setting2, our augmented test component may output inaccurate information when it determines that the $\\hat{p}$ is not $\\alpha$ -close to $p$ Our requirements for the augmented test component are the following: $i$ ) If the test is conclusive, i.e., it chooses to output accept or reject, the answer should be correct regardless of $\\hat{p}$ \u2019s accuracy. $i i$ ) If $\\hat{p}$ is indeed $\\alpha$ -close to $p$ , the test component should not output inaccurate information. ", "page_idx": 2}, {"type": "text", "text": "We emphasize that the guess $\\alpha$ is not guaranteed to be correct or may not even be a valid upper bound on the true TV distance. Thus, the algorithm is afforded a degree of flexibility to forego solving the problem when the distributions are not within $\\alpha$ proximity (and can try again with a new guess). ", "page_idx": 2}, {"type": "text", "text": "Our search component aims to identify an appropriate accuracy level $\\alpha$ such that the test component can test in a conclusive manner by returning accept or reject. Since the true value distance $\\|p-\\hat{p}\\|_{\\mathrm{TV}}$ is not known, we start by guessing a small $\\alpha$ , corresponding to the most accurate $\\hat{p}$ and the fewest samples needed, run the augmented test component with this $\\alpha$ and $\\hat{p}$ , and verify the conclusiveness of the testing. If inconclusive, our guess $\\alpha$ is increased to a level that we can afford testing by doubling the sample size, and the search component proceeds with the next $\\alpha$ . It continues until the desired accuracy is reached, and accept or reject is returned. Then, search halts with that result. ", "page_idx": 2}, {"type": "text", "text": "Clearly, if the accuracy level guess $\\alpha$ is at least $\\|p-\\hat{p}\\|_{\\mathrm{TV}}$ , the test component has to output accept or reject with high probability. Thus, we show that it is unlikely that search proceeds when $\\alpha\\gg\\|p-\\bar{p}\\|_{\\mathrm{TV}}$ . Therefore, this method does not significantly increase the sample complexity, potentially increasing it by at most an $O(\\log\\log(n/\\epsilon))$ factor in expectation. The search component is applicable to all of the problems we study and we defer all details of the search component to Section B. The remainder of the main body focuses on designing the augmented testers, i.e. the test component. ", "page_idx": 2}, {"type": "text", "text": "Definition 1.1 (Augmented tester). Suppose we are given four parameters, $\\alpha\\in[0,1],$ , $\\epsilon\\in(0,1)$ , $\\delta\\in(0,1).$ , $n\\in\\mathbb{N}$ , and two underlying distributions $p$ and $q_{\\mathrm{r}}$ , along with a prediction distribution $\\hat{p}$ over $[n]$ . Suppose $\\boldsymbol{\\mathcal{A}}$ is an algorithm that receives all these parameters, and the description of $\\hat{p}$ as input, and it has sample access to p and $q$ . We say algorithm $\\boldsymbol{\\mathcal{A}}$ is an $(\\alpha,\\epsilon,\\delta)$ -augmented tester for closeness testing if the following holds for every p, q, and $\\hat{p}$ over $[n]$ : ", "page_idx": 2}, {"type": "text", "text": "\u2022 If $\\hat{p}$ and $p$ are $\\alpha$ -close in total variation distance, the algorithm outputs inaccurate information with a probability at most $\\delta/2$ . \u2022 If $p=q,$ , then the algorithm outputs reject with a probability at most $\\delta/2$ . \u2022 If p is \u03f5-far from q, then the algorithm outputs accept with a probability at most $\\delta/2$ . ", "page_idx": 2}, {"type": "text", "text": "In this definition, if the description of q is known to the algorithm instead of having sample access, we say $\\boldsymbol{\\mathcal{A}}$ is an $(\\alpha,\\epsilon,\\delta)$ -augmented tester for identity testing. If q is a uniform distribution over $[n]$ , then we say $\\boldsymbol{\\mathcal{A}}$ is an $(\\alpha,\\epsilon,\\delta)$ -augmented tester for uniformity testing. ", "page_idx": 2}, {"type": "text", "text": "To highlight the distinction between this definition and the standard definition, note that in the standard regime, no prediction $\\hat{p}$ is available to the algorithm, and the algorithm lacks the option of outputting inaccurate information. ", "page_idx": 2}, {"type": "text", "text": "Remark 1. We assume that $\\delta$ is a small constant, but, a standard amplification technique via Chernoff bounds can achieve an arbitrarily small confidence parameter $\\delta$ with a $O(\\log(1/\\delta))$ overhead. ", "page_idx": 2}, {"type": "text", "text": "1.2 Our results ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our theoretical results: In this paper we demonstrate that predictions can indeed reduce the number of samples needed to solve the three aforementioned testing problems. Our algorithms are parameterized by both $\\epsilon$ and $\\alpha$ . We provide tight sample complexity results (matching upper and lower bounds) for these problems. The sample complexity drops drastically compared to the standard case, depending on the total variation distance between $q$ and $\\hat{p}$ . Our algorithms are also robust: if the prediction error is high, our algorithm succeeds by using (asymptotically) the same number of samples as the standard setting without predictions. ", "page_idx": 2}, {"type": "text", "text": "Theorem 2 (Informal version of Theorem 7). Augmented uniformity and identity testing for distributions over $[n]$ , with parameters $\\alpha,\\,\\epsilon_{i}$ , and $\\delta=2/3$ , require the following number of samples: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{s=\\left\\{\\begin{array}{l l}{\\Theta\\left(\\frac{\\sqrt{n}}{\\epsilon^{2}}\\right)}&{\\quad i f d\\leq\\alpha}\\\\ {\\Theta\\left(\\operatorname*{min}\\left(\\frac{1}{(d-\\alpha)^{2}},\\,\\frac{\\sqrt{n}}{\\epsilon^{2}}\\right)\\right)}&{\\quad i f d>\\alpha}\\end{array}\\right.,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $d=\\lVert q-\\hat{p}\\rVert_{T V}\\left(q$ is the known distribution for identity testing, or the uniform distribution for uniformity). ", "page_idx": 3}, {"type": "text", "text": "Remark 3. Note that $d$ is not an input parameter to the algorithm. However, $d$ is determined by $\\hat{p}$ and $q$ , which are known to the algorithm, allowing us to compute $d$ . ", "page_idx": 3}, {"type": "text", "text": "For closeness testing, we prove the following. ", "page_idx": 3}, {"type": "text", "text": "Theorem 4 (Informal version of Theorem 9). Augmented closeness\u221a testing for distributions over $[n]$ , with parameters \u03b1, \u03f5, and $\\delta=2/3$ , requires $\\Theta(n^{2/3}\\alpha^{1/3}/\\epsilon^{4/3}+\\sqrt{n}/\\epsilon^{2})$ samples. ", "page_idx": 3}, {"type": "text", "text": "It can be seen that e.g., for closeness testing, our non-trivial predictor improves over the best possible sampling bound in the standard model. Specifically, as long as $\\alpha=o(1)$ , our bound in the augmented model improves over the prior work. At the same time, our algorithms are resilient: even if $\\alpha=\\Theta(1)$ , our sampling bounds do not exceed those in the standard model. Note that all theorems are complemented by tight lower bounds. We highlight that all of our algorithms are also computationally efficient, running in polynomial time with respect to $n$ and $1/\\epsilon$ . The results are summarized in Table 1. ", "page_idx": 3}, {"type": "text", "text": "Table 1: Optimal sample complexity bounds in the standard model versus the augmented bounds. $\\alpha$ is the suggested accuracy level for the $L_{1}$ -distance between $p$ and $\\hat{p}$ . $d$ denotes the total variation distance between the known distribution $q$ and $\\hat{p}$ . ", "page_idx": 3}, {"type": "table", "img_path": "tAlMAcqK9s/tmp/1cb47b82735ef818a5a47c1b5b92afc6bd71ad5256748bcaa8eb5c8fad7b6bfd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "Our empirical Results: We empirically evaluate our augmented closeness testing algorithm on synthetic and real distributions and refer all details to Section E. ", "page_idx": 3}, {"type": "text", "text": "As a summary, our algorithm can indeed leverage predictions to obtain significantly improved sample complexity over the SOTA approach without predictions, as well as SOTA algorithms needing very accurate predictions [28]. ", "page_idx": 3}, {"type": "text", "text": "For distributions similar to our lower bound instances, our augmented algorithm achieves ${>}20\\mathbf{x}$ reduction in sample complexity to obtain comparable accuracy as the standard un-augmented algorithm, as shown in Figure 1. On real distributions curated from network traffic data, we see sample complexity reductions of up to $40\\%$ . Furthermore, our algorithm is empirically robust to noisy predictions, in contrast to prior state of the art approaches which assume very accurate, point-wise predictions (CRS\u201915 in Figure 1). ", "page_idx": 3}, {"type": "text", "text": "It is worth noting that our experiments on network traffic data reveal that the actual sample complexity is much lower than the anticipated worst-case sample complexity of our algorithm. In particular, this holds even when $\\hat{p}$ is far from $p$ in terms of total variation distance. Empirically, if $\\hat{p}$ accurately reflects the high-probability elements in $p$ , our algorithm can significantly reduce the sample complexity needed for testing by utilizing these heavy hitter \u201chints\u201d from $\\hat{p}$ . This is validated by our results and depicted in Figures 5 and 7(b). ", "page_idx": 3}, {"type": "image", "img_path": "tAlMAcqK9s/tmp/33dd4a2f30508a3497e88c2238e3c77f9b0b556e3cbd27ed1d150267eae1718b.jpg", "img_caption": ["Figure 1: Error vs sample complexity for the theoretically hard instance (See Sec. E). "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "1.3 Related works ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To the best of our knowledge, there have been only three prior works that studied any distribution property testing algorithms with predictions, each assuming a much stronger prediction model: ", "page_idx": 4}, {"type": "text", "text": "\u2022 Distribution testing with perfect predictors [29]: this work studied distribution testing problems, including closeness, identity and uniformity, assuming query access to a perfect predictor, i.e., ${\\hat{p}}=p$ . They show that, given a perfect predictor, it is possible to obtain highly efficient testers for a wide variety of problems with a small number of queries to the prediction. Unfortunately, the perfect-predictor assumption is often too strong to hold in practice, as demonstrated e.g., in [48] and in our experiments. \u2022 Distribution testing with $(1+\\epsilon)$ -approximate predictors [28, 67]: these works relax the assumption used in the above paper, requiring only that $\\hat{p}_{i}=(1\\pm\\epsilon/2)p_{i}$ for all $i$ and sufficiently small $\\epsilon>0$ . However, this assumption is still quite restrictive, especially for low values of $\\epsilon$ . Indeed in our experimental setting, such algorithms are not robust to prediction errors (see Section E). \u2022 Support estimation with $c$ -approximate predictors [48]: this work focused on the single problem of support estimation, i.e., estimating the fraction of coordinates $i$ such that $p_{i}>0$ . It further relaxes the assumption in [67] by allowing the predicted probabilities $\\hat{p}_{i}$ to be within a factor of $c$ of the true probabilities $p_{i}$ , for any constant approximation factor $c>1$ . This algorithm has been shown to work well in practice. However, the techniques presented in that paper seem to be applicable exclusively to support estimation. Furthermore, their result provably does not hold under the assumption that $p$ and $\\hat{p}$ are close in TV distance, as in this paper. ", "page_idx": 4}, {"type": "text", "text": "In summary, prior results required either highly restrictive assumptions, or were applicable to only a single problem. None of the previous algorithms worked under the TV distance assumption used in this paper which is arguably the most natural. Further exploration of measures such as $L_{p}$ -distances, KL-divergence, and Hellinger distance are interesting open questions. ", "page_idx": 4}, {"type": "text", "text": "We remark that we can mathematically show that these alternative oracles yield provably more power compared to ours (i.e., we make weaker assumptions about the predictor). We provide an alternative to our upper bound techniques for these alternate prediction models in Section G. We demonstrate how a variant of our algorithm, in conjunction with these stronger predictions, implies that unif\u221aormity testing, identity testing, and closeness testing in these models can be conducted using only $O(\\sqrt{n}/\\epsilon^{2})$ samples. This low sample complexity effectively circumvents our lower bound for closeness testing, suggesting that these models provide stronger, and arguably less realistic, predictions. ", "page_idx": 4}, {"type": "text", "text": "Other related works: Other related works discussing algorithms with predictions and general distribution testing are discussed in Appendix A. ", "page_idx": 4}, {"type": "text", "text": "1.4 Notation and organization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Notation: We use $[n]$ to denote the set $\\{1,2,\\ldots,n\\}$ . All of our distributions will be over the domain $[n]$ , and we assume $n$ is always known. For a distribution $p$ , we denote the probability of the $i$ -th element by $p_{i}$ . For any subset of the domain $S\\subseteq[n]$ we denote the probability mass of $S$ according to $p$ by $p(S)$ . We use $p^{\\otimes s}$ to denote the probability distribution of $s$ i.i.d. samples drawn from $p$ . We say $p$ is a known distribution, if we have access to every $p_{i}$ . We say $p$ is an unknown distribution if we have only sample access to $p$ . We use $\\mathbf{Poi}(\\lambda)$ to denote a random variable from a Poisson distribution with mean $\\lambda$ . Similarly, $\\mathbf{Ber}(\\lambda)$ indicates a random variable from the Bernoulli distribution that is one with probability $\\lambda$ . Given two distributions $p$ and $q$ over a domain $\\mathcal{X}$ , we use $\\|p-q\\|_{\\mathrm{TV}}:=\\operatorname*{sup}_{S\\subseteq\\mathcal{X}}|p(S\\bar{)}-q(S)|$ to denote the total variation distance between $p$ and $q$ . We say $p$ and $q$ are $\\epsilon$ -close $\\mathbf{\\chi}_{\\epsilon}$ -far), if the total variation distance between $p$ and $q$ is at most $\\epsilon$ (larger than $\\epsilon$ ). ", "page_idx": 4}, {"type": "text", "text": "Organization: We provide an overview of our theoretical results in Section 2. The upper bound on augmented closeness testing is presented in Section 3. The search algorithm is detailed in Section B. ", "page_idx": 4}, {"type": "text", "text": "Augmented uniformity and identity testing are discussed in Section C, where the upper bounds are presented in Section C.1, and the lower bounds are provided in Sections C.2 and C.3. ", "page_idx": 5}, {"type": "text", "text": "Augmented closeness testing is discussed in Section D, with the lower bounds provided in Section D.1.   \nOur empirical evaluations are presented in Section E. ", "page_idx": 5}, {"type": "text", "text": "2 Overview of our proofs ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Search algorithm: The search algorithm seeks to find the smallest value of $\\alpha$ for which the problem is solvable via the augmented tester. It starts with the lowest value of $\\alpha$ (most accurate prediction). Then, it iteratively increases the sample budget across rounds. In each round $i$ , it selects an $\\alpha_{i}$ for the augmented tester, $\\boldsymbol{\\mathcal{A}}$ , ensuring operation within the current sample budget. If $\\boldsymbol{\\mathcal{A}}$ outputs accept or reject, the algorithm echoes this outcome. If inaccurate information is returned, the sample budget doubles for the next round. It is worth noting that this search scheme is applicable to a general distribution testing algorithm with polynomial sample complexity. The algorithm\u2019s pseudocode and its performance proof are in Section B. ", "page_idx": 5}, {"type": "text", "text": "Upper bound for augmented identity testing Let $d$ represent the distance between $\\hat{p}$ and $q$ (the prediction and the known distribution). For establishing the upper bound, it is essential to assume $d>\\alpha$ . If not, the prediction proves unhelpful, and we might as well resort to the standard tester. ", "page_idx": 5}, {"type": "text", "text": "Our upper bound relies on a simple but fundamental observation regarding the total variation distance: this distance is the maximum discrepancy between the probability masses that two distributions assign to any subset of their domain. To prove that total variation between two distributions is small entails proving that the discrepancy across every domain subset is small. In contrast, to prove a large total variation distance, one only needs to identify a single subset with a large discrepancy as evidence of large total variation distance. We use the Scheff\u00e9 set of $\\hat{p}$ and $q$ \u2014characterized as the collection of elements $x\\in[n]$ where $\\hat{p}(x)\\,<\\,q(x)$ , symbolized by $S$ \u2014as evidence of $p$ \u2019s divergence from either $q$ or $\\hat{p}$ . More precisely, it is known that $S$ maximizing the discrepancy between the probability masses of $\\hat{p}$ and $q$ , implying: $d:=\\lVert q-\\hat{p}\\rVert_{\\mathrm{TV}}=|q(S)-\\bar{\\hat{p}}(S)|$ . Next, we estimate the probability of set $S$ according to $p$ with reasonably high accuracy. Given $q(S)$ and $\\hat{p}(S)$ , then $p(S)$ is either significantly different from $q(S)$ , or it deviates from $\\hat{p}(S)$ . In the first scenario, this discrepancy serves as evidence that $p\\neq q$ , allowing us to output reject. In the second scenario, the deviation confirms that $p$ is $\\alpha$ -distant from $\\hat{p}$ , leading us to output inaccurate information. Further details can be found in Section C.1. ", "page_idx": 5}, {"type": "text", "text": "Lower bound for augmented uniformity testing We provide two lower bounds for augmented uniformity testing. One is purely based on a reduction to standard uniformity testing for the case where $\\alpha\\geq d$ . (Recall that $d$ was the total variation distance between $q$ and $\\hat{p}$ ). See Section C.2. ", "page_idx": 5}, {"type": "text", "text": "The other lower bound applies to the setting where $\\alpha<d$ . One challenge of this problem was that it is hard to find two difficult distributions that the tester has to distinguish between; usually, for a pair of distributions, we could come up with one valid output that could serve them both. For example, for both uniform distribution, and the famous $\\epsilon$ -far uniform distribution that assigns probabilities $(1\\pm\\epsilon)/n$ to the elements, the algorithm may be able to output inaccurate information. Hence, we cannot draw lower bounds just by asking the algorithm to distinguish between two distributions. ", "page_idx": 5}, {"type": "text", "text": "For this reason, we provide three distributions that look similar when we draw too few samples. We formalize the similarity of these three distributions using a multivariate coupling argument. We show that these distributions are such that there is no possible answer that is valid for all three of them. Now, (similar to Le Cam\u2019s method), suppose we feed the algorithm with samples from one of these three distributions (each with probability 1/3). For any sample set, the algorithm outputs an answer (which may be randomized); however, this answer is considered wrong for at least one of the underlying distributions. This is due to the fact that there is no universally valid answer that is simultaneously correct for all three distributions. Hence, if the algorithm outputs a valid answer with high probability, it must be able to distinguish the underlying distributions to some degree. On the other hand, the indistinguishability result says it is impossible to tell these distributions apart. Thus, we reach a contradiction. And, the lower bound is concluded. See Section C.3 for further details. ", "page_idx": 5}, {"type": "text", "text": "Lower bound for augmented closeness testing: We provide two separate lower bounds for closeness testing based on the relationship between $\\alpha$ and $\\epsilon$ . Further details are available in Section D.1. ", "page_idx": 6}, {"type": "text", "text": "Our first lower bound, as detailed in Theorem 11, employs a reduction strategy from standard closeness testing to augmented closeness testing when $\\alpha\\geq\\epsilon$ . This is achieved by taking instances used in standard closeness testing for distributions over $[n-1]$ and embedding them into the first $n-1$ domain elements of a new distribution $p$ defined over $[n]$ . The key to this strategy is to put the majority of the distribution\u2019s mass $((1-\\alpha)$ mass) on its final element, and we set the prediction $\\hat{p}$ to be a singleton distribution over the last element, which is $\\alpha$ -close to $p$ . Clearly, the prediction does not reveal any information about the first $n-1$ elements of $p$ , implying that testing the closeness of $p$ in the augmented setting is as challenging as in the standard setting, once the parameters are appropriately scaled. ", "page_idx": 6}, {"type": "text", "text": "Our second lower bound, outlined in Theorem 12, is more involved. In the standard setting, the lower bound for closeness testing is derived from the hard instances for uniformity testing with one crucial adjustment: adding new elements with large probability (approximately $n^{-2/3}$ ) in the distributions. These large elements have identical probability masses in both $p$ and $q$ , indicating they do not contribute to the distance between the two distributions. However, their presence in the sample set confuses the algorithm: due to their high probabilities, their behavior in the sample set may misleadingly suggest non-uniformity, complicating the algorithm\u2019s task of determining the uniformity of the rest of the distribution. Therefore, the algorithm requires $s\\approx n^{2/3}$ samples to first identify these large elements before it can test the uniformity of the remaining distribution. Surprisingly, this requirement of $s\\approx n^{2/3}$ samples is significantly higher than the $O({\\sqrt{n}})$ samples typically sufficient for testing uniformity. ", "page_idx": 6}, {"type": "text", "text": "The challenge in our case arises because $\\hat{p}$ may disclose the large elements to the algorithm. To establish the lower bound, we set $\\hat{p}$ to be the uniform distribution, we generate hard instances of $p$ by adding as many large elements as possible, without altering $\\hat{p}$ , by keeping the overall probability mass of the large elements limited to $\\alpha$ . More precisely, $p$ assigns approximately $(1-\\bar{\\alpha})/n$ probability mass to $O(n)$ elements chosen at random, and assigns approximately $n^{-2/3}$ probability mass to $\\alpha\\cdot n^{2/3}$ elements in the domain. Now, $q$ has two scenarios. Half the time, $q$ is identical to $p$ . The other half, $q$ retains the same large elements but deviates slightly from uniformity for the rest of the distribution. Specifically, $q$ assigns probabilities $(1\\pm\\Theta(\\epsilon))\\bar{(}1-\\alpha)/n$ to the randomly chosen $O(n)$ elements, making it $\\epsilon_{}$ -far from $p$ . It is not hard to show that testing closeness of $p$ and $q$ is a symmetric property (since permuting the domain elements does not affect our construction). By leveraging the wishful thinking theorem from [78], we demonstrate that these two scenarios are indistinguishable unless $\\Omega(n^{2/3}\\bar{\\alpha^{1/3}})$ samples are drawn. ", "page_idx": 6}, {"type": "text", "text": "3 Upper bound for closeness testing ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our upper bound is based on a technique called flattening, which has been previously proposed by Diakonikolas and Kane [42]. This technique is instrumental in reducing the variance of the statistic used for closeness testing by reducing the $\\bar{\\ell}_{2}^{2}$ -norm of the input distributions. We adapt this technique for use in the augmented setting, aiming not only to flatten the distribution based on the samples received from it but also exploiting the prediction distribution $\\hat{p}$ . We demonstrate that augmented flattening can significantly reduce the $\\ell_{2}^{2}$ -norm of $p$ . In our algorithm, we initially check if the $\\bar{\\ell}_{2}^{2}$ -norm of $p$ is reduced after flattening to the desired bound. If not, this indicates that the prediction was not sufficiently accurate, leading us to output inaccurate. Conversely, if the $\\ell_{2}^{2}$ -norm of $p$ is small, we proceed with an efficient testing algorithm that requires fewer samples. We describe the standard flattening technique in Section 3.1. Our flattening technique presented in Section 3.2. Finally, we provide our algorithm in Section 3.2.1. ", "page_idx": 6}, {"type": "text", "text": "3.1 Background on flattening ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Suppose we are given $n$ parameters $m_{1},m_{2},\\ldots,m_{n}$ . One can create a randomized mapping $F$ that assigns each $i\\,\\in\\,[n]$ to a pair $(i,j)$ , where $j$ is drawn uniformly at random from $[m_{i}]$ . Now, consider a given distribution $p$ over $[n]$ and a sample $i$ drawn from $p$ . This mapping induces a distribution over pairs $(i,j)$ \u2019s in $D:=\\bar{\\{(i,j)|}i\\in[n]}$ and $j\\in[m_{i}]\\}$ . We denote this new distribution by $\\boldsymbol{p}^{(F)}$ satisfying the relation: $p_{(i,j)}^{(F)}=p_{i}/m_{i}$ . The core idea of the above mapping is to divide the probability of the $i$ -th element into $m_{i}$ buckets. If the values of $m_{i}$ \u2019s are large for elements in $p$ with higher probabilities, then the resulting distribution $\\boldsymbol{p}^{(F)}$ will avoid having any elements with disproportionately large probabilities, thereby naturally lowering its $\\ell_{2}$ -norm. Diakonikolas and Kane [42] showed that if we draw $\\mathbf{Poi}(t)$ samples from $p$ , and set each $m_{i}$ to be the frequency of element $i$ among these samples plus one, then we have: ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{E}\\left[\\left\\|p^{(F)}\\right\\|_{2}^{2}\\right]\\leq{\\frac{1}{t}}\\,,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where the expectation is taken over the randomness of the samples. ", "page_idx": 7}, {"type": "text", "text": "Connection to distribution testing: Chan et al. in [30] showed that one can test closeness of two distributions over $[n]$ using $O(n\\operatorname*{max}(\\|p\\|_{2}\\,,\\|q\\|_{2})/\\epsilon^{2})$ samples. Diakonikolas et al. [42] used this tester in combination of the flattening technique to map distributions to distributions over slightly larger domains with the goal of reducing their $\\ell_{2}$ -norms. They have shown that if we use the same mapping to reduce the $\\ell_{2}$ -norm of both $p$ and $q$ , the $\\ell_{2}$ -distance between the two distribution does not change the $\\ell_{1}$ distance between distributions. ", "page_idx": 7}, {"type": "text", "text": "Fact 3.1 ([42]). For any flattening scheme $F$ , the $\\ell_{1}$ -distance is preserved under $F$ . That is, for every pair of distributions $p$ and $q$ , we have: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\|p-q\\|_{1}=\\|p^{(F)}-q^{(F)}\\|_{1}\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Therefore, to test $p$ and $q$ , we can draw samples to come up with the flattening $F$ to reduce the $\\ell_{2}$ -norm of one of the underlying distributions. Then, we can test the closeness of $\\boldsymbol{p}^{(F)}$ and $q^{(F)}$ with $O(n\\operatorname*{max}(\\left|\\left|p^{(F)}\\right|\\right|_{2},\\left|\\left|q^{(\\check{F})}\\right|\\right|_{2}^{\\checkmark})/\\epsilon^{2})$ new samples. Later, Aliakbarpour et al. in [9] showed that $O(n\\operatorname*{min}(\\|p\\|_{2}\\,,\\|q\\|_{2})/\\epsilon^{2})$ samples is sufficient. ", "page_idx": 7}, {"type": "text", "text": "The exact sample complexity is determined by balancing the samples needed to determine the flattening and the samples needed to test closeness of $\\boldsymbol{p}^{(F)}$ and $q^{(F)}$ . Note that flattened distributions have a larger domain size. Thus, one must also balance the gains obtained from the reduction in $\\ell_{2}$ norm with the increase in the domain. ", "page_idx": 7}, {"type": "text", "text": "3.2 Augmented flattening ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We adapt the flattening argument in [42] to the augmented setting. ", "page_idx": 7}, {"type": "text", "text": "Lemma 3.2. Suppose we have an unknown distribution $p=(p_{1},p_{2},...,p_{n})$ over $[n]$ and $\\mathbf{Poi}(s_{f})$ samples from $p$ . Assume we are given a known distribution $\\hat{p}$ that is $\\alpha$ -close to $p$ in TV distance. Then for every $v\\leq1$ , there exists a flattening $F$ which increases the domain size by $1/v+s_{f}$ in expectation and the expected $\\ell_{2}^{2}$ -norm of the $\\boldsymbol{p}^{(F)}$ is bounded by: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{E}\\bigg[\\Big\\|p^{(F)}\\Big\\|_{2}^{2}\\bigg]\\leq\\frac{2\\alpha}{s_{f}}+4\\cdot\\nu\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The expectation above is taken over the randomness of the samples from $p$ . ", "page_idx": 7}, {"type": "text", "text": "Proof. Let $\\hat{s}_{f}:=\\mathbf{Poi}(s_{f})$ be a Poisson random variable with mean $s_{f}$ . Let $\\mathcal{S}_{f}$ denote the multiset of $\\hat{s}_{f}$ samples from $p$ . Let $f_{i}$ denotes the frequency of element $i$ in $\\scriptstyle{S_{f}}$ . For every $i$ , we define the number of buckets for element $i$ as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\nm_{i}:=\\left\\lfloor\\frac{\\hat{p}_{i}}{\\nu}\\right\\rfloor+f_{i}+1\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Let $\\boldsymbol{p}^{(F)}$ denote the flattened version of $p$ where we split every element $i$ into $m_{i}$ buckets. We show that the expected $\\ell_{2}$ -norm of $\\boldsymbol{p}^{(F)}$ is low. Define $\\Delta_{i}$ to be $p_{i}-\\hat{p}_{i}$ . Suppose $A$ is a set of indices $i\\in[n]$ for which $\\Delta_{i}\\geq\\hat{p}_{i}$ . For every $i\\in A$ , $p_{i}$ is bounded from above by $2\\,\\Delta_{i}$ . On the other hand, for every $i\\in[n]\\setminus A,p_{i}$ is bounded by $2\\,\\hat{p}_{i}$ . ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\|p^{(F)}\\right\\|_{2}^{2}=\\sum_{i\\in[n]}\\sum_{j\\in[m_{i}]}p^{(F)}(i,j)^{2}=\\sum_{i\\in[n]}\\frac{p_{i}^{2}}{m_{i}}=\\sum_{i\\in A}\\frac{2\\,\\Delta_{i}\\cdot p_{i}}{m_{i}}+\\sum_{i\\in[n]\\backslash A}\\frac{(2\\,\\hat{p}_{i})^{2}}{m_{i}}}\\\\ {\\displaystyle\\leq\\sum_{i\\in A}\\frac{2\\,\\Delta_{i}\\cdot p_{i}}{f_{i}+1}+\\sum_{i\\in[n]\\backslash A}\\frac{(2\\,\\hat{p}_{i})^{2}}{\\hat{p}_{i}/\\nu}}\\\\ {\\displaystyle\\leq\\sum_{i\\in A}\\frac{2\\,\\Delta_{i}\\cdot p_{i}}{f_{i}+1}+4\\,\\nu.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "As shown in [42], the expected value of $p_{i}/(f_{i}+1)$ taken over the randomness in the $\\ensuremath{\\mathcal{S}}_{f}$ is bounded by $1/s_{f}$ : ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbf{E}_{F}\\Bigg[\\frac{p_{i}}{f_{i}+1}\\Bigg]\\leq\\frac{1}{s_{f}}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Therefore, we obtain: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbf{E}_{F}\\bigg[\\Big\\|p^{(F)}\\Big\\|_{2}^{2}\\bigg]\\le\\sum_{i\\in A}\\frac{2\\,\\Delta_{i}}{s_{f}}+4\\,\\nu\\le\\frac{2\\,\\alpha}{s_{f}}+4\\,\\nu\\,.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The last inequality above is due to the fact that: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\sum_{i\\in A}\\Delta_{i}=p(A)-\\hat{p}(A)\\leq\\|p-\\hat{p}\\|_{\\mathrm{TV}}=\\alpha\\,.\\quad\\Omega\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "3.2.1 The algorithm ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we provide an algorithm for testing closeness of two distribution in the augmented setting. Our algorithm receives a suggested accuracy level $\\alpha$ . Based on $\\alpha$ , we draw $s_{f}$ (which depends on $\\alpha$ ) samples from $p$ and use them to flatten $p$ . If the resulting distribution has sufficiently small $\\ell_{2}^{2}$ -norm we proceed to the testing phase. Otherwise, we declare that the accuracy level provided is not correct. The pseudocode of our algorithm is provided in Algorithm 1, and we prove its performance in Theorem 5. ", "page_idx": 8}, {"type": "text", "text": "Algorithm 1 An augmented tester for testing closeness of $p$ and $q$ with a suggested value $\\alpha$ ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "1: procedure AUGMENTED-CLOSENESS-TESTER(n, \u03b1, \u03f5, $\\delta_{1}=0.1$ , $\\delta_{2}=2/3$ , $\\hat{p}$ , sample access   \nto $p$ and $q$ )   \n2: $s_{f}\\gets\\operatorname*{min}\\left(n^{2/3}\\alpha^{1/3}/\\epsilon^{4/3},n\\right)$   \n3: $\\hat{s}_{f}\\gets\\mathbf{Poi}(s_{f})$   \n4: if $\\hat{s}_{f}>10\\,s_{f}$ then   \n5: return reject.   \n6: $F\\gets F(m_{1},m_{2},\\ldots,m_{n})$ where $m_{i}$ are define in Equation 2.   \n7: $\\begin{array}{r}{L_{p}\\gets\\mathrm{ESTIMATE-}\\ell_{2}^{2}\\,\\left(p^{(F)},\\,\\sum_{i=1}^{n}m_{i}\\right.}\\end{array}$ , 0.05   \n8: $\\begin{array}{r}{L_{q}\\gets\\mathrm{ESTIMATE}{\\ \u2013\\ell_{2}^{2}}\\ \\big(q^{(F)},\\ \\sum_{i=1}^{n}m_{i}}\\end{array}$ , 0.05   \n9: if $\\begin{array}{r}{L_{p}>30\\left(\\frac{2\\alpha}{s_{f}}+\\frac{4}{n}\\right)}\\end{array}$ then   \n10: return inaccurate information.   \n11: else   \n12: if $\\begin{array}{r}{L_{q}>90\\left(\\frac{2\\alpha}{s_{f}}+\\frac{4}{n}\\right)}\\end{array}$ then   \n13: return reject.   \n14: else   \n15: return CLOSENESS-TESTER $\\begin{array}{r}{\\Big(\\epsilon,\\;\\delta=0.1,\\;b=90\\left(\\frac{2\\alpha}{s_{f}}+\\frac{4}{n}\\right),\\;p^{(F)},\\;q^{(F)}\\Big).}\\end{array}$ ", "page_idx": 8}, {"type": "text", "text": "Theorem 5. For any given parameters $n$ , $\\alpha$ , \u03f5, and any two unknown distributions $p$ and $q$ and a known predicted distribution $\\hat{p}$ for $p_{\\mathrm{:}}$ , Algorithm $^{l}$ is an $(\\alpha,\\,\\epsilon,\\delta=0.3)$ -augmented tester for testing closeness of $p$ and $q$ which uses $O(n^{2/3}\\alpha^{1/3}/\\epsilon^{4/3})$ samples. ", "page_idx": 8}, {"type": "text", "text": "Proof. We begin the proof by setting our parameters. Set $s_{f}:=\\operatorname*{min}\\left(n^{2/3}\\alpha^{1/3}/\\epsilon^{4/3},n\\right)$ . Suppose we draw $\\hat{s}_{f}=\\mathbf{Poi}(s_{f})$ samples from $p$ . We flatten the distribution $p$ according to the augmented flattening we described in Section 3.2 replacing $\\nu$ with $1/n$ . We denote the flattening by ${\\cal F}=$ $F(m_{1},\\ldots,m_{n})$ where $m_{i}$ \u2019s are defined in Equation (1): ", "page_idx": 9}, {"type": "equation", "text": "$$\nm_{i}:=\\lfloor n\\cdot\\hat{p}_{i}\\rfloor+f_{i}+1\\,.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "In the above definition, $f_{i}$ is the frequency of element $i$ among the $\\hat{s}_{f}$ drawn samples. ", "page_idx": 9}, {"type": "text", "text": "Proof of correctness: Suppose we flatten both $p$ and $q$ according to $F$ . The algorithm estimates the $\\ell_{2}^{2}$ of $\\boldsymbol{p}^{(F)}$ and $q^{(F)}$ . According to Fact F.1, these estimates are within a constant factor of their true values each with probability at least $1-0.05$ : ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\frac{\\left\\|p^{(F)}\\right\\|_{2}}{2}\\leq L_{p}\\leq\\frac{3\\left\\|p^{(F)}\\right\\|_{2}}{2}\\qquad\\mathrm{and}\\qquad\\frac{\\left\\|q^{(F)}\\right\\|_{2}}{2}\\leq L_{q}\\leq\\frac{3\\left\\|q^{(F)}\\right\\|_{2}}{2}\\,.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "To prove the correctness of the algorithm, we show the three desired property of an augmented tester which are defined in Definition 1.1. First, assume $\\hat{p}$ is $\\alpha$ -close to $p$ . We show the probability of outputting inaccurate information is small. Using Lemma 3.2, after applying $F$ , the expected $\\ell_{2}^{2}$ -norm of $\\boldsymbol{p}^{(F)}$ is bounded by: ", "page_idx": 9}, {"type": "equation", "text": "$$\n{\\bf E}\\Big[\\Big\\|p^{(F)}\\Big\\|_{2}\\Big]\\le\\frac{2\\alpha}{s_{f}}+\\frac{4}{n}\\,.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Using Markov\u2019s inequality, with probability at least 0.95, $\\ell_{2}^{2}$ -norm of $\\boldsymbol{p}^{(F)}$ is at most 20 times the above bound. Combined by the upper bound for $L_{p}$ in Equation (3) and the union bound, in this case $L_{p}$ is bounded by $30\\cdot(2\\alpha{\\bar{/}}s_{f}+{\\bar{4/}}n)$ with probability at least 0.9. Hence, we the algorithm does not output inaccurate information in Line 10 with probability more than 0.1. ", "page_idx": 9}, {"type": "text", "text": "Second, we show that if $p=q$ , the algorithm outputs reject with small probability. We may output reject in three cases: 1) in Line 5 2) in Line 13 and 3) in Line 15. Using Markov\u2019s inequality, a Poisson random variable is more than 10 times its expectation with probability at most 0.1. We show the other two cases are unlikely as long as $L_{p}$ and $L_{q}$ are accurate estimates of the $\\ell_{2}^{2}\\mathbf{s}$ of $\\boldsymbol{p}^{(F)}$ and $q^{(F)}$ (which happens with probability of 0.9). If $\\underline{{p}}=q,p^{(F)}$ and $q^{(F)}$ are identical. Therefore, in the algorithm $L_{p}$ and $L_{q}$ are the estimations of the $\\ell_{2}^{2}$ -norm of the same distribution. Note that we output reject in Line 13, only when $L_{p}<3L_{q}$ . Using Equation (3), this event does not happen unless at least of the estimates are inaccurate (which has a probability at most 0.1). Furthermore if the estimated $\\ell_{2}^{2}$ -norm are accurate, the minimum $\\ell_{2}^{2}$ -norm of $\\boldsymbol{p}^{(F)}$ and $q^{(F)}$ are bounded by $90\\cdot(2\\alpha/s_{f}+4/n)$ implying the tester would work correctly with probability at least 0.9. Using the union bound, the probability of outputting reject is bounded by 0.3. ", "page_idx": 9}, {"type": "text", "text": "Third and last, we show that if $p$ is $\\epsilon\\cdot$ -far from $q^{(F)}$ , the probability of outputting accept is small. We only output accept in Line 15. Similar to the second case we have discussed, as long as the $\\ell_{2}^{2}$ -norm are accurate, the tester does not make a mistake with probability more than 0.1. Therefore, the overall probability of making a mistake is bounded by 0.2. ", "page_idx": 9}, {"type": "text", "text": "Analysis of sample complexity: For the flattening step we have used $\\hat{s}_{f}\\le10s_{f}$ . After flattening, the new domain size is bounded from above by: ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}m_{i}\\leq\\sum_{i=1}^{n}n\\cdot\\hat{p}_{i}+f_{i}+1=n+\\hat{s}_{f}+n\\leq12n.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Using Fact F.1, estimation of $\\ell_{2}^{2}$ -norm requires $O({\\sqrt{n}})$ samples. Using Fact F.2, the number of samples we use for the test in Line 15 is ${\\bar{O(n\\cdot b/\\epsilon^{2})}}$ samples. Hence, the total sample complexity is: ", "page_idx": 9}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{{\\mathrm{~}}{\\#}\\operatorname{samples}=O\\left(s_{f}+{\\sqrt{n}}+{\\frac{n\\cdot b}{\\epsilon^{2}}}\\right)=O\\left(s_{f}+{\\sqrt{n}}+{\\frac{n\\cdot{\\sqrt{2\\alpha/s_{f}+4/n}}}{\\epsilon^{2}}}\\right)}\\\\ &{\\qquad\\qquad=O\\left({\\frac{\\sqrt{n}}{\\epsilon^{2}}}+{\\frac{n^{2/3}\\,\\alpha^{1/3}}{\\epsilon^{4/3}}}\\right)\\,.}\\end{array}}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "In the last line we use that $s_{f}:=\\operatorname*{min}\\left(n^{2/3}\\alpha^{1/3}/\\epsilon^{4/3},n\\right)$ . Thus, the proof is complete. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Part of this work was conducted while the authors were visiting the Simons Institute for the Theory of Computing. M.A. was supported by NSF awards CNS-2120667, CNS-2120603, CCF-1934846, and BU\u2019s Hariri Institute for Computing. This work was initiated while M.A. was affliiated with Boston University and Northeastern University and was done in part while M.A. was a research fellow at the Simons Institute for the Theory of Computing. P.I. was supported by the NSF TRIPODS program (award DMS-2022448) and Simons Investigator Award. R.R. was supported by the NSF TRIPODS program (award DMS-2022448) and CCF-2310818. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Anders Aamand, Alexandr Andoni, Justin Y. Chen, Piotr Indyk, Shyam Narayanan, and Sandeep Silwal. Data structures for density estimation. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 1\u201318. PMLR, 23\u201329 Jul 2023. ", "page_idx": 10}, {"type": "text", "text": "[2] Anders Aamand, Justin Y. Chen, Huy L\u00ea Nguyen, Sandeep Silwal, and Ali Vakilian. Improved frequency estimation algorithms with and without predictions. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.   \n[3] J. Acharya, C. Canonne, C. Freitag, and H. Tyagi. Test without trust: Optimal locally private distribution testing. In Proceedings of Machine Learning Research, volume 89 of Proceedings of Machine Learning Research, pages 2067\u20132076. PMLR, 2019.   \n[4] J. Acharya, Z. Sun, and H. Zhang. Differentially private testing of identity and closeness of discrete distributions. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, pages 6879\u20136891, 2018.   \n[5] Jayadev Acharya, Constantinos Daskalakis, and Gautam Kamath. Optimal testing for properties of distributions. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 3591\u20133599, 2015.   \n[6] Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, and Ananda T. Suresh. Sublinear algorithms for outlier detection and generalized closeness testing. In IEEE ISIT, pages 3200\u20133204, 2014.   \n[7] Maryam Aliakbarpour, Eric Blais, and Ronitt Rubinfeld. Learning and testing junta distributions. In Proceedings of the 29th Conference on Learning Theory, COLT 2016, New York, USA, June 23-26, 2016, pages 19\u201346, 2016.   \n[8] Maryam Aliakbarpour, Mark Bun, and Adam Smith. Hypothesis selection with memory constraints. To appear in NeurIPS 2023, 2023.   \n[9] Maryam Aliakbarpour, Ilias Diakonikolas, Daniel Kane, and Ronitt Rubinfeld. Private testing of distributions via sample permutations. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pages 10877\u201310888, 2019.   \n[10] Maryam Aliakbarpour, Ilias Diakonikolas, and Ronitt Rubinfeld. Differentially private identity and equivalence testing of discrete distributions. In Proceedings of the 35th International Conference on Machine Learning (ICML), volume 80, pages 169\u2013178, 2018.   \n[11] Maryam Aliakbarpour, Themis Gouleakis, John Peebles, Ronitt Rubinfeld, and Anak Yodpinyanee. Towards testing monotonicity of distributions over general posets. In Proceedings of the Thirty-Second Conference on Learning Theory, COLT, pages 34\u201382, 2019.   \n[12] Maryam Aliakbarpour, Ravi Kumar, and Ronitt Rubinfeld. Testing mixtures of discrete distributions. In Alina Beygelzimer and Daniel Hsu, editors, Proceedings of the Thirty-Second Conference on Learning Theory, volume 99 of Proceedings of Machine Learning Research, pages 83\u2013114, Phoenix, USA, 25\u201328 Jun 2019. PMLR.   \n[13] Maryam Aliakbarpour and Sandeep Silwal. Testing Properties of Multiple Distributions with Few Samples. In Thomas Vidick, editor, 11th Innovations in Theoretical Computer Science Conference (ITCS 2020), volume 151 of Leibniz International Proceedings in Informatics (LIPIcs), pages 69:1\u201369:41, Dagstuhl, Germany, 2020. Schloss Dagstuhl\u2013Leibniz-Zentrum fuer Informatik.   \n[14] Kareem Amin, Matthew Joseph, and Jieming Mao. Pan-private uniformity testing. In Jacob D. Abernethy and Shivani Agarwal, editors, Conference on Learning Theory, COLT 2020, 9-12 July 2020, Virtual Event [Graz, Austria], volume 125 of Proceedings of Machine Learning Research, pages 183\u2013218. PMLR, 2020.   \n[15] Tugkan Batu, Lance Fortnow, Ronitt Rubinfeld, Warren D. Smith, and Patrick White. Testing that distributions are close. In 41st Annual Symposium on Foundations of Computer Science, FOCS 2000, 12-14 November 2000, Redondo Beach, California, USA, pages 259\u2013269, 2000.   \n[16] Tugkan Batu, Lance Fortnow, Ronitt Rubinfeld, Warren D. Smith, and Patrick White. Testing closeness of discrete distributions. JACM, 60(1):4:1\u20134:25, 2013.   \n[17] Tugkan Batu, Ravi Kumar, and Ronitt Rubinfeld. Sublinear algorithms for testing monotone and unimodal distributions. In Proceedings of the 36th Annual ACM Symposium on Theory of Computing, Chicago, IL, USA, June 13-16, 2004, pages 381\u2013390, 2004.   \n[18] Thomas Berrett and Cristina Butucea. Locally private non-asymptotic testing of discrete distributions is faster using interactive mechanisms. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.   \n[19] Bhaswar B. Bhattacharya and Gregory Valiant. Testing closeness with unequal sized samples. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 2611\u20132619, 2015.   \n[20] Arnab Bhattacharyya, Eldar Fischer, Ronitt Rubinfeld, and Paul Valiant. Testing monotonicity of distributions over general partial orders. Electronic Colloquium on Computational Complexity (ECCC), 17:27, 2010.   \n[21] B. Cai, C. Daskalakis, and G. Kamath. Priv\u2019it: Private and sample efficient identity testing. In International Conference on Machine Learning, ICML, pages 635\u2013644, 2017.   \n[22] Cl\u00e9ment L. Canonne. Big data on the rise: Testing monotonicity of distributions. In ICALP, 2015.   \n[23] Cl\u00e9ment L. Canonne. Corrigendum: Are few bins enough: Testing histogram distributions. In Proceedings of the 42nd ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems, PODS 2023, Seattle, WA, USA, June 18-23, 2023, page 359. ACM, 2023.   \n[24] Cl\u00e9ment L. Canonne, Ilias Diakonikolas, Daniel Kane, and Sihan Liu. Nearly-tight bounds for testing histogram distributions. In Advances in Neural Information Processing Systems 35 (NeurIPS), 2022. To appear.   \n[25] Cl\u00e9ment L. Canonne, Ayush Jain, Gautam Kamath, and Jerry Li. The price of tolerance in distribution testing. In Po-Ling Loh and Maxim Raginsky, editors, Conference on Learning Theory, 2-5 July 2022, London, UK, volume 178 of Proceedings of Machine Learning Research, pages 573\u2013624. PMLR, 2022.   \n[26] Cl\u00e9ment L. Canonne, Gautam Kamath, Audra McMillan, Adam D. Smith, and Jonathan R. Ullman. The structure of optimal private tests for simple hypotheses. In Moses Charikar and Edith Cohen, editors, Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, STOC 2019, Phoenix, AZ, USA, June 23-26, 2019, pages 310\u2013321. ACM, 2019.   \n[27] Cl\u00e9ment L Canonne, Gautam Kamath, Audra McMillan, Jonathan Ullman, and Lydia Zakynthinou. Private identity testing for high-dimensional distributions. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 10099\u201310111. Curran Associates, Inc., 2020.   \n[28] Cl\u00e9ment L Canonne, Dana Ron, and Rocco A Servedio. Testing probability distributions using conditional samples. SIAM Journal on Computing, 44(3):540\u2013616, 2015.   \n[29] Cl\u00e9ment L. Canonne and Ronitt Rubinfeld. Testing probability distributions underlying aggregated data. In Automata, Languages, and Programming - 41st International Colloquium, ICALP, pages 283\u2013295, 2014.   \n[30] Siu-on Chan, Ilias Diakonikolas, Paul Valiant, and Gregory Valiant. Optimal algorithms for testing closeness of discrete distributions. In SODA, pages 1193\u20131203, 2014.   \n[31] Justin Y. Chen, Talya Eden, Piotr Indyk, Honghao Lin, Shyam Narayanan, Ronitt Rubinfeld, Sandeep Silwal, Tal Wagner, David P. Woodruff, and Michael Zhang. Triangle and four cycle counting with predictions in graph streams. In 10th International Conference on Learning Representations, ICLR, 2022.   \n[32] Xi Chen, Rajesh Jayaram, Amit Levi, and Erik Waingarten. Learning and testing junta distributions with sub cube conditioning. In Mikhail Belkin and Samory Kpotufe, editors, Proceedings of Thirty Fourth Conference on Learning Theory, volume 134 of Proceedings of Machine Learning Research, pages 1060\u20131113. PMLR, 15\u201319 Aug 2021.   \n[33] Edith Cohen, Ofir Geri, and Rasmus Pagh. Composable sketches for functions of frequencies: Beyond the worst case. In Proceedings of the 37th International Conference on Machine Learning, 2020.   \n[34] Constantinos Daskalakis, Ilias Diakonikolas, and Rocco A. Servedio. Learning poisson binomial distributions. In Howard J. Karloff and Toniann Pitassi, editors, Proceedings of the 44th Symposium on Theory of Computing Conference, STOC 2012, New York, NY, USA, May 19 - 22, 2012, pages 709\u2013728. ACM, 2012.   \n[35] Constantinos Daskalakis, Ilias Diakonikolas, Rocco A. Servedio, Gregory Valiant, and Paul Valiant. Testing k-modal distributions: Optimal algorithms via reductions. In Proceedings of the Twenty-fourth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201913, pages 1833\u20131852, Philadelphia, PA, USA, 2013. Society for Industrial and Applied Mathematics.   \n[36] Constantinos Daskalakis and Gautam Kamath. Faster and sample near-optimal algorithms for proper learning mixtures of gaussians. In Conference on Learning Theory, pages 1183\u20131213. PMLR, 2014.   \n[37] Ilias Diakonikolas, Themis Gouleakis, Daniel M. Kane, and Sankeerth Rao. Communication and memory efficient testing of discrete distributions. In Alina Beygelzimer and Daniel Hsu, editors, Proceedings of the Thirty-Second Conference on Learning Theory, volume 99 of Proceedings of Machine Learning Research, pages 1070\u20131106. PMLR, 25\u201328 Jun 2019.   \n[38] Ilias Diakonikolas, Themis Gouleakis, J. Peebles, and Eric Price. Collision-based testers are optimal for uniformity and closeness. Chicago Journal of Theoretical Computer Science, 2019(1), MAY 2019.   \n[39] Ilias Diakonikolas, Themis Gouleakis, John Peebles, and Eric Price. Sample-optimal identity testing with high probability. In 45th International Colloquium on Automata, Languages, and Programming, ICALP 2018, July 9-13, 2018, Prague, Czech Republic, pages 41:1\u201341:14, 2018.   \n[40] Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Robust estimators in high-dimensions without the computational intractability. SIAM J. Comput., 48(2):742\u2013864, 2019.   \n[41] Ilias Diakonikolas, Daniel Kane, Vasilis Kontonis, Sihan Liu, and Nikos Zarifis. Efficient testable learning of halfspaces with adversarial label noise. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.   \n[42] Ilias Diakonikolas and Daniel M. Kane. A new approach for testing properties of discrete distributions. In IEEE 57th Annual Symposium on Foundations of Computer Science, FOCS 2016, 9-11 October 2016, Hyatt Regency, New Brunswick, New Jersey, USA, pages 685\u2013694, 2016.   \n[43] Ilias Diakonikolas, Daniel M. Kane, and Vladimir Nikishkin. Optimal algorithms and lower bounds for testing closeness of structured distributions. In FOCS, pages 1183\u20131202, 2015.   \n[44] Ilias Diakonikolas, Daniel M. Kane, and Vladimir Nikishkin. Testing identity of structured distributions. In SODA, pages 1841\u20131854, 2015.   \n[45] Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. Properly learning poisson binomial distributions in almost polynomial time. In Vitaly Feldman, Alexander Rakhlin, and Ohad Shamir, editors, Proceedings of the 29th Conference on Learning Theory, COLT 2016, New York, USA, June 23-26, 2016, volume 49 of JMLR Workshop and Conference Proceedings, pages 850\u2013878. JMLR.org, 2016.   \n[46] Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. Sharp bounds for generalized uniformity testing. In NeurIPS, pages 6204\u20136213, 2018.   \n[47] Elbert Du, Franklyn Wang, and Michael Mitzenmacher. Putting the \u201clearning\" into learningaugmented algorithms for frequency estimation. In Proceedings of the 38th International Conference on Machine Learning, pages 2860\u20132869, 2021.   \n[48] Talya Eden, Piotr Indyk, Shyam Narayanan, Ronitt Rubinfeld, Sandeep Silwal, and Tal Wagner. Learning-based support estimation in sublinear time. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.   \n[49] M. Gaboardi, H. W. Lim, R. M. Rogers, and S. P. Vadhan. Differentially private chi-squared hypothesis testing: Goodness of fit and independence testing. In International Conference on Machine Learning, ICML, pages 2111\u20132120, 2016.   \n[50] M. Gaboardi and R. Rogers. Local private hypothesis testing: Chi-square tests. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, pages 1612\u20131621, 2018.   \n[51] Oded Goldreich. The uniform distribution is complete with respect to testing identity to a fixed distribution. In Computational Complexity and Property Testing - On the Interplay Between Randomness and Computation, pages 152\u2013172. 2020.   \n[52] Oded Goldreich and Dana Ron. On Testing Expansion in Bounded-Degree Graphs, pages 68\u201375. Springer Berlin Heidelberg, Berlin, Heidelberg, 2011.   \n[53] Aravind Gollakota, Adam R. Klivans, and Pravesh K. Kothari. A moment-matching approach to testable learning and a new characterization of rademacher complexity. In Proceedings of the 55th Annual ACM Symposium on Theory of Computing, STOC 2023, pages 1657\u20131670, New York, NY, USA, 2023. Association for Computing Machinery.   \n[54] Aravind Gollakota, Adam R. Klivans, Konstantinos Stavropoulos, and Arsen Vasilyan. Testerlearners for halfspaces: Universal algorithms. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.   \n[55] Aravind Gollakota, Adam R. Klivans, Konstantinos Stavropoulos, and Arsen Vasilyan. An efficient tester-learner for halfspaces. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024, 2024.   \n[56] Sivakanth Gopi, Gautam Kamath, Janardhan Kulkarni, Aleksandar Nikolov, Zhiwei Steven Wu, and Huanyu Zhang. Locally private hypothesis selection. In Jacob D. Abernethy and Shivani Agarwal, editors, Conference on Learning Theory, COLT 2020, 9-12 July 2020, Virtual Event [Graz, Austria], volume 125 of Proceedings of Machine Learning Research, pages 1785\u20131816. PMLR, 2020.   \n[57] Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency estimation algorithms. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.   \n[58] Piotr Indyk, Reut Levi, and Ronitt Rubinfeld. Approximating and testing $k$ -histogram distributions in sub-linear time. In PODS, pages 15\u201322, 2012.   \n[59] Piotr Indyk, Ali Vakilian, and Yang Yuan. Learning-based low-rank approximations. In Advances in Neural Information Processing Systems, pages 7400\u20137410, 2019.   \n[60] Tanqiu Jiang, Yi Li, Honghao Lin, Yisong Ruan, and David P. Woodruff. Learning-augmented data stream algorithms. In International Conference on Learning Representations, 2020.   \n[61] Adam R. Klivans, Konstantinos Stavropoulos, and Arsen Vasilyan. Learning intersections of halfspaces with distribution shift: Improved algorithms and SQ lower bounds. In The Thirty Seventh Annual Conference on Learning Theory, June 30 - July 3, 2023, Edmonton, Canada, Proceedings of Machine Learning Research. PMLR, 2024.   \n[62] Adam R. Klivans, Konstantinos Stavropoulos, and Arsen Vasilyan. Testable learning with distribution shift. In The Thirty Seventh Annual Conference on Learning Theory, June 30 - July 3, 2023, Edmonton, Canada, Proceedings of Machine Learning Research. PMLR, 2024.   \n[63] Reut Levi, Dana Ron, and Ronitt Rubinfeld. Testing properties of collections of distributions. Theory of Computing, 9(8):295\u2013347, 2013.   \n[64] Yi Li, Honghao Lin, Simin Liu, Ali Vakilian, and David Woodruff. Learning the positions in countsketch. In 11th International Conference on Learning Representations, ICLR, 2023.   \n[65] Shyam Narayanan. Private high-dimensional hypothesis testing. In Conference on Learning Theory, 2-5 July 2022, London, UK, volume 178 of Proceedings of Machine Learning Research, pages 3979\u20134027. PMLR, 2022.   \n[66] Jerzy Neyman and Egon S. Pearson. On the problem of the most efficient tests of statistical hypotheses. Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character, 231(694-706):289\u2013337, 1933.   \n[67] Krzysztof Onak and Xiaorui Sun. Probability\u2013revealing samples. In International Conference on Artificial Intelligence and Statistics, volume 84 of Proceedings of Machine Learning Research, pages 2018\u20132026. PMLR, 2018.   \n[68] Aadil Oufkir, Omar Fawzi, Nicolas Flammarion, and Aur\u00e9lien Garivier. Sequential algorithms for testing closeness of distributions. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 11655\u201311664, 2021.   \n[69] Liam Paninski. A coincidence-based test for uniformity given very sparsely sampled discrete data. IEEE Trans. Inf. Theory, 54(10):4750\u20134755, 2008.   \n[70] Sampriti Roy and Yadu Vasudev. Testing properties of distributions in the streaming model. In 34th International Symposium on Algorithms and Computation, ISAAC 2023, December 3-6, 2023, Kyoto, Japan, volume 283 of LIPIcs, pages 56:1\u201356:17, 2023.   \n[71] Ronitt Rubinfeld and Arsen Vasilyan. Testing distributional assumptions of learning algorithms. In Proceedings of the 55th Annual ACM Symposium on Theory of Computing, STOC 2023, pages 1643\u20131656, New York, NY, USA, 2023. Association for Computing Machinery.   \n[72] O. Sheffet. Locally private hypothesis testing. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, pages 4612\u20134621, 2018.   \n[73] Sandeep Silwal, Sara Ahmadian, Andrew Nystrom, Andrew McCallum, Deepak Ramachandran, and Seyed Mehran Kazemi. Kwikbucks: Correlation clustering with cheap-weak and expensivestrong signals. In The Eleventh International Conference on Learning Representations, 2023.   \n[74] Alistair Stewart, Ilias Diakonikolas, and Cl\u00e9ment L. Canonne. Testing for families of distributions via the Fourier transform. In NeurIPS, pages 10084\u201310095, 2018.   \n[75] Ananda Theertha Suresh, Alon Orlitsky, Jayadev Acharya, and Ashkan Jafarpour. Near-optimalsample estimators for spherical gaussian mixtures. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27, 2014.   \n[76] Gregory Valiant and Paul Valiant. An automatic inequality prover and instance optimal identity testing. SICOMP, 46(1):429\u2013455, 2017.   \n[77] Gregory Valiant and Paul Valiant. Estimating the unseen: Improved estimators for entropy and other properties. J. ACM, 64(6):37:1\u201337:41, 2017.   \n[78] Paul Valiant. Testing symmetric properties of distributions. SIAM J. Comput., 40(6):1927\u20131968, 2011. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Other related works ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "General property testing of distributions: Testing properties of distributions has been extensively studied over the past few decades. Distribution testing under computational constraints has also been explored in [37, 70]. Hypothesis testing (and hypothesis selection) have received significant interest within the privacy community in machine learning [49, 21, 10, 4, 50, 72, 26, 27, 65]. Examples of other distributional properties that have been examined include testing monotonicity [20, 22, 11], testing histograms [24, 23], testing junta-ness [7, 32], and testing under structural properties [17, 58, 35, 44, 43]. ", "page_idx": 16}, {"type": "text", "text": "Connections to tolerant testing: Tolerant testing asks us estimate the true TV distance between a known distribution and another which we only have sample access to, up to a small additive error. Readers familiar with strong lower bounds in tolerant testing (see e.g., [78, 77, 25] where it\u2019s shown that $\\Omega(n/\\log(n))$ samples are needed in the case where the known distribution is uniform, compared to only $O({\\sqrt{n}})$ samples needed for uniformity testing) might find our results surprising. It is incorrect to conclude that our algorithm in Section B, i.e. the search component (which is adaptive to the distance between $\\hat{p}$ and $p$ ), can perform tolerant testing between $p$ and $\\hat{p}$ . In reality, the algorithm finds an $\\alpha$ which allows the testing component to terminate (either with accept or reject). Thus, while we know that the $\\alpha$ found by our algorithm is never larger than $\\lVert\\boldsymbol{p}-\\hat{\\boldsymbol{p}}\\rVert_{\\mathrm{TV}}$ , it could in fact be much lower than $\\|p-\\hat{p}\\|_{\\mathrm{TV}}$ , meaning the $\\alpha$ found by the search algorithm is not a good estimate for $\\lVert\\boldsymbol{p}-\\hat{\\boldsymbol{p}}\\rVert_{\\mathrm{TV}}$ . ", "page_idx": 16}, {"type": "text", "text": "Testable learning: Our framework bears some resemblance to testable learning as introduced in [71]. In this framework, the focus is on designing learning algorithms that can check whether the required underlying assumptions hold. If the assumptions do not hold, the algorithm may forego solving the problem. However, if it chooses to solve the problem, it must do so accurately, regardless of whether the assumptions hold. This is similar to our notion of testing with a suggested accuracy level, where the algorithm can either forego solving the problem if the assumption does not hold or solve it accurately regardless. Some examples of results in this framework are presented in [53, 54, 41, 55, 62, 61]. ", "page_idx": 16}, {"type": "text", "text": "Algorithms with predictions: Recently, there has been a burgeoning interesting in augmenting classical algorithm design with learned information. Most relevant to us are works which study learning-augmented algorithms under sublinear constraints, such as memory or sample complexity [57, 59, 60, 33, 47, 48, 31, 64, 73, 2]. We refer to the interested reader to the website https: //algorithms-with-predictions.github.io/ for an up-to-date collection of literature on learning-augmented algorithms. ", "page_idx": 16}, {"type": "text", "text": "B Searching for the appropriate accuracy level ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Early on, we defined the augmented tester for testing identity, uniformity, and closeness. We generalize this concept for other properties of distributions. More formally, a property $\\mathcal{P}$ is a set of distributions, and we say a distribution has property $\\mathcal{P}$ iff it is in $\\mathcal{P}$ . The goal is to distinguish whether an unknown distribution $p$ is in $\\mathcal{P}$ , or it is far from all distributions that have the property. Similar to Definition 1.1, we define the notion of an augmented tester for $\\mathcal{P}$ as follows: ", "page_idx": 16}, {"type": "text", "text": "Definition B.1. Suppose we are given three parameters, $\\alpha\\in[0,1]$ , $\\epsilon\\in(0,1)$ , $\\delta\\in(0,1)$ . Assume $\\boldsymbol{\\mathcal{A}}$ is an algorithm that receives all these parameters, and the description of a known distribution $\\hat{p}$ as input, and it has sample access to an unknown underlying distribution $p$ . We say algorithm $\\boldsymbol{\\mathcal{A}}$ is an $(\\alpha,\\epsilon,\\delta)$ -augmented tester for property $\\mathcal{P}$ for every $p,\\ q,$ , and $\\hat{p}$ over $[n]$ : ", "page_idx": 16}, {"type": "text", "text": "\u2022 If $\\hat{p}$ and $p$ are $\\alpha$ -close in total variation distance, the algorithm outputs inaccurate information with a probability at most $\\delta/2$ .   \n\u2022 If $p\\in\\mathcal{P}$ , then the algorithm outputs reject with a probability at most $\\delta/2$ .   \n\u2022 If p is $\\epsilon$ -far from every member of $\\mathcal{P}$ , then the algorithm outputs accept with a probability at most $\\delta/2$ . ", "page_idx": 16}, {"type": "text", "text": "With this definition in mind, suppose we have an augmented tester for a property $\\mathcal{P}$ . Our goal is to find an appropriate $\\alpha$ such that the augmented tester solves the problem: it outputs accept or reject, but also, it does not use too many samples. In this section, we introduce a search algorithm that seeks to find this appropriate $\\alpha$ . It is important to note that the $\\alpha$ identified by our algorithm may not necessarily match the true accuracy level of the prediction, i.e., $\\lVert\\boldsymbol{p}-\\hat{\\boldsymbol{p}}\\rVert_{\\mathrm{TV}}$ . Instead, it corresponds to the minimum number of samples that the augmented tester can solve the problem. ", "page_idx": 17}, {"type": "text", "text": "Our search algorithm runs in rounds, each set by a sample budget that increases as the process progresses. In round $i$ , the algorithm determines an appropriate value for $\\alpha_{i}$ and invokes the augmented tester, namely $\\boldsymbol{\\mathcal{A}}$ , with this chosen $\\alpha_{i}$ . The value of $\\alpha_{i}$ is selected such that the augmented tester operates within the sample budget allocated for that round. If the tester outputs accept or reject, the search algorithm replicates this response. However, if the tester returns inaccurate information, we double the sample budget and proceed to the next round. The pseudocode for this procedure is provided in Algorithm 2. In the following theorem, we prove its performance. ", "page_idx": 17}, {"type": "table", "img_path": "tAlMAcqK9s/tmp/7767a8153b09855b825cabc9f3aba4b158b79a4bdf6ce74b573fb7e7d3c4dfd3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Theorem 6. Fix a parameter $\\delta<1/2$ . Suppose $\\boldsymbol{\\mathcal{A}}$ is an $(\\alpha,\\epsilon,\\delta)$ -augmented tester for property $\\mathcal{P}$ that receives a suggested accuracy level $\\alpha$ and confidence parameter $\\delta$ . Let $f:[0,1]\\times[0,1]\\rightarrow\\mathbb{N}\\,b e$ a non-decreasing function for which $\\boldsymbol{\\mathcal{A}}$ uses $f(\\alpha,\\delta)^{3}$ samples when it is invoked with parameter \u03b1 and aims for the confidence parameter $\\delta$ . Algorithm 2 is $a$ $(\\epsilon,\\delta^{\\prime})$ -augmented tester, without knowledge of $\\alpha$ , for property $\\mathcal{P}$ where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\delta^{\\prime}:=\\delta\\cdot\\left(1+\\left\\lceil\\log\\left(\\frac{\\operatorname*{max}_{\\alpha\\in[0,1]}f(\\alpha)}{\\operatorname*{min}_{\\alpha\\in[0,1]}f(\\alpha)}\\right)\\right\\rceil\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In addition, $i f\\alpha^{*}$ is the true accuracy level, then Algorithm 2 uses $O(f(\\alpha^{*}))$ samples in expectation. ", "page_idx": 17}, {"type": "text", "text": "Proof. First, we define the notation we use in this proof. Let $s_{\\mathrm{min}}$ and $s_{\\mathrm{max}}$ represent the smallest and largest sample sizes used by $\\boldsymbol{\\mathcal{A}}$ , respectively. $s_{\\mathrm{min}}$ may be one or higher, and $s_{\\mathrm{max}}$ is the sample size when the prediction did not make any improvements (the sample complexity of a standard tester). Let $t:=\\log_{2}\\left(s_{\\operatorname*{max}}/s_{\\operatorname*{min}}\\right)$ . The algorithms runs in $t+1$ rounds $i\\in\\{0,1,\\ldots,t\\}$ . In round $i<t$ , our sample budget is $2^{i}\\cdot s_{\\operatorname*{min}}$ . We run the algorithm with a parameter $\\alpha_{i}$ ensuring the sample complexity of $\\boldsymbol{\\mathcal{A}}$ remains at most $2^{i-1}\\cdot s_{\\mathrm{min}}$ samples. In cases where multiple $\\alpha$ values satisfy this criterion, we select the largest. We use (abuse in fact) the inverse function notation, defining $\\alpha_{i}$ as $\\alpha_{i}=f^{-1}\\left(2^{i}\\cdot s_{\\operatorname*{min}}\\right)$ . If $\\boldsymbol{\\mathcal{A}}$ finds an answer (accept or reject), we return the answer; Otherwise, we proceed to the next round. In round $i=t$ , where we have $s_{\\operatorname*{max}}\\approx2^{i}\\cdot s_{\\operatorname*{min}}$ samples, we run the standard tester and return its answer. ", "page_idx": 17}, {"type": "text", "text": "Now, we focus on proof of correctness. Note that we (may) call $\\boldsymbol{A}\\,t$ times and the standard tester one time. Each of these tester works with probability at least $1-\\delta$ . Thus, by the union bound, we can assume that they return the correct answer with probability at least $1-\\delta^{\\prime}$ where $\\delta^{\\prime}:=\\delta/(t+1)$ . As we have noted in Definition 1.1, $\\boldsymbol{\\mathcal{A}}$ is resilient to inaccuracies, implying that even if the suggested accuracy level is not valid, if the tester does not output a false accept or reject with probability more than $\\delta/2$ . The same statement is correct for the standard tester. Now, our algorithm here replicates the output that is produced by one of the testers. Thus, if they all of them outputs the correct answer, our algorithm outputs the correct answer as well. And, this event happens with probability at least $1-\\delta^{\\prime}$ . ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Next, we focus on the analysis of the sample complexity of this algorithm. Let $\\alpha^{*}$ be the true prediction accuracy, and let $i^{*}$ be the first round where $\\alpha_{i}\\geq\\alpha^{*}$ . Recall that we assume that $\\alpha_{i}$ is the largest $\\alpha$ such that $f(\\alpha_{i})\\leq2^{i}\\cdot s_{\\operatorname*{min}}$ . In addition, we assume that $f$ is non-decreasing. Thus, we have4: ", "page_idx": 18}, {"type": "equation", "text": "$$\n2^{i^{*}-1}\\cdot s_{\\operatorname*{min}}<f(\\alpha^{*})\\leq f(\\alpha_{i^{*}})\\leq2^{i^{*}}\\cdot s_{\\operatorname*{min}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "If the algorithm ends at round $i$ , we have used the following number of samples: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\centering}&{\\mathrm{\\#{\\mathrm{~samples~for~the~first}~}i~r o u n d s}\\leq\\sum_{j=0}^{i}2^{j}\\cdot s_{\\operatorname*{min}}\\leq2^{i+1}\\cdot s_{\\operatorname*{min}}}\\\\ &{\\hphantom{\\leq}\\leq2^{i-i^{*}+2}\\cdot\\left(2^{i^{*}-1}\\cdot s_{\\operatorname*{min}}\\right)\\leq2^{i-i^{*}+2}\\cdot f(\\alpha^{*})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "On the other hand, the probability that algorithms end at round $i>i^{*}$ cannot be too high. If the algorithm ends at round $i$ or later, it means that $\\boldsymbol{\\mathcal{A}}$ in rounds $i^{*},i^{*}+1,\\ldots,i-1$ must have returned inaccurate information. However, given our assumption, the true accuracy is not worse than $\\alpha_{i}$ , which makes outputting inaccurate information wrong. And, this event does not happen in each round with probability more than $\\delta/2$ for each round independently. Let $I_{\\mathrm{end}}$ indicate a random variable that indicates the index of the round for which the algorithms ends. Thus, we have for every $i\\geq i^{*}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{Pr}[I_{\\mathrm{end}}\\geq i]\\leq\\left(\\frac{\\delta}{2}\\right)^{i-i^{*}}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now, we are ready to bound the expected value of the number of samples we use. Let $S$ denote the number of samples we use. Then, we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbf{E}[S]=\\sum_{i=0}^{t}\\mathbf{E}[S\\mid I_{\\mathrm{end}}=i]\\cdot\\mathbf{Pr}[I_{\\mathrm{end}}=i]}\\\\ {\\displaystyle\\leq4\\,f(\\alpha^{*})\\cdot\\sum_{i=0}^{t}2^{i-i^{*}}\\cdot\\mathbf{Pr}[I_{\\mathrm{end}}=i]}\\\\ {\\displaystyle\\leq4\\,f(\\alpha^{*})\\cdot\\left(\\sum_{i=0}^{i^{*}}\\mathbf{Pr}[I_{\\mathrm{end}}=i]+\\sum_{i=i^{*}+1}^{t}\\left(1+\\sum_{j=0}^{i-i^{*}-1}2^{j}\\right)\\cdot\\mathbf{Pr}[I_{\\mathrm{end}}=i]\\right)}\\\\ {\\displaystyle=4\\,f(\\alpha^{*})\\cdot\\left(\\sum_{i=0}^{t}\\mathbf{Pr}[I_{\\mathrm{end}}=i]+\\sum_{i=i^{*}+1}^{t}\\sum_{j=0}^{i-i^{*}-1}2^{j}\\cdot\\mathbf{Pr}[I_{\\mathrm{end}}=i]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Next, we write the second sum in terms of a new variable $k:=i-i^{*}-1$ . Then, we swap the order of the summations on $k$ and $j$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{E}[S]\\leq4\\,f(\\alpha^{*})\\cdot\\left(1+\\displaystyle\\sum_{k=0}^{t-i-1}\\sum_{j=0}^{k}2^{j}\\cdot\\mathbf{Pr}[I_{\\mathrm{ed}}=k+i^{*}+1]\\right)}\\\\ &{\\qquad\\leq4\\,f(\\alpha^{*})\\cdot\\left(1+\\displaystyle\\sum_{j=0}^{t-i-1}2^{j}\\cdot\\left(\\frac{t^{-i}}{k-i}-\\frac{1}{\\mathrm{Pr}[I_{\\mathrm{ed}}=k+i^{*}+1]}\\right)\\right)}\\\\ &{\\qquad\\leq4\\,f(\\alpha^{*})\\cdot\\left(1+\\displaystyle\\sum_{j=0}^{t-i-1}2^{j}\\cdot\\mathbf{Pr}[I_{\\mathrm{ed}}\\geq j+i^{*}+1]\\right)}\\\\ &{\\qquad\\leq4\\,f(\\alpha^{*})\\cdot\\left(1+\\displaystyle\\sum_{j=0}^{t-i-1}2^{j}\\cdot\\left(\\frac{\\delta}{2}\\right)^{j+1}\\right)}\\\\ &{\\qquad\\leq4\\,f(\\alpha^{*})\\cdot\\left(1+\\displaystyle\\sum_{j=0}^{t-i-1}2^{j}\\cdot\\left(\\frac{\\delta}{2}\\right)^{j+1}\\right)}\\\\ &{\\qquad\\leq6\\,f(\\alpha^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "C Identity and uniformity testing ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we focus on the problem of identity testing, which involves testing the equality between a known distribution $q$ and an unknown distribution $p$ . Specifically, our goal is to determine whether $p=q$ or if they are $\\epsilon_{}$ -far from each other. In an augmented setting, we are provided with another known distribution $\\hat{p}$ , predicted to represent $p$ , along with a suggested level of accuracy $\\alpha$ . Surprisingly, our findings indicate that the sample complexity for this problem is influenced by a new parameter: the total variation distance between $q$ and $\\hat{p}$ , denoted by $d$ . In particular, we have the following theorem: ", "page_idx": 19}, {"type": "text", "text": "Theorem 7. Fix two arbitrary parameters $\\epsilon\\in(0,1]$ and $\\alpha\\in[0,1]$ . Algorithm $3$ is an $(\\alpha,\\epsilon,\\delta=0.1)$ - augmented tester for identity that uses the following number of samples $s$ , where: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{s=\\left\\{\\begin{array}{l l}{O\\left(\\frac{\\sqrt{n}}{\\epsilon^{2}}\\right)}&{\\quad i f\\,d\\leq\\alpha}\\\\ {O\\left(\\operatorname*{min}\\left(\\frac{1}{(d-\\alpha)^{2}},\\,\\frac{\\sqrt{n}}{\\epsilon^{2}}\\right)\\right)}&{\\quad i f\\,d>\\alpha}\\end{array}\\right.,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and $d$ refers to $\\|q-\\hat{p}\\|_{T V}$ . For any $\\alpha\\in[0,1].$ , and $\\epsilon\\in[0,1/2]$ , we show the same number of samples is necessary for any $(\\alpha,\\epsilon,\\delta=2/3)$ -augmented identity tester. In fact, the lower bound holds even when $q$ is a uniform distribution over $[n]$ . ", "page_idx": 19}, {"type": "text", "text": "The proof of this theorem follows from Proposition C.1, Proposition C.2, and Proposition C.3. ", "page_idx": 19}, {"type": "text", "text": "Remark 8. A particular instance of this problem is uniformity testing, where q is a uniform distribution over $[n]$ . In Theorem 7, our upper bound applies to any arbitrary q. Furthermore, our tight lower bound is based on a hard instance, where $q$ is a uniform distribution. These results establish optimal upper and lower bounds for both identity testing and uniformity testing simultaneously. ", "page_idx": 19}, {"type": "text", "text": "C.1 Upper bound for identity testing ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Our upper bound relies on a fundamental observation regarding the total variation distance: this distance is the maximum discrepancy between the probability masses that two distributions assign to any subset of their domain. Demonstrating a small total variation distance between two distributions entails proving that the discrepancy across every domain subset is minimal. In contrast, to prove a large total variation distance, one only needs to identify a single subset with a significant discrepancy. We employ the Scheff\u00e9 set of $\\hat{p}$ and $q$ \u2014defined as the set of elements $x\\in[n]$ where $\\hat{p}(x)<\\bar{q}(x)$ , denoted by $S$ \u2014to serve as a witness for the farness of $p$ from either $q$ or $\\hat{p}$ . It is known that $S$ maximizing the discrepancy between the probability masses of $\\hat{p}$ and $q$ , implying: ", "page_idx": 19}, {"type": "equation", "text": "$$\nd:=\\lVert q-\\hat{p}\\rVert_{\\mathrm{TV}}=|q(S)-\\hat{p}(S)|\\ .\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In scenarios where the distance $d>\\alpha$ , accurately estimating the probability of $S$ according to $p$ with an accuracy of $(d-\\alpha)/4$ provides a basis for distinguishing the farness from either $q$ or $\\hat{p}$ . If the probability masses assigned to $S$ by $q$ and $p$ differ by more than $(d-\\alpha)/4$ , it clearly indicates that $p$ is not identical to $q$ , leading to outputting reject. Conversely, if the estimated probability of $p(S)$ is close to $q(S)$ , then the discrepancy between $p(S)$ and $\\hat{p}(S)$ must exceed $\\alpha$ , leading to outputting inaccurate information. The pseudocode of our algorithm presented in Algorithm 3. We prove the performance of our algorithm in the following proposition: ", "page_idx": 20}, {"type": "text", "text": "Proposition C.1. Fix two arbitrary parameters $\\epsilon\\,\\in\\,(0,1]$ and $\\alpha\\,\\in\\,[0,1]$ . Algorithm $3$ is an $(\\alpha,\\epsilon,\\delta=0.1)$ -augmented tester for identity that uses the following number of samples $s$ , where: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{s=\\left\\{\\begin{array}{l l}{O\\left(\\frac{\\sqrt{n}}{\\epsilon^{2}}\\right)}&{\\quad i f\\,d\\leq\\alpha}\\\\ {O\\left(\\operatorname*{min}\\left(\\frac{1}{(d-\\alpha)^{2}},\\,\\frac{\\sqrt{n}}{\\epsilon^{2}}\\right)\\right)}&{\\quad i f\\,d>\\alpha}\\end{array}\\right.,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and $d$ refers to $\\lVert q-\\hat{p}\\rVert_{T V}$ . ", "page_idx": 20}, {"type": "text", "text": "Algorithm 3 An augmented tester for testing identity of $p$ and $q$ with a suggested value $\\alpha$   \n1: procedure AUGMENTED-IDENTITY-TESTER(n, \u03b1, \u03f5, \u03b4 = 0.1, q, p\u02c6, sample access to $p$ )   \n2: $d\\leftarrow\\Vert q-\\hat{p}\\Vert_{\\mathrm{TV}}$   \n3: if d \u2264\u03b1 or(d\u22121\u03b1)2 $\\begin{array}{r}{\\frac{1}{(d-\\alpha)^{2}}>\\frac{\\sqrt{n}}{\\epsilon^{2}}}\\end{array}$ 2n then   \n4: return the standard testers answer.   \n5: $S\\gets S$ cheff\u00e9 set of $\\hat{p}$ and $q$ \u25b7 $x\\in[n]$ is in the Scheff\u00e9 set $S$ iff $\\hat{p}(x)<q(x)$ .   \n6: Draw $\\begin{array}{r}{m=O\\left(\\frac{1}{(d-\\alpha)^{2}}\\right)}\\end{array}$ samples from $p$   \n7: $\\sigma\\gets$ fraction of samples that are in $S$   \n8: if $\\begin{array}{r}{|U_{n}(S)-\\sigma|>\\frac{d-\\alpha}{4}}\\end{array}$ d\u2212\u03b1then   \n9: return reject.   \n10: else   \n11: return inaccurate information. ", "page_idx": 20}, {"type": "text", "text": "Proof. The proof of theorem is trivial in the setting where $d\\leq\\alpha$ and $\\begin{array}{r}{\\frac{1}{(d-\\alpha)^{2}}>\\frac{\\sqrt{n}}{\\epsilon^{2}}}\\end{array}$ . In these cases, we use the standard tester for identity testing (e.g., [76, 30, 5]) with $\\delta^{\\prime}=\\delta/2=0.05$ . Clearly, a wro\u221ang answer was produced with probability less than $\\delta/2$ . It is known that these tester only use $O(\\sqrt{\\bar{n}}/\\epsilon^{2})$ samples. ", "page_idx": 20}, {"type": "text", "text": "Now, suppose $d\\,>\\,\\alpha$ . Simple application of Chernoff\u2019s bound shows that one can estimate the probability of the Scheff\u00e9 set of $q$ and $\\hat{p},\\,S$ , up to error $(d-\\alpha)/4$ with probability $1-0.95$ using $\\dot{O}(1/(d-\\dot{\\alpha})^{2})$ samples. We refer to this estimate as $\\sigma$ , and we have with probability 0.95: ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\sigma-p(S)|\\leq{\\frac{d-\\alpha}{4}}\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that if $p=q$ , then ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\sigma-q(S)|=|\\sigma-p(S)|\\leq{\\frac{d-\\alpha}{4}}\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence, we do not output reject with probability more than $0.05=\\delta/2$ . On the other hand, assume $\\|\\hat{p}-p\\|_{\\mathrm{TV}}\\leq\\alpha$ , then we show it is unlikely for us to output inaccurate information. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\alpha\\geq\\Vert\\hat{p}-p\\Vert_{\\mathrm{TV}}\\geq|p(S)-\\hat{p}(S)|}\\\\ &{\\quad\\geq|\\hat{p}(S)-q(S)|-|p(S)-q(S)|}&{\\mathrm{(by~triangle~inequality)}}\\\\ &{\\quad\\geq\\Vert q-\\hat{p}\\Vert_{\\mathrm{TV}}-|p(S)-q(S)|=d-\\vert p(S)-q(S)\\vert\\qquad\\qquad\\mathrm{(by~definition~of~Scheff\\'{\\epsilon}~s e t)}}\\\\ &{\\quad\\geq d-\\vert\\sigma-p(S)\\vert-\\vert\\sigma-q(S)\\vert\\geq d-\\frac{d-\\alpha}{4}-\\vert\\sigma-q(S)\\vert\\ .}&\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "(by triangle inequality and Eq. (6)) ", "page_idx": 20}, {"type": "text", "text": "Therefore, we obtain that with probability 0.95: ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\sigma-q(S)|\\geq{\\frac{3\\,(d-\\alpha)}{4}}\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence, we do not output inaccurate information with probability more than $0.05\\,=\\,\\delta/2$ . Hence, Algorithm 3 is an $(\\alpha,\\epsilon,\\delta\\,=\\,0.1)$ -agumented tester for identity. The sample complexity of the algorithm, in the case where $\\alpha\\ <\\ d$ and $\\begin{array}{r}{\\frac{1}{(d-\\alpha)^{2}}\\ \\leq\\ \\frac{\\sqrt{n}}{\\epsilon^{2}}}\\end{array}$ is $O(1/(d-\\alpha)^{2})$ . Thus, the proof is complete. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "C.2 Lower bound for uniformity testing when $\\alpha\\geq d$ ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we focus on the case where $\\alpha\\geq d$ . We observed for this problem the number of samples depends on an additional parameter: the distance between $q$ and $\\hat{p}$ . If this distance is at most $\\alpha$ then prediction does not help the algorithm. The algorithm would still be required to draw $\\Omega(n/\\epsilon^{2})$ samples (which is the sufficient amount of sample for the standard case). Our proof is based on a reduction from standard identity testing to the augmented version of this problem which establishes the desired lower bound. ", "page_idx": 21}, {"type": "text", "text": "Proposition C.2. Suppose we are given two known distributions $q$ and ${\\hat{p}},$ , and an unknown distribution $p$ over $[n]$ . Let $d:=\\|q-\\hat{p}\\|_{T V}$ . For any $\\alpha$ in $[0,1]$ , if $\\dot{\\cdot}_{\\alpha}\\geq d_{:}$ , then any $(\\alpha,\\epsilon,\\delta=2/3)$ )-augmented tester requires $\\Omega(\\sqrt{n}/\\epsilon^{2})$ samples. ", "page_idx": 21}, {"type": "text", "text": "Proof. We prove that standard identity testing can be reduced to augmented version of this problem if $\\alpha\\geq d$ . For the standard tester, we are given a known distribution $q$ and an unknown distribution $p$ . Let $\\hat{p}$ be any arbitrary distribution that is in distance $d$ from $q$ . Consider $\\boldsymbol{\\mathcal{A}}$ as an $(\\alpha,\\epsilon,\\delta=2/3)$ - augmented tester. The procedure for the standard tester is straightforward: Run $\\boldsymbol{\\mathcal{A}}$ on $p,\\,\\hat{p}_{!}$ , and $q$ . If it returns accept, we also return accept; if it returns reject or inaccurate information, we return reject. ", "page_idx": 21}, {"type": "image", "img_path": "tAlMAcqK9s/tmp/561f5821618eb495c9658f21b390be41a92d130cd2a584653bb8fffeb376c6db.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 2: A diagram indicating the valid answer for the augmented tester $\\boldsymbol{\\mathcal{A}}$ based on the total variation distances of $p$ from $q$ and $\\hat{p}$ assuming $d\\leq\\alpha$ . The standard tester requires to output accept if $p=q$ , the green dot, and reject if $\\|p-q\\|_{\\mathrm{TV}}\\geq\\epsilon$ , the red shaded region, with high probability. In addition, the augmented tester may output inaccurate information for when $\\|p-q\\|_{\\mathrm{TV}}\\geq\\epsilon$ and $\\|p-\\hat{p}\\|_{\\mathrm{TV}}\\geq\\alpha$ . ", "page_idx": 21}, {"type": "text", "text": "We show that the augmented tester distinguishes between the cases where $p=q$ and $\\|p-q\\|_{\\mathrm{TV}}>\\epsilon$ . See Figure 2. More precisely, Note that if $p=q$ , then $p$ is within $\\alpha$ distance of $\\hat{p}$ . Thus, the only valid answer of $\\boldsymbol{\\mathcal{A}}$ in this case is accept. In this case, according to Definition 1.1, $\\boldsymbol{\\mathcal{A}}$ returns reject and inaccurate information each with a probability of at most $\\bar{\\delta}/2=1/6$ . Therefore, by the union bound, we return accept with a probability of at least $2/3$ . When $p$ and $q$ are $\\epsilon$ -far from each other, $\\boldsymbol{\\mathcal{A}}$ returns accept with a probability of at most $\\delta/2=1/6$ . Consequently, we output the correct answer with a probability greater than $2/3$ . ", "page_idx": 21}, {"type": "text", "text": "This reduction indicates that $\\boldsymbol{\\mathcal{A}}$ must use at least as many samples as required for identity testing. Considering the existing lower bound for unifor\u221amity testing (where $q$ is a uniform distribution over $[n])$ , augmented identity testing necessitates $O(\\sqrt{n}/\\epsilon^{2})$ samples [69]. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "C.3 Lower bound for uniformity testing when $\\alpha<d$ ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we consider the lower bound for the uniformity testing problem in the setting where $\\alpha<d$ . On the \u221aother hand, if $d-\\alpha$ is not too small, the required lower bound is only $\\Omega(1/(d-\\alpha)^{2})$ . Otherwise, $\\Omega(\\sqrt{n}/\\epsilon^{2})$ samples is needed (as it is required for the standard tester). ", "page_idx": 22}, {"type": "text", "text": "At a high level, our proof consists of the following steps. First, we construct three distributions that look similar when we draw too few samples. We formalize the similarity of these three distributions using a multivariate coupling argument. Next, we describe valid answers for each of the distributions. The main message of this part is that there is no possible answer that is valid for all three distributions. Now, (similar to Le Cam\u2019s method), suppose we feed the algorithm with samples from one of these three distributions (each with probability 1/3). For any sample set, the algorithm outputs an answer (which may be randomized), however, this answer is considered as wrong for at least one of the underlying distributions. This is due to the fact that there is no universally valid answer that is simultaneously correct for all three distributions. Hence, if the algorithm outputs a valid answer with high probability, it must be able to distinguish the underlying distributions to some degree. On the other hand, the indistinguishably result says, it is impossible to tell these distributions apart. Thus, we reach a contradiction. And, the lower bound is concluded. More formally, we have the following proposition: ", "page_idx": 22}, {"type": "text", "text": "Proposition C.3. Suppose we are given two known distributions q and ${\\hat{p}},$ , and an unknown distribution $p$ over $[n]$ . Let $d:=\\|q-\\hat{p}\\|_{T V}$ . For any $\\alpha\\in[0,1]$ , $\\epsilon\\,\\in\\,[0,1/2]$ , and $d\\in[0,1/2]$ , if $d>\\alpha$ any $(\\alpha,\\epsilon,\\delta\\stackrel{.}{=}2/3)$ -augmented algorithm for testing identity of $p$ and $q$ with prediction $\\hat{p}$ requires $\\begin{array}{r}{s=\\Omega\\left(\\operatorname*{min}\\left(\\frac{1}{(d-\\alpha)^{2}},\\,\\frac{\\sqrt{n}}{\\epsilon^{2}}\\right)\\right)}\\end{array}$ samples. ", "page_idx": 22}, {"type": "text", "text": "Proof. Without loss of generality, assume $d-\\alpha\\leq0.4$ . Otherwise, we are required to establish a lower bound of $\\Omega(1)$ , which is necessary for any non-trivial testing problem.5 As we have discussed it early, we prove this proposition in the following steps: ", "page_idx": 22}, {"type": "text", "text": "Construction of distributions: We assume $q=U_{n}$ is a uniform distribution over $[n]$ . Without loss of generality assume $n$ is even. Otherwise, we can set the probability of one of the elements in all the distributions to zero. We define the prediction distribution as follows for every $i\\in[n]$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\hat{p}}_{i}:={\\left\\{\\begin{array}{l l}{{\\frac{1+2d}{n}}\\quad\\quad}&{i{\\mathrm{~is~even}};}\\\\ {{\\frac{1-2d}{n}}\\quad\\quad}&{i{\\mathrm{~is~odd}}.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "It is not hard to assert that $\\lVert\\hat{p}-U_{n}\\rVert_{\\mathrm{TV}}$ is $d$ satisfying the assumption we had in the statement of the proposition. Next, we construct two distributions $p^{\\bullet}$ and $p^{\\diamond}$ . ", "page_idx": 22}, {"type": "text", "text": "Suppose we have a random vector $Z$ in $\\{-1,+1\\}^{n/2}$ where each coordinate $Z_{i}$ is one with probability 1/2 independently. Now, we define the following distributions over $[n]$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\np_{i}^{\\bullet}:=\\left\\{\\begin{array}{l l}{\\frac{1+2\\,Z_{i/2}\\cdot\\epsilon}{n}}\\\\ {\\frac{1-2\\,Z_{(i+1)/2}\\cdot\\epsilon}{n}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\np_{i}^{\\diamond}:=\\left\\{\\begin{array}{l l}{\\displaystyle\\frac{1+2\\left(d-\\alpha\\right)}{n}}\\\\ {\\displaystyle\\frac{1-2\\left(d-\\alpha\\right)}{n}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "It is not hard to see that the probabilities of each distributions sum up to one by observing the probability of two consecutive elements is $2/n$ . ", "page_idx": 22}, {"type": "text", "text": "Indistinguishably of the distributions Fix the number of samples $s$ , and let $S_{|p}$ denote a random variable that is a sample set of size $s$ drawn from $p$ . We show that $S_{|U_{n}}$ , $S_{|p^{\\diamond}}$ , and $S_{|p}\\bullet$ are three random variables with small total variation distances between each pair. These distances are so small that it is practically impossible for the algorithm to tell them apart. We formalized this by providing a multivariate coupling between these random variables. We extend Le Cam\u2019s method for three random variables, and show that no algorithm with low error probability exists unless $s$ is large, establishing the desired lower bound for the problem. ", "page_idx": 23}, {"type": "text", "text": "Lemma C.4. Suppose we are given the following parameters $\\epsilon,d,\\alpha,n,$ , and $s$ . Assume $d-\\alpha\\leq0.4$ . Let $U_{n}$ denote the uniform distribution over $[n]$ , and let $p^{\\bullet}$ and $p^{\\diamond}$ be the distributions defined in Equation (7) and Equation (8). Let $S_{|U_{n}}$ , $S_{|p^{\\diamond}}$ , and $S_{|p}\\bullet$ be three random variables that are sample sets of size s from each of these distributions. If $\\begin{array}{r}{s\\leq\\operatorname*{min}\\left(\\frac{0.00005}{(d-\\alpha)^{2}},\\frac{0.004\\sqrt{n}}{\\epsilon^{2}}\\right)}\\end{array}$ 0.000025 , 0.0042  n , then there exists a distribution over triples of three sample sets of size $s$ (that is $([n]^{s})^{3},$ , which we call a multivariate coupling $\\mathcal{C}$ , between $S_{|U_{n}},\\,S_{|p^{\\circ}}$ , and $S_{|p}\\bullet$ such that: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The marginals of $\\mathcal{C}$ correspond to the three probability distributions $U_{n}^{\\otimes s},\\,p^{\\bullet\\otimes s}$ , and $p^{\\diamondsuit\\otimes s}$ More precisely, for a sample $(S_{1},S_{2},S_{3})$ drawn from $\\mathcal{C}$ , and every $S\\in[n]^{s}$ , we have: $\\begin{array}{r l}&{\\mathbf{Pr}_{(S_{1},S_{2},S_{3})\\sim c}[S_{1}=S]=\\mathbf{Pr}_{S_{|U_{n}}\\sim U_{n}^{\\otimes s}}\\left[S_{|U_{n}}=S\\right],}\\\\ &{\\mathbf{Pr}_{(S_{1},S_{2},S_{3})\\sim c}[S_{2}=S]=\\mathbf{Pr}_{S_{|p^{\\bullet}}\\sim p^{\\bullet\\otimes s}}\\left[S_{|p^{\\bullet}}=S\\right],}\\\\ &{\\mathbf{Pr}_{(S_{1},S_{2},S_{3})\\sim c}[S_{3}=S]=\\mathbf{Pr}_{S_{|p^{\\diamond}}\\sim p^{\\diamond\\otimes s}}\\left[S_{|p^{\\diamond}}=S\\right].}\\end{array}$ \u2022 $\\mathbf{Pr}_{(S_{1},S_{2},S_{3})\\sim\\mathcal{C}}[S_{1}=S_{2}=S_{3}]\\geq0.98.$ ", "page_idx": 23}, {"type": "text", "text": "For the proof, see Section C.3.1. This lemma states that when the number of samples is too small, the sample sets looks very similar in all three cases: $p=U_{n}$ , $p=p^{\\bullet}$ , and $p=p^{\\diamond}$ . ", "page_idx": 23}, {"type": "image", "img_path": "tAlMAcqK9s/tmp/4c2f03dc7db63597bf8c515232fd6928695b02cb1421aafb7916fa648f0e0520.jpg", "img_caption": ["Figure 3: A diagram indicating the invalid answer for the three distributions $U_{n},p^{\\bullet}$ , and $p^{\\diamond}$ . "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Valid answers for each distributions: We focus on the valid output of the algorithm in these three cases using the Definition 1.1. According to this definition, we have: ", "page_idx": 23}, {"type": "text", "text": "\u2022 If $p=q$ , then reject is not a valid answer.   \n\u2022 If $\\|p-q\\|_{\\mathrm{TV}}>\\epsilon$ , then accept is not a valid answer.   \n\u2022 If $\\|p-\\hat{p}\\|_{\\mathrm{TV}}\\leq\\alpha$ then inaccurate information is not considered as a valid answer. ", "page_idx": 23}, {"type": "text", "text": "We denote the set of valid output for each case by $V_{|p}$ . An accurate augmented tester with confidence parameter $\\delta$ , has to output an answer in $V_{|p}$ with probability at least $1-\\delta$ . ", "page_idx": 24}, {"type": "text", "text": "1. $p=U_{n}$ : In this case we have $p=q=U_{n}$ . Clearly, in this case, reject is not a valid answer: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathsf{r e j e c t}\\not\\in V_{\\left|U_{n}\\right.}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "2. $\\pmb{p}=\\pmb{p}^{\\bullet}$ : The total variation distance between $q=U_{n}$ and $p^{\\bullet}$ is $\\epsilon>\\epsilon$ . Thus, accept is not considered a valid output: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathsf{a c c e p t}\\not\\in V_{|p}\\bullet.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "3. $p=p^{\\diamond}:\\operatorname{It}$ is not hard to see that $\\|p^{\\diamond}-\\hat{p}\\|_{\\mathrm{TV}}=\\alpha$ . Thus, inaccurate information is not a valid answer: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathsf{i n a c c u r a t e\\ i n f o r m a t i o n\\not\\in{\\cal V}}_{|p^{\\diamond}}\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The diagram of the invalid outputs is shown in Figure 3 for an instance of these distributions. Let us explain what these valid sets are implying. The intersection of the three valid sets is empty, because among all possible outputs each set lacks at least one of them : ", "page_idx": 24}, {"type": "equation", "text": "$$\n{\\cal V}_{|U_{n}}\\cap{\\cal V}_{|p}{\\bullet\\cap\\!\\!\\!\\!\\cap}{\\cal V}_{|p^{\\circ}}=\\emptyset\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, there is no possible output that is considered valid on all the three distributions6. And, since these distributions are indistinguishable with high probability, no algorithm should perform well under all three possibilities. ", "page_idx": 24}, {"type": "text", "text": "Deriving a contradiction. Our proof is via contradiction. Suppose there exists an $(\\alpha,\\epsilon,\\delta)$ - augmented tester, called $\\boldsymbol{\\mathcal{A}}$ , for uniformity testing that uses $s$ samples. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{3\\cdot(1-\\delta)\\leq\\mathbf{Pr}_{S_{|U_{n}}\\sim U_{n}}\\big[A(S_{|U_{n}})\\in V_{|U_{n}}\\big]+\\mathbf{Pr}_{S_{|p}\\star\\sim p^{*}}\\big[A(S_{|p^{*}})\\in V_{|p^{*}}\\big]+\\mathbf{Pr}_{S_{|p^{*}}\\sim p^{*}}\\big[A(S_{|p^{*}})\\in V_{|p^{*}}\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Let $I$ represent the internal coin tosses of the algorithm. Since the probability of $\\boldsymbol{\\mathcal{A}}$ outputting an invalid answer is bounded by $\\delta$ , we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta\\geq\\mathbf{Pr}_{I},\\,s_{|U_{n}\\sim U_{n}^{\\otimes s}}\\big[\\mathcal{A}(S_{|U_{n}})\\notin V_{|U_{n}}\\big]=\\mathbf{E}_{S_{|U_{n}\\sim U_{n}^{\\otimes s}}}\\big[\\mathbf{Pr}_{I}\\big[\\mathcal{A}(S_{|U_{n}})\\notin V_{|U_{n}}\\big]\\big]}\\\\ &{\\quad=\\mathbf{E}_{S_{|U_{n}\\sim U_{n}^{\\otimes s}}}\\big[\\mathbf{Pr}_{I}\\big[\\mathcal{A}(S_{|U_{n}})=\\mathsf{r e j e c t}\\big]\\big]=\\mathbf{E}_{(S_{1},S_{2},S_{3})\\sim\\mathcal{C}}\\big[\\mathbf{Pr}_{I}\\big[\\mathcal{A}(S_{1})=\\mathsf{r e j e c t}\\big]\\big]}\\\\ &{\\quad\\geq\\mathbf{E}_{(S_{1},S_{2},S_{3})\\sim\\mathcal{C}}\\big[\\mathbf{Pr}_{I}\\big[\\mathcal{A}(S_{1})=\\mathsf{r e j e c t}\\big]\\mid S_{1}=S_{2}=S_{3}\\big]\\cdot\\mathbf{Pr}_{(S_{1},S_{2},S_{3})\\sim\\mathcal{C}}\\big[S_{1}=S_{2}=S_{3}\\big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We can write the above equation for $p^{\\bullet}$ and $p^{\\diamond}$ and sum of them up. Thus, we obtain: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta\\geq\\frac{1}{3}\\cdot\\mathbf{Pr}_{(S_{1},S_{2},S_{3})\\sim c}[S_{1}=S_{2}=S_{3}]\\cdot\\mathbf{E}_{(S_{1},S_{2},S_{3})\\sim c}[\\mathbf{Pr}_{I}[\\mathcal{A}(S_{1})=r\\mathsf{e j e c t}]+\\mathbf{Pr}_{I}[\\mathcal{A}(S_{2})=\\mathsf{a c c}]]}\\\\ &{\\quad+\\,\\mathbf{Pr}_{I}[\\mathcal{A}(S_{3})=\\mathsf{i n a c c u r a t e\\ i n f o r m a t i o n}]\\mid S_{1}=S_{2}=S_{3}]}\\\\ &{\\quad=\\frac{1}{3}\\cdot\\mathbf{Pr}_{(S_{1},S_{2},S_{3})\\sim c}[S_{1}=S_{2}=S_{3}]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For the last line above, note that when $S_{1}=S_{2}=S_{3}$ , the terms inside the expectations are the sum of probabilities of all possible outputs for $\\boldsymbol{\\mathcal{A}}$ . Hence, the sum of these probabilities is one. ", "page_idx": 24}, {"type": "text", "text": "Recall that earlier, we assumed that $\\delta=0.3$ , and using the properties of the coupling, we know that $S_{1}=S_{2}=S_{3}$ with probability at least 0.98. Thus, we get: ", "page_idx": 24}, {"type": "equation", "text": "$$\n0.3=\\delta\\geq\\frac{1}{3}\\cdot{\\bf P r}_{(S_{1},S_{2},S_{3})\\sim{\\mathscr C}}[S_{1}=S_{2}=S_{3}]\\geq\\frac{0.98}{3}>0.32\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Hence, we reach a contradiction. Thus, no such algorithm like $\\boldsymbol{\\mathcal{A}}$ exists. ", "page_idx": 24}, {"type": "text", "text": "C.3.1 Multivariate coupling between the three hard distributions ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we prove the lemma on coupling that we used to prove our lower bound. This lemma implies indistinguishability between $S_{|U_{n}},S_{|p}\\bullet$ , and $S_{|p^{\\diamond}}$ . For a definition and some basics on coupling, see Section F.3. ", "page_idx": 25}, {"type": "text", "text": "Lemma C.4. Suppose we are given the following parameters $\\epsilon,d,\\alpha,n,$ , and $s$ . Assume $d-\\alpha\\leq0.4$ . Let $U_{n}$ denote the uniform distribution over $[n]$ , and let $p^{\\bullet}$ and $p^{\\diamond}$ be the distributions defined in Equation (7) and Equation (8). Let $S_{|U_{n}}$ , $S_{|p^{\\diamond}}$ , and $S_{|p}\\bullet$ be three random variables that are sample sets of size s from each of these distributions. If $\\begin{array}{r}{s\\leq\\operatorname*{min}\\left(\\frac{0.00005}{(d-\\alpha)^{2}},\\frac{0.004\\sqrt{n}}{\\epsilon^{2}}\\right)}\\end{array}$ , then there exists a distribution over triples of three sample sets of size $s$ (that is ${([n]^{s})^{3}}\\,$ , which we call a multivariate coupling $\\mathcal{C}$ , between $S_{|U_{n}},\\,S_{|p^{\\circ}}$ , and $S_{|p}\\bullet$ such that: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The marginals of $\\mathcal{C}$ correspond to the three probability distributions $U_{n}^{\\otimes s}$ , $p^{\\bullet\\ast}$ , and $p^{\\diamondsuit\\otimes s}$ . More precisely, for a sample $(S_{1},S_{2},S_{3})$ drawn from $\\mathcal{C}$ , and every $S\\in[n]^{s}$ , we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{Pr}_{(S_{1},S_{2},S_{3})\\sim c}[S_{1}=S]=\\operatorname*{Pr}_{S_{|U_{n}}\\sim U_{n}^{\\otimes s}}\\left[S_{|U_{n}}=S\\right],}\\\\ {\\operatorname*{Pr}_{(S_{1},S_{2},S_{3})\\sim c}[S_{2}=S]=\\operatorname*{Pr}_{S_{|P_{\\mathbf{\\tilde{S}}}}\\sim p^{\\bullet}\\otimes s}\\left[S_{|p^{\\bullet}}=S\\right],}\\\\ {\\operatorname*{Pr}_{(S_{1},S_{2},S_{3})\\sim c}[S_{3}=S]=\\operatorname*{Pr}_{S_{|p^{\\diamond}}\\sim p^{\\diamond}\\otimes s}\\left[S_{|p^{\\diamond}}=S\\right].}\\\\ {\\bullet\\operatorname*{Pr}_{(S_{1},S_{2},S_{3})\\sim c}[S_{1}=S_{2}=S_{3}]\\geq0.98.\\qquad\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. We start by bounding the total variation distance between two pairs of distributions: $\\|U_{n}^{\\otimes s}-$ $p^{\\otimes s}\\|_{\\mathrm{TV}}$ , and $\\lVert U_{n}^{\\otimes s}-p^{\\diamond\\otimes s}\\rVert_{\\mathrm{TV}}$ . ", "page_idx": 25}, {"type": "text", "text": "In [69], the author has shown that the total variation distance between $S_{|U_{n}}$ and $S_{|p}\\bullet$ is bounded by: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|U_{n}^{\\otimes s}-p^{\\bullet\\otimes s}\\|_{\\mathrm{TV}}\\leq\\frac{\\sqrt{\\exp\\left(\\frac{s^{2}\\cdot(2\\,\\epsilon)^{4}}{n}\\right)-1}}{2}\\leq0.01\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The last inequality above comes from our assumption that $s\\leq{\\frac{0.004\\,{\\sqrt{n}}}{\\epsilon^{2}}}$ . ", "page_idx": 25}, {"type": "text", "text": "Next, we focus on the total variation distance between $S_{|p^{\\diamond}}$ and $S_{|U_{n}}$ . Note that $p^{\\diamond}$ is a distribution that has some bias towards the even elements, but it is uniform on both sets of odd and even elements. Therefore, to bound the total variation distance between $U_{n}^{\\otimes s}$ and $p^{\\diamondsuit\\otimes s}$ , we use a similar argument for showing a lower bound of $\\Omega(1/\\Delta)^{2}$ to distinguish whether a coin is fair or it has a bias of $\\bar{(1+\\Delta)}/2$ . Below is our formal argument. ", "page_idx": 25}, {"type": "text", "text": "It is not hard to see that one can use the following process to generate a sample from $p^{\\diamond}$ . First, we draw a sample $X\\sim\\mathbf{Ber}(1/2+(d-\\alpha))$ from the Bernoulli distribution with a success probability $1/2+(d-\\alpha)$ . If $X=1$ , we pick an even element (uniformly); Otherwise, we pick an odd element. Similarly, we can draw samples from a uniform distribution as follows: draw $\\bar{X^{\\prime}}\\sim\\mathbf{Ber}(1/2)$ from the Bernoulli distribution with a success probability $1/2$ . If $X^{\\prime}=1$ , we pick an even element (uniformly); Otherwise, we pick an odd element. One can view this process of generating a sample in $[n]$ from a binary variable ( $X$ or $X^{\\prime}$ ) as a channel. The data processing inequality says that the distance (more precisely any $f$ -divergences) between two random variables does not increases after they pass through a channel. Hence, we use this fact and bound the total variation distance between $p^{\\diamondsuit s}$ and $U_{n}^{\\otimes s}$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|U_{n}^{\\otimes s}-p^{\\otimes\\otimes s}\\|_{\\mathrm{TV}}\\leq\\sqrt{\\frac{1}{2}\\cdot K L\\left(U_{n}^{\\otimes s}\\|p^{\\otimes\\otimes s}\\right)}}\\quad}&{\\mathrm{(by~Pinsker's~inequality)}}\\\\ &{}&{\\leq\\sqrt{\\frac{s}{2}\\cdot K L\\left(U_{n}\\|p^{\\circ}\\right)}\\quad}&{\\mathrm{(since~samples~are~drawn~i.i.d.)}}\\\\ &{}&{\\leq\\sqrt{\\frac{s}{2}\\cdot K L\\left(\\mathbf{Ber}(1/2)\\|\\mathbf{Ber}(1/2+(d-\\alpha)\\right)\\right)}\\;.}\\\\ &{}&{\\mathrm{(by~data~processing~inequality)}}\\\\ &{}&{\\leq\\sqrt{2s}\\cdot(d-\\alpha)\\quad}&{\\mathrm{(when~2(d-\\alpha)\\leq0.8)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since we had this assumption where $s$ is at most (d\u2212\u03b1)2 , we get. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|U_{n}^{\\otimes s}-p^{\\diamond\\otimes s}\\|_{\\mathrm{TV}}\\leq\\sqrt{2\\,s}\\cdot(d-\\alpha)\\leq0.01\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "So far, we have shown that $\\lVert U_{n}^{\\otimes s}-p^{\\bullet\\otimes s}\\rVert_{\\mathrm{TV}}$ and $\\lVert U_{n}^{\\otimes s}-p^{\\diamond\\otimes s}\\rVert_{\\mathrm{TV}}$ are at most 0.01. Using the coupling lemma (See Fact F.3), there exist two maximal couplings $\\mathcal{C^{\\bullet}}$ and $\\mathcal{C^{\\diamond}}$ between the following pairs of distributions: $(U_{n}^{\\otimes s},\\;p^{\\bullet\\otimes s})$ and $(U_{n}^{\\otimes s},\\;p^{\\diamond\\otimes s})$ . The properties of these maximal couplings are: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The marginals of the couplings are equal to the two pairs of probability distributions. That is: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{C}_{1}^{\\bullet}=U_{n}^{\\otimes s},\\,\\mathrm{and}\\,\\mathcal{C}_{2}^{\\bullet}=p^{\\bullet\\otimes s}\\,,}\\\\ {\\mathcal{C}_{1}^{\\diamond}=U_{n}^{\\otimes s},\\,\\mathrm{and}\\,\\mathcal{C}_{2}^{\\diamond}=p^{\\diamond\\otimes s}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{Pr}_{(S_{1},S_{2})\\sim\\mathcal{C}^{\\bullet}}[S_{1}=S_{2}]\\geq0.99,\\,\\mathrm{and}\\,\\,\\mathbf{Pr}_{(S_{1},S_{2})\\sim\\mathcal{C}^{\\diamond}}[S_{1}=S_{2}]\\geq0.99.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For every $S\\in[n]^{m}$ , let $\\mathcal{C}_{2|S}^{\\diamond}$ be a probability distribution over $[n]^{m}$ . The probability of $S^{\\prime}$ according to $\\mathcal{C}_{2|S}^{\\diamond}$ is: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{C}_{2|S}^{\\diamond}(S^{\\prime}):=\\mathbf{Pr}_{(S_{1},S_{2})\\sim\\mathcal{C}^{\\diamond}}[S_{2}=S^{\\prime}|S_{1}=S]\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "With these definitions in mind, we create our multivariate coupling $\\mathcal{C}$ by essentially joining the two couplings $\\mathcal{C^{\\bullet}}$ and $\\mathcal{C}^{\\diamond}$ . The following randomized procedure produces a random sample $(S_{1},S_{2},S_{3})$ from $\\mathcal{C}$ : ", "page_idx": 26}, {"type": "text", "text": "1. Let $(S_{1},S_{2})$ be a sample from $\\mathcal{C^{\\bullet}}$ .   \n2. Let S3 be a sample from C2\u22c4|S1.   \n3. Output $(S_{1},S_{2},S_{3})$ as a sample from $\\mathcal{C}$ . ", "page_idx": 26}, {"type": "text", "text": "Given the definition of $\\mathcal{C^{\\bullet}}$ , it is clear that the first two marginals of $\\mathcal{C}$ are exactly $U_{n}^{\\otimes s}$ and $p^{\\bullet\\otimes s}$ . To see that the third marginal of $\\mathcal{C}$ is equal to $p^{\\diamond}$ , we have the following for every $S^{\\prime}\\in[n]^{m}$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{Pr}[S_{3}=S^{\\prime}]=\\displaystyle\\sum_{S_{1}\\in[n]^{m}}\\mathbf{Pr}_{(S_{1}^{\\prime},S_{2}^{\\prime})\\sim C^{\\bullet}}[S_{1}^{\\prime}=S_{1}]\\cdot C_{2|S_{1}}^{\\circ}(S^{\\prime})}\\\\ &{\\phantom{=}=\\displaystyle\\sum_{S_{1}\\in[n]^{m}}\\mathbf{Pr}_{(S_{1}^{\\prime},S_{2}^{\\prime})\\sim C^{\\bullet}}[S_{1}^{\\prime}=S_{1}]\\cdot\\mathbf{Pr}_{(S_{1}^{\\prime\\prime},S_{2}^{\\prime\\prime})\\sim C^{\\circ}}[S_{2}^{\\prime\\prime}=S^{\\prime}|S_{1}^{\\prime\\prime}=S_{1}]}\\\\ &{\\phantom{=}=\\displaystyle\\sum_{S_{1}\\in[n]^{m}}\\mathbf{Pr}_{(S_{1}^{\\prime},S_{2}^{\\prime})\\sim C^{\\circ}}[S_{1}^{\\prime}=S_{1}]\\cdot\\mathbf{Pr}_{(S_{1}^{\\prime\\prime},S_{2}^{\\prime\\prime})\\sim C^{\\circ}}[S_{2}^{\\prime\\prime}=S^{\\prime}|S_{1}^{\\prime\\prime}=S_{1}]}\\\\ &{\\phantom{=}=\\displaystyle\\sum_{S_{1}\\in[n]^{m}}\\mathcal{C}^{\\circ}(S_{1},S^{\\prime})=\\mathcal{C}_{2}^{\\circ}=p^{\\circ}(S^{\\prime})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Now, we show that $S_{1},S_{2}$ , and $S_{3}$ are equal with high probability. This stems from the fact that $\\mathcal{C^{\\bullet}}$ and $\\mathcal{C^{\\diamond}}$ were maximal couplings, and with high probability, the pairs drawn from them are equal. More formally, via the union bound, we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{Pr}_{(S_{1},S_{2},S_{3})\\sim\\mathcal{C}}[S_{1},S_{2},S_{3}]\\geq1-\\mathbf{Pr}_{(S_{1},S_{2},S_{3})\\sim\\mathcal{C}}[S_{1}\\neq S_{2}]-\\mathbf{Pr}_{(S_{1},S_{2},S_{3})\\sim\\mathcal{C}}[S_{1}\\neq S_{3}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathbf{Pr}_{(S_{1},S_{2},S_{3})\\sim\\mathcal{C}}[S_{1}=S_{3}]-\\mathbf{Pr}_{(S_{1},S_{2})\\sim\\mathcal{C}^{\\bullet}}[S_{1}\\neq S_{2}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\underset{S\\in[n]^{m}}{\\sum}\\mathbf{Pr}_{(S_{1},S_{2},S_{3})\\sim\\mathcal{C}}[S_{1}=S\\mathrm{~and~}S_{3}=S]-\\mathbf{Pr}_{(S_{1},S_{2})\\sim\\mathcal{C}^{\\bullet}}[S_{1}\\neq S_{3}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\underset{S\\in[n]^{m}}{\\sum}\\mathcal{C}^{\\circ}(S,S)-\\mathbf{Pr}_{(S_{1},S_{2})\\sim\\mathcal{C}^{\\bullet}}[S_{1}\\neq S_{2}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "(With a similar argument as above) ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{=\\mathbf{Pr}_{(S_{1},S_{2})\\sim\\mathcal{C}^{\\diamond}}[S_{1}=S_{2}]-\\mathbf{Pr}_{(S_{1},S_{2})\\sim\\mathcal{C}^{\\bullet}}[S_{1}\\neq S_{2}]}\\\\ &{}&{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(\\mathrm{by~I}}\\\\ &{}&{=0.99-(1-0.99)=0.98\\,.\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "D Augmented closeness testing ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we focus on the problem of closeness testing in the augmented setting. More formally, we have the following theorem. ", "page_idx": 27}, {"type": "text", "text": "Theorem 9. For every $\\alpha$ and $\\epsilon$ in $(0,1)$ , there exists an algorithm for augmented closeness testing that uses $\\Theta\\left({\\textstyle{\\frac{n^{2/3}\\alpha^{1/3}}{\\epsilon^{4/3}}}}+{\\textstyle{\\frac{\\sqrt{n}}{\\epsilon^{2}}}}\\right)$ samples and succeeds with probability at least $2/3.$ . In addition, any algorithm for this task is required to use $\\begin{array}{r}{\\Omega\\left(\\frac{n^{2/3}\\alpha^{1/3}}{\\epsilon^{4/3}}+\\frac{\\sqrt{n}}{\\epsilon^{2}}\\right)}\\end{array}$ samples. ", "page_idx": 27}, {"type": "text", "text": "For the proof of the upper bound see section 3. And, for the proof of lower bound see section D.1. ", "page_idx": 27}, {"type": "text", "text": "D.1 Lower bound for closeness testing ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we prove a lower bound for augmented closeness testing where a known distribution $\\hat{p}$ is provided satisfying $\\|p-\\hat{p}\\|_{\\mathrm{TV}}=\\alpha$ . We have the following theorem. ", "page_idx": 27}, {"type": "text", "text": "Theorem 10. Suppose we are given $\\alpha$ and $\\epsilon$ in $(0,1)$ . Any $(\\alpha,\\epsilon,\\delta:=11/24)$ -augmented tester for closeness testing of distributions over $[n]$ uses $\\begin{array}{r}{\\Omega\\left(\\frac{n^{2/3}\\alpha^{1/3}}{\\epsilon^{4/3}}+\\frac{\\sqrt{n}}{\\epsilon^{2}}\\right)}\\end{array}$ samples. The lower bound holds even when $\\alpha$ is provided to the algorithm. ", "page_idx": 27}, {"type": "text", "text": "Proof. The $\\Omega(\\sqrt{n}/\\epsilon^{2})$ term comes from the existing lower bound for uniformity testing since we can reduce uniformity testing to augmented closeness testing as follows: Suppose we wish to test whether a distribution $q$ over $[n]$ is uniform or $\\epsilon$ -far from uniform. Suppose $\\boldsymbol{\\mathcal{A}}$ is an $\\left(\\alpha,\\ \\epsilon,\\ 11/24\\right)$ -augmented closeness tester. Set $p$ to be the uniform distribution over $[n]$ . Let $\\hat{p}$ be any distribution that is $\\alpha$ -far from uniform. For instance, $\\hat{p}$ can be a distribution that is $(1+\\alpha)/n$ on half of the elements and $(1-\\alpha)/n$ on the other half. Since $\\boldsymbol{\\mathcal{A}}$ is an augmented tester, it can be used to distinguish whether $p=q$ or $\\|p-q\\|_{\\mathrm{TV}}>\\epsilon$ with probability $11/24$ . ", "page_idx": 27}, {"type": "text", "text": "Upon receiving $\\hat{p}$ and samples from $p$ and $q$ , if $\\boldsymbol{\\mathcal{A}}$ returns inaccurate information or reject, we output reject; otherwise, we output accept. Given Definition 1.1, if $q$ is uniform, we will not output inaccurate information or reject with probability more than $11/24$ . And, if $q$ is $\\epsilon$ -far from the uniform distribution (equivalently $p$ ), we will not output accept with probability more than $11/48$ . Hen\u221ace, we have a tester with confidence parameter $11/48$ for uniformity testing. Thus, $s$ must be $\\Omega(\\sqrt{n}/\\epsilon^{2})$ , which is the number of samples required for uniformity testing [69]. ", "page_idx": 27}, {"type": "text", "text": "To prove the lower bound of $\\Omega\\left(n^{2/3}\\alpha^{1/3}/\\epsilon^{4/3}\\right)$ samples, we only need to focus on the case where $\\alpha\\geq(\\sqrt{n}\\epsilon^{2})^{-1}$ . Otherwise, $n^{2/3}\\alpha^{1/3}/\\epsilon^{4/3}=O(\\sqrt{n}/\\epsilon^{2})$ , making the lower bound trivial due to the $\\Omega(\\sqrt{n}/\\epsilon^{2})$ lower bound shown earlier. We consider the following cases depending on $\\epsilon$ : ", "page_idx": 27}, {"type": "text", "text": "Case 1: $\\epsilon\\,\\leq\\,(\\sqrt{n}\\epsilon^{2})^{-1}$ . Therefore, in this case, the interesting regime of parameter for $\\alpha$ is that $\\alpha\\geq\\epsilon$ . We prove the lower bound for this case in Theorem 11 in Section D.1.1. ", "page_idx": 27}, {"type": "text", "text": "Case 2: $\\epsilon>(\\sqrt{n}\\epsilon^{2})^{-1}$ . In this case, in add\u221aition to considering the case where $\\alpha\\geq\\epsilon$ , we also need to study the case where $\\alpha$ is in $\\left((\\sqrt{n}\\epsilon^{2})^{-1},\\epsilon\\right)$ . We prove the lower bound for this case in Theorem 12 in Section D.1.2. ", "page_idx": 27}, {"type": "text", "text": "This completes the proof. ", "page_idx": 27}, {"type": "text", "text": "D.1.1 Lower bound for $\\alpha\\geq\\epsilon$ ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Below, we provide a lower bound for augmented closeness testing when $\\alpha\\geq\\epsilon$ . At a high level, Theorem 11 achieves this lower bound by creating a challenging instance. This involves embedding the difficult instances of the standard closeness testing problem into another distribution, spanning a domain of $[n-1]$ elements, while concentrating the majority of the distribution\u2019s mass on the last element. The prediction, being a singleton distribution centered on the last element, does not facilitate solving the standard closeness testing problem, which is embedded within the first $[n-1]$ elements. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "Theorem 11. Suppose we are given $\\alpha$ and $\\epsilon$ in $(0,1)$ . Assume we have two unknown distributions $p$ and $q$ and a known distribution $\\hat{p}$ over the domain $[n]$ . If $\\alpha\\geq\\epsilon,$ , then any $(\\alpha,\\epsilon,\\delta:=11/24)$ - augmented tester for testing closeness of $p$ and $q$ requires the following number of samples: ", "page_idx": 28}, {"type": "equation", "text": "$$\ns=\\Omega\\left(\\frac{n^{2/3}\\,\\alpha^{1/3}}{\\epsilon^{4/3}}+\\frac{\\sqrt{n}\\,\\alpha}{\\epsilon^{2}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. Our proof is based on a reduction from standard closeness testing to augmented closeness testing. Assume there exists an algorithm, $\\boldsymbol{\\mathcal{A}}$ , that upon receiving $\\hat{p}$ and $s$ samples from $p$ and $q$ , can distinguish whether $p=q$ or $\\|p-q\\|_{\\mathrm{TV}}\\geq\\epsilon$ with probability 11/24. We show that this algorithm can then be used to test closeness of two distributions, $\\tilde{p}$ and $\\tilde{q}$ over $[n-1]$ with proximity parameter $\\epsilon^{\\prime}:=\\epsilon/\\alpha$ . ", "page_idx": 28}, {"type": "text", "text": "Suppose we have sample access to $\\tilde{p}$ and $\\tilde{q}$ . We construct $p$ and $q$ based on $\\tilde{p}$ and $\\tilde{q}$ as follows: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{p_{i}=\\alpha\\cdot\\tilde{p}_{i}\\quad\\forall i\\in[n-1]\\,,}}&{{\\qquad p_{n}=1-\\alpha\\,,}}\\\\ {{\\mathrm{}}}&{{\\mathrm{}}}\\\\ {{q_{i}=\\alpha\\cdot\\tilde{q}_{i}\\quad\\forall i\\in[n-1]\\,,}}&{{\\qquad q_{n}=1-\\alpha\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Note that given a sample from $\\tilde{p}$ (respectively $\\tilde{q},$ ), we can generate a sample from $p$ (respectively $q$ ) by outputting that sample with probability $\\alpha$ and $n$ otherwise. Also, if $\\tilde{p}$ is equal to $\\tilde{q}$ , then $p$ and $q$ are equal. And, if $\\|\\widetilde{\\boldsymbol{p}}-\\widetilde{\\boldsymbol{q}}\\|_{\\mathrm{TV}}\\ge\\epsilon^{\\prime}$ , then $\\|p-q\\|_{\\mathrm{TV}}\\geq\\epsilon$ due to the following: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|p-q\\|_{\\mathrm{TV}}=\\frac{1}{2}\\sum_{i\\in[n-1]}|p_{i}-q_{i}|=\\frac{\\alpha}{2}\\sum_{i\\in[n-1]}|\\tilde{p}_{i}-\\tilde{q}_{i}|=\\alpha\\cdot\\|\\tilde{p}-\\tilde{q}\\|_{\\mathrm{TV}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Let $\\hat{p}$ be the singleton distribution on $n$ (i.e., $\\hat{p}_{n}=1$ ). In this case, $\\|p-\\hat{p}\\|_{\\mathrm{TV}}$ equals $\\alpha$ . We feed $\\boldsymbol{\\mathcal{A}}$ with $\\hat{p}$ and samples from $p$ and $q$ . Now, if $\\boldsymbol{\\mathcal{A}}$ outputs $p=q$ , we declare $\\tilde{p}=\\tilde{q}$ . Otherwise, we declare $\\tilde{p}$ is $\\epsilon^{\\prime}$ -far from $\\tilde{q}$ . Given our construction, this answer is true with probability 11/24. ", "page_idx": 28}, {"type": "text", "text": "Note that in this process, in expectation, we draw $\\alpha s$ samples from $\\tilde{p}$ and $\\tilde{q}$ . Thus, using Markov\u2019s inequality, the probability of drawing more than $100\\,\\alpha\\,s$ samples from each of $\\tilde{p}$ and $\\tilde{q}$ is at most 0.01. Hence, using $\\boldsymbol{\\mathcal{A}}$ , there exists an algorithm for testing closeness of $\\tilde{p}$ and $\\tilde{q}$ with probability $1-(11/24+0.01)>0.53$ that uses $100\\,\\alpha\\,s$ samples from $\\tilde{p}$ and $\\tilde{q}$ . Using the existing lower bound for the closeness testing problem (see Theorem 4 in [69] and Proposition 4.1 in [30]), we have: ", "page_idx": 28}, {"type": "equation", "text": "$$\n100\\,\\alpha\\,s=\\Omega\\left(\\frac{n^{2/3}}{\\epsilon^{\\prime4/3}}+\\frac{\\sqrt{n}}{\\epsilon^{\\prime2}}\\right)=\\Omega\\left(\\frac{n^{2/3}\\,\\alpha^{4/3}}{\\epsilon^{4/3}}+\\frac{\\sqrt{n}\\,\\alpha^{2}}{\\epsilon^{2}}\\right)\\,{.}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus, by dividing by $100\\alpha$ , we get the following bound as desired: ", "page_idx": 28}, {"type": "equation", "text": "$$\ns=\\Omega\\left(\\frac{n^{2/3}\\,\\alpha^{1/3}}{\\epsilon^{4/3}}+\\frac{\\sqrt{n}\\,\\alpha}{\\epsilon^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "D.1.2 Lower bound for $\\alpha$ in $\\left(c\\cdot({\\sqrt{n}}\\epsilon^{2})^{-1},\\epsilon\\right)$ ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We extend the existing lower bound for closeness testing to the augmented setting. In the standard setting, the lower bound for closeness testing is derived from the hard instances for uniformity testing with one key adjustment: introducing elements with large probability $(\\approx n^{-2/3})$ in the distributions. These large elements share identical probability masses in both $p$ and $q$ , indicating that they do not contribute to the distance between the two distributions. However, their presence in the sample set confuses the algorithm: Because of their large probabilities, their behaviour in the sample set may be deceivingly hint \u201cnot uniform\u201d, making it difficult for the algorithm to decide whether the rest of the distributions are uniform or not. Therefore, the algorithm needs $s\\approx n^{2/3}$ samples to first identify these large elements, and then it can test the uniformity on the re\u221ast of the distributions. The surprising part about this result is that $s\\approx n^{2/3}$ is much larger than the $\\sqrt{n}$ samples which suffice for testing uniformity. ", "page_idx": 28}, {"type": "text", "text": "The challenge in our case is that $\\hat{p}$ may give away the large elements to the algorithm. Hence, to prove the lower bound, we create hard instances by adding as many large elements as possible, without changing $\\hat{p}$ by limiting the overall probability mass of the large elements to $\\alpha$ . ", "page_idx": 29}, {"type": "text", "text": "Here is our instance: we assume $\\hat{p}$ is the uniform distribution; $p$ assigns $\\approx(1-\\alpha)/n$ probability mass to $O(n)$ elements chosen at random, and assigns $\\approx n^{-2/3}$ probability mass to $\\rvert\\alpha\\cdot n^{2/3}$ many elements in the domain. Now, $q$ has two options. Half the time, $q$ is equal to $p$ . The other half, $q$ shares the same large elements, but it is not quite uniform on the rest of the distributions. In particular, $q$ assigns probabilities $(1\\pm\\Theta(\\epsilon))(1-\\alpha)\\bar{/}n$ . to the chosen $O(n)$ elements randomly, making it $\\epsilon$ -far from $p$ .7 Using this construction, we use a result of Valiant [78] to show that these two cases are indistinguishable unless we draw $\\Omega(n^{2/3}\\alpha^{1/3})$ samples. More precisely, we have the following theorem: ", "page_idx": 29}, {"type": "text", "text": "Theorem 12. Suppose we are given the following parameters: a positive integer $n$ , $\\epsilon\\,\\in(0,1/6)$ , and $\\alpha\\in\\big((\\sqrt{n}\\epsilon^{2})^{-1},\\epsilon\\big)$ (with this implicit assumption that $(\\sqrt{n}\\epsilon^{2})^{-1}$ is less than $\\epsilon_{.}$ ). There exists a distribution $\\hat{p}$ and $a$ family of pairs of distribution $p$ and $q$ such that any augmented algorithm for closeness testing must use ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\Omega\\left({\\frac{n^{2/3}\\,\\alpha^{1/3}}{\\epsilon^{4/3}}}\\right)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "samples. ", "page_idx": 29}, {"type": "text", "text": "Proof. Let $k$ be an integer that is at most ", "page_idx": 29}, {"type": "equation", "text": "$$\nk\\le c_{k}\\left(\\frac{n^{2/3}\\,\\alpha^{1/3}}{\\epsilon^{4/3}}\\right)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for a sufficiently small absolute constant $c_{k}$ which we determine later. Assume we have $k$ samples from $p$ and $q$ that are available to the algorithm. We show that there are two (families of) distributions that any augmented tester has to distinguish them with high probability. However, they are information-theoretically indistinguishable using only $k$ samples. ", "page_idx": 29}, {"type": "text", "text": "Setting up parameters: First, we focus on determining a series of parameters which we use in our proof. We define $\\ell$ as follows: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\ell:=\\left\\lfloor\\frac{n^{2/3}\\,\\alpha^{4/3}}{2\\,\\epsilon^{4/3}}\\right\\rfloor-\\mathbb{1}_{n\\neq\\left\\lfloor\\frac{n^{2/3}\\,\\alpha^{4/3}}{2\\,\\epsilon^{4/3}}\\right\\rfloor(\\mathrm{~mod~}2\\,)}\\,.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "It is not hard to see that $\\ell$ is an integer. For our proof, we require that $\\ell$ and $n$ are either both even or both odd. Here, the role of the indicator variable is to reduce $\\ell$ by one in case they are not. Also, $\\ell$ is positive, since due to the range of $\\alpha$ and $\\epsilon$ , we have: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{n^{2/3}\\,\\alpha^{4/3}}{2\\,\\epsilon^{4/3}}>\\frac{n^{2/3}}{2\\,\\epsilon^{4/3}}\\cdot\\left(\\frac{1}{\\sqrt{n}\\,\\epsilon^{2}}\\right)^{4/3}=\\frac{1}{2\\,\\epsilon^{4}}\\geq\\frac{6^{4}}{2}\\,.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Given this definition, there exists an absolute constant $c\\ell<0.5$ , such that $\\ell=c_{\\ell}\\cdot n^{2/3}\\,\\alpha^{4/3}/\\epsilon^{4/3}$ . We use this identity as the definition of $\\ell$ throughout this proof. Given the assumption about the range of $\\alpha$ , we also have: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\ell={\\frac{c_{\\ell}\\cdot n^{2/3}\\alpha^{4/3}}{\\epsilon^{4/3}}}<c_{\\ell}\\,n^{2/3}\\leq{\\frac{n}{2}}\\,.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Let $L$ denote a set of $\\ell$ random elements in $[n]$ to be our large elements. $\\overline{{L}}:=\\left[n\\right]\\backslash L$ denotes the set of small elements. Since $n$ and $\\ell$ are either both even or both odd, $\\overline{{L}}$ has even number of elements. Now, partition $\\overline{{L}}$ to two random subset of the same size $\\overline{{L}}_{1}$ and $\\overline{{L}}_{2}$ . Set $\\epsilon^{\\prime}:=6\\,\\epsilon<1$ . Set ", "page_idx": 29}, {"type": "equation", "text": "$$\nc_{k}:=\\operatorname*{min}\\left(\\frac{1}{5080320\\cdot c_{\\ell}},\\:\\frac{c_{\\ell}}{2000},\\:\\frac{1}{2000}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "image", "img_path": "tAlMAcqK9s/tmp/8170c6fa75b002c5c3a5b787ace56c8f6c760c2c2c3d5b8d1f062dd27c98fb07.jpg", "img_caption": ["Figure 4: A visualization of $\\hat{p},\\,p^{+}$ , and $p^{-}$ . "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Constructing the distributions: We set $\\hat{p}$ to be a uniform distribution over $[n]$ . That is, $\\hat{p}_{i}=1/n$ for every $i\\,\\in\\,[n]$ . We construct two distributions $p^{+}$ and $p^{-}$ as follows. For every $i\\,\\in\\,[n]$ , the probability mass of $i$ according to these distributions are as follows: ", "page_idx": 30}, {"type": "equation", "text": "$$\np_{i}^{+}:=\\left\\{\\begin{array}{l l}{\\displaystyle\\frac{1}{n}+\\frac{\\alpha}{\\ell}}&{\\quad i\\in L}\\\\ {\\displaystyle\\frac{1}{n}-\\frac{\\alpha}{n-\\ell}}&{\\quad i\\in\\overline{{L}}}\\end{array}\\right.\\qquad p_{i}^{-}:=\\left\\{\\begin{array}{l l}{\\displaystyle\\frac{1}{n}+\\frac{\\alpha}{\\ell}}&{\\quad i\\in L}\\\\ {\\displaystyle(1+\\epsilon^{\\prime})\\cdot\\left(\\frac{1}{n}-\\frac{\\alpha}{n-\\ell}\\right)}&{\\quad i\\in\\overline{{L}}_{1}}\\\\ {\\displaystyle(1-\\epsilon^{\\prime})\\cdot\\left(\\frac{1}{n}-\\frac{\\alpha}{n-\\ell}\\right)}&{\\quad i\\in\\overline{{L}}_{2}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For a visual representation of these distributions, refer to Figure 4. It is not hard to verify that the probability masses sum up to one since $|L|=\\ell$ , and $|{\\overline{{L}}}|\\,=\\,n-\\ell$ . Also, these probabilities are non-negative due to Equation (13) and the fact that $\\alpha<\\epsilon\\leq1/6$ : ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{1}{n}-\\frac{\\alpha}{n-\\ell}\\geq\\frac{1}{n}-\\frac{\\alpha}{n/2}>0.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Also, given these definitions, $p^{+}$ is $\\alpha$ -close to $\\hat{p}$ , and $p^{+}$ and $p^{-}$ are $\\epsilon$ -far from each other: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|p^{+}-\\hat{p}\\|_{\\mathrm{TV}}=\\frac{1}{2}\\left(\\sum_{i\\in L}\\frac{\\alpha}{\\ell}+\\sum_{i\\in\\overline{{L}}}\\frac{\\alpha}{n-\\ell}\\right)=\\alpha\\,,\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|p^{+}-p^{-}\\|_{\\mathrm{TV}}=\\frac{1}{2}\\sum_{i\\in\\overline{{L}}}\\epsilon^{\\prime}\\cdot\\left(\\frac{1}{n}-\\frac{\\alpha}{n-\\ell}\\right)=\\frac{\\epsilon^{\\prime}}{2}\\cdot\\left(\\frac{n-\\ell}{n}-\\alpha\\right)>\\frac{\\epsilon^{\\prime}}{2}\\cdot\\left(\\frac{1}{2}-\\frac{1}{6}\\right)\\geq\\epsilon\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof of indistinguishability: Now, consider two pairs of distributions $(p=p^{+},q=p^{+})$ and $(p\\,=\\,p^{+},q\\,=\\,p^{-})$ . Given the distances above any algorithm for augmented closeness testing should output accept on $(p^{+},p^{+})$ , and reject on $(p^{+},p^{-})$ . To establish our lower bound, we show these pairs of distributions are indistinguishable with high probability. We use the moment based indistinguishably result for symmetric properties8 of pairs of distributions in [78]. ", "page_idx": 30}, {"type": "text", "text": "Before stating their theorem, we need to define the notion of the $(k,k)$ -based moments of a pair distributions $(p,q)$ as follows: ", "page_idx": 31}, {"type": "text", "text": "Definition D.1 $\\left(k,k\\right)$ -based moments). For every positive integer $k$ the $(k,k)$ -based moments of $a$ pair of distributions $p$ and $q$ are the following values: ", "page_idx": 31}, {"type": "equation", "text": "$$\n{\\cal M}_{a,b}^{(k)}(p,q):=k^{a+b}\\sum_{i\\in[n]}p_{i}^{a}q_{i}^{b}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for every non-negative integers a and $b$ . ", "page_idx": 31}, {"type": "text", "text": "Fact D.2 (Adapted from [78]). Given an integer $k$ , suppose we have four distributions, $p^{(1)},p^{(2)},q^{(1)}$ , and $q^{(2)}$ over $[n]$ where the maximum probability that each of them assigns to an element is less than $1/(1000\\,k)$ . If ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\sum_{a+b\\geq2}\\frac{\\left|M_{a,b}^{(k)}\\left(p^{(1)},q^{(1)}\\right)-M_{a,b}^{(k)}\\left(p^{(2)},q^{(2)}\\right)\\right|}{\\left\\lfloor\\frac{a}{2}\\right\\rfloor!\\left\\lfloor\\frac{b}{2}\\right\\rfloor!\\sqrt{1+\\operatorname*{max}\\left(M_{a,b}^{(k)}\\left(p^{(1)},q^{(1)}\\right),\\ M_{a,b}^{(k)}\\left(p^{(2)},q^{(2)}\\right)\\right)}}<\\frac{1}{360}\\,,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "then no algorithm can distinguish the pair $\\left(p^{(1)},q^{(1)}\\right)$ from $\\left(p^{(2)},q^{(2)}\\right)$ with a success probability more than $13/24$ upon receiving $\\mathrm{Poi}(k)$ samples from each distribution in the pair. ", "page_idx": 31}, {"type": "text", "text": "First, we verify that all the domain elements of $p^{+}$ and $p^{-}$ have probability masses at most $1/(1000\\,k)$ according to both $p^{+}$ and $p^{-}$ . Recall that $\\epsilon<\\,1$ . Observe that it suffices to show that $1/n\\stackrel{!}{<}1/(2000\\,\\bar{k})$ and $\\alpha/\\ell<1/(2000\\,k)$ . The first inequality holds due to the fact that $\\alpha$ is in $\\left(\\sqrt{n}\\epsilon^{2}\\right)^{-1},\\epsilon)$ : ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{k}{n}=\\frac{c_{k}\\,n^{2/3}\\,\\alpha^{1/3}}{n\\,\\epsilon^{4/3}}=c_{k}\\cdot\\left(\\frac{\\alpha}{\\left(\\sqrt{n}\\epsilon^{2}\\right)^{2}}\\right)^{1/3}<c_{k}\\cdot\\left(\\alpha\\,\\epsilon^{2}\\right)^{1/3}<c_{k}\\cdot\\epsilon<c_{k}\\leq\\frac{1}{2000}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "And, the second inequality comes from the definitions of $\\ell,k$ , and $c_{k}$ : ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{\\alpha}{\\ell}=\\frac{\\alpha\\,\\epsilon^{4/3}}{c_{\\ell}\\,n^{2/3}\\,\\alpha^{4/3}}=\\frac{\\epsilon^{4/3}}{c_{\\ell}\\,n^{2/3}\\,\\alpha^{1/3}}=\\frac{c_{k}}{c_{\\ell}\\,k}\\leq\\frac{1}{2000\\,k}\\,.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "For the rest of this proof, we focus on showing Equation (14) for the pairs $(p^{+},p^{+})$ and $(p^{+},p^{-})$ . For simplicity in our equations, we use the following notation: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\lambda_{\\mathrm{large}}:=\\frac{1}{n}+\\frac{\\alpha}{\\ell}\\,,\\qquad\\lambda_{\\mathrm{small}}:=\\frac{1}{n}-\\frac{\\alpha}{n-\\ell}\\,.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Using the Definition D.1, it is not hard to see that: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M_{a,b}^{(k)}\\left(p^{+},p^{+}\\right)=k^{a+b}\\left(\\displaystyle\\sum_{i\\in L}\\lambda_{\\mathrm{large}}a^{+b}+\\displaystyle\\sum_{i\\in\\overline{{L}}}\\lambda_{\\mathrm{smal}}a^{+b}\\right)=k^{a+b}\\left(\\ell\\cdot\\lambda_{\\mathrm{large}}a^{+b}+(n-\\ell)\\cdot\\lambda_{\\mathrm{smal}}a^{+b}\\right)}\\\\ &{M_{a,b}^{(k)}\\left(p^{+},p^{-}\\right)=k^{a+b}\\left(\\displaystyle\\sum_{i\\in L}\\lambda_{\\mathrm{large}}a^{+b}+\\displaystyle\\sum_{i\\in\\overline{{L}}_{1}}(1+\\epsilon^{\\prime})^{b}\\cdot\\lambda_{\\mathrm{smal}}a^{+b}+\\displaystyle\\sum_{i\\in\\overline{{L}}_{2}}(1-\\epsilon^{\\prime})^{b}\\cdot\\lambda_{\\mathrm{smal}}a^{+b}\\right)}\\\\ &{\\qquad\\qquad=k^{a+b}\\left(\\ell\\cdot\\lambda_{\\mathrm{large}}a^{+b}+(n-\\ell)\\cdot\\lambda_{\\mathrm{smal}}a^{+b}\\cdot\\left(\\frac{(1+\\epsilon^{\\prime})^{b}+(1-\\epsilon^{\\prime})^{b}}{2}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Therefore, we get: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left|M_{a,b}^{(k)}\\left(p^{+},p^{+}\\right)-M_{a,b}^{(k)}\\left(p^{+},p^{-}\\right)\\right|=(n-\\ell)\\cdot\\lambda_{\\mathrm{small}}a^{+b}\\cdot\\left(\\frac{(1+\\epsilon^{\\prime})^{b}+(1-\\epsilon^{\\prime})^{b}}{2}-1\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "symmetric property since permuting the labels of the elements does not change the distance between $p$ and $q$ ;   \nFurthermore, $\\hat{p}$ is identical for every permutation of domain elements. ", "page_idx": 31}, {"type": "text", "text": "On the other hand, we have: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sqrt{1+\\operatorname*{max}\\left(M_{a,b}^{(k)}\\left(p^{(1)},q^{(1)}\\right),\\;M_{a,b}^{(k)}\\left(p^{(2)},q^{(2)}\\right)\\right)}\\geq\\sqrt{k^{a+b}\\cdot\\ell\\cdot\\lambda_{\\mathrm{large}}{}^{a+b}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Using the two equations above, we can bound the right hand side of Equation (14): ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathrm{RHS~of~Eq.~}(14)\\leq\\frac{n-\\ell}{\\sqrt{\\ell}}\\sum_{a+b\\geq2}\\frac{k^{(a+b)/2}\\cdot\\lambda_{\\mathrm{small}}^{\\mathrm{}}a^{+b}}{\\left[\\frac{a}{2}\\right]!\\cdot\\left[\\frac{b}{2}\\right]!\\lambda_{\\mathrm{large}}(a+b)/2}\\cdot\\left(\\frac{(1+\\epsilon^{\\prime})^{b}+(1-\\epsilon^{\\prime})^{b}}{2}-1\\right)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Note that for $b=0$ or 1, the term in the sum is zero. Hence, we only need to iterate over all $b\\geq2$ and $a\\geq0$ . Let $T$ denote $\\sqrt{k/\\lambda_{\\mathrm{large}}}\\cdot\\lambda_{\\mathrm{small}}$ . Then, we can write the above equation in the following form: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathrm{S~of~Eq.~}(14)\\leq\\frac{n-\\ell}{\\sqrt{\\ell}}\\cdot\\left(\\sum_{a\\geq0}\\frac{T^{a}}{\\left\\lfloor\\frac{a}{2}\\right\\rfloor!}\\right)\\cdot\\left(\\frac{1}{2}\\sum_{b\\geq2}\\frac{\\left(T\\cdot\\left(1+\\epsilon^{\\prime}\\right)\\right)^{b}}{\\left\\lfloor\\frac{b}{2}\\right\\rfloor!}+\\frac{1}{2}\\sum_{b\\geq2}\\frac{\\left(T\\cdot\\left(1-\\epsilon^{\\prime}\\right)\\right)^{b}}{\\left\\lfloor\\frac{b}{2}\\right\\rfloor!}-\\sum_{b\\geq2}\\frac{\\left(T\\right)^{b}}{\\left\\lfloor\\frac{b}{2}\\right\\rfloor!}\\right)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "To bound these terms, we have the following lemma: ", "page_idx": 32}, {"type": "text", "text": "Lemma D.3. For $x\\in\\mathbb R$ , we have ", "text_level": 1, "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{\\infty}{\\frac{x^{i}}{\\left\\lfloor{\\frac{i}{2}}\\right\\rfloor!}}=(1+x)e^{x^{2}},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and for $x\\in[0,1.33]$ , we have: ", "page_idx": 32}, {"type": "equation", "text": "$$\nx^{2}+x^{3}\\leq\\sum_{i=2}^{\\infty}{\\frac{x^{i}}{\\lfloor{\\frac{i}{2}}\\rfloor!}}<x^{2}+x^{3}+3\\,x^{4}\\,.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. We start by splitting the terms corresponding to odd and even $i$ \u2019s in the sum, and obtain the following: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{\\infty}{\\frac{x^{i}}{\\left\\lfloor{\\frac{i}{2}}\\right\\rfloor!}}=\\sum_{j=0}^{\\infty}{\\frac{x^{2j}}{j!}}+{\\frac{x^{2j+1}}{j!}}=(1+x)\\sum_{j=0}^{\\infty}{\\frac{x^{2j}}{j!}}=(1+x)\\cdot e^{x^{2}}\\,.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The last equality above is due to the Taylor expansion of $e^{y}=1+y/1!+y^{2}/2!+.\\;.\\;.$ about $y=0$ . Next, we have: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{i=2}^{\\infty}{\\frac{x^{i}}{\\left\\lfloor{\\frac{i}{2}}\\right\\rfloor!}}=\\sum_{i=0}^{\\infty}{\\frac{x^{i}}{\\left\\lfloor{\\frac{i}{2}}\\right\\rfloor!}}-x-1=(1+x)\\cdot\\left(e^{x^{2}}-1\\right)\\,.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Now, if $0\\,<\\,y\\,\\leq\\,1.79$ , then the following inequality holds: $y\\,<\\,e^{y}\\,-\\,1\\,<\\,y\\,+\\,y^{2}$ . Thus, for $x\\in(0,1.33)$ , we have: ", "page_idx": 32}, {"type": "equation", "text": "$$\nx^{2}+x^{3}=(1+x)\\cdot x^{2}\\leq(1+x)\\cdot\\left(e^{x^{2}}-1\\right)<(1+x)\\cdot(x^{2}+x^{4})=x^{2}+x^{3}+(1+x)\\,x^{4}<x^{2}+x^{3}+3\\,x^{4}\\,.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Now, assume $T\\cdot(1+\\epsilon^{\\prime})\\leq1.33$ . We continue bounding the right hand side of Equation (14) via the above fact: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S\\operatorname{of}\\mathbb{E}\\mathbb{q}_{+}\\left(14\\right)\\leq\\displaystyle\\frac{n-\\ell}{\\sqrt{\\ell}}\\cdot\\left(\\sum_{a\\geq0}\\frac{T^{a}}{\\left\\vert\\frac{\\ell}{2}\\right\\vert}\\right)\\cdot\\left(\\frac{1}{2}\\sum_{b\\geq2}\\frac{\\left(T\\cdot\\left(1+\\ell\\right)\\right)^{b}}{\\left\\vert\\frac{\\ell}{2}\\right\\vert}+\\frac{1}{2}\\sum_{b\\geq2}\\frac{\\left(T\\cdot\\left(1-\\ell\\right)\\right)^{b}}{\\left\\vert\\frac{\\ell}{2}\\right\\vert}-\\sum_{b\\geq2}\\frac{\\left(T\\right)^{b}}{\\left\\vert\\frac{\\ell}{2}\\right\\vert}\\right)}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{n}{\\sqrt{\\ell}}\\cdot\\left(\\left(1+T\\right)\\cdot\\epsilon^{T^{2}}\\right)\\cdot\\left(\\frac{1}{2}\\left(T^{2}\\cdot\\left(1+\\epsilon^{\\prime}\\right)^{2}+T^{3}\\cdot\\left(1+\\epsilon^{\\prime}\\right)^{3}+3T^{4}\\cdot\\left(1+\\epsilon^{\\prime}\\right)^{4}\\right)\\right.}\\\\ &{\\qquad\\qquad+\\left.\\frac{1}{2}\\left(T^{2}\\cdot\\left(1-\\epsilon^{\\prime}\\right)^{2}+T^{3}\\cdot\\left(1-\\epsilon^{\\prime}\\right)^{3}+3T^{4}\\cdot\\left(1-\\epsilon^{\\prime}\\right)^{4}\\right)-T^{2}-T^{3}\\right)}\\\\ &{\\qquad=\\displaystyle\\frac{n}{\\sqrt{\\ell}}\\cdot4\\cdot\\left(\\left(\\frac{\\left(1+\\epsilon^{\\prime}\\right)^{2}+\\left(1-\\epsilon^{\\prime}\\right)^{2}}{2}-1\\right)\\cdot T^{2}+\\left(\\frac{\\left(1+\\epsilon^{\\prime}\\right)^{3}+\\left(1-\\epsilon^{\\prime}\\right)^{3}}{2}-1\\right)\\cdot T^{3}\\right.}\\\\ &{\\qquad\\left.+\\left(\\frac{3\\left(1+\\epsilon^{\\prime}\\right)^{4}+3\\left(1-\\epsilon^{\\prime}\\right)^{4}}{2}\\right)\\cdot T^{4}\\right)}\\\\ &{\\qquad\\qquad\\leq\\frac{n}{\\sqrt{\\ell}}\\cdot14\\cdot\\left(\\epsilon^{\\prime}2^{T}+3\\epsilon^{2}\\cdot T^{3}+24T^{4}\\right)\\:.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We can bound $T$ from above given the trivial bounds we have that $\\lambda_{\\mathrm{{small}}}\\leq1/n$ and $\\lambda_{\\mathrm{large}}\\geq\\alpha/\\ell$ by their definitions in Equation (15): ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T:=\\sqrt{\\frac{k}{\\lambda_{\\mathrm{large}}}}\\cdot\\lambda_{\\mathrm{small}}\\leq\\sqrt{\\frac{k\\cdot\\ell}{\\alpha}}\\cdot\\frac{1}{n}\\leq\\frac{\\sqrt{c_{\\ell}\\cdot c_{k}}\\cdot\\big(n^{2/3}\\alpha^{1/3}/\\epsilon^{4/3}\\big)}{n}=\\sqrt{c_{\\ell}\\cdot c_{k}}\\cdot\\Big(\\frac{\\alpha}{n\\epsilon^{4}}\\Big)^{1/3}}\\\\ &{\\quad\\leq\\sqrt{c_{\\ell}\\cdot c_{k}}\\cdot\\alpha<6\\epsilon=\\epsilon^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The second to last inequality above is due to the range we assumed for $\\alpha$ : it is greater than $(\\sqrt{n}\\epsilon^{2})^{-1}$ . F\u221aor the last inequality, we assumed that $\\alpha<\\epsilon$ . Moreover, using $c_{k}\\leq(5080320\\cdot c_{\\ell})^{-1}$ , we have $\\sqrt{c_{\\ell}\\cdot c_{k}}<6$ . Therefore, we obtain: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathrm{RHS~of~Eq.~(14)}<\\cfrac{n}{\\sqrt{\\ell}}\\cdot14\\cdot28\\cdot(\\epsilon^{\\prime}T)^{2}}&{\\mathrm{(using~1\\geq~\\epsilon'>T~in~Eq.~(16))}}\\\\ &{\\leq\\left(14\\cdot28\\cdot6^{2}\\right)\\cdot\\cfrac{n}{\\sqrt{\\ell}}\\cdot\\epsilon^{2}\\cdot\\cfrac{k\\cdot\\ell}{\\alpha}\\cdot\\cfrac{1}{n^{2}}}&\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(\\mathrm{since~}\\epsilon^{\\prime}:=6\\epsilon\\mathrm{~and~}T\\leq\\sqrt{k\\cdot\\ell/\\alpha}/n\\mathrm{~in~middle~of~Eq.~(16))}}\\\\ &{=14112\\cdot\\cfrac{\\epsilon^{2}k}{\\alpha n}\\cdot\\sqrt{\\ell}\\leq(14112\\cdot c_{\\ell})\\cdot\\cfrac{\\epsilon^{4/3}k}{n^{2/3}\\,\\alpha^{1/3}}}&{\\mathrm{(using~}\\ell:=c_{\\ell}\\,n^{2/3}\\alpha^{4/3}/\\epsilon^{4/3})}\\\\ &{\\leq14112\\cdot c_{\\ell}\\cdot c_{k}\\leq\\frac{1}{360}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "For the last inequality, recall that early on we set $k$ to be at most $c_{k}\\left(n^{2/3}\\,\\alpha^{1/3}/\\epsilon^{4/3}\\right)$ and $c_{k}$ is defined to be at most $(5080320\\cdot c_{\\ell})^{-1}$ . Therefore, the requirements in Fact D.2 hold as desired in our setting. Thus, the proof is complete. \u53e3 ", "page_idx": 33}, {"type": "text", "text": "E Empirical Evaluation ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "We evaluate our algorithm empirically on real and synthetic data on the closeness testing problem. Recall that given samples to two unknown distributions $p$ and $q$ over the domain $[n]$ , we wish to test if $p=q$ or if $\\|p-q\\|_{\\mathrm{TV}}\\geq\\epsilon$ . They are referred to as the $(p,p)$ case or the $(p,q)$ case respectively. Our experiments test the performance of our algorithm and baselines. ", "page_idx": 33}, {"type": "text", "text": "Datasets. ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u2022 Hard Instance: This is a simplified version of the hard instance described in Section D.1 for Theorem 12. A conceptually similar hard instance is also used in the classical closeness testing setting without any learned augmentation, as detailed in Section D.1.2 Let $[n]$ be the domain. In the instance used in the experiments, $p$ and $q$ agree on the first $m:=$ $\\lfloor n^{2/3}\\rfloor$ elements. Both distributions have probability mass $1/(2m)$ on these domain elements. $p$ also has probability mass $2/n$ on domain elements between $n/2$ and $3n/4$ and $q$ has probability mass $2/\\bar{n}$ on domain elements between $3n/4$ and $n$ . All other probability masses are 0. We can check that $\\|p-q\\|_{\\mathrm{TV}}=1/2$ . The first $m$ domain elements represent \u2018heavy hitters\u2019 which conspire to fool any testing algorithm and the rest of the domain are items which contribute to all the TV distance but are rarely sampled. Indeed, we built upon this intuition to prove a formal lower bound in Theorem 12. ", "page_idx": 33}, {"type": "image", "img_path": "tAlMAcqK9s/tmp/40efaa094dbba3de1da2f970380916b1fd9bb4f3832481b07d3ae4e39d1865b4.jpg", "img_caption": ["Figure 5: Error as a function of prediction quality for the \u2018Hard Instance\u2019 dataset "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "tAlMAcqK9s/tmp/9bb3e42b05c68d856c4ae126b1a9619da7c8c5de5b70bfaac792fca68f9bb605.jpg", "img_caption": ["Figure 6: Additional figures for the \u2018Hard Instance\u2019 dataset "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "We consider hints of the form $\\hat{p}=(1\\!-\\!\\beta)\\!\\cdot\\!p\\!+\\!\\beta\\!\\cdot\\!\\operatorname{Unif}(n)$ , where ${\\mathrm{Unif}}(n)$ is the uniform distribution on $[n]$ elements and $\\alpha\\in[0,1]$ is an interpolation parameter. $\\alpha=0$ corresponds to a perfect hint and the quality of the hint degrades as $\\alpha$ increase. When $\\alpha=1$ , we receive $\\hat{p}=\\mathrm{Unif}(n)$ , i.e. a hint which not require any learned information to instantiate. In our experiments, we set $n$ , the domain size, to $n=10^{6}$ and our goal is to determine if we are sampling from $(p,p)$ or $(p,q)$ . ", "page_idx": 34}, {"type": "text", "text": "\u2022 IP: The data is internet traffic data collected at a backbone link of a Tier1 ISP in a data center9. Using the preprocessing in [1], we construct the empirical distribution over source $\\mathrm{IP}$ addresses in consecutive chunks of time. Each distribution represents approximately $170m s$ of empirical network traffic data. The support size of each distribution is $\\approx45,000$ while the total domain size is $>2.5\\cdot10^{6}$ . Distributions curated from network traffic closer in time are closer in TV distance. This dataset has been used in other distribution testing literature [1, 48]. ", "page_idx": 34}, {"type": "image", "img_path": "tAlMAcqK9s/tmp/4537b60480bfccf48d2d88613f1b03a351ed93d1d9b9e8f2631b30b5b1243f82.jpg", "img_caption": ["Figure 7: Results for the IP dataset "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "We set $p$ to be distribution for the first chunk of time and we let $q$ be the distribution for the last chunk of time. Empirically, we have $\\Vert p-q\\Vert_{\\mathrm{TV}}\\approx0.72$ . The hint is for $p$ , denoted as $\\hat{p}$ , is the second chunk of time. Unlike the previous case, the hint is relatively far from $p$ in terms of TV distance $\\langle||p-\\hat{p}||_{\\mathrm{TV}}\\approx0.59)$ ). Nevertheless, they share similar \u2018heavy\u2019 items (IP addresses with large probability mass), since $p$ and $\\hat{p}$ represent distributions of network traffic which are close in time. ", "page_idx": 35}, {"type": "text", "text": "Baselines. We compare our main algorithm with the following baselines. ", "page_idx": 35}, {"type": "text", "text": "\u2022 CRS \u201915. This is the state of the art algorithm with access to almost-perfect predictions from [28]. More precisely, they assume predictions to both distributions $p$ and $q$ , denoted as $\\hat{p}$ and $\\hat{q}$ . Furthermore, they require $\\hat{p}(i)\\overset{*}{=}(1\\pm\\varepsilon/100)p(i)$ and $\\hat{q}(i)=(1\\pm\\varepsilon/100)q(i)$ for all $i\\in[n]$ . Note that we only assume access to (much weaker) predictions for only $p$ . \u2022 Closeness Tester. This is the state of the art algorithm of [42] without any predictor access. We refer to this algorithm as the standard or the un-augmented tester as well. ", "page_idx": 35}, {"type": "text", "text": "Error Measurement. At a high level, both our algorithm (denoted as \u2018Augmented Tester\u2019 and the prior SOTA un-augmented approach, (abbreviated as \u2018Closeness Tester\u2019) compute an estimator $Z$ based on the samples requested. Then, both algorithms threshold the value of $Z$ to determine which case we are in (either $p=q$ or $\\|p-q\\|_{\\mathrm{TV}}\\geq\\varepsilon$ . Note that $Z$ is a random variable in both cases (since it depends on random samples). Ideally, the distribution of $Z$ is well-separated in the two different hypotheses. ", "page_idx": 35}, {"type": "text", "text": "Thus in our experiments, given a sample budget, we compute the empirical distribution of $Z$ for both algorithms under both hypothesis cases across 100 trials. For a fixed algorithm, our error measurement computes the dissimilarity of the two empirical histograms: if the histograms are \u2018well-separated\u2019 then we know the algorithm can adequately distinguish the two hypotheses. On the other hand, if the histograms of $Z$ in the two cases are highly overlapping, then it is clear that the algorithm cannot distinguish well. Thus, given the two empirical histograms, we calculate the best empirical threshold which best separates them. This is done for both algorithms, ours and the classical Closeness Tester. Then we measure the fraction of data points in the histogram which are misclassified as the error. Calculating the best empirical threshold also makes both of the algorithms more practical as the theoretical thresholds are only stated asymptotically with un-optimized constant factors, making the theoretical values impractical. Finally, note that it is trivial to get $50\\%$ error. ", "page_idx": 35}, {"type": "text", "text": "CRS \u201815 is a fundamentally different algorithm. Rather than calculating an estimator, it queries the hints $\\hat{p}$ and $\\hat{q}$ on the samples received and check if they are consistent with each other. If all samples $i$ satisfy $\\hat{p}(i)=(1\\pm\\varepsilon/1\\bar{0})\\hat{q}(i)$ , the algorithm declares we are in the $p=q$ case. ", "page_idx": 35}, {"type": "text", "text": "Note that as implemented, our algorithm and the standard Closeness Tester do not require the knowledge of $\\varepsilon$ ( $\\overline{{\\varepsilon}}$ is not needed to calculate the estimator $Z$ ). On the other hand CRS \u201815 crucially requires the knowledge of $\\varepsilon$ . Thus, we make this baseline even stronger by providing it with the exact knowledge of $\\varepsilon$ (which the other two algorithms do not have access to). ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "Results. ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "\u2022 Hard Instance. Our results are shown in Figures 1 and 5. We now describe the plots. Figure 1 displays the performance of all algorithms as a function of the number of samples seen. Note that the $y$ axis measures the fraction of times we can distinguish the two hypotheses across 100 trials. In Figure 1, our algorithm has access to ${\\hat{p}}=p$ and CRS \u201815 has access to ${\\hat{p}}=p$ . Furthermore, it has access to a hint $\\hat{q}$ which satisfies $\\Vert\\hat{q}-q\\Vert_{T V}<0.002$ . $\\hat{q}$ hat is created by removing a small fraction of the mass on a small fraction of the heavy domain elements between 1 and $m$ . ", "page_idx": 36}, {"type": "text", "text": "We see that the performance of both our Augmented Tester and the standard Closeness Tester improves as the sample complexity increases. On the other hand, CRS \u201815 essentially always obtains the trivial guarantee of $50\\%$ error even when it is fed a large number of samples. This is because with decent probability, we sample an element where $\\bar{q}(i)=0$ and $\\hat{p}(i)\\neq0$ . Thus in the $p=q$ case, it is very likely to output the incorrect answer. ", "page_idx": 36}, {"type": "text", "text": "Figure 1 also demonstrates that our augmented tester achieves up to $>20\\mathbf{x}$ reduction in error than the standard Closeness Tester. It also requires $>2\\mathbf{x}$ fewer samples to achieve $0\\%$ error empirically. ", "page_idx": 36}, {"type": "text", "text": "Our improvements are due to the fact that the empirical estimators of $Z$ in the two cases of $p=q$ and $p\\neq q$ are very well-separated for the Augmented Tester. In fact, as shown in Figures 6(a) and 6(b), if we fix the number of samples to $10^{4}$ , the empirical distributions of $Z$ in the two cases do not overlap at all for the augmented algorithm. On the other hand, thee is a significant overlap for the un-augmented tester (the standard Closeness Tester). This implies that the Closeness Tester baseline cannot distinguish the two cases as well as the augmented algorithm. ", "page_idx": 36}, {"type": "text", "text": "\u2022 IP Data. Our results are shown in Figure 7(a). Again we see the same qualitative behavior: CRS \u201815 is quite brittle to prediction errors: in the case $(p,p)$ case, we feed CRS \u201815 with the perfect prediction $p$ for the first distribution, and $\\hat{p}$ for the second. In this case, it typically outputted the incorrect answer. Furthermore, our augmented algorithm has a better sample vs error trade-off than the un-augmented approach. Indeed, we observed that while the augmented tester only required $\\sim\\,500$ samples to obtain $0\\%$ error in 100 trials, the standard tester required $>\\,700$ samples, implying our augmented approach is $>40\\%$ more efficient. ", "page_idx": 36}, {"type": "text", "text": "Robustness to Prediction Error. Our experiments also demonstrate the robustness of our augmented approach to the quality of the predictions. Our theory crisply quantifies how the theoretical sample complexity is affected by a predictor which satisfies $\\|p-\\hat{p}\\|_{\\mathrm{TV}}=\\alpha$ , and correspondingly in practice, we observe that our algorithm is able to take advantage of predictors of varying quality. ", "page_idx": 36}, {"type": "text", "text": "Indeed, for the hard instance, we observe that the augmented approach still obtains better error than the standard tester, even if $\\lVert\\boldsymbol{p}-\\hat{\\boldsymbol{p}}\\rVert_{\\mathrm{TV}}$ is high. This is demonstrated in Figure 5 where we monotonically increase $\\beta$ (as described above). The plot shows that while the error also increases monotonically, it is only when $\\|p-\\hat{p}\\|_{\\mathrm{TV}}\\geq.7$ that the quality is comparable to the standard closeness tester. ", "page_idx": 36}, {"type": "text", "text": "Indeed, even at high values of $\\beta,\\hat{p}$ is still informative of the heavy-hitter structure of $p$ . Therefore, the augmented algorithm can still reliably use $\\hat{p}$ to perform a suitable flattening. ", "page_idx": 36}, {"type": "text", "text": "The phenomenon repeats itself in for the IP data. In Figure 7(b), we compare the performance of using two different predictors. $\\hat{p}_{1}$ is the same predictor as Figure 7(a) whereas $\\hat{p}_{2}$ is the IP distribution that is in between $p$ and $q$ (it is formed using the chunk of time that is exactly in between the first and the last). We observed that now $\\Vert p-\\hat{p}_{2}\\Vert_{\\mathrm{TV}}\\approx0.69$ which is much higher than $\\|p-\\hat{p}_{1}\\|_{\\mathrm{TV}}$ . Nevertheless, it is still the case that many heavy-hitters between $p$ and $\\hat{p}_{2}$ are shared. Thus even though the TV distribution between $p$ and the hint $\\hat{p}_{2}$ maybe high, our algorithm can still exploit the shared heavy-hitter structure to obtain improved sample complexity over the un-augmented approach. This demonstrates the practical versatility of our algorithm beyond the precise theoretical bounds. ", "page_idx": 36}, {"type": "table", "img_path": "tAlMAcqK9s/tmp/0c6a7140ac6284649c65a2c3090b03fc3ccd485717d8068926ae023cf84e4920.jpg", "table_caption": [], "table_footnote": [], "page_idx": 37}, {"type": "text", "text": "F Background and existing results ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "F.1 Estimating the $\\ell_{2}$ -norm ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "In this secti\u221aon, we show that one can estimate the $\\ell_{2}$ -norm of a distribution via the number of collisions among $O({\\sqrt{n}})$ samples. This result was implicitly known from previous work including [52]. Here, we include a formal statement and a proof for the sake of completeness. ", "page_idx": 37}, {"type": "text", "text": "Fact F.1. [Adapted from [52]] Suppose we have a parameter $\\delta\\in(0,1)$ and an arbitrary\u221a distribution $p$ over $[n]$ for which we have sample access to. Algorithm 4 receives $n$ and $\\delta$ and $s=O\\left({\\sqrt{n}}\\cdot\\log(1/\\delta)\\right)$ samples from $p$ and outputs an estimate $L$ of the the $\\ell_{2}^{2}$ -norm of $p$ such that with probability $1-\\delta$ , we have: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{\\|p\\|_{2}}{2}\\leq L\\leq\\frac{3\\ \\|p\\|_{2}}{2}\\,.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. Suppose we have $s$ samples drawn from $p$ : $x_{1},x_{2},\\ldots,x_{s}$ . Let $X$ denote the number of collisions among these samples. That is, the number of pairs of equal samples $\\textstyle X:=\\sum_{i<j}\\mathbb{1}_{x_{i}=x_{j}}$ . In [52], they have shown that $\\mathbf{E}[X]={\\binom{s}{2}}\\cdot{\\Vert p\\Vert_{2}^{2}}$ and $\\mathbf{Var}[X]$ is at most 2 $\\left(\\mathbf{E}[X]\\right)^{3/2}$ . Hence, using Chebyshev\u2019s inequality, we obtain: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{Pr}\\Bigg[\\left|\\frac{X}{\\binom{s}{2}}-\\|p\\|_{2}^{2}\\right|\\geq\\frac{\\|p\\|_{2}^{2}}{2}\\Bigg]\\leq\\frac{\\mathbf{V}\\mathbf{ar}\\big[X/\\binom{s}{2}\\big]}{\\|p\\|_{2}^{4}/4}\\leq\\frac{8\\,\\mathbf{E}[X]^{3/2}}{\\binom{s}{2}^{2}\\cdot\\|p\\|_{2}^{4}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{8}{\\binom{s}{2}^{1/2}\\cdot\\|p\\|_{2}}\\leq\\frac{8}{(s/2)\\cdot1/\\sqrt{n}}\\leq0.1}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "The seco\u221and to the last inequality holds since the $\\ell_{2}$ -nor\u221am of any discrete distribution over $[n]$ is at most $1/\\sqrt{n}$ . And, the last inequality holds for $s\\geq160{\\sqrt{n}}$ . The above inequality implies that each $L_{i}$ in the algorithm is within the desired bound with probability at least 0.9. ", "page_idx": 37}, {"type": "text", "text": "To find an accurate estimate with a probability $1-\\delta$ , we use standard amplification technique: We repeat this process $O(\\log(1/\\delta))$ times and take the median of these estimates. Using the Chernoff bound, one can show that median is accurate with probability at least $1-\\delta$ . ", "page_idx": 37}, {"type": "text", "text": "F.2 Standard closeness tester ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "In this section, we present the result of [30] for robust $\\ell_{2}$ -distance estimation between two unknown distributions $p$ and $q$ . Their results implies an $\\ell_{1}$ -tester for closeness testing of $p$ and $q$ as it has been used in [42]. The important feature of this tester is its adaptive sample complexity to $\\operatorname*{max}\\left(\\left\\|p\\right\\|_{2}^{2},\\left\\|q\\right\\|_{2}^{2}\\right)$ which combined with the flattening technique would achieve the optimal sample complexities for several distribution testing problems. Later [9] have shown that one can modify the this closeness tester such that the sample complexity only deepens on $\\operatorname*{min}\\left(\\left\\|p\\right\\|_{2}^{2},\\left\\|q\\right\\|_{2}^{2}\\right)$ . ", "page_idx": 37}, {"type": "text", "text": "Fact F.2 (Adapted from [30, 42, 9]). Suppose we have two unknown distributions $p$ and q over $[n]$ . Let $b\\geq\\operatorname*{min}\\left(\\left\\|p\\right\\|_{2}^{2},\\left\\|q\\right\\|_{2}^{2}\\right)$ . Algorithm 5 can distinguish whether $p=q$ or $\\epsilon$ -far from each other with probability $1-\\delta$ using $O(n\\sqrt{b}/\\epsilon^{2})$ samples. ", "page_idx": 37}, {"type": "text", "text": "Algorithm 5 $\\ell_{2}^{2}$ -norm estimation   \n1: procedure STANDARD CLOSENESS TESTER(sample access to p, n, \u03b4)   \n2: $L_{p}\\gets\\mathrm{ESTIMATE}$ - $\\ell_{2}^{2}(p,\\;n,\\;\\delta/3)$   \n3: $L_{q}\\leftarrow$ ESTIMATE- $\\ell_{2}^{2}(q,\\;n,\\;\\delta/3)$   \n4: if $\\dot{L}_{p}/L_{q}\\not\\in[1/3,3]$ then   \n5: return reject   \n6: $\\begin{array}{r}{t\\leftarrow O(\\log(1/\\delta)}\\\\ {m\\leftarrow O\\left(\\frac{n\\cdot\\sqrt{b}}{\\epsilon^{2}}\\right)}\\end{array}$   \n7:   \n8: accept-count $\\gets0$   \n9: for $i=1,\\dots,t$ do   \n10: Draw $m$ samples from $p$ and $q$ .   \n11: Let $X_{i}$ and $Y_{i}$ denote the number of occurrences of element $i$ in the samples drawn from   \ndistributions $p$ and $q$ respectively.   \n12: $\\begin{array}{r}{Z\\gets\\frac{\\overbrace{\\sum_{i=1}^{n}(X_{i}-Y_{i})^{2}-X_{i}-Y_{i}}^{\\star}}{m}}\\\\ {\\mathbf{if}\\;Z\\leq\\frac{\\epsilon}{2\\sqrt{n}}\\;\\mathbf{then}}\\\\ {\\mathrm{accept-count}\\gets\\mathrm{accept-count}+1}\\end{array}$   \n13:   \n14:   \n15: if accept-count $\\geq t/2$ then   \n16: return accept   \n17: return reject ", "page_idx": 38}, {"type": "text", "text": "F.3 Useful facts ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Coupling: Suppose we have two probability distributions $p$ and $q$ over a finite domain $\\Omega$ . A coupling between $p$ and $q$ is a joint distribution over $\\Omega^{2}$ where the two marginals of $\\mathcal{C}$ are $p$ and $q$ . Below is the fundamental coupling lemma: ", "page_idx": 38}, {"type": "text", "text": "Fact F.3. (Coupling Lemma) For every two probability distributions over a finite domain $\\Omega_{-}$ , we have: ", "page_idx": 38}, {"type": "text", "text": "\u2022 For any coupling $\\mathcal{C}$ of $p$ and $q$ , we have: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbf{Pr}_{(X,Y)\\sim c}[X\\neq Y]\\geq\\|p-q\\|_{T V}\\,.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "\u2022 There exists a maximal coupling $\\mathcal{C}^{*}$ (also known as optimal coupling) for which the above inequality holds: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbf{Pr}_{(X,Y)\\sim{\\mathcal{C}}^{*}}[X\\neq Y]=\\|p-q\\|_{T V}\\,.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "G Augmented flattening for guarantees other than total variation distance ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Below, we provide a simple lemma that indicates how one can use $\\hat{p}$ to flatten a distribution based on various guarantees of the prediction. Suppose our target $\\ell_{2}^{2}$ -norm which we desire is $v$ . In the following lemma, we propose a flattening $F$ to decrease the $\\ell_{2}^{2}$ to $v$ up to some multiplicative factor that depends on the quality of the estimates. ", "page_idx": 38}, {"type": "text", "text": "Lemma G.1. Suppose we have an unknown distribution $p=(p_{1},p_{2},...,p_{n})$ over $[n]$ . Assume for every $i\\in[n]$ we are give an estimate $\\hat{p}_{i}$ . Then for every $v\\leq1$ , there exists a flattening such that $i t$ increases the domain size by $1/v$ and the $\\ell_{2}^{2}$ -norm of the $\\boldsymbol{p}^{(F)}$ is bounded by: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\left\\|p^{(F)}\\right\\|_{2}^{2}\\leq v\\cdot\\left(\\sum_{i=1}^{n}\\frac{p_{i}^{2}}{\\hat{p}_{i}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. For every $i\\in[n]$ , we set $m_{i}$ as follows: ", "page_idx": 38}, {"type": "equation", "text": "$$\nm_{i}:=\\left\\lfloor\\frac{\\hat{p}_{i}}{v}\\right\\rfloor+1\\,.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "It is not hard to see: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left\\|p^{(F)}\\right\\|_{2}^{2}=\\sum_{i=1}^{n}\\sum_{j=1}^{m_{i}}\\left({\\frac{p_{i}}{m_{i}}}\\right)^{2}=\\sum_{i=1}^{n}{\\frac{p_{i}^{2}}{m_{i}}}\\leq\\sum_{i=1}^{n}{\\frac{p_{i}^{2}}{\\hat{p}_{i}/v}}\\leq v\\cdot\\left(\\sum_{i=1}^{n}{\\frac{p_{i}^{2}}{\\hat{p}_{i}}}\\right)\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "In addition, the new domain size is bounded as follows: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}m_{i}\\leq n+\\frac{1}{v}\\cdot\\sum_{i=1}^{n}\\hat{p}_{i}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Using $\\textstyle\\sum_{i=1}^{n}{\\hat{p}}_{i}=1$ , we conclude the statement of the lemma. ", "page_idx": 39}, {"type": "text", "text": "In the following, we discuss a few cases for which we have some guarantee on the accuracy of the estimates and how it affects the $\\ell_{2}$ -norm reduction. Note that our desired case here is \u221ato not blow up the domain size by more than a constant factor and bring down the $\\ell_{2}$ -norm to $O(1/{\\sqrt{n}})$ . ", "page_idx": 39}, {"type": "text", "text": "1. Exact values: Assume $\\hat{p}_{i}=p_{i}$ . By setting $v=1/n$ , the $\\ell_{2}$ -norm reduces to $\\Theta(1/{\\sqrt{n}})$ , and the new domain size is $\\Theta(n)$ . 2. Multiplicative upper bounds: Suppose for every $i$ , we are given an estimate $\\hat{p}_{i}$ such that $\\hat{p}_{i}\\geq\\alpha p_{i}$ for a parameter $\\alpha\\geq1$ . Then , we have $\\textstyle\\sum_{i=1}^{n}p_{i}^{2}/{\\bar{p}}_{i}$ is at most $1/\\alpha$ . Also, let $\\beta$ denote $\\textstyle\\sum_{i=1}^{n}{\\hat{p}}_{i}$ . Thus, the sample complexity of the tester in [30], is: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\#\\mathrm{~samples}=O\\left(\\frac{\\mathrm{new~domain~size}\\cdot\\operatorname*{min}\\big(\\|p^{(F)}\\|_{2},\\|q^{(F)}\\|_{2}\\big)}{\\epsilon^{2}}\\right)}\\\\ &{\\qquad\\qquad=O\\left(\\frac{(n+\\beta/v)\\cdot\\sqrt{v/\\alpha}}{\\epsilon^{2}}\\right)=O\\left(\\frac{n\\sqrt{v/\\alpha}+\\beta/\\sqrt{\\alpha v}}{\\epsilon^{2}}\\right)}\\\\ &{\\qquad\\qquad=O\\left(\\frac{\\sqrt{n\\,\\beta/\\alpha}}{\\epsilon^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Note that the last inequality is achieved by minimizing over $v$ and setting $\\textit{v}=\\,\\beta/n$ .   \nMoreover, this bound gives us the optimal sample complexity for constant $\\alpha$ and $\\beta$ . ", "page_idx": 39}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: All theoretical claimed are backed by full proofs. ", "page_idx": 40}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We prove tight lower bounds for our setting and discuss alternate prediction models. ", "page_idx": 40}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: All theorems have full, formal proofs. ", "page_idx": 40}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: A Jupyter notebook is included in the submission which shows has the full code. ", "page_idx": 40}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We include a link to the CAIDA dataset and a Jupyter notebook is attached to the submission. ", "page_idx": 40}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: All experimental details are given in Section E. ", "page_idx": 40}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] Justification: See e.g. Figures 6(a), 6(b). ", "page_idx": 40}, {"type": "text", "text": "8. Experiments Compute Resources ", "page_idx": 40}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: We use sample complexity as a measure of efficiency, not computational time (although all of our algorithms are very efficient). ", "page_idx": 40}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] Justification: The paper conforms to the NeurIPS Code of Ethics. ", "page_idx": 41}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] Justification: There is no societal impact of the work performed. ", "page_idx": 41}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper has no language models, image generators, or scraped datasets and poses no such risks. ", "page_idx": 41}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: The datasets used are fully described in Section E. ", "page_idx": 41}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] Justification: No new assets introduced. ", "page_idx": 41}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 41}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: No human subjects used. ", "page_idx": 41}]