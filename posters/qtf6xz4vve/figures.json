[{"figure_path": "Qtf6Xz4VvE/figures/figures_4_1.jpg", "caption": "Figure 1: Learning behavior of the BG-RBM with one hidden node, using data from the Mattis model at different inverse temperatures, system sizes and learning rates \u03b2, \u039d\u03bd, \u20ac. The argument of the exponential curves is set to m\u00b2\u2208N, where e is the learning rate. Inset: (top) behavior of the susceptibility x (bottom) magnetization h* of the learning RBM. The vertical line marks the point at which the susceptibility diverges, indicating the onset of spontaneous magnetization. Right: Learning curves for RBMs learning two correlated patterns. The dashed curves represent the weights of the two hidden nodes projected onto \u00bf\u00b9 + \u00a32, while the dashed-dotted curves are projected onto \u00a3\u00b9 \u2013 \u00a32. Inset: Exponential growth during the two phases: top shows growth in the direction \u00a7\u00b9 + \u00a72 at a rate r\u00b2(1+\u043a)/2, and bottom shows growth in the direction \u00a7\u00b9 \u2013 \u00a32 at a rate p\u00b2(1 \u2013 \u043a)/2. The arguments of the exponentials are not adjusted.", "description": "This figure shows the learning dynamics of a Binary-Gaussian Restricted Boltzmann Machine (BG-RBM) with one hidden node trained on data generated by the Mattis model.  The left panel illustrates the model's susceptibility and magnetization, revealing a phase transition.  The right panel shows learning curves for RBMs learning two correlated patterns, demonstrating exponential growth in different phases. Insets provide detailed views of the susceptibility and magnetization near the transition and illustrate exponential weight growth.", "section": "Theory of learning dynamics for simplified high-dimensional models of data"}, {"figure_path": "Qtf6Xz4VvE/figures/figures_5_1.jpg", "caption": "Figure 2: Human genome dataset. Progressive coding of the main directions of the dataset when training an RBM with the human genome dataset [34]. In A, we show the dataset projected along the first two principal components of the dataset na with a = 1,2, and mPCA = Na \u00b7 x(d)/\u221aNv, with \u00e6(d) referring to the different entries in the dataset, i.e. an human individual. Points are colored according to the individual continental origin. In B, we show the evolution of the singular values Wa of the RBM weight matrix W as a function of the number of training epochs, and in C, we show the scalar product of the corresponding singular vectors u with the corresponding PCA component na. In D, we show the magnetization of the samples generated by the model at different epochs, projected along the first two eigenvectors of W, which shows that the specialization of the model occurs through the progressive encoding of the main modes of the data in W.", "description": "Figure 2 shows the progressive learning of the main directions of a human genome dataset using a Restricted Boltzmann Machine (RBM). Panel A displays the dataset projected onto its first two principal components, colored by continental origin. Panel B illustrates the evolution of the RBM's singular values during training, reflecting the progressive learning of the dataset's structure. Panel C depicts the alignment of the RBM's singular vectors with the dataset's principal components. Finally, panel D visualizes the model's learned distribution at different training stages, highlighting the progressive encoding of the main modes through a series of phase transitions.", "section": "Numerical Analysis"}, {"figure_path": "Qtf6Xz4VvE/figures/figures_7_1.jpg", "caption": "Figure 3: Traning with the MNIST dataset. In A we show the evolution of the singular values of the RBM's coupling matrix W as a function of the training time. In B we show the evolution of the susceptibilities associated with the magnetizations along the right singular vectors of W, ma = (\u03ba\u03b1\u00b7\u03c5)/N. In both figures, we consider the standard Nv = 282 MNIST dataset, different colors refer to different modes. In C we show the susceptibility associated with the overlaps q and q between visible and hidden variables. In D we show the susceptibility of the first mode as a function of the first singular value w\u2081 obtained with trainings on MNIST data scaled to different system sizes above and below L = 28. The numerical curves are compared with the theoretical expectation using the Mattis model in Eq. (6) using W1,c = 4.45. The same data are shown in E, scaled using the mean-field finite-size scaling ansatz of Eq. (7). In F, we show the first 10 modes' susceptibilities Xma as a function of their corresponding singular value wa and compare them with the theoretical curve in D. In G, we show the MCMC relaxation time of the machines trained with different N datasets as a function of w\u2081, together with the theoretical expectation for local moves in dashed lines.", "description": "This figure shows the results of training Restricted Boltzmann Machines (RBMs) on the MNIST dataset with different system sizes.  Panel A displays the evolution of singular values of the weight matrix over training epochs. Panel B shows the susceptibilities associated with magnetizations along right singular vectors. Panel C shows susceptibilities related to overlaps between visible and hidden units. Panel D compares the susceptibility of the first mode (obtained with various system sizes) to a theoretical model prediction. Panel E shows the same data after finite-size scaling. Panel F shows susceptibilities for the first 10 modes, and panel G shows MCMC relaxation time.", "section": "5 Numerical Analysis"}, {"figure_path": "Qtf6Xz4VvE/figures/figures_8_1.jpg", "caption": "Figure 4: Training with the CELEBA and HGD datasets: In A, we plot the hidden susceptibility for different system sizes in the CELEBA dataset, with dashed lines indicating the expected divergence at w\u2081,c = 4. In B, we show the mean-field FFS associated with the first transition using mean-field exponents. In C and D, we present the visible susceptibility for the first phase transition in the HGD dataset, using w\u2081,c = 5.25 for scaling. In E, typical hysteresis in the low-temperature phase is illustrated for CELEBA (128\u00d7128), similar to the mean-field Ising model in external fields.", "description": "Figure 4 presents the numerical analysis performed on two datasets, CelebA and HGD. Panel A shows the hidden susceptibility for different system sizes in the CelebA dataset. Panel B shows the mean-field finite-size scaling (FFS) associated with the first transition. Panels C and D show the visible susceptibility for the first phase transition in the HGD dataset. Panel E shows the hysteresis in the low-temperature phase for CELEBA (128x128).", "section": "5 Numerical Analysis"}, {"figure_path": "Qtf6Xz4VvE/figures/figures_16_1.jpg", "caption": "Figure 5: Left: learning behavior of the Binary-Binary RBM, using data from the Mattis model. The different curves correspond to systems of size Nv = 900 at inverse temperature \u03b2 = 1.4 with learning rate  \u03b5 = 0.03, 0.04, 0.05 and Nv = 400, 700, 1000 respectively. The argument of the exponential curves are not adjusted but set to m\u00b2\u03b5/\u03b1. Right: we illustrate the RBM\u2019s dynamics in the binary-binary case with \u03b2 = 1.4 and Nv = 900, Nh = 400. First the eigenvector ua=1 aligns itself with the pattern \u03be. Then, the eigenvalue Wa=1 grows exponentially until reaching saturation and when it crosses the value 1, the system develops a spontaneous magnetization.", "description": "This figure shows the learning behavior of a Binary-Binary Restricted Boltzmann Machine (RBM) trained on data generated by the Mattis model. The left panel displays the exponential growth of the weights (singular values) as a function of training epochs for different system sizes and learning rates. The right panel illustrates the dynamics of the RBM, showing the alignment of the eigenvector with the pattern \u03be and the subsequent exponential growth of the eigenvalue until a phase transition occurs.", "section": "4.2 Learning multiple features though a cascade of phase transitions"}, {"figure_path": "Qtf6Xz4VvE/figures/figures_18_1.jpg", "caption": "Figure 6: Left: the empirical dynamics of the eigenvalues of the weight matrix, here denotes wa in blue. In green, the predicted dynamics as in eq. 11, adjusting only the initial conditions Un1 (0) and Un2 (0). We see that the curves cross the line y = 1 at the same moments t1 and 111. Right: the free energy in the plane (h1, h2), the order parameters of the model. For different value of the weights during learning, we reconstruct the free energy of the system. We clearly see how the RBM first creates two minima, in the direction of n\u00b9, and then, split again to obtain the four fixed points.", "description": "The left panel shows the evolution of eigenvalues of the weight matrix during learning. The blue line represents empirical data, while the green line represents the model prediction.  Both lines cross y=1 at the same times (t1 and t11), indicating phase transitions. The right panel visualizes the free energy landscape of the model at different learning stages. It shows how the RBM first develops two minima (along the direction n\u00b9) and subsequently splits into four minima.", "section": "4.2 Learning multiple features though a cascade of phase transitions"}, {"figure_path": "Qtf6Xz4VvE/figures/figures_21_1.jpg", "caption": "Figure 3: Traning with the MNIST dataset. In A we show the evolution of the singular values of the RBM's coupling matrix W as a function of the training time. In B we show the evolution of the susceptibilities associated with the magnetizations along the right singular vectors of W, ma = (\u03ba\u03b1\u00b7\u03c5)/N. In both figures, we consider the standard Nv = 282 MNIST dataset, different colors refer to different modes. In C we show the susceptibility associated with the overlaps q and q between visible and hidden variables. In D we show the susceptibility of the first mode as a function of the first singular value w\u2081 obtained with trainings on MNIST data scaled to different system sizes above and below L = 28. The numerical curves are compared with the theoretical expectation using the Mattis model in Eq. (6) using W1,c = 4.45. The same data are shown in E, scaled using the mean-field finite-size scaling ansatz of Eq. (7). In F, we show the first 10 modes' susceptibilities Xma as a function of their corresponding singular value wa and compare them with the theoretical curve in D. In G, we show the MCMC relaxation time of the machines trained with different N datasets as a function of w\u2081, together with the theoretical expectation for local moves in dashed lines.", "description": "This figure shows the results of training a Restricted Boltzmann Machine (RBM) on the MNIST dataset.  It demonstrates the progressive learning of data features through a series of phase transitions, visualized through singular value decomposition (SVD) of the weight matrix, susceptibilities, and relaxation times. The figure provides both empirical results and theoretical comparisons using a Mattis model.", "section": "5 Numerical Analysis"}]