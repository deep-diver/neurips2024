[{"figure_path": "0Gl5WxY6es/tables/tables_8_1.jpg", "caption": "Table 1: Comparing the effect of the RVQ action space adapter on the success rate of non-LLM based policies. Red indicates RVQ hurts over Pred and green indicates RVQ helps over Pred. RVQ typically has a negative impact on the Scratch policy, and helps the smaller RT-Inspired policy.", "description": "This table compares the performance of three different policy architectures (MLLM, Scratch, and RT-Inspired) when using either a standard prediction (Pred) or residual vector quantization (RVQ) action space adapter.  The results highlight that the benefits of RVQ are specific to MLLMs, improving performance significantly in those models, but causing a decrease in performance for the other, less-complex, architectures.  The improvements shown are in terms of the success rate on the CALVIN, MetaWorld, and Habitat Pick tasks.", "section": "4.2 Continuous Action Space Adapter Comparison"}, {"figure_path": "0Gl5WxY6es/tables/tables_13_1.jpg", "caption": "Table 2: Comparing our investigation to prior work. Prior work typically analyzes a single action adapter in a single environment. We study a variety of action adapters across a variety of environments.", "description": "This table compares the authors' work to previous research on adapting large language models (LLMs) for embodied tasks.  It highlights that prior works often focused on a single action space adapter within a limited set of environments, whereas the authors' work took a broader approach by evaluating multiple adapters across five different environments, encompassing 114 tasks. The table provides a summary of the different works, their methods, action space types, and whether they leveraged LLMs. This comparison underscores the comprehensive and systematic nature of the authors' investigation.", "section": "Related Work"}, {"figure_path": "0Gl5WxY6es/tables/tables_17_1.jpg", "caption": "Table 3: Hyperparameters for all imitation learning experiments. Most hyperparameters are the same between environments but the number of training epochs, context length and batch size per GPU are adjusted to fit the need for history, environment dataset size and task complexity.", "description": "This table presents the hyperparameters used in the imitation learning experiments across different environments.  While most parameters remain consistent, adjustments were made to training epochs, context length, and batch size per GPU to accommodate variations in data size, task complexity, and historical requirements.", "section": "3 Method"}, {"figure_path": "0Gl5WxY6es/tables/tables_18_1.jpg", "caption": "Table 4: Evaluation results at 20M steps of RL training for all results in Language Rearrangement. We show averages and standard deviations over 2 random seeds of full policy training.", "description": "This table presents the performance of three different action space adapters (SemLang, Lang, Pred) on the Language Rearrangement task after 20 million reinforcement learning steps.  The results are broken down by various task aspects, including overall success rate and performance across different instruction types (paraphrastic robustness, novel objects, multiple objects, etc.).  The table shows average success rates and standard deviations across two random training seeds, offering insights into the generalization capability and sample efficiency of each action space adapter.", "section": "4.3 Discrete Action Adapter Comparison"}, {"figure_path": "0Gl5WxY6es/tables/tables_19_1.jpg", "caption": "Table 5: Breakdown on every CALVIN task. Note there are not an equal proportion of all tasks in the evaluation dataset.", "description": "This table presents a detailed breakdown of the performance of different action space adapters (RVQ, Pred, VQ, Uniform) on each individual task within the CALVIN benchmark.  It shows the success rate (%) achieved by each adapter on each task. The note highlights that the distribution of tasks in the evaluation dataset is not perfectly uniform, meaning some task types are represented more often than others.", "section": "4.2 Continuous Action Space Adapter Comparison"}, {"figure_path": "0Gl5WxY6es/tables/tables_19_2.jpg", "caption": "Table 6: Breakdown on every BabyAI task.", "description": "This table presents a performance breakdown for different action space adapters (SemLang, Lang, Pred) on various tasks within the BabyAI environment.  Each row represents a distinct task type (e.g., 'goto', 'pickup', 'open'), and the columns show the success rate for each of the three adapters.  The results quantify the effectiveness of each action space adaptation method in tackling different types of tasks in the BabyAI environment.", "section": "4.3 Discrete Action Adapter Comparison"}, {"figure_path": "0Gl5WxY6es/tables/tables_20_1.jpg", "caption": "Table 7: Breakdown on every Meta-World task.", "description": "This table presents the success rate of different action space adapters (ASA) for each of the 45 tasks in the Meta-World environment.  The ASAs compared are Residual Vector Quantized Tokenization (RVQ), Regression (Pred), Vector Quantized Tokenization (VQ), and Uniform. The success rate is shown for each ASA and each task, allowing for a detailed comparison of their performance across a variety of tasks and difficulty levels.", "section": "4.2 Continuous Action Space Adapter Comparison"}]