[{"figure_path": "0Gl5WxY6es/figures/figures_1_1.jpg", "caption": "Figure 1: We empirically analyze how to ground MLLMs in actions across 114 tasks in continuous and discrete action spaces. In each environment, we train a multi-task policy with different Action Space Adapters (ASAs) to re-parameterize the MLLM to output actions. For continuous actions, learning a tokenization with several tokens per-action performs best (Residual VQ). For discrete actions, mapping actions to semantically related language tokens performs best (Semantic Tokenization).", "description": "This figure summarizes the main results of the paper, showing how different methods of grounding Multimodal Large Language Models (MLLMs) into actions perform across various tasks and environments.  The figure compares the performance of different Action Space Adapters (ASAs) for both continuous and discrete action spaces. For continuous actions, a learned tokenization method (Residual VQ) showed the best performance, while for discrete actions, semantically aligning actions with the MLLM's token space yielded superior results (Semantic Tokenization). The figure visually displays the success rates of each ASA on a bar chart, along with example action outputs for both continuous and discrete actions.", "section": "1 Introduction"}, {"figure_path": "0Gl5WxY6es/figures/figures_2_1.jpg", "caption": "Figure 2: Generic architecture studied here for adapting MLLMs for action-specific decision making. The MLLM takes the embedding of the task instruction, prompt, and visual tokens as input. The MLLM then autoregressively predicts a sequence of m action tokens. These action tokens are then decoded into an environment-specific action.", "description": "This figure illustrates the general architecture used to adapt Multimodal Large Language Models (MLLMs) for action-specific decision-making.  The architecture comprises several key components:\n\n1. **Input:** The MLLM receives embeddings of the task instruction, a prompt, and visual tokens.  The visual tokens are processed by a visual encoder and then downsampled to reduce dimensionality before being fed to the MLLM.\n2. **MLLM Processing:** The MLLM processes these embeddings autoregressively to predict a sequence of *m* action tokens.\n3. **Action Space Adapter (ASA):**  An ASA (the focus of the paper) is employed, consisting of an adapter head, an adapter embedding, and an adapter decoder. The adapter head processes the MLLM's final hidden state, which are then embedded and passed autoregressively through the MLLM to further refine the action tokens.\n4. **Action Decoding:** Finally, the adapter decoder transforms the sequence of action tokens into an action that can be executed within a specific environment.", "section": "3 Method"}, {"figure_path": "0Gl5WxY6es/figures/figures_5_1.jpg", "caption": "Figure 3: Comparing ASAs for continuous and discrete action spaces across 5 environments. For continuous actions, the RVQ tokenization performs best. For discrete actions, SemLang performs best. Each bar gives the average over all tasks in the environment with the full breakdown in Appendix E.", "description": "This figure compares the performance of different Action Space Adapters (ASAs) for both continuous and discrete action spaces across five different environments.  For continuous control tasks, the Residual Vector Quantized Tokenization (RVQ) method achieved the highest success rate.  In contrast, for discrete action tasks, the Semantic Language Tokenization (SemLang) method performed best.  The figure shows the average success rate across all tasks within each environment, with a more detailed breakdown provided in Appendix E of the paper.  The five environments are CALVIN, Meta-World, Habitat Pick, Language Rearrangement, and BabyAI.", "section": "4 Experiments"}, {"figure_path": "0Gl5WxY6es/figures/figures_6_1.jpg", "caption": "Figure 4: (a,b) show the effect of the number of codes in the codebook for RVQ and VQ on final policy success rate (see (a)) and reconstruction on unseen action trajectories in Meta-World (see (b)). (c,d) show the effect of number of codebooks on final policy success rate (see (c)) and action reconstruction (see (d)). All metrics are computed on Meta-World.", "description": "This figure analyzes the impact of codebook size and the number of codebooks in Residual Vector Quantized (RVQ) and Vector Quantized (VQ) tokenization methods on the success rate and reconstruction error in Meta-World environment.  Subfigures (a) and (b) show how changing the codebook size affects success rate and reconstruction loss, respectively, for both RVQ and VQ. Subfigures (c) and (d) demonstrate the effects of varying the number of codebooks used in the RVQ method on both success rate and reconstruction loss.  The results highlight the importance of carefully selecting these hyperparameters for optimal performance.", "section": "4.2 Continuous Action Space Adapter Comparison"}, {"figure_path": "0Gl5WxY6es/figures/figures_7_1.jpg", "caption": "Figure 3: Comparing ASAs for continuous and discrete action spaces across 5 environments. For continuous actions, the RVQ tokenization performs best. For discrete actions, SemLang performs best. Each bar gives the average over all tasks in the environment with the full breakdown in Appendix E.", "description": "This figure compares the performance of different action space adapters (ASAs) on continuous and discrete action tasks across five different embodied AI environments.  For continuous control tasks, the Residual Vector Quantized (RVQ) tokenization method outperforms other methods.  For discrete action tasks, the Semantic Language (SemLang) approach is the most successful.  The chart displays the average success rate across all tasks within each environment; a more detailed breakdown can be found in Appendix E of the paper.", "section": "4 Experiments"}, {"figure_path": "0Gl5WxY6es/figures/figures_8_1.jpg", "caption": "Figure 3: Comparing ASAs for continuous and discrete action spaces across 5 environments. For continuous actions, the RVQ tokenization performs best. For discrete actions, SemLang performs best. Each bar gives the average over all tasks in the environment with the full breakdown in Appendix E.", "description": "This figure compares the performance of different Action Space Adapters (ASAs) on five different environments, categorized by continuous and discrete action spaces.  The results show that Residual Vector Quantized Tokenization (RVQ) is the best-performing ASA for continuous control tasks, while Semantic Language (SemLang) outperforms other ASAs for discrete action tasks. The success rates are averages across all tasks within each environment, with a detailed breakdown provided in Appendix E of the paper.", "section": "4 Experiments"}, {"figure_path": "0Gl5WxY6es/figures/figures_8_2.jpg", "caption": "Figure 6: (a) Adapting to 5 holdout tasks from Meta-World ML-45 with 50 demos per task using the fixed RVQ tokenization. (b) RL training curves in Language Rearrangement comparing the ASAs and utility of the token filter. Displayed are averages over 2 seeds with the shaded area as the standard deviation between seeds. SemLang learns faster than other ASAs and the token filter is crucial.", "description": "This figure contains two subfigures. Subfigure (a) shows the result of fine-tuning a model trained on 45 Meta-World tasks to 5 unseen tasks using the RVQ tokenization. It demonstrates the transferability of the learned RVQ tokens to new tasks. Subfigure (b) presents RL training curves for different action space adapters (ASAs) in the Language Rearrangement environment, highlighting the impact of a token filter on SemLang's performance. It showcases the superior sample efficiency and faster convergence of SemLang with the token filter compared to other ASAs.", "section": "4 Experiments"}, {"figure_path": "0Gl5WxY6es/figures/figures_14_1.jpg", "caption": "Figure 1: We empirically analyze how to ground MLLMs in actions across 114 tasks in continuous and discrete action spaces. In each environment, we train a multi-task policy with different Action Space Adapters (ASAs) to re-parameterize the MLLM to output actions. For continuous actions, learning a tokenization with several tokens per-action performs best (Residual VQ). For discrete actions, mapping actions to semantically related language tokens performs best (Semantic Tokenization).", "description": "This figure summarizes the empirical study on grounding Multimodal Large Language Models (MLLMs) in actions across various environments and action spaces.  It compares the performance of different action space adapters (ASAs) for both continuous and discrete action spaces. The key finding is that for continuous control, learning a tokenization scheme (Residual VQ) is superior, while for discrete actions, semantically aligning actions with MLLM's output tokens (Semantic Tokenization) yields the best results.", "section": "1 Introduction"}, {"figure_path": "0Gl5WxY6es/figures/figures_18_1.jpg", "caption": "Figure 1: We empirically analyze how to ground MLLMs in actions across 114 tasks in continuous and discrete action spaces. In each environment, we train a multi-task policy with different Action Space Adapters (ASAs) to re-parameterize the MLLM to output actions. For continuous actions, learning a tokenization with several tokens per-action performs best (Residual VQ). For discrete actions, mapping actions to semantically related language tokens performs best (Semantic Tokenization).", "description": "This figure summarizes the empirical analysis of grounding Multimodal Large Language Models (MLLMs) in actions across various tasks with continuous and discrete action spaces.  It shows different environments (CALVIN, Meta-World, BabyAI, Habitat, LangR) used for the experiments and compares the success rate of different Action Space Adapters (ASAs).  The ASAs re-parameterize the MLLM output to produce actions, and the figure highlights that for continuous actions, a learned tokenization (Residual VQ) is best; while for discrete actions, semantic alignment (Semantic Tokenization) is superior.", "section": "1 Introduction"}, {"figure_path": "0Gl5WxY6es/figures/figures_18_2.jpg", "caption": "Figure 7: Visualizations of the environments we study. The top row shows an observation in the environment. The bottom row shows the associated instruction in that episode.", "description": "This figure shows five different robotic manipulation environments used in the paper: CALVIN, Meta-World, Habitat Pick, BabyAI, and LangR.  For each environment, the top row displays a visual observation from the robot's perspective during task execution. The bottom row provides the corresponding natural language instruction given to the robot to guide its actions in that specific episode.", "section": "4 Experiments"}, {"figure_path": "0Gl5WxY6es/figures/figures_18_3.jpg", "caption": "Figure 7: Visualizations of the environments we study. The top row shows an observation in the environment. The bottom row shows the associated instruction in that episode.", "description": "This figure shows five different robotic manipulation environments used in the paper's experiments. Each environment's image is accompanied by a sample instruction task given to the robot.  The figure aims to visually illustrate the variety of tasks and visual inputs the model processes.", "section": "4 Experiments"}, {"figure_path": "0Gl5WxY6es/figures/figures_18_4.jpg", "caption": "Figure 1: We empirically analyze how to ground MLLMs in actions across 114 tasks in continuous and discrete action spaces. In each environment, we train a multi-task policy with different Action Space Adapters (ASAs) to re-parameterize the MLLM to output actions. For continuous actions, learning a tokenization with several tokens per-action performs best (Residual VQ). For discrete actions, mapping actions to semantically related language tokens performs best (Semantic Tokenization).", "description": "This figure summarizes the main results of the paper by showing the success rate of different action space adapters (ASAs) for grounding multimodal large language models (MLLMs) in actions. The figure compares the performance of various ASAs on continuous and discrete action spaces across multiple environments and tasks.  The key finding is that for continuous actions, learning a tokenization (Residual VQ) is best, and for discrete actions, aligning actions semantically with the MLLM's output (Semantic Tokenization) is best.", "section": "1 Introduction"}, {"figure_path": "0Gl5WxY6es/figures/figures_18_5.jpg", "caption": "Figure 1: We empirically analyze how to ground MLLMs in actions across 114 tasks in continuous and discrete action spaces. In each environment, we train a multi-task policy with different Action Space Adapters (ASAs) to re-parameterize the MLLM to output actions. For continuous actions, learning a tokenization with several tokens per-action performs best (Residual VQ). For discrete actions, mapping actions to semantically related language tokens performs best (Semantic Tokenization).", "description": "This figure summarizes the empirical analysis of grounding Multimodal Large Language Models (MLLMs) in actions across various tasks.  It compares the performance of different Action Space Adapters (ASAs) for both continuous and discrete action spaces. The results show that for continuous actions, a learned tokenization method (Residual VQ) outperforms other methods, while for discrete actions, aligning actions with the MLLM's native token space semantically achieves the best performance. The figure illustrates this comparison across multiple environments and tasks.", "section": "1 Introduction"}]