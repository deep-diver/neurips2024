[{"type": "text", "text": "Grounding Multimodal Large Language Models in Actions ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Andrew Szot1,2 Bogdan Mazoure1 Harsh Agrawal1 Devon Hjelm1,3 Zsolt Kira2 Alexander Toshev1 1 Apple, 2 Georgia Tech, 3 Mila a.szot@apple.com, toshev@apple.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multimodal Large Language Models (MLLMs) have demonstrated a wide range of capabilities across many domains, including Embodied AI. In this work, we study how to best ground a MLLM into different embodiments and their associated action spaces, with the goal of leveraging the multimodal world knowledge of the MLLM. We first generalize a number of methods through a unified architecture and the lens of action space adaptors. For continuous actions, we show that a learned tokenization allows for sufficient modeling precision, yielding the best performance on downstream tasks. For discrete actions, we demonstrate that semantically aligning these actions with the native output token space of the MLLM leads to the strongest performance. We arrive at these lessons via a thorough study of seven action space adapters on five different environments, encompassing over 114 embodied tasks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multimodal Large Language Models (MLLMs), defined as Large Foundation Models that take as input text and images and generate text, have recently seen rapid progress and impressive performance $[1-$ 13]. These models are important as they solve a large range of useful yet difficult natural language and image tasks, such as describing images, answering visual and textual questions, reasoning, and learning from a small number of examples. They have only recently improved to the point of being usable enough for general deployment with human non-experts [14\u201316]. ", "page_idx": 0}, {"type": "text", "text": "While MLLMs are capable of describing real-world embodied concepts, their capabilities in embodied tasks are limited to using text for actions through generating code [17, 18], representing actions as text [19], or extracting actions from internal representations [20, 21]. Grounding [22] MLLMs to generate actions extends their capabilities to embodied tasks, such as robot manipulation and navigation, and is of tremendous value for practical problems, potentially overcoming the high cost of training tabula rasa. Extending MLLMs to multimodal image generation enables object detection and segmentation, and image and video generation [3, 23\u201327]. In embodied settings, grounding MLLMs via predicting agent affordances and generating actions yields effective policies capable of generalizing to new tasks [19, 21, 28, 29]. ", "page_idx": 0}, {"type": "text", "text": "A key and open challenge in grounding MLLMs, which limits their capabilities in embodied tasks, is the gap between the native output space, natural language, and the action space of embodied agents. This problem is particularly acute in continuous action spaces, where low-level controllers may require a high degree of precision. Across the literature, a number of architectures and ways of handling action spaces have been proposed, but there has not been a systematic study of these designs. Our contributions generalize prior attempts to adapt MLLMs to generate actions through an empirical study on which principles and strategies are necessary to effectively close the gap between the action spaces of MLLMs and embodied agents. We study various grounding re-parameterization strategies, which we refer to as Action Space Adapter (ASA), across a range of embodiments, action spaces, and environments. In particular, we explore the following types of ASAs: (1) ASAs that directly generate actions from a new prediction policy using the MLLM hidden representations as input; (2) ASAs that reuse the native token space of the MLLM to encode actions; (3) and ASAs that introduce a new token space to encode the actions of the agent while adapting the MLLMs to predict these new tokens. ", "page_idx": 0}, {"type": "image", "img_path": "0Gl5WxY6es/tmp/d87ed973a8d63ed4058f9aadac6ea698a693ccceed83c4e6c20e9bbcb2e329b6.jpg", "img_caption": ["Figure 1: We empirically analyze how to ground MLLMs in actions across 114 tasks in continuous and discrete action spaces. In each environment, we train a multi-task policy with different Action Space Adapters (ASAs) to re-parameterize the MLLM to output actions. For continuous actions, learning a tokenization with several tokens per-action performs best (Residual VQ). For discrete actions, mapping actions to semantically related language tokens performs best (Semantic Tokenization). "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Further, we empirically identify important principles for designing ASAs. For continuous action spaces, learned tokenization with several vocabularies that residually model continuous actions gives the right modeling precision while using vocabularies of manageable sizes and, as a result, yields the best performance across all continuous control environments. This learned tokenization outperforms direct action prediction, indicating this approach allows the model to effectively learn a multimodal distribution over action spaces. In addition, the above tokenization strategy boosts performance when the policy is a MLLM, compared to other standard non-LLM-based policies, indicating that it manages to better tap into the knowledge of the model. ", "page_idx": 1}, {"type": "text", "text": "For discrete action spaces, we study ASAs that better align the embodied actions with the output space of the MLLM. We demonstrate that a semantic alignment between these \u2013 mapping discrete actions to semantically related tokens in the MLLM vocabulary \u2013 yields the best strategy compared to other adapters that either reuse or define a new vocabulary. The superiority of this strategy is evident in performance on environments with discrete action spaces and also in RL sample efficiency. ", "page_idx": 1}, {"type": "text", "text": "Finally, the above principles are thoroughly validated across five embodied AI environments, three of which are robotic continuous control and two with discrete actions as illustrated in Figure 1. Altogether, we consider 114 language specified tasks. In the continuous case, the best tokenization achieves $72\\%$ on CALVIN [30], up from $68\\%$ for direct action regression and $28\\%$ for uniform action tokenization; and $84\\%$ on Meta-World [31], up from $61\\%$ for direct action regression and $75\\%$ for uniform tokenization. Similarly, in the case of discrete actions, the proposed semantically aligned action tokens yield $51\\%$ on LangR [21], up from $42\\%$ for direct action prediction. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Prior works propose different Action Space Adapters (ASAs) to adapt MLLMs into policies. Some works use LLMs or MLLMs as zero-shot policies by prompting them to output text or code that can be executed as actions [18, 32\u201338]. The ASA in this case is a given executor or low-level controller that takes text as input and outputs actions in the environment. Other works investigate adapting MLLMs for actions, but focus on a single ASA and environment. For example, RT-2 [19] uniformly discretizes continuous actions and predicts tokens corresponding to each of the action dimensions. RoboFlamingo [20], Lamo [39], and LLaRP [21] use an MLP to predict an environment action from an LLM hidden state. GFlan [40] treats discrete actions as text and ranks actions by the LLM log probability to form a distribution over actions. At a high level, our work is distinct in that we study a variety of methods across multiple environments for learning ASAs. We focus on tasks with low zero-shot VLM performance, such as low-level control or long-horizon planning tasks. We summarize the differences between our investigation and prior work adapting VLMs for action in Appendix A. ", "page_idx": 1}, {"type": "image", "img_path": "0Gl5WxY6es/tmp/bfe675ae0d438aebe0344f7f1469f3638b9c39be6d365e5a64c676b8d346050a.jpg", "img_caption": ["Figure 2: Generic architecture studied here for adapting MLLMs for action-specific decision making. The MLLM takes the embedding of the task instruction, prompt, and visual tokens as input. The MLLM then autoregressively predicts a sequence of $m$ action tokens. These action tokens are then decoded into an environment-specific action. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Investigating action representations in embodied settings is not new. Some works learn representations of actions to help generalization to new actions or operating in large action spaces [41, 42] in the context of Reinforcement Learning (RL). Our study proposes ASAs for tokenizing continuous actions, and other works use different types of discretization or tokenization strategies on continuous action spaces. [43, 44] use $\\boldsymbol{\\mathrm{k}}$ -means to discretize continuous actions to help learn from multimodal behavior datasets, such as from play data or data from different experts. VQ-BeT [45] finds learning a residual VQA (RVQ) codebook for continuous actions works best but does not apply this idea to MLLMs. [46] predicts actions as text. [47] learns a multi-task transformer policy and models actions with a diffusion head. ", "page_idx": 2}, {"type": "text", "text": "More broadly, prior works have adapted MLLMs for modalities other than actions, such as object bounding boxes and image generation, both being continuous in nature while the latter of high dimension. For example, [27, 48] train MLLMs to output spatial reference tokens to ground text responses in image regions. For image generation, [49] adapt MLLMs to generate image patches; [50, 51] tokenize images using a VQ-VAE model and adapt MLLMs to generate images by decoding these image tokens, which has inspired us to use the same learned tokenization; [52] uses an RVQ model [53] to generate images, similarly to our best performing tokenization scheme. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In order to solve an embodied task, an agent learning in an interactive environment must select a decision from a set of valid actions. For example, an action space could be a set of keyboard presses for a video game or a real-valued vector that controls a robotic manipulator. Our work studies how to best adapt a MLLM, which is originally trained to output text tokens, to instead model actions from a given environment. We refer to the module that bridges a MLLM with a certain action space as an Action Space Adapter (ASA) (see Figure 2). ", "page_idx": 2}, {"type": "text", "text": "3.1 Problem Setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our analysis focuses on language-specified tasks with visual observations. Specifically, we consider a goal-specified Partially-Observable Markov Decision Process (POMDP) [54] that has an observation space $\\scriptscriptstyle\\mathcal{O}$ , action space $\\boldsymbol{\\mathcal{A}}$ , and goal space $\\mathcal{G}$ . For brevity, we omit other elements of the MDP. In our setting, $\\mathcal{G}$ is a textual description of the task to solve. $\\scriptscriptstyle\\mathcal{O}$ consists of RGB visual perception and agent proprioception. We consider a range of different action spaces $\\boldsymbol{\\mathcal{A}}$ that broadly fall into two categories \u2013 discrete and continuous. The primary objective is to learn a language-conditioned policy that maps observations and the instruction text to an action $\\pi(a|o,g)$ . As later described in Section 3.3, we learn this policy through supervised fine tuning from expert demonstrations or reinforcement learning that maximizes the expected discounted cumulative reward of the POMDP. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3.2 From Vision and Language to Action ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The process studied here for adapting MLLMs for decision making is illustrated in Figure 2. The MLLM policy takes as input a textual instruction describing the downstream task, a sequence of past observations in the task and outputs an action in the agent\u2019s action space. In the bottom left of Fig. 2, the task description, as well as the environment description, are first encoded to produce language embeddings. To these embeddings, the MLLM then appends a sequence of visual embeddings from the current observation $o_{t}$ . Since visual embeddings can often be comprised of a large number of tokens (the popular LLaVA-1.5 model [6] has 556), we introduce a downsampling layer to enable the MLLM to attend over a longer history of observations. In practice, we take the downsampling layer to be a Perceiver model [55], a learnable transformation that reduces the number of tokens from the visual encoder before being used as input to the MLLM. ", "page_idx": 3}, {"type": "text", "text": "The sequence of language and visual embeddings is passed through the MLLM, whose final hidden state $h_{t}^{\\bar{1}}$ encodes the entire input. The ASA, whose trainable parameters are denoted $\\theta$ , is comprised of three parts: (1) an adapter head, (2) an adapter embedding, and (3) an adapter decoder. The hidden state is first passed through the adapter head to produce action tokens $u_{t}^{\\mathrm{I}^{\\star}}=A_{\\theta}(h_{t}^{1})$ . The action tokens are then embedded using the action embedding into $E_{\\theta}(u_{t}^{1})$ , and passed autoregressively ,  rMeLsuLltMin tgo i pn rtoodtaulc utortkheenr s hpiderd etinm ee mstbeepd.d iTnhges $h_{t}^{2},\\ldots,u_{t}^{m}$ taionnd  taosskoecnisa taerde  tahcetino nd etcookdeends $u_{t}^{2},\\ldots,u_{t}^{m}$ $m$ into the final action $a_{t}$ by the adapter decoder, which produces the final action $a_{t}=D_{\\theta}(u_{t}^{1},..,u_{t}^{m})$ As $a_{t}\\in\\mathcal A$ , it is then executed in the environment to produce $o_{t+1}$ , and the process continues. ", "page_idx": 3}, {"type": "text", "text": "Next, we describe possible ASA implementations for discrete and continuous action spaces. ", "page_idx": 3}, {"type": "text", "text": "3.2.1 Discrete Action Spaces ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We define the following action spaces adapters for a discrete action space $\\boldsymbol{\\mathcal{A}}$ : ", "page_idx": 3}, {"type": "text", "text": "Categorical Prediction (Pred): Implement the action space adapter as an MLP network, which predicts the logits of a categorical distribution over environment actions from the MLLM hidden state. The adapter head is an MLP that maps the hidden state $h^{1}$ directly to an action $a\\in A$ . This amounts to producing a single action token $\\textstyle{\\Bigr\\dot{u}}^{1}$ , which directly corresponds to the action $a$ , with the action decoder being an identity map. Both the adapter head and token embeddings are initialized from scratch. This type of ASA is used by [21]. ", "page_idx": 3}, {"type": "text", "text": "Semantic Language (SemLang): The action space adapter predicts natural language text that maps to a discrete action. First, each action $a\\in A$ is described with freeform text tokenized as $(l_{1},\\ldots,l_{m})$ . The MLLM then autoregressively predicts a sequence of $m$ tokens, which are then decoded by the adapter decoder to the corresponding action. For example, in an action space choosing a high-level skill $a$ could be described as \u201cpick apple\", which is tokenized as [5839, 26163] with the LLaMA tokenizer. The MLLM then must sequentially predict token 5839, then token 26163 to call this action. Sequences of tokens corresponding to invalid actions are either avoided entirely with the token fliter described in Section 3.3 or treated as a no-op. Both the adapter head and the token embeddings are re-used to be the pretrained LLM\u2019s language head and embedding layer, respectively, meaning no additional parameters over the pretrained MLLM are added. This type of ASA is used by [29]. ", "page_idx": 3}, {"type": "text", "text": "Non-Semantic Language (Lang): Actions are mapped to language tokens, but instead of semantically meaningful descriptions of the actions as with SemLang, the actions are mapped to sequences of numbers. For example, \u201cpick apple\" is represented with the string $\\mathit{\\Omega}^{\\leftarrow}5\\;3\"$ . The policy must then output the tokens corresponding to this text to call this pick action. Note that we can pick any text for this mapping and the choice of integers is arbitrary. However, the selected text is not semantically representative of the action. ", "page_idx": 3}, {"type": "text", "text": "3.2.2 Continuous Action Space Adaptors ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We define the following four ASAs for a continuous $D$ -dimensional action space $\\boldsymbol{\\mathcal{A}}$ : the first ASA predicts in the original action space while the other three use tokenization. At training time, we learn ", "page_idx": 3}, {"type": "text", "text": "a policy to predict these action tokens from the ASA. At test time, we employ an action decoder that maps these action tokens to actions in the original space $\\boldsymbol{\\mathcal{A}}$ . ", "page_idx": 4}, {"type": "text", "text": "Continuous Regression (Pred): Regress to the original continuous action from the MLLM hidden state $h_{t}^{1}$ . This is achieved via a single-layer MLP network, which is trained using MSE loss. This ASA is used by [20, 39]. ", "page_idx": 4}, {"type": "text", "text": "Uniform Action Tokenization (Uniform): The simplest approach is to use uniform binning of the action space. In particular, we express each action as a sequence of $D$ tokens by quantizing each of the $D$ action dimensions into one out of $K$ uniform bins: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{Uniform(a)}=\\left(k_{1}\\dots k_{D}\\right)\\quad\\mathrm{such\\;that}\\quad a_{d}\\in\\mathfrak{b i n}(k_{d},d)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\sin(k,d)$ denotes the $k^{\\mathrm{th}}$ bin along the $d^{\\mathrm{th}}$ action dimension. If $m_{d}$ and $M_{d}$ denote the lower and upper bounds respectively of the $d^{\\mathrm{th}}$ action dimension, then its definition reads $\\mathsf{b i n}(k,d)\\,=$ $\\begin{array}{r}{[m_{d}+k\\frac{M_{d}-m_{d}}{K},m_{d}+(k+1)\\frac{M_{d}-m_{d}}{K_{\\rightarrow}}]}\\end{array}$ . At test time, we decode predicted action tokens to the center of the corresponding bins for each dimension. This type of ASA is used by [19]. ", "page_idx": 4}, {"type": "text", "text": "Vector Quantized Tokenization (VQ): To adapt the tokenization to the particular action space, we propose to use learned tokenization. In particular, we express each action as a single token that corresponds to the closest action code from a learned codebook $V$ . Using encoder network $f_{\\theta}$ that maps actions to a latent embedding space: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{\\bfVQ}(a)=(k_{1})\\quad\\mathrm{where}\\quad k_{1}=\\arg\\operatorname*{min}_{k}||f_{\\theta}(a)-v_{k}||_{2}^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $v_{k}\\in V$ . The codebook $V$ of size $K$ is learned over an offline dataset $\\mathcal{D}$ of actions using a VQ-VAE [56] trained with the mean-squared error for action reconstruction and commitment loss. We overwrite $K$ infrequently used tokens from the LLM vocabulary to represent $V$ . We defer the full details of this tokenization process to Appendix C.2. ", "page_idx": 4}, {"type": "text", "text": "Residual Vector Quantized Tokenization (RVQ): Precise control requires precise action modeling that can suffer after tokenization. To increase the precision of a learned tokenization, we further investigate the use of a sequence of several action tokens as in Uniform. Similar to VQ, these tokens are from $M$ action codebooks $V_{m},m\\in\\{1,\\ldots,M\\}$ . However, each codebook models the residual space obtained after modeling the action using preceding codebooks, thus each subsequent token captures increasingly finer action information: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{RVQ}(a)=(k_{1},\\dots k_{M})\\quad\\mathrm{where}\\quad k_{m}=\\arg\\operatorname*{min}_{k}\\left\\|\\left(f_{\\theta}(a)-\\sum_{i=1}^{m-1}v_{k_{i}}^{i}\\right)-v_{k}^{m}\\right\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $v_{k}^{i}\\in V_{i}$ is the $k^{\\mathrm{th}}$ code from the $i^{\\mathrm{th}}$ codebook. Such tokenization can be learned using Residual VQ-VAE [RVQ-VAE, 52] on an offline dataset of actions. The actual number of token sequences we can represent is $K^{M}$ . Hence, RVQ presents the opportunity to exponentially increase the action space quantization without having to drastically increase the size of the learned individual codebooks. ", "page_idx": 4}, {"type": "text", "text": "3.3 Training ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We use LLaVA-1.5-7B [6] as the base MLLM. We finetune the MLLM with interactive (i.e., actionlabeled) data to make it more suited for interacting with a embodied and interactive environment. ", "page_idx": 4}, {"type": "text", "text": "Supervised Fine Tuning (SFT) with Expert Demonstrations: We finetune the MLLM for interactive tasks using a dataset of expert demonstrations. Each demonstration contains (1) a language description of the task, (2) a sequence of observations, and (3) a sequence of actions that successfully solve the task. Note that in this work, we are primarily interested in learning imitation policies from offline data, which can be extended to offline reinforcement learning if per-timestep rewards are included in the dataset. Specifically, we train the MLLM with supervised learning to predict the expert actions from the observations and language description in the data. While the pre-trained LLM and the visual encoder remain frozen, we finetune the ASA, the visual downsampler, and parts of the LLM with LoRA [57]. In total, the model has $\\approx100M$ learnable LLM parameters and $\\approx40M$ learnable downsampler and ASA parameters. The learned tokenization schemes (RVQ and VQ) have an additional pre-training phase, where the VAE models are first trained on actions from the offline dataset and then frozen to prevent further updates in later stages. ", "page_idx": 4}, {"type": "image", "img_path": "0Gl5WxY6es/tmp/a8752807fe71079c3cc742b702e73515757d15cc11f41d67b216a403c59ecbf8.jpg", "img_caption": ["Figure 3: Comparing ASAs for continuous and discrete action spaces across 5 environments. For continuous actions, the RVQ tokenization performs best. For discrete actions, SemLang performs best. Each bar gives the average over all tasks in the environment with the full breakdown in Appendix E. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Reinforcement Learning (RL) from Environment Feedback We can also optionally finetune the MLLM to optimize an environment reward using RL. However, predicting actions in the MLLM token space dramatically increases the number of possible action predictions, with many possible predictions corresponding to no valid action. For example, there are 32,000 tokens in the LLaMA text tokenizer, giving 32, $000^{m}$ possible predictions by the model with $m$ tokens per action. This makes exploration difficult in RL as only a small fraction of the possible actions are valid. We therefore use a token filter to restrict the autoregressive sampling to only be from token sequences corresponding to valid actions. The token filter is a function $M(l_{t}^{1},\\ldots,l_{t}^{j-1})$ that produces a binary mask over all tokens to represent valid tokens for the $j$ th decoding step. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Settings ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We study adapting MLLMs for action across a variety of environments with different embodiments and action spaces. All environments provide RGB visual observations and a natural language instruction specifying the goal to achieve. We provide the important environment details below and defer complete details to Appendix B. ", "page_idx": 5}, {"type": "text", "text": "CALVIN [30]: This manipulation benchmark tests the ability of a tabletop robot to interact with an object to complete a natural language instruction. The continuous actions specify 6DoF end-effector control and the binary gripper state. The observation is a $200\\times200$ RGB image from a fixed-position camera. We use the $A B C\\rightarrow D$ split of the benchmark with 34 tasks, and the agent is evaluated on unseen instruction phrasings and table background. ", "page_idx": 5}, {"type": "text", "text": "Meta-World [58]: We use the ML-45 version of this tabletop manipulation benchmark which has 45 tasks. The action space is continuous control specifying 3DoF end-effector translation and the continuous gripper state. The observations are $200\\times200$ RGB images from a fixed camera. The agent is evaluated on unseen object and robot starting states. ", "page_idx": 5}, {"type": "text", "text": "Habitat Pick (HabPick) [59]: A mobile manipulation robot must pick up an object specified by name from a receptacle. The continuous actions specify the 7DoF relative joint positions of the arm, the 2D base velocity, and the gripper state. The observations are $336\\times336$ RGB images from the robot\u2019s egocentric head camera. The instruction specifies the name of the object type to pick up. The evaluation distribution is on unseen houses and new arrangements of objects. ", "page_idx": 5}, {"type": "text", "text": "BabyAI [60]: BabyAI is a grid world task where an agent navigates and interacts with objects to complete an instruction. The discrete action space consists of navigation and interaction actions. The observation is a $200\\times200$ RGB top-down view. We use the five tasks from [40], and we report generalization to instructions rephrased with synonyms. ", "page_idx": 5}, {"type": "text", "text": "Language Rearrangement (LangR) [21]: A mobile manipulation robot must rearrange objects to complete instructions like \u201cstore all the fruit in the fridge\u201d. The discrete actions are 70 high-level skills to interact with objects and navigate. The observation is a $336\\times336$ RGB head camera. ", "page_idx": 5}, {"type": "image", "img_path": "0Gl5WxY6es/tmp/e7136a8c8843782e5be327d610a13ee2adbcf233d27834ecbe4dd5ba87e5a77d.jpg", "img_caption": ["Figure 4: (a,b) show the effect of the number of codes in the codebook for RVQ and VQ on final policy success rate (see (a)) and reconstruction on unseen action trajectories in Meta-World (see (b)). (c,d) show the effect of number of codebooks on final policy success rate (see (c)) and action reconstruction (see (d)). All metrics are computed on Meta-World. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Evaluation instructions test generalization to unseen houses and 10 unseen instruction datasets measuring paraphrastic robustness and behavior generalization. ", "page_idx": 6}, {"type": "text", "text": "In all environments, we report the success rate as the fraction of episodes in which the agent completed the language instruction. We use the success criteria provided by each environment. We train a policy per action adapter for each environment and report the generalization performance in the main text. When reporting a single success rate per environment, it is the success averaged between all evaluation episodes containing all tasks. We give the full per-task breakdown for results in Appendix E. CALVIN, Meta-World, HabPick, and BabyAI provide expert demonstrations succeeding at the task. CALVIN has $17.9k$ from humans, Meta-World $22.5k$ from a scripted policy, HabPick $6.7k$ generated from an RL policy, and BabyAI $5k$ from a scripted policy. Full details on the train and evaluation setups per environment are in Appendix B. ", "page_idx": 6}, {"type": "text", "text": "We train with supervised finetuning for CALVIN, Meta-World, HabPick, and BabyAI. We train with reinforcement learning on Language Rearrangement. As described in Section 3.3 we train $\\approx140M$ parameters with LoRA [57]. We use the AdamW optimizer [61] with a learning rate of $\\mathrm{3e^{-4}}$ , a warmup period of $10\\%$ of the total number of training steps, and cosine learning rate decay to 0 by the end of training. For RL, we use PPO [62]. For the learned tokenization action space adapters, we, by default, use a codebook size of 512 with 512 dimensions per codebook element. Complete hyperparameter and policy details are in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "4.2 Continuous Action Space Adapter Comparison ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first study adapting MLLMs through Uniform, Pred, VQ, and RVQ action space adapters for the continuous action environments CALVIN, Meta-World and HabPick. ", "page_idx": 6}, {"type": "text", "text": "RVQ is the best performing continuous action ASA. The results in Figure 3 show that the RVQ action adapter consistently outperforms all other ASA approaches across all environments. While Pred is the second best performing method on all tasks, except on Meta-World, RVQ outperforms it by a $12\\%$ average absolute difference. One hypothesized reason for this is that Pred only learns unimodal distributions of actions, which hurts performance when learning from diverse demonstrations [43\u201345]. Another potential reason is the tokenization from RVQ allows the MLLM to better leverage its existing knowledge, whereas the Pred ASA requires training a new MLP network from scratch. ", "page_idx": 6}, {"type": "text", "text": "Uniform performs poorly on the majority of the tasks, where RVQ outperforms on average by a $27\\%$ absolute increase. A reason for this is that the Uniform discretization can fail to accurately represent the continuous actions. The performance of Uniform is also closely related to the action dimension. In Meta-World with 4 action dimensions, Uniform performs well. However, Uniform suffers with the 7 action dimensions in CALVIN and the 10 action dimensions in HabPick. ", "page_idx": 6}, {"type": "text", "text": "RVQ also outperforms VQ by a $18\\%$ absolute difference averaged over all environments. This is due to VQ having worse action reconstructions than RVQ. In Meta-World, both RVQ and VQ policies reach a similar cross-entropy loss on holdout trajectories during finetuning. However, on this same data, RVQ has a reconstruction mean squared error (MSE) of 0.005 while VQ has a $10\\mathrm{x}$ higher reconstruction MSE of 0.05. Increasing the VQ codebook size does not close this gap. We vary the VQ codebook size in powers of 2 from $2^{7}$ to $2^{11}$ . Figure $^\\mathrm{4b}$ shows the VQ reconstruction loss decreases with larger codebooks but does not even close the gap to the $2^{7}$ RVQ codebook size. This poor reconstruction manifests in poor downstream policy performance as demonstrated by Figure 4a where policies trained with the VQ ASA plateau in success rate at codebook size $2^{9}$ . VQ policies even decrease in performance at codebook size $2^{11}$ , potentially due to overfitting to the large codebook. ", "page_idx": 6}, {"type": "image", "img_path": "0Gl5WxY6es/tmp/a0b38a3c3782b61fdd7160648140924b3dd0e7f8ba80e31d30d3d1b2cb8a9864.jpg", "img_caption": ["Figure 5: RVQ and VQ success per-task grouping (defined in Supp. B.6) on CALVIN and MetaWorld. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "We further characterize the performance of RVQ and VQ in Figure 5 by breaking down the performance per task group in Meta-World and CALVIN. The task groups, which are fully listed in Appendix B.6, correspond to tasks with related required behaviors. Both RVQ and VQ do similarly on \u201carticulated\" object interactions (like opening drawers or doors). These tasks require less precise control since many contact points on the articulated link and broad pushing or pulling behavior can achieve the desired behavior. On the other hand, RVQ outperforms VQ on \u201cpressing\" tasks that require pushing a button. These tasks require more precise control since the agent needs to push the button all the way to a desired state. VQ often reaches the button but fails to press it all the way. The same is also true of other precise control tasks like picking, pulling, and rotating. ", "page_idx": 7}, {"type": "text", "text": "A potential explanation of RVQ\u2019s success can be attributed to adaptive localization of the model\u2019s errors, similar to prior work in residual reinforcement learning [63] and Bellman error bases [64]. ", "page_idx": 7}, {"type": "text", "text": "A sufficient codebook size and number of codebooks are necessary for RVQ. In Figure 4a, we show that RVQ policy performance improves in performance with a larger codebook size in Meta-World. Notably, RVQ performs poorly at $29\\%$ success rate with codebook size 16 compared to $84\\%$ success at codebook size 512. These observations also align with the codebook size decreasing reconstruction error in Figure 4b. In Figure $4\\mathrm{c}$ , we compare the effect of the number of codebooks on performance. As earlier discussed with the performance of VQ, one codebook results in poor action reconstruction and, thus, bad policy performance. However, increasing the number of codebooks too much to 6 also hurts performance despite decreasing reconstruction loss. Likewise to the finding that Uniform performs poorly with larger action dimension since there are more tokens per action, increasing the number of codebooks also hurts policy learning. ", "page_idx": 7}, {"type": "text", "text": "RVQ tokens transfer to new tasks. We take the model trained on the 45 Meta-World tasks and finetune it on 5 unseen tasks. We collect 50 demonstrations for per task and finetune the policy on all task data. We use the same RVQ ASA trained only on data from the 45 tasks. Figure 6a shows the success rate of adapting RVQ compared to an Pred ASA. RVQ outperforms Pred across all tasks, achieving a $50\\%$ vs. $20\\bar{\\%}$ overall success rate. This demonstrates the RVQ tokens are flexible enough to be applied to new tasks. ", "page_idx": 7}, {"type": "text", "text": "The gains from RVQ are unique to MLLMs. Next, we analyze the unique interaction between the RVQ tokens and the MLLM policy. While we demonstrated that the RVQ ASA performs best, is this improvement due to the MLLM being able to leverage these new tokens or the added action representation ability from the separately trained RVQ decoder? To test this, we compare to two policy architectures that do not use LLMs: ", "page_idx": 7}, {"type": "text", "text": "\u2022 Scratch: This is the same architecture as the MLLM-based policy, but with a smaller 300M parameter non-pretrained transformer. ", "page_idx": 7}, {"type": "text", "text": "\u2022 RT-Inspired: This method uses a ResNet visual encoder, pretrained Flan [65] language embedding and decoder transformer-based policy. The entire policy is trained from scratch. This method is inspired by RT-1 [66], which does not have publicly released code. ", "page_idx": 7}, {"type": "text", "text": "Table 1 compares the effect of Pred versus RVQ ASAs on CALVIN, Meta-World and HabPick for these three policy policy architectures. As already established for the MLLM, RVQ is consistently better than VQ. However, for the same policy architecture trained from scratch, RVQ can hurt the performance over Pred. In CALVIN the success drops $-7\\%$ and in Meta-World the performance drops $-15\\%$ . This highlights that MLLM can leverage its existing knowledge about sequencing language tokens to sequencing action tokens. However, we find that for the smaller RT-Inspired policy network, the RVQ ASA consistently helps, which we hypothesize is because the added RVQ network and separate training help compensate for the lack of policy network capacity. We also note that RVQ may more consistently outperform Pred on demonstrations that explicitly contain multimodal action sequences [43\u201345]. ", "page_idx": 7}, {"type": "table", "img_path": "0Gl5WxY6es/tmp/1188facdf6076ef39a286bf14a9e140c9dba9d4ac40b6d1229ed426eb317d618.jpg", "table_caption": ["Table 1: Comparing the effect of the RVQ action space adapter on the success rate of non-LLM based policies. Red indicates RVQ hurts over Pred and green indicates RVQ helps over Pred. RVQ typically has a negative impact on the Scratch policy, and helps the smaller RT-Inspired policy. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.3 Discrete Action Adapter Comparison ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "SemLang performs the best. In Figure 3, SemLang outperforms the next best ASA (Pred), by $9\\%$ on Language Rearrangement and $8\\bar{\\%}$ on BabyAI. SemLang performs especially well on tasks with explicit high-level language actions in Language Rearrangement (e.g., \u201cpick apple\u201d) where prior work has shown text-only LLM policies achieve non-zero success [21]. SemLang also does well on the BabyAI tasks with discrete low-level actions like \u201cmove left\". Additionally, Lang performs the worst in both environments, achieving $14\\%$ lower success on Language Rearrangement and $11\\%$ lower on BabyAI than SemLang. We hypothesize this is because the MLLM has to repurpose its knowledge to leverage these newly assigned action tokens, whereas a newly initialized Pred allows extracting this knowledge from the MLLM hidden state. ", "page_idx": 8}, {"type": "text", "text": "SemLang enables sample efficient RL. In Figure 6b, we compare the RL training curves for the ASAs in Language Rearrangement. In addition to helping with better generalization, SemLang also enables sample efficient RL training. SemLang converges in training performance after just $20M$ training samples, whereas Pred requires up to $70M$ steps to fully converge. ", "page_idx": 8}, {"type": "text", "text": "Token filter is crucial for language-based action spaces. In Figure 6b, we show the training of SemLang without the token fliter, which restricts policy outputs to only valid action token sequences. Without the token filter, SemLang is unable to learn in the large text action space. ", "page_idx": 8}, {"type": "image", "img_path": "0Gl5WxY6es/tmp/05f8269cecc063c756f5ed36d17c783037fad60d614601c9717b6b25cc12752c.jpg", "img_caption": ["(a) Finetuning on Holdout Tasks "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "0Gl5WxY6es/tmp/3f1b449c8d3c4db788d843d6eaf0a839c0fabdf825418e72af9d903c79ade10f.jpg", "img_caption": ["(b) Language Rearrangement RL. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 6: (a) Adapting to 5 holdout tasks from Meta-World ML-45 with 50 demos per task using the fixed RVQ tokenization. (b) RL training curves in Language Rearrangement comparing the ASAs and utility of the token fliter. Displayed are averages over 2 seeds with the shaded area as the standard deviation between seeds. SemLang learns faster than other ASAs and the token filter is crucial. ", "page_idx": 8}, {"type": "text", "text": "4.4 Empirical Comparison to Prior Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The contributions of this work are an empirical analysis of ASAs under controlled settings on various embodied environments. Direct comparisons to prior work are challenging due to different training algorithms, policy architectures, or assumptions about input modalities. Regardless, in this section, we seek to contextualize our RVQ and SemLang MLLM results against prior work. In Meta-World, to the best of our knowledge, RVQ at $84\\%$ success on ML-45 sets a new state-of-the-art result, compared to $79\\%$ from DualMind [67]. In CALVIN, RVQ at $72\\%$ success underperforms a similar work RoboFlamingo which achieves $82\\%$ success on the $A B C\\to D$ split. However, RoboFlamingo uses a different MLLM and uses an additional gripper camera input. In Language Rearrangement, SemLang sets a state-of-the-art result with $51\\%$ success compared to $42\\%$ from LLaRP [21]. In BabyAI, SemLang at $40\\%$ success rate underperforms GFlan [40], which achieves $55\\%$ success. However, we use RGB visual observations, while GFlan operates from a compact, ground truth language state description. In Appendix A.1, we compare these differences in more detail. ", "page_idx": 9}, {"type": "text", "text": "5 Limitations and Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we studied various action space adapters (ASAs) across a variety of embodiments, action spaces, and environments. We provide a generalization of prior works through the lens of action space adaptors, and for both discrete and continuous action spaces demonstrate designs that we show can leverage the knowledge within the MLLM. Our findings conclude that for continuous actions, it is best to learn action tokens that accurately model the action distribution, while for discrete actions, it is best to reason over semantic language descriptions of actions. We verify these ideas across 114 embodied AI tasks in 5 diverse environments. ", "page_idx": 9}, {"type": "text", "text": "A limitation of our work is all our analysis is under a single MLLM (LLaVA). Another limitation is that RVQ, the best performing ASA in continuous action spaces, requires collecting demonstrations to train the VQ model. Our analyses are also under only a single LoRA training setting. Future analyses can explore different base MLLMs under different training regimes like full LLM finetuning. While our investigation of ASAs enables connecting a MLLM to various action spaces, the performance of these methods is still subpar for real-robot deployment where high success and safety are critical. MLLMs with the best ASA still struggle on simple environments like BabyAI, only achieving $40\\%$ success rate. Further work is needed to improve the performance of these methods for real-world usage. Our investigation also only studies adapting MLLMs through behavioral cloning or on-policy RL. Future work can investigate if the choice of ASA varies when adapting the MLLM with other learning algorithms such as off-policy RL or offline RL. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023.   \n[2] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei. Language is not all you need: Aligning perception with language models, 2023.   \n[3] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023.   \n[4] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, 2023.   \n[5] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.   \n[6] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.   \n[7] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng Gao. Multimodal foundation models: From specialists to general-purpose assistants. arXiv preprint arXiv:2309.10020, 2023.   \n[8] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.   \n[9] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023.   \n[10] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023.   \n[11] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Mimic-it: Multi-modal in-context instruction tuning. arXiv preprint arXiv:2306.05425, 2023.   \n[12] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis & insights from multimodal llm pre-training. arXiv preprint arXiv:2403.09611, 2024.   \n[13] IDEFICS. Introducing idefics: An open reproduction of state-of-the-art visual language model. https: //huggingface.co/blog/idefics, 2023.   \n[14] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   \n[15] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[16] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[17] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. arXiv preprint arXiv:2209.07753, 2022.   \n[18] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, et al. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022.   \n[19] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023.   \n[20] Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, et al. Vision-language foundation models as effective robot imitators. arXiv preprint arXiv:2311.01378, 2023.   \n[21] Andrew Szot, Max Schwarzer, Harsh Agrawal, Bogdan Mazoure, Walter Talbott, Katherine Metcalf, Natalie Mackraz, Devon Hjelm, and Alexander Toshev. Large language models as generalizable policies for embodied tasks. arXiv preprint arXiv:2310.17722, 2023.   \n[22] Stefanie Tellex, Nakul Gopalan, Hadas Kress-Gazit, and Cynthia Matuszek. Robots that use language. Annual Review of Control, Robotics, and Autonomous Systems, 3:25\u201355, 2020.   \n[23] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm\u2019s referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023.   \n[24] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. In ICLR, 2024.   \n[25] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. arXiv preprint arXiv:2305.11175, 2023.   \n[26] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. arXiv preprint arXiv:2308.00692, 2023.   \n[27] Hao Zhang, Hongyang Li, Feng Li, Tianhe Ren, Xueyan Zou, Shilong Liu, Shijia Huang, Jianfeng Gao, Lei Zhang, Chunyuan Li, et al. Llava-grounding: Grounded visual chat with large multimodal models. arXiv preprint arXiv:2312.02949, 2023.   \n[28] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.   \n[29] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. PaLM-E: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023.   \n[30] Oier Mees, Lukas Hermann, Erick Rosete-Beas, and Wolfram Burgard. Calvin: A benchmark for languageconditioned policy learning for long-horizon robot manipulation tasks. IEEE Robotics and Automation Letters, 7(3):7327\u20137334, 2022.   \n[31] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pages 1094\u20131100. PMLR, 2020.   \n[32] Dhruv Shah, B\u0142a\u02d9zej Osi\u00b4nski, Sergey Levine, et al. Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action. In Conference on Robot Learning, pages 492\u2013504. PMLR, 2023.   \n[33] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022.   \n[34] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 9493\u20139500. IEEE, 2023.   \n[35] Wenlong Huang, Fei Xia, Dhruv Shah, Danny Driess, Andy Zeng, Yao Lu, Pete Florence, Igor Mordatch, Sergey Levine, Karol Hausman, et al. Grounded decoding: Guiding text generation with grounded models for robot control. arXiv preprint arXiv:2303.00855, 2023.   \n[36] Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, and Thomas Funkhouser. Tidybot: Personalized robot assistance with large language models. arXiv preprint arXiv:2305.05658, 2023.   \n[37] Tom Silver, Soham Dan, Kavitha Srinivas, Joshua B Tenenbaum, Leslie Pack Kaelbling, and Michael Katz. Generalized planning in pddl domains with pretrained large language models. arXiv preprint arXiv:2305.11014, 2023.   \n[38] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023.   \n[39] Ruizhe Shi, Yuyao Liu, Yanjie Ze, Simon S Du, and Huazhe Xu. Unleashing the power of pre-trained language models for offline reinforcement learning. arXiv preprint arXiv:2310.20587, 2023.   \n[40] Thomas Carta, Cl\u00e9ment Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-Yves Oudeyer. Grounding large language models in interactive environments with online reinforcement learning. arXiv preprint arXiv:2302.02662, 2023.   \n[41] Ayush Jain, Andrew Szot, and Joseph J Lim. Generalization to new actions in reinforcement learning. arXiv preprint arXiv:2011.01928, 2020.   \n[42] Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sunehag, Timothy Lillicrap, Jonathan Hunt, Timothy Mann, Theophane Weber, Thomas Degris, and Ben Coppin. Deep reinforcement learning in large discrete action spaces. arXiv preprint arXiv:1512.07679, 2015.   \n[43] Nur Muhammad Shafiullah, Zichen Cui, Ariuntuya Arty Altanzaya, and Lerrel Pinto. Behavior transformers: Cloning $k$ modes with one stone. Advances in neural information processing systems, 35:22955\u201322968, 2022.   \n[44] Zichen Jeff Cui, Yibin Wang, Nur Muhammad Mahi Shafiullah, and Lerrel Pinto. From play to policy: Conditional behavior generation from uncurated robot data. arXiv preprint arXiv:2210.10047, 2022.   \n[45] Seungjae Lee, Yibin Wang, Haritheja Etukuru, H Jin Kim, Nur Muhammad Mahi Shafiullah, and Lerrel Pinto. Behavior generation with latent actions. arXiv preprint arXiv:2403.03181, 2024.   \n[46] Georgios Pantazopoulos, Malvina Nikandrou, Amit Parekh, Bhathiya Hemanthage, Arash Eshghi, Ioannis Konstas, Verena Rieser, Oliver Lemon, and Alessandro Suglia. Multitask multimodal prompted training for interactive embodied task completion. arXiv preprint arXiv:2311.04067, 2023.   \n[47] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al. Octo: An open-source generalist robot policy. arXiv preprint arXiv:2405.12213, 2024.   \n[48] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023.   \n[49] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, et al. Generative multimodal models are in-context learners. arXiv preprint arXiv:2312.13286, 2023.   \n[50] Tianhe Yu, Ted Xiao, Austin Stone, Jonathan Tompson, Anthony Brohan, Su Wang, Jaspiar Singh, Clayton Tan, Jodilyn Peralta, Brian Ichter, et al. Scaling robot learning with semantically imagined experience. arXiv preprint arXiv:2302.11550, 2023.   \n[51] Emanuele Aiello, Lili Yu, Yixin Nie, Armen Aghajanyan, and Barlas Oguz. Jointly training large autoregressive multimodal models. arXiv preprint arXiv:2309.15564, 2023.   \n[52] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11523\u201311532, 2022.   \n[53] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Soundstream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:495\u2013507, 2021.   \n[54] Richard Bellman. A markovian decision process. Indiana Univ. Math. J., 6:679\u2013684, 1957. ISSN 0022-2518.   \n[55] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. Perceiver io: A general architecture for structured inputs & outputs. arXiv preprint arXiv:2107.14795, 2021.   \n[56] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017.   \n[57] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \n[58] Tianhe Yu, Deirdre Quillen, Zhanpeng He, R. Julian, Karol Hausman, Chelsea Finn, and S. Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In CoRL, 2019.   \n[59] Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, et al. Habitat 2.0: Training home assistants to rearrange their habitat. Advances in Neural Information Processing Systems, 34, 2021.   \n[60] Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio. BabyAI: First steps towards grounded language learning with a human in the loop. In ICLR, 2019. URL https://openreview.net/forum?id=rJeXCo0cYX.   \n[61] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \n[62] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n[63] Tobias Johannink, Shikhar Bahl, Ashvin Nair, Jianlan Luo, Avinash Kumar, Matthias Loskyll, Juan Aparicio Ojea, Eugen Solowjow, and Sergey Levine. Residual reinforcement learning for robot control. In 2019 international conference on robotics and automation (ICRA), pages 6023\u20136029. IEEE, 2019.   \n[64] Ronald Parr, Lihong Li, Gavin Taylor, Christopher Painter-Wakefield, and Michael L Littman. An analysis of linear models, linear value-function approximation, and feature selection for reinforcement learning. In Proceedings of the 25th international conference on Machine learning, pages 752\u2013759, 2008.   \n[65] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.   \n[66] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022.   \n[67] Yao Wei, Yanchao Sun, Ruijie Zheng, Sai Vemprala, Rogerio Bonatti, Shuhang Chen, Ratnesh Madaan, Zhongjie Ba, Ashish Kapoor, and Shuang Ma. Is imitation all you need? generalized decision-making with dual-phase training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16221\u201316231, 2023.   \n[68] Ankesh Anand, Jacob Walker, Yazhe Li, Eszter V\u00e9rtes, Julian Schrittwieser, Sherjil Ozair, Th\u00e9ophane Weber, and Jessica B Hamrick. Procedural generalization by planning with self-supervised world models. arXiv preprint arXiv:2111.01587, 2021.   \n[69] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604\u2013609, 2020.   \n[70] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175, 2022.   \n[71] Tsung-Wei Ke, Nikolaos Gkanatsios, and Katerina Fragkiadaki. 3d diffuser actor: Policy diffusion with 3d scene representations. arXiv preprint arXiv:2402.10885, 2024.   \n[72] Andrew Szot, Karmesh Yadav, Alex Clegg, Vincent-Pierre Berges, Aaron Gokaslan, Angel Chang, Manolis Savva, Zsolt Kira, and Dhruv Batra. Habitat rearrangement challenge 2022. https://aihabitat.org/ challenge/2022_rearrange, 2022.   \n[73] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. Huggingface\u2019s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.   \n[74] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.   \n[75] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 3505\u20133506, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "table", "img_path": "0Gl5WxY6es/tmp/9113c1ae09c5fc7a40826a45fecdd41a51b466f5df3e6f74aca9d6f771d85788.jpg", "table_caption": [], "table_footnote": ["Table 2: Comparing our investigation to prior work. Prior work typically analyzes a single action adapter in a single environment. We study a variety of action adapters across a variety of environments. "], "page_idx": 13}, {"type": "text", "text": "A Prior Work Comparison ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section we expand on the differences between the prior work in action space adaptation mentioned in Section 2 and our investigation. Table 2 compares our investigation to prior work along several key dimensions. We emphasize that unlike prior works, ours studies a variety of action space adapters under a greater diversity of environments. ", "page_idx": 13}, {"type": "text", "text": "A.1 Empirical Comparison to Prior Work ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We report performance on standard benchmarks which prior work has also extensively studied. However, even within the benchmarks there are differences in training algorithms and sensor input assumptions that make direct comparison to prior work difficult. Regardless of these differences, we study different ASAs for MLLMs in a consistent experimental setting. We also describe differences between the empirical setups of ours and prior works that perform well on these benchmarks. ", "page_idx": 13}, {"type": "text", "text": "Meta-World (MLLM $+\\mathrm{RV}\\mathrm{Q}\\,84\\%$ success rate on ML-45): To the best of our knowledge, our $84\\%$ is the highest reported on Meta-World ML-45 so far. Anand et al. [68] operates under similar sensor assumptions and achieves $77\\%$ success with MuZero [69]. DualMind [67] achieves $79\\%$ success rate on ML-45 and outperforms other generalist agents like Gato [70]. However, DualMind uses privileged simulator information about the joint states and object positions while we only use RGB visual observations. ", "page_idx": 13}, {"type": "text", "text": "CALVIN $(\\mathbf{M}\\mathbf{L}\\mathbf{M}+\\mathbf{R}\\mathbf{V}\\mathbf{Q}\\ 72\\%$ success rate): RoboFlamingo achieves a higher $82\\%$ success rate on the same $A B C\\rightarrow D$ task. However, RoboFlamingo uses the OpenFlamingo VLM while we use LLaVA. RoboFlamingo use the gripper and fixed camera while we only use the fixed camera. More recent work like 3D Diffuser Actor [71] practically solves the $A B C\\rightarrow D$ task, achieving $96\\%$ success rate. However, this work uses depth inputs, and a diffusion model policy that predicts keypoints for the end-effector rather than underlying actions. Our work uses only RGB visuals, uses a MLLM policy and predicts relative end-effector poses rather than keypoints. ", "page_idx": 13}, {"type": "text", "text": "Language Rearrangement (SemLang $51\\%$ success rate): This outperforms the prior highest reported number of $42\\%$ on the overall evaluation set from LLaRP [21]. ", "page_idx": 13}, {"type": "text", "text": "BabyAI (SemLang $40\\%$ success rate): GFlan [40] achieves $55\\%$ success on the same evaluation split. However, the GFlan policy takes as input a ground truth language description of the state, while our policies take as input a $200\\times200$ RGB top down rendering of the environment. GFlan also trains the policy with reinforcement learning while we train with supervised learning. ", "page_idx": 13}, {"type": "text", "text": "B Environment Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "An overview of the environments is visualized in Figure 7. This figure visualizes the training observations input to the agent. We run experiments on 5 environments, and each environment in turn consists of multiple tasks. We arrive at the task count of 114 in the main paper through 45 tasks in Meta-World, 34 in CALVIN, 20 in HabPick where we count each object goal as a different task, 10 in Language Rearrangement for each of the evaluation splits, and 5 in BabyAI. The task count for Language Rearrangement is conservative since technically it consists of 282 instruction templates, each of which corresponds to a distinct task and goal. ", "page_idx": 13}, {"type": "image", "img_path": "0Gl5WxY6es/tmp/b5c720e2e17540e1053a0d196b428d16e841aa00b5eacc8ca7f319a78d4acf95.jpg", "img_caption": ["Figure 7: Visualizations of the environments we study. The top row shows an observation in the environment. The bottom row shows the associated instruction in that episode. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "B.1 Meta-World ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Tasks: We use the ML-45 benchmark from Meta-World [58]. Each of the 45 tasks are specified with a fixed language instruction. We use the task descriptions from Appendix Section A of Yu et al. [58]. ", "page_idx": 14}, {"type": "text", "text": "Observation Space: $200\\times200$ RGB images from a fixed camera position. To render the visual observations, we only use the \u201ccorner4\" camera position as this gives an unobstructed view of the robot in most of the tasks. ", "page_idx": 14}, {"type": "text", "text": "Action Space: 4DoF continuous control of the arm and gripper. The first 3 dimensions specify the relative end-effector translation. The last dimension specifies the desired gripper state. ", "page_idx": 14}, {"type": "text", "text": "Training: We use 40 start and goal configurations for each of the tasks. We generate 500 demonstrations for each of the 45 tasks. We use the scripted policies from Yu et al. [58]. At each step we add Gaussian noise $\\mathcal{N}(0,0.1)$ to the actions produced by the scripted policy before executing it in the environment. We generate 500 successful trajectories per task, resulting in $45\\cdot500=22.5k$ total trajectories. ", "page_idx": 14}, {"type": "text", "text": "Evaluation: We evaluate performance on 10 unseen start and goal configurations for each of the 45 tasks. So in total, we evaluate on 450 unseen configurations and report the average performance over these 450 episodes. ", "page_idx": 14}, {"type": "text", "text": "B.2 CALVIN ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Tasks: We use the CALVIN $A B C\\to D$ dataset split. ", "page_idx": 14}, {"type": "text", "text": "Observation Space: $200\\times200$ RGB observations from the fixed camera view. ", "page_idx": 14}, {"type": "text", "text": "Action Space: 7DoF continuous control of the arm and gripper. The first 6 dimensions specify the relative position and rotation of the end-effector. The final dimension is a binary indicator for if the gripper should be open or closed. We hold out 1024 subsequences of the policy context length from these trajectories for reporting validation performance during the SFT process. ", "page_idx": 14}, {"type": "text", "text": "Training: We use the 17,871 demonstrations provided in the CALVIN $A B C\\to D$ dataset. These demonstrations are in 3 different table backgrounds. This also includes 1,088 demonstrations for validation. ", "page_idx": 14}, {"type": "text", "text": "Evaluation: We report performance on the $D$ split. This evaluation scene is a different color than that encountered during training. All the start positions and goals are also different. Many of the language instructions are also unseen from training. We report the average performance over the 1,000 evaluation sequences. We report the success of the first task completed in the sequence. ", "page_idx": 14}, {"type": "text", "text": "B.3 Habitat Pick ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Tasks: We use the same Pick task as in Habitat 2.0 Geometric Goal object rearrangement [59, 72], except we provide the agent the name of the object to rearrange rather than the starting coordinates of the object and increase the observation resolution. The task is successful when the agent picks up the object and returns the end-effector within a fixed offset to a \u201cresting position\" in front of the robot. ", "page_idx": 14}, {"type": "text", "text": "The task ends in failure if the agent excessively collides with the scene, drops the object, or picks up the wrong object. The agent starts within 2 meters of the object and facing towards the receptacle but with random noise $\\mathcal{N}(0,1.57)$ applied to the direction of facing directly at the receptacle. The maximum number of steps per episode is 300 steps. ", "page_idx": 15}, {"type": "text", "text": "Observation Space: A $336\\times336$ head-mounted RGB camera. ", "page_idx": 15}, {"type": "text", "text": "Action Space: The action space is 10DoF control of the arm, base and gripper. The first 2 dimensions control the linear and angular velocity of the base. The next 7 dimensions control the relative joint offsets of the arm. The final dimension controls whether the suction gripper is engaged or not. ", "page_idx": 15}, {"type": "text", "text": "Training: We first train a privileged policy with RL to complete the task. This policy takes as input the egocentric depth image and the ground truth position of the target object to pick up. We collect $20k$ successful trajectories. ", "page_idx": 15}, {"type": "text", "text": "Evaluation: We evaluate on the test episodes from Szot et al. [72] which are 1, 000 episodes in unseen home layouts. ", "page_idx": 15}, {"type": "text", "text": "B.4 BabyAI ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Tasks: The tasks all occur in a $6\\times6$ grid populated with interactable objects. We use the task definitions from Carta et al. [40]. This consists of the following 5 instruction templates: \u201cGo to <object>\", \u201cPick up <object>\", \u201cPut $<$ object $\\mathrm{A}\\!>$ next to $<$ <object $\\mathbf{B}\\!>$ ,\", \u201cPick up $<$ <object $_{\\mathrm{A>}}$ then go to $<$ object $\\mathbf{B}\\!>$ and Go to $<$ <object $\\mathbf{B}\\!>$ after pick up $<$ <object $\\mathbf{A}\\!>^{\\prime}$ \", \u201cUnlock <door>\". The maximum number of steps per episode is 50 steps. ", "page_idx": 15}, {"type": "text", "text": "Observation Space: $200\\times200$ RGB observation as a top down of the $6\\times6$ scene. Note this is a more challenging observation space than prior gridworld navigation tasks that provide the current view as a compact entity specific array [60] or by a language description [40]. ", "page_idx": 15}, {"type": "text", "text": "Action Space: The action space consists of 6 actions consisting of: turn left, turn right, move forward, pick, drop and toggle. ", "page_idx": 15}, {"type": "text", "text": "Training: We collect 1,000 demonstrations for each of the 5 templates. We randomly sample an instruction and starting state configuration for every demonstration. We use the expert planner from Chevalier-Boisvert et al. [60] to generate the demonstrations. ", "page_idx": 15}, {"type": "text", "text": "Evaluation: We report performance on the unseen synonyms generalization test, described in Section 4.2 of Carta et al. [40]. We evaluate on 200 episodes per template type, giving 1000 total evaluation episodes. ", "page_idx": 15}, {"type": "text", "text": "B.5 Language Rearrangement ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Tasks: An agent starts in an unseen house and must complete a rearrangement task from a language instruction. ", "page_idx": 15}, {"type": "text", "text": "Observation Space: The agent has a $336\\times336$ head-mounted RGB camera. We increase the camera resolution from $256\\times256$ in the original Language Rearrangement task to match the input resolution of the LLaVA CLIP encoder. ", "page_idx": 15}, {"type": "text", "text": "Action Space: We use the same action space as from the original Language Rearrangement benchmark Szot et al. [21]. The agent can select between 70 high-level skills that include picking up objects by name, navigating to receptacles, placing on receptacles by name, and opening and closing receptacles by name. ", "page_idx": 15}, {"type": "text", "text": "Training: Since Language Rearrangement does not provide any demonstrations and due to the emphasis on exploration in the problem, they are not readily obtainable, even with oracle planners. Therefore, we opt to train policies with reinforcement learning from the environment reward provided by the Language Rearrangement task. ", "page_idx": 15}, {"type": "text", "text": "Evaluation: We evaluate on all 10 evaluation datasets from Language Rearrangement consisting of 1,000 evaluation episodes on unseen scenes. ", "page_idx": 15}, {"type": "text", "text": "B.6 Task Groupings ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In Section 4 we breakdown the performance on CALVIN and MetaWorld for task groupings. Each of the task groupings consists of multiple tasks from the benchmark. We grouped tasks in the following way: ", "page_idx": 16}, {"type": "text", "text": "MetaWorld: ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "\u2022 Articulated: \u201cdoor-close\", \u201cdoor-open\", \u201cdrawer-close\", \u201cdrawer-open\", \u201cfaucet-open\", \u201cfaucetclose\", \u201chandle-press-side\", \u201chandle-press\", \u201cwindow-open\", \u201cwindow-close\"   \n\u2022 Press: \u201cbutton-press-topdown\", \u201cbutton-press-topdown-wall\", \u201cbutton-press\", \u201cbutton-press-wall\", \u201ccoffee-button\"   \n\u2022 Push: \u201cplate-slide\", \u201cplate-slide-side\", \u201cplate-slide-back\", \u201cplate-slide-back-side\", \u201cpush-back\", \u201cpush\", \u201cpush-wall\", \u201cstick-push\", \u201csweep-into\", \u201csweep\", \u201csoccer\", \u201ccoffee-push\"   \n\u2022 Pick: \u201cassembly\", \u201cbasketball\", \u201cdial-turn\", \u201cdisassemble\", \u201chammer\", \u201cpeg-insert-side\", \u201cpegunplug-side\", \u201cpick-out-of-hole\", \u201cpick-place\", \u201cpick-place-wall\", \u201creach\", \u201creach-wall\", \u201cshelfplace\"   \n\u2022 Pull: \u201ccoffee-pull\", \u201chandle-pull-side\", \u201chandle-pull\", \u201clever-pull\", \u201cstick-pull\" ", "page_idx": 16}, {"type": "text", "text": "CALVIN: ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "\u2022 Articulated: \u201cmove slider left\", \u201copen drawer\", \u201cclose drawer\", \u201cmove slider right\"   \n\u2022 Press: \u201cturn off led\", \u201cturn on led\", \u201cturn on lightbulb\", \u201cturn off lightbulb\"   \n\u2022 Lift: \u201clift blue block slider\", \u201clift pink block table\", \u201clift red block slider\", \u201clift red block table\", \u201clift pink block slider\", \u201clift blue block table\"   \n\u2022 Push: \u201cpush pink block right\", \u201cpush blue block right\", \u201cpush red block left\", \u201cpush pink block left\", \u201cpush red block right\", \u201cpush blue block left\", \u201cpush into drawer\"   \n\u2022 Rotate: \u201crotate red block right\", \u201crotate red block left\", \u201crotate pink block left\", \u201crotate pink block right\", \u201crotate blue block right\", \u201crotate blue block left\" ", "page_idx": 16}, {"type": "text", "text": "C Further Policy Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Prompt Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In addition to inputting the task instruction to the LLM, we also format the instruction with a prompt. We base our prompt off the prompt used in LLaVA. For all continuous control tasks, we use the prompt template \u201cPrompt: control the robot. USER: <INSTRUCTION $>$ ASSISTANT: \". For discrete action space tasks, we describe the available actions to the agent in the prompt as well. For BabyAI, this is the prompt template \u201cPrompt: Control the red triangle to complete the instruction using left, right, forward, pick, drop and toggle. USER: <INSTRUCTION $>$ ASSISTANT: \". For Language Rearrangement, this is the prompt template \u201cPrompt: You are a home robot assistant. Your possible actions are: pick object, place receptacle, nav receptacle, open receptacle, close receptacle, STOP. - Objects: ball, clamp, hammer, screwdriver, padlock, scissors, block, drill, spatula, knife, spoon, plate, sponge, cleanser, plum, pear, peach, apple, lemon, can, box, banana, strawberry, lego, cube, book, bowl, cup, mug, orange, lid, toy, wrench. - Receptacles: chair, black table, brown table, TV stand, sink, right counter, left counter, sofa, fridge, left drawer, right drawer, middle drawer. USER: <INSTRUCTION $>$ ASSISTANT: \". ", "page_idx": 16}, {"type": "text", "text": "C.2 Action Space Adapter Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We use the same ASA details between all environments. We detail the architecture and training decisions for the different ASAs when applicable. ", "page_idx": 16}, {"type": "text", "text": "VQ: Use a codebook size of 512 with 512 dimensions per codebook element. These 512 tokens are mapped to token indices $31000-31512$ from the LLaMA language modeling head. The encoder and decoder networks for predicting the latent and decoding from the latent are 4 layer MLP networks with hidden size 2048 using ReLU activations. The VQ network is trained on the actions in the same dataset used to train the policy. The network is trained with MSE loss to reconstruct the original actions. We VQ network for 3 epochs over the dataset. ", "page_idx": 16}, {"type": "text", "text": "RVQ: Use all the same details as VQ, but with a Residual-VQ that uses 2 codebooks. ", "page_idx": 16}, {"type": "table", "img_path": "0Gl5WxY6es/tmp/cf813be67761abe0837a336d8290dd92b7dcbc2bcfa35172a2d24c5a8d5b22e5.jpg", "table_caption": ["Hyperparameter CALVIN Meta-World BabyAI HabPick "], "table_footnote": ["Table 3: Hyperparameters for all imitation learning experiments. Most hyperparameters are the same between environments but the number of training epochs, context length and batch size per GPU are adjusted to fit the need for history, environment dataset size and task complexity. "], "page_idx": 17}, {"type": "text", "text": "Pred: We use a 2 layer MLP network with a hidden size of 2048 and ReLU activations. We use this same MLP network architecture for discrete and continuous action space tasks. In the robot manipulation tasks, we also found it useful to include the robot proprioception as input to the MLP network and included this as input to the network layer. The robot proprioception consists of the robot robot joint angles and the gripper state. This ASA requires no separate training. ", "page_idx": 17}, {"type": "text", "text": "Uniform: In the tasks we consider, the actions are already normalized to be in $[-1,1]$ . We then create 512 evenly spaced bins within this interval and assign each action dimension based on which bin it is within. Like with VQ, we assign the 512 tokens to indices $31000-31512$ from the LLaMA language modeling head. This ASA requires no separate training. ", "page_idx": 17}, {"type": "text", "text": "Lang: Starting from the same semantic tokenization as with SemLang, we remap each token to the token corresponding to a digit \u201c $\\mathrm{{}^{\\circ}}\\mathrm{{0^{\\prime\\prime}}}$ to \u201c9\". Therefore, the token count per action is the same between Lang and SemLang, but the Lang action tokens have no semantic meaning being just digits. ", "page_idx": 17}, {"type": "text", "text": "C.3 Training and Architecture Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We use all pretrained components from LLaVA. For the visual token downsampler, we use a 2 layer Perceiver network [55] with 4 output latents and hidden size 4096. ", "page_idx": 17}, {"type": "text", "text": "We detail the hyperparameters used for imitation learning in in Table 3. We trained with the HuggingFace Transformers library [73], PyTorch [74], DeepSpeed [75]. For reinforcement learning, we use learning rate $3e^{-4}$ , 32 steps per rollout. 18 parallel environment workers per GPU, an entropy coefficient of 0.01, 2 epochs over the data batch per rollout, 6 PPO minibatches, a maximum gradient norm of 0.2 and $\\gamma=0.99$ . ", "page_idx": 17}, {"type": "text", "text": "We train the CALVIN, Meta-World and HabPick imitation learning results on a 4xA40 GPU setup.   \nWe train the Language Rearrangement and BabyAI experiments on a 8xA100-80GB GPU setup. ", "page_idx": 17}, {"type": "text", "text": "We train the LLM weights with LoRA and fine tune the entire ASA and downsampler module. For LoRA we use rank value 128, alpha parameter 32 and dropout 0.1. ", "page_idx": 17}, {"type": "text", "text": "D Qualitative Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "See Figure 8 for qualitative results of results from Figure 3. The RVQ ASA is visualized for Meta-World, CALVIN and Habitat Pick. SemLang is visualized for Language Rearrangement. ", "page_idx": 17}, {"type": "text", "text": "E Per-Task Breakdown ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we show results for each environment by task type. Table 4 shows performance on Language Rearrangement for each of the evaluation datasets. Table 5 shows performance on CALVIN for each of the CALVIN tasks. Table 6 shows performance on BabyAI for each of the BabyAI instruction types. Table 7 shows performance on Meta-World for each of the 45 Meta-World task types. ", "page_idx": 17}, {"type": "text", "text": "Meta-World Success:\u201cGrasp a stick and pull a box with the stick.\" ", "page_idx": 18}, {"type": "text", "text": "Meta-World Failure: \"Pull a handle up sideways (a) The robot successfully picks the stick and push the box to the goal position. ", "page_idx": 18}, {"type": "image", "img_path": "0Gl5WxY6es/tmp/157930bc8d3158a7c65fd1dce08104a7880c17650c5d31946b0328de9078643b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "(b) The robot only partially lifts the handle and fails to lift it up all the way. ", "page_idx": 18}, {"type": "image", "img_path": "0Gl5WxY6es/tmp/ca2f4e095f665fb8bf4b306e3c2f4c35517ce393c637c270fe6e4e47d81c7dd1.jpg", "img_caption": ["CALVIN Success:\u201cLift the pink block\" "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "CALVIN Failure:\u201cRotate theblue block right\u201d ", "page_idx": 18}, {"type": "text", "text": "(c) The robot grasps the pink block and lifts it to th goal height. ", "page_idx": 18}, {"type": "image", "img_path": "0Gl5WxY6es/tmp/b564ad0b40f0381a4ec545887e9f9974326c5f3de85ee0fd31a752384c31b115.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "0Gl5WxY6es/tmp/2e91040ec3e9e807981f18b072741d233b689ffb934c93980f236cbb86676743.jpg", "img_caption": ["Habitat Pick Success:\u201c\"pick a cleaner\" "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "(d) The robot attempts to grasp the blue block but grasps too high, failing to pick the block. ", "page_idx": 18}, {"type": "text", "text": "Habitat Pick Failure:\u201cpick a lemon\" ", "page_idx": 18}, {"type": "text", "text": "(e) The robot moves closer to the cleaner bottle with(f) The robot correctly finds the lemon in the sink, but its base and moves the arm to grasp the cleaner. Itthe tight sink receptacle results in the arm colliding with then returns the end-effector to the resting position tothe sink and the episode terminating due to excessive successfully end the task. collisions. ", "page_idx": 18}, {"type": "text", "text": "LangRFailure: \u201cMove a strawberry from the TV stand to the right of the left counter\" ", "page_idx": 18}, {"type": "image", "img_path": "0Gl5WxY6es/tmp/bc89a9f75ea86acdaa589948a09ab6b8adb816b7e96f23e23a6436cd2136b1c7.jpg", "img_caption": ["LangR Success:\u201cBring something to pour hot liquid in to the blue table\" ", "e(h) The robot picks the strawberry navigates to the counter area, but puts the strawberry on the right counter as opposed to the correct receptacle of the sink. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "(g) The robot searches the house, eventually finds th mug and then brings it to the blue table. ", "page_idx": 18}, {"type": "text", "text": "Figure 8: Qualitative visualizations of successes and failures from the results in Figure 1 of the main paper. The RVQ action space adapter is visualized for Meta-World, CALVIN and Habitat Pick. SemLang is visualized for Language Rearrangement. ", "page_idx": 18}, {"type": "table", "img_path": "0Gl5WxY6es/tmp/9daf5703424f309635598a466e72fb894c00304bc0ec1cb7255620b83dcda7e4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 4: Evaluation results at 20M steps of RL training for all results in Language Rearrangement.   \nWe show averages and standard deviations over 2 random seeds of full policy training. ", "page_idx": 18}, {"type": "table", "img_path": "0Gl5WxY6es/tmp/923340c278d7e87dff1aa749cda371722bb93cb28e8b49a6f33c23501ca867eb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "0Gl5WxY6es/tmp/c4db758355182f173e1314fcb79467fdc662578cdc2b0dbe2792e52fa3dcf08e.jpg", "table_caption": [], "table_footnote": ["Table 6: Breakdown on every BabyAI task. "], "page_idx": 19}, {"type": "table", "img_path": "0Gl5WxY6es/tmp/556255e919b246e3a79284a6e651057808dbd029cb0e5f01bd3619e2976fdc00.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Yes, our claims in the abstract and introduce are experimentally verified in Section 4. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Yes, we mention limitations in Section 5. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not include theoretical results. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Yes, we describe all experimental settings in detail in Appendix C.3, we build on open-source models and benchmarks and we plan to open-source the code. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We train and evaluate our method only on open benchmarks and train from an open source model. We also plan to release our code. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Details provided in Appendix C.3. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: In our reinforcement learning results in Section 4.3 we show results over 2 seeds and display the standard deviation between seeds as a shaded region in the plot. For other experiments we only report results on 1 seed since we are finetuning 7B parameter models, making it computationally intensive to run multiple seeds. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Yes, complete details are proivded in Appendix C.3. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Yes, the paper adheres to the ethics code. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper is on foundational research not tied to any particular application and we do not feel it is important to highlight any negative societal impacts of our work. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 24}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We cite all datasets used. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]