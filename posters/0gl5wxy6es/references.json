{"references": [{"fullname_first_author": "Jinze Bai", "paper_title": "Qwen-vl: A frontier large vision-language model with versatile abilities", "publication_date": "2023-08-12", "reason": "This paper introduces a large vision-language model that is foundational to the work presented, providing a strong base for multimodal understanding and action generation."}, {"fullname_first_author": "Shaohan Huang", "paper_title": "Language is not all you need: Aligning perception with language models", "publication_date": "2023-00-00", "reason": "This paper highlights a key challenge in embodied AI, namely the need to effectively align perception and language models, a problem addressed by the current research."}, {"fullname_first_author": "Zhiliang Peng", "paper_title": "Kosmos-2: Grounding multimodal large language models to the world", "publication_date": "2023-06-14", "reason": "This paper directly addresses the grounding problem of MLLMs, which is the core focus of this current research."}, {"fullname_first_author": "Junnan Li", "paper_title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "publication_date": "2023-00-00", "reason": "This paper presents a significant advancement in vision-language models, which is highly relevant given the multimodal nature of the task."}, {"fullname_first_author": "Anthony Brohan", "paper_title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control", "publication_date": "2023-07-15", "reason": "This paper demonstrates the successful application of vision-language-action models in robotics, providing a benchmark and inspiration for the current work."}]}