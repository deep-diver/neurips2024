[{"figure_path": "SAQXbnvv4t/tables/tables_6_1.jpg", "caption": "Table 1: Results of pass@1 on HumanEval (HumanEval+) and MBPP (MBPP+) benchmarks. We report the results of HumanEval and MBPP consistently from the EvalPlus [27] and the bold scores denote the best performance among models of the same size.", "description": "This table presents the pass@1 scores achieved by various large language models (LLMs) on the HumanEval and MBPP benchmarks.  The models are categorized into closed-source and open-source, with each model's performance reported along with its size (in parameters).  The table highlights the superior performance of AlchemistCoder models compared to other open-source models of similar size and even larger ones in some cases.  The scores are presented both with and without the use of EvalPlus enhancements.", "section": "3.2 Evaluation on code generation task"}, {"figure_path": "SAQXbnvv4t/tables/tables_6_2.jpg", "caption": "Table 2: Results of pass@1 on HumanEval-X. Table 3: Pass@1 results of models with 6.7B/7B parameters on DS-1000. pd, np, tf, sp, skl, torch, and plt represent Pandas, Numpy, Tensorflow, Scipy, Sklearn, Pytorch, and Matplotlib, respectively.", "description": "Table 2 presents the pass@1 scores on the HumanEval-X benchmark for various models, showcasing their multilingual code generation capabilities.  Table 3 shows pass@1 scores on the DS-1000 benchmark focusing on data science code completion, comparing performance across different models and highlighting the specific libraries (Pandas, NumPy, TensorFlow, SciPy, Scikit-learn, PyTorch, Matplotlib) involved in the tasks.", "section": "3.2 Evaluation on code generation task"}, {"figure_path": "SAQXbnvv4t/tables/tables_7_1.jpg", "caption": "Table 4: Ablation study on the effectiveness of multi-source harmonization (i.e., Multi-source Integration, Data Decontamination, and AlchemistPrompt Harmonization) and code understanding tasks (i.e., Instruction Evolution Task, Data Filtering Task, and Code Review Task) for the AlchemistCoder-CL-7B model, evaluated on the HumanEval and MBPP benchmarks.", "description": "This table presents the ablation study results of the AlchemistCoder-CL-7B model's performance on HumanEval and MBPP benchmarks.  It analyzes the impact of different components of the proposed method: multi-source data integration, data decontamination, AlchemistPrompt harmonization, and three code understanding tasks (instruction evolution, data filtering, and code review). Each row represents a different combination of these components, showing the pass@1 scores on both benchmarks. The final row shows the model's performance with all components included, demonstrating the overall improvement achieved.", "section": "3.3 Ablation study"}, {"figure_path": "SAQXbnvv4t/tables/tables_9_1.jpg", "caption": "Table 5: Results of models (6.7B/7B) on various benchmarks, including MMLU for multitask language understanding, BBH for comprehensive reasoning, and GSM8K for mathematical ability.", "description": "This table presents the performance of AlchemistCoder models (6.7B and 7B parameters), along with several baseline models, across three diverse benchmarks: MMLU (evaluating multitask language understanding), BBH (assessing comprehensive reasoning), and GSM8K (testing mathematical ability).  The average score across all three benchmarks is also provided for each model, offering a comparative overview of their general-purpose capabilities.  The results highlight AlchemistCoder's superior performance compared to the base models on these benchmarks, showcasing its improved abilities in various reasoning and comprehension tasks.", "section": "3. Experiments"}, {"figure_path": "SAQXbnvv4t/tables/tables_14_1.jpg", "caption": "Table 1: Results of pass@1 on HumanEval (HumanEval+) and MBPP (MBPP+) benchmarks. We report the results of HumanEval and MBPP consistently from the EvalPlus [27] and the bold scores denote the best performance among models of the same size.", "description": "This table presents the pass@1 scores achieved by various large language models (LLMs) on two popular code generation benchmarks: HumanEval and MBPP.  It compares the performance of both closed-source models (GPT-3.5-Turbo and GPT-4-Turbo) and various open-source models across different parameter scales, highlighting the relative performance of AlchemistCoder models (AlchemistCoder-L, AlchemistCoder-CL, AlchemistCoder-DS) compared to other models of similar size and larger models.", "section": "3.2 Evaluation on code generation task"}, {"figure_path": "SAQXbnvv4t/tables/tables_14_2.jpg", "caption": "Table A2: Ablation study on the efficacy of multi-source integration and AlchemistPrompt harmonizations, evaluated on the HumanEval (Pass@1) and MBPP (Pass@1) benchmarks.", "description": "This table presents the results of an ablation study conducted to evaluate the effectiveness of multi-source data integration and AlchemistPrompt harmonizations on the HumanEval and MBPP benchmarks. The study systematically increased the number of data sources used for fine-tuning, while incorporating AlchemistPrompt harmonizations to address inherent conflicts between different data sources.  The results demonstrate the impact of each component on the model's performance, showcasing the effectiveness of the proposed method in enhancing code generation capabilities.", "section": "Appendix"}]