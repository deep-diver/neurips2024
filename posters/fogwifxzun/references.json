{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017", "reason": "This paper introduced the Transformer architecture, which is fundamental to the current research on large language models and is the foundation of the models used in the current work."}, {"fullname_first_author": "Emmanuel Abbe", "paper_title": "Polynomial-time universality and limitations of deep learning", "publication_date": "2023", "reason": "This paper provides theoretical results on the limitations of deep learning, which are directly relevant to the study of global reasoning and the globality barrier investigated in this paper."}, {"fullname_first_author": "Chiyuan Zhang", "paper_title": "Pointer value retrieval: A new benchmark for understanding the limits of neural network generalization", "publication_date": "2021", "reason": "This paper introduced a new benchmark for evaluating the generalization capabilities of neural networks, which is highly relevant to this study on the limitations of Transformer models for reasoning tasks."}, {"fullname_first_author": "Cem Anil", "paper_title": "Exploring length generalization in large language models", "publication_date": "2022", "reason": "This paper investigates the problem of length generalization in large language models, which is one of the main motivations for the current work."}, {"fullname_first_author": "Ryan O'Donnell", "paper_title": "Analysis of Boolean Functions", "publication_date": "2014", "reason": "This book provides a comprehensive overview of Boolean functions, which are essential for understanding the theoretical aspects of global reasoning and the globality barrier."}]}