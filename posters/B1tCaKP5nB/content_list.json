[{"type": "text", "text": "A Conditional Independence Test in the Presence of Discretization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Testing conditional independence (CI) has many important applications, such as   \n2 Bayesian network learning and causal discovery. Although several approaches have   \n3 been developed for learning CI structures for observed variables, those existing   \n4 methods generally fail to work when the variables of interest can not be directly   \n5 observed and only discretized values of those variables are available. For example,   \n6 if $X_{1}$ , $\\tilde{X}_{2}$ and $X_{3}$ are the observed variables, where $\\tilde{X}_{2}$ is a discretization of the   \n7 latent variable $X_{2}$ , applying the existing methods to the observations of $X_{1}$ , $\\tilde{X}_{2}$   \n8 and $X_{3}$ would lead to a false conclusion about the underlying CI of variables   \n9 $X_{1}$ , $X_{2}$ and $X_{3}$ . Motivated by this, we propose a CI test specifically designed to   \n10 accommodate the presence of discretization. To achieve this, a bridge equation   \n11 and nodewise regression are used to recover the precision coefficients reflecting   \n12 the conditional dependence of the latent continuous variables under the nonpara  \n13 normal model. An appropriate test statistic has been proposed, and its asymptotic   \n14 distribution under the null hypothesis of CI has been derived. Theoretical analysis,   \n15 along with empirical validation on various datasets, rigorously demonstrates the   \n16 effectiveness of our testing methods. ", "page_idx": 0}, {"type": "text", "text": "17 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "18 Independence and conditional independence (CI) are fundamental concepts in statistics. They are   \n19 leveraged for exploring queries in statistical inference, such as sufficiency, parameter identification,   \n20 adequacy, and ancillarity [9]. They also play a central role in emerging areas such as causal discovery   \n21 [18], graphical model learning, and feature selection [36]. Tests for CI have attracted increasing   \n22 attention from both theoretical and application sides.   \n23 Formally, the problem is to test the CI of two variables $X_{j_{1}}$ and $X_{j_{2}}$ given a random vector (a set   \n24 of other variables) $Z$ . In statistical notation, the null hypothesis is written as $H_{0}:X_{j_{1}}\\perp X_{j_{2}}\\mid Z_{:}$   \n25 where $\\bot$ denotes \u201cindependent from.\u201d The alternative hypothesis is written as $H_{1}:X_{j_{1}}\\not\\perp X_{j_{2}}\\mid Z_{j}$ ,   \n26 where $\\nangle$ denotes \u201cdependent with.\u201d The null hypothesis implies that once $Z$ is known, the values of   \n27 $X_{j_{1}}$ provide no additional information about $X_{j_{2}}$ , and vice versa. Different tests have been designed   \n28 to handle different scenarios, including Gaussian variables with linear dependence [37, 25, 22, 26]   \n29 and non-linear dependence [16, 38, 31, 27, 1] (For detailed related work, please refer to App. $D$ ).   \n30 Given observations of $X_{j_{1}}$ , $X_{j_{2}}$ , and $_{z}$ , the CI can be effectively tested with existing methods.   \n31 However, in many scenarios, accurately measuring continuous variables of interest is challenging   \n32 due to limitations in data collection. Sometimes the data obtained are approximations represented as   \n33 discretized values. For example, in finance, variables such as asset values cannot be measured and are   \n34 binned into ranges for assessing investment risks (e.g., sell, hold, and strong buy) [7, 8]. Similarly,   \n35 in mental health, anxiety levels are often assessed using scales like the GAD-7, which categorizes   \n36 responses into levels such as mild, moderate, or severe [23, 17]. In the entertainment industry, the   \n37 quality of movies is typically summarized through viewer ratings [29, 10].   \n38 When discretization is present, existing CI tests   \n39 can fail to determine the CI of underlying con  \n40 tinuous variables. This issue arises because ex  \n41 isting CI tests treat discretized observations as   \n42 observations of continuous variables, leading   \n43 to incorrect conclusions about their CI relation  \n44 ships. More precisely, the problem lies in the   \n45 discretization process, which introduces new dis  \n46 crete variables. Consequently, although the in  \n47 tent is to test the CI of the underlying continuous   \n48 variables, what is actually being tested is the $C I$ ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "B1tCaKP5nB/tmp/690beb4b20cdd6d977d79e94dd12f2005a9c71de4bbcbeb01aed2e8aa3d3534c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: We illustrate different data generative processes with causal graphical models. The discretization process introduces new discrete variables which are denoted with a tilde $(\\sim)$ . ", "page_idx": 1}, {"type": "text", "text": "49 involving a mix of both continuous and newly introduced discrete variables. In general, this CI   \n50 relationship is inconsistent with the one among the underlying continuous variables.   \n51 As illustrated in Fig. 1, we show different data-generative processes using causal graphical models   \n52 [24] in the presence of discretization. A gray node indicates an observable variable, while a white   \n53 node indicates a latent variable. Variables denoted by $X_{j}$ (without a tilde $\\sim$ ) represent continuous   \n54 variables, which may not be observed; while variables denoted by $\\tilde{X}_{j}$ represent observed discretized   \n55 variables derived from $X_{j}$ due to discretization. In Fig. 1(a), $X_{2}$ is latent, and only its discrete   \n56 counterpart $\\tilde{X}_{2}$ is observed. In this case, rather than observing $X_{1}$ , $X_{2}$ , and $X_{3}$ , we only observe   \n57 $X_{1},{\\tilde{X}}_{2}$ , and $X_{3}$ . Existing CI methods use these observations to test whether $X_{1}\\perp X_{3}\\mid\\{X_{2}\\}$ , but   \n58 what is actually being tested is whether $X_{1}\\perp X_{3}\\mid\\{\\tilde{X}_{2}\\}$ . In fact, according to the causal Markov   \n59 condition [30], , it can be inferred from Fig. 1(a) that $X_{1}\\,\\bot\\,X_{3}\\mid\\,\\{X_{2}\\}~$ and $X_{1}\\;\\rlap{/}\\;X_{3}\\mid\\;\\{\\tilde{X}_{2}\\}$ .   \n60 This mismatch leads to existing CI methods, that employ observations to check the CI relationships   \n61 between $X_{1}$ and $X_{3}$ given $X_{2}$ , to reach incorrect conclusions. Due to the same reason, checking the   \n62 CI also fails in Fig 1(b) and Fig 1(c).   \n63 In this paper, we design a CI test specifically for handling the presence of discretization. An appropri  \n64 ate test statistic for the CI of latent continuous variables, based solely on discretized observations, is   \n65 derived. The key is to build connections between the discretized observations and the parameters   \n66 needed for testing the CI of the latent continuous variables. To achieve this, we first develop bridge   \n67 equations that allow us to estimate the covariance of the underlying continuous variables with dis  \n68 cretized observations. Then, we leverage a node-wise regression [5] to derive appropriate test statistics   \n69 for CI relationships from the estimated covariance. By assuming that the continuous variables follow   \n70 a Gaussian distribution, we can derive the asymptotic distributions of the test statistics under the null   \n71 hypothesis of CI. The major contributions of our paper include that   \n72 \u2022 We develop a CI test for ensuring accurate analysis in scenarios where data has been discretized,   \n73 which are common due to limitations in data collection or measurement techniques, such as in   \n74 financial analysis and healthcare.   \n75 \u2022 Our CI test can handle various scenarios including 1). Both variables $X_{j_{1}}$ and $X_{j_{2}}$ are discretized   \n76 2). Both variables $X_{j_{1}}$ and $X_{j_{2}}$ are continuous. 3). One of the variables $X_{j_{1}}$ or $X_{j_{2}}$ is discretized.   \n77 \u2022 We compare our test with the existing methods on both synthetic and real-world datasets, confirm  \n78 ing that our method can effectively estimate the CI of the underlying continuous variables and   \n79 outperform the existing tests applied on the discretized observations. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "80 2 DCT: A CI Test in the Presence of Discretization ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "81 Problem Setting Consider a set of independent and identically distributed (i.i.d.) $p$ -dimensional   \n82 random vectors, denoted as $\\tilde{\\mathbf{X}}=(X_{1},X_{2};\\ldots,\\tilde{X}_{j},\\ldots,\\tilde{X}_{p})^{T}$ . In this set, some variables, indicated   \n83 by a tilde $(\\sim)$ , such as $\\tilde{X}_{j}$ , follow a discrete distribution. For each such variable, there exists a   \n84 corresponding latent Gaussian random variable $X_{j}$ . The transformation from $X_{j}$ to $\\tilde{X}_{j}$ is governed   \n85 by an unknown monotone nonlinear function $g_{j}$ . This function, $g_{j}:\\mathcal{X}\\rightarrow\\tilde{\\mathcal{X}}$ , maps the continuous   \n86 domain of $X_{j}$ onto the discrete domain of $\\tilde{\\mathcal{X}}_{j}$ , such that $\\tilde{X}_{\\overset{.}{\\sim}}=g_{j}(X_{\\overset{.}{\\sim}})$ for each observation. Given $n$   \n87 observations $\\{\\tilde{x}^{1},\\tilde{x}^{2},\\ldots,\\tilde{x}^{n}\\}$ randomly sampled from $\\tilde{X}$ , specifically, for each variable $X_{j}$ , there ", "page_idx": 1}, {"type": "text", "text": "88 exists a constant vector $\\mathbf{d}=(d_{1},\\hdots,d_{M})$ characterized by strictly increasing elements such that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\tilde{x}_{j}^{i}=\\left\\{\\!\\!\\begin{array}{l l}{1}&{0<g_{j}(x_{j}^{i})<d_{1}}\\\\ {m}&{d_{m-1}<g_{j}(x_{j}^{i})<d_{m}}\\\\ {M}&{g_{j}(x_{j}^{i})>d_{m}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "89 This model is also known as the nonparanormal model [20]. The cardinality of the domain after   \n90 discretization is at least 2 and smaller than infinity. Our goal is to assess both conditional and   \n91 unconditional independence among the variables of the vector $\\boldsymbol{X}=(X_{1},X_{2},\\ldots,X_{j},\\ldots,X_{p})^{T}$ .   \n92 In our model, we assume $\\pmb{X}\\sim N(0,\\Sigma)$ , $\\Sigma$ only contain 1 among its diagonal, i.e., $\\sigma_{j j}=1$ for all   \n93 $j\\in[1,\\ldots,p]$ . One should note this assumption is without loss of generality. We provide a detailed   \n94 discussion of our assumption in App. A.8.   \n95 Preliminary Framework of DCT To develop an independence test, one needs to design a test   \n96 statistic that can reflect the dependence relation and be calculated from observations. Next, it is   \n97 essential to derive the underlying distribution of this statistic under the null hypothesis that the tested   \n98 variables are conditionally (or unconditionally) independent. By calculating the value of the test   \n99 statistic from observations and determining if this statistic is likely to be sampled from the derived   \n100 distribution (i.e., calculating the $p$ -value and comparing it with the significance level $\\alpha$ ), we can   \n101 decide if the null hypothesis should be rejected.   \n102 Our objective is to deduce the independence and CI relationships within the original multivariate   \n103 Gaussian model, based on its discretized observations. In the context of a multivariate Gaussian   \n104 model, this challenge is directly equivalent to constructing statistical inferences for its covariance   \n105 matrix $\\pmb{\\Sigma}=(\\sigma_{j_{1},j_{2}})$ and its precision matrix $\\Omega\\,=\\,(\\omega_{j,k})\\,=\\,\\Sigma^{-1}$ [3]. The covariance matrix $\\Sigma$   \n106 captures the pairwise covariances between variables, while the precision matrix $\\pmb{\\Omega}$ (also known as the   \n107 concentration matrix) provides information about the CI between variables. Specifically, the entry   \n108 $\\omega_{j,k}$ in the precision matrix is related to the partial correlation coefficient between variables $X_{j}$ and   \n109 $X_{k}$ , which can be used to test whether these variables are conditionally independent given some other   \n110 variables. Technically, we are interested in two things: (1) the calculation of the covariance $\\hat{\\sigma}_{j_{1},j_{2}}$   \n111 and the precision coefficient (or the partial correlation coefficient) $\\hat{\\omega}_{j,k}$ , serving as the estimation   \n112 of $\\sigma_{j_{1},j_{2}}$ and $\\omega_{j,k}$ respectively (in this paper, a variable with a hat indicates its estimation); and   \n113 (2) the derivation of the distribution of $\\hat{\\sigma}_{j_{1},j_{2}}-\\sigma_{j_{1},j_{2}}$ and $\\hat{\\omega}_{j,k}-\\omega_{j,k}$ under the null hypothesis of   \n114 independence and CI.   \n115 In the subsequent section, 1). we first introduce bridge equations to address the estimation challenge   \n116 of the covariance $\\sigma_{j_{1},j_{2}};2)$ . we proceed to derive the distribution of $\\hat{\\sigma}_{j_{1},j_{2}}-\\sigma_{j_{1},j_{2}}$ , demonstrating it   \n117 is asymptotically normal; 3). utilizing nodewise regression, we establish the relationship between   \n118 the covariance matrix $\\Sigma$ and the precision matrix $\\pmb{\\Omega}$ , where the regression parameter $\\beta_{j,k}$ acts as an   \n119 effective surrogate for $\\omega_{j,k}$ . Leveraging the distribution of $\\hat{\\sigma}_{j_{1},j_{2}}-\\sigma_{j_{1},j_{2}}$ , we further illustrate that   \n120 $\\hat{\\beta}_{j,k}-\\beta_{j,k}$ is also asymptotically normal. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "121 2.1 Design Bridge Equation for Test Statistics ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "122 Estimating Covariance with Bridge Equations The bridge equation establishes a connection   \n123 between the underlying covariance $\\sigma_{j_{1},j_{2}}$ of two continuous variables $X_{j_{1}}$ and $X_{j_{2}}$ with the ob  \n124 servations. When in the presence of discretization, the discrete transformations make the sample   \n125 covariance matrix based on $\\tilde{X}$ inconsistent with the covariance matrix of $\\mathbf{\\deltaX}$ . To obtain the estimation   \n126 $\\hat{\\sigma}_{j_{1},j_{2}}$ of $\\sigma_{j_{1},j_{2}}$ , the bridge equation is leveraged. In general, its form is as follows. ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{\\tau}_{j_{1},j_{2}}=T(\\sigma_{j_{1},j_{2}};\\hat{\\mathbf{A}}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "127 where $\\sigma_{j_{1},j_{2}}$ is the covariance needed to be estimated, $\\hat{\\tau}_{j_{1},j_{2}}$ is a statistic that can also be estimated   \n128 from observations, and $\\hat{\\bf A}$ is a set of additional parameters required by the function $T(\\cdot)$ . The specific   \n129 form of the function $T(\\cdot)$ will be derived later. Both $\\hat{\\tau}_{j_{1},j_{2}}$ and $\\hat{\\bf A}$ should be able to be calculated   \n130 purely relying on observations. Then, given the calculated $\\hat{\\tau}_{j_{1},j_{2}}$ and $\\hat{\\mathbf{A}},\\,\\hat{\\boldsymbol{\\sigma}}_{j_{1},j_{2}}$ can be obtained by   \n131 solving the bridge equation $\\hat{\\tau}_{j_{1},j_{2}}=T(\\sigma_{j_{1},j_{2}};\\hat{\\bf{A}})$ . As a result, the covariance matrix $\\Sigma$ of $\\mathbf{\\deltaX}$ can be   \n132 estimated, which contains information about both unconditional independence and CI (which can be   \n133 derived from its inverse).   \n134 To estimate the covariance of a latent multivariate Gaussian distribution, we need to design appropriate   \n135 $\\hat{\\tau}_{j_{1},j_{2}},\\hat{\\mathbf{A}}$ , and $T(\\cdot)$ . Notably, bridge equations have to be designed to handle all three possible cases:   \n136 C1. both observed variables are discretized; C2. one variable is continuous while the other is   \n137 discretized; and C3. both variables remain continuous. We will show that cases C1 and C2 can be   \n138 merged into a single form of bridge equation with different parameters and a binarization operation   \n139 applied to the observations. Our bridge equations are presented in Def. 2.2, Def. 2.3, and Def. 2.4.   \n140 Bridge Equations for Discretized and Mixed Pairs Let us first address the challenging cases   \n141 where both observed variables are discretized or where one variable is continuous while the other   \n142 is discretized. In general, different bridge equations would need to be designed to handle each case   \n143 individually. However, in our analysis, we provide a unified bridge equation that is applicable to both   \n144 cases. This is achieved by binarizing the observed variables, thereby unifying both cases into a binary   \n145 case. As some information may be lost in the binarization process, this unification may require more   \n146 examples compared to using tailored bridge functions for each specific case. Developing specific   \n147 bridge equations for each case to improve sample efficiency is left in future work.   \n148 Intuitionally, for the original continuous variable $X_{j}$ , binarization separates it into two parts based on   \n149 a boundary $h_{j}$ : the part for $X_{j}$ larger than $h_{j}$ and the part for $X_{j}$ smaller than $h_{j}$ . In this case, we can   \n150 estimate the boundary by calculating the proportion of $X_{j}$ that exceeds the boundary. In the scenario   \n151 of two variables where the threshold $h_{j_{1}}$ and $h_{j_{2}}$ divide the space into four regions, the proportions of   \n152 these areas are influenced by the covariance $\\sigma_{j_{1},j_{2}}$ , which connects the relation between the binarized   \n153 variables with the latent covariance. This approach allows us to initially estimate the threshold $h_{j_{1}}$ ,   \n154 $h_{j_{2}}$ of a pair of variables, followed by estimating the covariance $\\sigma_{j_{1},j_{2}}$ .   \n155 Let $\\mathbb{P}_{n}Z$ denote the average of a random variable $Z$ given $n$ i.i.d. observation of $Z$ and $E[Z]$ as the   \n156 true mean of $Z,\\mathbb{P}$ as the probability and $\\hat{P}$ as the empirical probability. We then define the boundary   \n157 $h_{j}$ as follows: for any single discretized variable $\\tilde{X}_{j}$ , there exists a constant $c_{j}$ such that: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{1}\\{\\tilde{x}_{j}^{i}>E[\\tilde{X}_{j}]\\}=\\mathbb{1}\\{g_{j}(x_{j}^{i})>c_{j}\\}=\\mathbb{1}\\{x_{j}^{i}>h_{j}\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "158 where $h_{j}=g_{j}^{-1}(c_{j})$ . Specifically, $h_{j}$ is the boundary in the original continuous domain to determine   \n159 if the discretized observation $\\tilde{X}_{k}$ is larger than its mean. When the continuous variable $X_{j}$ follows   \n160 a normal distribution, there is a relation $\\mathbb{P}(\\tilde{X}_{j}>E[\\tilde{X}_{j}])=1-\\Phi(h_{j})$ , where $\\Phi$ is the cumulative   \n161 distribution function (cdf) of a standard normal distribution. We then provide the following definition:   \n162 Definition 2.1. The estimated boundary can be expressed as $\\hat{h}_{j}\\;=\\;\\Phi^{-1}(1\\,-\\,\\hat{\\tau}_{j})$ , where $\\hat{\\tau}_{j}\\;=\\;$   \n163 $\\textstyle\\sum_{i=1}^{n}\\mathbb{1}_{\\{\\tilde{x}_{j}^{i}>\\mathbb{P}_{n}\\tilde{X}_{j}\\}}/n$ , serving as the estimation of $\\mathbb{P}(\\tilde{X}_{j}>E[\\tilde{X}_{j}])$ .   \n164 Let $\\bar{\\Phi}(z_{1},z_{2};\\rho)=\\mathbb{P}(Z_{1}>z_{1},Z_{2}>z_{2})$ , where $(Z_{1},Z_{2})^{T}$ follows a bivariate normal distribution   \n165 with mean zero, variance one and covariance $\\rho$ . We define ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tau_{j_{1},j_{2}}=\\mathbb{P}(\\tilde{x}_{j_{1}}^{i}>E[\\tilde{X}_{j_{1}}],\\tilde{x}_{j_{2}}^{i}>E[\\tilde{X}_{j_{2}}])=\\bar{\\Phi}(h_{j_{1}},h_{j_{2}};\\sigma_{j_{1},j_{2}}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "166 That is, the proportion of discretized variables larger than their mean can be expressed as a function   \n167 of underlying covariance. This equation serves as the key of estimating latent covariance based on the   \n168 discretized observations. Specifically, we can substitute those true parameters with their estimation   \n169 and construct the bridge equation to get the estimated covariance:   \n170 Definition 2.2 (Bridge Equation for A Discretized-Variable Pair). For discretized variables $\\tilde{X}_{j_{1}}$ and   \n171 $\\tilde{X}_{j_{2}}$ , the bridge equation is defined as: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathsf{\\Pi}_{j_{1},j_{2}}=\\hat{P}(\\tilde{X}_{j_{1}}>\\mathbb{P}_{n}\\tilde{X}_{j_{1}},\\tilde{X}_{j_{2}}>\\mathbb{P}_{n}\\tilde{X}_{j_{2}})=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{1}_{\\{\\tilde{x}_{j_{1}}^{i}>\\mathbb{P}_{n}\\tilde{X}_{j_{1}},\\tilde{x}_{j_{2}}^{i}>\\mathbb{P}_{n}\\tilde{X}_{j_{2}}\\}}=T(\\sigma_{j_{1},j_{2}};\\{\\hat{h}_{j_{1}},\\hat{h}_{j_{2}}\\}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "172 where $\\phi$ is the probability density function of a bivariate normal distribution, $\\hat{h}_{j_{1}},\\hat{h}_{j_{2}}$ can be simply   \n173 calculated using Def. 2.1.   \n174 Following the same intuition, we can directly apply the same bridge equation to estimate the co  \n175 variance of mixed pairs. The only difference is there is no need to estimate the boundary $\\hat{h}_{j}$ for the   \n176 continuous variable. Instead, we can incorporate its true mean of zero into the equation.   \n177 Definition 2.3 (Bridge Equation for A Continuous-Discretized-Variable Pair). For one continuous   \n178 variable $X_{j_{1}}$ and one discretized variable $\\tilde{X}_{j_{2}}$ , the bridge function is defined as follows: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{\\tau}_{j_{1},j_{2}}=\\hat{P}(X_{j_{1}}>0,\\tilde{X}_{j_{2}}>\\mathbb{P}_{n}\\tilde{X}_{j_{2}})=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{1}_{\\{x_{j_{1}}^{i}>0,\\tilde{x}_{j_{2}}^{i}>\\mathbb{P}_{n}\\tilde{X}_{j_{2}}\\}}=T(\\sigma_{j_{1},j_{2}};\\{0,\\hat{h}_{j_{2}}\\}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "179 and the function $T(\\cdot)$ has the same form of Def. 2.2. ", "page_idx": 4}, {"type": "text", "text": "180 A Bridge Equation for A Continuous-Variable Pair When there is no discretized transformation,   \n181 the sample covariance of $X_{j_{1}}$ and $X_{j_{2}}$ provides a consistent estimation. In this context, the function   \n182 $T$ acts merely as an identity mapping.   \n183 Definition 2.4 (A Bridge Equation for A Continuous-Variable Pair). For two continuous variables   \n184 $X_{j_{1}}$ and $X_{j_{2}}$ , the bridge equation is defined as: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\tau}_{j_{1},j_{2}}:=\\hat{\\sigma}_{j_{1},j_{2}}=\\frac{1}{n}\\sum_{i=1}^{n}x_{j_{1}}^{i}x_{j_{2}}^{i}-\\frac{1}{n}\\sum_{i=1}^{n}x_{j_{1}}^{i}\\frac{1}{n}\\sum_{i=1}^{n}x_{j_{2}}^{i}=T(\\sigma_{j_{1},j_{2}};\\emptyset).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "185 For two continuous variables $X_{j_{1}}$ and $X_{j_{2}}$ , the analytic solution of the estimated covariance can be   \n186 simply obtained using Def. 2.4.   \n187 Calculation of Estimated Covariance For the continuous case, the analytic solution of $\\hat{\\sigma}_{j_{1},j_{2}}$   \n188 can be simply obtained using Def. 2.4. For the cases involving the discretized variable as proposed   \n189 in Def. 2.2 and Def. 2.3, we can rely on the property that variance $\\Sigma$ only contains 1 among the   \n190 diagonal, which implies the covariance $\\sigma_{j_{1},j_{2}}$ should vary from $-1$ to 1. Thus, we can calculate the   \n191 estimated covariance by solving the objective ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\sigma_{j_{1},j_{2}}}||\\hat{\\tau}_{j_{1},j_{2}}-T(\\sigma_{j_{1},j_{2}};\\{\\hat{h}_{j_{1}},\\hat{h}_{j_{2}}\\})||^{2}\\quad s.t.-1<\\sigma_{j_{1},j_{2}}<1.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "192 The $\\hat{\\tau}_{j_{1},j_{2}}$ is a one-to-one mapping with calculated $\\hat{\\sigma}_{j_{1},j_{2}},\\hat{h}_{j_{1}}$ and $\\hat{h}_{j_{2}}$ , which is proved in App. A.2 ", "page_idx": 4}, {"type": "text", "text": "193 2.2 Unconditional Independence Test ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "194 The estimation of covariance $\\hat{\\sigma}_{j_{1},j_{2}}$ can be effectively solved using the designed bridge equation.   \n195 Now, we focus on deriving the distribution of $\\hat{\\sigma}_{j_{1},j_{2}}-\\sigma_{j_{1},j_{2}}$ . These results is used as an unconditional   \n196 independence test in the presence of the discretization. Moreover, Thm. 2.5, Lem. 2.6, Lem. 2.7   \n197 and Lem. 2.8 will be leveraged in the derivation process of the $\\mathrm{CI}$ test in Section 2.3. The detailed   \n198 derivation steps for both unconditional test and CI test are relatively intricate, therefore, we will   \n199 provide a general intuition. For a complete derivation, please refer to the App. A.3.   \n200 Assume we are interested in the true parameter $\\theta_{0}$ . We denote $\\hat{\\theta}$ as its estimation which is close to $\\theta_{0}$ ,   \n201 and $f(\\theta)$ is a continuous function. By leveraging Taylor expansion, we have ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nf(\\hat{\\theta})=f(\\theta_{0})+f^{\\prime}(\\theta_{0})(\\hat{\\theta}-\\theta_{0}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "202 which directly constructs the relationship between the estimated parameter with the true one. Re  \n203 arrange the term, we get $\\hat{\\theta}-\\theta_{0}=(f(\\hat{\\theta})\\overset{\\cdot}{-}f(\\theta_{0}))/f^{\\prime}(\\theta_{0})$ . If the denominator is a constant and the   \n204 numerator can be expressed as a sum of i.i.d samples, we can see $\\hat{\\theta}-\\theta_{0}$ will be asymptotically   \n205 normal according to the central limit theorem [35].   \n206 Let $\\psi_{\\hat{\\theta}}=[f_{\\hat{\\theta}}^{1}(\\cdot),f_{\\hat{\\theta}}^{2}(\\cdot),f_{\\hat{\\theta}}^{3}(\\cdot)]^{T}$ contains a group of functions parameterized by $\\hat{\\theta}$ (For discretized   \n207 pairs, $\\hat{\\theta}=(\\hat{\\sigma}_{j_{1},j_{2}},\\hat{h}_{j_{1}},\\hat{h}_{j_{2}}))$ . Define $\\mathbb{P}_{n}\\psi_{\\hat{\\theta}}$ as sample mean of these functions evaluated at $n$ sample   \n208 points. Similarly, $\\mathbb{P}_{n}\\boldsymbol{\\psi}_{\\hat{\\boldsymbol{\\theta}}}\\boldsymbol{\\psi}_{\\hat{\\boldsymbol{\\theta}}}^{\\textit{T}}$ is defined as sample mean of the outer product $\\psi_{\\hat{\\theta}}{\\psi_{\\hat{\\theta}}}^{T}$ . The notation   \n209 $P\\psi_{\\hat{\\theta}}:=E\\mathbb{P}_{n}\\psi_{\\hat{\\theta}}$ denotes the expectations of the functions in $\\psi_{\\hat{\\theta}}$ . Furthermore, let $\\psi_{\\hat{\\theta}}^{\\prime}$ denote the   \n210 derivative of the functions contained in $\\psi_{\\hat{\\theta}}$ . We now provide the main result of derived distribution   \n211 $\\hat{\\sigma}_{j_{1},j_{2}}-\\sigma_{j_{1},j_{2}}$ under the hull hypothesis that test pairs are independent.   \n212 Theorem 2.5 (Independence Test). In our settings, under the null hypothesis that two observed   \n213 variables indexed with $j_{1}$ and $j_{2}$ are statistically independent under our framework, i.e., $\\sigma_{j_{1},j_{2}}=0$ ,   \n214 the independence can be tested using the statistic ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\sigma}_{j_{1},j_{2}}=T^{-1}(\\hat{\\tau}_{j_{1},j_{2}};\\hat{\\theta}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "215 This statistic is approximated to follow a normal distribution, as detailed below: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{\\sigma}_{j_{1},j_{2}}\\overset{a p p r o x}{\\sim}N\\left(0,\\frac{1}{n}((\\mathbb{P}_{n}\\psi_{\\hat{\\theta}}^{\\prime})^{-1}\\mathbb{P}_{n}\\psi_{\\hat{\\theta}}\\psi_{\\hat{\\theta}}^{T}(\\mathbb{P}_{n}\\psi_{\\hat{\\theta}}^{\\prime T})^{-1})_{1,1}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "216 where the specific form of $\\psi_{\\hat{\\theta}}$ are presented in Lem. 2.6,Lem. 2.7 and Lem. 2.8. ", "page_idx": 5}, {"type": "text", "text": "217 We now provide the specific forms of $\\psi_{\\hat{\\theta}}$ . Since the variables being tested for independence can be   \n218 both discretized, only one being discretized, or neither being discretized. This results in different   \n219 forms of $\\psi_{\\hat{\\theta}}$ consequently differs across these scenarios. Let $Z_{j_{1}}$ and $Z_{j_{2}}$ be any two random   \n220 variables indexed by $j_{1}$ and $j_{2}$ . Let $\\hat{\\sigma}_{j_{1},j_{2}}^{i}=z_{j_{1}}^{i}\\cdot z_{j_{2}}^{i}-\\mathbb{P}_{n}Z_{j_{1}}\\cdot\\mathbb{P}_{n}Z_{j_{2}}$ denote the sample covariance   \n221 based on a $i$ -th pairwise observation of the variables $Z_{j_{1}}$ and $Z_{j_{2}}$ . Let $\\hat{\\tau}_{j_{1}}^{i}\\,=\\,\\mathbb{1}_{\\{z_{j_{1}}^{i}>\\mathbb{P}_{n}Z_{j_{1}}\\}}$ and   \n222 $\\hat{\\tau}_{j_{2}}^{i}\\ =\\ \\mathbb{1}_{\\{Z_{j_{2}}^{i}>\\mathbb{P}_{n}Z_{j_{2}}\\}}$ , each calculated based on $i$ -th observations of the variables $Z_{j_{1}}$ and $Z_{j_{2}}$ ,   \n223 respectively. Let $\\hat{\\tau}_{j_{1},j_{2}}^{i}$ be $\\hat{\\tau}_{j_{1}}^{i}\\cdot\\hat{\\tau}_{j_{2}}^{i}$ . We further denote $\\bar{\\Phi}(\\cdot)=1-\\Phi(\\cdot)$ . The different forms of $\\psi_{\\hat{\\theta}}$   \n224 that arise in different cases are defined as follows: ", "page_idx": 5}, {"type": "text", "text": "225 Lemma 2.6. $(\\psi_{\\hat{\\theta}}$ for A Continuous-Variable Pair). For two continuous variables $X_{j_{1}}$ and $X_{j_{2}}$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\psi_{\\hat{\\theta}}:=\\hat{\\sigma}_{j_{1},j_{2}}^{i}-\\hat{\\sigma}_{j_{1},j_{2}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "226 Lemma 2.7 ( $\\psi_{\\hat{\\theta}}$ for A Discretized-Variable Pair). For discretized variables $\\tilde{X}_{j_{1}}$ and $\\tilde{X}_{j_{2}}$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\psi_{\\hat{\\theta}}:=\\left(\\begin{array}{c}{\\hat{\\tau}_{j_{1},j_{2}}^{i}-T(\\hat{\\sigma}_{j_{1},j_{2}};\\{\\hat{h}_{j_{1}},\\hat{h}_{j_{2}}\\})}\\\\ {\\hat{\\tau}_{j_{1}}^{i}-\\bar{\\Phi}(\\hat{h}_{j_{1}})}\\\\ {\\hat{\\tau}_{j_{2}}^{i}-\\bar{\\Phi}(\\hat{h}_{j_{2}})}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "227 Lemma 2.8 $\\psi_{\\hat{\\theta}}$ for A Continuous-Discretized-Variable Pair). For one discretized variable $\\tilde{X}_{j_{2}}$ and   \n228 one continuous variable $X_{j_{1}}$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\psi_{\\hat{\\theta}}:=\\left(\\hat{\\tau}_{j_{1},j_{2}}^{i}-T(\\hat{\\sigma}_{j_{1},j_{2}};\\{0,\\hat{h}_{j_{2}})\\}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "229 Derivation of forms of $\\psi_{\\hat{\\theta}}$ for different cases and their corresponding distribution defined in Eq (6)   \n230 can be found in App. A.4, App. A.5, App. A.6. Up to this point, our discussion has been confined to   \n231 the case of covariance $\\sigma_{j_{1},j_{2}}$ , the indicator of unconditional independence. In the next section, we   \n232 will present the results of our CI test. ", "page_idx": 5}, {"type": "text", "text": "233 2.3 Conditional Independence (CI) Test ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "234 To construct a $\\mathrm{CI}$ test of our model, we are interested at two things: calculation of the estimated   \n235 precision coefficient $\\hat{\\omega}_{j,k}$ and the derivation of the corresponding distribution $\\hat{\\omega}_{j,k}-\\omega_{j,k}$ . In the   \n236 following, we first build $\\beta_{j,k}$ , which is obtained using nodewise regression and show it serves as a   \n237 surrogate of testing for $\\omega_{j,k}=0$ , we then construct the formulation of $\\hat{\\beta}_{j,k}-\\beta_{j,k}$ as the combination   \n238 of formulation of $\\hat{\\sigma}_{j_{1},j_{2}}-\\sigma_{j_{1},j_{2}}$ and show it will also be asymptotically normal.   \n239 Nodewise Regression for CI To utilize covariance for testing $\\mathrm{CI}$ , it is necessary to establish a   \n240 relationship between the estimated covariance and a metric capable of reflecting CI. To achieve this,   \n241 we employ the nodewise regression which effectively builds the connection between covariance   \n242 and precision matrix. Suppose we can access observations $\\{\\pmb{x}^{1},\\pmb{x}^{2},\\dots,\\pmb{x}^{n}\\}$ from latent continuous   \n243 variables $\\pmb{X}=(X_{1},\\pmb{\\mathscr{s}}...\\,,X_{p})\\sim N(0,\\pmb{\\Sigma})$ , nodewise regression will do regression on every dimension   \n244 with all other dimensions as predictors. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\nx_{j_{1}}^{i}=\\sum_{j_{1}\\neq j_{2}}x_{j_{2}}^{i}\\beta_{j}+\\epsilon_{j_{1}}^{i}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "245 It can be shown that there are deterministic relationships between the regression coefficients and the   \n246 covariance and precision matrices of $\\mathbf{\\deltaX}$ , as illustrated below and proved in App. A.7.1. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\beta_{j}=\\Sigma_{-j-j}^{-1}\\Sigma_{-j j}\\in\\mathbb{R}^{p-1},\\quad\\beta_{j,k}=-\\frac{\\omega_{j,k}}{\\omega_{j,j}},\\quad j\\neq k,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "247 where $\\Sigma_{-j-j}$ is the submatrix of $\\Sigma$ without $j$ th column and $j$ th row, and the $\\Sigma_{-j j}$ is the vector of $j$ th   \n248 column without $j$ th row. $\\beta_{j,k}\\in\\mathbb{R}$ is the surrogate of $\\omega_{j,k}$ to capture the independence relationship of   \n249 $X_{j}$ with $X_{k}$ conditioning on other variables. We can use Def. 2.2, Def. 2.3 and Def. 2.4 to get the   \n250 estimation $\\hat{\\Sigma}_{-j-j}$ and $\\hat{\\Sigma}_{-j j}$ and thus get the estimation $\\hat{\\beta}_{j}$ .   \n251 Statistical Inference for $\\beta_{j,k}$ Nodewise regression offers a robust solution for the estimation   \n252 problem. A pertinent inquiry pertains to the construction of the distribution of $\\hat{\\beta}_{j}-\\beta_{j}$ . It is crucial   \n253 to recognize that the distribution of $\\hat{\\sigma}_{j_{1},j_{2}}\\,-\\,\\sigma_{j_{1},j_{2}}$ is already established. Therefore, if we can   \n254 conceptualize $\\hat{\\beta}_{j}-\\beta_{j}$ as a linear combination of $\\hat{\\sigma}_{j_{1},j_{2}}-\\sigma_{j_{1},j_{2}}$ , the problem is directly solved, i.e.,   \n255 the $\\hat{\\beta}_{j}-\\beta_{j}$ is linear combination of dependent Gaussian variables. The underlying relationship   \n256 between these variables is as follows: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\beta}_{j}-\\beta_{j}=-\\hat{\\Sigma}_{-j-j}^{-1}\\left((\\hat{\\Sigma}_{-j-j}-\\Sigma_{-j-j})\\beta_{j}-(\\hat{\\Sigma}_{-j j}-\\Sigma_{-j j})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "257 The derivation is provided in App. A.7.2. For ease of notation, we further express the distribution of   \n258 the difference between the estimated covariance and the true covariance as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{\\sigma}_{j_{1},j_{2}}-\\sigma_{j_{1},j_{2}}=\\frac{1}{n}\\sum_{i=1}^{n}\\xi_{j_{1},j_{2}}^{i}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "259 The specific form of $\\xi_{j_{1},j_{2}}^{i}$ is given in App. A.4, A.5, A.6 respectively for different cases. For   \n260 notational convenience, we express $\\begin{array}{r}{\\hat{\\Sigma}_{-j-j}\\,-\\,\\Sigma_{-j-j}\\;=\\;\\frac{1}{n}\\sum_{i=1}^{n}\\Xi_{-j,-j}^{i}\\;}\\end{array}$ and $\\hat{\\Sigma}_{-j j}\\,-\\,\\Sigma_{-j j}\\,=$   \n261 $\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}\\Xi_{-j,j}^{i}}\\end{array}$ , where $\\xi_{j_{1},j_{2}}$ is the element of the matrix $\\Xi$ at the position indexed by $(j_{1},j_{2})$ . We   \n262 now propose the statistic and its asymptotic distribution for the CI test in the following theorem.   \n263 Theorem 2.9 (Conditional Independence test). In our settings, under the null hypothesis that $X_{j}$ and   \n264 $X_{k}$ are conditional statistically independent given a set of variables $_{z}$ , i.e., $\\beta_{j,k}=0$ , the statistic ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{\\beta}_{j,k}=(\\hat{\\Sigma}_{-j-j}^{-1}\\hat{\\Sigma}_{-j j})_{[k]},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "265 where $[k]$ denotes the element corresponding to the variable $X_{k}$ in $\\hat{\\Sigma}_{-j-j}^{-1}\\hat{\\Sigma}_{-j}$ . The statistic $\\hat{\\beta}_{j,k}$   \n266 has the asymptotic distribution: ", "page_idx": 6}, {"type": "text", "text": "267 ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\hat{\\beta}_{j,k}\\sim N(0,a^{[k]}^{T}\\frac{1}{n^{2}}\\displaystyle\\sum_{i=1}^{n}v e c(B_{-j}^{i})v e c(B_{-j}^{i})^{T})a^{[k]}),}\\\\ {w h e r e\\ B^{i}=\\left[\\Xi_{-j,j}^{i}\\right],\\quad a_{l}^{[k]}=\\left\\{\\left(\\hat{\\Sigma}_{-j-j}^{-1}\\right)_{[k],l},\\right.}\\\\ {\\left.\\sum_{q=1}^{n}\\left(\\hat{\\Sigma}_{-j-j}^{-1}\\right)_{[k],l}\\left(\\tilde{\\beta}_{j}\\right)_{q},\\quad f o r\\ l\\in\\{p,\\ldots,p^{2}-p\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "268 and $\\tilde{\\beta}_{j}$ is $\\beta_{j}$ whose $\\beta_{j,k}=0$ . ", "page_idx": 6}, {"type": "text", "text": "269 In practice, we can plug in the estimation of regression parameter $\\hat{\\beta}_{j}$ and set $\\hat{\\beta}_{j,k}\\,=\\,0$ as the   \n270 substitution of $\\tilde{\\beta}_{j}$ to calculate the variance and do the CI test. Specifically, we can obtain the $\\hat{\\beta}_{j,k}$   \n271 using Eq. (13) where the estimated covariance terms can be calculated by solving the bridge equation   \n272 Eq. 2. Under the null hypothesis that $\\beta_{j,k}=0$ (conditional independence), we can take the calculated   \n273 $\\hat{\\beta}_{j,k}$ into the distribution defined in Thm. 2.9 and obtain the $\\mathfrak{p}$ -value. If the $\\mathbf{p}$ -value is smaller than the   \n274 predefined significance level $\\alpha$ (normally set at 0.05), we will infer the tested pairs are conditionally   \n275 dependent; otherwise, we do not. The detailed derivation of the Thm. 2.9 can be found in App. A.7.2. ", "page_idx": 6}, {"type": "text", "text": "276 3 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "277 We applied the proposed method DCT to synthetic data to evaluate its practical performance and   \n278 compare it with Fisher-Z test [14] (for all three data types) and Chi-Square test [15] (for discrete data   \n279 only) as baselines. Specifically, we investigated its Type I and Type II error and its application in   \n280 causal discovery. The experiments investigating its robustness, performance in denser graphs and   \n281 effectiveness in a real-world dataset can be found in App. C. ", "page_idx": 6}, {"type": "text", "text": "282 3.1 On the Effect of the Cardinality of Conditioning Set and the Sample Size ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "283 Our experiment investigates the variations in Type I and Type II error (1 minus power) probabilities   \n284 under two conditions. In the first scenario, we focus on the effects of modifying the sample size,   \n285 denoted as $n=(100,500,1000,2000)$ , while conditioning on a single variable. In the second, the   \n286 sample size is held constant at 2000, and we vary the cardinality of the conditioning set, represented   \n287 as $D=(1,2,\\ldots,5)$ . It is assumed that every variable within this conditioning set is effective, i.e.,   \n288 they influence the CI of the tested pairs. We repeat each test 1500 times.   \n289 We use $Y,W$ to denote the variables being tested and use $Z$ to denote the variables being conditioned   \n290 on. The discretized versions of the variables are denoted with a tilde symbol (e.g., $\\tilde{Z}$ ). For both con  \n291 ditions, we evaluate three distinct types of observations of tested variables: continuous observations   \n292 for both variables $(Y,W)$ , discrete observations for both variables $(\\tilde{Y},\\tilde{W})$ and a mixed type $(\\tilde{Y},W)$ .   \n293 The variables in the conditioning set will always be discretized observations $(\\tilde{Z})$ .   \n294 To see how well the derived asymptotic null distribution approximates the true one, we verify if   \n295 the probability of Type I error aligns with the significance level $\\alpha$ preset in advance. We generate   \n296 true continuous multivariate Gaussian data $Y,W$ from $Z_{i}$ (single $i=1$ for the first scenario, and   \n297 summed over $n$ for the second), structured as $a_{i}Z_{i}+E$ and in=1 aiZi + E, where ai is sampled   \n298 from $U(0.5,1.5)$ and $E$ follows a standard normal distributi on, independent of all other variables.   \n299 This ensures $Y\\perp\\!\\!\\!\\perp W|Z$ . The data are then discretized into $K=(2,4,8,12)$ levels, with boundaries   \n300 randomly set based on the variable range. The first column in Fig. 2 (a) (b) shows the resulting   \n301 probability of Type I errors at the significance level $\\alpha=0.05$ compared with other methods.   \n302 A good test should have as small a probability of Type II error as possible, i.e., a larger power. To   \n303 test the power of our DCT, we generate the continuous multivariate Gaussian data $Z_{i}$ from $Y,W$ ;   \n304 constructed as $Z_{i}=a_{i}Y+b_{i}W+E$ , where $a_{i},b_{i}$ are sampled from $U(0.5,1.5)$ and $E$ follows a   \n305 standard normal distribution independent with all others, i.e., $Y$ \u0338\u22a5\u22a5 $W|Z$ . The same discretization   \n306 approach is applied here. The second column in Fig. 2 (a) (b) shows the Type II error with the   \n307 changing number of samples and cardinality of the conditioning set compared with other methods.   \n308 From Fig. 2 (a), we note that the Type I error rates with our derived null distribution are well  \n309 approximated at 0.05 across all three data types in both scenarios. In contrast, other testing methods   \n310 show significantly higher Type I error rates, increasing with the number of samples and the size of   \n311 the conditioning set. This indicates that such methods are more prone to erroneously concluding   \n312 that tested variables are conditionally dependent. Additionally, while alternative tests demonstrate   \n313 considerable power with smaller sample sizes, our approach requires a sample size of 2000 to achieve   \n314 satisfactory power, particularly in mixed and continuous cases. A possible explanation for this   \n315 phenomenon is that our method binarizes discretized data, which may not effectively utilize all   \n316 observations. This aspect warrants further investigation in future research. Moreover, our test shows   \n317 remarkable stability in response to changes in the number of conditioning sets. ", "page_idx": 6}, {"type": "image", "img_path": "B1tCaKP5nB/tmp/9ef134f55e86b3656404f9b4e7dee335f8acc94d2be7820414c63b485f0441ad.jpg", "img_caption": ["Figure 2: Comparison of results of Type I and Type II error (1 \u2212power) for all three types of tested data (continuous, mixed, discrete) and different number of samples and cardinality of conditioning set. The suffix attached to a test\u2019s name denotes the cardinality of discretization; for example, \"Fsherz_4\" signifies the application of the Fsher-z test to data discretized into four levels. Chi-square test is only applicable for the discrete case. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "B1tCaKP5nB/tmp/49a78a4b7ba8acae2939e409371a1a89c2d2b907be032439ceb95e3f0904bd38.jpg", "img_caption": ["(b) fixed sample size $n=5000$ , changing node $p=(4,6,8,10)$ "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 3: Experiment result of skeleton discovery on synthetic data for changing sample size (a) and changing number of nodes (b). Fisherz_nodis is the Fisher-z test applied to original continuous data. We evaluate $F_{1}$ (\u2191), Precision $(\\uparrow)$ , Recall (\u2191) and SHD (\u2193). ", "page_idx": 8}, {"type": "text", "text": "318 3.2 Application in Causal Discovery ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "319 Causal discovery aims to recover the true causal structure from the data. Constraint-based causal   \n320 discovery methods like the PC algorithm [30] rely on testing CI from observations to discover causal   \n321 graphs. However, in the presence of discretization, failures in testing CI leads to false conclusions   \n322 about causal graphs. To evaluate the efficacy of the DCT, we construct causal graphs utilizing the   \n323 Bipartite Pairing (BP) model as detailed in [2], with the number of edges being one fewer than   \n324 the number of nodes. The detailed generation process is provided in App. B due to limited space.   \n325 Our experiment is divided into two scenarios: (a) fixed data samples $n\\,=\\,5000$ , with changing   \n326 number of nodes $\\ensuremath{p}\\,=\\,(4,6,8,10)$ ; (b) fixed number of nodes $p\\,=\\,8$ and changing sample sizes   \n327 $n=(500,1000,5000,10000)$ .   \n328 Comparative analysis is conducted using the PC algorithm integrated with different testing methods.   \n329 Specifically, we compare DCT against the Fisher-Z test applied to discretized data, the chi-square   \n330 test, and the Fisher-Z test on original continuous data, the latter serving as a theoretical upper bound   \n331 for comparison. Since the PC algorithm can only return a completed partially directed acyclic graph   \n332 (CPDAG), we use the same orientation rules [11] implemented by Causal-DAG [6] to convert a   \n333 CPDAG into a DAG. We evaluate both the undirected skeleton and the directed graph using criteria   \n334 such as structural Hamming distance (SHD), F1 score, precision, and recall. For each setting, we   \n335 run 10 graph instances with different seeds and report the mean and standard deviation of skeleton   \n336 discovery in Fig. 3, and DAG in Fig. 4 in App B.   \n337 According to the result, DCT exhibits performance nearly on par with the theoretical upper bound   \n338 across metrics such as F1 score, precision, and Structural Hamming Distance (SHD) when the number   \n339 of variables $(p)$ is small and the sample size $(n)$ is large. Despite a decline in performance as the   \n340 number of variables increases with a smaller sample size, DCT significantly outperforms both the   \n341 Fisher-Z test and the Chi-square test. Notably, in almost all settings, the recall of DCT is lower than   \n342 that of the baseline tests, which is a reasonable outcome since these tests tend to infer conditional   \n343 dependencies, thereby retaining all edges given the discretized observations. For instance, a fully   \n344 connected graph, would achieve a recall of 1. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "345 4 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "346 In this paper, we present a new testing method tailored for scenarios commonly encountered in   \n347 real-world applications, where variables, though inherently continuous, are only observable in their   \n348 discretized forms. Our method distinguishes itself from existing CI tests by effectively mitigating the   \n349 misjudgment introduced by discretization and accurately recovering the CI relationships of latent   \n350 continuous variables. We substantiate our approach with theoretical results and empirical validation,   \n351 underscoring the effectiveness of our testing methods. ", "page_idx": 8}, {"type": "text", "text": "352 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "353 [1] Constantin F Aliferis, Alexander Statnikov, Ioannis Tsamardinos, Subramani Mani, and Xenofon D   \n354 Koutsoukos. Local causal and markov blanket induction for causal discovery and feature selection for   \n355 classification part i: algorithms and empirical evaluation. Journal of Machine Learning Research, 11(1),   \n356 2010.   \n357 [2] Armen S Asratian, Tristan MJ Denley, and Roland H\u00e4ggkvist. Bipartite graphs and their applications,   \n358 volume 131. Cambridge university press, 1998.   \n359 [3] Kunihiro Baba, Ritei Shibata, and Masaaki Sibuya. Partial correlation and conditional correlation as   \n360 measures of conditional independence. Australian & New Zealand Journal of Statistics, 46(4):657\u2013664,   \n361 2004.   \n362 [4] Kunihiro Baba, Ritei Shibata, and Masaaki Sibuya. Partial correlation and conditional correlation as   \n363 measures of conditional independence. Australian & New Zealand Journal of Statistics, 46(4):657\u2013664,   \n364 2004.   \n365 [5] Laurent Callot, Mehmet Caner, Esra Ulasan, and A. \u00d6zlem \u00d6nder. A nodewise regression approach to   \n366 estimating large portfolios, 2019.   \n367 [6] Chandler Squires. causaldag: creation, manipulation, and learning of causal models, 2018.   \n368 [7] Hu Changsheng and Wang Yongfeng. Investor sentiment and assets valuation. Systems Engineering   \n369 Procedia, 3:166\u2013171, 2012.   \n370 [8] Aswath Damodaran. Investment valuation: Tools and techniques for determining the value of any asset,   \n371 volume 666. John Wiley & Sons, 2012.   \n372 [9] A Philip Dawid. Conditional independence in statistical theory. Journal of the Royal Statistical Society   \n373 Series B: Statistical Methodology, 41(1):1\u201315, 1979.   \n374 [10] Simon Dooms, Toon De Pessemier, and Luc Martens. Movietweetings: a movie rating dataset collected   \n375 from twitter. In Workshop on Crowdsourcing and human computation for recommender systems, CrowdRec   \n376 at RecSys, volume 2013, page 43, 2013.   \n377 [11] Dorit Dor and Michael Tarsi. A simple algorithm to construct a consistent extension of a partially oriented   \n378 graph. 1992.   \n379 [12] Gary Doran, Krikamol Muandet, Kun Zhang, and Bernhard Sch\u00f6lkopf. A permutation-based kernel   \n380 conditional independence test. In UAI, pages 132\u2013141, 2014.   \n381 [13] Jianqing Fan, Han Liu, Yang Ning, and Hui Zou. High dimensional semiparametric latent graphical model   \n382 for mixed data. Journal of the Royal Statistical Society Series B: Statistical Methodology, 79(2):405\u2013421,   \n383 2017.   \n384 [14] Ronald Aylmer Fisher. On the \"Probable Error\" of a Coefficient of Correlation Deduced from a Small   \n385 Sample. Metron, 1:3\u201332, 1921.   \n386 [15] Karl Pearson F.R.S. X. on the criterion that a given system of deviations from the probable in the case of   \n387 a correlated system of variables is such that it can be reasonably supposed to have arisen from random   \n388 sampling. Philosophical Magazine Series 1, 50:157\u2013175, 2009.   \n389 [16] Kenji Fukumizu, Francis R Bach, and Michael I Jordan. Dimensionality reduction for supervised learning   \n390 with reproducing kernel hilbert spaces. Journal of Machine Learning Research, 5(Jan):73\u201399, 2004.   \n391 [17] Sverre Urnes Johnson, P\u00e5l Gunnar Ulvenes, Tuva \u00d8ktedalen, and Asle Hoffart. Psychometric properties   \n392 of the general anxiety disorder 7-item (gad-7) scale in a heterogeneous psychiatric sample. Frontiers in   \n393 psychology, 10:449461, 2019.   \n394 [18] D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques. Adaptive   \n395 computation and machine learning. MIT Press, 2009.   \n396 [19] Loka Li, Ignavier Ng, Gongxu Luo, Biwei Huang, Guangyi Chen, Tongliang Liu, Bin Gu, and Kun Zhang.   \n397 Federated causal discovery from heterogeneous data, 2024.   \n398 [20] Han Liu, John Lafferty, and Larry Wasserman. The nonparanormal: Semiparametric estimation of high   \n399 dimensional undirected graphs. Journal of Machine Learning Research, 10(10), 2009.   \n400 [21] Dimitris Margaritis. Distribution-free learning of bayesian network structure in continuous domains. In   \n401 AAAI, volume 5, pages 825\u2013830, 2005.   \n402 [22] Karthik Mohan, Mike Chung, Seungyeop Han, Daniela Witten, Su-In Lee, and Maryam Fazel. Structured   \n403 learning of gaussian graphical models. Advances in neural information processing systems, 25, 2012.   \n404 [23] Sarah A Mossman, Marissa J Luft, Heidi K Schroeder, Sara T Varney, David E Fleck, Drew H Barzman,   \n405 Richard Gilman, Melissa P DelBello, and Jeffrey R Strawn. The generalized anxiety disorder 7-item   \n406 (gad-7) scale in adolescents with generalized anxiety disorder: signal detection and validation. Annals of   \n407 clinical psychiatry: official journal of the American Academy of Clinical Psychiatrists, 29(4):227, 2017.   \n408 [24] Judea Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, 2000.   \n409 [25] Christine Peterson, Francesco C Stingo, and Marina Vannucci. Bayesian inference of multiple gaussian   \n410 graphical models. Journal of the American Statistical Association, 110(509):159\u2013174, 2015.   \n411 [26] Zhao Ren, Tingni Sun, Cun-Hui Zhang, and Harrison H Zhou. Asymptotic normality and optimalities in   \n412 estimation of large gaussian graphical models. 2015.   \n413 [27] Rajat Sen, Ananda Theertha Suresh, Karthikeyan Shanmugam, Alexandros G Dimakis, and Sanjay   \n414 Shakkottai. Model-powered conditional independence test. Advances in neural information processing   \n415 systems, 30, 2017.   \n416 [28] Shohei Shimizu, Takanori Inazumi, Yasuhiro Sogawa, Aapo Hyvarinen, Yoshinobu Kawahara, Takashi   \n417 Washio, Patrik O. Hoyer, and Kenneth Bollen. Directlingam: A direct method for learning a linear   \n418 non-gaussian structural equation model, 2011.   \n419 [29] E Isaac Sparling and Shilad Sen. Rating: how difficult is it? In Proceedings of the ffith ACM conference on   \n420 Recommender systems, pages 149\u2013156, 2011.   \n421 [30] P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search. MIT press, 2nd edition, 2000.   \n422 [31] Eric V Strobl, Kun Zhang, and Shyam Visweswaran. Approximate kernel-based conditional independence   \n423 tests for fast non-parametric causal discovery. Journal of Causal Inference, 7(1):20180017, 2019.   \n424 [32] Liangjun Su and Halbert White. A nonparametric hellinger metric test for conditional independence.   \n425 Econometric Theory, 24(4):829\u2013864, 2008.   \n426 [33] A. W. van der Vaart. M\u2013and Z-Estimators, page 41\u201384. Cambridge Series in Statistical and Probabilistic   \n427 Mathematics. Cambridge University Press, 1998.   \n428 [34] A. W. van der Vaart. Stochastic Convergence, page 5\u201324. Cambridge Series in Statistical and Probabilistic   \n429 Mathematics. Cambridge University Press, 1998.   \n430 [35] Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000.   \n431 [36] Eric P Xing, Michael I Jordan, Richard M Karp, et al. Feature selection for high-dimensional genomic   \n432 microarray data. In Icml, volume 1, pages 601\u2013608. Citeseer, 2001.   \n433 [37] Ming Yuan and Yi Lin. Model selection and estimation in the gaussian graphical model. Biometrika,   \n434 94(1):19\u201335, 2007.   \n435 [38] Kun Zhang, Jonas Peters, Dominik Janzing, and Bernhard Sch\u00f6lkopf. Kernel-based conditional indepen  \n436 dence test and application in causal discovery. arXiv preprint arXiv:1202.3775, 2012.   \n437 [39] Yishi Zhang, Zigang Zhang, Kaijun Liu, and Gangyi Qian. An improved iamb algorithm for markov   \n438 blanket discovery. J. Comput., 5(11):1755\u20131761, 2010.   \n439 [40] Yujia Zheng, Biwei Huang, Wei Chen, Joseph Ramsey, Mingming Gong, Ruichu Cai, Shohei Shimizu,   \n440 Peter Spirtes, and Kun Zhang. Causal-learn: Causal discovery in python, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "442 443 \u201cA Conditional Independence Test in the Presence of Discretization\u201d ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "444 Appendix organization:   \n445   \n446 A Proof of Things 12   \n447 A.1 Proof of $\\hat{\\theta}\\stackrel{p}{\\to}\\theta_{0}$ 12   \n448 A.2 Proof of one-to-one mapping between $\\hat{\\tau}_{j_{1},j_{2}}$ with $\\hat{\\sigma}_{j_{1},j_{2}}$ 13   \n449 A.3 Proof of Thm. 2.5 13   \n450 A.4 Derivation of Lem. 2.7 14   \n451 A.5 Derivation of Lem. 2.8 15   \n452 A.6 Derivation of Lem. 2.6 16   \n453 A.7 Proof of Thm. 2.9 . 16   \n454 A.7.1 Proof of Relation between $\\Sigma$ , $\\Omega$ with $\\beta$ 16   \n455 A.7.2 Detailed derivation of inference for $\\beta_{j}$ 17   \n456 A.8 Discussion of assumption of zero mean and identity variance 19 ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "457 B Data Generation and Figure of main experiments: causal discovery 20 ", "page_idx": 11}, {"type": "text", "text": "458 C Additional experiments 21   \n459 C.1 Linear non-Gaussian and nonlinear 21   \n460 C.2 Denser graph 21   \n461 C.3 multivariate Gaussian with nonzero mean and non-unit variance 21   \n462 C.4 Real-world dataset 22 ", "page_idx": 11}, {"type": "text", "text": "463 D Related Work 24 ", "page_idx": 11}, {"type": "text", "text": "464 E Resource Usage 25 ", "page_idx": 11}, {"type": "text", "text": "465 F Limiation and Broader Impacts ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "25 ", "page_idx": 11}, {"type": "text", "text": "467 ", "page_idx": 11}, {"type": "text", "text": "468 A Proof of Things ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "469 A.1 Proof of $\\hat{\\theta}\\xrightarrow{p}\\theta_{0}$ ", "page_idx": 11}, {"type": "text", "text": "470 Lemma A.1. For the estimation $\\hat{\\theta}$ which is calculated using bridge equation 2.4 2.2 2.3,   \n471 as a zero of $\\Psi_{n}$ defined in Eq. (26),(33), (36) , will converge in probability to $\\theta_{0}\\quad=$   \n472 $(\\sigma_{j_{1},j_{2}},h_{j_{1}},h_{j_{2}}),(\\sigma_{j_{1},j_{2}},h_{j_{2}}),(\\sigma_{j_{1},j_{2}})$ respectively.   \n473 Proof We first focus on the most challenging one where both variables are discrete. According to   \n474 the law of large numbers, for the estimated boundary $\\hat{h}_{j_{1}}$ and $\\hat{h}_{j_{2}}$ whose calculations are defined as ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "475 $\\hat{h}_{j}=\\Phi^{-1}(1-\\hat{\\tau}_{j})$ , we should have ", "page_idx": 12}, {"type": "equation", "text": "$$\nn\\to\\infty,\\quad\\hat{\\tau}_{j}=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{1}_{\\{\\tilde{x}_{j}^{i}>\\mathbb{P}_{n}\\tilde{X}_{j}\\}}\\stackrel{p}{\\rightarrow}\\mathbb{P}(\\tilde{X}_{j}>E[\\tilde{X}_{j}]).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "476 Recall the definition $\\mathbb{P}(\\tilde{X}_{j}>E[\\tilde{X}_{j}])=1-\\Phi(h_{j})$ , according to continuous mapping theorem [34],   \n477 as long as the function $\\Phi^{-1}(1-\\cdot)$ is continuous, we should have $\\hat{h}_{j}\\xrightarrow{p}h_{j}$ . And thus $\\hat{h}_{j_{1}}\\xrightarrow{p}h_{j_{1}}$ ,   \n478 $\\hat{h}_{j_{2}}\\xrightarrow{p}h_{j_{2}}$ .   \n479 We have $\\hat{\\tau}_{j1,j_{2}}=\\bar{\\Phi}(\\hat{h}_{j1},\\hat{h}_{j2},\\hat{\\sigma}_{j1,j2})$ and the estimation $\\hat{\\sigma}_{j_{1},j_{2}}$ can be obtained through solving the   \n480 function. Similarly, we also have ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\n\\i\\to\\infty,\\quad\\hat{\\tau}_{j_{1},j_{2}}=\\frac{1}{n}\\sum_{i=1}^{n}{\\mathbb{I}_{\\{\\tilde{x}_{j_{1}}\\>\\mathbb{P}_{n}\\tilde{X}_{j_{1}}\\}}}{\\mathbb{I}_{\\{\\tilde{x}_{j_{2}}^{i}>\\mathbb{P}_{n}\\tilde{X}_{j_{2}}\\}}}\\xrightarrow{p}{\\mathbb{P}}(\\tilde{x}_{j_{1}}^{i}>E[\\tilde{X}_{j_{1}}],\\tilde{x}_{j_{2}}^{i}>E[\\tilde{X}_{j_{2}}])={\\tau}_{j_{1},j_{2}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "481 Similarly, according to the continuous mapping theorem, we have $\\hat{\\sigma}_{j_{1},j_{2}}\\ \\stackrel{p}{\\to}\\ \\sigma_{j_{1},j_{2}}$ . Thus, the   \n482 parameter $(\\hat{\\sigma}_{j_{1},j_{2}},\\hat{h}_{j_{1}},\\hat{h}_{j_{2}})\\overset{p}{\\rightarrow}(\\sigma_{j_{1},j_{2}},h_{j_{1}},h_{j_{2}})$ .   \n483 Apparently, the result above could easily extend to the mixed case where we fix $\\hat{h}_{1}=h_{1}=0$ . Using   \n484 the same procedure, we should have $(\\hat{\\sigma}_{j_{1},j_{2}},\\hat{h}_{j_{2}})\\overset{p}{\\rightarrow}(\\sigma_{j_{1},j_{2}},h_{j_{2}})$ .   \n485 For the continuous case whose estimated variance is calculated as $\\begin{array}{r l r}{\\hat{\\sigma}_{j_{1},j_{2}}\\!\\!}&{{}=}&{\\!\\!\\frac{1}{n}\\sum_{i=1}^{n}x_{j_{1}}^{i}x_{j_{2}}^{i}\\;-}\\end{array}$   \n486 $\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}x_{j_{1}}^{i}\\frac{1}{n}\\sum_{i=1}^{n}x_{j_{2}}^{i}}\\end{array}$ in=1 xij2., according to law of large numbers, we should have ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\n\\nu\\to\\infty,\\quad\\hat{\\sigma}_{j_{1},j_{2}}=\\frac{1}{n}\\sum_{i=1}^{n}x_{j_{1}}^{i}x_{j_{2}}^{i}-\\frac{1}{n}\\sum_{i=1}^{n}x_{j_{1}}^{i}\\frac{1}{n}\\sum_{i=1}^{n}x_{j_{2}}^{i}\\to E(X_{j_{1}}X_{j_{2}})-E(X_{j_{1}})E(X_{j_{2}})=\\sigma_{j_{1},j_{2}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "487 A.2 Proof of one-to-one mapping between $\\hat{\\tau}_{j_{1},j_{2}}$ with $\\hat{\\sigma}_{j_{1},j_{2}}$ ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "488 Lemma A.2. For any fixed $\\hat{h}_{j_{1}}$ and $\\hat{h}_{j_{2}}$ , $T(\\sigma_{j_{1},j_{2}};\\{\\hat{h}_{j_{1}},\\hat{h}_{j_{2}}\\})$ $=$   \n489 $\\begin{array}{r}{\\int_{x_{1}>\\hat{h}_{j_{1}}}\\int_{x_{2}>\\hat{h}_{j_{2}}}\\phi(x_{j_{1}},x_{j_{2}};\\sigma)d x_{j_{1}}d x_{j_{2}}}\\end{array}$ , is a strictly monotonically increasing function on   \n490 $\\sigma\\in(-1,1)$ . ", "page_idx": 12}, {"type": "text", "text": "491 Proof To prove the lemma, we just need to show the gradien t \u2202T (\u03c3j1,j2;{\u02c6hj1,\u02c6hj2} > 0 for \u03c3 \u2208(\u22121, 1). ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\frac{\\partial T(\\sigma_{j_{1},j_{2}};\\{\\hat{h}_{j_{1}},\\hat{h}_{j_{2}}\\}}{\\partial\\sigma}==\\frac{1}{2\\pi\\sqrt{(1-\\sigma^{2})}}\\mathrm{exp}\\left(-\\frac{(\\hat{h}_{j_{1}}^{2}-2\\sigma\\hat{h}_{j_{1}}\\hat{h}_{j_{2}}+\\hat{h}_{j_{2}}^{2})}{2(1-\\sigma^{2})}\\right),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "492 which is obviously positive for $\\sigma\\in(-1,1)$ . Thus, we have one-to-one mapping between $\\hat{\\tau}_{j_{1}j_{2}}$ with   \n493 the calculated $\\hat{\\sigma}_{j_{1},j_{2}}$ for fixed $\\hat{h}_{j_{1}}$ and $\\hat{h}_{j_{2}}$ . ", "page_idx": 12}, {"type": "text", "text": "494 A.3 Proof of Thm. 2.5 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "495 In this section, we provide the proof of Thm. 2.5, which utilizes a regular statistical tool: ${{Z}}_{\\mathrm{{}}}$ -estimator   \n496 [33]. Specifically, we are interested in the parameter $\\theta$ and we have it estimation $\\hat{\\theta}$ . Let $\\mathbf{\\Delta}x_{1},\\dots,x_{n}$   \n497 are sampled from some true distribution $P$ , we can construct the function characterized by the   \n498 parameter $\\theta$ related the $\\textbf{\\em x}$ as $\\psi_{\\boldsymbol\\theta}(\\boldsymbol{x})$ . As long as we have $n$ observations, we can construct the function   \n499 as follows ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\Psi_{n}(\\theta)={\\frac{1}{n}}\\sum_{i=1}^{n}\\psi_{\\theta}(\\mathbf{x}_{i})=\\mathbb{P}_{n}\\psi_{\\theta}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "500 We further specify the form ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\Psi(\\theta)=\\int\\psi_{\\theta}(\\mathbf{x})d\\mathbf{x}=P\\psi_{\\theta}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "501 Assume the estimator $\\hat{\\theta}$ is a zero of $\\Psi_{n}$ , i.e., $\\Psi_{n}({\\hat{\\theta}})=0$ and will converge in probability to $\\theta_{0}$ , which   \n502 is a zero of $\\Psi$ , i.e., $\\Psi(\\theta_{0})=0$ . Expand $\\Psi_{n}({\\hat{\\theta}})$ in a Taylor series around $\\theta_{0}$ , we should have ", "page_idx": 12}, {"type": "equation", "text": "$$\n0=\\Psi_{n}(\\hat{\\theta})=\\Psi_{n}(\\theta_{0})+(\\hat{\\theta}-\\theta_{0})\\Psi_{n}^{\\prime}(\\theta_{0})+\\frac{1}{2}(\\hat{\\theta}-\\theta_{0})\\Psi_{n}^{\\prime\\prime}(\\theta_{0}).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "503 Rearrange the equation above, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\hat{\\theta}-\\theta_{0}=-\\frac{\\Psi_{n}({\\boldsymbol{\\theta}}_{0})}{\\Psi_{n}^{\\prime}({\\boldsymbol{\\theta}}_{0})+\\frac{1}{2}(\\hat{\\boldsymbol{\\theta}}-\\theta_{0})\\Psi_{n}^{\\prime\\prime}({\\boldsymbol{\\theta}}_{0})}}}\\\\ &{}&{=-\\frac{\\frac{1}{n}\\sum_{i=1}^{n}\\psi_{\\boldsymbol{\\theta}}({\\boldsymbol{x}}_{i})}{\\Psi_{n}^{\\prime}({\\boldsymbol{\\theta}}_{0})+\\frac{1}{2}(\\hat{\\boldsymbol{\\theta}}-\\theta_{0})\\Psi_{n}^{\\prime\\prime}({\\boldsymbol{\\theta}}_{0})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "504 According to the central limit theorem, the numerator will be asymptotic normal with variance   \n505 $P\\psi_{\\theta_{0}}^{2}/n$ as the mean $\\Psi(\\theta_{0})\\,=\\,0$ is zero. The first term of denominator $\\Psi_{n}^{\\prime}(\\theta_{0})$ will converge in   \n506 probability to $\\Psi^{\\prime}(\\theta_{0})$ according to the law of large numbers. The second term ${\\hat{\\theta}}-\\theta_{0}=o_{P}(1)$ . 1   \n507 As long as the denominator converges in probability and the numerator converges in distribution,   \n508 according to Slusky\u2019s lemma, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sqrt{n}(\\hat{\\theta}-\\theta_{0})\\rightsquigarrow N\\left(0,\\frac{P\\psi_{\\theta_{0}}^{2}}{(P\\psi_{\\theta_{0}}^{\\prime})^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "509 Extend into the high-dimensional case we should have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\theta}-\\theta_{0}=-(\\Psi_{n}^{\\prime}(\\theta_{0}))^{-1}\\Psi_{n}(\\theta_{0}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "510 where the second order term is omitted, further assume the matrix $P\\psi_{\\theta_{0}}^{\\prime}$ is invertible, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sqrt{n}(\\hat{\\theta}-\\theta_{0})\\rightsquigarrow N\\left(0,(P\\psi_{\\theta_{0}}^{\\prime})^{-1}P\\psi_{\\theta_{0}}\\psi_{\\theta_{0}}^{T}(P\\psi_{\\theta_{0}}^{\\prime T})^{-1}\\right),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "511 Specifically, in our case $\\theta_{0}=(\\sigma_{j_{1},j_{2}},\\mathbf{A})$ , where $\\pmb{\\Lambda}$ is another parameter set influencing the estimation   \n512 of $\\sigma_{j_{1},j_{2}}$ (will discuss case in case in later proof). In the practical scenario, we only have access to   \n513 the estimated parameter $\\hat{\\theta}$ and the empirical distribution $\\mathbb{P}_{n}$ , thus we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\hat{\\sigma}_{j_{1},j_{2}}-\\sigma_{j_{1},j_{2}}\\overset{\\mathrm{approx}}{\\sim}N\\left(0,((\\mathbb{P}_{n}\\psi_{\\hat{\\theta}}^{\\prime})^{-1}\\mathbb{P}_{n}\\psi_{\\hat{\\theta}}\\psi_{\\hat{\\theta}}^{T}(\\mathbb{P}_{n}\\psi_{\\hat{\\theta}}^{\\prime T})^{-1})_{1,1}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "514 Under the null hypothesis of independent, $\\sigma_{j_{1},j_{2}=0}$ . We provide the proof that $\\hat{\\theta}\\overset{p}{\\underset{\\rightharpoondown}{\\rightleftharpoons}}\\theta_{0}$ of our case   \n515 in App. A.1. Thus, $\\mathbb{P}_{n}\\psi_{\\hat{\\theta}}$ , the function parameterized by $\\hat{\\theta}$ , should also converge in $\\mathbb{P}_{n}\\psi_{\\hat{\\theta}_{0}}$ when   \n516 $n\\to\\infty$ . Besides, by the law of large numbers, $\\mathbb{P}_{n}\\psi_{\\hat{\\theta}_{0}}$ will converge to $P\\psi_{\\hat{\\theta}_{0}}$ . Thus, the equation   \n517 above will converge to Eq. (24) when $n\\to\\infty$ . ", "page_idx": 13}, {"type": "text", "text": "518 A.4 Derivation of Lem. 2.7 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "519 Let\u2019s first focus on the most challenging case where both variables are discretized observations   \n520 and our interested parameter will include $\\hat{\\theta}=(\\hat{\\sigma}_{j_{1},j_{2}},\\hat{h}_{j_{1}},\\hat{h}_{j_{2}})$ (Although we only care about the   \n521 distribution of $\\hat{\\sigma}_{j_{1},j_{2}}-\\sigma_{j_{1},j_{2}}$ , the estimation of boundary $\\hat{h}_{j_{1}}$ and $\\hat{h}_{j_{2}}$ will influence the estimation of   \n522 $\\hat{\\sigma}_{j_{1},j_{2}}$ , thus we need to consider all of them).   \n523 The next step will be to construct an appropriate criterion function $\\psi$ such that $\\Psi_{n}({\\hat{\\theta}})=\\mathbf{0}$ . Given $n$   \n524 observations $\\{\\tilde{x}^{1},\\tilde{x}^{2},\\ldots,\\tilde{x}^{n}\\}$ , which are discretized version of $\\{\\pmb{x}^{1},\\pmb{x}^{2},\\dots,\\pmb{x}^{n}\\}$ we should have ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "525 ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Psi_{n}(\\hat{\\theta})=\\left(\\begin{array}{l}{\\Psi_{n}(\\hat{\\sigma}_{j_{1},j_{2}})}\\\\ {\\Psi_{n}(\\hat{h}_{j_{1}})}\\\\ {\\Psi_{n}(\\hat{h}_{j_{2}})}\\end{array}\\right)=\\frac{1}{n}\\sum_{i=1}^{n}\\psi_{\\hat{\\theta}}(\\tilde{x}^{i})=\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\begin{array}{l l}{\\hat{\\tau}_{j_{1},j_{2}}^{i}-T(\\hat{\\sigma}_{j_{1},j_{2}};\\{\\hat{h}_{j_{1}},\\hat{h}_{j_{2}}\\})}\\\\ {\\hat{\\tau}_{j_{1}}^{i}-\\bar{\\Phi}(\\hat{h}_{j_{1}})}\\\\ {\\hat{\\tau}_{j_{2}}^{i}-\\bar{\\Phi}(\\hat{h}_{j_{2}})}\\end{array}\\right)=\\mathbf{0}.}\\\\ &{\\Psi_{n}(\\theta_{0})=\\left(\\begin{array}{l}{\\Psi_{n}(\\sigma_{j_{1},j_{2}})}\\\\ {\\Psi_{n}(h_{j_{1}})}\\\\ {\\Psi_{n}(h_{j_{2}})}\\end{array}\\right)=\\frac{1}{n}\\sum_{i=1}^{n}\\psi_{\\theta_{0}}(\\tilde{x}^{i})=\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\begin{array}{l l}{\\hat{\\tau}_{j_{1},j_{2}}^{i}-T(\\sigma_{j_{1},j_{2}};\\{h_{j_{1}},h_{j_{2}}\\})}\\\\ {\\hat{\\tau}_{j_{1}}^{i}-\\bar{\\Phi}(h_{j_{1}})}\\\\ {\\hat{\\tau}_{j_{2}}^{i}-\\bar{\\Phi}(h_{j_{2}})}\\end{array}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "526 The difference between the estimated parameter with the true parameter can be expressed as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\theta}-\\theta_{0}=\\left(\\begin{array}{c c}{\\hat{\\sigma}_{j_{1},j_{2}}-\\sigma_{j_{1},j_{2}}}\\\\ {\\hat{h}_{j_{1}}-h_{j_{1}}}\\\\ {\\hat{h}_{j_{2}}-h_{j_{2}}}\\end{array}\\right)=-\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\left(\\begin{array}{c c c}{\\frac{\\partial\\Psi_{n}(\\sigma_{j_{1},j_{2}})}{\\partial\\sigma_{j_{1},j_{2}}}}&{\\frac{\\partial\\Psi_{n}(\\sigma_{j_{1},j_{2}})}{\\partial h_{j_{1}}}}&{\\frac{\\partial\\Psi_{n}(\\sigma_{j_{1},j_{2}})}{\\partial h_{j_{2}}}}\\\\ {\\frac{\\partial\\Psi_{n}(h_{j_{1}})}{\\partial\\sigma_{j_{1},j_{2}}}}&{\\frac{\\partial\\Psi_{n}(h_{j_{1}})}{\\partial h_{j_{1}}}}&{\\frac{\\partial\\Psi_{n}(h_{j_{1}})}{\\partial h_{j_{2}}}}\\\\ {\\frac{\\partial\\Psi_{n}(h_{j_{2}})}{\\partial\\sigma_{j_{1},j_{2}}}}&{\\frac{\\partial\\Psi_{n}(h_{j_{2}})}{\\partial h_{j_{1}}}}&{\\frac{\\partial\\Psi_{n}(h_{j_{2}})}{\\partial h_{j_{2}}}}\\end{array}\\right)^{-1}}\\\\ {\\cdot\\displaystyle\\left(\\begin{array}{c c}{\\hat{\\tau}_{j_{1},j_{2}}^{i}-T(\\sigma_{j_{1},j_{2}};\\{h_{j_{1}},h_{j_{2}}\\})}\\\\ {\\hat{\\tau}_{j_{1}}^{i}-\\bar{\\Phi}(h_{j_{1}})}\\\\ {\\hat{\\tau}_{j_{2}}^{i}-\\bar{\\Phi}(h_{j_{2}})}\\end{array}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "1We will not provide proof of this in this paper; however, interested readers may refer to [33] ", "page_idx": 13}, {"type": "text", "text": "527 where the specific form of each entry of the gradient matrix is expressed as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial\\hat{\\rho}_{\\mathrm{A}}(\\rho_{\\mathrm{A}},t_{\\mathrm{A}})}{\\partial\\rho_{\\mathrm{A}}\\partial\\rho_{\\mathrm{A}}}=-\\frac{1}{2\\gamma\\left(1-e^{\\frac{\\gamma}{\\beta_{2}}}\\right)\\Delta}\\mathrm{er}\\left(\\frac{\\left(\\Delta_{\\mathrm{B}}^{2}-2\\rho_{\\mathrm{B}},\\Delta_{\\mathrm{B}}\\right)B_{\\mathrm{i}}+B_{\\mathrm{o}}^{2}}{21-e^{\\frac{\\gamma}{\\beta_{2}}}\\left(1-e^{\\frac{\\gamma}{\\beta_{2}}}\\right)^{2}}\\right);}\\\\ &{\\frac{\\partial\\hat{\\rho}_{\\mathrm{A}}(\\rho_{\\mathrm{A}},t_{\\mathrm{A}})}{\\partial\\rho_{\\mathrm{A}}}=\\frac{1}{2\\gamma_{2}\\left(1-e^{\\frac{\\gamma}{\\beta_{2}}}\\right)\\Delta}\\mathrm{er}\\left(\\frac{\\left(\\Delta_{\\mathrm{B}}^{2}-2\\rho_{\\mathrm{B}},\\Delta_{\\mathrm{B}}\\right)B_{\\mathrm{i}}+B_{\\mathrm{o}}^{2}}{21-e^{-\\frac{\\gamma}{\\beta_{2}}}\\left(1-e^{\\frac{\\gamma}{\\beta_{2}}}\\right)^{2}}+\\right)d\\alpha_{1}}\\\\ &{\\frac{\\partial\\hat{\\rho}_{\\mathrm{A}}(\\rho_{\\mathrm{A}},t_{\\mathrm{A}})}{\\partial\\rho_{\\mathrm{A}}}=\\int_{\\gamma_{1}}^{\\infty}\\frac{1}{2\\gamma_{1}-e^{-\\frac{\\gamma}{\\beta_{2}}}}\\mathrm{er}\\left(\\frac{\\left(\\Delta_{\\mathrm{B}}^{2}-2\\rho_{\\mathrm{B}},\\Delta_{\\mathrm{B}}\\right)B_{\\mathrm{i}}+B_{\\mathrm{o}}^{2}}{21-e^{-\\frac{\\gamma}{\\beta_{2}}}\\left(1-e^{\\frac{\\gamma}{\\beta_{2}}}\\right)^{2}}+\\right)d\\alpha_{1}}\\\\ &{\\frac{\\partial\\hat{\\rho}_{\\mathrm{A}}(\\rho_{\\mathrm{A}},t_{\\mathrm{A}})}{\\partial\\rho_{\\mathrm{A}}}=\\mathrm{e}}\\\\ &{\\frac{\\partial\\hat{\\rho}_{\\mathrm{A}}(\\rho_{\\mathrm{A}},t_{\\mathrm{A}})}{\\partial\\rho_{\\mathrm{A}}}=\\frac{1}{\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "528 For simplicity of notation, we define ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{\\sigma}_{j_{1},j_{2}}-\\sigma_{j_{1},j_{2}}=\\frac{1}{n}\\sum_{i=1}^{n}\\xi_{j_{1},j_{2}}^{i},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "529 where the specific form is of $\\{\\xi_{j_{1},j_{2}}^{i}\\}$ is defined in Eq. (28). We should note that $\\{\\xi_{j_{1},j_{2}}^{i}\\}$ are i.i.d   \n530 random variables with mean zero (this property will be the key to the derivation of inference of CI).   \n531 As long as our estimation $\\hat{\\theta}$ converge in probability to $\\theta_{0}$ as proved in A.1, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sqrt{n}(\\hat{\\theta}-\\theta_{0})\\rightsquigarrow N\\left(0,((P\\psi_{\\theta_{0}}^{\\prime})^{-1}P\\psi_{\\theta_{0}}\\psi_{\\theta_{0}}^{T}(P\\psi_{\\theta_{0}}^{\\prime T})^{-1})_{1,1}\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "532 where $\\psi_{\\theta_{0}}$ is defined in Eq. (27). However, in practice, we don\u2019t have access to either $P$ or $\\theta_{0}$ . In this   \n533 scenario, we can plug in the empirical distribution of $\\mathbb{P}_{n}\\psi_{\\hat{\\theta}}$ to get the estimated variance, i.e., the   \n534 actual variance used in the calculation of $\\hat{\\sigma}_{j_{1},j_{2}}-\\sigma_{j_{1},j_{2}}$ is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\left((\\mathbb{P}_{n}\\psi_{\\hat{\\theta}}^{\\prime})^{-1}\\mathbb{P}_{n}\\psi_{\\hat{\\theta}}\\psi_{\\hat{\\theta}}^{T}(\\mathbb{P}_{n}\\psi_{\\hat{\\theta}}^{\\prime T})^{-1}\\right)_{1,1}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "535 A.5 Derivation of Lem. 2.8 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "536 Use the same line of procedure as in the derivation of Lem. 2.7, for mixed pair of observations where   \n537 $X_{j_{1}}$ is continuous and $\\tilde{X}_{j_{2}}$ is discrete, we can construct the criterion function ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Psi_{n}(\\hat{\\theta})=\\binom{\\Psi_{n}(\\hat{\\sigma}_{j_{1},j_{2}})}{\\Psi_{n}(\\hat{h}_{j_{2}})}=\\frac{1}{n}\\sum_{i=1}^{n}\\psi_{\\hat{\\theta}}(\\hat{x}^{i})=\\frac{1}{n}\\sum_{i=1}^{n}\\binom{\\hat{\\tau}_{j_{1},j_{2}}^{i}-T(\\hat{\\sigma}_{j_{1},j_{2}};\\{0,\\hat{h}_{j_{2}}\\})}{\\hat{\\tau}_{j_{2}}^{i}-\\bar{\\Phi}(\\hat{h}_{j_{2}})}=\\mathbf{0}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "538 ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Psi_{n}(\\theta_{0})=\\left(\\Psi_{n}(\\sigma_{j_{1},j_{2}})\\right)=\\frac{1}{n}\\sum_{i=1}^{n}\\psi_{\\theta_{0}}(\\tilde{x}^{i})=\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\hat{\\tau}_{j_{1},j_{2}}^{i}-T(\\sigma_{j_{1},j_{2}};\\{0,h_{j_{2}}\\})\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "539 The difference between the estimated parameter with the true parameter can be expressed as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\small\\mathfrak{i}-\\theta_{0}=\\left(\\widehat{\\widehat{\\sigma}}_{j_{1},j_{2}}-\\sigma_{j_{1},j_{2}}\\right)=-\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\frac{\\partial\\Psi_{n}(\\sigma_{j_{1},j_{2}})}{\\partial\\sigma_{j_{1},j_{2}}}\\quad\\frac{\\partial\\Psi_{n}(\\sigma_{j_{1},j_{2}})}{\\partial h_{j_{2}}}\\right)^{-1}\\left(\\widehat{\\tau}_{j_{1},j_{2}}^{i}-T(\\sigma_{j_{1},j_{2}};\\{0,h_{j_{2}}\\})\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "540 where the specific form of each entry of the gradient matrix can be found in Eq. (29). Using exactly   \n541 the same procedure, we should have the same formation of the variance calculated as Eq. (32) with a   \n542 different definition of $\\psi_{\\theta_{0}}$ and $\\psi_{\\hat{\\theta}}$ defined in Eq. (34) (33). ", "page_idx": 15}, {"type": "text", "text": "543 A.6 Derivation of Lem. 2.6 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "544 Use the same line of procedure as in derivation of Lem. 2.7, for a continuous pair of variables, we   \n545 can construct the criterion function ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Psi_{n}(\\hat{\\theta})=\\Psi_{n}(\\hat{\\sigma}_{j_{1},j_{2}})=\\frac{1}{n}\\sum_{i=1}^{n}x_{j_{1}}^{i}x_{j_{2}}^{i}-\\frac{1}{n}\\sum_{i=1}^{n}x_{j_{1}}^{i}\\frac{1}{n}\\sum_{i=1}^{n}x_{j_{2}}^{i}-\\hat{\\sigma}_{j_{1},j_{2}}=0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "546 ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Psi_{n}(\\theta_{0})=\\Psi_{n}(\\sigma_{j_{1},j_{2}})=\\frac{1}{n}\\sum_{i=1}^{n}x_{j_{1}}^{i}x_{j_{2}}^{i}-\\frac{1}{n}\\sum_{i=1}^{n}x_{j_{1}}^{i}\\frac{1}{n}\\sum_{i=1}^{n}x_{j_{2}}^{i}-\\sigma_{j_{1},j_{2}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "547 Denote $\\textstyle{\\frac{1}{n}}\\sum_{i=1}^{n}x_{j_{1}}^{i}$ as $\\bar{x}_{j_{1}}$ and $\\textstyle{\\frac{1}{n}}\\sum_{i=1}^{n}x_{j_{2}}^{i}$ as $\\bar{x}_{j_{2}}$ . We should have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{\\sigma}_{j_{1},j_{2}}-\\sigma_{j_{1},j_{2}}=\\frac{1}{n}\\sum_{i=1}^{n}x_{j_{1}}^{i}x_{j_{2}}^{i}-\\bar{x}_{j_{1}}\\bar{x}_{j_{2}}-\\sigma_{j_{1},j_{2}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "548 According to Eq. (22), we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sqrt{n}(\\hat{\\sigma}_{j_{1},j_{2}}-\\sigma_{j_{1},j_{2}})\\rightsquigarrow N\\left(0,\\frac{P\\psi_{\\theta_{0}}^{2}}{(P\\psi_{\\theta_{0}}^{\\prime})^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "549 where $(P\\psi_{\\theta_{0}}^{\\prime})^{2}=1$ . In practical calculation, we have the variance ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\mathbb{P}_{n}\\psi_{\\hat{\\theta}}^{2}/(\\mathbb{P}_{n}\\psi_{\\hat{\\theta}}^{\\prime})^{2}=\\frac{1}{n^{2}}\\sum_{i=1}^{n}(x_{j_{1}}^{i}x_{j_{2}}^{i}-\\bar{x}_{j_{1}}\\bar{x}_{j_{2}}-\\hat{\\sigma}_{j_{1},j_{2}})^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "550 A.7 Proof of Thm. 2.9 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "551 A.7.1 Proof of Relation between $\\Sigma,\\Omega$ with $\\beta$ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "552 Consider our latent continuous variables $\\pmb{X}=(X_{1},\\pmb{\\mathscr{s}}...\\,,X_{p})\\sim N(0,\\pmb{\\Sigma})$ and do nodewise regression ", "page_idx": 15}, {"type": "equation", "text": "$$\nX_{j}=X_{-j}\\beta_{j}+\\epsilon_{j}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "553 We can divide its covariance $\\Sigma$ and its precision matrix $\\Omega=\\Sigma^{-1}$ into $X$ and $Y$ part in our regression: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pmb{\\Sigma}=\\left(\\pmb{\\Sigma}_{j j}\\;\\;\\;\\;\\;\\;\\pmb{\\Sigma}_{j-j}\\right)\\;\\;\\;\\pmb{\\Omega}=\\left(\\pmb{\\Omega}_{j j}\\;\\;\\;\\;\\;\\Omega_{j-j}\\right)\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "554 Just like regular linear regression, we can get ", "page_idx": 15}, {"type": "equation", "text": "$$\nn\\rightarrow\\infty,\\quad\\beta_{j}=\\Sigma_{-j-j}^{-1}\\Sigma_{-j j}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "555 From the invertibility of a block matrix ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left[{\\begin{array}{r r}{A}&{B}\\\\ {C}&{D}\\end{array}}\\right]^{-1}=\\left[{\\begin{array}{c c}{(A-B D^{-1}C)^{-1}}&{-(A-B D^{-1}C)^{-1}B D^{-1}}\\\\ {-D^{-1}C(A-B D^{-1}C)^{-1}}&{D^{-1}+D^{-1}C(A-B D^{-1}C)^{-1}B D^{-1}}\\end{array}}\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "556 If $A$ and $D$ is invertible, we will have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left[\\!\\!{\\begin{array}{c c}{A}&{B}\\\\ {C}&{D}\\end{array}}\\!\\!\\right]^{-1}=\\left[\\!\\!{\\begin{array}{c c}{A-B D^{-1}C}&{0}\\\\ {0}&{(D-C A^{-1}B)^{-1}}\\end{array}}\\!\\!\\right]\\left[\\!\\!{\\begin{array}{c c}{I}&{-B D^{-1}}\\\\ {-C A^{-1}}&{I}\\end{array}}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "557 Thus, we can get: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\Omega_{j j}=\\Sigma_{j j}-\\left(\\Sigma_{j-j}\\Sigma_{-j-j}^{-1}\\Sigma_{-j j}\\right)^{-1};}\\\\ &{}&{\\Omega_{j-j}=-\\left(\\Sigma_{j j}-\\left(\\Sigma_{j-j}\\Sigma_{-j-j}^{-1}\\Sigma_{-j j}\\right)^{-1}\\right)\\Sigma_{j-j}(\\Sigma_{-j-j})^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "558 Move one step forward: ", "page_idx": 16}, {"type": "equation", "text": "$$\n-\\Omega_{j j}^{-1}\\Omega_{j-j}=\\Sigma_{j-j}(\\Sigma_{-j-j})^{-1}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "559 Take transpose for both sides, as long as $\\Omega$ is a symmetric matrix and $\\Omega_{-j j}=\\Omega_{j-j}^{T}$ , we will have ", "page_idx": 16}, {"type": "equation", "text": "$$\n-\\Omega_{j j}^{-1}\\Omega_{-j j}=\\Sigma_{-j-j}^{-1}\\Sigma_{-j j}=\\beta_{j}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "560 We should note testing $\\Omega_{-j j}=0$ is equivalent to testing $\\beta_{j}=0$ as the $\\Omega_{j j}$ will always be nonzero.   \n561 The variable $\\Omega_{-j j}$ captures the CI of $X_{j}$ with other variables. As long as the variable $\\Omega_{j j}$ is just one   \n562 scalar, we can get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\beta_{j,k}=-\\frac{\\omega_{j,k}}{\\omega_{j,j}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "563 capturing the independence relationship between variable $X_{j}$ with $X_{k}$ conditioning on all other   \n564 variables. ", "page_idx": 16}, {"type": "text", "text": "565 A.7.2 Detailed derivation of inference for $\\beta_{j}$ ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "566 Nodewise regression allows us to use the regression parameter $\\beta_{j}$ as the surrogate of $\\Omega_{-j j}$ . The   \n567 problem now transfers to constructing the inference for $\\beta_{j}$ , specifically, the derivation of distribution   \n568 of $\\hat{\\beta}_{j}-\\beta_{j}$ . The overarching concept is that we are already aware of the distribution of $\\hat{\\sigma}_{j_{1},j_{2}}-\\sigma_{j_{1},j_{2}}$   \n569 and we know that there exists a deterministic relationship between $\\beta_{j}$ with $\\Sigma$ . Consequently, we can   \n570 express $\\hat{\\beta}_{j}-\\beta_{j}$ as a composite of $\\hat{\\sigma}_{j_{1},j_{2}}-\\sigma_{j_{1},j_{2}}$ to establish such an inference. Specifically, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\beta}_{j}-\\beta_{j}=\\hat{\\Sigma}_{-j-j}^{-1}\\hat{\\Sigma}_{-j j}-\\Sigma_{-j-j}^{-1}\\Sigma_{-j j}}\\\\ &{\\qquad\\quad=\\hat{\\Sigma}_{-j-j}^{-1}\\left(\\hat{\\Sigma}_{-j j}-\\hat{\\Sigma}_{-j-j}\\Sigma_{-j-j}^{-1}\\Sigma_{-j j}\\right)}\\\\ &{\\qquad\\quad=-\\hat{\\Sigma}_{-j-j}^{-1}\\left(\\hat{\\Sigma}_{-j-j}\\beta_{j}-\\Sigma_{-j-j}\\beta_{j}+\\Sigma_{-j-j}\\beta_{j}-\\hat{\\Sigma}_{-j j}\\right)}\\\\ &{\\qquad\\quad=-\\hat{\\Sigma}_{-j-j}^{-1}\\left((\\hat{\\Sigma}_{-j-j}-\\Sigma_{-j-j})\\beta_{j}-(\\hat{\\Sigma}_{-j j}-\\Sigma_{-j j})\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "571 where each entry in matrix $(\\hat{\\Sigma}_{-j-j}-\\Sigma_{-j-j})$ and $(\\hat{\\Sigma}_{-j j}-\\Sigma_{-j j})$ denotes the difference between   \n572 estimated covariance with true covariance. Suppose that we want to test the $\\mathrm{CI}$ of the variable $X_{1}$   \n573 with other variables, $j=1$ , then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\Sigma}_{-j-j}-\\Sigma_{-j-j}=\\left[\\hat{\\sigma}_{j-1,1}\\hdots\\hat{\\sigma}_{j-1,j-1}\\hat{\\sigma}_{1,j+1}\\hdots\\hat{\\sigma}_{1,p}\\right]}\\\\ {\\hat{\\Sigma}_{-j-j}-\\Sigma_{-j-j}=\\left[\\hat{\\sigma}_{j-1,1}\\hdots\\hat{\\sigma}_{j-1,j-1}\\hat{\\sigma}_{j-1,j+1}\\hdots\\hat{\\sigma}_{j-1,p}\\right]}\\\\ {\\hdots\\cdot\\cdot\\cdot}\\\\ {\\hat{\\sigma}_{p,1}\\hdots\\hat{\\sigma}_{p,j-1}\\hat{\\sigma}_{p,j+1}\\hdots\\hat{\\sigma}_{p,p}}\\\\ {\\sigma_{1,1}\\hdots\\cdot\\sigma_{1,j-1}\\L,\\sigma_{1,j+1}\\hdots\\cdot\\sigma_{1,p}}\\\\ {\\hdots\\cdot\\cdot\\cdot}\\\\ {\\sigma_{j-1,1}\\hdots\\cdot\\sigma_{j-1,j-1}\\L,\\sigma_{j-1,j+1}\\hdots\\cdot\\sigma_{j-1,p}}\\\\ {\\hdots\\cdot\\cdot\\cdot}\\\\ {\\sigma_{p,1}\\hdots\\cdot\\sigma_{p,j-1}\\L,\\sigma_{p,j+1}\\hdots\\cdot\\sigma_{p,p}\\mathrm{.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "574 Suppose that we want to test the $\\mathrm{CI}$ of the variable $X_{1}$ with other variables, $j=1$ . then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\bf\\cal S}_{-1-1}-\\{{\\bf\\cal\\Sigma}_{-1-1}=\\left[\\hat{\\sigma}_{2,2}\\cdot\\cdot\\cdot\\hat{\\sigma}_{2,p}\\right]-\\left[\\hat{\\sigma}_{2,2}\\cdot\\cdot\\cdot\\sigma_{2,p}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\left[\\hat{\\sigma}_{p,2}\\cdot\\cdot\\cdot\\hat{\\sigma}_{p,p}\\right]-\\left[\\hat{\\sigma}_{p,2}\\cdot\\cdot\\cdot\\cdot\\hat{\\sigma}_{p,p}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad:=\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\left[\\xi_{2,2}^{i}\\cdot\\cdot\\cdot\\xi_{2,p}^{i}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "557756 Ewqh.e (r3e $\\{\\xi_{j_{1},j_{2}}^{i}\\}$ xaerde  ci.ai.sde  raanndd Eoqm.  (v3a8r)i aibnl ecso nwtiinthu osupse cciafsiec.  fPorutm t hdeemfi nteodg eitnh eEr:q. (28) for discrete case, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[\\hat{\\beta}_{1,2}-\\beta_{1,2}\\right]}\\\\ {\\left[\\hat{\\beta}_{1,3}-\\beta_{1,3}\\right]=-\\hat{\\Sigma}_{-1-1}^{-1}\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\left[\\xi_{3,2}^{i}\\quad\\xi_{3,3}^{i}\\quad\\cdot\\cdot\\cdot\\quad\\xi_{2,p}^{i}\\right]\\left[\\beta_{1,3}\\right]-\\left[\\xi_{3,1}^{i}\\right]\\right)}\\\\ {\\left[\\hat{\\beta}_{1,p}\\cdot\\cdot\\cdot\\right]=-\\hat{\\Sigma}_{-1,p}^{-1}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "577 As 1 in=1 \u03beji1,j2 is asymptotically normal, the who vector of \u03b2\u02c61 \u2212\u03b21 is a linear combination of   \n578 Gaussian distribution. However, We cannot merely engage in a linear combination of its variance as   \n579 they are dependent with each other. For example, if $Y_{1},Y_{2}$ are dependent and we are trying to find   \n580 out $V a r(a\\bar{Y}_{1}+b Y_{2})$ , we should have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\boldsymbol{V}a\\boldsymbol{r}(a Y_{1}+b Y_{2})=[a\\quad b]\\left[\\boldsymbol{C}o v(Y_{1},Y_{2})\\quad\\boldsymbol{C}o v(Y_{1},Y_{2})\\right]\\left[\\boldsymbol{a}\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "581 Now, suppose we are interested in the distribution of $\\hat{\\beta}_{1,2}-\\beta_{1,2}$ , we should have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{\\beta}_{1,2}-\\beta_{1,2}=\\frac{1}{n}\\sum_{i=1}^{n}(\\hat{\\Sigma}_{-1-1}^{-1})_{[2],:}\\left(\\left[\\begin{array}{l l l l l}{\\xi_{2,2}^{i}}&{\\xi_{2,3}^{i}}&{\\cdots}&{\\xi_{2,p}^{i}}\\\\ {\\xi_{3,2}^{i}}&{\\xi_{3,3}^{i}}&{\\cdots}&{\\xi_{3,p}^{i}}\\\\ {\\cdot\\cdot}&{\\cdot\\cdot}&{\\cdot\\cdot}&{\\cdot\\cdot}\\\\ {\\xi_{p,2}^{i}}&{\\xi_{p,3}^{i}}&{\\cdots}&{\\xi_{p,p}^{i}}\\end{array}\\right]\\left[\\begin{array}{l}{\\beta_{1,2}}\\\\ {\\beta_{1,3}}\\\\ {\\cdot\\cdot}\\\\ {\\cdot\\cdot}\\\\ {\\beta_{1,p}}\\end{array}\\right]-\\left[\\begin{array}{l}{\\xi_{2,1}^{i}}\\\\ {\\xi_{3,1}^{i}}\\\\ {\\cdot\\cdot}\\\\ {\\cdot\\cdot}\\\\ {\\xi_{p,1}^{i}}\\end{array}\\right]\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "582 where $(\\hat{\\Sigma}_{-1-1}^{-1})_{[2]},$ : is the row of index of $X_{2}$ of $\\hat{\\Sigma}_{-1-1}^{-1}$ ([2] denotes the index of the variable). For   \n583 ease of notation, let ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Xi_{-1,-1}^{i}=\\left[\\begin{array}{l l l l}{\\xi_{2,2}^{i}}&{\\xi_{2,3}^{i}}&{\\dots}&{\\xi_{2,p}^{i}}\\\\ {\\xi_{3,2}^{i}}&{\\xi_{3,3}^{i}}&{\\dots}&{\\xi_{3,p}^{i}}\\\\ {\\dots}&{\\dots}&{\\dots}&{\\dots}&{\\dots}\\\\ {\\xi_{p,2}^{i}}&{\\xi_{p,3}^{i}}&{\\dots}&{\\xi_{p,p}^{i}}\\end{array}\\right],\\qquad\\Xi_{-1,1}^{i}=\\left[\\begin{array}{l}{\\xi_{2,1}^{i}}\\\\ {\\xi_{3,1}^{i}}\\\\ {\\dots}\\\\ {\\xi_{p,1}^{i}}\\end{array}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "584 and let ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{B_{-1}^{i}=\\left(\\begin{array}{l l l l l}{\\xi_{2,1}^{i}}&{\\xi_{3,1}^{i}}&{\\cdot\\cdot\\cdot}&{\\xi_{p,1}^{i}}\\\\ {\\xi_{2,2}^{i}}&{\\xi_{2,3}^{i}}&{\\cdot\\cdot\\cdot}&{\\xi_{2,p}^{i}}\\\\ {\\xi_{3,2}^{i}}&{\\xi_{3,3}^{i}}&{\\cdot\\cdot\\cdot}&{\\xi_{3,p}^{i}}\\\\ {\\cdot\\cdot}&{\\cdot\\cdot\\cdot}&{\\cdot\\cdot}&{\\cdot\\cdot}\\\\ {\\xi_{p,2}^{i}}&{\\xi_{p,3}^{i}}&{\\cdot\\cdot\\cdot}&{\\xi_{p,p}^{i}\\Bigg)}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "585 as the concatenation of those two matrices. The variance is calculated as ", "page_idx": 17}, {"type": "equation", "text": "$$\nV a r\\left(\\sqrt{n}(\\hat{\\beta}_{1,2}-\\beta_{1,2})\\right)=a^{[2]}{}^{T}\\frac{1}{n}\\sum_{i=1}^{n}v e c(B_{-1}^{i})v e c(B_{-1}^{i})^{T}a^{[2]},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "586 where ", "page_idx": 17}, {"type": "equation", "text": "$$\na_{l}^{[2]}=\\left\\{\\!\\!\\begin{array}{l l}{\\left(\\hat{\\Sigma}_{-1-1}^{-1}\\right)_{[2],l},}&{\\mathrm{for}\\;l\\in\\{1,\\dots,p-1\\}}\\\\ {\\sum_{q=1}^{n}\\left(\\hat{\\Sigma}_{-1-1}^{-1}\\right)_{[2],l}(\\beta_{1})_{q}\\,,}&{\\mathrm{for}\\;l\\in\\{p,\\dots,p^{2}-p\\}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "587 $v e c(B_{-1}^{i})$ is the squeezed vector form of matrix $v e c(B_{-1}^{i})\\in\\mathbb R^{p\\times p-1}$ , i.e., ", "page_idx": 17}, {"type": "equation", "text": "$$\nv e c(B_{-1}^{i})=\\left(\\stackrel{\\xi_{2,1}^{i}}{\\xi_{3,1}^{i}}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "588 Thus, the distribution of $\\hat{\\beta}_{j,k}-\\beta_{j,k}$ is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{\\beta}_{j,k}-\\beta_{j,k}\\sim N(0,a^{[k]}{}^{T}\\frac{1}{n^{2}}\\sum_{i=1}^{n}v e c(B_{-j}^{i})v e c(B_{-j}^{i})^{T})a^{[k]}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "589 In practice, we can plug in the estimates of $\\beta_{j}$ to estimate the interested distribution and do the CI   \n590 test by hypothesizing $\\beta_{j,k}=0$ .   \n592 In this section, we engage in a more thorough discussion regarding our assumptions about $\\mathbf{\\deltaX}$ .   \n593 Specifically, we demonstrate that this assumption of mean and variance does not compromise the   \n594 generality. In other words, the true model may possess different mean and variance values, but we   \n595 proceed by treating it as having a mean of zero and identity variance. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "The key ingredient allowing us to assume such a model is, the discretization function $g_{j}$ is an unknown nonlinear monotonic function. Suppose the $g_{j}^{\\prime}$ maps the continuous domain to a binary variable, and we have the \"groundtruth\" variable, denoted ${\\dot{X}}_{j}^{\\prime}$ , with mean $a$ and variance $b$ . Assume the cardinality of the discretized domain is only 2, i.e., our observation $\\tilde{X}_{j}$ can only be 0 or 1. We further have the constant $d_{j}^{\\prime}$ as the discretization boundary such that we have the observation ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\tilde{X}}_{j}=\\mathbb{1}(g_{j}^{\\prime}(X_{j}^{\\prime})>d_{j}^{\\prime})=\\mathbb{1}(X_{j}^{\\prime}>g_{j}^{\\prime-1}(d_{j}))\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "596 We can always produce our assumed variable $X_{j}$ with mean 0 and variance 1, such that $X_{j}\\,=$   \n597 $\\begin{array}{r}{\\frac{1}{\\sqrt{b}}X_{j}^{\\prime}\\,-\\,\\frac{a}{\\sqrt{b}}}\\end{array}$ and the same observation with a different nonlinear transformation $g_{j}$ and decision   \n598 boundary $d_{j}$ , such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\tilde{X}_{j}=\\mathbb{1}(g_{j}(X_{j})>d_{j})=\\mathbf{1}(X_{j}>g_{j}^{-1}(d_{j}))=\\mathbb{1}(X_{j}^{\\prime}>\\sqrt{b}g_{j}^{-1}(d_{j})+a)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "599 As long as the observation $\\tilde{X}_{j}$ is the same, we should have $\\sqrt{b}g_{j}^{-1}(d_{j})+a=g_{j}^{\\prime-1}(d_{j})$ . Our assumed   \n600 model $X_{j}$ clearly mimics the \"groundtruth\" $X_{j}^{\\prime}$ . Besides, according to Lem. A.2, we have one-to  \n601 one mapping between $\\hat{\\tau}_{j_{1}j_{2}}$ with the estimated covariance for fixed $\\hat{h}_{j_{1}},\\hat{h}_{j_{2}}$ . Thus, as long as the   \n602 observation is the same, the estimation of covariance $\\hat{\\sigma}_{j_{1},j_{2}}$ remains unaffected by our assumptions   \n603 regarding the mean and variance of $\\mathbf{\\deltaX}$ , so do the following inference.   \n604 We further conduct casual discovery experiments to empirically validate our statement, which is   \n605 shown in App. C.3.   \n607 Data Generation and Code We construct the true DAG $\\mathcal{G}$ using the Bipartite Pairing (BP) model   \n608 [2], with the number of edges being one fewer than the number of nodes. The subsequent generation of   \n609 true multivariate Gaussian data involves assigning causal weights drawn from a uniform distribution   \n610 $U\\,\\sim\\,(0.5,2)$ and incorporating noise via samples from a standard normal distribution for each   \n611 variable. Following this, we binarize the data, setting the threshold randomly based on each variable\u2019s   \n612 range. The code implementation is based on [40] . ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "image", "img_path": "B1tCaKP5nB/tmp/2aa4c414fe50ada15bca3a0b69d556b020035fa325c5902c47bc733df0fab6a6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 4: Experiment result of DAG discovery on synthetic data for changing sample size (a) and changing number of nodes (b). Fisherz_nodis is the Fisher-z test applied to original continuous data. We evaluate $F_{1}$ $\\uparrow)$ , Precision (\u2191), Recall $(\\uparrow)$ and SHD (\u2193). ", "page_idx": 19}, {"type": "text", "text": "613 C Additional experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "614 C.1 Linear non-Gaussian and nonlinear ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "615 Our model requires that the original data must adhere to the hypothesis of following a multivariate   \n616 normal distribution, which appears to potentially limit the generalizability. Therefore, it is worthwhile   \n617 to explore its robustness when such assumptions are violated. In this regard, we conducted several   \n618 experiments, including scenarios involving linear non-Gaussian and nonlinear Gaussian.   \n619 For both cases, we follow the setting of our experiment where there are $p\\,=\\,8$ nodes and $p-1$   \n620 edges. We explore the effect of changing sample size $n=(100,500,2000,5000)$ . Specifically for   \n621 linear non-Gaussian case, we adhere to some of the settings outlined by [28], conducting experiments   \n622 where the original continuous data followed: (1) a Student\u2019s t-distribution with 3 degrees of freedom,   \n623 (2) a uniform distribution, and (3) an exponential distribution. Each variable is generated as $X_{i}=$   \n624 $f(P A_{i})+n o i s e$ , where noise follows the distribution in (1), (2), (3) correspondingly and $f$ is a   \n625 linear function. The first three rows of Fig. 5 and Fig. 6 show the result of the linear non-Gaussian   \n626 case.   \n627 For the nonlinear cases, we follow setting in [19], where every variable $X_{i}$ is generated as $X_{i}=$   \n628 $f(W P A_{i}+n o i s e)$ , $n o i s e\\sim N(0,1)$ and $f$ is a function randomly chosen from (a) $f(x)=s i n(x)$ ,   \n629 (b) $f(x)=x^{3}$ , (c) $f(x)=t a n h(x)$ , and (d) $f(x)=R e L U(x)$ . $W$ is a linear function. Similarly,   \n630 we set the number of nodes at $p=8$ and change the number of samples $n=(500,2000,5000)$ .   \n631 For both cases, we run 10 graph instances with different seeds and report the result of skeleton   \n632 discovery in Fig. 5 and DAG in Fig. 6 (The same orientation rules [11] used in the main experiment   \n633 are employed to convert a CPDAG [6] into a DAG). The last row of Fig. 5 and Fig. 6 shows the result   \n634 of the nonlinear case.   \n635 Based on the experimental outcomes, DCT demonstrates marginally superior or comparable efficacy   \n636 in terms of the F1-score, precision, and SHD relative to both the Fisher-Z test and the Chi-square test   \n637 when dealing with small sample sizes. Nevertheless, as the sample size increases, DCT\u2019s performance   \n638 clearly surpasses that of the aforementioned tests across all three evaluated metrics, especially in the   \n639 linear case. Consistent with observations from the main experiment, DCT exhibits a lower recall in   \n640 comparison to the baseline tests. This discrepancy can be attributed to the baseline tests being prone   \n641 to incorrectly infer conditional dependence and connect a large proportion of nodes. According to   \n642 the results, our test shows notable robustness under the case assumptions are violated, confirming its   \n643 practical effectiveness. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "644 C.2 Denser graph ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "645 DCT primarily works on cases where CI is mistakenly judged as conditional dependence due   \n646 to discretization. Consequently, its efficacy is more pronounced in scenarios characterized by a   \n647 relatively sparse graph, as numerous instances are truly conditionally independent. Nevertheless, the   \n648 investigation of causal discovery with a dense latent graph is essential for evaluating the power of a   \n649 test, i.e., its ability to successfully reject the null hypothesis when the tested pairs are conditionally   \n650 dependent. Thus, we conduct the experiment where $p=8,n=10000$ and changing edges $\\left(p+\\right.$   \n651 $2,p+4,p+6)$ . Similarly, the latent continuous data follows a multivariate Gaussian model and   \n652 the true DAG $\\mathcal{G}$ is constructed using BP model. We run 10 graph instances with different seeds and   \n653 report the result of the skeleton discovery and DAG in Fig. 7.   \n654 According to the experiment results, DCT exhibits better performance in terms of the F1-score,   \n655 precision, and SHD relative to both the Fisher-Z test and the Chi-square test. As the graph becomes   \n656 progressively denser, the superiority of the Discrete Causality Test (DCT) correspondingly diminishes   \n657 as there are few conditional independent cases in the true DAG. Due to the same reason, The recall   \n658 remains lower than that of other baseline methods. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "659 C.3 multivariate Gaussian with nonzero mean and non-unit variance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "660 We employed a setting nearly identical to the main experiment, with the only difference being the   \n661 alteration in data generation: instead of using a standard normal distribution, we used a Gaussian   \n662 distribution with mean sampled from $U(-2,2)$ and variance sampled from $U(0,3)$ . We fix the   \n663 number of variables as $p=8$ and change the number of samples $n=(100,500,2000,5000)$ . The   \n664 Fig. 8 shows the result and demonstrates the effectiveness of our method. ", "page_idx": 20}, {"type": "image", "img_path": "B1tCaKP5nB/tmp/1e7e0aee134ea4081a41ceb1cb35b377721b90f83544d512c2e85fbe21349c6a.jpg", "img_caption": ["(d) Nonlinear Gaussian. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 5: Experiment result of causal discovery on synthetic data with $\\textit{p}~=~8$ , $n\\ =$ (100, 500, 2000, 5000) where the data generation process violates our assumptions. The data are generated with either nongaussian distributed (a), (b), (c) or the relations are not linear (d). The figure reports $F_{1}$ (\u2191), Precision $(\\uparrow)$ , Recall $(\\uparrow)$ and SHD $\\left(\\downarrow\\right)$ on skeleton. ", "page_idx": 21}, {"type": "text", "text": "665 C.4 Real-world dataset", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "666 To further validate DCT, we employ it on a real-world dataset: Big Five Personality   \n667 https://openpsychometrics.org/, which includes 50 personality indicators and over 19000 data sam  \n668 ples. Each variable contains 5 possible discrete values to represent the scale of the corresponding   \n669 questions, where $1\\mathrm{=}$ Disagree, $2=$ Weakly disagree, $3{=}$ Neutral, $4\\mathrm{{=}}$ Weakly agree and $5{=}\\mathrm{A}$ gree, e.g.,   \n670 $\"\\mathrm{N}3{=}1\"$ means $\"\\mathbf{I}$ agree that I worry about things\". This scenario clearly suits DCT, where the degree   \n671 of agreement with a certain question must be a continuous variable while we can only observe the   \n672 result after categorization. We choose three variables respectively: [N3: I worry about things], [N10:   \n673 I often feel blue ], [N4: I seldom feel blue]. We then do the casual discovery using PC algorithm with   \n674 DCT and compare it with the Chi-square test and Fisher-Z test. The result can be found in Fig. 9.   \n675 Based on the experimental outcomes, despite the absence of a groundtruth for reference, we observe   \n676 that the results obtained via DCT appear more plausible than those derived from Fisher-Z and Chi  \n677 square tests. Specifically, DCT suggests the relationship $N_{3}$ \u22a5\u22a5 $N4|N10$ , which is reasonable as   \n678 intuitively, the answer of $^{\\circ}\\mathrm{I}$ often feel blue\u2019 already captures the information of $\\mathord{\\leftmoon}$ seldom feel blue\u2019. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "image", "img_path": "B1tCaKP5nB/tmp/8c437f524723329b634e72077302d507fe3d60d75ea785d620840c63301ff6fd.jpg", "img_caption": ["(d) Nonlinear Gaussian. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 6: Experiment result of causal discovery on synthetic data with $\\textit{p}~=~8$ , $n\\ =$ (100, 500, 2000, 5000) where the data generation process violates our assumptions. The data are generated with either nongaussian distributed (a), (b), (c) or the relations are not linear (d). The figure reports $F_{1}$ (\u2191), Precision (\u2191), Recall (\u2191) and SHD (\u2193) on DAG. ", "page_idx": 22}, {"type": "text", "text": "679 As a comparison, both Fisher-Z and Chi-square return a fully connected graph. The results directly   \n680 correspond to our illustrative example shown in Fig. 1, substantiating the necessity of our proposed   \n681 test. ", "page_idx": 22}, {"type": "image", "img_path": "B1tCaKP5nB/tmp/c0e9b9f4089342e94cfeb75178cb350e520f51785317a44b6bab8a4e2ecd733b.jpg", "img_caption": ["Figure 7: Experimental comparison of causal discovery on synthetic datasets for denser graphs with $p=8$ , $n=10000$ and edges varying $p+2,p+4,p+6$ . We evaluate $F_{1}$ ( $\\uparrow)$ , Precision $(\\uparrow)$ , Recall $(\\uparrow)$ and SHD $\\left(\\downarrow\\right)$ on both skeleton and DAG. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "B1tCaKP5nB/tmp/e33f29a5858a79333beb7c7f59c08db4fb9b70ced5670f35e99ab1102eace420.jpg", "img_caption": ["Figure 8: Experimental comparison of causal discovery on synthetic datasets for multivariate Gaussian model with $p=8,n=(100,500,2000,5000)$ and where mean is not zero. We evaluate $F_{1}$ (\u2191), Precision $(\\uparrow)$ , Recall (\u2191) and SHD (\u2193) on both skeleton and DAG. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "682 D Related Work ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "683 Testing for CI is pivotal in the field of causal discovery [30], and a variety of methods exist for   \n684 performing CI tests (CI tests). An important group of CI test methods involves the assumption of   \n685 Gaussian variables with linear dependencies. For example, under this assumption, Gaussian graphical   \n686 models are extensively studied [37, 25, 22, 26]. To address CI test under Gaussian assumption, partial   \n687 correlation serves as a viable method for CI testing [4]. To evaluate the independence of variables   \n688 $X_{1}$ and $X_{2}$ conditional on $_{z}$ , The technique proposed by [32] determines CI by comparing the   \n689 estimations of $p(X_{1}|X_{2},Z)$ and $p(X_{1}|X_{2})$ .   \n690 Another approach involves discretizing $Z$ and performing independent tests within each resulting bin   \n691 [21]. Our work, however, diverges from these existing methods in two significant ways. Firstly, we   \n692 are equipped to handle data, where partial variables are discretized. Additionally, we postulate that   \n693 discrete variables are derived from the transformation of continuous variables in a latent Gaussian   \n694 model. With the same assumption, the most closely related study is by [13], where the authors   \n695 developed a novel rank-based estimator for the precision matrix of mixed data. However, their work   \n696 stops short of providing a CI test for this method. Our research fills this gap, offering the ability to   \n697 estimate the precision matrix for both discrete and mixed data and providing a rigorous CI test for   \n698 our methodology.   \n699 Recent advancements in CI testing have utilized kernel methods for continuous variables influenced   \n700 by nonlinear relationships. [16] describes non-parametric CI relationships using covariance operators   \n701 in reproducing kernel Hilbert spaces (RKHS). KCI test [38] assesses the partial associations of   \n702 regression functions linking $x,y$ , and $z$ , while RCI test [31] aims to enhance the KCI test\u2019s efficiency.   \n703 In KCIP test [12] employs permutations of samples to emulate CI scenarios. CCI test [27] further   \n704 reformulates testing into a process that leverages the capabilities of supervised learning models. For   \n705 discrete variable analysis, the $G^{2}$ test [1] and conditional mutual information [39] are commonly   \n706 employed. However, their method cannot deal with our setting where only discretized version of   \n707 latent variables can be observed. ", "page_idx": 23}, {"type": "image", "img_path": "B1tCaKP5nB/tmp/5e05dfcfc209b55e7652fec7747390106f4be716b4a326bda12c1e83fc6c0885.jpg", "img_caption": ["Figure 9: Experimental comparison of causal discovery on the real-world dataset. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "708 E Resource Usage ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "709 All the experiments are run using Intel(R) Xeon(R) CPU E5-2680 v4 with 55 processors. It costs 4   \n710 hours to run experiments in Section 3.1. ", "page_idx": 24}, {"type": "text", "text": "711 F Limiation and Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "712 Limitation So far, the largest limitation of our method is to treat discretized variables as binary,   \n713 which wastes the available information. Besides that, the parametric assumption limits its generaliz  \n714 ability. However, we need to point out this is pretty normal in CI test fields.   \n715 Broader Impacts The goal of our proposed method is to test the conditional independence relation  \n716 ship given discretized observation. This task is essential and has broad applications. We are confident   \n717 that our method will be beneficial and will not result in negative societal impacts. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "718 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "719 1. Claims   \n720 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n721 paper\u2019s contributions and scope?   \n722 Answer: [Yes]   \n723 Justification: Section1 Introduction and Abstract   \n724 Guidelines:   \n725 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n726 made in the paper.   \n727 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n728 contributions made in the paper and important assumptions and limitations. A No or   \n729 NA answer to this question will not be perceived well by the reviewers.   \n730 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n731 much the results can be expected to generalize to other settings.   \n732 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n733 are not attained by the paper.   \n734 2. Limitations   \n735 Question: Does the paper discuss the limitations of the work performed by the authors?   \n736 Answer: [Yes]   \n737 Justification: Section2.1 line145-line147, Appendix F   \n738 Guidelines:   \n739 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n740 the paper has limitations, but those are not discussed in the paper.   \n741 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n742 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n743 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n744 model well-specification, asymptotic approximations only holding locally). The authors   \n745 should reflect on how these assumptions might be violated in practice and what the   \n746 implications would be.   \n747 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n748 only tested on a few datasets or with a few runs. In general, empirical results often   \n749 depend on implicit assumptions, which should be articulated.   \n750 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n751 For example, a facial recognition algorithm may perform poorly when image resolution   \n752 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n753 used reliably to provide closed captions for online lectures because it fails to handle   \n754 technical jargon.   \n755 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n756 and how they scale with dataset size.   \n757 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n758 address problems of privacy and fairness.   \n759 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n760 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n761 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n762 judgment and recognize that individual actions in favor of transparency play an impor  \n763 tant role in developing norms that preserve the integrity of the community. Reviewers   \n764 will be specifically instructed to not penalize honesty concerning limitations.   \n765 3. Theory Assumptions and Proofs   \nQuestion: For each theoretical result, does the paper provide the full set of assumptions and   \n766   \n767 a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Assumption: Section2 line81 to line 94, Proof: Appendix A. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include theoretical results. \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n3 referenced. \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n5 \u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n8 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n9 by formal proofs provided in appendix or supplemental material. \u2022 Theorems and lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "781 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "82 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n83 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n84 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "86 Justification: Secition3 and Appendix B,C.   \n87 Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "823 Answer: [Yes]   \n824 Justification: We provide the full code in our supplementary.   \n825 Guidelines:   \n826 \u2022 The answer NA means that paper does not include experiments requiring code.   \n827 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n828 public/guides/CodeSubmissionPolicy) for more details.   \n829 \u2022 While we encourage the release of code and data, we understand that this might not be   \n830 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n831 including code, unless this is central to the contribution (e.g., for a new open-source   \n832 benchmark).   \n833 \u2022 The instructions should contain the exact command and environment needed to run to   \n834 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n835 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n836 \u2022 The authors should provide instructions on data access and preparation, including how   \n837 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n838 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n839 proposed method and baselines. If only a subset of experiments are reproducible, they   \n840 should state which ones are omitted from the script and why.   \n841 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n842 versions (if applicable).   \n843 \u2022 Providing as much information as possible in supplemental material (appended to the   \n844 paper) is recommended, but including URLs to data and code is permitted.   \n845 6. Experimental Setting/Details   \n846 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n847 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n848 results?   \n849 Answer: [Yes]   \n850 Justification: Section3 and Appendix B, C.   \n851 Guidelines:   \n852 \u2022 The answer NA means that the paper does not include experiments.   \n853 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n854 that is necessary to appreciate the results and make sense of them.   \n855 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n856 material.   \n857 7. Experiment Statistical Significance   \n858 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n859 information about the statistical significance of the experiments?   \n860 Answer: [Yes]   \n861 Justification: Section 3 and Appendix B, C.   \n862 Guidelines:   \n863 \u2022 The answer NA means that the paper does not include experiments.   \n864 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n865 dence intervals, or statistical significance tests, at least for the experiments that support   \n866 the main claims of the paper.   \n867 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n868 example, train/test split, initialization, random drawing of some parameter, or overall   \n869 run with given experimental conditions).   \n870 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n871 call to a library function, bootstrap, etc.)   \n872 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n873 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n874 of the mean.   \n875 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n876 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n877 of Normality of errors is not verified.   \n878 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n879 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n880 error rates).   \n881 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n882 they were calculated and reference the corresponding figures or tables in the text.   \n883 8. Experiments Compute Resources   \n884 Question: For each experiment, does the paper provide sufficient information on the com  \n885 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n886 the experiments?   \n887 Answer: [Yes]   \n888 Justification: Appendix E.   \n889 Guidelines:   \n890 \u2022 The answer NA means that the paper does not include experiments.   \n891 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n892 or cloud provider, including relevant memory and storage.   \n893 \u2022 The paper should provide the amount of compute required for each of the individual   \n894 experimental runs as well as estimate the total compute.   \n895 \u2022 The paper should disclose whether the full research project required more compute   \n896 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n897 didn\u2019t make it into the paper).   \n898 9. Code Of Ethics   \n899 Question: Does the research conducted in the paper conform, in every respect, with the   \n900 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n901 Answer: [Yes]   \n902 Justification: We completely follow NeurIPS Code of Ethics.   \n903 Guidelines:   \n904 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n905 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n906 deviation from the Code of Ethics.   \n907 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n908 eration due to laws or regulations in their jurisdiction).   \n909 10. Broader Impacts   \n910 Question: Does the paper discuss both potential positive societal impacts and negative   \n911 societal impacts of the work performed?   \n912 Answer: [Yes]   \n913 Justification: We propose a new conditional independence test with applications range in   \n914 multiple fields. Please refer to Appendix F.   \n915 Guidelines:   \n916 \u2022 The answer NA means that there is no societal impact of the work performed.   \n917 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n918 impact or why the paper does not address societal impact.   \n919 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n920 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n921 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n922 groups), privacy considerations, and security considerations.   \n923 \u2022 The conference expects that many papers will be foundational research and not tied   \n924 to particular applications, let alone deployments. However, if there is a direct path to   \n925 any negative applications, the authors should point it out. For example, it is legitimate   \n926 to point out that an improvement in the quality of generative models could be used to   \n927 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n928 that a generic algorithm for optimizing neural networks could enable people to train   \n929 models that generate Deepfakes faster.   \n930 \u2022 The authors should consider possible harms that could arise when the technology is   \n931 being used as intended and functioning correctly, harms that could arise when the   \n932 technology is being used as intended but gives incorrect results, and harms following   \n933 from (intentional or unintentional) misuse of the technology.   \n934 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n935 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n936 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n937 feedback over time, improving the efficiency and accessibility of ML).   \n938 11. Safeguards   \n939 Question: Does the paper describe safeguards that have been put in place for responsible   \n940 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n941 image generators, or scraped datasets)?   \n942 Answer: [NA]   \n943 Justification: Method proposed in this paper don\u2019t pose such risks.   \n944 Guidelines:   \n945 \u2022 The answer NA means that the paper poses no such risks.   \n946 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n947 necessary safeguards to allow for controlled use of the model, for example by requiring   \n948 that users adhere to usage guidelines or restrictions to access the model or implementing   \n949 safety filters.   \n950 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n951 should describe how they avoided releasing unsafe images.   \n952 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n953 not require this, but we encourage authors to take this into account and make a best   \n954 faith effort.   \n955 12. Licenses for existing assets   \n956 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n957 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n958 properly respected?   \n959 Answer: [Yes]   \n960 Justification: We have cited the dataset we use and we provide the code we based in   \n961 Appendix B.   \n962 Guidelines:   \n963 \u2022 The answer NA means that the paper does not use existing assets.   \n964 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n965 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n966 URL.   \n967 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n968 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n969 service of that source should be provided.   \n970 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n971 package should be provided. For popular datasets, paperswithcode.com/datasets   \n972 has curated licenses for some datasets. Their licensing guide can help determine the   \n973 license of a dataset.   \n974 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n975 the derived asset (if it has changed) should be provided.   \n976 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n977 the asset\u2019s creators.   \n978 13. New Assets   \n979 Question: Are new assets introduced in the paper well documented and is the documentation   \n980 provided alongside the assets?   \n981 Answer: [Yes]   \n982 Justification: We have submitted the code.   \n983 Guidelines:   \n984 \u2022 The answer NA means that the paper does not release new assets.   \n985 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n986 submissions via structured templates. This includes details about training, license,   \n987 limitations, etc.   \n988 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n989 asset is used.   \n990 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n991 create an anonymized URL or include an anonymized zip file.   \n992 14. Crowdsourcing and Research with Human Subjects   \n993 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n994 include the full text of instructions given to participants and screenshots, if applicable, as   \n995 well as details about compensation (if any)?   \n996 Answer: [NA]   \n997 Justification: We don\u2019t use any crowdsourcing resoruce.   \n998 Guidelines:   \n999 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n000 human subjects.   \n001 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n002 tion of the paper involves human subjects, then as much detail as possible should be   \n003 included in the main paper.   \n004 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n005 or other labor should be paid at least the minimum wage in the country of the data   \n006 collector. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]