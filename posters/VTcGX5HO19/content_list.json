[{"type": "text", "text": "Bayesian Kernelized Tensor Factorization as Surrogate for Bayesian Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Bayesian optimization (BO) mainly uses Gaussian processes (GP) with a stationary   \n2 and separable kernel function (e.g., the squared-exponential kernel with automatic   \n3 relevance determination [SE-ARD]) as the surrogate model. However, such lo  \n4 calized kernel specifications are deficient in learning complex functions that are   \n5 non-stationary, non-separable and multi-modal. In this paper, we propose using   \n6 Bayesian Kernelized Tensor Factorization (BKTF) as a new surrogate model for   \n7 Bayesian optimization (BO) in a $D$ -dimensional grid with both continuous and   \n8 categorical variables. Our key idea is to approximate the underlying $D$ -dimensional   \n9 solid with a fully Bayesian low-rank tensor CP decomposition, in which we place   \n10 GP priors on the latent basis functions for each dimension to encode local consis  \n11 tency and smoothness. With this formulation, the information from each sample   \n12 can be shared not only with neighbors but also across dimensions, thus fostering a   \n13 more global search strategy. Although BKTF no longer has an analytical posterior,   \n14 we efficiently approximate the posterior distribution through Markov chain Monte   \n15 Carlo (MCMC). We conduct numerical experiments on several test functions with   \n16 continuous variables and two machine learning hyperparameter tuning problems   \n17 with mixed variables. The results show that BKTF offers a flexible and highly   \n18 effective approach to characterizing and optimizing complex functions, especially   \n19 in cases where the initial sample size and budget are severely limited. ", "page_idx": 0}, {"type": "text", "text": "20 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "21 For many applications in science and engineering, such as emulation-based studies, experiment   \n22 design, and automated machine learning, the goal is to optimize a complex black-box function $f({\\pmb x})$   \n23 in a $D$ -dimensional space, for which we have limited prior knowledge. The main challenge in   \n24 such optimization problems is that we aim to efficiently find the global optima, while the objective   \n25 function $f$ is often gradient-free, multimodal and computationally expensive to evaluate. Bayesian   \n26 optimization (BO) offers a powerful statistical approach to these problems, particularly when the   \n27 observation budgets are limited [1, 2, 3]. A typical BO framework consists of two components\u2014a   \n28 surrogate model and an acquisition function (AF)\u2014to balance exploitation and exploration. The   \n29 surrogate is a probabilistic model that allows us to estimate $f(x)$ with uncertainty at a new location   \n30 $\\textbf{\\em x}$ , and the AF is used to determine which location to query next.   \n31 Gaussian process (GP) regression is the most widely used surrogate for BO [3, 4], thanks to its   \n32 appealing properties in providing analytical derivations and uncertainty quantification (UQ). The   \n33 choice of kernel/covariance function is a critical decision in GP models; for multidimensional   \n34 BO problems, perhaps the most popular kernel is the ARD (automatic relevance determination)\u2014   \n35 Squared-Exponential (SE) or Mat\u00e9rn kernel [4]. Although this specification has certain numerical   \n36 advantages and can help automatically learn the importance of input variables, a key limitation is   \n37 that it implies/assumes that the underlying stochastic process is stationary and separable, and the   \n38 value of the covariance function between two random points quickly goes to zero with the increase   \n39 of input dimensionality. These assumptions can be problematic for complex real-world processes   \n40 with long-range dependencies, because estimating the underlying function with a simple ARD kernel   \n41 would require a large number of observations. A potential solution to address this issue is to use   \n42 more flexible kernel structures. The additive kernel, for example, is designed to characterize a more   \n43 \u201cglobal\u201d structure by restricting variable interactions [5]. However, in practice using additive kernels   \n44 requires strong prior knowledge to determine the proper interactions and involves a large number   \n45 of kernel hyperparameters to learn [6]. Another emerging solution is to use deep GP [7], such as in   \n46 [8, 9]; however, learning deep GP often becomes a more challenging task due to the inference of   \n47 latent layers. In addition, these GP related surrogates can be more deficient to tune when taking into   \n48 account both continuous and categorical inputs.   \n49 In this paper, we propose using Bayesian Kernelized Tensor Factorization (BKTF) as a flexible   \n50 and adaptive surrogate model for BO in a $D$ -dimensional Cartesian product space (i.e., grid) when   \n51 $D$ is relatively small (say $D\\leq10,$ ). BKTF is initially developed for modeling multidimensional   \n52 spatiotemporal data with UQ, for tasks such as spatiotemporal kriging/cokriging [10, 11]. This   \n53 paper adapts BKTF to the BO setting, and our key idea is to characterize the multivariate objective   \n54 function ${\\bar{f}}\\left(x\\right)\\,=\\,f\\left(x_{1},\\ldots,x_{D}\\right)$ for a specific BO problem using the low-rank tensor CANDE  \n55 COMP/PARAFAC (CP) factorization with random basis functions. Unlike other basis-function   \n56 models that rely on known/deterministic basis functions [12], BKTF uses a hierarchical Bayesian   \n57 framework to achieve high-quality UQ in a more flexible way\u2014GP priors are used to model the basis   \n58 functions, and hyperpriors are used to model kernel hyperparameters in particular for the lengthscale   \n59 that characterizes the scale of variation. In addition, BKTF also provides a natural solution for   \n60 categorical variables, for which we can simply introduce an inverse-Wishart prior on the covariance   \n61 matrix of the basis functions.   \n62 Figure 1 shows the comparison between BKTF and GP surrogates when optimizing a bivariate   \n63 function $\\mathcal{D}=2$ ) that is nonstationary, nonseparable, and multimodal. The details of this function   \n64 and the BO experiments are provided in Appendix C. This $2D$ case clearly shows that GP surrogate   \n65 is limited by the local kernel and becomes ineffective in finding the global solution, while BKTF   \n66 offers superior flexibility and adaptability to characterize the multidimensional process from limited   \n67 data. Unlike GP-based surrogate models, BKTF no longer has an analytical posterior; however,   \n68 efficient inference and acquisition can be achieved through Markov chain Monte Carlo (MCMC)   \n69 in an element-wise learning way, in which we update basis functions and kernel hyperparameters   \n70 using Gibbs sampling and slice sampling, respectively [11]. For optimization, we first use MCMC   \n71 samples to approximate the posterior distribution of the entire tensor and then naturally define the   \n72 upper confidence bound (UCB) of the posterior as AF. This process is feasible for many real-world   \n73 applications that can be studied in a discretized tensor product space, such as experimental design.   \n74 We conduct extensive experiments on both standard optimization and ML hyperparameter tuning   \n75 tasks. Our results show that BKTF achieves a fast global search for optimizing complex objective   \n76 functions with limited initial data and budget. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "VTcGX5HO19/tmp/2b3bfaa40a8b00cd07b61a64b111d55727e79bea7b1ad38b67f99ef490e7817b.jpg", "img_caption": ["Figure 1: BO for a 2D function: (a) True function surface, where the global maximum is marked; (b) Comparison between BO models using GP surrogates (with two AFs) and BKTF with 30 random initial observations, averaged over 20 replications; (c) Specific results of one run, including the final estimated mean surface for $f$ , in which green dots denote the locations of the selected candidates, and the corresponding AF surface. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "77 2 Bayesian optimization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "78 Let $f:\\mathcal{X}=\\mathcal{X}_{1}\\times...\\times\\mathcal{X}_{D}\\to\\mathbb{R}$ be a black-box function that could be nonconvex, derivative-free,   \n79 and expensive to evaluate. BO aims to address the global optimization problem: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{x}^{\\star}=\\arg\\operatorname*{max}_{\\pmb{x}\\in\\mathcal{X}}f(\\pmb{x}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "80 BO solves this problem by first building a probabilistic model for $f(x)$ (i.e., surrogate model)   \n81 based on initial observations and then using the predictive distribution to decide where in $\\mathcal{X}$ to   \n82 evaluate/query next. The overall goal of BO is to find the global optimum of the objective function   \n83 using as few evaluations as possible. Most BO models rely on a GP prior for $f({\\boldsymbol{x}})$ to achieve   \n84 prediction and UQ: ", "page_idx": 2}, {"type": "equation", "text": "$$\nf(\\pmb{x})=f(x_{1},\\-\\pmb{\\mathscr{s}},x_{D})\\sim\\mathcal{G P}\\left(m\\left(\\pmb{x}\\right),k\\left(\\pmb{x},\\pmb{x}^{\\prime}\\right)\\right),\\ \\mathrm{with}\\ x_{d}\\in\\mathcal{X}_{d},\\ d=1,\\ldots,D,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "85 where $k\\left(\\cdot,\\cdot\\right)$ is a valid kernel/covariance function and $m\\left(\\cdot\\right)$ is a mean function that can be generally   \n86 assumed to be 0. Given a finite set of observation points $\\{x_{i}\\}_{i=1}^{n}$ with $\\mathbf{x}_{i}\\;=\\;\\left(x_{1}^{i},\\ldots,x_{D}^{i}\\right)^{\\top}$ ,   \n87 the vector of function values $\\pmb{f}\\,=\\,\\left(f(\\pmb{x}_{1}),\\dots,f(\\pmb{x}_{n})\\right)^{\\intercal}$ has a multivariate Gaussian distribution   \n88 ${\\pmb f}\\sim\\mathcal{N}\\left({\\bf0},{\\pmb K}\\right)$ , where $\\kappa$ is the $n\\times n$ covariance matrix. For a set of observed data $D_{n}=\\left\\{x_{i},y_{i}\\right\\}_{i=1}^{n}$   \n89 with i.i.d. Gaussian noise, i.e., $y_{i}\\,=\\,f(\\pmb{x}_{i})+\\epsilon_{i}$ where $\\epsilon_{i}\\,\\sim\\mathcal{N}(0,\\tau^{-1})$ , GP gives an analytical   \n90 posterior distribution of $f(x)$ at an unobserved point $x^{*}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f({\\pmb x}^{*})\\mid\\mathcal{D}_{n}\\sim\\mathcal{N}\\Big(k_{x^{*}{\\pmb x}}\\pmb{\\chi}\\left({\\pmb K}+{\\tau}^{-1}{\\pmb I}_{n}\\right)^{-1}{\\pmb y},\\;k({\\pmb x}^{*},{\\pmb x}^{*})-k_{x^{*}{\\pmb x}}\\left({\\pmb K}+{\\tau}^{-1}{\\pmb I}_{n}\\right)^{-1}{\\pmb k}_{x^{*}{\\pmb x}}^{\\top}\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $k_{x^{*}X}=\\left[k(\\pmb{x}^{*},\\pmb{x}_{1}),\\pmb{\\therefore}\\,.\\,.\\,,k(\\pmb{x}^{*},\\pmb{x}_{n})\\right]^{\\intercal}$ and $\\pmb{y}=\\left(y_{1},\\ldots,y_{n}\\right)^{\\top}$ . ", "page_idx": 2}, {"type": "text", "text": "92 Based on the posterior distributions of $f$ , one can   \n93 compute an AF, denoted by $\\alpha~:~\\mathcal{X}~\\rightarrow~\\mathbb{R}$ , for a   \n94 new candidate $x^{*}$ and evaluate how promising $x^{*}$   \n95 is. In BO, the next query point is often determined   \n96 by maximizing a selected/predefined AF, i.e., $x_{n+1}=$   \n97 arg $\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{X}}\\alpha\\left(\\mathbf{x}\\mid D_{n}\\right)$ . Most AFs are based on the   \n98 predictive mean and variance; for example, a com  \n99 monly used AF is the expected improvement (EI)   \n100 [1]: ", "page_idx": 2}, {"type": "table", "img_path": "VTcGX5HO19/tmp/64a2a01ba4d40e15bfb99ff78921a280bfc6bb11b032dc0c2fc6cd2f19b9da83.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "equation", "text": "$$\n\\alpha_{\\mathrm{EI}}\\left({\\pmb x}\\mid{\\mathcal D}_{n}\\right)=\\sigma({\\pmb x})\\varphi\\left(\\frac{\\Delta({\\pmb x})}{\\sigma({\\pmb x})}\\right)\\!+\\!|\\Delta({\\pmb x})|\\,\\Phi\\left(\\frac{\\Delta({\\pmb x})}{\\sigma({\\pmb x})}\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "101 where $\\Delta(\\pmb{x})=\\mu(\\pmb{x})-f_{n}^{\\star}$ is the expected difference between the proposed point $\\textbf{\\em x}$ and the current   \n102 best solution, $f_{n}^{\\star}=\\operatorname*{max}_{\\pmb{x}\\in\\{\\pmb{x}_{i}\\}_{i=1}^{n}}f(\\pmb{x})$ denotes the best function value obtained so far; $\\mu(x)$ and   \n103 $\\sigma(x)$ are the predictive mean and predictive standard deviation at $\\textbf{\\em x}$ , respectively; and $\\varphi(\\cdot)$ and $\\Phi(\\cdot)$   \n104 denote the probability density function (PDF) and the cumulative distribution function (CDF) of   \n105 standard normal, respectively. Another widely applied AF for maximization problems is the upper   \n106 confidence bound (UCB) [13]: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\alpha_{\\mathrm{UCB}}\\left({\\pmb x}\\mid{\\mathcal D}_{n},\\beta\\right)=\\mu({\\pmb x})+\\beta\\sigma({\\pmb x}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "107 where $\\beta$ is a tunable parameter that balances exploration and exploitation. The general BO procedure   \n108 can be summarized as Algorithm 1. ", "page_idx": 2}, {"type": "text", "text": "109 3 BKTF for Bayesian optimization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "110 3.1 Bayesian hierarchical model specification ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "111 Before introducing BKTF, we first construct a $D$ -dimensional grid space corresponding to   \n112 the search space $\\mathcal{X}$ , where $\\scriptstyle{\\mathcal{X}}_{d}$ could be continuous, integer-valued, or categorical. We de  \n113 fine it on $D$ sets $\\{S_{1},\\ldots,S_{D}\\}$ and denote the whole grid by $\\Pi_{d=1}^{D}\\,S_{d}$ : $S_{1}\\,\\times\\,.\\,.\\,\\times\\,S_{D}\\;=$   \n114 $\\{\\left(s_{1},\\ldots,s_{D}\\right)\\mid\\forall d\\in\\{1,\\ldots,D\\},s_{d}\\in S_{d}\\}$ . For dimensions with integer-valued and categorical   \n115 input, we consider $S_{d}$ the set of corresponding discrete values. For dimensions with continuous input,   \n116 the coordinate set $S_{d}$ is formed by $m_{d}$ interpolation points $c_{i}^{d}$ that are distributed over the bounded   \n117 interval $\\mathcal{X}_{d}=[a_{d},b_{d}]$ , i.e., $S_{d}=\\left\\{c_{i}^{d}\\right\\}_{i=1}^{m_{d}}$ with $c_{i}^{d}\\in\\mathcal{X}_{d}$ . The size of $S_{d}$ becomes $|S_{d}|=m_{d}$ , and   \n118 size of the entire grid space is $\\textstyle\\prod_{d=1}^{D}\\left|S_{d}\\right|$ . Note that we do not restrict $S_{d}$ to be uniformly distributed.   \n119 We assume the underlying function $f$ as a stochastic process that is zero-centered. We randomly   \n120 sample an initial dataset including $n_{0}$ input-output data pairs $\\mathcal{D}_{0}\\,=\\,\\{{\\pmb x}_{i},y_{i}\\}_{i=1}^{n_{0}}$ , where $\\{{x}_{i}\\}_{i=1}^{n_{0}}$   \n121 are located in $\\textstyle\\prod_{d=1}^{D}S_{d}$ , and this yields an incomplete $D$ -dimensional tensor $\\pmb{\\mathscr{y}}\\in\\mathbb{R}^{|S_{1}|\\times\\cdots\\times|S_{D}|}$   \n122 with $n_{0}$ obser ved points. BKTF approximates the entire data tensor $\\boldsymbol{y}$ by a kernelized tensor CP   \n123 decomposition: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pmb{\\mathscr{y}}=\\sum_{r=1}^{R}\\lambda_{r}\\cdot\\pmb{g}_{1}^{r}\\circ\\pmb{g}_{2}^{r}\\circ\\cdot\\cdot\\cdot\\circ\\pmb{g}_{D}^{r}+\\pmb{\\mathscr{E}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "124 where $R$ is a pre-specified tensor CP rank, $\\pmb{\\lambda}=\\left(\\lambda_{1},\\ldots,\\lambda_{R}\\right)^{\\top}$ denote weight coefficients that capture   \n125 the magnitude/importance of each rank in the factorization, $\\begin{array}{r}{\\pmb{g}_{d}^{r}=[g_{d}^{r}(s_{d}):s_{d}\\in S_{d}]\\in\\mathbb{R}^{|S_{d}|}}\\end{array}$ denotes   \n126 the $r$ th latent factor for the $d$ th dimension, entries in $\\varepsilon$ are i.i.d. white noises following $\\mathcal{N}(0,\\tau^{-1})$ .   \n127 It should be particularly noted that both the coefficients $\\{\\lambda_{r}\\}_{r=1}^{R}$ and the latent basis functions   \n128 $\\{g_{1}^{r},\\ldots,g_{D}^{r}\\}_{r=1}^{R}$ are random variables. The function approximation for $\\mathbf{\\boldsymbol{x}}=\\left(x_{1},\\ldots,x_{D}\\right)^{\\top}$ is: ", "page_idx": 3}, {"type": "equation", "text": "$$\nf(\\pmb{x})=\\sum_{r=1}^{R}\\lambda_{r}g_{1}^{r}\\left(x_{1}\\right)\\cdot\\cdot\\cdot g_{D}^{r}\\left(x_{D}\\right)=\\sum_{r=1}^{R}\\lambda_{r}\\prod_{d=1}^{D}g_{d}^{r}\\left(x_{d}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "129 For priors, we assume $\\lambda_{r}\\sim\\mathcal{N}\\left(0,1\\right)$ for $r=1,\\ldots,R$ and use a GP prior on the latent factors for   \n130 dimension $d$ with continuous input: ", "page_idx": 3}, {"type": "equation", "text": "$$\ng_{d}^{r}\\left(\\boldsymbol{x}_{d}\\right)\\mid l_{d}^{r}\\sim\\mathcal{G P}\\left(0,k_{d}^{r}\\left(\\boldsymbol{x}_{d},\\boldsymbol{x}_{d}^{\\prime};l_{d}^{r}\\right)\\right),\\;\\mathrm{for}\\;r=1,\\ldots,R,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "131 where $k_{d}^{r}$ is a valid kernel function. In this paper, we choose a Mat\u00e9rn $3/2$ kernel $k_{d}^{r}\\left(x_{d},x_{d}^{\\prime};l_{d}^{r}\\right)=$   \n132 $\\begin{array}{r}{\\sigma^{2}\\left(1+\\frac{\\sqrt{3}|x_{d}-x_{d}^{\\prime}|}{l_{d}^{r}}\\right)\\exp\\left(-\\frac{\\sqrt{3}|x_{d}-x_{d}^{\\prime}|}{l_{d}^{r}}\\right)}\\end{array}$ . We fix the kernel variance of $k_{d}^{r}$ as $\\sigma^{2}=1$ , and only learn   \n133 the lengthscale hyperparameters $l_{d}^{r}$ , since the variances of the model can be captured by $\\lambda$ . One can   \n134 also exclude $\\lambda$ but introduce variance $\\sigma^{2}$ as a kernel hyperparameter on one of the basis functions;   \n135 however, learning kernel hyperparameters is computationally more expensive than learning $\\lambda$ . For   \n136 simplicity, we can also assume the lengthscale parameters to be identical, i.e., $l_{d}^{1}=.\\ldots=\\bar{l}_{d}^{R}=l_{d}$ ,   \n137 for each dimension . The prior distribution for the corresponding latent factor $\\pmb{g}_{d}^{r}$ is then a Gaussian   \n138 distribution: $\\pmb{g}_{d}^{r}\\sim\\mathcal{N}\\left(\\mathbf{0},\\bar{\\pmb{K}}_{d}^{r}\\right)$ , where $K_{d}^{r}$ is the $|S_{d}|\\times|\\bar{S}_{d}|$ covariance matrix computed from $k_{d}^{r}$ .   \n139 We place Gaussian hyperpriors on the log-transformed kernel hyperparameters to ensure positive   \n140 values, i.e., $\\log\\left(l_{d}^{r}\\right)\\sim\\mathcal{N}\\left(\\mu_{l},\\tau_{l}^{-1}\\right)$ . For categorical input, we assume that the corresponding latent   \n141 basis functions $\\pmb{g}_{d}^{r}\\mid\\pmb{\\Lambda}_{d}\\sim\\mathcal{N}\\left(\\mathbf{0},\\pmb{\\Lambda}_{d}^{-1}\\right)$ for $r=1,\\ldots,R$ , where the precision matrix $\\Lambda_{d}$ follows a   \n142 Wishart prior with an identity scale matrix and $|S_{d}|$ degrees of freedom, i.e., $\\mathbf{\\boldsymbol{\\Lambda}}_{d}\\sim\\mathcal{W}\\left(\\boldsymbol{I}_{\\left|S_{d}\\right|},\\left|S_{d}\\right|\\right)$ .   \n143 For noise precision $\\tau$ , we assume a conjugate Gamma prior $\\tau\\sim\\mathrm{Gamma}\\left(a_{0},b_{0}\\right)$ . For dimensions   \n144 with integer variables, we could model the covariance of the basis functions either using a kernel   \n145 matrix or with an inverse-Wishart prior, depending on specific situations. ", "page_idx": 3}, {"type": "text", "text": "146 For observations, we assume each $y_{i}$ in the initial dataset $\\mathcal{D}_{0}$ follows a Gaussian distribution: ", "page_idx": 3}, {"type": "equation", "text": "$$\ny_{i}\\ \\big|\\ \\{g_{d}^{r}\\left(x_{d}^{i}\\right)\\},\\big\\{\\lambda_{r}\\},\\tau\\sim\\mathcal{N}\\left(f\\left(\\pmb{x}_{i}\\right),\\tau^{-1}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "147 3.2 BKTF as a two-layer deep GP ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "148 Here we show the representation of BKTF as a two-layer deep GP. The first layer characterizes   \n149 the generation of latent functions $\\{g_{d}^{r}\\}_{r=1}^{R}$ for dimension $d$ . For the second layer, if we consider   \n150 $\\{g_{1}^{r},\\ldots,g_{D}^{r}\\}_{r=1}^{R}$ as parameters and rewrite the functional decomposition in Eq. (7) as a linear function   \n151 $\\begin{array}{r}{f\\left(x;\\left\\{\\lambda_{r}\\right\\}\\right)=\\sum_{r=1}^{R}\\lambda_{r}\\prod_{d=1}^{D}g_{d}^{r}\\left(x_{d}\\right)}\\end{array}$ with $\\lambda_{r}$ ii\u223cd $\\mathcal{N}(0,1)$ , we can marginalize $\\{\\lambda_{r}\\}$ and obtain a   \n152 fully symmetric multilinear kernel/covariance function for any two data points $\\mathbf{\\boldsymbol{x}}=\\left(x_{1},\\ldots,x_{D}\\right)^{\\top}$   \n153 and $\\mathbf{x}^{\\prime}=\\left(x_{1}^{\\prime},\\ldots,x_{D}^{\\prime}\\right)^{\\top}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nk\\left(\\pmb{x},\\pmb{x}^{\\prime};\\{g_{1}^{r},\\dotsc,g_{D}^{r}\\}_{r=1}^{R}\\right)=\\sum_{r=1}^{R}\\prod_{d=1}^{D}g_{d}^{r}\\left(x_{d}\\right)g_{d}^{r}\\left(x_{d}^{\\prime}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "154 As can be seen, the second layer has a multilinear product kernel function parameterized by   \n155 $\\{g_{1}^{r},\\ldots,g_{D}^{r}\\}_{r=1}^{R}$ . There are some properties to highlight: (i) the kernel is nonstationary since   \n156 the value of $g_{d}^{r}(\\cdot)$ is location specific, and (ii) the kernel is nonseparable when $R>1$ . Therefore,   \n157 this specification is very different from traditional GP surrogates, such as: ", "page_idx": 4}, {"type": "text", "text": "GP ARD: $\\begin{array}{r}{k\\left(\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{x}}^{\\prime}\\right)=\\prod_{d=1}^{D}k_{d}\\left(x_{d},x_{d}^{\\prime}\\right),}\\end{array}$ kernel function is stationary and separable   \n\uf8f4Additive GP (2nd order): $\\begin{array}{r}{k\\left(\\pmb{x},\\pmb{x}^{\\prime}\\right)=\\sum_{d=1}^{D}k_{d}^{1}\\left(x_{d},x_{d}^{\\prime}\\right)+\\sum_{d=1}^{D-1}\\sum_{e=d+1}^{D}k_{d}^{2}\\left(x_{d},x_{d}^{\\prime}\\right)k_{e}^{2}\\left(x_{e},x_{e}^{\\prime}\\right),}\\end{array}$ kerenl is stationary and nonseparable ", "page_idx": 4}, {"type": "text", "text": "158 where all kernel functions are stationary with different hyperparameters (e.g., length scale and   \n159 variance). Compared to the GP-based kernel specification, the multilinear kernel in Eq. (10) has a   \n160 much larger set of hyperparameters and becomes more flexible and adaptive for the data. From a GP   \n161 perspective, learning the hyperparameter in the kernel function in Eq. (10) will be computationally   \n162 expensive; however, we can achieve efficient Bayesian inference of $\\{\\lambda_{r},g_{1}^{r},\\ldots,g_{D}^{r}\\}_{r=1}^{\\mathbf{\\hat{}{\\cal R}}}$ under a   \n163 kernelized tensor factorization framework. ", "page_idx": 4}, {"type": "text", "text": "164 3.3 Model inference ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "165 Unlike GP, BKTF no longer enjoys an analytical posterior distribution. Based on the aforementioned   \n166 prior and hyperprior settings, we adapt the MCMC updating procedure in Ref. [10, 11] to an efficient   \n167 Gibbs sampling algorithm for model inference. This allows us to accommodate observations that are   \n168 not located in the grid space $\\Pi_{d=1}^{D}\\,S_{d}$ . The detailed derivation of the sampling algorithm is given in   \n169 Appendix A. In terms of co mputational cost, we note that updating $\\pmb{g}_{d}^{r}$ and kernel hyperparameters   \n170 requires $\\operatorname*{min}\\left\\{\\mathcal{O}(n^{3}),\\mathcal{O}(|S_{d}|^{3})\\right\\}$ in time. Sparse GP (such as [14]) could be introduced when $n,|S_{d}|$   \n171 become large. See Appendix A, F for detailed discussion/comparison about computation complexity. ", "page_idx": 4}, {"type": "text", "text": "172 3.4 Prediction and AF computation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "173 In each step of function evaluation, we run the MCMC sampling process $K$ iterations for model   \n174 inference, where the first $K_{0}$ samples are taken as burn-in and the last $K\\,-\\,K_{0}$ samples are   \n175 used for posterior approximation. The predictive distribution for any entry $f^{*}$ in the defined grid   \n176 space conditioned on the observed dataset $\\mathcal{D}_{n}$ can be obtained by the Monte Carlo approximation   \n177 $\\begin{array}{r}{p\\left(f^{*}\\mid\\mathcal{D}_{n},\\pmb{\\theta}_{0}\\right)\\approx\\frac{1}{K-K_{0}}\\times\\sum_{k=K_{0}+1}^{K}p\\left(f^{*}\\mid\\left(g_{d}^{r}\\right)^{\\left(k\\right)},\\pmb{\\lambda}^{\\left(k\\right)},\\tau^{\\left(k\\right)}\\right)}\\end{array}$ , where $\\pmb{\\theta}_{0}=\\{\\mu_{l},\\tau_{l},a_{0},b_{0}\\}$ is   \n178 the set of all parameters used in hyperpriors. Although a direct analytical predictive distribution does   \n179 not exist in BKTF, we can use MCMC samples to obtain the mean and variance of all points on the   \n180 grid, thus offering an enumeration-based approach to define AF.   \n181 We define a Bayesian variant of the UCB as the AF by approximating the predictive mean and variance   \n182 (or uncertainty) in ordinary GP-based UCB with the values calculated from MCMC sampling.   \n183 For every MCMC sample after burn-in, i.e., $k\\ >\\ K_{0}$ , we can estimate an output tensor $\\tilde{\\mathcal{F}}^{(k)}$   \n184 over the entire grid space using the latent factors (grd)(k) and the weight vector \u03bb(k): F\u02dc(k)   \n185 $\\begin{array}{r}{\\sum_{r=1}^{R}\\lambda_{r}^{(k)}\\left(\\pmb{g}_{1}^{r}\\right)^{(k)}\\circ\\left(\\pmb{g}_{2}^{r}\\right)^{(k)}\\circ\\cdots\\circ\\left(\\pmb{g}_{D}^{r}\\right)^{(k)}}\\end{array}$ . We can then compute the corresponding mean and   \n186 variance tensors of the $(K-K_{0})$ samples $\\{\\tilde{\\mathcal{F}}_{\\mathrm{~\\hdots~}}^{(k)}\\}_{k=K_{0}+1}^{K}$ k)}kK=K0+1, and denote the two tensors by U and   \n187 $\\boldsymbol{\\nu}$ , respectively. The approximated predictive distribution at each point $\\textbf{\\em x}$ in the space becomes   \n188 $\\tilde{f}(\\pmb{x})\\sim\\mathcal{N}\\left(u(\\pmb{x}),v(\\pmb{x})\\right)$ . Following the definition of UCB in Eq. (5), we define Bayesian UCB   \n189 (B-UCB) at location $\\textbf{\\em x}$ as \u03b1B-UCB $\\left(\\mathbf{\\boldsymbol{x}}\\mid\\mathcal{D},\\beta,\\mathbf{\\boldsymbol{g}}_{d}^{r},\\lambda\\right)=u(\\mathbf{\\boldsymbol{x}})+\\beta\\sqrt{v(\\mathbf{\\boldsymbol{x}})}$ . The next search/query point   \n190 can be determined via $\\begin{array}{r}{\\pmb{x}_{\\mathrm{next}}=\\arg\\operatorname*{max}_{\\pmb{x}\\in\\{\\prod_{d=1}^{D}S_{d}-{D_{n-1}}\\}}\\alpha_{\\mathrm{E}}}\\end{array}$ -UCB ${\\bf\\Pi}({\\bf\\boldsymbol{x}})$ .   \n191 We summarize the implementation procedure of BKTF for BO in Appendix B (see Algorithm 2).   \n192 Given the sequential nature of BO, when a new data point arrives at step $n$ , we can start the MCMC   \n193 with the last iteration of the Markov chains at step $n-1$ to accelerate model convergence. The main   \n194 computational and storage cost of BKTF is to interpolate and save the tensors F\u02dc \u2208R|S1|\u00d7\u00b7\u00b7\u00b7\u00d7|SD|   \n195 over $(K-K_{0})$ iterations for Bayesian AF estimation. This could be prohibitive when the MCMC   \n196 sample size $K$ or the dimensionality $D$ becomes large. To avoid saving the tensors, in practice, we   \n197 can simply use the maximum values of each entry over the $(K-K_{0})$ iterations through iterative   \n198 pairwise comparison. The number of samples after burn-in then implies the value of $\\beta$ in $\\alpha_{\\mathrm{B-UCB}}$ . We   \n199 adopt this simple AF in our numerical experiments.   \n200 A critical challenge in BKTF is that tensor size grows exponentially with the number of dimensions.   \n201 To decrease the computational burden of enumeration-based AF, we also implement BKTF with   \n202 random discretization\u2014randomly selecting candidate points instead of reconstructing the whole   \n203 space, denoted as BKTFrandom. BKTFrandom can be applied to functions with higher dimensions   \n204 (e.g., $D>10,$ ). We discuss the comparison between BKTF and BKTFrandom in Experiments on test   \n205 functions, see Section 5.1. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "206 4 Related work ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "207 The key of BO is to effectively characterize the posterior distribution of the objective function   \n208 from a limited number of observations. The most relevant work to our study is the Bayesian   \n209 Kernelized Factorization (BKF) framework, which has been mainly used for modeling large-scale and   \n210 multidimensional spatiotemporal data with UQ. The key idea is to parameterize the multidimensional   \n211 stochastic processes using a factorization model, in which specific priors are used to encode spatial   \n212 and temporal dependencies. Signature examples of BKF include spatial dynamic factor model   \n213 (SDFM) [15], variational Gaussian process factor analysis (VGFA) [16], and Bayesian kernelized   \n214 matrix/tensor factorization (BKMF/BKTF) [10, 11, 17]. A common solution in these models is to   \n215 use GP prior to modeling the factor matrices, thus encoding spatial and temporal dependencies. In   \n216 addition, for categorical dimensions, BKTF uses an inverse-Wishart prior to modeling the covariance   \n217 matrix for the latent factors. A key difference among these methods is how inference is performed.   \n218 SDFM and BKMF/BKTF are fully Bayesian hierarchical models and they rely on MCMC for model   \n219 inference, where the factors can be updated via Gibbs sampling with conjugate priors; for learning   \n220 the posterior distributions of kernel hyperparameters, SDFM uses the Metropolis-Hastings sampling,   \n221 while BKMF/BKTF uses the more efficient slice sampling. On the other hand, VGFA uses variational   \n222 inference to learn factor matrices, while kernel hyperparameters are learned through maximum a   \n223 posteriori (MAP) estimation without UQ. Overall, BKTF has shown superior performance in modeling   \n224 multidimensional spatiotemporal processes with high-quality UQ for 2D/3D spaces [11, 17].   \n225 The proposed BKTF surrogate models the objective function\u2014as a single realization of a random   \n226 process\u2014using low-rank tensor factorization with random basis functions. This basis function  \n227 based specification is closely related to multidimensional Karhunen-Lo\u00e8ve (KL) expansion [18] for   \n228 stochastic (spatial, temporal, and spatiotemporal) processes. Empirical analysis of KL expansion is   \n229 also known as proper orthogonal decomposition (POD). With a known kernel/covariance function,   \n230 truncated KL expansion allows us to approximate the underlying random process using a set of   \n231 eigenvalues and eigenfunctions derived from the kernel function. Numerical KL expansion is often   \n232 referred to as the Garlekin method, and in practice, the basis functions are often chosen as prespecified   \n233 and deterministic functions [12, 19], such as Fourier basis, wavelet basis, orthogonal polynomials,   \n234 B-splines, empirical orthogonal functions, radial basis functions (RBF), and Wendland functions   \n235 (i.e., compactly supported RBF) (see, e.g., [20], [21], [22], [23]). However, the quality of UQ will be   \n236 undermined as the randomness is fully attributed to the coefficients $\\{\\lambda_{r}\\}$ ; in addition, these methods   \n237 also require a large number of basis functions (or a large rank) to fit complex stochastic processes.   \n238 Different from methods with fixed/known basis functions, BKTF uses a Bayesian hierarchical   \n239 modeling framework to better capture the randomness and uncertainty in the data, in which GP priors   \n240 are used to model the latent factors (i.e., basis functions are also random processes) on different   \n241 dimensions, and hyperpriors are introduced on the kernel hyperparameters. Therefore, BKTF becomes   \n242 a fully Bayesian version of multidimensional KL expansion for stochastic processes with unknown   \n243 covariance from partially observed data, however, without imposing any orthogonal constraint on   \n244 the basis functions. Following the analysis in section 3.2, BKTF is also a special case of a two-layer   \n245 deep Gaussian process [24, 7], where the first layer produces latent factors for each dimension, and   \n246 the second layer has a multilinear kernel parameterized by all latent factors. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "247 5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "248 5.1 Optimization for benchmark test functions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "249 We test the proposed BKTF model for BO on seven benchmark functions that are used for global   \n250 optimization problems [25], with dimension $D$ ranging from 2 to 10. All selected standard functions   \n251 are multimodal; detailed descriptions are given in Appendix D. In fact, we can visually see that   \n252 the standard Damavandi/Schaffer/Griewank functions in Figure 7 (see Appendix D) indeed have a   \n253 low-rank structure. For each function, we assume the initial dataset $\\mathcal{D}_{0}$ contains $n_{0}=D$ observed   \n254 data pairs, and we set the total number of query points to $N=80$ for 4D Griewank and 6D Hartmann   \n255 function, $N=200$ for 10D Griewank, and $N=50$ for others. We rescale the input search range to   \n256 $[0,1]$ for all dimensions and normalize the output data using z-score normalization.   \n257 Model configuration When applying BKTF to continuous test functions, we introduce $m_{d}$ interpo  \n258 lation points for the dth dimension of the input space. The values of $m_{d}$ used for each benchmark   \n259 function are predefined and given in Table 1 (see Appendix D). Setting the resolution grid will require   \n260 certain prior knowledge (e.g., smoothness of the function); and it also depends on the available   \n261 computational resources and the number of entries in the tensor, which grows exponentially with $m_{d}$ .   \n262 In practice, we find that setting $m_{d}=10\\sim100$ is sufficient for most problems. We set the CP rank   \n263 $R=2$ , and for each BO function evaluation run 400 MCMC iterations for model inference where   \n264 the first 200 iterations are taken as burn-in. We use Mat\u00e9rn 3/2 kernel as the covariance function for   \n265 all the test functions.   \n266 Note that for the 10D Griewank function, the grid-based models do not work, and only models built   \n267 in continuous space and BKTFrandom can be performed. For BKTFrandom, in each evaluation we   \n268 randomly select $20\\mathbf{k}$ points in the search space as candidates and choose the one with the best AF as   \n269 the next evaluation location.   \n270 Baselines We compare BKTF with the following BO methods that use GP as the surrogate model:   \n271 (1) GP $\\alpha_{\\mathrm{EI}}$ and (2) GPgrid $\\alpha_{\\mathrm{EI}}$ : GP as the surrogate model and EI as the AF, in continuous space   \n272 $\\textstyle\\prod_{d=1}^{D}{\\mathcal{X}}_{d}$ and Cartesian grid space $\\Pi_{d=1}^{D}\\,S_{d}$ , respectively; (3) GP $\\alpha_{\\mathrm{UCB}}$ and (4) GPgrid $\\alpha_{\\mathrm{UCB}}$ : GP   \n273 as the surrogate model with UCB as the AF where $\\beta=2$ , in $\\textstyle\\prod_{d=1}^{D}{\\mathcal{X}}_{d}$ and $\\Pi_{d=1}^{D}\\,S_{d}$ , respectively;   \n274 (5) additive GP: the sum of two 1st-order additive kernels per dimension as the surrogate with EI   \n275 as the AF, in continuous space. This baseline has the same number of latent functions as BKTF   \n276 $\\mathcal{R}=2$ ) but in a sum-based manner; (6) deepGP: a two-layer fully-Bayesian deep GP with EI as the   \n277 AF, implemented with the deepgp package1.   \n278 Similar as the setting for BKTF, we use Mat\u00e9rn 3/2 kernel in all GP models. Given the computational   \n279 cost, we only compare deepGP on 2D functions [9]. For AF optimization in GP $\\alpha_{\\mathrm{EI}}$ and GP $\\alpha_{\\mathrm{UCB}}$ , we   \n280 first use the DIRECT algorithm [26] and then apply the Nelder-Mead algorithm [27] to further search   \n281 if there exist better solutions. We also compare with SAASBO (Sparse Axis-Aligned Subspace) [28]   \n282 with Hamiltonian Monte Carlo sampling, implemented using BoTorch [29], on the 6D Hartmann and   \n283 10D Griewank functions.   \n284 Results To compare the optimization performance of different models on the benchmark functions,   \n285 we define the absolute error between the global optimum $f^{\\star}$ and the current estimated global   \n286 optimum ${\\hat{f}}^{\\star}$ , i.e., $\\left|f^{\\star}-{\\hat{f}}^{\\star}\\right|$ , as the regret, and examine how regret varies with the number of function   \n287 evaluations. We run the optimization 10 times for every test function with a different set of initial   \n288 observations. The results are summarized in Figure 2. We see that for the 2D functions Branin   \n289 and Schaffer, BKTF clearly finds the global optima much faster than GP surrogate-based baselines.   \n290 For Damavandi function, where the global minimum $(f({\\pmb x}^{\\star})=0)$ is located in a small sharp area   \n291 while the local optimum $(f(\\pmb{x})=2)$ is located at a large smooth area (see Figure 7 in Appendix D),   \n292 GP-based models are trapped around the local optima in most cases ( i.e., $r e g r e t=2$ ) and cannot   \n293 jump out. In contrast, BKTF explores the global characteristics of the objective function over the   \n294 entire search space and reaches the global optimum within 10 iterations of function evaluations.   \n295 For higher dimensional Griewank and Hartmann functions, BKTF successfully arrives at the global   \n296 optima under the given observation budgets, while GP-based comparison methods are prone to be   \n297 stuck around local optima. We compare the regret at the last iteration in Table 2 (Appendix E.2),   \n298 along with the average cost of evaluations. The enumeration-based GP surrogates, i.e., GPgrid $\\alpha_{\\mathrm{EI}}$   \n299 and GPgrid $\\alpha_{\\mathrm{UCB}}$ , perform a little better than direct GP-based search, i.e., GP $\\alpha_{\\mathrm{EI}}$ and GP $\\alpha_{\\mathrm{UCB}}$ on   \n300 Damavandi function, but worse on others. This means that the discretization, to some extent, offers   \n301 possibilities for searching all the alternative points in the space, since in each function evaluation,   \n302 every sample in the space is equally compared solely based on the predictive distribution. Additive   \n303 GP is comparable with $R=2$ BKTF; while the results demonstrate that BKTF can be much more   \n304 flexible than additive GP. As for BKTFrandom, we see that it can alleviate the curse of dimensionality   \n305 and be applied for higher-dimensional problems that may not be performed with a grid but at the   \n306 cost of more evaluation budgets, particularly it costs more iterations for lower-dimensional functions   \n307 compared with BKTF.   \n308 Overall, BKTF reaches the global optimum for every test function and shows superior performance   \n309 for complex objective functions with a faster convergence rate. To intuitively compare the overall per  \n310 formance of different models across multiple experiments/functions, we further estimate performance   \n311 proflies (PPs) [30] (see Appendix E.1), and compute the area under the curve (AUC) for quantitative   \n312 analysis (see bottom right of Figure 2 and Table 2 in Appendix E.2). Our results show that BKTF   \n313 obtains the best performance across all functions.   \n314 Interpretable latent space. The sampled latent functions are interpretable and imply the under  \n315 lying correlations of the objective function. We illustrate the learned periodic (global) patterns in   \n316 Appendix E.3. Effects of hyperpriors. Since we build a fully Bayesian model, the hyperparameters   \n317 of the covariance functions can be updated automatically from the data likelihood and hyperprior.   \n318 Note that in optimization scenarios where the observations are scarce, the prediction performance of   \n319 BKTF highly depends on the hyperprior settings, i.e., $\\pmb{\\theta}_{0}=\\{\\mu_{l},\\tau_{l},a_{0},b_{0}\\}$ . We discuss the effects of   \n320 hyperpriors in Appendix E.4. Effects of rank. The only predefined model parameter is the model   \n321 rank, all other model parameters and hyperparameters are sampled with MCMC. We discuss the   \n322 effects of rank on the 2D nonstationary nonseparable function defined in Introduction (see Figure 1)   \n323 in Appendix E.5. We see that BKTF is robust to rank specification and can find the global solution   \n324 efficiently with rank set as 2, 4, 6 and 8. ", "page_idx": 5}, {"type": "image", "img_path": "VTcGX5HO19/tmp/5f90c53ace0308d884ef9e3c043c1d960542dc3a1455e3adf5be49571b3ea677.jpg", "img_caption": ["Figure 2: Optimization on benchmark test functions, where medians with $25\\%$ and $75\\%$ quartiles of 10 runs are compared. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "325 5.2 Hyperparameter tuning for machine learning ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "326 In this section, we evaluate the performance of BKTF for automatic machine learning (ML) tasks.   \n327 We compare different models to optimize the hyperparameters of two ML algorithms\u2014random forest   \n328 (RF) and neural network (NN)\u2014on classification for the MNIST database of handwritten digits2   \n329 and regression for the Boston housing dataset3. The tuning tasks involve both integer-valued and   \n330 categorical parameters, and the details are given in the Appendix G. We treat those integer-valued   \n331 dimensions as continuous and use a Mat\u00e9rn 3/2 kernel for the basis functions. Given the size of the   \n332 hyperparameter space, we perform BKTFrandom for classification and BKTF for regression. We   \n333 assume that the number of initial observations $\\mathcal{D}_{0}$ equals the number of tuning hyperparameters.   \n334 The total budget $N$ is 20 for the classification task and 50 for the regression. We implement RF   \n335 algorithms using the scikit-learn package and construct NN models by Keras with 2 hidden layers.   \n336 All other model hyperparameters are set as the default values.   \n337 Model configuration We treat all the integer hyperparameters as samples from a continuous   \n338 space and then generate the corresponding Cartesian product space $\\textstyle\\prod_{d=1}^{D}S_{d}$ . One can interpret   \n339 the candidate values for each hyperparameter as the interpolation points in the corresponding input   \n340 dimension. The size of the spanned space $\\Pi\\,S_{d}$ is $91\\times46\\times64\\times10\\times11\\times2$ and $91\\times46\\times13\\times10$   \n341 for RF classifier and RF regressor, respe ctively; $91\\times49\\times31\\times18\\times3\\times2$ and $91\\times49\\times31$ for   \n342 NN classifier and NN regressor respectively. We set the tensor rank $R=4$ for BKTF, set $K=400$   \n343 and $K_{0}=200$ for MCMC inference, and use the Mat\u00e9rn 3/2 kernel for capturing correlations.   \n344 Baselines In addition to GP surrogate-based GP $\\alpha_{\\mathrm{EI}}$ and GP $\\alpha_{\\mathrm{UCB}}$ , we also compare with a baseline   \n345 method: random search (RS), and two non-GP models: particle swarm optimization (PSO) [31] and   \n346 Tree-structured Parzen Estimator (BO-TPE) [32], which are common methods for hyperparameter   \n347 tuning. We exclude grid-based GP models as sampling the entire grid becomes infeasible.   \n348 Results We compare the accuracy for MNIST classification and MSE (mean squared error) for   \n349 Boston housing regression both in terms of the number of function evaluations and still run the   \n350 optimization processes ten times with different initial datasets $\\mathcal{D}_{0}$ . The results obtained by different   \n351 BO models are given in Figure 3, and the final classification accuracy and regression MSE are   \n352 compared in Table 5 (see Appendix H). For BKTF, we see from Figure 3 that the width between the   \n353 two quartiles of accuracy and error decreases as more iterations are evaluated, and the median curves   \n354 present better convergence rates compared to the baselines. Table 5 also shows that the proposed   \n355 BKTF surrogate achieves the best final mean accuracy and regression error with small standard   \n356 deviations. The results above demonstrate the advantage of BKTF as a surrogate. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "VTcGX5HO19/tmp/eabf2ac2e9a16a577614d055e75fe747c649c47cb4762c475289a52e0afb2ebe.jpg", "img_caption": ["Figure 3: Comparison of hyperparameter tuning for ML tasks: (a) classification; (b) regression. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "357 6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "358 This paper proposes to use Bayesian Kernelized Tensor Factorization (BKTF) as a new surrogate   \n359 model for Bayesian optimization with mixed variables (both discrete/categorical and continuous)   \n360 when the dimensionality is relatively small (e.g., say $D<10$ ). Compared with traditional GP surro  \n361 gates, the BKTF surrogate is more flexible and adaptive to data thanks to the Bayesian hierarchical   \n362 specification, which provides high-quality UQ for BO tasks. The tensor factorization model behind   \n363 BKTF offers an effective solution to capture global/long-range correlations and cross-dimension cor  \n364 relations. The inference of BKTF is achieved through MCMC, which provides a natural solution for   \n365 acquisition. Experiments on both test function optimization and ML hyperparameter tuning confirm   \n366 the superiority of BKTF as a surrogate for BO. Moreover, the tensor factorization framework makes   \n367 it straightforward to adapt BKTF to handle multivariate and functional output (see e.g., [33, 34]), by   \n368 directly treating the output coordinates as part of the input. A limitation of BKTF is that we restrict   \n369 BO to a grid search space in order to leverage tensor factorization; however, we believe that designing   \n370 a compatible grid space based on prior knowledge is not a challenging task.   \n371 There are several directions to be explored to make BKTF more scalable. Scalable GP solutions, such   \n372 as sparse GP [14] and Gaussian Markov Random Field (GMRF) [35], can be introduced to reduce   \n373 the inference cost when $|S_{d}|$ becomes large. The current MCMC-based acquisition method requires   \n374 explicit reconstruction of the whole tensor, which quickly becomes infeasible when $D$ becomes large   \n375 (e.g., $D>20)$ ). A natural question is whether it is possible to achieve efficient acquisition directly   \n376 using the basis functions and the corresponding weights without explicitly constructing the tensors.   \n377 This problem corresponds to finding/locating the maximum entry of a tensor given its low-rank   \n378 decomposition (see e.g., [36]).   \n379 This work aims to advance the field of probabilistic machine learning, particularly Bayesian opti  \n380 mization. Regardless of the model limitations, it has the potential of misuse for ML algorithms. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "381 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "382 [1] Roman Garnett. Bayesian Optimization. Cambridge University Press, 2023.   \n383 [2] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas. Taking   \n384 the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE,   \n385 104(1):148\u2013175, 2015.   \n386 [3] Robert B Gramacy. Surrogates: Gaussian Process Modeling, Design, and Optimization for the   \n387 Applied Sciences. Chapman and Hall/CRC, 2020.   \n388 [4] Christopher KI Williams and Carl Edward Rasmussen. Gaussian Processes for Machine   \n389 Learning. MIT Press, Cambridge, MA, 2006.   \n390 [5] David K Duvenaud, Hannes Nickisch, and Carl Rasmussen. Additive Gaussian processes.   \n391 Advances in neural information processing systems, 24, 2011.   \n392 [6] Mickael Binois and Nathan Wycoff. A survey on high-dimensional Gaussian process modeling   \n393 with application to Bayesian optimization. ACM Transactions on Evolutionary Learning and   \n394 Optimization, 2(2):1\u201326, 2022.   \n395 [7] Andreas Damianou and Neil D Lawrence. Deep Gaussian processes. In International Conference   \n396 on Artificial Intelligence and Statistics, pages 207\u2013215, 2013.   \n397 [8] Ali Hebbal, Lo\u00efc Brevault, Mathieu Balesdent, El-Ghazali Talbi, and Nouredine Melab.   \n398 Bayesian optimization using deep gaussian processes with applications to aerospace system   \n399 design. Optimization and Engineering, 22:321\u2013361, 2021.   \n400 [9] Annie Sauer, Robert B Gramacy, and David Higdon. Active learning for deep gaussian process   \n401 surrogates. Technometrics, 65(1):4\u201318, 2023.   \n402 [10] Mengying Lei, Aurelie Labbe, Yuankai Wu, and Lijun Sun. Bayesian kernelized matrix   \n403 factorization for spatiotemporal traffic data imputation and kriging. IEEE Transactions on   \n404 Intelligent Transportation Systems, 23(10):18962\u201318974, 2022.   \n405 [11] Mengying Lei, Aurelie Labbe, and Lijun Sun. Bayesian complementary kernelized learning for   \n406 multidimensional spatiotemporal data. arXiv preprint arXiv:2208.09978, 2022.   \n407 [12] Noel Cressie, Matthew Sainsbury-Dale, and Andrew Zammit-Mangion. Basis-function models   \n408 in spatial statistics. Annual Review of Statistics and Its Application, 9:373\u2013400, 2022.   \n409 [13] Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine   \n410 Learning Research, 3(Nov):397\u2013422, 2002.   \n411 [14] Joaquin Quinonero-Candela and Carl Edward Rasmussen. A unifying view of sparse approxi  \n412 mate Gaussian process regression. The Journal of Machine Learning Research, 6:1939\u20131959,   \n413 2005.   \n414 [15] Hedibert Freitas Lopes, Esther Salazar, and Dani Gamerman. Spatial dynamic factor analysis.   \n415 Bayesian Analysis, 3(4):759\u2013792, 2008.   \n416 [16] Jaakko Luttinen and Alexander Ilin. Variational Gaussian-process factor analysis for modeling   \n417 spatio-temporal data. Advances in Neural Information Processing Systems, 22:1177\u20131185,   \n418 2009.   \n419 [17] Mengying Lei, Aurelie Labbe, and Lijun Sun. Scalable spatiotemporally varying coefficient   \n420 modeling with bayesian kernelized tensor regression. arXiv preprint arXiv:2109.00046, 2021.   \n421 [18] Limin Wang. Karhunen-Loeve expansions and their applications. London School of Economics   \n422 and Political Science (United Kingdom), 2008.   \n423 [19] Holger Wendland. Scattered Data Approximation, volume 17. Cambridge university press,   \n424 2004.   \n425 [20] Rommel G Regis and Christine A Shoemaker. A stochastic radial basis function method for the   \n426 global optimization of expensive functions. INFORMS Journal on Computing, 19(4):497\u2013509,   \n427 2007.   \n428 [21] Gregory Beylkin, Jochen Garcke, and Martin J Mohlenkamp. Multivariate regression and   \n429 machine learning with sums of separable functions. SIAM Journal on Scientific Computing,   \n430 31(3):1840\u20131857, 2009.   \n431 [22] Christopher K Wikle and Noel Cressie. A dimension-reduced approach to space-time kalman   \n432 filtering. Biometrika, 86(4):815\u2013829, 1999.   \n433 [23] Mathilde Chevreuil, R\u00e9gis Lebrun, Anthony Nouy, and Prashant Rai. A least-squares method   \n434 for sparse low rank approximation of multivariate functions. SIAM/ASA Journal on Uncertainty   \n435 Quantification, 3(1):897\u2013921, 2015.   \n436 [24] Alexandra M Schmidt and Anthony O\u2019Hagan. Bayesian inference for non-stationary spatial   \n437 covariance structure via spatial deformations. Journal of the Royal Statistical Society: Series B   \n438 (Statistical Methodology), 65(3):743\u2013758, 2003.   \n439 [25] Momin Jamil and Xin-She Yang. A literature survey of benchmark functions for global   \n440 optimization problems. arXiv preprint arXiv:1308.4008, 2013.   \n441 [26] Donald R Jones, Cary D Perttunen, and Bruce E Stuckman. Lipschitzian optimization without   \n442 the lipschitz constant. Journal of optimization Theory and Applications, 79(1):157\u2013181, 1993.   \n443 [27] John A Nelder and Roger Mead. A simplex method for function minimization. The computer   \n444 journal, 7(4):308\u2013313, 1965.   \n445 [28] David Eriksson and Martin Jankowiak. High-dimensional bayesian optimization with sparse   \n446 axis-aligned subspaces. In Uncertainty in Artificial Intelligence, pages 493\u2013503. PMLR, 2021.   \n447 [29] Maximilian Balandat, Brian Karrer, Daniel Jiang, Samuel Daulton, Ben Letham, Andrew G Wil  \n448 son, and Eytan Bakshy. Botorch: A framework for efficient monte-carlo bayesian optimization.   \n449 Advances in neural information processing systems, 33:21524\u201321538, 2020.   \n450 [30] Elizabeth D Dolan and Jorge J Mor\u00e9. Benchmarking optimization software with performance   \n451 profiles. Mathematical programming, 91(2):201\u2013213, 2002.   \n452 [31] Jun Tang, Gang Liu, and Qingtao Pan. A review on representative swarm intelligence algorithms   \n453 for solving optimization problems: Applications and trends. IEEE/CAA Journal of Automatica   \n454 Sinica, 8(10):1627\u20131643, 2021.   \n455 [32] James Bergstra, R\u00e9mi Bardenet, Yoshua Bengio, and Bal\u00e1zs K\u00e9gl. Algorithms for hyper  \n456 parameter optimization. Advances in neural information processing systems, 24, 2011.   \n457 [33] Dave Higdon, James Gattiker, Brian Williams, and Maria Rightley. Computer model calibration   \n458 using high-dimensional output. Journal of the American Statistical Association, 103(482):570\u2013   \n459 583, 2008.   \n460 [34] Shandian Zhe, Wei Xing, and Robert M Kirby. Scalable high-order gaussian process regression.   \n461 In The 22nd International Conference on Artificial Intelligence and Statistics, pages 2611\u20132620.   \n462 PMLR, 2019.   \n463 [35] Havard Rue and Leonhard Held. Gaussian Markov random fields: theory and applications.   \n464 Chapman and Hall/CRC, 2005.   \n465 [36] Anastasiia Batsheva, Andrei Chertkov, Gleb Ryzhakov, and Ivan Oseledets. Protes: probabilistic   \n466 optimization with tensor sampling. Advances in Neural Information Processing Systems, 36:808\u2013   \n467 823, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "468 Appendix ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "469 Contents (Appendix) ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "470 A Model inference 12 ", "page_idx": 11}, {"type": "text", "text": "471 A.1 Sampling latent functions . 12   \n472 A.2 Sampling kernel hyperparameters 13   \n473 A.3 Sampling $\\Lambda_{d}$ for latent functions on categorical inputs 13   \n474 A.4 Sampling weight vector . . . 14   \n475 A.5 Sampling model noise precision 14 ", "page_idx": 11}, {"type": "text", "text": "476 B Algorithm of BKTF for BO 15 ", "page_idx": 11}, {"type": "text", "text": "477 C Optimization for the 2D nonstationary and nonseparable function 15 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "478 C.1 Data generation . . 15   \n479 C.2 Experimental setting 15   \n480 C.3 Results . 15 ", "page_idx": 11}, {"type": "text", "text": "481 D Benchmark test functions 17 ", "page_idx": 11}, {"type": "text", "text": "483 E.1 Performance profiles 19   \n484 E.2 Quantitative comparison . . 20   \n485 E.3 Interpretation of results . . . 21   \n486 E.4 Effects of hyperpriors . . 22   \n487 E.5 Effects of rank . . 23 ", "page_idx": 11}, {"type": "text", "text": "488 F Comparison of computational complexity 24 ", "page_idx": 11}, {"type": "text", "text": "489 G Hyperparameter tuning for machine learning 25 ", "page_idx": 11}, {"type": "text", "text": "490 H Supplementary results on ML hyperparameter tuning 26 ", "page_idx": 11}, {"type": "text", "text": "491 A Model inference ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "492 Assume an observation dataset $\\boldsymbol{D}_{n}\\;=\\;\\{\\mathbf{x}_{i},y_{i}\\}_{i=1}^{n}$ , we exploit an efficient element-wise Gibbs   \n493 sampling algorithm for model inference. ", "page_idx": 11}, {"type": "text", "text": "494 A.1 Sampling latent functions ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "495 Given the Gaussian prior and Gaussian likelihood of the latent factors $\\pmb{g}_{d}^{r}$ , their posterior distributions   \n496 are still Gaussian. Let $\\begin{array}{r}{y_{r}^{i}\\;=\\;y_{i}\\:-\\:\\sum_{\\stackrel{h=1}{h\\neq r}}^{R}\\lambda_{h}\\prod_{d=1}^{D}g_{d}^{h}\\left(x_{d}^{i}\\right)}\\end{array}$ and $\\pmb{y}_{r}\\;=\\;[\\pmb{y}_{r}^{1},\\...\\;,\\pmb{y}_{r}^{n}]^{\\top}\\;\\in\\;\\mathbb{R}^{n}$ for   \n497 $r=1,\\ldots,R$ . Every ${\\pmb y}_{r}$ generates a $D$ -dimensional tensor $\\pmb{\\mathscr{D}}_{r}\\,\\in\\,\\mathbb{R}^{|S_{1}|\\,\\times\\,\\dots\\,\\times\\,|S_{D}|}$ in the Cartesian   \n498 product space $\\Pi_{d=1}^{D}\\,S_{d}$ . We define a binary tensor $\\sigma$ with the same size of $\\boldsymbol{y}$ indicating the locations   \n499 of the observa tion data, where $o\\left(\\pmb{x}_{i}\\right)=1$ for $i\\in[1,n]$ , and other values are zero. The posterior of ", "page_idx": 11}, {"type": "text", "text": "500 $\\pmb{g}_{d}^{r}$ is given by ", "page_idx": 12}, {"type": "equation", "text": "$$\np\\left(\\pmb{g}_{d}^{r}\\mid-\\right)=\\mathcal{N}\\left(\\pmb{g}_{d}^{r}\\;\\Big|\\;\\big[\\pmb{\\mu}_{d}^{r}\\big]^{*}\\;,\\left(\\big[\\pmb{\\Lambda}_{d}^{r}\\big]^{*}\\right)^{-1}\\right),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "501 where ", "page_idx": 12}, {"type": "equation", "text": "$$\n[\\mu_{d}^{r}]^{*}=\\left([\\Lambda_{d}^{r}]^{*}\\right)^{-1}\\left(\\tau\\left(Y_{r(d)}\\circledast O_{(d)}\\right)\\left(\\lambda_{r}\\bigotimes_{h=D\\atop h\\neq d}g_{h}^{r}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "502 ", "page_idx": 12}, {"type": "equation", "text": "$$\n[\\mathbf{A}_{d}^{r}]^{*}=\\tau\\mathop{\\mathrm{diag}}\\underbrace{\\left(O_{(d)}\\left(\\lambda_{r}\\bigotimes_{h=D}^{1}g_{h}^{r}\\right)^{2}\\right)}_{b\\in\\mathbb{R}^{|S_{d}|}}+\\left(K_{d}^{r}\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "503 $\\boldsymbol{Y}_{r(d)}$ and $O_{(d)}$ are mode- $d$ unfoldings of ${\\mathfrak{y}}_{r}$ and $\\sigma$ , respectively, with the size of $|S_{d}|~\\times$   \n504 $(\\prod_{h=1}^{D}\\,\\left|S_{h}\\right|)$ . Note that the vector term $\\textbf{\\em a}$ in $[\\pmb{\\mu}_{d}^{r}]^{*}$ and $^{b}$ in $ \\left[\\mathbf{A}_{d}^{r}\\right]^{*}$ , which are only relevant to   \n$h\\not=d$   \n505 the $n$ observations and corresponding function values, can be computed element-wise instead of   \n506 using matrix multiplication and Kronecker product. The point-wise computation can dramatically   \n507 reduce the computational cost, especially for a relatively large $D$ , since in such case the number of   \n508 observations in BO can be much smaller compared to the number of samples in the entire grid space,   \n509 i.e., $\\begin{array}{r}{n\\ll\\prod_{d=1}^{D}\\left|S_{d}\\right|}\\end{array}$ . ", "page_idx": 12}, {"type": "text", "text": "510 A.2 Sampling kernel hyperparameters ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "511 We update the kernel lengthscale hyperparameters $l_{d}^{r}$ from their marginal posteriors by integrating   \n512 out the latent factors. Let $[\\pmb{y}_{r}]_{d}=O_{d}\\operatorname{vec}\\left(\\pmb{Y}_{r(d)}\\right)\\in\\mathbb{R}^{n}$ , where $O_{d}\\in\\mathbb{R}^{n\\times\\left(\\prod_{d=1}^{D}\\vert S_{d}\\vert\\right)}$ is a binary   \n513 matrix obtained by removing the rows corresponding to zeros in vec $\\left(O_{(d)}\\right)$ from $I_{\\prod_{d=1}^{D}\\mid S_{d}\\mid}$ . When   \n514 sampling the posteriors for kernel hyperparameters under a given $d$ and $r$ , their marginal likelihoods   \n515 only relate to $\\left[{\\pmb y}_{r}\\right]_{d}$ . The log marginal likelihood of $l_{d}^{r}$ , for example, is: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log p\\left(\\left[\\boldsymbol{y}_{r}\\right]_{d}\\mid l_{d}^{r}\\right)\\propto-\\frac{1}{2}\\left(\\left[\\boldsymbol{y}_{r}\\right]_{d}\\right)^{\\top}\\mathbf{\\Sigma}_{\\left[\\boldsymbol{y}_{r}\\right]_{d}\\mid l_{d}^{r}}^{-1}\\left[\\boldsymbol{y}_{r}\\right]_{d}-\\frac{1}{2}\\log\\operatorname*{det}\\left(\\boldsymbol{\\Sigma}_{\\left[\\boldsymbol{y}_{r}\\right]_{d}\\mid l_{d}^{r}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\propto\\frac{\\tau^{2}}{2}\\underbrace{\\left(\\left[\\boldsymbol{y}_{r}\\right]_{d}\\right)^{\\top}H\\left(\\left[\\boldsymbol{\\Lambda}_{d}^{r}\\right]^{*}\\right)^{-1}H^{\\top}\\left[\\boldsymbol{y}_{r}\\right]_{d}}_{c}-\\frac{1}{2}\\log\\operatorname*{det}\\left(\\left[\\boldsymbol{\\Lambda}_{d}^{r}\\right]^{*}\\right)-\\frac{1}{2}\\log\\operatorname*{det}\\left(\\boldsymbol{K}_{d}^{r}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "516 where $H=O_{d}\\left(\\lambda_{r}\\bigotimes_{\\stackrel{h=D}{h\\neq d}}^{1}g_{h}^{r}\\otimes I_{(|S_{d}|)}\\right)\\in\\mathbb{R}^{n\\times|S_{d}|}$ and $\\pmb{\\Sigma}_{[\\pmb{y}_{r}]_{d}}|_{l_{d}^{r}}=\\pmb{H}\\pmb{K}_{d}^{r}\\pmb{H}^{\\top}+\\tau^{-1}\\pmb{I}_{n}$ . The ", "page_idx": 12}, {"type": "text", "text": "517 term $c$ in Eq. (14) is a scalar that can be computed with $\\pmb{u}^{\\top}\\pmb{u}$ , where $\\pmb{u}\\,=\\,(\\pmb{L}_{d}^{r})^{-1}\\,\\pmb{a}$ is a vector   \n518 of length $|S_{d}|$ ${\\it{d}}|;\\,{\\cal L}_{d}^{r}\\,=\\,\\mathrm{chol}\\left([{\\bf A}_{d}^{r}]^{*}\\right)$ is the Cholesky factor matrix of $ \\left[\\mathbf{A}_{d}^{r}\\right]^{*}$ . This means that the   \n519 complicated term $c$ can also be calculated element-wise, and it leads to a fast learning process. With   \n520 the marginal likelihood and predefined log-normal hyperpriors, we can get the marginal posteriors   \n521 of the kernel hyperparameters straightforwardly; and we update them by using the slice sampling   \n522 algorithm presented in [17]. ", "page_idx": 12}, {"type": "text", "text": "523 A.3 Sampling $\\Lambda_{d}$ for latent functions on categorical inputs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "524 For latent factors in dimensions with categorical variables/inputs, we sample the precision hyperpa  \n525 rameter $\\Lambda_{d}$ in its prior distribution from a Wishart distribution: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbf{\\boldsymbol{\\Lambda}}_{d}\\mid-\\sim\\mathcal{W}\\left(\\left(G_{d}\\mathbf{\\boldsymbol{G}}_{d}^{\\top}+\\mathbf{\\boldsymbol{I}}_{\\mid S_{d}\\mid}\\right)^{-1},|S_{d}|+R\\right).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "526 A.4 Sampling weight vector ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "527 Every observed data point has the following distribution: ", "page_idx": 13}, {"type": "equation", "text": "$$\ny_{i}\\sim\\mathcal{N}\\left(\\sum_{r=1}^{R}\\lambda_{r}\\prod_{d=1}^{D}g_{d}^{r}\\left(x_{d}^{i}\\right)=\\left[\\prod_{d=1}^{D}g_{d}^{1}\\left(x_{d}^{i}\\right),\\ldots,\\prod_{d=1}^{D}g_{d}^{R}\\left(x_{d}^{i}\\right)\\right]\\lambda,\\tau^{-1}\\right),\\;i=1,\\ldots,n.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "528 Let $\\begin{array}{r}{\\pmb{g}\\left(\\pmb{x}_{i}\\right)=\\left(\\prod_{d=1}^{D}g_{d}^{1}\\left(x_{d}^{i}\\right),\\ldots,\\prod_{d=1}^{D}g_{d}^{R}\\left(x_{d}^{i}\\right)\\right)^{\\top}\\in\\mathbb{R}^{R}}\\end{array}$ and $\\pmb{y}=\\left(y_{1},\\ldots,y_{n}\\right)^{\\top}\\in\\mathbb{R}^{n}$ ; we then   \n529 have $\\pmb{y}\\sim\\mathcal{N}\\left(\\tilde{\\pmb{G}}^{\\top}\\pmb{\\lambda},\\tau^{-1}\\pmb{I}_{n}\\right)$ , where $\\tilde{G}=[\\pmb{g}(\\pmb{x}_{1}),\\dots,\\pmb{g}(\\pmb{x}_{n})]\\in\\mathbb{R}^{R\\times n}$ . The posterior of $\\lambda$ follows   \n530 a Gaussian distribution ", "page_idx": 13}, {"type": "equation", "text": "$$\np\\left(\\mathbf{\\lambda}\\right\\vert-)\\sim\\mathcal{N}\\left(\\mu_{\\lambda}^{*},(\\mathbf{\\Lambda}\\mathbf{A}_{\\lambda}^{*})^{-1}\\right),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "531 where ", "page_idx": 13}, {"type": "text", "text": "532 ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{\\mu}_{\\lambda}^{*}=\\tau\\left(\\pmb{\\Lambda}_{\\lambda}^{*}\\right)^{-1}\\tilde{G}\\pmb{y},}\\\\ {\\pmb{\\Lambda}_{\\lambda}^{*}=\\tau\\tilde{G}\\tilde{G}^{\\top}+\\pmb{I}_{R}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "533 A.5 Sampling model noise precision ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "534 For precision $\\tau$ , we have a Gamma posterior ", "page_idx": 13}, {"type": "equation", "text": "$$\np\\left(\\tau\\mid-\\right)=\\mathrm{Gamma}\\left(\\tau\\mid a^{*},b^{*}\\right),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "535 where ", "page_idx": 13}, {"type": "text", "text": "536 ", "page_idx": 13}, {"type": "equation", "text": "$$\na^{*}=a_{0}+\\frac{1}{2}n,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\nb^{*}=b_{0}+\\frac{1}{2}\\sum_{i=1}^{n}\\left(y_{i}-\\sum_{r=1}^{R}\\lambda_{r}\\prod_{d=1}^{D}g_{d}^{r}\\left(x_{d}^{i}\\right)\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "table", "img_path": "VTcGX5HO19/tmp/5cd77824aee005a9660d7659ea12e8c991bce316dc9c1f77badda8e067df2d5d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "539 C Optimization for the 2D nonstationary and nonseparable function ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "540 C.1 Data generation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "541 The function in Figure 1(a) of the paper (see Section 1 Introduction) is a modification of the case   \n542 study used in [11]. It is a $40\\times40\\,2\\mathrm{D}$ process generated in a $[1,2]\\times[-1,0]$ square, with ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Y(x_{1},x_{2})=(\\cos\\left(4\\left[f_{1}(x_{1})+f_{2}(x_{2})\\right]\\right)+\\sin\\left(4\\left[f_{1}(x_{2})-f_{2}(x_{1})\\right]\\right)-1)}\\\\ &{\\phantom{Y(x_{1},x_{2})=(\\cos\\left(4\\left[f_{1}(x_{1})+f_{2}(x_{2})\\right]\\right)+\\sin\\left(4\\left[f_{1}(x_{2})-f_{2}(x_{1})\\right]\\right))-1}\\times}\\\\ &{\\phantom{Y(x_{1},x_{2})=(\\cos\\left(4\\left[f_{1}(x_{1})+f_{2}(x_{2})\\right]\\right)+\\sin\\left(4\\left[f_{2}(x_{2})-f_{2}(x_{1})\\right]\\right))-1}\\times}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "543 where $f(x_{1})=x_{1}$ $\\!\\,{\\bigl(}\\sin2x_{1}+2{\\bigr)}$ , $f(x_{2})=0.2x_{2}\\sqrt{99(x_{2}+1)+4}$ , $x_{1}\\in[1,2]$ , $x_{2}\\in[-1,0]$ . This   \n544 is a nonstationary, nonseparable, and multimodel function, with the global maximum $f(\\pmb{x}^{\\star})=0.6028$   \n545 at $\\pmb{x}^{\\star}=(1.75,-0.55)$ . ", "page_idx": 14}, {"type": "text", "text": "546 C.2 Experimental setting ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "547 We randomly select $n_{0}~=~30\\$ data points as the initial data and run 50 iterations (i.e., budget)   \n548 of evaluation for optimization. To compare different surrogates, we run the optimization for 20   \n549 replications with different initial datasets. For the proposed BKTF surrogate, we place Mat\u00e9rn $3/2$   \n550 kernel functions on the latent factors, set the rank $R=4$ , and run 1000 MCMC samples for model   \n551 inference where the first 600 samples are burn-in. For comparison of BO methods, we consider   \n552 typical GP surrogate with both EI and UCB $(\\beta=2)$ as the AF, denoted by GP $\\alpha_{\\mathrm{EI}}$ and GP $\\alpha_{\\mathrm{UCB}}$   \n553 respectively, and use the same Mat\u00e9rn $3/2$ kernel for GP surrogate. ", "page_idx": 14}, {"type": "text", "text": "554 C.3 Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "555 Figure 1(b) in Section 1 Introduction of the paper shows the medians along with the $25\\%$ and $75\\%$   \n556 quantiles of the optimization results from 20 runs. We see that GP $\\alpha_{\\mathrm{EI}}$ and GP $\\alpha_{\\mathrm{UCB}}$ cannot find the   \n557 global optimum in most cases, and they easily get stuck in the lower left flat area which contains   \n558 easily find local optima. Figure 1(c) illustrates the estimation surface of the function and the estimated   \n559 AF surface from one run. It is clear that BKTF can capture global correlations with limited data. The   \n560 search points contain areas of almost every peak in the true function, and the peaks of its AF surface   \n561 also reflect the peak area in $f$ .   \n562 Figure 4 shows the approximated posterior distributions of model parameters in BKTF. Figure 5 and   \n563 Figure 6 illustrate the posterior mean and the last 20 MCMC samples of the latent factors, respectively.   \n564 Samples of the latent factors in panels (a) and (b) of Figure 6 explain the uncertainty learned by   \n565 BKTF for this optimization problem. ", "page_idx": 14}, {"type": "image", "img_path": "VTcGX5HO19/tmp/49feb9544c4933df8481916556009fbe1d2fa230ae12d9e23e5c1a51577896e4.jpg", "img_caption": ["Figure 4: Optimization on the 2D function (Eq. 23): Posterior distributions of model hyperparameters learned by BKTF. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "VTcGX5HO19/tmp/6f9cf6851d81854dc28e7596a15a9923925edb7e30c6281eb36833bd67e7e3e2.jpg", "img_caption": ["Figure 5: Optimization on the 2D function (Eq. 23): Mean of latent factors sampled by BKTF. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "image", "img_path": "VTcGX5HO19/tmp/21a6c08d65d816826b901edefc3b27c26dba2954d1324fc8e1d5b0ea27d349b6.jpg", "img_caption": ["(a) MCMC samples for latent factors $(d=1)$ ). "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "VTcGX5HO19/tmp/d4c3b4931272c24bd6e83c020c16d7f29eada8de0aa66e79beada17682679dd9.jpg", "img_caption": ["(b) MCMC samples for latent factors $[d=2]$ ). "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 6: Optimization on the 2D function (Eq. 23): The last 20 MCMC samples of the latent factors in the two dimensions learned by BKTF. ", "page_idx": 16}, {"type": "text", "text": "566 D Benchmark test functions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "567 We summarize the characteristics of the benchmark functions tested in Table 1. Figure 7 shows   \n568 the functions with 2-dimensional inputs together with the 2D Griewank function. The functional   \n569 expressions and more detailed features of these test functions are provided in the following. ", "page_idx": 16}, {"type": "table", "img_path": "VTcGX5HO19/tmp/da514425cc2ef4ff73df282fff4c3c7e6a9275acb7dfc1169770890a533a0762.jpg", "table_caption": ["Table 1: Summary of the studied benchmark functions. "], "table_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "", "img_caption": ["Figure 7: Examples of the tested benchmark functions. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "(1) Branin function $\\langle D=2\\rangle$ ) ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\nf(x_{1},x_{2})=\\left(x_{2}-\\frac{5.1}{4\\pi}x_{1}^{2}+\\frac{5}{\\pi}x_{1}-6\\right)^{2}+10\\left(1-\\frac{1}{8\\pi}\\right)\\cos(x_{1})+10,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "570 where $x_{1}\\in[-5,10]$ and $x_{2}\\in[0,15]$ . It is a smooth but multimodal function with global minima   \n571 $f(\\pmb{x}^{\\ast})=0.3978873$ at three input points $\\pmb{x}^{*}=(-\\pi,12.275),(\\pi,2.275),(3\\pi,2.425)$ . ", "page_idx": 17}, {"type": "text", "text": "(2) Damavandi function $\\mathcal{D}=2$ ) ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\nf(x_{1},x_{2})=\\left[1-\\left|\\frac{\\sin[\\pi(x_{1}-2)]\\sin[\\pi(x_{2}-2)]}{\\pi^{2}(x_{1}-2)(x_{2}-2)}\\right|^{5}\\right]\\left[2+(x_{1}-7)^{2}+2(x_{2}-7)^{2}\\right],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "572 where $x_{d}\\in[0,14]$ . This is a multimodal function with the global minimum $f(\\pmb{x}^{*})=0$ at $\\pmb{x}^{*}=(2,2)$ . ", "page_idx": 17}, {"type": "text", "text": "(3) Schaffer function $\\langle D=2|$ ) ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\nf(x_{1},x_{2})=0.5+{\\frac{\\sin^{2}\\sqrt{x_{1}^{2}+x_{2}^{2}}-0.5}{\\left[1+0.001\\left(x_{1}^{2}+x_{2}^{2}\\right)\\right]^{2}}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "573 where $x_{d}\\in[-10,10]$ . The global minimum value is $f(\\pmb{x}^{*})=0$ at ${\\boldsymbol{x}}^{*}=(0,0)$ . One characteristic   \n574 of this function is that the global minimum is located very close to the local minima. ", "page_idx": 17}, {"type": "text", "text": "(4) Griewank function $[D=3,4,10$ ) ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\nf(\\pmb{x})=1+\\sum_{d=1}^{D}\\frac{x_{d}^{2}}{4000}-\\prod_{d=1}^{D}\\cos\\left(\\frac{x_{d}}{\\sqrt{d}}\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "575 where $x_{d}\\in[-10,10]$ . This is a multimodal function with global minimum $f(\\pmb{x}^{*})=0$ at ${\\boldsymbol{x}}^{*}=(0,0)$ . ", "page_idx": 17}, {"type": "text", "text": "576 (5) Hartmann function $'D\\,=\\,6$ ) A nonseparable function with multidimensional inputs and   \n577 multiple local minima. ", "page_idx": 17}, {"type": "equation", "text": "$$\nf({\\pmb x})=-\\sum_{j=1}^{4}c_{j}\\exp{\\left(-\\sum_{d=1}^{6}a_{j d}(x_{d}-b_{j d})^{2}\\right)},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "578 where ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{A=[a_{j d}]=\\left[{\\begin{array}{l l l l l l}{10}&{3}&{17}&{3.5}&{1.7}&{8}\\\\ {0.05}&{10}&{17}&{0.1}&{8}&{14}\\\\ {3}&{3.5}&{1.7}&{10}&{17}&{8}\\\\ {17}&{8}&{0.05}&{10}&{0.1}&{14}\\end{array}}\\right],\\;c=[c_{j}]=\\left[{\\begin{array}{l}{1}\\\\ {1.2}\\\\ {3}\\\\ {1.2}\\end{array}}\\right],}\\\\ &{B=[b_{j d}]=\\left[{\\begin{array}{l l l l l l}{0.1312}&{0.1696}&{0.5569}&{0.0124}&{0.8283}&{0.5586}\\\\ {0.2329}&{0.4135}&{0.8307}&{0.3736}&{0.1004}&{0.9991}\\\\ {0.2348}&{0.1451}&{0.3522}&{0.2833}&{0.3047}&{0.6650}\\\\ {0.4047}&{0.8828}&{0.8732}&{0.5743}&{0.1091}&{0.0381}\\end{array}}\\right],}\\end{array}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "579 $x_{d}\\in[0,1]$ . The 6-dimensional case has 6 local minima, and the global minimum is $f(\\pmb{x}^{*})\\;=$   \n580 $-3.32237$ at $\\pmb{x}^{*}=(0.20169,0.150011,0.476874,0.275332,0.31165\\bar{2},0.657301).$   \n581 Note that all these minimization problems can be easily transformed as a maximization optimization   \n582 problem, i.e., max $-f({\\pmb x})$ . ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "583 E Supplementary results on benchmark test functions ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "584 E.1 Performance profiles ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "585 When computing the performance profiles (PPs), i.e., Dolan-Mor\u00e9 curves [30], we consider the   \n586 number of function evaluations to find the global optimum as the performance measure. Specifically,   \n587 let $t_{p,a}$ denote the number of evaluations used by method/solver $a$ to reach the global solution in   \n588 experiment $p$ (a lower value is better). The value is equal to $N_{p}+100$ if the method cannot find the   \n589 global optimum with $N_{p}$ being the given observation budget for experiment $p$ . The performance ratio ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\gamma_{p,a}=\\frac{t_{p,a}}{\\operatorname*{min}\\{t_{p,a}:a\\in A\\}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "590 where $\\boldsymbol{\\mathcal{A}}$ represents the set that includes all comparing models, and the performance proflie for each   \n591 method is the distribution of $p\\left(\\gamma_{p,a}\\leq\\rho\\right)$ in terms of a factor $\\rho$ . We set $\\rho=1:N_{\\mathrm{max}}+1$ , where   \n592 $N_{\\mathrm{max}}=\\operatorname*{max}N_{p}$ is the largest observation budget assumed for the compared experiments. We define   \n593 the problem set $\\{\\mathcal{P}\\mid\\forall p\\in\\mathcal{P}\\}$ as the 10 runs for every function and draw the performance profiles   \n594 of each model, also set $\\mathcal{P}$ as the 70 experiments in the 7 tested functions and estimate the overall   \n595 performance profiles.   \n596 We show the obtained PPs across test functions (i.e., overall PPs) in Figure 2 (bottom right) for   \n597 illustration (see Section 5.1 of the paper); the results of all the tested functions are shown below in   \n598 Figure 8. Note we only compute PPs for the methods that compared on all test functions, i.e., deepGP   \n599 and SAASBO are not considered.   \n600 Table 2 gives the AUC (area under the curve) values of these curves, where the AUC of the overall   \n601 performance profiles is taken as the metric to compare the overall performances. As can be seen,   \n602 BKTF obtains the best performance across all the considered functions. In addition, for most of   \n603 the test functions, the AUC of grid-based baseline models is comparable with those of continuous   \n604 GP-based models, suggesting that discretization of the continuous space is feasible to simplify the   \n605 optimization problem. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "image", "img_path": "VTcGX5HO19/tmp/bff594c317834de898aa97f09ffe2a42d81def80b4b90cff09ba51697ee9bbec.jpg", "img_caption": ["Figure 8: Performance profiles on the standard test functions. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "606 E.2 Quantitative comparison ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "607 We compare the last step regret, average costs of evaluations, and AUC of PPs of different methods   \n608 on benchmark test functions in Table 2. ", "page_idx": 19}, {"type": "text", "text": "Table 2: Optimization on test functions: regret when $n\\,=\\,N$ (mean $\\pm$ std.) / Average costs of evaluations / AUC of PPs. ", "page_idx": 20}, {"type": "table", "img_path": "VTcGX5HO19/tmp/9ad6a1c07c5815852eff63dd3c2ed411b155de56cd9fceed4400b99903367860.jpg", "table_caption": [], "table_footnote": ["Best results are highlighted in bold fonts. B: Branin; D: Damavandi; S: Schaffer; G: Griewank; H: Hartmann. "], "page_idx": 20}, {"type": "text", "text": "609 E.3 Interpretation of results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "610 The basis functions learned by BKTF are interpretable. For example, Figure 9 shows the latent factors   \n611 $\\left.r=1\\right)$ ) obtained at the last iteration of function evaluation in one run on 3D Griewank function.   \n612 We see that BKTF can learn the periodicity (global structure) of the function benefited from the   \n613 low-rank modeling. On the other hand, a stationary and separable GP cannot, other than using a   \n614 specific kernel function such as the periodic kernel, which however requires strong prior knowledge   \n615 to set the periodicity kernel hyperparameter. ", "page_idx": 20}, {"type": "image", "img_path": "VTcGX5HO19/tmp/d1504b1221ac808d091c7b8b71055833482f8329ce556b0d308e15fce9f38f02.jpg", "img_caption": ["Figure 9: Examples of latent factors learned by BKTF on 3D Griewank function. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "VTcGX5HO19/tmp/3b0ec674e275d07b7127f0ceec7f5b72177651c6f0327aba444eaefe9d739daa.jpg", "img_caption": ["Figure 10: Effects of hyperpriors on Branin function: Optimization with different hyperpriors. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "616 E.4 Effects of hyperpriors ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "617 We compare the optimization performance on Branin function with different hyperprior settings in   \n618 Figure 10 as an example to illustrate the effects of hyperpriors. Specifically, we compare the optimiza  \n619 tion results under several hyperprior assumptions of $\\mu_{l}$ , when $\\tau_{l}^{-1}$ is set as 0.5. As can be seen, BKTF   \n620 is not able to reach the global minimum with too small or too large mean assumptions (comparable to   \n621 $[0,1])$ on the kernel lengthscales $l$ , for example in the cases where $\\mu_{l}=\\{\\log{(0.05)},\\log{(2)}\\}.$ . In con  \n622 trast, it finds the global optimum after 4 iterations of function evaluations when $\\mu_{l}=\\log{(0.5)}$ , see the   \n623 purple line. These imply the importance of hyperprior selection. The reason is that in the first several   \n624 evaluations, since the observations are rare, the prior basically determines the exploration-exploitation   \n625 balance and guides the search process.   \n626 Figure 11 shows the approximated posterior distributions for kernel hyperparameters and model noise   \n627 variance when $\\tau_{l}^{-1}=0.5,\\mu_{l}=\\log\\left(0.5\\right)$ . We see that for the re-scaled input space and normalized   \n628 function output, the sampled length scales are around half of the input domain. Such settings are   \n629 reasonable to capture the correlations between the observations and are also interpretable.   \n630 The effects of hyper-priors on other functions are similar, and we choose an appropriate setting   \n631 relevant to the input range. The hyper-prior on $\\tau$ impacts the uncertainty of the latent factors, for   \n632 example a large model noise assumption allows more variances in the factors. The role of $\\{a_{0},b_{0}\\}$   \n633 becomes more important when the objective function is complex that BKTF cannot well describe the   \n634 function with limited observations. Generally, we select the priors that make the noise variances not   \n635 quite large, such as the results of $\\tau^{-1}$ shown in Figure 4 and Figure 11. An example of the uncertainty   \n636 provided by BKTF is explained in Appendix C (see Figure 6). ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "image", "img_path": "VTcGX5HO19/tmp/badafb729eb2a3ce6ed0922defca59774dfcd42b70c41901408acaba355ee403.jpg", "img_caption": ["Figure 11: Effects of hyperpriors on Branin function: Posterior probability distributions of lengthscales and model noise variance when $\\tau_{l}^{-1}=0.5,\\mu_{l}=\\log(0.5)$ . "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "637 E.5 Effects of rank ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "638 Under a fully Bayesian treatment, kernel hyperparameters of BKTF are automatically sampled by   \n639 MCMC with proper priors; the only selected parameter is the model rank. We test the effects of rank   \n640 specification for the proposed BKTF surrogate on the 2D nonstationary nonseparable function defined   \n641 in Section 1 Introduction (see Figure 1 and Appendix C). We use the same experiment settings as in   \n642 Appendix C, i.e., 30 initial observations and 50 budget.   \n643 The results are given in Figure 12 and 13, where we compare the optimization performance of BKTF   \n644 with rank $\\bar{R}=\\bar{\\{2,4,6,8\\}}$ and two GP-based surrogate models: GP $\\alpha_{\\mathrm{EI}}$ and GP $\\alpha_{\\mathrm{UCB}}$ . To clearly   \n645 illustrate the results, we only show the comparison on one run. In Figure 12, we compare the regret   \n646 from different models, and in Figure 13 we compute and compare the mean CRPS (continuous ranked   \n647 probability score) on the unobserved points.   \n648 CRPS is a widely applied metric for evaluating the performance of UQ for probabilistic models. With   \n649 Gaussian likelihoods, CRPS can be defined as: ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{CRPS}=-\\frac{1}{n^{\\prime}}\\sum_{i=1}^{n^{\\prime}}\\sigma_{i}\\left[\\frac{1}{\\sqrt{\\pi}}-2\\psi\\left(\\frac{f_{i}-\\hat{y_{i}}}{\\sigma_{i}}\\right)-\\frac{f_{i}-\\hat{y_{i}}}{\\sigma_{i}}\\left(2\\Phi\\left(\\frac{f_{i}-\\hat{y_{i}}}{\\sigma_{i}}\\right)-1\\right)\\right],\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "650 where $n^{\\prime}$ is the number of unknown points in the defined space, i.e., $\\begin{array}{r}{n^{\\prime}=\\prod_{d=1}^{D}m_{d}-n,\\,\\hat{y}_{i}}\\end{array}$ and   \n651 $\\sigma_{i}$ are the approximated posterior mean and std. for the ith data point, respectively, $f_{i}$ denotes the   \n652 true value for the $i$ th point, and $\\psi\\left(\\cdot\\right)$ and $\\Phi\\left(\\cdot\\right)$ are the PDF (probability density function) and CDF   \n653 (cumulative distribution function) of standard normal, respectively.   \n654 We see that BKTF successfully finds the global optimum with rank from 2 to 8, and obtains better   \n655 (lower) CRPS values than GP baseline surrogates during the search processes. These results indicate   \n656 that the proposed fully Bayesian framework is robust to the rank setting and can avoid overfitting. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "image", "img_path": "VTcGX5HO19/tmp/4e522b799440c51afdf0f5c2851cd47d561911d79e56474be3c7561f9b13221c.jpg", "img_caption": ["Figure 12: Effects of rank specification on the test function defined in Introduction (see Appendix C, Eq. 23): Comparison of optimization performance. ", ""], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "VTcGX5HO19/tmp/2c797104797b799aa0cfb3d914ac472b760441edc0bdec94bfafd9f772344f0a.jpg", "img_caption": ["Figure 13: Effects of rank specification on the test function defined in Introduction (see Appendix C, Eq. 23): Comparison of CRPS. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "657 F Comparison of computational complexity ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "658 We compare the computational complexity of BKTF, BKTFrandom and the baseline methods applied   \n659 on test functions in Table 3, where $n$ is the number of observations, $m_{d}$ is the number of interpo  \n660 lation points for the dth dimension, and $n_{\\mathrm{random}}$ denotes the number of candidates we selected in   \n661 BKTFrandom, which is 20k in the conducted experiments.   \n662 As can be seen, theoretically the proposed model BKTF/BKTFrandom has the lowest computational   \n663 complexity. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "table", "img_path": "VTcGX5HO19/tmp/103df4cac7a8ece13f60ef08178fb78c057d6a15a251f7f2d27148ad1d998707.jpg", "table_caption": ["Table 3: Comparison of model complexity. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "664 G Hyperparameter tuning for machine learning ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "665 Table 4 lists all the hyperparameters in the tuning tasks. ", "page_idx": 24}, {"type": "table", "img_path": "VTcGX5HO19/tmp/15da3e75f8392b9319bd19aefaad69a42cf6b06e6f1916573e99fd74e20480ab.jpg", "table_caption": ["Table 4: Hyperparameters of the tested ML algorithms. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "666 H Supplementary results on ML hyperparameter tuning ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "667 We summarize the final accuracy obtained with different BO methods for MNIST classification and   \n668 the final MSE for the regression task in Table 5. ", "page_idx": 25}, {"type": "table", "img_path": "VTcGX5HO19/tmp/2f5c71338d6f2ac1034a4e39b8c37d3ce01b22e70c374082f9a7e330a3a32c81.jpg", "table_caption": ["Table 5: Final accuracy for (a) MNIST classification and MSE for (b) Boston housing regression. "], "table_footnote": ["The values are presented as mean $\\pm$ std. Best results are highlighted in bold fonts. "], "page_idx": 25}, {"type": "text", "text": "669 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "671 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n672 paper\u2019s contributions and scope?   \n673 Answer: [Yes]   \n674 Justification: Please see Abstract and Section Introduction for more details.   \n675 Guidelines:   \n676 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n677 made in the paper.   \n678 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n679 contributions made in the paper and important assumptions and limitations. A No or   \n680 NA answer to this question will not be perceived well by the reviewers.   \n681 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n682 much the results can be expected to generalize to other settings.   \n683 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n684 are not attained by the paper.   \n685 2. Limitations   \n686 Question: Does the paper discuss the limitations of the work performed by the authors?   \n687 Answer: [Yes]   \n688 Justification: We discuss the limitations in Section 6 Conclusion.   \n689 Guidelines:   \n690 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n691 the paper has limitations, but those are not discussed in the paper.   \n692 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n693 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n694 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n695 model well-specification, asymptotic approximations only holding locally). The authors   \n696 should reflect on how these assumptions might be violated in practice and what the   \n697 implications would be.   \n698 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n699 only tested on a few datasets or with a few runs. In general, empirical results often   \n700 depend on implicit assumptions, which should be articulated.   \n701 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n702 For example, a facial recognition algorithm may perform poorly when image resolution   \n703 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n704 used reliably to provide closed captions for online lectures because it fails to handle   \n705 technical jargon.   \n706 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n707 and how they scale with dataset size.   \n708 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n709 address problems of privacy and fairness.   \n710 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n711 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n712 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n713 judgment and recognize that individual actions in favor of transparency play an impor  \n714 tant role in developing norms that preserve the integrity of the community. Reviewers   \n715 will be specifically instructed to not penalize honesty concerning limitations.   \n716 3. Theory Assumptions and Proofs   \nQuestion: For each theoretical result, does the paper provide the full set of assumptions and   \n717   \n718 a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We illustrate methodology and technical details of the proposed model in Section 3 BKTF for Bayesian optimization and Appendix A Model inference. ", "page_idx": 27}, {"type": "text", "text": "2 Guidelines: \u2022 The answer NA means that the paper does not include theoretical results.   \n4 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n6 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems. \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n8 they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n0 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "733 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "734 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n735 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n736 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Justification: We illustrate detailed information for experiment implementation in Section 5 Experiments and Appendices C-G. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "772 5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "773 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n774 tions to faithfully reproduce the main experimental results, as described in supplemental   \n775 material?   \n776 Answer: [Yes]   \n777 Justification: We provide the code for the 2D test function in supplementary material.   \n778 Guidelines:   \n779 \u2022 The answer NA means that paper does not include experiments requiring code.   \n780 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n781 public/guides/CodeSubmissionPolicy) for more details.   \n782 \u2022 While we encourage the release of code and data, we understand that this might not be   \n783 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n784 including code, unless this is central to the contribution (e.g., for a new open-source   \n785 benchmark).   \n786 \u2022 The instructions should contain the exact command and environment needed to run to   \n787 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n788 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n789 \u2022 The authors should provide instructions on data access and preparation, including how   \n790 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n791 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n792 proposed method and baselines. If only a subset of experiments are reproducible, they   \n793 should state which ones are omitted from the script and why.   \n794 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n795 versions (if applicable).   \n796 \u2022 Providing as much information as possible in supplemental material (appended to the   \n797 paper) is recommended, but including URLs to data and code is permitted.   \n798 6. Experimental Setting/Details   \n799 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n800 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n801 results?   \n802 Answer: [Yes]   \n803 Justification: We specify the experimental setting and implementation details in Section 5   \n804 Experiments and Appendices C-G.   \n805 Guidelines:   \n806 \u2022 The answer NA means that the paper does not include experiments.   \n807 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n808 that is necessary to appreciate the results and make sense of them.   \n809 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n810 material.   \n811 7. Experiment Statistical Significance   \n812 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n813 information about the statistical significance of the experiments?   \n814 Answer: [Yes]   \n815 Justification: We repeat the experiments certain times and report the mean with std. results   \n816 in Figure 1, 2, 3, and Table 2, 5.   \n817 Guidelines:   \n818 \u2022 The answer NA means that the paper does not include experiments.   \n819 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n820 dence intervals, or statistical significance tests, at least for the experiments that support   \n821 the main claims of the paper.   \n822 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n823 example, train/test split, initialization, random drawing of some parameter, or overall   \n824 run with given experimental conditions).   \n825 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n826 call to a library function, bootstrap, etc.)   \n827 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n828 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n829 of the mean.   \n830 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n831 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n832 of Normality of errors is not verified.   \n833 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n834 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n835 error rates).   \n836 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n837 they were calculated and reference the corresponding figures or tables in the text.   \n838 8. Experiments Compute Resources   \n839 Question: For each experiment, does the paper provide sufficient information on the com  \n840 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n841 the experiments?   \n842 Answer: [Yes]   \n843 Justification: All the experiments can be performed with a 16-core 2.40 GHz CPU and 32   \n844 GB RAM.   \n845 Guidelines:   \n846 \u2022 The answer NA means that the paper does not include experiments.   \n847 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n848 or cloud provider, including relevant memory and storage.   \n849 \u2022 The paper should provide the amount of compute required for each of the individual   \n850 experimental runs as well as estimate the total compute.   \n851 \u2022 The paper should disclose whether the full research project required more compute   \n852 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n853 didn\u2019t make it into the paper).   \n854 9. Code Of Ethics   \n855 Question: Does the research conducted in the paper conform, in every respect, with the   \n856 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n857 Answer: [Yes]   \n858 Justification: The research conducted in this paper conform with the NeurIPS code of ethics.   \n859 Guidelines:   \n860 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n861 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n862 deviation from the Code of Ethics.   \n863 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n864 eration due to laws or regulations in their jurisdiction).   \n865 10. Broader Impacts   \n866 Question: Does the paper discuss both potential positive societal impacts and negative   \n867 societal impacts of the work performed?   \n868 Answer: [Yes]   \n869 Justification: This paper presents work whose goal is to advance the field of probabilistic   \n870 Machine Learning, particularly Bayesian Optimization. We discussed such impacts in   \n871 Section 6 Conclusion.   \n872 Guidelines:   \n873 \u2022 The answer NA means that there is no societal impact of the work performed.   \n874 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n875 impact or why the paper does not address societal impact.   \n876 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n877 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n878 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n879 groups), privacy considerations, and security considerations.   \n880 \u2022 The conference expects that many papers will be foundational research and not tied   \n881 to particular applications, let alone deployments. However, if there is a direct path to   \n882 any negative applications, the authors should point it out. For example, it is legitimate   \n883 to point out that an improvement in the quality of generative models could be used to   \n884 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n885 that a generic algorithm for optimizing neural networks could enable people to train   \n886 models that generate Deepfakes faster.   \n887 \u2022 The authors should consider possible harms that could arise when the technology is   \n888 being used as intended and functioning correctly, harms that could arise when the   \n889 technology is being used as intended but gives incorrect results, and harms following   \n890 from (intentional or unintentional) misuse of the technology.   \n891 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n892 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n893 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n894 feedback over time, improving the efficiency and accessibility of ML).   \n895 11. Safeguards   \n896 Question: Does the paper describe safeguards that have been put in place for responsible   \n897 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n898 image generators, or scraped datasets)?   \n899 Answer: [Yes]   \n900 Justification: This work has the potential of misuse for machine learning algorithms. How  \n901 ever the current model has certain limitations on applying for high-dimensional problems,   \n902 thus such risks are low. We mentioned such risks in the last paragraph in Section 6 Conclu  \n903 sion.   \n904 Guidelines:   \n905 \u2022 The answer NA means that the paper poses no such risks.   \n906 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n907 necessary safeguards to allow for controlled use of the model, for example by requiring   \n908 that users adhere to usage guidelines or restrictions to access the model or implementing   \n909 safety filters.   \n910 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n911 should describe how they avoided releasing unsafe images.   \n912 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n913 not require this, but we encourage authors to take this into account and make a best   \n914 faith effort.   \n915 12. Licenses for existing assets   \n916 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n917 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n918 properly respected?   \n919 Answer: [Yes]   \n920 Justification: We include the URLs for the datasets we used in this work.   \n921 Guidelines:   \n922 \u2022 The answer NA means that the paper does not use existing assets.   \n923 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n924 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n925 URL.   \n926 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n927 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n928 service of that source should be provided.   \n929 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n930 package should be provided. For popular datasets, paperswithcode.com/datasets   \n931 has curated licenses for some datasets. Their licensing guide can help determine the   \n932 license of a dataset.   \n933 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n934 the derived asset (if it has changed) should be provided.   \n935 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n936 the asset\u2019s creators.   \n937 13. New Assets   \n938 Question: Are new assets introduced in the paper well documented and is the documentation   \n939 provided alongside the assets?   \n940 Answer: [Yes]   \n941 Justification: We submit partial of the code in supplementary material and select a license   \n942 when submitting the paper.   \n943 Guidelines:   \n944 \u2022 The answer NA means that the paper does not release new assets.   \n945 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n946 submissions via structured templates. This includes details about training, license,   \n947 limitations, etc.   \n948 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n949 asset is used.   \n950 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n951 create an anonymized URL or include an anonymized zip file.   \n952 14. Crowdsourcing and Research with Human Subjects   \n953 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n954 include the full text of instructions given to participants and screenshots, if applicable, as   \n955 well as details about compensation (if any)?   \n956 Answer: [NA]   \n957 Justification: This paper does not involve crowdsourcing nor research with human subjects.   \n958 Guidelines:   \n959 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n960 human subjects.   \n961 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n962 tion of the paper involves human subjects, then as much detail as possible should be   \n963 included in the main paper.   \n964 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n965 or other labor should be paid at least the minimum wage in the country of the data   \n966 collector.   \n967 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n968 Subjects   \n969 Question: Does the paper describe potential risks incurred by study participants, whether   \n970 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n971 approvals (or an equivalent approval/review based on the requirements of your country or   \n972 institution) were obtained?   \n973 Answer: [NA]   \n974 Justification: This paper does not involve crowdsourcing nor research with human subjects.   \n975 Guidelines:   \n976 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n977 human subjects.   \n978 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n979 may be required for any human subjects research. If you obtained IRB approval, you   \n980 should clearly state this in the paper. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]