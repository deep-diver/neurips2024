[{"figure_path": "btLLWaOrFs/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of CAST. The feature pyramid network down-samples the point clouds and learns features in multiple resolutions. The coarse matching module extracts consistency-aware semi-dense correspondences via a group of alternate consistency-aware self-attention modules and spot-guided cross-attention modules with multi-scale feature fusion. Finally, the fine matching module predicts correspondences for both sparse keypoints and dense features and estimates the transformation.", "description": "This figure presents a high-level overview of the CAST architecture. It shows the flow of data through the different modules, starting with the feature pyramid network that processes the input point clouds at multiple resolutions.  The coarse matching module uses a combination of self-attention and cross-attention mechanisms to identify potential correspondences between the point clouds. The spot-guided cross-attention focuses on local regions to avoid interference from irrelevant areas. The fine matching module further refines these correspondences and estimates the final transformation.", "section": "3 Method"}, {"figure_path": "btLLWaOrFs/figures/figures_4_1.jpg", "caption": "Figure 2: Illustration of consistency-aware self-attention and spot-guided cross-attention (Left), as well as visualization of the global cross-attention and spot-guided cross-attention (Right). For the left part, the green nodes are query nodes, while the red ones with correct correspondences (green dot lines) are reliable neighbors, and the blue one with a false correspondence (red dot line) is an unreliable neighbor. The self-attention (black lines) only attends to salient nodes while the cross-attention (black lines) only attends to spots (nodes within black circles).", "description": "This figure illustrates the concepts of consistency-aware self-attention and spot-guided cross-attention. The left panel shows how these attention mechanisms work by focusing on specific nodes (spots) and salient nodes, respectively, to avoid interference from irrelevant areas. The right panel visually compares global cross-attention with spot-guided cross-attention, demonstrating the effectiveness of spot-guided cross-attention in selecting relevant features for matching.", "section": "3.2 Consistency-Aware Spot-Guided Attention"}, {"figure_path": "btLLWaOrFs/figures/figures_7_1.jpg", "caption": "Figure 3: Qualitative registration results on KITTI dataset. We show three examples in three columns. The first two rows present the raw point clouds and highlight the 3D keypoints with low uncertainty in red. Our keypoints are typically located in sharp corners and edges of buildings, pillars, and vehicles. The third row shows the predicted sparse keypoint correspondences with high scores, while the last row presents the aligned point clouds after pose estimation. Although a few outliers colored in red have not been filtered out, their distances are acceptable for accurate registration.", "description": "This figure showcases three qualitative examples of point cloud registration on the KITTI dataset using the proposed CAST method.  It visually demonstrates the different stages of the process: initial point clouds with detected keypoints highlighted, the matching of these keypoints between the source and target frames, and the final aligned point clouds after pose estimation. While some outliers remain, the overall alignment is highly accurate.", "section": "4.1 Outdoor Scenarios: KITTI and NuScenes"}, {"figure_path": "btLLWaOrFs/figures/figures_14_1.jpg", "caption": "Figure 4: The detailed architecture of the KPConv-based feature pyramid network.", "description": "This figure shows the detailed architecture of the KPConv-based feature pyramid network used in the CAST model.  It consists of an encoder-decoder structure. The encoder downsamples the point cloud features through several KPConv and ResBlock layers, while the decoder upsamples the features back to the original resolution.  The specific number of layers and channels are indicated in the diagram.", "section": "A.1 Neural Network Architectures and Hyper-parameters"}, {"figure_path": "btLLWaOrFs/figures/figures_15_1.jpg", "caption": "Figure 5: The detailed architecture of the consistency-aware spot-guided Transformer with multi-scale feature fusion for coarse feature matching.", "description": "This figure shows the detailed architecture of the consistency-aware spot-guided transformer used for coarse feature matching in the paper.  It highlights the multi-scale feature fusion, spot-guided cross-attention, and consistency-aware self-attention modules. The process starts with semi-dense and coarse features which go through multiple blocks. Each block includes a self-attention module focusing on salient nodes and a cross-attention module focusing on neighboring spots to aggregate features.  Finally, a linear attention module integrates the multi-scale features to produce matching scores.", "section": "3.2 Consistency-Aware Spot-Guided Attention"}, {"figure_path": "btLLWaOrFs/figures/figures_15_2.jpg", "caption": "Figure 6: Attentive keypoint detector and descriptor.", "description": "This figure illustrates the architecture of the attentive keypoint detector and descriptor used in the CAST model.  It shows how grouped features (from multiple points in a local neighborhood) are processed through multiple Multilayer Perceptrons (MLPs) and operations such as maxpooling, product, sum, softmax, and softplus to generate attentive scores and ultimately keypoints and their corresponding descriptors.  The uncertainty of each detected keypoint is also estimated.", "section": "3.3 Sparse-to-Dense Fine Matching"}, {"figure_path": "btLLWaOrFs/figures/figures_21_1.jpg", "caption": "Figure 3: Qualitative registration results on KITTI dataset. We show three examples in three columns. The first two rows present the raw point clouds and highlight the 3D keypoints with low uncertainty in red. Our keypoints are typically located in sharp corners and edges of buildings, pillars, and vehicles. The third row shows the predicted sparse keypoint correspondences with high scores, while the last row presents the aligned point clouds after pose estimation. Although a few outliers colored in red have not been filtered out, their distances are acceptable for accurate registration.", "description": "This figure shows three examples of point cloud registration on the KITTI dataset using the proposed method.  Each example is displayed across three columns showing the source and target point clouds with detected keypoints highlighted, the sparse keypoint correspondences, and the final pose-aligned point clouds. The keypoints are selected for being located at distinctive features like corners and edges.  While some outliers remain, their error is small enough that accurate registration is still achieved.", "section": "4.1 Outdoor Scenarios: KITTI and NuScenes"}, {"figure_path": "btLLWaOrFs/figures/figures_21_2.jpg", "caption": "Figure 8: Qualitative registration results of CoFiNet [11], GeoTransformer [12], RoITr [28], and CAST compared with the ground truth alignment on 3DMatch dataset. We present five examples in five rows, which demonstrate the robustness and accuracy of our method.", "description": "This figure compares the qualitative registration results of four different methods (CoFiNet, GeoTransformer, RoITr, and CAST) against the ground truth on the 3DMatch dataset. Five example pairs of point clouds are shown, with each row illustrating a different pair.  Each column shows the results of a different method, demonstrating the accuracy and robustness of the proposed CAST method compared to others.", "section": "4.2 Indoor Scenarios: 3DMatch and 3DLoMatch"}]