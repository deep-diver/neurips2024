[{"figure_path": "31xWlIdxTm/figures/figures_1_1.jpg", "caption": "Figure 1: Input the same question with two different prompts to guide the LLM to answer it. Blue words are format tokens and prompts, red words mark wrong reasoning steps.", "description": "This figure shows an example of how different prompts can affect the LLM's reasoning process.  The same question is given to the LLM with two different prompts: \"Let's think step by step.\" and \"Don't think. Just feel.\". The first prompt leads the LLM through a step-by-step reasoning process, resulting in the correct answer but with an extra, erroneous step. The second prompt leads to a concise, incorrect answer. This highlights the impact of prompt engineering on the LLM's performance and reasoning accuracy.", "section": "Information Flow Analysis on Zero-shot CoT"}, {"figure_path": "31xWlIdxTm/figures/figures_2_1.jpg", "caption": "Figure 2: The visualization comparison of the saliency matrices between good and bad reasoning instances with two prompts, the darker the color of the pixel point in the image represents a larger saliency score. (a) and (b) are good and bad reasoning instances under \"Let's think step by step.\", and so as (c) and (d) under \"Don't think. Just feel.\", respectively. The red, blue, and green boxes in each subfigure depict the question-to-prompt, question-to-rationale, and prompt-to-rationale information flow, respectively.", "description": "This figure visualizes the saliency matrices for good and bad reasoning examples using two different prompts: \"Let's think step by step\" and \"Don't think, just feel.\"  The color intensity represents the strength of information flow between the question, prompt, and rationale.  The red, blue, and green boxes highlight the question-to-prompt, question-to-rationale, and prompt-to-rationale information flow, respectively.  The figure demonstrates how the information flow differs between successful and unsuccessful reasoning instances.", "section": "Information Flow Analysis on Zero-shot CoT"}, {"figure_path": "31xWlIdxTm/figures/figures_3_1.jpg", "caption": "Figure 3: Comparison between mean values of randomly sampled 50 good and bad instances from GSM8K in question-to-prompt, question-to-rationale, and prompt-to-rationale.", "description": "This bar chart compares the average saliency scores for three different information flows (question-to-prompt, question-to-rationale, prompt-to-rationale) between good and bad reasoning instances from the GSM8K dataset.  The higher saliency scores for good reasoning instances across all three flows suggest that successful reasoning involves a strong interplay between the question, prompt, and rationale, where the prompt effectively captures the question's semantics and the rationale aggregates information from both the question and the prompt.", "section": "Information Flow Analysis on Zero-shot CoT"}, {"figure_path": "31xWlIdxTm/figures/figures_4_1.jpg", "caption": "Figure 4: Saliency scores of question-to-prompt, question-to-rationale, and prompt-to-rationale across layers. The yellow lines represent prompts that effectively guide the LLMs to generate the correct answer, indicating good prompts. Conversely, the blue lines denote ineffective prompts.", "description": "This figure shows the saliency scores for the interaction between the question, prompt, and rationale across different layers of the LLM.  The saliency score represents the strength of the semantic information flow between these components.  The results indicate that effective prompts show a high saliency score in the initial layers (shallow layers), transferring information from the question to the prompt and then to the rationale. Ineffective prompts show much lower saliency scores, especially in the shallow layers.", "section": "2 Information Flow Analysis on Zero-shot CoT"}, {"figure_path": "31xWlIdxTm/figures/figures_5_1.jpg", "caption": "Figure 5: Saliency scores from question to prompt, question to rationale, and prompt to rationale. The color intensity across the heatmap denotes varying degrees of engagement among heads, the darker color denotes the higher score.", "description": "This figure visualizes the saliency scores for three different information flows within the LLM: question to prompt, question to rationale, and prompt to rationale.  The heatmaps show the distribution of saliency scores across different layers and heads of the transformer model.  Darker colors indicate stronger information flow.  The figure helps to illustrate the different information aggregation patterns between successful and unsuccessful reasoning paths.", "section": "2.3 Head analysis"}, {"figure_path": "31xWlIdxTm/figures/figures_8_1.jpg", "caption": "Figure 6: Efficiency comparison with LLaMA-3-8B-Instruct on the Tracking Shuffle Objects, 0-shot denotes the best task-level prompt.", "description": "This figure shows a comparison of the efficiency and accuracy of different prompting strategies for the \"Tracking Shuffle Objects\" task, using the LLaMA-3-8B-Instruct language model.  The strategies compared are 0-shot (using the best single prompt found for the task), Self-discover, IAP-mv (Instance-Adaptive Prompting - majority vote), and IAP-ss (Instance-Adaptive Prompting - sequential substitution). The x-axis represents the time taken per iteration in seconds, and the y-axis represents the accuracy achieved.  The figure demonstrates the trade-off between efficiency and accuracy offered by the different approaches; IAP-mv achieves high accuracy but at the cost of longer processing time.", "section": "4.3 Ablation Studies"}, {"figure_path": "31xWlIdxTm/figures/figures_13_1.jpg", "caption": "Figure 4: Saliency scores of question-to-prompt, question-to-rationale, and prompt-to-rationale across layers. The yellow lines represent prompts that effectively guide the LLMs to generate the correct answer, indicating good prompts. Conversely, the blue lines denote ineffective prompts.", "description": "This figure visualizes the saliency scores (representing the flow of semantic information) between different components (question, prompt, rationale) across multiple layers of an LLM during zero-shot chain-of-thought reasoning.  It compares the information flow for both \"good\" prompts (leading to correct answers) and \"bad\" prompts (leading to incorrect answers).  The different lines show the saliency at each layer for each of the three relationships. The graph shows that effective prompts have a strong information flow from the question to the prompt in the earlier layers and a sustained flow from both the question and the prompt to the rationale across most layers, while ineffective prompts show a weaker flow of information.", "section": "Information Flow Analysis on Zero-shot CoT"}, {"figure_path": "31xWlIdxTm/figures/figures_13_2.jpg", "caption": "Figure 5: Saliency scores from question to prompt, question to rationale, and prompt to rationale. The color intensity across the heatmap denotes varying degrees of engagement among heads, the darker color denotes the higher score.", "description": "This figure visualizes the saliency scores across different heads and layers of a transformer model for three different semantic relationships: question-to-prompt, question-to-rationale, and prompt-to-rationale.  The intensity of the color represents the strength of the interaction, with darker colors indicating stronger interactions. The figure allows for a detailed analysis of how information flows between these components during the zero-shot chain of thought (CoT) reasoning process.", "section": "2.3 Head analysis"}]