[{"figure_path": "31xWlIdxTm/tables/tables_7_1.jpg", "caption": "Table 1: Zero-shot CoT results with LLaMA-3-8B-Instruct and Qwen-14B-Chat under various prompts, the results of LLaMA-3-70B-Instruct and LLaMA-2-13B-Chat is in Appendix A.1. Each column stands for a group of task categories, T-Obj. is for Tracking Shuffled Objects which are from the BBH. The \"Optimizer-generated prompt\" refers to the prompts for each task generated with the algorithm in [7].", "description": "This table presents the zero-shot chain-of-thought (CoT) reasoning results for different prompts on several LLMs (LLaMA-3-8B-Instruct, Qwen-14B-Chat) across various tasks categorized as Math, Logic, and Commonsense reasoning.  It compares the performance of different prompts (#1 through #9) against several baseline methods (AMV, OPPR, Self-dis, IAP-ss, and IAP-mv).  The results show the accuracy achieved by each prompt and method on GSM8K, SVAMP, C-Judge, T-Obj, CSQA, and MMLU datasets.", "section": "4.2 Results"}, {"figure_path": "31xWlIdxTm/tables/tables_7_2.jpg", "caption": "Table 1: Zero-shot CoT results with LLaMA-3-8B-Instruct and Qwen-14B-Chat under various prompts, the results of LLaMA-3-70B-Instruct and LLaMA-2-13B-Chat is in Appendix A.1. Each column stands for a group of task categories, T-Obj. is for Tracking Shuffled Objects which are from the BBH. The \"Optimizer-generated prompt\" refers to the prompts for each task generated with the algorithm in [7].", "description": "This table presents the results of zero-shot chain-of-thought (CoT) prompting experiments using different prompts on several large language models (LLMs) across various reasoning tasks.  The tasks are categorized into math, logic, and commonsense reasoning. Results show the accuracy of each LLM with different prompts, including the performance of some baseline methods (AMV, OPPR, Self-dis).  The table highlights that no single prompt performs best across all tasks and models. ", "section": "4.2 Results"}, {"figure_path": "31xWlIdxTm/tables/tables_8_1.jpg", "caption": "Table 3: Accuracy and inference time (s) with different prompt orders and numbers of LLaMA-3-8B-Instruct on SVAMP.", "description": "This table presents the accuracy and inference time taken by the LLaMA-3-8B-Instruct model on the SVAMP dataset using different orders and numbers of prompts in the Instance-Adaptive Prompting (IAP) strategy.  It showcases how the order and quantity of prompts impact the model's performance and computational cost. The results highlight the trade-off between efficiency and accuracy when employing the IAP-ss approach.", "section": "4.3 Ablation Studies"}, {"figure_path": "31xWlIdxTm/tables/tables_12_1.jpg", "caption": "Table 1: Zero-shot CoT results with LLaMA-3-8B-Instruct and Qwen-14B-Chat under various prompts, the results of LLaMA-3-70B-Instruct and LLaMA-2-13B-Chat is in Appendix A.1. Each column stands for a group of task categories, T-Obj. is for Tracking Shuffled Objects which are from the BBH. The \"Optimizer-generated prompt\" refers to the prompts for each task generated with the algorithm in [7].", "description": "This table presents the results of zero-shot Chain-of-Thought (CoT) prompting experiments using different prompts on various tasks and LLMs. It compares the performance of different prompts, including those generated by optimization algorithms,  and shows the accuracy achieved on math, logic, and commonsense reasoning tasks.", "section": "4.2 Results"}, {"figure_path": "31xWlIdxTm/tables/tables_12_2.jpg", "caption": "Table 1: Zero-shot CoT results with LLaMA-3-8B-Instruct and Qwen-14B-Chat under various prompts, the results of LLaMA-3-70B-Instruct and LLaMA-2-13B-Chat is in Appendix A.1. Each column stands for a group of task categories, T-Obj. is for Tracking Shuffled Objects which are from the BBH. The \"Optimizer-generated prompt\" refers to the prompts for each task generated with the algorithm in [7].", "description": "This table presents the results of zero-shot chain-of-thought (CoT) prompting experiments using different prompts on various tasks and two large language models (LLMs): LLaMA-3-8B-Instruct and Qwen-14B-Chat.  It shows accuracy scores for math, logic, and commonsense reasoning tasks.  Results for other LLMs are included in the appendix.  The table also includes a comparison with other state-of-the-art methods for finding optimal prompts.", "section": "4.2 Results"}, {"figure_path": "31xWlIdxTm/tables/tables_12_3.jpg", "caption": "Table 6: Accuracy (%) of Consistency and Complementary prompts with IAP-mv on 3 tasks with LLaMA-2-13B-Chat.", "description": "This table presents the accuracy results of the Instance-Adaptive Prompting (IAP-mv) method on three categories of reasoning tasks (Math, Logic, and Commonsense) using the LLaMA-2-13B-Chat language model.  It compares the performance of different prompt combinations, categorized as 'Instructive', 'Misleading', and 'Irrelevant',  and combinations thereof, to assess the effect of prompt consistency and complementarity on reasoning accuracy.  The results showcase how combining different types of prompts affects the overall performance of the model on various reasoning challenges.", "section": "4.3 Ablation Studies"}, {"figure_path": "31xWlIdxTm/tables/tables_13_1.jpg", "caption": "Table 1: Zero-shot CoT results with LLaMA-3-8B-Instruct and Qwen-14B-Chat under various prompts, the results of LLaMA-3-70B-Instruct and LLaMA-2-13B-Chat is in Appendix A.1. Each column stands for a group of task categories, T-Obj. is for Tracking Shuffled Objects which are from the BBH. The \"Optimizer-generated prompt\" refers to the prompts for each task generated with the algorithm in [7].", "description": "This table presents the results of zero-shot chain-of-thought (CoT) prompting experiments using various prompts on several language models (LLMs): LLaMA-3-8B-Instruct and Qwen-14B-Chat.  It compares the performance across different prompts (\"#1\" to \"#9\") and baselines (AMV, OPPR, Self-discover, IAP-ss, IAP-mv) on a range of tasks categorized as math, logic, and commonsense reasoning.  The results are shown as accuracy percentages for each task and LLM.", "section": "4.2 Results"}, {"figure_path": "31xWlIdxTm/tables/tables_14_1.jpg", "caption": "Table 8: Accuracy of different thresholds with LLaMA-3-8B-Instruct on GSM8K.", "description": "This table presents the accuracy achieved by the LLaMA-3-8B-Instruct model on the GSM8K dataset using different threshold values for the instance-adaptive prompting strategy (IAP-ss).  The threshold determines whether a prompt is considered 'good' or 'bad' based on its saliency score. The table shows how accuracy changes with different thresholds, helping to find the optimal value for separating good and bad prompts.", "section": "A.4 Thresholds and Majority Number"}, {"figure_path": "31xWlIdxTm/tables/tables_14_2.jpg", "caption": "Table 9: Accuracy of different thresholds with LLaMA-3-8B-Instruct on GSM8K.", "description": "This table presents the accuracy results obtained using different thresholds with the LLaMA-3-8B-Instruct model on the GSM8K dataset.  It shows how the accuracy varies as the threshold changes, indicating the impact of the threshold on the model's performance for this specific task and model.", "section": "A.4 Thresholds and Majority Number"}]