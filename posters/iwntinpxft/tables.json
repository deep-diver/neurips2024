[{"figure_path": "IwNTiNPxFt/tables/tables_5_1.jpg", "caption": "Table 1: Results on Human-Art dataset and LAION-Human subset. Methods with * are evaluated on released checkpoints.", "description": "This table presents a quantitative comparison of Stable-Pose against other state-of-the-art methods on two datasets: Human-Art and a subset of LAION-Human.  The metrics used assess pose accuracy (Average Precision (AP), Pose Cosine Similarity-based AP (CAP), and People Counting Error (PCE)), image quality (Fr\u00e9chet Inception Distance (FID) and Kernel Inception Distance (KID)), and text-to-image alignment (CLIP-score).  The results show Stable-Pose's superior performance in pose accuracy and competitive results in image quality and alignment, especially when compared to ControlNet and other baselines.", "section": "4 Experimental Results"}, {"figure_path": "IwNTiNPxFt/tables/tables_7_1.jpg", "caption": "Table 1: Results on Human-Art dataset and LAION-Human subset. Methods with * are evaluated on released checkpoints.", "description": "This table presents a quantitative comparison of Stable-Pose against other state-of-the-art (SOTA) methods on two datasets: Human-Art and a subset of LAION-Human.  The metrics used evaluate pose accuracy (AP, CAP, PCE), image quality (FID, KID), and text-to-image alignment (CLIP-score).  The results demonstrate that Stable-Pose achieves the highest pose accuracy scores across both datasets, while showing competitive performance in image quality and alignment.", "section": "4 Experimental Results"}, {"figure_path": "IwNTiNPxFt/tables/tables_7_2.jpg", "caption": "Table 3: Results of varying pose orientations on the UBC Fashion dataset. We report mean Average Precision (AP) across different methods for three orientations: front, side, and back.", "description": "This table presents the mean Average Precision (AP) for pose estimation across three different pose orientations (front, side, and back) on the UBC Fashion dataset.  It compares the performance of several methods, including Stable-Pose, highlighting Stable-Pose's ability to handle varied and challenging poses.", "section": "4 Experimental Results"}, {"figure_path": "IwNTiNPxFt/tables/tables_7_3.jpg", "caption": "Table 1: Results on Human-Art dataset and LAION-Human subset. Methods with * are evaluated on released checkpoints.", "description": "This table presents a quantitative comparison of Stable-Pose and several state-of-the-art methods for pose-guided text-to-image generation on two datasets: Human-Art and LAION-Human.  The metrics used evaluate pose accuracy (AP, CAP, PCE), image quality (FID, KID), and text-image alignment (CLIP-score).  The results show that Stable-Pose achieves the highest scores in pose accuracy metrics on both datasets, demonstrating its superior ability to generate images with correct poses.  Note that results marked with * are based on pre-trained models.", "section": "4 Experimental Results"}, {"figure_path": "IwNTiNPxFt/tables/tables_7_4.jpg", "caption": "Table 1: Results on Human-Art dataset and LAION-Human subset. Methods with * are evaluated on released checkpoints.", "description": "This table presents a quantitative comparison of Stable-Pose against other state-of-the-art methods on two datasets: Human-Art and a subset of LAION-Human.  The metrics used assess pose accuracy (AP, CAP, PCE), image quality (FID, KID), and text-to-image alignment (CLIP-score).  The results show that Stable-Pose achieves higher pose accuracy than other methods while maintaining comparable image quality and alignment.", "section": "4 Experimental Results"}, {"figure_path": "IwNTiNPxFt/tables/tables_8_1.jpg", "caption": "Table 1: Results on Human-Art dataset and LAION-Human subset. Methods with * are evaluated on released checkpoints.", "description": "This table presents a quantitative comparison of Stable-Pose against other state-of-the-art methods on two datasets: Human-Art and LAION-Human.  The evaluation metrics include Pose Accuracy (AP, CAP, PCE), Image Quality (FID, KID), and T2I Alignment (CLIP-score).  The results show Stable-Pose achieving the highest pose accuracy scores (AP and CAP) on both datasets, with comparable or slightly better image quality and alignment results.  The asterisk (*) indicates that some methods' results are based on publicly released model checkpoints, rather than the authors' own fully trained models.", "section": "4 Experimental Results"}, {"figure_path": "IwNTiNPxFt/tables/tables_8_2.jpg", "caption": "Table 1: Results on Human-Art dataset and LAION-Human subset. Methods with * are evaluated on released checkpoints.", "description": "This table presents quantitative results on the Human-Art and LAION-Human datasets, comparing Stable-Pose to several state-of-the-art methods.  It shows the performance of each method across various metrics measuring pose accuracy (Average Precision (AP), Pose Cosine Similarity-based AP (CAP), and People Counting Error (PCE)), image quality (Fr\u00e9chet Inception Distance (FID) and Kernel Inception Distance (KID)), and text-image alignment (CLIP score).  The results highlight Stable-Pose's superior performance in pose accuracy and overall image quality.", "section": "4 Experimental Results"}, {"figure_path": "IwNTiNPxFt/tables/tables_8_3.jpg", "caption": "Table 1: Results on Human-Art dataset and LAION-Human subset. Methods with * are evaluated on released checkpoints.", "description": "This table presents a quantitative comparison of Stable-Pose against other state-of-the-art methods on two datasets: Human-Art and a subset of LAION-Human.  The metrics used evaluate pose accuracy (AP, CAP, PCE), image quality (FID, KID), and text-to-image alignment (CLIP-score).  The results show Stable-Pose's superior performance in pose accuracy, particularly on the LAION-Human dataset, while maintaining comparable image quality and text-to-image alignment.", "section": "4 Experimental Results"}, {"figure_path": "IwNTiNPxFt/tables/tables_9_1.jpg", "caption": "Table 1: Results on Human-Art dataset and LAION-Human subset. Methods with * are evaluated on released checkpoints.", "description": "This table presents a quantitative comparison of Stable-Pose and several state-of-the-art (SOTA) methods for pose-guided text-to-image generation.  The evaluation metrics include pose accuracy (Average Precision (AP), Pose Cosine Similarity-based AP (CAP), People Counting Error (PCE)), image quality (Fr\u00e9chet Inception Distance (FID), Kernel Inception Distance (KID)), and text-to-image alignment (CLIP-score).  The results are shown for two datasets: Human-Art and a subset of LAION-Human.  The asterisk (*) indicates that results for certain methods were obtained using pre-trained model checkpoints rather than training from scratch.", "section": "4 Experimental Results"}, {"figure_path": "IwNTiNPxFt/tables/tables_14_1.jpg", "caption": "Table 1: Results on Human-Art dataset and LAION-Human subset. Methods with * are evaluated on released checkpoints.", "description": "This table presents a quantitative comparison of Stable-Pose against other state-of-the-art methods on two datasets: Human-Art and a subset of LAION-Human.  The metrics used to evaluate the models include pose accuracy (AP, CAP, PCE), image quality (FID, KID), and text-to-image alignment (CLIP-score).  The table shows Stable-Pose's performance relative to other methods, highlighting its strengths in pose accuracy and overall image generation.", "section": "4 Experimental Results"}, {"figure_path": "IwNTiNPxFt/tables/tables_15_1.jpg", "caption": "Table 1: Results on Human-Art dataset and LAION-Human subset. Methods with * are evaluated on released checkpoints.", "description": "This table presents the quantitative results of Stable-Pose and other state-of-the-art methods on two datasets: Human-Art and a subset of LAION-Human.  The metrics used assess pose accuracy (AP, CAP, PCE), image quality (FID, KID), and text-to-image alignment (CLIP-score).  The results show that Stable-Pose achieves higher pose accuracy compared to other methods, while maintaining comparable image quality and text-to-image alignment.", "section": "4 Experimental Results"}, {"figure_path": "IwNTiNPxFt/tables/tables_15_2.jpg", "caption": "Table 1: Results on Human-Art dataset and LAION-Human subset. Methods with * are evaluated on released checkpoints.", "description": "This table presents quantitative results comparing Stable-Pose against other state-of-the-art methods on two benchmark datasets: Human-Art and a subset of LAION-Human.  The evaluation metrics include Pose Accuracy (AP, CAP, PCE), Image Quality (FID, KID), and Text-to-Image Alignment (CLIP-score).  The results show that Stable-Pose achieves superior pose accuracy and comparable performance on image quality and alignment.  The asterisk (*) indicates that results for certain methods were obtained using pre-trained checkpoints rather than training from scratch.", "section": "4 Experimental Results"}, {"figure_path": "IwNTiNPxFt/tables/tables_16_1.jpg", "caption": "Table 1: Results on Human-Art dataset and LAION-Human subset. Methods with * are evaluated on released checkpoints.", "description": "This table presents a quantitative comparison of Stable-Pose against several state-of-the-art (SOTA) methods on two datasets: Human-Art and a subset of LAION-Human.  The evaluation metrics cover pose accuracy (AP, CAP, PCE), image quality (FID, KID), and text-to-image alignment (CLIP-score).  The results demonstrate Stable-Pose's superior performance, particularly in pose accuracy, on both datasets.", "section": "4 Experimental Results"}, {"figure_path": "IwNTiNPxFt/tables/tables_16_2.jpg", "caption": "Table 1: Results on Human-Art dataset and LAION-Human subset. Methods with * are evaluated on released checkpoints.", "description": "This table presents a quantitative comparison of Stable-Pose against other state-of-the-art methods on two datasets: Human-Art and a subset of LAION-Human.  It shows the performance across metrics like Pose Accuracy (AP, CAP, PCE), Image Quality (FID, KID), and T2I Alignment (CLIP-score).  The results demonstrate Stable-Pose's performance in terms of pose accuracy and overall image quality.", "section": "4 Experimental Results"}, {"figure_path": "IwNTiNPxFt/tables/tables_16_3.jpg", "caption": "Table 1: Results on Human-Art dataset and LAION-Human subset. Methods with * are evaluated on released checkpoints.", "description": "This table presents a quantitative comparison of Stable-Pose against several state-of-the-art methods on two benchmark datasets: Human-Art and a subset of LAION-Human.  The evaluation metrics assess both pose accuracy (AP, CAP, PCE) and image quality (FID, KID, CLIP-score).  The table highlights Stable-Pose's superior performance in pose accuracy, showing comparable results in image quality and text-image alignment.", "section": "4 Experimental Results"}, {"figure_path": "IwNTiNPxFt/tables/tables_17_1.jpg", "caption": "Table 1: Results on Human-Art dataset and LAION-Human subset. Methods with * are evaluated on released checkpoints.", "description": "This table presents the quantitative results of Stable-Pose and several state-of-the-art methods on two datasets: Human-Art and LAION-Human.  The metrics used assess pose accuracy (AP, CAP, PCE), image quality (FID, KID), and text-to-image alignment (CLIP-score).  The results show Stable-Pose achieving superior performance, particularly in pose accuracy, compared to the other methods. The asterisk (*) indicates that results for certain methods were obtained from already released checkpoints, not from training conducted as part of this study.", "section": "4 Experimental Results"}, {"figure_path": "IwNTiNPxFt/tables/tables_17_2.jpg", "caption": "Table 1: Results on Human-Art dataset and LAION-Human subset. Methods with * are evaluated on released checkpoints.", "description": "This table presents a quantitative comparison of Stable-Pose against other state-of-the-art methods on two benchmark datasets: Human-Art and LAION-Human.  The metrics used assess pose accuracy (AP, CAP, PCE), image quality (FID, KID), and text-to-image alignment (CLIP-score).  The results show that Stable-Pose achieves superior performance in terms of pose accuracy on both datasets.  Note that some methods (*) use pre-trained checkpoints instead of being trained from scratch on the datasets, potentially explaining differences in the results.", "section": "4 Experimental Results"}]