[{"type": "text", "text": "Stable-Pose: Leveraging Transformers for Pose-Guided Text-to-Image Generation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jiajun Wang\u22171, Morteza Ghahremani\u22171,3, Yitong $\\mathrm{Li}^{*1,3}$ , Bj\u00f6rn Ommer2,3, and Christian Wachinger1,3 ", "page_idx": 0}, {"type": "text", "text": "1Lab for AI in Medical Imaging, Technical University of Munich (TUM), Germany 2CompVis $@$ LMU Munich, Germany 3Munich Center for Machine Learning (MCML), Germany ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Controllable text-to-image (T2I) diffusion models have shown impressive performance in generating high-quality visual content through the incorporation of various conditions. Current methods, however, exhibit limited performance when guided by skeleton human poses, especially in complex pose conditions such as side or rear perspectives of human figures. To address this issue, we present Stable-Pose, a novel adapter model that introduces a coarse-to-fine attention masking strategy into a vision Transformer (ViT) to gain accurate pose guidance for T2I models. Stable-Pose is designed to adeptly handle pose conditions within pre-trained Stable Diffusion, providing a refined and efficient way of aligning pose representation during image synthesis. We leverage the query-key self-attention mechanism of ViTs to explore the interconnections among different anatomical parts in human pose skeletons. Masked pose images are used to smoothly refine the attention maps based on target pose-related features in a hierarchical manner, transitioning from coarse to fine levels. Additionally, our loss function is formulated to allocate increased emphasis to the pose region, thereby augmenting the model\u2019s precision in capturing intricate pose details. We assessed the performance of Stable-Pose across five public datasets under a wide range of indoor and outdoor human pose scenarios. Stable-Pose achieved an AP score of 57.1 in the LAION-Human dataset, marking around $13\\%$ improvement over the established technique ControlNet. The project link and code are available at https://github.com/ai-med/StablePose. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Pose-guided text-to-image (T2I) generation holds immense potential for swiftly producing photorealistic images that exhibit contextual relevance and accurate posing through the integration of text prompts and pose instructions. The kinematic or skeleton pose provides a set of key points (joints) that represent the skeletal framework of the human body (shown in Figure A.1). Despite the sparsity, skeleton-pose data offers sufficient details of human poses with high flexibility and computational efficiency for T2I generation in various applications such as animation, robotics, sports training, and e-commerce, making it user-friendly and ideal for real-time applications [21]. Juxtaposed with other forms of pose information like volumetric pose with dense content, skeleton pose is capable of conveying heightened articulation information, facilitating intuitive interpretation and flexible manipulation of human poses [28; 20]. ", "page_idx": 0}, {"type": "text", "text": "Traditional pose-guided human image generation methods require a source image during training for dictating the style of the generated images [22; 23; 42; 38; 47]. Such methods, while offering control over the appearance, limit the flexibility and diversity of the output and depend heavily on the need for paired source-target data during training. In contrast, recent advancements in controllable T2I diffusion models have shown the potential to eliminate the need for source images and allowed for higher creative freedom by relying on text prompts and external conditions [44; 46; 13; 25; 18]. Enabling more versatile visual content creation, these methods often face challenges in precisely aligning conditional images with sparse representations such as skeleton pose data, especially when dealing with complex pose scenarios like those depicting the back or side views of human figures (Figure 1). Moreover, existing methods may also fail to maintain accurate body proportions, resulting in an unnatural appearance of the body. ", "page_idx": 0}, {"type": "image", "img_path": "IwNTiNPxFt/tmp/f380e3c92949e11cbbc72c8a73d18bae5b516362247c6b95cd46b856e69081ac.jpg", "img_caption": ["Figure 1: Stable-Pose leverages the patch-wise attention of ViTs to address the complex pose conditioning problem in T2I generation, showing superior performance compared to current techniques. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To address the insufficient binding of sparse pose data in T2I generation models, a potential strategy is to capture long-range patch-wise relationships among various anatomical parts of human poses. In this paper, we introduce Stable-Pose that integrates vision Transformers (ViT) into pre-trained T2I diffusion models like Stable Diffusion (SD) [33], with the goal of improving pose control by capturing patch-wise relationships from the specified pose. In Stable-Pose, the learnable attentions adhere to an innovative coarse-to-fine masking approach, ensuring that pose conditioning is directed toward the relevant pose areas while preserving the diversity of the overall image. To further enhance this effect, a pose-mask guided loss is introduced to optimize the fidelity of the generated images in adherence to the given pose instructions. We evaluated Stable-Pose across five distinct datasets, covering indoor and outdoor image/video datasets. Compared to the state-of-the-art methods, Stable-Pose achieved the highest accuracy and robustness in pose adherence and generation fidelity, making it a promising solution for enhancing pose control in T2I generation. We further performed comprehensive ablation studies to demonstrate the effectiveness of our design. In summary, our contributions are: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Addressing the challenge of generating photo-realistic human images in pose-guided T2I by integrating a novel ViT, achieving highly accurate synthesis in pose adherence and image fidelity, even under challenging conditions.   \n\u2022 Introducing a hierarchical integration of pose masks for coarse-to-fine guidance, with a novel pose-masked self-attention mechanism and pose-mask guided loss function. Stable-Pose is designed as a lightweight adapter that can be easily integrated into any pre-trained T2I diffusion models to effectively enhance pose control.   \n\u2022 Stable-Pose effectively learns to preserve intricate human shape structures and accurate body proportions, achieving exceptional performance across five publicly available datasets, encompassing both image and video data. ", "page_idx": 1}, {"type": "image", "img_path": "IwNTiNPxFt/tmp/c6dd2fc9056a3d8e1734bda8ecd0c3b7669ebf6803d07c94afdecd6210574aa6.jpg", "img_caption": ["Figure 2: The Stable Diffusion architecture with Stable-Pose: operating on the pose skeleton image, Stable-Pose integrates a trainable ViT unit into the frozen-weight Stable Diffusion [33] to improve the generation of pose-guided human images. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Pose-Guided Human Image Generation: Traditional pose-guided human image generation takes a source image and pose as input, aiming to generate photo-realistic human images in specific poses while preserving the appearance from the source image. Prior works [22; 23; 42; 38; 9] primarily utilized generative adversarial network (GAN) or variational autoencoder (VAE), treating the synthesis task as conditional image generation. Zhu et al. [47] integrated attention mechanism for appearance optimization in a Pose Attention Transfer Network (PATN). Zhang et al. [45] implemented a Dual-task Pose Transformer Network (DPTN), using a Transformer module to incorporate features from two tasks: an auxiliary source image reconstruction task and the main pose-guided target image generation task, thereby capturing the dual-task correlation. With the recent emergence of diffusion models [12], Bhunia et al. [4] proposed a texture diffusion module to transfer texture patterns from the source image to the denoising process. Moreover, classifier-free guidance [11] is applied to provide disentangled guidance for style and pose. Shen et al. [37] proposed a three-stage synthesis pipeline which progressively performs global feature extraction, target prediction, and refinement with three diffusion models. While the source image offers control over appearance, a limitation arises from the necessity of paired source-target data during training. Text, however, obviates this need and offers higher flexibility and diversity for the synthesis. Thus, incorporating text conditions for pose-skeleton-guided human image generation shows significant promise [19; 16]. ", "page_idx": 2}, {"type": "text", "text": "Controllable Diffusion Models: Large-scale T2I diffusion models [33; 31; 32; 35; 26] excel at creating diverse and high-quality images, yet they often lack precise control with solely text-based prompts. Recent studies aim to enhance the control of T2I models using various conditions such as canny edge, sketch, and human pose [13; 44; 25; 18; 46; 16; 24; 29]. These approaches can be broadly classified into two groups: training the entire T2I model or developing plug-in adapters for pre-trained T2I models. As in the first group, Composer [13] trains a diffusion model from scratch with a decomposition-composition paradigm, enabling multi-control capability. HumanSD [16] fine-tunes the entire SD model using a heatmap-guided loss tailored for pose control. In contrast, T2I-Adapter [25] and GLIGEN [18] train lightweight adapters whose outputs are incorporated into the frozen SD. Similarly, ControlNet [44] employs a trainable copy of the SD encoder to encode conditions for the frozen SD. Uni-ControlNet [46] introduces a uni-adapter for multiple conditions injection to the trainable branch in a multi-scale manner. ControlNet $^{++}$ [24] proposes to improve controllable generation by explicitly optimizing the cycle consistency between generated images and conditional controls. Our method aligns with the latter category, as it is characterized by reduced training time, cost-effectiveness, and generalizability. ", "page_idx": 2}, {"type": "text", "text": "3 Proposed Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Stable-Pose has a trainable ViT unit that is integrated into the pre-trained T2I diffusion models to direct diffusion models toward the conditioned pose. In latent diffusion models (LDMs) [33], a pre-trained encoder $\\mathcal{E}$ and decoder $\\mathcal{D}$ are employed to transform an input RGB image $x\\in\\mathcal{R}^{H\\times\\bar{W}\\times3}$ with height $H$ and width $W$ into a latent space with reduced spatial dimensions and vice versa. The diffusion process is then efficiently conducted in the down-scaled latent space. During the training, the forward process in diffusion models adds noise to the encoded RGB image ${\\bf z}_{0}=\\mathcal{E}(\\bar{x})$ to generate its noisy sample $\\mathbf{z}_{t}\\in\\mathcal{R}^{C\\times h\\times w}$ with height $h$ , width $w$ , and channel $C$ via ", "page_idx": 2}, {"type": "image", "img_path": "IwNTiNPxFt/tmp/800da1d16c5a8f8968f6c5b4154a8364f9a88955047314db2cc9b5a409e55440.jpg", "img_caption": ["Figure 3: Stable-Pose consists of a pose encoder $\\beta_{\\theta}$ and a coarse-to-fine Pose-Masked Self-Attention (PMSA) ViT $\\mathcal{F}_{\\theta}$ for seeking the patch-wise relationship of human parts. PMSA restricts attention to embedding tokens within a specific attention mask to ensure that each embedding token can only attend to pose embedding tokens, not non-pose ones. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{z}_{t}=\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{z}_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon,\\quad\\epsilon\\sim\\mathcal{N}(0,I),\\quad t=1,\\cdot\\cdot\\cdot\\cdot,T,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ${\\bar{\\alpha}}_{t}$ is a pre-determined hyperparameter that controls the noise level at step $t$ . The reverse process of diffusion models, so-called denoising, learns the statistics of the Gaussian distribution at each time step. The reverse process is formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{\\theta}(\\mathbf{z}_{t-1}|\\mathbf{z}_{t})=\\mathcal{N}(\\mathbf{z}_{t-1};\\mu_{\\theta}(\\mathbf{z}_{t},t),\\Sigma_{\\theta}(\\mathbf{z}_{t},t)).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "As shown in Figure 2, the denoising network $\\epsilon_{\\theta}$ adopts a UNet backbone, which is equipped with Stable-Pose for augmenting the latent encoding given the input conditional pose image. Let $\\epsilon_{\\theta}(\\mathbf{z}_{t},t,p)$ , $t\\in\\{1,\\cdot\\cdot\\cdot\\,,T\\}$ , represent a T-step denoising UNet with gradients $\\nabla\\theta$ over a batch and input text prompt $p$ . The conditional LDM is learned through $T$ steps by minimizing ${\\mathcal{L}}=$ $\\bar{\\mathbb{E}}_{{\\mathbf{z}},{p},\\varphi,\\epsilon\\sim\\mathcal{N}\\left(0,I\\right),t}\\left[\\left|\\epsilon-\\epsilon_{\\theta}\\left(\\mathbf{z}_{t},t,\\tau_{\\theta}\\left(p\\right),v_{\\theta}\\left(\\mathbf{z}_{t},\\varphi\\right)\\right)\\right|\\left|_{2}^{2}\\right]$ , where Stable-Pose $\\upsilon_{\\theta}$ conditions the latent encoding $\\mathbf{z}_{t}$ on the input pose skeleton $\\varphi\\in\\mathcal{R}^{h\\times w\\times3}$ , and $\\tau_{\\theta}$ is a text encoder that maps the text prompt $p$ to an intermediate sequence. The proposed framework is detailed in Figure 3. Stable-Pose aims at improving the frozen decoder in the UNet of SD to condition the input latent encoding $\\mathbf{z}_{t}$ on the conditional pose image $\\varphi$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{z}_{t}^{\\prime}=\\mathbf{z}_{t}+v_{\\theta}(\\mathbf{z}_{t},\\varphi).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In Stable-Pose, the pose image $\\varphi$ and the given latent encoding $\\mathbf{z}_{t}$ are processed by two main blocks named Pose-Masked Self-Attention (PMSA) $\\mathcal{F}_{\\theta}$ and pose encoder $\\beta_{\\theta}$ in such a way that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{v}_{\\theta}(\\mathbf{z}_{t},\\varphi)=\\mathcal{F}_{\\theta}(\\mathbf{z}_{t},\\varphi)+\\beta_{\\theta}(\\varphi),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the pose encoder $\\beta_{\\theta}$ provides high-level features for the input pose while PMSA $\\mathcal{F}_{\\theta}$ explores the patch-wise relationship across input $\\mathbf{z}_{t}$ using a self-attention mechanism and the binary-masked version of the pose image. PMSA employs a coarse-to-fine framework that provides additional guidance to the latent encoding, directing it towards attending to the conditioned pose. We detail each block in the subsequent sections. The updated latent encoding $\\mathbf{z}_{t}^{\\prime}$ is subsequently fed through the encoder of SD, followed by a series of zero convolutional blocks, a structural resemblance to the architecture employed in ControlNet [44] for ensuring a robust encoding of conditional images. ", "page_idx": 3}, {"type": "text", "text": "Pose encoder $\\beta_{\\theta}:\\mathcal{R}^{H\\times W\\times3}\\rightarrow\\mathcal{R}^{f\\times h\\times w}$ is a trainable encoder that maps input pose skeleton image into a feature with height $h$ , width $w$ , and channel $f$ . To this end, we employ a combination of six convolutional layers with SiLU activation layers [8], downsampling the input pose image by a factor of 8. A zero-convolutional layer is added in the end. As the input pose image contains sparse information, this straightforward pose encoder is sufficient for accurate encoding of the skeleton pose. ", "page_idx": 3}, {"type": "text", "text": "PMSA $\\mathcal{F}_{\\theta}:\\mathcal{R}^{f_{i n}\\times h\\times w}\\rightarrow\\mathcal{R}^{f\\times h\\times w}$ seeks the potential relationships between patches within the latent encoding $\\mathbf{z}_{t}$ . The interconnections of various parts of the human body suggest the presence of ", "page_idx": 3}, {"type": "text", "text": "1: procedure ATTNMASK $(m_{k})$   \n2: $m_{k}$ : Patchified and binarized pose masks of size $(N,H,W)$   \n3: $m_{k}\\leftarrow$ Flatten $m_{k}$ along the last two dimensions   \n4: N \u2190mk.shape[0]   \n5: $L\\gets m_{k}.s h a p e[1]$   \n6: a $t t n\\_m a s k\\gets$ initialize as tensor of size $(N,L,L)$ with entries set to $-i n f$   \n7: for $i\\in[0,N-1]$ do   \n8: indice $s\\gets$ find indices where $m_{k}[i]==1$   \n9: attn_mask[i][indices, : $;]\\leftarrow0$   \n10: attn_mask[i][:, indices] \u21900   \n11: end for   \n12: return attn_mask   \n13: end procedure ", "page_idx": 4}, {"type": "text", "text": "cohesive relationships among them. To capture this, we leverage the self-attention mechanism. We divide $\\mathbf{z}_{t}$ into $L$ non-overlapping patches of size $p\\times p$ , i.e., $\\bar{\\mathbf{z}_{t}}=\\{\\mathbf{z}_{t}^{(1)},\\dots,\\mathbf{z}_{t}^{(L)}|\\mathbf{z}_{t}^{(l)}\\in\\mathbb{R}^{p^{2}\\times f_{i n}}\\}$ . PMSA projects the patch embeddings $\\mathbf{z}_{t}$ into Query $\\mathbf{Q}=\\mathbf{z}_{t}\\mathbf{W}_{Q}$ , Key ${\\bf K}={\\bf z}_{t}{\\bf W}_{K}$ , and Value ${\\bf V}=$ $\\mathbf{z}_{t}\\mathbf{W}_{V}$ via three learnable weight matrices $\\mathbf{W}_{Q}\\in\\mathbb{R}^{f_{i n}\\times f_{q}}$ , $\\mathbf{W}_{K}\\in\\mathbb{R}^{f_{i n}\\times f_{k}}$ , and $\\mathbf{W}_{V}\\in\\mathbb{R}^{f_{i n}\\times f_{v}}$ , respectively. Then it computes the attention scores between all patches via ", "page_idx": 4}, {"type": "equation", "text": "$$\nA_{k}=\\sf a t t e n t i o n\\left(\\mathbf{Q},\\mathbf{K},\\mathbf{V},\\mathbf{m}_{k}\\right)=\\sf s o f t m a x\\left(\\frac{\\mathbf{Q}\\mathbf{K}^{\\top}}{\\sqrt{f_{q}}}+\\sf A t t n M a s k(\\mathbf{m}_{k})\\right)\\mathbf{V}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In this equation, ${\\bf m}_{k}$ denotes a binary mask derived from the input pose image, which is expanded by AttnMask $(\\cdot)$ for being used in the self-attention computation (see Figure 3). $\\mathbf{m}_{k}$ is obtained by converting the pose image into a binary mask and downsampling it into the same size of the latent vector $\\mathbf{z}_{t}$ . The resultant mask is then dilated by a Gaussian kernel of length $k$ . The dilated-binary mask ${\\bf m}_{k}$ is then partitioned into $L$ non-overlapping patches of size $p\\times p$ . The patches containing pose are labelled as 1 while others are marked as 0. We form a resulting $L\\times L$ attention mask based on these $L$ patches through the function $\\mathtt{A t t r a M a s k(\\cdot)}$ . As illustrated in Algorithm 1, for patch entries that correspond to pose regions, both the respective row and column in the mask are set to 0. For all other regions not associated with pose, we assign an extremely small integer value. The attention mask helps to enhance the focus of PMSA on the pose-specific regions. ", "page_idx": 4}, {"type": "text", "text": "We implement a sequence of $N$ blocks of ViTs, each associated with a unique pose mask, arranged in a coarse-to-fine progression on the latent encoding. This approach gradually steers the latent encoding towards conforming to the specified pose condition. If $\\bar{\\pmb{k}}=\\{\\bar{k_{1}},\\bar{\\bf\\Phi}...\\,,\\bar{k_{N}}\\}$ denotes a set of Gaussian kernels where $k_{1}>\\cdot\\cdot\\cdot>k_{N}$ , then the coarse-to-fine self-attention is obtained via ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{1}=\\mathbf{attention}\\left(\\mathbf{Q},\\mathbf{K},\\mathbf{V},\\mathbf{m}_{k_{1}}\\right)}\\\\ &{A_{n}=\\mathbf{attention}\\left(\\mathbf{Q}_{A_{n-1}},\\mathbf{K}_{A_{n-1}},\\mathbf{V}_{A_{n-1}},\\mathbf{m}_{k_{n}}\\right),\\quad\\mathrm{for}\\ n=\\{2,\\cdots\\,,N\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Each encoding $A_{n}$ undergoes further processing by a Feed Forward unit, with the resulting $A_{N}$ integrated into the feature from the pose encoder $\\beta_{\\theta}$ , as shown in Figure 3. The Feed Forward block consists of two linear transformations [3] combined with dropout layers, followed by ReLU non-linear activation functions [2]. ", "page_idx": 4}, {"type": "text", "text": "Pose-mask guided loss criterion: The training of SD models requires high costs in hardware and datasets. Therefore, plug-in adapters for the frozen SD models, such as Stable-Pose, can enhance training efficiency by eliminating the need to compute gradients or maintain optimizer states for SD parameters. Instead, the optimization process focuses on improving Stable-Pose parameters. The loss in Stable-Pose aligns with the coarse-to-fine approach and is defined as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}=\\underset{\\mathbf{z},p,\\varphi,\\epsilon\\sim\\mathcal{N}\\left(0,I\\right),t}{\\mathbb{E}}\\left[\\left|\\right|\\left(\\epsilon-\\epsilon_{\\theta}\\left(\\mathbf{z}_{t},t,\\tau_{\\theta}\\left(p\\right),v_{\\theta}\\left(\\mathbf{z}_{t},\\varphi\\right)\\right)\\odot\\left(1-\\mathbf{m}_{k_{N}}\\right)\\right)\\left|\\right|_{2}^{2}\\right]}\\\\ {+\\left.\\alpha\\underset{\\mathbf{z},p,\\varphi,\\epsilon\\sim\\mathcal{N}\\left(0,I\\right),t}{\\mathbb{E}}\\left[\\left|\\right|\\left(\\epsilon-\\epsilon_{\\theta}\\left(\\mathbf{z}_{t},t,\\tau_{\\theta}\\left(p\\right),v_{\\theta}\\left(\\mathbf{z}_{t},\\varphi\\right)\\right)\\odot\\mathbf{m}_{k_{N}}\\right)\\left|\\right|_{2}^{2}\\right].\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $\\alpha\\geq1$ represents a predetermined pose-mask guidance hyperparameter that emphasizes the significance of masked region contents. ", "page_idx": 4}, {"type": "table", "img_path": "IwNTiNPxFt/tmp/6fce7dfdf50ed0cc40e3fe53a2de508e05cca27b8990060908819347cd2b8836.jpg", "table_caption": ["Table 1: Results on Human-Art dataset and LAION-Human subset. Methods with \\* are evaluated on released checkpoints. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "4 Experimental Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets. We assessed the performance of the proposed Stable-Pose as well as competing methods on five large-scale human-centric datasets including Human-Art [15], LAION-Human [16], UBC Fashion [43], Dance Track [40], and DAVIS [27] dataset. Details of the datasets and processing steps can be found in Sec. A.1. ", "page_idx": 5}, {"type": "text", "text": "Implementation Details. Similar to previous work [44; 25; 46], we fine-tuned our model on SD with version 1.5. We utilized Adam [17] optimizer with a learning rate of $1\\times10^{-5}$ . For our proposed PMSA ViT module, we adopted a depth of 2 and a patch size of 2, where coarse-to-fine pose masks were generated using two Gaussian fliters, each with a sigma value of 3 but with differing kernel sizes of 23 and 13, respectively. We will explore the effects of these hyperparameters with more details in Sec. 4.2. In the pose-mask guided loss function, we set an $\\alpha$ of 5 as the guidance factor. We also followed [44] to randomly replace text prompts as empty strings at a probability of 0.5, which aims to strengthen the control of the pose input. During inference, no text prompts were removed and a DDIM sampler [39] with time steps 50 was utilized to generate images. On the Human-Art dataset, we trained all techniques, including ours for 10 epochs to ensure a fair comparison. On the LAIONHuman subset, we trained Stable-Pose, HumanSD [16], GLIGEN [18] and Uni-ControlNet [46] for 10 epochs, while we used released checkpoints from other techniques due to computational limitations. The training was executed using two NVIDIA A100 GPUs, with our method completing in 15 hours for the Human-Art dataset and 70 hours for the LAION-Human subset. This represents a substantial decrease in GPU hours compared to SOTA techniques. For instance, the T2I-Adapter requires approximately 300 GPU hours to train on a large-scale dataset. In contrast, our approach requires less than a quarter of that time and still delivers superior performance. We kept the same seed list for all techniques, including ours, during both training and inference time, to ensure fair comparison and reproducibility. More detailed information is provided in Sec. A.2. ", "page_idx": 5}, {"type": "text", "text": "Evaluation Metrics. We adopt six metrics for evaluation, covering pose accuracy, image quality, and text-image alignment. For pose accuracy, we employ mean Average Precision (AP), Pose Cosine Similarity-based AP (CAP) [1], and People Counting Error (PCE) [7], measuring the accuracy between the provided poses and the pose results extracted from the generated images by the pretrained pose estimator HigherHRNet [6]. For image quality, we use Fr\u00e9chet inception distance (FID) [10] and Kernel Inception Distance (KID) [5]. Both metrics measure the diversity and fidelity of generated images and are widely used in image synthesis tasks. For text-image alignment, we include the CLIP score [30] that indicates how well the CLIP model believes the text describes the image. Details of the evaluation metrics can be found in Sec. A.4. ", "page_idx": 5}, {"type": "text", "text": "Garage kits, afgurineofagirlwitha gun ontopofa smallplatform ", "page_idx": 6}, {"type": "image", "img_path": "IwNTiNPxFt/tmp/b37f78e62cfeafc4c024d080a903ee5f7ee4d182d456090a57301753f849e56b.jpg", "img_caption": ["Figure 4: Qualitative results of SOTA techniques and our Stable-Pose on Human-Art (first two rows) and LAION-Human (last two rows). An illustration of the pose input is shown in Figure A.1. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.1 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Quantitative and Qualitative Results. Table 1 reports the quantitative results on both datasets among different methods. We reported the mean Average Precision (AP), Pose Cosine Similaritybased AP (CAP), People Count Error (PCE), Fr\u00e9chet Inception Distance (FID), Kernel Inception Distance (KID), and the CLIP Similarity (CLIP-score). KID is multiplied by 100 for Human-Art and 1000 for LAION-Human for readability. Table 1 shows that Stable-Pose achieved the highest AP (48.87 on Human-Art and 57.41 on LAION-Human) and CAP (71.04 on Human-Art and 68.06 on LAION-Human), surpassing the SOTA methods by more than $10\\%$ . This highlights Stable-Pose\u2019s superiority in pose alignment. In terms of image quality and text-image alignment, Stable-Pose achieved comparable results against other methods, with only marginal discrepancy in FID/KID scores, yet the difference is negligible and the resulting quality remains high. Overall, these results underscore Stable-Pose\u2019s exceptional accuracy and robustness in both pose control and visual fidelity. ", "page_idx": 6}, {"type": "text", "text": "The qualitative results obtained from Human-Art [15] and LAION-Human [16] are illustrated in Figure 4. Consistent with the quantitative results, Stable-Pose demonstrates superior control compared to the other SOTA methods in both pose accuracy and text alignment, even in scenarios involving complex poses (the first row of Figure 4, which is a back view of the figure), and multiple individuals (the third row of Figure 4), while the other methods fail to consistently maintain the integrity of the original pose instructions. This is particularly evident in dynamic poses (e.g., yoga poses and athletic activities), where Stable-Pose manages to capture the pose dynamism more faithfully than others. ", "page_idx": 6}, {"type": "text", "text": "Stable-Pose as a generic adapter. Stable-Pose is designed as a lightweight generic adapter that can be easily integrated into any pre-trained T2I diffusion models to effectively enhance pose control. To further validate its generalizability, we conducted additional experiments by applying Stable-Pose on top of a pre-trained HumanSD [16] model. As shown in Table 2, the inclusion of Stable-Pose considerably improved the baseline HumanSD by over $10\\%$ in AP and $12\\%$ in KID, highlighting its high generalizability and effectiveness in enhancing both pose control and image quality. ", "page_idx": 6}, {"type": "text", "text": "Results on Varying Pose Orientations. Our experiments revealed that the current SOTA methods often faltered when tasked with creating images of humans in less common orientations, such as side or back poses. To investigate the capabilities of these methods in rendering atypical poses, we assembled a collection of around 2,650 images from the UBC Fashion dataset [43], which comprises exclusively front, side, and back poses. We evaluated the checkpoints of each technique from the LAION-Human dataset to assess pose alignment. As reported in Table 3, Stable-Pose significantly outperforms other methods in recognizing and generating humans in all pose orientations, especially for rarer poses in side and back views, which surpasses the other methods by around $20\\%$ in AP. This further validates the robust controllability of Stable-Pose. ", "page_idx": 6}, {"type": "table", "img_path": "IwNTiNPxFt/tmp/b943a015ada4377f4aafa27cd2d68c170ed4834f9e46de6b2946303f1d63048b.jpg", "table_caption": ["Table 2: Stable-Pose as a lightweight adapter on pre-trained HumanSD model. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "IwNTiNPxFt/tmp/fcbd9b926b6af06cb11667cca6aaba77defa37e196dcee9cd3f53338c033b004.jpg", "table_caption": ["Table 3: Results of varying pose orientations on the UBC Fashion dataset. We report mean Average Precision (AP) across different methods for three orientations: front, side, and back. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Results on the Outdoor and Indoor Poses. We extend the evaluation on both outdoor and indoor poseguided T2I generation. We selected approximately 2,000 frames from the DAVIS dataset [27], which comprises videos of human outdoor activities, as our outdoor pose assessment. In addition, we randomly chose around 2,000 images from the Dance Track dataset [40], which is characterized by its group dance videos where most videos were shot indoors with multi", "page_idx": 7}, {"type": "table", "img_path": "IwNTiNPxFt/tmp/3fe2de8d0afc8f4be6adb434935e95be7e23506b320632fc744fd257091eff05.jpg", "table_caption": ["Table 4: Results on the outdoor poses from the DAVIS dataset and the indoor poses from the Dance Track dataset. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "ple individuals and complex poses, as indoor pose-alignment evaluation. As shown in Table 4, the consistently highest AP and CAP scores achieved by Stable-Pose demonstrate its robustness in pose-controlled T2I generation across diverse environments, highlighting its potential as a backbone for pose-guided video generation. Further results can be found in Table A.6. ", "page_idx": 7}, {"type": "text", "text": "4.2 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conducted a comprehensive ablation study of Stable-Pose on the Human-Art dataset, including the effectiveness of pose masks, coarse-to-fine design, pose-mask guidance strength, and the effectiveness of PMSA and its ViT backbone. Further ablations on model parameters can be found in Sec. A.6. ", "page_idx": 7}, {"type": "text", "text": "Effectiveness of Pose Masks. To evaluate the impact of pose masks as input to our proposed PMSA and pose-mask guided loss function, we compared with removing them from the PMSA and/or from the associated loss function. As shown in Table 5, incorporating pose masks in both PMSA and loss function significantly enhanced the performance in both pose alignment and image quality. ", "page_idx": 7}, {"type": "table", "img_path": "IwNTiNPxFt/tmp/3197755d3d1cb9416fb4b4e7a6e62e3750dee2ee102b8e3deb6de6110e134549.jpg", "table_caption": ["Table 5: Results of the pose mask ablation on Human-Art dataset. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Coarse-to-Fine Masking Guidance. The granularity of pose-masks is specified by the Gaussian kernels in Gaussian Filters, where a larger kernel generates coarser pose-masks. We compared the results of constant granularity, fine-to-coarse as well as coarse-to-fine setting. All experiments are based on a ViT with PMSA and depth of 2 and Gaussian Filters with fixed sigma $\\sigma=3$ . Details can be found in Sec. A.3. As indicated in Table 6, the coarse-to-fine approach consistently offers the best performance across metrics for pose alignment and image quality. This improvement is likely due to its progressive refinement from coarser to finer granularity in pose regions. By methodically narrowing the focus to more precise controllable areas, this strategy smoothly enhances the accuracy of pose adjustments and the overall quality of the generated images. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "IwNTiNPxFt/tmp/b910fb126bda3232bf1f59f2b13bc0adb14f7823a45f888dcd8735ffc1d90b36.jpg", "table_caption": ["Table 6: Results of different Gaussian kernels $k$ used for pose masks in Stable-Pose. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Effectiveness of PMSA and its ViT backbone. Our PMSA incorporates additional pose masks, derived from pose skeletons that have been expanded using Gaussian filters. To evaluate the effectiveness of PMSA, we instead only integrated these augmented pose masks into ControlNet without our PMSA block. We explored two configurations: one in which the original pose skeleton was concatenated with one coarsely enlarged pose mask, denoted as ControlNet-PM1, and another where it was concatenated with both the coarsely and finely enlarged pose masks, referred to as ControlNet-PM2. Table 7 indicates that the enlarged pose masks yield only marginal improvements in ControlNet, suggesting that the substantial enhancements observed in Stable-Pose are primarily due to the innovative design of PMSA, rather than the additional pose masks input. ", "page_idx": 8}, {"type": "table", "img_path": "IwNTiNPxFt/tmp/efed9b0897d70fac69a47891c93f394e0541ac3a221b8efaa4798c6d448ac068.jpg", "table_caption": ["Table 7: Ablation study on the effectiveness of PMSA and its ViT design. We show the performance of ControlNet with additional pose masks input, and PMSA with ResNet or ViT as backbone. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Further, to validate the effectiveness of the ViT backbone in PMSA (PMSA w/ ViT), we replaced it with a conventional pose-masked self-attention module operating between residual blocks (PMSA w/ ResNet). We integrated the same pose masks in both configurations to ensure a fair comparison. Table 7 demonstrates that the ViT design in PMSA significantly outperforms the conventional approach. This substantiates the superior capability of ViT to capture long-range, patch-wise interactions among various anatomical parts of human poses to enhance the pose alignment. ", "page_idx": 8}, {"type": "text", "text": "Pose Encoding in Stable-Pose. To further validate the design of pose encoding in Stable-Pose, we implemented an ablation study by removing either the pose encoder $\\beta$ or PMSA-ViT, retaining only one type of pose encoding. The results in Table 8 show that using only PMSA-ViT yields an AP of 36.48, which is expected due to the absence of color-coding information for distinguishing body parts. While using $\\beta$ alone increases the AP to 45.03. However, the most significant improvement is observed when integrating both local and global information encoding into the Stable-Pose architecture, achieving the highest AP of 48.88. ", "page_idx": 8}, {"type": "table", "img_path": "IwNTiNPxFt/tmp/5ab83cf22c570a8c41a3d20dbe01423c93659f6d6db7b90a0b00b4174110e849.jpg", "table_caption": ["Table 8: Ablation study on the pose encoding design in Stable-Pose. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Pose Masks During Inference. We incorporate the pose masks during inference by default to enhance pose control. To further validate their effectiveness, we additionally conducted experiments with removing the pose masks during inference. As shown in Table 9, this led to approximately a $3\\%$ drop in AP. This could be due to two main reasons: 1) The pose masks provided additional guidance, thus enhancing control; 2) The inclusion of pose masks maintains consistency between the model\u2019s behavior during training and inference. Thus, including pose masks beneftis pose control in the generation. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "table", "img_path": "IwNTiNPxFt/tmp/1f0a96e70430b15e5c9b4e3678f7106e3f77736073754997d66ec0e2c4a8c109.jpg", "table_caption": ["Table 9: Ablation study on the presence of pose masks during inference. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Pose-mask Guidance Strength in Loss. In our proposed loss function in Eq. (7), the parameter $\\alpha$ , referred to as the pose-mask guidance strength, controls the intensity of penalization applied to the pose regions. We evaluated the impact of varying $\\alpha$ from 1 to 10 on pose alignment and image quality, with the results presented in Figure 5. Increasing $\\alpha$ in our proposed loss means putting more attention on the foreground pose regions. However, when $\\alpha$ is getting too large, it forces the model to learn irrelevant texture information like clothing, which negatively impacts training and slightly decreases AP. Despite this, Stable-Pose still outperforms others across an $\\alpha$ range of 1-10. Notably, increasing $\\alpha$ has a significant impact on FID, worsening it from 11.0 at $\\alpha{=}5$ to 13.0 at $\\alpha{=}10$ . This indicates that focusing solely on the pose regions may decrease the quality of generated content in non-pose regions. Thus there exists a slight trade-off in selecting $\\alpha$ to maintain both high pose accuracy and image quality, in which a value around 5 to 6 turns out to be optimal. ", "page_idx": 9}, {"type": "image", "img_path": "IwNTiNPxFt/tmp/578015d2737d8e3c0c81a18eb47196c0b2ae9891121797eb582f9aab7f3976cb.jpg", "img_caption": ["Figure 5: Ablation on pose-mask guidance strength in the loss function. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Discussion and Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduced Stable-Pose, a novel adapter that leverages vision transformers with a coarse-to-fine pose-masked self-attention strategy, specifically designed to efficiently manage precise pose controls during T2I generation. Stable-Pose outperforms current controllable T2I generation methods across five distinct public datasets, demonstrating high generation robustness across diverse environments and various pose scenarios. Notably, in complex scenarios involving rare poses such as side or back views and multiple figures, Stable-Pose exhibits exceptional performance in both pose and visual fidelity. This can be attributed to its advanced capability in capturing long-range patchwise relationships between different anatomical parts of human pose images through our intricate conditioning design. As a result, Stable-Pose holds significant potential in applications demanding high pose accuracy. It can be easily integrated into any pre-trained T2I diffusion models as a lightweight generic adapter to effectively enhance pose control. One limitation of Stable-Pose is its slightly longer inference time (Sec. A.2), primarily due to the integration of self-attention mechanisms within the ViT. In addition, despite excelling in pose control, Stable-Pose has yet to be evaluated with other conditions such as edge maps. Nevertheless, its design allows for straightforward adaptation to various external conditions, suggesting high potential for diverse applications. ", "page_idx": 9}, {"type": "text", "text": "Broader Impacts: Stable-Pose\u2019s excellent pose control makes it a valuable tool in creating diverse artworks, animations, movies, and sports training programs. Additionally, it can be a reliable tool in healthcare and rehabilitation for correcting posture and preventing patients from musculoskeletal issues. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the Munich Center for Machine Learning (MCML) and the German Research Foundation (DFG). The authors gratefully acknowledge the computational and data resources provided by the Leibniz Supercomputing Centre. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Posenet similarity. URL https://github.com/freshsomebody/posenet-similarity.   \n[2] A. F. Agarap. Deep learning using rectified linear units (relu). arXiv preprint arXiv:1803.08375, 2018.   \n[3] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.   \n[4] A. K. Bhunia, S. Khan, H. Cholakkal, R. M. Anwer, J. Laaksonen, M. Shah, and F. S. Khan. Person image synthesis via denoising diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5968\u20135976, 2023.   \n[5] M. Bi\u00b4nkowski, D. J. Sutherland, M. Arbel, and A. Gretton. Demystifying mmd gans. arXiv preprint arXiv:1801.01401, 2018.   \n[6] B. Cheng, B. Xiao, J. Wang, H. Shi, T. S. Huang, and L. Zhang. Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5386\u20135395, 2020.   \n[7] S. Y. Cheong, A. Mustafa, and A. Gilbert. Kpe: Keypoint pose encoding for transformer-based image generation. arXiv preprint arXiv:2203.04907, 2022.   \n[8] S. Elfwing, E. Uchibe, and K. Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural networks, 107:3\u201311, 2018.   \n[9] P. Esser, E. Sutter, and B. Ommer. A variational u-net for conditional appearance and shape generation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8857\u20138866, 2018.   \n[10] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.   \n[11] J. Ho and T. Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.   \n[12] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[13] L. Huang, D. Chen, Y. Liu, Y. Shen, D. Zhao, and J. Zhou. Composer: Creative and controllable image synthesis with composable conditions. arXiv preprint arXiv:2302.09778, 2023.   \n[14] S. Jayasumana, S. Ramalingam, A. Veit, D. Glasner, A. Chakrabarti, and S. Kumar. Rethinking fid: Towards a better evaluation metric for image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9307\u20139315, 2024.   \n[15] X. Ju, A. Zeng, J. Wang, Q. Xu, and L. Zhang. Human-art: A versatile human-centric dataset bridging natural and artificial scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 618\u2013629, 2023.   \n[16] X. Ju, A. Zeng, C. Zhao, J. Wang, L. Zhang, and Q. Xu. Humansd: A native skeleton-guided diffusion model for human image generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15988\u201315998, 2023.   \n[17] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[18] Y. Li, H. Liu, Q. Wu, F. Mu, J. Yang, J. Gao, C. Li, and Y. J. Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22511\u201322521, 2023.   \n[19] D. Liu, L. Wu, F. Zheng, L. Liu, and M. Wang. Verbal-person nets: Pose-guided multi-granularity language-to-person generation. IEEE Transactions on Neural Networks and Learning Systems, 34(11): 8589\u20138601, 2023.   \n[20] J. Liu, G. Wang, L.-Y. Duan, K. Abdiyeva, and A. C. Kot. Skeleton-based human action recognition with global context-aware attention lstm networks. IEEE Transactions on Image Processing, 27(4):1586\u20131599, 2017.   \n[21] Z. Liu, J. Zhu, J. Bu, and C. Chen. A survey of human pose estimation: the body parts parsing based methods. Journal of Visual Communication and Image Representation, 32:10\u201319, 2015.   \n[22] L. Ma, X. Jia, Q. Sun, B. Schiele, T. Tuytelaars, and L. Van Gool. Pose guided person image generation. Advances in neural information processing systems, 30, 2017.   \n[23] Y. Men, Y. Mao, Y. Jiang, W.-Y. Ma, and Z. Lian. Controllable person image synthesis with attributedecomposed gan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5084\u20135093, 2020.   \n[24] L. Ming, Y. Taojiannan, K. Huafeng, W. Jie, W. Zhaoning, X. Xuefeng, and C. Chen. Controlnet++: Improving conditional controls with efficient consistency feedback. arXiv preprint arXiv:2404.07987, 2024.   \n[25] C. Mou, X. Wang, L. Xie, Y. Wu, J. Zhang, Z. Qi, Y. Shan, and X. Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv preprint arXiv:2302.08453, 2023.   \n[26] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever, and M. Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.   \n[27] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. V. Gool, M. Gross, and A. Sorkine-Hornung. A benchmark dataset and evaluation methodology for video object segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.   \n[28] C. R. Qi, H. Su, M. Nie\u00dfner, A. Dai, M. Yan, and L. J. Guibas. Volumetric and multi-view cnns for object classification on 3d data. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5648\u20135656, 2016.   \n[29] Z. Qiu, W. Liu, H. Feng, Y. Xue, Y. Feng, Z. Liu, D. Zhang, A. Weller, and B. Sch\u00f6lkopf. Controlling text-to-image diffusion by orthogonal finetuning. In NeurIPS, 2023.   \n[30] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[31] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 8821\u20138831. Pmlr, 2021.   \n[32] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.   \n[33] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n[34] N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aberman. Dreambooth: Fine tuning text-toimage diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 22500\u201322510, 2023.   \n[35] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:36479\u201336494, 2022.   \n[36] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:25278\u201325294, 2022.   \n[37] F. Shen, H. Ye, J. Zhang, C. Wang, X. Han, and W. Yang. Advancing pose-guided image synthesis with progressive conditional diffusion models. arXiv preprint arXiv:2310.06313, 2023.   \n[38] A. Siarohin, E. Sangineto, S. Lathuiliere, and N. Sebe. Deformable gans for pose-based human image generation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3408\u20133416, 2018.   \n[39] J. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.   \n[40] P. Sun, J. Cao, Y. Jiang, Z. Yuan, S. Bai, K. Kitani, and P. Luo. Dancetrack: Multi-object tracking in uniform appearance and diverse motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.   \n[41] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818\u20132826, 2016.   \n[42] H. Tang, S. Bai, L. Zhang, P. H. Torr, and N. Sebe. Xinggan for person image generation. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXV 16, pages 717\u2013734. Springer, 2020.   \n[43] P. Zablotskaia, A. Siarohin, B. Zhao, and L. Sigal. Dwnet: Dense warp-based network for pose-guided human video generation. arXiv preprint arXiv:1910.09139, 2019.   \n[44] L. Zhang, A. Rao, and M. Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836\u20133847, 2023.   \n[45] P. Zhang, L. Yang, J.-H. Lai, and X. Xie. Exploring dual-task correlation for pose guided person image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7713\u20137722, 2022.   \n[46] S. Zhao, D. Chen, Y.-C. Chen, J. Bao, S. Hao, L. Yuan, and K.-Y. K. Wong. Uni-controlnet: All-in-one control to text-to-image diffusion models. Advances in Neural Information Processing Systems, 36, 2024.   \n[47] Z. Zhu, T. Huang, B. Shi, M. Yu, B. Wang, and X. Bai. Progressive pose attention transfer for person image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2347\u20132356, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "image", "img_path": "IwNTiNPxFt/tmp/916725d05923b4e7175223c35afdadcb3ee2ad4bb42a563dbfeec9373f80fa9a.jpg", "img_caption": ["Figure A.1: Illustration of pose input, where each body part is marked in different color. The pose-image pair is from UBC Fashion dataset [43]. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.1 Datasets and Preprocessing ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Our method and the other techniques are trained and evaluated on five distinct datasets as below: ", "page_idx": 13}, {"type": "text", "text": "Human-Art [15]: The Human-Art dataset comprises 38,000 images distributed across 19 scenarios, encompassing natural scenes, 2D artificial scenarios, and 3D artificial scenarios. We adopt the same train-validation split as the authors suggested. The annotations in Human-Art belong to the International Digital Economy Academy (IDEA) and are licensed under the Attribution-Non Commercial-Share Alike 4.0 International License (CC-BY-NC-SA 4.0). ", "page_idx": 13}, {"type": "text", "text": "LAION-Human [16]: The LAION-Human is derived from the LAION-5B dataset [36], consisting of approximately 1 million images filtered by human estimation confidence scores. We randomly selected a subset of 200,000 images for training and 20,000 images for validation. The dataset is licensed under the Creative Common CC-BY 4.0 license, which poses no particular restriction. The images are under their copyright. ", "page_idx": 13}, {"type": "text", "text": "UBC Fashion [43]: The UBC Fashion dataset comprises sequences showcasing fashion models executing turns. We extracted frames representing various orientations: front, side, and back, to rigorously test our model\u2019s aptitude for handling both complex and infrequently encountered poses. The dataset yields approximately 1100 front, 450 side, and 1100 back frames. The dataset is licensed under the Creative Commons Attribution-Non Commercial 4.0 International Public License. ", "page_idx": 13}, {"type": "text", "text": "Dance Track [40]: The Dance Track dataset presents group dance footage, typified by multiple subjects and intricate postures. We curated 20 videos from the Dance Track validation set and extracted a total of 2000 frames to assess our model. The annotations of DanceTrack are licensed under a Creative Commons Attribution 4.0 License and the dataset of DanceTrack is available for non-commercial research purposes only. ", "page_idx": 13}, {"type": "text", "text": "DAVIS [27]: The DAVIS dataset is a widely used dataset for video-related tasks. We randomly chose 26 human-centric scenarios from the DAVIS Test-Dev 2017 set and the DAVIS Test-Challenge 2017 set, which provided around 2000 frames for evaluation. The DAVIS dataset is released under the BSD License. ", "page_idx": 13}, {"type": "text", "text": "All datasets adhere to a standardized protocol featuring 17 keypoints and a maximum of 10 persons per image, following COCO and Human-Art. Despite the original checkpoints of ControlNet and ", "page_idx": 13}, {"type": "table", "img_path": "IwNTiNPxFt/tmp/6b891703f9fa5524aeb699c35c357e550209dbb98f33efe10ca84554e8e00bda.jpg", "table_caption": ["Table A.1: Trainable parameters (millions), training time on Human-Art (hours), and inference time (seconds per image) for each technique. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "T2I-Adapter being anchored in the OpenPose keypoint protocol, it is feasible to convert keypoints between different styles without loss of accuracy. In preprocessing the datasets for network input, we applied a score threshold of 0.05 to filter out keypoints and connected corresponding keypoints, following the procedure outlined by the authors of Human-Art. Each connecting line is depicted in distinct colors, as illustrated in Figure A.1, enabling the network to learn associations between each line and specific body parts. Please note that the colors of the pose skeleton presented in this paper are solely for visualization purposes and do not correspond to those used in the experiments. During training, we employed consistent data augmentations, including random cropping at 512 pixels, random rotation, random color shift, and random brightness contrast. These augmentations were applied uniformly across all techniques. ", "page_idx": 14}, {"type": "text", "text": "It is worth noting that the scale of the LAION-Human subset and its data distribution closely align with those reported in the SOTA works, such as ControlNet [44], which was trained on 200,000 images sourced from the Internet. Thus, ensuring a fair comparison is possible. Since none of the aforementioned video datasets are annotated with poses or textual descriptions, we employed the GPT-4 API for generating prompts and HigherHRNet [6] for deducing pose labels. ", "page_idx": 14}, {"type": "text", "text": "A.2 Training and Evaluation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To ensure a fair comparison, we trained and evaluated all the techniques on Human-Art dataset. On LAION-Human subset, due to computational constraints, we only trained our method, HumanSD, GLIGEN and Uni-ControlNet, while ControlNet and T2I-Adapter were evaluated using the released checkpoints. The rationale behind this was that HumanSD had been previously trained on a segment of LAION-Human, which might overlap with our validation set, leading to potential data leakage. GLIGEN had been trained with considerably more GPU hours compared to other techniques, posing a risk of an unfair comparison. Uni-ControlNet\u2019s available checkpoints are tailored for a multicondition model, however, our research is dedicated exclusively to pose control, necessitating further training. Conversely, for ControlNet and T2I-Adapter, the amount of data and GPU hours for training were on par with our methods, allowing for fair comparison using the checkpoints they provided. ", "page_idx": 14}, {"type": "text", "text": "Training was conducted on two NVIDIA A100 GPUs. Our method required 15 hours to complete training on the Human-Art dataset and 70 hours for the LAION-Human subset. During training, the peak RAM usage for our technique was 25 GB. We reported the number of trainable parameters and training time on Human-Art dataset for each technique in Table A.1. Our technique has comparable number of trainable parameters and training time to ControlNet but produces significantly better results, highlighting the effectiveness of our model design. We also reported the inference speed of each technique in Table A.1, measured by the average seconds needed to generate an image. To ensure a fair comparison, these measurements were conducted on the same NVIDIA A100 GPUs. Due to the self-attention mechanism in the ViT, our method\u2019s inference speed is slightly slower than that of ControlNet. ", "page_idx": 14}, {"type": "text", "text": "A.3 Selecting Gaussian Filters in PMSA ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A common way to choose the kernel size $k$ of a Gaussian Filter with sigma $\\sigma$ is $k=2\\times\\lceil3\\sigma\\rceil+1.$ . This ensures that the kernel captures the majority of the Gaussian function\u2019s weight. When using the kernels and sigmas as specified in the above equation, we observed that the difference between the generated coarse and fine masks was too large to yield satisfactory results. We measured this via the computation of the activation rate, reported in Table A.2. We applied the pose mask to the patches and then calculated the ratio of pose-masked patches to the total number of patches. We opted for an alternative method to control the blurring effect by fixing $\\sigma$ and adjusting $k$ , which changes the number of pixels to which the Gaussian weights are applied. Larger values for $k$ dilate the mask, resulting in coarser masks. ", "page_idx": 14}, {"type": "table", "img_path": "IwNTiNPxFt/tmp/ed7d7d7350934cdc4e62448b74e8aed666887d7f0d596747ac6b6c93ae772868.jpg", "table_caption": ["Table A.2: Ratios of unmasked patches $(\\%)$ under different sigma $\\sigma$ and kernel size $k$ in a coarse-tofine manner. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "IwNTiNPxFt/tmp/46efee1ca49c87ac7a1428b8090fbe8ede7aed831b9305d7e4dc2abe223a9cfa.jpg", "table_caption": ["Table A.3: Ratios of unmasked patches $(\\%)$ under fixed sigma $\\sigma=3$ and varying kernel size $k$ in a coarse-to-fine manner. Selected ones are in bold. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "In our experiment, we fix $\\sigma$ at 3 and then change $k$ for obtaining different kernels. The activation rate of patches under the fixed $\\sigma$ and varying $k$ is reported in Table A.3, where $k=23$ is the largest kernel size for $\\sigma=3$ , indicating that any larger $k$ does not affect the activation rate. This size was chosen to generate the most coarse mask. To ensure smooth guidance of our pose-mask in the proposed PMSA, we selected kernel sizes with approximately one percent difference in activation rate. As a result, $k=13$ was chosen to generate the fine mask. For our ablation study with 3 pose masks, we selected $k=9$ , which also yielded about a one percent difference in activation rate compared to $k=13$ . Details of this ablation study can be found in Sec. A.6. ", "page_idx": 15}, {"type": "text", "text": "A.4 Evaluation Metrics ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We define the evaluation metrics in detail here. FID assumes that the features extracted from real and generated images by the Inception v3 model [41] follow a Gaussian distribution. It measures the Fr\u00e9chet distance between these distributions. KID, however, relaxes the assumption of a Gaussian distribution and calculates the squared Maximum Mean Discrepancy (MMD) between the Inception features of real and generated images using a polynomial kernel. Mean Average Precision (AP) computes the alignment between keypoints in real images and generated images. Poses of generated images are detected by the same Considering that images may contain multiple persons, we included People Counting Error (PCE)[7] as a metric, measuring the false positive rate when generating images featuring multiple people. The CLIP score measures the similarity between embeddings of generated images and text prompts, both of which are encoded by CLIP. ", "page_idx": 15}, {"type": "text", "text": "Challenges in the Current Metrics. We followed the common metrics widely adopted in the current generative AI field, however, despite the rapid advancements in generative AI, existing metrics have not evolved to provide a more accurate evaluation [14]. Some of the issues are 1) CLIP score relies on cosine similarity between the model\u2019s semantic understanding and the given text, which may not align with pose assessments or the relevance of generated images. Additionally, this score is sensitive to the arrangement and composition of elements in images; even minor changes can result in significant fluctuations in the score, which may not accurately reflect the overall generative quality. [34] suggests a Dino-based score; 2) FID estimates the distance between a distribution of Inception-v3 features of real images and those of images generated by the generative models. However, Inception\u2019s poor representation of the rich and varied content generated by modern text-to-image models incorrect normality assumptions and poor sample complexity [14]. Thus, the FID score does not account for semantic correctness or content relevance\u2014specifically pose\u2014in relation to the specified text or conditions. Relying solely on FID and CLIP scores does not provide a comprehensive assessment of the generative model. Therefore, we further evaluated our method with a new state-of-the-art metric CMMD [14], which is based on richer CLIP embeddings and the maximum mean discrepancy distance with the Gaussian RBF kernel. It is an unbiased estimator that does not make any assumptions on the probability distribution of the embeddings, offering a more robust and reliable assessment of image quality. As shown in Table A.4, our method achieves better CMMD value compared to HumanSD, demonstrating comparably high image quality. ", "page_idx": 15}, {"type": "text", "text": "Table A.4: CMMD evaluation for HumanSD and Stable-Pose. ", "page_idx": 16}, {"type": "table", "img_path": "IwNTiNPxFt/tmp/e298fa8e55600eee08021444ef982c358f383ac2fc1c5352e49d841d368320c3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "IwNTiNPxFt/tmp/e5e357d4bf2924c3dfc616a5e3553352ba00d29dc164cdc4488a9d8212dff5f5.jpg", "table_caption": ["Table A.5: Detailed results on UBC Fashion dataset with front, side, and back orientations. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "IwNTiNPxFt/tmp/0cd73e9d602b9094e81ccfd36f76e2fefe8e918009ad37853e9b22cb72072d4a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.5 Detailed Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We reported detailed results for the UBC Fashion, DAVIS, and Dance Track datasets in Tables A.5&A.6, where KID is multiplied by 100 for readability. The results demonstrate that our Stable-Pose consistently provides better controllability over poses, even for challenging poses such as back poses. ", "page_idx": 16}, {"type": "text", "text": "A.6 Additional Ablation Studies ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "ViT Parameters. We studied the impact of different ViT parameters, including depth and patch sizes, as reported in Table A.7 and A.8. Table A.7 shows results for varying depths with a fixed patch size of 2, using coarse-to-fine pose masks as guidance. While these masks provide smooth guidance, a higher depth with overly fine masks might lose valuable information from previous attention layers. Table A.8 presents results for a ViT with PMSA of depth 2 and different patch sizes. Our default patch size of 2 yields the best results, likely because the latent encoding has typically lower dimension (here $64\\times64$ ) compared to the dimensions of input high-resolution images. Larger patches may dilute learning of interconnections among anatomical parts for encompassing too much information. ", "page_idx": 16}, {"type": "table", "img_path": "IwNTiNPxFt/tmp/bb0cf90d4a9c187a6ff9db7b4baeca7d555319ca678f255cf000996528dd7c11.jpg", "table_caption": ["Table A.7: Comparison of different depths in our proposed ViT. Kernel sizes of Gaussian Filters are noted. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "IwNTiNPxFt/tmp/27f030d37547dc43d2ae3c57af4e08d441700b7e74c1e86d26f29cf987335da7.jpg", "table_caption": ["Table A.8: Comparison of different patch sizes in our proposed ViT. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "A.7 Attention Maps of PMSA ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In Figure A.2, we present examples of attention maps extracted from our ViT. These maps are derived from the last block of the ViT and represent an average across all attention heads. The attention maps were approximately overlapped with the pose region, which corroborates the objectives of our proposed PMSA. This alignment underscores the efficacy of our model in focusing on relevant pose features, a critical aspect of our approach to improving model interpretability. ", "page_idx": 17}, {"type": "text", "text": "A.8 Failure Case ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Stable-Pose may face failure cases when generating the wrong number of people in very crowded scenes. Stable-Pose enhances SD\u2019s accuracy in pose-mask regions, whereas a pre-trained SD may still produce human-like figures in the background. For example, in the first row of Figure A.3, Stable-Pose generates an extra half-shaped person on the very right side. ", "page_idx": 17}, {"type": "text", "text": "A.9 Extreme cases ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The generation of some extreme pose cases is shown in Figure A.4, such as bending the upper body backward in some dancing poses. As shown in the figures, Stable-Pose still maintains very high pose accuracy on generated images under these challenging scenarios, whereas ControlNet fails to depict the correct pose and body structures. ", "page_idx": 17}, {"type": "text", "text": "A.10 More Visualization Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We include additional qualitative results from the five datasets we evaluated in Figure A.5,A.6,A.7,A.8, & A.9. ", "page_idx": 17}, {"type": "image", "img_path": "IwNTiNPxFt/tmp/d2db8318e98f18dc46adf1376a272799bf522947c182dfc39537453132a6632c.jpg", "img_caption": ["Figure A.2: Samples from LAION-Human dataset with original images and attention maps. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "IwNTiNPxFt/tmp/928cf76ddc7dac54ca3c2705003fb920c080627c02a4c76ace3b9dfb6e419be1.jpg", "img_caption": ["Ballet performers in white tutus gracefully dancing on stage with a winter background. ", "Figure A.3: Failure cases sampled from Dance Track dataset. Unexpected humans generated are marked in red circles. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Acrobatics, a man and a woman performing aerial acrobatics ", "text_level": 1, "page_idx": 19}, {"type": "image", "img_path": "IwNTiNPxFt/tmp/b114739cde465de51635a716a0d86a8452396403267f4e369930c72d0f34f3be.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Acrobatics, a man doing a handstand on a white background. ", "page_idx": 19}, {"type": "image", "img_path": "IwNTiNPxFt/tmp/3bef2e5c34ea75aa32157ca643436ccf001f4790db07dfd382eabc319f71a8b7.jpg", "img_caption": ["Figure A.4: Extreme poses sampled from the Human-Art dataset. ControlNet creates wrong limbs in extreme cases while Stable-Pose achieves accurate target poses. ", ""], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Garage kits, afigurine with blue hair and wings ", "text_level": 1, "page_idx": 19}, {"type": "image", "img_path": "IwNTiNPxFt/tmp/7351caf755d5eafddf30c04d5b23ea98b731f9c74c2495deb41093d129ff5ba2.jpg", "img_caption": ["Figure A.5: Results on Human-Art dataset. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Gucci Spring 2016 Ready-to-Wear Fashion Show - Eline Klein Onstenk. ", "page_idx": 20}, {"type": "image", "img_path": "IwNTiNPxFt/tmp/67a9a5991d66cbbce0ee5525e95ec73ebc2f0ab17ee36f45f0f2d5efd3c527f0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Flower girl with a ballerina dress pink and ivory colour by angel's face ", "page_idx": 20}, {"type": "image", "img_path": "IwNTiNPxFt/tmp/0f63b3ddbb8671b73f930ea72a01e148157a272b96398e8c73781947955b105d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "The drawing of little girl with a daisy painting by Anna Abramska ", "page_idx": 20}, {"type": "image", "img_path": "IwNTiNPxFt/tmp/6c1b41641da8ee7c7fa65fff8ef2db6e22abc2f8239d215414a0ae2d71d122b2.jpg", "img_caption": ["Figure A.6: Results on LAION-Human dataset. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "A model wearing a floral knee-length dress ", "text_level": 1, "page_idx": 20}, {"type": "image", "img_path": "IwNTiNPxFt/tmp/eb777e584134db1cdb58fba3346c35a4404c5fd8274034531de929ff78654779.jpg", "img_caption": ["Figure A.7: Results on UBC Fashion dataset with side orientation. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "A model showcasing a floral dress against a plain background. ", "page_idx": 21}, {"type": "image", "img_path": "IwNTiNPxFt/tmp/ece876f86edf4f579ea71a27ae48053248bbdb2c865bfed5faf346780a5cb1d1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "IwNTiNPxFt/tmp/a38b9964af9e9fa7efba7d9877e39a41ed1d3fb8c9079da701f9173e0fe5e9d9.jpg", "img_caption": ["Figure A.8: Results on UBC Fashion dataset with back orientation. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Dance crew performing synchronized routine with artistic masks ", "page_idx": 21}, {"type": "image", "img_path": "IwNTiNPxFt/tmp/0fedf722756c097727633971f11f6748b51884866d8d4a504677dd270224d5eb.jpg", "img_caption": ["Figure A.9: Results on Dance Track (first row) and DAVIS dataset (the rest rows). "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our claims in the abstract and introduction accurately match the theoretical and experimental results of our methodology. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We discussed the limitations of our method in Sec. 5: Discussion and Conclusion. We also discussed the computational efficiency in Sec. A.2 and Table A.1. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our method does not need theoretical results. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We included detailed information on the structure of our model as well as the introduced algorithms and training objective in Sec. 3. We also included implementation details in Sec. 4 and Sec. A.2 to fully promote the reproducibility. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We commit to release our source code on GitHub upon paper acceptance. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provided all the training and test details in the Implementation Details in Sec. 4 and Sec. A.2. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [No] ", "page_idx": 24}, {"type": "text", "text": "Justification: The error bars for our experimental results are not applicable. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We provided detailed information on the computer resources in Sec. A.2. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics and confirm that we conform with the NeurIPS Code of Ethics. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our paper have discussed both potential positive societal impacts and negative societal impacts of the work performed in Sec. 5. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 25}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We commit to provide effective requirement for users to adhere to usage guidelines once we release our source code on GitHub upon acceptance. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have properly cited the original paper of the code package and dataset used in our work. We also included the information of license and terms of use of the datasets in Sec. A.1. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We commit to release the source code of our work on GitHub with wellstructured documentation upon acceptance. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}]