[{"figure_path": "R8znYRjxj3/figures/figures_4_1.jpg", "caption": "Figure 1: Left: The asymptotic MMSE of eq. (7) for the noiseless (\u2206 = 0) case, as a function of the sample complexity \u03b1, for various width ratios \u03ba. Right: Phase diagram representing the MMSE, brighter color indicates a higher value. The red curve is the perfect recovery transition line APR, see eq. (1), and its origin is discussed in Section 5.", "description": "The figure shows two plots related to the minimum mean-squared error (MMSE) in learning a single hidden layer neural network with quadratic activation function. The left plot shows the MMSE as a function of sample complexity (\u03b1 = n/d\u00b2) for different width ratios (\u03ba = m/d). The right plot is a phase diagram showing the MMSE as a function of both sample complexity and width ratio. The red curve in the right plot represents the perfect recovery transition line (APR), which indicates the minimum sample complexity needed to achieve zero test error in the absence of noise.", "section": "Main results"}, {"figure_path": "R8znYRjxj3/figures/figures_8_1.jpg", "caption": "Figure 2: Mean squared error (MSE) as a function of the sample complexity a for \u03ba=1/2. Dots are simulations using GD with a single initialization averaged over 32 realizations of the dataset, crosses are averages over 64 initializations with 2 realizations of the dataset. The continuous lines are the asymptotic MMSE given by (7). Left: noiseless \u2206 = 0 case. The colors indicate the size d. We can see how AGD appears to be well described by the theoretical MMSE. We used the learning rates 0.2 for d=200 and 0.07 for d=100. Right: Comparison of GD between the noisy \u221aA=0.25 case (red) and noiseless A=0 case (blue). Adding noise makes AGD worse than the MMSE, and for sample complexity a\u2265 0.3, all the initializations of GD converge to the same point, making the GD and AGD curves collapse.", "description": "The figure compares the mean squared error (MSE) achieved by gradient descent (GD) and averaged gradient descent (AGD) with the theoretical minimum mean squared error (MMSE) for different sample complexities and dimensions.  The left panel shows the noiseless case, demonstrating that AGD closely matches the theoretical MMSE. The right panel introduces noise, showing that AGD performs worse than the MMSE and that GD converges to a single point for larger sample complexities regardless of initialization.", "section": "3 Main results"}, {"figure_path": "R8znYRjxj3/figures/figures_19_1.jpg", "caption": "Figure 3: Comparison of the performance of GAMP-RIE with the asymptotic MMSE (7) both in the noiseless (\u2206 = 0) and in a noisy (\u221a\u2206 = 0.25) case, with \u03ba = 0.5. Each dot is the average over 8 runs of GAMP-RIE at a moderate size of either d = 100 (circle dots) or d = 200 (crosses). The error bars are the standard deviations of the MSE.", "description": "This figure compares the performance of the GAMP-RIE algorithm with the theoretical asymptotic minimum mean-squared error (MMSE) for learning a one-hidden layer neural network with quadratic activation.  The comparison is shown for both noiseless and noisy scenarios (noise level \u221a\u2206 = 0.25), with a width-to-dimension ratio \u03ba = 0.5. The results show a good agreement between the algorithm's performance and the theoretical MMSE, even for moderately sized networks (d = 100 and d = 200). Error bars represent the standard deviation of the MSE.", "section": "B The GAMP-RIE algorithm"}, {"figure_path": "R8znYRjxj3/figures/figures_24_1.jpg", "caption": "Figure 4: Behavior of the asymptotic MMSE in the noiseless (\u2206 = 0) case as \u03ba gets increasingly small. The continuous lines are given by eq. (7), which we compare with the asymptotic \u03ba \u2192 0 curve obtained by eq. (51). We emphasize that the horizontal axis is \u03b1/\u03ba, which remains of order (1) as \u03ba \u2192 0: it corresponds to a number of samples n of the same order as the number of parameters dm.", "description": "This figure shows the asymptotic minimum mean squared error (MMSE) for different width ratios (\u03ba) in the noiseless case (no noise in target function). The x-axis represents the sample complexity (\u03b1/\u03ba) which remains of order 1 as \u03ba goes to 0. The curves show that MMSE decreases smoothly as sample complexity \u03b1/\u03ba increases and reaches 0 at perfect recovery. The plot also includes the asymptotic MMSE as \u03ba approaches 0, obtained via an analytical expression.", "section": "E Large and small \u03ba limits"}, {"figure_path": "R8znYRjxj3/figures/figures_39_1.jpg", "caption": "Figure 5: Left: Mean squared error as a function of the sample complexity a, for \u03ba = 1/2 and \u0394 = 0.252. Dots are simulations using GD with a single initialization averaged over 32 realizations of the dataset, crosses are averages over 64 initializations. The continuous line is the asymptotic MMSE given by (7). The colors indicate the strength of the regularization. Right: Trivialization threshold in the sample complexity \u03b1\u03c4 as a function of the noise level \u0394 in the teacher without regularization, \u03bb = 0. The measurement has a resolution of 0.1 on the noise level and of 0.007 on the sample complexity", "description": "The left panel shows the mean squared error (MSE) as a function of sample complexity (\u03b1) for different regularization strengths (\u03bb). The black line represents the theoretical minimum mean squared error (MMSE). The right panel shows the trivialization threshold (\u03b1\u03c4) as a function of noise level (\u0394) in the teacher network, where trivialization means that gradient descent converges to the same solution regardless of the initialization.", "section": "H Details on the numerics"}]