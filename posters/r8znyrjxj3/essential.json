{"importance": "This paper is crucial for researchers working on **high-dimensional learning**, **neural network theory**, and **Bayesian inference**.  It provides **new theoretical tools** and **algorithmic approaches** to understand the fundamental limitations and optimal performance in learning complex neural networks, impacting various fields that leverage these techniques.", "summary": "This study solves a key challenge in neural network learning, deriving a closed-form expression for the Bayes-optimal test error of extensive-width networks with quadratic activation functions from quadratically many samples.", "takeaways": ["A closed-form expression for Bayes-optimal test error in learning a single-hidden-layer neural network with quadratic activation is derived.", "A novel algorithm, GAMP-RIE, that asymptotically achieves the Bayes-optimal performance is proposed.", "The study establishes a link between optimal denoising of extensive-rank matrices and learning neural networks, impacting multiple fields."], "tldr": "Learning complex functions using neural networks is a central problem in machine learning.  While significant progress has been made, understanding the optimal sample complexity (the minimum data needed to accurately learn the function) remains a challenge, especially for deep and wide networks.  Existing theoretical tools often struggle with the complexity of large neural networks, limiting our ability to provide precise predictions of learning performance.  Previous work has demonstrated Bayes-optimal error, but only for the regime of linearly many samples in relation to the input dimension.  The challenge of extending this to the more practical and interesting quadratically-many-samples regime remained open.\nThis paper addresses this open challenge for **single hidden layer neural networks** with quadratic activation functions, deriving a **closed-form expression for the Bayes-optimal test error**. It also introduces a novel algorithm, **GAMP-RIE**, combining approximate message passing (AMP) with rotationally invariant matrix denoising (RIE) and shows that it asymptotically achieves this optimal error. The results are enabled by a connection to recent advances in extensive-rank matrix denoising, linking seemingly disparate fields and opening up new avenues of research.", "affiliation": "ETH Zurich", "categories": {"main_category": "AI Theory", "sub_category": "Optimization"}, "podcast_path": "R8znYRjxj3/podcast.wav"}