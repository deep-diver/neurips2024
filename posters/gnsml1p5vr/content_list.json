[{"type": "text", "text": "Visual SKETCHPAD: Sketching as a Visual Chain of Thought for Multimodal Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yushi Hu\u22171,2 Weijia Shi\u22171 Xingyu $\\mathbf{Fu}^{3}$ Dan Roth3 Mari Ostendorf1 Luke Zettlemoyer1 Noah A. Smith1,2 Ranjay Krishna1,2 ", "page_idx": 0}, {"type": "text", "text": "1University of Washington 2Allen Institute for AI 3 University of Pennsylvania ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Humans draw to facilitate reasoning: we draw auxiliary lines when solving geometry problems; we mark and circle when reasoning on maps; we use sketches to amplify our ideas and relieve our limited-capacity working memory. However, such actions are missing in current multimodal language models (LMs). Current chain-of-thought and tool-use paradigms only use text as intermediate reasoning steps. In this work, we introduce SKETCHPAD, a framework that gives multimodal LMs a visual sketchpad and tools to draw on the sketchpad. The LM conducts planning and reasoning according to the visual artifacts it has drawn. Different from prior work, which uses text-to-image models to enable LMs to draw, SKETCHPAD enables LMs to draw with lines, boxes, marks, etc., which is closer to human sketching and better facilitates reasoning. SKETCHPAD can also use specialist vision models during the sketching process (e.g., draw bounding boxes with object detection models, draw masks with segmentation models), to further enhance visual perception and reasoning. We experiment on a wide range of math tasks (including geometry, functions, graph, chess) and complex visual reasoning tasks. SKETCHPAD substantially improves performance on all tasks over strong base models with no sketching, yielding an average gain of $12.7\\%$ on math tasks, and $8.6\\%$ on vision tasks. GPT-4o with SKETCHPAD sets a new state of the art on all tasks, including $V^{*}$ Bench $(80.3\\%)$ , BLINK spatial reasoning $(83.9\\%)$ , and visual correspondence $(80.8\\%)$ .All codes and data are in https://visualsketchpad.github.io/. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sketching is a fundamental human activity, serving as a versatile tool for communication [11], ideation [48], and problem-solving [47]. Unlike written language, sketches have the advantage of conveying visuo-spatial ideas directly, for example by using spatial relations on paper to convey spatial relations or other more abstract relationships in the world. This may explain their ubiquity; maps [40] and architectural plans [12] have been found incised in stone, etched on leather, impressed in clay, and drawn on paper in diverse cultures scattered across the world [39]. Sketches are so fundamental that we use them to teach school children how to solve geometry problems by drawing support lines, to aid engineers conveying prototypes, to support architects creating blueprints, and to allow scientists like us to convey scientific contributions (see Figure 1). ", "page_idx": 0}, {"type": "text", "text": "As multimodal language models (LMs) [35, 42, 28, 27, 1, 3, 49, 7, 31, 41, 6, 5] have begun to mature, we now expect them to solve tasks like the ones mentioned above, i.e., ones where people draw intermediate sketches to simplify reasoning. Popular benchmarks now include questions about geometry (e.g., Geometry3K [32]) and complex math problems (e.g., IsoBench [8]). In these benchmarks, models are given images of diagrams and asked questions requiring symbolic grounding and spatial understanding, where intermediate sketches like auxiliary lines can enhance reasoning. Even benchmarks in computer vision now have a similar flavor. Specialist vision models can be viewed as sketching on natural images. For example, object detection is plotting bounding boxes around objects; depth estimation is drawing colormaps according to depth. The recently proposed BLINK benchmark [9] would benefti significantly from such intermediate visual sketches. Similarly, the $V^{*}$ Bench benchmark [51] could focus reasoning on image crops to find answers. Unfortunately, current LMs lack a scaffold for using sketch-based reasoning when solving tasks. ", "page_idx": 0}, {"type": "image", "img_path": "GNSMl1P5VR/tmp/29d3865855b8b850ef8fc8cd984381181e5166d59c86169d54ff6565608b7de2.jpg", "img_caption": ["Figure 1: SKETCHPAD equips GPT-4 with the ability to generate intermediate sketches to reason over tasks. Given a visual input and query, such as proving the angles of a triangle equal $180^{\\circ}$ , SKETCHPAD enables the model to draw auxiliary lines which help solve the geometry problem. The examples are from [8, 51, 44]. For all these examples, without SKETCHPAD, GPT-4o fails to get the correct answer, while SKETCHPAD $^+$ GPT-4o achieves the correct solution. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we introduce Visual SKETCHPAD: a framework that provides multimodal LMs with the tools necessary to generate intermediate sketches to reason over tasks. Inspired by textual chain-of-thought reasoning in LMs [50, 62], SKETCHPAD prompts the underlying visual LM to produce visual artifacts as part of a chain of mixed textual, programmatic, and visual reasoning. For example, to prove that the angles of triangles sum up to 180 degrees in Figure 1 (a), SKETCHPAD enables agents to modify the diagram by introducing a new auxiliary line. This new line, along with new annotated angles, provides the critical information to solve the geometry task. Similarly, SKETCHPAD improves models\u2019 spatial reasoning for computer vision. To determine if there are cookies stacked on top of other cookies in the image (Figure 1b), the model first produces an intermediate depth estimate. By analyzing the depth estimate, which reveals cookies overlapping at different depths, the model is able to correctly answer that the cookies are indeed stacked. ", "page_idx": 1}, {"type": "text", "text": "We demonstrate the effectiveness of visual SKETCHPAD across a wide range of mathematics and computer vision tasks. For math, we tackle problems including (1) geometry [32], (2) mathematical functions, (3) graph algorithms, and (4) strategy games [8]. For geometry questions, SKETCHPAD enables models to generate Matplotlib code with auxiliary lines and variables, given the diagram input and questions (Figure 1a). Notably, even when the input is solely language-based, such as mathematical functions, SKETCHPAD enables models to plot the functions and reason about their properties, using only the mathematical function expression as input (Figure 1b). These results highlight the ability of SKETCHPAD to aid reasoning, even in tasks with purely language-based inputs. Across all four categories of mathematical tasks, SKETCHPAD consistently improves the baseline GPT-4o performance, yielding an average gain of $11.2\\%$ . For computer vision, we tackle diverse tasks including (1) depth, (2) spatial reasoning, (3) jigsaw, (4) visual correspondence, (5) semantic correspondence, as well as questions from (6) the MMVP and (7) the $V^{*}$ Bench benchmarks [9, 44, 51]. For this domain, SKETCHPAD enables models to generate segmentation masks, crop images, draw bounding boxes, zoom into image regions, overlay images, etc. Similar to math, SKETCHPAD shows consistent improvements across all seven types of computer vision tasks. For example, GPT-4o, augmented with SKETCHPAD, sees $14.3\\%$ improvement on $V^{*}$ Bench, $12.1\\%$ , and $9.7\\%$ improvements on BLINK\u2019s depth and semantic correspondence tasks, setting a new state of the arts across all tasks. Finally, we analyze the effectiveness of SKETCHPAD by comparing the plans generated by our model with human-created plans, showing that they are well-aligned and exhibit similar reasoning patterns. We hope SKETCHPAD opens up new research opportunities toward more capable and interpretable multimodal intelligence. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "SKETCHPAD generalizes recent work on multimodal tool-use and visual prompting. We also place our work within the larger sphere exploring LMs as agents. ", "page_idx": 2}, {"type": "text", "text": "Visual programming and tool-use. With the advancement of LMs [4, 35, 42, 45, 13], researchers have demonstrated the possiblity of decomposing complex vision tasks into simpler substeps that can each be solved using vision tools [57, 60, 18, 17]. Among them, the most relevant to us are Visprog [14] and ViperGPT [38]. They use LMs to generate Python code, which sequencially invokes specialized vision tools. These methods share a common problem that the multimodal modules follow a pre-defined plan outlined by the LM. By contrast, SKETCHPAD allows LMs to change their plan according to the intermediate visual artifacts, yielding better performance and robustness when solving complex multimodal tasks. ", "page_idx": 2}, {"type": "text", "text": "Visual prompting. Recent work shows that multimodal models can be augmented by visual prompts added to natural images [37]. For example, SoM [55] shows that adding labeled segmentation masks on images unleashes GPT-4V\u2019s visual grounding ability. Prior work also reports similar findings in 3D [25] and Robotics [34]. SKETCHPAD is a generalized framework for all these methods, allowing LMs to decide what visual prompting to use as part of the multimodal reasoning process. ", "page_idx": 2}, {"type": "text", "text": "LMs as agents. Recent work has started to treat LMs as agents that can both reason and act [58, 33, 52, 36, 59]. Researchers have applied this idea to software engineering [20, 61, 15], robotics [34], vision [29, 57], and GUI navigation [54, 23, 53]. SKETCHPAD can also be viewed as an agent that accepts multimodal inputs and outputs. One big difference is that SKETCHPAD can create visual artifacts to facilitate reasoning, while prior LM agents only generate text during reasoning. ", "page_idx": 2}, {"type": "text", "text": "3 Visual SKETCHPAD ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We introduce visual SKETCHPAD, a general framework that enables multimodal LMs to draw sketches as intermediate reasoning steps and to use these sketches to facilitate further reasoning. Figure 2 shows examples of how SKETCHPAD works. Given a multimodal query, SKETCHPAD agent generates a sketching plan to address the query (Thought), and then synthesizes a program to create visual sketches (Action). By analyzing the resulting sketches (Observation), which serve as a visual representation of the reasoning process, the model generates a final response to the query. ", "page_idx": 2}, {"type": "text", "text": "Our framework requires no finetuning or training. Multimodal LMs, out of the box, can be prompted to sketch using our framework. Our implementation is based on the AutoGen [52] framework. We give the overview of our SKETCHPAD framework in $\\S3.1$ , and delve deep into how it integrates sketching into the reasoning process in $\\S3.2$ . ", "page_idx": 2}, {"type": "text", "text": "3.1 Overview of SKETCHPAD ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The SKETCHPAD agent solves tasks by engaging in an iterative interaction process with an environment. Given a multimodal query $q$ that includes both visual and textual components, the model generates a series of thoughts, actions, and observations to gather the information needed to answer the query. At each time step $t$ , the model performs three key steps: ", "page_idx": 2}, {"type": "image", "img_path": "GNSMl1P5VR/tmp/0a56b5483d9b6a53783e6ada235a248b116f39abd4343bf09441fe65f7941d0d.jpg", "img_caption": ["Figure 2: Overview of SKETCHPAD. Given a multimodal query, the SKETCHPAD agent generates a sketching plan to address the query (Thought), and then synthesizes a program to create visual sketches (Action). By analyzing the resulting sketches (Observation), which serve as a visual representation of the reasoning process, the model generates a final response to the query. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Thought: The model analyzes the current context $c_{t}$ , which includes the query, previous thoughts, actions, and observations, to generate a thought plan $p_{t}$ for the next action. For example, given the query q \u2013 \u201cfind the $\\angle E I C^{\\circ}$ in Figure 2a, the model\u2019s thought plan $p_{1}$ is to draw an auxiliary line $I X$ parallel to $B D$ serving as a visual sketch to help solve the problem. ", "page_idx": 3}, {"type": "text", "text": "Action: Based on the thought plan, the model executes an action $a_{t}$ , which can manipulate both visual and textual content. In the geometry example, to realize the proposed thought of drawing the auxiliary line, the model generates Python code to modify the original geometry diagram. The generated code is then compiled and executed. ", "page_idx": 3}, {"type": "text", "text": "Observation: Based on the action $a_{t}$ , SKETCHPAD\u2019s environment returns a new observation $o_{t+1}$ , such as a new diagram with the auxiliary line drawn in the geometry example. The multimodal context is then updated to $c_{t+1}=(c_{t},p_{t},a_{t},o_{t+1})$ . ", "page_idx": 3}, {"type": "text", "text": "The multi-turn interaction process continues until time step $T$ , when the model determines that it has gathered enough information from the context $c_{T}$ to answer the query. At this point, it generates a special Terminate action and provides the answer. ", "page_idx": 3}, {"type": "text", "text": "Different from prior work [58], where LMs primarily generate and manipulate text-based observations and actions, SKETCHPAD enables the model to work with multimodal observations $o_{t}$ and actions $a_{t}$ , manipulating both visual and textual content. This allows the model to plan and reason with the visual sketches it has drawn, enhancing its problem-solving capabilities. ", "page_idx": 3}, {"type": "text", "text": "3.2 Sketching via Code Generation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The core component of SKETCHPAD is sketching, which enables the LM to generate visual sketches by synthesizing programs that call different specialist vision models or Python plotting packages. ", "page_idx": 3}, {"type": "text", "text": "Program Generation. Similar to recent works like ViperGPT and VPD [14, 38, 18], SKETCHPAD enables LMs to sketch through code generation. The LM is provided, through a prompt, with a detailed description of the available tools that can generate multimodal content (an example prompt and description can be found in $\\mathbf{\\xi}\\S$ ). The prompt includes Python function signatures and docstrings [16] for these modules, but does not contain their full implementation. The LM generates Python code in a code block, using the provided tools, which, when executed, generates new image and text outputs. A special display function allows the LM to visualize the sketch image in the next observation ot+1. ", "page_idx": 3}, {"type": "text", "text": "Modules for sketching. SKETCHPAD uses various tools to facilitate the sketching process, depending on the task at hand. For mathematical tasks, SKETCHPAD uses common Python packages like matplotlib and networkx for plotting (see $\\S4,$ . For vision tasks, the LM leverages specialist vision models during the sketching process. These models include detection tools that draw bounding boxes on the image, as well as segmentation and marking tools (inspired by SoM [55]) that draw colorful masks on the image and use numbers to label segments. We find these specialists possess useful perception skills for visual reasoning tasks, and SKETCHPAD is an effective way to combine them into a multimodal LM (see $\\S5.1\\$ ). ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "4 Sketching to Solve Math Problems ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we experiment with SKETCHPAD on four complex mathematical tasks $:$ (1) geometry, (2) mathematical functions, (3) graph algorithms, and (4) game strategies. We demonstrate that incorporating sketching capabilities into LMs significantly improves their performance on these mathematical problems, setting new state-of-the-art results (\u00a74.1). ", "page_idx": 4}, {"type": "text", "text": "Details of our evaluation tasks and the tools employed for visual reasoning are as follows: ", "page_idx": 4}, {"type": "text", "text": "Geometry Problems. Drawing auxiliary lines in geometry diagrams is often helpful for problemsolving. For example, in Figure 2 (a), when asked to find $\\angle E I C$ , the LM plans to draw an auxiliary line $I X$ parallel to $B D$ , allowing it to use the properties of parallel lines to determine $\\angle E I C$ . To evaluate the effectiveness of SKETCHPAD, we use the problems from the Geometry3K dataset [32]. ", "page_idx": 4}, {"type": "text", "text": "To realize the line drawing process, SKETCHPAD takes a geometry diagram and its corresponding matplotlib code as input. The model then proposes and modifies the code to generate auxiliary lines, and executes the modified code to visualize the updated diagram with the added lines. ", "page_idx": 4}, {"type": "text", "text": "Mathematical functions. Understanding mathematical functions is crucial for various applications in science, engineering, and economics. We focus on two tasks related to mathematical functions from the IsoBench datasets [8]: ", "page_idx": 4}, {"type": "text", "text": "\u2022 Classifying parity aims to determine whether a function is even, odd, or neither. Even functions satisfy $\\!\\,{\\dot{f}}(-x)=f(x)$ for all $x$ , while odd functions satisfy $f(-x)=-f(x)$ . \u2022 Identifying convexity/concavity aims to determine whether a function is convex or concave. ", "page_idx": 4}, {"type": "text", "text": "Existing LMs can only analyze functions and attempt to prove their properties analytically. 1 However, SKETCHPAD enables them to visually sketch functions to solve problems more efficiently. For instance, to determine the convexity of the function in Figure 1b, SKETCHPAD allows the model to plot the function using matplotlib, and visually inspect its overall shape. ", "page_idx": 4}, {"type": "text", "text": "Graph algorithms. Many real-world problems, such as those related to computer networks and transportation systems, can be formulated as graph problems. We evaluate SKETCHPAD on three graph problems from IsoBench [8]: ", "page_idx": 4}, {"type": "text", "text": "\u2022 Graph connectivity determines whether there exists a path between two vertices in a graph. ", "page_idx": 4}, {"type": "text", "text": "\u2022 Maximum flow aims to find the maximum amount of flow that can be sent through a network from a source vertex to a sink vertex, subject to capacity constraints on the edges. ", "page_idx": 4}, {"type": "text", "text": "\u2022 Graph isomorphism tests whether two graphs are structurally equivalent. ", "page_idx": 4}, {"type": "text", "text": "Given an adjacency matrix of a graph like in Figure 2(b), SKETCHPAD can draw the actual graph structure, using using Python\u2019s networkx library, enabling direct visual reasoning about graph properties and relationships. ", "page_idx": 4}, {"type": "text", "text": "Game strategies. Chess games can be represented in various formats, including visual board states and textual move notations. Given only the textual move notations, SKETCHPAD can draw the visual representations of the chess board to analyze positions and formulate strategies. We evaluate the performance of SKETCHPAD on the winnder identification task from the IsoBench datasets [8] that aims to find the outcome of a chess game (win for White, win for Black, or draw) based on the final board state. To create the graphical board, SKETCHPAD uses Python\u2019s chess library to draw the board from the Forsyth-Edwards Notation (FEN) of chess. ", "page_idx": 4}, {"type": "table", "img_path": "GNSMl1P5VR/tmp/29549e5db3fa9ec6a186d92a4eba6445936357705c566d528f769c02de98bc07.jpg", "table_caption": [], "table_footnote": ["Table 1: Accuracy scores on geometry problems, graph algorithms, mathematical functions, and game. SKETCHPAD yields large performance gains on most tasks and outperform all baselines. "], "page_idx": 5}, {"type": "text", "text": "4.1 Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We evaluate the performance of SKETCHPAD on multimodal LMs with API access, including gpt-4-turbo-2024-04-29 and gpt-4o-2024-05-13. We compare these results to baselines without the Visual Sketchpad and other notable closed-source models, such as Claude 3 and GeminiPro, as well as open-source models like Mistral [19] and LLaMA-2 70B [46]. ", "page_idx": 5}, {"type": "text", "text": "Main results. As shown in Table 1, SKETCHPAD consistently improves base model performance across all tasks, with an average improvement of $11.2\\%$ for GPT-4o and $23.4\\%$ for GPT-4 Turbo. In particular, we observe large gains on graph algorithms such as maximum flow and connectivity. For instance, GPT-4o with SKETCHPAD achieves an accuracy of $66.3\\%$ on the maximum flow problem, improving over the base model by $41.3\\%$ . Similarly, SKETCHPAD substantially improves the performance on mathematical functions, with GPT-4 Turbo achieving over $90\\%$ accuracy and GPT-4o over $88\\%$ accuracy on convexity and parity classification tasks. Furthermore, we observe gains $3\\%\\sim10\\%)$ on game strategies, demonstrating that drawn game boards drawn can improve reasoning about game strategies. Overall, these results highlight the effectiveness of SKETCHPAD in enhancing the reasoning capabilities of multimodal language models across diverse domains. ", "page_idx": 5}, {"type": "text", "text": "5 Sketching to Solve Computer Vision Tasks ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we experiment with SKETCHPAD on complex visual reasoning tasks. Recent work (BLINK) [9] finds that many core visual perception abilities are still missing from existing multimodal LMs\u2014even though many computer vision specialist models possess such abilities. Also, SoM [55] shows that drawing segmentation masks on images unleashes the strong visual grounding ability of GPT-4V. We generalize these ideas with SKETCHPAD, allowing LMs to use specialist vision models to sketch. Details of these modules are in $\\S5.1$ . SKETCHPAD enhances multimodal LMs\u2019 visual reasoning abilities and establishes new SOTAs on all 7 tasks (\u00a75.2). ", "page_idx": 5}, {"type": "text", "text": "Tasks. We experiment with a wide range of complex visual reasoning tasks: (1) $V^{*}$ Bench [51]. This benchmark contains questions about small items in an image. (2) MMVP benchmark from Eyes Wide Shut [44]. This benchmark contains visual questions specially designed to reveal the visual shortcomings of CLIP-based multimodal LMs. (3) BLINK [9]. This benchmark contains visual perception tasks that are easy for humans, but post significant challenge for multimodal LMs. Specifically, we experiment with relative depth, spatial reasoning, jigsaw puzzle, visual correspondence, and semantic correspondence tasks. More details of each task are in $\\S D$ . ", "page_idx": 5}, {"type": "text", "text": "5.1 Vision Specialists as Sketching Tools in SKETCHPAD ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "LMs can use the following modules to sketch and manipulate images. We wrap these modules into Python functions that the LMs can call. Refer to $\\S C$ for the function definitions. ", "page_idx": 5}, {"type": "text", "text": "Detection. This module takes an image and a simple text query (e.g., \u201ccat\u201d) as input. We run the Grounding-DINO [30] open-vocabulary objection detection model and plot the detected bounding boxes (together with a number label) on the image. It also returns the bounding box coordinates. ", "page_idx": 5}, {"type": "image", "img_path": "GNSMl1P5VR/tmp/5df433adfa6346adebb6c80137fdb5c20ef2771a678b85e07de3b5ab82293435.jpg", "img_caption": ["Figure 3: Examples of SKETCHPAD applied to vision tasks. The figure shows actual outputs generated by SKETCHPAD. By contrast, the baseline GPT-4o model cannot answer these questions correctly. Note that for demonstration purposes, the \u201cA\u201d and \u201cB\u201d marks in (a) are different from the actual images in the experiments. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Segmentation. This module takes an image as input and returns an image with colorful segmentation masks on it. Each mask also has a number label. We follow the implementation of SoM [55]. The underlying segmentation models are SegmentAnything [22] and Semantic-SAM [24]. ", "page_idx": 6}, {"type": "text", "text": "Depth estimation. This module takes an image as input and returns a depth map. The underlying model is DepthAnything [56]. ", "page_idx": 6}, {"type": "text", "text": "Visual search via sliding window. This module mimics how humans search for small items on an image. It takes a text query as input and runs a sliding window over the image. The window size is 1/3 of the image size, and the step size is 2/9 of the image size (so an image will have $4\\times4=16$ windows). It returns a sequence of image patches in which the query is detected. ", "page_idx": 6}, {"type": "text", "text": "Other image manipulation modules. Other modules include (1) zoom-in and crop, which takes an image and a bounding box as input and returns the image patch inside the box; (2) Overlay images, which takes two images and alpha values as input, and returns the overlayed image. ", "page_idx": 6}, {"type": "text", "text": "5.2 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We experiment with the same multimodal LMs as in $\\S4$ on complex visual reasoning tasks. We compare the performance with and without SKETCHPAD, as well as other notable multimodal LMs, including Gemini [42], Claude 3 [2], and the open-source LLaVA 1.5 [26], LLaVA-NeXT [27]. ", "page_idx": 6}, {"type": "text", "text": "Main results. Table 2 shows the performance of our SKETCHPAD and baselines. SKETCHPAD consistently improves base model performance across all tasks. GPT-4o with SKETCHPAD sets the new state-of-the-art results on all tasks. SKETCHPAD is particularly effective on $V^{*}$ Bench, yielding $18.5\\%$ accuracy improvement for GPT-4 Turbo and $14.3\\%$ improvement for GPT-4o, surpassing the previous state of the art SEAL [51] which used a visual search model specifically trained for this task. On BLINK tasks, SKETCHPAD on average yields $6.6\\%$ absolute accuracy gain for GPT-4 Turbo and $9.0\\%$ gain for GPT-4o. Interestingly, despite the fact that all modules in SKETCHPAD work on a single image, the LMs also get substantial improvement on multi-image tasks, including jigsaw puzzles, visual correspondence, and semantic correspondence. Finally, GPT-4o, the LM with stronger multimodal ability than GPT-4 Turbo, benefits more from SKETCHPAD. For example, on the relative depth task, GPT-4o gets $12.1\\%$ accuracy improvement, while GPT-4 Turbo only gets $2.4\\%$ , showing that GPT-4o is better at understanding the depth map SKETCHPAD generated. Overall, our experiments show that SKETCHPAD is an effective way to improve multimodal LMs\u2019 performance on visual reasoning tasks. ", "page_idx": 6}, {"type": "text", "text": "How many times is each vision specialist used? We count the number of times each vision specialist is used in each task, as shown in Figure 4. Here we choose the four tasks that achieve the largest improvement: $V^{*}$ Bench, relative depth, spatial reasoning, and semantic correspondence. ", "page_idx": 6}, {"type": "table", "img_path": "GNSMl1P5VR/tmp/5f0e2c4ff1ee7fa1842fd9a16bf0b4f02e6fd9e2b7ace173bc6066ed730d6f41.jpg", "table_caption": [], "table_footnote": ["Table 2: Accuracy on complex visual reasoning tasks. SKETCHPAD enhances both GPT-4 Turbo and GPT-4o performance, establishing new SOTA performance levels on all the tasks. "], "page_idx": 7}, {"type": "text", "text": "We observe that (1) the use of vision specialist is task-dependent, and the two LMs analyzed utilize similar tools. For example, for $V^{*}$ , which needs to locate small objects, the LMs mainly use detection, sliding window search, and zoom-in, similar to how people would search. For the relative depth task, both models rely on depth estimation. For spatial reasoning, the LMs use detection and segmentation to facilitate visual reasoning. (2) GPT-4o likes to use more tools. GPT-4o uses the vision specialists more often than GPT-4 Turbo. Also, the two LMs behave differently for the semantic correspondence tasks. GPT-4o uses the segmentation module for $40\\%$ of the task instances, while GPT-4 Turbo uses the detection module for less than $20\\%$ of times, and rarely uses the segmentation module. This difference may explain the performance gap between the two LMs $58.3\\%$ v.s. $42.4\\%$ ) on this task. ", "page_idx": 7}, {"type": "image", "img_path": "GNSMl1P5VR/tmp/43021e7a814fdccf48b0cae0193357bf309121b65c7a1171834c3001c26c6e3f.jpg", "img_caption": ["Figure 4: Percentage of times GPT-4o and GPT4 Turbo use a visual module in SKETCHPAD when solving $V^{*}$ Bench, relative depth, spatial reasoning, and semantic correspondence tasks. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "GNSMl1P5VR/tmp/84258984ae60946dfcad705357d7d2342c75c0ad2064a3f6065f8539d5a01654.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 3: Comparison with other augmentation frameworks for multimodal LMs on singleimage tasks. For fair comparison, we modify the original Visprog [14] framework by replacing the LM and VQA components with the corresponding GPT-4 model. ", "page_idx": 7}, {"type": "text", "text": "Comparison with visual prompting and tool-use frameworks. In Table 3, we compare SKETCHPAD with the visual prompting framework SoM [55] and the LLM tool-use framework Visprog [14]. Details of these methods can be found in $\\S2$ . For a fair comparison, we make the following adaptations: (1) we find that prompting LMs with SoM images can hurt performance, likely because the visual prompts confuse the model. To make a stronger baseline, we prompt the LM with both the original image and the SoM image (full prompt in $\\S C)$ , which we refer as $^{\\leftarrow}\\mathrm{SoM}+$ orig.\u201d (2) We replace the LM and VQA modules in Visprog with the corresponding GPT-4 model. (3) Since baseline methods are developed on single-image tasks, we compare SKETCHPAD on such tasks. From Table 3, we can see that SKETCHPAD is the only framework that yields consistent improvement on all tasks. SoM can boost spatial reasoning ability, as the authors reported. However, it can hurt the performance on other tasks, even in the \u201c $\\mathrm{^{\\bullet}S o M+}$ orig.\u201d setting. Visprog performs worse than the base LM on all the tasks. As prior work [21, 18] suggests, one possible reason is that the vision modules themselves have errors, and the error propagates when the modules are composed by a program. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "6 Analysis and Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Why does SKETCHPAD work? First, vision is a versatile and informational interface that complements language. Dense information like depth and segmentation cannot be described easily through language [9]. In a broader perspective, humans have developed many visualization techniques that are direct, efficient, and informational. SKETCHPAD provides LMs the opportunity to use them. Second, in SKETCHPAD, multimodal LMs can plan and reason based on the intermediate visual artifacts they created. In contrast, in prior modular vision work [14, 38, 55], multimodal modules follow a predefined plan by either humans or code. SKETCHPAD is much more flexible and robust to errors. For example, suppose object detection makes an error. The LM can (in principle) find the error by viewing the bounding boxes, and change its following plans, but prior methods cannot. Third, as discussed next, the plans of multimodal LMs are similar to human plans, and therefore likely benefit from the fact that the underlying LMs have seen data with similar reasoning patterns. ", "page_idx": 8}, {"type": "text", "text": "Do LMs have the same plans as humans? We conduct a human study on all geometry problems and 10 problems on each vision task. On geometry, humans draw the same auxiliary line as GPT-4o $80\\%$ of the time. On vision, we show 2 human subjects the full plan of GPT-4o, which they rate is valid in $92.8\\%$ of instances. Most errors are caused by failures in the vision specialists (e.g., fail to detect an object) and mistakes in simple visual question answering, rather than planning. ", "page_idx": 8}, {"type": "table", "img_path": "GNSMl1P5VR/tmp/1669175a3e780113f0f9b674705c03b1ad230a401aa7b1221c11cbff3841e94a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 4: Open-source LLaVA models\u2019 performance on math tasks. The oracle Sketchpad uses the visual artifact generated in the last action of GPT- $40+8$ KETCHPAD as inputs. ", "page_idx": 8}, {"type": "text", "text": "Experiments on open-source models. Can sketches like diagrams, plots, and auxiliary lines facilitate existing open-source multimodal LMs? To answer this question, we conduct the experiments in Table 4. We use the visual artifacts generated in the last action of GPT-4o + SKETCHPAD experiment as the image input for open-source LLaVA-NEXT models [27]. We can see that this oracle SKETCHPAD brings consistent improvement to math tasks and boosts mathematical reasoning. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We present Visual SKETCHPAD, a framework that provides multimodal LMs with the tools necessary to generate intermediate sketches to reason over tasks. For complex mathematical reasoning tasks, SKETCHPAD yields large performance gains, by visualizing auxiliary lines, math functions, graphs, and games during reasoning. For visual reasoning tasks, we add vision specialists to SKETCHPAD. The LM can call these specialists during reasoning, observing the visualization of these specialists\u2019 predictions (e.g., bounding boxes from the object detection model; masks from the segmentation model), and then conduct further planning and reasoning. Experiments show that SKETCHPAD enhances the LMs\u2019 performance across all tasks, and sets new state-of-the-art results. Ultimately, SKETCHPAD represents a step toward endowing LMs with more human-like multimodal intelligence, leveraging the complementary strengths of language and vision to tackle increasingly complex reasoning challenges. ", "page_idx": 8}, {"type": "text", "text": "Limitations and future directions. First, SKETCHPAD requires more computing resources than directly outputting language tokens. We discuss more about computing costs in E. Second, this work focuses on existing off-the-shelf LMs. Future work may explore the training side of SKETCHPAD. For example, recent multimodal models like Unified-IO 2 [31] and Chameleon [41] are natively multimodal and can output both text and images. SKETCHPAD may emerge as a new paradigm for instruction tuning these models. Finally, SKETCHPAD can be applied in more areas. For example, for robotics, we can apply SKETCHPAD to search for small things in a crowded space, highlight the object of interest, and zoom the camera for a better view or use depth estimation to help navigation. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716\u201323736, 2022.   \n[2] Anthropic. Introducing the next generation of claude. https://www.anthropic.com/news/ claude-3-family, March 2024.   \n[3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond, 2023.   \n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[5] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri, Mostafa Dehghani, Daniel Salz, Mario Lucic, Michael Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, Bo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin Ritter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic, Austin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas Beyer, Julien Amelot, Kenton Lee, Andreas Peter Steiner, Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu, Keran Rong, Alexander Kolesnikov, Mojtaba Seyedhosseini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. Pali-x: On scaling up a multilingual vision and language model, 2023.   \n[6] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023.   \n[7] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.   \n[8] Deqing $\\mathrm{Fu^{*}}$ , Ghazal Khalighinejad\u2217, Ollie Liu\u2217, Bhuwan Dhingra, Dani Yogatama, Robin Jia, and Willie Neiswanger. IsoBench: Benchmarking multimodal foundation models on isomorphic representations, 2024.   \n[9] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390, 2024.   \n[10] Peng Gao, Renrui Zhang, Chris Liu, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, et al. Sphinx-x: Scaling data and parameters for a family of multi-modal large language models. arXiv preprint arXiv:2402.05935, 2024.   \n[11] Vinod Goel. Sketches of thought. MIT press, 1995.   \n[12] Gabriela Goldschmidt. The dialectics of sketching. Creativity research journal, 4(2):123\u2013143, 1991.   \n[13] Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. Olmo: Accelerating the science of language models. arXiv preprint arXiv:2402.00838, 2024.   \n[14] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14953\u201314962, 2023.   \n[15] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and J\u00fcrgen Schmidhuber. Metagpt: Meta programming for a multi-agent collaborative framework, 2023.   \n[16] Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa Fujii, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, and Tomas Pfister. Tool documentation enables zero-shot tool-usage with large language models. arXiv preprint arXiv:2308.00675, 2023.   \n[17] Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A Smith, and Jiebo Luo. Promptcap: Prompt-guided task-aware image captioning. arXiv preprint arXiv:2211.09699, 2022.   \n[18] Yushi Hu, Otilia Stretcu, Chun-Ta Lu, Krishnamurthy Viswanathan, Kenji Hata, Enming Luo, Ranjay Krishna, and Ariel Fuxman. Visual program distillation: Distilling tools and programmatic reasoning into vision-language models. arXiv preprint arXiv:2312.03052, 2023.   \n[19] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mistral 7b, 2023.   \n[20] Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023.   \n[21] Apoorv Khandelwal, Ellie Pavlick, and Chen Sun. Analyzing modular approaches for visual question decomposition. arXiv preprint arXiv:2311.06411, 2023.   \n[22] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4015\u20134026, 2023.   \n[23] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024.   \n[24] Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu, Jianwei Yang, Chunyuan Li, Lei Zhang, and Jianfeng Gao. Semantic-sam: Segment and recognize anything at any granularity. arXiv preprint arXiv:2307.04767, 2023.   \n[25] Dingning Liu, Xiaomeng Dong, Renrui Zhang, Xu Luo, Peng Gao, Xiaoshui Huang, Yongshun Gong, and Zhihui Wang. 3daxiesprompts: Unleashing the 3d spatial task capabilities of gpt-4v. arXiv preprint arXiv:2312.09738, 2023.   \n[26] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023.   \n[27] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024.   \n[28] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.   \n[29] Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, et al. Llava-plus: Learning to use tools for creating multimodal agents. arXiv preprint arXiv:2311.05437, 2023.   \n[30] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023.   \n[31] Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision, language, audio, and action. arXiv preprint arXiv:2312.17172, 2023.   \n[32] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. In The 59th Annual Meeting of the Association for Computational Linguistics (ACL), 2021.   \n[33] Zixian Ma, Weikai Huang, Jieyu Zhang, Tanmay Gupta, and Ranjay Krishna. m&m\u2019s: A benchmark to evaluate tool-use for multi-step multi-modal tasks. In Synthetic Data for Computer Vision Workshop@ CVPR 2024, 2024.   \n[34] Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess, Ayzaan Wahid, Zhuo Xu, Quan Vuong, Tingnan Zhang, Tsang-Wei Edward Lee, Kuang-Huei Lee, Peng Xu, Sean Kirmani, Yuke Zhu, Andy Zeng, Karol Hausman, Nicolas Heess, Chelsea Finn, Sergey Levine, and Brian Ichter. Pivot: Iterative visual prompting elicits actionable knowledge for vlms. 2024.   \n[35] OpenAI. Gpt-4 technical report, 2023.   \n[36] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models. In NAACL, 2024.   \n[37] Aleksandar Shtedritski, Christian Rupprecht, and Andrea Vedaldi. What does clip know about a red circle? visual prompt engineering for vlms. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11987\u201311997, 2023.   \n[38] D\u00eddac Sur\u00eds, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. Proceedings of IEEE International Conference on Computer Vision (ICCV), 2023.   \n[39] Holly A Taylor and Barbara Tversky. Descriptions and depictions of environments. Memory & cognition, 20:483\u2013496, 1992.   \n[40] Holly A Taylor and Barbara Tversky. Spatial mental models derived from survey and route descriptions. Journal of Memory and language, 31(2):261\u2013292, 1992.   \n[41] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. 2024.   \n[42] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   \n[43] InternLM Team. Internlm: A multilingual language model with progressively enhanced capabilities. https://github.com/InternLM/InternLM, 2023.   \n[44] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209, 2024.   \n[45] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.   \n[46] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. ", "page_idx": 12}, {"type": "text", "text": "[47] Barbara Tversky and Masaki Suwa. Thinking with sketches. 2009. ", "page_idx": 12}, {"type": "text", "text": "[48] Barbara Tversky, Masaki Suwa, Maneesh Agrawala, Julie Heiser, Chris Stolte, Pat Hanrahan, Doantam Phan, Jeff Klingner, Marie-Paule Daniel, Paul Lee, et al. Sketches for design and design of sketches. Human Behaviour in Design: Individuals, Teams, Tools, pages 79\u201386, 2003.   \n[49] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023.   \n[50] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824\u201324837, 2022.   \n[51] Penghao Wu and Saining Xie. V\\*: Guided visual search as a core mechanism in multimodal llms. ArXiv, abs/2312.14135, 2023.   \n[52] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023.   \n[53] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. arXiv preprint arXiv:2404.07972, 2024.   \n[54] An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, et al. Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation. arXiv preprint arXiv:2311.07562, 2023.   \n[55] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023.   \n[56] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In CVPR, 2024.   \n[57] Zhengyuan Yang\\*, Linjie $\\mathrm{Li^{*}}$ , Jianfeng Wang\\*, Kevin Lin\\*, Ehsan Azarnasab\\*, Faisal Ahmed\\*, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. 2023.   \n[58] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.   \n[59] Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Richard James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-Tau Yih. Retrieval-augmented multimodal language modeling. In International Conference on Machine Learning, pages 39755\u201339769. PMLR, 2023.   \n[60] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, et al. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022.   \n[61] Jieyu Zhang, Ranjay Krishna, Ahmed H Awadallah, and Chi Wang. Ecoassistant: Using llm assistant more affordably and accurately. arXiv preprint arXiv:2310.03046, 2023.   \n[62] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Acknowledgement 16   \nB More Qualitative Examples 16   \nC Prompts 17   \nD Dataset Statistics 26   \nE Costs 26   \nF Impact Statement 26 ", "page_idx": 14}, {"type": "text", "text": "A Acknowledgement ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "This work is in part supported by gifts from Allen Institute for Artificial Intelligence, Meta, Apple, and Amazon Science. We also greatly appreciate the help from Zixian Ma, Wei-Chiu Ma, Benlin Liu, Jieyu Zhang, UW NLP, AllenNLP, AI2 PRIOR, and UW RAIVN for their help. ", "page_idx": 15}, {"type": "text", "text": "B More Qualitative Examples ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here we provide more details of how GPT-4o and SKETCHPAD solve complex mathematical and visual reasoning tasks. ", "page_idx": 15}, {"type": "image", "img_path": "GNSMl1P5VR/tmp/798921c10485c86c555a997635430a010d299262d5f2ffd6f5f9c3d7e8d7999b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 5: Qualitative examples of how GPT-4o and SKETCHPAD solve (a) complex mathematical and (b) visual reasoning tasks. ", "page_idx": 15}, {"type": "text", "text": "C Prompts ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Prompts for computer vision tasks. Here we provide the running log of GPT- $40+S$ KETCHPAD tackling a visual search problem from $V^{*}$ Bench [51]. We use the same prompt template for all computer vision problems in this work. For visualization purposes, we present the prompts that contain codes differently. In our implementation, they are all text inputs to the LLM. ", "page_idx": 16}, {"type": "text", "text": "SYSTEM PROMPT ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "You are a helpful multimodal AI assistant. [MORE INSTRUCTIONS ...] ", "page_idx": 16}, {"type": "text", "text": "For each turn, you should first do a \"THOUGHT\", based on the images and text you see. If you think you get the answer to the intial user request, you can reply with \"ANSWER: <your answer>\" and ends with \"TERMINATE\". ", "page_idx": 16}, {"type": "text", "text": "Initial Prompt + Request ", "text_level": 1, "page_idx": 16}, {"type": "table", "img_path": "GNSMl1P5VR/tmp/259a49bb417a9b3f8fd64934ff52d0c618ed99a6616771a5d942d5096dc67a77.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "57 possible_patches , possible_boxes $=$ search_object_and_zoom(image, [\" bird \", \"sign \"])   \n5589 for i , patch in enumerate( possible_patches ):   \nprint (f\"Patch {i }:\")   \n6601 display (patch.annotated_image)   \n62 # print the bounding boxes of the detected objects in the first patch   \n63 print ( possible_boxes [0]) # [[0.24, 0.21, 0.3, 0.4], [0.6, 0.3, 0.2, 0.3]]   \n64   \n65   \n66 def segment_and_mark(image, anno_mode:lis $=\\,[\\,^{\\circ}\\mathrm{Mask}^{\\circ}$ , \u2019Mark\u2019]):   \n67 \"\"\"Use a segmentation model to segment the image, and add colorful masks on the segmented objects . Each segment is also labeled with a number.   \n68 The annotated image is returned along with the bounding boxes of the segmented objects .   \n69 This tool may help you to better reason about the relationship between objects , which can be useful for spatial reasoning etc .   \n70 DO NOT use this tool to search or detect an object . It is likely the object is small and segmentaiton does not help.   \n71 Segmentation and marking can also be helpful for 3D and video reasoning . For example, helping you to see more clearly and analyzes the   \nrelationship between different frames of a video.   \n72   \n73 Args:   \n74 image (PIL.Image.Image): the input image   \n75 anno_mode (list , optional ): What annotation is added on the input image. Mask is the colorful masks. And mark is the number labels .   \nDefaults to [\u2019Mask\u2019, \u2019Mark\u2019].   \n76   \n77 Returns:   \n78 output_image (AnnotatedImage): the original image annotated with colorful masks and number labels . Each mask is labeled with a number. The   \nnumber label starts at 1.   \n79 bboxes ( List ): listthe bounding boxes of the masks.The order of the boxes is the same as the order of the number labels .   \n80   \n81 Example:   \n82 User request : I want to find a seat close to windows, where should I sit ?   \n83 Code:   \n8845 \u2018\u2018\u2018 python   \nimage Image.open(\"sample_img.jpg\")   \n86 output_image, bboxes $=$ segment_and_mark(image)   \n87 display (output_image.annotated_image)   \n88   \n89 Model reply: You can sit on the chair numbered as 5, which is close to the window.   \n90 User: Give me the bounding box of that chair .   \n91 Code:   \n9923 \u2018\u2018\u2018 python   \nprint (bboxes[4]) # [0.24, 0.21, 0.3, 0.4]   \n94   \n95 Model reply: The bounding box of the chair numbered as 5 is [0.24, 0.21, 0.3, 0.4].   \n96 1   \n97   \n98 def depth(image):   \n99 100 A colormap is used to represent the depth. It uses Inferno colormap. The closer the object , the warmer the color . \"\"\"Depth estimation using DepthAnything model. It returns the depth map of the input image.   \n101 This tool may help you to better reason about the spatial relationship , like which object is closer to the camera.   \n102   \n103 Args:   \n104 image (PIL.Image.Image): the input image   \n105   \n106 Returns:   \n107 output_image (PIL.Image.Image): the depth map of the input image   \n108   \n109 Example:   \n110 image $=$ Image.open(\"sample_img.jpg\")   \n111 output_image $=$ depth(image)   \n112 display (output_image)   \n113 \"\"\"   \n114   \n115 def zoom_in_image_by_bbox(image, box, padding=0.05):   \n116 \"\"\"A simple wrapper function to crop the image based on the bounding box.   \n117 When you want to answer question about visual details in a bounding box annotated by the detection tool , you would like to zoom in on the   \nobject using this function .   \n118   \n119 Args:   \n120 image (PIL.Image.Image): the input image   \n121 box ( List [ float ]) : the bounding box in the format of [x, y, w, h]   \n122 padding ( float , optional ): The padding for the image crop, outside of the bounding box. Defaults to 0.1. The zoom factor cannot be too   \nsmall. Minimum is 0.05   \n123   \n124 Returns:   \n112256 cropped_img (PIL.Image.Image): the cropped image   \n127 Example:   \n128 image $=$ Image.open(\"sample_img.jpg\")   \n129 annotated_img, boxes $=$ detection (image, \"bus\")   \n113301 cropped_img $=$ zoom_in_image_by_bbox(image, boxes[0], padding=0.05)   \ndisplay (cropped_img)   \n132   \n133   \n134 def overlay_images(background_img, overlay_img, alpha=0.3, bounding_box=[0, 0, 1, 1]) :   \n135   \n136 Overlay an image onto another image with transparency .   \n137 This is particularly useful visualizing heatmap while preserving some info from the original image.   \n138 For example, you can overlay a segmented image on a heatmap to better understand the spatial relationship between objects .   \n139 It will also help seeing the labels , circles on the original image that may not be visible on the heatmap.   \n140   \n141 Args:   \n142 background_img_pil (PIL.Image.Image): The background image in PIL format.   \n143 overlay_img_pil (PIL.Image.Image): The image to overlay in PIL format.   \n144 alpha ( float ): Transparency of the overlay image.   \n145 bounding_box (List[ float ]) : The bounding box of the overlay image. The format is [x, y, w, h]. The coordinates are normalized to the   \nbackground image. Defaults to [0, 0, 1, 1].   \n146   \n147 Returns:   \n148 PIL.Image.Image: The resulting image after overlay , in PIL format.   \n149 s   \n150 Example:   \n151 image $=$ Image.open(\u2019 original .jpg \u2019)   \n152 depth_map $=$ depth(image)   \n153 overlayed_image $=$ overlay_images(depth_map, image, alpha=0.3)   \n154 display (overlayed_image)   \n155   \n156 \u2018\u2018\u2018   \n157 # GOAL #: Based on the above tools , I want you to reason about how to solve the # USER REQUEST # and generate the actions step by step (each action   \nis a python jupyter notebook code block) to solve the request .   \n158 You may need to use the tools above to process the images and make decisions based on the visual outputs of the previous code blocks.   \n159 Your visual ability is not perfect , so you should use these tools to assist you in reasoning about the images.   \n160   \n161 # [six in\u2212context examples here]   \n162   \n163 # USER REQUEST #: ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "image", "img_path": "GNSMl1P5VR/tmp/f8a96d3eac2425a2999fe21a1f15febc329e5a476a43402ad0dcf03a6690a197.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Is the red balloon above or below the white balloon? Options: (A) The red balloon is below the white balloon . (B) The red balloon is above the white balloon . ", "page_idx": 18}, {"type": "text", "text": "# USER IMAGE stored in image_1 as PIL image.   \nNow please generate only THOUGHT 0 and ACTION 0 in RESULT. If no action needed, also reply with ANSWER: <your answer> and ends with TERMINATE in the RESULT: ", "page_idx": 18}, {"type": "text", "text": "# RESULT #:", "page_idx": 18}, {"type": "text", "text": "Thought + Action 0 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "THOUGHT 0: To determine the relative positions of the red and white balloons , I need to detect the balloons in the image. I will use the detection tool to find the balloons .   \n2 ACTION 0: \u2018\u2018\u2018 python   \n4 image_1_balloon_detection , image_1_balloon_boxes $=$ detection (image_1, [\"red balloon\", \"white balloon\"])   \n5 display (image_1_balloon_detection .annotated_image) ", "page_idx": 18}, {"type": "text", "text": "Observation 0 ", "page_idx": 19}, {"type": "image", "img_path": "GNSMl1P5VR/tmp/9bb436a7a929a4e88eebc651d772ddc3813ccbc1513f8880c7d4103819d21670.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Please generate the next THOUGHT and ACTION. If you can get the answer, please also reply with ANSWER: <your answer> and ends with TERMINATE. ", "page_idx": 19}, {"type": "text", "text": "Thought + Action 1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "THOUGHT 1: The detection tool did not annotate the balloons in the image. I will use the sliding_window_detection tool to search for the red and white balloons in the image. 3 ACTION 1: \u2018\u2018\u2018 python possible_patches , possible_boxes $=$ sliding_window_detection (image_1, [\"red balloon\", \"white balloon\"]) for i , patch in enumerate( possible_patches ): print (f\"Patch {i+1}:\") display (patch.annotated_image) ", "page_idx": 19}, {"type": "text", "text": "Observation 1 ", "text_level": 1, "page_idx": 19}, {"type": "image", "img_path": "GNSMl1P5VR/tmp/244c9e375b6339b42e652df08e9718a3348a74b7d4135215cb38696eba67a824.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "GNSMl1P5VR/tmp/320fa51fcbfe4461e7691991692be9d232191ae65281985342e331daabe9101a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Please generate the next THOUGHT and ACTION. If you can get the answer, please also reply with ANSWER: <your answer> and ends with TERMINATE. ", "page_idx": 20}, {"type": "text", "text": "Thought + Action 2 ", "page_idx": 20}, {"type": "text", "text": "THOUGHT 2: From Patch 4 and 5, I can see both the red and white balloons. The red balloon is clearly below the white balloon in these patches.   \nACTION 2: No action needed.   \nANSWER: The red balloon is below the white balloon. So the answer is (A). TERMINATE ", "page_idx": 20}, {"type": "text", "text": "Prompts for math tasks. The prompts are similar to the vision task for math tasks, except we remove the computer vision specialists, and add example codes for math plotting. Besides, the user query is different. For each task, the user query part of the prompt is as follows. ", "page_idx": 21}, {"type": "text", "text": "PROMPT ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "You are given a real-valued, scalar function $f(x)$ .   \nYOUR TASK is to determine whether $f(x)$ is an even function, an odd function, or neither. Definition of an odd function: A function such that ", "page_idx": 21}, {"type": "equation", "text": "$$\nf(-x)=-f(x)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the sign is reversed but the absolute value remains the same if the sign of the independent variable is reversed. A function is neither even nor odd if it does not satisfy either condition. Here is the expression of $f(x)$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\nf(x)={\\frac{-2x^{5}}{2x^{8}-4x^{6}+12x^{4}+4x^{2}+11.16}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Respond with \u2018even\u2019, \u2018odd\u2019, \u2018neither\u2019 first on whether the function $f(x)$ is even, odd, or neither, based on the definitions and your observation of the function. You can generate matplotlib code to visualize the function. ", "page_idx": 21}, {"type": "text", "text": "If you can get the answer, please reply with ANSWER: <your answer>, extract the final answer in FINAL ANSWER: <final answer> and ends with TERMINATE in the RESULT. Answer: ", "page_idx": 21}, {"type": "text", "text": "Figure 6: Prompt for the Math Parity task. We follow a similar prompt format to [8], except prompting the models to write the code to generate images. ", "page_idx": 21}, {"type": "text", "text": "PROMPT ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "You are given a real-valued, scalar function $f(x)$ .   \nYOUR TASK is to determine whether $f(x)$ is an convex function or concave function. Definition of a convex function: A function such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\forall x,y,0\\leq t\\leq1,f(t x+(1-t)y)\\leq t f(x)+(1-t)f(y)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Definition of a concave function: A function such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\forall x,y,0\\leq t\\leq1,f(t x+(1-t)y)\\geq t f(x)+(1-t)f(y)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Here is the expression of $f(x)$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\nf(x)=7.57-0.08*A b s(x)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Respond with \u2018convex\u2019 or \u2018concave\u2019 first on whether the function f (x) is convex or concave, based on the definitions and your observation of the function. You can generate matplotlib code to visualize the function. ", "page_idx": 21}, {"type": "text", "text": "If you can get the answer, please reply with ANSWER: <your answer>, extract the final answer in FINAL ANSWER: <final answer> and ends with TERMINATE in the RESULT. Answer: ", "page_idx": 21}, {"type": "text", "text": "Figure 7: Prompt for the Math Convexity task. We follow the similar prompt format to [8], except prompting the models to write the code to generate images. ", "page_idx": 21}, {"type": "text", "text": "PROMPT ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "You are given an adjacency matrix of a graph and two query nodes.   \nYOUR TASK is to find if there is a path between the two nodes.   \nDefinition of connectivity: In an undirected graph G, two vertices u and v are called connected if G contains a path from u to v. A path in a graph is a finite sequence of edges which joins a sequence of vertices. In the query example, the nodes and the adjacency matrix are zero-indexed.   \nQuery Example: ", "page_idx": 22}, {"type": "text", "text": "Adjacency Matrix: ", "text_level": 1, "page_idx": 22}, {"type": "equation", "text": "$\\left[\\begin{array}{l l l l l l l l l l}{0}&{0}&{0}&{0}&{0}&{1}&{0}&{0}&{0}\\\\ {0}&{0}&{1}&{0}&{0}&{0}&{1}&{0}&{0}\\\\ {0}&{1}&{0}&{0}&{1}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{1}&{0}&{0}&{0}&{0}&{0}&{0}\\\\ {1}&{0}&{0}&{0}&{0}&{0}&{1}&{0}&{1}\\\\ {0}&{1}&{0}&{0}&{0}&{1}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{0}&{1}&{0}&{0}&{0}\\end{array}\\right]$ ", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Query nodes indices (zero-indexed): 4 and 0 ", "page_idx": 22}, {"type": "text", "text": "Respond with \u2018yes\u2019 or \u2018no\u2019 first on whether the query nodes are connected or not in the graph. If there is a path, first provide the path as a sequence of vertices (nodes), and then explain your reasoning. You can use networkx to draw the graph. If there is no path, explain why in details. Answer (start with \u2019yes\u2019 or \u2019no\u2019): If you can get the answer, please reply with ANSWER: <your answer>, extract the final answer in FINAL ANSWER: <final answer> and ends with TERMINATE in the RESULT. ", "page_idx": 22}, {"type": "text", "text": "Figure 8: Prompt for the Graph Connectivity task. We follow the similar prompt format to [8], except prompting the models to write the code to generate images. ", "page_idx": 22}, {"type": "text", "text": "PROMPT ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "You are given a visual representation of two graphs, graph $_\\mathrm{G}$ on the left and graph $_\\mathrm{H}$ on the right.   \nYOUR TASK is to determine whether the two graphs are isomorphic to each other. ", "page_idx": 23}, {"type": "text", "text": "Definition of graph isomorphism: In graph theory, an isomorphism of graphs $_\\mathrm{G}$ and $_\\mathrm{H}$ is a bijection $f$ between the vertex sets of G and $\\mathrm{H}$ , denoted as $f:V(G)\\rightarrow V(H)$ . G and $_\\mathrm{H}$ are said to be isomorphic when $f$ satisfies the following: any two vertices ${\\boldsymbol u}$ and $v$ of G are adjacent in $_\\mathrm{G}$ if and only if $f(u)$ and $f(v)$ are adjacent in H. This kind of bijection is commonly described as \"edge-preserving bijection\", in accordance with the general notion of isomorphism being a structure-preserving bijection. In the query example, the adjacency matrices are zero-indexed. ", "page_idx": 23}, {"type": "text", "text": "Adjacency Matrix $\\bar{G}$ : ", "page_idx": 23}, {"type": "table", "img_path": "GNSMl1P5VR/tmp/62fe3f287028f3e66efdcccf609859575efc72f30bc249a1f3a0a4985ff6f630.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Adjacency Matrix H: ", "page_idx": 23}, {"type": "table", "img_path": "GNSMl1P5VR/tmp/131499b84092cc67b6498b9f4225dfd44539065ddd1c56a7ce9c4bc350f07652.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Respond with \u2019yes\u2019 or \u2019no\u2019 first on whether the two graphs are isomorphic to each other. You can use networkx to draw the graph. If they are isomorphic, first provide the bijection between the two graphs, and then explain your reasoning. You can use networkx to draw the graph. If they are not isomorphic, explain why in detail. Answer (start with \u2019yes\u2019 or $'\\mathrm{no}^{\\prime}$ ): If you can get the answer, please reply with ANSWER: <your answer>, extract the final answer in FINAL ANSWER: <final answer> and ends with TERMINATE in the RESULT. ", "page_idx": 23}, {"type": "text", "text": "Answer: ", "page_idx": 23}, {"type": "text", "text": "Figure 9: Prompt for the Graph Isomorphism task. We follow a similar prompt format to [8], except prompting the models to write the code to generate images. ", "page_idx": 23}, {"type": "text", "text": "PROMPT ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "You are given an adjacency matrix of a graph and two query nodes (one source node and one sink node). The source node is the node where the flow starts and the sink node is the node where the flow ends.   \nYOUR TASK is to solve the maxflow problem given the weighted directed graph.   \nDefinition of Maxflow problem: In the max flow problem, we have a directed graph with a source node $s$ and a sink node $t,$ and each edge has a capacity (integer valued, colored in green) that represents the maximum amount of flow that can be sent through it. The goal is to find the maximum amount of flow that can be sent from $s$ to $t,$ while respecting the capacity constraints on the edges.   \nQuery Example: ", "page_idx": 24}, {"type": "text", "text": "Adjacency Matrix: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[0\\quad1\\quad4\\right]}\\\\ {\\left[0\\quad0\\quad6\\right]}\\\\ {\\quad0\\quad0\\quad0}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Source node (zero-indexed): 0 ", "page_idx": 24}, {"type": "text", "text": "Sink node (zero-indexed): 2 ", "page_idx": 24}, {"type": "text", "text": "In the query example, the nodes and the adjacency matrix are zero-indexed. You can use networkx to draw the graph. If you can get the answer, please reply with ANSWER: <your answer>, extract the final answer in FINAL ANSWER: <final answer> and ends with TERMINATE in the RESULT. ", "page_idx": 24}, {"type": "text", "text": "Figure 10: Prompt for Graph Maxflow task. We follow the similar prompt format to [8], except prompting the models to solve the maxflow problem. ", "page_idx": 24}, {"type": "text", "text": "PROMPT ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Given the following FEN of the chess game:   \n1r1q1rk1/1b2b1Qp/4pp1B/pp1nP3/2pPN3/P1P5/1PB3PP/R4RK1 b - - 0 18   \nDetermine the game\u2019s outcome. Who won: White or Black? Answer (start with \u2019white\u2019 or \u2019black\u2019 or \u2019draw\u2019):   \nYou can draw the chess board using Python given the FEN string. If you can get the answer, please reply with ANSWER: <your answer>, extract the final answer in FINAL ANSWER: <final answer> and ends with TERMINATE in the RESULT.   \nAnswer: ", "page_idx": 24}, {"type": "text", "text": "Figure 11: Prompt for Winner ID task. We follow a similar prompt format to [8], except prompting the models to analyze the game outcome. ", "page_idx": 24}, {"type": "text", "text": "D Dataset Statistics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Table 5 and 6 show the statistics of the datasets we used, including IsoBench [8], BLINK [9], MMVP [44], and $V^{*}$ Bench [51]. ", "page_idx": 25}, {"type": "table", "img_path": "GNSMl1P5VR/tmp/e03eec578effc7a155574b56b51cd08407a9068f4a0eaaa1dd4ff4139720fb54.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "GNSMl1P5VR/tmp/024f3b09a1ff49f8ae518296872c228748a359b51b54296fdc36ef3f8f1ea132.jpg", "table_caption": ["Table 5: IsoBench [8] data statistics. "], "table_footnote": ["Table 6: Vision tasks data statistics. "], "page_idx": 25}, {"type": "text", "text": "E Costs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The cost of running each task using GPT-4o is in Table 7. ", "page_idx": 25}, {"type": "table", "img_path": "GNSMl1P5VR/tmp/da89650b8ab739c250526de79e1b1fa7e128af0f8efccb8f9c7f47e7a0233e57.jpg", "table_caption": [], "table_footnote": ["Table 7: The cost of running SKETCHPAD on each task. "], "page_idx": 25}, {"type": "text", "text": "F Impact Statement ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Our work proposes SKETCHPAD, a framework aiming at advancing academic research and meeting industry needs. In a broader perspective, SKETCHPAD proposes a new way that humans can interact with LMs, and makes LMs more interpretable by eliciting their reasoning process with both language and sketches. On the other hand, if misused, the LMs may be used to generate harmful vision and text artifacts. Nevertheless, this is not directly related to our research, and more researchers can be involved to research on the safety issue in a multimodal context. ", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Both the abstract and introduction reflect our paper\u2019s contributions and scope. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We discuss the limitations of this work in section 7. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: We do not make any theoretical claims. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide implementation details in $\\S3$ and $\\S C$ . ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [No] ", "page_idx": 28}, {"type": "text", "text": "Justification: The dataset is accessible online. We are committed to releasing the code. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The test details are specified in D, 4 and 5. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: Since our work focus on inference of language models, the results are provided as evaluation scores on validation/test set. And we have explained how they are calculated in 4. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide details of computing resources like compute device, number of workers, memory and time of execution in $\\S E$ . ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics, and strictly followed it in this work. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Yes, we talk about boarder impacts in Appendix A. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This paper focus on inference of language models, and thus there is no data or models to release. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Yes. We acknowledge the codebases we have referred to, cite and provide the urls of datasets and models in ??. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]