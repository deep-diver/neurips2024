{"references": [{"fullname_first_author": "Andy Zou", "paper_title": "Universal and transferable adversarial attacks on aligned language models", "publication_date": "2023-07-15", "reason": "This paper is highly relevant because it introduces a novel attack method against aligned LLMs, a crucial aspect in the paper's discussion of LLM safety and robustness."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-09", "reason": "This paper is significant because Llama 2 is a prominent open-source LLM used in the experiments, making it central to the paper's empirical findings and analysis of open-source models."}, {"fullname_first_author": "Ronen Eldan", "paper_title": "Who's harry potter? Approximate unlearning in LLMs", "publication_date": "2023-10-02", "reason": "This paper is important as it explores the concept of unlearning in LLMs, a key subject in the paper's investigation of mitigating the risks associated with open-source LLMs."}, {"fullname_first_author": "Mantas Mazeika", "paper_title": "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal", "publication_date": "2024-02-04", "reason": "This paper provides a benchmark dataset (Harmbench) for evaluating the safety and robustness of LLMs, forming a significant part of the paper's experimental setup and evaluation metrics."}, {"fullname_first_author": "Pratyush Maini", "paper_title": "TOFU: A task of fictitious unlearning for LLMs", "publication_date": "2024-01-06", "reason": "This paper is highly relevant due to its introduction of the TOFU benchmark, which is used in the paper's experiments to evaluate the effectiveness of the proposed embedding space attack on unlearning in LLMs."}]}