[{"figure_path": "CLxcLPfARc/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of discrete and embedding space attacks (this work). Discrete attacks manipulate discrete one-hot tokens Tadv \u2208 T, whereas embedding space attacks directly attack their continuous embedding space representation.", "description": "The figure illustrates two different types of attacks on large language models (LLMs): discrete attacks and embedding space attacks.  Discrete attacks manipulate the individual tokens of the input text, while embedding space attacks directly modify the continuous vector representation of the input tokens. The figure shows how these attacks differ in their approach and highlights the embedding space attack as a novel method proposed in the paper.", "section": "Abstract"}, {"figure_path": "CLxcLPfARc/figures/figures_4_1.jpg", "caption": "Figure 3: Illustration of the multi-layer attack. From a regular generated sequence T\u0142, we decode alternative output sequence T from intermediate layers of the neural network.", "description": "This figure illustrates the multi-layer attack, a variation of the embedding space attack.  Instead of directly attacking the input embeddings, this method decodes intermediate hidden state representations from various layers within the neural network (represented by h1, h2,...hL). By doing so, the attack aims to extract information that may have been propagated through the model's layers, possibly revealing information that would not be accessible through standard attacks targeting only the final output layer.  The decoded sequences from each layer (T1k+1, T2k+1,... TLk+1) provide alternative interpretations of the input sequence.", "section": "4 Experiment Configurations"}, {"figure_path": "CLxcLPfARc/figures/figures_6_1.jpg", "caption": "Figure 4: Attack success rate and average compute time of diverse discrete attacks and the proposed embedding attack for different models. Embedding attacks achieve higher success rates and are considerably more efficient compared to existing methods for all tested models.", "description": "This figure compares the attack success rate and computation time of various discrete attacks (AutoDAN, PAIR, GCG, Adaptive) against the proposed embedding attack method.  The results show that the embedding attack consistently achieves higher success rates while requiring significantly less computation time across different language models (Llama2, Vicuna, Mistral, and their circuit-breaking versions). This highlights the efficiency and effectiveness of the embedding space attack in bypassing safety mechanisms compared to traditional discrete methods.", "section": "Breaking Safety Alignment"}, {"figure_path": "CLxcLPfARc/figures/figures_7_1.jpg", "caption": "Figure 5: The two rows show the perplexity and toxicity (obtained from toxic-bert) of generated responses of different LLMs with and without embedding space attacks on the harmful behavior dataset. Additionally, the scores of the fine-tuned Llama2 model are compared to attacking the regular Llama2. Embedding attacks decrease perplexity for all models while significantly increasing toxicity for most models (significant differences with a Mann-Whitney U test are indicated with *).", "description": "This figure compares the perplexity and toxicity of large language models (LLMs) with and without embedding space attacks.  The results show embedding space attacks reduce perplexity but significantly increase toxicity in most models.  A comparison is also made to fine-tuning Llama2 to remove safety alignment; embedding attacks are shown to be more effective in increasing toxicity.", "section": "Breaking Safety Alignment"}, {"figure_path": "CLxcLPfARc/figures/figures_7_2.jpg", "caption": "Figure 6: Number of toxic responses (y-axis) and the binned attack loss (x-axis) for a universal embedding space attack on the Llama2 model is shown. It can be observed that lower loss values are associated with higher toxicity.", "description": "This figure shows the correlation between the loss of a universal embedding space attack on the Llama2 model and the toxicity of the generated responses.  The x-axis represents the binned loss values from the attack optimization, while the y-axis shows the number of toxic responses generated for each loss bin.  The figure demonstrates that lower loss values (indicating more successful attacks) are associated with a higher number of toxic responses, suggesting a relationship between attack success and the generation of toxic content.", "section": "5.2 Impact on Perplexity and Toxicity"}, {"figure_path": "CLxcLPfARc/figures/figures_18_1.jpg", "caption": "Figure 7: Embedding attack cumulative success rate of universal and individual attacks on the HP Q&A benchmark using the Llama2-7b-WhoIsHarryPotter model. The cumulative success rate for each layer is calculated over 100 attack iterations.", "description": "This figure shows the contribution of individual layers of the Llama2-7b-WhoIsHarryPotter model to the success rate of both universal and individual embedding attacks.  The x-axis represents the layer number, and the y-axis shows the cumulative success rate, calculated over 100 attack iterations.  The results indicate that the last layer contributes most significantly to the success of the attack, while the earlier layers contribute little or nothing.", "section": "H Multi-layer Attack"}, {"figure_path": "CLxcLPfARc/figures/figures_18_2.jpg", "caption": "Figure 8: Perplexity (a) and attack success rate (b) of the LlamaHP model on the Harry Potter Q&A dataset. Large perturbation norms hurt generation quality and are associated with high perplexity.", "description": "This figure shows the relationship between the L2 norm perturbation magnitude, perplexity, and attack success rate (ASR) for the LlamaHP model on the Harry Potter Q&A dataset.  The left panel shows that perplexity increases as the perturbation magnitude increases, indicating a decrease in generation quality. The right panel shows that ASR initially increases with perturbation magnitude, but then decreases after reaching a peak, suggesting that excessive perturbations hurt the model's ability to generate accurate answers. The overall trend highlights the need to find a balance between perturbation strength and generation quality to effectively attack the model.", "section": "I Overfitting embedding space attacks"}]