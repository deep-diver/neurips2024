[{"figure_path": "8rcFOqEud5/figures/figures_3_1.jpg", "caption": "Figure 1: The left part presents the process of inferring process rewards and how we conduct process reward guide tree-search. The right part denotes the self-training of both the process reward model and the policy model.", "description": "This figure illustrates the ReST-MCTS* framework. The left side shows how process rewards are inferred using a Monte Carlo Tree Search (MCTS*) algorithm guided by a process reward model (PRM).  The PRM estimates the probability that a given step will lead to a correct answer. These inferred rewards are used as value targets for refining the PRM and selecting high-quality reasoning traces. The right side details the self-training process, showing how the PRM and policy model are iteratively improved using the traces generated by the MCTS*. This iterative process leads to continuously enhanced language models.", "section": "3 The ReST-MCTS* Method"}, {"figure_path": "8rcFOqEud5/figures/figures_7_1.jpg", "caption": "Figure 2: Accuracy of different searches on MATH and SciBench with varied sampling budget.", "description": "This figure compares the accuracy of different search methods (ReST-MCTS*, PRM+Best-of-N, ORM+Best-of-N, and Self-Consistency) on the MATH and SciBench datasets. The x-axis represents the average number of completion tokens used per question, which serves as a proxy for the computational budget. The y-axis shows the accuracy achieved by each method.  The figure demonstrates that ReST-MCTS* consistently outperforms the other methods across various budget levels, highlighting its efficiency in finding accurate solutions.", "section": "4.2 Evaluating Self-Improvement of ReST-MCTS*"}, {"figure_path": "8rcFOqEud5/figures/figures_7_2.jpg", "caption": "Figure 2: Accuracy of different searches on MATH and SciBench with varied sampling budget.", "description": "This figure compares the accuracy of different search methods (ReST-MCTS*, PRM+Best-of-N, ORM+Best-of-N, and Self-Consistency) on two datasets (MATH and SciBench) with varying completion token budgets.  It shows that ReST-MCTS* consistently outperforms other methods, particularly as the token budget increases.  The error bars represent the variability in accuracy across multiple runs for each method.", "section": "4.2 Evaluating Self-Improvement of ReST-MCTS*"}, {"figure_path": "8rcFOqEud5/figures/figures_22_1.jpg", "caption": "Figure 1: The left part presents the process of inferring process rewards and how we conduct process reward guide tree-search. The right part denotes the self-training of both the process reward model and the policy model.", "description": "This figure illustrates the ReST-MCTS* framework. The left side shows how process rewards are inferred using a tree search algorithm (MCTS*) and used to guide the search process.  The right side illustrates the iterative self-training process for both the process reward model (PRM) and the policy model. The PRM learns to estimate the probability that a given step will lead to the correct answer, while the policy model learns to generate high-quality reasoning traces.  The interaction between these two models drives continuous improvement in the LLM's reasoning abilities.", "section": "3 The ReST-MCTS* Method"}, {"figure_path": "8rcFOqEud5/figures/figures_23_1.jpg", "caption": "Figure 4: Detailed process of new sample data generation for the self-training framework.", "description": "This figure shows a detailed illustration of the data generation process for the self-training framework. It consists of four stages: search, prune, verify, and infer reward & value. The search stage involves using MCTS* to explore the search space and generate multiple reasoning traces. The prune stage involves removing unfinished branches and those with incorrect final answers. The verify stage involves verifying the correctness of the remaining traces using string matching or LLM judging. Finally, the infer reward & value stage involves inferring the per-step process rewards and quality values for each step in the verified reasoning traces. These inferred rewards and values are then used to train the process reward and policy models.", "section": "C Algorithm Detail and Process Example"}, {"figure_path": "8rcFOqEud5/figures/figures_25_1.jpg", "caption": "Figure 2: Accuracy of different searches on MATH and SciBench with varied sampling budget.", "description": "This figure compares the accuracy of different search methods (ReST-MCTS*, PRM+Best-of-N, ORM+Best-of-N, and Self-Consistency) on two datasets, MATH and SciBench.  The x-axis represents the average number of completion tokens used per question, indicating the computational cost of each method.  The y-axis shows the accuracy achieved by each method.  The figure demonstrates that ReST-MCTS* consistently outperforms other methods across different token budgets, highlighting its efficiency in finding accurate solutions.", "section": "4.2 Evaluating Self-Improvement of ReST-MCTS*"}, {"figure_path": "8rcFOqEud5/figures/figures_27_1.jpg", "caption": "Figure 6: Comparison between existing self-training methods with our proposed ReST-MCTS*. ", "description": "This figure compares the proposed ReST-MCTS* self-training method with other existing methods such as RFT/STaR/ReSTEM, V-STaR, MATH-SHEPHERD, and Self-Rewarding.  It visually depicts the flow of each method, highlighting key components like the reasoning policy used (CoT, ToT, BoN, MCTS*), reward models (PRM, ORM), selection mechanisms (select), and fine-tuning steps (SFT).  The figure effectively illustrates the differences in how each approach generates training data and improves the language model.  ReST-MCTS* stands out by incorporating a value model guided MCTS* for automated per-step reward generation and improved trace selection, leading to a more efficient and effective self-training process.", "section": "3 The ReST-MCTS* Method"}, {"figure_path": "8rcFOqEud5/figures/figures_28_1.jpg", "caption": "Figure 1: The left part presents the process of inferring process rewards and how we conduct process reward guide tree-search. The right part denotes the self-training of both the process reward model and the policy model.", "description": "This figure illustrates the ReST-MCTS* framework. The left side shows how process rewards are inferred using Monte Carlo Tree Search (MCTS*) and used to guide the tree search.  The right side depicts the iterative self-training process for both the process reward model and the policy model, improving the accuracy and quality of reasoning traces over multiple iterations.", "section": "3 The ReST-MCTS* Method"}]