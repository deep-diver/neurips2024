{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-18", "reason": "This paper introduces CLIP, a foundational model for multimodal learning that is extensively used and cited in the current paper."}, {"fullname_first_author": "Junnan Li", "paper_title": "BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "publication_date": "2023-07-18", "reason": "BLIP-2 is a highly influential multimodal model that directly inspires the architecture and methodology of the current paper."}, {"fullname_first_author": "Maxime Oquab", "paper_title": "DINOv2: Learning robust visual features without supervision", "publication_date": "2023-04-07", "reason": "DINOv2 is a key vision model used as an expert in the current paper's multimodal approach."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "publication_date": "2024-00-00", "reason": "This paper introduces the Visual Instruction Tuning (VIT) method, a crucial technique used in the current paper's training process."}, {"fullname_first_author": "Haotian Liu", "paper_title": "LLaVA-1.5", "publication_date": "2023-10-00", "reason": "LLaVA-1.5 serves as a baseline model for comparison and is a key component in the current paper's experimental setup."}]}