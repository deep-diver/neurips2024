[{"type": "text", "text": "MoVA: Adapting Mixture of Vision Experts to Multimodal Context ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhuofan Zong1,2,\u2217 Bingqi $\\mathbf{M}\\mathbf{a}^{2,*}$ Dazhong Shen3 Guanglu Song2 ", "page_idx": 0}, {"type": "text", "text": "Hao Shao1 Dongzhi Jiang1 Hongsheng Li1,3,4,\u2020 Yu Liu2,\u2020 ", "page_idx": 0}, {"type": "text", "text": "1CUHK MMLab 2SenseTime Research 3Shanghai AI Laboratory 4CPII under InnoHK ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "As the key component in multimodal large language models (MLLMs), the ability of the visual encoder greatly affects MLLM\u2019s understanding on diverse image content. Although some large-scale pretrained vision encoders such as vision encoders in CLIP and DINOv2 have brought promising performance, we found that there is still no single vision encoder that can dominate various image content understanding, e.g., the CLIP vision encoder leads to outstanding results on general image understanding but poor performance on document or chart content. To alleviate the bias of CLIP vision encoder, we first delve into the inherent behavior of different pre-trained vision encoders and then propose the MoVA, a powerful and novel MLLM, adaptively routing and fusing task-specific vision experts with a coarse-to-fine mechanism. In the coarse-grained stage, we design a contextaware expert routing strategy to dynamically select the most suitable vision experts according to the user instruction, input image, and expertise of vision experts. This benefits from the powerful model function understanding ability of the large language model (LLM). In the fine-grained stage, we elaborately conduct the mixture-of-vision-expert adapter (MoV-Adapter) to extract and fuse task-specific knowledge from various experts. This coarse-to-fine paradigm effectively leverages representations from experts based on multimodal context and model expertise, further enhancing the generalization ability. We conduct extensive experiments to evaluate the effectiveness of the proposed approach. Without any bells and whistles, MoVA can achieve significant performance gains over current state-ofthe-art methods in a wide range of challenging multimodal benchmarks. Codes and models are available at https://github.com/TempleX98/MoVA. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Significant achievements in multimodal large language models (MLLMs) [1, 2, 3, 4, 5, 6, 7] have been witnessed due to their remarkable proficiency in solving open-world tasks. MLLMs acquire visual perception capacity while inheriting sophisticated reasoning abilities and knowledge from large language models (LLMs) [8, 9, 10]. The core idea behind MLLMs is projecting the vision representation into an LLM through a projector, facilitating a general-purpose multimodal understanding. ", "page_idx": 0}, {"type": "text", "text": "General multimodal understanding requires comprehending complex image contexts across various tasks and scenarios. The CLIP [11] vision encoder, pre-trained on large-scale image-text pairs with a contrastive loss, is widely considered as a flexible and popular choice among the latest leading MLLMs. However, training data and optimization target of the vision encoder determine its inconsistent performance across tasks and scenarios, which will bias the generalization of multimodal large language models. For instance, MLLMs with a single CLIP vision encoder usually perform poorly on fine-grained tasks such as grounding and optical character recognition (OCR) [12]. Several works have attempted to incorporate extra state-of-the-art vision encoder experts to cope with the challenge. For example, both SPHINX [13] and MoF [14] integrate vision self-supervised learning features of DINOv2 [15] with MLLMs to enhance their visual grounding capabilities. Vary [12] introduces a new vision encoder expert for improved fine-grained document and chart parsing ability. Intuitively, it is necessary to explore the utilization of more task-specific vision encoder experts in MLLMs to promote model generalization across various domains. ", "page_idx": 0}, {"type": "table", "img_path": "uHs6RJFDsg/tmp/bfdeb56f0c6ac41e65b7ddfa830673cccc9b8284cbfc2e3dcd3e6ee644c031f6.jpg", "table_caption": ["Table 1: Comparison of CLIP vs. state-of-the-art task-specific vision encoders. Our evaluation criteria encompass a variety of dimensions: comprehensive benchmarks [16], text-oriented Visual Question Answering (VQA) [17, 18], general VQA [19], object hallucination [20], Referring Expression Comprehension (REC) [21], Referring Expression Segmentation (RES) [21], and medical VQA benchmark SLAKE [22]. We use the same data for each model. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We aim to start the exploration through empirical analysis of readily available vision experts. In particular, we focus on the multimodal capabilities of seven distinct state-of-the-art vision encoders based on LLaVA-1.5-7B [28]. The results in Table 1 reveal that MLLMs with these task-specific vision encoders achieve optimal performance in their respective area. Concurrently, we note that the plain fusion (concatenation) of vision encoder experts adopted in previous works [13] would not bring consistent improvement compared with the single task-specific vision expert in its proficient task. The inherent bias of each expert introduces biased information and leads to performance degradation in the plain fusion paradigm. For example, DINOv2 serves as an expert in visual grounding but performs poorly at text-oriented tasks. Representation of DINOv2 would be regarded as biased information in text-related scenarios so incorporating DINOv2 for these tasks would inevitably cause performance decrease. Consequently, a flexible method of vision encoder ensemble that dynamically activates and weights context-relevant task-specific vision experts can fully unleash the capacity of these models while avoiding model bias. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose MoVA, a powerful MLLM, adaptively routing and fusing task-specific vision experts with a coarse-to-fine mechanism. Inspired by the powerful tool-use capabilities of LLM [29], the coarse-grained context-aware expert routing aims to employ LLM to select vision experts with strong relevance to the user\u2019s image and instruction from the expert model pool. Thanks to the strong generalization ability of LLM, we also can perform model routing for vision experts in open scenarios. The fine-grained expert fusion facilitates better extraction and integration of expert representations based on multimodal context. Specifically, the expert knowledge extractor in the mixture-of-vision-expert adapter (MoV-Adapter) will extract diverse task-specific knowledge from various vision experts through mixture-of-expert (MoE) cross-attention layers. The dynamic gating network can allocate precise expert-wise soft weights for the integration of extracted taskspecific knowledge. Under the coarse-to-fine paradigm, we provide a flexible and effective manner of leveraging representation from experts based on multimodal context and model expertise, further enhancing the model generalization ability. As presented in Table 1, MoVA can preserve the optimal performance of a single relevant vision encoder by ignoring non-relevant experts on the GQA, POPE, and REC task. Besides, MoVA can further boost performances via the fine-grained fusion of multiple relevant vision experts on other tasks. ", "page_idx": 1}, {"type": "text", "text": "We conduct comprehensive experiments on various benchmarks to evaluate the effectiveness of MoVA, including MLLM benchmarks, visual question answering (VQA), visual grounding, and biomedical understanding. Without any bells and whistles, MoVA can achieve significant performance gains over current state-of-the-art methods. ", "page_idx": 2}, {"type": "text", "text": "The contributions of this work are three-fold: (i) By analyzing the performance of individual vision encoders versus the plain fusion of multiple encoders across various tasks, we reveal that the inherent bias of each vision encoder can diminish its generalization ability across other irrelevant domains. (ii) We propose MoVA, a powerful MLLM composed of coarse-grained context-aware expert routing and fine-grained expert fusion with MoV-Adapter. Based on multimodal context and model expertise, MoVA fully leverages representation from multiple context-relevant vision experts flexibly while avoiding biased information of irrelevant experts. (iii) We demonstrate the effectiveness of each component in MoVA by elaborate ablation studies. MoVA can achieve significant performance gains over state-of-the-art methods in a wide range of challenging benchmarks. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Multimodal architectures [1, 3, 6, 30, 31, 32, 33, 34], optimization paradigm [35, 36], applications [37, 38, 39, 40], and benchmarks [41, 42, 43, 44, 45, 46, 47] have recently achieved remarkable progress and garnered unprecedented attention within the academic community. Multimodal large language models (MLLMs) usually leverage the alignment from visual features to the linguistic feature space to achieve superior vision-language understanding capabilities based on off-the-shelf LLMs and vision encoders. CLIP vision encoder [11], which is trained in contrastive learning from billions of diverse image-text pairs [48, 49], is widely used among these works. For example, LLaVA [3] adopts an MLP projector to align visual tokens from the frozen CLIP vision encoder to the embedding layer of LLM. However, The representation from CLIP exhibits strong discriminative abilities in classification and recognition but only has limited performance on downstream tasks like location and relation understanding [50]. To break through this bottleneck, some works [4, 51] turn to unlock the CLIP vision encoder and further fine-tune the parameter with training data for downstream tasks. For instance, Qwen-VL [6] collected massive training data for grounding and OCR to jointly optimize the CLIP vision encoder and LLM. Recent works propose to involve an extra frozen vision encoder to enhance the performance of MLLMs. SPHINX [13] is one of the pioneers, where grounding capabilities have been significantly improved with the assistance of the DINOv2 [15]. Vary [12] introduces an extra encoder training on large-scale charts and document data to improve the performance on related downstream tasks. ", "page_idx": 2}, {"type": "text", "text": "3 MoVA Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Overview ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "MoVA comprises five key components: (i) a pre-trained large language model (LLM) that generates accurate responses given the image tokens and instructions; (ii) a base vision encoder; (iii) vision experts that generate task-specific vision latent features; (iv) mixture-of-vision-expert adapter (MoVAdapter) that performs fine-grained expert fusion based on the multimodal context. ", "page_idx": 2}, {"type": "text", "text": "As illustrated in Figure 1, MoVA consists of two stages: coarse-grained context-ware expert routing and fine-grained expert fusion with MoV-Adapter. First, our coarse-grained context-ware expert routing leverages the tool-use capabilities of LLM, routing the most appropriate experts from $N$ expert candidates via LLM to help the model answer the user\u2019s question. In the second stage, we turn to enhance the visual representation with a novel MoV-Adapter module in a fine-grained manner. More specifically, we leverage the mixture-of-expert (MoE) cross-attention layers to extract the taskspecific knowledge of representations from chosen experts. Meanwhile, the dynamic gating network in MoV-Adapter can allocate soft weights to the extracted knowledge of each expert according to the input image and instruction. Then the extracted knowledge can be effectively integrated into the foundational representation of the base vision encoder. Finally, the enhanced visual representation with instruction tokens is fed to the LLM to generate an accurate response. In Section 3.2 and Section 3.3, we will focus on our core contributions, the context-aware expert routing strategy, and the expert fusion with MoV-Adapter. In Section 3.4, we will introduce the training process. ", "page_idx": 2}, {"type": "image", "img_path": "uHs6RJFDsg/tmp/981f36a587f33ad446595af0af2131a74527cdf73aa9d74885aceedfa5b77c6d.jpg", "img_caption": ["Figure 1: The pipeline of MoVA. MoVA performs coarse-to-fine routing to solve a given question. The coarse context-aware expert routing is performed in the first stage to select context-relevant experts. Next, we adopt the MoV-Adapter to extract and fuse the task-specific knowledge from these selected experts in a fine-grained manner. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Pretrained Vision Encoders and LLM. The vision encoders in MoVA consist of a base encoder and multiple task-specific vision encoder experts. We choose the pre-trained CLIP ViT-L-336px as the base encoder. Our vision experts include several state-of-the-art task-specific encoders: DINOv2, CoDETR, SAM, Pix2Struct, Deplot, Vary, and BiomedCLIP. The corresponding expertise is presented in Table 1. For example, both Pix2Struct and Vary will be used when the user asks the MLLM to scan the document image. MoVA is flexible and easy to generalize to all decoder-only LLMs. We mainly consider Vicuna-7B [8], Llama3-8B 3, and Yi-34B [52] as our language models in this work. ", "page_idx": 3}, {"type": "text", "text": "3.2 Coarse-grained Context-aware Expert Routing ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Pipeline of Context-aware Routing. The context-aware expert routing strategy aims to employ the impressive tool-use capacity of LLM to select vision experts with strong relevance to the user\u2019s image and instruction from a model pool. Specifically, we perform the context-aware expert routing in three steps during inference. First, the input image, user questions, and descriptions of expert models are converted into appropriate instructions that prompt the MLLM to perform expert selection. An example of the prompt instruction input and selection output is shown in Table 2. Such a routing task does not require image details and high-resolution input images, hence we directly downsample the base encoder\u2019s visual feature to obtain a coarse image embedding (e.g., 144 image tokens). The downsampled image tokens and instruction tokens are then fed to the LLM as inputs. Finally, the LLM generates the output text and we parse it to determine which vision expert should be selected for fine-grained knowledge extraction in the second stage. For instance, as depicted in Table 2, the LLM directly outputs the option\u2019s letter of DINOv2 and Pix2Struct, thus we only utilize them for the subsequent extraction. During training, we do not perform context-aware expert routing and replace the routing outputs with our routing annotations to improve efficiency. ", "page_idx": 3}, {"type": "text", "text": "Routing Data Construction. Compared with other MLLMs, MoVA requires additional routing annotations. We first introduce the formal definition of the data structure for an unambiguous understanding of the routing data. The data structure for expert routing introduces additional routing annotation $\\mathcal{R}$ to the conventional multimodal data $({\\mathcal{T}},{\\mathcal{Q}},A)$ . Here, $\\mathcal{T}$ represents the image, $\\mathcal{Q}$ and $\\boldsymbol{\\mathcal{A}}$ refer to the question-answer pair, and $\\mathcal{R}$ refers to the expert set which contains the most appropriate ones to solve this question. Then the construction process for routing data can be formulated as $(\\mathbb{Z},\\mathbb{Q},{\\mathcal{A}})\\to\\mathcal{R}$ , with the primary objective being to derive vision experts that optimally align with the sample $({\\mathcal{T}},{\\mathcal{Q}},A)$ . Intuitively, the language modeling loss can serve as an effective metric for evaluating how a data sample aligns with the vision expert. Specifically, we can reuse the LLaVA-1.5- 7B models with various vision encoders presented in Section 1 to perform loss computation. Here, we denote the model with the base encoder as $\\mathcal{M}_{0}$ and the model with $j$ -th expert among $N$ experts as ", "page_idx": 3}, {"type": "text", "text": "Table 2: One example of the instruction-following data for context-aware expert routing. We present the multimodal inputs in the top block and the language response in the bottom block. The detailed model descriptions are released in the Appendix. ", "page_idx": 4}, {"type": "image", "img_path": "uHs6RJFDsg/tmp/0552472cc9d99f58f4c2b8af7d89e2b2935fa1d843df2c80b0cf6c78eeabf9b7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "$\\mathcal{M}_{j}$ . For the $i^{\\th}$ -th sample $(\\mathcal{T}_{i},\\mathcal{Q}_{i},\\mathcal{A}_{i})$ , we send it to models $\\{\\mathcal{M}_{j}|j\\in\\{0,1,\\ldots,N\\}\\}$ and calculate the language modeling loss $\\{{\\mathcal{L}}_{i,j}|j\\in\\{0,1,\\ldots,N\\}\\}$ . The $j$ -th expert is regarded as a useful expert for the $i$ -th sample only if ${\\mathcal L}_{i,j}\\,<\\,{\\mathcal L}_{i,0}$ and will be added to the routing set $\\mathcal{R}_{i}$ . Note that we only keep up to 3 vision experts to avoid computation costs brought by too many additional experts. All the routing annotations of our training data are generated offline. We can directly parse and input these offline results to the subsequent expert fusion component during training. ", "page_idx": 4}, {"type": "text", "text": "Routing Data Augmentation. To preserve the expert routing robustness and generalization ability in open scenarios, we only randomly select 2K samples for training, remove the model name in model description, and rewrite the model descriptions using ChatGPT [53] for each expert. We also shuffle the model pool and randomly truncate the model pool during training. ", "page_idx": 4}, {"type": "text", "text": "3.3 Fine-grained Expert Fusion with MoV-Adapter ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We propose the MoV-Adapter to facilitate fine-grained expert representation extraction and integration based on multimodal context. As shown in Figure 2, the MoV-Adapter consists of $L$ adapter blocks and a text encoder. Each block contains an expert knowledge extractor, a dynamic gating network, and a transformer block. For the $i$ -th block, the input feature is denoted as $\\mathbf{\\dot{X}}^{i}\\in\\mathbb{R}^{\\tilde{C}\\times H\\times\\Breve{W}}$ and we take the CLIP base encoder feature $\\mathbf{X}\\in\\mathbb{R}^{C\\times H\\times W}$ as the input feature ${\\bf X}^{1}$ of the first block. We use $\\mathbf{G}$ to indicate the indices of chosen $K$ experts. The expert feature set is $\\{\\mathcal{F}_{j}|j\\in\\mathbf{G}\\}$ . The final output feature of $L$ adapter blocks is ${\\bf X}^{L+1}$ . Additionally, we apply two residual blocks [54] with an average pooling to ${\\bf X}^{L+1}$ to obtain a coarser image feature $\\mathbf{X}_{o u t}^{\\bar{L}+\\bar{1}}\\in\\mathbb{R}^{C\\times\\frac{H}{2}\\times\\frac{W}{2}}$ , which is further connected to the LLM text embedding space by an MLP layer. ", "page_idx": 4}, {"type": "text", "text": "Text Encoder. We introduce a pre-trained BERT as the text encoder to extract language context information from the user\u2019s instruction. We take the [CLS] token from the output of the text encoder as the text token $\\mathbf{X}_{T}\\in\\mathbb{R}^{C_{T}}$ . It is worth noting that all the adapter blocks share the same text token. ", "page_idx": 4}, {"type": "text", "text": "Expert Knowledge Extractor. We adopt $N$ cross-attention layers as the expert knowledge extractor to achieve efficient knowledge extraction. Note that only the expert features $\\{\\mathcal{F}_{j}|j\\in\\bar{\\mathbf{G}}\\}$ and their corresponding cross-attention layers are involved in the extraction. For each selected expert feature $\\mathcal{F}_{j}\\in\\dot{\\mathbb{R}}^{C_{j}\\times H_{j}^{\\smile}\\times W_{j}}$ , we first align its resolution to $\\mathbf{X}^{i}$ with bilinear interpolation: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{F}}_{j}=\\mathrm{Interpolate}(\\mathcal{F}_{j},H,W).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "image", "img_path": "uHs6RJFDsg/tmp/cc93439f5c4a91f09c69059d95c022225de4ba46c0bd9c174fed9b539ad3966c.jpg", "img_caption": ["Figure 3: The training strategy of MoVA. We enhance the task-specific knowledge extraction capacity in the first stage. Then, we excite model multimodal capacities in the last stage. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "For the $i$ -th MoV-Adapter block and the $j$ -th cross-attention layer, we take input feature $\\mathbf{X}^{i}$ as query, and the aligned expert feature $\\hat{\\mathcal{F}}_{j}$ as the key and value: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{Y}_{j}^{i}=\\mathbf{X}^{i}+\\operatorname{Attention}(\\mathbf{X}^{i},\\hat{\\mathcal{F}}_{j}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Dynamic Gating Network. We employ a dynamic gating network to contribute to a fine-grained knowledge integration process for the conditional representation $\\{\\mathbf{Y}_{j}^{i}\\bar{|}j\\in\\bar{\\mathbf{G}}\\}$ . It is implemented with the softmax over the logits of an MLP layer, processing multimodal representation to generate expert-wise soft weight $\\mathbf{P}^{i}\\,\\in\\,\\mathbb{R}^{K}$ for the output of each cross-attention layer in the extractor. Specifically, the input to the gating network is the concatenated vector of a visual token $\\mathbf{X}_{V}^{i}\\in\\mathbb{R}^{\\tilde{C}}$ and the text token $\\mathbf{X}_{T}\\in\\mathbb{R}^{C_{T}}$ . We obtain $\\mathbf{X}_{V}^{i}$ with a global average pooling operation to $\\mathbf{X}^{i}$ . Then we concatenate them to compute the gating weights and the expert-wise outputs by computing the weighted sum: ", "page_idx": 5}, {"type": "image", "img_path": "uHs6RJFDsg/tmp/7515af8943615938f0ef3970553805a60cc7d3b5ffe736647ce7e6f3584c46d4.jpg", "img_caption": ["Figure 2: MoV-Adapter architecture. "], "img_footnote": [], "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{X}}^{i}=\\sum_{j\\in\\mathbf{G}}\\mathbf{Y}_{j}^{i}\\cdot\\mathbf{P}_{j}^{i},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{P}_{j}^{i}\\in(0,1)$ is the soft weight for the $j$ -th expert in the $i$ -th block. ", "page_idx": 5}, {"type": "text", "text": "Transformer Block. The transformer block in the adapter block follows the vanilla design, consisting of a self-attention layer and an FFN layer. Taking the fused visual representation ${\\hat{\\mathbf{X}}}^{i}$ , its output will serve as the input feature $\\mathbf{X}^{i+1}$ for the next adapter block. ", "page_idx": 5}, {"type": "text", "text": "3.4 Training Paradigm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As shown in Figure 3, the training process of MoVA consists of pretraining and supervised finetuning. ", "page_idx": 5}, {"type": "text", "text": "Pretraining. To improve multimodal generalization, we first construct 15M visual instruction samples across diverse domains as the training data: (i) Image caption data that covers 4M randomly selected samples from DataComp-1B [55], ShareGPT4V-PT [51], and ALLaVA-4V [56]. (ii) Visual grounding and localization dataset that encompasses Objects365 [57], RefCOCO [21], VisualGenome [58], PointQA [59], and Flickr30K [60]. (iii) Chart understanding data that includes MMC-Instruction [61], Chart2Text [62], DVQA [63], and SciGraphQA [64]. (iv) Text recognition and document parsing data that covers LLaVAR-PT [65] and 3M English document images from Common Crawl 4. (v) LLaVAMed [66] for biomedical image understanding. During the pretraining phase, we only optimize the MoV-Adapter along with the base vision encoder while preserving the capabilities of the initial large language model. Meanwhile, we leverage the routing annotations generated via the method proposed in Section 3.2 to choose experts and ignore representations from irrelevant ones during training. ", "page_idx": 5}, {"type": "text", "text": "Supervised Finetuning. We utilize high-quality visual instruction tuning data that build upon LLaVA-665K [28] for finetuning. Additionally, we integrate several visual question answering datasets across various domains, such as DocVQA [17], ChartQA [18], InfographicVQA [67], AI2D [68], ST-VQA [69], TextVQA [70], SynthDoG-en [71], Geometry3K [72], PGPS9K [73], Geo170K [74], RefCOCO, LLaVA-Med, VQA-RAD [75], and SLAKE [22]. We also encompass equivalent comprehensive captions [51, 56, 76, 77] generated by the advanced GPT4-V [53] for improved world knowledge. Apart from the above instruction tuning data, we convert the selected 2K routing annotations to instructions and incorporate them into the training data. In the supervised finetuning stage, only task-specific vision experts are frozen and we jointly optimize other components. The objective of supervised fine-tuning is to align the visual representation and the embedding of LLM, boosting its visual instruction-following capabilities. ", "page_idx": 5}, {"type": "table", "img_path": "uHs6RJFDsg/tmp/70f5b23e73a13feebe8d1da4d220004420b2d4d2d4ed54a8619fee04fb00f0b4.jpg", "table_caption": ["Table 3: Performance comparison with current state-of-the-art frameworks on popular MLLM benchmarks. PT and SFT indicate the number of multimodal training samples in pretraining and finetuning stage. #IMG means the number of image tokens processed by LLM. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Implementation Details ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As mentioned in Section 3.4, our training pipeline consists of two stages. In the pretraining stage, we use the AdamW optimizer with an initial learning rate of $2\\!\\times\\!10^{-4}$ , a batch size of 1024, and train the model for 1 epoch. We jointly finetune the weights of all components except additional vision experts with a batch size of 128 and an initial learning rate of $2\\!\\times\\!1\\bar{0}^{-5}$ during supervised finetuning. We use 3 transformer blocks $\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!L\\!\\!\\!\\!=\\!\\!\\!\\!3)$ in the MoV-Adapter and its hidden dimension is 1024, which is consistent with the base vision encoder CLIP. The input resolution of the base vision encoder is $672\\!\\times\\!672$ . Two residual blocks with an average pooling are employed in the MoV-Adapter to reduce the number of output image tokens from 2304 to 576. For the experiment performed in Table 1, we follow the default setting of LLaVA-1.5 but incorporate several additional datasets, including DocVQA [17], ChartQA [18], RefCOCO [21], LLaVA-Med [66], VQA-RAD [75], and SLAKE [22]. More details about vision experts, ablations, and analysis are available in Appendix A.1 and A.3. ", "page_idx": 6}, {"type": "text", "text": "4.2 MLLM Benchmarks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We empirically analyze the multimodal capacity and generalization ability of MoVA on a wide range of challenging MLLM benchmarks in Table 3. This comprehensive assessment is conducted on MME [81], MMBench [16], QBench [82], MathVista [83], MathVerse [84], and POPE [20]. Compared to other open-source MLLMs with similar model complexity, MoVA with Vicuna-7B achieves the best performance across 7 MLLM benchmarks while offering a more favorable balance between training efficiency and performance. For instance, MoVA-7B surpasses the recent state-ofthe-art LLaVA-NeXT-7B [80] with a dynamic high resolution design, processing only $20\\%$ image tokens. Furthermore, we adopt Hermes-Yi-34B [85] as the LLM to validate the scaling property of MoVA. As depicted in Table 3, the performance of MoVA-34B is on par with popular proprietary MLLMs (e.g., Gemini-Pro [78]) and outperforms Qwen-VL-Plus [6] on 5 MLLM benchmarks. For example, MoVA establishes new records on MMBench and MMBench-CN, even surpassing the GPT-4V [53]. These results suggest that the ensemble of vision experts with adaptive expert routing can serve as an effective dimension for MLLM model scaling. ", "page_idx": 6}, {"type": "table", "img_path": "uHs6RJFDsg/tmp/04739f100b3f4e6cd9fb5f2a3052ae6a74d4bb1809832ca17ae432c3f9b43c24.jpg", "table_caption": ["Table 4: Performance comparison on VQA benchmarks. We present the number of model parameters of each MLLM for a clear complexity comparison. \\* denotes zero-shot evaluation. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "uHs6RJFDsg/tmp/3b96ac1b16afc5f1aceb29d286b9c972e967eb25967fdb8980aaddcc4632352f.jpg", "table_caption": ["Table 5: Performance comparison $\\left(\\mathbf{Acc}@\\mathbf{0.5}\\right)$ on RefCOCO REC task. Specialists are specifically designed for the grounding task or finetuned on RefCOCO data. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.3 Visual Question Answering ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The evaluation results on VQA benchmarks are outlined in Table 4. We divide these benchmarks into general VQA benchmarks [88, 19, 89] and text-oriented VQA benchmarks [70, 18, 17, 68]. Thanks to the dynamic and efficient task-specific knowledge extraction, MoVA achieves state-ofthe-art performances across diverse VQA benchmarks. For general VQA benchmarks, MoVA-7B outperforms SPHINX-2k [4] equipped with Vicuna-13B on VQAv2 [88] and GQA by $4.2\\%$ and $1.9\\%$ , respectively. Besides, MoVA shows its proficiency in text recognition in various scenarios, encompassing scene text, chart, document, and diagram. For instance, MoVA-7B catches up to the current state-of-the-art generalist CogAgent [86] with 18 billion parameters on these text-oriented benchmarks with smaller model size. The MoVA model with 38B parameters even surpasses the wellestablished specialist model PALI-X-55B [87] by clear margins. These outstanding performances demonstrate MoVA\u2019s robust generalization capabilities across diverse domains. ", "page_idx": 7}, {"type": "text", "text": "4.4 Visual Grounding ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct experiments on Referring Expression Comprehension (REC) benchmarks [21] to evaluate the visual grounding ability of MoVA. The results are presented in Table5. The performance of MoVA-7B is on par with the state-of-the-art specialist models that are elaborately designed for grounding tasks. For example, MoVA-7B achieves a score of $90.22\\%$ on $\\mathbf{RefCOCO+}$ val, which is $2.46\\%$ higher than the score of UNINEXT-H [92]. Our largest model MoVA-34B further pushes the performance bound of visual grounding on these benchmarks. These impressive results demonstrate MoVA\u2019s remarkable visual grounding capacity. ", "page_idx": 7}, {"type": "table", "img_path": "uHs6RJFDsg/tmp/ff6a260943f3fa3e7dec3901f0f833e44365661c231007d2c10fabd5686bfa67.jpg", "table_caption": ["Table 6: Comparisons on the biomedical VQA datasets. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "uHs6RJFDsg/tmp/5fb31b2d50808ce70f0eaad4fdba76b3bdc0867c300e749fbb93413af717e6e6.jpg", "table_caption": ["Table 7: Results of component-wise ablation studies. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "uHs6RJFDsg/tmp/78f9abbe32b0de70f2f17b857595fe790775e6f6277ba6636e7ff15ab9ea37ce.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "uHs6RJFDsg/tmp/94cd0cbee822532e15f6429fea9f7201fc6e135edb7bd5d91f0ccc2f434fd179.jpg", "table_caption": ["Table 9: Comparisons of expert routing criteria. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "uHs6RJFDsg/tmp/52e02eaa261b37a23a121f5a170a3d70fb05e45238e4d3ca491a2bc11d2a9f2e.jpg", "table_caption": ["Table 10: Open-world ex- Table 11: Performance of varipert routing results. ous MoV-Adapter variants. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.5 Medical Visual Question Answering ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This experiment is conducted on popular medical VQA benchmarks VQA-RAD and SLAKE. We directly leverage the medical VQA evaluation metric adopted by LLaVA-Med. Each sample of VQA-RAD and SLAKE is observed only once during the training process of MoVA and LLaVA-1.5. For a fair comparison, we compare MoVA with the LLaVA-Med variant that is finetuned with only 1 epoch on the benchmark. The performance of the LLaVA-Med specialist that is fully finetuned on downstream tasks is also reported. As presented in Table 6, MoVA-7B consistently yields higher scores than LLaVA-Med and LLaVA-1.5, exhibiting its medical visual chat ability. ", "page_idx": 8}, {"type": "text", "text": "4.6 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Component-wise analysis. As presented in Table 7, we perform an ablation to thoroughly delve into the effect of each component. First, we try to replace the context-aware routing with random routing. Without task-relevant vision experts, the performance drops by a large margin, especially on the textoriented VQA benchmarks. Removing context-aware routing to leverage all vision experts also leads to similar results. It proves that both these modifications introduce biased information from irrelevant vision experts due to the removal of context-aware routing. Then, we ablate the effectiveness of the MoV-Adapter by replacing it with simple linear layers. The removal of fine-grained expert feature fusion downgrades performance across all datasets. These results delineate that each component in MoVA can consistently yield significant gains. ", "page_idx": 8}, {"type": "text", "text": "Number of activated experts. In the context-aware routing phase, the number of activated experts $K$ is dynamic. We compare such a data-dependent design with other variations of constant $K$ in this experiment. As presented in Table 8, the overall performance of dynamic $K$ consistently outperforms other models with constant $K$ . This reveals this dynamic implementation can fully exploit the task-specific knowledge of relevant experts while avoiding the incorporation of biased information. ", "page_idx": 8}, {"type": "text", "text": "Criteria for choosing better experts. To reduce the costs, our method only adopts $\\binom{N}{1}$ models with $N$ various encoders to identify the better vision expert separately. However, we do not explicitly consider the combination of the chosen vision experts. In this experiment, we compare our method with another strategy that considers vision encoders combination and enumerates $\\bar{\\sum}_{i=1}^{3}\\binom{N}{i}$ models for routing data construction. Specifically, we set $N\\,=\\,4$ and employ a smaller model pool of DINOv2, Co-DETR, Pix2Struct, and Deplot to reduce training costs. As shown in Table 9, our method achieves comparable performance while requiring much less models for data construction. ", "page_idx": 8}, {"type": "text", "text": "Expert routing in open scenarios. We develop 105 human-verified testing samples that should be answered using novel experts for the expert routing task. These novel experts encompass 7 vision models [93, 71, 94, 91, 95, 54, 96] on various computer vision tasks and each expert corresponds to 15 evaluation samples. We manually check the correctness of the expert routing result. As presented in Table 10, a lightweight network, such as a MLP classifier fails to generalize to this open-world setting. Besides, increasing the routing training samples and removing data augmentations also lead to severe performance degradation. The results demonstrate our coarse-grained context-aware routing preserves the generalization ability for expert routing in open scenarios. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Adapter Design. In this section, we conduct ablation studies on the design of the MoV-Adapter. As presented in Table 11, we compared the impact of using 2, 3, and 4 adapter blocks on the model\u2019s performance. We observed that the baseline with 3 blocks can achieve better performance than other settings. Then, we substituted our multimodal gating for uniform gating to investigate its effectiveness. Each of the experts is assigned the same soft weight in the uniform gating. We find uniform gating brings consistent performance drops in the test benchmarks. It indicates that the lack of the dynamic soft-weight harms the overall performance since it fails to perform precise knowledge extraction. ", "page_idx": 9}, {"type": "text", "text": "Inference Analysis. As illustrated in Figure 1, MoVA consists of two stages: coarse-grained contextware expert routing and fine-grained expert fusion with MoV-Adapter. This two-stage inference pipeline can be further broken down into five steps: (i) Data preprocessing. We first process the input image with image processors and convert the input text into a token sequence with the LLM tokenizer. (ii) Base encoder forward. We extract the base image feature using the base CLIP encoder. Note that we only run the base encoder once since its output feature can be preserved and reused in the fourth step. (iii) LLM routing generation. We compress the base image features into 144 image tokens. The LLM generates a concise routing answer based on the compressed image feature and routing instruction. Vision experts and MoV-Adapter forward. (iv) According to the multimodal context and routing results generated in the previous step, we fuse vision features of the base encoder and activated experts in a coarse-to-fine manner. (v) LLM response generation. The LLM generates the final response given the fused vision features and user instructions. To investigate the inference efficiency of each step, we randomly select 200 images from the COCO val2017 dataset and adopt the common image caption instruction: Describe this image. The temperature for generation is 0. The latency is measured using bfloat16 and flash-attention 2 on an A100 80G GPU. We present the average inference latency of each step and show the average sequence length of the routing output and final response. The inference latencies for each step are 0.19s, 0.05s, 0.14s, 0.07s, and 10.24s, respectively. The average length of the routing output is 3.24 tokens, while the average length of the final response is 405.06 tokens. Compared to the LLM response generation (Step 5), the LLM expert routing (Step 3) generates much fewer output tokens and its latency is negligible (0.14s v.s. 10.24s). Therefore, our method does not bring significant inference costs. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we reveal that the inherent bias of each vision encoder can diminish its generalization ability across other irrelevant domains by analyzing the performance of individual vision encoders versus the plain fusion of multiple encoders across various tasks. To deal with the problem, we propose MoVA, a powerful MLLM composed of coarse-grained context-aware expert routing and fine-grained expert fusion with MoV-Adapter. Based on multimodal context and model expertise, MoVA fully leverages representation from multiple context-relevant vision encoder experts flexibly and effectively while avoiding biased information brought by irrelevant experts. MoVA can achieve significant performance gains over current state-of-the-art methods in a wide range of benchmarks. ", "page_idx": 9}, {"type": "text", "text": "Limitations. We acknowledge some limitations in our paper that require attention. One limitation is the hallucination, which refers to the generation of text that appears plausible or coherent but is factually incorrect and misleading. This issue potentially presents in all powerful MLLMs. Additionally, the performance may be affected by failure cases of the context-relevant vision experts, leading to potential degradation. We plan to explore solutions for these limitations in future works. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The work was supported by the National Key R&D Program of China under Grant 2021ZD0201300. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 19730\u201319742. PMLR, 2023.   \n[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716\u201323736, 2022.   \n[3] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024.   \n[4] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023.   \n[5] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm\u2019s referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023.   \n[6] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023.   \n[7] Wenliang Dai, Junnan Li, Dongxu Li, AnthonyMeng Huat, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose visionlanguage models with instruction tuning. arXiv preprint arXiv:2305.06500, 2023.   \n[8] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90\\%*$ chatgpt quality, March 2023.   \n[9] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.   \n[10] Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, et al. Map-neo: Highly capable and transparent bilingual large language model series. arXiv preprint arXiv:2405.19327, 2024.   \n[11] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[12] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and Xiangyu Zhang. Vary: Scaling up the vision vocabulary for large visionlanguage models. arXiv preprint arXiv:2312.06109, 2023.   \n[13] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et al. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575, 2023.   \n[14] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209, 2024.   \n[15] Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.   \n[16] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023.   \n[17] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 2200\u20132209, 2021.   \n[18] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: A benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022.   \n[19] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6700\u20136709, 2019.   \n[20] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023.   \n[21] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expressions. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 69\u201385. Springer, 2016.   \n[22] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. Slake: A semanticallylabeled knowledge-enhanced dataset for medical visual question answering. In 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), pages 1650\u20131654. IEEE, 2021.   \n[23] Zhuofan Zong, Guanglu Song, and Yu Liu. Detrs with collaborative hybrid assignments training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6748\u20136758, 2023.   \n[24] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023.   \n[25] Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct: Screenshot parsing as pretraining for visual language understanding. In International Conference on Machine Learning, pages 18893\u201318912. PMLR, 2023.   \n[26] Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, and Yasemin Altun. Deplot: One-shot visual language reasoning by plot-to-table translation. arXiv preprint arXiv:2212.10505, 2022.   \n[27] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao, Mu Wei, Naveen Valluri, Cliff Wong, et al. Large-scale domain-specific pretraining for biomedical vision-language processing. arXiv preprint arXiv:2303.00915, 2023.   \n[28] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023.   \n[29] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master $16000+$ real-world apis. arXiv preprint arXiv:2307.16789, 2023.   \n[30] Hao Shao, Yuxuan Hu, Letian Wang, Steven L Waslander, Yu Liu, and Hongsheng Li. Lmdrive: Closed-loop end-to-end driving with large language models. arXiv preprint arXiv:2312.07488, 2023.   \n[31] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Unleashing chain-of-thought reasoning in multi-modal language models. arXiv preprint arXiv:2403.16999, 2024.   \n[32] Yang Jiao, Shaoxiang Chen, Zequn Jie, Jingjing Chen, Lin Ma, and Yu-Gang Jiang. Lumen: Unleashing versatile vision-centric capabilities of large multimodal models. arXiv preprint arXiv:2403.07304, 2024.   \n[33] Bingqi Ma, Zhuofan Zong, Guanglu Song, Hongsheng Li, and Yu Liu. Exploring the role of large language models in prompt encoding for diffusion models, 2024.   \n[34] Hongcheng Guo, Jian Yang, Jiaheng Liu, Liqun Yang, Linzheng Chai, Jiaqi Bai, Junran Peng, Xiaorong Hu, Chao Chen, Dongfeng Zhang, et al. Owl: A large language model for it operations. arXiv preprint arXiv:2309.09298, 2023.   \n[35] Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. Aligning modalities in vision large language models via preference fine-tuning. arXiv preprint arXiv:2402.11411, 2024.   \n[36] Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, and Huaxiu Yao. Calibrated self-rewarding vision language models. arXiv preprint arXiv:2405.14622, 2024.   \n[37] Dongzhi Jiang, Guanglu Song, Xiaoshi Wu, Renrui Zhang, Dazhong Shen, Zhuofan Zong, Yu Liu, and Hongsheng Li. Comat: Aligning text-to-image diffusion model with image-to-text concept matching. arXiv preprint arXiv:2404.03653, 2024.   \n[38] Peng Xia, Kangyu Zhu, Haoran Li, Hongtu Zhu, Yun Li, Gang Li, Linjun Zhang, and Huaxiu Yao. Rule: Reliable multimodal rag for factuality in medical vision language models. arXiv preprint arXiv:2407.05131, 2024.   \n[39] Peng Xia, Kangyu Zhu, Haoran Li, Tianze Wang, Weijia Shi, Sheng Wang, Linjun Zhang, James Zou, and Huaxiu Yao. Mmed-rag: Versatile multimodal rag system for medical vision language models. arXiv preprint arXiv:2410.13085, 2024.   \n[40] Zaiquan Yang, Yuhao Liu, Jiaying Lin, Gerhard Hancke, and Rynson WH Lau. Boosting weakly-supervised referring image segmentation via progressive comprehension. arXiv preprint arXiv:2410.01544, 2024.   \n[41] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9556\u20139567, 2024.   \n[42] Yian Li, Wentao Tian, Yang Jiao, Jingjing Chen, and Yu-Gang Jiang. Eyes can deceive: Benchmarking counterfactual reasoning abilities of multi-modal large language models. arXiv preprint arXiv:2404.12966, 2024.   \n[43] Jiacheng Zhang, Yang Jiao, Shaoxiang Chen, Jingjing Chen, and Yu-Gang Jiang. Eventhallusion: Diagnosing event hallucinations in video llms. arXiv preprint arXiv:2409.16597, 2024.   \n[44] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanmin Wu, Jiayi Lei, Pengshuo Qiu, Pan Lu, Zehui Chen, Guanglu Song, Peng Gao, et al. Mmsearch: Benchmarking the potential of large models as multi-modal search engines. arXiv preprint arXiv:2409.12959, 2024.   \n[45] Peng Xia, Ze Chen, Juanxi Tian, Yangrui Gong, Ruibo Hou, Yue Xu, Zhenbang Wu, Zhiyuan Fan, Yiyang Zhou, Kangyu Zhu, et al. Cares: A comprehensive benchmark of trustworthiness in medical vision language models. arXiv preprint arXiv:2406.06007, 2024.   \n[46] Peng Xia, Siwei Han, Shi Qiu, Yiyang Zhou, Zhaoyang Wang, Wenhao Zheng, Zhaorun Chen, Chenhang Cui, Mingyu Ding, Linjie Li, et al. Mmie: Massive multimodal interleaved comprehension benchmark for large vision-language models. arXiv preprint arXiv:2410.10139, 2024.   \n[47] Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Jian Yang, et al. Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. arXiv preprint arXiv:2310.00746, 2023.   \n[48] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.   \n[49] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:25278\u201325294, 2022.   \n[50] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: A comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024.   \n[51] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023.   \n[52] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024.   \n[53] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[54] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[55] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. Advances in Neural Information Processing Systems, 36, 2024.   \n[56] Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4vsynthesized data for a lite vision-language model. arXiv preprint arXiv:2402.11684, 2024.   \n[57] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 8430\u20138439, 2019.   \n[58] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:32\u201373, 2017.   \n[59] Arjun Mani, Nobline Yoo, Will Hinthorn, and Olga Russakovsky. Point and ask: Incorporating pointing into visual question answering. arXiv preprint arXiv:2011.13681, 2020.   \n[60] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, pages 2641\u20132649, 2015.   \n[61] Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen, Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong Yu. Mmc: Advancing multimodal chart understanding with large-scale instruction tuning. arXiv preprint arXiv:2311.10774, 2023.   \n[62] Shankar Kantharaj, Rixie Tiffany Leong, Xiang Lin, Ahmed Masry, Megh Thakkar, Enamul Hoque, and Shafiq Joty. Chart-to-text: A large-scale benchmark for chart summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4005\u20134023, 2022.   \n[63] Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5648\u20135656, 2018.   \n[64] Shengzhi Li and Nima Tajbakhsh. Scigraphqa: A large-scale synthetic multi-turn questionanswering dataset for scientific graphs. arXiv preprint arXiv:2308.03349, 2023.   \n[65] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. Llavar: Enhanced visual instruction tuning for text-rich image understanding. arXiv preprint arXiv:2306.17107, 2023.   \n[66] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision assistant for biomedicine in one day. Advances in Neural Information Processing Systems, 36, 2024.   \n[67] Minesh Mathew, Viraj Bagal, Rub\u00e8n Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1697\u20131706, 2022.   \n[68] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is worth a dozen images. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11\u201314, 2016, Proceedings, Part IV 14, pages 235\u2013251. Springer, 2016.   \n[69] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Mar\u00e7al Rusinol, Ernest Valveny, CV Jawahar, and Dimosthenis Karatzas. Scene text visual question answering. In Proceedings of the IEEE/CVF international conference on computer vision, pages 4291\u20134301, 2019.   \n[70] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8317\u20138326, 2019.   \n[71] Geewook Kim, Teakgyu Hong, Moonbin Yim, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Donut: Document understanding transformer without ocr. arXiv preprint arXiv:2111.15664, 7:15, 2021.   \n[72] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165, 2021.   \n[73] Ming-Liang Zhang, Fei Yin, and Cheng-Lin Liu. A multi-modal neural geometric solver with textual clauses parsed from diagram. arXiv preprint arXiv:2302.11097, 2023.   \n[74] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric problem with multi-modal large language model. arXiv preprint arXiv:2312.11370, 2023.   \n[75] Jason J Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. A dataset of clinically generated visual questions and answers about radiology images. Scientific data, 5(1):1\u201310, 2018.   \n[76] LAION. Gpt-4v dataset. https://huggingface.co/datasets/laion/gpt4v-dataset, 2023.   \n[77] Carter Jimmy. Textocr-gpt4v. https://huggingface.co/datasets/jimmycarter/ textocr-gpt4v, 2024.   \n[78] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   \n[79] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. arXiv preprint arXiv:2311.04257, 2023.   \n[80] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024.   \n[81] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023.   \n[82] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, et al. Q-bench: A benchmark for general-purpose foundation models on low-level vision. arXiv preprint arXiv:2309.14181, 2023.   \n[83] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023.   \n[84] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? arXiv preprint arXiv:2403.14624, 2024.   \n[85] 01-AI. Yi. https://huggingface.co/01-ai, 2023.   \n[86] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: A visual language model for gui agents. arXiv preprint arXiv:2312.08914, 2023.   \n[87] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, et al. Pali-x: On scaling up a multilingual vision and language model. arXiv preprint arXiv:2305.18565, 2023.   \n[88] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904\u20136913, 2017.   \n[89] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022.   \n[90] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. arXiv preprint arXiv:2310.07704, 2023.   \n[91] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023.   \n[92] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Zehuan Yuan, and Huchuan Lu. Universal instance perception as object discovery and retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15325\u201315336, 2023.   \n[93] Zhuofan Zong, Dongzhi Jiang, Guanglu Song, Zeyue Xue, Jingyong Su, Hongsheng Li, and Yu Liu. Temporal enhanced training of multi-view 3d object detector via historical object prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3781\u20133790, 2023.   \n[94] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and HeungYeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022.   \n[95] Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. Layoutlmv3: Pre-training for document ai with unified text and image masking. In Proceedings of the 30th ACM International Conference on Multimedia, pages 4083\u20134091, 2022.   \n[96] Zhuofan Zong, Kunchang Li, Guanglu Song, Yali Wang, Yu Qiao, Biao Leng, and Yu Liu. Self-slimmed vision transformer. In European Conference on Computer Vision, pages 432\u2013448. Springer, 2022.   \n[97] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. arXiv preprint arXiv:2308.00692, 2023.   \n[98] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. Advances in Neural Information Processing Systems, 36, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "A Appendix / supplemental material ", "text_level": 1, "page_idx": 17}, {"type": "table", "img_path": "uHs6RJFDsg/tmp/de0bc5a61ed03bc7fc2a727c7e18f681528bd631e6bc85df01a5b2fbc5d8993a.jpg", "table_caption": ["Table 12: Vision expert model configurations of vision experts in MoVA. Methods with \\* use a convolution layer to compress the output feature. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "uHs6RJFDsg/tmp/160c8cb4b7c2be7b9a42fe6367a4622ea43215b009e3a7dc698a19f38f5d259d.jpg", "table_caption": ["Table 13: Introduction of datasets used in the MoV-Adapter pretraining stage. The <class> placeholder represents the object category in the object detection task. The <expr> placeholder represents the expression in the REC task. The <bbox> placeholder denotes the bounding box coordinates. The <point> placeholder denotes the coordinate of a point. We directly use the original question as the instruction for MMC-Instruction and ScigraphQA. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "A.1 Vision Experts ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Model Configuration. We present the detailed model configurations of our task-specific vision experts in 12. We adopt the official checkpoint weights that are publicly available. ", "page_idx": 17}, {"type": "text", "text": "Model Description. The model descriptions used in the routing prompt are released in Table 17. ", "page_idx": 17}, {"type": "text", "text": "A.2 Training Data Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The training process of MoVA consists of two stages. In the Appendix, we present the training datasets with corresponding task templates of the first stage in Table 13. For the training data of the second stage, we follow the prompt format of LLaVA-1.5 [28]. The MoVA models with Vicuna-7B and LLama3-8B are pretrained using 64 A100 80G GPUs for 2 days, and finetuned using 32 A100 80G GPUs for 1 day. The MoVA with 34B LLM is pretrained using 128 A100 80G GPUs for 5 days and finetuned using 64 A100 80G GPUs for 2 days. ", "page_idx": 17}, {"type": "table", "img_path": "uHs6RJFDsg/tmp/44b8f1e1dc5b24db452924ce9372ed50ef47141291ba62097ce3b96798ef704b.jpg", "table_caption": ["Table 14: Performance of various routing component. ", "Table 15: Results of various vision en- Table 16: Effects o coder combination for routing. routing image tokens. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "uHs6RJFDsg/tmp/f9ade5c169ff7ca9d65ea86e791ddfd3566c0d20ce4eda28c63485b8a4c4d074.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "A.3 More Experiments ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Image segmentation. In this experiment, we aim to investigate if task-specific knowledge can improve MoVA on the segmentation task. Therefore, we introduce a simple design to extend MoVA to segmentation tasks. Unlike segmentation generalists [97] that adopt an additional pixel decoder with high-resolution images for high-quality mask generation, we just formulate the referring segmentation task as sequential polygon generation [98]. We finetune MoVA and the baseline with a SAM-Huge [24] backbone on the RefCOCO referring segmentation datasets. MoVA achieves $57.1\\%$ gIoU on the testA benchmark, which is $2.6\\%$ higher than the $54.5\\%$ of baseline. This result indicates that MoVA is capable of exploiting task-specific knowledge to solve segmentation tasks. ", "page_idx": 18}, {"type": "text", "text": "Effect of LLM for expert routing. In this experiment, we investigate the effect of the LLM in our coarse-grained expert routing. As presented in Table 14, expert routing with LLM achieves the best performance. When the LLM is substituted for a lightweight MLP classifier and a BERT encoder, we need to increase the number of routing training samples from 2K to 1.6M to preserve model performance. Besides, both MLP classifier and BERT encoder fail to perform expert routing in open scenarios as stated in Table 10. Therefore, the strong tool-use capacity and generalization ability of LLM is critical to our flexible and effective expert routing. ", "page_idx": 18}, {"type": "text", "text": "Vision encoder for expert routing. In the coarse-grained expert routing, we only adopt the image feature of base vision encoder CLIP. As presented in Table 15, the method with CLIP achieves slightly better performance than other methods since such a routing task does not require elaborate expert knowledge. Besides, incorporating other vision experts with plain fusion also brings biased information and increases cost. To achieve a better trade-off between efficiency and performance, we only use CLIP for coarse-grained expert routing. ", "page_idx": 18}, {"type": "text", "text": "Number of image tokens in expert routing. We ablate the number of image tokens used in the expert routing stage. As shown in Table 16, routing with 144 tokens can achieve comparable performance to methods with more tokens. Considering the additional cost brought by processing more image tokens, we only use 144 tokens for routing. ", "page_idx": 18}, {"type": "text", "text": "Data construction. In this experiment, we analyze the effectiveness of our routing data construction method. The test split is constructed by randomly selecting 500 samples from the routing data. We first prompt GPT4-V [53] to determine whether the expert routing result is reasonable or not. These evaluation results are then manually checked and we finally compute the routing accuracy using the human-verified evaluation results. Our loss-driven data construction approach achieves $94.6\\%$ accuracy (473 samples are regarded as \u201creasonable\u201d by GPT4-V), demonstrating its effectiveness. ", "page_idx": 18}, {"type": "text", "text": "A.4 Qualitative Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We apply MoVA to a wide range of real-world understanding and reasoning tasks to investigate its multimodal comprehension capacity. As presented in Figure 4 and Figure 5, MoVA successfully solves these diverse and complex cases thanks to the context-relevant expert knowledge. ", "page_idx": 18}, {"type": "text", "text": "A.5 Potential Societal Impacts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Positive impacts. The positive impact is to use MoVA to solve various open-world tasks in real-world scenarios. For example, MoVA can serve as a useful assistant to help people improve work efficiency and answer their questions. ", "page_idx": 18}, {"type": "text", "text": "Negative impacts. The potential negative social impact is to use MoVA model to generate misleading and false contents. This issue potentially presents in all multimodal large language models. We will try to improve its safety in future works. ", "page_idx": 18}, {"type": "image", "img_path": "uHs6RJFDsg/tmp/a4e6fbf09db77048dc32156c0da42b921c7e9357df0e1f5514ec09867efd222a.jpg", "img_caption": ["Figure 4: Qualitative multimodal understanding results of MoVA. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "uHs6RJFDsg/tmp/056ee8feb50c9b6e77bc1136a3d49003b4e02350d03883d0ffc44f42163806f1.jpg", "img_caption": ["Figure 5: Qualitative multimodal understanding results of MoVA. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 17: Model descriptions used in the context-aware expert routing. We describe the pros and cons of each expert model in the routing prompt. We only present 3 captions for each expert here. ", "page_idx": 21}, {"type": "text", "text": "DINOv2 description ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "(1) This model demonstrates exceptional prediction capabilities across a range of image-related tasks, including image classification, object detection, segmentation, and image retrieval. The model leverages advanced self-supervised learning techniques to achieve high performance without relying heavily on labeled data. ", "page_idx": 21}, {"type": "text", "text": "(2) This model shows very strong prediction capabilities on tasks such as image classification, detection, segmentation, and image retrieval. However, it encounters challenges in accurately reading text within images. ", "page_idx": 21}, {"type": "text", "text": "(3) This model can effectively extract the accurate spatial and semantic information from natural images. ", "page_idx": 21}, {"type": "text", "text": "Co-DETR description ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "(1) This model is a state-of-the-art object detector pretrained on natural images. It can enable models to solve object-centric problems. Nonetheless, this model struggles with processing background elements in natural scenes.   \n(2) This model is a cutting-edge object detection model that can accurately detect objects in images. However, it struggles with identifying text in images.   \n(3) This model is a state-of-the-art object detector that can identify objects in images. ", "page_idx": 21}, {"type": "text", "text": "SAM description ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "(1) This model is an image segmentation model. This model can segment the precise location of either specific objects in an image or every object in an image.   \n(2) This model is a leading image segmentation framework and achieves strong zero-shot segmentation performance.   \n(3) This model is a promotable segmentation system with zero-shot generalization to unfamiliar objects and images. ", "page_idx": 21}, {"type": "text", "text": "Pix2Struct description ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "(1) This model excels in text recognition, achieving state-of-the-art text analysis results across distinct domains: documents, illustrations, user interfaces, natural images containing text, and images of charts.   \n(2) This model demonstrates exceptional proficiency in text recognition, delivering cutting-edge text analysis performance across various domains.   \n(3) This model can automate the extraction of information from scanned documents, making it easier to digitize and manage large volumes of paperwork. ", "page_idx": 21}, {"type": "text", "text": "Deplot description ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "(1) This model is a specialized model designed to achieve state-of-the-art plot and chart understanding performance. (2) This model is a fine-tuned version of an existing text recognition model. It has been specifically trained to achieve superior performance in plot and chart understanding tasks. (3) This model can help detect the text within the input document, diagram, and chart images. ", "page_idx": 21}, {"type": "text", "text": "Vary description ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "(1) This model can achieve more fine-grained vision perception for images with text, such as document-level Chinese/English OCR, book image to markdown or LATEX, Chinese/English chart understanding.   \n(2) This model can handle images with text effectively and accurately, enabling advanced tasks such as document OCR and chart understanding.   \n(3) This model can accurately process images with text, enabling tasks such as OCR. However, it cannot process natural images without text. ", "page_idx": 21}, {"type": "text", "text": "BiomedCLIP description ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "(1) This model is a foundation model designed for biomedical vision-language processing. (2) This model is capable of biomedical images, such as chest X-ray and radiology images. (3) This model is a state-of-the-art biomedical vision-language model. It has been shown to achieve significant improvements in biomedical image-text tasks. ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 22}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 22}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 22}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 22}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 22}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 22}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We clarify our main claims and contribution in the abstract and introduction section. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We discuss the limitations of MoVA in the conclusion section. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We fully release the implementation details and experiment settings in the Section Experiments and Section Appendix. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The codes and models will be released when the paper is accepted. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 24}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We present the experiment details in Section Experiments and Section Appendix. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [No] ", "page_idx": 25}, {"type": "text", "text": "Justification: We conduct experiments only once following the previous baseline LLaVA [28]. It would be too computationally expensive to conduct the pretraining and finetuning multiple times. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We release the experiments compute resources in the Appendix. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We discuss the potential societal impacts of our work in the Appendix. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Please see Section Experiments. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Please see Section Experiment and supplementary material. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]