[{"figure_path": "snxWD0Q4EI/tables/tables_7_1.jpg", "caption": "Table 1: Pruning results for ViTs (DeiT) using SparseGPT. We report Top-1 accuracy results on the ImageNet validation set.", "description": "This table presents the Top-1 accuracy results on the ImageNet validation set for three different DeiT models (DeiT-Tiny, DeiT-Small, and DeiT-Base) using SparseGPT for pruning.  It shows the accuracy for different numbers of iterations of the I-OBS algorithm, highlighting the impact of iterative pruning on model accuracy.  The results demonstrate that I-OBS consistently improves accuracy across all model sizes, with larger improvements seen in smaller models and quickly saturating for larger models.", "section": "4.2 Applying I-OBS to Model Pruning"}, {"figure_path": "snxWD0Q4EI/tables/tables_8_1.jpg", "caption": "Table 1: Pruning results for ViTs (DeiT) using SparseGPT. We report Top-1 accuracy results on the ImageNet validation set.", "description": "This table presents the Top-1 accuracy results on the ImageNet validation set for three different DeiT models (DeiT-Tiny, DeiT-Small, and DeiT-Base) after applying the SparseGPT pruning method.  The results are shown for different numbers of iterations of the pruning algorithm, including the results for the dense (unpruned) models as a baseline.  The table demonstrates how the accuracy changes as the number of iterations and model size change.", "section": "4.2 Applying I-OBS to Model Pruning"}, {"figure_path": "snxWD0Q4EI/tables/tables_8_2.jpg", "caption": "Table 2: Pruning results for Phi-1.5M using SparseGPT. We report perplexity (the lower, the better).", "description": "This table shows the perplexity scores achieved by three different methods on the WikiText2 and C4 datasets. The three methods are: Dense (the original, unpruned model), SparseGPT (a baseline one-shot pruning method), and I-OBS(3) (the proposed iterative method with 3 iterations).  Lower perplexity indicates better performance.", "section": "4.2 Applying I-OBS to Model Pruning"}, {"figure_path": "snxWD0Q4EI/tables/tables_14_1.jpg", "caption": "Table 3: Performance of I-OBS on Llama-2-7b", "description": "This table shows the performance of the Iterative Optimal Brain Surgeon (I-OBS) algorithm on the Llama-2-7B model for the MMLU (5-shot) task.  The iterations column indicates the number of I-OBS iterations performed, starting from a dense model (iteration 0). The MMLU (5-shot) column shows the performance of the model on the MMLU benchmark after each I-OBS iteration. The best performance is highlighted in bold.", "section": "A.2 Extra experiments"}, {"figure_path": "snxWD0Q4EI/tables/tables_14_2.jpg", "caption": "Table 4: Performance of I-OBS on Llama-3-8B", "description": "This table presents the performance of the Iterative Optimal Brain Surgeon (I-OBS) algorithm on the Llama-3-8B model, measured by the MMLU (5-shot) score. It shows the performance for various numbers of iterations, starting from the dense model (0 iterations).  The MMLU score likely represents performance on the Multi-lingual Language Understanding Evaluation benchmark. The results highlight the I-OBS algorithm's ability to improve model performance with each iteration, suggesting a trade-off between sparsity and performance.", "section": "A.2 Extra experiments"}, {"figure_path": "snxWD0Q4EI/tables/tables_14_3.jpg", "caption": "Table 5: Comparison of I-OBS and CBS at different sparsity levels", "description": "This table compares the performance of the proposed Iterative Optimal Brain Surgeon (I-OBS) algorithm with the Combinatorial Brain Surgeon (CBS) algorithm on the MobileNetV1 model.  The comparison is made across various sparsity levels (30%, 40%, 50%, 60%, 70%, and 80%).  The results show the accuracy obtained by each method at each sparsity level.  The performance of the dense (0% sparsity) model is also provided as a baseline for comparison.", "section": "4.2 Applying I-OBS to Model Pruning"}]