[{"type": "text", "text": "Beyond Primal-Dual Methods in Bandits with Stochastic and Adversarial Constraints ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Martino Bernasconi Matteo Castiglionit Andrea CellitFederico Fusco\\* ", "page_idx": 0}, {"type": "text", "text": "t Bocconi university Politecnico di Milano \\* Sapienza University of Rome {martino.bernasconi,andrea.celli2}@unibocconi.it, matteo.castiglioni@polimi.it, federico.fusco@uniromal.it ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We address a generalization of the bandit with knapsacks problem, where a learner aims to maximize rewards while satisfying an arbitrary set of long-term constraints. Our goal is to design best-of-both-worlds algorithms that perform optimally under both stochastic and adversarial constraints. Previous works address this problem via primal-dual methods, and require some stringent assumptions, namely the Slater's condition, and in adversarial settings, they either assume knowledge of a lower bound on the Slater's parameter, or impose strong requirements on the primal and dual regret minimizers such as requiring weak adaptivity. We propose an alternative and more natural approach based on optimistic estimations of the constraints. Surprisingly, we show that estimating the constraints with an UCBlike approach guarantees optimal performances. Our algorithm consists of two main components: (i) a regret minimizer working on moving strategy sets and (ii) an estimate of the feasible set as an optimistic weighted empirical mean of previous samples. The key challenge in this approach is designing adaptive weights that meet the different requirements for stochastic and adversarial constraints. Our algorithm is significantly simpler than previous approaches, and has a cleaner analysis. Moreover, ours is the first best-of-both-worlds algorithm providing bounds logarithmic in the number of constraints. Additionally, in stochastic settings, it provides $\\widetilde{O}(\\sqrt{T})$ regret without Slater's condition. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We address the problem faced by a decision maker who aims to maximize its cumulative reward over a time horizon $T$ , while satisfying an arbitrary set of $m$ long-term constraints. At each round $t$ ,the learner selects an action $a_{t}$ from a finite set of $K$ actions, and then observes a reward $f_{t}(a_{t})$ and some costs $g_{t}(a_{t})\\in[-1,1]^{m}$ . The goal is to design best-of-both-worlds algorithms for this problem that perform optimally under both stochastic and adversarial constraints. We always assume rewards are generated adversarially. This is because the real complexity of the problem is captured by the nature of the constraints, so that transitioning from adversarial to stochastic rewards under the same type of constraints does not affect our results. ", "page_idx": 0}, {"type": "text", "text": "The first works on bandits with constraints focus on budget constraints, a.k.a bandit with knapsack (BwK) [7] study the settings in which both rewards and constraints are i.i.d. and propose an UCBbased approach, combined with primal-dual method. Agrawal and Devanur [2] provide an UCB-like approach for more general rewards and costs. Immorlica et al. [21], Kesselheim and Singla [22] analyse settings with adversarial constraints and rewards, providing a primal-dual algorithm to tackle the problem. Castiglioni et al. [15] show that a similar primal-dual approach provides best-of-bothworlds guarantees. Many subsequent works extend the setting to more general constraints, mostly employing primal-dual methods [16, 17, 28, 11, 13, 10, 18]. Primal-dual methods have been the only effective method that provides best-of-both-worlds guarantees for bandits with constraints [16, 17, 11, 13, 10]. However, such methods require assumptions that are particularly stringent in settings beyond knapsack constraints. First, they require the existence of a strictly feasible solution (i.e., Slater's condition) to avoid a regret of order $O(T^{3/4})$ [16, 28]. While this assumption always holds in bandits with knapsack setting (where \u201cdoing nothing\u201d incurs in a negative cost equal to the per-round budget), this assumption is far more stringent with general constraints. Moreover, some works require the knowledge of a lower bound on the Slater's parameter [16, 28]. Subsequent works circumvent this assumption at the expense of strong requirements on the primal and dual regret minimizers [17, 11, 13, 1]. In particular, such approaches require weakly-adaptive primal and dual regret minimizers. The challenge of applying such primal-dual algorithms to bandit beyond knapsack constraint is reflected in regret bounds that exhibit non-optimal dependencies on some parameters. For instance, a polynomial (instead of logarithmic) dependence on the number of constraints [17, 11, 13]. For further pointers to the literature, we refer to Appendix A. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "1.1 Our contribution ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We propose an alternative and insightful approach to design best-of-both-worlds algorithms for bandit with long-term constraints. Our method relies on optimistic estimations of the constraints through a weighted empirical mean of past samples. Surprisingly, we demonstrate that using a UCB-based approach to estimate the constraints ensures optimal performance under both stochastic and adversarial constraints. Our algorithm differs significantly from previous UCB-based approaches. For instance, it guarantees no-regret even with adversarial rewards and stochastic constraints, unlike previous works [2, 7, 23]. Moreover, it is the first UCB-like approach that provides an optimal competitiveratioof $1+{^1\\!/\\rho}$ with adversarial constraints, where $\\rho$ is the unknown Slater's parameter. ", "page_idx": 1}, {"type": "text", "text": "Our algorithm consists of two simple components. The first is an adversarial regret minimizer working on moving strategy sets. In particular, at each round, the regret minimizer chooses a strategy in the current optimistic estimation of the feasible set, and is required to achieve no-regret with respect to any strategy in the intersection of all feasibility set estimations. The second component is a tool for estimating the feasible set using an optimistic weighted mean of previous samples. The key challenge in this approach is designing adaptive weights that meet the different requirements for stochastic and adversarial constraints. Intuitively, in stochastic settings, we aim to converge to the (unweighted) empirical mean of the observed constraints. Conversely, in adversarial settings, we should assign larger weights to recent samples to address time-dependent constraints. ", "page_idx": 1}, {"type": "text", "text": "Not only is our algorithm significantly simpler than previous approaches, with a clean and insightful analysis, but it also provides better theoretical performance than primal-dual methods. Indeed, it is the first best-of-both-worlds algorithm to provide bounds logarithmic in the number of constraints. Moreover, in stochastic settings, it is the first algorithm to provide $\\widetilde{O}(\\sqrt{T})$ regret without requiring Slater's condition. Finally, it guarantees that the expected violation in the current round converges to zero, making our algorithm \u201cconverge\" to strategies that are feasible in expectation. This provides a more stable and consistent control on the violations. ", "page_idx": 1}, {"type": "text", "text": "2  Model and Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We address the problem faced by an agent aiming at maximizing its cumulative reward over a time horizon $T$ , while satisfying $[\\![m]\\!]$ long-term constraints.1 The agent has a set $[\\![K]\\!]$ of available actions and, at each round $t\\in[T]$ , selects $\\mathbf{\\dot{\\rho}}_{a_{t}}\\in[\\mathbb{K}]$ . The agent then observes the corresponding reward $f_{t}(a_{t})\\in[0,1]$ and a cost $g_{t}^{(i)}(a_{t})\\in[-1,1]$ , for each constraint $i\\in[\\![m]\\!]$ . We define the cumulative violation of the $i^{t h}$ constraint as ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V_{T}^{(i)}:=\\sum_{t\\in[\\![T]\\!]}g_{t}^{(i)}(a_{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "While $V_{T}:=\\operatorname*{max}_{i\\in[[m]]}V_{T}^{(i)}$ is the maximum violation acros all constraints. At a high level, we want tominmize  gret while keping thevilatinfeachconstrant sublinear in $T$ ", "page_idx": 2}, {"type": "text", "text": "The focus of this paper is on handling both stochastic and adversarial constraints. Conversely, we always assume the rewards to be generated up-front by an adversary; we do not treat explicitly the situation where the rewards are generated i.i.d. because our guarantees are already tight for the harder case o adversarial rewards.2 In the stochastic seting, we asume that $g_{t}=\\dot{\\{g_{t}^{(i)}\\}}_{i\\in\\mathbb{I}^{m}}$ is drawn i.i.d. from a fixed but unknown distribution $\\mathcal{G}$ , and we let $\\bar{g}^{(i)}(a)=\\mathbb{E}_{g\\sim\\mathcal{G}}[g^{(i)}(a)]$ be the expected cost of action $a$ for the $i^{t h}$ constraint. On the other hand, in the adversarial seting $\\{g_{t}\\}_{t\\in[T]}$ is an arbitrary sequence of cost functions. ", "page_idx": 2}, {"type": "text", "text": "Let $\\Delta_{K}$ to be the set of discrete probability distributions over the set $[\\![K]\\!]$ . Then, at round $t\\in[T]$ given a randomized strategy $x_{t}\\in\\Delta_{K}$ , the expected learner reward is $\\begin{array}{r}{\\sum_{a\\in[[K]]}f_{t}(a)x_{t}(a)=\\langle x_{t},\\overbar{f}_{t}\\rangle}\\end{array}$ Similarly. $\\langle x_{t},g_{t}^{(i)}\\rangle$ denotes the expected cost of te $i^{t h}$ constraint Fially, we use $n_{t}(a)$ to denote the number of times arm $a$ was played up to time $t$ i.e., $\\begin{array}{r}{n_{t}(a)=\\sum_{\\tau=1}^{t}\\mathbb{I}(a_{\\tau}=a)}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "We want to design algorithms which achieve good performances in both the adversarial and the stochastic setting. As it is customary in the literature, we compare our learning algorithm with different benchmarks according to the setting. ", "page_idx": 2}, {"type": "text", "text": "Stochastic Benchmark In the stochastic seting, the constraints $g_{t}^{(i)}$ are i.i.d. samples with mean $\\bar{g}^{(i)}$ and thus we consider as benchmark the best fixed randomized strategy that satisfies the constraints in expectation, which is a standard choice in bandits with constraints [11, 28, 16]. Formally, in the stochastic setting, we can define the feasible sets $\\mathbf{\\mathcal{X}}_{i}^{\\star}$ and $\\varkappa^{\\star}$ asfollows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{X}_{i}^{\\star}:=\\Big\\{x\\in\\Delta_{K}:\\langle x,\\bar{g}^{(i)}\\rangle\\leq0\\Big\\}\\quad\\mathrm{~and~}\\quad\\mathcal{X}^{\\star}:=\\cap_{i\\in[\\![m]\\!]}\\mathcal{X}_{i}^{\\star}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Then, we can define the stochastic baseline as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathsf{O P T}_{\\mathrm{S}}:=\\operatorname*{max}_{x\\in\\mathcal{X}^{\\star}}\\sum_{t\\in[\\![T]\\!]}\\langle x,f_{t}\\rangle.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We naturally assume the existence of safe mixed strategies, i.e., that $\\mathcal{X}^{\\star}\\neq\\varnothing$ . This is equivalent to assume the existence of a randomized strategy $x^{\\mathcal{B}}$ such that $\\langle x^{\\mathcal{Q}},\\bar{g}^{(i)}\\rangle\\leq\\dot{0}$ for all $i$ . Notice that this is a weaker assumption than the one commonly assumed by best-of-both-worlds algorithms in which $\\langle x^{\\otimes},\\bar{g}^{(i)}\\rangle\\leq-\\rho$ where $\\rho$ is a strictly positive constant (see, e.g., [16, 11, 10]). ", "page_idx": 2}, {"type": "text", "text": "Adversarial Benchmark In the adversarial setting, $\\{g_{t}\\}_{t\\in[T]}$ is an arbitrary sequence of constraints. We consider as benchmark the best unconstrained strategy: ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\mathsf{O P T}}_{\\mathsf{A}}:=\\operatorname*{max}_{x\\in\\Delta_{K}}\\sum_{t\\in\\left\\|T\\right\\|}\\langle x,f_{t}\\rangle.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "While this baseline has already been used [e.g., 11, 13]), other works on adversarial bandit with constraints employ weaker baselines [e.g., 21, 16]. For instance, Castiglioni et al. [16] consider the best fixed strategy which is feasible on average. However, we show that, despite using a stronger baseline, we obtain a competitive ratio that is optimal even for the weaker baselines commonly adopted in the literature [16, 10, 9]. ", "page_idx": 2}, {"type": "text", "text": "2.1 Best-Of-Both-Worlds Guarantees ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our goal is to design learning algorithms that exhibit optimal guarantees both in the stochastic and adversarial settings. In the stochastic setting, we are interested in minimizing the regret $R_{T}$ W.r.t. $\\mathsf{O P T}_{\\mathsf{S}}$ ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R_{T}=\\mathsf{O P T}_{\\mathsf{S}}-\\sum_{t\\in\\mathbb{I}\\mathbb{T}}\\,f_{t}(a_{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Algorithm 1 ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Require: bonuses $b_{t}(a)$ , weights $w_{t,a}^{(i)}$ and parameter $\\beta$   \n1: Initialize regret minimizer $\\mathcal{R}$ with $\\beta$   \n2: for each step $t=1,\\dots,T$ do   \n34 $\\begin{array}{r}{\\overline{{\\hat{g}_{t}^{(i)}(a)}}\\leftarrow\\sum_{\\tau\\in\\mathcal{T}_{t-1,a}}w_{t,a}^{(i)}(\\tau)g_{\\tau}^{(i)}(a)}\\end{array}$ for al $a\\in[|K|]$ $i\\in[m]$   \n5: $\\widehat{\\mathcal{X}}_{t}^{(i)}\\leftarrow\\{x\\in\\Delta_{K}:\\langle x,\\hat{g}_{t}^{(i)}-b_{t}\\rangle\\leq0\\}$   \n6:   \n7: Regret minimization:   \n8: Get prediction from $\\mathcal{R}$ on set $\\widehat{\\mathcal{X}}_{t}$ $x_{t}\\gets\\mathcal{R}(\\widehat{\\mathcal{X}}_{t})$   \n9: Sample $a_{t}\\sim x_{t}$ and receive $\\{g_{t}^{(i)}(a_{t})\\}_{i\\in[{m}]}$ and $f_{t}(a_{t})$ ", "page_idx": 3}, {"type": "text", "text": "and specifically we require both $R_{T}$ and $V_{T}$ to be in $\\widetilde{O}(\\sqrt{T})$ with high probability. This clearly matches the standard $\\Omega({\\sqrt{T}})$ lower bound that holds even without constraints [5]. ", "page_idx": 3}, {"type": "text", "text": "In the (harder) adversarial setting, we pose the less ambitious goal of achieving a constant competitive ratio with respect to $\\mathsf{O P T_{A}}$ , or equivalently sublinear $\\alpha$ -regret with constant $\\alpha$ . Formally, given an $\\alpha<1$ , we define the $\\alpha$ -regret as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\alpha{\\boldsymbol{-}}{\\boldsymbol{R}}_{T}=\\alpha\\cdot{\\mathsf{O P T}}_{\\mathsf{A}}-\\sum_{t\\in\\mathbb{I}\\mathbb{T}}f_{t}(a_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "As it is customary in the literature [15], the competitive ratio $\\alpha$ obtained by our algorithms depends on the following Slater's parameter $\\rho$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\rho=-\\operatorname*{inf}_{a\\in\\mathbb{I}^{K}\\mathbb{I}}\\operatorname*{max}_{t\\in\\mathbb{I}^{T}\\mathbb{I},i\\in\\mathbb{I}^{m}\\mathbb{I}}g_{t}^{(i)}(a).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The parameter $\\rho$ is related to the existence of strictly-feasible actions, and only depends on the constraints. Our definition is slightly stronger than the one in most previous works where the inf is over randomized strategies. To guarantee the existence of a feasible strategy we assume that $\\rho\\ge0$ Then, our goal is to guarante that both $V_{T}$ and the $\\alpha$ -regret, with $\\alpha=\\rho/\\rho{+}1$ , belong to $\\widetilde{O}(\\sqrt{T})$ with high probability. Note that this matches the lower bound of Bernasconi et al. [11]. ", "page_idx": 3}, {"type": "text", "text": "3 Our Approach ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present the main components of our algorithm, while the following sections will describe the specific components in details. We refer to Algorithm 1 for the pseudocode. At each step $t$ , the algorithm works in two phases: i) it estimates the feasible set, and i) it plays a strategy in the estimated set. Each phase requires a specific ingredient: ", "page_idx": 3}, {"type": "text", "text": "i  An estimator $\\hat{g}_{t}^{(i)}$ of the costs functions $g_{t}^{(i)}$ that is used togther withth optimistic bonus $b_{t}$ to define the estimation of the feasible set defined as $\\widehat{\\mathcal{X}}_{t}:=\\cap_{i\\in[m]}\\widehat{\\mathcal{X}}_{t}^{(i)}$ . In the stochastic case, we would like $\\widehat{\\mathcal{X}}_{t}\\supseteq\\mathcal{X}^{\\star}$ , while in the adversarial case our goal is to maintain a sequence of sets that always contains a version of the action set $\\mathcal{X}$ , properly scaled around $a^{\\mathcal{O}}$ (see Equation (2) for a formal definition). i) A regret minimizer $\\mathcal{R}$ for adversarial linear reward function that, at each round, takes in input a convex set of feasible strategies $\\widehat{\\mathcal{X}}_{t}\\subseteq\\Delta_{K}$ , and then selects a strategy $x_{t}\\in\\widehat{\\mathcal X}_{t}$ .We require the regret minimizer to achieve $\\tilde{O}(\\sqrt{K T})$ regret with respect to any $x\\in\\cap_{t\\in[T]}\\widehat{\\mathcal{X}}_{t}$ .\uff0c In the following we define the two phases more in details. Let ${\\mathcal{T}}_{t,a}:=\\{\\tau\\leq t:a_{t}=a\\}$ be the set of rounds in which the algorithm plays action $a$ . Then, at each round $t$ , Algorithm 1 computes the estimate ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\hat{g}}_{t}^{(i)}(a)=\\sum_{\\tau\\in{\\cal{T}}_{t-1,a}}w_{t,a}^{(i)}(\\tau)g_{\\tau}(a)\\quad\\forall a\\in\\left[\\![K]\\!\\ \\mathrm{and}\\ i\\in\\left[\\![m]\\!\\right]\\!\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Algorithm 2 No Regret on Moving Sets ", "page_idx": 4}, {"type": "text", "text": "Require: Parameter $\\beta>0$   \n1: Set $\\gamma={\\beta}/{2}$   \n2: for each step $t=1,\\dots,T$ do   \n3: Receive $\\widehat{\\mathcal{X}}_{t}$   \n4: $\\hat{x}_{t}(a)\\gets\\bar{x}_{t-1}(a)e^{\\beta(\\hat{f}_{t-1}(a)-1)}$ for all $a\\in[|K|]$   \n5: $x_{t}\\gets\\Pi_{\\widehat{\\mathcal{X}}_{t}}(\\widehat{x}_{t}):=\\arg\\operatorname*{min}_{x\\in\\widehat{\\mathcal{X}}_{t}}B(x||\\widehat{x}_{t})$   \n6: Sample $a_{t}\\sim x_{t}$   \n7: Observe f(at) and set f\u00b1(a) \u2190 1forall a \u2260 at and f(at) \u2190 1 - =:() ", "page_idx": 4}, {"type": "text", "text": "as the weighted mean of available past observations $\\{g_{\\tau}^{(i)}(a)\\}_{\\tau\\in[\\![t-1]\\!]}$ for each actions $a\\in[|K|]$ and constraint i E [m], for some weights w23 wThen the estimate tgethe with thoptimisticbonus $\\{b_{t}(a)\\}_{a\\in\\mathbb{I}^{K}\\mathbb{I}}$ are used to define the moving sets $\\widehat{\\mathcal{X}}_{t}$ , which are fed to the regret minimizer $\\mathcal{R}$ which in turn selects a point $x_{t}\\in\\widehat{\\mathcal{X}}_{t}$ ", "page_idx": 4}, {"type": "text", "text": "One crucial property that is required for the execution of the regret minimizer $\\mathcal{R}$ is that all the sets $\\widehat{\\mathcal{X}}_{t}$ are non-empty (as otherwise the regret minimizer has no feasible strategies). To simplify exposition, in the following sections we assume that the clean event $\\mathcal{C}\\,:=\\,\\Bigl\\{\\widehat{\\mathcal{X}_{t}}\\neq\\{\\varnothing\\}\\,\\forall t\\in\\dot{\\mathbb{[}T]}\\Bigr\\}$ holds. In Corollary 6.3, we prove that this event holds with high probability in the stochastic setting, while in Theorem 5.2 we argue that it holds deterministically in the adversarial one. ", "page_idx": 4}, {"type": "text", "text": "In Algorithm 1 we left unspecified two crucial parts of our approach. The first is how to build the regret minimizer $\\mathcal{R}$ , and the second concerns how to actually generate the sets $\\widehat{\\mathcal{X}}_{t}$ ,i.e., the weights $w_{t,a}^{(i)}$ and the bonus $b_{t}(a)$ . We delve into these details in Section 4 and Section 5, respectively. ", "page_idx": 4}, {"type": "text", "text": "4  No-regret on moving sets ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We describe the regret minimizer $\\mathcal{R}$ that exhibits no-regret with respect to any $x\\;\\in\\;\\cap_{t\\in[\\![T]\\!]}\\widehat{\\mathcal{X}}_{t}$ We achieve this via a simple modification to the EXP-IX algorithm of Neu [25] that provides high probability results for multi-armed bandits via implicit exploration. More specifically, our algorithm maintains a randomized strategy $x_{t}\\,\\in\\,\\Delta_{K}$ which is updated using the biased reward estimate $\\hat{f}_{t}(a)$ as in Neu [25] and then projected onto ${\\widehat{X}}_{t}$ according to the negative entropy Bregman divergence $\\begin{array}{r}{B(x||y)=\\sum_{a\\in\\mathbb{I}K\\mathbb{I}}\\left[x(a)\\log\\left(x(a)/y(a)\\right)-x(a)+y(a)\\right]}\\end{array}$ . We refer to Algorithm 2 for the pseudocode, and present here the main result of the Section. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.1.Let $x_{t}$ be selected accordingly to Algorithm 2 run with arbitrary sequence of convex sets $\\widehat{\\mathcal{X}}_{t}\\subseteq\\Delta_{K}$ with $\\begin{array}{r}{\\gamma=\\frac{\\beta}{2}}\\end{array}$ and $\\begin{array}{r}{\\beta=\\sqrt{\\frac{\\log\\left(K/\\delta_{1}\\right)}{K T}}}\\end{array}$ og(K/o)  The, with probabiliy at least - 8 it holdsthat ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\sum_{t\\in[T]}\\langle f_{t},x\\rangle-f_{t}(a_{t})\\leq4\\sqrt{K T\\log(^{K}/\\delta_{1})},\\qquad\\forall x\\in\\bigcap_{t\\in[\\![T]\\!]}\\widehat{\\mathcal{X}}_{t}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This result establishes no-regret in the case of moving sets, taking as benchmark the optimal strategy in the intersection of all sets. To exploit this result in Algorithm 1, we have to make sure that in both the stochastic and adversarial setting the intersection of the sets $\\widehat{\\mathcal{X}}_{t}$ contains \u201cgood\" strategies. In the stochastic setting, we show that with high probability it includes $\\varkappa^{\\star}$ , while, in the adversarial setting, it includes a strategy with utility $\\rho_{\\big/1+\\rho\\cdot\\bigcirc\\mathsf{P T_{A}}}$ ", "page_idx": 4}, {"type": "text", "text": "5 How to build the sets $\\widehat{\\mathcal X}_{t}$ ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we show how to design estimations $\\widehat{\\mathcal{X}}_{t}$ of the feasible sets that, surprisingly, are effective both in stochastic and adversarial settings. Indeed, the main challenge is to design sets $\\widehat{\\mathcal{X}}_{t}$ ", "page_idx": 4}, {"type": "text", "text": "that accommodate the different requirements of the two settings. First, in Section 5.1, we discuss howtosetheopimisticbouses $b_{t}$ and hen in Setion 5.2 we fouso howto sthe weights $w_{t,a}^{(i)}$ ", "page_idx": 5}, {"type": "text", "text": "5.1  How to set the optimistic bonus ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The optimistic bonuses have the main purpose of balancing the estimation error in the stochastic setting. As the following lemma show, we simply need that $|\\hat{g}_{t}^{(i)}(a)-\\bar{g}^{(i)}(a)|\\leq b_{t}(a)$ With high probability. Indeed, this is sufficient to show that $\\mathcal{X}^{\\star}\\subseteq\\cap_{t\\in[\\![T]\\!]}\\widehat{\\mathcal{X}}_{t}$ in the stochastic setting. ", "page_idx": 5}, {"type": "text", "text": "Theorem 5.1. Consider the stochastic setting. Given any $\\delta>0,$ let $b_{t}(a)$ be such that with probability atleast $1-\\delta$ it holds: ", "page_idx": 5}, {"type": "equation", "text": "$$\n|\\hat{g}_{t}^{(i)}(a)-\\bar{g}^{(i)}(a)|\\leq b_{t}(a)\\quad\\forall t\\in[\\![T]\\!],i\\in[\\![m]\\!],a\\in[\\![K]\\!].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then, it holds $\\mathcal{X}^{\\star}\\subseteq\\cap_{t\\in[T]}\\widehat{\\mathcal{X}}_{t}$ with probability at least $1-\\delta$ ", "page_idx": 5}, {"type": "text", "text": "Even tough it is crucial in the stochastic setting, it turns out that in the adversarial setting the optimistic bonus $b_{t}$ is not really needed. Indeed, as we will show in the following, we are interested in obtaining no-regret with respect to the set $\\mathcal{X}_{\\mathcal{B}}^{\\star}$ which is obtained via interpolation of points in $\\mathcal{X}$ and thestrictly feasible actions $a^{\\mathcal{O}}$ . Let $x^{\\mathcal{B}}$ be such that $x^{\\mathcal{B}}(a^{\\mathcal{B}})=1$ and $x^{\\mathcal{B}}(a)=0$ for all $a\\neq a^{\\mathcal{B}}$ . Formally: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{X}_{\\mathcal{O}}^{\\star}:=\\frac{1}{1+\\rho}\\{x^{\\mathcal{O}}\\}+\\frac{\\rho}{1+\\rho}\\mathcal{X},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $A+B$ is the Minkowski sum between sets and $\\alpha A$ indicates the set that contains each element of $A$ multiplied by $\\alpha$ The following theorem proves that $\\mathcal X_{\\mathcal X}^{\\star}\\subseteq\\widehat\\mathcal X_{t}$ for all $t$ ", "page_idx": 5}, {"type": "text", "text": "Theorem 5.2. In the adversarial setting, it holds $\\mathcal{X}_{\\mathcal{O}}^{\\star}\\subseteq\\widehat{\\mathcal{X}}_{t}$ for all $t\\in[T]$ ", "page_idx": 5}, {"type": "text", "text": "Notice that having no-regret with respect to the set $\\mathcal{X}_{\\mathcal{B}}^{\\star}$ is not sufficient to achieve no-regret in the adversarial setting. Nonetheless, we will show that this is sufficient to guarantee no- $\\alpha$ -regret,for $\\alpha=\\rho/1\\!+\\!\\rho$ with respect to any strategy $x\\in\\Delta_{K}$ ", "page_idx": 5}, {"type": "text", "text": "5.2 How to set the weights ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We focus on the design of estimators $\\hat{g}_{t}^{(i)}$ that are good approximations of the real functions $g_{t}^{(i)}$ Algorithm 1 computes the estimators $\\hat{g}_{t}^{(i)}$ by using a weighted mean of all past observations: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{g}_{t}^{(i)}(a)=\\sum_{\\tau\\in\\mathcal{T}_{t-1,a}}w_{t,a}^{(i)}(\\tau)g_{\\tau}^{(i)}(a)\\quad\\forall t\\in[\\![T]\\!],a\\in[\\![K]\\!],i\\in[\\![M]\\!].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "However, to simplify the exposition, we use the following equivalence between online gradient descent (OGD) on quadratic losses $\\begin{array}{r}{\\hat{g}_{t}^{(i)}(a_{t})\\mapsto\\frac{1}{2}\\left(g_{t}^{(i)}(a_{t})-\\hat{g}_{t}^{(i)}(a_{t})\\right)^{2}}\\end{array}$ and weighted means. In particular this equivalence is realized by observing that such loss has gradient $g_{t}^{(i)}(a_{t})-\\hat{g}_{t}^{(i)}(a_{t}).$ ", "page_idx": 5}, {"type": "text", "text": "Lemma 5.3. Given any sequence $\\{y_{t}\\}_{t\\in[T]}$ such that $y_{1}=0$ and any sequence of learning rates $\\{\\eta_{t}\\}_{t\\in[T]}$ such that $\\eta_{1}=1$ let $\\{\\hat{y}_{t}\\}_{t\\in[T]}$ be the estimator updated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{y}_{t+1}=\\hat{y}_{t}+\\eta_{t}\\big(y_{t}-\\hat{y}_{t}\\big).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then, it holds that $\\begin{array}{r c l}{\\hat{y}_{t}}&{=}&{\\sum_{\\tau=1}^{t-1}y_{\\tau}w_{t}(\\tau)}\\end{array}$ where $\\begin{array}{r l r}{w_{t}(\\tau)\\!}&{{}=}&{\\!\\eta_{\\tau}\\prod_{k=\\tau+1}^{t-1}(1\\;-\\;\\eta_{k})}\\end{array}$ Moreover, $\\begin{array}{r}{\\sum_{\\tau=1}^{t-1}w_{t}(\\tau)=1}\\end{array}$ for any $t\\geq2$ \uff0c ", "page_idx": 5}, {"type": "text", "text": "Clearly, in the OGD interpretation of our update, we only update $\\hat{g}_{t}^{(i)}(a)$ only when $a_{t}=a$ , and thus we only need to define learning rates for action $a$ for the times $t$ in which $a_{t}\\,=\\,a$ . Based on this observation, we are going to update $\\hat{g}_{t}^{(i)}(a)$ as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\left\\{\\hat{g}_{t+1}^{(i)}(a_{t})=\\hat{g}_{t}^{i}(a_{t})+\\eta_{t}^{(i)}(a_{t})\\left(g_{t}^{(i)}(a_{t})-\\hat{g}_{t}^{i}(a_{t})\\right)\\right.}\\\\ &{\\left.\\hat{g}_{t+1}^{(i)}(a)=\\hat{g}_{t}^{(i)}(a)\\right.}&{\\forall a\\neq a_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4Formally Minkowski sum between sets $A+B$ is defined as $A+B:=\\{a+b:a\\in A,b\\in B\\}$ ", "page_idx": 5}, {"type": "text", "text": "Thus, given an action $a\\in[|K|]$ and a time $t\\in[T]$ the corresponding weights $\\{w_{t,a}^{(i)}(\\tau)\\}_{\\tau\\in T_{t-1,a}}$ are: ", "page_idx": 6}, {"type": "equation", "text": "$$\nw_{t,a}^{(i)}(\\tau)=\\eta_{\\tau}^{(i)}(a)\\prod_{k\\in\\mathcal{T}_{t-1,a:k>\\tau}}(1-\\eta_{k}^{(i)}(a))\\quad\\forall\\tau\\in\\mathcal{T}_{t-1,a}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We now proceed to give two notable examples on how to instantiate the learning rates and recover commonly used estimators such as the empirical mean and the exponentially weighted mean. ", "page_idx": 6}, {"type": "text", "text": "Proposition 5.4. If $\\begin{array}{r}{\\eta_{t}^{(i)}(a_{t})=\\frac{1}{n_{t}(a_{t})}}\\end{array}$ for each $\\tau\\in\\mathcal{T}_{t-1,a}$ then $\\begin{array}{r}{w_{t,a}^{(i)}(\\tau)=\\frac{1}{n_{t-1}(a)}}\\end{array}$ nt-i(a) and we recover the empirical mean estimator for $\\begin{array}{r}{\\hat{g}_{t}^{(i)}(a)=\\frac{1}{n_{t-1}(a)}\\sum_{\\tau\\in\\mathcal{T}_{t-1,a}}g_{\\tau}^{(i)}(a).}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "Proposition 5.5. If $\\eta_{t}^{(i)}(a_{t})=\\eta$ then ", "page_idx": 6}, {"type": "equation", "text": "$$\nw_{t,a}^{(i)}(\\tau)=\\eta(1-\\eta)^{|\\{k\\in{\\mathcal{T}}_{t-1,a}:k>\\tau\\}|}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "foreach $\\tau\\in\\tau_{t-1,a}$ and we recover an exponentially weighted average estimator for $\\hat{g}_{t}^{(i)}(a)$ ", "page_idx": 6}, {"type": "text", "text": "As it will turns out, these are the two extreme cases that we want to interpolate between. Indeed, the empirical mean estimator is particularly effective in the stochastic case but ineffective in the adversarial case, while the converse happens with the exponentially weighted estimator. ", "page_idx": 6}, {"type": "text", "text": "Now, we show that the OGD interpretation is particularly useful to bounds the violations suffered by the algorithm. First, we define the violations in an interval $[t_{1},t_{2}]:=\\{t\\in[\\![T]\\!]:t_{1}\\leq t\\leq t_{2}\\}$ as: ", "page_idx": 6}, {"type": "equation", "text": "$$\nV_{[t_{1},t_{2}]}^{(i)}=\\sum_{t=t_{1}}^{t_{2}}g_{t}^{(i)}(a_{t}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then, in the following lemma we show that the violations in the interval are related to the variation of thestimates $\\hat{g}_{t}^{(i)}(a)$ ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.6. Given an interval $[t_{1},t_{2}]\\subseteq[T]$ an $i\\in[\\![m]\\!]$ and a $\\delta>0$ with probability at least $1-\\delta$ it holds: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\gamma_{[t_{1},t_{2}]}^{(i)}\\leq\\sum_{a\\in[K]}\\sum_{\\tau\\in\\mathcal{T}_{t_{2},a}\\cap[t_{1},t_{2}]}\\frac{1}{\\eta_{\\tau}^{(i)}(a)}\\left(\\hat{g}_{\\tau+1}^{(i)}(a)-\\hat{g}_{\\tau}^{(i)}(a)\\right)+\\sum_{\\tau=t_{1}}^{t_{2}}\\langle x_{\\tau},b_{\\tau}\\rangle+4\\sqrt{(t_{2}-t_{1})\\log(1/\\delta)}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "By a simple telescoping argument, we have the following corollary, which holds whenever the learning rates are non-increasing within a time interval. Let $\\ell(a,[t_{1},t_{2}])$ be the last rounds in the interval $[t_{1},t_{2}]$ in which action $a$ is played. ", "page_idx": 6}, {"type": "text", "text": "Corollary 5.7. Given an interval $[t_{1},t_{2}]\\subseteq\\mathbb{[T]}$ $a\\ i\\in[\\![m]\\!]$ , and a $\\delta\\,>\\,0$ assume that for any $a\\in[|K|]$ it holds $\\eta_{\\tau}^{(i)}(a)\\geq\\eta_{\\tau^{\\prime}}^{(i)}(a)\\,\\forall\\tau<\\tau^{\\prime}\\in T_{t_{2},a}\\cap[t_{1},t_{2}]$ .Then, with probability at least $1-\\delta\\;i t$ holds: ", "page_idx": 6}, {"type": "equation", "text": "$$\nV_{[t_{1},t_{2}]}^{(i)}\\leq\\sum_{a\\in[K]}\\frac{2}{\\eta_{\\ell(a,[t_{1},t_{2}])}^{(i)}(a)}+\\sum_{\\tau=t_{1}}^{t_{2}}\\langle x_{\\tau},b_{\\tau}\\rangle+4\\sqrt{(t_{2}-t_{1})\\log(^{1/\\delta})}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Corollary5.7 shows how to bound the violation as a function o the leaming rates $\\eta_{t}^{(i)}$ andthe bonus terms $b_{\\tau}$ . The following lemma shows how to bound the second term of the violations depending on the structure of the bonus terms. ", "page_idx": 6}, {"type": "text", "text": "Lemma 5.8. Given a $c>0$ $\\alpha\\,\\in\\,(0,1)$ $a\\ t\\in[T]$ and a $\\delta\\,>\\,0$ let $\\begin{array}{r}{b_{t}(a)\\,=\\,\\frac{c}{n_{t}(a)^{\\alpha}}}\\end{array}$ for all $a\\in[|K|]$ .Then,withprobability atleast $1-\\delta$ it holds: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{\\tau=1}^{t}\\langle x_{\\tau},b_{\\tau}\\rangle\\leq\\frac{c}{1-\\alpha}K^{\\alpha}t^{1-\\alpha}+4\\sqrt{t\\log(^{1/\\delta})}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In this section, we saw how the choice of the learning rates of the estimator affects the estimators. In the following section, we will see how to adaptively set those learning rates to handle both stochastic and adversarial settings. ", "page_idx": 6}, {"type": "text", "text": "6  Adaptive learning rates ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The previous section highlights the main difficulties of obtaining best-of-both-world algorithms: we need to setheweights $w_{t,a}^{(i)}$ (or equivalently -by Lemma 5.3 - the learning rates $\\eta_{t}^{(i)}(a_{t}))$ and the optimisticbonuses $b_{t}$ so that they meet, at the same time, the requirements needed by the stochastic and the adversarial settings. ", "page_idx": 7}, {"type": "text", "text": "We start presenting two possible choices and show that they fail either in the stochastic or the adversarial setting. Then, we show how adaptive learning rates combine the strengths of both approaches. The first, natural, choice of setting the learning rate is to use an exponentially weighted estimator, i.e., choose $\\eta_{t}^{(i)}(a_{t})=1/\\sqrt{T}$ With this choice, we can apply a weighted version of AzumaHoeffding inequality and find that $|\\hat{g}_{t}^{(i)}(a)-\\bar{g}^{(i)}(a)|\\in\\widetilde O\\left(n_{t}(a)^{-1/4}\\right)$ , with high probability. Thus, as discussed in Section 5.1, we would need to define $b_{t}(a)\\in\\tilde{O}\\left(n_{t}(a)^{-1/4}\\right)$ , which, by Corollary 5.7 and Lemma 5.8 would imply a suboptimal $\\tilde{O}(T^{3/4})$ rate for the violations. ", "page_idx": 7}, {"type": "text", "text": "The second option is to set $\\eta_{t}^{(i)}(a_{t})\\,=\\,1/{n_{t}(a_{t})}$ . In the stochastic setting, we have an optimal rate of concentration of the terms $|\\hat{g}_{t}^{(i)}(a)-\\bar{g}^{(i)}(a)|\\,\\in\\,\\widetilde{O}\\left(n_{t}(a)^{-1/2}\\right)$ as, by Proposition 5.4, this is equivalent to compute the empirical mean. However, this second option fails disastrously in the adversarial setting as highlighted in Corollary 5.7, where the first component of the violations becomes linear in $T$ . Intuitively, a learning rate of order $^1/n_{t}(a)$ makes the update of the estimates t00 slow when the underlying constraints change, as it does happen in the adversarial setting. ", "page_idx": 7}, {"type": "text", "text": "This trade-off forces us to employ adaptive learning rates. Our idea is to use learning rates of the order $^1/n_{t}(a)$ with an adaptive multiplicative term that depends on the current violation of the constraint. Formally, we use learning rates: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\eta_{t}^{(i)}(a_{t}):=\\frac{1}{n_{t}(a)}\\left(1+\\Gamma_{t}^{(i)}\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\Gamma_{t}^{\\left(i\\right)}$ is a bonus term defined as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\Gamma_{t}^{(i)}:=\\left[V_{t-1}^{(i)}-21\\sqrt{K t\\log(^{1/\\delta_{2}})}\\right]_{0}^{21\\sqrt{K t\\log(^{1/\\delta_{2}})}},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and $[x]_{a}^{b}:=\\operatorname*{min}(\\operatorname*{max}(x,a),b)$ is the clipping of $x$ between $a$ and $b$ . Morever, we set the exploraton bonus as ", "page_idx": 7}, {"type": "equation", "text": "$$\nb_{t}(a)=\\sqrt{\\frac{2\\log(2/\\delta_{2})}{n_{t-1}(a)}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The following theorem shows that such approach guarantees $\\widetilde{O}(\\sqrt{K T})$ violations in both adversarial and stochastic settings. ", "page_idx": 7}, {"type": "text", "text": "Theorem 6.1. Both in the stochastic and the adversarial setting, with probability at least $1-2m T^{2}\\delta_{2}$ itholdsthat ", "page_idx": 7}, {"type": "equation", "text": "$$\nV_{t}\\leq53\\sqrt{K t\\log(2/\\delta_{2})}\\quad\\forall t\\in\\lVert T\\rVert.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The previous theorem shows that this choice of learning rates is sufficient to guarantee optimal bounds on the violations. However, to achieve this result we are setting $b_{t}(a)\\in\\tilde{O}(n_{t}(a)^{-1/2})$ .Aswe showed in theorem 5.1, this requires a concentration on the estimates $|\\hat{g}_{t}^{(i)}(a)-\\bar{g}_{t}^{(i)}(a)|$ of the same magnitude (in the stochastic setting). This is crucially needed to ensure that the regret minimizer $\\mathcal{R}$ provides the desired guarantees and that the event $\\mathcal{C}$ defined in Section 3 actually holds with high probability. ", "page_idx": 7}, {"type": "text", "text": "Lemma 6.2. In the stochastic setting, with probability at least $1-5m K T\\delta_{2}$ itholdsthat: ", "page_idx": 7}, {"type": "equation", "text": "$$\n|\\hat{g}_{t}^{(i)}(a)-\\bar{g}_{t}^{(i)}(a)|\\leq b_{t}(a)\\quad\\forall a\\in[K],t\\in[T],i\\in[m]\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The proof of the previous result relies on the fact that in the stochastic case the bonus $\\Gamma_{t}^{\\left(i\\right)}$ doesnot \u201ckick in' ensuring that $\\eta_{t}^{(i)}(a)=1/{n_{t}(a)}$ Thus, $\\hat{g}_{t}^{(i)}$ is the empirical average of past observations. The previous result, together with Theorem 5.1 proves the following corollary. ", "page_idx": 7}, {"type": "text", "text": "Corollary 6.3. In the stochastic setting, with probability at least $1-5m K T\\delta_{2}$ it holds that $\\mathcal{X}^{\\star}\\in\\widehat{\\mathcal{X}}_{t}$ for all $t\\in[T]$ ", "page_idx": 8}, {"type": "text", "text": "This proves that the clean event $\\mathcal{C}$ holds with high probability, as promised in Section 3. ", "page_idx": 8}, {"type": "text", "text": "7   Putting everything together ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Now, we have everything in place to easily prove the our main theorems. First, we define the parameters $\\delta_{1}=\\delta_{1}(\\epsilon)$ and $\\delta_{2}=\\delta_{2}(\\epsilon)$ in order to guarantee that our theorems hold with probability at least $1-\\epsilon$ . In particular, we set $\\delta_{1}(\\epsilon)=\\epsilon/2$ , where we recall that $\\delta_{1}$ is the parameter used to set $\\beta$ and $\\gamma$ in Algorithm 2, and $\\delta_{2}(\\epsilon)=\\epsilon/(14m K T^{2})$ , where $\\delta_{2}$ is used to set the optimistic bonus and learning rate of Algorithm 1. ", "page_idx": 8}, {"type": "text", "text": "In the stochastic setting, the violation guarantees directly follow from Theorem 6.1, while the regret guarantee follows by combining Theorem 4.1 and Corollary 6.3. Formally: ", "page_idx": 8}, {"type": "text", "text": "Theorem 7.1. In the stochastic setting, for any $\\epsilon>0$ Algorithm $^{\\,l}$ guarantees that with probability at least $1-\\epsilon$ ", "page_idx": 8}, {"type": "equation", "text": "$$\nR_{T}\\leq4\\sqrt{K T\\log(2K/\\epsilon)}\\quad a n d\\quad V_{t}\\leq53\\sqrt{K t\\log(28m K T^{2}/\\epsilon)}\\quad\\forall t\\in\\lVert T\\rVert.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Now, we turn to the adversarial setting. Theorem 6.1 guarantee $\\widetilde{O}(\\sqrt{T})$ violations evenwith adversarial constraint, while the regret guarantees follows by combining Theorem 5.2 and Theorem 4.1 ", "page_idx": 8}, {"type": "text", "text": "Theorem 7.2. In the adversarial setting, for any $\\epsilon>0$ Algorithm $^{\\,l}$ guarantees that with probability atleast $1-\\epsilon$ ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\alpha\\cdot R_{T}\\leq4\\sqrt{K T\\log(^{2K}/\\epsilon)}\\quad a n d\\quad V_{t}\\leq53\\sqrt{K t\\log(28m K T^{2}/\\epsilon)}\\quad\\forall t\\in\\ensuremath{[T]},}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\alpha=\\rho/(1{+}\\rho)$ ", "page_idx": 8}, {"type": "text", "text": "Note that in both settings, the regret upper bound is of order $\\widetilde{O}(\\sqrt{K T})$ and it is independent from the number of constraints $m$ , while the violations are of order $\\tilde{O}(\\sqrt{K T\\log(m)})$ and depend only logarithmically on $m$ . This is in contrast to the other best-of-both-world algorithms for bandits with long term constraints, based on primal-dual methods, in which both the regret and the violations depends polynomially in $m$ ", "page_idx": 8}, {"type": "text", "text": "Another interesting characteristic of our methodology is that we guarantee an anytime bound on the constraint violation. Indeed, this matches the guarantees provided by the most recent primal-dual methods [11, 1] that, however, require weakly-adaptive underlying regret minimizers. ", "page_idx": 8}, {"type": "text", "text": "7.1  Convergence rate in the stochastic setting ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To conclude, we point to a nice byproduct of our analysis. In the stochastic setting, we can easily prove a sort of \u201cconvergence rate\u201d of $x_{t}$ to the set $\\mathcal{X}^{\\star}$ . Formally, we can prove that positive violations are bounded by ${\\widetilde{O}}({\\sqrt{K t\\log m}})$ as long as we consider expected violations. Let us define $x^{+}:=\\operatorname*{max}(x,0)$ and ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathcal{V}_{t}^{+}:=\\operatorname*{max}_{i\\in[m]}\\sum_{\\tau=1}^{t}\\left[\\left<x_{\\tau},\\bar{g}_{\\tau}^{(i)}\\right>\\right]^{+}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Then, we can state the following theorem: ", "page_idx": 8}, {"type": "text", "text": "Theorem 7.3.Algorithm $^{\\,l}$ in the stochastic setting, guarantees that with probability at least $1-\\epsilon$ it holds that: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathcal{V}_{t}^{+}\\leq16\\sqrt{K t\\log(28m K T^{2}/\\epsilon)}\\quad\\forall t\\in[[T]].\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Intuitively, our result shows that our algorithm plays only a sublinear number of times \u201cfar\u2019 from the set $\\mathcal{X}^{\\star}$ , or that our algorithm plays a linear number of times \u201cclose\u201d to the set $\\mathcal{X}^{\\star}$ . This is a much stronger result then just guaranteeing that $V_{T}$ is sublinear, as in that case it might be a linear number of times the algorithm plays \u201cfar from $\\varkappa^{\\star}$ as long as it plays strictly inside of $\\varkappa^{\\star}$ often enough. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "MB, MC, AC, FF are partially supported by the FAIR (Future Artificial Intelligence Research) project PEO000013, funded by the NextGenerationEU program within the PNRR-PE-AI scheme (M4C2, investment 1.3, line on Artificial Intelligence). FF is also partially supported by ERC Advanced Grant 788893 AMDROMA \u201cAlgorithmic and Mechanism Design Research in Online Markets\", and PNRR MUR project IR0000013-SoBigData.it. MC is also partially supported by the EU Horizon project ELIAS (European Lighthouse of AI for Sustainability, No. 101120237). AC is partially supported by MUR - PRIN 2022 project 2022R45NBB funded by the NextGenerationEU program. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1]  Gagan Aggarwal, Giannis Fikioris, and Mingfei Zhao. No-regret algorithms in non-truthful auctions with budget and ROI constraints. arXiv preprint, abs/2404.09832, 2024.   \n[2]  Shipra Agrawal and Nikhil R. Devanur. Bandits with concave rewards and convex knapsacks. In EC, pages 989-1006. ACM, 2014.   \n[3]  Shipra Agrawal and Nikhil R Devanur. Bandits with global convex constraints and objective. Operations Research, 67(5):1486-1502, 2019.   \n[4] Peter Auer and Chao-Kai Chiang. An algorithm with nearly optimal pseudo-regret for both stochastic and adversarial bandits. In COLT, volume 49 of JMLR Workshop and Conference Proceedings, pages 116-120. JMLR.org, 2016.   \n[5] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48-77, 2002.   \n[6]  Ashwinkumar Badanidiyuru, Robert Kleinberg, and Aleksandrs Slivkins. Bandits with knapsacks. In 2013 IEEE 54th Annual Symposium on Foundations of Computer Science, FOCS 2013, pages 207-216. IEEE, 2013.   \n[7]  Ashwinkumar Badanidiyuru, Robert Kleinberg, and Aleksandrs Slivkins. Bandits with knapsacks. J. ACM, 65(3), 2018.   \n[8]  Santiago Balseiro, Christian Kroer, and Rachitesh Kumar. Online resource allocation under horizon uncertainty. SIGMETRICS Perform. Eval. Rev., 51(1):63-64, 2023.   \n[9]  Santiago R Balseiro and Yonatan Gur. Learning in repeated auctions with budgets: Regret minimization and equilibrium. Management Science, 65(9):3952-3968, 2019.   \n[10]  Santiago R Balseiro, Haihao Lu, and Vahab Mirrokni. The best of many worlds: Dual mirror descent for online allocation problems. Operations Research, 2022.   \n[11]  Martino Bernasconi, Matteo Castiglioni, and Andrea Celli. No-regret is not enough! bandits with general constraints through adaptive regret minimization. arXiv preprint, abs/2405.06575, 2024.   \n[12]  Martino Bernasconi, Matteo Castiglioni, Andrea Celli, and Federico Fusco. No-regret learning in bilateral trade via global budget balance. In STOC. ACM, 2024.   \n[13] Martino Bernasconi, Matteo Castiglioni, Andrea Celli, and Federico Fusco. Bandits with replenishable knapsacks: the best of both worlds. In International Conference on Learning Representations (ICLR), 2024.   \n[14]  Sebastien Bubeck and Aleksandrs Slivkins. The best of both worlds: Stochastic and adversarial bandits. In COLT, volume 23 of JMLR Proceedings, pages 42.1-42.23. JMLR.org, 2012.   \n[15] Matteo Castiglioni, Andrea Celli, and Christian Kroer. Online learning with knapsacks: the best of both worlds. In International Conference on Machine Learning, pages 2767-2783. PMLR, 2022.   \n[16] Matteo Castiglioni, Andrea Celli, Alberto Marchesi, Giulia Romano, and Nicola Gatti. A unifying framework for online optimization with long-term constraints. In Advances in Neural Information Processing Systems, volume 35, pages 33589-33602, 2022.   \n[17]  Matteo Castiglioni, Andrea Celli, and Christian Kroer. Online learning under budget and ROI constraints via weak adaptivity. In ICML. OpenReview.net, 2024.   \n[18] Giannis Fikioris and Eva Tardos. Approximately stationary bandits with knapsacks. In Proceedings of Thirty Sixth Conference on Learning Theory, volume 195, pages 3758-3782, 12-15 Jul 2023.   \n[19] Elad Hazan et al. Introduction to online convex optimization, volume 2. Now Publishers, Inc., 2016.   \n[20] Nicole Immorlica, Karthik Abinav Sankararaman, Robert Schapire, and Aleksandrs Slivkins. Adversarial bandits with knapsacks. In 60th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2019, pages 202-219. IEEE Computer Society, 2019.   \n[21] Nicole Immorlica, Karthik Sankararaman, Robert Schapire, and Aleksandrs Slivkins. Adversarial bandits with knapsacks. J. ACM, 69(6), 2022. ISSN 0004-5411.   \n[22] Thomas Kesselheim and Sahil Singla. Online learning with vector costs and bandits with knapsacks. In Conference on Learning Theory, pages 2286-2305. PMLR, 2020.   \n[23]  Raunak Kumar and Robert Kleinberg. Non-monotonic resource utilization in the bandits with knapsacks problem. In Advances in Neural Information Processing Systems (NeurIPS), 2022.   \n[24] Shang Liu, Jiashuo Jiang, and Xiaocheng Li. Non-stationary bandits with knapsacks. Advances in Neural Information Processing Systems, 35:16522-16532, 2022.   \n[25]  Gergely Neu. Explore no more: Improved high-probability regret bounds for non-stochastic bandits. Advances in Neural Information Processing Systems, 28, 2015.   \n[26]  Yevgeny Seldin and Gabor Lugosi. An improved parametrization and analysis of the $\\mathrm{EXP3++}$ algorithm for stochastic and adversarial bandits. In COLT, volume 65 of Proceedings of Machine Learning Research, pages 1743-1759. PMLR, 2017.   \n[27] Yevgeny Seldin and Aleksandrs Slivkins. One practical algorithm for both stochastic and adversarial bandits. In ICML, volume 32 of JMLR Workshop and Conference Proceedings, pages 1287-1295. JMLR.0rg, 2014.   \n[28] Aleksandrs Slivkins, Karthik Abinav Sankararaman, and Dylan J Foster. Contextual bandits with packing and covering constraints: A modular lagrangian approach via regression. In The Thirty Sixth Annual Conference on Learning Theory, pages 4633-4656. PMLR, 2023.   \n[29]  Chen- Yu Wei and Haipeng Luo. More adaptive algorithms for adversarial bandits. In COLT, volume 75 of Proceedings of Machine Learning Research, pages 1263-1291. PMLR, 2018.   \n[30] Julian Zimmert and Yevgeny Seldin. Tsallis-inf: An optimal algorithm for stochastic and adversarial bandits. J. Mach. Learn. Res., 22:28:1-28:49, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "A Further Related Works ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Best-of-Both-Worlds. A long line of work has investigated Best-of-Both-Worlds algorithms for bandits without constraints. These algorithms aim to achieve an instance-dependent logarithmic regret bound in stochastic environments, while also ensuring the worst-case $\\Theta({\\sqrt{T}})$ regretbound that characterizes the adversarial settings [14, 4, 27, 26, 29, 30]. Although our focus is on the generation model of the constraints, our motivation in this paper is affine: retaining the best of the stochastic (sublinear regret with respect to the optimal dynamic policy) and adversarial world (tight competitive ratio with respect to the adversarial benchamrk). Furthermore, our idea of setting an adaptive learning rate that forces the learning algorithm to interpolate between an adversarial and a stochastic routine is reminescent of some of the techniques adopted in, e.g., Bubeck and Slivkins [14]. ", "page_idx": 11}, {"type": "text", "text": "Bandits with Knapsacks. The (stochastic) BwK problem, where the rewards $f_{t}$ as well as the $g_{t}^{i}$ are drawn i.i.d. from a non-negative distribution (so that the budget available for each resource can only decrease over time) is formally introduced and solved in Badanidiyuru et al. [6] (see also its journal version [7]). Agrawal and Devanur [2] studies a more general stochastic setting, which subsumes knapsack and exhibit optimal guarantees via optimism in the face of uncertainty [see also 3]. Moving to the adversarial BwK problem (which corresponds to our model when the $g_{t}^{i}$ are all non-negative), an optimal solution is proposed in Immorlica et al. [20] [see also 21]; there, the authors propose the LagrangeBwK framework, which has a natural interpretation: arms can be thought of as primal variables, and resources as dual variables. The framework works by setting up a repeated two-player zero-sum game between a primal and a dual player, and by showing convergence to a Nash equilibrium of the expected Lagrangian game. Differently from the stochastic version, the adversarial BwK does not admit no-regret algorithms, but $\\Theta(\\log T)$ competitive ratio. In a subsequent work, [22] provides a new analysis obtaining a $O(\\log m\\log T)$ competitive ratio, which is optimal both in the time horizon $T$ and in the number of resources $m$ (and improves on the $O(m\\log T)$ of Immorlica et al. [20, 21]). In the special case in which budgets are $\\bar{\\Omega}(T)$ , Castiglioni et al. [15] further improves the competitive ratio to $^1\\!/\\!\\rho$ where $\\rho$ is the per-iteration budget. ", "page_idx": 11}, {"type": "text", "text": "More general constraints. Castiglioni et al. [15] studies a setting with general constraints, and show how to adapt the LagrangeBwK framework to obtain best-of-both-worlds guarantees when Slater's parameter is known a priori. Similar guarantees are also provided, in the stochastic setting, by Slivkins et al. [28], which then extend the results to the contextual model. Finally, Castiglioni et al. [17] introduces the use of weakly adaptive regret minimizers within the LagrangeBwK framework, and provides guarantees in the specific case of one budget constraint and one return-on-investments constraint. ", "page_idx": 11}, {"type": "text", "text": "Other related works. In an effort to bridge the results for adversarial and stochastic BwK, Fikioris and Tardos [18] investigates a data generation model that interpolate between the fully stochastic and the fully adversarial setting, depending on the magnitude of fluctuations in expected rewards and resources consumption across rounds. A similar effort is undertaken in Liu et al. [24], that study a non-stationary setting and provide no-regret guarantees against the best dynamic policy through a UCB-based algorithm. A recent line of work also investigates the natural situation where resources can be replenished in certain rounds (as also captured in our model) [23, 13, 12]. Finally, a related line of works is the one on online allocation problems with fixed per-iteration budget, where the input pair of reward and costs is observed before the learner makes a decision [10, 8]. ", "page_idx": 11}, {"type": "text", "text": "B Proofs omitted from Section 4 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Theorem 4.1. Let $x_{t}$ be selected accordingly to Algorithm 2 run with arbitrary sequence of convex sets &t \u2264 \u25b3k with  =  and \u03b2 = \u221a $\\begin{array}{r}{\\beta=\\sqrt{\\frac{\\log(K/\\delta_{1})}{K T}}}\\end{array}$ /og(K/so)  Then, wth probablity at least - 81 it holds that ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\sum_{t\\in[T]}\\langle f_{t},x\\rangle-f_{t}(a_{t})\\leq4\\sqrt{K T\\log(^{K}/\\delta_{1})},\\qquad\\forall x\\in\\bigcap_{t\\in[\\![T]\\!]}\\widehat{\\mathcal{X}}_{t}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Proof. Let us define the negative entropy for a vector $\\boldsymbol{x}\\in\\mathbb{R}_{\\ge0}^{K}$ \u22650as: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\Psi(x):=\\sum_{a\\in[\\![K]\\!]}x(a)\\left(\\log(x(a))-1\\right)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and the Bregman divergence using $\\Psi$ can be written as ", "page_idx": 12}, {"type": "equation", "text": "$$\nB(x||y):=\\Psi(x)-\\Psi(y)-\\langle\\nabla\\Psi(y),x-y\\rangle.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "For the Bregman divergence it holds the following: ", "page_idx": 12}, {"type": "text", "text": "Claim B.1 ([19]). For any $z_{1},z_{2}$ , and $z_{3}$ , it holds: ", "page_idx": 12}, {"type": "equation", "text": "$$\nB(z_{1}||z_{2})+B(z_{2}||z_{3})-B(z_{1}||z_{3})=\\langle z_{1}-z_{2},\\nabla\\Psi(z_{3})-\\nabla\\Psi(z_{2})\\rangle.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Moreover, given $z$ , define $z^{\\prime}=\\mathrm{arg}\\,\\mathrm{min}_{\\bar{z}\\in K}\\,B(\\bar{z}||z)$ . Then: ", "page_idx": 12}, {"type": "equation", "text": "$$\nB(\\tilde{z}||z^{\\prime})\\leq B(z^{\\prime}||z)+B(\\tilde{z}||z^{\\prime})\\leq B(\\tilde{z}||z)\\quad\\forall\\tilde{z}\\in\\mathcal{K}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "At this point, is more convenient to work with losses rather then rewards. Define $\\ell_{t}(a):=1-f_{t}(a)$ and $\\hat{\\ell}_{t}(a):=1-\\hat{f}_{t}(a)$ Note that: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\ell}_{t}(a)=1-\\hat{f}_{t}(a)=\\left\\{\\!\\!\\begin{array}{l l}{0}&{\\mathrm{if}\\ a\\neq a_{t}}\\\\ {\\frac{1-f_{t}(a)}{x_{t}(a)+\\gamma}}&{\\mathrm{if}\\ a=a_{t}.}\\end{array}\\!\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Then, it is easy to verify that $\\nabla\\Psi(x)=\\log(x)$ in which $\\log(x)$ has to be interpreted to be applied entry-wise. Simple calculations also show that $\\beta\\hat{\\ell}_{t}=\\log(x_{t})-\\log(\\hat{x}_{t+1})$ . Thus, we can apply Claim B.1 with $z_{1}=x,z_{2}=x_{t}$ and $z_{3}=\\hat{x}_{t+1}$ and this gives us the following: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\beta\\langle x_{t}-x,\\hat{\\ell}_{t}\\rangle=B(x||x_{t})+B(x_{t}||\\hat{x}_{t+1})-B(x||\\hat{x}_{t+1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Moreover using the second part of Claim B.1 in which $z={\\hat{x}}$ $z^{\\prime}=x_{t}$ \uff0c $\\tilde{z}=x$ , and $\\mathcal{K}=\\widehat{\\mathcal{X}}_{t}$ , we can conclude that $B(x||x_{t})\\leq B(x||\\hat{x}_{t})$ .Notice that here we use $x\\in\\widehat{\\mathcal{X}}_{t}$ for each $t$ . Then, we have the following chain of inequalities: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\beta\\sum_{t\\in[T]}\\langle x_{t}-x,\\hat{\\ell}_{t}\\rangle=\\displaystyle\\sum_{t\\in[T]}B(x_{t}||\\hat{x}_{t})+B(x_{t}||\\hat{x}_{t+1})-B(x||\\hat{x}_{t+1})}}&{\\quad\\mathrm{(By~Equation~(3))}}\\\\ &{}&{=B(x||x_{1})-B(x||\\hat{x}_{T+1})+\\displaystyle\\sum_{t=2}^{T-1}\\left(B(x||x_{t})-B(x||\\hat{x}_{t})\\right)+\\displaystyle\\sum_{t\\in[T]}B(x_{t}||\\hat{x}_{t+1})}\\\\ &{}&{\\leq B(x||x_{1})+\\displaystyle\\sum_{t\\in[T]}B(x_{t}||\\hat{x}_{t+1})\\,\\,\\mathrm{~(B~is~non-negative~and~}B(\\cdot||x_{t})\\leq B(\\cdot||\\hat{x}_{t}))}\\\\ &{}&{=B(x||x_{1})+\\displaystyle\\sum_{t\\in[T-1]}B(x_{t}||\\hat{x}_{t+1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Combining the two we can find that: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\beta\\sum_{t\\in[T]}\\left\\langle x_{t}-x,\\hat{\\ell}_{t}\\right\\rangle\\leq\\displaystyle\\sum_{t\\in[T]}\\left[B(x||\\hat{x}_{t})+B(x_{t}||\\hat{x}_{t+1})-B(x||\\hat{x}_{t+1})\\right]}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\leq B(x||x_{1})+\\sum_{t\\in[T]}B(x_{t}||\\hat{x}_{t+1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Now we analyze the term $B(x_{t}||\\hat{x}_{t+1})$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{B(x_{t}|\\hat{x}_{t+1})\\leq B(x_{t}|\\vert\\hat{x}_{t+1})+B(\\hat{x}_{t+1}\\vert\\vert x_{t})}\\\\ &{=\\langle x_{t}-\\hat{x}_{t+1},\\nabla\\psi(x_{t})-\\nabla\\Psi(\\hat{x}_{t+1})\\rangle}\\\\ &{=\\beta(x_{t}-\\hat{x}_{t+1},\\hat{\\xi}_{t})\\qquad\\qquad(\\nabla\\Psi(x)=\\log(x)}\\\\ &{=\\beta\\displaystyle\\sum_{\\alpha\\in[K]}x_{t}(\\alpha)(1-e^{-\\beta\\hat{\\xi}_{t}(\\alpha)})\\hat{\\xi}_{t}(\\alpha)}\\\\ &{\\leq\\beta^{2}\\displaystyle\\sum_{\\alpha\\in[K]}x_{t}(\\alpha)\\hat{\\xi}_{t}(\\alpha)^{2}}\\\\ &{\\leq\\beta^{2}\\displaystyle\\sum_{\\alpha\\in[K]}\\frac{1-f_{t}(\\alpha)}{x_{t}(\\alpha)+\\gamma}x_{t}(\\alpha)\\hat{\\xi}_{t}(\\alpha)}\\\\ &{\\leq\\beta^{2}\\displaystyle\\sum_{\\alpha\\in[K]}\\hat{\\xi}_{t}(\\alpha),}\\\\ &{\\leq\\beta^{2}\\displaystyle\\sum_{\\alpha\\in[K]}\\hat{\\xi}_{t}(\\alpha),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n(1-e^{-x}\\leq x)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where, in the last inequality, we ue that $x_{t}(a)/(x_{t}(a)\\!+\\!\\gamma)$ is at most 1. Thus, by choosing $x_{1}(a)={^1\\!/}K$ for all $a$ , we have that $B({\\dot{x}}||x_{1})\\leq\\log(K)$ and thus: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{t\\in\\mathbb{[}T\\mathbb{I}]}\\langle x_{t}-x,\\hat{\\ell}_{t}\\rangle\\leq\\frac{\\log(K)}{\\beta}+\\beta\\sum_{t\\in\\mathbb{[}T\\mathbb{I}]}\\sum_{a\\in\\mathbb{[}K\\mathbb{I}]}\\hat{\\ell}_{t}(a)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Form [25, Corollary 1] we know that with probability at least $1-\\delta_{1}$ wehave: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{t\\in\\ensuremath{[T]}}\\hat{\\ell}_{t}(a)-(1-f_{t}(a))\\leq\\frac{\\log\\bigl(K/\\delta_{1}\\bigr)}{2\\gamma}\\quad\\forall a\\in\\ensuremath{[K]}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Moreover, it is easy to verify that: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle1-f_{t}(a_{t})=\\sum_{a\\in[K]}\\mathbb{I}(a_{t}=a)(1-f_{t}(a))\\frac{x_{t}(a)+\\gamma}{x_{t}(a)+\\gamma}}\\\\ {\\displaystyle=\\sum_{a\\in[K]}\\hat{\\ell}_{t}(a)x_{t}(a)+\\gamma\\sum_{a\\in[K]}\\frac{\\ell_{t}(a)\\mathbb{I}(a_{t}=a)}{x_{t}(a)+\\gamma}}\\\\ {\\displaystyle=\\langle x_{t},\\hat{\\ell}_{t}\\rangle+\\gamma\\sum_{a\\in[K]}\\hat{\\ell}_{t}(a)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The regret is with probability at least $1-\\delta_{1}$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\displaystyle\\sum_{t\\in[T]}\\left\\{(1-f_{i}(\\alpha))-(1-\\langle x,f_{i}\\rangle)\\right\\}}\\\\ &{=\\displaystyle\\sum_{t\\in[T]}\\left\\{(1-f_{i}(\\alpha))-(x,\\hat{t}_{i})\\right\\}+\\displaystyle\\sum_{t\\in[T]}\\left\\{(x,\\hat{t}_{i})-(1-\\langle x,f_{i}\\rangle)\\right\\}}\\\\ &{\\overset{,,}{\\le}\\displaystyle\\sum_{t\\in[T]}\\left\\{(x-x,\\hat{t}_{i})+\\sum_{t\\in[T]}\\left\\{(x,\\hat{t}_{i})-(1-\\langle x,f_{i}\\rangle)\\right\\}+\\displaystyle\\sum_{t\\in[T]}\\sum_{t\\in[T]}\\sum_{t\\in[k]}\\hat{t}_{i}()\\quad\\mathrm{(Gpation~(A\\hat{p}))}}\\\\ &{\\le\\displaystyle\\sum_{t\\in[T]}\\left\\{\\sum_{t\\in[T]}\\left\\{(x,\\hat{t}_{i})+(\\gamma)+\\sum_{t\\in[T]}\\sum_{t\\in[k]}\\hat{t}_{i}()\\right\\}\\right\\}\\Big\\{(6\\pi\\mathrm{ind}\\omega\\ \\Theta\\log(6)\\log(1\\omega\\pi))}}\\\\ &{\\leq\\displaystyle\\frac{\\log(K)}{\\beta}+\\displaystyle\\frac{\\log(K/\\delta_{i})}{2\\gamma}+(\\gamma+\\beta)\\left[\\sum_{t\\in[T]}\\sum_{t\\in[k]}\\left\\{(1-f_{i}(\\alpha))+K\\frac{\\log(K/\\delta_{i})}{2\\gamma}\\right\\}\\right]}\\\\ &{\\le\\displaystyle\\frac{\\log(K)}{\\beta}+\\displaystyle\\frac{\\log(K/\\delta_{i})}{2\\gamma}+(\\gamma+\\beta)K\\frac{\\log(K/\\delta_{i})}{\\gamma+(\\gamma+\\beta)K\\log(K/\\delta_{i})}}\\\\ &{=\\displaystyle\\frac{\\log(K)}{\\beta}+\\displaystyle\\frac{\\log(K/\\delta_{i})}{2\\gamma}+2\\beta K T+2\\mathrm{KHe}\\{(\\gamma+\\beta)K\\frac{\\log(K/\\delta_{i})}{2\\gamma}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where in the last inequality we used that $\\beta=2\\gamma$ . By taking $\\begin{array}{r}{\\beta=\\sqrt{\\frac{\\log(K/\\delta_{1})}{K T}}}\\end{array}$ we obtain, that with probability at least $1-\\delta_{1}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{t\\in\\mathbb{I}\\mathbb{T}}\\left[\\langle x,f_{t}\\rangle-f_{t}(a_{t})\\right]\\leq4\\sqrt{K T\\log(K/\\delta_{1})},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "as desired. ", "page_idx": 14}, {"type": "text", "text": "C Proofs omitted from Section 5.1: How to set the optimistic bonus ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Theorem 5.1. Consider the stochastic setting. Given any $\\delta>0,$ let $b_{t}(a)$ be such that with probability atleast $1-\\delta$ it holds: ", "page_idx": 14}, {"type": "equation", "text": "$$\n|\\hat{g}_{t}^{(i)}(a)-\\bar{g}^{(i)}(a)|\\leq b_{t}(a)\\quad\\forall t\\in[\\![T]\\!],i\\in[\\![m]\\!],a\\in[\\![K]\\!].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, it holds $\\mathcal{X}^{\\star}\\subseteq\\cap_{t\\in[T]}\\widehat{\\mathcal{X}}_{t}$ with probability at least $1-\\delta$ ", "page_idx": 14}, {"type": "text", "text": "Proof. In the following, we assume that the condition in the statement of the theorem holds. Hence, our result with hold with probability $1-\\delta$ as promised. Let $x\\in\\mathcal{X}_{i}^{\\star}$ . Consider a $t\\in[T]$ and an $i\\in[m]$ . Then, consider the following inequalities: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle x,\\hat{g}_{t}^{(i)}\\rangle=\\langle x,\\hat{g}_{t}^{(i)}-\\bar{g}^{(i)}\\rangle+\\langle x,\\bar{g}^{(i)}\\rangle}\\\\ &{\\qquad\\quad\\leq\\langle x,\\hat{g}_{t}^{(i)}-\\bar{g}^{(i)}\\rangle}\\\\ &{\\qquad\\quad=\\displaystyle\\sum_{a\\in\\mathbb{I}K\\mathbb{I}}x(a)(\\hat{g}_{t}^{(i)}(a)-\\bar{g}^{(i)}(a))}\\\\ &{\\qquad\\quad\\leq\\langle x,b_{t}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n(x\\in\\mathcal{X}_{i}^{\\star})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, $\\langle x,\\hat{g}_{t}^{(i)}-b_{t}\\rangle\\le0$ which, by definition, proves that $x\\in\\widehat{\\mathcal{X}}_{t}^{(i)}$ . This concludes the proof. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Theorem 5.2. In the adversarial setting, it holds $\\mathcal{X}_{\\mathcal{O}}^{\\star}\\subseteq\\widehat{\\mathcal{X}}_{t}$ for all $t\\in[T]$ ", "page_idx": 14}, {"type": "text", "text": "Proof. In the adversarial setting, by Equation (1) we have that ", "page_idx": 14}, {"type": "equation", "text": "$$\ng_{t}^{(i)}(a^{\\mathcal{O}})\\leq-\\rho,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for all $t\\in[T]$ and constraint $i\\in[m]$ . Moreover, for each $t\\in[T],\\,i\\in[m]$ , and $a\\in[|K|]$ , it holds ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{g}_{t}^{(i)}(a)=\\sum_{\\tau\\in{\\mathcal T}_{t-1,a}}w_{t,a}^{(i)}(\\tau)\\;g_{\\tau}^{(i)}(a)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "$\\begin{array}{r}{\\sum_{\\tau\\in\\mathcal{T}_{t-1,a}}w_{t,a}^{(i)}(\\tau)\\,=\\,1}\\end{array}$ Then, for all $t\\,\\in\\,[T]$ and consraint $i\\,\\in\\,[\\![m]\\!],\\,\\hat{g}_{t}^{(i)}(a^{\\otimes})\\,\\le\\,-\\rho$ and $\\hat{g}_{t}^{(i)}(a)\\leq1$ for each $a\\neq a^{\\mathcal{B}}$ . 6 Thus, we can consider the following inequalities for any $\\tilde{x}\\in\\mathcal{X}_{\\mathcal{O}}^{\\star}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\tilde{x},\\hat{g}_{t}^{(i)}\\rangle=\\displaystyle\\frac{1}{1+\\rho}\\hat{g}_{t}^{(i)}(a^{\\mathcal{O}})+\\displaystyle\\frac{\\rho}{1+\\rho}\\langle x,\\hat{g}_{t}^{(i)}\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\displaystyle\\frac{1}{1+\\rho}(-\\rho)+\\displaystyle\\frac{\\rho}{1+\\rho}}\\\\ &{\\quad\\quad\\quad\\quad\\leq0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "thus proving that $\\tilde{x}\\in\\widehat{\\mathcal{X}}_{t}$ ", "page_idx": 14}, {"type": "text", "text": "DProofs omitted from Section 5.2: How to set the weights ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma 5.3. Given any sequence $\\{y_{t}\\}_{t\\in[T]}$ such that $y_{1}=0$ and any sequence of learning rates $\\{\\eta_{t}\\}_{t\\in[T]}$ such that $\\eta_{1}=1$ let $\\{\\hat{y}_{t}\\}_{t\\in[T]}$ be the estimator updated as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{y}_{t+1}=\\hat{y}_{t}+\\eta_{t}\\big(y_{t}-\\hat{y}_{t}\\big).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, it holds that $\\begin{array}{r c l}{\\hat{y}_{t}}&{=}&{\\sum_{\\tau=1}^{t-1}y_{\\tau}w_{t}(\\tau)}\\end{array}$ where $\\begin{array}{r l r}{w_{t}(\\tau)\\!}&{{}=}&{\\!\\eta_{\\tau}\\prod_{k=\\tau+1}^{t-1}(1\\;-\\;\\eta_{k})}\\end{array}$ \uff1aMoreover, $\\begin{array}{r}{\\sum_{\\tau=1}^{t-1}w_{t}(\\tau)=1}\\end{array}$ for any $t\\geq2$ ", "page_idx": 15}, {"type": "text", "text": "Proof. The first part of the statement is trivial as it can be easily checked that: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{y}_{t}=\\sum_{\\tau=1}^{t-1}y_{\\tau}\\left(\\eta_{\\tau}\\prod_{k=\\tau+1}^{t-1}(1-\\eta_{k})\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, we prove the second part of the lemma by induction on $t$ . The base case holds trivially as $w_{2}(1)=\\eta_{1}=1$ Moreover,assuming $\\textstyle\\sum_{\\tau=1}^{t-2}w_{\\tau}^{t}=1$ itholds: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{\\tau=1}^{t-1}w_{t}(\\tau)=\\sum_{\\tau=1}^{t-2}w_{t-1}(\\tau)(1-\\eta_{t-1})+w_{t}(t-1)=(1-\\eta_{t-1})+\\eta_{t-1}=1,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where in the second-to-last equality we use the inductive hypothesis. This concludes the proof. ", "page_idx": 15}, {"type": "text", "text": "Proposition 5.4. 1f $\\begin{array}{r}{\\eta_{t}^{(i)}(a_{t})=\\frac{1}{n_{t}(a_{t})}}\\end{array}$ for each $\\tau\\in\\mathcal{T}_{t-1,a}$ then $\\begin{array}{r}{w_{t,a}^{(i)}(\\tau)=\\frac{1}{n_{t-1}(a)}}\\end{array}$ nt-1(a) and we recover theempiricalmean estinatofor $\\begin{array}{r}{\\hat{g}_{t}^{(i)}(a)=\\frac{1}{n_{t-1}(a)}\\sum_{\\tau\\in\\mathcal{T}_{t-1,a}}g_{\\tau}^{(i)}(a).}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "Proof. Consider an $a\\in[|K|]$ , an $i\\in[m]$ , and a $t\\in[T]$ . Then, by applying Lemma 5.3 to the set of rounds $\\mathcal{T}_{t-1,a}$ we have that: ", "page_idx": 15}, {"type": "equation", "text": "$$\nw_{t,a}^{(i)}(\\tau)=\\frac{1}{n_{\\tau}(a)}\\prod_{k\\in\\mathcal{T}_{t-1,a}:k>\\tau}\\left(1-\\frac{1}{n_{k}(a)}\\right)\\quad\\forall\\tau\\in\\mathcal{T}_{t-1,a}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now, we show that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\prod_{k\\in\\mathcal{T}_{t-1,a}:k>\\tau}\\left(1-\\frac{1}{n_{k}(a)}\\right)=\\displaystyle\\prod_{k\\in\\mathcal{T}_{t-1,a}:k>\\tau}\\frac{n_{k}(a)-1}{n_{k}(a)}}&{}\\\\ {=\\displaystyle\\prod_{j=n_{\\tau}(a)+1}^{n_{t-1}(a)}\\frac{j-1}{j}}&{}\\\\ {=\\frac{n_{\\tau}(a)}{n_{t-1}(a)},}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and thus $\\begin{array}{r}{w_{t,a}^{(i)}(\\tau)=\\frac{1}{n_{t-1}(a)}}\\end{array}$ nt-i(a) , as desired. ", "page_idx": 15}, {"type": "text", "text": "Theorem 5.6. Given an interval $[t_{1},t_{2}]\\subseteq[T]$ an $i\\in[\\![m]\\!]$ and a $\\delta>0,$ with probability at least $1-\\delta$ it holds: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\gamma_{[t_{1},t_{2}]}^{(i)}\\leq\\sum_{a\\in[K]}\\sum_{\\tau\\in\\mathcal{T}_{t_{2},a}\\cap[t_{1},t_{2}]}\\frac{1}{\\eta_{\\tau}^{(i)}(a)}\\left(\\hat{g}_{\\tau+1}^{(i)}(a)-\\hat{g}_{\\tau}^{(i)}(a)\\right)+\\sum_{\\tau=t_{1}}^{t_{2}}\\langle x_{\\tau},b_{\\tau}\\rangle+4\\sqrt{(t_{2}-t_{1})\\log(1/\\delta)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. First, applying Lemma G.1, we have that with probability $1-\\delta$ it holds: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{\\tau=t_{1}}^{t_{2}}\\langle g_{\\tau}^{(i)},x_{\\tau}\\rangle\\ge\\sum_{\\tau=t_{1}}^{t_{2}}g_{\\tau}^{(i)}(a_{\\tau})-4\\sqrt{(t_{2}-t_{1})\\log(^{1/\\delta}\\!)}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Consider the following chain of inequalities: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{V_{[\\imath,\\,]_{\\ell}}^{(i)}=}&{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last equality follows by the definition of the update: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{g}_{\\tau+1}^{(i)}(a)=\\left(1-\\eta_{\\tau}^{(i)}(a)\\right)\\hat{g}_{\\tau}^{(i)}(a)+\\eta_{\\tau}^{(i)}(a)g_{\\tau}^{(i)}(a)\\quad\\mathrm{for}\\;a=a_{\\tau}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 16}, {"type": "text", "text": "Corollary 5.7. Given an interval $[t_{1},t_{2}]\\subseteq\\mathbb{[T]}$ $a\\ i\\in[\\![m]\\!]$ , and a $\\delta\\,>\\,0$ assume that for any $a\\in[|K|]$ it holds $\\eta_{\\tau}^{(i)}(a)\\geq\\eta_{\\tau^{\\prime}}^{(i)}(a)\\,\\forall\\tau<\\tau^{\\prime}\\in T_{t_{2},a}\\cap[t_{1},t_{2}]$ Then, with probability at least $1-\\delta$ \u00fc holds: ", "page_idx": 16}, {"type": "equation", "text": "$$\nV_{[t_{1},t_{2}]}^{(i)}\\leq\\sum_{a\\in[K]}\\frac{2}{\\eta_{\\ell(a,[t_{1},t_{2}])}^{(i)}(a)}+\\sum_{\\tau=t_{1}}^{t_{2}}\\langle x_{\\tau},b_{\\tau}\\rangle+4\\sqrt{(t_{2}-t_{1})\\log(^{1/\\delta})}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. We assume that Theorem 5.6 holds, and hence our statement holds with probability $1-\\delta$ Then, to prove the statement it is sufficient to show that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{a\\in[K]}\\sum_{\\substack{\\tau\\in\\mathcal{T}_{t_{2},a}\\cap[t_{1},t_{2}]}}\\frac{1}{\\eta_{\\tau}^{(i)}(a)}\\left(\\hat{g}_{\\tau+1}^{(i)}(a)-\\hat{g}_{\\tau}^{(i)}(a)\\right)\\leq\\sum_{a\\in[K]}\\sum_{\\substack{\\tau\\in\\mathcal{T}_{t_{2},a}\\cap[t_{1},t_{2}]}}\\frac{1}{\\eta_{t_{2}}^{(i)}(a)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Fix any $a\\,\\in\\,[|K|]$ , and let $k=|T_{t_{2},a}\\cap[t_{1},t_{2}]|$ be the number of times action $a$ is played in the interval $[t_{1},t_{2}]$ . Moreover, let $\\tau(j)$ be the rounds in which action $a$ is played the $j$ -th time in the interval $[t_{1},t_{2}]$ . Then: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\sum_{j\\in\\mathbb{F}_{\\neq_{j}}}\\frac{1}{\\eta_{\\ell}^{\\star}(k,1)}\\frac{1}{\\eta_{\\ell}^{\\star}(k)}\\left(\\widehat{g}_{\\ell^{\\star}(1)}^{(i)}(a)-\\widehat{g}_{\\ell^{\\star}}^{(i)}(a)\\right)}\\\\ &{\\qquad=\\sum_{j\\in\\mathbb{F}_{k-1}}\\frac{1}{\\eta_{\\ell}^{\\star}(k,1)}\\frac{1}{(\\eta_{\\ell}^{\\star}(k,1))}\\left(\\widehat{g}_{\\ell^{\\star}(j)+1}^{(i)}(a)-\\widehat{g}_{\\ell^{\\star}(j)}^{(i)}(a)\\right)+\\frac{1}{\\eta_{\\ell}^{\\star}(k,1)}\\left(\\widehat{g}_{\\ell^{\\star}(k)+1}^{(i)}(a)-\\widehat{g}_{\\ell^{\\star}(k)}^{(i)}(a)\\right)}\\\\ &{\\qquad\\le\\sum_{j\\in\\mathbb{F}_{k-1}}\\left(\\frac{1}{\\eta_{\\ell}^{\\star}(k,1)}\\widehat{g}_{\\ell^{\\star}(j+1)}^{(i)}(a)-\\frac{1}{\\eta_{\\ell}^{\\star}(i)}\\widehat{g}_{\\ell^{\\star}(j)}^{(i)}(a)\\right)+\\frac{1}{\\eta_{\\ell}^{\\star}(k,1)}\\left(\\widehat{g}_{\\ell^{\\star}(k)+1}^{(i)}(a)-\\widehat{g}_{\\ell^{\\star}(k)}^{(i)}(a)\\right)}\\\\ &{\\qquad=\\frac{1}{\\eta_{\\ell}^{\\star}(k,1)}\\partial_{\\ell}^{i}(\\widehat{v}_{i}^{(i)}(a)-\\frac{1}{\\eta_{\\ell}^{\\star}(1)}\\widehat{a}_{\\ell^{\\star}(1)}^{(i)}(a)}\\\\ &{\\qquad\\le\\frac{2}{\\eta_{\\ell}^{\\star}(k,1)}\\alpha}\\\\ &{\\qquad=\\frac{2}{\\eta_{\\ell}^{\\star}(k,1)}\\frac{2}{(\\eta_{\\ell}^{\\star}(k,1))(a)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Summing over all the actions we obtain the desired inequality. ", "page_idx": 16}, {"type": "text", "text": "Lemma 5.8. Given a $c>0$ an $\\alpha\\,\\in\\,(0,1)$ a\uff0c $t\\,\\in\\,[T]$ and a $\\delta\\,>\\,0$ let $\\begin{array}{r}{b_{t}(a)\\,=\\,\\frac{c}{n_{t}(a)^{\\alpha}}}\\end{array}$ for all $a\\in[|K|]$ . Then, with probability at least $1-\\delta$ , it holds: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{\\tau=1}^{t}\\langle x_{\\tau},b_{\\tau}\\rangle\\leq\\frac{c}{1-\\alpha}K^{\\alpha}t^{1-\\alpha}+4\\sqrt{t\\log(^{1/\\delta})}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Consider the following inequalities: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\displaystyle\\sum_{\\tau=1}^{t}b_{\\tau}(a_{\\tau})=c\\sum_{a\\in[K]}\\sum_{\\tau\\in[t]}\\frac{1}{n_{\\tau}(a)^{\\alpha}}\\mathbb{I}(a_{\\tau}=a)}\\\\ &{\\quad}&{\\displaystyle=c\\sum_{a\\in[K]}\\sum_{k=1}^{n_{\\tau}(a)}\\frac{1}{k^{\\alpha}}}\\\\ &{\\quad}&{\\displaystyle\\leq\\frac{c}{1-\\alpha}\\sum_{a\\in[K]}n_{t}(a)^{1-\\alpha}}&&{(\\sum_{k=1}^{N}k^{-\\alpha}\\leq\\int_{0}^{N}x^{-\\alpha}d x)}\\\\ &{\\quad}&{\\displaystyle\\leq\\frac{c}{1-\\alpha}K^{\\alpha}t^{1-\\alpha}}&&{(\\mathrm{fensen^{\\alpha}~s i n e q u a l i t y})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The proof is concluded by using Lemma G.1. ", "page_idx": 17}, {"type": "text", "text": "E Proofs omitted from Section 6 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Theorem 6.1. Both in the stochastic and the adversarial setting, with probability at least $1-2m T^{2}\\delta_{2}$ itholdsthat ", "page_idx": 17}, {"type": "equation", "text": "$$\nV_{t}\\leq53\\sqrt{K t\\log(2/\\delta_{2})}\\quad\\forall t\\in\\lVert T\\rVert.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. We prove that given an $i\\in[\\![m]\\!]$ , it holds: ", "page_idx": 17}, {"type": "equation", "text": "$$\nV_{t}^{(i)}\\leq53\\sqrt{K t\\log(2/\\delta_{2})}\\quad\\forall t\\in\\lVert T\\rVert\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with probability $1-2T^{2}\\delta_{2}$ . Then, a union bound over $i$ completes the proof. ", "page_idx": 17}, {"type": "text", "text": "Given an $i\\,\\in\\,[m]$ : we first assume some high-probability events. In particular, we assume that Corollary 5.7 with $\\delta=\\delta_{2}$ holds for any interval, and that Lemma 5.8 with $\\delta=\\delta_{2}$ holds for all $t\\in[T]$ This happens with probability a leat $1\\!-\\!2T^{2}\\delta_{2}$ We consider two cases. If $V_{t}^{(i)}\\leq53\\sqrt{K T\\log(\\underline{{2}}/\\delta_{2})}$ for all $\\bar{t}\\in[T]$ , then the statement it is trivially satisfied. Otherwise, there exists an a time $\\bar{t}$ for which $V_{\\bar{t}}^{(i)}\\,\\geq\\,53\\sqrt{{K t}\\log(^{1/\\delta_{2}})}$ . Clearly, this implies that there exists a $\\underline{{t}}<\\bar{t}$ such that $V_{t}^{(i)}\\geq$ $42\\sqrt{K t\\log(^{2}\\!/\\delta_{2})}$ for all $t\\in[\\underline{{t}},\\bar{t}]$ and $V_{\\underline{{t}}-1}^{(i)}\\leq42\\sqrt{K t\\log(^{1}/\\delta_{2})}$ Since $V_{t}^{(i)}\\geq42\\sqrt{K t\\log(1/\\delta_{2})}$ for all $t\\in[\\underline{{t}},\\bar{t}]$ we have that: ", "page_idx": 17}, {"type": "equation", "text": "$$\nV_{t}^{(i)}-21\\sqrt{K t\\log(1/\\delta_{2})}\\geq42\\sqrt{K t\\log(1/\\delta_{2})}-21\\sqrt{K t\\log(1/\\delta_{2})}\\geq21\\sqrt{K t\\log(1/\\delta_{2})}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and thus $\\Gamma_{t}^{(i)}=21\\sqrt{K t\\log(1/\\delta_{2})}$ forall $t\\in[\\underline{{t}},\\bar{t}]$ .Hence, on the interval $t\\in[\\underline{{t}},\\bar{t}]$ we known that the learning rate can be lower bounded by a non-increasing function of time as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\eta_{t}^{(i)}(a_{t})=\\frac{1+21\\sqrt{K t\\log(^{1/\\delta_{2}})}}{n_{t}(a_{t})}\\geq21\\sqrt{\\frac{K\\log(^{1/\\delta_{2}})}{n_{t}(a_{t})}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This let us use Corollary 5.7 (that we assumed to hold) to show that: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{V_{[\\ell,\\bar{t}]}^{(i)}\\le\\displaystyle\\frac{2}{21\\sqrt{K\\log(1/\\delta_{2})}}\\sum_{a\\in[K]}\\sqrt{n_{\\bar{t}}(a)}+\\displaystyle\\sum_{\\tau=\\underline{{{t}}}}^{\\bar{t}}\\langle x_{\\tau},b_{\\tau}\\rangle+4\\sqrt{t\\log(1/\\delta_{2})}}&{}\\\\ &{\\le\\displaystyle\\frac{2\\sqrt{K\\bar{t}}}{21\\sqrt{K\\log(1/\\delta_{2})}}+\\displaystyle\\sum_{\\tau=\\underline{{{t}}}}^{\\bar{t}}\\langle x_{\\tau},b_{\\tau}\\rangle+4\\sqrt{t\\log(1/\\delta_{2})}}&{\\mathrm{(fensen`s~inequalit~}}\\\\ &{\\le\\displaystyle\\frac{2\\sqrt{K\\bar{t}}}{21\\sqrt{K\\log(1/\\delta_{2})}}+2\\sqrt{2K t\\log(2/\\delta_{2})}+8\\sqrt{t\\log(1/\\delta_{2})}}&{\\mathrm{(Lemma~5.}}\\\\ &{\\le(1/\\mathfrak{l}_{0}+10)\\sqrt{K t\\log(2/\\delta_{2})}.}&\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now, $V_{\\bar{t}}^{(i)}\\leq V_{\\underline{{\\bar{t}}}}+V_{[\\underline{{\\bar{t}}},\\bar{t}]}^{(i)}\\leq(42+1/10+10)\\sqrt{K t\\log(2/\\delta)}<53\\sqrt{K t\\log(2/\\delta)}.$ We thus reached a contradiction and there is no such a $\\bar{t}$ . The union bound on all $i\\in[\\![m]\\!]$ concludes the proof. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Lemma 6.2. In the stochastic setting, with probability at least $1-5m K T\\delta_{2}$ itholdsthat: ", "page_idx": 18}, {"type": "equation", "text": "$$\n|\\hat{g}_{t}^{(i)}(a)-\\bar{g}_{t}^{(i)}(a)|\\leq b_{t}(a)\\quad\\forall a\\in[K],t\\in[T],i\\in[m]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. First, we show some concentration inequalities that will be useful in the following. By an Hoeffding's inequality and an union bound with probability at least $1-m K T\\delta_{2}$ , it holds: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{n_{t-1}(a)}\\sum_{\\tau\\in T_{t-1,a}}g_{\\tau}^{(i)}(a)-\\bar{g}^{(i)}a\\right|\\leq\\sqrt{\\frac{2\\log(2/\\delta_{2})}{n_{t}(a)}}\\quad\\forall t\\in\\ensuremath{[\\![T]\\!]},k\\in\\ensuremath{[\\![K]\\!]},i\\in\\ensuremath{[\\![m]\\!]}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Moreover, by Lemma Lemma G.1 and an union bound, with probability at least $1-m T\\delta_{2}$ , it holds: ", "page_idx": 18}, {"type": "equation", "text": "$$\nV_{t}^{(i)}\\leq\\sum_{\\tau=1}^{t-1}\\langle x_{\\tau},g_{\\tau}^{(i)}\\rangle+4\\sqrt{t\\log(^{1}\\!/\\delta_{2})}\\quad\\forall t\\in[T],i\\in[m]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Similarly, by Lemma Lemma G.1 and an union bound, with probability at least $1-m T\\delta_{2}$ , it holds: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{\\tau=1}^{t}\\langle x_{\\tau},\\bar{g}_{\\tau}^{(i)}\\rangle\\leq\\sum_{\\tau=1}^{t}\\bar{g}^{(i)}(a_{\\tau})+4\\sqrt{t\\log(^{1}/\\delta_{2})}\\quad\\forall t\\in\\lVert T\\rVert,i\\in\\lVert m\\rVert\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By Lemma G.2 and an union bound, with probability at least $1-m T\\delta_{2}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{\\tau=1}^{t}\\langle x_{\\tau},g_{\\tau}^{(i)}\\rangle\\leq\\sum_{\\tau=1}^{t}\\langle x_{\\tau},\\bar{g}^{(i)}\\rangle+4\\sqrt{t\\log(^{1}/\\delta_{2})}\\quad\\forall t\\in\\lVert T\\rVert,i\\in\\lVert m\\rVert\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Finally, by Lemma 5.8 and an union bound, with probability $1-T\\delta_{2}$ , it holds: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{\\tau=1}^{t}\\langle x_{\\tau},b_{\\tau}\\rangle\\leq2\\sqrt{2K t\\log(2/\\delta_{2})}+4\\sqrt{t\\log(1/\\delta_{2})}\\quad\\forall t\\in\\lVert T\\rVert\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In the following, we will assume the the previous events hold, and hence our result holds with probability at least $1-5m K T\\delta_{2}$ ", "page_idx": 18}, {"type": "text", "text": "First, we show that $V_{t}^{i}\\,\\leq\\,21\\sqrt{K t\\log\\bigl(2/\\delta_{2}\\bigr)}$ for each $t$ and $i$ . Our proof works by induction on $t$ . Clearly, the inequality holds for $t\\,=\\,1$ . Now, assume that it holds for all $\\tau\\leq t-1$ . By the defnitionof,theinductionassmpionimliestat(@)=(@ for all $a\\in[|K|]$ $i\\in[m]$ and $\\tau\\leq t-1$ . Then, thanks to Proposition 5.4 we have that: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{g}_{\\tau}^{(i)}(a)=\\frac{1}{n_{\\tau-1}(a)}\\sum_{\\hat{t}\\in\\mathcal{T}_{\\tau-1,a}}g_{\\hat{t}}^{(i)}(a)\\quad\\forall\\tau\\leq t-1.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hence, by Equation (10), it holds that: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left|\\hat{g}_{\\tau}^{(i)}(a)-\\bar{g}^{(i)}(a)\\right|\\leq\\sqrt{\\frac{2\\log(2/\\delta_{2})}{n_{\\tau}(a)}}\\quad\\forall\\tau\\leq t-1.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and thus that $\\widehat{\\mathcal{X}}_{\\tau}^{(i)}\\neq\\{\\varnothing\\}$ for all $\\tau\\leq t-1$ .Assmin that the ents abvehls, conidew following inequalities: ", "page_idx": 19}, {"type": "equation", "text": "$$\n(g_{t}^{(i)}(a)\\leq1)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n(x_{\\tau}\\in\\hat{\\mathcal{X}}_{\\tau}^{(i)})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Phi^{\\prime}=-\\frac{1}{n}(x_{n}^{2}+\\bar{x}_{n}^{2}(n))}&{}\\\\ {\\bar{\\Phi}^{\\prime}=\\frac{1}{n}(x_{n}^{2}+\\bar{x}_{n}^{2}(n)+\\bar{x}_{n}^{2}(n)+4\\bar{x}_{n}^{2}\\Re{[\\bar{x}]_{n}})}\\\\ &{\\quad\\le\\frac{1}{n}(x_{n}^{2}+\\bar{x}_{n}^{2}(n)-\\bar{x}_{n}^{2}(n)+\\bar{x}_{n}^{2}(n)+4\\bar{x}_{n}^{2}(\\log(\\bar{x}_{n})+\\bar{x}_{n}^{2}(\\log(\\bar{x}_{n})}\\\\ &{\\quad+\\frac{1}{n!}\\log(\\bar{x}_{n}^{2}(n)-\\bar{x}_{n}^{2}(\\log(\\bar{x}_{n})+\\bar{x}_{n}^{2}(\\log(\\bar{x}_{n})+\\bar{x}_{n}^{2}(\\log(\\bar{x}_{n})}\\\\ &{\\quad+\\frac{1}{n!}\\log(\\bar{x}_{n}^{2}(n)-\\bar{x}_{n}^{2}(\\log(\\bar{x}_{n})+1)+\\bar{x}_{n}^{2}(\\log(\\bar{x}_{n})+\\bar{x}_{n}^{2}(\\log(\\bar{x}_{n})}\\\\ &{\\quad+\\frac{1}{n!}\\log(\\bar{x}_{n}^{2}(n)-\\bar{x}_{n}^{2}(\\log(\\bar{x}_{n})+1)+1)\\times\\sqrt{n!\\log(\\bar{x}_{n}^{2}(n)}}\\\\ &{\\quad\\le\\frac{1}{n!}\\log(\\bar{x}_{n}^{2}(n)-\\bar{x}_{n}^{2}(\\log(\\bar{x}_{n})(\\log(\\bar{x}_{n})+1+2\\bar{x}_{n}^{2}(\\log(\\bar{x}_{n}))}\\\\ &{\\quad\\le\\frac{1}{n!}(x_{n}^{2}(\\log(\\bar{x}_{n})-\\bar{x}_{n}^{2}(\\log(\\bar{x}_{n})+2)\\sqrt{n!\\log(\\bar{x}_{n}^{2}(n)+1)+1}+1)\\log(\\log(\\bar{x}_{n}^{2}(\\log(\\bar{x}_{n\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and thus $V_{t}^{(i)}\\leq21\\sqrt{K t\\log(2/\\delta_{2})}$ ", "page_idx": 19}, {"type": "text", "text": "Thus $\\Gamma_{t}^{(i)}=0$ and $\\hat{g}_{t}^{(i)}(a)$ isthpiralmaf pasteatiohisonlestt step, showingthat $V_{t}^{i}\\leq21\\sqrt{K t\\log(1/\\delta_{2})}$ for all $t\\in[T]$ and ${\\Gamma}_{t}^{\\left(i\\right)}=0$ forall $t\\in[T]$ and $i\\in[m]$ Now, we proved that with probability $1-3m K T\\delta_{2}$ , all $\\Gamma_{t}^{(i)}=0$ , and hence by Equation (10) we have that: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left|{\\hat{g}}_{t}^{(i)}(a)-{\\bar{g}}^{(i)}(a)\\right|\\leq{\\sqrt{\\frac{2\\log(2/\\delta_{2})}{n_{t}(a)}}}\\quad\\forall i\\in\\left[\\![m]\\!],t\\in\\left[\\![T]\\!\\right]\\!,a\\in\\left[\\![K]\\!\\right]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "as desired. ", "page_idx": 19}, {"type": "text", "text": "F Proofs omitted from Section 7 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Theorem 7.1. In the stochastic setting, for any $\\epsilon>0$ Algorithm $^{\\,l}$ guarantees that with probability at least $1-\\epsilon$ ", "page_idx": 19}, {"type": "equation", "text": "$$\nR_{T}\\leq4\\sqrt{K T\\log(2K/\\epsilon)}\\quad a n d\\quad V_{t}\\leq53\\sqrt{K t\\log(28m K T^{2}/\\epsilon)}\\quad\\forall t\\in\\lVert T\\rVert.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. To prove the upper bound on the regret, we simply have to combine Corollary 6.3 with Theorem 4.1. By Corollary 6.3 which probability at least $1-5m K T\\delta$ ,it holds $\\chi^{\\star}\\subseteq\\cap_{t\\in[T]}\\widehat{\\chi}_{t}$ ", "page_idx": 19}, {"type": "text", "text": "Moreover, by Theorem 4.1, we have that for each $x\\in\\mathcal{X}^{\\star}$ with probability at least $1-\\delta_{1}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{t\\in[T]}\\langle f_{t},x\\rangle-f_{t}(a_{t})\\leq4\\sqrt{K T\\log(^{K/\\delta_{1}})}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let $\\begin{array}{r}{x^{\\star}=\\arg\\operatorname*{max}_{x\\in\\mathcal{X}^{\\star}}\\sum_{t\\in[\\![T]\\!]}\\langle x,f_{t}\\rangle}\\end{array}$ Then,byuudwehav that wih probabi at $1-5m K T\\delta_{2}-\\delta_{1}$ it holds: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{t\\in\\mathbb{Z}\\Vert}\\langle f_{t},x^{\\star}\\rangle-f_{t}(a_{t})\\leq4\\sqrt{K T\\log(K/\\delta_{1})},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "proving the bound on the regret. The bound on the violations holds with probability at least $1-$ $\\dot{2}m T^{2}\\bar{\\delta}_{2}$ by Theorem 6.1, and guarantees: ", "page_idx": 20}, {"type": "equation", "text": "$$\nV_{t}\\leq53\\sqrt{K t\\log(2/\\delta_{2})}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By an union bounds on all events, theguaranteeshold with probability at east $1-7m K T^{2}\\delta_{2}-\\delta_{1}$ Thus by taking $\\delta_{1}=\\epsilon/2$ and $\\delta_{2}=\\epsilon/(14m K T^{2})$ we obtain the desired result. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Theorem 7.2. In the adversarial setting, for any $\\epsilon>0$ Algorithm $^{\\,l}$ guarantees that with probability atleast $1-\\epsilon$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\alpha\\cdot R_{T}\\leq4\\sqrt{K T\\log(^{2K}/\\epsilon)}\\quad a n d\\quad V_{t}\\leq53\\sqrt{K t\\log(28m K T^{2}/\\epsilon)}\\quad\\forall t\\in\\ensuremath{[T]},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\alpha=\\rho/(1{+}\\rho)$ ", "page_idx": 20}, {"type": "text", "text": "Proof. Combining Theorem 5.2 and Theorem 4.1 readily proves that with probability at least $1-\\delta_{1}$ we have that for all $\\tilde{x}\\in\\mathcal{X}_{\\mathcal{O}}^{\\star}\\subseteq\\widehat{\\mathcal{X}}_{t}$ ,wehave: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{t\\in[T]}\\langle f_{t},\\tilde{x}\\rangle-f_{t}(a_{t})\\leq4\\sqrt{K T\\log(^{K/\\delta_{1}})}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let $\\begin{array}{r}{\\boldsymbol{x}^{\\star}=\\arg\\operatorname*{max}_{\\boldsymbol{x}\\in\\Delta_{K}}\\sum_{t\\in\\|T\\|}\\langle\\boldsymbol{x},f_{t}\\rangle}\\end{array}$ Then, observethat $\\begin{array}{r}{\\bar{x}\\,=\\,\\frac{1}{1+\\rho}x^{\\otimes}+\\frac{\\rho}{1+\\rho}x^{*}\\,\\in\\,\\mathcal{X}^{\\otimes}}\\end{array}$ where $x^{\\mathcal{Q}}(a^{\\mathcal{Q}})=1$ and $x^{\\mathcal{B}}(a)=0$ for each $a\\neq a^{\\mathcal{B}}$ . Then, we have that: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{t\\in[T]}\\langle\\bar{x},f_{t}\\rangle=\\sum_{t\\in[T]}\\left\\langle\\frac{1}{1+\\rho}x^{\\mathcal{O}}+\\frac{\\rho}{1+\\rho}x^{\\star},f_{t}\\right\\rangle\\geq\\frac{\\rho}{1+\\rho}\\sum_{t\\in[T]}\\langle x^{\\star},f_{t}\\rangle.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "since $f_{t}(a^{\\mathcal{D}})\\geq0$ . This proves that with probability at least $1-\\delta_{1}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\frac{\\rho}{1+\\rho}\\right)-R_{T}\\leq4\\sqrt{K T\\log(^{K}/\\delta_{1})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Similarly to the proof of Theorem 7.1, we can prove that the bound on the violations holds with probability at least $1-2m T^{2}\\delta_{2}$ by Theorem 6.1, and give: ", "page_idx": 20}, {"type": "equation", "text": "$$\nV_{t}\\leq53\\sqrt{K t\\log(2/\\delta_{2})}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Overall these events hold with probability at least $1\\,-\\,2m T^{2}\\delta_{2}\\,-\\,\\delta_{1}$ .By defining $\\delta_{1}\\,=\\,\\epsilon/2$ and $\\delta_{2}=\\epsilon/(14m K T^{2})$ we have that the desired results hold with probability at least $1-\\epsilon$ \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Theorem 7.3. Algorithm $^{\\,l}$ , in the stochastic seting, guarantees that with probability at least $1-\\epsilon$ itholds that: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{V}_{t}^{+}\\leq16\\sqrt{K t\\log(28m K T^{2}/\\epsilon)}\\quad\\forall t\\in[[T]].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Define for each $i\\in[m]$ and $t\\in[T]$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{V}_{t}^{i,+}:=\\sum_{\\tau=1}^{t}\\left[\\langle x_{\\tau},\\bar{g}_{\\tau}^{(i)}\\rangle\\right]^{+}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, given an $i$ and a $t$ consider the following chain of inequalities: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\gamma_{t}^{i,+}}&{=\\displaystyle\\frac{\\hat{\\gamma}}{r\\ln\\hat{r}}\\left[\\left(x,\\hat{y}_{t}^{(i)}\\right)^{+}\\right.}\\\\ &{\\left.\\phantom{\\frac{1}{r}}\\!\\!\\!\\!-\\sum_{t=1}^{r}\\left[\\left(x,\\hat{y}_{t}^{(i)}-\\hat{y}_{t}^{(i)}+\\hat{y}_{t}^{(i)}\\right)^{+}\\right.\\right.}\\\\ &{\\left.\\phantom{\\frac{1}{r}}\\!\\!\\!\\!-\\sum_{t=1}^{r}\\left[\\left(x,\\hat{y}_{t}^{(i)}-\\hat{y}_{t}^{(i)}\\right)+\\left.\\left(x,\\hat{y}_{t}^{(i)}\\right)^{+}\\right.\\right.}\\\\ &{\\left.\\phantom{\\frac{1}{r}}\\!\\!\\!\\!-\\sum_{t=1}^{r}\\left[\\left(x,\\hat{y}_{t}^{(i)}-\\hat{y}_{t}^{(i)}\\right)+\\left.(x,\\hat{y}_{t})^{+}\\right.\\right.}\\\\ &{\\left.\\left.\\phantom{\\frac{1}{r}}\\!\\!\\!\\!-\\sum_{t=1}^{r}\\left[\\left(x,\\hat{y}_{t}^{(i)}-\\hat{y}_{t}^{(i)}\\right)+\\left.(x,\\hat{y}_{t})^{+}\\right.\\right.\\right]^{+}}\\\\ &{\\left.\\left.\\phantom{\\frac{1}{r}}\\!\\!\\!\\!-\\sum_{t=1}^{r}\\left[\\left(x,\\hat{y}_{t}^{(i)}-\\hat{y}_{t}^{(i)}\\right)^{+}+\\left.(x,\\hat{y}_{t})^{-}\\right]^{+}\\right.}\\\\ &{\\left.\\left.\\kern-\\nulldelimiterspace+\\underbrace{\\frac{1}{r}}\\!\\!\\!-\\Biggl.\\left(x,\\hat{y}_{t}\\right)^{+}\\right.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where last inequality both hold with probability $1-5m K T\\delta_{2}$ jointly for each $i$ and $t$ ", "page_idx": 21}, {"type": "text", "text": "Since bt = \u221a ant-1(a) we can apply Lemma 5.8 and an union bound on all $t$ to find that with probability at least $1-T\\delta_{2}-5m K T\\delta_{2}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{V}_{t}^{i,+}\\leq4\\sqrt{2K t\\log(^{2}/\\delta_{2})}+8\\sqrt{t\\log(^{1}/\\delta_{2})}\\quad\\forall i\\in[[m],t\\in[T]].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, we can conclude that: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{V}_{t}^{+}\\leq16\\sqrt{K t\\log(2/\\delta_{2})}\\quad\\forall i\\in[\\![m]\\!],t\\in[\\![T]\\!]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "with probability at least $1-6m K T\\delta_{2}$ . Recalling that $\\delta_{2}=\\epsilon/(14m K T^{2})$ we obtain the result. ", "page_idx": 21}, {"type": "text", "text": "G Further technical lemmas ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Lemma G.1. For any sequence of function $r_{t}:\\lVert K\\rVert\\to[-1,1]$ which is $t-1$ predictable and any sequence of randomized strategy $x_{t}\\in\\Delta_{K}$ , it holds that with probability at least $1-\\delta$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left|\\sum_{t\\in\\mathbb{I}\\mathbb{T}\\mathbb{J}}\\langle x_{t},r_{t}\\rangle-\\sum_{t\\in\\mathbb{I}\\mathbb{T}\\mathbb{J}}r_{t}(a_{t})\\right|\\leq4\\sqrt{T\\log(1/\\delta)}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. By definition $\\begin{array}{r}{\\mathbb{E}_{a\\sim x_{t}}[r_{t}(a)]\\,=\\,\\sum_{a\\in[[K]]}r_{t}(a)x_{t}(a)\\,=\\,\\langle x_{t},r_{t}\\rangle}\\end{array}$ . Thus the sequence $X_{t}\\ =$ $\\begin{array}{r}{\\sum_{\\tau=1}^{t}[r_{\\tau}(a_{\\tau})-\\langle x_{\\tau},r_{\\tau}\\rangle]}\\end{array}$ is a martingale and $|X_{t}-X_{t-1}|\\leq2$ Thus we can apply Azumainequality and find that with probability at least $1-\\delta$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left|\\sum_{t\\in\\mathbb{I}\\mathbb{T}\\mathbb{J}}\\langle x_{t},r_{t}\\rangle-\\sum_{t\\in\\mathbb{I}\\mathbb{T}\\mathbb{J}}r_{t}(a_{t})\\right|\\leq4\\sqrt{T\\log(^{1/\\delta})}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma G.2. For any sequence of randomized strategy $x_{t}\\in\\Delta_{K}$ and any function $\\bar{r}(a)$ such that $r_{t}(a)$ are sampled from a distributionwith mean $\\bar{r}(a)$ ,i.e., $\\mathbb{E}[r_{t}(a)]=\\bar{r}(a)$ and $\\mathbb{P}(|r_{t}(a)|\\le1)=1$ it holds that with probability at least $1-\\delta$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left|\\sum_{t\\in\\mathbb{I}\\mathbb{T}\\mathbb{J}}\\left\\langle x_{t},r_{t}\\right\\rangle-\\sum_{t\\in\\mathbb{I}\\mathbb{T}\\mathbb{J}}\\left\\langle x_{t},\\bar{r}\\right\\rangle\\right|\\leq4\\sqrt{T\\log(1/\\delta)}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. This holds by simple application of Azuma's inequality, similarly to the proof of Lemma G.1. ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We included the main contributions and scope in the abstract ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Yes, the paper discusses limitations. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Assumptions are explicitly discussed and all the proofs are provided. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper is theoretical and we do not have any experimental results. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper is theoretical and we do not have any experimental results. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https : / /nips . CC/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (ht tps : / /nips.cc/public/guides/CodeSubmissionPolicy)for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper is theoretical and we do not have any experimental results Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper is theoretical and we do not have any experimental results Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of themean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper is theoretical and we do not have any experimental results Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: The paper conforms. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper is theoretical without any immediate societal impacts. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 25}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 26}, {"type": "text", "text": "\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: No data has been used in this paper. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: No data has been used in this paper. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/ dataset s has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: No new assets have been introduced in this paper. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: No crowdsourcing have been used in this paper. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with humansubjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: No crowdsourcing have been used in this paper. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]