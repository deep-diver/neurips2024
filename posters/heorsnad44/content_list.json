[{"type": "text", "text": "Unified Guidance for Geometry-Conditioned Molecular Generation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sirine Ayadi\u22171,2 Leon Hetzel\u22171,2,3 Johanna Sommer\u22171,2 ", "page_idx": 0}, {"type": "text", "text": "Fabian Theis1,2,3 Stephan G\u00fcnnemann1, ", "page_idx": 0}, {"type": "text", "text": "1 School of Computation, Information and Technology, Technical University of Munich 2 Munich Data Science Institute, Technical University of Munich 3 Center for Computation Health, Helmholtz Munich {si.ayadi, l.hetzel, jm.sommer, f.theis, s.guennemann}@tum.de ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Effectively designing molecular geometries is essential to advancing pharmaceutical innovations, a domain, which has experienced great attention through the success of generative models and, in particular, diffusion models. However, current molecular diffusion models are tailored towards a specific downstream task and lack adaptability. We introduce UniGuide, a framework for controlled geometric guidance of unconditional diffusion models that allows flexible conditioning during inference without the requirement of extra training or networks. We show how applications such as structure-based, fragment-based, and ligand-based drug design are formulated in the UniGuide framework and demonstrate on-par or superior performance compared to specialised models. Offering a more versatile approach, UniGuide has the potential to streamline the development of molecular generative models, allowing them to be readily used in diverse application scenarios. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion models have emerged as an important class of generative models in various domains, including computer vision [1], signal processing [2], computational chemistry, and drug discovery [3\u20138]. By gradually adding noise to data samples and learning the reverse process of removing noise, diffusion models effectively transform noisy samples into structured data [9, 10]. In the context of drug discovery, it is essential to effectively address downstream tasks, which often pose specific geometric conditions. Examples of this include (i) Structure-based drug design (SBDD) that aims to create small ligands that fit given receptor binding sites [11], (ii) Fragment-based drug design (FBDD) that designs molecules by elaborating known scaffolds [12, 13], or (iii) Ligand-based drug design (LBDD) which generates molecules that fti a certain shape [14]. Recent works address these tasks by either incorporating specialised models or focusing on conditions that directly resemble molecular structures. In both cases, this narrow focus restricts their adaptability to new or slightly altered settings. ", "page_idx": 0}, {"type": "text", "text": "We address the challenge of adaptability by introducing UniGuide, a method that unifies guidance for geometry-conditioned molecular generation, see Fig. 1. The key element for achieving this unification is the condition map, which transforms complex geometric conditions to match the diffusion model\u2019s configuration space, thereby enabling self-guidance without the need for external models. Like other guidance-based approaches, UniGuide does not constrain the generality of the underlying model. Moreover, our method is the most versatile, extending beyond guiding molecular structures to leveraging complex geometric conditions such as volumes, surfaces, and densities, thereby enabling the unified tackling of diverse drug discovery tasks. For complex conditions specifically, previous works primarily rely on conditional diffusion models for effective condition encoding [12\u201314]. With our method, we are able to tackle the same tasks, while overcoming major drawbacks: UniGuide eliminates the need for additional training and, more importantly, avoids constraining the model to specific tasks. ", "page_idx": 0}, {"type": "image", "img_path": "HeoRsnaD44/tmp/118f2c44738c77c404a465344d263ba26ab6b6522c6c4d0fff0a43c839d860ba.jpg", "img_caption": ["Figure 1: UniGuide handles diverse conditioning modalities for guidance, including: (i) a target receptor for SBDD, (ii) additional molecular fragments for FBDD, or (iii) a predefined 3D shape for LBDD. It combines a source condition $s\\in S$ and the unconditional model $\\epsilon_{\\theta}(\\mathbf{z}_{t},t)$ within its condition map to enable self-guidance. The flexible formulation of our approach can be generalised to new geometric tasks, for example, conditioning on atomic densities. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We demonstrate the wide applicability of UniGuide by tackling a variety of geometry-constrained drug discovery tasks. With performance either on par with or superior to tailored models, we conclude that UniGuide offers advantages beyond its unification. Firstly, while the novelty of conditional models often stems from the condition incorporation, our method redirects focus to advancing unconditional generation, which directly beneftis multiple applications. Furthermore, this separation of model training and conditioning allows us to tackle tasks with minimal data, a common scenario in the biological domain. ", "page_idx": 1}, {"type": "text", "text": "In summary, our contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We present UniGuide: A unified guidance method for generating geometry-conditoned molecular structures, requiring neither additional training nor external networks used to guide the generation.   \n\u2022 We demonstrate UniGuide\u2019s wide applicability by tackling various conditioning scenarios in structure-based, fragment-based, and ligand-based drug design.   \n\u2022 We show UniGuide\u2019s favourable performance over task-specific baselines, highlighting the practical relevance of our approach. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Diffusion models and controllable generation Diffusion models [9, 10] are generative models achieving state-of-the-art performance across various domains, including the generation of images [1, 9], text [15], or point clouds [16]. Conditional diffusion models [17\u201321] are based on the same principle but incorporate a particular condition in their training, allowing for the controlled generation. Alternatively, classifier guidance [22, 23] relies on external models for controllable generation. Prior works in this context primarily focused on global properties [22, 24], lacking the capacity to condition on the geometric conditions central to our work. For instance, Bao et al. [24] demonstrate control over molecule generation based on desired quantum properties. ", "page_idx": 1}, {"type": "text", "text": "De novo molecule generation Research on de novo molecule generation focused extensively on generating molecules using their chemical graph representations [7, 25\u201334]. However, these methods are limited in modelling the molecules\u2019 conformation information and are, therefore, not ideally suited for several drug-discovery settings, such as target-aware drug design. Recently, attention has shifted towards generating molecules in 3D space, utilising variational autoencoders [35], autoregressive models [36\u201338], flow-based models [39, 40], and diffusion-based approaches [20, 41\u201347]. ", "page_idx": 1}, {"type": "text", "text": "Conditional generation of molecules Downstream applications of molecular generation can be categorised by their condition modality. In the case of SBDD [38, 48, 49], Schneuing et al. [11] and Guan et al. [50], for example, introduce models that simultaneously operate on protein pockets and ligands. In the conditional case, the pocket context is fixed throughout the generation. Moreover, FBDD imposes (multiple) scaffolds as a constraint [11, 12, 51\u201353]. Igashov et al. [13] expand given scaffolds by generating the molecule around the fixed scaffolds. In a related task of FBDD, linker design with pose estimation, as discussed in [54], further generate the rotation of the given scaffolds. SBDD and FBDD rely on the availability of high-quality data of protein pockets, which is often scarce. For this reason, LBDD aims to generate molecules that match the same 3D volume of reference ligands that are known to bind to the target of interest [55, 56]. Chen et al. [14] specifically train a shape encoder to capture the molecular shape of a reference ligand and use the resulting embedding to train a conditional diffusion model. ", "page_idx": 2}, {"type": "text", "text": "3 Controlling the generation of diffusion models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffusion Models [9, 57] learn a Markov Chain that involves a forward process to perturb data from a distribution $q(\\mathbf{z})$ and learn to reverse the process to generate new samples from a tractable prior, for example, a normal distribution. Given a data point sampled from the true underlying distribution, $\\mathbf{z}_{\\mathrm{data}}\\sim q(\\mathbf{z})$ , the forward process $q\\big(\\mathbf{z}_{t}|\\mathbf{z}_{t-1}\\big)$ gradually adds Gaussian noise: ", "page_idx": 2}, {"type": "equation", "text": "$$\nq(\\mathbf{z}_{t}|\\mathbf{z}_{t-1})=\\mathcal{N}\\big(\\mathbf{z}_{t}\\,\\big|\\,\\sqrt{1-\\beta_{t}}\\mathbf{z}_{t-1},\\beta_{t}\\pmb{I}\\big)\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\{\\beta_{t}\\in(0,1)\\}_{t=1}^{T}$ defines a variance schedule. Defining the forward process this way, one can readily sample from $q(\\mathbf{\\bar{z}}_{t}\\mid\\mathbf{z}_{\\mathrm{data}})$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{z}_{t}=\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{z}_{\\mathrm{data}}+\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon\\,,\\quad\\epsilon\\sim\\mathcal{N}(\\mathbf{0},I)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with $\\alpha_{t}=1-\\beta_{t}$ and $\\begin{array}{r}{\\bar{\\alpha}_{t}=\\prod_{i=1}^{t}\\alpha_{i}}\\end{array}$ . Since the time-reverse process $q(\\mathbf{z}_{t-1}|\\mathbf{z}_{t})$ depends on $\\mathbf{z}_{\\mathrm{data}}$ , which is not available at gen eration time, it is approximated by modelling $p_{\\theta}\\big(\\mathbf{z}_{t-1}\\,\\big|\\,\\mathbf{z}_{t}\\big)$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{\\theta}\\big(\\mathbf{z}_{t-1}\\mid\\mathbf{z}_{t}\\big)=\\mathcal{N}\\big(\\mathbf{z}_{t-1}\\mid\\mu_{\\theta}(\\mathbf{z}_{t},t),\\sigma_{t}{\\cal I}\\big)\\;,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the mean $\\pmb{\\mu}_{\\theta}$ is parameterised by a noise-predicting neural network $\\epsilon_{\\theta}$ in the form of: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mu_{\\theta}({\\bf z}_{t},t)=\\frac{1}{\\sqrt{\\alpha_{t}}}\\Big({\\bf z}_{t}-\\frac{\\beta_{t}}{\\sqrt{1-\\bar{\\alpha}_{t}}}\\epsilon_{\\theta}({\\bf z}_{t},t)\\Big)\\;.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The model $\\epsilon_{\\theta}$ is trained to optimise the variational lower bound through the simplified training objective: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{train}}=\\frac{1}{2}\\big\\|\\epsilon-\\epsilon_{\\theta}(\\mathbf{z}_{t},t)\\big\\|_{2}^{2}\\quad.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Self-guiding diffusion models Using Bayes\u2019 rule, the conditional probability $p_{\\theta}(\\mathbf{z}_{t}\\mid c)$ given a condition $^c$ can be expressed as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{\\theta}(\\mathbf{z}_{t}\\,|\\,\\boldsymbol{c})\\propto p_{\\theta}(\\mathbf{z}_{t})\\,p_{\\theta}(\\boldsymbol{c}\\,|\\,\\mathbf{z}_{t})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This allows us to decompose the score function as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{\\mathbf{z}_{t}}\\mathrm{log}\\,p_{\\theta}(\\mathbf{z}_{t}\\,|\\,\\mathbf{c})=\\nabla_{\\mathbf{z}_{t}}\\mathrm{log}\\,p_{\\theta}(\\mathbf{z}_{t})+S\\,\\nabla_{\\mathbf{z}_{t}}\\mathrm{log}\\,p_{\\theta}(\\mathbf{c}\\,|\\,\\mathbf{z}_{t})\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the second term is used for guiding the unconditional generation, with $S>0$ controlling the guidance strength. Using that $\\nabla_{\\mathbf{z}_{t}}\\mathrm{log}\\,p_{\\theta}(\\mathbf{z}_{t})=-(1-\\bar{\\alpha}_{t})^{-\\frac{1}{2}}\\epsilon_{\\theta}(\\mathbf{z}_{t},t)$ [22], we can rewrite the score function from Eq. (7) and identify the modified noise predictor $\\hat{\\epsilon}_{\\theta}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{z}_{t}}\\mathrm{log}\\,p_{\\theta}(\\mathbf{z}_{t}\\,|\\,c)=-\\frac{1}{\\sqrt{1-\\bar{\\alpha}_{t}}}\\bigg[\\underbrace{\\epsilon_{\\theta}(\\mathbf{z}_{t},t)-\\sqrt{1-\\bar{\\alpha}_{t}}S\\,\\nabla_{\\mathbf{z}_{t}}\\mathrm{log}\\,p_{\\theta}(c\\,|\\,\\mathbf{z}_{t})}_{=:\\epsilon_{\\theta}(\\mathbf{z}_{t},t,c)}\\bigg]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The modified mean function $\\hat{\\pmb{\\mu}}_{\\theta}$ then follows from the modified version of Eq. (4), enabling us to sample from $p_{\\theta}(\\mathbf{z}_{t-1}\\mid\\mathbf{z}_{t},\\boldsymbol{c})\\sim\\mathcal{N}\\big(\\hat{\\mu}_{\\theta}(\\mathbf{z}_{t},t,\\boldsymbol{c}),\\sigma_{t}I\\big)$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{\\mu}_{\\theta}(\\mathbf{z}_{t},t,c)=\\frac{1}{\\sqrt{\\alpha_{t}}}\\Big(\\mathbf{z}_{t}-\\frac{\\beta_{t}}{\\sqrt{1-\\bar{\\alpha}_{t}}}\\hat{\\epsilon}_{\\theta}(\\mathbf{z}_{t},t,c)\\Big)\\!=\\mu_{\\theta}(\\mathbf{z}_{t},t)+\\lambda(t)\\nabla_{\\mathbf{z}_{t}}\\log p_{\\theta}(c\\,|\\,\\mathbf{z}_{t})\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\lambda(t)~=~(\\alpha_{t})^{-\\frac{1}{2}}\\beta_{t}S$ balances the conditional update. Eq. (9) requires sampling from $\\log p_{\\theta}(\\mathbf{c}\\,|\\,\\mathbf{z}_{t})$ to which we do not have access. Assuming the condition $^c$ lies in the same space as $\\mathbf{z}_{t}$ , we can follow Kollovieh et al. [58] and approximate $\\log p_{\\theta}(\\mathbf{c}\\,|\\,\\mathbf{z}_{t})$ as a multivariate Gaussian distribution: ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{\\theta}(\\mathbf{\\boldsymbol{c}}\\,|\\,\\mathbf{z}_{t})=\\mathcal{N}\\big(c\\,|\\,f_{\\theta}(\\mathbf{z}_{t},t),I\\big)\\;,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ${\\bf\\nabla}f_{\\theta}({\\bf z}_{t},t)$ approximates the clean data point, enabling to estimate the condition in data space. Using Eq. (2), we can readily predict the clean data point given the noisy sample $\\mathbf{z}_{t}$ via ", "page_idx": 3}, {"type": "equation", "text": "$$\nf_{\\theta}(\\mathbf{z}_{t},t)=\\frac{\\mathbf{z}_{t}-\\sqrt{1-\\bar{\\alpha}_{t}}\\,\\epsilon_{\\theta}(\\mathbf{z}_{t},t)}{\\sqrt{\\bar{\\alpha}_{t}}}=:\\hat{\\mathbf{z}}_{0}\\quad.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "With this, the guiding term becomes a direct differentiation of the squared error with respect to the noisy sample $\\mathbf{z}_{t}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{z}_{t}}\\mathrm{log}\\,p_{\\theta}(c\\,|\\,\\mathbf{z}_{t})=-\\frac{1}{2}\\nabla_{\\mathbf{z}_{t}}\\left\\|f_{\\theta}(\\mathbf{z}_{t},t)-c\\right\\|_{2}^{2}\\ \\ .\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "By directly leveraging the prediction of the unconditional model $\\epsilon_{\\theta}$ , Eq. (12) establishes our selfguiding conditioning, thereby defining the self-guided noise predictor $\\hat{\\epsilon}_{\\theta}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\epsilon}_{\\theta}(\\mathbf{z}_{t},t,c)=\\epsilon_{\\theta}(\\mathbf{z}_{t},t)+\\frac{\\sqrt{1-\\bar{\\alpha}_{t}}S}{2}\\nabla_{\\mathbf{z}_{t}}\\big\\|\\hat{\\mathbf{z}}_{0}-c\\big\\|_{2}^{2}\\quad.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "4 UniGuide ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To enable the application of unconditional molecular diffusion models $\\epsilon_{\\theta}$ to geometric downstream tasks in drug discovery, we aim to develop a unified guidance framework, UniGuide, see Fig. 1. Importantly, we seek to enable guidance from arbitrary geometric conditions $s\\in S$ , where $\\boldsymbol{S}$ denotes a general space of source conditions. However, the source conditions $\\pmb{s}$ cannot be directly used for the loss computation in Eq. (12) when they do not match the configuration space $\\mathcal{Z}$ . ", "page_idx": 3}, {"type": "text", "text": "To address this challenge, we introduce condition maps $C$ , which bridge the gap between arbitrary source conditions $\\pmb{s}$ and target conditions $^c$ suitable for guidance. In Sec. 4.1, we start with its general formulation and continue to derive a condition map $C z$ for the special case where $s=\\mathcal{Z}$ . This will be useful when discussing the application of UniGuide to various drug discovery tasks in Sec. 4.2. We also demonstrate how to derive a task-specific condition map $C_{\\partial V}$ for ligand-based drug design. ", "page_idx": 3}, {"type": "text", "text": "Notation In 3D space, the configuration of molecules, including proteins, can be represented by a set of tuples $\\tilde{\\mathbf{z}}\\,=\\,\\{(\\mathbf{x}_{i},h_{i})\\}_{i=1}^{\\tilde{N}}\\,\\in\\,\\mathcal{Z}$ , where $\\pmb{x}_{i}\\,\\in\\,\\mathbb{R}^{3}$ and $\\bar{h_{i}}^{\\bar{~}}\\in\\mathbb{R}^{d}$ refer to coordinates and features of a node $z_{i}\\,=\\,({\\pmb x}_{i},{\\pmb h}_{i})$ , respectively. The space of configurations is denoted by $\\mathcal{Z}$ and includes configurations of varying size $N$ . We distinguish between different configuration entities via superscripts, i.e. refer to molecules $\\mathcal{M}$ and proteins $\\mathcal{P}$ through $\\mathbf{z}^{\\mathcal{M}}$ and $\\mathbf{z}^{\\mathcal{P}}$ , respectively. The collection of coordinates $\\mathbf{x}=\\{x_{1},\\dots,x_{N}\\}\\in\\mathring{\\mathbb{R}}^{N\\times3}\\in\\mathcal{X}$ defines the conformation of a molecule $\\mathcal{M}$ or protein $\\mathcal{P}$ . We represent arbitrary geometric conditions with the variable $s\\in S$ , and conditions that can be used for guidance with the variables $c\\in{\\mathcal{Z}}$ . ", "page_idx": 3}, {"type": "text", "text": "4.1 Unified self-guidance from geometric conditions $s\\in s$ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The concept of a condition map $C$ is essential to our method, enabling guidance from conditions $s\\in S$ in a unified fashion, where $\\boldsymbol{S}$ represents a space of general geometric objects such as structures, densities, or surfaces. These geometric objects do not necessarily match the configuration space $\\mathcal{Z}$ , i.e. $S\\neq{\\mathcal{Z}}$ , preventing the computation of the guiding score function from Eq. (12). We overcome this challenge by defining $C$ as a transformation that maps $\\pmb{s}$ to a suitable target condition $c\\in{\\mathcal{Z}}$ , which is then utilised for self-guidance. ", "page_idx": 3}, {"type": "text", "text": "In the most general case, $C$ takes the form of ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{C:\\cal{S}\\times\\cal{Z}\\to\\cal{Z}}\\\\ {s\\times\\mathbf{z}\\ \\mapsto\\mathbf{\\Sigma}c}\\end{array},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the source condition $\\pmb{s}$ together with a configuration ${\\bf z}$ are mapped to a target condition $c\\in{\\mathcal{Z}}$ . Including the condition map $C$ in the guidance, we obtain our guidance signal: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{z}_{t}}\\mathrm{log}\\,p_{\\theta}(c\\,|\\,\\mathbf{z}_{t})=-\\frac{1}{2}\\nabla_{\\mathbf{z}_{t}}\\big\\|\\hat{\\mathbf{z}}_{0}-C(\\pmb{\\mathscr{s}},\\hat{\\mathbf{z}}_{0})\\big\\|_{2}^{2}=-\\nabla_{\\mathbf{z}_{t}}\\mathcal{L}(\\hat{\\mathbf{z}}_{0},\\pmb{\\mathscr{s}})\\quad,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\hat{\\mathbf{z}}_{0}=f_{\\theta}(\\mathbf{z}_{t},t)$ is the estimate of $\\mathbf{z}_{\\mathrm{0}}$ given the unconditional model $\\epsilon_{\\theta}(\\mathbf{z}_{t},t)$ obtained according to Eq. (11) and $\\pmb{c}\\;=\\;C(\\pmb{s},\\hat{\\mathbf{z}}_{0})$ is the target condition produced by the condition map. In this formulation, $^c$ can also be understood as guidance target of the unconditional model. ", "page_idx": 4}, {"type": "text", "text": "It is important to highlight that Eq. (15) should not destroy the underlying properties of the unconditional generative process. In particular, if the unconditional model $\\epsilon_{\\theta}$ is equivariant to a set of transformations $G$ , e.g. rotations and translations, as is common in the molecular domain, we want to retain equivariance also in the guidance signal. Hence, the self-guided model $\\hat{\\epsilon}_{\\theta}$ should satisfy ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\epsilon}_{\\boldsymbol{\\theta}}\\bigl(G(\\mathbf{z}_{t}),t,c\\bigr)=G\\bigl(\\hat{\\epsilon}_{\\boldsymbol{\\theta}}(\\mathbf{z}_{t},t,c)\\bigr)\\,,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for all transformations $G$ to which $\\epsilon_{\\theta}$ is equivariant. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.1. Consider a function $C:S\\times\\mathcal{Z}\\to\\mathcal{Z}$ . If $C(s,\\mathbf{z})$ is invariant to rigid transformations $G$ in the first argument and equivariant in the second argument, then the gradient $\\nabla_{\\mathbf{z}}\\|\\mathbf{\\boldsymbol{v}}\\|_{2}^{2}$ of the vector $\\pmb{v}=\\mathbf{z}-C(\\pmb{s},\\mathbf{z})$ is equivariant to transformations of ${\\bf z}$ . ", "page_idx": 4}, {"type": "text", "text": "Proof. We prove Theorem 4.1 in App. B. ", "page_idx": 4}, {"type": "text", "text": "Using Theorem 4.1, we can guarantee equivariant guidance signals if the condition maps $C(s,\\mathbf{z})$ are invariant and equivariant under rigid transformations concerning the source condition $\\pmb{s}$ and configuration $\\mathbf{z}$ , respectively. ", "page_idx": 4}, {"type": "text", "text": "Guidance in the special case of $s=\\mathcal{Z}$ In the case where the source condition $\\pmb{s}$ directly defines subset $\\boldsymbol{\\mathcal{A}}$ of $m<N$ nodes of the configuration, i.e. $s=\\mathcal{Z}$ , we can fully specify the condition map. This is feasible because the condition map no longer needs to bridge different spaces; it only needs to ensure equivariance, as the loss computation between $\\pmb{s}$ and the configuration is already possible. To distinguish this special case from the general setting, we denote $\\pmb{s}=\\tilde{\\mathbf{z}}\\in\\mathbb{R}^{m\\times(3+d)}$ and refer to the defined subset within the configuration $\\hat{\\mathbf{z}}_{0}$ by $\\hat{\\mathbf{z}}_{0}^{A}$ . ", "page_idx": 4}, {"type": "text", "text": "In order to satisfy the requirements on $C\\!\\left(\\tilde{\\mathbf{z}},\\hat{\\mathbf{z}}_{0}^{A}\\right)$ as stated by Theorem 4.1, we align $\\tilde{\\mathbf{z}}$ with $\\hat{\\mathbf{z}}_{0}^{A}$ by using the Kabsch algorithm [59, 60]. Denoting the resulting transformation with $T_{\\hat{\\mathbf{z}}_{0}^{A}}$ , we get an $\\hat{\\mathbf{z}}_{0}$ -equivariant condition map: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{C_{\\mathcal{Z}}\\colon\\ \\mathbb{R}^{m\\times(3+d)}\\times\\mathbb{R}^{m\\times(3+d)}\\rightarrow\\mathbb{R}^{m\\times(3+d)}}\\\\ &{}&{\\tilde{\\textbf{z}}\\times\\ \\hat{\\textbf{z}}_{0}^{A}\\quad\\quad\\quad\\mapsto\\quad T_{\\hat{\\mathbf{z}}_{0}^{A}}\\tilde{\\textbf{z}}\\ \\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Taken together, we can compute the guidance signal based on the following loss $\\mathcal{L}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}\\big(\\hat{\\mathbf{z}}_{0}^{A},\\tilde{\\mathbf{z}}\\big)=\\frac12\\big\\|\\hat{\\mathbf{z}}_{0}^{A}-T_{\\hat{\\mathbf{z}}_{0}^{A}}\\tilde{\\mathbf{z}}\\big\\|_{2}^{2}\\quad.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We emphasise that although the loss ${\\mathcal{L}}\\!\\left({\\hat{\\mathbf{z}}}_{0}^{A},{\\tilde{\\mathbf{z}}}\\right)$ is computed on the subset $\\boldsymbol{\\mathcal{A}}$ , the gradient, as presented in Eq. (15), is still computed with respect the full configuration $\\mathbf{z}_{t}$ . ", "page_idx": 4}, {"type": "text", "text": "In summary, our method requires only an unconditionally trained model $\\epsilon_{\\theta}$ and a suitable condition map $C$ , eliminating the need for additional networks or training. Together, this facilitates unified self-guidance from arbitrary geometric sources. Importantly, the separation of model training and conditioning enables us to tackle tasks even with minimal data, which is crucial in practical scenarios. In the following section, we discuss the wide applicability of UniGuide by illustrating its application to multiple drug discovery tasks. ", "page_idx": 4}, {"type": "text", "text": "4.2 UniGuide for drug discovery ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Having introduced both the guidance framework and the condition map, we will continue to discuss how to tackle a set of drug discovery tasks within the UniGuide framework. We start with its application to ligand-based drug design (LBDD), which aims to generate a ligand that satisfies a predefined molecular shape. ", "page_idx": 4}, {"type": "text", "text": "Ligand-based drug design LBDD aims to generate novel ligands with a similar 3D shape as a reference ligand $\\mathcal{M}_{\\mathrm{ref}}$ . In this setting, one operates on the molecule level only since the protein information is assumed to be unknown. However, to still generate active ligands that bind to a protein pocket, one leverages the 3D shape information of a reference molecule. Specifically, the goal is to modify the generative process $\\hat{\\epsilon}_{\\theta}$ to generate a ligand $\\mathbf{z}_{\\mathrm{0}}$ with a similar 3D shape but different molecular structure than $\\mathcal{M}_{\\mathrm{ref}}$ . With Sec. 4.1 introducing all required concepts, we can readily formulate a surface condition map $C_{\\partial V}$ suitable to tackle the task of LBDD, see Fig. 2: ", "page_idx": 5}, {"type": "text", "text": "To represent $\\mathcal{M}_{\\mathrm{ref}}$ \u2019s 3D shape, we identify our source condition $\\pmb{s}$ with a set of $K$ points $\\mathbf{y}$ sampled uniformly from the reference ligand\u2019s surface $\\partial V$ , $\\mathbf{y}\\in\\dot{\\mathbb{R}}^{K\\times3}\\,\\dot{=}\\,\\mathcal{S}$ . As no features are guided, we formulate $C_{\\partial V}$ with respect to the conformation space $\\boldsymbol{\\bar{\\chi}}=\\mathbb{R}^{N\\times3}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{C_{\\partial V}:\\mathbb{R}^{K\\times3}\\times\\mathbb{R}^{N\\times3}\\rightarrow\\mathbb{R}^{N\\times3}}\\\\ &{}&{\\mathbf{y}\\times\\hat{\\mathbf{x}}_{0}\\quad\\mapsto\\mathbf{c_{x}}\\quad,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\hat{\\mathbf{x}}_{\\mathrm{0}}$ denotes the conformation of the clean data point estimation $\\hat{\\mathbf{z}}_{0}$ as computed by Eq. (11). To satisfy Theorem 4.1, $C_{\\partial V}$ first aligns y with $\\hat{\\mathbf{x}}_{\\mathrm{0}}$ by a rotation $R_{\\hat{\\mathbf{x}}_{0}}\\in\\mathbb{R}^{3\\times3}$ resulting from the ICP algorithm [61]. For every atom coordinate $\\hat{\\pmb{x}}_{i}$ , $C_{\\partial V}$ subsequently computes the mean $\\bar{\\pmb{y}}_{i}$ over $\\hat{\\pmb{x}}_{i}$ \u2019s $k$ closest surface points: ", "page_idx": 5}, {"type": "image", "img_path": "HeoRsnaD44/tmp/26adec53f7776e0f51a634f8c41a8650dc20ef531972db1a4e33fdcc0a8cd06b.jpg", "img_caption": ["Figure 2: Surface condition map $C_{\\partial V}$ : For each atom coordinate $\\pmb{x}_{i}$ , the closest surface points $\\pmb{y}_{j}$ are computed. The target condition $c_{\\mathbf{x},i}$ is the projection along the mean of neighbours $\\bar{\\pmb{y}}_{i}$ to the inside of the volume by a margin $\\alpha$ , where $d=\\lVert\\bar{\\pmb y}_{i}-\\hat{\\pmb x}_{i}\\rVert_{2}$ . "], "img_footnote": [], "page_idx": 5}, {"type": "equation", "text": "$$\n\\bar{y}_{i}=\\frac{1}{k}\\sum_{j\\in\\mathcal{N}_{\\hat{\\alpha}_{i}}}R_{\\hat{\\mathbf{x}}_{0}}y_{j}\\quad\\mathrm{~,~with~}\\quad\\mathcal{N}_{\\hat{\\pmb{x}}_{i}}\\mathrm{=arg\\,\\operatorname*{min}_{\\substack{I\\subset\\{1,\\dots,K\\},\\,|I|=k}}\\sum_{j\\in I}\\left\\|R_{\\hat{\\mathbf{x}}_{0}}y_{j}-\\hat{\\pmb{x}}_{i}\\right\\|_{2}\\quad.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Finally, the individual components $c_{\\mathbf{x},i}$ of the target condition compute as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nc_{\\mathbf{x},i}=\\left\\{\\!\\!\\begin{array}{l l}{\\!\\bar{y}_{i}+\\frac{\\alpha}{d}\\big(\\bar{y}_{i}-\\hat{x}_{i}\\big)\\,,}&{\\mathrm{if}\\;\\hat{x}_{i}\\mathrm{~outside~}V}\\\\ {\\!\\bar{y}_{i}-\\frac{\\alpha}{d}\\big(\\bar{y}_{i}-\\hat{x}_{i}\\big)\\,,}&{\\mathrm{if}\\;\\hat{x}_{i}\\mathrm{~inside~}V\\wedge d<\\alpha}\\\\ {\\!\\hat{x}_{i}\\,,}&{\\mathrm{otherwise}\\,,}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $d$ denotes the distance to the surface, $d=\\lVert\\bar{\\pmb{y}}_{i}-\\hat{\\pmb{x}}_{i}\\rVert_{2}$ , and $\\alpha$ the required distance to the surface. Note that the target condition $c_{\\mathbf{x}}$ represents a valid conformation inside the surface $\\partial V$ , and that $C_{\\partial V}$ effectively bridges spaces from $\\boldsymbol{S}$ to $\\mathcal{X}$ . Consequently, when using $C_{\\partial V}$ , the guidance signal is derived from Eq. (15) with the loss function $\\mathcal{L}(\\hat{\\mathbf{z}}_{0},\\mathbf{y})$ . The full algorithm for guidance using $C_{\\partial V}$ is presented in App. D.1. ", "page_idx": 5}, {"type": "text", "text": "Structure-based drug design The goal of SBDD is to design a ligand that binds to a target protein   \npocket $\\pmb{s}$ . In this setting, one operates on both the molecule and protein level. Technically, we   \nare interested in generating a ligand $\\mathbf{z}_{0}^{\\mathcal{M}}$ conditioned on the protein configuration $\\tilde{\\mathbf{z}}^{\\mathcal{P}}$ . With the   \nluingcaondn-dpitriootneianl  pdiafifrsu $\\epsilon_{\\theta}(\\mathbf{z}_{t},t)$ $\\mathbf{z}_{t}\\,=\\,(\\mathbf{z}_{t}^{\\mathcal{M}},\\mathbf{z}_{t}^{\\mathcal{P}})$ ,t haep rxciem catoinndgi ttihoen  jdoirientc tldyi sctroirbrue oonn dosf $p(\\mathbf{z}_{\\mathrm{data}}^{\\mathcal{M}},\\mathbf{z}_{\\mathrm{data}}^{\\mathcal{P}})$ $C z$ $\\tilde{\\mathbf{z}}$   \n$\\tilde{\\mathbf{z}}^{\\mathcal{P}}$ . The guidance signal then follows from the loss $L(\\hat{\\mathbf{z}}_{0}^{\\mathcal{P}},\\tilde{\\mathbf{z}}^{\\mathcal{P}})$ with $\\boldsymbol{c}^{\\mathcal{P}}=\\boldsymbol{C}_{\\mathcal{Z}}(\\tilde{\\mathbf{z}}^{\\mathcal{P}},\\hat{\\mathbf{z}}_{0}^{\\mathcal{P}})$ as defined in   \nEq. (18). We describe the sampling algorithm for the SBDD task in App. E.1. ", "page_idx": 5}, {"type": "text", "text": "Fragment-based drug design FBDD aims to design a ligand by optimising a molecule around fragments $\\mathcal{F}$ that bind weakly to a receptor. Similarly to SBDD, one operates on both the molecule and protein level. Technically, we are interested in generating a ligand $\\mathbf{z}_{0}^{\\mathcal{M}}$ conditioned on both the protein and the fragment configuration, $\\tilde{\\mathbf{z}}^{\\mathcal{P}}$ and $\\tilde{\\mathbf{z}}^{\\breve{\\mathcal{F}}}$ , respectively. Considering the same kind of unconditional model $\\epsilon_{\\theta}(\\mathbf{z}_{t},t)$ as in SBDD, we can use $C z$ from Sec. 4.1. Only now, we identify $\\tilde{\\mathbf{z}}$ with both $\\tilde{\\mathbf{z}}^{\\mathcal{P}}$ and $\\tilde{\\mathbf{z}}^{\\mathcal{F}}$ and write $\\tilde{\\mathbf{z}}^{A}$ with ${\\mathcal{A}}={\\mathcal{P}}\\cup{\\mathcal{F}}$ . Using Eq. (18), the guidance signal directly follows from $\\mathcal{L}(\\hat{\\mathbf{z}}_{0}^{A},\\tilde{\\mathbf{z}}^{\\mathcal{P}\\cup\\mathcal{F}})$ with $c^{\\mathcal{P}}=C_{\\mathcal{Z}}(\\tilde{\\mathbf{z}}^{\\mathcal{P}\\cup\\mathcal{F}},\\hat{\\mathbf{z}}_{0}^{\\mathcal{P}\\cup\\mathcal{F}})$ . The sampling algorithm is similar to the one described in App. E.1. ", "page_idx": 5}, {"type": "text", "text": "Several tasks exist within the FBDD setting [62\u201365]. Examples are scaffold hopping [64], where the core structure of $\\mathbf{z}_{0}^{\\mathcal{M}}$ has to be generated, but functional groups that interact with the receptor are fixed, or linker design [65], where the connection between separated fragments has to be optimised through the generative process, see Fig. 5. Note that these tasks differ primarily in their application and can be treated identically from a technical perspective within UniGuide. In addition, one can also consider variations where the protein information $\\dot{\\mathbf{z}}^{\\mathcal{P}}$ is discarded. This usually aligns with switching to an unconditional model $\\epsilon_{\\theta}$ that solely models the distribution over molecules. We present results for this configuration in Sec. 5.3. ", "page_idx": 5}, {"type": "table", "img_path": "HeoRsnaD44/tmp/1f25e218726da7e4b77cb723b4cc8b78f54744b4ce89dff717d6f624ff23357b.jpg", "table_caption": ["Table 1: Ligand-Based Drug Design. Results taken from Chen et al. [14] are indicated with $\\left({}^{*}\\right)$ . We highlight the best conditioning approach for the ShapeMol backbone in bold and underline the best approach across all methods. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Furthermore, we would like to highlight that it is possible to combine guidance strategies within UniGuide. For example, one could incorporate a version of the surface condition map $C_{\\partial V}$ for FBDD to provide an additional geometric guidance signal for the atoms not included in $\\mathcal{F}$ . ", "page_idx": 6}, {"type": "text", "text": "Limitations Drug discovery also involves tasks beyond purely geometric conditions, encompassing global graph properties [24]. These are excluded from the UniGuide framework. Additionally, UniGuide requires the unconditional model to be trained on a matching configuration space. We discuss the broader impact of our work in App. A. ", "page_idx": 6}, {"type": "text", "text": "5 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we compare UniGuide to state-of-the-art models across various drug discovery tasks. To highlight the wide range of tasks to which unconditional models can be adapted through UniGuide, we conduct experiments on ligand-based (Sec. 5.1), structure-based (Sec. 5.2) and fragment-based (Sec. 5.3) drug design. We demonstrate that UniGuide performs competitively or even surpasses specialised baseline models, underscoring its practical relevance and transferability to diverse drug discovery scenarios. ", "page_idx": 6}, {"type": "text", "text": "5.1 Ligand-based drug design ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Dataset Following Chen et al. [14], we employ the MOSES dataset for the ligand-based drug design task [66]. We evaluate on a test set consisting of 1000 reference ligands, from which the 3D shape conditions are extracted. For every shape condition $\\mathcal{M}_{\\mathrm{ref}}$ , 50 samples are generated. We refer to App. D.1 for further details on the evaluation setup. ", "page_idx": 6}, {"type": "text", "text": "Baselines For the LBDD task, we compare UniGuide to ShapeMol, a conditional diffusion model that is trained by conditioning on learned latent embeddings of the molecular surfaces [14]. Chen et al. [14] also propose a correction technique that adjusts the atom positions based on their distance to the reference ligand\u2019s nodes, which is refered to as ShapeMol+g. Additionally, we include as baselines Virtual Screening (VS) [14], a shape-based virtual screening tool, and SQUID [55], a variational autoencoder that decodes molecules by sequentially attaching fragments with fixed bond lengths and angles. For this task, we evaluate UniGuide equipped with the surface condition map $C_{\\partial V}$ from Eq. (21) in conjunction with two unconditionally trained diffusion models, ShapeMol [U] and EDM [14, 20] as well as the conditional model ShapeMol [14]. The \u201conly shape\u201d column in Tab. 1 indicates whether a method uses solely the reference ligand\u2019s shape or also incorporates its atom positions. ", "page_idx": 6}, {"type": "text", "text": "We compare UniGuide with an alternative guidance approach adapted from Guan et al. [67] in App. D.4 and refer to App. C and App. D.3 for further information on the unconditional models and the guidance parameters, respectively. In addition, inspired by the performance of UniGuide on the ", "page_idx": 6}, {"type": "text", "text": "Table 2: Structure-Based Drug Design. Quantitative comparison of generated ligands for target pockets from the CrossDocked and Binding MOAD test sets. Results taken from the respective works are indicated with $(^{*})$ . We highlight the best conditioning approach for the DiffSBDD backbone in bold and underline the best approach over all methods. ", "page_idx": 7}, {"type": "table", "img_path": "HeoRsnaD44/tmp/f152ea661356e4db27113513f3f5f7d851ee08a1fe89c95a9fdf4730474f1583.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "LBDD task, we further motivate its applicability for the generation of molecules given atom densities, see App. G. ", "page_idx": 7}, {"type": "text", "text": "Evaluation The goal of LBDD is to discover novel molecules that fti within a given 3D shape. This can be quantified by a high 3D shape similarity and low graph similarity compared to the reference ligand, as illustrated in Fig. 3 as well as App. D.2. We highlight this trade-off by reporting the ratio of these similarities in Tab. 1 as $\\mathrm{Sim}_{S}/\\mathrm{Sim}_{G}$ , which constitutes the most important metric for this task. We follow Chen et al. [14] and further evaluate the mean and maximum shape similarities $\\mathrm{Sim}_{S}$ and max $\\mathrm{Sim}_{S}$ , respectively, per reference ligand, measured via the volume overlap between the two aligned molecules. Additionally, we report the graph similarity $\\mathrm{Sim}_{G}$ defined as the Tanimoto similarity between the generated and reference ligand, and the graph similarity max $\\mathrm{Sim}_{G}$ ", "page_idx": 7}, {"type": "image", "img_path": "HeoRsnaD44/tmp/5e2cb7d98d40d389d2580a5963acab9b7355bb5283edccca95d2581a13645f0f.jpg", "img_caption": ["Figure 3: Examples of the two shape-conditioned ligands generated by UniGuide. The goal is to have low molecular graph similarity and high shape similarity. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "of the generated molecule with the maximum shape similarity. Further metrics concerning the quality of the generated ligands are provided in App. D.2. ", "page_idx": 7}, {"type": "text", "text": "Both in terms of shape similarity and graph similarity, guiding the generation of EDM with UniGuide outperforms other task-specific conditioning mechanisms and even the Virtual Screening baseline. Emphasised by the Ratio metric across all evaluated methods, UniGuide demonstrates that it is able to generate diverse molecules with very similar shapes compared to the reference ligand. Remarkably, UniGuide achieves higher shape similarity than ShapeMol+g, even though the conditional model is explicitly guided towards the position of the reference ligand through the position correction technique. UniGuide, on the other hand, does not require information about the reference\u2019s atom positions at all to generate novel, high-quality ligands. This highlights how UniGuide and the design of condition maps enables unconditional models like EDM, that have not been tailored or trained for the LBDD task, to achieve state-of-the-art performance on new tasks. ", "page_idx": 7}, {"type": "text", "text": "5.2 Structure-based drug design ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Datasets Following Schneuing et al. [11], we evaluate UniGuide on two protein-ligand datasets: the CrossDocked dataset [68] and the Binding MOAD dataset [69]. For the CrossDocked dataset, we follow the preprocessing as described by [38] and conduct the evaluation on 100 test protein pockets. ", "page_idx": 7}, {"type": "table", "img_path": "HeoRsnaD44/tmp/4a6742ff6259d6bbb5b1b6d67ca16cf1890661effe7ab42a570d3f74fbfe4fdb.jpg", "table_caption": ["Table 3: Linker Design. Results taken from Igashov et al. [13] are indicated with $(^{*})$ . We underline the best method overall. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "The Binding MOAD dataset is preprocessed as discussed in Schneuing et al. [11], resulting in 130 test proteins. Per target pocket, 100 ligands are generated. We evaluate the generation of ligands on models that are trained on the full-atom context of the pockets in Tab. 2 and results of models trained on the $C_{\\alpha}$ representation of the pockets are provided in App. E.5. ", "page_idx": 8}, {"type": "text", "text": "Baselines We compare UniGuide to two autoregressive models designed for the SBDD task: 3D-SBDD [38] and Pocket2Mol [48]. We further include TargetDiff [50] and DecompDiff [67], conditional diffusion models for SBDD that fix the protein pocket context during every step of the diffusion process. We exclude approaches with explicit drift terms like Guan et al. [67] and Huang et al. [70] from the comparison, as UniGuide\u2019s SBDD condition map does not include drift terms currently, but can be readily extended to do so. Schneuing et al. [11] present two techniques for controlled structure-based generation: (i) DiffSBDD-cond, a conditional diffusion model similar to [50] and (ii) DiffSBDD, an inpainting-inspired technique that modifies the generative process of an unconditional diffusion model that jointly generates protein-ligand pairs. Across datasets, both UniGuide and DiffSBDD control the same unconditional ligand-protein diffusion model. We provide more information and further evaluation regarding this base model in App. E.2 and App. E.3 and investigate the influence of the guidance scale $S$ as well as the resampling trick [71], a technique that modifies the generative process to better harmonise the generated ligand with the controlled pockets, in App. E.4 and App. E.5. ", "page_idx": 8}, {"type": "text", "text": "Evaluation As the task of SBDD is to generate ligands that bind well to a given protein pocket, we assess generated ligands based on affinity-related metrics (Vina Score, Vina Min and Vina Dock), which estimate the binding affinity between the generated ligands and a given test receptor [72]. Additionally, we measure the quality of the generated ligands using two chemical properties: the drug-likeness (QED) and the synthetic accessibility (SA) [66, 73]. ", "page_idx": 8}, {"type": "text", "text": "Tab. 2 demonstrates that, without additional training or external networks, UniGuide performs competitively with even the highly specialised conditional models like TargetDiff and DecompDiff. Our results indicate that not fully converging to the target protein pocket due to soft guidance, compared to, for example, DiffSBDD\u2019s inpainting-inspired technique, is not a limitation in practice. Rather, it suggests that utilising self-guidance in combination with a suitable condition map generates wellharmonised ligand-protein pairs. This is also reflected in the properties of the generated ligands, where UniGuide achieves ", "page_idx": 8}, {"type": "image", "img_path": "HeoRsnaD44/tmp/668c185e987cb11c15b5cdd2781158655939e5c8fedfec27354325b58ef9facd.jpg", "img_caption": ["Figure 4: Qualitative example of a test protein pocket (6c0b) from the Binding MOAD dataset. We show the reference ligand (grey) and samples generated by UniGuide (blue). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "good drug-likeness (QED) and synthetic accessibility (SA) scores. We provide additional qualitative examples for the SBDD task in Fig. 4, which showcase that UniGuide not only generates drug-like ligands but is even able to improve over the VINA Dock metric of the reference ligand. ", "page_idx": 8}, {"type": "text", "text": "5.3 Fragment-based drug design ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Datasets & Baselines In the following, we investigate linker design, a subfield of fragment-based drug design. We follow Igashov et al. [13] and decompose ligands from the ZINC dataset [74] with the MMPA algorithm [75]. Note that the ZINC dataset does not contain pocket information, and the evaluated approaches operate solely at the molecular level. We compare UniGuide to DiffLinker [13], a diffusion-based conditional model that fixes fragments in space. Additionally, we evaluate the variational autoencoder-based methods DeLinker [53] and 3DLinker [52], adapted as described in Igashov et al. [13]. We provide more information on the experimental setup as well as the unconditionally trained EDM model in App. F.1 and App. C. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Evaluation Following Igashov et al. [13], we evaluate the generated linkers and ligands with respect to their properties (SA, QED, Number of Rings and 2D Filters). We additionally measure (i) the uniqueness of the generated samples, (ii) the recovery of the reference ligands, and (iii) the validity, which combines the chemical validity and the successful linking of the fragments. ", "page_idx": 9}, {"type": "text", "text": "Using UniGuide to control the EDM generation enables the successful combination of the condition fragments and the generation of diverse linkers. Even compared to taskspecific models, UniGuide is able to perform competitively across different metrics. Importantly, UniGuide enables the same unconditional model (EDM) to tackle both the linker design task as presented in Tab. 3 as well as the LBDD task as presented in Tab. 1 without additional training. Note that, while DiffLinker is specifically designed to generate linkers, UniGuide readily generalises to other tasks within the FBDD setting, such as ", "page_idx": 9}, {"type": "image", "img_path": "HeoRsnaD44/tmp/3d0c928dad369884663eb8edb07a1790b87a9ced0d86197330d94014dee9c7a4.jpg", "img_caption": ["Figure 5: For various pocket-conditioned FBDD tasks, we show reference ligands (grey), desired fragments (magenta), and ligands generated by UniGuide (blue). "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "fragment growing and scaffolding, see Fig. 5. Additionally, UniGuide is agnostic to the fragmentation procedure used to obtain the condition scaffolds, meaning that UniGuide will generalise to unseen fragments as long as the underlying molecule fits within the training distribution. In App. F.2, we demonstrate how the same unconditional model can be adapted for these tasks. Our quantitative evaluation highlights the beneftis achieved through the unification of controlled generation provided by UniGuide. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we present UniGuide, a unified way of controlling the generation of molecular diffusion models towards geometric constraints. UniGuide generalises to a multitude of drug discovery tasks without the need for conditioning networks or specialised training protocols, enabling UniGuide to find applicability also in scenarios where little data is available. By demonstrating that specialisation is not a necessity and that a more flexible, unified method outperforms specialised approaches across tasks and datasets, we open up new avenues for streamlined and flexible generative models with wide-ranging applications. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements SA, LH, and JS are thankful for valuable feedback from Marcel Kollovieh, Leo Schwinn, and Alessandro Palma from the DAML group and Theis Lab. SA is supported by the DAAD programme Konrad Zuse Schools of Excellence in Artificial Intelligence, sponsored by the Federal Ministry of Education and Research. LH is supported by the Helmholtz Association under the joint research school \u201cMunich School for Data Science - MUDS\u201d. FJT acknowledges support from the Helmholtz Association\u2019s Initiative and Networking Fund through Helmholtz AI (ZT-I-PF-5-01). FJT further acknowledges support by the BMBF (01IS18053A). In addition, FJT consults for Immunai Inc., Singularity Bio B.V., CytoReason Ltd, and Omniscope Ltd and has an ownership interest in Dermagnostix GmbH and Cellarity. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n[2] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation. arXiv preprint arXiv:2009.00713, 2020.   \n[3] N Anand and T Achim. Protein structure and sequence generation with equivariant denoising diffusion probabilistic models. arXiv, 2022. doi: 10.48550. arXiv preprint arXiv.2205.15019.   \n[4] Brian L. Trippe, Jason Yim, Doug Tischer, David Baker, Tamara Broderick, Regina Barzilay, and Tommi Jaakkola. Diffusion probabilistic modeling of protein backbones in 3D for the motif-scaffolding problem, March 2023. URL http://arxiv.org/abs/2206.04119. arXiv:2206.04119 [cs, q-bio, stat].   \n[5] Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim, Helen E Eisenach, Woody Ahern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, and others. De novo design of protein structure and function with RFdiffusion. Nature, 620(7976):1089\u20131100, 2023. Publisher: Nature Publishing Group UK London.   \n[6] Gabriele Corso, Bowen Jing, Regina Barzilay, Tommi Jaakkola, and others. DiffDock: Diffusion steps, twists, and turns for molecular docking. In International conference on learning representations (ICLR 2023), 2023.   \n[7] Yuanqi Du, Tianfan Fu, Jimeng Sun, and Shengchao Liu. MolGenSurvey: A Systematic Survey in Machine Learning Models for Molecule Design. Technical Report arXiv:2203.14500, arXiv, March 2022. URL http://arxiv.org/abs/2203.14500. arXiv:2203.14500 [cs, q-bio] type: article.   \n[8] Leon Hetzel, Simon B\u00f6hm, Niki Kilbertus, Stephan G\u00fcnnemann, Mohammad Lotfollahi, and Fabian Theis. Predicting single-cell perturbation responses for unseen drugs. Technical report, ICLR2022 Machine Learning for Drug Discovery, April 2022. arXiv:2204.13545 [cs, q-bio, stat] type: article.   \n[9] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[10] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   \n[11] Arne Schneuing, Yuanqi Du, Charles Harris, Arian Jamasb, Ilia Igashov, Weitao Du, Tom Blundell, Pietro Li\u00f3, Carla Gomes, Max Welling, Michael Bronstein, and Bruno Correia. Structure-based Drug Design with Equivariant Diffusion Models, June 2023. URL http: //arxiv.org/abs/2210.13695. arXiv:2210.13695 [cs, q-bio].   \n[12] Jos Torge, Charles Harris, Simon V Mathis, and Pietro Lio. Diffhopp: A graph diffusion model for novel drug design via scaffold hopping. arXiv preprint arXiv:2308.07416, 2023.   \n[13] Ilia Igashov, Hannes St\u00e4rk, Cl\u00e9ment Vignac, Victor Garcia Satorras, Pascal Frossard, Max Welling, Michael Bronstein, and Bruno Correia. Equivariant 3D-Conditional Diffusion Models for Molecular Linker Design, October 2022. URL http://arxiv.org/abs/2210.05274. arXiv:2210.05274 [cs, q-bio].   \n[14] Ziqi Chen, Bo Peng, Srinivasan Parthasarathy, and Xia Ning. Shape-conditioned 3D molecule generation via equivariant diffusion models. arXiv preprint arXiv:2308.11890, 2023.   \n[15] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori B Hashimoto. Diffusion-lm improves controllable text generation. Advances in Neural Information Processing Systems, 35:4328\u20134343, 2022.   \n[16] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten Kreis. LION: Latent point diffusion models for 3D shape generation. arXiv preprint arXiv:2210.06978, 2022.   \n[17] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.   \n[18] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.   \n[19] Weilun Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Dong Chen, Lu Yuan, and Houqiang Li. Semantic image synthesis via diffusion models. arXiv preprint arXiv:2207.00050, 2022.   \n[20] Emiel Hoogeboom, Victor Garcia Satorras, Cl\u00e9ment Vignac, and Max Welling. Equivariant Diffusion for Molecule Generation in 3D, June 2022. URL http://arxiv.org/abs/2203. 17003. arXiv:2203.17003 [cs, q-bio, stat].   \n[21] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 18208\u201318218, 2022.   \n[22] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780\u20138794, 2021.   \n[23] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Universal guidance for diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 843\u2013852, 2023.   \n[24] Fan Bao, Min Zhao, Zhongkai Hao, Peiyao Li, Chongxuan Li, and Jun Zhu. Equivariant Energy-Guided SDE for Inverse Molecular Design, February 2023. URL http://arxiv.org/ abs/2209.15408. arXiv:2209.15408 [physics, q-bio].   \n[25] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction Tree Variational Autoencoder for Molecular Graph Generation. Technical report, International Conference on Machine Learning, 2018. URL https://arxiv.org/abs/1802.04364.   \n[26] Krzysztof Maziarz, Henry Jackson-Flux, Pashmina Cameron, Finton Sirockin, Nadine Schneider, Nikolaus Stief,l Marwin Segler, and Marc Brockschmidt. Learning to Extend Molecular Scaffolds with Structural Motifs. Technical Report arXiv:2103.03864, arXiv, April 2022. URL http://arxiv.org/abs/2103.03864. arXiv:2103.03864 [cs, q-bio] type: article.   \n[27] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Hierarchical Generation of Molecular Graphs using Structural Motifs. Technical report, International Conference on Machine Learning, 2020.   \n[28] Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pascal Frossard. DiGress: Discrete Denoising diffusion for graph generation, October 2022. URL http://arxiv.org/abs/2209.14734. arXiv:2209.14734 [cs].   \n[29] Xiangzhe Kong, Wenbing Huang, Zhixing Tan, and Yang Liu. Molecule Generation by Principal Subgraph Mining and Assembling, September 2022. URL http://arxiv.org/abs/2106. 15098. arXiv:2106.15098 [cs, q-bio].   \n[30] Nicola De Cao and Thomas Kipf. MolGAN: An implicit generative model for small molecular graphs. Technical Report arXiv:1805.11973, arXiv, May 2018. URL http://arxiv.org/ abs/1805.11973. arXiv:1805.11973 [cs, stat] type: article.   \n[31] Leon Hetzel, Johanna Sommer, Bastian Rieck, Fabian Theis, and Stephan G\u00fcnnemann. MAGNet: Motif-agnostic generation of molecules from shapes. arXiv preprint arXiv:2305.19303, 2023.   \n[32] Zijie Geng, Shufang Xie, Yingce Xia, Lijun Wu, Tao Qin, Jie Wang, Yongdong Zhang, Feng Wu, and Tie-Yan Liu. De Novo Molecular Generation via Connection-aware Motif Mining, February 2023. URL http://arxiv.org/abs/2302.01129. arXiv:2302.01129 [cs].   \n[33] Johanna Sommer, Leon Hetzel, David L\u00fcdke, Fabian J. Theis, and Stephan G\u00fcnnemann. The Power of Motifs as Inductive Bias for Learning Molecular Distributions. March 2023. URL https://openreview.net/forum?id $=$ cS3_jJ0se3z.   \n[34] Mohamed Amine Ketata, Nicholas Gao, Johanna Sommer, Tom Wollschl\u00e4ger, and Stephan G\u00fcnnemann. Lift Your Molecules: Molecular Graph Generation in Latent Euclidean Space, June 2024. URL http://arxiv.org/abs/2406.10513. arXiv:2406.10513.   \n[35] Matthew Ragoza, Tomohide Masuda, and David Ryan Koes. Learning a continuous representation of 3D molecular structures with deep generative models. arXiv preprint arXiv:2010.08687, 2020.   \n[36] Niklas W. A. Gebauer, Michael Gastegger, and Kristof T. Sch\u00fctt. Symmetry-adapted generation of 3d point sets for the targeted discovery of molecules, January 2020. URL http://arxiv. org/abs/1906.00957. G-SchNet.   \n[37] Youzhi Luo and Shuiwang Ji. An Autoregressive Flow Model for 3D Molecular Geometry Generation from Scratch. October 2021. URL https://openreview.net/forum?id= C03Ajc-NS5W. G-SphereNet.   \n[38] Shitong Luo, Jiaqi Guan, Jianzhu Ma, and Jian Peng. A 3D generative model for structure-based drug design. Advances in Neural Information Processing Systems, 34:6229\u20136239, 2021.   \n[39] Victor Garcia Satorras, Emiel Hoogeboom, Fabian B. Fuchs, Ingmar Posner, and Max Welling. E(n) Equivariant Normalizing Flows, January 2022. URL http://arxiv.org/abs/2105. 09016. arXiv:2105.09016 [physics, stat].   \n[40] Yuxuan Song, Jingjing Gong, Minkai Xu, Ziyao Cao, Yanyan Lan, Stefano Ermon, Hao Zhou, and Wei-Ying Ma. Equivariant flow matching with hybrid probability transport for 3D molecule generation. In Thirty-seventh conference on neural information processing systems, 2023.   \n[41] Minkai Xu, Alexander Powers, Ron Dror, Stefano Ermon, and Jure Leskovec. Geometric Latent Diffusion Models for 3D Molecule Generation, May 2023. URL http://arxiv.org/abs/ 2305.01140. GeoLDM.   \n[42] Lemeng Wu, Chengyue Gong, Xingchao Liu, Mao Ye, and Qiang Liu. Diffusion-based Molecule Generation with Informative Prior Bridges, September 2022. URL http://arxiv.org/abs/ 2209.00865. Bridge.   \n[43] Alex Morehead and Jianlin Cheng. Geometry-Complete Diffusion for 3D Molecule Generation and Optimization, June 2023. URL http://arxiv.org/abs/2302.04313. GCDM.   \n[44] Bo Qiang, Yuxuan Song, Minkai Xu, Jingjing Gong, Bowen Gao, Hao Zhou, Wei-Ying Ma, and Yanyan Lan. Coarse-to-fine: a hierarchical diffusion model for molecule generation in 3d. In International conference on machine learning, pages 28277\u201328299. PMLR, 2023.   \n[45] Clement Vignac, Nagham Osman, Laura Toni, and Pascal Frossard. Midi: Mixed graph and 3d denoising diffusion for molecule generation. arXiv preprint arXiv:2302.09048, 2023.   \n[46] Han Huang, Leilei Sun, Bowen Du, and Weifeng Lv. Learning Joint 2D & 3D Diffusion Models for Complete Molecule Generation, June 2023. URL http://arxiv.org/abs/2305.12347. JODO.   \n[47] Lei Huang, Hengtong Zhang, Tingyang Xu, and Ka-Chun Wong. MDM: Molecular Diffusion Model for 3D Molecule Generation, September 2022. URL http://arxiv.org/abs/2209. 05710. MDM.   \n[48] Xingang Peng, Shitong Luo, Jiaqi Guan, Qi Xie, Jian Peng, and Jianzhu Ma. Pocket2mol: Efficient molecular sampling based on 3d protein pockets. In International conference on machine learning, pages 17644\u201317655. PMLR, 2022.   \n[49] Meng Liu, Youzhi Luo, Kanji Uchino, Koji Maruhashi, and Shuiwang Ji. Generating 3d molecules for target protein binding. arXiv preprint arXiv:2204.09410, 2022.   \n[50] Jiaqi Guan, Wesley Wei Qian, Xingang Peng, Yufeng Su, Jian Peng, and Jianzhu Ma. 3d equivariant diffusion for target-aware molecule generation and affinity prediction. arXiv preprint arXiv:2303.03543, 2023.   \n[51] Fergus Imrie, Thomas E Hadfield, Anthony R Bradley, and Charlotte M Deane. Deep generative design with 3D pharmacophoric constraints. Chemical science, 12(43):14577\u201314589, 2021. Publisher: Royal Society of Chemistry.   \n[52] Yinan Huang, Xingang Peng, Jianzhu Ma, and Muhan Zhang. 3DLinker: an E (3) equivariant variational autoencoder for molecular linker design. arXiv preprint arXiv:2205.07309, 2022.   \n[53] Fergus Imrie, Anthony R Bradley, Mihaela van der Schaar, and Charlotte M Deane. Deep generative models for 3D linker design. Journal of chemical information and modeling, 60(4): 1983\u20131995, 2020. Publisher: ACS Publications.   \n[54] Jiaqi Guan, Xingang Peng, PeiQi Jiang, Yunan Luo, Jian Peng, and Jianzhu Ma. LinkerNet: Fragment Poses and Linker Co-Design with 3D Equivariant Diffusion. Advances in Neural Information Processing Systems, 36:77503\u201377519, December 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/hash/ f4821075019a058700f6e6738eea1365-Abstract-Conference.html.   \n[55] Keir Adams and Connor W Coley. Equivariant shape-conditioned generation of 3d molecules for ligand-based drug design. arXiv preprint arXiv:2210.04893, 2022.   \n[56] Siyu Long, Yi Zhou, Xinyu Dai, and Hao Zhou. Zero-shot 3d drug design by sketching and generating. Advances in Neural Information Processing Systems, 35:23894\u201323907, 2022.   \n[57] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 2256\u20132265. PMLR, 2015.   \n[58] Marcel Kollovieh, Abdul Fatir Ansari, Michael Bohlke-Schneider, Jasper Zschiegner, Hao Wang, and Yuyang Wang. Predict, refine, synthesize: Self-guiding diffusion models for probabilistic time series forecasting. arXiv preprint arXiv:2307.11494, 2023.   \n[59] Wolfgang Kabsch. A solution for the best rotation to relate two sets of vectors. Acta Crystallographica Section A: Crystal Physics, Diffraction, Theoretical and General Crystallography, 32 (5):922\u2013923, 1976. Publisher: International Union of Crystallography.   \n[60] Wolfgang Kabsch. A discussion of the solution for the best rotation to relate two sets of vectors. Acta Crystallographica Section A: Crystal Physics, Diffraction, Theoretical and General Crystallography, 34(5):827\u2013828, 1978. Publisher: International Union of Crystallography.   \n[61] Paul J Besl and Neil D McKay. Method for registration of 3-D shapes. In Sensor fusion IV: control paradigms and data structures, volume 1611, pages 586\u2013606. Spie, 1992.   \n[62] Robert Abel and Sathesh Bhat. Chapter Seven - Free Energy Calculation Guided Virtual Screening of Synthetically Feasible Ligand R-Group and Scaffold Modifications: An Emerging Paradigm for Lead Optimization. In Robert A. Goodnow, editor, Annual Reports in Medicinal Chemistry, volume 50 of Platform Technologies in Drug Discovery and Validation, pages 237\u2013262. Academic Press, January 2017. doi: 10.1016/bs.armc.2017.08.007. URL https: //www.sciencedirect.com/science/article/pii/S0065774317300106.   \n[63] Qingxin Li. Application of Fragment-Based Drug Discovery to Versatile Targets. Frontiers in Molecular Biosciences, 7:180, 2020. ISSN 2296-889X. doi: 10.3389/fmolb.2020.00180.   \n[64] Hans-Joachim B\u00f6hm, Alexander Flohr, and Martin Stahl. Scaffold hopping. Drug Discovery Today. Technologies, 1(3):217\u2013224, December 2004. ISSN 1740-6749. doi: 10.1016/j.ddtec. 2004.10.009.   \n[65] Chunquan Sheng and Wannian Zhang. Fragment informatics and computational fragment-based drug design: an overview and update. Medicinal Research Reviews, 33(3):554\u2013598, May 2013. ISSN 1098-1128. doi: 10.1002/med.21255.   \n[66] Daniil Polykovskiy, Alexander Zhebrak, Benjamin Sanchez-Lengeling, Sergey Golovanov, Oktai Tatanov, Stanislav Belyaev, Rauf Kurbanov, Aleksey Artamonov, Vladimir Aladinskiy, Mark Veselov, Artur Kadurin, Simon Johansson, Hongming Chen, Sergey Nikolenko, Alan Aspuru-Guzik, and Alex Zhavoronkov. Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models, October 2020. URL http://arxiv.org/abs/1811.12823. arXiv:1811.12823 [cs, stat].   \n[67] Jiaqi Guan, Xiangxin Zhou, Yuwei Yang, Yu Bao, Jian Peng, Jianzhu Ma, Qiang Liu, Liang Wang, and Quanquan Gu. DecompDiff: Diffusion Models with Decomposed Priors for StructureBased Drug Design, 2024. URL https://arxiv.org/abs/2403.07902. Version Number: 1.   \n[68] Paul G Francoeur, Tomohide Masuda, Jocelyn Sunseri, Andrew Jia, Richard B Iovanisci, Ian Snyder, and David R Koes. Three-dimensional convolutional neural networks and a crossdocked data set for structure-based drug design. Journal of chemical information and modeling, 60(9):4200\u20134215, 2020. Publisher: ACS Publications.   \n[69] Liegi Hu, Mark L Benson, Richard D Smith, Michael G Lerner, and Heather A Carlson. Binding MOAD (mother of all databases). Proteins: Structure, Function, and Bioinformatics, 60(3): 333\u2013340, 2005. Publisher: Wiley Online Library.   \n[70] Zhilin Huang, Ling Yang, Xiangxin Zhou, Zhilong Zhang, Wentao Zhang, Xiawu Zheng, Jie Chen, Yu Wang, CUI Bin, and Wenming Yang. Protein-ligand interaction prior for bindingaware 3d molecule diffusion models. The Twelfth International Conference on Learning Representations 2024.   \n[71] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11461\u201311471, 2022.   \n[72] Amr Alhossary, Stephanus Daniel Handoko, Yuguang Mu, and Chee-Keong Kwoh. Fast, accurate, and reliable molecular docking with QuickVina 2. Bioinformatics (Oxford, England), 31(13):2214\u20132216, 2015. Publisher: Oxford University Press.   \n[73] Greg Landrum and others. RDKit: A software suite for cheminformatics, computational chemistry, and predictive modeling. Greg Landrum, 8:31, 2013.   \n[74] John J. Irwin, Khanh G. Tang, Jennifer Young, Chinzorig Dandarchuluun, Benjamin R. Wong, Munkhzul Khurelbaatar, Yurii S. Moroz, John Mayfield, and Roger A. Sayle. ZINC20\u2014A Free Ultralarge-Scale Chemical Database for Ligand Discovery. Journal of Chemical Information and Modeling, 2020. ISSN 1549-9596. doi: 10.1021/acs.jcim.0c00675.   \n[75] Alexander G Dossetter, Edward J Griffen, and Andrew G Leach. Matched molecular pair analysis in drug discovery. Drug Discovery Today, 18(15-16):724\u2013731, 2013. Publisher: Elsevier.   \n[76] Priyank Jaini, Lars Holdijk, and Max Welling. Learning equivariant energy based models with equivariant stein variational gradient descent. Advances in Neural Information Processing Systems, 34:16727\u201316737, 2021.   \n[77] Maciej W\u00f3jcikowski, Piotr Zielenkiewicz, and Pawel Siedlecki. Open Drug Discovery Toolkit (ODDT): a new open-source player in the drug discovery field. Journal of cheminformatics, 7 (1):1\u20136, 2015. Publisher: BioMed Central.   \n[78] Mikko J Vainio, J Santeri Puranen, and Mark S Johnson. ShaEP: molecular overlay based on shape and electrostatic potential, 2009.   \n[79] Freyr Sverrisson, Jean Feydy, Bruno E. Correia, and Michael M. Bronstein. Fast end-to-end learning on protein surfaces. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 15267\u201315276, June 2021. doi: 10.1109/CVPR46437.2021.01502. URL https://ieeexplore.ieee.org/document/9577686. ISSN: 2575-7075.   \n[80] Maciej W\u00f3jcikowski, Piotr Zielenkiewicz, and Pawel Siedlecki. Open Drug Discovery Toolkit (ODDT): a new open-source player in the drug discovery field. Journal of Cheminformatics, 7(1):26, December 2015. ISSN 1758-2946. doi: 10.1186/s13321-015-0078-2. URL https: //jcheminf.biomedcentral.com/articles/10.1186/s13321-015-0078-2.   \n[81] Octavian-Eugen Ganea, Xinyuan Huang, Charlotte Bunne, Yatao Bian, Regina Barzilay, Tommi Jaakkola, and Andreas Krause. Independent se (3)-equivariant models for end-to-end rigid protein docking. arXiv preprint arXiv:2111.07786, 2021.   \n[82] Jan Zaucha, Charlotte A. Softley, Michael Sattler, Dmitrij Frishman, and Grzegorz M. Popowicz. Deep learning model predicts water interaction sites on the surface of proteins using limitedresolution data. Chemical Communications, 56(98):15454\u201315457, December 2020. ISSN 1364-548X. doi: 10.1039/D0CC04383D. URL https://pubs.rsc.org/en/content/ articlelanding/2020/cc/d0cc04383d. Publisher: The Royal Society of Chemistry.   \n[83] Huimin Zhu, Renyi Zhou, Dongsheng Cao, Jing Tang, and Min Li. A pharmacophore-guided deep learning approach for bioactive molecular generation. Nature Communications, 14(1): 6234, October 2023. ISSN 2041-1723. doi: 10.1038/s41467-023-41454-9. URL https:// www.nature.com/articles/s41467-023-41454-9. Publisher: Nature Publishing Group.   \n[84] William E. Lorensen and Harvey E. Cline. Marching cubes: A high resolution 3D surface construction algorithm. In Proceedings of the 14th annual conference on Computer graphics and interactive techniques, SIGGRAPH \u201987, pages 163\u2013169, New York, NY, USA, August 1987. Association for Computing Machinery. ISBN 978-0-89791-227-3. doi: 10.1145/37401.37422. URL https://dl.acm.org/doi/10.1145/37401.37422. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Impact Statement ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Our research holds the promise of significant contributions to the advancement of drug discovery, possibly assisting in the discovery of novel pharmaceutical compounds. Nevertheless, because of its applications in drug discovery, this strategy is not without its hazards. The ability to produce various molecules with desired properties may not only serve the purpose of beneficial drug development but may also unintentionally result in the creation of dangerous substances or compounds with unexpected effects. These concerns underline the critical need for careful handling when working with the structures this method can generate. ", "page_idx": 16}, {"type": "text", "text": "B Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "First, recall Theorem 4.1 that we provide in Sec. 4: ", "page_idx": 16}, {"type": "text", "text": "Theorem 1. Consider a function $C:S\\times\\mathcal{Z}\\to\\mathcal{Z}$ . If $C(s,\\mathbf{z})$ is invariant to rigid transformations $G$ in the first argument and equivariant in the second argument, then the gradient $\\nabla_{\\mathbf{z}}\\big|\\big|\\mathbf{v}\\big|\\big|_{2}^{2}$ of the vector $\\pmb{v}=\\mathbf{z}-C(\\pmb{s},\\mathbf{z})$ is equivariant to transformations of ${\\bf z}$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. We start the proof by showing that $\\lVert\\pmb{v}\\rVert_{2}$ is invariant to transformations of both $\\mathbf{z}$ and $\\pmb{s}$ . ", "page_idx": 16}, {"type": "text", "text": "1. $\\|\\mathbf{z}-C(\\pmb{\\mathscr{s}},\\mathbf{z})\\|_{2}$ is invariant to transformations in $_{\\textit{z}}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\big\\|G\\mathbf{z}-C(\\pmb{s},G\\mathbf{z})\\big\\|_{2}=\\big\\|G\\mathbf{z}-G C(\\pmb{s},\\mathbf{z})\\big\\|_{2}}&{\\quad[C\\mathrm{~is~equivariant~in~}z]}\\\\ {=\\big\\|G(\\mathbf{z}-C(\\pmb{s},\\mathbf{z}))\\big\\|_{2}}&{}\\\\ {=\\big\\|\\mathbf{z}-C(\\pmb{s},\\mathbf{z})\\big\\|_{2}}&{\\quad[G\\mathrm{~is~a~rigid~transformation}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "2. $\\|\\mathbf{z}-C(\\pmb{s},\\mathbf{z})\\|_{2}$ is invariant to transformations $\\pmb{s}$ follows immediately: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\big\\|\\mathbf{z}-C(G s,\\mathbf{z})\\big\\|_{2}=\\big\\|\\mathbf{z}-C(\\pmb{\\mathscr{s}},\\mathbf{z})\\big\\|_{2}\\qquad[C\\mathrm{~is~invariant~in~}\\pmb{s}]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In a second step, we make use of the fact that for a group of transformations $G$ , it holds that if $\\mathcal{L}(\\cdot,\\cdot)$ is a $G$ -invariant function, $\\nabla_{x}\\mathcal{L}(\\cdot,x)$ is $G$ -equivariant [76]. From the invariance of $\\lVert\\pmb{v}\\rVert_{2}$ , it follows immediately that $\\nabla_{\\mathbf{z}}\\big\\|\\mathbf{z}-C(\\pmb{s},\\mathbf{z})\\big\\|_{2}^{2}$ is equivariant to transformations of $\\mathbf{z}$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "C Unconditional Equivariant Diffusion Model ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "UniGuide guides an unconditional diffusion model given an arbitrary condition and a natural choice for a model operating only on the molecule level is the EDM model as proposed in Hoogeboom et al. [20]. ", "page_idx": 16}, {"type": "text", "text": "We adapt this model for two tasks presented in this work, namely the LBDD task discussed in Sec. 5.1 and the Linker Design task as presented in Sec. 5.3. For these tasks, we train an unconditional EDM model both on the MOSES dataset [66] in the configuration as described in Chen et al. [14] and on the ZINC dataset [74] as described in Igashov et al. [13]. For both trainings, we employ the hyperparameter configuration for the GEOM dataset as described in Hoogeboom et al. [20]. We run multi-GPU trainings on 4 NVIDIA A100 GPUs until convergence, however, a single NVIDIA A100 GPU is sufficient for this training and will only increase the training time. For inference, we employ the Resampling trick as discussed in Lugmayr et al. [71] with $R=10$ resampling steps and $T=100$ timesteps. EDM is available under the MIT License. ", "page_idx": 16}, {"type": "text", "text": "D Ligand-based drug design ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D.1 Implementation details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We train two unconditional diffusion models, ShapeMol [U] and EDM, to generate 3D molecules on the MOSES dataset [66], licensed under the MIT License, for which we generate 3D conformers with RDKit [73], available under the BSD 3-Clause License. We use 1, 593, 653 training samples and randomly select 1000 samples for validation. The model architecture of ShapeMol[U] is an unconditional version of the ShapeMol model proposed in Chen et al. [14], and it is trained with 1000 diffusion steps. ShapeMol [U] is trained with a batch size of 32 on two NVIDIA A100 GPUs for 500 epochs. Unlike ShapeMol, we do not concatenate the molecular surface embedding of the ligands to the features. For the shape-conditioned generation with position correction (ShapeMol+g), we follow the scheme proposed in Chen et al. [14]. It provides further guidance to the conditional generation by sampling 20 query points from a Gaussian distribution centred around every atom in the reference ligand. The position correction adjusts the coordinates of the predicted atom positions during every generation step by pushing the coordinates close to the query points as follows: ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{x}}=(1-\\sigma)\\hat{\\mathbf{x}}+\\sigma\\sum_{\\mathbf{z}\\in n(\\hat{\\mathbf{x}},Q)}\\mathbf{z}/n,\\operatorname{if}\\sum_{\\mathbf{z}\\in n(\\hat{\\mathbf{x}},Q)}d(\\hat{\\mathbf{x}},\\mathbf{z})/n>\\gamma,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $d({\\hat{\\mathbf{x}}},\\mathbf{z})$ is the Euclidean distance, $n(\\hat{{\\pmb x}},{\\pmb\\mathcal Q})$ is the set of $n$ nearest neighbors of $\\hat{\\pmb{x}}$ in $\\mathcal{Q}$ and $\\gamma\\,>\\,0$ is a distance threshold. We follow the implementation of Chen et al. [2] for the position correction method by setting $\\gamma=0.2$ and only guiding during the first 700 denoising steps. ", "page_idx": 17}, {"type": "text", "text": "For the shape-conditioned generation with UniGuide, we extract the mesh of the condition ligand using the Open Drug Discovery Toolkit [77], which is available under the BSD 3-Clause revised License. The query points we use for guidance are 512 points sampled uniformly on the mesh surface. For the evaluation, we measure the shape similarity $\\mathrm{Sim}_{S}$ as the volume overlap between the aligned generated ligand and the condition ligand. For the alignment, we utilise the ShaEP tool [78]. ", "page_idx": 17}, {"type": "text", "text": "We provide a detailed description of the LBDD sampling algorithm in Algorithm ", "page_idx": 17}, {"type": "text", "text": "Algorithm 1: Sampling algorithm to generate a ligand that is conditioned on a reference ligand $\\mathcal{M}_{\\mathrm{ref}}$ \u2019s surface, using an unconditional model $\\epsilon_{\\theta}(\\mathbf{z}_{t},t)$ modelling the distribution over molecules. The points $\\mathbf{y}\\in\\mathbb{R}^{K\\times3}$ are sampled uniformly from the surface of $\\mathcal{M}_{\\mathrm{ref}}$ , enclosing the volume $V$ . ", "page_idx": 17}, {"type": "text", "text": "Require: y, $\\alpha$ : desired margin to surface, $k$ : number of nearest neighbours   \n$\\mathbf{z}_{T}\\sim\\mathcal{N}(\\mathbf{0},I)$ {Sample from normal prior}   \nfor $t=T$ to 1 do $\\begin{array}{r l}&{\\mathbf{x}_{t},\\mathbf{h}_{t}=\\mathbf{z}_{t}}\\\\ &{\\hat{\\mathbf{x}}_{0}=\\frac{\\mathbf{x}_{t}-\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon_{\\theta}^{\\mathbf{x}}(\\mathbf{z}_{t},t)}{\\sqrt{\\bar{\\alpha}_{t}}}}\\end{array}$ {Compute the conformation $\\hat{\\mathbf{x}}_{\\mathrm{0}}$ of the clean approximation $\\hat{\\mathbf{z}}_{0}$ } For every atom $\\hat{\\pmb x}_{i}$ in $\\hat{\\mathbf{x}}_{0}$ do: $\\begin{array}{r}{\\bar{y}_{i}=\\frac{1}{k}\\sum_{y\\in\\mathcal{N}_{\\hat{x}_{i}}}y}\\end{array}$ {Compute the mean of $k$ nearest neighbors of $\\hat{\\pmb{x}}_{i}$ in y} Compute $(c_{\\mathbf{x}})_{i}$ based on Eq. (21) {Compute component-wise condition map} $\\begin{array}{r l}&{\\mathcal{L}=\\mathcal{L}\\hat{(}\\hat{\\mathbf{x}}_{0},c_{\\mathbf{x}})}\\\\ &{\\pmb{g}=\\nabla_{\\mathbf{x}_{t}}\\mathcal{L}}\\\\ &{\\pmb{\\mu}_{t}=\\pmb{\\mu}_{\\theta}(\\mathbf{z}_{t},t)-\\lambda(t)\\cdot\\pmb{g}}\\\\ &{\\mathbf{z}_{t-1}\\sim\\mathcal{N}(\\pmb{\\mu}_{t},\\sigma_{t}\\pmb{I})}\\end{array}$ {Compute gradient of guidance loss} {Update the mean function}   \nend for   \nreturn $\\mathbf{z}_{\\mathrm{0}}$ ", "page_idx": 17}, {"type": "text", "text": "D.2 Additional results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For completeness, we report additional quantitative evaluation of the generated ligands\u2019 properties in Tab. 4. We also provide further qualitative results of the generated ligands for the LBDD task in Fig. 6. UniGuide generates ligands with better shape similarity to the reference ligands compared to the conditional model ShapeMol with the position correction technique. ", "page_idx": 17}, {"type": "table", "img_path": "HeoRsnaD44/tmp/e03cd193846e7fa55b99a22a31f73062a0ef22a0fd267fdb1931473e3a811f16.jpg", "table_caption": ["Table 4: Additional ligand property results for the methods discussed in Sec. 5.1. We report mean and standard deviation and highlight the best result in bold. "], "table_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "HeoRsnaD44/tmp/6480991b095340f90bc8d2c0da302d79b489c5367cae694bf3c91b8e53177a9f.jpg", "img_caption": ["Figure 6: Examples of the ligands generated by ShapeMol, Pos-Correct and UniGuide. Pos-Correct is the position correction technique proposed by Chen et al. [14]. Both Pos-Correct and UniGuide are combined with the unconditionally trained model ShapeMol [U]. We plot the reference ligand as well as the generated ligands with their shapes. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 5: Comparison of UniGuide with validity guidance for shape-based generation. We highlight the ratio metric as the most critical indicator, reflecting the balance between shape similarity and graph dissimilarity. ", "page_idx": 18}, {"type": "table", "img_path": "HeoRsnaD44/tmp/bb442b61b7b0738afa43259d4b49983383c9c734f1f3d6d9e8ec92279648f75e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "D.3 Guidance parameters ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For the LBDD task, the guidance strength $S$ is weighted by an exponentially decreasing function $\\textstyle{\\frac{\\beta_{t}}{\\sqrt{\\alpha_{t}}}}$ . For the guided generation using the unconditional ShapeMol [U] model under the UniGuide framework, we define a scale scheduler that increases with an exponent of 1.01 and weight it with $\\frac{\\beta_{t}}{\\sqrt{\\alpha_{t}}}$ and guide from the diffusion step 1000 to the diffusion step 200. For the guided generation using the EDM model, we use a linear scale function that increases from 5 to 15. The guidance is applied from the diffusion step 920 to the last timestep 1. ", "page_idx": 18}, {"type": "text", "text": "D.4 Comparison of UniGuide with an alternative loss formulation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We adapt the validity guidance loss from Guan et al. [50] to the LBDD setting. The proposed loss is grounded in the smooth distance function $S(x)$ from Sverrisson et al. [79], which computes as: ", "page_idx": 18}, {"type": "equation", "text": "$$\nS(x)=-\\sigma\\log\\Big(\\sum_{i}^{N}\\exp(-\\|x_{t}-y_{i}\\|_{2}^{2}/\\sigma)\\Big)\\quad.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This function provides an alternative approach to shape-based generation by deriving an appropriate loss function $\\overline{{\\sum_{x}S(x)}}$ , rather than modifying the condition map as proposed by UniGuide. Here, $S(x)$ implicit ly defines a surface through $S(x)-\\gamma=0$ and points $x_{t}$ inside satisfy $S(x_{t})<\\gamma$ . ", "page_idx": 18}, {"type": "text", "text": "On a technical level, the gradient for validity guidance computes as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\nabla_{x_{t}}S(x_{t})\\!\\!}&{=\\nabla_{x_{t}}\\Big[-\\sigma\\log\\Big(\\displaystyle\\sum_{i}^{N}\\exp(-\\|x_{t}-y_{i}\\|_{2}^{2}/\\sigma)\\Big)\\Big]}\\\\ &{=\\displaystyle\\frac{1}{\\sum\\exp(\\cdot\\cdot\\cdot)}\\sum_{i}^{N}\\underbrace{\\exp(\\cdot\\cdot\\cdot)}_{\\omega_{i}}\\nabla_{x_{t}}\\|x_{t}-y_{i}\\|_{2}^{2}}\\\\ &{=\\displaystyle\\frac{1}{\\sum\\omega_{i}}\\nabla_{x_{t}}\\sum_{i}^{N}\\omega_{i}\\|x_{t}-y_{i}\\|_{2}^{2}\\quad.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This gradient formulation is quite similar (up to the weighting) to UniGuide\u2019s special case $S={\\mathcal{Z}}$ , as it computes an $L_{2}$ loss on a given conformation $(\\{y_{i}\\})$ , see Eq. (18), meaning that it does not generalise to arbitrary geometric conditions. ", "page_idx": 19}, {"type": "text", "text": "We emphasise that UniGuide is more broadly applicable because it separates surface computation from gradient computation, offering two key beneftis. First, since the condition map does not require differentiability, there is greater flexibility in computing surface points. Second, the precise geometric intuition behind the condition map makes it easier to adapt to new scenarios, as demonstrated by our application to generating density-guided molecules. ", "page_idx": 19}, {"type": "text", "text": "For the empirical comparison, we selected the hyperparameters $\\sigma$ and $\\gamma$ in the surface loss computation to achieve a high DICE score between the implicitly defined surface and the meshes UniGuide utilises for LBDD $\\smash{\\sigma=1}$ , $\\gamma=2$ , $\\mathrm{DICE}>0.8)$ ). Our surface calculations use the Open Drug Discovery Toolkit (ODDT), which assigns specific radii to individual atom types and employs the marching cubes algorithm to generate meshes [80]. ", "page_idx": 19}, {"type": "text", "text": "We performed several runs around the above-specified hyperparameter configuration. The runs performed similarly, and we report the best result in Tab. 5. Although validity guidance for LBDD yields low graph similarity, the shape similarity remains suboptimal compared to UniGuide. Additionally, we frequently encounter numerical instability when computing the guidance term, an issue not present with UniGuide\u2019s formulation of LBDD. One possible explanation for this numerical instability is that the surface is defined implicitly, unlike UniGuide where it is explicitly defined. The explicit definition in UniGuide allows for relating the gradient updates directly to the surface, as shown in Eq. (21). ", "page_idx": 19}, {"type": "text", "text": "E Structure-based drug design ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Algorithm 2: Sampling algorithm to generate a ligand conditioned on a protein pocket $\\tilde{\\mathbf{z}}^{\\mathcal{P}}$ using the unconditional joint model $\\epsilon_{\\theta}(\\mathbf{z}_{t},t)$ , where $\\mathbf{z}_{t}=\\left[\\mathbf{z}_{t}^{\\mathcal{M}},\\mathbf{z}_{t}^{\\mathcal{P}}\\right]$ , that models the distribution $P(\\mathbf{z}^{\\mathcal{M}},\\mathbf{z}^{\\mathcal{P}})$ The guidance signal is controlled via the guidance strength $S$ . Note that samples from the generative process $p_{\\theta}\\big(\\mathbf{z}_{t-1}\\big|\\mathbf{z}_{t}\\big)$ are assumed to be CoM-free. ", "page_idx": 19}, {"type": "text", "text": "Require: $\\tilde{\\mathbf{z}}^{\\mathcal{P}}$ , $S$   \n$\\mathbf{z}_{T}\\sim\\mathcal{N}(\\mathbf{0},\\pmb{I})$   \nfor $t=T$ to 1 do $\\hat{\\mathbf{z}}_{0}^{\\mathcal{P}}=\\mathbf{z}_{t}^{\\mathcal{P}}-\\sqrt{1-\\bar{\\alpha_{t}}}\\epsilon_{\\theta}^{\\mathcal{P}}(\\mathbf{z}_{t},t)/\\sqrt{\\bar{\\alpha}_{t}}$ $\\mathcal{L}=\\mathcal{L}(\\hat{\\mathbf{z}}_{0}^{\\mathcal{P}},\\tilde{\\mathbf{z}}^{\\mathcal{P}})$ g = (\u2207xtL \u2212\u2207xtL, \u2207htL) \u00b5t = \u00b5\u03b8(zt, t) \u2212\u03bb(t) \u00b7 g zt\u22121 \u223cN(\u00b5t, \u03c3tI)   \nend for   \nreturn $\\mathbf{z}=(\\mathbf{z}_{0}^{\\mathcal{M}},\\mathbf{z}_{0}^{\\mathcal{P}})$ ", "page_idx": 19}, {"type": "text", "text": "{Sample from normal prior} {Compute the clean data of the pocket} {Compute gradient and substract the CoM} {Update the mean of the pocket} ", "page_idx": 19}, {"type": "text", "text": "E.1 SBDD sampling algorithm ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We provide the algorithm for inference in the SBDD task scenario in Algorithm 2. ", "page_idx": 19}, {"type": "text", "text": "SBDD aims to generate a ligand given a protein pocket: $p_{\\theta}(\\mathbf{z}^{\\mathcal{M}}\\mid\\mathbf{z}_{\\mathrm{test}}^{\\mathcal{P}},t)$ . We adopt DiffSBDD [11], an unconditional joint diffusion model that approximates the joint distribution $p(\\mathbf{z}_{\\mathrm{data}}^{\\mathcal{M}},\\mathbf{z}_{\\mathrm{data}}^{\\mathcal{P}})$ of generating ligand-protein pairs, where the noise predictor $\\epsilon_{\\theta}(\\mathbf{z}_{t}^{\\mathcal{M}},\\mathbf{z}_{t}^{\\mathcal{P}},t)$ is parametrised by EGNN. DiffSBDD is available under the MIT License. To process ligand and pocket nodes with a single GNN, atom types and residue types are embedded jointly. Atom and residue features are then decoded separately using atom decoder and residue decoder to $\\dot{\\epsilon}_{\\theta}^{\\mathcal{M}}(\\mathbf{z}_{t}^{\\mathcal{M}},\\mathbf{z}_{t}^{\\mathcal{P}},t)$ and $\\epsilon_{\\theta}^{\\mathcal{P}}(\\mathbf{z}_{t}^{\\mathcal{M}},\\mathbf{z}_{t}^{\\mathcal{P}},t)$ [11]. ", "page_idx": 20}, {"type": "text", "text": "For the unconditional sampling with the joint model, the number of ligand and pocket nodes is sampled from the joint node distribution $\\bar{p}(N^{\\mathcal{M}},N^{\\mathcal{P}})$ , measured across a training set of $(\\mathcal{M},\\mathcal{P})$ pairs. During the modified generative process with the inpainting-inspired technique or with UniGuide the number of pocket nodes is set to be equal to the number of nodes in $\\mathcal{P}_{\\mathrm{test}}$ , while the size of the ligand is generated from a conditional distribution $p(N^{\\mathcal{M}}\\mid N^{\\mathcal{P}})$ . Since this sampling procedure leads to ligands that are much smaller compared to the reference ligands found in the test set, the mean size of sampled ligands is increased by 10 for Binding MOAD and 5 for CrossDocked during ligand generation [11]. We utilize the unconditional base models from Schneuing et al. [11], which are trained on either the $C_{\\alpha}$ or full-atom context from the Binding MOAD or CrossDocked datasets. However, we retrain the DiffSBDD model specifically on the full-atom context of the CrossDocked data, as we were unable to reproduce the reported results in this configuration from Schneuing et al. [11]. We find that contrary to what is reported in Schneuing et al. [11], the model converges early and does not need a full 1000 epochs to fully train. We employ this checkpoint to evaluate both the DiffSBDD inpainting-inspired approach as well as UniGuide. We train the model on four NVIDIA A100 GPU with a batch size of 2. 8 training epochs take approximately 24 hours. ", "page_idx": 20}, {"type": "table", "img_path": "HeoRsnaD44/tmp/af8a3bb6e666984a2ca0cc0402f1c6ea4b6c3162e96e9df70d85bf466a68a69b.jpg", "table_caption": ["Table 6: Hyperparameters of ligand and proteins graphs in joint models "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Representing ligands and proteins as graphs Proteins consist of amino acids, where every amino acid is a set of amino $(N H)$ , carboxyl $(C O)$ , $\\alpha$ -carbon atom and a side chain $(R)$ that is specific to every amino acid type [81]. The $C_{\\alpha}$ -representation of a protein pocket is a residue-level graph, in which the node features of the protein are represented as one-hot encodings of the amino acid type. The full-atom representation of the receptor is an atom-level graph and represents the full context of the protein pocket. Details on processed graphs of the join model $p(\\mathbf{z}^{\\mathcal{M}},\\mathbf{\\dot{z}}^{\\mathcal{P}})$ are provided in Tab. 6. We refer the reader to Schneuing et al. [11] for more information on the hyperparameters of the joint model. ", "page_idx": 20}, {"type": "text", "text": "Table 7: Quantitative evaluation of samples generated by the unconditional joint models [11] trained on Crossdocked (C.D.) and Binding MOAD (B.M). We report the mean over all generated ligands. ", "page_idx": 20}, {"type": "table", "img_path": "HeoRsnaD44/tmp/c48ef69d9c86fcfd4fedb5850aff70b4e3faa4fe30d8962b5c923a633c06162d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "E.3 Further Comparison to DiffSBDD ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In addition to Tab. 2, we follow the experimental setup as utilised in Schneuing et al. [11] to compare UniGuideto DiffSBDD, which uses the same base model, in particular. In Tab. 8, we further investigate the advantages of using self-guidance in combinations with UniGuide over both the conditional DiffSBDD model (DiffSBDD-cond) as well as the inpainting-inspired technique (DiffSBDD). UniGuide reliably achieves superior VINA Dock scores compared to both DiffSBDD models and performs competitively with the conditional TargetDiff model. In App. E.4 and App. E.5, we expand on this experimental comparison with further analysis of the effects of Resampling as well as the guidance strength. ", "page_idx": 20}, {"type": "table", "img_path": "HeoRsnaD44/tmp/428dd2dd95d0b51e174403a281f3215bf6e3b2dc44fbf36e13fd4e717ee752e8.jpg", "table_caption": ["Table 8: Quantitative comparison of generated ligands for target pockets from the CrossDocked and Binding MOAD test sets. Results taken from Schneuing et al. [11] are indicated with $\\left(^{*}\\right)$ . We report mean and standard deviation and highlight the best diffusion-based approach in bold. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "E.4 Resampling ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Inpainting is introduced for diffusion models to condition outputs with fixed parts [71] and can be applied for structure-based molecular tasks. Given a model that generates $\\bar{(\\mathbf{z}_{t}^{\\mathcal{M}},\\mathbf{z}_{t}^{\\mathcal{P}})}$ pairs at denoising step $t$ , the protein pocket $P_{t}$ is replaced with the noised representation of protein context $\\tilde{\\mathbf{z}}_{t}^{\\mathcal{P}}$ . This noised representation can be obtained through the forward process of diffusion models as specified in Eq. (2). However, the direct application of this method leads to locally harmonised samples that struggle to incorporate the global context [71]. In order to effectively harmonise the generated information during the entire generative process, Lugmayr et al. [71] propose a technique they call \u201cResampling\u201d. This modifies the reverse Markov chain by moving back and forth in the diffusion process to enable the model to better incorporate the replaced components. ", "page_idx": 21}, {"type": "text", "text": "Schneuing et al. [11] propose to use the same resampling technique to harmonise the replaced protein context with the ligand, since the replaced receptor is sampled independently of the ligand. During resampling, each latent representation is repeatedly diffused back and forth before advancing to the next time step. We found that resampling further improves the general performance of the unconditional generation, and thus improves the guided generation as well. We report results for this in App. E.5, where we evaluate how the unconditional generation of the joint model is improved across different metrics with added resampling steps. We follow Schneuing et al. [11] in using the setting of $R=10$ resampling steps and $T=50$ timesteps. While DiffSBDD resamples the ligand and the noised target protein pocket, we resample the guided protein pocket and ligand with UniGuide. In general, the concept of resampling can be applied to harmonise the configuration $\\mathbf{z}_{t}$ with the condition $^c$ . ", "page_idx": 21}, {"type": "text", "text": "E.5 Guidance parameters ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The guidance scale $S$ controls the strength of the guiding signal, see Eq. (7) and it is weighted by $\\begin{array}{r}{w(t)=\\frac{\\beta(t)}{\\sqrt{\\alpha_{t}}}}\\end{array}$ during the generation. We use a constant scale $S$ for structure-based drug design experiments and evaluate for several guidance scale values in Tab. 9 and Tab. 10 for models trained on the Binding MOAD dataset with $C_{\\alpha}$ and full-atom representation respectively. The quantitative evaluation on the CrossDocked data is shown under Tab. 11 and Tab. 12 with additional metrics reported in Tab. 7. For the generation with the $C_{\\alpha}$ -models, we generate 100 samples for every test pocket with a batch size of 50. The full generation takes approximately 5 hours for Binding MOAD and 6 hours for CrossDocked. For the DiffSBDD model trained on the Binding MOAD fullatom pocket data, we use a batch size of 15 for the generation. We use a batch size of 2 to sample with the DiffSBDD model trained on CrossDocked (fullatom). ", "page_idx": 21}, {"type": "table", "img_path": "HeoRsnaD44/tmp/cdaf1e3232b4a608562ca03174b25ab0b7a9b33b0045f272a11d19d88c623b2e.jpg", "table_caption": ["Table 9: Results for the Binding MOAD test set with the unconditional DiffSBDD base model trained on the $C_{\\alpha}$ -representation of the pockets combined with UniGuide and the inpainting-inspired technique DiffSBDD [11]. We provide results for varying the guidance scales $S$ during our controlled generation. We also report results for the DiffSBDD-cond $(C_{\\alpha})$ model trained on the $C_{\\alpha}$ pockets. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "For all tables, we conduct the experiments both with and without resampling. The VINA Dock score is measured with QuickVina2 [72], available under the Apache License, and the chemical properties (QED, SA, Lipinski) are measured with RDKit. We note that in all ablation tables we measure the VINA Dock score on the processed molecules, following Schneuing et al. [11], while the VINA Dock score in Tab. 2 is measured following Guan et al. [67]. Both the VINA Dock score and chemical properties improve with additional resampling steps $(R=10,T=50)$ for both datasets. Additionally, increasing the guidance scale improves the RMSD with respect to the target protein, and results in generating ligands with an improved binding affinity (lower VINA). ", "page_idx": 22}, {"type": "text", "text": "E.6 Additional Results for SBDD ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Supplementary to Tab. 2 we provide additional metrics for the evaluation of the generated ligands in Tab. 14: the validity as measured by RDKit [73] and the connectivity, representing the percentage of valid molecules without any disconnected fragments. Additionally, we report the uniqueness and novelty of the valid connected ligands. ", "page_idx": 22}, {"type": "text", "text": "E.7 Runtime Comparison ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In Tab. 13, we provide a comparison of the different controlled generation mechanisms regarding their runtime. While UniGuide has a higher runtime compared to other conditioning mechanisms, as it has to compute gradients through the diffusion model at inference time, it stays comparable to other mechanisms such as inpainting. ", "page_idx": 22}, {"type": "text", "text": "Table 10: Results for the Binding MOAD test set with the unconditional DiffSBDD base model trained on the full-atom context of the pockets combined with UniGuide and the inpainting-inspired technique DiffSBDD [11]. We provide results for varying the guidance scales $S$ during our controlled generation. We also report results for the conditional diffusion model DiffSBDD-cond. ", "page_idx": 22}, {"type": "table", "img_path": "HeoRsnaD44/tmp/db655e7690c691c19e5d7eacab0fe525b50d7cd4b724647bb7e63f802604e214.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 11: Evaluation of the samples generated for the CrossDocked test set using the joint ligandprotein diffusion model trained on the $C_{\\alpha}$ pocket representation for varying guidance scales $S$ . The base model is combined either with the inpaitning-inspired technique (DiffSBDD) or UniGuide. We further report the evaluation of the molecules generated by the conditional model DiffSBDD-cond that is trained on the $C_{\\alpha}$ pocket representation. ", "page_idx": 23}, {"type": "table", "img_path": "HeoRsnaD44/tmp/568e350567c5400cea6e6f9005ced8841cb706b55adcca246686b5c1d5d15dfc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "F Fragment-based drug design ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "F.1 Linker Design ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "For the experimental evaluation of the linker design task, we follow Igashov et al. [13], employ the ZINC dataset [74] and preprocess it following Igashov et al. [13]. That is, 3D conformers are generated from the SMILES strings present in the dataset with RDKit [73]. We fragment the dataset ligands using an MMPA-based algorithm [75, 73], generating multiple fragment conditions per molecule. We train an unconditional EDM model for this task as specified in App. C. For the evaluation metrics, we follow Igashov et al. [13]. Note that the synthetic accessibility score computation (SA) in Tab. 3 differs from the remaining experimental evaluations. While Igashov et al. [13] report the SA score $s_{S A}$ directly, Schneuing et al. [11] report the SA score as $(10-s_{S A})/9$ . ", "page_idx": 23}, {"type": "text", "text": "For the task of linker design, we adjust the condition map as discussed in Sec. 4.2 slightly to include anchor information, similar in spirit to the DiffLinker model incorporating anchor information [13]. That is, additionally to guiding parts of the molecule to the desired fragment configuration, we additionally define a cuboid\u2019s surface that is defined from the specified anchor atoms. We can then utilise this surface condition $C_{\\partial V}$ to guide the linker atoms in accordance with Eq. (21). Additionally, we can expand this surface based on the linker size to ensure chemical validity of the generated linker. This condition map highlights the flexibility of UniGuide condition maps in various tasks, especially through the combination of two definitions of the condition map. For the experimental evaluation, we sample the size of the linker nodes uniformly in accordance with Igashov et al. [13] and compare to the DiffLinker model without an external network to predict the linker size. Note, however, that also the unconditional EDM model combined with UniGuide can be adapted to include such predictors. ", "page_idx": 23}, {"type": "table", "img_path": "HeoRsnaD44/tmp/6f258f6ef4139583335f0851dc8752de68382e8a2ecff96109a1d6c47142ff44.jpg", "table_caption": ["Table 12: Results for the CrossDocked test set with the joint model trained on the full-atom pocket representation of the pocket for varying guidance scales $S$ . The unconditional model is either controlled by the inpainting-inspired technique (DiffSBDD) or UniGuide. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 13: We evaluate the runtime of UniGuide and compare it to DiffSBDD-cond and DiffSBDD from Schneuing et al. [11]. We report the average time (in seconds) to generate 100 ligands per pocket for the CrossDocked $(C_{\\alpha})$ , Binding Moad $(C_{\\alpha})$ and Binding Moad (fullatom). ", "page_idx": 24}, {"type": "table", "img_path": "HeoRsnaD44/tmp/48f6a4375ed982a0febeca201969413853d9bede55494fee1078961ded272d6c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "HeoRsnaD44/tmp/ff05b1de7615f98933f47dab53fa5a28d32e37796c82b74669c6202e9d465982.jpg", "table_caption": ["Table 14: Additional metrics for the methods discussed in Sec. 5.2. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "F.2 General Fragment Conditions ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "To assess the performance of UniGuide for the task of FBDD, we create an experimental setup with the goal of generating ligands conditioned on desired fragments roughly following [13]. We select 10 random protein targets from the Binding MOAD dataset and decompose their corresponding reference ligands using an MMPA-based algorithm [75, 73]. This decomposition results in a set of 40 different scenarios, including separated fragments we want to link, a fragment to grow or small functional groups to perform scaffolding. For every set of fixed fragments, we aim to guide the unconditional generation of ligands towards the generation of a ligand containing the desired fragments. As the protein is not the target of the guidance, we employ the DiffSBDD-cond model, which is conditionally trained on the $\\left(C_{\\alpha}\\right)$ -representation of the protein pocket. For every set of fixed fragments, we generate 100 ligands and use a constant guidance scale of 8. ", "page_idx": 24}, {"type": "text", "text": "We provide quantitative results for the task of fragment-based drug design in Tab. 15. On the one hand, the task requires the desired fragments to be present in the generated molecule. Thus, we measure the success rate of recovery (Hit Ratio) and the RMSD between the generated fragments and desired fragments. On the other hand, given that the target fragments are met in the generated ligand, the generation has to achieve favourable chemical properties, high binding affinity, as well as high diversity within the set of generated ligands and low similarity to the reference ligand. As the Inpaint mechanism enforces the fragment during generation more strictly, it is able to achieve a better Hit Ratio and RMSD. Nevertheless, UniGuide achieves competitive results but also better VINA docking scores, better properties, and lower similarity compared to the reference ligand. ", "page_idx": 24}, {"type": "text", "text": "The FBDD task puts a hard constraint on the generated ligands, namely that a set of desired fragments has to be present in the generated ligand. However, neither DiffSBDD nor UniGuide can guarantee that the condition fragments are present in the generated samples. ", "page_idx": 24}, {"type": "text", "text": "We provide further qualitative results of the generated ligands for the FBDD task in Fig. 7. ", "page_idx": 24}, {"type": "image", "img_path": "HeoRsnaD44/tmp/87d49acaca7bdd1ce9c9e60b790f568cd5379c9462a9091a33f2af518387234a.jpg", "img_caption": ["Figure 7: Examples of the generated fragment conditioned ligands. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Table 15: Quantitative comparison between DiffSBDD and UniGuide for the FBDD task on the Binding MOAD $(C_{\\alpha})$ dataset. As the condition in this FBDD scenario is a hard constraint that entails the condition to be exactly present in the generation, we add a post-hoc step for both methods where we replace the inpainted or guided parts with the exact condition atoms. We report mean and standard deviation and highlight the best method in bold. ", "page_idx": 25}, {"type": "table", "img_path": "HeoRsnaD44/tmp/680b22245d874359f6ca8d60091403291ddab00973721d9ee6a5d5549c506780.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "G Atom densities in 3D space ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Similar to the guidance by the volume enclosed by the molecular surface, UniGuide allows to guide towards multiple point clouds simultaneously. A natural extension of LBDD would be to harness atom densities as described in Zaucha et al. [82]. Such a setting combines aspects of LBDD and SBDD as it provides conditions also on the feature space, yet the source can only be represented by point clouds. ", "page_idx": 25}, {"type": "text", "text": "In particular, we anticipate UniGuide to be useful in scenarios where explicit information about advantageous features of the ligand is provided in the form of 3D densities. Examples of this include a) volumetric densities that indicate beneficial placement of certain atom types, such as oxygen atoms [82] or b) pharmacophore-like retrieval of advantageous positions for aromatic rings, as utilised in e.g. Zhu et al. [83]. On a technical level, this setting assumes that instead of a reference ligand\u2019s structure, we only have access to (multiple) atom type densities that indicate preferred locations for optimal interaction with the protein. Additionally, instead of conditioning on a reference ligand\u2019s shape, we could condition on a protein pocket\u2019s surface, which primarily defines exclusion zones rather than precise atom placement. ", "page_idx": 25}, {"type": "text", "text": "Adapting UniGuide for such scenarios requires only minor adjustments, as the protein surface can treated like shapes in standard LBDD, defining an exclusion zone based on proximity to the surface. The atom densities are thresholded to reflect regions of high interest and converted to surfaces using the marching cubes algorithm [84]. To also include feature information, we effectively employ a modified condition map similar to Eq. (21) that extends the transformation from the conformation to the configuration space. Moreover, the number of atoms guided by each density is adjusted based on its volume, reflecting the varying influence of each density, and guidance is only applied if atoms are sufficiently close. ", "page_idx": 26}, {"type": "text", "text": "We show explorative results for the guided generation of molecules towards desired atom densities using UniGuide in Fig. 8. While our current approach represents a promising first step in tackling this task, we acknowledge the potential for further refinement and are eager to explore future improvements within the UniGuide framework. ", "page_idx": 26}, {"type": "image", "img_path": "HeoRsnaD44/tmp/e53a2764349625b5a6e2856febca8c13b52a4617bdb08479af8cd76a147be152.jpg", "img_caption": ["Figure 8: Given a source density of oxygens, we can extend UniGuide to generate ligands satisfying the condition. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The claims made in the abstract and introduction reflect the paper\u2019s contribution and scope: Sec. 4 details how UniGuide is readily adaptable to various tasks in drug design, attesting to the unification provided by the UniGuide framework. Sec. 5 emphasises this aspect through competitive or superior performance across various tasks, even when compared to task-specific baselines. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We discuss the limitations of UniGuide in Sec. 4. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We discuss in Sec. 4 that the generative process retains equivariance with an appropriately chosen condition map and provide a full proof for this discussion in App. B. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The setup of all experimental evaluations is described in App. E, App. F and App. D for the SBDD, FBDD and LBDD tasks respectively, including hyperparameters for UniGuide, dataset preprocessing and inference algorithms. For experimental evaluations performed according to previous work, we reference them accordingly. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 28}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We made the code available as part of the supplementary material with the submission. We have included the link to UniGuide\u2019s project page, which will reference the public codebase. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Both the discussion of the experiments provided in Sec. 5 as well as the supplementary information provided throughout the appendix ensures that the results are sufficiently contextualised for the reader. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Throughout the experimental evaluation we provide the mean and standard deviation for all metrics that can be computed e.g. per-sample or per-pocket to ensure statistical significance of the presented results. In cases where the metric aggregates the entire set of samples, we report the mean. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide details on the hardware requirements for the training of the evaluated unconditional models in App. E.2, App. D.1 and App. C for the DiffSBDD, ShapeMol and EDM model respectively. Additionally, we provide runtime comparisons for the inference with UniGuide compared to the evaluated baselines in App. E.7. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The research presented in this work conforms with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. \u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 30}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We discuss the broader impact of our work in App. A. We discuss the positive societal impacts of the proposed unification and the resulting flexibility of unconditional models to be adapted to various new drug discovery tasks in Sec. 1 and Sec. 6. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: The research discussed in this paper does not require safeguards to be put in place. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Where applicable, we credit and cite owners and authors of previous works and the accompanying codebases or datasets and provide the license under which the assets were made public. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Accompanying the supplementary material, we provide documentation and instructions to navigate and utilise the UniGuide codebase. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This work did not conduct research on human subjects or crowdsourcing experiments. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This work did not conduct experiments where human subject were involved and therefore does not require IRB approvals. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]