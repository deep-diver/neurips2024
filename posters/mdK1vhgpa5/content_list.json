[{"type": "text", "text": "Variational Continual Test-Time Adaptation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Continual Test-Time Adaptation (CTTA) task investigates effective domain adapta  \n2 tion under the scenario of continuous domain shifts during testing time. Due to the   \n3 utilization of solely unlabeled samples, there exists significant uncertainty in model   \n4 updates, leading CTTA to encounter severe error accumulation issues. In this paper,   \n5 we introduce VCoTTA, a variational Bayesian approach to measure uncertainties   \n6 in CTTA. At the source stage, we transform a pretrained deterministic model into   \n7 a Bayesian Neural Network (BNN) via a variational warm-up strategy, injecting   \n8 uncertainties into the model. During the testing time, we employ a mean-teacher   \n9 update strategy using variational inference for the student model and exponential   \n10 moving average for the teacher model. Our novel approach updates the student   \n11 model by combining priors from both the source and teacher models. The evidence   \n12 lower bound is formulated as the cross-entropy between the student and teacher   \n13 models, along with the Kullback-Leibler (KL) divergence of the prior mixture.   \n14 Experimental results on three datasets demonstrate the method\u2019s effectiveness in   \n15 mitigating error accumulation within the CTTA framework. Our code is anony  \n16 mously available at https://anonymous.4open.science/r/vcotta-D2C3/. ", "page_idx": 0}, {"type": "text", "text": "17 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "18 Continual Test-Time Adaptation (CTTA) [51] aims to enable a model to accommodate a sequence   \n19 of distinct distribution shifts during the testing time, making it applicable to various risk-sensitive   \n20 applications in open environments, such as autonomous driving and medical imaging. However, real  \n21 world non-stationary test data exhibit high uncertainty in their temporal dynamics [23], presenting   \n22 challenges related to error accumulation [51]. Previous CTTA studies rely on methods that enforce   \n23 prediction confidence, such as entropy minimization. However, these approaches often lead to   \n24 predictions that are overly confident and less well-calibrated, thus limiting the model\u2019s ability to   \n25 quantify risks during predictions. The reliable estimation of uncertainty becomes particularly crucial   \n26 in the context of continual distribution shift [40]. It is meaningful to design a model capable of   \n27 encoding the uncertainty associated with temporal dynamics and effectively handling distribution   \n28 shifts. The objective of this paper is to devise a CTTA procedure that not only enhances predictive   \n29 accuracy under distribution shifts but also provides reliable uncertainty estimates.   \n30 To address the above problem, we refer to the Bayesian Inference (BI) [1], which retains a distribution   \n31 over model parameters that indicates the plausibility of different settings given the observed data, and   \n32 it has been witnessed as effective in traditional continual learning tasks [38]. In Bayesian continual   \n33 learning, the posterior in the last learning task is set to be the current prior which will be multiplied   \n34 by the current likelihood. This kind of prior transmission is designed to reduce catastrophic forgetting   \n35 in continual learning. However, this is not feasible in CTTA because unlabeled data may introduce   \n36 unreliable prior. As shown in Fig. 1, an unreliable prior may lead to a poor posterior, which may then   \n37 propagate errors to the next inference, leading to the accumulation of errors.   \n38 Thus, we delve into the utilization of BI framework to evaluate model uncertainty in CTTA, aiming   \n39 to mitigate the impact of unreliable priors and reduce the error propagation. To approximate the   \n40 intractable likelihood in BI, we adopt to use online Variational Inference (VI) [49, 42], and accordingly   \n41 name our method Variational Continual Test-Time Adaptation (VCoTTA). At the source stage,   \n42 we first transform a pretrained deterministic model, say CNN, into a Bayesian Neural Network   \n43 (BNN) by a variational warm-up strategy, where the local reparameterization trick [27] is used to   \n44 inject uncertainties into the source model. During the testing phase, we employ a mean-teacher   \n45 update strategy, where the student model is updated via VI and the teacher model is updated by   \n46 the exponential moving average. Specifically, for the update of the student model, we propose to   \n47 use a mixture of priors from both the source and teacher models, then the Evidence Lower BOund   \n48 (ELBO) becomes the cross-entropy between the student and teachers plus the KL divergence of the   \n49 prior mixture. We demonstrate the effectiveness of the proposed method on three datasets, and the   \n50 results show that the proposed method can mitigate the error accumulation in CTTA and obtain clear   \n51 performance improvements. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "mdK1vhgpa5/tmp/d8cffe1cd63c3eca73560cc7f77e295a8a7b46675427098a583ceea62b2c24ba.jpg", "img_caption": ["Figure 1: In CTTA task, a BNN model is first trained on a source dataset, and then is used to adapt to updated with unreliable priors, which may result in error accumulations. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "52 Our contributions are three-fold: ", "page_idx": 1}, {"type": "text", "text": "53 (1) This paper develops VCoTTA, a simple yet general framework for continual test-time adaptation   \n54 that leverages online VI within BNN.   \n55 (2) We propose to transform an off-the-shelf model into a BNN via a variational warm-up strategy,   \n56 which injects uncertainties into the model.   \n57 (3) We build a mean-teacher structure for CTTA, and propose a strategy to blend the teacher\u2019s prior   \n58 with the source\u2019s prior to mitigate unreliable prior problem. ", "page_idx": 1}, {"type": "text", "text": "59 2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "60 2.1 Continual Test-Time Adaptation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "61 Test-Time Adaptation (TTA) enables the model to dynamically adjust to the characteristics of the   \n62 test data, i.e. target domain, in a source-free and online manner [25, 46, 50]. Previous works have   \n63 enhanced TTA performance through the designs of unsupervised loss [37, 58, 32, 9, 7, 17]. These   \n64 endeavours primarily focus on enhancing adaptation within a fixed target domain, representing a   \n65 single-domain TTA setup, where models adapt to a specific target domain and then reset to their   \n66 original pretrained state with the source domain, prepared for the next target domain adaptation.   \n67 Recently, CTTA [51] has been introduced to tackle TTA within a continuously changing target   \n68 domain, involving long-term adaptation. This configuration often grapples with the challenge of error   \n69 accumulation [47, 51]. Specifically, prolonged exposure to unsupervised loss from unlabeled test   \n70 data during long-term adaptation may result in significant error accumulation. Additionally, as the   \n71 model is intent on learning new knowledge, it is prone to forgetting source knowledge, which poses   \n72 challenges when accurately classifying test samples similar to the source distribution.   \n73 To solve the two challenges, the majority of the existing methods focus on improving the confidence of   \n74 the source model during the testing phase. These methods employ the mean-teacher architecture [47]   \n75 to mitigate error accumulation, where the student learns to align with the teacher and the teacher   \n76 updates via moving average with the student. As to the challenge of forgetting source knowledge,   \n77 some methods adopt augmentation-averaged predictions [51, 2, 11, 55] for the teacher model,   \n78 strengthening the teacher\u2019s confidence to reduce the influence from highly out-of-distribution samples.   \n79 Some methods, such as [11, 6], propose to adopt the contrastive loss to maintain the already learnt   \n80 semantic information. Some methods believe that the source model is more reliable, thus they are   \n81 designed to restore the source parameters [51, 2]. Though the above methods keep the model from   \n82 confusion of vague pseudo labels, they may suffer from overly confident predictions that are less   \n83 calibrated. To mitigate this issue, it is helpful to estimate the uncertainty in the neural network. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "84 2.2 Bayesian Neural Network ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "85 Bayesian framework is natural to incorporate past knowledge and sequentially update the belief with   \n86 new data [59]. The bulk of work on Bayesian deep learning has focused on scalable approximate   \n87 inference methods. These methods include stochastic VI [22, 34], dropout [16, 27] and Laplace   \n88 approximation [41, 15] etc., and leveraging the stochastic gradient descent (SGD) trajectory, either   \n89 for a deterministic approximation or sampling. In a BNN, we specify a prior $p(\\pmb\\theta)$ over the neural   \n90 network parameters, and compute the posterior distribution over parameters conditioned on training   \n91 data, $p(\\pmb\\theta|\\mathcal D)\\propto p(\\pmb\\theta)p(\\mathcal D|\\pmb\\theta)$ . This procedure should give considerable advantages for reasoning   \n92 about predictive uncertainty, which is especially relevant in the small-data setting.   \n93 Crucially, when performing Bayesian inference, we need to choose a prior distribution that accurately   \n94 reflects the prior beliefs about the model parameters before seeing any data [18, 14]. In conventional   \n95 static machine learning, the most common choice for the prior distribution over the BNN weights   \n96 is the simplest one: the isotropic Gaussian distribution. However, this choice has been proved   \n97 indeed suboptimal for BNNs [14]. Recently, some studies estimate uncertainty in continual learning   \n98 within a BNN framework, such as [38, 12, 13, 28]. They set the current prior to the previous   \n99 posterior to mitigate catastrophic forgetting. However, the prior transmission is not reliable in the   \n100 unsupervised CTTA task. Any prior mistakes will be enlarged by adaptation progress, manifesting   \n101 error accumulation. To solve the unreliable prior problem, this paper proposes a prior mixture method   \n102 based on VI. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "103 3 Variational Inference in CTTA ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "104 We start from the supervised BI in typical continual learning, where the model aims to learn multiple   \n105 classification tasks in sequence. Let $\\raisebox{\\depth}{\\(D\\)}=\\{(x_{n},y_{n})\\}_{n=1}^{N}$ be the training set, where $x_{n}$ and $y_{n}$   \n106 denotes the training sample and the corresponding class label. The task $t$ is to learn a direct posterior   \n107 approximation over the model parameter $\\pmb{\\theta}$ as follows. ", "page_idx": 2}, {"type": "equation", "text": "$$\np(\\pmb{\\theta}|\\mathcal{D}_{1:t})\\;\\propto\\;p_{t}(\\pmb{\\theta})p(\\mathcal{D}_{t}|\\pmb{\\theta}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "108 where $p(\\pmb{\\theta}|\\mathcal{D}_{1:t})$ denotes the posterior of sequential tasks on the learned parameter and $p(\\mathcal{D}_{t}|\\pmb{\\theta})$ is   \n109 the likelihood of the current task. The current prior $p_{t}(\\pmb\\theta)$ is regarded as the given knowledge. [38]   \n110 proposes that this current prior can be the posterior learned in the last task, i.e., $p_{t}(\\pmb\\theta)=p(\\pmb\\theta|\\bar{\\mathcal{D}}_{1:t-1})$ ,   \n111 where the inference becomes ", "page_idx": 2}, {"type": "equation", "text": "$$\np(\\pmb\\theta|\\mathcal{D}_{1:t})\\;\\propto\\;p(\\pmb\\theta|\\mathcal{D}_{1:t-1})p(\\mathcal{D}_{t}|\\pmb\\theta).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "112 The detailed process can be shown in Appendix A. ", "page_idx": 2}, {"type": "text", "text": "113 In contrast to continual learning, CTTA faces a sequence of learning tasks in test time without any   \n114 label information, requiring the model to adapt to each novel domain sequentially. In this case,   \n115 we assume that each domain is i.i.d. and the classes are separable following many unsupervised   \n116 studies [36, 48, 5], more details about the assumption can be seen in Appendix B.1. We use   \n117 $\\mathcal{U}=\\{x_{n}\\}_{n=1}^{N}$ to represent the unlabeled test dataset. The CTTA model is first trained on a source   \n118 dataset $\\mathcal{D}_{0}$ , and then adapted to unlabeled test domains starting from $\\mathcal{U}_{1}$ . For the $t$ -th adaptation, we   \n119 have ", "page_idx": 2}, {"type": "equation", "text": "$$\np(\\pmb{\\theta}|\\mathcal{U}_{1:t}\\cup\\mathcal{D}_{0})\\propto p_{t}(\\pmb{\\theta})p(\\mathcal{U}_{t}|\\pmb{\\theta}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "120 Similarly, we can set the last posterior to be the current prior, i.e., $p_{t}(\\pmb{\\theta})=p(\\pmb{\\theta}|\\mathcal{U}_{1:t-1}\\cup\\mathcal{D}_{0})$ and   \n121 $p_{1}(\\pmb\\theta)\\,\\stackrel{.}{=}\\,p(\\pmb\\theta|\\mathcal{D}_{0})$ . However, employing BI for adaptation on unlabeled testing data can result   \n122 in untrustworthy posterior estimates. Therefore, during subsequent adaptation, the untrustworthy   \n123 posterior automatically transform into unreliable priors, leading to error accumulation. In other words,   \n124 an unreliable prior $p_{t}(\\pmb\\theta)$ will make the current posterior even less trustworthy. Moreover, the joint   \n125 likelihood $p(\\mathcal{U}_{t}|\\pmb{\\theta})$ for $t>0$ is intractable on unlabeled data.   \n126 To make the BI feasible in CTTA task, in this paper, we transform the question to an easy-to-compute   \n127 form. Referring to [20], the unsupervised inference can be transformed into ", "page_idx": 2}, {"type": "image", "img_path": "mdK1vhgpa5/tmp/c90c3f86c5cc4585778eddb679b4f92dd189758ac4de05a514ea4126e17d8bc8.jpg", "img_caption": ["Figure 2: VCoTTA is built on mean-teacher structure, and conducts VI in CTTA using a mixture of teacher prior and source prior. The next teacher prior is updated by the exponential moving average. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\np(\\pmb\\theta|\\mathcal{U})\\propto p(\\pmb\\theta)\\exp\\left(-\\lambda H(\\mathcal{U}|\\pmb\\theta)\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "128 where $H$ denotes the conditional entropy and $\\lambda$ is a scalar hyperparameter to weigh the entropy term.   \n129 This simple form reveals that the prior belief about the conditional entropy of labels is given by the   \n130 inputs. The observation of the input $\\boldsymbol{\\mathcal{U}}$ provides information on the drift of the input distribution, which   \n131 can be used to update the belief over the learned parameters $\\pmb{\\theta}$ through Eq. (4). Consequently, this   \n132 allows the utilization of unlabeled data for BI. More detailed derivations can be seen in Appendix B.2.   \n133 In a BNN, the posterior distribution is often intractable and some approximation methods are required,   \n134 even when calculating the initial posterior. In this paper, we leverage online VI, as it typically   \n135 outperforms the other methods for complex models in the static setting [4]. VI defines a variational   \n136 distribution $q(\\pmb\\theta)$ to approxmiate the posterior $p(\\pmb\\theta|\\mathcal{U})$ . The approximation process is as follows. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nq_{t}(\\pmb\\theta)=\\arg\\operatorname*{min}_{\\pmb q\\in\\mathbb Q}\\mathrm{KL}\\left[q(\\pmb\\theta)\\mid\\mid\\frac{1}{Z_{t}}p_{t}(\\pmb\\theta)e^{-\\lambda H(\\mathcal{U}_{t}\\mid\\pmb\\theta)}\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "137 where $\\mathbb{Q}$ is the distribution searching space and $Z_{t}$ is the intractable normalizing hyperparameter.   \n138 Thus, referring to the derivations in Appendix $\\mathbf{C}$ , the ELBO is computed by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathtt{E L B O}=-\\lambda\\mathbb{E}_{\\theta\\sim q(\\theta)}H(\\mathcal{U}_{t}|\\theta)-\\mathrm{KL}\\left(q(\\theta)||p_{t}(\\theta)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "139 Optimizing with Eq. (6) makes model adapt to domain shift. While VI offers a good framework   \n140 for measuring uncertainty in CTTA, it is noteworthy that VI does not directly address the issue of   \n141 unreliable priors. The error accumulation remains a significant concern.   \n142 Despite this, the form of the ELBO in variational inference offers a pathway for mitigating the impact   \n143 of unreliable priors. In Eq. (6), the entropy term may result in overly confident predictions that are   \n144 less calibrated, while the $K L$ term may be directly affected by an unreliable prior. In the following   \n145 section, we will discuss how to solve the problems when computing the two terms. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "146 4 Adaptation and Inference in VCoTTA ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "147 4.1 Entropy term: VI by Mean-Teacher Architecture ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "148 In the above section, we introduce the VI in CTTA but challenges remain, i.e., the unreliable prior.   \n149 To mitigate the challenge in the entropy term, we adopt a Mean-Teacher (MT) structure [47] in the   \n150 Bayesian inference process. MT is initially proposed in semi-supervised and unsupervised learning,   \n151 where the teacher model guides the unlabeled data, helping the model generalize and improve   \n152 performance with the utilization of large-scale unlabeled data.   \n153 MT structure is composed of a student model and a teacher model, where the student model learns   \n154 from the teacher and the teacher updates using Exponential Moving Average (EMA) [24]. In VI, the   \n155 student is set to be the variational distribution $q(\\pmb\\theta)$ , which is a Gaussian mean-field approximation   \n156 for its simplicity. It is achieved by stacking the biases and weights of the network as follows. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nq(\\pmb{\\theta})=\\prod_{d}\\mathcal{N}\\left(\\pmb{\\theta}_{d};\\mu_{d},\\mathrm{diag}(\\sigma_{d}^{2})\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "157 where $d$ denotes each dimension of the parameter. The teacher model $\\bar{p}(\\pmb\\theta)$ (we use bar to distinguish   \n158 the general prior) is also a Gaussian distribution. Thus, the student model is updated by aligning it   \n159 with the teacher model through the use of a cross-entropy (CE) loss ", "page_idx": 4}, {"type": "equation", "text": "$$\nL_{\\mathrm{CE}}(q,\\bar{p})=-\\mathbb{E}_{{\\pmb\\theta}\\sim{\\pmb q}({\\pmb\\theta})}\\mathbb{E}_{\\boldsymbol{x}\\sim\\mathcal{U}}\\left[\\bar{p}(\\boldsymbol{x}|{\\pmb\\theta})\\log q(\\boldsymbol{x}|{\\pmb\\theta})\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "160 In our implementation, we also try to use Symmetric Cross-Entropy (SCE) [53] in CTTA, ", "page_idx": 4}, {"type": "equation", "text": "$$\nL_{\\mathrm{SCE}}(q,\\bar{p})=-\\mathbb{E}_{\\theta\\sim q(\\theta)}\\mathbb{E}_{x\\sim\\mathcal{U}}\\left[\\bar{p}(x|\\theta)\\log q(x|\\theta)+q(x|\\theta)\\log\\bar{p}(x|\\theta)\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "161 SCE balances the gradient for high and low confidence, benefiting the unsupervised learning. ", "page_idx": 4}, {"type": "text", "text": "162 4.2 KL term: Mixture-of-Gaussian Prior ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "163 For the KL term, to reduce the impact of unreliable prior, we propose a mixing-up approach to   \n164 combining the teacher and source prior adaptatively. The source prior is warmed up upon the   \n165 pretrained deterministic model $p_{1}(\\pmb{\\theta})\\;=\\;p(\\pmb{\\theta}|\\mathcal{D}_{0})$ (see Sec. 4.3.1). The teacher model $\\bar{\\bar{p}}_{t}(\\pmb\\theta)$ is   \n166 updated by EMA (see Sec. 4.3.3). We assume that the prior should be the mixture of the two Gaussian   \n167 priors. Using only the source prior, the adaptation is limited. While using only the teacher prior, the   \n168 prior is prone to be unreliable.   \n169 We use the mean entropy derived from a given serious data augmentation to represent the confidence   \n170 of the two prior models, and mix up the two priors with a modulating factor ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\alpha=\\frac{1}{|\\mathcal{Z}|}\\sum_{i\\in\\mathcal{Z}}\\frac{e^{H(x|\\pmb{\\theta}_{0})/\\tau}}{e^{H(x|\\pmb{\\theta}_{0})/\\tau}+e^{H(x|\\pmb{\\bar{\\theta}})/\\tau}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "171 where $\\mathcal{T}$ denotes augmentation types. $\\pmb\\theta_{0}$ and $\\bar{\\pmb\\theta}$ are the parameters of the source model and the teacher   \n172 model. $\\tau$ means the temperature factor. Thus, as shown in Fig. 3(b), the current prior $p_{t}(\\pmb\\theta)$ is set to   \n173 the mixture of priors as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{t}(\\pmb{\\theta})=\\alpha\\cdot p_{1}(\\pmb{\\theta})+(1-\\alpha)\\cdot\\bar{p}_{t}(\\pmb{\\theta}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "174 In the VI, we use the upper bound to update the KL term [31] (see Appendix D.1) for simplicity, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{KL}\\left(q\\vert\\vert p_{t}\\right)\\leq\\alpha\\cdot\\mathrm{KL}\\left(q\\vert\\vert p_{0}\\right)+\\left(1-\\alpha\\right)\\cdot\\mathrm{KL}\\left(q\\vert\\vert\\bar{p}_{t}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "175 Furthermore, we also improve the teacher-student alignment in the entropy term (see Eq. (9)) by   \n176 picking up the augmented logits with a larger confidence than the raw data. That is, we replace the   \n177 teacher log-likelihood $\\log{\\bar{p}}(x|\\pmb\\theta)$ by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\log\\bar{p}^{\\prime}(x|\\pmb{\\theta})=\\frac{\\sum_{i\\in\\mathbb{Z}}\\mathbf{1}\\left(f(\\bar{p}(x_{i}^{\\prime}))>f(\\bar{p}(x))+\\epsilon\\right)\\cdot\\log\\bar{p}(x_{i}^{\\prime})}{\\sum_{i\\in\\mathbb{Z}}\\mathbf{1}\\left(f(\\bar{p}(x_{i}^{\\prime}))>f(\\bar{p}(x))+\\epsilon\\right)},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "178 where, for brevity, we let $\\bar{p}(x_{i}^{\\prime})\\,=\\,\\bar{p}(x_{i}^{\\prime}|\\pmb{\\theta})$ and $\\bar{p}(x)\\,=\\,\\bar{p}(x)|\\pmb\\theta)$ in short. $f(\\cdot)$ is the confidence   \n179 function. $\\epsilon$ denotes the confidence margin and $\\mathbf{1}(\\cdot)$ is an indicator function. Eq. (13) can be regarded   \n180 as a filter, meaning that for each sample, the reliable teacher is represented by the average of its   \n181 augmentations with $\\epsilon$ more confidence. In Appendix D.2, we prove that the proposed mixture-of  \n182 Gaussian is benifical to CTTA. In Appendix E.1, we discuss the influence of different $\\epsilon$ . ", "page_idx": 4}, {"type": "text", "text": "183 4.3 Adaptation and Inference ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "184 4.3.1 Variational Warm-up ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "185 To obtain a source BNN, instead of training a model from scratch on the source data $\\mathcal{D}_{0}$ , we transform   \n186 a pretrained deterministic CNN to a BNN by variational warm-up strategy. Specifically, we leverage   \n187 the local reparameterization trick [27] to add stochastic parameters, and warm up the model: ", "page_idx": 4}, {"type": "equation", "text": "$$\nq_{0}(\\pmb{\\theta})=\\arg\\operatorname*{min}_{\\pmb{q}\\in\\mathbb{Q}}\\mathrm{KL}\\left[\\pmb{q}(\\pmb{\\theta})\\ \\lVert\\ \\frac{1}{Z_{0}}p(\\pmb{\\theta})p(\\mathcal{D}_{0}\\lvert\\pmb{\\theta})\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "188 where $p(\\pmb\\theta)$ represents the prior distribution, say the pretrained deterministic model. Eq. (14) denotes   \n189 a standard VI on the source data, and we optimize the ELBO to obtain the variational distribution [49].   \n190 By the variational warm-up, we can easily transform an off-the-shelf pretrained model into a BNN   \n191 with a stochastic dynamic. The variational warm-up strategy is outlined in Algorithm 1.   \n192 The warm-up strategy is a common   \n193 approach in TTA and CTTA tasks to   \n194 further build knowledge structure for   \n195 the source model, such as [26, 45, 11,   \n196 8]. Some other methods may not use   \n197 warm-up but still use the source data,   \n198 such as [39]. The warm-up strategy   \n199 uses the source data only before deploying the model to CTTA scenario, and it is regarded as a part   \n200 of pretraining. All of these methods using source data are operationalized in source-free at test time   \n201 and find it is beneficial to CTTA. We use the warm-up to inject the uncertainties into a given source   \n202 model, i.e., turning an off-the-shelf pretrained CNN model into a pretrained BNN model. This is   \n203 convenient to obtain a pretrained BNN, because the warm-up strategy uses only a few epochs. We   \n204 offer more discussions and experiments on the proposed variational warm-up strategy in Appendix F. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "mdK1vhgpa5/tmp/e34479815b213ab8e0b5ab1fd6ae1baa68ac9b62acffc40326dd027c5acdec38.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "205 4.3.2 Student update via VI ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "206 The student model $q_{t}(\\pmb\\theta)$ is adapted by approximating using Eq. (5), and is optimized on: ", "page_idx": 5}, {"type": "equation", "text": "$$\nL(q_{t})=L_{\\mathrm{SCE}}(q_{t},\\bar{p}_{t}^{\\prime})+\\alpha\\cdot\\mathrm{KL}\\left(q_{t}||q_{0}\\right)+(1-\\alpha)\\cdot\\mathrm{KL}\\left(q_{t}||\\bar{q}_{t}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "207 where $\\bar{p}_{t}^{\\prime}$ is the current augmented teacher model in Eq. (13), and $p_{1}(\\pmb\\theta)\\approx q_{0}(\\pmb\\theta)$ , $\\bar{p}_{t}(\\pmb\\theta)\\approx\\bar{q}_{t}(\\pmb\\theta)$ .   \n208 The KL term between two Gaussians can be computed in a closed form. ", "page_idx": 5}, {"type": "text", "text": "209 4.3.3 Teacher update via EMA ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "210 The teacher model is updated using EMA. Let $(\\mu,\\sigma)$ and $(\\bar{\\pmb{\\mu}},\\bar{\\pmb{\\sigma}})$ be the mean and standard deviation   \n211 of the student and teacher model, respectively. At test time, the teacher model $\\bar{q}_{t}(\\pmb\\theta)$ is updated by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\bar{\\pmb{\\mu}}\\leftarrow\\beta\\bar{\\pmb{\\mu}}+(1-\\beta)\\pmb{\\mu},\\quad\\bar{\\pmb{\\sigma}}\\leftarrow\\beta\\bar{\\pmb{\\sigma}}+(1-\\beta)\\pmb{\\sigma}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "212 Although the std is not used in the cross entropy to compute the likelihood, the teacher prior   \n213 distribution is important to adjust the student distribution via the KL term. ", "page_idx": 5}, {"type": "text", "text": "214 4.3.4 Model inference ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "215 At any time, CTTA model needs to predict and adapt to the unlabeled test data. In our VCoTTA, we   \n216 also use the mixed prior to serve as the inference model. That is, for a test data point $x$ , the model   \n217 inference is represented by ", "page_idx": 5}, {"type": "equation", "text": "$$\np_{t}(x)=\\int p(x|\\pmb\\theta)p_{t}(\\pmb\\theta)d\\pmb\\theta=\\int\\alpha p(x|\\pmb\\theta)p_{1}(\\pmb\\theta)+(1-\\alpha)p(x|\\pmb\\theta)\\bar{p}_{t}(\\pmb\\theta)d\\pmb\\theta,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "218 For the data prediction, the model only uses the expectation to reduce the stochastic, but leverages   \n219 stochastic dynamics in domain adaptation. ", "page_idx": 5}, {"type": "text", "text": "220 4.3.5 The algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "221 We illustrate the whole algorithm in Al  \n222 gorithm 2. We first transform an off-the  \n223 shelf pretrained model into BNN via the   \n224 variational warm-up strategy (Sec. 4.3.1).   \n225 After that, we obtain a BNN, and for each   \n226 domain shift, we forward and adapt each   \n227 test data point in an MT architecture. For   \n228 a data point $x$ , we first predict the class la  \n229 bel using the mixture of the source model   \n230 and the teacher model (Sec. 4.3.4). Then,   \n231 we update the student model using VI,   \n232 where we use cross entropy to compute   \n233 the entropy term and use the mixture of   \n234 priors for the KL term (Sec. 4.3.2). Finally, we update the BNN teacher model via EMA (Sec. 4.3.3).   \n235 See more details in Appendix G. The process is feasible for any test data without labels. ", "page_idx": 5}, {"type": "table", "img_path": "mdK1vhgpa5/tmp/6a42e3ad7131ff267065744a196ca117a9b04ad19475325015baffb5a7796b11.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "mdK1vhgpa5/tmp/eb00bcbb9c46639a2d039eddaa384700c428c79ae032ad006f7db291f645efb0.jpg", "table_caption": ["Table 1: Classification error rate $(\\%)$ for the standard CIFAR10-to-CIFAR10C CTTA task. All results are evaluated with the largest corruption severity level 5 in an online fashion. C1 to C15 are 15 corruptions for the datasets (see Sec. 5.1). CIFAR100C and ImagenetC use the same setup. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "mdK1vhgpa5/tmp/3b2b56f635978e6b4449b2940f8e95f7f56925efe2a7324b6f6229aa86648f04.jpg", "table_caption": ["Table 2: Classification error rate $(\\%)$ for the standard CIFAR100-to-CIFAR100C CTTA task. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "236 5 Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "237 5.1 Experimental Setting ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "238 Dataset. In our experiments, we employ the CIFAR10C, CIFAR100C, and ImageNetC datasets as   \n239 benchmarks to assess the robustness of classification models. Each dataset comprises 15 distinct   \n240 types of corruption, each applied at five different levels of severity (from 1 to 5). These corruptions   \n241 are systematically applied to test images from the original CIFAR10 and CIFAR100 datasets, as well   \n242 as validation images from the original ImageNet dataset. For simplicity in tables, we use C1 to C15   \n243 to represent the 15 types of corruption, i.e., C1: Gaussian, C2: Shot, C3: Impulse C4: Defocus, C5:   \n244 Glass, C6: Motion, C7: Zoom, C8: Snow, C9: Frost, C10: Fog, C11: Brightness, C12: Contrast, C13:   \n245 Elastic, C14: Pixelate, C15: Jpeg.   \n246 Pretrained Model. Following previous studies [50, 51], we adopt pretrained WideResNet-28 [57]   \n247 model for CIFAR10to-CIFAR10C, pretrained ResNeXt-29 [54] for CIFAR100-to-CIFAR100C, and   \n248 standard pretrained ResNet-50 [21] for ImageNet-to-ImagenetC. Note in our VCoTTA [51], we   \n249 further warm up the pretrained model to obtain the stochastic dynamics for each dataset. Similar to   \n250 CoTTA, we update all the trainable parameters in all experiments. The augmentation number is set to   \n251 32 for all compared methods that use the augmentation strategy. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "252 5.2 Methods to be Compared ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "253 We compare our VCoTTA with multiple state-of-the-art (SOTA) methods. SOURCE denotes the   \n254 baseline pretrained model without any adaptation. BN [30, 43] keeps the network parameters frozen,   \n255 but only updates Batch Normalization. TENT [50] updates via Shannon entropy for unlabeled   \n256 test data. CoTTA [51] builds the MT structure and uses randomly restoring parameters to the   \n257 source model. SATA [6] modifies the batch-norm affine parameters using source anchoring-based   \n258 self-distillation to ensure the model incorporates knowledge of newly encountered domains while   \n259 avoiding catastrophic forgetting. SWA [55] refines the pseudo-label learning process from the   \n260 perspective of the instantaneous and long-term impact of noisy pseudo-labels. PETAL [2] tries to   \n261 estimate the uncertainty in CTTA, which is similar to BNN, but it ignores the unreliable prior problem.   \n262 All compared methods adopt the same backbone, pretrained model and hyperparameters. ", "page_idx": 6}, {"type": "table", "img_path": "mdK1vhgpa5/tmp/22dcac8cfd30be88ee2c54e12a371165310027b9c010b50762d99e8019dc0cad.jpg", "table_caption": ["Table 3: Classification error rate $(\\%)$ for the standard ImageNet-to-ImageNetC CTTA task. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "263 5.3 Comparison Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "264 We show the major comparisons with the SOTA methods in Tables 1, 2 and 3. We have the following   \n265 observations. First, no adaptation at the test time (SOURCE) suffers from serious domain shift, which   \n266 shows the necessity of the CTTA. Second, traditional TTA methods that ignore the continual shift   \n267 in test time perform poorly such as TENT and BN. We also find that simple Shannon entropy is   \n268 effective in the first several domain shifts, especially in complex 1,000-classes ImageNetC, but shows   \n269 significant performance drops in the following shifts. Third, the mean-teacher structure is very useful   \n270 in CTTA, such as COTTA and PETAL, which means that the pseudo-label is useful in domain shift.   \n271 In the previous method, the error accumulation leads to the unreliable pseudo labels, then the model   \n272 may get more negative transfers in CTTA along the timeline. The proposed VCOTTA outperforms   \n273 other methods on all the three datasets, such as $13.1\\%$ vs. $15.3\\%$ (SWA) on CIFAR10C, $28.4\\%$   \n274 vs. $30.0\\%$ (SWA) on CIFAR100C and $64.2\\%$ vs. $66.7\\%$ (COTTA) on ImageNetC. We hold the   \n275 opinion that the prior will inevitably drift in CTTA, but VCOTTA slows down the process via the   \n276 prior mixture. We also find that the superiority is more obvious in the early adaptation, which may be   \n277 influenced by the different corruption orders. We analyze the order problem in Appendix H. ", "page_idx": 7}, {"type": "text", "text": "278 5.4 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "279 We evaluate the two components in Table 4, i.e., the Variational Warm-Up (VWU) and the Symmetric   \n280 Cross-Entropy (SCE) via ablation. The ablation results show that the two components are both   \n281 important for VCOTTA. First, the VWU is used to inject stochastic dynamics into an off-the-shelf   \n282 pretrained model. Without the VWU, the performance of VCOTTA drops to $18.4\\%$ from $13.9\\%$ on   \n283 CIFAR10C, $31.5\\%$ from $28.8\\%$ on CIFAR100C and $68.1\\%$ from $64.2\\%$ on ImageNetC. Also, the   \n284 SCE can further improve the performance on CIFAR10C and CIFAR100C, because SCE balances   \n285 the gradient for high and low confidence predictions. We also find that SCE is ineffective for complex   \n286 ImageNetC, and the reason may be the class sensitivity imbalance, causing the model to lean more   \n287 towards one direction during optimization. ", "page_idx": 7}, {"type": "table", "img_path": "", "table_caption": ["Table 4: Ablation study on under severity 5. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "mdK1vhgpa5/tmp/1a4a266a031f3907d80457c306d6491fc807b079e099ea54f88f5047681b64f7.jpg", "table_caption": ["Table 5: Different weights for mixture of priors. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "288 5.5 Mixture of Priors ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "289 In Sec. 4.2, we introduce a Gaussian mixture strategy, where the current prior is approximated as the   \n290 weighted sum of the source prior and the teacher prior. The weights are determined by computing the   \n291 entropy over multiple augmentations of two models. To assess the effectiveness of these weights, we   \n292 compare them with three naive weighting configurations: using only the source model, using only the   \n293 teacher model, and a simple average with equal weights for both models. The results, as presented in   \n294 Table 5, reveal that relying solely on the source model or the teacher model (i.e., weighting with $(1,0)$   \n295 and $(0,1)$ ) results in suboptimal performance. Additionally, naive weighting with equal contributions   \n296 from both models (i.e., (0.5, 0.5)) proves ineffective for CTTA due to the inherent uncertainty in both   \n297 models. In contrast, the proposed adaptive weights for the Gaussian mixture in CTTA demonstrate its   \n298 effectiveness. This underscores the significance of striking a balance between the two prior models in   \n299 an unsupervised environment. The trade-off implies the need to discern when the source model\u2019s   \n300 knowledge is more applicable and when the teacher model\u2019s shifting knowledge takes precedence. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "301 5.6 Uncertainty Estimation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "302 To evaluate the uncertainty estimation, we use negative loglikelihood (NLL) and Brier Score (BS) [3].   \n303 Both NLL and BS are proper scoring rules [19], and they are minimized if and only if the predicted   \n304 distribution becomes identical to the actual distribution: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{NLL}=-\\mathbb{E}_{(x,y)\\in\\mathcal{D}^{\\mathrm{test}}}\\log(p(y|x,\\theta)),\\quad\\mathrm{BS}=\\mathbb{E}_{(x,y)\\in\\mathcal{D}^{\\mathrm{test}}}\\left(p(y|x,\\theta)-\\mathrm{Onehot}(y)\\right)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "305 where $\\mathcal{D}^{\\mathrm{test}}$ denotes the test set, i.e., the unsupervised test dataset $\\boldsymbol{\\mathcal{U}}$ with labels. We evaluate NLL and   \n306 BS with a severity level of 5 for all corruption types, and the compared results with SOTAs are shown   \n307 in Table 6. We have the following observations. First, most methods suffer from low confidence in   \n308 terms of NLL and BS because of the drift priors, where the model is unreliable gradually, and the error   \n309 accumulation makes the model perform poorly. Our approach outperforms most other approaches in   \n310 terms of NLL and BS, demonstrating the superiority in improving uncertainty estimation. We also   \n311 find that PETAL [2] shows good NLL and BS, because PETAL forces the prediction over-confident   \n312 to unreliable priors, thus PETAL shows unsatisfactory results on adaptation accuracy, such as $31.5\\%$   \n313 vs. $28.4\\%$ (Ours) on CIFAR100C. ", "page_idx": 8}, {"type": "table", "img_path": "mdK1vhgpa5/tmp/cadc5776f214117e8b37f52893254eefe6195fc5f5e180ba732b3e940b525646.jpg", "table_caption": ["Table 6: Uncertainty estimation via NLL and BS. Table 7: Gradually changing on severity 5. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "314 5.7 Gradually Corruption ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "315 We also show gradual corruption results instead of constant severity in the major comparison, and the   \n316 results are reported in Table 7. Specifically, each corruption adopts the gradual changing sequence:   \n317 $1\\rightarrow2\\rightarrow3\\rightarrow4\\rightarrow5\\rightarrow4\\rightarrow3\\rightarrow2\\rightarrow1$ , where the severity level is the lowest 1 when corruption   \n318 type changes, therefore, the type change is gradual. The distribution shift within each type is also   \n319 gradual. Under this situation, our VCoTTA also outperforms other methods, such as $8.9\\%$ vs. $10.5\\%$   \n320 (PETAL) on CIFAR10C, and $24.4\\%$ vs. $26.3\\%$ (COTTA) on CIFAR100C. The results show that the   \n321 proposed VCOTTA based on BNN is also effective when the distribution change is uncertain. ", "page_idx": 8}, {"type": "text", "text": "322 6 Conclusion and Limitation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "323 Conclusion: In this paper, we proposed a variational Bayesian inference approach, termed VCoTTA,   \n324 to estimate uncertainties in CTTA. At the pretrained stage, we first transformed an off-the-shelf   \n325 pretrained deterministic CNN into a BNN using a variational warm-up strategy, thereby injecting   \n326 uncertainty into the source model. At the test time, we implemented a mean-teacher update strategy,   \n327 where the student model is updated via variational inference, while the teacher model is refined by the   \n328 exponential moving average. Specifically, to update the student model, we proposed a novel approach   \n329 that utilizes a mixture of priors from both the source and teacher models. Consequently, the ELBO   \n330 can be formulated as the cross-entropy between the student and teacher models, combined with the   \n331 KL divergence of the prior mixture. We demonstrated the effectiveness of the proposed method on   \n332 three datasets, and the results show that the proposed method can mitigate the issue of unreliable   \n333 prior within the CTTA framework.   \n334 Limitation: The efficacy of the proposed method relies on injecting uncertainty into the model during   \n335 the pre-training phase, which may be unavailable in scenarios where pretraining is already completed,   \n336 and original data is inaccessible. Additionally, constructing and training BNN models are inherently   \n337 more complex compared to CNNs, highlighting the importance of enhancing computational efficiency.   \n338 The Gaussian mixture method relies on multiple data augmentations, which also incurs computational   \n339 costs. Future endeavors could explore more efficient approaches for Gaussian mixture. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "340 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "341 [1] George EP Box and George C Tiao. Bayesian inference in statistical analysis. John Wiley & Sons, 2011.   \n342 [2] Dhanajit Brahma and Piyush Rai. A probabilistic framework for lifelong test-time adaptation. In Proceed  \n343 ings of the Computer Vision and Pattern Recognition, 2023.   \n344 [3] Glenn W Brier. Verification of forecasts expressed in terms of probability. Journal of the Monthly Weather   \n345 Review, 78(1):1\u20133, 1950.   \n346 [4] Thang Bui, Daniel Hern\u00e1ndez-Lobato, Jose Hernandez-Lobato, Yingzhen Li, and Richard Turner. Deep   \n347 gaussian processes for regression using approximate expectation propagation. In Proceedings of the   \n348 International Conference on Machine Learning, 2016.   \n349 [5] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised   \n350 learning of visual features. In Proceedings of the European Conference on Computer Vision, pages   \n351 132\u2013149, 2018.   \n352 [6] Goirik Chakrabarty, Manogna Sreenivas, and Soma Biswas. Sata: Source anchoring and target alignment   \n353 network for continual test time adaptation. arXiv preprint arXiv:2304.10113, 2023.   \n354 [7] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In   \n355 Proceedings of the Computer Vision and Pattern Recognition, 2022.   \n356 [8] Ziyang Chen, Yiwen Ye, Mengkang Lu, Yongsheng Pan, and Yong Xia. Each test image deserves a   \n357 specific prompt: Continual test-time adaptation for 2d medical image segmentation. arXiv preprint   \n358 arXiv:2311.18363, 2023.   \n359 [9] Sungha Choi, Seunghan Yang, Seokeon Choi, and Sungrack Yun. Improving test-time adaptation via shift  \n360 agnostic weight regularization and nearest source prototypes. In Procedings of the European Conference   \n361 on Computer Vision, 2022.   \n362 [10] Thomas M Cover. Elements of information theory. John Wiley & Sons, 1999.   \n363 [11] Mario D\u00f6bler, Robert A Marsden, and Bin Yang. Robust mean teacher for continual and gradual test-time   \n364 adaptation. In Proceedings of the Computer Vision and Pattern Recognition, 2023.   \n365 [12] Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, and Marcus Rohrbach. Uncertainty-guided continual   \n366 learning with bayesian neural networks. In Procedings of the International Conference on Learning   \n367 Representations, 2019.   \n368 [13] Sebastian Farquhar and Yarin Gal. A unifying bayesian view of continual learning. arXiv preprint   \n369 arXiv:1902.06494, 2019.   \n370 [14] Vincent Fortuin, Adri\u00e0 Garriga-Alonso, Sebastian W Ober, Florian Wenzel, Gunnar Ratsch, Richard E   \n371 Turner, Mark van der Wilk, and Laurence Aitchison. Bayesian neural network priors revisited. In   \n372 Procedings of the International Conference on Learning Representations, 2021.   \n373 [15] Karl Friston, J\u00e9r\u00e9mie Mattout, Nelson Trujillo-Barreto, John Ashburner, and Will Penny. Variational free   \n374 energy and the laplace approximation. Journal of the Neuroimage, 34(1):220\u2013234, 2007.   \n375 [16] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty   \n376 in deep learning. In Procedings of the International Conference on Machine Learning, 2016.   \n377 [17] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei Efros. Test-time training with masked autoencoders.   \n378 In Procedings of the Advances in Neural Information Processing Systems, 2022.   \n379 [18] Andrew Gelman, John B Carlin, Hal S Stern, and Donald B Rubin. Bayesian data analysis. Chapman and   \n380 Hall/CRC, 1995.   \n381 [19] Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation. Journal   \n382 of the American Statistical Association, 102(477):359\u2013378, 2007.   \n383 [20] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In Proceedings   \n384 of the Advances in Neural Information Processing Systems, 2004.   \n385 [21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.   \n386 In Proceedings of the Computer Vision and Pattern Recognition, 2016.   \n387 [22] Jos\u00e9 Miguel Hern\u00e1ndez-Lobato and Ryan Adams. Probabilistic backpropagation for scalable learning of   \n388 bayesian neural networks. In Procedings of the International Conference on Machine Learning, 2015.   \n389 [23] Hengguan Huang, Xiangming Gu, Hao Wang, Chang Xiao, Hongfu Liu, and Ye Wang. Extrapolative   \n390 continuous-time bayesian neural network for fast training-free test-time adaptation. In Proceedings of the   \n391 Advances in Neural Information Processing Systems, 2022.   \n392 [24] J Stuart Hunter. The exponentially weighted moving average. Journal of the Quality Technology, 18(4):203\u2013   \n393 210, 1986.   \n394 [25] Vidit Jain and Erik Learned-Miller. Online domain adaptation of a pre-trained cascade of classifiers. In   \n395 Proceedings of the Computer Vision and Pattern Recognition, 2011.   \n396 [26] Sanghun Jung, Jungsoo Lee, Nanhee Kim, Amirreza Shaban, Byron Boots, and Jaegul Choo. Cafa:   \n397 Class-aware feature alignment for test-time adaptation. In Proceedings of the IEEE/CVF International   \n398 Conference on Computer Vision, pages 19060\u201319071, 2023.   \n399 [27] Durk P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameterization   \n400 trick. In Proceedings of the Advances in Neural Information Processing Systems, 2015.   \n401 [28] Richard Kurle, Botond Cseke, Alexej Klushyn, Patrick Van Der Smagt, and Stephan G\u00fcnnemann. Continual   \n402 learning with bayesian neural networks for non-stationary data. In Procedings of the International   \n403 Conference on Learning Representations, 2019.   \n404 [29] Dong-Hyun Lee. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural   \n405 networks. In Workshop on Challenges in Representation Learning, International Conference on Machine   \n406 Learning, volume 3, page 896, 2013.   \n407 [30] Zhizhong Li and Derek Hoiem. Learning without forgetting. Journal of the IEEE Transactions on Pattern   \n408 Analysis and Machine Intelligence, 40(12):2935\u20132947, 2017.   \n409 [31] GuoJun Liu, Yang Liu, MaoZu Guo, Peng Li, and MingYu Li. Variational inference with gaussian mixture   \n410 model and householder flow. Journal of the Neural Networks, 109:43\u201355, 2019.   \n411 [32] Yuejiang Liu, Parth Kothari, Bastien Van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre   \n412 Alahi. Ttt++: When does self-supervised test-time training fail or thrive? In Procedings of the Advances in   \n413 Neural Information Processing Systems, 2021.   \n414 [33] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Unsupervised domain adaptation with   \n415 residual transfer networks. In Proceedings of the Advances in Neural Information Processing Systems,   \n416 2016.   \n417 [34] Christos Louizos and Max Welling. Multiplicative normalizing flows for variational bayesian neural   \n418 networks. In Procedings of the International Conference on Machine Learning, 2017.   \n419 [35] Wesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon Wilson. A simple   \n420 baseline for bayesian uncertainty in deep learning. In Proceedings of the Advances in Neural Information   \n421 Processing Systems, 2019.   \n422 [36] David J Miller and Hasan Uyar. A mixture of experts classifier with learning based on both labelled and   \n423 unlabelled data. In Proceedings of the Advances in Neural Information Processing Systems, 1996.   \n424 [37] Chaithanya Kumar Mummadi, Robin Hutmacher, Kilian Rambach, Evgeny Levinkov, Thomas Brox, and   \n425 Jan Hendrik Metzen. Test-time adaptation to distribution shift by confidence maximization and input   \n426 transformation. arXiv preprint arXiv:2106.14999, 2021.   \n427 [38] Cuong V Nguyen, Yingzhen Li, Thang D Bui, and Richard E Turner. Variational continual learning. In   \n428 Proceedings of the International Conference on Learning Representations, 2018.   \n429 [39] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan.   \n430 Efficient test-time model adaptation without forgetting. In Proceedings of the International Conference on   \n431 Machine Learning, pages 16888\u201316905, 2022.   \n432 [40] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua Dillon,   \n433 Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model\u2019s uncertainty? evaluating predictive   \n434 uncertainty under dataset shift. In Proceedings of the Advances in Neural Information Processing Systems,   \n435 2019.   \n436 [41] Hippolyt Ritter, Aleksandar Botev, and David Barber. A scalable laplace approximation for neural networks.   \n437 In Procedings of the International Conference on Learning Representations, 2018.   \n438 [42] Masa-Aki Sato. Online model selection based on the variational bayes. Journal of the Neural Computation,   \n439 13:1649\u20131681, 2001.   \n440 [43] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge.   \n441 Improving robustness against common corruptions by covariate shift adaptation. In Proceedings of the   \n442 Advances in Neural Information Processing Systems, 2020.   \n443 [44] Yoram Singer and Manfred KK Warmuth. Batch and on-line parameter estimation of gaussian mixtures   \n444 based on the joint entropy. In Procedings of the Advances in Neural Information Processing Systems, 1998.   \n445 [45] Junha Song, Jungsoo Lee, In So Kweon, and Sungha Choi. Ecotta: Memory-efficient continual test-time   \n446 adaptation via self-distilled regularization. In Proceedings of the IEEE/CVF Conference on Computer   \n447 Vision and Pattern Recognition, pages 11920\u201311929, 2023.   \n448 [46] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with   \n449 self-supervision for generalization under distribution shifts. In Procedings of the International Conference   \n450 on Machine Learning, 2020.   \n451 [47] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency tar  \n452 gets improve semi-supervised deep learning results. In Proceedings of the Advances in Neural Information   \n453 Processing Systems, 2017.   \n454 [48] Jesper E Van Engelen and Holger H Hoos. A survey on semi-supervised learning. Machine Learning,   \n455 109(2):373\u2013440, 2020.   \n456 [49] Chong Wang, John Paisley, and David M Blei. Online variational inference for the hierarchical dirichlet   \n457 process. In Proceedings of the International Conference on Artificial Intelligence and Statistics, 2011.   \n458 [50] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test  \n459 time adaptation by entropy minimization. In Proceedings of the International Conference on Learning   \n460 Representations, 2020.   \n461 [51] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In   \n462 Proceedings of the Computer Vision and Pattern Recognition, 2022.   \n463 [52] Yanshuo Wang, Jie Hong, Ali Cheraghian, Shafin Rahman, David Ahmedt-Aristizabal, Lars Petersson, and   \n464 Mehrtash Harandi. Continual test-time domain adaptation via dynamic sample selection. In Proceedings   \n465 of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1701\u20131710, 2024.   \n466 [53] Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric cross entropy   \n467 for robust learning with noisy labels. In Proceedings of the Computer Vision and Pattern Recognition,   \n468 2019.   \n469 [54] Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual trans  \n470 formations for deep neural networks. In Proceedings of the Computer Vision and Pattern Recognition,   \n471 2017.   \n472 [55] Xu Yang, Yanan Gu, Kun Wei, and Cheng Deng. Exploring safety supervision for continual test-time   \n473 domain adaptation. In Proceedings of the International Joint Conference on Artificial Intelligence, 2023.   \n474 [56] Longhui Yuan, Binhui Xie, and Shuang Li. Robust test-time adaptation in dynamic scenarios. In   \n475 Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15922\u2013   \n476 15932, 2023.   \n477 [57] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Procedings of the British Machine   \n478 Vision Conference, 2016.   \n479 [58] Marvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and   \n480 augmentation. In Procedings of the Advances in Neural Information Processing Systems, 2022.   \n481 [59] Tingting Zhao, Zifeng Wang, Aria Masoomi, and Jennifer Dy. Deep bayesian unsupervised lifelong   \n482 learning. Journal of the Neural Networks, 149:95\u2013106, 2022.   \n483 [60] Aurick Zhou and Sergey Levine. Bayesian adaptation for covariate shift. In Proceedings of the Advances   \n484 in Neural Information Processing Systems, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Variational Continual Test-Time Adaptation (Appendix) ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "485 A Bayesian Inference (BI) in Traditional CL and CTTA ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "486 As described in Sec. 3, we first illustrate the BI has been studied in traditional Continual Learning   \n487 (CL) methods. In this section, we compare the BI in CL and CTTA in detail and show the differences   \n488 with some related works. The comparison can be seen in Fig. 3. For the CL, BI is conducted by the   \n489 posterior propagation, that is, the prior of next task is equal to the current posterior. This is feasible in   \n490 supervised CL, where the data label is provided. For the CTTA, the posterior is not trustworthy using   \n491 only pseudo labels to adapt to a new domain. Thus, propagate the untrustworthy posterior to the next   \n492 stage would make unreliable prior, which will result in error accumulation. In the proposed VCoTTA,   \n493 we propose to solve the problem via enhancing the two terms in VI (see Sec. 4).   \n494 VCL [38] is a classic CL study that uses VI, our work is also inspired by VCL but has the following   \n495 difference. (1) The tasks are different: VCL studies supervised CL task, while our VCoTTA studies   \n496 unsupervised CTTA task. (2) The challenges are differnt: CL only suffers from catastrophic forgetting   \n497 (CF), while CTTA sufffers from both CF and error accumulation. (3) Ways of BI are different: To   \n498 conduct BI, one needs to compute prior and likelihood. For the prior, the current prior of VCL is set   \n499 to be the previous posterior, while in CTTA such a prior may be unreliable. For the likelihood, VCL   \n500 can directly compute likelihood, CTTA is under unsupervised setting, thus in our work, we deduce   \n501 the BI in CTTA using conditional entropy. (4) The update strategies are different: To reduce error   \n502 accumulation in unsupervised scenario, we employ a mean-teacher update strategy using VI for the   \n503 student model and exponential moving average for the teacher model, and compute a prior mixture   \n504 to guide the student update. Moreover, VCL maintains an extra coreset from the training set, while   \n505 VCoTTA never store any data during the test time.   \n506 We also find another recent work named PETAL [2] that estimates uncertainties in CTTA. The   \n507 BI formulation is similar between PETAL and ours, which is derived from [20], but PETAL use   \n508 different method to conduct the inference: (1) PETAL only uses CNN and does not estimate the model   \n509 uncertainties, while VCoTTA uses BNN to model the uncertainties during test time. (2) PETAL   \n510 ignores the unreliable prior in CTTA, and follow the VCL setting that use the previous posterior   \n511 as the current prior. (3) We conduct BI using variational inference while PETAL use SWAG [35].   \n512 SWAG has advantages in terms of computational efficiency and stability during training, especially in   \n513 scenarios where computational resources are limited. However, SWAG might not handle unreliable   \n514 priors as effectively as VI since it doesn\u2019t explicitly model the posterior distribution. (4) We have   \n515 compared with PETAL in our experiment (see Tables 1, 2, 3), and our method outperforms PETAL   \n516 on all datasets. ", "page_idx": 12}, {"type": "image", "img_path": "mdK1vhgpa5/tmp/e216d22bd211a1af2642ef6c75f6317afe573da1b8771f27be9938f36f46cd14.jpg", "img_caption": ["Figure 3: Bayesian inference comparison between continual learning and CTTA. We find the traditional prior transmission is infeasible in CTTA because of the unreliable prior from unlabeled data. In our method, we place CTTA in a mean-teacher structure, and design BI in CTTA using a mixture of teacher prior and source prior. The next teacher prior is updated by the exponential moving average. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "517 B CTTA Approximation by BI ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "518 B.1 Assumption on Class Separability ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "519 In our method, we use the conditional entropy to alternate the intractable computing of likelihood.   \n520 Note that the use of entropy in unsupervised scenario needs to satisfy the class-separable assumption.   \n521 In fact, unlabeled data do not convey category information but still carry information. Miller and   \n522 Uyar [36] theoretically proved that utilizing unlabeled samples to train classifiers can improve   \n523 classification performance if there is a connection between the target and sample distributions.   \n524 It is a common practice in unsupervised/semi-supervised learning to establish the relationship   \n525 between unlabeled data and the target by making some reasonable assumptions to obtain category  \n526 relevant information from unlabeled data. Common assumptions include the Smoothness assumption,   \n527 Cluster assumption, Manifold assumption, Low-density separation assumption, etc. For example,   \n528 the well-known clustering-based methods utilize the cluster assumption to generate pseudo-labels   \n529 for unsupervised learning [48]. Caron et al. [5] assumes that \"the model trained on labeled data   \n530 will produce high uncertainty estimation for unseen data\" in domain adaptation tasks to benefit the   \n531 classifier from unlabeled data lacking category information.   \n532 Bengio et al. in [20] proposed the conditional entropy and point out that \"These studies conclude that   \n533 the (asymptotic) information content of unlabeled examples decreases as classes overlap. Thus, the   \n534 assumption that classes are well separated is sensible if we expect to take advantage of unlabeled   \n535 examples.\" This assumption has been applied to many studies, for example in [29, 33, 60, 2]. In   \n536 the CTTA task of this paper, as the task progresses, the domain shifts, but the categories in the task   \n537 remain unchanged. Therefore, under the assumption that unlabeled data contains information, we   \n538 can reasonably continue to use conditional entropy in the current scenario. To sum up, whether in   \n539 unsupervised TTA or in the Bayesian field, this assumption is not difficult to achieve or has never   \n540 been applied. We can quite naturally continue to use this assumption in the context of this paper. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "541 B.2 BI during Test Time ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "542 The goal of CTTA is to learn a posterior distribution $p(\\pmb{\\theta}|\\mathcal{U}_{1:T}\\cup\\mathcal{D}_{0})$ from a source dataset $\\mathcal{D}_{0}$ ,   \n543 and a sequence of unlabeled test data from $\\mathcal{U}_{1}$ to $\\mathcal{U}_{T}$ . Following [60], assuming we have multiple   \n544 input-generating distributions that the source dataset $\\mathcal{D}_{0}$ is drawn from a distribution $\\phi$ , and $\\tilde{\\phi}_{t}$   \n545 specifies the shifted of the $t$ -th unlabeled test dataset which we aim to adapt to. Let the parameters   \n546 of the model be $\\pmb{\\theta}$ ,then following the semi-supervised learning framework [20], we incorporate all   \n547 input-generating distributions into the belief over the model parameters $\\pmb{\\theta}$ as follows ", "page_idx": 13}, {"type": "equation", "text": "$$\np(\\pmb\\theta|\\phi,\\tilde{\\phi}_{1},\\cdots,\\tilde{\\phi}_{T})\\propto p(\\pmb\\theta)\\exp\\left(-\\lambda_{0}H_{\\pmb\\theta,\\phi}(Y|X)\\right)\\prod_{t=1}^{T}\\exp\\left(-\\lambda_{t}H_{\\pmb\\theta,\\tilde{\\phi}_{t}}(Y|X)\\right),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "548 where the inputs $X$ are sampled i.i.d. from a generative model with parameters $\\phi$ , while the corre  \n549 sponding labels $Y$ are sampled from a conditional distribution $p(Y|X,\\theta)$ , which is parameterized   \n550 by the model parameters $\\pmb{\\theta}$ . $p(\\pmb\\theta)$ is a prior distribution over $\\pmb{\\theta}$ . $\\{\\lambda_{0},\\lambda_{1},\\dotsb,\\lambda_{T}\\}$ are the factors for   \n551 approximation weighting. Generally, the entropy term $H_{\\theta,\\phi}(Y|X)$ represents the cross entropy of   \n552 the supervised learning, and the entropy term $H_{\\pmb\\theta,\\tilde{\\phi}_{t}}(Y|X)$ for $t>0$ denotes the Shannon entropy of   \n553 the unsupervised learning.   \n554 Following [60], we can empirically use a point estimation to get a plug-in Bayesian approach to   \n555 approximate the above formula: ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(\\theta|\\mathcal{U}_{1:T}\\cup\\mathcal{D}_{0})}\\\\ {\\propto}&{p(\\theta)\\displaystyle\\prod_{\\forall x,y\\in\\mathcal{D}_{0}}p(y|x,\\theta)\\exp\\left(-\\frac{\\lambda_{0}}{|\\mathcal{D}_{0}|}\\sum_{\\forall x\\in\\mathcal{D}_{0}}H(Y|x,\\theta)\\right)\\displaystyle\\prod_{t=1}^{T}\\exp\\left(-\\frac{\\lambda_{t}}{|\\mathcal{U}_{t}|}\\sum_{\\forall x\\in\\mathcal{U}_{t}}H(Y|x,\\theta)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "556 To make the formula feasible to CTTA, that is, no source data is available at the test time, we set   \n557 $\\lambda_{0}\\,=\\,0$ . And the source knowledge can be represented by $\\begin{array}{r}{p(\\pmb{\\theta}|\\mathcal{D}_{0})\\,\\propto\\,p(\\pmb{\\theta})\\prod_{\\forall x,y\\in\\mathcal{D}_{0}}p(y|x,\\pmb{\\theta})}\\end{array}$ .   \n558 Thus, for the $t$ -th test domain, the Bayesian inference in CTTA can be represented as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(\\pmb{\\theta}|\\mathcal{U}_{1:t}\\cup\\mathcal{D}_{0})\\propto p(\\pmb{\\theta}|\\mathcal{D}_{0})\\prod_{i=1}^{t}\\exp\\left(-\\frac{\\lambda_{i}}{|\\mathcal{U}_{i}|}\\sum_{\\forall x\\in\\mathcal{U}_{i}}H(Y|x,\\pmb{\\theta})\\right)}\\\\ &{\\qquad\\qquad\\qquad\\propto p(\\pmb{\\theta}|\\mathcal{U}_{1:t-1}\\cup\\mathcal{D}_{0})\\exp\\left(-\\frac{\\lambda_{t}}{|\\mathcal{U}_{t}|}\\sum_{\\forall x\\in\\mathcal{U}_{t}}H(Y|x,\\pmb{\\theta})\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "559 where $\\boldsymbol{H}(\\mathcal{U}_{t}|\\boldsymbol{\\theta})=\\frac{1}{|\\mathcal{U}_{t}|}\\!\\sum_{\\forall x\\in\\mathcal{U}_{t}}\\boldsymbol{H}(Y|x,\\boldsymbol{\\theta})$ |U| \u2200x\u2208Ut H(Y |x, \u03b8) and the above formula can be rewritten in simplicity as ", "page_idx": 14}, {"type": "equation", "text": "$$\np(\\pmb{\\theta}|\\mathcal{U}_{1:t}\\cup\\mathcal{D}_{0})\\propto p(\\pmb{\\theta}|\\mathcal{U}_{1:t-1}\\cup\\mathcal{D}_{0})e^{-\\lambda H(\\mathcal{U}_{t}|\\pmb{\\theta})}=p_{t}(\\pmb{\\theta})e^{-\\lambda H(\\mathcal{U}_{t}|\\pmb{\\theta})},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "560 which specifies the Bayesian inference process on continuously arriving unlabeled data in CTTA. ", "page_idx": 14}, {"type": "text", "text": "561 C ELBO of the VI in CTTA ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "562 We built VI for CTTA in Sec. 3, where we initialize a variational distribution $q(\\pmb\\theta)$ to approximate the   \n563 real posterior. For the test domain $t$ , we optimize the variational distribution as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\nq_{t}(\\pmb\\theta)=\\arg\\operatorname*{min}_{\\pmb q\\in\\mathbb Q}\\mathrm{KL}\\left[q(\\pmb\\theta)\\mid\\mid\\frac{1}{Z_{t}}p_{t}(\\pmb\\theta)e^{-\\lambda H(\\mathcal{U}_{t}\\mid\\pmb\\theta)}\\right],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "564 where $\\mathbb{Q}$ is the distribution searching space, and $p_{t}(\\pmb\\theta)$ is the current prior. ", "page_idx": 14}, {"type": "text", "text": "565 Following the definition of KL divergence and the standard derivation of the Evidence Lower BOund   \n566 (ELBO) is as the following formulas. Specifically, the KL divergence is expanded as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\quadKL}\\left[q(\\theta)\\parallel\\frac{1}{Z}p_{t}(\\theta)e^{-\\lambda H(U_{t}|\\theta)}\\right]}\\\\ &{=-\\int_{\\theta}q(\\theta)\\log\\frac{\\frac{1}{Z_{t}}p_{t}(\\theta)e^{-\\lambda H(U_{t}|\\theta)}}{q(\\theta)}d\\theta}\\\\ &{=-\\int_{\\theta}q(\\theta)\\log\\frac{1}{Z_{t}}e^{-\\lambda H(U_{t}|\\theta)}d\\theta-\\int_{\\theta}q(\\theta)\\log\\frac{p_{t}(\\theta)}{q(\\theta)}d\\theta}\\\\ &{=\\int_{\\theta}q(\\theta)\\log Z_{t}d\\theta+\\lambda\\int_{\\theta}q(\\theta)H(U_{t}|\\theta)d\\theta-\\int_{\\theta}q(\\theta)\\log\\frac{p_{t}(\\theta)}{q(\\theta)}d\\theta}\\\\ &{=\\log Z_{t}+\\lambda\\mathbb{E}_{\\theta\\sim q(\\theta)}H(U_{t}|\\theta)+\\mathrm{KL}\\left(q(\\theta)\\parallel p_{t}(\\theta)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "567 where the first constant term can be reduced in the optimization. Thus, we can optimize the variational   \n568 distribution via the ELBO: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q_{t}(\\pmb{\\theta})=\\underset{q\\in\\mathbb{Q}}{\\arg\\operatorname*{min}}\\,\\mathrm{KL}\\left[q(\\pmb{\\theta})\\parallel\\frac{1}{Z_{t}}p_{t}(\\pmb{\\theta})e^{-\\lambda H(\\mathcal{U}_{t}|\\pmb{\\theta})}\\right]}\\\\ &{\\qquad=\\underset{q\\in\\mathbb{Q}}{\\arg\\operatorname*{max}}-\\lambda\\mathbb{E}_{\\pmb{\\theta}\\sim q(\\pmb{\\theta})}H(\\mathcal{U}_{t}|\\pmb{\\theta})-\\mathrm{KL}\\left(q(\\pmb{\\theta})\\parallel p_{t}(\\pmb{\\theta})\\right)}\\\\ &{\\qquad=\\underset{q\\in\\mathbb{Q}}{\\arg\\operatorname*{max}}\\,\\mathrm{ELBO}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "569 In our case, the former entropy term can be more effectively replaced by the cross entropy or   \n570 symmetric cross entropy (SCE) between the student model and the teacher model in a mean-teacher   \n571 architecture (see Sec. 4.1). For the latter KL term, we can substitute a variational approximation   \n572 that we deem closest to the current-stage prior $p_{t}(\\pmb\\theta)$ into the KL divergence. When the prior is a   \n573 multivariate Gaussian distribution, this term can be computed in closed form as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}\\left(\\mathcal{N}(\\mu_{1},\\Sigma_{1})\\parallel\\mathcal{N}(\\mu_{2},\\Sigma_{2})\\right)}\\\\ {=}&{\\frac{1}{2}\\left(\\mathrm{tr}(\\Sigma_{2}^{-1}\\Sigma_{1})+(\\mu_{2}-\\mu_{1})^{\\top}\\Sigma_{2}^{-1}(\\mu_{2}-\\mu_{1})-k+\\ln\\left(\\frac{\\operatorname*{det}(\\Sigma_{2})}{\\operatorname*{det}(\\Sigma_{1})}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "574 where $\\Sigma=\\mathrm{diag}(\\sigma^{2})$ , $k$ represents the dimensionality of the distributions, $\\operatorname{tr}(\\cdot)$ denotes the trace of a   \n575 matrix, and $\\operatorname*{det}(\\cdot)$ stands for the determinant of a matrix. For the case that the prior is a mixture of   \n576 Gaussian distributions, we can refer to the next section to get its upper bound. ", "page_idx": 15}, {"type": "text", "text": "577 D Mixture-of-Gaussian Prior ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "578 D.1 Upper Bound of the Mixture of Two KL Divergencies ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "579 We refer to the lemma that was stated for the mixture of Gaussian in [44]. The KL divergence   \n580 between two mixture distributions $\\textstyle p=\\sum_{i=1}^{k}\\alpha_{i}p_{i}$ and $\\textstyle p^{\\prime}=\\sum_{i=1}^{k}\\alpha_{i}p_{i}^{\\prime}$ is upper-bounded by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{KL}(p\\parallel p^{\\prime})\\leq\\mathrm{KL}(\\pmb{\\alpha}\\parallel\\pmb{\\alpha}^{\\prime})+\\sum_{i=1}^{k}\\alpha_{i}\\mathrm{KL}(p_{i}\\parallel p_{i}^{\\prime}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "581 where $\\pmb{\\alpha}=(\\alpha_{1},\\alpha_{2},\\cdot\\cdot\\cdot\\,,\\alpha_{k})$ and $\\pmb{\\alpha}^{\\prime}=(\\alpha_{1}^{\\prime},\\alpha_{2}^{\\prime},\\cdot\\cdot\\cdot\\,,\\alpha_{k}^{\\prime})$ are the weights of the mixture components.   \n582 The equality holds if and only if $\\begin{array}{r}{\\alpha_{i}p_{i}/{\\sum_{j=1}^{k}\\alpha_{j}p_{j}}=\\alpha_{i}^{\\prime}p_{i}^{\\prime}/{\\sum_{j=1}^{k}\\alpha_{j}^{\\prime}p_{j}^{\\prime}}}\\end{array}$ for all $i$ . Using the log-sum   \n583 inequality [10], we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathrm{KL}\\big(\\sum_{i=1}^{k}\\alpha_{i}p_{i}\\parallel\\sum_{i=1}^{k}\\alpha_{i}p_{i}^{\\prime}\\big)=\\int\\Bigg(\\displaystyle\\sum_{i=1}^{k}\\alpha_{i}p_{i}\\Bigg)\\log\\frac{\\sum_{i=1}^{k}\\alpha_{i}p_{i}}{\\sum_{i=1}^{k}\\alpha_{i}p_{i}^{\\prime}}}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\leq\\int\\sum_{i=1}^{k}\\alpha_{i}p_{i}\\log\\frac{\\alpha_{i}p_{i}}{\\alpha_{i}p_{i}^{\\prime}}}\\\\ {\\displaystyle\\qquad=\\sum_{i=1}^{k}\\alpha_{i}\\left(\\int p_{i}\\log\\frac{\\alpha_{i}}{\\alpha_{i}^{\\prime}}+\\int p_{i}\\log\\frac{p_{i}}{p_{i}^{\\prime}}\\right)}\\\\ {\\displaystyle\\qquad=\\mathrm{KL}(\\alpha\\parallel\\alpha^{\\prime})+\\sum_{i=1}^{k}\\alpha_{i}\\mathrm{KL}(p_{i}\\parallel p_{i}^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "584 In our algorithm, $q(\\pmb\\theta)$ is set to be a mixture of Gaussian distributions, i.e., $p_{t}(\\pmb{\\theta})=\\alpha\\cdot p_{1}(\\pmb{\\theta})+(1-$   \n585 $\\alpha)\\cdot\\bar{p}_{t}(\\pmb\\theta)$ . In the above inequality, let $\\begin{array}{r}{q(\\pmb{\\theta})=\\sum_{i=1}^{k}\\alpha_{i}q(\\pmb{\\theta})}\\end{array}$ , we can get the upper bound of the KL   \n586 divergence between $q(\\pmb\\theta)$ and $p_{t}(\\pmb\\theta)$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{KL}(q\\parallel p_{t})\\leq\\alpha\\cdot\\mathrm{KL}\\left(q\\Vert p_{1}\\right)+\\left(1-\\alpha\\right)\\cdot\\mathrm{KL}\\left(q\\Vert\\bar{p}_{t}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "587 So the lower bound (24) can be redefined as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}=-\\lambda\\mathbb{E}_{\\theta\\sim q(\\theta)}H(\\mathcal{U}_{t}|\\theta)-\\mathrm{KL}\\left(q(\\theta)\\parallel p_{t}(\\theta)\\right)}\\\\ &{\\quad\\geq-\\lambda\\mathbb{E}_{\\theta\\sim q(\\theta)}H(\\mathcal{U}_{t}|\\theta)-\\alpha\\cdot\\mathbf{KL}\\left(q||p_{1}\\right)-(1-\\alpha)\\cdot\\mathbf{KL}\\left(q||\\bar{p}_{t}\\right)}\\\\ &{\\quad\\overset{\\mathrm{def}}{=}\\mathcal{L}^{\\prime},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "588 Then, we have obtained a lower bound that can be optimized through closed-form calculations as   \n589 the source prior distribution $q_{0}(\\pmb\\theta)$ and the teacher prior distribution $\\bar{q}_{t}(\\pmb\\theta)$ are multivariate Gaussian   \n590 distributions, which means we can also optimize ${\\mathcal{L}}^{\\prime}$ with Eq. (25).   \n592 In this subsection, we illustrate why the mixture of Gaussian prior are beneficial to CTTA. First of   \n593 all, we can start from defining what is a better distribution for CTTA. Assume there exists an ideal   \n594 prior distribution $\\hat{p}_{t}$ , which effectively represents the distribution of the model after learning all past   \n595 knowledge, including that from the source and unlabeled datasets. Then we can use the difference   \n596 between a distribution and the ideal distribution $\\hat{p}_{t}$ (here we use KL divergence) to measure the   \n597 goodness of a distribution, i.e., $\\mathrm{KL}(\\cdot||\\hat{p}_{t})$ .   \n598 Generally, neither the source prior $p_{1}$ (trained on labeled data) nor the adapted prior $\\bar{p}_{t}$ (adapt   \n599 on unlabeled data, being unreliable) can be completely consistent with $\\hat{p}_{t}$ . Considering that, as $t$   \n600 increases, the difference between $\\bar{p}_{t}$ and $\\hat{p}_{t}$ will increase without an upper bound due to the error   \n601 accumulation (since $t$ is infinitely growing). The source prior $p_{1}$ cannot adapt to the unlabeled data,   \n602 but it contains important information from the labeled data, and the ideal distribution cannot forget the   \n603 source information too much, so we can assume that the difference between $p_{1}$ and $\\hat{p}_{t}$ is a constant,   \n604 i.e., $\\mathrm{KL}(p_{1}||\\hat{p}_{t})<U$ , where $U$ is a constant upper bound. Accordingly, it can be considered that   \n605 mixing the source prior $p_{1}$ and the adapted prior $\\bar{p}_{t}$ in some way is beneficial for reducing $\\mathrm{KL}(\\cdot||\\hat{p}_{t})$ .   \n606 In our paper, we consider using a simple Gaussian mixture, i.e., $p_{t}=\\alpha_{t}p_{1}+(1-\\alpha_{t})\\bar{p}_{t}$ , where $\\alpha$ is   \n607 computed by Eq. (10). It is easy to illustrate the benefits of this idea using the following inequality: ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}(p_{t}||\\hat{p}_{t})=\\mathrm{KL}\\left[(\\alpha_{t}p_{1}+(1-\\alpha_{t})\\bar{p}_{t})||\\hat{p}_{t}\\right]}\\\\ &{\\qquad\\qquad\\leq\\alpha_{t}\\mathrm{KL}(p_{1}||\\hat{p}_{t})+(1-\\alpha_{t})\\mathrm{KL}(\\bar{p}_{t}||\\hat{p}_{t})}\\\\ &{\\qquad\\qquad\\leq\\alpha_{t}U+(1-\\alpha_{t})\\mathrm{KL}(\\bar{p}_{t}||\\hat{p}_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In Eq. (29), if $\\mathrm{KL}(\\bar{p}_{t}||\\hat{p}_{t})\\geq U$ , which can be satisfied as mentioned above, then we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{KL}(p_{t}||\\hat{p}_{t})\\leq\\mathrm{KL}(\\bar{p}_{t}||\\hat{p}_{t}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "608 This indicates that the mixed distribution $p_{t}$ is closer to the ideal distribution $\\hat{p}_{t}$ than the adapted   \n609 prior $\\bar{p}_{t}$ . A similar idea can be found in the stochatic restoration in CoTTA [51], where the author   \n610 randomly restore parts of parameters of the current model into the parameters of source model. ", "page_idx": 16}, {"type": "text", "text": "611 E Augmentation Analysis ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "612 In our method, we use the standard augmentation following CoTTA [51]. In this subsection, we   \n613 analyze the some characteristics via experiments. ", "page_idx": 16}, {"type": "text", "text": "614 E.1 Confidence Margin ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "615 First, we analyze the margin $\\epsilon$ in Eq. (13). We experimentally validate different margins with more   \n616 choices. Experimental results are shown in Tables 8. The results indicate that different datasets   \n617 may require different margins to control confidence. Moreover, Eq. (13) signifies that the reliable   \n618 teacher likelihood is represented by the mean of its augmentations with $\\epsilon$ more confidence than the   \n619 teacher itself. Tables 8 illustrates the selection of $\\epsilon$ in our approach on CIFAR10C, CIFAR100C   \n620 and ImageNetC. Note that when $\\epsilon=-1$ , it means no margin is used and the method will use all   \n621 augmentated samples, i.e., without using Eq. (13). The results show that the proposed margin can   \n622 effectively filter out unreliable augmented samples and achieve a better teacher log-likelihood.   \n624 In our method, we also use augmentation to enhance the confidence. We then evaluate the the number   \n625 of augmentation in Eq. (10). The results can be seen in Table 9, and shows that increasing the number   \n626 of augmentations can enhance effectiveness, but this hyperparameter ceases to have a significant   \n627 impact after reaching 32. ", "page_idx": 16}, {"type": "table", "img_path": "mdK1vhgpa5/tmp/6549f3ca538e2a0d1087055021ddba3e26f9c3add28ead1a87b0724d17f227d5.jpg", "table_caption": ["Table 8: Analysis on confidence margin. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "table", "img_path": "mdK1vhgpa5/tmp/c69072934e80f090b73e874415cda8780d03975190c23627fdccff42d3c3fd3d.jpg", "table_caption": ["Table 9: Different number of augmentation. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "628 F Further Discussion on Variational Warm-up Strategy ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "629 We have discussed the Variational Warm-Up (VWU) strategy in Sec. 4.3.1, and explain that the   \n630 warm-up strategy is a common practice in TTA and CTTA. In this section, we further discuss some   \n631 attributes of the proposed variational warm-up strategy.   \n632 In our method, the VWU strategy is used to turn an off-the-shelf CNN to a pretrained BNN. The   \n633 advantage of this approach is that pretrained CNNs are readily available (e.g., directly leveraging   \n634 official models in PyTorch), while pretrained BNNs are challenging to obtain, especially for large  \n635 scale datasets. Moreover, training BNNs is more difficult compared to training CNNs. Therefore,   \n636 constructing BNN pretrained models based on existing CNN pretrained models is a feasible approach.   \n637 Additionally, we find that such a warm-up strategy requires only a few epochs to achieve satisfactory   \n638 results. To validate the characteristics of the proposed VWU strategy, we designed the following   \n639 experiments. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "640 F.1 Warm-up on CNN vs. Directly Pretraining BNN ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "641 First, we conducted experiments to compare the performance of obtaining pretrained BNN models   \n642 using the warm-up approach versus directly training the source model with BNN. We pretrain the   \n643 BNN also use VI as describing in Sec. 4.3.1. The results can be seen in Table 10. As we can see, the   \n644 results are at the same level, for example VI pretraining is with $13.2\\%$ error rate while the proposed   \n645 VWU achieves $13.1\\%$ on CIFAR10C. However, if we direct turn a pretrained CNN to a BNN by   \n646 adding random stochastic parameters, without warm-up strategy, the results drop to $17.1\\%$ . This   \n647 shows that VWU is a feasible strategy to obtain a pretrained BNN. ", "page_idx": 17}, {"type": "table", "img_path": "mdK1vhgpa5/tmp/fa32455399aea3395bf58bccc11616ac489c26534d920d97ad5f39340f085099.jpg", "table_caption": ["Table 10: Error comparison between varional warm-up on CNN and directly pretraining BNN. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "648 F.2 Number of Warm-up Epochs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "649 In our implementation, we employ only a limited number of epochs for variational warm-up, say 5   \n650 epochs. This is due to the fact that the pretrained model fits well in CNN, thus requiring minimal   \n651 adjustments to the mean of BNN. Additionally, the standard deviation (std) is initialized to be small.   \n652 Consequently, only a small number of iterations are necessary to update the BNN, and the step size is   \n653 also kept small. Experimentation on the epoch number of variational warm-up reveals that keeping   \n654 increasing epochs $(\\mathrm{~>~}5)$ ) will diminishes performance, as shown in Fig. 5. ", "page_idx": 17}, {"type": "image", "img_path": "mdK1vhgpa5/tmp/36cb4025d6792189ef195853950f20dc311c78ffa2110f279419e58294f0808a.jpg", "img_caption": ["Figure 4: Comparisons on different warm-up Figure 5: Comparisons on different warm-up data epochs (CIFAR10C). scale (CIFAR10C). "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "655 F.3 Only Portion Usage of Source Dataset in Warm-up ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "656 As we response to the weakness, the warm-up strategy is a common approach in TTA and CTTA   \n657 tasks and it is regarded as a part of pretraining stage. We also evaluate how if we only use partial   \n658 data for warm-up, and the results are as follow. The experimental results demonstrate that a moderate   \n659 reduction in sample size still maintains certain effectiveness of the warmup strategy. However,   \n660 excessive reduction, such as reducing to 1/10, leads to a certain decline in effectiveness. This is   \n661 because the warmup strategy aims to incorporate statistical information of the dataset into the model,   \n662 and insufficient data may result in inaccurate performance. ", "page_idx": 18}, {"type": "text", "text": "663 G Recursive Variational Approximation Process in VCoTTA ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "664 In this section, we show the algorithmic workflow utilizing variational approximation in VCoTTA. ", "page_idx": 18}, {"type": "text", "text": "665 Before testing time: First, we adopt a variational warm-up strategy to inject stochastic dynamics into   \n666 the model before adaptation. Given the source dataset $\\mathcal{D}_{0}$ , we can use a variational approximation of   \n667 $p(\\pmb{\\theta}|\\mathcal{D}_{0})$ as follows ", "page_idx": 18}, {"type": "equation", "text": "$$\np(\\pmb\\theta|\\mathcal D_{0})=p_{1}(\\pmb\\theta)\\approx q_{0}(\\pmb\\theta)=\\arg\\operatorname*{min}_{\\pmb q\\in\\mathbb Q}\\mathrm{KL}\\left[q(\\pmb\\theta)\\parallel\\frac{1}{Z_{0}}p(\\pmb\\theta)p(\\mathcal D_{0}|\\pmb\\theta)\\right],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "668 where we use the pretrained deterministic model $p_{0}(\\pmb\\theta)$ as the prior distribution. ", "page_idx": 18}, {"type": "text", "text": "669 When the domain shift: Then, at the beginning of the test time, we set the prior in task $t$ as   \n670 $p_{t}(\\pmb\\theta)\\,=\\,\\alpha\\cdot p_{1}(\\pmb\\theta)+(1-\\alpha)\\cdot\\bar{p}_{t}(\\pmb\\theta)$ and variational approximation, where $p_{1}(\\pmb\\theta)\\,\\approx\\,q_{0}(\\pmb\\theta)$ and   \n671 $\\bar{p}_{t}(\\pmb\\theta)\\approx\\bar{q}_{t}(\\pmb\\theta)$ . For $\\bar{q}_{t}(\\pmb\\theta)$ , which means the real-time posterior probability of the teacher model for   \n672 the $t$ -th test domain, is constantly updated by $q_{t}(\\pmb\\theta)$ via EMA (see Sec. 4.3.3) during the test phase.   \n673 Note that we do not have $\\bar{q}_{t}(\\pmb\\theta)$ for the first update in the $t$ -th phase. In fact, we use $q_{t-1}(\\pmb\\theta)$ construct   \n674 the prior, thus we have $\\bar{p_{t}}(\\pmb{\\dot{\\theta}})\\approx\\alpha\\cdot p_{1}(\\pmb{\\theta})\\,\\bar{+}\\,(1-\\alpha)\\cdot q_{t-1}\\bar{(\\pmb{\\theta})}$ . This is the variational distribution   \n675 that should be used to approximate the prior in the absence of a teacher model in the first step, as   \n676 well as the approximation that should be used when not employing the MT architecture. Note that   \n677 the process is not required to inform the model that the domain produces a shift.   \n678 During the testing time of a domain: With the approximation to $p_{t}(\\pmb\\theta)$ and analysis from Ap  \n679 pendix B.2, we get $q_{t}(\\pmb\\theta)$ for student model at the test domain $t$ as follows: ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "equation", "text": "$$\nq_{t}(\\pmb\\theta)=\\arg\\operatorname*{min}_{\\pmb q\\in\\mathbb Q}\\mathrm{KL}\\left[q(\\pmb\\theta)\\mid\\mid\\frac{1}{Z_{t}}p_{t}(\\pmb\\theta)e^{-\\lambda H(\\mathcal{U}_{t}\\mid\\pmb\\theta)}\\right],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "680 which means, we can recursively derive $p_{t+1}(\\pmb\\theta)$ and the following variational distributions, thereby   \n681 achieving the goal of VCoTTA. ", "page_idx": 18}, {"type": "text", "text": "682 H Different Orders of Corruption ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "683 As we discuss in the major comparisons (see Sec 5.3), the performance may be affected by the   \n684 corruption order. To provide a more comprehensive evaluation of the matter of the order, we conduct   \n685 10 different orders from Sec 5.3, and show the average performance of all compared methods.   \n686 10 independent random orders of corruption are all under the severity level of 5. The results   \n687 are shown in Table 11. We find that the order of corruption is minor on simple datasets such as   \n688 CIFAR10C and CIFAR100C, but small std on difficult datasets such as ImageNetC. The proposed   \n689 VCOTTA outperforms other methods on the average error of CIFAR10C and CIFAR100C under 10   \n690 different corruption orders, which shows the effectiveness of the prior calibration in CTTA. Moreover,   \n691 VCOTTA has comparable results with PETAL on ImageNetC, but smaller std over 10 orders, which   \n692 shows the robustness of the proposed method. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "table", "img_path": "mdK1vhgpa5/tmp/4d78244825f678ed22b00928313dff9bb6daa65e9c6428000d43bb7f0e9a846b.jpg", "table_caption": ["Table 11: Comparisons over 10 orders $({\\mathrm{avg}}\\pm{\\mathrm{std}})$ . "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "693 I Corruption Loops ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "694 In the real-world scenario, the testing domain may reappear in the future. We evaluate the test   \n695 conditions continually 10 times to evaluate the long-term adaptation performance on CIFAR10C.   \n696 That is, the test data will be re-inference and re-adapt for 9 more turns under severity 5. Full   \n697 results can be found in Fig. 6. The results show that most compared methods obtain performance   \n698 improvement in the first several loops, but suffer from performance drop in the following loops. This   \n699 means that the model drift can be even useful in early loops, but the drift becomes hard because of   \n700 the unreliable prior. The results also indicate that our method outperforms others in this long-term   \n701 adaptation situation and has only small performance drops. ", "page_idx": 19}, {"type": "image", "img_path": "mdK1vhgpa5/tmp/ec480a8211ba1c6dafaaef6339b4c8b34cf8039c4976290f2570b73154edc1fc.jpg", "img_caption": ["Figure 6: 10 loops under a same corruption order (CIFAR10C). "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "702 J Experiment on Online Setting ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "703 CTTA does operate in an online setting, where all testing data is used only once. However, the current   \n704 focus of CTTA research primarily revolves around batch-mode online settings, with batch sizes   \n705 typically set to 200 in our experiments like other SOTAs. In CTTA, strict online learning settings   \n706 where each data point is processed individually are under-researched. In fact, our method can be   \n707 applied in scenarios with online learning or small batch sizes. However, it\u2019s important to note that the   \n708 batch normalization (BN) layers is disabled when the batch size is 1. We experimented with batch   \n709 size of 1 on CIFAR10C, and compare the results with some baseline methods. The comparison results   \n710 are shown in Table 12. The results show that small batch size in CTTA makes worse performance.   \n711 We believe this is because a small batch size amplifies the uncertainty in model training. ", "page_idx": 19}, {"type": "table", "img_path": "mdK1vhgpa5/tmp/cec4cf29cf81f9cf941899e6ac01edb322b33e13976edaeec081f2f40324bd40.jpg", "table_caption": ["Table 12: Error comparisons of strict online learning (batch size $=1$ ). "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "712 K Time and Memory Cost ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "713 We implement our method using a single RTX-4090 GPU card. We provide the memory and time cost   \n714 in Table 13. Our proposed VCoTTA method does not offer an advantage in terms of memory usage.   \n715 This is because in the BNN framework, additional standard deviations are required for implementing   \n716 local reparameterization tricks. However, during the testing phase, this does not significantly impact   \n717 the efficiency of the model. This is because during testing, only the student model employs variational   \n718 inference, which requires uncertainty parameters.   \n720 The checklist is designed to encourage best practices for responsible machine learning research,   \n721 addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove   \n722 the checklist: The papers not including the checklist will be desk rejected. The checklist should   \n723 follow the references and follow the (optional) supplemental material. The checklist does NOT count   \n724 towards the page limit.   \n725 Please read the checklist guidelines carefully for information on how to answer these questions. For   \n726 each question in the checklist:   \n727 \u2022 You should answer [Yes] , [No] , or [NA] .   \n728 \u2022 [NA] means either that the question is Not Applicable for that particular paper or the   \n729 relevant information is Not Available.   \n730 \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA).   \n731 The checklist answers are an integral part of your paper submission. They are visible to the   \n732 reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it   \n733 (after eventual revisions) with the final version of your paper, and its final version will be published   \n734 with the paper.   \n735 The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.   \n736 While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a   \n737 proper justification is given (e.g., \"error bars are not reported because it would be too computationally   \n738 expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering   \n739 \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we   \n740 acknowledge that the true answer is often more nuanced, so please just use your best judgment and   \n741 write a justification to elaborate. All supporting evidence can appear either in the main paper or the   \n742 supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification   \n743 please point to the section(s) where related material for the question can be found. ", "page_idx": 20}, {"type": "table", "img_path": "mdK1vhgpa5/tmp/6fb99b09d259395c15dd26cdc730c514785fa37f3713cd97abea4a7456f809c6.jpg", "table_caption": ["Table 13: Time and memory cost comparisons. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "744 IMPORTANT, please: ", "page_idx": 21}, {"type": "text", "text": "745 \u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\",   \n746 \u2022 Keep the checklist subsection headings, questions/answers and guidelines below.   \n747 \u2022 Do not modify the questions and only use the provided macros for your answers.   \n748 1. Claims   \n749 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n750 paper\u2019s contributions and scope?   \n751 Answer: [Yes]   \n752 Justification: We made clear claims to illustrate that we evaluate the uncertainty in CTTA   \n753 task using variational inference.   \n754 Guidelines:   \n755 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n756 made in the paper.   \n757 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n758 contributions made in the paper and important assumptions and limitations. A No or   \n759 NA answer to this question will not be perceived well by the reviewers.   \n760 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n761 much the results can be expected to generalize to other settings.   \n762 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n763 are not attained by the paper.   \n764 2. Limitations   \n765 Question: Does the paper discuss the limitations of the work performed by the authors?   \n766 Answer: [Yes]   \n767 Justification: We discuss the limitation in the last section.   \n768 Guidelines:   \n769 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n770 the paper has limitations, but those are not discussed in the paper.   \n771 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n772 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n773 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n774 model well-specification, asymptotic approximations only holding locally). The authors   \n775 should reflect on how these assumptions might be violated in practice and what the   \n776 implications would be.   \n777 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n778 only tested on a few datasets or with a few runs. In general, empirical results often   \n779 depend on implicit assumptions, which should be articulated.   \n780 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n781 For example, a facial recognition algorithm may perform poorly when image resolution   \n782 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n783 used reliably to provide closed captions for online lectures because it fails to handle   \n784 technical jargon.   \n785 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n786 and how they scale with dataset size.   \n787 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n788 address problems of privacy and fairness.   \n789 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n790 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n791 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n792 judgment and recognize that individual actions in favor of transparency play an impor  \n793 tant role in developing norms that preserve the integrity of the community. Reviewers   \n794 will be specifically instructed to not penalize honesty concerning limitations.   \n795 3. Theory Assumptions and Proofs   \n796 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n797 a complete (and correct) proof?   \n798 Answer: [Yes]   \n799 Justification: We provide the assumption and proofs mostly in appendix.   \n800 Guidelines:   \n801 \u2022 The answer NA means that the paper does not include theoretical results.   \n802 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n803 referenced.   \n804 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n805 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n806 they appear in the supplemental material, the authors are encouraged to provide a short   \n807 proof sketch to provide intuition.   \n808 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n809 by formal proofs provided in appendix or supplemental material.   \n810 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n811 4. Experimental Result Reproducibility   \n812 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n813 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n814 of the paper (regardless of whether the code and data are provided or not)?   \n815 Answer: [Yes]   \n816 Justification: We use open-source dataset and provide a anonymous code link.   \n817 Guidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n819 \u2022 If the paper includes experiments, a No answer to this question will not be perceived   \n820 well by the reviewers: Making the paper reproducible is important, regardless of   \n821 whether the code and data are provided or not.   \n822 \u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken   \n823 to make their results reproducible or verifiable.   \n824 \u2022 Depending on the contribution, reproducibility can be accomplished in various ways.   \n825 For example, if the contribution is a novel architecture, describing the architecture fully   \n826 might suffice, or if the contribution is a specific model and empirical evaluation, it may   \n827 be necessary to either make it possible for others to replicate the model with the same   \n828 dataset, or provide access to the model. In general. releasing code and data is often   \n829 one good way to accomplish this, but reproducibility can also be provided via detailed   \n830 instructions for how to replicate the results, access to a hosted model (e.g., in the case   \n831 of a large language model), releasing of a model checkpoint, or other means that are   \n832 appropriate to the research performed.   \n833 \u2022 While NeurIPS does not require releasing code, the conference does require all submis  \n834 sions to provide some reasonable avenue for reproducibility, which may depend on the   \n835 nature of the contribution. For example   \n836 (a) If the contribution is primarily a new algorithm, the paper should make it clear how   \n837 to reproduce that algorithm.   \n838 (b) If the contribution is primarily a new model architecture, the paper should describe   \n839 the architecture clearly and fully.   \n840 (c) If the contribution is a new model (e.g., a large language model), then there should   \n841 either be a way to access this model for reproducing the results or a way to reproduce   \n842 the model (e.g., with an open-source dataset or instructions for how to construct   \n843 the dataset).   \n844 (d) We recognize that reproducibility may be tricky in some cases, in which case   \n845 authors are welcome to describe the particular way they provide for reproducibility.   \n846 In the case of closed-source models, it may be that access to the model is limited in   \n847 some way (e.g., to registered users), but it should be possible for other researchers   \n848 to have some path to reproducing or verifying the results.   \n849 5. Open access to data and code   \n850 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n851 tions to faithfully reproduce the main experimental results, as described in supplemental   \n852 material?   \n853 Answer: [Yes]   \n854 Justification: We provide the anonymous code link.   \n855 Guidelines:   \n856 \u2022 The answer NA means that paper does not include experiments requiring code.   \n857 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n858 public/guides/CodeSubmissionPolicy) for more details.   \n859 \u2022 While we encourage the release of code and data, we understand that this might not be   \n860 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n861 including code, unless this is central to the contribution (e.g., for a new open-source   \n862 benchmark).   \n863 \u2022 The instructions should contain the exact command and environment needed to run to   \n864 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n865 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n866 \u2022 The authors should provide instructions on data access and preparation, including how   \n867 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n868 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n869 proposed method and baselines. If only a subset of experiments are reproducible, they   \n870 should state which ones are omitted from the script and why.   \n871 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n872 versions (if applicable).   \n873 \u2022 Providing as much information as possible in supplemental material (appended to the   \n874 paper) is recommended, but including URLs to data and code is permitted.   \n875 6. Experimental Setting/Details   \n876 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n877 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n878 results?   \n879 Answer: [Yes]   \n880 Justification: We follow previous to set the experiments.   \n881 Guidelines:   \n882 \u2022 The answer NA means that the paper does not include experiments.   \n883 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n884 that is necessary to appreciate the results and make sense of them.   \n885 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n886 material.   \n887 7. Experiment Statistical Significance   \n888 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n889 information about the statistical significance of the experiments?   \n890 Answer: [Yes]   \n891 Justification: We offer the 10 different task orders to reduce the influence of stochastic and   \n892 provide the avg $\\pm$ std in Appendix H.   \n893 Guidelines:   \n894 \u2022 The answer NA means that the paper does not include experiments.   \n895 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n896 dence intervals, or statistical significance tests, at least for the experiments that support   \n897 the main claims of the paper.   \n898 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n899 example, train/test split, initialization, random drawing of some parameter, or overall   \n900 run with given experimental conditions).   \n901 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n902 call to a library function, bootstrap, etc.)   \n903 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n904 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n905 of the mean.   \n906 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n907 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n908 of Normality of errors is not verified.   \n909 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n910 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n911 error rates).   \n912 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n913 they were calculated and reference the corresponding figures or tables in the text.   \n914 8. Experiments Compute Resources   \n915 Question: For each experiment, does the paper provide sufficient information on the com  \n916 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n917 the experiments?   \n918 Answer: [Yes]   \n919 Justification: We provide the compute resources in Appendix K.   \n920 Guidelines:   \n921 \u2022 The answer NA means that the paper does not include experiments.   \n922 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n923 or cloud provider, including relevant memory and storage.   \n924 \u2022 The paper should provide the amount of compute required for each of the individual   \n925 experimental runs as well as estimate the total compute.   \n926 \u2022 The paper should disclose whether the full research project required more compute   \n927 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n928 didn\u2019t make it into the paper).   \n929 9. Code Of Ethics   \n930 Question: Does the research conducted in the paper conform, in every respect, with the   \n931 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n932 Answer: [Yes]   \n933 Justification: We confirm that we conducted in the paper conform with the NeurIPS Code of   \n934 Ethics.   \n935 Guidelines:   \n936 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n937 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n938 deviation from the Code of Ethics.   \n939 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n940 eration due to laws or regulations in their jurisdiction).   \n941 10. Broader Impacts   \n942 Question: Does the paper discuss both potential positive societal impacts and negative   \n943 societal impacts of the work performed?   \n944 Answer: [NA]   \n945 Justification: Nor applicable. We study machine learning problem on public dataset such as   \n946 CIFAR10.   \n947 Guidelines:   \n948 \u2022 The answer NA means that there is no societal impact of the work performed.   \n949 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n950 impact or why the paper does not address societal impact.   \n951 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n952 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n953 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n954 groups), privacy considerations, and security considerations.   \n955 \u2022 The conference expects that many papers will be foundational research and not tied   \n956 to particular applications, let alone deployments. However, if there is a direct path to   \n957 any negative applications, the authors should point it out. For example, it is legitimate   \n958 to point out that an improvement in the quality of generative models could be used to   \n959 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n960 that a generic algorithm for optimizing neural networks could enable people to train   \n961 models that generate Deepfakes faster.   \n962 \u2022 The authors should consider possible harms that could arise when the technology is   \n963 being used as intended and functioning correctly, harms that could arise when the   \n964 technology is being used as intended but gives incorrect results, and harms following   \n965 from (intentional or unintentional) misuse of the technology.   \n966 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n967 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n968 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n969 feedback over time, improving the efficiency and accessibility of ML).   \n970 11. Safeguards   \n971 Question: Does the paper describe safeguards that have been put in place for responsible   \n972 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n973 image generators, or scraped datasets)?   \n974 Answer: [NA]   \n975 Justification: No such risks.   \n976 Guidelines:   \n977 \u2022 The answer NA means that the paper poses no such risks.   \n978 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n979 necessary safeguards to allow for controlled use of the model, for example by requiring   \n980 that users adhere to usage guidelines or restrictions to access the model or implementing   \n981 safety filters.   \n982 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n983 should describe how they avoided releasing unsafe images.   \n984 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n985 not require this, but we encourage authors to take this into account and make a best   \n986 faith effort. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "987 12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "988 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n989 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n990 properly respected? ", "page_idx": 26}, {"type": "text", "text": "Justification: We referred to open-source code from various methods and developed our own implementation of the core algorithm. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1011 Question: Are new assets introduced in the paper well documented and is the documentation   \n012 provided alongside the assets?   \n1013 Answer: [NA]   \n1014 Justification: No new assets will be released.   \n1015 Guidelines: ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "24 14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "25 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n26 include the full text of instructions given to participants and screenshots, if applicable, as   \n27 well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA]   \nJustification: We use public dataset. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "1028   \n1029   \n1030   \n1031   \n1032   \n1033   \n1034   \n1035   \n1036   \n1037   \n1038   \n1039   \n1040   \n1041   \n1042   \n1043   \n1044   \n1045   \n1046   \n1047   \n1048   \n1049   \n1050   \n1051   \n1052   \n1053   \n1054   \n1055   \n1056   \n1057 ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: We do not involve crowdsourcing nor research with human subjects. Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. \u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. \u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]