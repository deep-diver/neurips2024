[{"figure_path": "JVtwC9RzlI/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of the proposed structure token approach. (1a) The model takes the input text graph (left) and the partially generated sub-graph (right) and then generates a new structure token. Each structure token contains a (sub-)word token with the locational information of that word token in the text graph. (1b) An example of autoregressive decoding with structure token. The procedure is mostly the same as normal text decoding with Transformer model.", "description": "This figure illustrates the proposed structure token approach for unifying graph and text processing.  Part (a) shows the model architecture, highlighting how structure tokens, combining sub-word tokens with location information from the input text graph, are generated. Part (b) demonstrates the autoregressive decoding process, showing how the model iteratively generates structure tokens to build the output graph, similar to standard text generation.", "section": "3 Method"}, {"figure_path": "JVtwC9RzlI/figures/figures_1_2.jpg", "caption": "Figure 1: Overview of the proposed structure token approach. (1a) The model takes the input text graph (left) and the partially generated sub-graph (right) and then generates a new structure token. Each structure token contains a (sub-)word token with the locational information of that word token in the text graph. (1b) An example of autoregressive decoding with structure token. The procedure is mostly the same as normal text decoding with Transformer model.", "description": "This figure illustrates the structure token approach for text graph generation.  Part (a) shows the model architecture, highlighting how the model takes an input text graph and a partially generated subgraph to create new structure tokens.  These tokens combine sub-word information with location data within the graph. Part (b) provides a visual example of the autoregressive decoding process, showcasing how the model iteratively generates structure tokens to build the final text graph, mirroring standard transformer text decoding.", "section": "3 Method"}, {"figure_path": "JVtwC9RzlI/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of the proposed structure token approach. (1a) The model takes the input text graph (left) and the partially generated sub-graph (right) and then generates a new structure token. Each structure token contains a (sub-)word token with the locational information of that word token in the text graph. (1b) An example of autoregressive decoding with structure token. The procedure is mostly the same as normal text decoding with Transformer model.", "description": "This figure illustrates the proposed \"Structure Token\" approach for unifying graph and text processing.  Part (a) shows the model's architecture, highlighting how it takes an input text graph and a partially generated subgraph to produce a new structure token. This token incorporates both word and location information within the graph.  Part (b) demonstrates the autoregressive decoding process, similar to standard transformer decoding, but using the structure tokens.", "section": "3 Method"}, {"figure_path": "JVtwC9RzlI/figures/figures_3_2.jpg", "caption": "Figure 1: Overview of the proposed structure token approach. (1a) The model takes the input text graph (left) and the partially generated sub-graph (right) and then generates a new structure token. Each structure token contains a (sub-)word token with the locational information of that word token in the text graph. (1b) An example of autoregressive decoding with structure token. The procedure is mostly the same as normal text decoding with Transformer model.", "description": "This figure illustrates the structure token approach.  (a) shows how the model processes both text and graph data using structure tokens, which encode sub-word tokens along with their locations within the graph.  (b) shows the autoregressive decoding process, demonstrating how the model generates a graph by sequentially producing structure tokens, similar to how standard transformers generate text.", "section": "3 Method"}, {"figure_path": "JVtwC9RzlI/figures/figures_4_1.jpg", "caption": "Figure 3: The Structure Predictor predicts graph-level identifier of the k-th token by taking the hidden state of previously generated tokens plus the segment ID of the next tokens. The output is then multiplied with all possible token IDs.", "description": "This figure illustrates the structure predictor component of the TextGraphBART model.  The predictor uses the hidden state from previously generated tokens and the segment ID of the next token as input to a causal transformer layer. This layer outputs logits for all possible unique token IDs, which are then multiplied by a softmax function to predict the probability of each unique ID. The highest probability ID is then selected as the predicted graph-level identifier for the k-th token.", "section": "3.3 Generation"}, {"figure_path": "JVtwC9RzlI/figures/figures_12_1.jpg", "caption": "Figure 1: Overview of the proposed structure token approach. (1a) The model takes the input text graph (left) and the partially generated sub-graph (right) and then generates a new structure token. Each structure token contains a (sub-)word token with the locational information of that word token in the text graph. (1b) An example of autoregressive decoding with structure token. The procedure is mostly the same as normal text decoding with Transformer model.", "description": "This figure illustrates the proposed structure token approach for unifying graph and text processing.  (a) shows the model's input (text graph) and how it generates structure tokens (containing sub-word tokens and locational information). (b) shows an example of autoregressive decoding using the structure tokens, similar to standard Transformer decoding.", "section": "3 Method"}]