[{"figure_path": "re2jPCnzkA/figures/figures_0_1.jpg", "caption": "Figure 1: The MIDGARD generative pipeline.", "description": "The figure illustrates the MIDGARD generative pipeline, which consists of two main components: a structure generator and a shape generator. The structure generator takes partial or noisy information about an articulated object (e.g., a nightstand with drawers and a door) and uses a graph attention network (GAT) to generate a denoised graph representing the object's structure and kinematics. This denoised graph is then used to condition the shape generator, which is a 3D generative model that creates high-fidelity 3D meshes for each part of the articulated object. The output of the pipeline is a complete, fully simulatable articulated 3D asset.", "section": "1 Introduction"}, {"figure_path": "re2jPCnzkA/figures/figures_5_1.jpg", "caption": "Figure 2: Architecture of the Shape Generator.", "description": "The figure illustrates the architecture of the Shape Generator component of MIDGARD.  It details how the model takes in multimodal inputs (image, text, and graph features), processes them through a U-Net architecture for denoising, incorporates a bounding box constraint for consistent shape generation, and finally outputs the mesh representation of the object part. The diagram highlights the original SDFusion pipeline components and new additions made within MIDGARD to achieve improved shape generation.", "section": "3 Synthesis of 3D Articulated Mechanisms"}, {"figure_path": "re2jPCnzkA/figures/figures_7_1.jpg", "caption": "Figure 3: A) PartToMotion constrained structural generation. B) Side-by-side comparison between our approach and NAP. C) Typical failure cases. D) Part-level image conditioning.", "description": "This figure presents a comparative analysis of MIDGARD and NAP across various aspects of articulated object generation. (A) showcases MIDGARD's ability to generate articulated assets using either image+text or part-level information, highlighting its controllability and conditional generation capabilities. (B) provides a direct comparison of assets generated by MIDGARD and NAP, illustrating MIDGARD's superior quality, consistency, and natural joint motions. (C) illustrates typical failure cases encountered with NAP, including mismatched parts and unnatural joint movements, while emphasizing MIDGARD's robustness to these issues. Finally, (D) demonstrates MIDGARD's part-level image conditioning capability, showing how different input images influence the generated part appearances.", "section": "4.2 Results and Discussion"}, {"figure_path": "re2jPCnzkA/figures/figures_7_2.jpg", "caption": "Figure 4: Image guidance of the shape generation process", "description": "This figure demonstrates the impact of image inputs on the shape generation process within MIDGARD's framework. By modifying the image inputs, while keeping other factors constant (such as text and graph features), users can adjust the generated parts to achieve a desired aesthetic or functional outcome. This showcases MIDGARD's flexibility and controllability on a part-level.", "section": "4 Experiments, Results and Discussion"}, {"figure_path": "re2jPCnzkA/figures/figures_8_1.jpg", "caption": "Figure 5: Failure cases due to due to axis-aligned bounding boxes and parameter discretization. A) The use of Axis Alligned Bounding Boxes tends to confuse the 3D generation model. B) The use of categorical joint variable in MIDGArD allows better joint motion resolution and helps filtering unrealistic solutions obtained with former pipelines.", "description": "This figure shows the failure cases due to the use of axis-aligned bounding boxes and parameter discretization. The left part (A) shows that using axis-aligned bounding boxes tends to confuse the 3D generation model, leading to incorrect results. The right part (B) demonstrates that using the categorical joint variable in MIDGARD helps achieve better joint motion resolution and filters out unrealistic solutions generated by previous methods.", "section": "4 Experiments, Results and Discussion"}, {"figure_path": "re2jPCnzkA/figures/figures_17_1.jpg", "caption": "Figure 6: Detail of the structure generation training pipeline.", "description": "The figure shows the detailed process of the structure generation training pipeline in MIDGARD. It illustrates how an image VQVAE encodes different views of each object part into an 8x8 latent code that is then reshaped into a 64x1 latent vector.  These vectors, along with textual descriptions of the parts (e.g., \"Night Table Body,\" \"Drawer\") and their joint types (e.g., \"Revolute,\" \"Prismatic\"), are combined to form the node and edge features of the articulation graph.  The graph then serves as input to a Graph Attention Network (GAT) for processing. The entire process aims to generate coherent and interpretable articulation features from noisy or incomplete inputs.", "section": "A.1 Training and Inference of the Structure Generator"}, {"figure_path": "re2jPCnzkA/figures/figures_17_2.jpg", "caption": "Figure 6: Detail of the structure generation training pipeline.", "description": "This figure shows the detailed structure generation training pipeline.  The input is an articulated asset represented as a graph. Each part of the asset is encoded into an 8x8 latent image using a VQVAE. This latent representation is then concatenated with other relevant features to form a 64x1 latent vector, which is further processed by a Graph Attention Network (GAT). This GAT attempts to predict a clean version of the noisy articulation graph.  The figure illustrates the flow of data through the different components (VQVAE, Reshape, GAT, and the Articulation Graph) and their interactions.", "section": "A Details of the Training and Inference Process"}, {"figure_path": "re2jPCnzkA/figures/figures_18_1.jpg", "caption": "Figure 8: Detail of the structure generation inference pipeline.", "description": "This figure illustrates the inference stage of the structure generator in the MIDGARD model. An incomplete articulation graph, representing the object's structure and joints, is input to a Graph Attention Network (GAT). The GAT processes the input graph and outputs a denoised articulation graph.  Latent image codes (8x8) are extracted for each link and then reshaped into 64x1 latent vectors. This denoised graph, along with the latent image codes, provides information for the subsequent shape generation step.", "section": "3.2 Structure Generator"}, {"figure_path": "re2jPCnzkA/figures/figures_19_1.jpg", "caption": "Figure 9: Mesh repair pipeline: (a) The original mesh exhibits holes and non-manifold edges; (b) After preprocessing, the mesh is converted into a watertight manifold suitable for TSDF generation.", "description": "This figure shows the result of the mesh repair pipeline used in the preprocessing step.  The original mesh (a) contains holes and non-manifold edges, which are common issues in 3D models. The pipeline processes these issues to create a watertight manifold mesh (b), making it suitable for further processing such as generating a Truncated Signed Distance Function (TSDF), a representation commonly used in 3D generative models.", "section": "B Data Preprocessing"}, {"figure_path": "re2jPCnzkA/figures/figures_19_2.jpg", "caption": "Figure 1: The MIDGArD generative pipeline.", "description": "This figure illustrates the pipeline of the MIDGARD generative model. It starts with a VQVAE (Vector Quantized Variational Autoencoder) which encodes the input images. Then, the decoded images and partial information are processed by the Structure Generator (using GAT, Graph Attention Network) to generate a denoised articulation graph. This denoised graph is further used by the Shape Generator to generate the final articulated asset.", "section": "1 Introduction"}, {"figure_path": "re2jPCnzkA/figures/figures_20_1.jpg", "caption": "Figure 11: An articulated asset simulated in the MuJoCo[86] physics engine.", "description": "This figure shows an example of an articulated object generated by MIDGARD and simulated within the MuJoCo physics engine.  It showcases the successful generation of a teapot, complete with handle and lid, and its ability to be rendered and simulated realistically in a physics environment. The green translucent boxes represent the oriented bounding boxes calculated during the preprocessing stage, indicating proper alignment and size consistency of the generated parts.", "section": "C. Controlling Generation with Asset Labels"}, {"figure_path": "re2jPCnzkA/figures/figures_20_2.jpg", "caption": "Figure 12: Controlled Generation with Asset Labels.", "description": "This figure shows examples of 3D articulated objects generated by MIDGARD using only asset-level labels as input.  The top row displays various \"storage furniture\" assets, showcasing differences in overall structure and component arrangement, while the bottom row shows variations of \"fan\" assets. These examples highlight MIDGARD's ability to generate diverse and plausible articulated objects based on high-level semantic information alone, demonstrating its controllability and flexibility.", "section": "C Controlling Generation with Asset Labels"}, {"figure_path": "re2jPCnzkA/figures/figures_21_1.jpg", "caption": "Figure 13: Controlling the shape appearance of object parts via image conditioning. Top: Image inputs allow to control the object design on a part-level. Bottom: NAP struggles to generate objects with many parts and yields unrealistic motion.", "description": "This figure showcases the controllability of MIDGARD's shape generation module.  By changing the input image, the generated part's appearance changes accordingly. The top row shows examples where replacing the input image successfully changes the appearance of the generated part.  The bottom row contrasts this with results from NAP, a previous method, which shows the failure of that method to generate high-quality and realistic parts, particularly when many parts are involved.", "section": "D Controlling Generation with Image Input"}]