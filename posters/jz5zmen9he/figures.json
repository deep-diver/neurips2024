[{"figure_path": "jz5ZMeN9He/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Matting methods [20, 4\u20137] commonly predict the alpha matte and then infer the foreground color by post-processing [21], which often relies on empirical assumptions such as local smoothing of the foreground and background, leading to the accumulation of errors. (b) In contrast, our joint prediction approach estimates both the foreground and alpha simultaneously. By leveraging LDM\u2019s [8] powerful natural vision prior, our predicted foreground is closer to natural images.", "description": "This figure compares traditional two-stage matting methods with the authors' proposed method. Traditional methods first predict the alpha matte and then estimate the foreground color using post-processing, which may lead to error accumulation.  In contrast, the authors' method, Drip, jointly predicts both foreground and alpha simultaneously, leveraging the prior knowledge embedded in pre-trained Latent Diffusion Models (LDM) for improved accuracy and naturalness.", "section": "1 Introduction"}, {"figure_path": "jz5ZMeN9He/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of Drip. (a) During training, the input image X, trimap Tri, ground-truth foreground F, and ground-truth alpha map a are first encoded into latent representations ZX, ZTri, ZF, and Za respectively using the original Stable Diffusion VAE encoder E. After adding noise to ZF and Za, all the latents are fed into a U-Net, which generates the output in the foreground or alpha domain guided by a switcher (\u00a73.2.1). The U-Net is then fine-tuned by optimizing the standard diffusion objective(\u00a73.2.3). (b) After executing the T-step denoising schedule, the resulting latents Zf and Za are decoded by a transparent latent decoder (\u00a73.3).", "description": "This figure illustrates the training and inference processes of the Drip model.  In training (a), input image and trimap are encoded using a VAE, noise is added to the foreground and alpha latent codes, and these are fed to a U-Net for joint generation, guided by a switcher. The U-Net is trained to minimize the standard diffusion objective. In inference (b), after T-step denoising, the generated latent codes are decoded by a transparent latent decoder to produce the final foreground and alpha matte.", "section": "3 Drip"}, {"figure_path": "jz5ZMeN9He/figures/figures_4_1.jpg", "caption": "Figure 3: Demonstration of Cross-Domain Attention(\u00a73.2.2). To enhance mutual guidance and ensure contextual consistency, we instead utilize a cross-domain self-attention mechanism instead of self-attention to associate the foreground and alpha latent.", "description": "This figure illustrates the Cross-Domain Attention mechanism used in the Drip model.  It shows how the model uses a cross-domain self-attention layer (instead of a standard self-attention layer) within the U-Net architecture. This modification allows for improved mutual guidance and consistency between the foreground and alpha latent representations during the joint generation process. The input from a residual block is processed through self-attention, then modified to include both alpha and foreground latents, enabling the cross-domain self-attention mechanism to work before finally being processed by a cross-attention layer.", "section": "3.2 LDM-Based Matting Model"}, {"figure_path": "jz5ZMeN9He/figures/figures_5_1.jpg", "caption": "Figure 4: Structure of Transparent Latent Decoder (\u00a73.3). Due to the non-negligible reconstruction loss introduced by the compression of the LDM's VAE, we employ a transparent latent decoder, which takes the output images of the LDM-based matting model and the corresponding latent as inputs, generating results that are more consistent with the details of the composite image.", "description": "This figure illustrates the architecture of the Transparent Latent Decoder, a component of the Drip model.  The decoder addresses the reconstruction loss from the Latent Diffusion Model's Variational Autoencoder (VAE) by taking the LDM's outputs (foreground and alpha) and their corresponding latent representations as inputs.  It generates refined foreground and alpha predictions that are more aligned with the details of the original composite image, improving overall fidelity.", "section": "3 Drip"}, {"figure_path": "jz5ZMeN9He/figures/figures_6_1.jpg", "caption": "Figure 5: Qualitative Result of Foreground. The visual results compared with FBA [31] on AIM-500[6]. Please zoom in for the best view.", "description": "This figure presents a qualitative comparison of the foreground prediction results between the proposed Drip method and the FBA method [31] on the AIM-500 dataset [6].  It shows three example images where the Drip method outperforms FBA, producing more realistic and detailed foreground predictions, especially in challenging scenarios involving complex textures or semi-transparent objects. The improved quality is visually apparent, with Drip\u2019s results demonstrating greater fidelity and reduced artifacts.", "section": "4.1 Experiment"}, {"figure_path": "jz5ZMeN9He/figures/figures_7_1.jpg", "caption": "Figure 5: Qualitative Result of Foreground. The visual results compared with FBA [31] on AIM-500[6]. Please zoom in for the best view.", "description": "This figure showcases a qualitative comparison of foreground prediction results between the proposed Drip method and the FBA [31] method on the AIM-500 dataset. It presents visual examples of image matting results, comparing the ground truth foreground, the results produced by FBA, and those produced by Drip. The figure aims to demonstrate the superior quality and realism of foreground predictions achieved by Drip compared to FBA, particularly in terms of detail preservation and visual fidelity.", "section": "4.1 Experiment"}, {"figure_path": "jz5ZMeN9He/figures/figures_8_1.jpg", "caption": "Figure 7: Ablation Result. When the transparent latent decoder is not used, the generated output exhibits significant differences in low-level details with the original input.", "description": "This figure presents a qualitative comparison of the results obtained with and without the proposed latent transparency decoder. The top row shows the original composite image and its ground truth matte.  The bottom row shows the results generated by the model without the latent transparency decoder and the full model, respectively. The red boxes highlight regions where the lack of the decoder leads to artifacts and discrepancies in the generated matte, particularly concerning fine details and high-frequency components. This visualization empirically demonstrates the effectiveness of the latent transparency decoder in improving the quality and consistency of the generated matte.", "section": "3 Drip"}, {"figure_path": "jz5ZMeN9He/figures/figures_14_1.jpg", "caption": "Figure 8: Impact of Denoising Steps on Performance. The performance of our method improves as the number of denoising steps increases, with diminishing marginal gains as the number of timesteps becomes larger.", "description": "The figure shows a plot of the SAD (Sum of Absolute Differences) metric against the number of inference steps used during the denoising process.  The x-axis is presented on a logarithmic scale, showing the number of steps (1, 2, 4, 10, 25, 50, 100). The y-axis represents the SAD values. The plot demonstrates that increasing the number of denoising steps generally leads to improved performance (lower SAD), but the rate of improvement diminishes as the number of steps increases. This suggests a point of diminishing returns where additional steps provide minimal benefit.  This is a key result illustrating the efficiency of the proposed method.", "section": "4.3 Ablation Study"}]