{"references": [{"fullname_first_author": "Pieter Abbeel", "paper_title": "Apprenticeship learning via inverse reinforcement learning", "publication_date": "2004-01-01", "reason": "This paper introduces the concept of apprenticeship learning via inverse reinforcement learning, a foundational concept for offline imitation learning with imperfect demonstrations."}, {"fullname_first_author": "Jonathan Ho", "paper_title": "Generative adversarial imitation learning", "publication_date": "2016-01-01", "reason": "This paper introduces Generative Adversarial Imitation Learning (GAIL), a prominent algorithm in imitation learning that addresses the limitations of behavioral cloning by using adversarial training."}, {"fullname_first_author": "Siddharth Reddy", "paper_title": "SQIL: Imitation learning via reinforcement learning with sparse rewards", "publication_date": "2019-01-01", "reason": "This paper introduces Soft Q-Imitation Learning (SQIL), which improves upon behavioral cloning by leveraging reinforcement learning principles and handling sparse rewards effectively."}, {"fullname_first_author": "Divyansh Garg", "paper_title": "IQ-Learn: Inverse soft-q learning for imitation", "publication_date": "2021-01-01", "reason": "This paper presents IQ-Learn, a novel algorithm that transforms the offline imitation learning problem into a convex optimization problem, enhancing scalability and efficiency."}, {"fullname_first_author": "Ilya Kostrikov", "paper_title": "Imitation learning via off-policy distribution matching", "publication_date": "2019-01-01", "reason": "This paper introduces a method for imitation learning that focuses on matching the occupancy distributions of the expert and the learned policies, addressing issues related to distributional shift."}]}