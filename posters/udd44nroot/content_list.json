[{"type": "text", "text": "SPRINQL: Sub-optimal Demonstrations driven Offline Imitation Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Huy Hoang Singapore Management Univerisity mhhoang@smu.edu.sg ", "page_idx": 0}, {"type": "text", "text": "Tien Mai Singapore Management Univerisity atmai@smu.edu.sg ", "page_idx": 0}, {"type": "text", "text": "Pradeep Varakantham Singapore Management Univerisity pradeepv@smu.edu.sg ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We focus on offilne imitation learning (IL), which aims to mimic an expert\u2019s behavior using demonstrations without any interaction with the environment. One of the main challenges in offilne IL is the limited support of expert demonstrations, which typically cover only a small fraction of the state-action space. While it may not be feasible to obtain numerous expert demonstrations, it is often possible to gather a larger set of sub-optimal demonstrations. For example, in treatment optimization problems, there are varying levels of doctor treatments available for different chronic conditions. These range from treatment specialists and experienced general practitioners to less experienced general practitioners. Similarly, when robots are trained to imitate humans in routine tasks, they might learn from individuals with different levels of expertise and efficiency. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we propose an offline IL approach that leverages the larger set of sub-optimal demonstrations while effectively mimicking expert trajectories. Existing offline IL methods based on behavior cloning or distribution matching often face issues such as overfitting to the limited set of expert demonstrations or inadvertently imitating sub-optimal trajectories from the larger dataset. Our approach, which is based on inverse soft-Q learning, learns from both expert and sub-optimal demonstrations. It assigns higher importance (through learned weights) to aligning with expert demonstrations and lower importance to aligning with sub-optimal ones. A key contribution of our approach, called SPRINQL, is transforming the offilne IL problem into a convex optimization over the space of Q functions. Through comprehensive experimental evaluations, we demonstrate that the SPRINQL algorithm achieves state-of-the-art (SOTA) performance on offline IL benchmarks. Code is available at https://github.com/hmhuy0/SPRINQL. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement learning (RL) has established itself as a strong and reliable framework for sequential decision-making with applications in diverse domains: robotics [19, 18], healthcare [34, 28, 27], and environment generation [9, 24]. Unfortunately, RL requires an underlying simulator that can provide rewards for different experiences, which is usually not available. ", "page_idx": 0}, {"type": "text", "text": "Imitation Learning (IL) [16, 29, 13, 17] handles the lack of reward function by utilizing expert demonstrations to guide the learning scheme to compute a good policy. However, IL approaches still require the presence of a simulator that allows for online interactions. Initial works in Offline IL [35, 23, 40, 2] tackle the absence of simulator by considering an offline dataset of expert demonstrations. These approaches extend upon Behavioral Cloning (BC), where we aim to maximize the likelihood of the expert\u2019s decisions from the provided dataset. The key advantage with BC is the theoretical justification on converging to expert behaviors given sufficient trajectories. However, when there are not enough expert trajectories, it often suffers from distributional shift issues [30]. Thus, a key drawback of these initial $\\mathrm{IL}$ approaches is the need for a large number of expert demonstration datasets. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To deal with limited expert demonstrations, recent works utilize non-expert demonstration datasets to reduce the reliance on only expert demonstrations. These additional non-expert demonstrations are referred to as supplementary data. Directly applying BC to these larger supplementary datasets will lead to sub-optimal policies, so most prior work in utilizing supplementary data attempts to extract expert-like demonstrations from the supplementary dataset in order to expand the expert demonstrations [31, 21, 20, 37, 39]. These works assume expert-like demonstrations are present in the supplementary dataset and focus on identifying and utilizing those, while eliminating the non-expert demonstrations. Eliminating non-expert trajectories can result in loss of key information (e.g., transition dynamics) about the environment. Additionally, these works primarily rely on BC, which is known to overlook the sequential nature of decision-making problems \u2013 a small error can quickly accumulate when the learned policy deviates from the states experienced by the expert. ", "page_idx": 1}, {"type": "text", "text": "We develop our algorithm based on an inverse Q-learning framework that better captures the sequential nature of the decision-making [13] and can operate under the more realistic assumption that the data is collected from people/policies with lower expertise levels1 (not experts). To illustrate, consider a scenario in robotic manipulation where the goal is to teach a robot to assemble parts. Expert demonstrations might show precise and efficient methods to assemble parts, but are limited in number due to the high cost and time associated with expert involvement. On the other hand, sub-optimal demonstrations from novice users are easier to obtain and more abundant. Our SPRINQL approach effectively integrates these sub-optimal demonstrations, giving appropriate weight to the expert demonstrations to ensure the robot learns the optimal assembly method without overfitting to the limited expert data or the inaccuracies in the sub-optimal data. We utilize these non-expert trajectories to learn a Q function that contributes to our understanding of the environment and the ground truth reward function. ", "page_idx": 1}, {"type": "text", "text": "Contributions: Overall, we make the following key contributions in this paper: (i) We propose SPRINQL, a novel algorithm based on Q-learning for offilne imitation learning with expert and multiple levels of sub-optimal demonstrations. ", "page_idx": 1}, {"type": "text", "text": "(ii) We provide key theoretical properties of the SPRINQL objective function, which enable the development of a scalable and efficient approach. In particular, we leverage distribution matching and reward regularization to develop an objective function for SPRINQL that not only help address the issue of limited expert samples but also utilizes non-expert data to enhance learning. Our objective function is not only convex within the space of $Q$ functions but also guarantees the return of a $Q$ function that lower-bounds its true value. ", "page_idx": 1}, {"type": "text", "text": "(iii) We provide an extensive empirical evaluation of our approach in comparison to existing best algorithms for offilne IL with sup-optimal demonstrations. Our algorithms provide state-of-the-art (SOTA) performance on all the benchmark problems. Moreover, SPRINQL is able to recover a reward function that shows a high positive correlation with the ground-truth rewards, highlighting a unique advantage of our approach compared to other $\\mathrm{IL}$ algorithms in this context. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Imitation Learning. Imitation learning is recognized as a significant technique for learning from demonstrations. It begins with BC, which aims to maximize the likelihood of expert demonstrations. However, BC often under-performs in practice due to unforeseen scenarios [30]. To overcome this limitation, Generative Adversarial Imitation Learning (GAIL) [16] and Adversarial Inverse Reinforcement Learning (AIRL) [10] have been developed. These methods align the occupancy distributions of the policy and the expert within the Generative Adversarial Network (GAN) framework [14]. Alternatively, Soft Q Imitation Learning (SQIL) [29] bypasses the complexities of adversarial training by assigning a reward of $+1$ to expert demonstrations and 0 to the others, subsequently learning a value function based on these rewards.While the aforementioned imitation learning algorithms show promise, they require interaction with the environment to obtain the policy distribution, which is often impractical. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Offline Imitation Learning. ValueDICE [22] introduces a novel approach for off-policy training, suitable for offilne training, using Stationary Distribution Corrections [25, 26]. However, ValueDICE necessitates adversarial training between the policy network and the $Q$ network, which can make the training slow and unstable. Recently, algorithms like PWIL [8] and IQ-learn [13] have optimized distribution distance, offering an alternative to adversarial training schemes. Since such approaches rely on occupancy distribution matching, a large expert dataset is often required to achieve the desired performance. Our approach, SPRINQL is able to bypass this requirement of a large set of expert demonstrations through the use of non-expert demonstrations (which are typically more available) in conjunction with a small set of expert demonstrations. ", "page_idx": 2}, {"type": "text", "text": "Imitation Learning with Imperfect Demonstrations. T-REX [5] and D-REX [6] have shown that utilizing noise-ranked demonstrations as a reference-based approach can return a better policy without requiring expert demonstrations in online settings. Moreover, there are also several works [36, 33] that utilize the GAN framework [14] for sub-optimal datasets and have achieved several successes. Meanwhile, in the offilne imitation learning context, TRAIL [38] utilizes sup-optimal demonstrations to learn the environment\u2019s dynamics. It employs a feature encoder to map the high-dimensional state-action space into a lower dimension, thereby allowing for a scalable way of learning of dynamics. This approach may face challenges in complex environments where predicting dynamics accurately is difficult, as shown in our experimental results. Other works assume that they can extract expert-like state-action pairs from the sub-optimal demonstration set and use them for BC with importance sampling [31, 37, 39, 21, 20]. However, expert-like state-actions might be difficult to accurately identify, as true reward information is not available. In contrast, our approach is more general, as we do not assume that the sub-optimal set contains expert-like demonstrations. We also allow for the inclusion of demonstrations of various qualities. Moreover, while prior works only recover policies, our approach enables the recovery of both expert policies and rewards, justifying the use of our method for Inverse Reinforcement Learning (IRL)[1]. ", "page_idx": 2}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Preliminaries. We consider a MDP defined by the following tuple $\\mathcal{M}=\\langle S,A,r,P,\\gamma,s_{0}\\rangle$ , where $S$ denotes the set of states, $s_{0}$ represents the initial state set, $A$ is the set of actions, $r:S\\times A\\to\\mathbb{R}$ defines the reward function for each state-action pair, and $P:S\\times A\\to S$ is the transition function, i.e., $P(s^{\\prime}|s,a)$ is the probability of reaching state $s^{\\prime}\\in S$ when action $a\\in A$ is made at state $s\\in S$ , and $\\gamma$ is the discount factor. In reinforcement learning (RL), the aim is to find a policy that maximizes the expected long-term accumulated reward $\\mathrm{max}_{\\pi}$ $\\left\\{\\mathbb{E}_{(s,a)\\sim\\rho_{\\pi}}[r(s,a)]\\right\\}$ , where $\\rho_{\\pi}$ is the occupancy measure of policy $\\begin{array}{r}{\\pi\\colon\\rho_{\\pi}(s,a)=(1-\\gamma)\\pi(a|s)\\sum_{t=1}^{\\infty}\\gamma^{t}P(s_{t}=s|\\pi)}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "MaxEnt IRL The objective in MaxEnt IRL is to recover a reward function $r(s,a)$ from a set of expert demonstrations, $\\bar{\\mathcal{D}}^{E}$ . Let $\\rho^{E}$ be the occupancy measure of the expert policy. The MaxEnt IRL framework [41] proposes to recover the expert reward function by solving ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{r}\\operatorname*{min}_{\\pi}~\\left\\{\\mathbb{E}_{\\rho^{E}}[r(s,a)]-(\\mathbb{E}_{\\rho_{\\pi}}[r(s,a)]-\\mathbb{E}_{\\rho_{\\pi}}[\\log\\pi(s,a)])\\right\\}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Intuitively, the aim is to find a reward function that achieves the highest difference between the expected value of the expert policy and the highest expected value among all other policies (computed through the min loop). ", "page_idx": 2}, {"type": "text", "text": "IQ-Learn Given a reward function $r$ and a policy $\\pi$ , the soft Bellman equation is defined as $\\mathcal{B}_{r}^{\\pi}[Q](s,a)\\;=\\;r(s,a)\\,+\\,\\gamma\\mathbb{E}_{s^{\\prime}}[V^{\\pi}(s^{\\prime})]$ , where $V^{\\pi}(s)\\:=\\:\\mathbb{E}_{a\\sim\\pi(a|s)}[Q(s,a)-\\log\\pi(a|s)]$ . The Bellman equation $B_{r}^{\\pi}[Q]=Q$ is contractive and always yields a unique $\\mathrm{^Q}$ solution [13]. In IQ-learn, they further define an inverse soft-Q Bellman operator $\\bar{T^{\\pi}}[Q]=Q(\\bar{s_{,}}a)-\\gamma\\mathbb{E}_{s^{\\prime}}[V^{\\pi}(s^{\\prime})]$ . [13] show that for any reward function $\\textstyle r(a,s)$ , there is a unique $Q^{*}$ function such that $\\begin{array}{r}{\\mathcal{B}_{r}^{\\pi}[Q^{*}]=Q^{*}}\\end{array}$ , and for a $Q^{*}$ function in the $Q$ -space, there is a unique reward function $r$ such that $r=\\mathcal{T}^{\\pi}[Q^{*}]$ . This result suggests that one can safely transform the objective function of the MaxEnt IRL from $r$ -space to the ", "page_idx": 2}, {"type": "text", "text": "Q-space as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{Q}\\operatorname*{min}_{\\pi}\\ \\Phi(\\pi,Q)=\\mathbb{E}_{\\rho}[\\mathcal{T}^{\\pi}[Q](s,a))]-\\mathbb{E}_{\\rho_{\\pi}}[\\mathcal{T}^{\\pi}[Q](s,a)]+\\mathbb{E}_{\\rho_{\\pi}}[\\log\\pi(s,a)]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which has several advantages; [13] show that $\\Phi(\\pi,Q)$ is convex in $\\pi$ and linear in $Q$ , implying that (2) always yields a unique saddle point solution. In particular, (2) can be converted into a maximization over the Q-space, making the training problem no longer adversarial. ", "page_idx": 3}, {"type": "text", "text": "3 SPRINQL ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now describe our inverse soft-Q learning approach, referred to as SPRINQL (Sub-oPtimal demonstrations driven Reward regularized INverse soft Q Learning). We first describe the three key components in the SPRINQL formulation: ", "page_idx": 3}, {"type": "text", "text": "(1) We formulate the objective function that enables matching the occupancy distribution of not just expert demonstrations, but also sub-optimal demonstrations. ", "page_idx": 3}, {"type": "text", "text": "(2) To mitigate the effect of limited expert samples (and larger sets of sub-optimal samples) that can bias the distribution matching of the first step to sub-optimal demonstrations, we introduce a reward regularization term within the objective. This regularization term is to ensure reward function allocates higher values to state-action pairs that appear in higher expertise demonstrations. ", "page_idx": 3}, {"type": "text", "text": "(3) We show that while this new objective does not have the same advantageous properties as the one in inverse Q-learning [13], with some minor (yet significant) changes it is possible to restore all the important properties. ", "page_idx": 3}, {"type": "text", "text": "3.1 Distribution Matching with Expert and Suboptimal Demonstrations ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We consider a setting where there are demonstrations classified into several sets of different expertise levels $\\mathcal{D}^{1},\\mathcal{D}^{2},....,\\check{\\mathcal{D}}^{N}$ , where $\\mathcal{D}^{1}$ consists of expert demonstrations and all the other sets contains suboptimal ones. This setting is general than existing work in $\\mathrm{IL}$ with sup-optimal demonstrations, which typically assumes that there are only two quality levels: expert and sub-optimal. Let $\\begin{array}{r}{{\\mathcal{D}}=\\bigcup_{i\\in[N]}{\\mathcal{D}}^{i}}\\end{array}$ be the union of all the demonstration sets and $\\rho^{1},...,\\rho^{N}$ be the occupancy measures of the respective expert policies. The ordering of expected values across different levels of expert policies would then be given by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\rho^{1}}[r^{*}(s,a)]>\\mathbb{E}_{\\rho^{2}}[r^{*}(s,a)]>...>\\mathbb{E}_{\\rho^{N}}[r^{*}(s,a)],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $r^{*}(.,.)$ are the ground-truth rewards. Typically, the number of demonstrations in first level, $\\mathcal{D}^{1}$ is significantly lower than those from other expert levels, i.e., $|D^{1}|\\ll|D^{i}|$ , for $i=2,...,N$ The MaxEnt IRL objective from Equation 1 can thus be adapted as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{r}\\operatorname*{min}_{\\pi}~\\sum_{i\\in[N]}w_{i}\\mathbb{E}_{\\rho^{i}}[r(s,a)]-\\mathbb{E}_{\\rho_{\\pi}}[r(s,a)]+\\mathbb{E}_{\\rho_{\\pi}}[\\log\\pi(s,a)]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $w_{i}\\geq0$ is the weight associated with the expert level $i\\in[N]$ and we have $w_{1}>w_{2}>...>w_{N}$ and $\\begin{array}{r}{\\sum_{i\\in[N]}w_{i}\\,=\\,1}\\end{array}$ . There are two key intuitions in the above optimization: (a) Expert level $i$ accumulates higher expected values than expert levels greater than $i$ ; and (b) Difference in values accumulated by expert policies and the maximum of all other policies is maximized. The optimization term can be rewritten as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\rho^{U}}[r(s,a)]-\\mathbb{E}_{\\rho_{\\pi}}[r(s,a)]-\\mathbb{E}_{\\rho_{\\pi}}[\\log\\pi(s,a)],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{\\rho^{U}\\,=\\,\\sum_{i\\in[N]}w_{i}\\rho^{i}}\\end{array}$ . Here we note that the expected reward $\\begin{array}{r}{\\sum_{i\\in[N]}w_{i}\\mathbb{E}_{\\rho^{i}}[r(s,a)]}\\end{array}$ is empirically approximated by samples from the demonstration sets $\\mathcal{D}^{1},\\mathcal{D}^{2},....,\\mathcal{D}^{N}$ . The number of demonstrations in the best demonstration set $\\mathcal{D}^{1}$ (i.e. the set of expert demonstrations) is significantly lower when compared to other demonstration sets. So, an empirical approximation of $\\bar{\\mathbb{E}_{\\rho^{1}}}[r(s,a)]$ using samples from $\\mathcal{D}^{1}$ would be inaccurate. ", "page_idx": 3}, {"type": "text", "text": "3.2 Regularization with Reference Reward ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We create a reference reward based on the provided expert and sub-optimal demonstrations and utilize the reference function to compute a regularization term that is added to the objective of Equation 3. ", "page_idx": 3}, {"type": "text", "text": "Concretely, we define a reference reward function $\\overline{{r}}(s,a)$ such that: ", "page_idx": 4}, {"type": "text", "text": "The aim here is to assign higher rewards to demonstrations from higher expertise levels, and zero rewards to those that do not belong to provided demonstrations. We will discuss how to concretely estimate such reference reward values later. ", "page_idx": 4}, {"type": "text", "text": "We utilize this reference reward as part of the reward regularization term, which is added into the MaxEntIRL objective in (3) as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{r}\\operatorname*{min}_{\\pi}\\Big\\{\\underbrace{\\mathbb{E}_{\\rho^{U}}[r(s,a)]-\\mathbb{E}_{\\rho_{\\pi}}[r(s,a)]+\\mathbb{E}_{\\rho_{\\pi}}[\\log\\pi(s,a)]}_{\\mathrm{Occuparey~matching~}}-\\underbrace{\\alpha\\mathbb{E}_{\\rho^{U}}[(r(s,a)-\\overline{{r}}(s,a))^{2}]}_{\\mathrm{Reward~regulatizer~}}\\Big\\}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\alpha\\,>\\,0$ is a weight parameter for the reward regularizer term. With (4), the goal is to find a policy with an occupancy distribution that matches with the occupancy distribution of different expertise levels appropriately (characterized by the weights, $w_{i}$ ). Simultaneously, it ensures that the learning rewards are close to the pre-assigned rewards, aiming to guide the learning policy towards replicating expert demonstrations, while also learning from sub-optimal demonstrations. ", "page_idx": 4}, {"type": "text", "text": "3.3 Concave Lower-bound on Inverse Soft-Q with Reward Regularizer ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Even though (4) can be directly solved to recover rewards, prior research suggests that transforming (4) into the Q-space will enhance efficiency. We delve into this transformation approach in this section. As discussed in Section 2, there is a one-to-one mapping between any reward function $r$ and a function $Q$ in the Q-space. Thus, the maximin problem in (4) can be equivalently transformed as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{Q}{\\operatorname*{max}}\\underset{\\pi}{\\operatorname*{min}}\\left\\lbrace\\mathcal{H}(Q,\\pi)\\overset{d e f}{=}\\mathbb{E}_{\\rho^{U}}[\\mathcal{T}^{\\pi}[Q](s,a))]-\\mathbb{E}_{\\rho_{\\pi}}[\\mathcal{T}^{\\pi}[Q](s,a))]+\\mathbb{E}_{\\rho_{\\pi}}[\\log\\pi(s,a)]\\right.}\\\\ &{\\left.\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad-\\left.\\alpha\\mathbb{E}_{\\rho^{U}}[(\\mathcal{T}^{\\pi}[Q](s,a))-\\overline{{r}}(s,a))^{2}]\\right\\rbrace}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $r(s,a)$ is replaced by $\\tau^{\\pi}[Q](s,a)$ and ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{T}^{\\pi}[Q](s,a)=Q(s,a)-\\gamma\\mathbb{E}_{s^{\\prime}}[V^{\\pi}(s^{\\prime})],\\,V^{\\pi}(s)=\\mathbb{E}_{a\\sim\\pi(a|s)}[Q(s,a)-\\log\\pi(a|s)]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In the context of single-expert-level, [13] demonstrated that the objective function in the $Q$ -space as given in Equation 2 is concave in $Q$ and convex in $\\pi$ , implying that the maximin problem always has a unique saddle point solution. Unfortunately, this property does not hold in our case. ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.1. $\\mathcal{H}(Q,\\pi)$ (as defined in Equation 5) is concave in $Q$ but is not convex in $\\pi$ . ", "page_idx": 4}, {"type": "text", "text": "In general, we can see that the first and second term of (6) are convex in $Q$ , but the reward regularizer term, which can be written as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\alpha\\mathbb{E}_{\\rho^{U}}\\Big[(Q(s,a)-\\overline{{r}}(s,a)-\\mathbb{E}_{s^{\\prime}\\sim P(.\\vert s,a)}\\mathbb{E}_{a^{\\prime}\\sim\\pi(.\\vert s^{\\prime})}(Q(s^{\\prime},a^{\\prime})-\\log\\pi(s^{\\prime},a^{\\prime})))^{2}\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "is not concave in $\\pi$ (details are shown in the Appendix). The property indicated in Proposition 3.1 implies that the maximin problem within the $Q$ -space $\\mathrm{max}_{Q}\\operatorname*{min}_{\\pi}J(Q,\\pi)$ may not have a unique saddle point solution and would be more challenging to solve, compared to the original inverse IQ-learn problem. ", "page_idx": 4}, {"type": "text", "text": "Another key property of Equation 2 is with regards to the inner minimization problem over $\\pi$ , which yields a unique closed-form solution, enabling the transformation of the max-min problem into a non-adversarial concave maximization problem within the Q-space. The closed-form solution was given by $\\pi^{Q}=\\operatorname{argmax}_{\\pi}\\;V^{\\pi}(s)$ for all $s\\in S$ . Unfortunately, this result also does not hold with the new objective function in (6), as formally stated below: ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.2. $\\mathcal{H}(Q,\\pi)$ may not necessarily be minimized at $\\pi^{*}$ such that $\\pi^{*}=a r g m a x_{\\pi}\\;\\;V^{\\pi}(s),$ , for all $s\\in S$ . ", "page_idx": 4}, {"type": "text", "text": "To overcome the above challenges, our approach involves constructing a more tractable objective function that is a lower bound on the objective of (6). Let us first define $\\mathrm{\\bar{T}}(Q)=\\operatorname*{min}_{\\pi}\\mathcal{H}(Q,\\mathit{\\bar{\\pi}})$ . We then look at the regularization term, which causes all the aforementioned challenges, and write: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\mathcal{T}^{\\pi}[Q](s,a))\\!-\\!\\overline{{r}}(s,a))^{2}=(Q(s,a)-\\overline{{r}}(s,a)-\\mathbb{E}_{s^{\\prime}}[V^{\\pi}(s^{\\prime})])^{2}}\\\\ &{\\qquad\\qquad\\quad=(Q(s,a)-\\overline{{r}}(s,a))^{2}+(\\mathbb{E}_{s^{\\prime}}[V^{\\pi}(s^{\\prime})])^{2}+2(\\overline{{r}}(s,a)-Q(s,a))\\mathbb{E}_{s^{\\prime}}[V^{\\pi}(s^{\\prime})]}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We then take out the negative part of $({\\overline{{r}}}(s,a)-Q(s,a))$ using ReLU, and consider a slightly new objective function as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\widehat{\\mathcal{H}}(Q,\\pi)\\overset{d e f}{=}\\sum_{i\\in[N]}w_{i}\\mathbb{E}_{\\rho^{i}}[T^{\\pi}[Q](s,a))]-(\\mathbb{E}_{\\rho_{\\pi}}[T^{\\pi}[Q](s,a))]-\\mathbb{E}_{\\rho_{\\pi}}[\\log\\pi(s,a)])}}\\\\ {~~}\\\\ {{\\displaystyle-\\,\\alpha\\mathbb{E}_{\\rho^{U}}\\left[(Q(s,a)-\\bar{r}(s,a))^{2}+(\\mathbb{E}_{s^{\\prime}}V^{\\pi}(s^{\\prime}))^{2}+2\\mathrm{ReLU}(\\bar{r}(s,a)-Q(s,a))\\mathbb{E}_{s^{\\prime}}V^{\\pi}(s^{\\prime})\\right]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Let $\\widehat{\\Gamma}(Q)=\\operatorname*{min}_{\\pi}\\widehat{\\mathcal{H}}(Q,\\pi))$ . The proposition below shows that $\\widehat\\Gamma(Q)$ always lower-bounds $\\Gamma(Q)$ . ", "page_idx": 5}, {"type": "text", "text": "Proposition 3.3. For any $Q\\ge0$ , we have $\\widehat\\Gamma(Q)\\leq\\Gamma(Q)$ and $\\operatorname*{max}_{Q}\\widehat\\Gamma(Q)\\leq\\operatorname*{max}_{Q}\\Gamma(Q)$ . Moreover, $\\Gamma(Q)=\\widehat\\Gamma(Q)\\;i f Q(s,a)\\leq\\overline{{{r}}}(s,a)$ for all $(s,a)$ . ", "page_idx": 5}, {"type": "text", "text": "We note that assuming $Q\\geq0$ is not restrictive, as if the expert\u2019s rewards $r^{*}(s,a)$ are non-negative (typically the case), then the true soft-Q function, defined as $\\begin{array}{r}{Q^{*}(s,a)=\\mathbb{E}[\\sum_{s_{t},a_{t}}\\gamma^{t}(r^{*}(s,a)-}\\end{array}$ $\\log\\pi(s,a))|(s_{0},a_{0})=(s,a)]$ , should also be non-negative, for any $\\pi$ . As $\\widehat\\Gamma(Q)$ provides a lowerbound approximation of $\\Gamma(Q)$ , maximizing $\\widehat\\Gamma(Q)$ over the $Q$ -space would drive $\\Gamma(Q)$ towards its maximum value. It is important to note that , given that the inner problem involves minimization, obtaining an upper-bound approximation function is easier. However, since the outer problem is a maximization one, an upper bound would not be helpful in guiding the resulting solution towards optimal ones. The following theorem indicates that $\\widehat\\Gamma(Q)$ is more tractable to use. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.4. For any $Q\\,\\geq\\,0,$ , the following results hold: $(i)$ The inner minimization problem $\\operatorname*{min}_{\\pi}\\widehat{\\mathcal{H}}(Q,\\pi)$ has a unique optimal solution $\\pi^{Q}$ such that $\\pi^{Q}=a r g m i n_{\\pi}V^{\\pi}(s)$ for all $s\\in S$ and $\\begin{array}{r}{\\pi^{Q}(a|s)=\\frac{\\exp(Q(s,a))}{\\sum_{a}\\exp(Q(s,a))}}\\end{array}$ , $\\begin{array}{r}{(i i)\\operatorname*{max}_{\\pi}V^{\\pi}(s)=\\log(\\sum_{a}\\exp(Q(s,a)))\\stackrel{d e f}{=}V^{Q}(s),}\\end{array}$ , and (iii) $\\widehat\\Gamma(Q)$ is concave for $Q\\geq0$ . ", "page_idx": 5}, {"type": "text", "text": "The above theorem tells us that new objective $\\widehat\\Gamma(Q)$ has a closed form where $V^{\\pi}(s)$ is replaced by $V^{Q}(s)$ . Moreover $\\widehat\\Gamma(Q)$ is concave for all $Q\\ge0$ . The concavity is particularly advantageous, as it guarantees that th e optimization objective is well-behaved and has a unique solution $Q^{*}$ such that $(Q^{*},\\pi^{Q^{*}})$ form a unique saddle point of $\\operatorname*{max}_{Q}\\operatorname*{min}_{\\pi}$ ${\\widehat{\\mathcal{H}}}(Q,\\pi)$ . Thus, our tractable objective has all the nice properties that the original $\\b{I Q}$ -Learn objective had, while being able to work for the offline case with multiple levels of expert trajectories and our reward regularizer. ", "page_idx": 5}, {"type": "text", "text": "3.4 SPRINQL Algorithm", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Algorithm 1 provides the overall SPRINQL algorithm. We first estimate the reference rewards in lines 2-6 of Algorithm 1 and the overall process is described in Section 3.4.1. Before we proceed to the overall training, we have to estimate the weights, $w_{i}$ (associated with the ranked demonstration sets) employed in ${\\hat{\\mathcal{H}}}(Q,\\pi)$ . We provide a description of this estimation procedure in Section 3.4.2. Finally, to enhance stability and mitigate over-estimation issues commonly encountered in offilne Q-learning, we employ a conservative version of ${\\hat{\\mathcal{H}}}(Q,\\pi)$ in lines 8-15 of the algorithm and is described in Section 3.4.3. Some other practical considerations are discussed in the appendix. ", "page_idx": 5}, {"type": "table", "img_path": "uDD44NROOt/tmp/277a75a648ab7e73dc5ea746dc2049492692399b600379e2130648588589f433.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "3.4.1 Estimating the Reference Reward ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We outline our approach to automatically infer the reference rewards $\\overline{{r}}(s,a)$ from the ranked demonstrations. The general idea is to learn a function that assigns higher values to higher expert-level demonstrations. To achieve this, let us define $\\begin{array}{r}{R(\\tau)=\\sum_{(s,a)\\in\\tau}^{}\\overline{{r}}(s,a)}\\end{array}$ (i.e., accumulated reward of trajectory $\\tau$ ). For two trajectories $\\tau_{i},\\ \\tau_{j}$ , let $\\tau_{i}\\mathrm{~\\ensuremath~{~\\prec~}~}\\tau_{j}$ denote that $\\tau_{i}$ is lower in quality compared to $\\tau_{j}$ (i.e., $\\tau_{i}$ belongs to demonstrations from lower-expert policies, compared to $\\tau_{j}$ ). We follow the Bradley-Terry model of preferences [4, 5] to model the probability $P(\\tau_{i}\\,\\prec\\,\\tau_{j})$ as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\bar{r}}\\{\\mathcal{L}(\\bar{r})=\\sum_{i\\in[N]}\\sum_{(s,a),(s^{\\prime},a^{\\prime})\\in\\mathcal{D}^{i}}(\\bar{r}(s,a)-\\bar{r}(s^{\\prime},a^{\\prime}))^{2}-\\sum_{\\substack{h,k\\in[N],h>k,\\tau_{i}\\in\\mathcal{D}^{h},\\tau_{j}\\in\\mathcal{D}^{k}}}\\ln P(\\tau_{i}\\prec\\tau_{j})\\}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the first term of ${\\mathcal{L}}({\\overline{{r}}})$ serves to guarantee that the reward reference values for $(s,a)$ pairs within the same demonstration group are similar, and the second term aims to increase the likelihood that the accumulated rewards of trajectories adhere to the expert-level order. Importantly, it can be shown below that ${\\mathcal{L}}({\\overline{{r}}})$ is convex in $\\overline{r}$ (Proposition 3.5), making the learning well-behaved. In practice, one can model $\\overline{{r}}(s,a)$ by a neural network of parameters $\\theta$ and optimize ${\\mathcal{L}}(\\theta)$ over $\\theta$ -space. ", "page_idx": 6}, {"type": "text", "text": "Proposition 3.5. ${\\mathcal{L}}({\\overline{{r}}})$ is strictly convex in $\\overline{r}$ . ", "page_idx": 6}, {"type": "text", "text": "3.4.2 Preference-based Weight Learning for $w_{i}$ ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Each weight parameter $w_{i}$ used in (4) should reflect the quality of the corresponding demonstration set $\\mathcal{D}^{i}$ , which can be evaluated by estimating the average expert rewards of these sets. Although this information is not directly available in our setting, the reward reference values discussed earlier provide a convenient and natural way to estimate them. This leads to the following formulation for inferring the weights wi from the ranked data: wi =  j\u2208E[(Ns], aE)(\u223cs,Da)i\u223cDj [r\u00af(s,a)]. ", "page_idx": 6}, {"type": "text", "text": "3.4.3 Conservative soft-Q learning ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Over-estimation is a common issue in offline Q-learning due to out-of-distribution actions and function approximation errors [11]. We also observe this in our $\\mathrm{IL}$ context. To overcome this issue and enhance stability, we leverage the approach in [23] to enhance our inverse soft-Q learning. The aim is to learn a conservative soft-Q function that lower-bounds its true value. We formulate the conservative inverse soft- $Q$ objective as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{H}}^{C}(Q,\\pi)=-\\beta\\sum_{\\substack{s\\sim\\mathcal{D},\\,\\,a\\sim\\mu(a|s)}}[Q(s,a)]+\\widehat{\\mathcal{H}}(Q,\\pi)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mu(a|s)$ is a particular state-action distribution. We note that in (8), the conservative term is added to the objective function, while in the conservative Q-learning algorithm [23], this term is added to the Bellman error objective of each Q-function update. This difference makes the theory developed in [23] not applicable. In Proposition 3.6 below, we show that solving maxQ $\\widehat{\\mathcal{H}}^{C}(Q)$ will always yield a Q-function that is a lower bound to the Q function obtained by solving maxQ ${\\widehat{\\mathcal{H}}}(Q,\\pi)$ . Proposition 3.6. Let $\\begin{array}{l l l}{\\widehat{Q}}&{=}&{\\displaystyle{a r g m a x_{Q}\\widehat{\\mathcal{H}}(Q,\\pi)}}\\end{array}$ and $\\widehat{Q}^{C}\\;\\;=\\;\\;a r g m a x_{Q}\\widehat{\\mathcal{H}}^{C}(Q,\\pi),$ , we have $\\begin{array}{r}{\\sum_{\\stackrel{s\\sim p}{a\\sim\\mu(a|s)}}\\widehat{Q}^{C}(s,a)\\leq\\sum_{\\stackrel{s\\sim\\infty}{a\\sim\\mu(a|s)}}\\widehat{Q}(s,a)}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "We can adjust the scale parameter $\\beta$ in Equation 8 to regulate the conservativeness of the objective. Intuitively, if we optimize $Q$ over a lower-bounded $\\mathrm{^Q}$ -space, increasing the scale parameter $\\beta$ will force each $Q(s,a)$ towards its lower bound. Consequently, when $\\beta$ is sufficiently large, ${\\widehat{Q}}^{C}$ will point-wise lower-bound $\\widehat{Q}$ , i.e., $\\widehat{Q}^{C}(s,a)\\leq\\widehat{Q}(s,a)$ for all $(s,a)$ . ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Experiment Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Baselines. We compare our SPRINQL with SOTA algorithms in offline $\\mathrm{IL}$ with sub-optimal demonstrations: TRAIL [38], DemoDICE [21], and DWBC [37]. Moreover, we also compare with other straightforward baselines: BC with only sub-optimal datasets (BC-O), BC with only expert data (BC-E), BC with all datasets (BC-both), and BC with fixed weight for each dataset (W-BC), SQIL [29], and IQ-learn [13] with only expert data. In particular, since TRAIL, DemoDICE, and DWBC were designed to work with only two datasets (one expert and one supplementary), in problems with multiple sub-optimal datasets, we combine all the sub-optimal datasets into one single supplementary dataset. Meanwhile, BC-E, BC-O, and BC-both combine all available data into a large dataset for learning, while W-BC optimizes $\\begin{array}{r}{\\sum_{i}\\mathbb{E}_{s,a\\sim\\mathcal{D}^{i}-w_{i}}\\ln(\\pi(a|s))}\\end{array}$ , where $w_{i}$ are our weight parameters. T-REX [5] is a PPO-based IL algorithm that can work with ranked demonstrations. It is, however, not suitable for our offline setting, so we do not include it in the main comparisons. Nevertheless, we will later conduct an ablation study to compare SPRINQL with an adapted version of T-REX. Moreover, [15] developed IPL for learning from offline preference data. This approach requires comparisons between every pair of trajectories, thus is not suitable for our context. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Environments and Data Generation: We test on five Mujoco tasks [32] and four arm manipulation tasks from Panda-gym [12]. The maximum number transition of $(s,a)$ per trajectory is 1000 (or 1k for short) for the Mujoco and is 50 for Panda-gym tasks (descriptions in Appendix B.3). The sub-optimal demonstrations have been generated by randomly adding noise to expert actions and interacting with the environments. We generated large datasets of expert and non-expert demonstrations. For each seed, we randomly sample subsets of demonstrations for testing. This approach allows us to test with different datasets across seeds, rather than using fixed datasets for all seeds as in previous works. More details of these generated databases can be found in the Appendix B.4. ", "page_idx": 7}, {"type": "text", "text": "Metric: The return is normalized by score = E.rreettuurrnn\u2212\u2212R.Rr.erteutrunrn \u00d7 100 where R.return is the mean return of random policy and E.return is the mean return of the expert policy. The scores are calculated by taking the last ten evaluation scores of each seed, with five seeds per report. ", "page_idx": 7}, {"type": "text", "text": "Experimental Concerns. Throughout the experiments, we aim to answer the following questions: (Q1) How does SPRINQL perform compared to other baselines? (Q2) How do the distribution matching and reward regularization terms impact the performance of SPRINQL? (Q3) What happens if we augment (or reduce) the expert data while maintaining the sub-optimal datasets? (Q4) What happens if we augment (or reduce) the sub-optimal data while maintaining the expert dataset? (Q5) How does the conservative term help in our approach? (Q6) How does increasing $N$ (the number of expertise levels) affect the performance of SPRINQL? (Q7) Does the preference-based weight learning approach provide good values for the weights $w_{i}$ ? (Q8) How does SPRINQL perform in recovering the ground-truth reward function? ", "page_idx": 7}, {"type": "text", "text": "4.2 Main Comparison Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we provide comparison results to answer (Q1) with three datasets (i.e., $N=3$ ). Additional comparison results for $N=2$ can be found in the appendix. From lowest to highest expertise levels, we randomly sample $(25\\mathbf{k}{-}10\\mathbf{k}{-}1\\mathbf{k})$ transitions for Mujoco tasks and (10k-5k-100) transitions for Panda-gym tasks for every seed (details of these three-level dataset are provided in Appendix B.4). Table 1 shows comparison results across 3 Mujoco tasks and 3 Panda-gym tasks (the full results for all the nine environments are provided in Appendix C.1). In general, SPRINQL significantly outperforms other baselines on all the tasks. ", "page_idx": 7}, {"type": "table", "img_path": "uDD44NROOt/tmp/188fbc1bed7700d433f5ad60e31f801f59c5d9d436c5a5a5041875ae64a97eb9.jpg", "table_caption": [], "table_footnote": ["Table 1: Comparison results for three Mujoco and three Panda-gym tasks. "], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.3 Ablation Study - No Distribution Matching and No Reward Regularizer ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We aim to assess the importance of the distribution matching and reward regularizer terms in our objective (Q2). To this end, we conduct an ablation study comparing SPRINQL with two variants: (i) noReg-SPRINQL, derived by removing the reward regularizer term from (6), and (ii) noDMSPRINQL, obtained by removing the distribution matching term from (6). Here, we note that the noDM-SPRINQL performs Q-learning using the reward reference function, this can viewed as an adaption of the T-REX algorithm [5] to our offline setting. The conservative Q-learning term is employed in the SPRINQL and the two variants to enhance stability. The comparisons for $N=2$ and $N=3$ on five Mujoco tasks are shown in Figure 1 (the full comparison results for all tasks are provided in the appendix). These results clearly show that SPRINQL outperforms the other variants, indicating the value of both terms in our objective function. ", "page_idx": 8}, {"type": "image", "img_path": "uDD44NROOt/tmp/6edfa6508f900ac2edf2b9cfd17afd708050ed873c364c1a42a72e4d63d46c5c.jpg", "img_caption": ["Figure 1: Comparison of three variants of SPRINQL across five Mujoco environments. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.4 Other Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Experiments addressing the other questions are provided in the appendix. Specifically, Sections C.1 and C.2 provide full comparison results for all the Mujoco and Panda-gym tasks for three and two datasets (i.e., $N\\,=\\,3$ and $N\\,=\\,2$ ), complementing the answer to Q1. Section C.3 provides the learning curves of the three variants considered in Section 4.3 above (answering Q2). Section C.4 provides experiments to answer Q3 (what would happen if we augment the expert dataset?) and Section C.5 addresses Q4 (what would happen if we augment the sub-optimal dataset?). Section C.6 experimentally shows the impact of the conservative term in our approaches (i.e., Q5). Section C.7 reports the performance of SPRINQL with varying numbers of expertise levels $N$ (i.e., Q6). Section C.8 addresses Q7, and Section C.10 shows how SPRINQL performs in terms of reward recovering (i.e. Q8). In addition, Section C.11 reports the distributions of the reference rewards and Section C.12 provides $\\alpha$ choosing range. ", "page_idx": 8}, {"type": "text", "text": "Concretely, our extensive experiments reveal the following: (i) SPRINQL outperforms other baselines with two, three, or even larger numbers of datasets; (ii) the conservative term, distribution matching, and reward regularizer terms are essential to our objective\u2014all three significantly contribute to the success of SPRINQL; (iii) the preference-based weight learning provides good estimates for the weights $w_{i}$ ; and (iv) SPRINQL performs well in recovering rewards, showing a high positive correlation with the ground-truth rewards, justifying the use of our method for IRL. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion and Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "(Conclusion) We have developed SPRINQL, a novel non-adversarial inverse soft-Q learning algorithm for offline imitation learning from expert and sub-optimal demonstrations. We have demonstrated that our algorithm possesses several favorable properties, contributing to its well-behaved, stable, and scalable nature. Additionally, we have devised a preference-based loss function to automate the estimation of reward reference values. We have provided extensive experiments based on several benchmark tasks, demonstrating the ability of our SPRINQL algorithm to leverage both expert and non-expert data to achieve superior performance compared to state-of-the-art algorithms. ", "page_idx": 8}, {"type": "text", "text": "(Limitations) Some limitations of this work include: (i) SPRINQL (and other baselines) still requires a large amount of sub-optimal datasets with well-identified expertise levels to learn effectively, (ii) there is a lack of theoretical investigation on how the sizes of the expert and non-expert datasets affect the performance of Q-learning, which we find challenging to address, and (iii) it lacks a theoretical exploration of how the reward regularizer term enhances the distribution matching term when expert samples are low\u2014this question is relevant and interesting but also challenging to address. These limitations will pave the way for our future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research is supported by the National Research Foundation Singapore and DSO National Laboratories under the Al Singapore Programme (AISG Award No: AISG2-RP-2020-016) and Lee Kuan Yew Fellowship awarded to Pradeep Varakantham. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-first international conference on Machine learning, page 1, 2004.   \n[2] Anurag Ajay, Yilun Du, Abhi Gupta, Joshua B Tenenbaum, Tommi S Jaakkola, and Pulkit Agrawal. Is conditional generative modeling all you need for decision making? In The Eleventh International Conference on Learning Representations, 2022.   \n[3] Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.   \n[4] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324\u2013345, 1952.   \n[5] Daniel Brown, Wonjoon Goo, Prabhat Nagarajan, and Scott Niekum. Extrapolating beyond suboptimal demonstrations via inverse reinforcement learning from observations. In International conference on machine learning, pages 783\u2013792. PMLR, 2019.   \n[6] Daniel S Brown, Wonjoon Goo, and Scott Niekum. Better-than-demonstrator imitation learning via automatically-ranked demonstrations. In Conference on robot learning, pages 330\u2013359. PMLR, 2020.   \n[7] Letian Chen, Rohan Paleja, and Matthew Gombolay. Learning from suboptimal demonstration via self-supervised reward regression. In Conference on robot learning, pages 1262\u20131277. PMLR, 2021.   \n[8] Robert Dadashi, L\u00e9onard Hussenot, Matthieu Geist, and Olivier Pietquin. Primal wasserstein imitation learning. In ICLR, 2021.   \n[9] Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew Critch, and Sergey Levine. Emergent complexity and zero-shot transfer via unsupervised environment design. Advances in neural information processing systems, 33:13049\u201313061, 2020.   \n[10] Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforcement learning. arXiv preprint arXiv:1710.11248, 2017.   \n[11] Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In International conference on machine learning, pages 1587\u20131596. PMLR, 2018.   \n[12] Quentin Gallou\u00e9dec, Nicolas Cazin, Emmanuel Dellandr\u00e9a, and Liming Chen. pandagym: Open-source goal-conditioned environments for robotic learning. arXiv preprint arXiv:2106.13687, 2021.   \n[13] Divyansh Garg, Shuvam Chakraborty, Chris Cundy, Jiaming Song, and Stefano Ermon. Iq-learn: Inverse soft-q learning for imitation. Advances in Neural Information Processing Systems, 34:4028\u20134039, 2021.   \n[14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014.   \n[15] Joey Hejna and Dorsa Sadigh. Inverse preference learning: Preference-based rl without a reward function. Advances in Neural Information Processing Systems, 36, 2023.   \n[16] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in neural information processing systems, 29, 2016.   \n[17] Huy Hoang, Tien Mai, and Pradeep Varakantham. Imitate the good and avoid the bad: An incremental approach to safe reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 12439\u201312447, 2024.   \n[18] Minh-Huy Hoang, Long Dinh, and Hai Nguyen. Learning from pixels with expert observations. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1200\u20131206, 2023.   \n[19] Ozsel Kilinc and Giovanni Montana. Reinforcement learning for robotic manipulation using simulated locomotion demonstrations. Machine Learning, pages 1\u201322, 2022.   \n[20] Geon-Hyeong Kim, Jongmin Lee, Youngsoo Jang, Hongseok Yang, and Kee-Eung Kim. Lobsdice: Offline learning from observation via stationary distribution correction estimation. Advances in Neural Information Processing Systems, 35:8252\u20138264, 2022.   \n[21] Geon-Hyeong Kim, Seokin Seo, Jongmin Lee, Wonseok Jeon, HyeongJoo Hwang, Hongseok Yang, and Kee-Eung Kim. Demodice: Offilne imitation learning with supplementary imperfect demonstrations. In International Conference on Learning Representations, 2021.   \n[22] Ilya Kostrikov, Ofir Nachum, and Jonathan Tompson. Imitation learning via off-policy distribution matching. In International Conference on Learning Representations, 2019.   \n[23] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offilne reinforcement learning. Advances in Neural Information Processing Systems, 33:1179\u2013 1191, 2020.   \n[24] Wenjun Li and Pradeep Varakantham. Generalization through diversity: Improving unsupervised environment design. International Joint Conference on Artificial Intelligence, 2023.   \n[25] Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections. Advances in neural information processing systems, 32, 2019.   \n[26] Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice: Policy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019.   \n[27] Mila Nambiar, Supriyo Ghosh, Priscilla Ong, Yu En Chan, Yong Mong Bee, and Pavitra Krishnaswamy. Deep offline reinforcement learning for real-world treatment optimization applications. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 4673\u20134684, 2023.   \n[28] Aniruddh Raghu, Matthieu Komorowski, Imran Ahmed, Leo Celi, Peter Szolovits, and Marzyeh Ghassemi. Deep reinforcement learning for sepsis treatment. arXiv preprint arXiv:1711.09602, 2017.   \n[29] Siddharth Reddy, Anca D Dragan, and Sergey Levine. Sqil: Imitation learning via reinforcement learning with sparse rewards. arXiv preprint arXiv:1905.11108, 2019.   \n[30] St\u00e9phane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 627\u2013635. JMLR Workshop and Conference Proceedings, 2011.   \n[31] Fumihiro Sasaki and Ryota Yamashina. Behavioral cloning from noisy demonstrations. In International Conference on Learning Representations, 2020.   \n[32] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ international conference on intelligent robots and systems, pages 5026\u20135033. IEEE, 2012.   \n[33] Yunke Wang, Chang Xu, Bo Du, and Honglak Lee. Learning to weight imperfect demonstrations. In International Conference on Machine Learning, pages 10961\u201310970. PMLR, 2021.   \n[34] Wei-Hung Weng, Mingwu Gao, Ze He, Susu Yan, and Peter Szolovits. Representation and reinforcement learning for personalized glycemic control in septic patients. arXiv preprint arXiv:1712.00654, 2017.   \n[35] Yifan Wu, G. Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning. ArXiv, abs/1911.11361, 2019.   \n[36] Yueh-Hua Wu, Nontawat Charoenphakdee, Han Bao, Voot Tangkaratt, and Masashi Sugiyama. Imitation learning from imperfect demonstration. In International Conference on Machine Learning, pages 6818\u20136827. PMLR, 2019.   \n[37] Haoran Xu, Xianyuan Zhan, Honglei Yin, and Huiling Qin. Discriminator-weighted offline imitation learning from suboptimal demonstrations. In Proceedings of the 39th International Conference on Machine Learning, pages 24725\u201324742, 2022.   \n[38] Mengjiao Yang, Sergey Levine, and Ofir Nachum. Trail: Near-optimal imitation learning with suboptimal data. In International Conference on Learning Representations, 2021.   \n[39] Lantao Yu, Tianhe Yu, Jiaming Song, Willie Neiswanger, and Stefano Ermon. Offilne imitation learning with suboptimal demonstrations via relaxed distribution matching. In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 11016\u201311024, 2023.   \n[40] Chi Zhang, Sanmukh Rao Kuppannagari, and Viktor K. Prasanna. Brac+: Improved behavior regularized actor critic for offline reinforcement learning. ArXiv, abs/2110.00894, 2021.   \n[41] Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entropy inverse reinforcement learning. In Aaai, volume 8, pages 1433\u20131438. Chicago, IL, USA, 2008. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Missing Proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We provide proofs for the theoritical results claimed in the main paper. ", "page_idx": 12}, {"type": "text", "text": "Proposition 3.1 $J(Q,\\pi)$ is concave in $Q$ but is not convex in $\\pi$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. We recall that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J(Q,\\pi)=\\displaystyle\\sum_{i\\in[N]}w_{i}\\mathbb{E}_{\\rho^{i}}[\\mathcal{T}^{\\pi}[Q](s,a))]-\\mathbb{E}_{\\rho_{\\pi}}[\\mathcal{T}^{\\pi}[Q](s,a))]-\\mathbb{E}_{\\rho_{\\pi}}[\\log\\pi(s,a)]}\\\\ &{\\quad\\quad\\quad\\quad-\\alpha\\mathbb{E}_{\\rho^{U}}[(\\mathcal{T}^{\\pi}[Q](s,a))-\\bar{r}(s,a))^{2}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\mathcal{T}^{\\pi}[Q](s,a))\\;=\\;Q(s,a)\\;-\\;\\gamma\\mathbb{E}_{s^{\\prime}\\sim P(s^{\\prime}\\mid s,a)}[V^{\\pi}(s^{\\prime})]$ and $V^{\\pi}(s)~=~\\mathbb{E}_{a\\sim\\pi(a|s)}[Q(s,a)~-$ $\\log\\pi(a|s)]$ . We see that $\\tau^{\\pi}[Q](s,a)$ is linear in $Q$ for any $(s,a)$ . Thus, the first and second terms of ${\\dot{J}}(Q,\\pi)$ in (9) are linear in $Q$ . The last term of (9) involves a sum of squares of linear functions of $Q$ , which are convex. So, $J(Q,\\pi)$ is concave in $Q$ . ", "page_idx": 12}, {"type": "text", "text": "To see that ${\\cal J}(Q,\\pi)$ is generally not convex in $\\pi$ , we will consider a quadratic component of the reward regularization term $(T^{\\pi}[Q](s,a))-\\overline{{{r}}}(s,a))^{2}$ and show that there is an instance of $Q$ and $\\overline{r}$ values that makes this term convex. We first write: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{r^{\\pi}[Q](s,a))-\\bar{r}(s,a)=Q(s,a)-\\displaystyle\\gamma\\sum_{s^{\\prime}}P(s^{\\prime}|s,a)V^{\\pi}(s^{\\prime})-\\bar{r}(s,a)}}\\\\ {{=Q(s,a)-\\displaystyle\\gamma\\sum_{s^{\\prime}}P(s^{\\prime}|s,a)\\sum_{a^{\\prime\\prime}}\\pi(a^{\\prime\\prime}|s^{\\prime})(Q(s^{\\prime},a^{\\prime\\prime})-\\log\\pi(a^{\\prime\\prime}|s^{\\prime}))-\\bar{r}(s,a)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "For simplification, let us choose $Q(s^{\\prime},a^{\\prime\\prime})=0$ for all $s^{\\prime}$ such that $P(s^{\\prime}|s,a)>0$ . This allows us to simplify $\\mathcal{T}^{\\pi}[Q](s,a))-\\overline{{{r}}}(s,a)$ as ", "page_idx": 12}, {"type": "equation", "text": "$$\n{\\mathcal{T}}^{\\pi}[Q](s,a))-{\\overline{{r}}}(s,a)=\\gamma\\sum_{s^{\\prime}}P(s^{\\prime}|s,a)\\sum_{a^{\\prime\\prime}}\\pi(a^{\\prime\\prime}|s^{\\prime})\\log\\pi(a^{\\prime\\prime}|s^{\\prime})+Q(s,a)-{\\overline{{r}}}(s,a)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We further see that, for any $s\\in S,\\textstyle\\sum_{a\\in A}\\pi(a|s)\\log\\pi(a|s)$ achieves its minimum value at $\\pi(a|s)=$ $1/|A|$ for all $a\\in|A|$ , and $\\begin{array}{r}{\\sum_{a\\in A}\\pi(a|s)\\log\\pi(a|s)\\geq\\log1/|A|}\\end{array}$ for any policy $\\pi$ . As a result we have: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\gamma\\sum_{s^{\\prime}}P(s^{\\prime}|s,a)\\sum_{a^{\\prime\\prime}}\\pi(a^{\\prime\\prime}|s^{\\prime})\\log\\pi(a^{\\prime\\prime}|s^{\\prime})\\geq\\gamma\\log\\frac{1}{|A|}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "So if we select $Q(s,a)$ such that $Q(s,a)-\\overline{{r}}(s,a)-\\gamma\\log|A|\\geq0$ , then ${\\cal T}^{\\pi}[Q](s,a))-{\\bar{r}}(s,a)\\geq0$ for any $\\pi$ . Now we consider the quadratic function $\\Gamma(\\pi)=\\left(\\lambda(\\pi)\\right)^{2}$ where $\\lambda(\\pi)=\\mathcal{T}^{\\pi}[Q](s,a))\\:-\\:$ $\\overline{{r}}(s,a)$ . Since each term $\\pi(a^{\\prime\\prime})\\log\\pi(a^{\\prime\\prime}|s^{\\prime})$ is convex in $\\pi$ , $\\lambda(\\pi)$ is convex in $\\pi$ . To show $\\Gamma(\\pi)$ is convex in $\\pi$ , we will show that for any two policies $\\pi_{1},\\,\\pi_{2}$ and $\\in[0,1],\\,\\Gamma(\\alpha\\pi_{1}+(1-\\alpha)\\bar{\\pi}_{2})\\leq$ $\\alpha\\Gamma(\\pi_{1})+(1-\\alpha)\\Gamma(\\pi)$ . To this end, we write ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha\\Gamma(\\pi_{1})+(1-\\alpha)\\Gamma(\\pi)\\overset{(a)}{\\geq}\\left(\\alpha\\lambda(\\pi_{1})+(1-\\alpha)\\lambda(\\pi_{2})\\right)^{2}}\\\\ &{\\overset{(b)}{\\geq}\\left(\\lambda(\\alpha\\pi_{1}+(1-\\alpha)\\pi_{2})\\right)^{2}}\\\\ &{=\\Gamma(\\alpha\\pi_{1}+(1-\\alpha)\\pi_{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $(a)$ is because the function $h(t)=t^{2}$ is convex in $t$ , and $(b)$ is because ", "page_idx": 12}, {"type": "text", "text": "(i) $\\alpha\\lambda(\\pi_{1})+(1-\\alpha)\\lambda(\\pi_{2})\\geq\\lambda(\\alpha\\pi_{1}+(1-\\alpha)\\pi_{2})\\;({\\mathrm{as~}}\\lambda(\\pi){\\mathrm{~is~convex~in~}}\\pi)$ (ii) $\\alpha\\lambda(\\pi_{1})+(1-\\alpha)\\lambda(\\pi_{2})$ and $\\lambda(\\alpha\\pi_{1}+(1-\\alpha)\\pi_{2})$ are both non-negative, and function $h(t)=t^{2}$ is increasing for all $t\\geq0$ . ", "page_idx": 12}, {"type": "text", "text": "So, we see that with the $Q$ values chosen above, function $({\\mathcal{T}}^{\\pi}[Q](s,a))-{\\bar{r}}(s,a))^{2})$ is convex and $-\\alpha(T^{\\pi}[Q](s,a)-{\\bar{r}}(s,a))^{2}$ is concave. So, intuitively, when $\\alpha$ is sufficiently large, ${\\cal J}(Q,\\pi)$ would be almost concave (so not convex), which is the desired result. ", "page_idx": 12}, {"type": "text", "text": "Proposition 3.2 ${\\cal J}(Q,\\pi)$ may not necessarily be minimized at $\\pi_{Q}$ such that $\\pi_{Q}=a r g m a x_{\\pi}\\;\\;V^{\\pi}(s),$ , for all $s\\in S$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. We first write ${\\cal J}(Q,\\pi)$ as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{J(Q,\\pi)=\\sum_{i\\in[N]}w_{i}\\mathbb{E}_{\\rho^{i}}[\\mathcal{T}^{\\pi}[Q](s,a))]-\\mathbb{E}_{\\rho_{\\pi}}[\\mathcal{T}^{\\pi}[Q](s,a))]-\\mathbb{E}_{\\rho_{\\pi}}[\\log\\pi(s,a)]}}\\\\ &{\\quad\\quad\\quad-\\alpha\\mathbb{E}_{\\rho^{U}}[(\\mathcal{T}^{\\pi}[Q](s,a))-\\overline{{r}}(s,a))^{2}]}\\\\ &{\\quad\\quad=\\displaystyle\\sum_{i\\in[N]}w_{i}\\mathbb{E}_{\\rho^{i}}[Q(s,a)-\\gamma\\mathbb{E}_{s^{\\prime}}V^{\\pi}(s^{\\prime})]-(1-\\gamma)\\mathbb{E}_{s_{0}}V^{\\pi}(s_{0})}\\\\ &{\\quad\\quad\\quad-\\alpha\\mathbb{E}_{\\rho^{U}}[(Q(s,a)-\\overline{{r}}(s,a)-\\gamma\\mathbb{E}_{s^{\\prime}}V^{\\pi}(s^{\\prime}))^{2}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We then see that the terms $\\mathbb{E}_{\\rho^{i}}[Q(s,a)-\\gamma\\mathbb{E}_{s^{\\prime}}V^{\\pi}(s^{\\prime})]$ and $-\\gamma\\mathbb{E}_{s_{0}}V^{\\pi}\\big(s_{0}\\big)$ are minimized (over $\\pi$ ) when $V^{\\pi}(s)$ , for all $s$ , are maximized. We will prove that it would not be the case for the last term. Let us choose $Q$ and $\\overline{r}$ such that $Q(s,a)\\,-\\,\\overline{{r}}(s,a)\\,>\\,\\gamma\\mathbb{E}_{s^{\\prime}}V^{\\pi_{Q}}(s^{\\prime})$ . We see that for any policy $\\pi\\neq\\pi_{Q}$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad Q(s,a)-\\overline{{r}}(s,a)>\\gamma\\mathbb{E}_{s^{\\prime}}V^{\\pi_{Q}}(s^{\\prime})\\geq\\mathbb{E}_{s^{\\prime}}V^{\\pi}(s^{\\prime})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad\\mathrm{Thus},\\,Q(s,a)-\\overline{{r}}(s,a)-V^{\\pi}(s^{\\prime})\\geq Q(s,a)-\\overline{{r}}(s,a)-V^{\\pi_{Q}}(s^{\\prime})>0}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which implies that ", "page_idx": 13}, {"type": "equation", "text": "$$\n-\\alpha\\mathbb{E}_{\\rho^{U}}[(Q(s,a)-\\bar{r}(s,a)-\\gamma\\mathbb{E}_{s^{\\prime}}V^{\\pi}(s^{\\prime}))^{2}]\\le-\\alpha\\mathbb{E}_{\\rho^{U}}[(Q(s,a)-\\bar{r}(s,a)-\\gamma\\mathbb{E}_{s^{\\prime}}V^{\\pi Q}(s^{\\prime}))^{2}]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "So the last term of (10) would not be minimized at $\\pi=\\pi_{Q}$ . In fact, in the above scenario, this last term will be maximized at $\\pi=\\pi_{Q}$ . As a result, there is always $\\alpha$ sufficiently large such that the last term significantly dominates the other terms and ${\\cal J}(Q,\\pi)$ is not minimized at $\\pi=\\pi_{Q}$ . \u53e3 ", "page_idx": 13}, {"type": "text", "text": "Proposition 3.3 For any $Q\\ge0$ , we have $\\widehat\\Gamma(Q)\\leq\\Gamma(Q)$ and $\\begin{array}{r}{\\operatorname*{max}_{Q\\geq0}\\widehat\\Gamma(Q)\\leq\\frac{\\operatorname*{max}_{Q\\geq0}\\Gamma(Q)}{\\operatorname*{max}_{Q\\geq0}\\Gamma(Q)}.}\\end{array}$ Mover, $\\Gamma(Q)=\\widehat\\Gamma(Q)\\;i f Q(s,a)\\leq\\overline{{{r}}}(s,a)$ for all $s,a$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. We first write ${\\widehat{\\mathcal{H}}}(Q,\\pi)$ as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\widehat{\\mathcal{H}}(Q,\\pi)=\\sum_{i\\in[N]}w_{i}\\mathbb{E}_{\\rho^{i}}[\\mathcal{T}^{\\pi}[Q](s,a))]-\\mathbb{E}_{\\rho_{\\pi}}[\\mathcal{T}^{\\pi}[Q](s,a))]-\\mathbb{E}_{\\rho_{\\pi}}[\\log\\pi(s,a)]}}\\\\ {{\\displaystyle-\\,\\alpha\\mathbb{E}_{\\rho^{U}}\\left[(Q(s,a)-\\bar{r}(s,a))^{2}+(\\mathbb{E}_{s^{\\prime}}V^{\\pi}(s^{\\prime}))^{2}+2\\mathrm{ReLU}(\\bar{r}(s,a)-Q(s,a))\\mathbb{E}_{s^{\\prime}}V^{\\pi}(s)\\right]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since $Q\\ \\ \\geq\\ \\ 0$ , $V^{\\pi}(s)\\;\\;=\\;\\;\\mathbb{E}_{a\\sim\\pi(.|s)}[Q(s,a)\\;-\\;\\log\\pi(a|s)]\\;\\;\\geq\\;\\;0$ . Thus $\\mathrm{2ReLU}(\\overline{{r}}(s,a)\\:-$ $\\begin{array}{r}{Q(s,a))\\mathbb{E}_{s^{\\prime}}V^{\\pi}(s)\\geq2(\\overline{{r}}(s,a)\\!-\\!Q(s,a))\\mathbb{E}_{s^{\\prime}}V^{\\pi}(s).}\\end{array}$ . As a result, the last term of ${\\widehat{\\mathcal{H}}}(Q,\\pi)$ is bounded as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\,\\alpha\\mathbb{E}_{\\rho^{U}}\\Bigg[(Q(s,a)-\\overline{{r}}(s,a))^{2}+(\\mathbb{E}_{s^{\\prime}}V^{\\pi}(s^{\\prime}))^{2}+2\\mathrm{ReLU}(\\overline{{r}}(s,a)-Q(s,a))\\mathbb{E}_{s^{\\prime}}V^{\\pi}(s)\\Bigg]}\\\\ &{\\leq-\\alpha\\mathbb{E}_{\\rho^{U}}\\Bigg[(Q(s,a)-\\overline{{r}}(s,a))^{2}+(\\mathbb{E}_{s^{\\prime}}V^{\\pi}(s^{\\prime}))^{2}-2(Q(s,a)-\\overline{{r}}(s,a))\\mathbb{E}_{s^{\\prime}}V^{\\pi}(s)\\Bigg]}\\\\ &{=-\\alpha\\mathbb{E}_{\\rho^{U}}\\big[(T^{\\pi}[Q](s,a)-\\overline{{r}}(s,a))^{2}\\big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "It then follows that $\\widehat{\\mathcal{H}}(Q,\\pi)\\leq\\,J(Q,\\pi)$ . Thus, $\\begin{array}{r}{\\operatorname*{min}_{\\pi}\\widehat{\\mathcal{H}}(Q,\\pi)\\leq\\operatorname*{min}_{\\pi}J(Q,\\pi)}\\end{array}$ or $\\widehat\\Gamma(Q)\\leq\\Gamma(Q)$ . Mover, we see that if $\\overline{{r}}(s,a)\\geq Q(s,a)$ for all $(s,a)$ ,  then $\\mathrm{2ReLU}(\\overline{{r}}(s,a)-Q(s,a))\\mathbb{E}_{s^{\\prime}}V^{\\pi}(s)=$ $2(\\overline{{\\boldsymbol{r}}}(s,a)\\!-\\!Q(s,a))\\mathbb{E}_{s^{\\prime}}V^{\\pi}(s)$ , implying that $\\widehat{\\mathcal{H}}(Q,\\pi)=J(Q,\\pi)$ and $\\widehat\\Gamma(Q)=\\Gamma(Q)$ . This completes the proof. ", "page_idx": 13}, {"type": "text", "text": "Theorem 3.4 For any $Q\\ge0$ , the following results hold (i) The inner minimization problem $\\operatorname*{min}_{\\pi}\\widehat{\\mathcal{H}}(Q,\\pi)$ has a unique optimal solution $\\pi^{*}$ such that $\\pi^{Q}=a r g m i n_{\\pi}V^{\\pi}(s)$ for all $s\\in S$ an d ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\n\\pi^{Q}(a|s)=\\frac{\\exp(Q(s,a))}{\\sum_{a}\\exp(Q(s,a))}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{max}_{\\pi}V^{\\pi}(s)=\\log(\\sum_{a}\\exp(Q(s,a)))\\stackrel{d e f}{=}V^{Q}(s).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. We first rewrite the formulation of ${\\widehat{\\mathcal{H}}}(Q,\\pi)$ as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\widehat{\\mathcal{H}}(Q,\\pi)=\\sum_{i\\in[N]}w_{i}\\mathbb{E}_{\\rho^{i}}[Q(s,a)-\\gamma\\mathbb{E}_{s^{\\prime}}V^{\\pi}(s^{\\prime})]-(1-\\gamma)\\mathbb{E}_{s_{0}}V^{\\pi}(s_{0})}}\\\\ {~~}\\\\ {{\\displaystyle-\\,\\alpha\\mathbb{E}_{\\rho^{U}}\\left[(Q(s,a)-\\overline{{r}}(s,a))^{2}+(\\mathbb{E}_{s^{\\prime}}V^{\\pi}(s^{\\prime}))^{2}+2\\mathrm{ReLU}(\\overline{{r}}(s,a)-Q(s,a))\\mathbb{E}_{s^{\\prime}}V^{\\pi}(s)\\right]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We then see that the first and second term of ${\\widehat{\\mathcal{H}}}(Q,\\pi)$ are minimized when $V^{\\pi}(s)$ are minimized, i.e., at $\\pi\\ =\\ \\pi_{Q}$ . For the last term, since $V^{\\pi}(s)\\;\\geq\\;0$ (because $Q\\ \\geq\\ 0)$ , $-\\grave{(}\\mathbb{E}_{s^{\\prime}}V^{\\pi}(s^{\\prime}))^{2}$ and $-2\\mathrm{ReLU}(\\overline{{r}}(s,a)-Q(s,a))\\mathbb{E}_{s^{\\prime}}V^{\\pi}(s)$ are also minimized at $\\pi=\\pi_{Q}$ . So, ${\\widehat{\\mathcal{H}}}(Q,\\pi)$ is minimized at $\\pi=\\pi_{Q}$ as desired. ", "page_idx": 14}, {"type": "text", "text": "$(i i)$ is already proved in [13]. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\widehat{\\mathcal{H}}(Q,\\pi)=\\sum_{i\\in[N]}w_{i}\\mathbb{E}_{\\rho^{i}}[T^{\\pi}[Q](s,a)]-(1-\\gamma)\\mathbb{E}_{s_{0},a\\sim\\pi(a|s_{0})}[Q(s_{0},a)-\\log\\pi(a|s_{0})]}\\\\ &{\\hphantom{\\quad\\quad}-\\alpha\\mathbb{E}_{\\rho^{\\pi}}\\Bigg[(\\overline{{r}}(s,a)-Q(s,a))+\\mathbb{E}_{s^{\\prime}}V^{\\pi}(s))^{2}\\Bigg]+\\alpha\\mathbb{E}_{\\rho^{U}}\\Bigg[(\\operatorname*{min}\\{0,\\overline{{r}}(s,a)-Q(s,a))\\right\\}\\Bigg]}\\\\ &{\\displaystyle=\\sum_{i\\in[N]}w_{i}\\mathbb{E}_{\\rho^{i}}[T^{\\pi}[Q](s,a)]-(1-\\gamma)\\mathbb{E}_{s_{0},a\\sim\\pi(a|s_{0})}[Q(s_{0},a)-\\log\\pi(a|s_{0})]}\\\\ &{\\hphantom{\\quad\\quad}-\\alpha\\mathbb{E}_{\\rho^{U}}\\Bigg[(\\overline{{r}}(s,a)-Q(s,a))+\\mathbb{E}_{s^{\\prime}}V^{\\pi}(s))^{2}\\Bigg]+\\alpha\\mathbb{E}_{\\rho^{U}}\\Bigg[(\\operatorname*{min}\\{0,\\overline{{r}}(s,a)-Q(s,a))\\Bigg]}\\Bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, the first and second terms of (12) is linear in $Q$ . The fourth term is concave. For third term, let $\\Phi(Q)=|\\overline{{{r}}}(s,a)-Q(s,a)|+\\mathbb{E}_{s^{\\prime}}V^{\\pi}(s)$ . We see that $\\Phi(Q)\\geq0$ for any $Q\\geq0$ and $\\Phi(Q)$ is convex in $Q$ (because $V^{\\pi}(s)$ is linear in $Q$ ). It then follows that, for any $\\eta\\in[0,1]$ and $Q_{1},Q_{2}\\ge0$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\eta(\\Phi(Q))^{2}+(1-\\eta)(\\Phi(Q))^{2}\\stackrel{(a)}{\\geq}(\\eta\\Phi(Q)+(1-\\eta)\\Phi(Q))^{2}}}\\\\ &{}&{\\stackrel{(b)}{\\geq}(\\Phi(\\eta Q_{1}+(1-\\eta)Q_{2}))^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $(a)$ is due to the fact function $h(t)\\,=\\,t^{2}$ is convex, and $(b)$ is because $h(t)\\,=\\,t^{2}$ is nondecreasing for all $t\\,\\geq\\,0$ , and $\\Phi(Q)$ is convex and always takes non-negative values. The last inequality in(13) implies that $(\\Phi(Q))^{2}$ is convex in $Q$ . So the last term of (12) is concave in $Q$ . Putting all together we conclude that ${\\widehat{\\mathcal{H}}}(Q,\\pi)$ is concave in $Q$ as desired. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Proof. We first write ${\\mathcal{L}}({\\overline{{r}}})$ as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma(\\overline{{r}})=\\displaystyle\\sum_{i\\in[N]}\\displaystyle\\sum_{(s,a),(s^{\\prime},a^{\\prime})\\in\\mathcal{D}^{i}}\\left(\\overline{{r}}(s,a)-\\overline{{r}}(s^{\\prime},a^{\\prime})\\right)^{2}-\\displaystyle\\sum_{\\begin{array}{c}{h,k\\in[N],h<k}\\\\ {\\tau_{i}\\in\\mathcal{D}^{h},\\tau_{j}\\in\\mathcal{D}^{k}}\\end{array}}\\ln\\displaystyle\\frac{\\exp(R(\\tau_{j}))}{\\exp(R(\\tau_{j}))+\\exp(\\tau_{i})()}+\\phi(\\overline{{r}})}\\\\ &{~~=\\displaystyle\\sum_{i\\in[N]}\\displaystyle\\sum_{(s,a),(s^{\\prime},a^{\\prime})\\in\\mathcal{D}^{i}}\\left(\\overline{{r}}(s,a)-\\overline{{r}}(s^{\\prime},a^{\\prime})\\right)^{2}-\\displaystyle\\sum_{\\begin{array}{c}{h,k\\in[N],h<k}\\\\ {\\tau_{i}\\in\\mathcal{D}^{h},\\tau_{j}\\in\\mathcal{D}^{k}}\\end{array}}\\left(R(\\tau_{j})\\right.}\\\\ &{~~-\\ln\\left(\\exp(R(\\tau_{j}))+\\exp(R(\\tau_{i}))\\right)+\\phi(\\overline{{r}})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We then see that the first term is a sum of squares of linear functions of $\\overline{r}$ , thus is strictly convex. Moreover, since $R(\\tau_{i})$ is linear in $\\overline{r}$ for any $\\tau_{i}$ , the term $\\ln(\\exp(R(\\tau_{i}))+\\exp(R(\\tau_{j})))$ has a logsum-exp form. So this term is convex as well [3]. Putting all together we see that ${\\dot{\\boldsymbol{\\mathcal{L}}}}({\\overline{{\\boldsymbol{r}}}})$ is strictly convex in $\\overline{r}$ as desired. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Proposition 3.6 Let $\\widehat{Q}=a r g m a x_{Q}\\widehat{\\mathcal{H}}(Q,\\pi)$ and $\\widehat{Q}^{C}=a r g m a x_{Q}\\widehat{\\mathcal{H}}^{C}(Q,\\pi).$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{\\stackrel{s\\sim\\mathcal{D}}{a\\sim\\mu(a|s)}}\\widehat{Q}^{C}(s,a)\\leq\\sum_{\\stackrel{s\\sim\\mathcal{D}}{a\\sim\\mu(a|s)}}\\widehat{Q}(s,a)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. We write ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\mathcal{H}}^{C}(\\widehat{Q}^{C},\\pi)=-\\beta\\displaystyle\\sum_{\\stackrel{s\\sim\\kappa>D}{a\\sim\\mu(a|s)}}\\widehat{Q}^{C}(s,a)+\\widehat{\\mathcal{H}}(\\widehat{Q}^{C},\\pi)}\\\\ &{\\qquad\\qquad\\overset{(a)}{\\geq}-\\beta\\displaystyle\\sum_{\\stackrel{s\\sim\\kappa>D}{a\\sim\\mu(a|s)}}\\widehat{Q}(s,a)+\\widehat{\\mathcal{H}}(\\widehat{Q},\\pi)}\\\\ &{\\qquad\\overset{(b)}{\\geq}-\\beta\\displaystyle\\sum_{\\stackrel{s\\sim\\kappa>D}{a\\sim\\mu(a|s)}}\\widehat{Q}(s,a)+\\widehat{\\mathcal{H}}(\\widehat{Q}^{C},\\pi)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $(a)$ is because $\\widehat{Q}^{C}=\\mathrm{argmax}_{Q}\\widehat{\\mathcal{H}}^{C}(Q,\\pi)$ and $(b)$ is because $\\widehat{\\mathcal{H}}(\\widehat{Q},\\pi)\\geq\\widehat{\\mathcal{H}}(\\widehat{Q}^{C},\\pi)$ . Combine (16) and (17) we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\beta\\sum_{\\stackrel{s\\sim D}{a\\sim\\mu(a|s)}}\\widehat{Q}^{C}(s,a)\\leq\\beta\\sum_{\\stackrel{s\\sim D}{a\\sim\\mu(a|s)}}\\widehat{Q}(s,a)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "as desired. ", "page_idx": 15}, {"type": "text", "text": "B Additional Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Practical Training Objective ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We discuss a practical implementation of the training objective in (4) using samples from both the expert and sub-optimal demonstration sets. We first note that the term $\\bar{\\mathbb{E}}_{\\rho_{\\pi}}[\\bar{T}^{\\pi}[Q](s,a))]\\mathrm{~-~}$ $\\mathbb{E}_{\\rho_{\\pi}}[\\log\\pi(s,a)]$ can be written as [13]: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\rho_{\\pi}}[{T}^{\\pi}[Q](s,a))]\\!-\\!\\mathbb{E}_{\\rho_{\\pi}}[\\log\\pi(s,a)]=\\mathbb{E}_{(s,s^{\\prime})\\sim\\rho^{*}}[{V}^{\\pi}(s)-\\gamma\\mathbb{E}_{s^{\\prime}}{V}^{\\pi}(s^{\\prime})]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for any valid occupancy measure $\\rho^{*}$ . So, for any policy occupancy measure, we can write this term though the union of expert policies $\\rho^{U}$ as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\rho_{\\pi}}[{T}^{\\pi}[Q](s,a))]-\\mathbb{E}_{\\rho_{\\pi}}[\\log\\pi(s,a)]=\\sum_{i\\in[N]}w_{i}\\mathbb{E}_{\\rho^{i}}[{V}^{\\pi}(s)-\\gamma\\mathbb{E}_{s^{\\prime}}{V}^{\\pi}(s^{\\prime})]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As a result, by replacing $V^{\\pi}(s)$ with $V^{Q}(s)$ , we can write $\\widehat\\Gamma(Q)$ in a compact form as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\widehat\\Gamma(Q)=\\sum_{i\\in[N]}w_{i}\\mathbb{E}_{\\rho^{i}}[Q(s,a)-\\gamma\\mathbb{E}_{s^{\\prime}}V^{Q}(s^{\\prime})]}}\\\\ {~~~~~~~-\\displaystyle\\sum_{i\\in[N]}w_{i}\\mathbb{E}_{\\rho^{i}}[V^{Q}(s)-\\gamma\\mathbb{E}_{s^{\\prime}}V^{Q}(s^{\\prime})]-\\alpha\\displaystyle\\sum_{i\\in[N]}w_{i}\\mathbb{E}_{\\rho^{i}}[\\Delta_{\\overline{{r}}}^{Q}(s,a)]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Delta_{\\overline{{r}}}^{Q}(s,a)=(Q(s,a)-\\overline{{r}}(s,a))^{2}+(\\mathbb{E}_{s^{\\prime}}V^{Q}(s^{\\prime}))^{2}+2\\mathrm{ReLU}(\\overline{{r}}(s,a)-Q(s,a))\\mathbb{E}_{s^{\\prime}}V^{Q}(s^{\\prime}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In an empirical offline implementation, to maximize $\\widehat\\Gamma(Q)$ , samples $(s,a,s^{\\prime})$ from demonstrations can be used to approximate the expectations o ver $\\rho^{i}$ $\\begin{array}{r l}{\\stackrel{\\cdot\\,i.}{:}}&{\\sum_{\\left(s,a,s^{\\prime}\\right)\\in{\\mathscr D}^{i}}^{-}[Q(s,a)\\,-\\,\\gamma V^{Q}(s^{\\prime})]}\\end{array}$ , $\\begin{array}{r}{\\sum_{(s,a,s^{\\prime})\\in{\\mathscr{D}}^{i}}[V^{Q}(s)-\\gamma V^{Q}(s^{\\prime})]}\\end{array}$ , and $\\sum_{(s,a)\\in\\mathcal{D}^{i}}[\\Delta_{\\overline{{r}}}^{Q}(s,a)]$ . ", "page_idx": 16}, {"type": "text", "text": "We note that in continuous-action controls, the computation of $V^{Q}(s)$ involves a sum over infinitely many actions, which is impractical. In this case, we can update both $Q$ and $\\pi$ in a soft actor-critic (SAC) manner. That is, for each $\\pi$ , we update $Q$ towards $\\operatorname*{max}_{Q}\\{\\widehat{\\mathcal{H}}(Q,\\pi)\\}$ and for each $Q$ , we update $\\pi$ to bring it towards $\\pi^{Q}$ by solving $\\operatorname*{max}_{\\pi}\\{V^{\\pi}(s)\\}$ , for all $s$ . As shown above, ${\\widehat{\\mathcal{H}}}(Q,\\pi)$ in concave in $Q$ and $V^{\\pi}(s)$ is convex in $\\pi$ , so we can expect this SAC will exhibit good be havior and stability. ", "page_idx": 16}, {"type": "text", "text": "B.2 Algorithm Overview ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The Figure 2 provide the overview of our algorithm. ", "page_idx": 16}, {"type": "image", "img_path": "uDD44NROOt/tmp/43d16195681641cbf37d132a95f416a9a06a9da122ca8ff89e63e8226efa8511.jpg", "img_caption": ["Figure 2: Overview of SPRINQL. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.3 Environments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "This section provide a detailed descrition of the enviroments used in our experiments. ", "page_idx": 16}, {"type": "text", "text": "B.3.1 Mujoco ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "MuJoCo gym environments [32] like HalfCheetah, Ant, Walker2d, Hopper, and Humanoid are integral to the field of reinforcement learning (RL), particularly in the domain of continuous control and robotics: ", "page_idx": 16}, {"type": "text", "text": "\u2022 HalfCheetah: This environment simulates a two-dimensional cheetah-like robot. The objective is to make the cheetah run as fast as possible, which involves learning complex, coordinated movements across its body. \u2022 Ant: This environment features a four-legged robot resembling an ant. The challenge is to control the robot to move effectively, balancing stability and speed. \u2022 Walker2d: This environment simulates a two-dimensional bipedal robot. The goal is to make the robot walk forward as fast as possible without falling over. ", "page_idx": 16}, {"type": "text", "text": "\u2022 Hopper: The Hopper environment involves a single-legged robot. The primary challenge is to balance and hop forward continuously, which requires maintaining stability while in motion.   \n\u2022 Humanoid: The Humanoid environment is among the most complex, featuring a bipedal robot with a human-like structure. The task involves mastering various movements, from walking to more complex maneuvers, while maintaining balance. An expert is trying to maximize the reward function by moving with a trajectory length limit of 1000.   \nAll five environments are shown in Figure 3. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "image", "img_path": "uDD44NROOt/tmp/cc861aee4856e1c6a3a20f51aa1e5bc1b07c57ffed254bb88ebe84a0c4f1ad5b.jpg", "img_caption": ["Figure 3: Five different Mujoco environments. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.3.2 Panda-gym ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The Panda-gym environments [12], designed for the Franka Emika Panda robot in reinforcement learning, include PandaReach, PandaPush, PandaPickandPlace, and PandaSlide. Here\u2019s a short description of each: ", "page_idx": 17}, {"type": "text", "text": "\u2022 PandaReach: The task is to move the robot\u2019s gripper to a randomly generated target position within a specific volume. It focuses on precise control of the gripper\u2019s movement.   \n\u2022 PandaPush: In this environment, a cube placed on a table must be pushed to a target position. The gripper remains closed, emphasizing the robot\u2019s ability to manipulate objects through pushing actions.   \n\u2022 PandaPickandPlace: This more complex task involves picking up a cube and placing it at a target position above the table. It requires coordinated control of the robot\u2019s gripper for both lifting and accurate placement.   \n\u2022 PandaSlide: Here, the robot must slide a flat object (like a hockey puck) to a target position on a table. The gripper is fixed in a closed position, and the task demands imparting the right amount of force to slide the object to the target. ", "page_idx": 17}, {"type": "text", "text": "In these environments, the reward function is $^{-1}$ for every time step it has not finished the task. Moreover, the maximum horizon is extremely short, with a maximum of 50, while an expert can complete the task after several steps. All four different environments are shown in Figure 4. ", "page_idx": 17}, {"type": "image", "img_path": "uDD44NROOt/tmp/b183944c8ef2e88937e234be462130091ad29687b7063c347414ffa6a625b062.jpg", "img_caption": ["Figure 4: Five different Panda-gym environments. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.4 Qualities of the Generated Expert and Non-Expert Datasets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this paper, we provide a new setting utilizing ranked sub-optimal datasets to perform imitation learning in the offline setting (illustration in Figure 5). ", "page_idx": 17}, {"type": "text", "text": "Ranked sub-optimal datasets ", "text_level": 1, "page_idx": 18}, {"type": "image", "img_path": "uDD44NROOt/tmp/4bf2c863546bcb71eb7f43ea9739dac5c212367fb40b4484202bdf523b49330d.jpg", "img_caption": ["Figure 5: Training datasets illustration. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "We create sub-optimal datasets by adding noise to actions of the expert policy and let them interact with the environments to collect the sub-optimal trajectories. Each task has its own difficulty and sensitivity to the noise of the actions. The averaged returns of the generated datasets, computed as percentages w.r.t. the maximum returns, are reported in Figure 6. ", "page_idx": 18}, {"type": "image", "img_path": "uDD44NROOt/tmp/8458a1784b100dc1a15776f4752b823e720844a9bcf220e2c670d2892293ed3d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 6: Whisker plots illustrate the average returns of both expert and non-expert datasets nine distinct environments in Mujoco and Panda-gym. The numerical values following the task names represent the actual mean return of the expert policy. ", "page_idx": 18}, {"type": "text", "text": "B.5 Hyper Parameters and Experimental Implementations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 In our experiments, for every algorithm, we run five different seeds corresponding to five different datasets sampled from our databases in Appendix B.4.   \n\u2022 We use double Q critic network for our implementation to increase the stability in the offilne training scheme.   \n\u2022 For IQ-learn [13],SQIL [29], we conduct experiment from its official implement with double Q critic network.   \n\u2022 For DemoDICE [21] we conduct experiment from its official implementation.   \n\u2022 For DWBC [37] we conduct experiment from its official implementation.   \n\u2022 Inspired by double Q-learning, to avoid overfitting in the offline setting, some experiments apply a training trick using KL-divergence between the target actor and the training actor to prevent rapid policy changes.   \n\u2022 For SAC-based algorithms, we use a fixed exploration parameter which is commonly used in previous work.   \n\u2022 We conducted all experiments on a total of 8 NVIDIA RTX A5000 GPUs and 64 core CPUs. We use 1 GPUs and 8 core CPUs per task with approximately one day per 5 seeds. The detailed hyper-parameters are reported in Table 2. ", "page_idx": 18}, {"type": "table", "img_path": "uDD44NROOt/tmp/eba1d67f68b5a49247faaccf6298332356f000e928e3de1de9b273fa36304138.jpg", "table_caption": ["Table 2: Hyper parameters. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "C Supplementary Experiments ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we present additional experiments that complement those reported in the main paper, along with ablation studies to address the experimental questions stated therein. ", "page_idx": 19}, {"type": "text", "text": "C.1 Full Experiment Results for Mujoco and Panda-gym ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We report the full results for the 5 different Mujoco environments and 4 different Panda-gym environments, with 3 datasets (i.e., $N=3$ ), supplementing the results reported in Table 1 in the main paper. The detailed results for Mujoco in Table 3 and Panda-gym in Table 4 ", "page_idx": 19}, {"type": "table", "img_path": "uDD44NROOt/tmp/fddb1a5c56016c8e30bd027cb4a1410b25fbcbdb5a7b32a7e30c07e97a9b3d0a.jpg", "table_caption": ["Table 3: Comparison results for Mujoco tasks. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "uDD44NROOt/tmp/d6efa2bf6930c52da98fef152f8098fe8dffb5c4827eb01e199f7447799982be.jpg", "table_caption": ["Table 4: Comparison results for Panda-gym tasks. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "C.2 Comparison Results for $N=2$ , i.e., One Expert and One Sub-optimal Datasets - Q1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We provide additional comparison results for $N=2$ , complementing to the answer of (Q1), i.e., how does SPRINQL perform compared to other baselines? In this experiment, we want to test the ability of our algorithm in the same scenario of DemoDICE, DWBC which include expert dataset and only one supplementary dataset. Number of expert transitions in Mujoco domains is 1000 and 100 for Panda-gym. Meanwhile, for the supplementary dataset, it is 25000 transitions for Mujoco and 5000 for Panda-gym. In general, although we experience a downgrade in performance due to lack of ranking (only one supplementary dataset), leading to misunderstanding the true reward function, our method is still able to leverage the sub-optimal dataset for understanding the expert demonstrations and provide the highest average score. Reported results are shown in Table 5 6 and Figure 7. ", "page_idx": 20}, {"type": "table", "img_path": "uDD44NROOt/tmp/1c3b51222dcfd85bcdf6ad4576a66eff659021493c1595bffae5d30a5c67cf81.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "uDD44NROOt/tmp/75ac46bd1fa7b9ae632fa2c8d7f2f566f9cf51bd273db345216eb0498c5a201f.jpg", "table_caption": ["Table 5: Comparison results for Mujoco tasks in two expert datasets. ", "Table 6: Comparison results for Panda-gym tasks in two expert datasets. "], "table_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "uDD44NROOt/tmp/6a397fc42773cad370aab032552c29f744ca2d26738597b5aa229a9c898e95f0.jpg", "img_caption": ["Figure 7: Learning curves of two dataset experiment. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "C.3 Learning Curves of noReg-SPRINQL and noDM-SPRINQL ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "This section reports additional results for the comparison between SPRINQL and the two variants noReg-SPRINQL and noDM-SPRINQL (supplementing the answer of Q2 in the main paper). The comparison results for Panda-gym environments are reported in Figure 8 and the learning curves are plotted in Figure 9 and Figure 10. ", "page_idx": 21}, {"type": "image", "img_path": "uDD44NROOt/tmp/357a8ee9c4764d68b3016427c2e62b6e78302086f640a087ce886e147ba65014.jpg", "img_caption": ["Figure 8: Ablation study show the performance of three variants of SPRINQL across four Panda-gym environments. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "uDD44NROOt/tmp/0bb2098efc319e74230394befd8aa944e3a0af8328f570368ab75cab5504a5b1.jpg", "img_caption": ["Figure 9: Two term ablation study for two level dataset scenario. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "uDD44NROOt/tmp/ded40fc6e59ba86116d95ef025636f8d5761c45eb49753066fcea5b79c9fd2b3.jpg", "img_caption": ["Figure 10: Two term ablation study for three level dataset scenario. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "C.4 Augmented Expert Demonstrations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We provide experiments to address (Q3) \u2013 What happens if we augment the expert data while maintaining the sub-optimal datasets? To this end, we use two Mujoco and two Panda-gym tasks, keeping the same sup-optimal datasets and add more expert demonstrations to the training sets. The comparison results are reported in Figure 11. For the Mujoco tasks, which are more difficult, adding more expert trajectories significantly enhances the performance of all the algorithms. However, for the two Panda-gym tasks, the influence of adding more expert data appears to be less significant in Push and completely absent in $\\mathrm{PnP}.$ This would be because expert trajectories in these tasks are typically short, consisting of only 2-7 transitions. Hence, a larger quantity of additional expert data may be required to enhance performance. ", "page_idx": 22}, {"type": "image", "img_path": "uDD44NROOt/tmp/405c8e9875c43c28cd6dd4c9ee497d35e77b03d9566a5fbce1ea57a781e1bfdc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 11: Comparison results with additional expert demonstrations; $\\mathrm{(+E)}$ signifies that the expert dataset is increased from $1\\mathbf{k}$ to $5\\mathrm{k}$ for Mujoco and from 100 to 500 for Panda-gym, while (noE) indicates no expert data. ", "page_idx": 22}, {"type": "text", "text": "C.5 Augmented Sup-optimal Demonstrations ", "text_level": 1, "page_idx": 22}, {"type": "image", "img_path": "uDD44NROOt/tmp/764da069dd295ad526126b2c8b47a20f82c7e2ae9d37ec8385d49edbf9094b6c.jpg", "img_caption": ["Figure 12: Performance comparisons with varied sub-optimal data sizes. The $x$ -axis shows the size of Level 2 and 3 sup-optimal datasets, and $y$ -axis shows the scores. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "In this experiment, we want to answer the question (Q4) \u2013 What happens if we augment (or reduce) the sub-optimal data while maintaining the expert dataset?. We present numerical results to assess the impact of sub-optimal data on the performance of our SPRINQL and other baselines. To this end, we keep the same expert dataset and adjust the non-expert data used in Table 1. The performance is reported in Fig.12. It is evident that reducing the amount of data in sub-optimal datasets can result in a significant degradation in the performance of our SPRINQL and the other baselines. Conversely, adding more sub-optimal data can enhance overall stability and lead to improved performance. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "In the Figure 13 we show the learning curves with varied sizes for sub-optimal datasets. It can be seem that the overall performance tends to improve with more sup-optimal datasets. In particular, for the Ant task, our SPRINQL even fails to maintain stability when the sizes of sup-optimal datasets are low (10k-5k-1k). Moreover, while BC seems to show improvement with more sub-optimal data, the performance of DWBC and DemoDICE remains unchanged. This may be because these approaches rely on the assumption that expert demonstrations can be extracted from the sub-optimal data, which is not the case in our context. ", "page_idx": 23}, {"type": "image", "img_path": "uDD44NROOt/tmp/64f800f22c0253cdc0f015d9561a54144bac5a480b54dc0866202be0d69cbb04.jpg", "img_caption": ["Figure 13: Evaluation curves with different sub-optimal-dataset size. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "C.6 Impact of the Conservative Term ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this experiment, we aim to answer (Q5) \u2013 How does the conservative term help in our approach? The Equation 8 introduce the conservative Q learning (CQL) term into our work. Here we also test three variants of our method from Section 4.3 and show the impact of CQL to the final performance. The experimental results are shown in Figure 14. ", "page_idx": 24}, {"type": "image", "img_path": "uDD44NROOt/tmp/dbad0d60104f89065d8798fc7d48c54d23efcae3263d336419e55bbc338151eb.jpg", "img_caption": ["C.7 Performance with Varying Number of Expertise Levels "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 14: Performance of 3 variant with and without the CQL term in four different environments accross two domains. ", "page_idx": 24}, {"type": "image", "img_path": "uDD44NROOt/tmp/aaa26f96822f6df47081b3baa0e381ea7682282510f995785593390879951ec1.jpg", "img_caption": ["Figure 15: Average returns of the 5 HalfCheetah datasets (one expert and four sub-optimals). "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "uDD44NROOt/tmp/cf3951056b39a6a559964bd7ab138e4f31f72bfb1a5b836d17bb49ddbf5756d8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 16: Experiment results for different numbers of sub-optimal datasets. The learning curves are calculated by mean with shaded by the standard error of 5 data seeds. ", "page_idx": 24}, {"type": "text", "text": "We provide an experiment to answer (Q6) - How does increasing $N$ (the number of expertise levels) affect the performance of SPRINQL? We assess our algorithm with different numbers of expertise levels to investigate how adding or removing sub-optimal expertise levels influences performance. Specifically, we keep the same expert dataset comprising 1 expert trajectory and conduct tests with 1 to 4 sub-optimal datasets from the Cheetah task. Details of the average returns of the five datasets are reported in Fig. 15. In this context, SPRINQL outperforms other algorithms in utilizing non-expert demonstrations. Furthermore, BC successfully learns and achieves performance comparable to the performance of SPRINQL with 2 dataset, while DemoDICE and DWBC struggle to learn. The detailed results are plotted in Figure 16. In Section (C.2) below, We particularly delve into the situation of having two datasets (one expert and one sub-optimal), which is a typical setting in prior work. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "C.8 Ablation Study for the Preference-based Weight Learning ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We provide this experiment to answer (Q7) \u2013 Does the preference-based weight learning approach provide good values for the weights $w_{i}$ ? To this end, we compare the performance of SPRINQL based on the weights determined by the preference-based methods described in the main paper (denoted as auto W), and the following weighting scenarios: ", "page_idx": 25}, {"type": "text", "text": "\u2022 Uniform W: We chose the weights $w_{i}$ uniformly over $[0,1]$ as $\\mathbf{w}=\\{0.55,0.35,0.15\\}$ with a ratio of approximately $10:7:4$ . \u2022 Reduced W: Starting from Uniform W, we reduced the weights of the non-expert data and tested the weights $\\mathbf{w}=\\{0.65,0.2,0.15\\}$ with a ratio of approximately $10:3:2$ . \u2022 Increased W: Starting from Uniform $W_{\\cdot}$ , we increased the weights of the non-expert data and chose $\\mathbf{w}=\\{0.4,\\bar{0}.32,0.28\\}$ with a ratio of approximately $10:8:7$ . ", "page_idx": 25}, {"type": "text", "text": "These weight vectors w are normalized to follow $w_{1}>w_{2}>...>w_{N}$ and $\\begin{array}{r}{\\sum_{i\\in[N]}w_{i}=1}\\end{array}$ . ", "page_idx": 25}, {"type": "text", "text": "The comparison results are shown in Figure 17. ", "page_idx": 25}, {"type": "image", "img_path": "uDD44NROOt/tmp/1dfd60d0d8a12903aeef8e29abf100cfabee3779dfac4336c89547afda4148fd.jpg", "img_caption": ["Figure 17: Experiment results for SPRINQL with different manual selection of weight $W$ . "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "C.9 D4RL mujoco dataset ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we present additional experiments using the official D4RL dataset. Unfortunately, since our algorithm requires meaningful demonstrations, we exclude the Random dataset and are only able to test with $N=2$ (Medium and Expert datasets). The detailed results are shown in Figure 18. ", "page_idx": 25}, {"type": "image", "img_path": "uDD44NROOt/tmp/d16f4b6cb6e111bfa19338b574ebf25e2c374aa1cbeda8b8b97d53b1a5d70d02.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 18: Performance in D4RL dataset with Medium (25,000 transitions) and Expert(1,000 transitions. The results are reported from 5 seeds per method. ", "page_idx": 25}, {"type": "text", "text": "C.10 Reward Recovering ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this experiment, we want to answer (Q8) - How does SPRINQL perform in recovering the groundtruth reward function? Compared to BC-based algorithms, one notable advantage of Q-learning based algorithms is their ability to recover the reward function. Here, we present experiments demonstrating reward function recovery across five MuJoCo tasks, comparing recovered rewards to the actual reward function. To achieve this, we introduce increasing levels of random noise to the actions of a trained agent and observe its interactions with the environment. We collect the state, action, and next state for each trajectory, then predict the recovered reward and compare it to the true reward from the environment. For the sake of comparison, we include noReg-SPRINQL, which can be considered an an adaption of IQ-learn [13] to our setting, and noDM-SPRINQL, which is in fact an adaption of T-REX to our offline setting. ", "page_idx": 26}, {"type": "text", "text": "Comparison results are presented in Figure 19. We observe a linear relationship between the true and predicted rewards for SPRINQL across all testing tasks, whereas the other approaches fail to return correct relationships for some tasks. ", "page_idx": 26}, {"type": "image", "img_path": "uDD44NROOt/tmp/a82c46c485a04ec6aff88160631b1cc71b657aca6c233e79d2ebb7e37bae2944.jpg", "img_caption": ["Figure 19: Recovered return and the true return of five Mujoco environments. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "C.11 Reference Reward Distribution ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "From the experiment reported in Table 1, we plot the distributions of the reward reference values in Figure 20, where the $x$ -axis shows the level indexes and the $y$ -axis shows reward values. The rewards seem to follow desired distributions, with larger rewards assigned to higher expertise levels. Moreover, rewards learned for expert demonstrations are consistently and significantly higher and exhibit smaller variances compared to those learned for sub-optimal transitions. ", "page_idx": 28}, {"type": "image", "img_path": "uDD44NROOt/tmp/de41cfb0da301380872cfd6b9d2f76bf00572f6cb6497d99cdd6f601135def3e.jpg", "img_caption": ["Figure 20: Whisker plots illustrate the reward reference distribution of three datasets of each environment for one seed. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "C.12 Different alpha ablation study ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "As our objective is a combination of two terms with a balancing parameter $\\alpha$ , we conduct additional experiments to evaluate the performance of our method across a range of $\\alpha$ values. The detailed results are presented in Figure 21. Overall, the results indicate that $\\alpha$ can be selected within the range of 0.1 to 10 to achieve good performance. ", "page_idx": 28}, {"type": "image", "img_path": "uDD44NROOt/tmp/b4fe4b40c78e55ff10b8909fdbc325d28e139d489f231038ad74624c48250905.jpg", "img_caption": ["Figure 21: Performance of SPRINQL in different $\\alpha$ . The results are reported from 5 seeds per $\\alpha$ value. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our abstract includes our main claims reflecting our main contributions and finding. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We have a discussion on the limitations of our work in the conclusion section. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All the proofs of the theorems and propositions stated in the main paper are provided in the appendix with clear references. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide details on the environments and hyper-parameter settings in the appendix. We also uploaded our source code for re-productivity purposes. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have describe how to generate our data as well as provide it along with our submitted source code with sufficient instructions for their use. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We have detailed these information in the main paper and the appendix of our paper. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have reported the mean scores and standard deviations for the result tables. We have also shown training curves constructed from mean scores and shaded by standard error. All the experiments are reported with multiple training seeds as well as different datasets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 31}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have provided these information in the \u201cHyper parameter and Experimental Implementations\u201d section in our appendix. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper provides a general offline imitation learning with multiple expert levels and only testing on the simulated environments. As such, we do not foresee any direct societal impact. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our training data are generated from open source simulated environments which have no risk for misuse. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have provided clear citations to the source code and data we used in the paper. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Our source code is submitted alongside the paper, accompanied by sufficient instructions. We will share the code publicly for re-producibility or benchmarking purposes. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: We have no crowdsourcing experiments. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: We do not have study participants. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 34}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]