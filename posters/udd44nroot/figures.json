[{"figure_path": "uDD44NROOt/figures/figures_8_1.jpg", "caption": "Figure 1: Comparison of three variants of SPRINQL across five Mujoco environments.", "description": "This figure compares the performance of three variants of the SPRINQL algorithm across five different Mujoco environments.  The three variants are: SPRINQL (the full algorithm), noReg-SPRINQL (SPRINQL without the reward regularization term), and noDM-SPRINQL (SPRINQL without the distribution matching term). The performance is measured across different numbers of expertise levels (2 and 3 levels) for each environment.  The 'expert' performance is also shown as a baseline to compare to.  The graph shows the normalized score achieved by each approach in each environment, providing a visual summary of the effectiveness of each component of the SPRINQL algorithm.", "section": "4.3 Ablation Study - No Distribution Matching and No Reward Regularizer"}, {"figure_path": "uDD44NROOt/figures/figures_16_1.jpg", "caption": "Figure 2: Overview of SPRINQL.", "description": "This figure presents a flowchart illustrating the overall architecture of the SPRINQL algorithm.  It outlines the key steps involved, including reference reward training using expert and sub-optimal demonstrations, weight learning for the different expertise levels, Q-function training, and policy training. The diagram shows the interactions and dependencies between these components, highlighting the flow of information and updates during the learning process. The boxes and arrows visually represent the different modules and their interconnections.", "section": "3.4 SPRINQL Algorithm"}, {"figure_path": "uDD44NROOt/figures/figures_17_1.jpg", "caption": "Figure 3: Five different Mujoco environments.", "description": "This figure shows the five different MuJoCo environments used in the experiments of the paper.  These environments are HalfCheetah, Ant, Walker2d, Hopper, and Humanoid. Each environment simulates a different type of robot, with varying degrees of complexity and control challenges.  The images depict a visual representation of each robot within its simulated environment.", "section": "B.3 Environments"}, {"figure_path": "uDD44NROOt/figures/figures_17_2.jpg", "caption": "Figure 4: Five different Panda-gym environments.", "description": "This figure shows four different robotic arm manipulation tasks from the Panda-gym environment.  Each task involves a different manipulation goal: Reach (moving the gripper to a target position), Push (pushing a cube to a target location), Pick and Place (picking up a cube and placing it at a target location), and Slide (sliding a puck to a target position). The images show the robotic arm and the object(s) being manipulated in the task.", "section": "B.3.2 Panda-gym"}, {"figure_path": "uDD44NROOt/figures/figures_18_1.jpg", "caption": "Figure 5: Training datasets illustration.", "description": "This figure illustrates the structure of the datasets used in the training process.  The datasets are grouped by levels of expertise, ranging from N levels of sub-optimal demonstrations to a single expert dataset. The sub-optimal datasets are arranged in decreasing order of optimality, leading up to the expert dataset.  This setup allows the model to learn from a diverse range of demonstrations, leveraging the abundance of sub-optimal data while prioritizing alignment with expert behavior.", "section": "4 Experiments"}, {"figure_path": "uDD44NROOt/figures/figures_18_2.jpg", "caption": "Figure 1: Comparison of three variants of SPRINQL across five Mujoco environments.", "description": "This figure compares the performance of three variants of the SPRINQL algorithm across five different MuJoCo environments. The three variants are: SPRINQL (the full model), noReg-SPRINQL (without reward regularization), and noDM-SPRINQL (without distribution matching).  The results show the average return of the algorithm across multiple random seeds for each environment.  The figure helps to illustrate the importance of both distribution matching and reward regularization in achieving state-of-the-art performance with the SPRINQL algorithm.", "section": "4.3 Ablation Study - No Distribution Matching and No Reward Regularizer"}, {"figure_path": "uDD44NROOt/figures/figures_21_1.jpg", "caption": "Figure 7: Learning curves of two dataset experiment.", "description": "This figure compares the learning curves of several offline imitation learning algorithms on various MuJoCo and Panda-gym tasks.  The algorithms compared include DWBC, Weighted-BC, BC, DemoDICE, and SPRINQL. The plot shows the learning progress of each algorithm over a million training steps, and how they compare to the performance of an expert. The x-axis shows the number of training steps (iterations), and the y-axis shows the normalized average returns of the agents. This illustrates how each algorithm's policy converges to the expert's performance. The shaded regions represent the standard error of the mean across multiple runs.", "section": "4.3 Ablation Study - No Distribution Matching and No Reward Regularizer"}, {"figure_path": "uDD44NROOt/figures/figures_21_2.jpg", "caption": "Figure 8: Ablation study show the performance of three variants of SPRINQL across four Panda-gym environments.", "description": "This figure presents the results of an ablation study comparing three variants of SPRINQL across four Panda-gym environments. The three variants are: noReg-SPRINQL (without reward regularization), noDM-SPRINQL (without distribution matching), and SPRINQL (with both reward regularization and distribution matching). The results show that SPRINQL outperforms the other two variants, indicating the importance of both reward regularization and distribution matching for improving the performance of SPRINQL.  The results are shown as bar charts for each environment and each variant, with error bars representing the standard deviation.", "section": "4.3 Ablation Study - No Distribution Matching and No Reward Regularizer"}, {"figure_path": "uDD44NROOt/figures/figures_21_3.jpg", "caption": "Figure 1: Comparison of three variants of SPRINQL across five Mujoco environments.", "description": "This figure compares the performance of three variants of the SPRINQL algorithm across five different MuJoCo environments. The three variants are: SPRINQL (the complete algorithm), noReg-SPRINQL (without the reward regularization term), and noDM-SPRINQL (without the distribution matching term). The performance is measured as the average return over five different random seeds. The figure shows that SPRINQL consistently outperforms the other two variants across all environments, demonstrating the importance of both the reward regularization and distribution matching terms.", "section": "4.3 Ablation Study - No Distribution Matching and No Reward Regularizer"}, {"figure_path": "uDD44NROOt/figures/figures_22_1.jpg", "caption": "Figure 7: Learning curves of two dataset experiment.", "description": "The figure shows the learning curves for SPRINQL and other baselines across multiple MuJoCo and Panda-gym environments using two datasets.  The x-axis represents the number of training steps, and the y-axis shows the average return. The plot illustrates the training progress and performance of different algorithms over time, comparing SPRINQL to other state-of-the-art methods.", "section": "4.3 Ablation Study - No Distribution Matching and No Reward Regularizer"}, {"figure_path": "uDD44NROOt/figures/figures_22_2.jpg", "caption": "Figure 7: Learning curves of two dataset experiment.", "description": "This figure shows the learning curves of several algorithms including SPRINQL and baselines across five Mujoco tasks and four Panda-gym tasks when only two datasets are used (one expert and one sub-optimal dataset). The x-axis represents the number of training steps and the y-axis shows the performance (normalized score). The results demonstrate the superior performance of SPRINQL compared to other algorithms, especially in reaching higher scores more quickly.", "section": "4.3 Ablation Study - No Distribution Matching and No Reward Regularizer"}, {"figure_path": "uDD44NROOt/figures/figures_22_3.jpg", "caption": "Figure 1: Comparison of three variants of SPRINQL across five Mujoco environments.", "description": "This figure compares the performance of three variants of the SPRINQL algorithm against a baseline of expert performance across five MuJoCo environments. The three variants are: SPRINQL (the full model), noReg-SPRINQL (removing the reward regularization term), and noDM-SPRINQL (removing the distribution matching term). The results show that SPRINQL consistently outperforms the other variants across all environments, highlighting the importance of both distribution matching and reward regularization in the algorithm's success.", "section": "4.3 Ablation Study - No Distribution Matching and No Reward Regularizer"}, {"figure_path": "uDD44NROOt/figures/figures_23_1.jpg", "caption": "Figure 13: Evaluation curves with different sub-optimal-dataset size.", "description": "This figure shows the learning curves of different algorithms using different sizes of suboptimal datasets for several tasks. The x-axis shows the training steps, and the y-axis shows the normalized score. The different lines represent different algorithms: DWBC, BC, DemoDICE, SPRINQL, and Expert. The different subplots represent different tasks and datasets sizes.", "section": "C.5 Augmented Sup-optimal Demonstrations"}, {"figure_path": "uDD44NROOt/figures/figures_24_1.jpg", "caption": "Figure 14: Performance of 3 variant with and without the CQL term in four different environments accross two domains.", "description": "This ablation study compares three variants of SPRINQL across four different environments (Cheetah, Ant, Push, PnP) from two domains (MuJoCo and Panda-gym). The variants are: noReg-SPRINQL (no CQL), noDM-SPRINQL (no CQL), noReg-SPRINQL, noDM-SPRINQL, SPRINQL (no CQL), and SPRINQL. The figure shows the impact of the conservative Q-learning (CQL) term on the performance of each variant. The expert performance is included as a baseline for comparison. The results indicate that the CQL term improves the performance of SPRINQL across different environments and domains.", "section": "C.6 Impact of the Conservative Term"}, {"figure_path": "uDD44NROOt/figures/figures_24_2.jpg", "caption": "Figure 15: Average returns of the 5 HalfCheetah datasets (one expert and four sub-optimals).", "description": "This figure shows the average returns for the HalfCheetah task using different numbers of sub-optimal datasets.  It demonstrates the performance improvement with the increased number of sub-optimal datasets. The result shows that SPRINQL outperforms other algorithms in utilizing non-expert demonstrations.", "section": "C.7 Performance with Varying Number of Expertise Levels"}, {"figure_path": "uDD44NROOt/figures/figures_24_3.jpg", "caption": "Figure 15: Average returns of the 5 HalfCheetah datasets (one expert and four sub-optimals).", "description": "The figure shows the average returns for the HalfCheetah datasets with varying numbers of sub-optimal datasets.  It visually compares the performance of several algorithms (DemoDICE, BC, DWBC, SPRINQL) against an expert baseline as the number of sub-optimal datasets increases from 1 to 4. The x-axis represents the number of training steps, and the y-axis represents the average return, showing how well each algorithm learns to perform the task under different data conditions.", "section": "C.7 Performance with Varying Number of Expertise Levels"}, {"figure_path": "uDD44NROOt/figures/figures_25_1.jpg", "caption": "Figure 17: Experiment results for SPRINQL with different manual selection of weight W.", "description": "The figure compares the performance of SPRINQL using different weighting schemes for the datasets of varying expertise levels on two Mujoco environments, Cheetah and Ant. The weighting schemes include Uniform W (uniform weights), Reduced W (reduced weights for non-expert data), Increased W (increased weights for non-expert data), and auto W (weights automatically inferred by the preference-based method described in the paper). The results show that the automatically learned weights (auto W) achieve superior performance compared to other weighting schemes.", "section": "C.8 Ablation Study for the Preference-based Weight Learning"}, {"figure_path": "uDD44NROOt/figures/figures_25_2.jpg", "caption": "Figure 7: Learning curves of two dataset experiment.", "description": "This figure shows the learning curves for different algorithms, namely DWBC, Weighted-BC, BC, DemoDICE, and SPRINQL, across five different environments. It compares the performance of these algorithms when trained with only expert data, and when trained with both expert and sub-optimal data. The x-axis represents the number of training steps, while the y-axis represents the normalized score. The figure helps to visualize the convergence behavior of each algorithm and to compare their performance under different training conditions.", "section": "4.3 Ablation Study - No Distribution Matching and No Reward Regularizer"}, {"figure_path": "uDD44NROOt/figures/figures_27_1.jpg", "caption": "Figure 19: Recovered return and the true return of five Mujoco environments.", "description": "This figure compares the recovered rewards with the true rewards across five MuJoCo environments.  The three algorithms (noReg-SPRINQL, noDM-SPRINQL, and SPRINQL) are shown for each task (Cheetah, Ant, Walker, Hopper, Humanoid).  Each plot shows a scatter plot of the true return against the predicted return for a specific environment and algorithm. The plots visualize the performance of each algorithm in estimating the reward function.", "section": "C.10 Reward Recovering"}, {"figure_path": "uDD44NROOt/figures/figures_28_1.jpg", "caption": "Figure 1: Comparison of three variants of SPRINQL across five Mujoco environments.", "description": "This figure compares the performance of three variants of the SPRINQL algorithm across five different MuJoCo environments: Cheetah, Ant, Walker, Hopper, and Humanoid.  Each bar represents the average return of an algorithm variant. The algorithms compared are the full SPRINQL model, a version without the distribution matching term (noDM-SPRINQL), and a version without the reward regularization term (noReg-SPRINQL).  The figure helps visualize the contributions of these two components to the algorithm's overall effectiveness. The expert performance is also shown as a benchmark.", "section": "4.3 Ablation Study - No Distribution Matching and No Reward Regularizer"}, {"figure_path": "uDD44NROOt/figures/figures_28_2.jpg", "caption": "Figure 1: Comparison of three variants of SPRINQL across five Mujoco environments.", "description": "This figure compares the performance of three variants of the SPRINQL algorithm across five different MuJoCo environments.  The variants are: SPRINQL (the full algorithm), noReg-SPRINQL (without reward regularization), and noDM-SPRINQL (without distribution matching). The results show that the complete SPRINQL algorithm outperforms the other variants, highlighting the value of both distribution matching and reward regularization.", "section": "4.3 Ablation Study - No Distribution Matching and No Reward Regularizer"}]