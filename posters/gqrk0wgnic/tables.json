[{"figure_path": "GQrk0WGNiC/tables/tables_1_1.jpg", "caption": "Table 1: Comparing DP pre-training and DP fine-tuning.", "description": "This table summarizes the key differences between differentially private (DP) pre-training and DP fine-tuning.  It highlights that pre-training uses a large dataset and many training iterations, with DP noising being the main cause of performance degradation.  In contrast, DP fine-tuning uses a small dataset and fewer iterations, with DP clipping being the primary source of performance issues.  The table provides context for the authors' focus on addressing the challenges associated with DP pre-training.", "section": "1 Introduction"}, {"figure_path": "GQrk0WGNiC/tables/tables_7_1.jpg", "caption": "Table 2: Summary of \u03b1t by mixed data training methods.", "description": "This table summarizes the different methods for determining the ratio of privatized and non-privatized gradients (\u03b1t) in mixed data training.  The methods shown are 'Ours' (a piecewise function indicating a switch between public and private training), 'DPMD' (a cosine function), 'Sample' (a ratio based on the number of public and private samples), 'OnlyPublic' (\u03b1t = 1, only public data), and 'OnlyPrivate' (\u03b1t = 0, only private data). Each method offers a different strategy for balancing the use of public and private data during training.", "section": "4 Continual pre-training with DP"}, {"figure_path": "GQrk0WGNiC/tables/tables_8_1.jpg", "caption": "Table 3: Pre-training strategies of models. Standard non-DP training is marked in black; DP training is in green. \u2020 indicates self-supervised without using the labels. \u201cImages \u00d7\u201d is the total number of images used (dataset size \u00d7 epochs). \u201cNon-privacy\u201d means no DP guarantee on a subset of training data due to the non-DP pre-training phase.", "description": "This table compares different pre-training strategies used by various models in the paper.  It shows whether the models were trained using standard non-DP methods (black) or DP methods (green). The self-supervised methods are indicated by a \u2020.  The number of images used in pre-training is given, noting where the pre-training wasn't fully private.", "section": "5 DP vision foundation models on ImageNet"}, {"figure_path": "GQrk0WGNiC/tables/tables_9_1.jpg", "caption": "Table 4: Standard/DP fine-tuning accuracy with the same architecture (ViT-Base) and pre-training dataset (ImageNet-21k) up to subsampling and preprocessing. Number of processed images by each model is indicated in the parenthesis.", "description": "This table compares the fine-tuning accuracy of different models on four datasets (CIFAR10, CIFAR100, Food101, and SVHN) under standard and differentially private (DP) settings. The pre-training dataset used is ImageNet-21k for all models. The number of processed images during pre-training is also shown.  The table showcases the performance of the proposed DP continual pre-training method compared to other state-of-the-art methods. The different epsilon values (\u03b5) for DP training demonstrate the trade-off between privacy and accuracy.", "section": "5 Downstream performance"}, {"figure_path": "GQrk0WGNiC/tables/tables_9_2.jpg", "caption": "Table 5: Few-shot accuracy of DP pre-trained models (TAN, ViP and ours) and their non-DP initialization.", "description": "This table presents the few-shot accuracy results on several downstream tasks for three different differentially private (DP) pre-trained models: TAN, ViP, and the authors' model.  For each model, results are shown for both DP and non-DP initialization methods, and different few-shot settings (10-shot and 20-shot for Aircraft, and 10-shot and 30-shot for CIFAR100) are included.", "section": "5.4 Privacy protection"}, {"figure_path": "GQrk0WGNiC/tables/tables_9_3.jpg", "caption": "Table 6: Linear-probing accuracy (non-DP) of pre-trained models, except \"full\" indicating full-parameter.", "description": "This table compares the linear probing accuracy (non-DP) of several pre-trained models on three downstream tasks: Aircraft (10-shot and 20-shot), CIFAR100 (10-shot and 30-shot), and Places365.  It shows the performance of models like TAN, ViP, DINO, and the authors' model (with different epsilon values and with/without full parameter tuning), highlighting the effectiveness of the proposed DP continual pre-training strategy.", "section": "5.4 Privacy protection"}, {"figure_path": "GQrk0WGNiC/tables/tables_9_4.jpg", "caption": "Table 7: Membership inference attack results. Values closer to 0.5 indicate better privacy protection.", "description": "This table presents the results of a membership inference attack (MIA) to evaluate the privacy protection offered by the proposed differentially private (DP) pre-training methods.  The metrics shown are accuracy, precision, recall, F1-score, and AUC, each measuring a different aspect of the model's ability to prevent inference of training data membership.  Values closer to 0.5 indicate stronger privacy protection, as a perfect classifier would achieve 0.5 AUC (random guessing). The results are compared to MIIL, a state-of-the-art non-DP model, demonstrating the effectiveness of the DP approach at epsilon values of 2 and 8.", "section": "5.4 Privacy protection"}, {"figure_path": "GQrk0WGNiC/tables/tables_18_1.jpg", "caption": "Table 3: Pre-training strategies of models. Standard non-DP training is marked in black; DP training is in green. \u2020 indicates self-supervised without using the labels. \u201cImages \u00d7\u201d is the total number of images used (dataset size \u00d7 epochs). \u201cNon-privacy\u201d means no DP guarantee on a subset of training data due to the non-DP pre-training phase.", "description": "This table compares different pre-training strategies used in various models.  It indicates whether the model used differential privacy (DP) during pre-training and provides the dataset used and the number of images processed. It also notes if there was any non-private data used in the pre-training stage.", "section": "5 DP vision foundation models on ImageNet"}, {"figure_path": "GQrk0WGNiC/tables/tables_25_1.jpg", "caption": "Table 3: Pre-training strategies of models. Standard non-DP training is marked in black; DP training is in green. \u2020 indicates self-supervised without using the labels. \u201cImages \u00d7\u201d is the total number of images used (dataset size \u00d7 epochs). \u201cNon-privacy\u201d means no DP guarantee on a subset of training data due to the non-DP pre-training phase.", "description": "This table compares different pre-training strategies used to train vision transformer models. It highlights whether the training was differentially private (DP) or not, the type of self-supervised learning used, and the total number of images used in the training process.  It also notes if there was a non-private phase in the pre-training.", "section": "5 DP vision foundation models on ImageNet"}]