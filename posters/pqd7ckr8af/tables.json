[{"figure_path": "pqD7ckR8AF/tables/tables_4_1.jpg", "caption": "Table 1: Comparison of l2-norm perturbations using DF and SDF algorithms on CIFAR10, employing consistent model architectures and hyperparameters as those used in [5, 47] studies.", "description": "This table compares the performance of the DeepFool (DF) algorithm and the proposed SuperDeepFool (SDF) algorithms in finding adversarial perturbations for images from the CIFAR-10 dataset.  It shows the median l2-norm of the perturbations found and the average number of gradient computations required by each algorithm, demonstrating the efficiency and effectiveness of the SDF method.", "section": "4.1 Comparison with DeepFool (DF)"}, {"figure_path": "pqD7ckR8AF/tables/tables_5_1.jpg", "caption": "Table 1: Comparison of l2-norm perturbations using DF and SDF algorithms on CIFAR10, employing consistent model architectures and hyperparameters as those used in [5, 47] studies.", "description": "This table compares the median l2-norm of adversarial perturbations and the number of gradient computations required by different attack algorithms (DF and various SDF variants) on the CIFAR-10 dataset.  Consistent model architectures and hyperparameters were used across all algorithms for a fair comparison.  The results show that SDF(\u221e,1) achieves the smallest perturbations but with a slightly higher computational cost compared to DF.", "section": "4.1 Comparison with DeepFool (DF)"}, {"figure_path": "pqD7ckR8AF/tables/tables_5_2.jpg", "caption": "Table 2: The cosine similarity between the perturbation vector(r) and \u2207 f(x + r). We performed this experiment on three models trained on CIFAR10.", "description": "This table presents the results of an experiment comparing the orthogonality of perturbation vectors obtained from different attack methods (DF and variants of SDF) on three different models trained on the CIFAR-10 dataset.  The cosine similarity between the perturbation vector (r) and the gradient at the perturbed point (\u2207f(x+r)) is used as a metric to quantify orthogonality, higher values indicating greater orthogonality. The table helps demonstrate the effectiveness of SDF in producing perturbation vectors closer to being orthogonal to the decision boundary.", "section": "4.1 Comparison with DeepFool (DF)"}, {"figure_path": "pqD7ckR8AF/tables/tables_6_1.jpg", "caption": "Table 2: The cosine similarity between the perturbation vector(r) and \u2207f(x + r). We performed this experiment on three models trained on CIFAR10.", "description": "This table presents the cosine similarity between the perturbation vector (r) and the gradient at x + r (\u2207f(x+r)) for different attack methods (DF and various SDF configurations) across three different model architectures (LeNet, ResNet18, and WRN-28-10) trained on the CIFAR-10 dataset.  A higher cosine similarity indicates a stronger alignment of the perturbation vector with the gradient, suggesting a more optimal perturbation closer to the decision boundary. The results show that SDF consistently achieves higher cosine similarity values compared to DF, particularly for more complex models.", "section": "4.1 Comparison with DeepFool (DF)"}, {"figure_path": "pqD7ckR8AF/tables/tables_6_2.jpg", "caption": "Table 3: We evaluate the performance of iteration-based attacks on MNIST using IBP models, noting the iteration count in parentheses. Our analysis focuses on the best-performing versions, highlighting their significant costs when encountered powerful robust models.", "description": "This table compares the performance of several iterative adversarial attacks (DF, ALMA, DDN, FAB, FMN, C&W, and SDF) on an adversarially trained IBP model for the MNIST dataset.  It shows the fooling rate (FR), median l2-norm of perturbations, and the number of gradient computations required by each attack.  The results highlight the trade-off between attack effectiveness and computational cost.", "section": "4.2 Comparison with minimum-norm attacks"}, {"figure_path": "pqD7ckR8AF/tables/tables_7_1.jpg", "caption": "Table 4: Performance of attacks on the CIFAR-10 dataset with naturally trained WRN-28-10.", "description": "This table compares the performance of different attack methods (DF, ALMA, DDN, FAB, FMN, C&W, and SDF) on a naturally trained Wide Residual Network (WRN-28-10) model using the CIFAR-10 dataset.  The metrics shown are the fooling rate (FR), the median l2-norm of the perturbations, and the number of gradient computations required.  The results demonstrate that SDF outperforms other methods in achieving a high fooling rate with a significantly smaller median perturbation and fewer gradient computations.", "section": "4.2 Comparison with minimum-norm attacks"}, {"figure_path": "pqD7ckR8AF/tables/tables_7_2.jpg", "caption": "Table 5: Performance comparison of SDF with other SOTA attacks on ImageNet dataset with natural trained RN-50 and adversarially trained RN-50.", "description": "This table compares the performance of the SuperDeepFool (SDF) attack against other state-of-the-art (SOTA) minimum l2-norm attacks on the ImageNet dataset.  It shows the fooling rate (FR), median l2-norm of the perturbations, and the number of gradient computations required for both a naturally trained ResNet-50 (RN-50) model and an adversarially trained RN-50 model.  The results demonstrate SDF's superior performance in terms of finding smaller perturbations while requiring fewer computations.", "section": "4.2 Comparison with minimum-norm attacks"}, {"figure_path": "pqD7ckR8AF/tables/tables_7_3.jpg", "caption": "Table 6: The comparison between l2 robustness of our adversarially trained model and [47] model.", "description": "This table compares the l2 robustness of an adversarially trained model using the SDF attack with the model from the paper [47].  The comparison uses several different attacks (DDN, FAB, FMN, ALMA, and SDF) to assess the median and mean l2-norms of the adversarial perturbations obtained by each attack. The results show the impact of the adversarial training method used on the robustness of the model against these various attacks.", "section": "4.3 SDF Adversarial Training (AT)"}, {"figure_path": "pqD7ckR8AF/tables/tables_8_1.jpg", "caption": "Table 7: Average input curvature of AT models. According to the measures proposed in [52].", "description": "This table compares the average input curvature of three adversarially trained models: a standard model, a model trained using DDN, and a model trained using SDF.  The input curvature is a measure of the non-linearity of the model's decision boundary.  Lower curvature generally corresponds to higher robustness against adversarial attacks.  The table shows that the SDF adversarially trained model exhibits significantly lower input curvature than the other two models.", "section": "4.3 SDF Adversarial Training (AT)"}, {"figure_path": "pqD7ckR8AF/tables/tables_8_2.jpg", "caption": "Table 8: Analysis of robust accuracy for various defense strategies against AA++ and AA with \u03b5 = 0.5 for six adversarially trained models on CIFAR10. All models are taken from the RobustBench library [12].", "description": "This table compares the robust accuracy of six adversarially trained models on CIFAR-10 against two versions of the AutoAttack (AA) method: the original AA and a faster version called AA++.  It shows the clean accuracy, robust accuracy under AA, the number of gradient computations needed for AA, robust accuracy under AA++, and the gradient computations for AA++. The results demonstrate that AA++ achieves similar robust accuracy to AA but with significantly fewer gradient computations, highlighting its increased efficiency.", "section": "4.3 SDF Adversarial Training (AT)"}, {"figure_path": "pqD7ckR8AF/tables/tables_15_1.jpg", "caption": "Table 9: Comparison of the effectiveness of line search on the CIFAR10 data for SDF and DF. We use one regularly trained model S (WRN-28-10) and three adversarially trained models (shown with R1 [47], R2 [3] and R3 [42]). \u2713 and X indicate the presence and absence of line search respectively.", "description": "This table compares the effectiveness of adding a line search step to the DeepFool (DF) and SuperDeepFool (SDF) algorithms.  It shows the median l2-norm of adversarial perturbations found by each algorithm, both with and without the line search, on four different models: one regularly trained model and three adversarially trained models. The results demonstrate how the line search impacts the performance of each algorithm in finding minimal adversarial perturbations.", "section": "4.1 Comparison with DeepFool (DF)"}, {"figure_path": "pqD7ckR8AF/tables/tables_15_2.jpg", "caption": "Table 10: Comparison of the effectiveness of line search on the CIFAR-10 data for other attacks. Line search effects are a little for DDN and ALMA. For FMN and FAB because they use line search at the end of their algorithms (they remind this algorithm as a binary search and final search, respectively), line search does not become effective.", "description": "This table compares the impact of adding a line search step to four different minimum-norm adversarial attack algorithms (DDN, ALMA, FMN, FAB) on three adversarially trained models and one normally trained model on the CIFAR-10 dataset.  The results show that for DDN and ALMA, the line search provides only a marginal improvement.  However, for FMN and FAB, the line search does not significantly improve the attacks because these algorithms already incorporate a line search as part of their procedures.  The table demonstrates the effect of a line search on the effectiveness of minimum-norm adversarial attacks on different models, highlighting that its impact is highly dependent on the algorithm's design.", "section": "4 Experimental Results"}, {"figure_path": "pqD7ckR8AF/tables/tables_16_1.jpg", "caption": "Table 11: Comparison of SDF with other state-of-the-art attacks for median l2 on CIFAR-10 dataset for adversarially trained network (PRN-18 [42]).", "description": "This table compares the performance of the SuperDeepFool (SDF) attack against other state-of-the-art minimum-l2 norm attacks on an adversarially trained PreActResNet-18 model using the CIFAR-10 dataset.  It shows the fooling rate (FR), median l2 perturbation norm, and the number of gradient computations required for each attack.  The results indicate that SDF achieves a comparable fooling rate with a significantly smaller median l2 perturbation and far fewer gradient computations than the other methods.", "section": "4.2 Comparison with minimum-norm attacks"}, {"figure_path": "pqD7ckR8AF/tables/tables_16_2.jpg", "caption": "Table 12: Robustness results of adversarially trained models on CIFAR-10 with l\u221e-AA. We perform this experiment on 1000 samples for each \u025b.", "description": "This table presents the robustness results of adversarially trained models on the CIFAR-10 dataset against the l\u221e-norm AutoAttack (AA).  The experiment was conducted on 1000 samples for each epsilon (\u03b5) value, comparing the performance of a model adversarially trained using the DeepFool (DF) method against one trained with the SuperDeepFool (SDF) method.  The results showcase the robustness (in terms of percentage) of each model against the attack for different epsilon values.", "section": "4.1 Comparison with DeepFool (DF)"}, {"figure_path": "pqD7ckR8AF/tables/tables_16_3.jpg", "caption": "Table 13: Robustness results of adversarially trained models on CIFAR-10 with l2-AA. We perform this experiment on 1000 samples for each \u03b5.", "description": "This table presents the robustness results of adversarially trained models (DDN and SDF) on the CIFAR-10 dataset against l2-norm Auto-Attack (AA).  The models' robustness is evaluated across different perturbation levels (\u03b5 = 0.3, 0.4, 0.5, 0.6). The results showcase the performance of the adversarially trained models against the l2-AA attack, and the natural accuracy for comparison.", "section": "4.2 Comparison with minimum-norm attacks"}, {"figure_path": "pqD7ckR8AF/tables/tables_18_1.jpg", "caption": "Table 14: This table shows the l2-median for the minimum-norm attacks. For all networks, we set learning rate = 0.01 and weight decay = 0.01. For training with Lp-pooling, we set p = 2 for all settings.", "description": "This table compares the median l2-norm of perturbations achieved by different minimum-norm attacks (DF, DDN, FMN, C&W, ALMA, and SDF) on two different network architectures (ResNet-18 and MobileNet).  The comparison is done for networks trained without pooling, with max-pooling, and with Lp-pooling (p=2). The values represent the median l2-norm of the adversarial perturbations found by each attack method.", "section": "4.2 Comparison with minimum-norm attacks"}, {"figure_path": "pqD7ckR8AF/tables/tables_18_2.jpg", "caption": "Table 15: This table shows the robust accuracy for all networks against to the AA and PGD. For training with Lp-pooling, we set p = 2 for all settings.", "description": "This table compares the robust accuracy of different models (RN18 and MobileNet) against two types of adversarial attacks (AA and PGD).  It shows the impact of using different pooling techniques (no pooling, max-pooling, and Lp-pooling) on the model's robustness.  The results illustrate how the choice of pooling layer affects the model's resilience to adversarial examples.", "section": "4.2 Comparison with minimum-norm attacks"}, {"figure_path": "pqD7ckR8AF/tables/tables_18_3.jpg", "caption": "Table 16: Model geometry of different ResNet-18 models. W (with pooling) and W/O (without pooling).", "description": "This table presents a comparison of the geometric properties of two ResNet-18 models: one trained with max-pooling and another trained without. The properties compared include the average L2-norm of the gradient (||\u2207f(x)||2), the average spectral norm of the Hessian (||\u2207\u00b2f(x)||2), and the average normalized curvature (Cf(x)). The results show that the model trained with max-pooling exhibits significantly smaller values for all three properties, indicating a smoother decision boundary.", "section": "G.1 Max-pooling's effect on the decision boundary's curvature"}, {"figure_path": "pqD7ckR8AF/tables/tables_19_1.jpg", "caption": "Table 17: Model geometry for regular and adversarially trained models.", "description": "This table presents a comparison of the model geometry for regular, DDN adversarially trained, and SDF adversarially trained models.  It shows the average l2 norm of the gradient (||\u2207f(x)||2), the average l2 norm of the Hessian (||\u2207\u00b2f(x)||2), and the average normalized curvature (Cf(x)). The normalized curvature is a measure of the local non-linearity around data points, and lower values indicate greater robustness.  The results demonstrate that the SDF adversarially trained model achieves significantly lower values for all three metrics, indicating improved robustness.", "section": "4.3 SDF Adversarial Training (AT)"}, {"figure_path": "pqD7ckR8AF/tables/tables_19_2.jpg", "caption": "Table 1: Comparison of l2-norm perturbations using DF and SDF algorithms on CIFAR10, employing consistent model architectures and hyperparameters as those used in [5, 47] studies.", "description": "This table compares the performance of the DeepFool (DF) attack and the proposed SuperDeepFool (SDF) attack in finding minimal l2-norm adversarial perturbations on the CIFAR10 dataset.  Consistent model architectures and hyperparameters were used for a fair comparison, following the methodology of previous studies ([5, 47]). The table shows the median l2-norm of the perturbations and the number of gradient computations required by each method.", "section": "4.1 Comparison with DeepFool (DF)"}, {"figure_path": "pqD7ckR8AF/tables/tables_20_1.jpg", "caption": "Table 1: Comparison of l2-norm perturbations using DF and SDF algorithms on CIFAR10, employing consistent model architectures and hyperparameters as those used in [5, 47] studies.", "description": "This table presents a comparison of the median l2-norm of adversarial perturbations and the number of gradient computations required by different attack algorithms on the CIFAR-10 dataset.  The algorithms compared include DeepFool (DF) and several variants of SuperDeepFool (SDF). The results show that SDF consistently finds significantly smaller perturbations than DF, with only a modest increase in computational cost.", "section": "4.1 Comparison with DeepFool (DF)"}, {"figure_path": "pqD7ckR8AF/tables/tables_20_2.jpg", "caption": "Table K: We show the result of evaluating adversarial attacks on naturally trained SmallCNN on MNIST dataset.", "description": "This table presents the results of evaluating several adversarial attacks (ALMA, DDN, FAB, FMN, C&W, and SDF) on a naturally trained SmallCNN model using the MNIST dataset. The table shows the fooling rate (FR), the median l2-norm of the adversarial perturbations, and the number of gradient computations required for each attack.  The results highlight the relative performance of different attack methods in terms of effectiveness and computational efficiency.", "section": "K Natural (Regular) Trained MNIST Model"}, {"figure_path": "pqD7ckR8AF/tables/tables_20_3.jpg", "caption": "Table 18: Runtime comparison for adversarial attacks on WRN-28-10 architecture trained on CIFAR10, for both naturally trained model and adversarially trained models.", "description": "This table compares the runtime of different adversarial attacks (ALMA, DDN, FAB, FMN, C&W, and SDF) on a Wide ResNet-28-10 (WRN-28-10) model trained on the CIFAR-10 dataset.  It shows the runtime in seconds and the median l2-norm of the generated adversarial perturbations for both a naturally trained model and a model adversarially trained using the R1 method from [44].  The table highlights the significantly faster runtime of SDF compared to other methods, especially the computationally expensive Carlini & Wagner (C&W) attack.", "section": "4.2 Comparison with minimum-norm attacks"}, {"figure_path": "pqD7ckR8AF/tables/tables_21_1.jpg", "caption": "Table 19: Performance of SDFe on two robust networks trained on CIFAR-10 dataset.", "description": "This table compares the performance of the modified SDF (SDFe) with other state-of-the-art attacks (DF, FMN, FAB) on two pre-trained robust networks (M1 and M2) on the CIFAR-10 dataset. The results show that SDFe outperforms other algorithms in discovering smaller perturbations.  The metrics reported include the median l\u221e-norm of perturbations, the fooling rate (FR), and the number of gradient computations required.", "section": "N Limitations"}, {"figure_path": "pqD7ckR8AF/tables/tables_22_1.jpg", "caption": "Table 20: Performance of targeted SDF on a standard trained WRN-28-10 on CIFAR-10, measured using 1000 random samples.", "description": "This table presents the performance comparison of targeted and untargeted adversarial attacks (DDN, FMN, and SDF) on a standard-trained Wide Residual Network 28-10 model for CIFAR-10 dataset. The evaluation metrics include fooling rate (FR), mean L2 perturbation norm, median L2 perturbation norm, and the number of gradient computations (Grads). The results show that while the targeted attacks perform similarly in terms of FR, SDF achieves a significantly lower number of gradient computations.", "section": "4.3 SDF Adversarial Training (AT)"}]