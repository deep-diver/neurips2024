[{"figure_path": "GZnsqBwHAG/tables/tables_6_1.jpg", "caption": "Table 1: Finetuning on few-shot harmful data breaks LLM's safety alignment at different rates and our VISAGE safety metric successfully measures the rate. LLaMA2 has a higher VISAGE score than Vicuna, and the ASRS on AdvBench indicate that when finetuned with the same amount of harmful data, LLaMA2 remains safer than Vicuna. Additionally, we demonstrate that finetuning with a mixture of safe and harmful data helps the model maintain its safety alignment. The \"aligned\" column refers to the original off-the-shelf models.", "description": "This table presents the results of fine-tuning two LLMs (LLaMA2-7B-chat and Vicuna-7B-v1.5) with varying amounts of harmful data (10-shot, 50-shot, 100-shot) and a mix of harmful and safe data.  The VISAGE score, a new metric introduced in the paper, measures the robustness of the models' safety.  Lower ASR (Attack Success Rate) indicates better safety. The table shows that finetuning with harmful data significantly reduces the models' safety, while incorporating safe data helps mitigate this effect. LLaMA2 consistently shows a higher VISAGE score and lower ASR than Vicuna, indicating greater safety and robustness.", "section": "4.2 Finetuning on few-shot harmful data breaks LLM's safety alignment"}, {"figure_path": "GZnsqBwHAG/tables/tables_7_1.jpg", "caption": "Table 2: LLM safety landscape highlights the system prompt's critical role in protecting a model, and how this protection transfers to its perturbed variants in the safety basin. We measure the VISAGE Score of different system prompt for popular open-source LLMs. Higher VISAGE means safer model and \"-\" means not applicable. For LLaMA3, there is no default system prompt in the initial release. For all other LLMs in the \"safety\" column, we use the optimized safety prompts specific to each LLM from Zheng et al. [49], with only Mistral's safety system prompt provided.", "description": "This table presents the VISAGE scores for various LLMs under different system prompts.  The VISAGE score is a metric measuring the robustness of a model's safety to perturbations. It shows how the default, empty (no prompt), roleplay, LLaMA2's default prompt, and optimized safety prompts affect safety for each LLM. Higher scores indicate better safety.", "section": "5 System prompt"}, {"figure_path": "GZnsqBwHAG/tables/tables_17_1.jpg", "caption": "Table 1: Finetuning on few-shot harmful data breaks LLM's safety alignment at different rates and our VISAGE safety metric successfully measures the rate. LLaMA2 has a higher VISAGE score than Vicuna, and the ASRS on AdvBench indicate that when finetuned with the same amount of harmful data, LLaMA2 remains safer than Vicuna. Additionally, we demonstrate that finetuning with a mixture of safe and harmful data helps the model maintain its safety alignment. The \"aligned\" column refers to the original off-the-shelf models.", "description": "This table presents the results of finetuning two LLMs (LLaMA2 and Vicuna) with varying amounts of harmful data (10, 50, and 100 examples).  It shows the impact of this finetuning on the models' safety, measured by both the VISAGE metric and the ASR (attack success rate) on the AdvBench benchmark.  The table demonstrates that finetuning with harmful data significantly reduces safety, while finetuning with a mixture of harmful and safe data helps maintain safety.  It also highlights that LLaMA2 is more robust to harmful finetuning than Vicuna.", "section": "4.2 Finetuning on few-shot harmful data breaks LLM's safety alignment"}]