[{"figure_path": "GZnsqBwHAG/figures/figures_1_1.jpg", "caption": "Figure 1: A. \"Safety basin\", a new phenomenon observed universally in the model parameter space of popular open-source LLMs. Our discovery inspires us to propose the new VISAGE safety metric that measures the safety in LLM finetuning by probing its safety landscape. B. Visualizing the safety landscape of the aligned model also enables us to understand why finetuning with harmful data compromises safety but finetuning with both harmful and safe data preserves the safety.", "description": "Figure 1 shows two key findings from the paper.  Panel A demonstrates that a \"safety basin\" universally exists in the parameter space of several popular open-source large language models (LLMs). This basin is a region where random perturbations to the model's weights do not significantly affect its safety. However, outside this basin, the model's safety dramatically decreases. This discovery led to the creation of a new safety metric called VISAGE. Panel B illustrates how finetuning affects the model's position within the safety landscape. Finetuning with only harmful data pushes the model out of the safety basin, compromising its safety. Conversely, finetuning with a mixture of harmful and safe data allows the model to remain within the safety basin and maintain its safety.", "section": "1 Introduction"}, {"figure_path": "GZnsqBwHAG/figures/figures_4_1.jpg", "caption": "Figure 2: LLM safety landscape: (a) 1D-interpolation LLaMA2-7B \u2192 LLaMA2-7B-chat safety landscape. When given two models varied by fine-tuning, we utilize linear interpolation to visualize the changes between them. While interpolating the model weights between the base and the chat model, we need to ensure the chat format remains consistent. Thus, we ablate on both chat formats: text completion (no template) and LLaMA2 chat template. The chat model exhibits higher safety than the base model as expected. The base model also shows an increase in safety while using the LLaMA2 chat template. (b) 1D-random LLaMA2-7B safety landscape sampled over different random directions. When provided with a single model, we sample a random normalized direction to visualize its local variations along both positive and negative directions.", "description": "This figure shows the safety landscape of LLaMA2. The left panel (a) shows the 1D interpolation safety landscape between the pre-trained LLaMA2-7B and the aligned LLaMA2-7B-chat models. It demonstrates how the safety changes as the model parameters are linearly interpolated between the two models, considering two different chat formats: text completion and LLaMA2 chat template. The right panel (b) illustrates the 1D random safety landscape of the aligned LLaMA2-7B-chat model, showing how safety varies when the model weights are randomly perturbed along different directions. Both panels use ASR as the safety metric.", "section": "3.3 Safety Landscape of Open-source LLMs"}, {"figure_path": "GZnsqBwHAG/figures/figures_4_2.jpg", "caption": "Figure 1: A. \"Safety basin\", a new phenomenon observed universally in the model parameter space of popular open-source LLMs. Our discovery inspires us to propose the new VISAGE safety metric that measures the safety in LLM finetuning by probing its safety landscape. B. Visualizing the safety landscape of the aligned model also enables us to understand why finetuning with harmful data compromises safety but finetuning with both harmful and safe data preserves the safety.", "description": "Figure 1 shows two key findings from the paper.  Part A illustrates the \"safety basin\" phenomenon.  This shows that for several popular open-source LLMs, randomly perturbing the model's weights within a certain local neighborhood preserves the model's safety. However, outside this region, safety drops sharply. Part B shows how the safety landscape is useful for visualizing the impact of finetuning: finetuning with harmful data moves the model out of the safety basin and compromises its safety, while finetuning with a mix of harmful and safe data keeps it within the safety basin and preserves safety.", "section": "1 Introduction"}, {"figure_path": "GZnsqBwHAG/figures/figures_8_1.jpg", "caption": "Figure 3: The system prompt has a strong impact on LLM safety landscape. From an attacker's standpoint, we find that both removing the default system prompt and using simple roleplaying prompt jeopardizes the safety alignment, with the former exhibiting greater potency. From a defender's perspective, we discover that LLaMA2's original system prompt universally enhances safety across models, and safety prompts optimized through prompt tuning for a specific model also enhances safety for all models inside the safety basin.", "description": "This figure shows the impact of different system prompts on the safety landscape of two LLMs: Mistral and Vicuna.  It demonstrates that removing the default system prompt significantly reduces safety, while using a roleplaying prompt has a mixed effect.  Conversely, using LLaMA2's default system prompt or a safety prompt optimized for the specific LLM significantly improves safety across the entire safety basin. The figure visually represents the relationship between prompt variations, model safety, and perturbations to model weights.", "section": "5 System prompt"}, {"figure_path": "GZnsqBwHAG/figures/figures_8_2.jpg", "caption": "Figure 3: The system prompt has a strong impact on LLM safety landscape. From an attacker's standpoint, we find that both removing the default system prompt and using simple roleplaying prompt jeopardizes the safety alignment, with the former exhibiting greater potency. From a defender's perspective, we discover that LLaMA2's original system prompt universally enhances safety across models, and safety prompts optimized through prompt tuning for a specific model also enhances safety for all models inside the safety basin.", "description": "This figure shows the impact of different system prompts on the safety landscape of two LLMs: Mistral and Vicuna.  It compares the effects of removing the default system prompt, using a roleplaying prompt, using the default LLaMA2 system prompt, and using a safety prompt optimized for each specific model. The results demonstrate that the system prompt significantly influences the safety of the LLMs, and that the original LLaMA2 prompt can enhance safety across models.", "section": "5 System prompt"}, {"figure_path": "GZnsqBwHAG/figures/figures_9_1.jpg", "caption": "Figure 1: A. \"Safety basin\", a new phenomenon observed universally in the model parameter space of popular open-source LLMs. Our discovery inspires us to propose the new VISAGE safety metric that measures the safety in LLM finetuning by probing its safety landscape. B. Visualizing the safety landscape of the aligned model also enables us to understand why finetuning with harmful data compromises safety but finetuning with both harmful and safe data preserves the safety.", "description": "Figure 1 shows two key findings about the safety of Large Language Models (LLMs).  Part A illustrates the concept of a \"safety basin\" in the model's parameter space.  Randomly perturbing the model's weights within this basin maintains its safety. Outside the basin, safety is immediately compromised.  Part B demonstrates how fine-tuning with only harmful data moves the model out of the safety basin, compromising safety, while fine-tuning with a mixture of harmful and safe data helps keep the model within the safety basin, preserving safety.", "section": "1 Introduction"}, {"figure_path": "GZnsqBwHAG/figures/figures_15_1.jpg", "caption": "Figure 5: LLaMA2-7B-chat\u2019s perturbation shows a basin shape similar to the safety keyword detection.", "description": "The figure displays the results of perturbing the LLaMA2-7B-chat model's weights and measuring the attack success rate (ASR) using two different safety metrics: keyword detection and LLaMAGuard 2.  Both metrics reveal a similar \"safety basin\" pattern.  The ASR remains high near the original model's weights but drops significantly when the weights are perturbed outside of a certain region. This illustrates the concept of a safety basin where small changes to model parameters maintain safety, while larger changes quickly compromise it.", "section": "3.3 Safety Landscape of Open-source LLMs"}, {"figure_path": "GZnsqBwHAG/figures/figures_15_2.jpg", "caption": "Figure 6: Results on POSE benchmark again verifies the safety basin observed on the AdvBench benchmark. We evaluate the generated outputs using both safety keyword detection and LLaMAGuard 2 and both evaluation metrics show a similar basin shape.", "description": "This figure shows the results of evaluating the safety of the LLaMA2-7B-chat model on the POSE benchmark using two different safety metrics: keyword detection and LLaMAGuard 2. The x-axis represents the perturbation amount, and the y-axis represents the ASR (attack success rate). Both metrics show a similar basin-like shape, indicating that the model's safety is maintained within a local region of its parameter space, but it can be easily compromised outside of this region.", "section": "3.3 Safety Landscape of Open-source LLMs"}, {"figure_path": "GZnsqBwHAG/figures/figures_16_1.jpg", "caption": "Figure 7: The shape of the capability score varies significantly across different datasets, and differs from the safety landscape. We evaluate capabilities using the following three datasets from MMLU: abstract_algebra, high_school_us_history, and us_foreign_policy datasets, and present the results of perturbing the LLaMA2-7B-chat weights along a 1D-random direction. For controlled comparisons, all datasets are evaluated along the same random direction.", "description": "This figure compares the capability landscape across three different datasets from MMLU (abstract algebra, high school US history, and US foreign policy) with the safety landscape. The x-axis represents the perturbation amount, while the y-axis represents the accuracy. The figure shows that the shape of the capability landscape varies significantly across different datasets, unlike the safety landscape which exhibits a consistent basin shape. This confirms that the basin shape is unique to the safety of LLMs and not a general property of model landscapes.", "section": "Safety vs. Capability Landscape"}, {"figure_path": "GZnsqBwHAG/figures/figures_16_2.jpg", "caption": "Figure 8: LLMs speak fluently even when ASR is high. We evaluate the perplexity of the perturbed LLaMA2-7B-chat model along a random direction using all 80 prompts from MTBench. We also scale up the model size from LLaMA2-7B-chat to LLaMA2-13B-chat and plot the 1D safety landscape of both models. A larger model size exhibits a wider safety basin, which also aligns with the intuition that a wider basin seems to be more robust and a potential training goal for future LLM training.", "description": "This figure shows the perplexity and ASR for LLaMA2-7B-chat and LLaMA2-13B-chat models as their weights are perturbed along a random direction.  The perplexity, measuring fluency, remains relatively low even when the ASR (adversarial success rate), measuring safety, is high.  The larger 13B model displays a wider safety basin (region of parameter space where the model remains safe), supporting the idea that larger models are more robust.", "section": "3.4 VISAGE Safety Metric"}]