[{"figure_path": "OdJKB9jSa5/figures/figures_2_1.jpg", "caption": "Figure 1: STk Architecture. For any layer of neurons in a neural network, to solve the Top-k problem for its weights, insert an STk Module. The trainable parameter \u03bb will gradually approximate the k-th largest element during the optimization process. And this \u03bb can be used to filter neurons.", "description": "This figure illustrates the architecture of the Smoothed Top-k (STk) module.  The STk module is designed to be inserted into any layer of a neural network to efficiently solve the Top-k problem. The module utilizes a single trainable parameter, \u03bb (lambda), which dynamically approximates the k-th largest element within a set of elements.  This approximation allows for the selection of the top-k elements without requiring computationally expensive sorting algorithms, making the process differentiable and efficient within the neural network's computational graph. The diagram shows the STk module integrated between two fully-connected layers, highlighting its seamless integration into existing network structures. The graph to the right of the module illustrates the smoothed approximation function used.", "section": "2 STk Architecture"}, {"figure_path": "OdJKB9jSa5/figures/figures_3_1.jpg", "caption": "Figure 2: ReLU and SReLU with various smoothing coefficients \u03b4.", "description": "This figure compares the ReLU (Rectified Linear Unit) activation function with the smoothed ReLU (SReLU) function for different smoothing coefficients (\u03b4). The SReLU function is a smooth approximation of the ReLU function, which is non-differentiable at x = 0.  As the smoothing coefficient (\u03b4) decreases, the SReLU function increasingly approximates the ReLU function, demonstrating its uniform convergence to ReLU as \u03b4 approaches 0.  This is important because the SReLU function's differentiability allows for easier optimization in neural networks.", "section": "3 From ATk Loss to STk Loss"}, {"figure_path": "OdJKB9jSa5/figures/figures_4_1.jpg", "caption": "Figure 2: ReLU and SReLU with various smoothing coefficients \u03b4.", "description": "This figure compares the ReLU (Rectified Linear Unit) activation function with the smoothed ReLU (SReLU) activation function.  The SReLU function is a smoothed approximation of the ReLU function, designed to be differentiable everywhere.  The figure shows how the SReLU function approaches the ReLU function as the smoothing coefficient (\u03b4) decreases.  This is important because the SReLU function is used within the STk module described in the paper to make the Top-k operation fully differentiable.", "section": "3 From ATk Loss to STk Loss"}, {"figure_path": "OdJKB9jSa5/figures/figures_5_1.jpg", "caption": "Figure 3: A Synthetic Example on 2D-Plain.", "description": "This figure shows a synthetic example on a 2D plane to illustrate the capability of the STk loss in approximating the ideal decision boundary.  It displays a scatter plot of data points belonging to two categories (positive and negative), colored red and blue respectively.  A decision boundary line separates the two categories, visually demonstrating how well a model trained with STk loss approximates this theoretical boundary.", "section": "4.2 Gaussian Distributed Dataset"}, {"figure_path": "OdJKB9jSa5/figures/figures_6_1.jpg", "caption": "Figure 4: Accuracy and ParaF1-Score vs Negative Sample Ratio.", "description": "This figure shows the performance of different aggregate loss functions (STk, ATk, Average) under varying negative sample ratios.  The x-axis represents the negative sample ratio, while the y-axis shows both the accuracy and ParaF1-score.  It demonstrates how the performance of each method changes as the class imbalance increases. STk consistently outperforms ATk and Average across all negative ratios, showing its robustness to class imbalance.", "section": "4.2 Gaussian Distributed Dataset"}, {"figure_path": "OdJKB9jSa5/figures/figures_8_1.jpg", "caption": "Figure 5: ImageNet-LT.", "description": "This figure shows the class distribution in the ImageNet-LT dataset. The x-axis represents the classes, and the y-axis represents the number of samples per class.  The dataset is highly imbalanced; a few classes (head classes) have many samples while most classes (tail classes) have very few samples. Representative images are shown for some of the head and tail classes, illustrating the variety of images found within each class.", "section": "5.2 Long-Tailed Classification"}]