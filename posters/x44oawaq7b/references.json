{"references": [{"fullname_first_author": "S. Bhojanapalli", "paper_title": "Dropping convexity for faster semi-definite optimization", "publication_date": "2016-00-00", "reason": "This paper is foundational for understanding the convergence of first-order methods in non-convex optimization problems, which is the core topic of this paper."}, {"fullname_first_author": "S. Du", "paper_title": "Width provably matters in optimization for deep linear neural networks", "publication_date": "2019-00-00", "reason": "This paper provides a theoretical understanding of the importance of network width in the optimization of deep linear neural networks, which is relevant to this paper's analysis of linear neural networks."}, {"fullname_first_author": "N. Halko", "paper_title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "publication_date": "2011-00-00", "reason": "This paper introduces randomized algorithms for matrix decomposition, which are closely related to the unbalanced initialization techniques used in this paper."}, {"fullname_first_author": "Y. Nesterov", "paper_title": "Introductory lectures on convex optimization: A basic course", "publication_date": "2013-00-00", "reason": "This book is a fundamental reference in optimization, and it is essential for understanding the theoretical background of Nesterov's Accelerated Gradient (NAG), which is a key method analyzed in this paper."}, {"fullname_first_author": "R. Ward", "paper_title": "Convergence of alternating gradient descent for matrix factorization", "publication_date": "2023-00-00", "reason": "This paper presents an analysis of alternating gradient descent for matrix factorization using unbalanced initialization, which is directly compared with this paper's results for gradient descent and NAG."}]}