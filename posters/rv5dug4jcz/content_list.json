[{"type": "text", "text": "Learning a Single Neuron Robustly to Distributional Shifts and Adversarial Label Noise ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shuyao Li\\* University of Wisconsin-Madison shuyao.li@wisc.edu ", "page_idx": 0}, {"type": "text", "text": "Sushrut Karmalkar\\* University of Wisconsin-Madison skarmalkar@wisc.edu ", "page_idx": 0}, {"type": "text", "text": "Ilias Diakonikolas University of Wisconsin-Madison ilias@cs.wisc.edu ", "page_idx": 0}, {"type": "text", "text": "Jelena Diakonikolas University of Wisconsin-Madison jelena@cs.wisc.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the problem of learning a single neuron with respect to the $L_{2}^{2}$ -loss in the presence of adversarial distribution shifts, where the labels can be arbitrary, and the goal is to find a \u201cbest-fit\u2019 function. More precisely, given training samples from a reference distribution $\\mu_{0}$ , the goal is to approximate the vector $\\pmb{w}^{*}$ which minimizes the squared loss with respect to the worst-case distribution that is close in $\\chi^{2}$ -divergence to $\\mu_{0}$ . We design a computationally efficient algorithm that recovers a vector $\\hat{w}$ satisfying $\\begin{array}{r}{\\mathbb{E}_{p^{*}}(\\sigma(\\bar{\\pmb{w}}\\cdot\\pmb{x})-\\bar{\\b{y}})^{2}\\leq C\\,\\mathbb{E}_{p^{*}}(\\sigma(\\pmb{w}^{*}\\cdot\\bar{\\pmb{x}})-y)^{2}+\\epsilon}\\end{array}$ , where $C>1$ is a dimension-independent constant and $(\\pmb{w}^{*},\\pmb{\\,\\mu}^{*})$ is the witness attaining the min-max risk $\\begin{array}{r}{\\operatorname*{min}_{\\pmb{w}:\\|\\pmb{w}\\|\\leq W}\\operatorname*{max}_{p}\\mathbb{E}_{(\\pmb{x},y)\\sim p}(\\sigma(\\pmb{w}\\cdot\\pmb{x})-y)^{2}-\\nu\\chi^{2}(p,p_{0})}\\end{array}$ Our algorithm follows a primal-dual framework and is designed by directly bounding the risk with respect to the original, nonconvex $L_{2}^{2}$ loss. From an optimization standpoint, our work opens new avenues for the design of primal-dual algorithms under structured nonconvexity. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The problem of learning a single neuron from randomly drawn labeled examples is a fundamental problem extensively studied in the machine learning literature. Given labeled examples $\\{(x_{i},y_{i}):$ $(\\pmb{x}_{i},y_{i})\\in\\mathbb{R}^{d}\\times\\mathbb{R}\\}_{i=1}^{N}$ drawn from a reference distribution $\\mu_{0}$ , the goal in this context is to recover a parameter vector $\\pmb{w}_{0}^{*}$ that minimizes the squared loss $\\Lambda_{\\sigma,p_{0}}(w)$ over a ball of radius $W>0$ ", "page_idx": 0}, {"type": "equation", "text": "$$\nw_{0}^{*}:=\\operatornamewithlimits{a r g\\,m i n}_{\\substack{w\\in\\mathbb{R}^{d}:\\|w\\|_{2}\\leq W}}\\Lambda_{\\sigma,\\eta_{0}}(\\pmb{w});\\quad\\Lambda_{\\sigma,p_{0}}(\\pmb{w}):=\\mathbb{E}_{(\\pmb{x},\\pmb{y})\\sim p_{0}}(\\sigma(\\pmb{w}\\cdot\\pmb{x})-y)^{2},\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ is a known (typically non-linear) non-decreasing activation function (e.g., the ReLU activation $\\sigma(t)=\\operatorname*{max}(0,t))$ and we denote by $\\mathrm{OPT}_{0}\\,=\\,\\mathrm{min}_{{\\pmb w}:\\|{\\pmb w}\\|_{2}\\le W}\\,\\Lambda_{\\sigma,p_{0}}({\\pmb w})$ the minimum squared loss. In the realizable setting \u2014 where $y=\\sigma(\\pmb{w}_{0}^{*}\\cdot\\pmb{x})$ and thus $\\mathrm{OPT_{0}=0}$ \u2014 this problem is well-understood and by now part of the folklore (see, e.g., [Ks09; $\\mathrm{Kak}{+}11$ ; Sol17; YS201). The results for the realizable setting also naturally extend to zero-mean bounded-variance label noise. ", "page_idx": 0}, {"type": "text", "text": "The more realistic agnostic model [Hau92; KSS92] (a.k.a. adversarial label noise) aims to identify the best-fitting neuron for a reference distribution of the examples, without any assumptions on label structure. However, it is known that in this setting finding a parameter vector with square loss $\\mathrm{OPT_{0}+}\\epsilon$ requires $d^{\\mathrm{poly}(1/\\epsilon)}$ time, even if the $\\textbf{\\em x}$ marginal distribution is Gaussian [GKK19; DKZ20; GGK20; $\\mathrm{Dia}{+}21$ ; DKR23]. Even if we relax our goal to achieve error $O(\\mathrm{OPT_{0})+\\epsilon}$ efficient algorithms only exist under strong distributional assumptions. In fact, without such assumptions, this problem is NP-hard [Sim02; MR18]. Recent work has also shown that (under cryptographic assumptions) no polynomial-time constant-factor improper learner exists even for distributions supported on the unitball $[\\mathrm{Dia}+22\\mathrm{b}]$ . Given these intractability results, recent work has focused on developing efficient constant-factor approximate learners under minimal distributional assumptions (see, e.g.,[Dia $+20$ ; FCG20; Dia $\\pm22\\mathbf{a}$ ; ATV23; $\\mathrm{Gol}{+}23$ ; Wan $\\pm23\\mathrm{a}$ $Z\\mathrm{ar}+24]$ ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "This recent progress notwithstanding, prior work primarily focused on the setting where only the labels might be corrupted, without considering possible distributional shifts or heterogeneity of the data. Such distributional corruptions are frequently observed in practice and have motivated a long line of research in areas such as domain adaptation and (related to it) distributionally robust optimization (DRO); see e.g., [BEN09; ND16; RM22; $\\mathrm{Bla}{+24}]$ and references therein. Thus, the main question motivating our work is: ", "page_idx": 1}, {"type": "text", "text": "How do adversarial changes in the underlying distribution impact the learnability of a neuron? ", "page_idx": 1}, {"type": "text", "text": "We study this question within the DRO framework, where the goal is to minimize the model's loss on a worst-case distribution from a set of distributions close to the reference distribution.2 This set of distributions, known as the ambiguity set, models possible distributional shifts of the data. In addition to being interesting on its own merits, the DRO framework arises in diverse contexts, including algorithmic fairness $[\\mathrm{Has}{+}18\\mathrm{b}]$ and class imbalance $[X\\mathrm{u}+20]$ . Moreover, it has recently found a range of applications in reinforcement learning $[\\mathrm{Kal}+22$ ; Liu $+22$ ${\\mathrm{Lot}}{+}23$ $\\mathrm{Wan}{+}23\\mathrm{b}$ $\\mathrm{Yan}{+}23$ $\\mathrm{Yu}\\!+\\!23$ 1, robotics $[\\mathrm{Sha}+20]$ , language modeling $[\\mathrm{Liu}+21]$ , sparse neural network training $[\\mathrm{Sap}+23]$ \uff0c and defense against model extraction $[\\mathrm{Wan}+23\\mathrm{c}]$ ", "page_idx": 1}, {"type": "text", "text": "Despite a range of impressive results in the DRO literature (see, e.g., recent surveys $[\\mathrm{Kuh}+19$ ;CP20; RM22; $\\mathrm{Bla}{+24}]$ I and references therein), algorithmic results with rigorous approximation guarantees for the loss have almost exclusively been obtained under fairly strong assumptions about the loss function involving both convexity and either smoothness or Lipschitzness, with linear regression being the prototypical example; see, e.g., [CP18; BMN21; DN21]. Unfortunately, this vanilla setting does not capture a range of machine learning applications, where a typical loss function is nonconvex. In particular, even the simplest ReLU learning problem in the realizable setting (with noise-free labels) is nonconvex. Further, existing DRO approaches for nonconvex loss functions such as [SND18; $\\mathrm{Qi}{+}21]$ I only guarantee convergence to a stationary point, which is insufficient for learning a ReLU neuron even without distributional ambiguity [YS2O]. Motivated by this gap in our understanding, in this work we initiate a rigorous algorithmic investigation of learning a neuron (arguably the simplest non-convex problem) in the DRO setting. We hope that this work will stimulate future research in this direction, potentially addressing more complex models in a principled manner. ", "page_idx": 1}, {"type": "text", "text": "Due to space constraints, we defer further discussion of related work to Appendix A. ", "page_idx": 1}, {"type": "text", "text": "1.1 Problem Setup ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "To formally define our setting, we recall the definition of $\\chi^{2}$ -divergence between distributions $\\boldsymbol{\\mu}$ and $p^{\\prime}$ givenby $\\begin{array}{r}{\\chi^{2}(p,p^{\\prime}):=\\int\\big(\\frac{\\mathrm{d}p}{\\mathrm{d}p^{\\prime}}-1\\big)^{2}\\mathrm{d}p^{\\prime}}\\end{array}$ We focus on the class of monotone unbounded activations introduced in $[\\mathrm{Dia}{+}22\\mathrm{a}]$ , for which we additionally assume convexity. Example activations in this class include the ReLU, leaky ReLU, exponential linear unit (ELU), and normalized3 SoftPlus. ", "page_idx": 1}, {"type": "text", "text": "Definition 1.1 (Unbounded. $\\mathrm{[Dia+22a]+Convex}$ Activation). Let $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ be a non-decreasing convex function, and let $\\alpha$ $\\beta>0$ . We say $\\sigma$ is $(\\alpha,\\beta)$ -unbounded if it satisfies the following: $(i)\\,\\sigma$ is $\\beta$ Lipschitz; ( $i)\\;\\sigma(t_{1})-\\sigma(t_{2})\\geq\\alpha(t_{1}-t_{2})$ for all $t_{1}\\geq t_{2}\\geq0$ and (ii) $\\sigma(0)=0$ ", "page_idx": 1}, {"type": "text", "text": "To formally state the problem, we further define the loss, risk, and optimal value (denoted by OPT). ", "page_idx": 1}, {"type": "text", "text": "Definition 1.2 (Loss, Risk, and OPT). Given a regularization parameter $\\nu$ and a reference distribution $\\mu_{0}$ ,let $\\mathcal{P}=\\mathcal{P}(\\boldsymbol{p}_{0})$ denote the set of all distributions that are absolutely continuous with respect to $\\mu_{0}$ and $\\mathcal{B}(W):=\\{\\pmb{w}:\\|\\pmb{w}\\|_{2}\\leq W\\}$ . We define the following: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{\\sigma}(\\pmb{w},\\pmb{p};p_{0}):=\\mathbb{E}_{(\\pmb{x},\\pmb{y})\\sim p}(\\sigma(\\pmb{w}\\cdot\\pmb{x})-y)^{2}-\\nu\\chi^{2}(\\pmb{p},p_{0})=\\Lambda_{\\sigma,p}(\\pmb{w})-\\nu\\chi^{2}(\\pmb{p},p_{0}),}\\\\ &{\\qquad R(\\pmb{w};p_{0}):=\\underset{p\\in\\mathcal{P}(p_{0})}{\\operatorname*{max}}L_{\\sigma}(\\pmb{w},\\pmb{p};p_{0}),\\ \\llb{w}:=\\underset{p\\in\\mathcal{P}(p_{0})}{\\operatorname*{arg\\,max}}L_{\\sigma}(\\pmb{w},\\pmb{p};p_{0}),}\\\\ &{\\qquad\\quad w^{\\ast}:=\\underset{w\\in\\mathcal{B}(W)}{\\operatorname*{arg\\,min}}R(\\pmb{w};p_{0}),\\quad p^{\\ast}:=q_{w^{\\ast}},}\\\\ &{\\qquad\\quad\\mathrm{OPT}:=\\mathbb{E}_{(\\pmb{x},\\pmb{y})\\sim p^{\\ast}}(\\sigma(\\pmb{w}^{\\ast}\\cdot\\pmb{x})-y)^{2}=\\Lambda_{\\sigma,p^{\\ast}}(\\pmb{w}^{\\ast}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We say that $L_{\\sigma}(\\mathbf{w},p;p_{0})$ is the regularized square loss function of a vector $\\mathbf{\\nabla}w$ and a distribution $p\\in\\mathcal P$ ; and $R(w;p_{0})$ is the DRO risk of $\\pmb{w}$ with respect to $\\ensuremath{\\mu}_{0}$ . We call $\\boldsymbol{p}^{*}$ the target distribution. ", "page_idx": 2}, {"type": "text", "text": "The minimization of the DRO risk as defined above corresponds to the regularized/penalized DRO formulation studied in prior work; see, e.g., [SND18; Wan $.+23c$ ; MDH24]. An alternate formulation would have been to instead optimize over a restricted domain. The two are equivalent because of Lagrangian duality. We show in Claim E.1 a concrete relation between our regularization parameter $\\nu$ and the chi-squared distance between the population distribution $\\mu_{0}$ and the target distribution $\\boldsymbol{p}^{*}$ We further require that $\\nu$ is sufficiently large to ensure that the resulting $\\chi^{2}(p^{*},p_{0})$ is smaller than an absolute constant, which is in line with the DRO being used for not too large ambiguity sets [RM22]. ", "page_idx": 2}, {"type": "text", "text": "Empirical Version  If the reference distribution is the uniform distribution on $N$ labeled examples $(\\pmb{x}_{i},\\bar{y}_{i})\\in\\mathbb{R}^{d}\\times\\mathbb{R}$ drawn from $\\mu_{0}$ , we call it $\\widehat{\\boldsymbol{\\mu}}_{0}=\\widehat{\\boldsymbol{\\mu}}_{0}(N)$ , and similarly define $\\widehat{\\boldsymbol{p}}\\in\\mathcal{P}(\\widehat{\\boldsymbol{p}}_{0})$ . Note that $\\begin{array}{r}{R(\\pmb{w}^{*};\\widehat{p}_{0})=\\operatorname*{max}_{\\widehat{p}\\in\\mathcal{P}(\\widehat{p}_{0})}\\mathbb{E}_{(\\pmb{x},\\pmb{y})\\sim\\widehat{p}_{\\!*}}(\\sigma(\\pmb{w}^{*}\\cdot\\pmb{x})-y)^{2}-\\nu\\chi^{2}(\\widehat{p},\\widehat{p}_{0})}\\end{array}$ ; if we let $\\widehat{\\boldsymbol{p}}^{*}$ denote the distribution that achieves the maximum, $\\widehat{\\boldsymbol{p}}^{*}$ has the same support as ${\\widehat{\\boldsymbol{p}}}_{0}$ and can be interpreted as the reweighting of the samples that maximizes the regularized loss. ", "page_idx": 2}, {"type": "text", "text": "Formally, our goal is to solve the following learning problem. ", "page_idx": 2}, {"type": "text", "text": "Problem 1.3 (Robustly Learning a Single Neuron Under Distributional Shifts). Given error parameters $\\epsilon,\\delta\\in(0,1)$ , regularization parameter $\\nu>0$ , set radius $W>0$ , and sample access to labeled examples $(\\pmb{x},y)$ drawn i.i.d. from an unknown reference distribution $\\mu_{0}$ , output a parameter vector $\\hat{\\pmb{w}}\\in\\mathcal{B}(\\hat{W})$ that is competitive with the DRO risk minimizer $\\begin{array}{r}{\\pmb{w}^{*}=\\arg\\operatorname*{min}_{\\pmb{w}\\in\\mathcal{B}(W)}R(\\pmb{w};\\pmb{\\eta}_{0})}\\end{array}$ in the sense that with probability at least $1-\\delta$ $\\lVert\\pmb{\\hat{w}}-\\pmb{w}^{*}\\rVert_{2}^{2}\\leq C\\mathrm{OPT}+\\epsilon$ for an absolute constant $C$ ", "page_idx": 2}, {"type": "text", "text": "While the stated goal is expressed in terms of $\\lVert\\hat{\\boldsymbol{w}}-\\boldsymbol{w}^{*}\\rVert_{2}$ , under mild distributional assumptions that we make on the reference and target distributions, this guarantee implies being competitive with the best-fit function on $\\boldsymbol{p}^{*}$ in terms of both the square loss and the risk, namely $\\Lambda_{\\sigma,p^{*}}(\\hat{\\pmb w})=O(\\mathrm{OPT})+\\epsilon$ and $\\begin{array}{r}{R(\\pmb{w},p_{0})-\\operatorname*{min}_{\\pmb{w}\\in\\mathcal{B}(W)}R(\\pmb{\\hat{w}},p_{0})\\leq O(\\mathrm{OPT})+\\epsilon}\\end{array}$ . Further, our algorithm is primal-dual and it outputs a distribution $\\widehat{\\boldsymbol{\\mathscr{p}}}$ that is close to $\\widehat{\\boldsymbol{p}}^{*}$ in the chi-squared divergence. ", "page_idx": 2}, {"type": "text", "text": "Since the solution to Problem 1.3 has an error of $O(\\mathrm{OPT})+\\epsilon$ ,when we use the term\u201cconvergence\" in our paper, we refer to the following weaker notion: the iterates of our algorithm converge to the (set of) solutions such that asymptotically all iterates lie within the set of $\\mathbf{\\bar{\\rho}}$ solutions, which are the target solutions, as stated in Problem 1.3. ", "page_idx": 2}, {"type": "text", "text": "1.2 Main Result ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our main contribution is the first polynomial sample and time algorithm for learning a neuron in a distributionally robust setting for a broad class of activations (Definition 1.1) and under mild distributional assumption on the target distribution (Assumptions 2.1 and 2.2 in Section 2.1). ", "page_idx": 2}, {"type": "text", "text": "Theorem 1.4 (Main Theorem \u2014 Informal). Suppose that the learner has access to $N=\\tilde{\\Omega}(d/\\epsilon^{2})$ samplesdrawnfrom thereferencedistribution $\\mu_{0}$ If all samples arebounded and the distribution $\\boldsymbol{p}^{*}$ satisfies the \u201cmargin-like\u201d condition and concentration (Assumptions 2.1 and 2.2 in Section 2.1), then after $\\tilde{O}(d\\log(1/\\epsilon))$ iterations,each running in sample near-linear time,with high probability Algorithm $^{\\,l}$ recovers $\\hat{w}$ such that $\\|\\pmb{\\hat{w}}-\\pmb{w}^{*}\\|_{2}^{2}\\leq C\\,\\mathrm{OPT}+\\epsilon,$ for anabsoluteconstant $C$ ", "page_idx": 2}, {"type": "text", "text": "We emphasize that Theorem 1.4 simultaneously addresses two types of robustness: firstly, robustness concerning labels $(y)$ ; and secondly, robustness due to shifts in the distribution ( $\\scriptstyle\\ p_{0}$ being perturbed). This result is new even when specialized to any nontrivial activation like ReLU, realizable case (where $\\mathrm{{OPT}\\;=\\;0)}$ , and the simplest Gaussian $\\textbf{\\em x}$ -marginal distribution. Without distributional robustness, existing approaches, as previously discussed, yield an error of $O(\\mathrm{OPT})+\\epsilon$ under certain $\\textbf{\\em x}$ -marginal conditions. We demonstrate that this error rate can be also achieved with respect to $\\boldsymbol{p}^{*}$ in a distributionally robust context, as long as $\\boldsymbol{p}^{*}$ meets the same conditions specified in $[\\mathrm{Wan}+23\\mathrm{a}]-$ among the mildest in the literature addressing non-distributionally robust agnostic setting. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "1.3  Technical Overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our technical approach relies on three main components, described below: ", "page_idx": 3}, {"type": "text", "text": "Local Error Bounds  Our work is inspired by optimization-theory local error bounds (\"sharpness\") obtained for learning a single neuron with monotone unbounded activations under structured distributions without considering distributional shift or ambiguity [MBM18; $\\mathrm{Wan}{+}23\\mathrm{a}$ 1. These bounds are crucial as they quantify growth of a loss function outside the set of target solutions, essentially acting as a \u201csignal' to guide algorithms toward target solutions in our learning problems. Concretely, under distributional assumptions on $\\boldsymbol{p}^{*}$ from $[\\mathrm{Wan}+23\\mathrm{a}]$ , the following sharpness property can be established: there is an absolute constant $c_{1}>0$ such that $\\forall{\\pmb w}\\in\\mathcal{B}(2||{\\pmb w}^{*}||_{2})$ \uff0c ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|w-w^{*}\\|_{2}^{2}=\\Omega(\\mathrm{OPT})\\;\\Rightarrow\\;\\Lambda_{\\sigma,p^{*}}(w)-\\Lambda_{\\sigma,p^{*}}(w^{*})\\geq c_{1}\\|w-w^{*}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The local error bounds in [MBM18; $\\mathrm{Wan}{+}23\\mathrm{al}$ assume identical reference and target distributions. Introducing distributional ambiguity \u2014\u2014 as in our work \u2014 invalidates this assumption, and as a result necessary distributional assumptions for sharpness may not apply to all distributions in the ambiguity set. In this work, distributional assumptions are exclusively applied to the target distribution to exploit the sharpness property proved in $[\\mathrm{Wan}+23\\mathrm{a}]$ . We also assume that the sample covariates from the reference distribution are polynomially bounded; this assumption, which is without loss of generality, impacts only the sample and computational complexities and is satisfied by standard distributions. ", "page_idx": 3}, {"type": "text", "text": "Primal-Dual Algorithm  Our algorithm is a principled, primal-dual algorithm leveraging the sharpness property on the target distribution, the structure of the square loss, and properties of chi-squared divergence. We control a \u201cgap-like\u201d function of the iterates, $\\mathrm{Gap}(\\bar{\\hat{w}},\\bar{\\hat{\\mu}};\\widehat{\\mu}_{0})\\;:=$ $L_{\\sigma}(\\bar{\\pmb{w}},\\widehat{\\pmb{\\mathscr{n}}}^{*};\\widehat{\\pmb{\\mathscr{n}}}_{0})\\,-\\,\\mathbf{\\bar{\\mathit{L}}}_{\\sigma}(\\pmb{w}^{*},\\widehat{\\pmb{\\mathscr{n}}};\\widehat{\\pmb{\\mathscr{n}}}_{0})$ . The idea of approximating a gap and showing it reduces at a rate $1/A_{k}$ ,where $A_{k}$ is a monotonically increasing function of $k$ , comes from [DO19] and has been extended to primal-dual methods, including DRO settings, in [SWD21; $\\mathrm{Dia}{+}22\\mathrm{c}$ $\\mathrm{Son}{+22}$ ; MDH24]. ", "page_idx": 3}, {"type": "text", "text": "Unlike past work [SWD21; $\\mathrm{Dia}{+}22\\mathrm{c}$ $\\mathrm{Son}{+22}$ ; MDH24], our primal problem is nonconvex, even for ReLU activations without distributional ambiguity. Unfortunately, the previously mentioned results relying on convexity do not apply in our setting. Additionally, sharpness \u2014 which appears crucial to approximating the target loss \u2014 is a local property, applying only to $\\pmb{w}$ such that $\\|\\pmb{w}\\|_{2}\\leq2\\|\\pmb{w}^{*}\\|_{2}$ where $\\|\\pmb{w}^{*}\\|_{2}$ is unknown. This condition is trivially met at initialization, but proving it holds for all iterates requires convergence. We address this issue via an inductive argument, effectively coupling convergence analysis with localization of the iterates. ", "page_idx": 3}, {"type": "text", "text": "Additionally, standard primal-dual methods [CP11; $\\mathrm{Cha}{+}18$ ; SWD21; ACW22; $\\mathrm{Son}{+22}]$ rely on bilinear coupling between primal and dual variables in $L_{\\sigma}(\\pmb{w},\\widehat{p};\\widehat{p}_{0})$ . In our case, $L_{\\sigma}(\\pmb{w},\\widehat{p};\\widehat{p}_{0})$ is nonlinear and nonconvex in the first argument. Recent work [MDH24] handled nonlinearity by linearizing the function using convexity of the loss, which makes the function bounded below by its linear approximation at any point. However, this approach cannot be applied to our problem as the loss is nonconvex. Instead, we control the chi-squared divergence between the target distribution and the algorithm dual iterates to bound $L_{\\sigma}(w,\\widehat{p}^{*};\\widehat{p}_{0})$ from below, using a key structural result that we establish in Lemma 3.4. The challenges involved in proving this structural result require us to rely on chi-squared regularization and convex activation $\\sigma$ . Generalizing our result to all monotone unbounded activations and other strongly convex divergences like KL would need a similar structural lemma under these broader assumptions. ", "page_idx": 3}, {"type": "text", "text": "An interesting aspect of our analysis is that we do not rely on a convex surrogate for our problem. Instead, we constructively bound a quantity related to the DRO risk of the original square loss, justifying our algorithmic choices directly from the analysis. Although we do not consider convex surrogates, the vector field ${\\pmb v}({\\pmb w};{\\pmb x},y)$ , scaled by $2\\beta$ , corresponds to the gradient of the convex surrogate loss $\\begin{array}{r}{\\int_{0}^{\\pmb w\\cdot\\pmb x}(\\sigma(t)-y)\\,\\mathrm d t}\\end{array}$ which has been used in prior litrature on learning a single neuron under similar settings without distributional ambiguity $[\\mathrm{Kak}+11$ ; Dia $+20$ .\uff0c $\\mathrm{Wan}{+}23\\mathrm{a}$ 1. In our analysis, the vector field ${\\pmb v}({\\pmb w};{\\pmb x},y)$ is naturally motivated by the argument in the proof of Lemma 3.4. ", "page_idx": 3}, {"type": "text", "text": "\"Concentration\" of the Target Distribution To prove that our primal-dual algorithm converges, we need to prove both an upper bound and a lower bound for $\\mathrm{Gap}(\\hat{\\varpi},\\widehat{\\boldsymbol{\\mu}};\\widehat{\\boldsymbol{\\mu}}_{0})$ . The lower bound relies on sharpness; however, we need it to hold for the empirical target distribution $(\\widehat{\\boldsymbol{p}}^{*})$ . This requires us to translate distributional assumptions and/or their implications from $\\boldsymbol{p}^{*}$ to $\\widehat{\\boldsymbol{p}}^{*}$ . Unfortunately, $\\widehat{\\boldsymbol{p}}^{*}$ is not the uniform distribution over samples drawn from $\\boldsymbol{p}^{*}$ . Rather, it is the maximizing distribution in the empirical DRO risk, defined w.r.t. ${\\widehat{\\boldsymbol{p}}}_{0}$ . This means that prior uniform convergence results do not apply. Additionally, minimax risk rates from prior statistical results, such as those in [DN21], relate $R(w;\\widehat{\\boldsymbol{p}}_{0})$ and $R(w;p_{0})$ . However, they do not help in our algorithmic analysis since they do not guarantee that the sharpness holds for $\\widehat{\\boldsymbol{p}}^{*}$ ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "To address these challenges, we prove (in Corollary C.2) that as long as $\\nu$ is sufficiently large, there is a simple closed-form expression for $\\widehat{\\boldsymbol{p}}^{*}$ as a function of ${\\widehat{\\boldsymbol{p}}}_{0}$ and an analogous relationship holds between $\\boldsymbol{p}^{*}$ and $\\ensuremath{\\mu}_{0}$ . This allows us to leverage the fact that expectations of bounded functions with respect to ${\\widehat{\\boldsymbol{p}}}_{0}$ closely approximate those with respect to $\\ensuremath{\\mu}_{0}$ to show that expectations with respect to $\\widehat{\\boldsymbol{p}}^{*}$ and $\\boldsymbol{p}^{*}$ are similarly close. This result then implies that the sharpness also holds for $\\widehat{\\boldsymbol{p}}^{*}$ (Lemma C.6). Full details are provided in Appendix C. ", "page_idx": 4}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we introduce the necessary notation and state basic facts used in our analysis. ", "page_idx": 4}, {"type": "text", "text": "Notation Given a positive integer $N$ $[N]$ denotes the set $\\{1,2,\\ldots,N\\}$ . Given a set $\\mathcal{E},\\ \\mathcal{E}^{c}$ denotes the complement of $\\mathcal{E}$ when the universe is clear from the context. We use $\\mathbb{I}_{\\mathcal{E}}$ to denote the characteristic function of a set $\\mathcal{E}$ $\\mathbb{I}_{\\mathcal{E}}(x)\\,=\\,1$ if $x\\in\\mathcal{E}$ and $\\mathbb{I}_{\\mathcal{E}}(x)\\,=\\,0$ otherwise. For vectors $\\textbf{\\em x}$ and $\\widehat{\\mathbf{\\xi}}^{x}$ from the $d\\!\\cdot$ -dimensional Euclidean space $\\mathbb{R}^{d}$ ,we use $\\langle{\\pmb x},{\\widehat x}\\rangle$ and $x\\cdot{\\widehat{\\mathbf{x}}}$ to denote the standard inner product, while $\\|\\cdot\\|_{2}=\\sqrt{\\langle\\cdot,\\cdot\\rangle}$ denotes the $\\ell_{2}$ norm. We use $(\\pmb{x}^{(1)},\\pmb{x}^{(2)},\\dots,\\pmb{x}^{(d)})$ to denote the entries of $\\pmb{x}\\in\\mathbb{R}^{d}$ .We write $x\\leq{\\widehat{x}}$ to indicate $\\mathbf{\\Delta}\\pmb{x}^{(j)}\\le\\widehat{\\pmb{x}}^{(j)}$ for all coordinates $j$ . For $r\\,>\\,0$ $\\mathcal{B}(r):=\\{\\pmb{x}:\\|\\pmb{x}\\|_{2}\\leq r\\}$ denotes the centered ball of radius $r$ . We use $\\Delta_{N}$ to denote the probability simplex: $\\begin{array}{r}{\\Delta_{n}:=\\left\\{\\pmb{x}\\in\\dot{\\mathbb{R}}^{N}:\\sum_{j=1}^{N}\\pmb{x}^{(j)}=1,\\forall j\\in[N]:\\pmb{x}^{(j)}\\geq0\\right\\}}\\end{array}$ We denote by $\\pmb{I}_{d}$ the identity matrix of size $d\\times d$ .We write $A\\succeq B$ to indicate that ${\\pmb x}^{\\top}(A-B){\\pmb x}\\geq0$ for all $\\pmb{x}\\in\\mathbb{R}^{d}$ . For two functions $f$ and $g$ , we say $f=\\tilde{O}(g)$ if $f=O(g\\log^{k}(g))$ for some constant $k$ , and similarly define $\\tilde{\\Omega}$ We use notation $\\tilde{O}_{c}(\\cdot)$ and $\\tilde{\\Omega}_{c}(\\cdot)$ to hide polynomial factors in (typically absolute constant) parameters $c$ . For two distributions $\\boldsymbol{\\mu}$ and $p^{\\prime}$ , we use $\\boldsymbol{p}\\ll\\boldsymbol{p}^{\\prime}$ to denote that $\\boldsymbol{\\mu}$ is absolutely continuous with respect to $\\ensuremath{\\boldsymbol{p}}^{\\prime}$ , i.e., for all measurable sets $A$ \uff0c $p^{\\prime}(A)=0$ implies $p(A)=0$ . Typically, $\\hat{p}$ and $\\hat{q}$ are empirical distributions, and $\\hat{p}\\ll\\hat{q}$ is equivalent to the condition that the support of $\\hat{p}$ is a subset of the support of q. For p \u300a p', we use  t to denote their Radon-Nikodym derivative, which is the quotient of probability mass functions for discrete distributions. We use $\\chi^{2}(\\boldsymbol{p},\\boldsymbol{p}^{\\prime})$ to denote the chi-squared divergence of $\\boldsymbol{\\mu}$ w.r.t. $\\ensuremath{\\boldsymbol{p}}^{\\prime}$ , i.e., $\\begin{array}{r}{\\chi^{2}(p,p^{\\prime})=\\int(\\frac{\\mathrm{d}p}{\\mathrm{d}p^{\\prime}}-1)^{2}\\mathrm{d}p^{\\prime}}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "2.1  Distributional Assumptions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Similar to $[\\mathrm{Wan}+23\\mathrm{a}]$ , we make two assumptions about the target distribution of the covariates $(p_{\\pmb{x}}^{*})$ First, we assume that the optimal solution $\\pmb{w}^{*}$ satisfies the following \u201cmargin-like\u201d condition: ", "page_idx": 4}, {"type": "text", "text": "Assumption 2.1 (Margin). There exist absolute constants $\\begin{array}{r l r}{\\lambda,\\gamma}&{{}\\in}&{(0,1]}\\end{array}$ suchthat $\\mathbb{E}_{\\pmb{x}\\sim p_{\\alpha}^{*}}[\\pmb{x}\\pmb{x}^{T}\\mathbb{I}_{\\pmb{w}^{*}\\cdot\\pmb{x}\\geq\\gamma}||\\pmb{w}^{*}||_{2}]\\succeq\\lambda\\pmb{I}$ ,where $\\boldsymbol{p}_{x}^{*}$ is the $\\textbf{\\em x}$ -marginal distribution of $\\boldsymbol{p}^{*}$ ", "page_idx": 4}, {"type": "text", "text": "We also assume that $\\boldsymbol{p}_{x}^{*}$ is subexponential with parameter $B$ , which is an absolute constant. ", "page_idx": 4}, {"type": "text", "text": "Assumption 2.2 (Subexponential Concentration). There exists a parameter $B>0$ suchthatforany $\\pmb{u}\\in\\mathcal{B}(1)$ and any $r\\geq1$ it holdsthat $\\operatorname*{Pr}_{\\pmb{x}\\sim p_{\\pmb{x}}^{*}}[|\\pmb{u}\\cdot\\pmb{x}|\\geq r]\\leq\\exp(-B r)$ ", "page_idx": 4}, {"type": "text", "text": "AppendixEof $[\\mathrm{Wan}+23\\mathrm{a}]$ shows that Assumptions 2.1 and 2.2 are satisfied by several important families of distributions including Gaussians, discrete Gaussians, all isotropic log-concave distributions, the uniform distribution over $\\{-1,0,1\\}^{d}$ ,etc. ", "page_idx": 4}, {"type": "text", "text": "For simplicity, we assume the labeled samples $(\\pmb{x}^{(i)},y^{(i)})$ drawn from the reference distribution are bounded. This assumption, which does not affect the approximation constant for Problem 1.3, only impacts iteration and sample complexities. We state the bound on the covariates below, while a bound on the labels follows from prior work (Fact 2.6 stated in the next subsection). ", "page_idx": 4}, {"type": "text", "text": "Assumption 2.3 (Boundedness). There exists a parameter $S$ such that for any fixed $\\pmb{u}\\in\\mathcal{B}(1)\\,i t$ holdsthat ${\\pmb u}\\cdot{\\pmb x}\\leq S$ for all sample covariates $\\textbf{\\em x}$ in the support of ${\\widehat{\\boldsymbol{p}}}_{0}$ ", "page_idx": 4}, {"type": "text", "text": "We also assume without loss of generality that $\\|\\pmb{w}^{*}\\|_{2}^{2}\\geq C\\,\\mathrm{OPT}+\\epsilon$ for some absolute constant $C$ since otherwise O would be a valid $O(\\mathrm{OPT})+\\epsilon$ solution. Algorithmically, we can first compute the empirical risk (per Corollary C.3) of the output from our algorithm and of $\\hat{\\pmb w}={\\bf0}$ and then output the solution with the lower risk to get an $O(\\mathrm{OPT})+\\epsilon$ solution; see Claim E.2 for a detailed discussion. ", "page_idx": 5}, {"type": "text", "text": "2.2 Auxiliary Facts ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To achieve the claimed guarantees, we leverage structural properties of the loss function on the target distribution, implied by our distributional assumptions (Assumptions 2.1 and 2.2). Specifically, we make use of Lemma 2.2 and Fact C.4 from $[\\mathrm{Wan}{+}23\\mathrm{a}]$ , summarized in the fact below. ", "page_idx": 5}, {"type": "text", "text": "Fact 2.4 (Sharpness $\\left(\\mathrm{[Wan}{+}23\\mathrm{a}\\mathrm{]}\\right)$ ).Suppose $\\boldsymbol{p}^{*}$ and $\\pmb{w}^{*}$ satisfy Assumptions 2.1 and 2.2. Let Co =6Blog(2B/x). Forall  E S(2lu\\*l) and u E (1), ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\mathbf{x}\\sim p_{\\alpha}^{*}}[(\\sigma(\\pmb{w}\\cdot\\pmb{x})-\\sigma(\\pmb{w}^{*}\\cdot\\pmb{x}))(\\pmb{w}\\cdot\\pmb{x}-\\pmb{w}^{*}\\cdot\\pmb{x})]\\geq c_{0}\\|\\pmb{w}-\\pmb{w}^{*}\\|_{2}^{2},}&{}\\\\ {\\mathbb{E}_{\\mathbf{x}\\sim p_{\\alpha}^{*}}[(\\pmb{x}\\cdot\\pmb{u})^{\\tau}]\\leq5B\\quad f o r\\,\\tau=2,4.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Fact 2.4 applies to the population version of the problem. Such a result also holds for the target distribution of the empirical problem, which we state below. Note that this result cannot be obtained by appealing to uniform convergence results for learning a neuron (without distributional robustness). ", "page_idx": 5}, {"type": "text", "text": "Lemma 2.5 (Empirical Sharpness; Informal. See Lemma C.6). Under Assumptions 2.1 to 2.3, for $a$ sufficiently large sample size $N$ as a function of $B,W,S,\\nu,\\alpha,\\gamma,\\lambda,d$ andwithhighprobability,for all $\\pmb{w}\\in\\mathcal{B}(2||\\pmb{w}^{*}||)$ with $\\|\\pmb{w}-\\pmb{w}^{*}\\|\\geq\\sqrt{\\epsilon}$ and $\\pmb{u}\\in\\mathcal{B}(1)$ \uff0c ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathbf{x}\\sim\\widehat{p}_{\\mathbf{x}}^{*}}[(\\sigma(\\pmb{w}\\cdot\\pmb{x})-\\sigma(\\pmb{w}^{*}\\cdot\\pmb{x}))(\\pmb{w}\\cdot\\pmb{x}-\\pmb{w}^{*}\\cdot\\pmb{x})]\\geq(c_{0}/2)\\|\\pmb{w}-\\pmb{w}^{*}\\|_{2}^{2}}\\\\ {\\mathbb{E}_{\\mathbf{x}\\sim\\widehat{p}_{\\mathbf{x}}^{*}}[(\\pmb{x}\\cdot\\pmb{u})^{\\tau}]\\leq6B\\quad f o r\\,\\tau=2,4.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "As a consequence, for $c_{1}=c_{0}^{2}/(24B)$ and any $w\\in\\mathcal{B}(W)$ (where $c_{\\mathrm{0}}$ is defined in Fact 2.4), we have ", "page_idx": 5}, {"type": "equation", "text": "$$\nc_{1}\\|\\pmb{w}-\\pmb{w}^{*}\\|_{2}^{2}\\leq\\mathbb{E}_{\\pmb{x}\\sim\\hat{\\pmb{\\eta}}_{x}^{*}}[(\\sigma(\\pmb{w}\\cdot\\pmb{x})-\\sigma(\\pmb{w}^{*}\\cdot\\pmb{x}))^{2}]\\leq6B\\beta^{2}\\|\\pmb{w}-\\pmb{w}^{*}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the left inequality uses Cauchy-Schwarz and the right inequality uses $\\beta$ -Lipschitzness of $\\sigma(\\cdot)$ $[\\mathrm{Wan}+23\\mathrm{a}]$ also showed that the labels $y$ can be assumed to be bounded without loss of generality. ", "page_idx": 5}, {"type": "text", "text": "Fact 2.6.Suppose $\\boldsymbol{p}^{*}$ and $\\pmb{w}^{*}$ satisfy Assumption 2.1 and Assumption 2.2. Let $\\begin{array}{r l}{y^{\\prime}}&{{}=}\\end{array}$ $\\mathrm{sign}(y)\\operatorname*{max}\\lbrace|y|,M\\rbrace$ where for some sufficiently large absolute constant $C_{M}$ wedefine ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{c}{M=C_{M}W B\\log(\\beta B W/\\epsilon)}\\\\ {\\mathbb{E}_{\\boldsymbol{p}^{*}}(\\sigma(\\boldsymbol{w}^{*}\\boldsymbol{\\cdot}\\boldsymbol{x})-y^{\\prime})^{2}\\le\\mathbb{E}_{\\boldsymbol{p}^{*}}(\\sigma(\\boldsymbol{w}^{*}\\boldsymbol{\\cdot}\\boldsymbol{x})-y)^{2}+\\epsilon=\\mathrm{OPT}+\\epsilon.}\\end{array}\n$$Then ", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We also make use of the following facts from convex analysis. First, let $\\phi\\;:\\;\\mathbb{R}^{N}\\;\\rightarrow\\;\\mathbb{R}$ be a differentiable function and the Bregman divergence of $\\phi$ for any $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{N}$ be defined by ", "page_idx": 5}, {"type": "equation", "text": "$$\nD_{\\phi}({\\pmb y},{\\pmb x})=\\phi({\\pmb y})-\\phi({\\pmb x})-\\langle\\nabla\\phi({\\pmb x}),{\\pmb y}-{\\pmb x}\\rangle.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Fact 2.7. Let $\\psi(\\pmb{x})=\\phi(\\pmb{x})+\\langle\\pmb{a},\\pmb{x}\\rangle+b$ for some $\\pmb{a}\\in\\mathbb{R}^{N}$ and $b\\in\\mathbb{R}$ Then $D_{\\psi}({\\pmb y},{\\pmb x})=D_{\\phi}({\\pmb y},{\\pmb x})$ for all $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{N}$ , i.e., the Bregman divergence is blind to the addition of affine terms to function $\\phi$ ", "page_idx": 5}, {"type": "text", "text": "Second, we state the first-order necessary conditions that a local maximizer must satisfy. ", "page_idx": 5}, {"type": "text", "text": "Fact 2.8 (First-Order Optimality Condition). Let $\\Omega$ be a closed, convex, and nonempty set and let $f:\\Omega\\to\\mathbb{R}$ be continuously differentiable.If $x^{*}$ is a local maximizer of $f$ on $\\Omega$ then it holds that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla f(\\pmb{x}^{*})\\cdot(\\pmb{y}-\\pmb{x}^{*})\\leq0\\quad f o r\\,a l l\\,\\pmb{y}\\in\\Omega.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "f $f$ is also concave, then Equation (7) implies that $x^{*}$ is a global maximizer of $f$ ", "page_idx": 5}, {"type": "text", "text": "3  Algorithm and Convergence Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we introduce our algorithm and state our main results, summarized in Theorem 3.1. We highlight the main components of our technical approach, while most of the technical details are deferred to the appendix, due to space constraints. ", "page_idx": 5}, {"type": "text", "text": "To facilitate the presentation of results, we introduce the following auxiliary notation: $\\ell({\\pmb w};{\\pmb x},y):=$ $(\\sigma({\\pmb w}\\cdot{\\pmb x})-y)^{2}$ \uff0c ${\\pmb v}({\\pmb w};{\\pmb x},y):=2\\beta({\\pmb\\sigma}({\\pmb w}\\cdot{\\pmb x})-y){\\pmb x}$ and $\\widehat{\\mathrm{OPT}}=\\mathbb{E}_{(\\pmb{x},y)\\sim\\hat{\\mu}^{*}}\\ell(\\pmb{w}^{*};\\pmb{x},y)$ . We also note that Assumption 2.3 implies that for all samples $\\{\\pmb{x}_{i},y_{i}\\}$ , the function $\\pmb{w}\\mapsto\\pmb{v}(\\pmb{w};\\pmb{x}_{i},y_{i})$ is bounded above by $G$ and $\\kappa$ -Lipschitz for all $i\\in[N]$ and $w\\in\\mathcal{B}(W)$ , where $G=2\\beta S\\sqrt{d}(\\sqrt{2}\\beta W S\\!+\\!M)$ and $\\kappa=2\\beta^{2}S^{2}d$ (see Lemma B.4 in Appendix B). Starting from this section, we write $L(w,\\widehat{\\boldsymbol{n}})$ to denote $L_{\\sigma}(\\pmb{w},\\widehat{p};\\widehat{p}_{0})$ , hiding the dependence on ${\\widehat{\\boldsymbol{p}}}_{0}$ and $\\sigma$ . We also write $\\mathrm{Gap}(\\boldsymbol{w},\\widehat{\\boldsymbol{p}})$ for $\\mathrm{Gap}(w,\\widehat{p};\\widehat{\\boldsymbol{\\jmath}_{0}})$ ", "page_idx": 6}, {"type": "text", "text": "Our main algorithm (Algorithm 1) is an iterative primal-dual method with extrapolation on the primal side via $\\pmb{g}_{i}$ . The vector $\\mathbb{E}_{\\widehat{\\mu}_{i}}[\\pmb{v}(\\pmb{w}_{i};\\pmb{x},y)]$ equals the (scaled) gradient of a surrogate loss used in prior works $[\\mathrm{Kak}+11$ $\\mathrm{Dia}{+20}$ $\\mathrm{Wan}{+}23\\mathrm{a}]$ 1. In contrast to prior work, we directly bound the original square loss, with $\\mathbb{E}_{\\widehat{\\mu}_{i}}[\\pmb{v}(\\pmb{w}_{i};\\pmb{x},y)]$ naturally arising from our analysis. Both updates $\\pmb{w}_{i}$ and ${\\widehat{\\,p\\,}}_{i}$ are efficiently computable: $\\pmb{w}_{i}$ involves a simple projection onto a Euclidean ball, and ${\\widehat{\\,p\\,}}_{i}$ involves a projection onto a probability simplex, computable in near-linear time $[\\mathrm{Duc}{+}08]$ \uff1a ", "page_idx": 6}, {"type": "text", "text": "Algorithm 1: Main algorithm ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Iaput: $\\nu>0,\\kappa,G,c_{1},\\nu_{0}=768\\beta^{4}B\\epsilon/c_{1}$ sample set $\\{(\\pmb{x}_{i},y_{i})\\}_{i=1}^{N}$   \n1 Initialization: $A_{-1}=a_{-1}=A_{0}=a_{0}=0,{\\pmb w}_{-1}={\\pmb w}_{0}={\\pmb0},\\widehat{{\\pmb\\rho}}_{-1}=\\widehat{{\\pmb\\rho}}_{0};$   \n2 for $i=1,\\ldots,k\\,{\\bf d o}$   \n3 m/)-mn[,1/4}/(2max,G),A=a+A-1   \n4 $v(\\pmb{w};\\pmb{x},y)=2\\beta(\\sigma(\\pmb{w}\\cdot\\pmb{x})-\\mathrm{sign}(y)\\operatorname*{max}\\{|y|,M\\}$ })\u03b1, where $M$ is defined in Equation (6) ; 5 $\\begin{array}{r}{g_{i-1}=\\mathbb{E}_{\\widehat{\\beta}_{i-1}}[v\\big(w_{i-1};\\pmb{x},y\\big)]+\\frac{a_{i-1}}{a_{i}}\\big(\\mathbb{E}_{\\widehat{\\beta}_{i-1}}[v\\big(w_{i-1};\\pmb{x},y\\big)]-\\mathbb{E}_{\\widehat{\\beta}_{i-2}}[v\\big(w_{i-2};\\pmb{x},y\\big)]\\big);}\\end{array}$ 6 $\\begin{array}{r l}&{\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\cdots\\quad\\quad\\cdots\\quad\\cdots\\quad\\quad\\cdots\\quad\\mathrm{\\cdots}\\quad\\cdots\\quad\\cdots\\quad\\quad\\cdots\\quad\\mathrm{\\nabla{x}}_{i}\\mathrm{\\bfx}_{i}\\mathrm{\\bfx}_{i}\\mathrm{\\bfx}_{i}\\mathrm{\\bfx}_{i}\\mathrm{\\bfx}_{i}\\mathrm{\\bfx}_{i}\\mathrm{\\bfx}_{i}\\mathrm{\\bfx}_{i}\\mathrm{\\bfx}_{i}\\mathrm{\\bfx}_{i}\\mathrm{\\bfx}_{i}\\mathrm{\\bfx}_{i}\\mathrm{\\bfx}_{i}\\mathrm{\\bfx}_{i}\\mathrm\\mathrm{\\bfx}_{i}\\mathrm{\\bfx}_{i}\\mathrm\\mathrm{\\bfx\\cdot\\cdot\\cdot\\nabla\\cdot\\nabla{x}\\quad\\cdot\\mathrm\\cdot\\cdot\\nabla\\mathrm{\\cdots\\cdot\\nabla{\\mathrm{x}\\cdot\\mathrm\\}\\mathrm{\\cdot\\mathrm\\cdot\\!\\!\\cdot}\\mathrm\\cdot\\cdot\\mathrm{\\!\\!\\cdots\\!\\cdot\\!\\cdot\\cdot\\mathrm\\!\\cdot\\nabla\\cdot\\mathrm\\cdot\\mathrm{\\mathrm\\mathrm{\\mathrm\\mathrm{\\mathrm\\mathrm}\\mathrm{\\mathrm\\mathrm}\\mathrm\\mathrm{\\mathrm\\mathrm{\\mathrm\\mathrm}\\mathrm\\mathrm{\\mathrm\\mathrm}\\mathrm\\mathrm{\\mathrm\\mathrm{\\mathrm}\\mathrm\\mathrm\\mathrm{\\mathrm}\\mathrm\\mathrm\\mathrm{\\mathrm\\mathrm\\mathrm{}\\mathrm\\mathrm\\mathrm\\mathrm{}\\mathrm\\mathrm\\mathrm\\mathrm{\\mathrm}\\mathrm\\mathrm\\mathrm{\\mathrm\\mathrm\\mathrm{}\\mathrm\\mathrm\\mathrm\\mathrm{}\\mathrm\\mathrm\\mathrm\\mathrm{\\mathrm}\\mathrm\\mathrm\\mathrm\\mathrm{\\mathrm\\mathrm\\mathrm{}\\mathrm\\mathrm\\mathrm\\mathrm{}\\mathrm\\mathrm\\mathrm\\mathrm{\\mathrm}\\mathrm\\mathrm\\mathrm\\mathrm{\\mathrm\\mathrm\\mathrm{}\\mathrm\\mathrm\\mathrm\\mathrm{}\\mathrm\\mathrm\\mathrm\\mathrm\\mathrm{}\\mathrm\\mathrm\\mathrm\\mathrm\\mathrm{\\mathrm\\mathrm\\mathrm{}\\mathrm\\mathrm\\mathrm\\mathrm{}\\mathrm\\mathrm\\mathrm\\mathrm\\mathrm{}\\mathrm\\mathrm\\mathrm\\mathrm\\mathrm{}\\mathrm\\mathrm\\mathrm\\mathrm\\mathrm{\\mathrm\\mathrm{}\\$ $\\begin{array}{r}{\\widehat{\\mu}_{i}=\\arg\\operatorname*{max}_{\\widehat{p}\\in\\mathcal{P}}\\big\\{a_{i}L(w_{i},\\widehat{p})-(\\nu_{0}+\\nu A_{i-1})D_{\\chi^{2}(\\cdot,\\widehat{p}_{0})}(\\widehat{p},\\widehat{p}_{i-1})\\big\\};}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.1 (Main Theorem). Under Assumptions 2.1 to 2.3, suppose the sample size is such that $\\begin{array}{r}{N=\\widetilde{\\Omega}_{B,S,\\beta,\\alpha,\\gamma,\\lambda}\\big(\\frac{W^{4}}{\\epsilon^{2}}\\big(1+\\frac{W^{4}}{\\nu^{2}}\\big)(d+W^{4}\\log(1/\\delta))\\big)}\\end{array}$ and $\\nu\\ge8\\beta^{2}\\sqrt{6B}\\sqrt{\\mathrm{OPT}_{(2)}+\\epsilon}/c_{1}$ where $\\mathrm{OPT}_{(2)}=\\mathbb{E}_{p^{*}}[\\ell(\\pmb{w}^{*};\\pmb{x},y)^{2}]$ and $c_{1}$ is defined in Lemma 2.5. With probability at least $1-\\delta_{i}$ for all iterates $w_{k},\\widehat{p}_{k}$ , it holds that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{c_{1}}{4}\\|\\pmb{w}^{*}-\\pmb{w}_{k}\\|_{2}^{2}+\\nu D_{\\phi}(\\widehat{\\pmb{n}}^{*},\\widehat{\\pmb{\\mu}}_{k})\\leq\\frac{D_{0}}{A_{k}}+\\frac{60\\beta^{2}B\\,\\mathrm{OPT}}{c_{1}}+\\epsilon,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\begin{array}{r}{D_{0}=\\frac{1}{2}\\|w^{*}-w_{0}\\|_{2}^{2}+\\nu_{0}\\chi^{2}(\\widehat{\\pmb{n}}^{*},\\widehat{\\pmb{\\mu}}_{0})}\\end{array}$ and $\\chi^{2}(\\widehat{\\boldsymbol{p}}^{*},\\widehat{\\boldsymbol{p}}_{0})\\leq c_{1}/(1536\\beta^{4}B)$ (and therefore $D_{0}$ does not depend on the sample size $N$ ", "page_idx": 6}, {"type": "text", "text": "In particular, after at most $\\begin{array}{r}{k=\\widetilde O(\\frac{\\operatorname*{max}\\{\\kappa,G\\}}{\\operatorname*{min}\\{\\nu,c_{1}\\}}\\log(\\frac{D_{0}}{\\epsilon}))}\\end{array}$ iterations, it holds that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r l}{\\|\\boldsymbol{w}_{k}-\\boldsymbol{w}^{*}\\|_{2}\\le C_{3}\\sqrt{\\mathrm{OPT}}+\\sqrt{\\epsilon},}&{}\\\\ {\\mathbb{E}_{(\\boldsymbol{x},\\boldsymbol{y})\\sim\\boldsymbol{p}^{*}}[\\ell(\\boldsymbol{w}_{k};\\boldsymbol{x},\\boldsymbol{y})]\\le(2+20B\\beta^{2}C_{3}^{2})\\ \\mathrm{OPT}+10\\beta^{2}B\\epsilon,}&{}\\\\ {R(\\boldsymbol{w}_{k};\\boldsymbol{p}_{0})-\\displaystyle\\operatorname*{min}_{\\boldsymbol{w}\\in\\mathcal{B}(W)}R(\\boldsymbol{w};\\boldsymbol{p}_{0})=R(\\boldsymbol{w}_{k};\\boldsymbol{p}_{0})-R(\\boldsymbol{w}^{*};\\boldsymbol{p}_{0})\\le C_{4}(\\mathrm{OPT}+\\epsilon),}&{}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $C_{3}=16\\beta\\sqrt{B}/c_{1}$ and $C_{4}=1+2(10B\\beta^{2}+c_{1})C_{3}+c_{1}\\sqrt{5B}\\beta^{2}C_{3}^{2}.$ ", "page_idx": 6}, {"type": "text", "text": "We focus on the convergence of iterates $\\boldsymbol{w}_{i}$ as claimed in Equation (8); the loss bound (Equation (9)) follows directly from the iterate convergence, while the risk bound (Equation (10)) requires a more involved analysis. Complete details for Equations (9) and (10) are provided in Appendix F. ", "page_idx": 6}, {"type": "text", "text": "Our strategy for the convergence analysis is as follows. Consider $\\{a_{i}\\}$ , a sequence of positive step sizes, and define $A_{i}$ as their cumulative sum $\\textstyle\\sum_{j=1}^{i}a_{j}$ . Our algorithm produces a sequence of primal-dual pairs $w_{i},\\widehat{p_{i}}$ , tracking a quantity related to the primal-dual gap, defined by: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda\\mathrm{ap}(w_{i},\\widehat{p}_{i}):=L(w_{i},\\widehat{p}^{*})-L(w^{*},\\widehat{p}_{i})=(L(w_{i},\\widehat{p}^{*})-L(w^{*},\\widehat{p}^{*}))+(L(w^{*},\\widehat{p}^{*})-L(w^{*},\\widehat{p}_{i})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Weview $\\left(L(\\pmb{w}_{i},\\widehat{\\pmb{\\mathscr{n}}}^{*})-L(\\pmb{w}^{*},\\widehat{\\pmb{\\mathscr{n}}}^{*})\\right)$ as the \u201cprimal gap\u201d and $\\left(L(\\pmb{w}^{*},\\widehat{\\pmb{\\mathscr{n}}}^{*})-L(\\pmb{w}^{*},\\widehat{\\pmb{\\mathscr{n}}}_{i})\\right)$ as the \u201cdual gap?\u2019 Since the squared loss for ReLU and similar activations is nonconvex, $L(w,\\widehat{\\boldsymbol{p}}^{*})$ isnonconvex in its first argument. Note that this gap function is not trivially non-negative (see Remark B.7), requiring an explicit lower bound proof. ", "page_idx": 6}, {"type": "text", "text": "Our strategy consists of deriving \u201csandwiching\u201d\u2019 inequalities for the (weighted) cumulative gap $\\begin{array}{r}{\\sum_{i=1}^{k}a_{i}\\mathrm{Gap}({\\pmb w}_{i},\\widehat{\\pmb{\\mathscr{n}}}_{i})}\\end{array}$ deduce that unless we already have an $O(\\mathrm{OPT})+\\epsilon$ solution, the iterates must be converging to the target solutions at rate $1/A_{k}$ , which we argue can be made geometrically fast. ", "page_idx": 7}, {"type": "text", "text": "Organization   The rest of this section is organized as follows \u2014 under the standard assumptions we state in this paper, in Lemma 3.2, we prove a lower bound on $\\mathrm{Gap}(\\boldsymbol{w},\\hat{\\boldsymbol{\\mu}})$ for any choice of $\\mathbf{\\nabla}w$ and $\\hat{\\boldsymbol{\\jmath}}$ . This can be used to get a corresponding lower bound on the weighted sum $\\begin{array}{r}{\\sum_{i=1}^{k}a_{i}\\mathrm{Gap}({\\pmb w}_{i},\\hat{\\pmb p}_{i})}\\end{array}$ In Lemma 3.3we then state an upper bound on $\\begin{array}{r}{\\sum_{i=1}^{k}a_{i}\\mathrm{Gap}({\\pmb w}_{i},\\hat{\\pmb p}_{i})}\\end{array}$ ; the proof of this technical argument is deferred to Appendix D. These two bounds together give us the first inequality in Theorem 3.1. Claim B.6 then bounds below the convergence rate for our choice of $a_{i}$ in Algorithm 1; and indicates that it is geometric. Finally, we put everything together to prove Theorem 3.1. ", "page_idx": 7}, {"type": "text", "text": "To simplify the notation, we use $\\phi(\\widehat{\\boldsymbol{p}}):=\\chi^{2}(\\widehat{\\boldsymbol{p}},\\widehat{\\boldsymbol{p}}_{0})$ throughout this section. Note that $D_{\\phi}(\\widehat{p},\\widehat{q})=$ $\\begin{array}{r}{D_{\\phi}(\\widehat{\\boldsymbol{q}},\\widehat{\\boldsymbol{\\eta}})=\\sum_{i=1}^{N}\\frac{(\\widehat{\\boldsymbol{q}}^{(i)}-\\widehat{\\boldsymbol{p}}^{(i)})^{2}}{\\widehat{\\boldsymbol{p}}_{0}^{(i)}}}\\end{array}$ for any $\\widehat{\\boldsymbol{\\mu}}$ $\\widehat{q}$ in the domain. ", "page_idx": 7}, {"type": "text", "text": "3.1 Lower Bound on the Gap Function ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We begin the convergence analysis by demonstrating a lower bound on $\\mathrm{Gap}(\\pmb{w}_{i},\\widehat{\\pmb{\\mathscr{n}}}_{i})$ ", "page_idx": 7}, {"type": "text", "text": "Lemma 3.2 (Gap Lower Bound). Under the setting in which Lemma 2.5 holds, for all $\\textbf{\\em w}\\in$ $\\begin{array}{r}{\\mathcal{B}(2||\\boldsymbol{w}^{*}||_{2}),\\,\\mathrm{Gap}(\\boldsymbol{w},\\widehat{\\boldsymbol{p}})\\geq-\\frac{12\\beta^{2}B}{c_{1}}\\widehat{\\mathrm{OPT}}+\\frac{c_{1}}{2}||\\boldsymbol{w}-\\boldsymbol{w}^{*}||_{2}^{2}+\\nu D_{\\phi}(\\widehat{\\boldsymbol{p}}^{*},\\widehat{\\boldsymbol{p}}).}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "Proof. Writing $(\\sigma({\\pmb w}\\cdot{\\pmb x})-y)^{2}=\\left((\\sigma({\\pmb w}\\cdot{\\pmb x})-\\sigma({\\pmb w}^{*}\\cdot{\\pmb x}))+(\\sigma({\\pmb w}^{*}\\cdot{\\pmb x})-y)\\right)^{2}$ and expanding the square, we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad L(w,\\widehat{p}^{*})-L(w^{*},\\widehat{p}^{*})=\\mathbb{E}_{(x,y)\\sim\\widehat{p}^{*}}[(\\sigma(w\\cdot x)-y)^{2}-(\\sigma(w^{*}\\cdot x)-y)^{2}]}\\\\ &{=-\\,2\\mathbb{E}_{\\widehat{p}^{*}}[(\\sigma(w^{*}\\cdot x)-y)(\\sigma(w\\cdot x)-\\sigma(w^{*}\\cdot x))]+\\mathbb{E}_{\\widehat{p}^{*}}[((\\sigma(w\\cdot x)-\\sigma(w^{*}\\cdot x))^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "By the Cauchy-Schwarz inequality, we further have that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{\\widehat{p}^{*}}[(\\sigma(\\pmb{w}^{*}\\cdot\\pmb{x})-y)(\\sigma(\\pmb{w}\\cdot\\pmb{x})-\\sigma(\\pmb{w}^{*}\\cdot\\pmb{x}))]}\\\\ &{\\leq\\sqrt{\\mathbb{E}_{\\widehat{p}^{*}}[(\\sigma(\\pmb{w}^{*}\\cdot\\pmb{x})-y)^{2}]\\mathbb{E}_{\\widehat{p}^{*}}[(\\sigma(\\pmb{w}\\cdot\\pmb{x})-\\sigma(\\pmb{w}^{*}\\cdot\\pmb{x}))^{2}]}}\\\\ &{\\leq\\beta\\sqrt{6B}\\sqrt{\\widehat{\\mathrm{OPT}}}\\|\\pmb{w}-\\pmb{w}^{*}\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where in the second inequality weused thedenitof $\\widehat{\\mathrm{OPT}}$ and $\\begin{array}{r}{\\mathbb{E}_{\\mathbf{\\boldsymbol{x}}\\sim\\widehat{\\boldsymbol{p}}_{x}^{*}}[(\\sigma(\\pmb{\\boldsymbol{w}}\\cdot\\pmb{\\boldsymbol{x}})-\\sigma(\\pmb{\\boldsymbol{w}}^{*}\\cdot\\pmb{\\boldsymbol{x}}))^{2}]\\leq}\\end{array}$ $6B\\beta^{2}\\|\\pmb{w}-\\pmb{w}^{*}\\|_{2}^{2}$ from the right inequality in Equation (5). ", "page_idx": 7}, {"type": "text", "text": "On the other hand, by the lef inequality in Equation (5), we also have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\widehat{p}^{\\ast}}[(\\sigma(\\pmb{w}\\cdot\\pmb{x})-\\sigma(\\pmb{w}^{\\ast}\\cdot\\pmb{x}))^{2}]\\geq c_{1}\\|\\pmb{w}-\\pmb{w}^{\\ast}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Thus, combining Equation (11) and Equation (12), we get ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L(\\pmb{w},\\widehat{\\pmb{n}}^{*})-L(\\pmb{w}^{*},\\widehat{\\pmb{n}}^{*})\\geq-2\\beta\\sqrt{6B}\\|\\pmb{w}-\\pmb{w}^{*}\\|_{2}\\sqrt{\\widehat{\\mathrm{OPT}}}+c_{1}\\|\\pmb{w}-\\pmb{w}^{*}\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\geq-\\frac{12\\beta^{2}B}{c_{1}}\\widehat{\\mathrm{OPT}}+\\frac{c_{1}}{2}\\|\\pmb{w}-\\pmb{w}^{*}\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Wwhere te las inqualityis by $\\begin{array}{r}{2\\beta\\sqrt{6B}\\|\\pmb{w}-\\pmb{w}^{*}\\|_{2}\\sqrt{\\widetilde{\\mathrm{OPT}}}\\leq\\frac{4\\beta^{2}6B}{2c_{1}}\\widehat{\\mathrm{OPT}}+\\frac{c_{1}}{2}\\|\\pmb{w}-\\pmb{w}^{*}\\|_{2}^{2}.}\\end{array}$ Which comes from an application of Young's inequality (Fact B.1). ", "page_idx": 7}, {"type": "text", "text": "Finally, we use the optimality of $\\widehat{\\boldsymbol{p}}^{*}$ , which achieves the maximum over all ${\\widehat{\\boldsymbol{p}}}\\in{\\mathcal{P}}$ for $L(w^{*},\\widehat{\\boldsymbol{n}})$ .By the definition of a Bregman divergence, Fact 2.7, and first-order necessary condition in Fact 2.8: ", "page_idx": 7}, {"type": "equation", "text": "$$\n-L(w^{*},\\widehat{\\mu})-\\left(-L(w^{*},\\widehat{\\mu}^{*})\\right)=-\\langle\\nabla_{\\widehat{\\mu}}L(w^{*},\\widehat{\\mu}^{*}),\\widehat{\\mu}-\\widehat{\\mu}^{*}\\rangle+D_{-L(w^{*},\\cdot)}(\\widehat{\\mu},\\widehat{\\mu}^{*})\\geq\\nu D_{\\phi}(\\widehat{\\mu}^{*},\\widehat{\\mu}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Summing up Equation (13) and Equation (14) completes the proof. ", "page_idx": 7}, {"type": "text", "text": "3.2 Upper Bound on the Gap Function ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Having obtained a lower bound on the gap function, we now show an upper bound, leveraging our algorithmic choices. The proof is rather technical and involves individually bounding $L(\\pmb{w}_{i},\\widehat{\\pmb{\\mathscr{p}}}^{*})$ above andbounding $L(w^{*},\\widehat{{p}_{i}})$ below to obtain an upper bound on the gap function, which equals $L(\\pmb{w}_{i},\\widehat{\\pmb{\\mathscr{n}}}^{*})-L(\\pmb{w}^{*},\\widehat{\\pmb{\\mathscr{n}}}_{i})$ . We state this result in the next lemma, while the proof is in Appendix D. ", "page_idx": 8}, {"type": "text", "text": "Lemma 3.3 (Gap Upper Bound). Let $w_{i},\\widehat{p}_{i},a_{i},A_{i}$ evolve according to Algorithm $^{\\,l}$ , where we take, by convention, $a_{-1}=A_{-1}=a_{0}=A_{0}=0$ and ${\\pmb w}_{-1}\\,=\\,{\\pmb w}_{0}$ \uff0c $\\widehat{\\ensuremath{p}}_{-1}\\,=\\,\\widehat{\\ensuremath{p}}_{0}$ . Assuming Lemma 2.5 applies, then, for all $k\\geq1$ \uff0c $\\begin{array}{r}{\\sum_{i=1}^{k}a_{i}\\mathrm{Gap}({\\pmb w}_{i},\\widehat{\\boldsymbol{\\mu}}_{i})}\\end{array}$ is bounded above by ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{2}\\|w^{*}-w_{0}\\|_{2}^{2}+\\nu_{0}D_{\\phi}(\\hat{p}^{*},\\hat{p}_{0})-\\frac{1+0.5c_{1}A_{k}}{2}\\|w^{*}-w_{k}\\|_{2}^{2}-(\\nu_{0}+\\nu A_{k})D_{\\phi}(\\hat{p}^{*},\\hat{p}_{k})}\\\\ {\\displaystyle+\\sum_{i=1}^{k}a_{i}\\frac{c_{1}}{4}\\|w^{*}-w_{i}\\|_{2}^{2}+\\frac{8\\beta^{2}\\sqrt{6B}\\sqrt{\\widehat{\\mathrm{OPT}}_{(2)}}}{c_{1}}\\sum_{i=1}^{k}a_{i}\\chi^{2}(\\hat{p}_{i},\\hat{p}^{*})+\\frac{48\\beta^{2}B\\widehat{\\mathrm{OPT}}A_{k}}{c_{1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "A critical technical component in the proof of Lemma 3.3 is how we handle issues related to nonconvexity. A key technical result that we prove and use is the following. ", "page_idx": 8}, {"type": "text", "text": "Lemma 3.4. Let $S_{i}:=\\mathbb{E}_{\\widehat{\\mu}_{i}}[(\\sigma(w^{*}\\cdot x)\\,{-}\\,\\sigma(w_{i}\\cdot x))^{2}]\\,{+}\\,\\mathbb{E}_{\\widehat{\\mu}_{i}}[2(\\sigma(w_{i}\\cdot x)\\,{-}\\,y)(\\sigma(w^{*}\\cdot x)\\,{-}\\,\\sigma(w_{i}\\cdot x))],$ $\\pmb{w}_{i}$ evolve according to Line $^{6}$ in Algorithm $^{\\,l}$ and supposewe are in thesettingwhereLemma 2.5 holds. Then, $S_{i}\\geq\\mathbb{\\bar{E}}_{\\widehat{\\mu}_{i}}[\\langle v(w;x,y),\\bar{w}^{*}-w_{i}\\rangle]-E_{i}$ where ", "page_idx": 8}, {"type": "equation", "text": "$$\nE_{i}=\\frac{c_{1}}{4}\\|\\pmb{w}^{*}-\\pmb{w}_{i}\\|_{2}^{2}+\\Big(8\\beta^{2}\\sqrt{6B}\\sqrt{\\widehat{\\mathrm{OPT}}_{(2)}}/c_{1}\\Big)\\chi^{2}(\\widehat{\\pmb{\\eta}_{i}},\\widehat{\\pmb{p}}^{*})+(48\\beta^{2}B/c_{1})\\widehat{\\mathrm{OPT}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "This bound is precisely what forces us to choose chi-squared as the measure of divergence between distributions and introduce a dependence on $\\widehat{\\mathrm{OPT}}_{(2)}$ . One pathway to generalize our results to other divergences would be to find a corresponding generalization to Lemma 3.4. ", "page_idx": 8}, {"type": "text", "text": "3.3 Proof of Main Theorem ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Combining Lemma 3.2 and Lemma 3.3, we are now ready to prove our main result. ", "page_idx": 8}, {"type": "text", "text": "Proof of Theorem 3.1. Combining the lower bound on the gap function from Lemma 3.2 with the upper bound from Lemma 3.3 and rearranging, whenever $\\|\\bar{\\pmb{w}}_{i}\\|_{2}\\leq2\\|\\pmb{w}^{*}\\|_{2}$ for all $i\\leq k$ sothat Lemma 2.5 applies, we get that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle-\\,\\frac{12\\beta^{2}B}{c_{1}}\\widehat{\\mathrm{OPT}}A_{k}+\\displaystyle\\sum_{i=1}^{k}a_{i}\\frac{c_{1}}{2}\\|w_{i}-w^{*}\\|_{2}^{2}+\\displaystyle\\sum_{i=1}^{k}\\nu a_{i}D_{\\phi}(\\widehat{\\boldsymbol{p}}^{*},\\widehat{\\boldsymbol{p}}_{i})\\leq\\displaystyle\\sum_{i=1}^{k}a_{i}\\mathrm{Gap}(w_{i},\\widehat{p}_{i})}}\\\\ {{\\displaystyle\\leq\\displaystyle\\frac{1}{2}\\|{\\boldsymbol{w}}^{*}-{\\boldsymbol{w}}_{0}\\|_{2}^{2}+\\nu_{0}D_{\\phi}(\\widehat{\\boldsymbol{p}}^{*},\\widehat{p}_{0})-\\displaystyle\\frac{1+0.5c_{1}A_{k}}{2}\\|{\\boldsymbol{w}}^{*}-{\\boldsymbol{w}}_{k}\\|_{2}^{2}-({\\nu}_{0}+\\nu A_{k})D_{\\phi}(\\widehat{\\boldsymbol{p}}^{*},\\widehat{p}_{k})}}\\\\ {{\\displaystyle+\\sum_{i=1}^{k}a_{i}\\frac{c_{1}}{4}\\|{\\boldsymbol{w}}^{*}-{\\boldsymbol{w}}_{i}\\|_{2}^{2}+\\frac{8\\beta^{2}\\sqrt{6B}\\sqrt{\\widehat{\\mathrm{OPT}}}(2)}{c_{1}}\\sum_{i=1}^{k}a_{i}\\chi^{2}(\\widehat{\\boldsymbol{p}}_{i},\\widehat{\\boldsymbol{p}}^{*})+\\frac{48\\beta^{2}B\\widehat{\\mathrm{OPT}}A_{k}}{c_{1}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "To reach the first claimofthetheorem, we first argue that $\\begin{array}{r l r}{\\sum_{i=1}^{k}a_{i}\\Big((4\\beta^{2}\\sqrt{6B}\\sqrt{\\widehat{\\mathrm{OPT}}_{(2)}}/c_{1})\\chi^{2}(\\widehat{\\boldsymbol{p}}_{i},\\widehat{\\boldsymbol{p}}^{*})\\;-\\;\\nu D_{\\phi}(\\widehat{\\boldsymbol{p}}^{*},\\widehat{\\boldsymbol{p}}_{i})\\Big)}&{\\leq}&{0}\\end{array}$ This follows from (1) Corollary C.2, by which we have $\\widehat{\\boldsymbol{\\mu}}^{*(j)}\\ge\\widehat{\\boldsymbol{\\mu}}_{0}^{(j)}/2$ for all $j\\in[N]$ , hence ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\chi^{2}(\\widehat{\\mu}_{i},\\widehat{p}^{*})=\\sum_{j\\in[N]}(\\widehat{p}^{*(j)}-\\widehat{p}_{i}^{(j)})^{2}/\\widehat{p}^{*(j)}\\leq2\\sum_{j\\in[N]}(\\widehat{p}^{*(j)}-\\widehat{p}_{i}^{(j)})^{2}/\\widehat{p}_{0}^{(j)}=2D_{\\phi}(\\widehat{p}^{*},\\widehat{p}_{i})\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "and (2) our choice of $\\nu$ Which ensures, with high probabilit, that $\\nu\\geq8\\beta^{2}\\sqrt{6B}\\sqrt{\\mathrm{OPT}_{(2)}+\\epsilon}/c_{1}\\geq$ $8\\beta^{2}\\sqrt{6B}\\sqrt{\\widehat{\\mathrm{OPT}}_{(2)}}/c_{1}$ where the last inqualty is because for the spifd samle size, wehave that $\\widehat{\\mathrm{OPT}}_{(2)}+\\epsilon\\geq\\mathrm{OPT}_{(2)}$ by Corollary C.9. ", "page_idx": 8}, {"type": "text", "text": "Second, we similarly have that with probability $1-\\delta$ , OPT \u2264 OPT +e. Hence, since Bregman divergence of a convex function is non-negative, whenever $\\lVert\\pmb{w}_{i}\\rVert_{2}\\leq2\\lVert\\pmb{w}^{*}\\rVert_{2}$ for all $i\\leq k$ wehave ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\pmb{w}^{*}-\\pmb{w}_{k}\\|_{2}^{2}\\leq\\frac{\\|\\pmb{w}^{*}-\\pmb{w}_{0}\\|_{2}^{2}+2\\nu_{0}D_{\\phi}(\\widehat{\\pmb{n}}^{*},\\widehat{\\pmb{\\mu}_{0}})}{1+0.5c_{1}A_{k}}+\\frac{240\\beta^{2}B}{c_{1}}(\\mathrm{OPT}+\\epsilon)}\\\\ {D_{\\phi}(\\widehat{\\pmb{n}}^{*},\\widehat{\\pmb{\\mu}_{k}})\\leq\\frac{\\|\\pmb{w}^{*}-\\pmb{w}_{0}\\|_{2}^{2}/2+\\nu_{0}D_{\\phi}(\\widehat{\\pmb{n}}^{*},\\widehat{\\pmb{\\mu}_{0}})}{\\nu_{0}+\\nu A_{k}}+\\frac{60\\beta^{2}B}{\\nu}(\\mathrm{OPT}+\\epsilon)}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Thebound $\\chi^{2}(\\widehat{\\boldsymbol{p}}^{*},\\widehat{\\boldsymbol{p}}_{0})\\;\\leq\\;c_{1}/(1536\\beta^{4}B)$ is proved in Claim E.1. Finally, in Appendix $\\boldsymbol{\\mathrm E}$ we inductively prove the following claim so that assumptions in Lemma 2.5 are satisfied. ", "page_idx": 9}, {"type": "text", "text": "Claim 3.5. For all iterations $k\\geq0$ $\\lVert\\pmb{w}_{k}\\rVert_{2}\\leq2\\lVert\\pmb{w}^{*}\\rVert_{2}$ ", "page_idx": 9}, {"type": "text", "text": "The bound on the growth of $A_{k}$ follows by standard arguments and is provided as Claim B.6. Since $A_{k}$ grows exponentially with $(1+\\eta)^{k}$ where $\\begin{array}{r}{\\eta\\,=\\,\\frac{\\operatorname*{min}\\left\\{\\nu,c_{1}/8\\right\\}}{2\\,\\operatorname*{max}\\left\\{\\kappa,G\\right\\}}}\\end{array}$ and since $D_{0}(1+\\eta)^{-k}\\,\\le\\,\\epsilon$ can be enforced by setting $k\\,=\\,(1+1/\\eta)\\log(D_{0}/\\epsilon)\\,\\geq\\,\\log(D_{0}/\\epsilon)/\\log(1+\\eta)$ , we have that after $\\begin{array}{r}{\\widetilde{O}(\\frac{\\operatorname*{max}\\{\\kappa,G\\}}{\\operatorname*{min}\\{\\nu,c_{1}\\}}\\log(D_{0}/\\epsilon))}\\end{array}$ $\\|\\pmb{w}_{k}-\\pmb{w}^{*}\\|_{2}\\leq\\sqrt{\\epsilon}$ 00 $\\lVert\\pmb{w}_{i}-\\pmb{w}^{*}\\rVert_{2}\\leq C_{3}\\sqrt{\\mathrm{OPT}}$ ", "page_idx": 9}, {"type": "text", "text": "4 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we study the problem of learning a single neuron in the distributionally robust setting, with the square loss regularized by the chi-squared distance between the reference and target distributions. Our results serve as a preliminary exploration in this area, paving the way for several potential extensions. Future work includes generalizing our approach to single index models with unknown activations, expanding to neural networks comprising multiple neurons, and considering alternative ambiguity sets such as those based on the Wasserstein distance or Kullback-Leibler divergence. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Shuyao Li was supported in part by AFOSR Award FA9550-21-1-0084 and the U.S. Office of Naval Research under award number N00014-22-1-2348. Sushrut Karmalkar was supported by NSF under Grant #2127309 to the Computing Research Association for the CIFellows 2021 Project. Ilias Diakonikolas was supported by NSF Medium Award CCF-2107079 and an H.1. Romnes Faculty Fellowship. Jelena Diakonikolas was supported in part by the U.S. Office of Naval Research under contract number N00014-22-1-2348. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the U.S. Department of Defense. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[ACW22] A. Alacaoglu, V. Cevher, and S. J. Wright. \u201cOn the complexity of a practical primal-dual coordinate method\". In: arXiv preprint arXiv:2201.07684 (2022).   \n[AHW95] P. Auer, M. Herbster, and M. K. K. Warmuth. \u201cExponentially many local minima for single neurons\"'. In: Advances in Neural Information Processing Systems. 1995.   \n[ATV23] P. Awasthi, A. Tang, and A. Vijayaraghavan. \u201cAgnostic Learning of General ReLU Activation Using Gradient Descent\". In: The Eleventh International Conference on Learning Representations, ICLR. 2023.   \n[Bec17] A. Beck. First-order methods in optimization. SIAM, 2017.   \n[Ben+10] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. W. Vaughan.\u201c\"A theory of learning from different domains\". In: Machine learning 79 (2010).   \n[BEN09] A. Ben-Tal, L. ElGhaoui, and A. Nemirovski. Robust optimization. Vol. 28. Princeton university press, 2009.   \n[BBS07] S. Bickel, M Brcker, and T. Scheffer. \u201cDiscriminative learning for diffring training and test distributions\". In: Proceedings of the 24th international conference on Machine learning. 2007.   \n[BMN21] J. Blanchet, K. Murthy, and V. A. Nguyen. \u201cStatistical analysis of Wasserstein distriutionally robust estimators\". In: Tutorials in Operations Research: Emerging Optimization Methods and Modeling Techniques with Applications. INFORMS, 2021.   \n$[\\mathrm{Bla}+24]$ J. Blanchet, J. Li, S. Lin, and X. Zhang. \u201cDistributionally Robust Optimization and Robust Statistics\". In: 2024.   \n$[\\mathrm{Cha}+18]$ A. Chambolle, M. J. Ehrhardt, P. Richtarik, and C.-B. Schonlieb. \u201cStochastic primaldual hybrid gradient algorithm with arbitrary sampling and imaging applications\". In: SIAM Journal on Optimization 28.4 (2018)   \n[CP11] A. Chambolle and T. Pock. \u201cA first-order primal-dual algorithm for convex problems with applications to imaging\". In: Journal of mathematical imaging and vision 40 (2011).   \n[CP18] R. Chen and IC. PaschalidisA robust aring aproach forrgresonmodels basd on distributionally robust optimization\". In: Journal of Machine Learning Research 19.13 (2018).   \n[CP20] R. Chen and I. C. Paschalidis. \u201cDistributionally robust learning\". In: Foundations and Trends? in Optimization 4.1-2 (2020), pp. 1-243.   \n[Che+20] S. Chen, F. Koehler, A. Moitra, and M. Yau. \u201cClassification under misspecification: Halfspaces, generalized linear models, and evolvability\" In: Advances in Neral Iformation Processing Systems 33 (2020).   \n[Dia+20] I. Diakonikolas, S. Goel, S. Karmalkar, A. R. Klivans, and M. Soltanolkotabi. \u201cApproximation Schemes for ReLU regression\". In: Conference on Learning Theory, COLT 2020. 2020.   \n[DKZ20] I. Diakonikolas, D. Kane, and N. Zarifis. \u201cNear-Optimal SQ Lower Bounds for Agnostically Learning Halfspaces and ReLUs under Gaussian Marginals\"'. In: Annual Conference on Neural Information Processing Systems. 2020.   \n[Dia+21] 1. Diakonikolas, D. M. Kane, T. Pittas, and N. Zarifis. \u201c\"The Optimality of Polynomial Regression for Agnostic Learning under Gaussian Marginals in the SQ Model'. In: Proceedings of Machine Learning Research vol 134 (2021).   \n[Dia+22a] I. Diakonikolas, V. Kontonis, C. Tzamos, and N. Zarifis. \u201cLearning a Single Neuron with Adversarial Label Noise via Gradient Descent'. In: Conference on Learning Theory (COLT). 2022, pp. 4313-4361.   \n[DPT21] I. Diakonikolas, J. H. Park, and C. Tzamos. \u201cReLU Regression with Massart Noise\". In: Advances in Neural Information Processing Systems 34 (2021).   \n[Dia+22b] 1. Diakonikolas, D. Kane, P Manurangsi, and L. Ren. \u201cHardness of Learning a Single Neuron with Adversarial Label Noise\". In: Proceedings of The 25th International Conference on Artificial Intelligence and Statistics. 2022.   \n[DKR23] I. Diakonikolas, D. Kane, and L. Ren. \u201cNear-optimal cryptographic hardness of agnostically learning halfspaces and relu regression under gaussian marginals\". In: International Conference on Machine Learning. PMLR. 2023.   \n[Dia+22c]  J. Diakonikolas, C. Li, S. Padmanabhan, and C. Song. \u201c\"A fast scale-invariant algorithm for non-negative least squares with non-negative data\". In: Advances in Neural Information Processing Systems (2022).   \n[D019] J. Diakonikolas and L. Orecchia. \\*The Approximate Duality Gap Technique: A Unified Theory of First-Order Methods\". In: SIAM Journal on Optimization 29.1 (2019), pp. 660-689.   \n[DN19] J. Duchi and H. Namkoong.\"Variance-based regularization with convex objectives\". In: Journal of Machine Learning Research (2019).   \n[Duc+08] J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra. \u201cEffcient projections onto the $\\ell_{1}$ -ball for learning in high dimensions\". In: Proceedings of the 25th international conference on Machine learning. 2008.   \n[DGN21] J. C. Duchi, P. W. Glynn, and H. Namkoong. \u201cStatistics of robust optimization: A generalized empirical likelihood approach\". In: Mathematics of Operations Research 46.3 (2021).   \n[DN21] J. C. Duchi and H. Namkoong. \u201cLearning models with uniform performance via distributionally robust optimization\". In: The Annals of Statistics 49.3 (2021).   \n[Dwo+12] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel. \u201cFairness through awareness\". In: Proceedings of the 3rd Innovations in Theoretical Computer Science Conference. 2012.   \n[FCG20] S. Frei, Y. Cao, and Q. Gu.\"Agnostic learning of a single neuron with gradient descent\". In: Advances in Neural Information Processing Systems 33 (2020).   \n[GGK20] S. Goel, A. Gollakota, and A. R. Klivans. \u201cStatistical-Query Lower Bounds via Functional Gradients\". In: Annual Conference on Neural Information Processing Systems. 2020.   \n[GKK19] S. Goel, S. Karmalkar, and A. R. Klivans. \\*Time/Accuracy Tradeoffs for Learning a ReLU with respect to Gaussian Marginals\". In: Advances in Neural Information Processing Systems 32. 2019.   \n$[\\mathrm{Gol}+23]$ A. Gollakota, P. Gopalan, A. R. Klivans, and K. Stavropoulos. \"Agnostically Learning Single-Index Modeis using Omnipredictors\" In: Thirty-seventh Conference on Neural Information Processing Systems. 2023.   \n[GSS14] I. J. Goodfellow, J. Shlens, and C. Szegedy. \u201cExplaining and harnessing adversarial examples\". In: arXiv preprint arXiv: 1412.6572 (2014).   \n[HM13] M. Hardt and A. Moitra. \u201cUnderstanding Alternating Minimization for Matrix Completion'\". In: 2013.   \n$[\\mathrm{Has}{+}18\\mathrm{a}]$ T. Hashimoto, M. Srivastava, H. Namkoong, and P. Liang. \u201cFairness Without Demographics in Repeated Loss Minimization\". In: ICML. 2018.   \n$[\\mathrm{Has}+18\\mathrm{b}]$ T. Hashimoto, M. Srivastava, H. Namkoong, and P. Liang, \u201cFairness without demographics in repeated loss minimization\". In: International Conference on Machine Learning. PMLR. 2018.   \n[Hau92] D. Haussler. Decisin theoretic generalizations of the PACmodel for neural nt and other learning applications\". In: Information and Computation 100 (1992),pp. 78-150.   \n$[\\mathrm{Hu}+18]$ W. Hu, G. Niu, I. Sato, and M. Sugiyama. \u201cDoes distributionally robust supervised learning give robust classifiers? In: ICML. 2018.   \n[Hua+06] J. Huang, A. Gretton, K. Borgwardt, B. Scholkopf, and A. Smola. \u201cCorrecting sample selection bias by unlabeled data\". In: Advances in neural information processing systems 19 (2006).   \n$[\\mathrm{Kak}+11]$ S. M. Kakade, V. Kanade, O. Shamir, and A. Kalai. \u201cEfficient learning of generalized linear and single index models with isotonic regression\". In: Advances in Neural Information Processing Systems 24 (2011).   \n[KS09] A. T. Kalai and R. Sastry. \"The Isotron Algorithm: High-Dimensional Isotonic Regression.\" In: COLT. 2009.   \n[KSA19] S. M. M. Kalan, M. Soltanolkotabi, and S. Avestimehr. \u201cFitting relus via sgd and quantized sgd\". In: 2019 IEEE International Symposium on Information Theory (ISIT). IEEE.2019.   \n$[\\mathrm{Kal}+22]$ N. Kallus, X. Mao, K. Wang, and Z. Zhou. \u201cDoubly Robust Distributionally Robust Off-Policy Evaluation and Learning\". In: ICML. 2022.   \n[KMM20] S. Karmakar, A. Mukherjee, and R. Muthukumar.\u201cA Study of Neural Training with Iterative Non-Gradient Methods\". In: arXiv e-prints (2020), arXiv-2005.   \n[KSS92] M. J. Kearns, R. E. Schapire, and L. M. Sellie. Toward efficient agnostic learning\". In: Proceedings of the fifth annual workshop on Computational learning theory. 1992.   \n[KKM17] A. R. Klivans, P. K. Kothari, and R. Meka. \u201cLearning Halfspaces and Neural Networks with Random Initialization\". In: Proceedings of the 30th Annual Conference on Learning Theory. 2017.   \n[Kuh+19] D. Kuhn, P. M. Esfahani, V. A. Nguyen, and S. Shafieezadeh-Abadeh. \u201cWasserstein distributionally robust optimization: Theory and applications in machine learning\". In: Operations research & management science in the age of analytics. Informs, 2019, Pp. 130-166.   \n[Lam13] H. Lam. \u201cRobust Sensitivity Analysis for Stochastic Systems\". In: Mathematics of Operations Research (2013).   \n[Liu $+21$ E. Z. Liu, B. Haghgoo, A. S. Chen, A. Raghunathan, P. W. Koh, S. Sagawa, P. Liang, and C. Finn.\"Just Train Twice: Improving Group Robustness without Training Group Information\". In: ICML. 2021.   \n[Liu $+22$ \u4e00 Z. Liu, Q. Bai, J. Blanchet, P. Dong, W. Xu, Z. Zhou, and Z. Zhou. \u201cDistributionally Robust $Q$ -Learning\". In: ICML. 2022.   \n[Lot+23] K. Lotidis, N. Bambos, J. Blanchet, and J. Li. \"Wasserstein Distributionally Robust Linear-Quadratic Estimation under Martingale Constraints\". In: AISTATS. 2023.   \n[MMR09] Y. Mansour, M. Mohri, and A. Rostamizadeh. \u201cDomain adaptation: Learning bounds and algorithms\"'. In: arXiv preprint arXiv:0902.3430 (2009).   \n[MR18] P. Manurangsi and D. Reichman. \u201cThe computational complexity of raining relu (s)\". In: arXiv preprint arXiv: 1810.04207 (2018).   \n[MDH24] R. Mehta, J. Diakonikolas, and Z. Harchaoui. \u201cA Primal-Dual Algorithm for Faster Distributionally Robust Optimization\". In: arXiv preprint arXiv:2403.10763 (2024).   \n[MBM18] S. Mei, Y. Bai, and A. Montanari. \u201cTHE LANDSCAPE OF EMPIRICAL RISK FOR NONCONVEX LOSSES\". In: The Annals of Statistics 46.6A (2018), pp. 2747-2774.   \n[ND16] H. Namkoong and J. C. Duchi. \"Stochastic gradient methods for distributionally robust optimization with f-divergences\". In: Advances in neural information processing systems 29 (2016).   \n[NW72] J. A. Nelder and R. W. M. Wedderburn. \u201cGeneralized linear models\". In: Journal of the Royal Statistical Society: Series A (General) 135.3 (1972).   \n[Ore+19] Y. Oren, S. Sagawa, T. B.Hashimoto, and P Liang.\u201cDistributionall robust languae modeling\". In: arXiv preprint arXiv: 1909.02060 (2019).   \n[PY09] S. J. Pan and Q. Yang. \u201cA survey on transfer learning\". In: IEEE Transactions on knowledge and data engineering 22.10 (2009).   \n$\\mathrm{[Pat+15]}$ V. M. Patel, R. Gopalan, R.Li, and R. Chellappa. Visual domain adaptation: A survey of recent advances\". In: IEEE signal processing magazine 32.3 (2015).   \n$[Q\\mathrm{i}+21]$ Q. Qi, Z. Guo, Y. Xu, R. Jin, and T. Yang. \u201cAn online method for a class of distributionally robust optimization with non-convex objectives\". In: Advances in Neural Information Processing Systems 34 (2021), pp. 10067-10080.   \n[RM22] H. Rahimian and S. Mehrotra. \u201cFrameworks and results in distributionally robust optimization\". In: Open Journal of Mathematical Optimization (2022).   \n$[\\mathrm{Sap}+23]$ H. Sapkota, D. Wang, Z. Tao, and Q. Yu.\u201cDistributionally Robust Ensemble of Lottery Tickets Towards Calibrated Sparse Network Training\". In: NeurIPS. 2023.   \n[Sha17] A. Shapiro.\u201cDistributionally robust stochastic programming\". In: SIAM Journal on Optimization 27.4 (2017).   \n[Sha+20] V. D. Sharma, M. Toubeh, L. Zhou, and P Tokekar.\u201cRisk-Aware Planning and Assig ment for Ground Vehicles using Uncertain Perception from Aerial Vehicles\". In: 2020 IEEE/RSJ (IROS). 2020.   \n[Shi00] H. Shimodaira. \u201cImproving predictive inference under covariate shift by weighting the log-likelihood function\". In: Journal of statistical planning and inference 90.2 (2000).   \n[Sim02] J. Sima. \u201cTraining a single sigmoidal neuron is hard\". In: Neural computation 14.11 (2002).   \n[SND18]  A. Sinha, H. Namkoong, and J. Duchi. \u201c\"Certifying Some Distributional Robustness with Principled Adversarial Training\". In: International Conference on Learning Representations. 2018.   \n[Sol17] M. Soltanolkotabi. \u201cLearning ReLUs via gradient descent\". In: Advances in neural information processing systems 30 (2017).   \n$[\\mathrm{Son}+22]$ C. Song, C. Y. Lin, S. Wright, and J. Diakonikolas. \u201c\"Coordinate Linear Variance Reduction for Generalized Linear Programming\". In: NeurIPS. 2022.   \n[SWD21] C. Song, S. J. Wright, and J. Diakonikolas. \"Variance Reduction via Primal-Dual Accelerated Dual Averaging for Nonsmooth Convex Finite-Sums\". In: Proc. ICML'21. 2021.   \n[SJ19] M. Staib and S. Jegelka. \u201cDistributionally robust optimization and generalization in kernel methods\". In: Advances in Neural Information Processing Systems 32 (2019).   \n[Tan+18] C. Tan, F. Sun, T. Kong, W. Zhang, C. Yang, and C. Liu. \u201cA survey on deep transfer learning\". In: Artificial Neural Networks and Machine Learning-ICANN 2018. Springer. 2018.   \n[Wan+23a] P. Wang, N. Zarifis, I. Diakonikolas, and J. Diakonikolas. \u201c\"Robustly Learning a Single Neuron via Sharpness\". In: Proceedings of the 4Oth International Conference on Machine Learning. Vol. 202. Proceedings of Machine Learning Research. PMLR, July 2023, pp. 36541-36577.   \n[Wan+23b] S. Wang, N. Si, J. Blanchet, and Z. Zhou.\u201cA Finite Sample Complexity Bound for Distributionally Robust Q-learning\". In: AISTATS. 2023.   \n[Wan+23c] Z. Wang, L. Shen, T. Liu, T. Duan, Y. Zhu, D. Zhan, D. S. Doermann, and M. Gao. \"Defending against Data-Free Model Extraction by Distributionally Robust Defensive Training\". In: NIPS. 2023.   \n$[X\\mathrm{u}+20]$ Z. Xu, C. Dan, J. Khim, and P. Ravikumar. \"Class-weighted classification: Trade-offs and robust approaches\". In: International conference on machine learning. PMLR. 2020.   \n[Yan+23] Z. Yang, Y. Guo, P. Xu, A. Liu, and A. Anandkumar.\u201cDistributionally Robust Policy Gradient for Offline Contextual Bandits\". In: AISTATS. 2023.   \n[YS20] G. Yehudai and O. Shamir. \u201cLearning a single neuron with gradient methods\". In: Conference on Learning Theory. 2020, pp. 3756-3786.   \n$[\\mathrm{Yu}+23]$ Z. Yu, L. Dai, S. Xu, S. Gao, and C. P. Ho. \u201cFast Bellman Updates for Wasserstein Distributionally Robust MDPs\". In: NeurIPS. Curran Associates, Inc., 2023.   \n$[Z a r+24]$ N. Zarifis, P. Wang, I. Diakonikolas, and J. Diakonikolas. \u201cRobustly Learning SingleIndex Models via Alignment Sharpness\". In: 4lth International Conference on Machine Learning, arXiv preprint: 2402.17756 (2024).   \n[Zha+21] R. Zhai, C. Dan, Z. Kolter, and P. Ravikumar. \u201cDORO: Distributional and Outlier Robust Optimization\". In: Proceedings of the 38th International Conference on Machine Learning. Vol. 139. Proceedings of Machine Learning Research. PMLR, July 2021, pp. 12345-12355.   \n[Zhu+20] J.-J. Zhu, W. Jitkrittum, M. Diehl, and B. Scholkopf. \u201cKernel distributionally robust optimization\"'. In: arXiv preprint arXiv:2006.06981 (2020). ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Supplementary Material ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Organization _ In Appendix A we briefly discuss related work. In Appendix B we set up some additional preliminaries for the rest of the appendix. In Appendix C we show that expectations of some important functions with respect to $\\widehat{\\boldsymbol{p}}^{*}$ are close to their expectation with respect to $\\boldsymbol{p}^{*}$ . In Appendix D we give a detailed proof of an upper bound on the gap of the iterates our algorithm generates (i.e. Lemma 3.3). Finally, in Appendix F we show that the estimate of $\\pmb{w}^{*}$ our algorithm returns is a constant factor approximation to the squared loss of $\\pmb{w}^{*}$ with respect to the target distribution. ", "page_idx": 14}, {"type": "text", "text": "A Related Work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Learning Noisy Neurons  The problem of learning noisy neurons has been extensively explored in recent decades; notable works include $[\\mathrm{Kak}+11$ ; KS09; KKM17; NW72]. In the recent past, the focus has shifted towards specific activation functions such as ReLUs, under both easy noise models such as realizable/random additive noise [KSA19; Sol17; YS20] and more difficult ones like adversarial label noise $[\\mathrm{Dia}+20$ ; DKZ20; Dia $+21$ $\\mathrm{Dia}{+}22\\mathrm{a}$ $\\mathrm{Dia}{+}226$ ; GGK20; GKK19; Wan $\\pm23\\mathrm{a}$ \u4e00 ", "page_idx": 14}, {"type": "text", "text": "Even with clean labels, this problem has exponentially many local minima when using squared loss [AHW95]. Unfortunately, directly minimizing the squared loss using (S)GD on a bounded distribution does not converge to the global optimum with probability 1 [YS20]. Even so, gradient based methods can achieve suboptimal rates in the agnostic setting ( $\\dot{\\overline{{\\mathrm{\\Omega}}}}\\dot{\\overline{{\\mathrm{OPT}_{0}}}}$ VS $\\mathrm{OPT_{0}}$ ) for distributions with mild distributional assumptions [FCG20]. Making slightly stronger assumptions on the marginal does allow us to get efficient constant factor approximations. $[\\mathrm{Dia}+20]$ developed an efficient learning method that is able to handle this in the presence of adversarial label noise and for isotropic logconcave distributions of the covariates. This was later extended to broader classes of activation functions and under weaker distributional assumptions by $[\\mathrm{Dia}+22\\mathrm{a}$ $\\mathrm{Wan}{+}23\\mathrm{a}$ 1. Without specific distributional assumptions, learning remains computationally difficult $[\\mathrm{Dia}+22\\mathrm{b}$ ; HM13]. The challenges extend to distribution-free scenarios with semi-random label noise, where methods like those in [DPT21] address bounded noise, and [KMM20] and $[\\mathrm{Che}{+}20]$ explore stricter forms of Massart noise in learning a neuron with noise. In this paper, we consider the harder setting of distributionally robust optimization, where an adversary is allowed to impose not only errors in the labels, but also adversarial shifts in the underlying distribution of the covariates. ", "page_idx": 14}, {"type": "text", "text": "Distributionally Robust Optimization Distributional mismatches in data have been extensively studied in the context of learning from noisy data. This includes covariate shift, where the marginal distributions might be perturbed, [BBSO7; Hua $+06$ ; Shioo], and changes in label proportions $[\\mathrm{Dwo}{+}12$ $\\mathrm{Xu}{+20}]$ . This research also extends to domain adaptation and transfer learning $[\\mathsf{B e n}\\!+\\!10$ ; MMR09; PY09; $\\mathrm{Pat}+15$ $\\mathrm{Tan}{+}18\\$ 1. Distributionally robust optimization (DRO) has a rich history in optimization [BEN09; Sha17] and has gained traction in machine learning [DGN21; DN21; $\\mathrm{Kuh}{+}19$ ; ND16; SJ19; $Z\\mathrm{hu}{+20}$ I, showing mixed success across applications like language modeling $\\scriptstyle[0\\mathrm{re}+19]$ , class imbalance correction $[X\\mathrm{u}+20]$ , and group fairness $[\\mathrm{Has}+18\\mathrm{b}]$ ", "page_idx": 14}, {"type": "text", "text": "Specifically this has also been studied in the context of linear regression and other function approximation [BMN21; CP18; DN21]. Typically, DRO is often very sensitive to additional sources of noise, suchas outliers $\\scriptstyle\\left[\\mathrm{Has}+18\\mathrm{a}\\right.$ $\\mathrm{Hu}+18$ ; Zha+21l). However, prior work makes strong assumptions on the label noise as well as requiring convexity of the loss. We study the problem of learning a neuron where the labels have no guaranteed structure, effectively studying the setting for a combination of two notions of robustness \u2014\u2014 agnostic learning as well as covariate shift. ", "page_idx": 14}, {"type": "text", "text": "B Supplementary Preliminaries ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1  Additional Notation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Given an $m\\,\\times\\,n$ matrix $\\pmb{A}$ , the operator norm of $\\pmb{A}$ is defined in the usual way as $\\|A\\|_{\\mathrm{op}}\\,=$ $\\operatorname*{sup}\\{||A pmb{x}||_{2}:\\pmb{x}\\in\\mathbb{R}^{n},\\|\\pmb{x}\\|_{2}\\leq1\\}$ . For problems $(P)$ and $(P^{\\prime})$ , we use $(P)\\equiv(P^{\\prime})$ to denote the equivalence of $(P)$ and $(P^{\\prime})$ . For a vector space $\\mathbb{E}$ ,we use $\\mathbb{E}^{*}$ to denote its dual space. ", "page_idx": 14}, {"type": "text", "text": "B.2 Standard Facts and Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Fact B.1 (Young's inequality). If $a\\geq0$ and $b\\geq0$ arenonnegativereal numbersandif $p>1$ and $q>1$ arerealnumberssuchthat ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\frac{1}{p}}+{\\frac{1}{q}}=1,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "then ", "page_idx": 15}, {"type": "equation", "text": "$$\na b\\leq{\\frac{a^{p}}{p}}+{\\frac{b^{q}}{q}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Equality holds if and only if $a^{p}=b^{q}$ ", "page_idx": 15}, {"type": "text", "text": "Fact B.2 (Hoeffding's Inequality). Let $X_{1},X_{2},\\ldots,X_{n}$ be independent random variables such that $a_{i}\\leq X_{i}\\leq b_{i}$ almost surelyfor all i. Let $\\textstyle{\\overline{{X}}}={\\frac{1}{n}}\\sum_{i=1}^{n}X_{i}$ Then,for any $t>0$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[|{\\overline{{X}}}-\\mathbb{E}[{\\overline{{X}}}]|\\geq t\\right]\\leq2\\exp\\left(-{\\frac{2n t^{2}}{\\sum_{i=1}^{n}(b_{i}-a_{i})^{2}}}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Fact 2.7. Let $\\psi(\\pmb{x})=\\phi(\\pmb{x})+\\langle\\pmb{a},\\pmb{x}\\rangle+b$ for some $\\pmb{a}\\in\\mathbb{R}^{N}$ and $b\\in\\mathbb{R}$ Then $D_{\\psi}({\\pmb y},{\\pmb x})=D_{\\phi}({\\pmb y},{\\pmb x})$ for all $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{N}$ , i.e., the Bregman divergence is blind to the addition of affine terms to function $\\phi$ ", "page_idx": 15}, {"type": "text", "text": "Proof of Fact 2.7. The Bregman divergence $D_{\\phi}$ and $D_{\\psi}$ are defined by: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{D_{\\phi}({\\pmb y},{\\pmb x})=\\phi({\\pmb y})-\\phi({\\pmb x})-\\langle\\nabla\\phi({\\pmb x}),{\\pmb y}-{\\pmb x}\\rangle,}}\\\\ {{D_{\\psi}({\\pmb y},{\\pmb x})=\\psi({\\pmb y})-\\psi({\\pmb x})-\\langle\\nabla\\psi({\\pmb x}),{\\pmb y}-{\\pmb x}\\rangle.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $\\nabla\\psi({\\pmb x})=\\nabla\\phi({\\pmb x})+{\\pmb a}$ , substituting in the definition gives: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{D_{\\psi}({\\pmb y},{\\pmb x})=\\phi({\\pmb y})+\\langle{\\pmb a},{\\pmb y}\\rangle+b-(\\phi({\\pmb x})+\\langle{\\pmb a},{\\pmb x}\\rangle+b)-\\langle\\nabla\\phi({\\pmb x})+{\\pmb a},{\\pmb y}-{\\pmb x}\\rangle}}\\\\ {{=\\phi({\\pmb y})-\\phi({\\pmb x})-\\langle\\nabla\\phi({\\pmb x}),{\\pmb y}-{\\pmb x}\\rangle=D_{\\phi}({\\pmb y},{\\pmb x}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, the Bregman divergence is blind to the addition of linear terms to the function $\\phi$ ", "page_idx": 15}, {"type": "text", "text": "B.3 Auxiliary Facts ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We first state and prove Lemma B.3 to obtain upper bounds on the norm of each point, projections onto vectors of norm at most $W$ , and the loss value at each point. ", "page_idx": 15}, {"type": "text", "text": "Lemma B.3 (Boundedness). Fix $\\pmb{w}\\ \\in\\ \\mathcal{B}(W)$ .Forall samples $({\\pmb x}_{i},y_{i})$ withtruncatedlabels $|y_{i}|<M$ as per Fact 2.6 and bounded covariates as per Assumption 2.3, it holds that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\pmb w\\cdot\\pmb x_{i}\\leq W S}}\\\\ {{\\|\\pmb x_{i}\\|_{2}\\leq S\\sqrt{d}}}\\\\ {{(\\sigma(\\pmb w\\cdot\\pmb x_{i})-y_{i})^{2}\\leq2\\beta^{2}W^{2}(S^{2}+C_{M}^{2}B^{2}\\log^{2}(W B\\beta/\\epsilon))}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Equation (18) follows from Assumption 2.3, as ", "page_idx": 15}, {"type": "equation", "text": "$$\nw\\cdot x_{i}=\\|w\\|_{2}\\frac{w}{\\|w\\|_{2}}\\cdot x_{i}\\leq\\|w\\|_{2}S\\leq W S.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To prove Equation (19), for each coordinate $j\\,\\in\\,[d]$ we have $|{\\pmb x}^{(j)}|=\\mathrm{sign}({\\pmb x}^{(j)}){\\pmb e}_{j}\\cdot{\\pmb x}\\leq S$ by again using Assumption 2.3. Therefore, $\\|\\pmb{x}\\|_{2}\\leq S\\sqrt{d}$ ", "page_idx": 15}, {"type": "text", "text": "For Equation (20), we recall Fact 2.6 that for some sufficiently large absolute constant $C_{M}$ , it holds that $|y|\\,\\leq\\,M:=\\,C_{M}W B\\beta\\log(\\beta B W/\\epsilon)$ . Thus, Equation (20) follows from Young's inequality (Fact B.1) and Equation (19), since $|\\sigma(t)|\\,\\leq\\,\\beta|t|$ ,which follows from $\\beta$ -Lipschitzness of $\\sigma$ and $\\sigma(0)=0$ \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Lemma B.4 (Bounds on $\\pmb{v}$ ).Let ", "page_idx": 16}, {"type": "equation", "text": "$$\nv(w;x,y)=2(\\sigma(w\\cdot x)-y)\\beta x\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then v is uniformly bounded by $G$ in $\\ell_{2}$ -norm and $\\kappa$ -Lipschitz for all samples $x,y$ with truncated labels $|y|<M$ and bounded covariates as per Assumption 2.3, where $G=2\\beta S\\sqrt{d}(\\sqrt{2}\\beta W S+M)$ and $\\kappa=2\\beta^{2}S^{2}d.$ ", "page_idx": 16}, {"type": "text", "text": "Proof. We first uniformly upper bound $\\pmb{v}$ . An application of Lemma B.3 gives us, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|v(\\boldsymbol{w};\\boldsymbol{x},\\boldsymbol{y})\\|_{2}^{2}=4\\beta^{2}\\left(\\sigma(\\boldsymbol{w}\\cdot\\boldsymbol{x})-y)^{2}\\;\\|\\boldsymbol{x}\\|^{2}\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.\\leq4\\;\\beta^{2}\\;(2\\beta^{2}W^{2}S^{2}+M^{2})\\;S^{2}d.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Taking square roots, we get $\\|v(w;x,y)\\|_{2}\\leq2\\beta S\\sqrt{d}(\\sqrt{2}\\beta W S+M)=:G.$ ", "page_idx": 16}, {"type": "text", "text": "We now upper bound the Lipschitz constant $\\kappa$ . We will use the fact that $\\sigma$ is $\\beta$ -Lipschitz. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{w}v(w;x,y)\\|_{2}=2\\beta|\\sigma^{\\prime}({\\pmb w}\\cdot{\\pmb x})|\\|{\\pmb x}{\\pmb x}^{T}\\|_{2}}\\\\ &{\\qquad\\qquad\\qquad=2\\beta|\\sigma^{\\prime}({\\pmb w}\\cdot{\\pmb x})|\\|{\\pmb x}\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad=2\\beta\\cdot\\beta\\;{\\pmb S}^{2}d=2\\beta^{2}{\\pmb S}^{2}d=:\\kappa.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Corollary B.5. Fix a reference distribution $\\mu_{0}$ .Suppose $\\|\\pmb{v}(\\pmb{w};\\pmb{x},y)\\|_{2}\\leq G$ forall $\\pmb{w}$ almost surely. Then for all distributions $\\ p,q\\in\\mathcal{P}(p_{0})$ it holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbb{E}_{\\boldsymbol\\mu}[v({\\boldsymbol w};{\\boldsymbol x},{\\boldsymbol y})]-\\mathbb{E}_{\\boldsymbol\\ q}[v({\\boldsymbol w};{\\boldsymbol x},{\\boldsymbol y})]\\|_{2}^{2}\\le G^{2}D_{\\phi}(p,{\\boldsymbol q}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\mathbb{E}_{p}[v(w;\\mathbf{x},y)]-\\mathbb{E}_{q}[v(w;\\mathbf{x},y)]\\|_{2}^{2}=\\Big\\|\\int v(w;x,y)(\\mathrm{d}\\mu-\\mathrm{d}\\phi)\\Big\\|_{2}^{2}}&{}\\\\ {=\\Big\\|\\int v(w)\\Big(\\frac{\\mathrm{d}p}{\\mathrm{d}\\mu_{0}}-\\frac{\\mathrm{d}q}{\\mathrm{d}\\mu_{0}}\\Big)\\mathrm{d}\\mu_{0}\\Big\\|_{2}^{2}}&{}\\\\ {\\overset{(i)}{\\leq}\\int\\Big\\|v(w)\\Big(\\frac{\\mathrm{d}p}{\\mathrm{d}\\mu_{0}}-\\frac{\\mathrm{d}q}{\\mathrm{d}\\mu_{0}}\\Big)\\Big\\|_{2}^{2}\\mathrm{d}\\mu_{0}}&{}\\\\ {\\overset{(i i)}{\\leq}G^{2}\\int\\Big(\\frac{\\mathrm{d}p}{\\mathrm{d}\\mu_{0}}-\\frac{\\mathrm{d}q}{\\mathrm{d}\\mu_{0}}\\Big)^{2}\\mathrm{d}\\mu_{0}}&{}\\\\ {=G^{2}D_{x^{2}}(\\underbrace{\\mathrm{d}\\mu}_{\\geq,0})(p,q),}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $(i)$ is an application of Jensen's inquality and $(i i)$ follows from Lemma B.4. ", "page_idx": 16}, {"type": "text", "text": "Claim B.6 (Convergence Rate). For all $i\\geq0$ let $a_{i}$ be defined as in Line 3.Then it holds that $\\begin{array}{r}{\\frac{2{G^{2}}{a_{i}}^{2}}{1+0.5c_{1}A_{i}}\\,\\leq\\,\\nu_{0}+\\bar{\\nu}A_{i-1}}\\end{array}$ $\\begin{array}{r}{\\frac{2\\kappa^{2}{a_{i}}^{2}}{1+0.5c_{1}A_{i}}\\;\\leq\\;\\frac{1+0.5c_{1}A_{i-1}}{4}}\\end{array}$ 1+0.5cAi-1 for all i. Moreover, Ak = i=o ai = $\\begin{array}{r}{\\big(\\big(1+\\frac{\\operatorname*{min}\\{\\nu,c_{1}/8\\}}{2\\operatorname*{max}\\{\\kappa,G\\}}\\big)^{k}-1\\big)\\operatorname*{min}\\{\\nu_{0},1/4\\}/\\operatorname*{min}\\{\\nu,c_{1}/8\\}.}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "Proof. In order for both $\\begin{array}{r}{\\frac{2{G^{2}a_{i}}^{2}}{1+0.5c_{1}A_{i}}\\,\\leq\\,\\nu_{0}+\\nu A_{i-1}}\\end{array}$ and $\\begin{array}{r}{\\frac{2\\kappa^{2}{a_{i}}^{2}}{1+0.5c_{1}A_{i}}\\;\\leq\\;\\frac{1+0.5c_{1}A_{i-1}}{4}}\\end{array}$ 1+0.5c1 Ai-1 to hold for all iterations $i$ ,it suffices that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{4\\operatorname*{max}\\{G,\\kappa\\}^{2}{a_{i}}^{2}}{1+0.5c_{1}A_{i}}\\leq\\operatorname*{min}\\{\\nu_{0},1/4\\}+\\operatorname*{min}\\{\\nu,c_{1}/8\\}A_{i-1},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for which it suffices to enforce ", "page_idx": 16}, {"type": "equation", "text": "$$\n4\\operatorname*{max}\\{G,\\kappa\\}^{2}{a_{i}}^{2}=(\\operatorname*{min}\\{\\nu_{0},1/4\\}+\\operatorname*{min}\\{\\nu,c_{1}/8\\}A_{i-1})^{2},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we used $A_{i-1}\\leq A_{i}$ ", "page_idx": 16}, {"type": "text", "text": "Taking a square root on both sides using $a_{i}>0$ , we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n2\\operatorname*{max}\\{G,\\kappa\\}a_{i}=\\operatorname*{min}\\{\\nu_{0},1/4\\}+\\operatorname*{min}\\{\\nu,c_{1}/8\\}A_{i-1}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Solving this recurrence relation using Mathematica, we compute that for all iterations $i$ and $k$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle a_{i}=\\left(1+\\frac{\\operatorname*{min}\\{\\nu,c_{1}/8\\}}{2\\operatorname*{max}\\{\\kappa,G\\}}\\right)^{i-1}\\operatorname*{min}\\{\\nu_{0},1/4\\}/(2\\operatorname*{max}\\{\\kappa,G\\})}\\\\ {\\displaystyle A_{k}=\\sum_{i=0}^{k}a_{i}=\\big((1+\\frac{\\operatorname*{min}\\{\\nu,c_{1}/8\\}}{2\\operatorname*{max}\\{\\kappa,G\\}})^{k}-1\\big)\\operatorname*{min}\\{\\nu_{0},1/4\\}/\\operatorname*{min}\\{\\nu,c_{1}/8\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Remark B.7. Note that in our case, the gap is not guarenteed to be non-negative as is usually the case for convex-concave min-max problems. Recall that $\\operatorname{Gap}(\\pmb{w},\\widehat{p})=(L(\\pmb{\\dot{w}},\\widehat{p}^{*})-L(\\pmb{w}^{*},\\widehat{p}^{*}))+$ $\\big(L(\\pmb{w}^{*},\\widehat{\\pmb{\\mathscr{n}}}^{*})-L(\\pmb{w}^{*},\\widehat{\\pmb{\\mathscr{n}}})\\big)$ ", "page_idx": 17}, {"type": "text", "text": "Consider the following example: ", "page_idx": 17}, {"type": "text", "text": "Let $\\ensuremath{\\mu}_{0}$ be the uniform distribution over $\\{(-2,2),(2,1.5)\\}$ \uff0c $\\sigma\\equiv\\operatorname{ReLU}$ and $\\nu=0$ . Then, $w^{*}=-1$ and $\\boldsymbol{p}^{*}$ is the distribution which places all its mass on (2, 1.5). Then, $\\mathrm{Gap}(1,\\kappa^{*})=L(1,\\kappa^{*})\\;-$ $L(-\\mathrm{i},p^{*})=0.25-2.25<0$ ", "page_idx": 17}, {"type": "text", "text": "This is why we also need an explicit lower bound on the Gap that we have shown in Lemma 3.2. ", "page_idx": 17}, {"type": "text", "text": "C Concentration ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Recall that that $\\widehat{\\boldsymbol{q}}_{w}$ is not guaranteed to act as an empirical estimate of $q_{w}$ , because we cannot draw samples from the (unknown) distribution $q_{w}$ but only from $\\,\\!\\,n^{0}$ . In this section, we show that for certain important functions $f$ , it holds that $\\mathbb{E}_{\\widehat{\\boldsymbol{q}}_{w}}[f]\\approx\\dot{\\mathbb{E}}_{\\boldsymbol{q}_{w}}[f]$ . We will abuse terminology and say that $f$ \"concentrates\"\u201d with respect to $q_{w}$ ", "page_idx": 17}, {"type": "text", "text": "Organization: \u03b2 In Appendix C.1 we derive closed-form expressions for $q_{w}$ and $\\widehat{q}_{w}$ in terms of $\\mu_{0}$ and ${\\widehat{\\boldsymbol{p}}}_{0}$ respectively. Note that bounded functions concentrate with respect to $\\ensuremath{p_{0}}$ . In Appendix C.2 we use the closed-form expressions found in Appendix C.1 to translate these concentration properties to $\\widehat{q}_{w}$ . Finally, in Appendix C.3 we show that $\\widehat{\\boldsymbol{p}}^{*}$ satisfies sharpness, $\\widehat{\\mathrm{OPT}}\\approx\\mathrm{OPT}$ and $\\widehat{\\mathrm{OPT}}_{(2)}\\approx$ OPT(2)\u00b7 ", "page_idx": 17}, {"type": "text", "text": "C.1  Closed-form expression ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The following lemma gives us a closed-form expression for $q_{w}$ and $\\widehat{\\boldsymbol{q}}_{w}$ in terms of $\\mu_{0}$ and ${\\widehat{\\boldsymbol{p}}}_{0}$ respectively. ", "page_idx": 17}, {"type": "text", "text": "We start with an additional definition to Definition 1.2: ", "page_idx": 17}, {"type": "text", "text": "$R(\\pmb{w};\\widehat{p}_{0}):=\\operatorname*{max}_{\\widehat{p}\\in\\mathcal{P}}\\mathbb{E}_{(\\pmb{x},\\pmb{y})\\sim\\widehat{p}}(\\sigma(\\pmb{w}\\cdot\\pmb{x})-y)^{2}-\\nu\\chi^{2}(\\widehat{p},\\widehat{p}_{0})$ with the maximum achieved by $\\widehat{\\boldsymbol{q}}_{w}$ \uff0c ", "page_idx": 17}, {"type": "text", "text": "Lemma C.1 (Closed-form $q_{w}$ ). Let $\\ensuremath{\\mu}_{0}$ be afixed distribution. Then, there exists $\\xi\\in\\mathbb{R}$ such that, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\mathcal{q}_{w}}{\\mathrm{d}\\mu_{0}}(\\pmb{x},y)=\\frac{\\operatorname*{max}\\{\\ell(\\pmb{w};\\pmb{x},y)-\\xi+2\\nu,0\\}}{2\\nu}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "When $\\mu_{0}$ is the empirical distribution $\\widehat{\\ensuremath{p}}_{0}(N)$ this result implies that there exists $\\hat{\\xi}\\in\\mathbb{R}$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\widehat{\\phi}_{w}^{(i)}=\\widehat{p}_{0}^{(i)}\\frac{\\operatorname*{max}\\{\\ell({\\pmb w};{\\pmb x}_{i},y_{i})-\\widehat{\\xi}+2\\nu,0\\}}{2\\nu}\\;\\;\\;f o r\\,a l l\\,i\\in[N].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The constants $\\xi$ and $\\hat{\\xi}$ can be interpreted as normalization that ensures $\\begin{array}{r}{\\int\\mathrm{d}\\boldsymbol{q}_{w}=\\int\\mathrm{d}\\widehat{\\boldsymbol{q}}_{w}=1}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "Proof. Recallthat the dual feasible set is given by $\\begin{array}{r}{\\mathcal{P}=\\mathcal{P}(p_{0})=\\{p\\ll p_{0}:\\int\\mathrm{d}p=1,p\\ge0\\}=}\\end{array}$ $\\begin{array}{r}{\\{p\\ll p_{0}:\\int\\frac{\\mathrm{d}p}{\\mathrm{d}p_{0}}\\mathrm{d}p_{0}=1,\\frac{\\mathrm{d}p}{\\mathrm{d}p_{0}}\\ge0\\}}\\end{array}$ $\\boldsymbol{\\mathscr{p}}\\mapsto\\boldsymbol{L}(\\boldsymbol{w},\\boldsymbol{p})$ ", "page_idx": 17}, {"type": "text", "text": "Consider the following optimization problem ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\substack{p\\in\\mathcal{P}(p_{0})}}L(\\pmb{w},p)\\equiv\\operatorname*{max}_{\\substack{p\\in\\mathcal{P}(p_{0})}}\\mathbb{E}_{(\\pmb{x},y)\\sim p}\\ell(\\pmb{w};\\pmb{x},y)-\\nu\\chi^{2}(p_{0},p).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By Fact 2.8, the first-order necessary and sufficient condition that corresponds to $q_{w}\\quad:=$ arg $\\operatorname*{max}_{\\substack{p\\in\\mathcal{P}(p_{0})}}L(\\pmb{w},p)$ is the following: for any $p\\in\\mathcal P(p_{0})$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n0\\geq\\int\\nabla_{p}L(\\pmb{w},q_{w})\\mathrm{d}(p-q_{w})=\\int\\nabla_{p}L(\\pmb{w},q_{w})\\Big(\\frac{\\mathrm{d}p}{\\mathrm{d}p_{0}}-\\frac{\\mathrm{d}q_{w}}{\\mathrm{d}p_{0}}\\Big)\\mathrm{d}p_{0},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we recall that both $\\nabla_{\\boldsymbol{p}}L(\\boldsymbol{w},\\boldsymbol{q}_{w})$ and Radon-Nikodym derivatives dp , d are real-valued measurable functions on $\\mathbb{R}^{d}\\times\\mathbb{R}$ . We will also write $\\ell=\\ell(\\pmb{w}^{*},\\cdot,\\cdot)$ for short. ", "page_idx": 18}, {"type": "text", "text": "We claim Equation (21) is satisfied if there exists $\\xi\\in\\mathbb{R}$ and a bounded measurable function $\\psi\\geq0$ suchthat ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{p}L({\\pmb w},\\llangle_{{\\pmb w}})({\\pmb x},y)=\\left\\{\\xi\\qquad\\qquad\\qquad\\mathrm{~if~}\\frac{\\mathrm{d}\\mathscr{q}_{{\\pmb w}}}{\\mathrm{d}\\mathscr{p}_{0}}>0\\right.}\\\\ {\\nabla_{p}L({\\pmb w},\\llangle_{{\\pmb w}})({\\pmb x},y)\\quad\\mathrm{~otherwise.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Indeed, for any $p\\in\\mathcal P(p_{0})$ \uff0c", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int\\nabla_{y}L(w,q_{w})\\Big(\\frac{\\mathrm{d}p}{\\mathrm{d}p_{0}}-\\frac{\\mathrm{d}q_{w}}{\\mathrm{d}p_{0}}\\Big)\\mathrm{d}p_{0}}\\\\ &{=\\displaystyle\\int_{\\frac{\\mathrm{d}q_{w}}{\\mathrm{d}p_{0}}>0}\\nabla_{p}L(w,q_{w})\\Big(\\frac{\\mathrm{d}p}{\\mathrm{d}p_{0}}-\\frac{\\mathrm{d}q_{w}}{\\mathrm{d}p_{0}}\\Big)\\mathrm{d}p_{0}+\\displaystyle\\int_{\\frac{\\mathrm{d}q_{w}}{\\mathrm{d}p_{0}}=0}\\nabla_{p}L(w,q_{w})\\frac{\\mathrm{d}p}{\\mathrm{d}p_{0}}\\mathrm{d}p_{0}}\\\\ &{=\\displaystyle\\int_{\\frac{\\mathrm{d}q_{w}}{\\mathrm{d}p_{0}}>0}\\xi\\Big(\\frac{\\mathrm{d}p}{\\mathrm{d}p_{0}}-\\frac{\\mathrm{d}q_{w}}{\\mathrm{d}p_{0}}\\Big)\\mathrm{d}p_{0}+\\displaystyle\\int_{\\frac{\\mathrm{d}q_{w}}{\\mathrm{d}p_{0}}=0}\\xi-\\xi\\Big)\\frac{\\mathrm{d}p}{\\mathrm{d}p_{0}}\\mathrm{d}p_{0}}\\\\ &{\\le\\displaystyle\\int_{\\frac{\\mathrm{d}q_{w}}{\\mathrm{d}p_{0}}>0}\\xi\\Big(\\frac{\\mathrm{d}p}{\\mathrm{d}p_{0}}-\\frac{\\mathrm{d}q_{w}}{\\mathrm{d}p_{0}}\\Big)\\mathrm{d}p_{0}+\\displaystyle\\int_{\\frac{\\mathrm{d}q_{w}}{\\mathrm{d}p_{0}}=0}\\xi\\frac{\\mathrm{d}p}{\\mathrm{d}p_{0}}\\mathrm{d}p_{0}}\\\\ &{=\\displaystyle\\int_{\\frac{\\mathrm{d}p}{\\mathrm{d}p_{0}}>0}\\xi\\frac{\\mathrm{d}p}{\\mathrm{d}p_{0}}\\mathrm{d}p_{0}+\\displaystyle\\int_{\\frac{\\mathrm{d}q_{w}}{\\mathrm{d}p_{0}}=0}\\xi\\frac{\\mathrm{d}p}{\\mathrm{d}p_{0}}\\mathrm{d}p_{0}+\\displaystyle\\int_{\\frac{\\mathrm{d}q_{w}}{\\mathrm{d}p_{0}}>0}\\xi\\Big(-\\frac{\\mathrm{d}q_{w}}{\\mathrm{d}p_{0}}\\Big)\\mathrm{d}p_{0}}\\\\ &{=\\displaystyle\\int\\xi\\frac{\\mathrm{d}p}{\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where (i) is because $\\begin{array}{r}{\\int_{\\frac{\\mathrm{d}q_{w}}{\\mathrm{d}p_{0}}>0}\\bigg(\\frac{\\mathrm{d}q_{w}}{\\mathrm{d}p_{0}}\\bigg)\\mathrm{d}p_{0}=\\int\\bigg(\\frac{\\mathrm{d}q_{w}}{\\mathrm{d}p_{0}}\\bigg)\\mathrm{d}p_{0}.}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "Observe from the definition of $L(\\pmb{w},\\llangle\\llangle\\pmb{w})$ that $\\begin{array}{r}{\\nabla_{p}L(\\boldsymbol{w},\\boldsymbol{q}_{w})(\\boldsymbol{x},\\boldsymbol{y})=\\ell(\\boldsymbol{w};\\boldsymbol{x},\\boldsymbol{y})\\!-\\!2\\nu(\\frac{\\mathrm{d}q_{w}}{\\mathrm{d}p_{0}}(\\boldsymbol{x},\\boldsymbol{y})\\!-\\!1).}\\end{array}$ Plugging this into Equation (22) and rearranging, we have, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\boldsymbol{p}^{*}}{\\mathrm{d}\\boldsymbol{p}_{0}}=\\left\\{\\frac{2\\nu+\\ell-\\xi}{2\\nu}\\right.\\quad\\mathrm{if~}\\frac{\\mathrm{d}\\boldsymbol{q}_{w}}{\\mathrm{d}\\boldsymbol{p}_{0}}>0\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For te case where $\\begin{array}{r}{\\frac{\\mathrm{d}\\boldsymbol{p}^{*}}{\\mathrm{d}\\boldsymbol{p}_{0}}>0,\\,\\frac{\\mathrm{d}\\boldsymbol{p}^{*}}{\\mathrm{d}\\boldsymbol{p}_{0}}=\\frac{2\\nu+\\ell-\\xi}{2\\nu}}\\end{array}$ $\\begin{array}{r}{\\frac{\\mathrm{d}q_{w}}{\\mathrm{d}p_{0}}>0}\\end{array}$ hecomnes $2\\nu+\\ell-\\xi>0$ On the other hand, if the above condition fails, it has to be the case that $\\begin{array}{r}{\\frac{\\mathrm{d}q_{w}}{\\mathrm{d}p_{0}}=0}\\end{array}$ . Combining, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}q_{w}}{\\mathrm{d}p_{0}}=\\bigg\\{\\frac{2\\nu+\\ell-\\xi}{2\\nu}\\quad\\mathrm{if~}2\\nu+\\ell-\\xi>0\\,=\\frac{\\operatorname*{max}\\{2\\nu+\\ell-\\xi,0\\}}{2\\nu}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Instead of using the expression in Lemma C.1, we will set $\\nu$ to be big enough to ensure that there is no maximum in the expression for $q_{w}$ . This is captured in Corollary C.2. ", "page_idx": 18}, {"type": "text", "text": "Corollary C.2 (Simpler Closed-form $q_{w}$ ).Fix $\\pmb{w}\\in\\mathbb{R}^{d}$ If $\\begin{array}{r}{\\mathbf{\\nabla}\\nu\\geq\\frac{1}{2}\\mathbb{E}_{p_{0}}\\ell(\\mathbf{\\boldsymbol{w}}),}\\end{array}$ then ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}q_{w}}{\\mathrm{d}p_{0}}(\\pmb{x},y)=1+\\frac{\\ell(\\pmb{w};\\pmb{x},y)-\\mathbb{E}_{p_{0}}\\ell(\\pmb{w})}{2\\nu}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Similarly,ij $\\begin{array}{r}{{\\bf\\nabla}^{\\prime}\\nu\\geq\\frac{1}{2}\\mathbb{E}_{\\widehat{\\mu}_{0}}\\ell({\\bf w}),}\\end{array}$ .then $q_{w}{^{(i)}}>0$ for all $i\\in[N],$ and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widehat{\\pmb{\\mathscr{q}}}_{w}^{(i)}=\\widehat{\\pmb{\\mathscr{p}}}_{0}^{(i)}+\\frac{\\ell({\\pmb{w}};{\\pmb{x}}_{i},y_{i})-\\mathbb{E}_{\\widehat{\\pmb{p}}_{0}}\\ell({\\pmb{w}})}{2\\nu}\\widehat{\\pmb{\\mathscr{p}}}_{0}^{(i)}\\quad f o r\\,a l l\\,i\\in[N].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Furthermore,if $\\mathbf{\\dot{\\sigma}}_{\\nu}\\geq\\mathbb{E}_{\\widehat{\\mu}_{0}}\\ell(\\pmb{w})$ , then, in particular for each coordinate $j\\in[N]$ wehave ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\widehat{q}_{w}^{(j)}\\geq\\widehat{p}_{0}^{(j)}/2\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Similarly, $\\mathbf{\\nabla}^{f}\\nu\\geq\\mathbb{E}_{p_{0}}\\ell(\\pmb{w})$ then for any non-negative function $g$ we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int g\\;\\mathrm{d}\\mathscr{p}_{w}\\geq\\frac{1}{2}\\int g\\;\\mathrm{d}\\mathscr{p}_{0}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Recall from Definition 1.2 that when ${\\pmb w}\\,=\\,w^{*}$ ,we define $p^{*}\\,=\\,q_{w^{*}}$ and $\\widehat{\\boldsymbol{p}}^{*}\\,=\\,\\widehat{\\boldsymbol{q}}_{w^{*}}$ .If $\\nu\\,\\geq$ $8\\beta^{2}\\sqrt{6B}\\sqrt{\\mathrm{OPT_{(2)}}+}\\epsilon/c_{1}$ as assumed in Theorem 3.1, then both conditions $\\nu\\,\\geq\\,\\mathbb{E}_{\\widehat{p}_{0}}\\ell({\\pmb w})$ and $\\nu\\geq\\mathbb{E}_{p_{0}}\\ell(\\pmb{w})$ hold. ", "page_idx": 19}, {"type": "text", "text": "ProofSetting $\\begin{array}{r}{\\nu\\ge\\frac{1}{2}\\mathbb{E}_{p_{0}}\\ell(\\pmb{w})}\\end{array}$ and $\\xi=\\mathbb{E}_{\\mu_{0}}\\ell({\\pmb w})$ in Lemma C.1 implies $\\ell(\\pmb{w};\\pmb{x},y)\\!-\\!\\mathbb{E}_{p_{0}}\\ell(\\pmb{w};\\pmb{x},y)+$ $2\\nu>0$ ,which, in turn, means ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\mathrm{d}{q}_{w}}{\\mathrm{d}{p}_{0}}(\\pmb{x},y)=\\frac{\\operatorname*{max}\\{\\ell(\\pmb{w};\\pmb{x},y)-\\mathbb{E}_{p_{0}}\\ell(\\pmb{w};\\pmb{x},y)+2\\nu,0\\}}{2\\nu}}\\\\ {\\displaystyle=\\frac{\\ell(\\pmb{w};\\pmb{x},y)-\\mathbb{E}_{p_{0}}\\ell(\\pmb{w};\\pmb{x},y)+2\\nu}{2\\nu}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The empirical version follows analogously. ", "page_idx": 19}, {"type": "text", "text": "To establishthe last claim, we show that $8\\beta^{2}\\sqrt{6B}\\sqrt{\\widehat{\\mathrm{OPT}}_{(2)}}/c_{1}\\ge\\mathbb{E}_{\\widehat{\\mu}_{0}}\\ell(\\pmb{w})$ . By Corollary C.9,it holds that ", "page_idx": 19}, {"type": "text", "text": "$\\begin{array}{r}{\\sqrt{\\widehat{\\mathrm{OPT}}_{(2)}}\\geq\\widehat{\\mathrm{OPT}}=\\mathbb{E}_{\\widehat{\\mu}^{*}}(\\sigma(w^{*}\\cdot x)-y)^{2}\\geq\\mathbb{E}_{\\widehat{\\mu}^{*}}(\\sigma(w^{*}\\cdot x)-y)^{2}-\\nu\\chi^{2}(\\widehat{\\boldsymbol{\\mu}}^{*},\\widehat{\\boldsymbol{\\mu}}_{0})=L(w^{*},\\widehat{\\boldsymbol{\\mu}}^{*}).}\\end{array}$ By definition of $\\widehat{\\boldsymbol{p}}^{*}$ , we have $L(\\pmb{w}^{*},\\hat{p}^{*})\\geq L(\\pmb{w}^{*},\\hat{p}_{0})=\\mathbb{E}_{\\hat{p}_{0}}(\\sigma(\\pmb{w}^{*}\\cdot\\pmb{x})-y)^{2}-\\nu\\chi^{2}(\\hat{p}_{0},\\hat{p}_{0})=$ $\\mathbb{E}_{\\widehat{\\mu}_{0}}(\\sigma(\\pmb{w}^{\\ast}\\cdot\\pmb{x})-y)^{2}$ .Combining,weobtain $8\\beta^{2}\\sqrt{6B}\\sqrt{\\widehat{\\mathrm{OPT}}_{(2)}}/c_{1}\\ge\\mathbb{E}_{\\widehat{\\rho}_{0}}\\ell({\\pmb w})8\\beta^{2}\\sqrt{6B}/c_{1}$ We concludebyobserving $8\\beta^{2}\\sqrt{6B}/c_{1}\\ge1$ \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Another consequence of Corollary C.2 is a closed form expression for the risk, as a varianceregularized loss, similar to [DN19; Lam13]. ", "page_idx": 19}, {"type": "text", "text": "Corollary C.3. Fix an arbitrary distribution $\\mu_{0}$ . Recall the risk defined in Definition 1.2, ", "page_idx": 19}, {"type": "equation", "text": "$$\nR(\\pmb{w};p_{0}):=\\operatorname*{max}_{p\\ll p_{0}}\\mathbb{E}_{(\\pmb{x},y)\\sim p}\\ell(\\pmb{w};\\pmb{x},y)-\\nu\\chi^{2}(p,p_{0}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "$\\begin{array}{r}{I\\!f\\nu\\geq\\frac{1}{2}\\mathbb E_{\\widehat{\\mu}_{0}}\\ell(\\pmb{w}),}\\end{array}$ it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\chi^{2}(\\phi_{\\pmb{w}},p_{0})=\\frac{\\mathbb{E}_{p_{0}}[\\ell^{2}(\\pmb{w})]-(\\mathbb{E}_{p_{0}}[\\ell(\\pmb{w})])^{2}}{4\\nu^{2}}}\\\\ &{\\quad R(\\pmb{w};p_{0})=\\mathbb{E}_{p_{0}}[\\ell(\\pmb{w})]+\\frac{\\mathbb{E}_{p_{0}}[\\ell^{2}(\\pmb{w})]-(\\mathbb{E}_{p_{0}}[\\ell(\\pmb{w})])^{2}}{4\\nu}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Both of these follow from Corollary C.2. To see the first equality holds, observe that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\chi^{2}(\\mathcal{q}_{w},p_{0})=\\mathbb{E}_{p_{0}}\\left(\\frac{\\mathrm{d}\\boldsymbol{q}_{w}}{\\mathrm{d}p_{0}}-1\\right)^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\mathbb{E}_{p_{0}}\\frac{(\\ell(\\boldsymbol{w};\\boldsymbol{x},\\boldsymbol{y})-\\mathbb{E}_{p_{0}}\\ell(\\boldsymbol{w};\\boldsymbol{x},\\boldsymbol{y}))^{2}}{4\\nu^{2}}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{\\mathbb{E}_{p_{0}}[\\ell^{2}(\\boldsymbol{w})]-\\left(\\mathbb{E}_{p_{0}}[\\ell(\\boldsymbol{w})]\\right)^{2}}{4\\nu^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The second equality follows by a similar substitution. ", "page_idx": 19}, {"type": "text", "text": "Setting $\\mathrm{d}q_{w}$ as per Corollary C.2, we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R(\\boldsymbol{w};\\boldsymbol{p}_{0})=\\mathbb{E}_{\\boldsymbol{p}_{0}}\\left[\\left(\\frac{\\mathrm{d}\\varphi_{\\boldsymbol{w}}}{\\mathrm{d}p_{0}}\\right)\\ell(\\boldsymbol{w})\\right]-\\nu\\chi^{2}(\\boldsymbol{\\varphi}_{\\boldsymbol{w}},\\boldsymbol{p}_{0})}\\\\ &{\\phantom{=}=\\mathbb{E}_{\\boldsymbol{p}_{0}}\\left[\\ell(\\boldsymbol{w})\\left(1+\\frac{\\ell(\\boldsymbol{w})-\\mathbb{E}_{\\boldsymbol{p}_{0}}\\ell(\\boldsymbol{w})}{2\\nu}\\right)\\right]-\\frac{\\mathbb{E}_{\\boldsymbol{p}_{0}}[\\ell^{2}(\\boldsymbol{w})]-\\left(\\mathbb{E}_{\\boldsymbol{p}_{0}}[\\ell(\\boldsymbol{w})]\\right)^{2}}{4\\nu}}\\\\ &{\\phantom{=}=\\mathbb{E}_{\\boldsymbol{p}_{0}}[\\ell(\\boldsymbol{w})]+\\frac{\\mathbb{E}_{\\boldsymbol{p}_{0}}[\\ell(\\boldsymbol{w})^{2}]-\\left(\\mathbb{E}_{\\boldsymbol{p}_{0}}[\\ell(\\boldsymbol{w})]\\right)^{2}}{2\\nu}-\\frac{\\mathbb{E}_{\\boldsymbol{p}_{0}}[\\ell^{2}(\\boldsymbol{w})]-\\left(\\mathbb{E}_{\\boldsymbol{p}_{0}}[\\ell(\\boldsymbol{w})]\\right)^{2}}{4\\nu}}\\\\ &{\\phantom{=}=\\mathbb{E}_{\\boldsymbol{p}_{0}}[\\ell(\\boldsymbol{w})]+\\frac{\\mathbb{E}_{\\boldsymbol{p}_{0}}[\\ell^{2}(\\boldsymbol{w})]-\\left(\\mathbb{E}_{\\boldsymbol{p}_{0}}[\\ell(\\boldsymbol{w})]\\right)^{2}}{4\\nu}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Finally, an important consequence of Lemma C.1 is that it is possible to effciently compute the risk ofagivenvector $\\pmb{w}$ withrespectto ${\\widehat{\\boldsymbol{p}}}_{0}$ . We use this to compare the risk of our final output with the risk that is achieved by the zero vector. ", "page_idx": 20}, {"type": "text", "text": "C.2   Concentration ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The expression we get in Corollary C.2 for $\\boldsymbol{p}^{*}$ in terms of $\\mu_{0}$ allows us to translate concentration properties of $\\mu_{0}$ to $\\boldsymbol{p}^{*}$ . We first state and prove a helper lemma, Lemma C.4, that shows $\\mathbb{E}_{\\mu_{0}}\\ell(\\pmb{w}^{*})\\approx$ $\\mathbb{E}_{\\widehat{\\mu}_{0}}\\ell(\\pmb{w}^{\\ast})$ . Note that this is for the reference distribution $\\mu_{0}$ , and not the target distribution $\\boldsymbol{p}^{*}$ ", "page_idx": 20}, {"type": "text", "text": "For ease of notation, we define $U:=2\\beta^{2}W^{2}(S^{2}\\!+\\!C_{M}^{2}B^{2}\\log^{2}(W B/\\epsilon))$ , which is the upper bound for the loss value in Equation (20) throughout this section. ", "page_idx": 20}, {"type": "text", "text": "Lemma C.4. Suppose $\\scriptstyle{\\mu_{0}}$ satisfies Assumption 2.3. Then for any fixed $w\\in\\mathcal{B}(W)$ and all $t>0$ it holdsthat ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\mathbb{E}_{p_{0}}\\ell(\\pmb{w})-\\mathbb{E}_{\\widehat{p}_{0}(N)}\\ell(\\pmb{w})|\\le t\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with probability at least 1 - 2exp(8(we(\\$+CBlog\\*(WBB/)) . In particular, the above inequalityholdsfor $\\pmb{w}^{*}$ ", "page_idx": 20}, {"type": "text", "text": "Proof. By Equation (20), $\\forall i\\in[N]$ $I],0\\leq(\\sigma(\\pmb{w}\\cdot\\pmb{x}_{i})-y_{i})^{2}\\leq U$ . Hoeffding's inequality (Fact B.2) now implies that, for all $t>0$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\Big[\\sum_{i=1}^{N}\\frac{1}{N}\\ell({\\pmb w}^{*};{\\pmb x}_{i},y_{i})-\\mathbb{E}_{p_{0}}\\ell({\\pmb w}^{*})\\geq t\\Big]\\leq2\\exp\\Big(\\frac{-t^{2}N}{2U^{2}}\\Big).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Rearranging and plugging the definition of $U$ , we get the lemma. ", "page_idx": 20}, {"type": "text", "text": "We now use Lemma C.4 show that bounded Lipschitz functions concentrate with respect to $\\boldsymbol{p}^{*}$ ", "page_idx": 20}, {"type": "text", "text": "Lemma C.5. Fix $\\zeta>0$ Let $h=h(z;\\pmb{x},y):\\mathcal{B}(\\zeta)\\times\\mathbb{R}^{d}\\times\\mathbb{R}\\to\\mathbb{R}$ be a measurable function with respect to $x,y$ that satisfies the condition that $|h(z;\\cdot;\\cdot)|\\leq b$ almost surely. Then, for $N=$ $\\begin{array}{r}{O_{B,S,\\beta}\\Big(\\frac{b^{2}}{t^{2}}\\Big(1\\,+\\,\\frac{W^{4}\\log^{4}(W/\\epsilon)}{\\nu^{2}}\\Big)\\log(1/\\delta)\\Big)}\\end{array}$ samples drawn fro thereferenedisribuion $\\ensuremath{\\mu}_{0}$ 0 construct $\\widehat{\\ensuremath{p_{0}}}(N)$ , for any fixed $z\\in\\mathcal{B}(\\zeta)$ , with probability at least 1 - 40, it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\mathbb{E}_{(\\pmb{x},y)\\sim\\widehat{p}^{*}}[h(\\pmb{z};\\pmb{x},y)]-\\mathbb{E}_{(\\pmb{x},y)\\sim p^{*}}[h(\\pmb{z};\\pmb{x},y)]|\\leq t.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Moreover, suppose $z\\mapsto h(z;x,y)$ is $a$ -Lipschitz. Then, for ", "page_idx": 20}, {"type": "equation", "text": "$$\nN=O_{B,S,\\beta}\\Big(\\frac{b^{2}}{t^{2}}\\Big(1+\\frac{W^{4}\\log^{4}(W/\\epsilon)}{\\nu^{2}}\\Big)(d\\log(\\zeta a/t)+\\log(1/\\delta))\\Big)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with probability at least $1-4\\delta$ , it holds that for all $z\\in\\mathcal{B}(\\zeta)$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\mathbb{E}_{(\\pmb{x},y)\\sim\\widehat{p}^{*}}[h(\\pmb{z};\\pmb{x},y)]-\\mathbb{E}_{(\\pmb{x},y)\\sim p^{*}}[h(\\pmb{z};\\pmb{x},y)]|\\leq t.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. We use Lemma C.1 to change the distribution with respect to which we are taking the expectation, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\widehat{\\boldsymbol{\\pi}}^{*}}[h(z)]=\\mathbb{E}_{\\widehat{\\boldsymbol{\\pi}}_{0}}\\Big[h(z;\\boldsymbol{x},\\boldsymbol{y})\\frac{\\ell(\\boldsymbol{w}^{*};\\boldsymbol{x},\\boldsymbol{y})-\\mathbb{E}_{\\widehat{\\boldsymbol{\\pi}}_{0}}\\ell(\\boldsymbol{w}^{*};\\boldsymbol{x},\\boldsymbol{y})+2\\nu}{2\\nu}\\Big].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma C.4 now implies that with probability $1-2\\exp(-2N t^{2}/(b U/2\\nu)^{2})$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\widehat{\\pi}^{*}}[h(z)]=\\mathbb{E}_{\\widehat{\\pi}_{0}}\\Big[h(z;\\pmb{x},y)\\frac{\\ell(\\pmb{w}^{*};\\pmb{x},y)-\\mathbb{E}_{\\pmb{\\eta}_{0}}\\ell(\\pmb{w}^{*};\\pmb{x},y)+2\\nu}{2\\nu}\\Big]\\pmb{\\pmb{\\pmb{\\Psi}}}_{4}^{t}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We now show that the expectation on the right hand side concentrates. To this end, we will use Hoeffding's inequality (Fact B.2). To apply this, we will need a bound on the quantity in the expectation. We bound this via an application of Equation (20) and the fact that $|h|\\leq b$ to get, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left|h(z;\\pmb{x},y)\\frac{\\ell(\\pmb{w}^{*})-\\mathbb{E}_{p_{0}}\\ell(\\pmb{w}^{*})+2\\nu}{2\\nu}\\right|\\leq b\\left(1+\\frac{U}{2\\nu}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This means, with probability at least $1-2\\exp(-2t^{2}N/(b^{2}(1+U/2\\nu)^{2})),$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Big|\\mathbb{E}_{p_{0}}\\Big[h(z)\\frac{\\ell(w^{*})-\\mathbb{E}_{p_{0}}\\ell(w^{*})+2\\nu}{2\\nu}\\Big]-E_{\\widehat{p}_{0}}\\Big[h(z)\\frac{\\ell(w^{*})-\\mathbb{E}_{p_{0}}\\ell(w^{*})+2\\nu}{2\\nu}\\Big]\\Big|\\le\\frac{t}{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $w\\mapsto h(w)$ is $a$ -Lipschitz, a standard net argument over $\\exp(O(d\\log(\\zeta a/t)))$ vectors yields: with probability at least $\\dot{1}-2\\,\\mathrm{exp}(O(d\\log(\\zeta a/t)-t^{2}N/(b^{2}(1+U/2\\nu)^{2})))$ , it holds that for all $z\\in\\bar{\\mathcal{B}}(\\zeta)$ \uff0c ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\bigg|\\mathbb{E}_{p_{0}}\\bigg[h(z)\\frac{\\ell(w^{*})-\\mathbb{E}_{p_{0}}\\ell(w^{*})+2\\nu}{2\\nu}\\bigg]-E_{\\widehat{p}_{0}}\\bigg[h(z)\\frac{\\ell(w^{*})-\\mathbb{E}_{p_{0}}\\ell(w^{*})+2\\nu}{2\\nu}\\bigg]\\bigg|\\le t.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Putting things together, we see that if we choose ", "page_idx": 21}, {"type": "equation", "text": "$$\nN=\\Omega\\Big(\\frac{b^{2}}{t^{2}}\\Big(1+\\frac{U^{2}}{\\nu^{2}}\\Big)(d\\log(\\zeta a/t)+\\log(1/\\delta))\\Big),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "with probability at least $1-4\\delta$ , for all $z\\in{\\mathcal{B}}(\\zeta)$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n|\\mathbb{E}_{\\widehat{p}^{\\ast}}[h(z)]-\\mathbb{E}_{p^{\\ast}}[h(z)]|\\leq t.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "C.3  Sharpness and Optimal Loss Value ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Finally, as a consequence of Lemma C.5, we can derive that $\\widehat{\\boldsymbol{p}}^{*}$ satisfies sharpness, OPT  OPT and $\\widehat{\\mathrm{OPT}}_{(2)}\\approx\\mathrm{OPT}_{(2)}$ ", "page_idx": 21}, {"type": "text", "text": "Lemma C.6 (Shaprness for ${\\widehat{\\boldsymbol{p}}}^{*}$ ). Suppose Assumptions 2.1 to 2.3 are satisfied, then for large enough $N$ ", "page_idx": 21}, {"type": "equation", "text": "$$\nN=\\widetilde{O}_{B,S,\\beta,\\alpha,\\gamma,\\lambda}\\Big(\\frac{W^{4}}{\\epsilon^{2}}\\Big(1+\\frac{W^{4}\\log^{4}(1/\\epsilon)}{\\nu^{2}}\\Big)(d+\\log(1/\\delta))\\Big),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "with probability at least $1-4\\delta$ for all $\\pmb{w}\\in\\mathcal{B}(2||\\pmb{w}^{*}||)$ with $\\|\\pmb{w}-\\pmb{w}^{*}\\|\\geq\\sqrt{\\epsilon}$ and $\\pmb{u}\\in\\mathcal{B}(1)$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathbf{x}\\sim\\widehat{p}_{\\mathbf{x}}^{*}}[(\\sigma(\\pmb{w}\\cdot\\pmb{x})-\\sigma(\\pmb{w}^{*}\\cdot\\pmb{x}))(\\pmb{w}\\cdot\\pmb{x}-\\pmb{w}^{*}\\cdot\\pmb{x})]\\geq(c_{0}/2)\\|\\pmb{w}-\\pmb{w}^{*}\\|_{2}^{2}}\\\\ {\\mathbb{E}_{\\mathbf{x}\\sim\\widehat{p}_{\\mathbf{x}}^{*}}[(\\pmb{x}\\cdot\\pmb{u})^{\\tau}]\\leq6B\\quad f o r\\,\\tau=2,4.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Fact 2.4 shows that $\\boldsymbol{p}^{*}$ the conditions above (with different constants). We need to translate theseto $\\widehat{\\boldsymbol{p}}^{*}$ . For each of the inequalities above, we will do this via an application of Lemma C.5. ", "page_idx": 21}, {"type": "text", "text": "Proof of Equation (25): Set $h$ in Lemma C.5 to be $h(\\pmb{w};\\pmb{x},y):=(\\sigma(\\pmb{w}\\!\\cdot\\!\\pmb{x})\\!-\\!\\sigma(\\pmb{w}^{\\ast}\\!\\cdot\\!\\pmb{x}))(\\pmb{w}\\!\\cdot\\!\\pmb{x}\\!-\\!\\pmb{w}^{\\ast}\\!\\cdot\\!\\pmb{x})$ We proceed to set the constants $a$ and $b$ that used in Lemma C.5. Equation (18) implies that for all $(\\pmb{x},y)$ in the support of ${\\widehat{\\boldsymbol{p}}}_{0}$ \uff0c $|h({\\pmb w};{\\pmb x},y)|\\,\\leq\\,4\\beta W^{2}S^{2}\\,=:\\,b$ . Also, ${\\pmb w}\\,\\mapsto\\,h({\\pmb w})$ is $a:=$ $2W S^{2}(\\beta+1)\\sqrt{d}$ -Lipschitz as a consequence of Equations (18) and (19). ", "page_idx": 21}, {"type": "text", "text": "Lemma C.5 nowgives us that for $\\begin{array}{r}{N=\\widetilde{O}_{B,S,\\beta}\\Big(\\frac{W^{4}}{t^{2}}\\Big(1+\\frac{W^{4}\\log^{4}(1/\\epsilon)}{\\nu^{2}}\\Big)\\big(d+\\log(1/\\delta))\\Big)}\\end{array}$ with probability at least $1-4\\delta$ , for all $\\pmb{w}\\in\\mathcal{B}(2||\\pmb{w}^{*}||)$ \uff0c ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{x}\\sim\\widehat{p}_{x}^{*}}[(\\sigma(\\mathbf{w}\\cdot\\mathbf{x})-\\sigma(\\mathbf{w}^{*}\\cdot\\mathbf{x}))(\\mathbf{w}\\cdot\\mathbf{x}-\\mathbf{w}^{*}\\cdot\\mathbf{x})]\\geq c_{0}\\|\\mathbf{w}-\\mathbf{w}^{*}\\|_{2}^{2}-t.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using the fact that $\\lVert\\pmb{w}-\\pmb{w}^{*}\\rVert_{2}\\geq\\sqrt{\\epsilon},$ weset $t=c_{0}\\epsilon/2$ , giving us the sample complexity ", "page_idx": 22}, {"type": "equation", "text": "$$\nN=\\widetilde{O}_{B,S,\\beta,\\alpha,\\gamma,\\lambda}\\Big(\\frac{W^{4}}{\\epsilon^{2}}\\Big(1+\\frac{W^{4}\\log^{4}(1/\\epsilon)}{\\nu^{2}}\\Big)(d+\\log(1/\\delta))\\Big).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof of Equation (26): This follows analogously to the proof above. Set $h(\\boldsymbol{\\mathbf{u}};\\boldsymbol{\\mathbf{x}},y)=(\\boldsymbol{\\mathbf{x}}\\cdot\\boldsymbol{\\mathbf{u}})^{\\tau}$ in Lemma C.5 for $\\tau=2,4$ and we proceed to calculate constants $a,b$ .By Equations (18) and (19), it holds that $h(\\pmb{u})\\leq S^{4}=:b$ and ${\\pmb u}\\mapsto h({\\pmb u})$ is $a:=4S^{4}{\\sqrt{d}}$ -Lipschitz. Setting $t=B$ , by Lemma C.5, for $\\begin{array}{r}{N=\\widetilde{O}_{B,S,\\beta}\\Big(\\Big(1+\\frac{W^{4}\\log^{4}(W/\\epsilon)}{\\nu^{2}}\\Big)\\big(d+\\log(1/\\delta))\\Big)}\\end{array}$ the conclusinfolowNote that his is dominated by Equation (27). \u53e3 ", "page_idx": 22}, {"type": "text", "text": "We now show that $\\mathrm{OPT}\\approx\\widehat{\\mathrm{OPT}}$ ", "page_idx": 22}, {"type": "text", "text": "Lemma C.7. Suppose Assumptions 2.1 to 2.3 are satisfied and the sample size $N$ is largeenough and (W\\*log\\*(/0 (1 + Walog\\*(/2) log(1/86). Then for any fxed w E SB(W) and all $t>0$ itholdsthat ", "page_idx": 22}, {"type": "equation", "text": "$$\n|\\mathbb{E}_{p^{\\ast}}\\ell(\\pmb{w};\\pmb{x},y)-\\mathbb{E}_{\\widehat{p}^{\\ast}}\\ell(\\pmb{w};\\pmb{x},y)|\\leq t\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "with probability at least 1- 48. In particular, the above inequality holds for $\\pmb{w}^{*}$ ,i.e., $|\\mathrm{OPT-\\widehat{OPT}|\\leq}$ $t$ ", "page_idx": 22}, {"type": "text", "text": "Proof. Set $\\begin{array}{r}{\\begin{array}{l l l}{b}&{:=}&{\\beta^{2}W^{2}(2S^{2}\\,+\\,2C_{M}^{2}B^{2}\\log^{2}(W B/\\epsilon)}&{\\geq}&{\\|\\ell(\\pmb{w})\\|_{2}}\\end{array}}\\end{array}$ in Lemma C.5, where the inequality is a consequence of Lemma B.3. Then, with $\\begin{array}{r}{N\\ =\\ \\widetilde O_{B,S,\\beta}\\Big(\\frac{W^{4}\\log^{4}(1/\\epsilon)}{t^{2}}\\Big(1\\ +}\\end{array}$ $\\frac{W^{4}\\log^{4}(1/\\epsilon)}{\\nu^{2}}\\biggr)\\log(1/\\delta)\\biggr)$ $1-4\\delta$ $|\\mathbb{E}_{p^{\\ast}}\\ell(\\pmb{w};\\pmb{x},y)-\\mathbb{E}_{\\widehat{p}^{\\ast}}\\ell(\\pmb{w};\\pmb{x},y)|\\leq t$ ", "page_idx": 22}, {"type": "text", "text": "Finall, we show $\\mathrm{OPT}_{(2)}\\approx\\widehat{\\mathrm{OPT}}_{(2)}$ ", "page_idx": 22}, {"type": "text", "text": "Lemma C.8. Suppose Assumptions 2.1 to 2.3 are satisfied and the sample size $N$ is large enough and $\\begin{array}{r}{N=\\widetilde O_{B,S,\\beta}\\Big(\\frac{W^{8}\\log^{8}(1/\\epsilon)}{t^{2}}\\Big(1+\\frac{W^{4}\\log^{4}(1/\\epsilon)}{\\nu^{2}}\\Big)\\log(1/\\delta)\\Big)}\\end{array}$ Then for any fired $w\\in\\mathcal{B}(W)$ and all $t>0$ it holds that ", "page_idx": 22}, {"type": "equation", "text": "$$\n|\\mathbb{E}_{p^{*}}\\ell^{2}(\\pmb{w};\\pmb{x},y)-\\mathbb{E}_{\\widehat{p}^{*}}\\ell^{2}(\\pmb{w};\\pmb{x},y)|\\leq t\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "with probability at least $1\\:-\\:4\\delta$ .In particular, the above inequality holds for $\\pmb{w}^{*}$ \uff0ci.e., $|\\mathrm{OPT}_{(2)}-\\widehat{\\mathrm{OPT}}_{(2)}|\\leq t.$ ", "page_idx": 22}, {"type": "text", "text": "Proof Analogous to the previous proof, observe that $\\begin{array}{r l r}{\\|\\ell^{2}({\\pmb w})\\|_{2}}&{{}\\leq}&{8\\beta^{4}W^{4}(S^{4}{\\pmb\\mathscr{\\tau}}+}\\end{array}$ $C_{M}^{4}B^{4}\\log^{4}(W B/\\epsilon)\\quad=:\\quad b.$ By Lemma C.5, with $\\begin{array}{r l r}{N}&{{}=}&{\\widetilde{O}_{B,S,\\beta}\\bigg(\\frac{W^{8}\\log^{8}(1/\\epsilon)}{t^{2}}\\Big(1~+}\\end{array}$ W\\*log\\*(4/9 1o(1/6), with probabity 1- 46,itholds tha (IB=-(w; 2,9)  Ee- G(w; 2,)\u2264 $t$ \u53e3 ", "page_idx": 22}, {"type": "text", "text": "We capture properties of OPT and $\\mathrm{OPT}_{(2)}$ in the following corollary. ", "page_idx": 22}, {"type": "text", "text": "Corollary C.9 (Properties of OPT, $\\mathrm{OPT}_{(2)}$ ).Suppose Assumptions 2.1 to 2.3 are satisfied and the sample size $N$ is large enough and $\\begin{array}{r}{N=\\stackrel{\\cdot\\cdot}{\\widetilde O}_{B,S,\\beta}\\Big(\\frac{W^{8}\\log^{8}(1/\\epsilon)}{t^{2}}\\Big(1+\\frac{W^{4}\\log^{4}(1/\\epsilon)}{\\nu^{2}}\\Big)\\log(1/\\delta)\\Big)}\\end{array}$ for all $t>0$ the following hold: ", "page_idx": 22}, {"type": "text", "text": "1. With probability 1 - 48, $|\\mathrm{OPT}_{(2)}-\\widehat{\\mathrm{OPT}}_{(2)}|\\leq t.$   \n2. With probability 1 - 40, $|\\mathrm{OPT}-\\widehat{\\mathrm{OPT}}|\\leq t$ ", "page_idx": 22}, {"type": "text", "text": "Proof. The first two items follow immediately from Lemma C.8 and Lemma C.7. The third item is a consequence of Cauchy-Schwarz. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "D Gap Upper Bound ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "To prove Lemma 3.3, we need to construct an upper bound on the gap $\\mathrm{Gap}(\\pmb{w}_{i},\\widehat{\\pmb{\\mathscr{n}}}_{i})=L(\\pmb{w}_{i},\\widehat{\\pmb{\\mathscr{n}}}^{*})-$ $L(w^{*},\\widehat{{p}_{i}})$ . To achieve this, we establish an upper bound on $L(\\pmb{w}_{i},\\widehat{\\pmb{\\mathscr{p}}}^{*})$ , which motivates the update rule for ${\\widehat{\\boldsymbol{p}_{i}}}$ . We also establish a lower bound on $L(w^{*},\\widehat{{p}}_{i})$ , which guides the update rule for $\\pmb{w}_{i}$ and the construction of $\\scriptstyle\\pmb{g}_{i}$ . Note that the construction of the lower bound is more challenging here, due to the nonconvexity of the square loss. This is where most of the (non-standard) technical work happens. To simplify the notation, we use $\\phi(\\widehat{\\boldsymbol{p}}):=\\chi^{2}(\\widehat{\\boldsymbol{p}},\\widehat{\\boldsymbol{p}}_{0})$ throughout this section. ", "page_idx": 23}, {"type": "text", "text": "Upperbound on $L(\\pmb{w}_{i},\\widehat{\\pmb{\\mathscr{p}}}^{*})$ .We begin the analysis with the construction of the upper bound, which is used for defining the dual updates. Most of this construction follows a similar argument as used in other primal-dual methods such as $[\\mathrm{Dia}+22\\mathrm{c}$ \uff1bSWD21]. ", "page_idx": 23}, {"type": "text", "text": "Lemma D.1 (Upper Bound on $a_{i}L(\\pmb{w}_{i},\\widehat{\\pmb{\\mathscr{p}}}^{*})\\}$ 0.Let ${\\widehat{\\,p\\,}}_{i}$ evolve as outlined in Line 7. Then, for all $i\\geq1$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a_{i}L(w_{i},\\widehat{p}^{*})\\leq a_{i}L(w_{i},\\widehat{p}_{i})+(\\nu_{0}+\\nu A_{i-1})D_{\\phi}(\\widehat{p}^{*},\\widehat{p}_{i-1})-(\\nu_{0}+\\nu A_{i})D_{\\phi}(\\widehat{p}^{*},\\widehat{p}_{i})}\\\\ &{\\qquad\\qquad\\qquad-\\left(\\nu_{0}+\\nu A_{i-1}\\right)D_{\\phi}(\\widehat{p}_{i},\\widehat{p}_{i-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Recall that $\\phi(\\widehat{\\boldsymbol{p}}):=\\chi^{2}(\\widehat{\\boldsymbol{p}},\\widehat{\\boldsymbol{p}}_{0})$ . Observe that $L(\\pmb{w}_{i},\\widehat{\\pmb{\\mathscr{p}}}^{*})$ as a function of $\\widehat{\\boldsymbol{p}}^{*}$ is linear minus the nonlinearity $\\nu\\phi$ We could directly maximize this function and define ${\\widehat{\\,p\\,}}_{i}$ correspondingly(which would lead to a valid upper bound); however, such an approach appears insufficient for obtaining our results. Instead, adding and subtracting $(\\nu_{0}+\\nu A_{i-1})\\bar{D_{\\phi}}(\\widehat{\\mu}^{*},\\widehat{\\mu_{i-1}})$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a_{i}L(w_{i},\\widehat{p}^{*})=a_{i}L(w_{i},\\widehat{p}^{*})-(\\nu_{0}+\\nu A_{i-1})D_{\\phi}(\\widehat{p}^{*},\\widehat{p}_{i-1})+(\\nu_{0}+\\nu A_{i-1})D_{\\phi}(\\widehat{p}^{*},\\widehat{p}_{i-1})}\\\\ &{\\qquad\\qquad=h(\\widehat{p}^{*})+(\\nu_{0}+\\nu A_{i-1})D_{\\phi}(\\widehat{p}^{*},\\widehat{p}_{i-1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we define, for notational convenience: ", "page_idx": 23}, {"type": "equation", "text": "$$\nh(\\widehat{p}):=a_{i}L({\\pmb w}_{i},\\widehat{p})-(\\nu_{0}+\\nu A_{i-1})D_{\\phi}(\\widehat{p},\\widehat{p}_{i-1}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Observe that by the definition of $\\widehat{p}_{i},\\,h(\\widehat{p})$ is maximized by ${\\widehat{\\,p\\,}}_{i}$ . Hence, using the definition of a Bregman divergence, we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h(\\widehat{\\boldsymbol{p}}^{*})=h(\\widehat{\\boldsymbol{p}}_{i})+\\langle\\nabla h(\\widehat{\\boldsymbol{p}}_{i}),\\widehat{\\boldsymbol{p}}^{*}-\\widehat{\\boldsymbol{p}}_{i}\\rangle+D_{h}(\\widehat{\\boldsymbol{p}}^{*},\\widehat{\\boldsymbol{p}}_{i})}\\\\ &{\\qquad\\leq h(\\widehat{\\boldsymbol{p}}_{i})-(\\nu_{0}+\\nu A_{i})D_{\\phi}(\\widehat{\\boldsymbol{p}}^{*},\\widehat{\\boldsymbol{p}}_{i}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where in the inequality we used that $\\langle\\nabla h(\\widehat{\\mu}_{i}),\\widehat{\\mu}^{*}-\\widehat{\\mu}_{i}\\rangle\\leq0$ (as ${\\widehat{\\,p\\,}}_{i}$ maximizes $h$ ) and $D_{h}(\\widehat{p}^{*},\\widehat{p}_{i})=$ $-(\\nu_{0}+\\nu A_{i})D_{\\phi}^{\\;\\;\\subset}({\\widehat{\\mu}}^{\\ast},{\\widehat{\\mu}}_{i})$ (as $h(\\widehat{\\boldsymbol{\\mu}})$ canbe expressed as $-(\\nu_{0}+\\nu A_{i})\\phi(\\widehat{\\boldsymbol{p}})$ plus terms that are either linear in $\\widehat{\\boldsymbol{\\mu}}$ or independent of it. See Fact 2.7). Combining with Equation (28) and the definition of $h$ and simplifying, the claimed bound follows. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "An important feature of Lemma D.1 is that the first two Bregman divergence terms usefully telescope, while the last one is negative and can be used in controlling the error terms arising from the algorithmic choices. ", "page_idx": 23}, {"type": "text", "text": "Lower bound on $L(w^{*},\\widehat{{p}}_{i})$ . The more technical part of our analysis concerns the construction of a lower bound on $L(w^{*},\\widehat{{p}}_{i})$ , which leads to update rule for $\\pmb{w}^{*}$ . In standard, Chambolle-Pock-style primal-dual algorithms [ACW22; CP11; SWD21], where the coupling $L(w,\\widehat{\\boldsymbol{n}})$ between the primal and the dual is bilinear, the lower bound would be constructed using an analogue of the upper bound, with a small difference to correct for the fact that $\\boldsymbol{w}_{i}$ is updated before ${\\widehat{\\,p\\,}}_{i}$ and so one cannot use information about ${\\widehat{\\,p\\,}}_{i}$ in the $\\boldsymbol{w}_{i}$ update. This is done using an extrapolation idea, which replaces ${\\widehat{\\,p\\,}}_{i}$ with an extrapolated value from prior two iterations and controls for the introduced error. ", "page_idx": 23}, {"type": "text", "text": "In our case, however, the coupling is not only nonlinear, but also nonconvex because $\\ell({\\pmb w};{\\pmb x},y)=$ $(\\sigma({\\pmb w}\\cdot{\\pmb x})-y)^{2}$ is nonconvex. Nonlinearity is an issue because if we were to follow an analogue of the construction from Lemma D.1, we would need to assume that we can efficiently minimize over w the sum of $L(w,\\widehat{\\boldsymbol{\\mu}})$ and a convex function (e.g., a quadratic), which translates into proximal point updates for the $L_{2}^{2}$ loss for which efficient computation is generally unclear. Nonlinearity alone (but assuming convexity) has been handled in the very recent prior work [MDH24], where this issue is addressed using convexity of the nonlinear function to bound it below by its linear approximation around $\\pmb{w}_{i}$ . Unfortunately, as mentioned before, this approach cannot apply here as we do not have convexity. Instead, we use a rather intricate argument that relies on monotonicity and Lipschitzness properties of the activation $\\sigma$ and structural properties of the problem which only hold with respect to the target distribution $\\widehat{\\boldsymbol{p}}^{*}$ (and the empirical target distribution $\\boldsymbol{p}^{*}$ , due to our results from Lemma 2.5. Handling these issues related to nonconvexity of the loss in the construction of the upper bound is precisely what forces us to choose chi-square as the measure of divergence between distributions; see Lemma D.3 and the discussion therein. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "Proposition D.2. Consider the sequence $\\{w_{i}\\}_{i}$ evolving as per Line 6. Under the setting in which Lemma 2.5 holds, we have for all $i\\geq1$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a_{i}L(w^{*},\\widehat{p}_{i})\\geq L(w_{i},\\widehat{p}_{i})-a_{i}E_{i}-\\big(\\nu_{0}+\\nu A_{i-2}\\big)D_{\\phi}\\big(\\widehat{p}_{i-1},\\widehat{p}_{i-2}\\big)}\\\\ &{\\qquad\\qquad+\\cfrac{1+0.5c_{1}A_{i-1}}{2}\\|w^{*}-w_{i}\\|_{2}^{2}-\\cfrac{1+0.5c_{1}A_{i-1}}{2}\\|w^{*}-w_{i-1}\\|_{2}^{2}}\\\\ &{\\qquad\\qquad+\\cfrac{1+0.5c_{1}A_{i-1}}{4}\\|w_{i}-w_{i-1}\\|_{2}^{2}-\\cfrac{1+0.5c_{1}A_{i-2}}{4}\\|w_{i-1}-w_{i-2}\\|_{2}^{2}}\\\\ &{\\qquad\\qquad+a_{i}\\big\\langle\\mathbb{E}_{\\widehat{p}_{i}}[v(w_{i};x,y)]-\\mathbb{E}_{\\widehat{p}_{i-1}}[v(w_{i-1};x,y)],w^{*}-w_{i}\\big\\rangle}\\\\ &{\\qquad\\qquad-a_{i-1}\\big\\langle\\mathbb{E}_{\\widehat{p}_{i-1}}[v(w_{i-1};x,y)]-\\mathbb{E}_{\\widehat{p}_{i-2}}[v(w_{i-2};x,y)],w^{*}-w_{i-1}\\big\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "whereis $E_{i}$ is defined by Equation (37) ", "page_idx": 24}, {"type": "text", "text": "Proof. From the definition of $L(w^{*},\\widehat{{p}}_{i})$ , we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\nL(\\pmb{w}^{\\ast},\\widehat{p}_{i})=\\mathbb{E}_{\\widehat{p}_{i}}[(\\sigma(\\pmb{w}^{\\ast}\\cdot\\pmb{x})-y)^{2}]-\\nu D(\\widehat{p}_{i},\\widehat{p}_{0}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Writing $(\\sigma({\\pmb w}^{*}\\cdot{\\pmb x})-y)^{2}=((\\sigma({\\pmb w}^{*}\\cdot{\\pmb x})-\\sigma({\\pmb w}_{i}\\cdot{\\pmb x}))+(\\sigma({\\pmb w}_{i}\\cdot{\\pmb x})-y))^{2}\\colon$ ind expanding the square, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L(w^{*},\\widehat{p}_{i})=\\mathbb{E}_{\\widehat{p}_{i}}[(\\sigma(w_{i}\\cdot x)-y)^{2}]-\\nu D(\\widehat{p}_{i},\\widehat{p}_{0})+\\mathbb{E}_{\\widehat{p}_{i}}[(\\sigma(w^{*}\\cdot x)-\\sigma(w_{i}\\cdot x))^{2}]}\\\\ &{\\qquad\\qquad+\\mathbb{E}_{\\widehat{p}_{i}}[2(\\sigma(w_{i}\\cdot x)-y)(\\sigma(w^{*}\\cdot x)-\\sigma(w_{i}\\cdot x))]}\\\\ &{\\qquad\\qquad=L(w_{i},\\widehat{p}_{i})+S_{i},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where for notational convenience we define ", "page_idx": 24}, {"type": "equation", "text": "$$\nS_{i}:=\\mathbb{E}_{\\widehat{p}_{i}}[(\\sigma(\\pmb{w}^{*}\\cdot\\pmb{x})-\\sigma(\\pmb{w}_{i}\\cdot\\pmb{x}))^{2}]+\\mathbb{E}_{\\widehat{p}_{i}}[2(\\sigma(\\pmb{w}_{i}\\cdot\\pmb{x})-y)(\\sigma(\\pmb{w}^{*}\\cdot\\pmb{x})-\\sigma(\\pmb{w}_{i}\\cdot\\pmb{x}))].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Observe that $L(w_{i},\\widehat{\\boldsymbol{p}_{i}})$ on the right-hand side also appears in the upper bound on $L(\\pmb{w}_{i},\\widehat{\\pmb{\\mathscr{p}}}^{*})$ in Lemma D.1 and so it will get cancelled out when $L(w^{*},\\widehat{{p}_{i}})$ is subtracted from $L(\\pmb{w}_{i},\\widehat{\\pmb{\\mathscr{p}}}^{*})$ in the gap computation. Thus, we only need to focus on bounding $S_{i}$ . This requires a rather technical argument, which we defer to Lemma D.3 below. Instead, we call on Lemma D.3 to state that ", "page_idx": 24}, {"type": "equation", "text": "$$\nS_{i}\\geq\\mathbb{E}_{\\widehat{\\mu}_{i}}[\\langle v(\\pmb{w}_{i};\\pmb{x},y),\\pmb{w}^{*}-\\pmb{w}_{i}\\rangle]-E_{i}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and carry out the rest of the proof under this assumption (which is proved in Lemma D.3). ", "page_idx": 24}, {"type": "text", "text": "At this point, we have obtained a \u201clinearization\u201d that was needed to continue by mimicking the construction of the upper bound. However, $\\pmb{v}(\\pmb{w}_{i};\\pmb{x},y)$ depends on $\\pmb{w}_{i}$ , and so trying to define $\\boldsymbol{w}_{i}$ based on this quantity would lead to an implicitly defined update, which is generally not efficiently computable. Instead, here we use the idea of extrapolation: instead of defining a step w.r.t. $\\pmb{w}_{i}$ we replace $\\mathbb{E}_{\\widehat{\\mu}_{i}}[\\pmb{v}(\\pmb{w}_{i};\\pmb{x},y)]$ by an \u201cextrapolated gradient\u201d defined by (cf. Line 5 in Algorithm 1): ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{g_{i-1}=\\mathbb{E}_{\\widehat{p}_{i-1}}[v(w_{i-1};\\pmb{x},y)]+\\frac{a_{i-1}}{a_{i}}(\\mathbb{E}_{\\widehat{p}_{i-1}}[v(w_{i-1};\\pmb{x},y)]-\\mathbb{E}_{\\widehat{p}_{i-2}}[v(w_{i-2};\\pmb{x},y)]).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Combining with the bound on $S_{i}$ from Equation (31) and simplifying, we now have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a_{i}S_{i}\\geq a_{i}\\langle\\pmb{g}_{i-1},\\pmb{w}^{*}-\\pmb{w}_{i}\\rangle-a_{i}E_{i}}\\\\ &{\\qquad\\qquad+\\;a_{i}\\langle\\mathbb{E}_{\\widehat{\\pmb{\\mathscr{p}}_{i}}}[\\pmb{v}(\\pmb{w}_{i};\\pmb{x},\\pmb{y})-\\pmb{g}_{i-1}],\\pmb{w}^{*}-\\pmb{w}_{i}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Let $\\begin{array}{r}{\\psi(\\pmb{w})=a_{i}\\langle\\pmb{g}_{i-1},\\pmb{w}\\rangle+\\frac{1+0.5c_{1}A_{i-1}}{2}\\|\\pmb{w}-\\pmb{w}_{i-1}\\|_{2}^{2}}\\end{array}$ and observe that (by Line 6 in Algorithm 1) $\\begin{array}{r}{{\\pmb w}_{i}=\\arg\\operatorname*{min}_{{\\pmb w}\\in{\\mathcal B}(W)}\\psi({\\pmb w})}\\end{array}$ . Then, by a similar argument as in the proof of Lemma D.1, since $\\psi$ is minimized by $\\pmb{w}_{i}$ and is a quadratic function in $\\pmb{w}_{i}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a_{i}\\langle g_{i-1},w^{*}-w_{i}\\rangle\\geq\\displaystyle\\frac{1+0.5c_{1}A_{i-1}}{2}\\|w^{*}-w_{i}\\|_{2}^{2}-\\frac{1+0.5c_{1}A_{i-1}}{2}\\|w^{*}-w_{i-1}\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad+\\displaystyle\\frac{1+0.5c_{1}A_{i-1}}{2}\\|w_{i}-w_{i-1}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "On the other hand, by the definition of $\\scriptstyle\\mathbf{\\textit{g}}_{i}$ ,wehave ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\ \\ a_{i}\\langle\\mathbb{E}_{\\widehat{p}_{i}}[v(w_{i};\\boldsymbol{x},\\boldsymbol{y})]-g_{i-1},w^{*}-w_{i}\\rangle}\\\\ &{=a_{i}\\langle\\mathbb{E}_{\\widehat{p}_{i}}[v(w_{i};\\boldsymbol{x},\\boldsymbol{y})]-\\mathbb{E}_{\\widehat{p}_{i-1}}[v(w_{i-1};\\boldsymbol{x},\\boldsymbol{y})],w^{*}-w_{i}\\rangle}\\\\ &{\\ \\ \\ -\\ a_{i-1}\\langle\\mathbb{E}_{\\widehat{p}_{i-1}}[v(w_{i-1};\\boldsymbol{x},\\boldsymbol{y})]-\\mathbb{E}_{\\widehat{p}_{i-2}}[v(w_{i-2};\\boldsymbol{x},\\boldsymbol{y})],w^{*}-w_{i-1}\\rangle}\\\\ &{\\ \\ \\ +\\ a_{i-1}\\langle\\mathbb{E}_{\\widehat{p}_{i-1}}[v(w_{i-1};\\boldsymbol{x},\\boldsymbol{y})]-\\mathbb{E}_{\\widehat{p}_{i-2}}[v(w_{i-2};\\boldsymbol{x},\\boldsymbol{y})],w_{i}-w_{i-1}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The first two terms on the right-hand side of Equation (34) telescope, so we focus on bounding the last term. We do so using Young's inequality (Fact B.1) followed by $\\kappa$ -Lipschitzness of $\\pmb{v}$ which leadsto ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\,a_{i-1}\\langle\\mathbb{E}_{\\widehat{p}_{i-1}}[v(w_{i-1};x,y)]-\\mathbb{E}_{\\widehat{p}_{i-2}}[v(w_{i-2};x,y)],w_{i}-w_{i-1}\\rangle}\\\\ &{\\leq\\frac{a_{i-1}^{2}}{1+0.5c_{1}A_{i-1}}\\|\\mathbb{E}_{\\widehat{p}_{i-1}}[v(w_{i-1};x,y)]-\\mathbb{E}_{\\widehat{p}_{i-2}}[v(w_{i-2};x,y)]\\|_{2}^{2}+\\frac{1+0.5c_{1}A_{i-1}}{4}\\|w_{i}-w_{i-1}\\|_{2}^{2}}\\\\ &{\\overset{(i)}{\\leq}\\frac{2a_{i-1}^{2}K^{2}}{1+0.5c_{1}A_{i-1}}\\|w_{i-1}-w_{i-2}\\|_{2}^{2}+\\frac{2a_{i-1}^{2}\\widehat{G}^{2}}{1+0.5c_{1}A_{i-1}}D_{\\phi}(\\widehat{p}_{i-1},\\widehat{p}_{i-2})+\\frac{1+0.5c_{1}A_{i-1}}{4}\\|w_{i}-w_{i-2}\\|_{2}^{2}}\\\\ &{\\overset{(i)}{\\leq}\\frac{1+0.5c_{1}A_{i-2}}{4}\\|w_{i-1}-w_{i-2}\\|_{2}^{2}+\\frac{1+0.5c_{1}A_{i-1}}{4}\\|w_{i}-w_{i-1}\\|_{2}^{2}+(\\nu_{0}+\\nu A_{i-2})D_{\\phi}(\\widehat{p}_{i-1},\\widehat{p}_{i-2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Where in $(i i)$ we used $\\textstyle{\\frac{2a_{i-1}{}^{2}\\kappa^{2}}{1+0.5c_{1}A_{i-1}}}\\;\\leq\\;{\\frac{1+0.5c_{1}A_{i-2}}{4}}$ 1+0.5ct Ai-2 and $\\begin{array}{r}{\\frac{2{a_{i-1}}^{2}G^{2}}{1+0.5c_{1}A_{i-1}}\\ \\leq\\ \\nu_{0}+\\nu A_{i-2}}\\end{array}$ , which both hold by the choice of the step size, while $(i)$ followsbyboundedness and $\\kappa$ Lipschitznessof $\\pmb{v}$ and CorollaryB.5, using ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Vert\\mathbb{E}_{\\widehat{p}_{i-1}}[v(w_{i-1};x,y)]-\\mathbb{E}_{\\widehat{p}_{i-2}}[v(w_{i-2};x,y)]\\Vert_{2}^{2}}\\\\ &{=\\Vert\\mathbb{E}_{\\widehat{p}_{i-1}}[v(w_{i-1};x,y)]-\\mathbb{E}_{\\widehat{p}_{i-2}}[v(w_{i-1};x,y)]+\\mathbb{E}_{\\widehat{p}_{i-2}}[v(w_{i-1};x,y)]-\\mathbb{E}_{\\widehat{p}_{i-2}}[v(w_{i-2};x,y)]\\Vert_{2}^{2}}\\\\ &{\\leq2\\Vert\\mathbb{E}_{\\widehat{p}_{i-1}}[v(w_{i-1};x,y)]-\\mathbb{E}_{\\widehat{p}_{i-2}}[v(w_{i-1};x,y)]\\Vert_{2}^{2}+2\\Vert\\mathbb{E}_{\\widehat{p}_{i-2}}[v(w_{i-1};x,y)]-\\mathbb{E}_{\\widehat{p}_{i-2}}[v(w_{i-2};x,y)]}\\\\ &{\\leq2G^{2}D_{\\phi}(\\widehat{p}_{i-1},\\widehat{p}_{i-2})+2\\kappa^{2}\\Vert w_{i-1}-w_{i-2}\\Vert_{2}^{2}.}\\end{array}\n$$y)112 ", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combining Equations (32) to (35), we now have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a_{i}S_{i}\\geq\\,-\\,a_{i}E_{i}+\\frac{1+0.5c_{1}A_{i-1}}{2}\\|\\pmb{w^{*}}-\\pmb{w_{i}}\\|_{2}^{2}-\\frac{1+0.5c_{1}A_{i-1}}{2}\\|\\pmb{w^{*}}-\\pmb{w_{i-1}}\\|_{2}^{2}}\\\\ &{\\qquad\\quad-\\,(\\nu_{0}+\\nu A_{i-2})D_{\\phi}(\\hat{p}_{i-1},\\hat{p}_{i-2})}\\\\ &{\\qquad\\quad+\\,\\frac{1+0.5c_{1}A_{i-1}}{4}\\|\\pmb{w_{i}}-\\pmb{w_{i-1}}\\|_{2}^{2}-\\frac{1+0.5c_{1}A_{i-2}}{4}\\|\\pmb{w_{i-1}}-\\pmb{w_{i-2}}\\|_{2}^{2}}\\\\ &{\\qquad\\quad+\\,a_{i}\\langle\\mathbb{E}_{\\hat{p}_{i}}[v(\\pmb{w_{i}};\\pmb{x},y)]-\\mathbb{E}_{\\hat{p}_{i}}[v(\\pmb{w_{i-1}};\\pmb{x},y)],\\pmb{w^{*}}-\\pmb{w_{i}}\\rangle}\\\\ &{\\qquad\\quad-\\,a_{i-1}\\langle\\mathbb{E}_{\\hat{p}_{i-1}}[v(\\pmb{w_{i-1}};\\pmb{x},y)]-\\mathbb{E}_{\\hat{p}_{i-1}}[v(\\pmb{w_{i-2}};\\pmb{x},y)],\\pmb{w^{*}}-\\pmb{w_{i-1}}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "To complete the proof, it remains to combine the last inequality with Equation (29) ", "page_idx": 25}, {"type": "text", "text": "Lemma D.3. Let $S_{i}$ be defined by Equation (30). Then, under the setting of Proposition $D.2$ wehave ", "page_idx": 25}, {"type": "equation", "text": "$$\nS_{i}\\geq\\mathbb{E}_{\\widehat{\\pmb{n}}_{i}}[\\langle\\pmb{v}(\\pmb{w};\\pmb{x},y),\\pmb{w}^{\\ast}-\\pmb{w}_{i}\\rangle]-E_{i},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where ", "page_idx": 25}, {"type": "equation", "text": "$$\nE_{i}=\\frac{c_{1}}{4}\\|\\pmb{w}^{*}-\\pmb{w}_{i}\\|_{2}^{2}+\\frac{8\\beta^{2}\\sqrt{6B}\\sqrt{\\widehat{\\mathrm{OPT}}_{(2)}}}{c_{1}}\\chi^{2}(\\widehat{\\pmb{n}_{i}},\\widehat{\\pmb{n}}^{*})+\\frac{48\\beta^{2}B\\widehat{\\mathrm{OPT}}}{c_{1}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Define the event $\\mathcal{G}=\\left\\{(\\pmb{x},y):\\sigma(\\pmb{w}_{i}\\cdot\\pmb{x})-y\\geq0\\right\\}$ . Then, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{\\widehat{\\mathcal{P}}_{i}}[2(\\sigma(w_{i}\\cdot x)-y)(\\sigma(w^{*}\\cdot x)-\\sigma(w_{i}\\cdot x))]}\\\\ &{=\\mathbb{E}_{\\widehat{\\mathcal{P}}_{i}}[2(\\sigma(w_{i}\\cdot x)-y)(\\sigma(w^{*}\\cdot x)-\\sigma(w_{i}\\cdot x))]_{\\mathbb{S}}+2(\\sigma(w_{i}\\cdot x)-y)(\\sigma(w^{*}\\cdot x)-\\sigma(w_{i}\\cdot x))\\mathbb{I}_{\\mathbb{S}^{c}}]}\\\\ &{\\geq\\mathbb{E}_{\\widehat{\\mathcal{P}}_{i}}[\\mathbb{I}_{\\mathbb{S}^{2}}[\\sigma(w_{i}\\cdot x)-y)\\sigma^{\\prime}(w_{i}\\cdot x)(w^{*}\\cdot x-w_{i}\\cdot x)]}\\\\ &{\\quad+\\mathbb{E}_{\\widehat{\\mathcal{P}}_{i}}[\\mathbb{I}_{\\mathbb{S}^{c}}2(\\sigma(w_{i}\\cdot x)-y)\\sigma^{\\prime}(w^{*}\\cdot x)(w^{*}\\cdot x-w_{i}\\cdot x)],}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the last inequality uses convexity of $\\sigma(\\cdot)$ to bound the term that involves $\\mathbb{I}_{\\mathcal{G}}$ and concavity of $-\\sigma(\\cdot)$ to bound the term that involves $\\mathbb{I}_{\\mathcal{G}^{c}}$ and where $\\sigma^{\\prime}$ denotes any subderivative of $\\sigma$ (guaranteed to exist, due to convexity and Lipschitzness). ", "page_idx": 26}, {"type": "text", "text": "Recall that ${\\pmb v}({\\pmb w}_{i};{\\pmb x},y)=2\\beta(\\sigma({\\pmb w}_{i}\\cdot{\\pmb x})-y){\\pmb x}$ . Using that $\\sigma^{\\prime}(t)=\\beta+(\\sigma^{\\prime}(t)-\\beta)$ for all $t$ and combining with the inequality above, we see ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{\\hat{\\pi}_{i}}[2(\\sigma({\\pmb w}_{i}\\cdot{\\pmb x})-y)(\\sigma({\\pmb w}^{*}\\cdot{\\pmb x})-\\sigma({\\pmb w}_{i}\\cdot{\\pmb x}))]}\\\\ &{\\geq\\mathbb{E}_{\\hat{\\pi}_{i}}[\\langle{\\pmb v}({\\pmb w}_{i};{\\pmb x},y),{\\pmb w}^{*}-{\\pmb w}_{i}\\rangle]}\\\\ &{\\quad+\\,2\\mathbb{E}_{\\hat{\\pi}_{i}}[\\mathbb{I}_{\\mathcal{G}}(\\sigma({\\pmb w}_{i}\\cdot{\\pmb x})-y)(\\sigma^{\\prime}({\\pmb w}_{i}\\cdot{\\pmb x})-\\beta)({\\pmb w}^{*}\\cdot{\\pmb x}-{\\pmb w}_{i}\\cdot{\\pmb x})]}\\\\ &{\\quad+\\,2\\mathbb{E}_{\\hat{\\pi}_{i}}[\\mathbb{I}_{\\mathcal{G}^{c}}(\\sigma({\\pmb w}_{i}\\cdot{\\pmb x})-y)(\\sigma^{\\prime}({\\pmb w}^{*}\\cdot{\\pmb x})-\\beta)({\\pmb w}^{*}\\cdot{\\pmb x}-{\\pmb w}_{i}\\cdot{\\pmb x})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and so to prove the lemma we only need to focus on bounding the terms in the last two lines. ", "page_idx": 26}, {"type": "text", "text": "Recall that $\\sigma$ is assumed to be monotonically increasing and $\\beta$ -Lipschitz, and so $0\\leq\\sigma^{\\prime}(\\pmb{w}^{\\ast}\\cdot\\pmb{x})\\leq\\beta$ Thus, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad(\\sigma(\\pmb{w}_{i}\\cdot\\pmb{x})-y)(\\sigma^{\\prime}(\\pmb{w}^{\\ast}\\cdot\\pmb{x})-\\beta)(\\pmb{w}^{\\ast}\\cdot\\pmb{x}-\\pmb{w}_{i}\\cdot\\pmb{x})}\\\\ &{=(\\sigma(\\pmb{w}^{\\ast}\\cdot\\pmb{x})-y)(\\sigma^{\\prime}(\\pmb{w}^{\\ast}\\cdot\\pmb{x})-\\beta)(\\pmb{w}^{\\ast}\\cdot\\pmb{x}-\\pmb{w}_{i}\\cdot\\pmb{x})}\\\\ &{\\quad+\\,(\\sigma(\\pmb{w}_{i}\\cdot\\pmb{x})-\\sigma(\\pmb{w}^{\\ast}\\cdot\\pmb{x}))(\\sigma^{\\prime}(\\pmb{w}^{\\ast}\\cdot\\pmb{x})-\\beta)(\\pmb{w}^{\\ast}\\cdot\\pmb{x}-\\pmb{w}_{i}\\cdot\\pmb{x})}\\\\ &{\\geq(\\sigma(\\pmb{w}^{\\ast}\\cdot\\pmb{x})-y)(\\sigma^{\\prime}(\\pmb{w}^{\\ast}\\cdot\\pmb{x})-\\beta)(\\pmb{w}^{\\ast}\\cdot\\pmb{x}-\\pmb{w}_{i}\\cdot\\pmb{x}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where we have used $\\sigma^{\\prime}(\\pmb{w}^{\\ast}\\cdot\\pmb{x})-\\beta\\leq0$ (by Lipschitzness) and $(\\sigma({\\pmb w}_{i}\\cdot{\\pmb x})-\\sigma({\\pmb w}^{\\ast}\\cdot{\\pmb x}))({\\pmb w}^{\\ast}\\cdot{\\pmb x}-$ ${\\pmb w}_{i}\\cdot{\\pmb x})\\leq0$ (by monotonicity of $\\sigma$ ). By the same argument, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\sigma({\\pmb w}_{i}\\cdot{\\pmb x})-y)(\\sigma^{\\prime}({\\pmb w}_{i}\\cdot{\\pmb x})-\\beta)({\\pmb w}^{*}\\cdot{\\pmb x}-{\\pmb w}_{i}\\cdot{\\pmb x})}\\\\ {\\geq(\\sigma({\\pmb w}^{*}\\cdot{\\pmb x})-y)(\\sigma^{\\prime}({\\pmb w}_{i}\\cdot{\\pmb x})-\\beta)({\\pmb w}^{*}\\cdot{\\pmb x}-{\\pmb w}_{i}\\cdot{\\pmb x}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "To complete the proof of the lemma, it remains to bound the expectation of the term in Equation (40). We proceed using that $|\\sigma^{\\prime}(\\pmb{w}\\cdot\\pmb{x})-\\beta|\\leq\\beta,\\forall\\pmb{w}$ and thus for $\\bar{\\b w}\\in\\{\\pmb w^{*},\\pmb w_{i}\\}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad|({\\boldsymbol\\sigma}({\\boldsymbol{\\mathbf w}}^{*}\\cdot{\\boldsymbol{\\mathbf x}})-{\\boldsymbol\\y})({\\boldsymbol\\sigma}^{\\prime}({\\boldsymbol{\\mathbf w}}\\cdot{\\boldsymbol{\\mathbf x}})-\\beta)({\\boldsymbol{\\mathbf w}}^{*}\\cdot{\\boldsymbol{\\mathbf x}}-{\\boldsymbol{\\mathbf w}}_{i}\\cdot{\\boldsymbol{\\mathbf x}})|}\\\\ &{\\le\\beta|{\\boldsymbol\\sigma}({\\boldsymbol{\\mathbf w}}^{*}\\cdot{\\boldsymbol{\\mathbf x}})-y||{\\boldsymbol{\\mathbf w}}^{*}\\cdot{\\boldsymbol{\\mathbf x}}-{\\boldsymbol{\\mathbf w}}_{i}\\cdot{\\boldsymbol{\\mathbf x}}|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Taking the expectation on both sides, and combining with Equation (40), we further have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-2\\mathbb{E}_{\\hat{\\pi}}[\\|\\mathbf{g}_{\\ell}(\\sigma(w_{i}\\cdot x)-y)(\\sigma^{\\prime}(w_{i}\\cdot x)-\\beta)(w^{*}\\cdot x-w_{i}\\cdot x)\\|}\\\\ &{\\quad-2\\mathbb{E}_{\\hat{\\pi}}[\\|\\mathbf{g}_{\\ell}(\\sigma(w_{i}\\cdot x)-y)(\\sigma^{\\prime}(w^{*}\\cdot x)-\\beta)(w^{*}\\cdot x-w_{i}\\cdot x)\\|}\\\\ &{\\le2\\beta\\mathbb{E}_{\\hat{\\pi}_{i}}[(\\mathbb{d}_{\\ell}+\\mathbb{I}_{\\mathbb{S}^{\\prime}})|\\sigma(w^{*}\\cdot x)-y||w^{*}\\cdot x-w_{i}\\cdot x|]}\\\\ &{=2\\beta\\mathbb{E}_{\\hat{\\pi}_{i}}[|\\sigma(w^{*}\\cdot x)-y||w^{*}\\cdot x-w_{i}\\cdot x|]}\\\\ &{=2\\beta\\int|\\sigma(w^{*}\\cdot x)-y||w^{*}\\cdot x-w_{i}\\cdot x|\\mathrm{d}\\hat{p}_{i}}\\\\ &{=2\\beta\\int|\\sigma(w^{*}\\cdot x)-y||w^{*}\\cdot x-w_{i}\\cdot x|\\mathrm{d}\\hat{p}_{i}}\\\\ &{=2\\beta\\int|\\sigma(w^{*}\\cdot x)-y||w^{*}\\cdot x-w_{i}\\cdot x|\\mathrm{d}\\hat{p}^{*}}\\\\ &{\\quad+2\\beta\\int|\\sigma(w^{*}\\cdot x)-y||w^{*}\\cdot x-w_{i}\\cdot x|(\\mathrm{d}\\hat{p}_{i}-\\mathrm{d}\\hat{p}^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "In the last equality, the first integral is just the expectation with respect to $\\widehat{\\boldsymbol{p}}^{*}$ , and thus using CauchySchwarz inequality, the definition of OPT, and Lemma C.6, the first term in Equation (41) can be ", "page_idx": 26}, {"type": "text", "text": "bounded by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\int|\\sigma(\\boldsymbol{w}^{*}\\cdot\\boldsymbol{x})-y||w^{*}\\cdot\\boldsymbol{x}-w_{i}\\cdot\\boldsymbol{x}|\\mathrm{d}\\hat{p}^{*}}\\\\ &{\\le\\sqrt{\\mathbb{E}_{\\hat{\\pi}^{*}}[(\\sigma(\\boldsymbol{w}^{*}\\cdot\\boldsymbol{x})-y)^{2}]\\mathbb{E}_{\\hat{\\pi}^{*}}[(\\boldsymbol{w}^{*}\\cdot\\boldsymbol{x}-w_{i}\\cdot\\boldsymbol{x})^{2}]}}\\\\ &{\\le\\sqrt{\\widehat{\\mathrm{OPT}}\\sqrt{6B}}\\|w^{*}-w_{i}\\|_{2}}\\\\ &{\\le\\frac{24\\beta B\\widehat{\\mathrm{OPT}}}{c_{1}}+\\frac{c_{1}}{16\\beta}\\|\\boldsymbol{w}^{*}-\\boldsymbol{w}_{i}\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the last line is by Young's inequality and the second last line uses Lemma 2.5. ", "page_idx": 27}, {"type": "text", "text": "For the remaining integral in Equation (41), using the defnition of chi-square divergence and CauchySchwarz inequality, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad\\displaystyle\\int|\\sigma(\\boldsymbol{w}^{*}\\cdot\\boldsymbol{x})-y||\\boldsymbol{w}^{*}\\cdot\\boldsymbol{x}-\\boldsymbol{w}_{i}\\cdot\\boldsymbol{x}|(\\mathrm{d}\\hat{p}_{i}-\\mathrm{d}\\hat{p}^{*})}\\\\ &{=\\displaystyle\\int|\\sigma(\\boldsymbol{w}^{*}\\cdot\\boldsymbol{x})-y||\\boldsymbol{w}^{*}\\cdot\\boldsymbol{x}-\\boldsymbol{w}_{i}\\cdot\\boldsymbol{x}|\\frac{(\\mathrm{d}\\hat{p}_{i}-\\mathrm{d}\\hat{p}^{*})}{\\sqrt{\\mathrm{d}\\hat{p}^{*}}}\\sqrt{\\mathrm{d}\\hat{p}^{*}}}\\\\ &{\\overset{(i)}{\\leq}\\sqrt{\\lambda^{2}(\\hat{p}_{i},\\hat{p}^{*})}\\mathbb{E}_{\\hat{p}^{*}}[(\\sigma(\\boldsymbol{w}^{*}\\cdot\\boldsymbol{x})-y)^{2}(\\boldsymbol{w}^{*}\\cdot\\boldsymbol{x}-\\boldsymbol{w}_{i}\\cdot\\boldsymbol{x})^{2}]}\\\\ &{\\overset{(i i)}{\\leq}\\chi^{2}(\\hat{p}_{i},\\hat{p}^{*})^{1/2}\\mathbb{E}_{\\hat{p}^{*}}[(\\sigma(\\boldsymbol{w}^{*}\\cdot\\boldsymbol{x})-y)^{4}|^{1/4}\\mathbb{E}_{\\hat{p}^{*}}[(\\boldsymbol{w}^{*}\\cdot\\boldsymbol{x}-\\boldsymbol{w}_{i}\\cdot\\boldsymbol{x})^{4}]^{1/4}}\\\\ &{\\overset{(i i i i)}{\\leq}\\chi^{2}(\\hat{p}_{i},\\hat{p}^{*})^{1/2}\\widehat{\\Omega\\nabla}\\bar{\\Gamma}_{(2)}^{1/4}(\\mathrm{d}B\\|\\boldsymbol{w}^{*}-\\boldsymbol{w}_{i}\\|_{2}^{4})^{1/4}}\\\\ &{\\overset{(i v)}{\\leq}\\frac{4\\beta\\sqrt{6B}\\sqrt{\\Omega}\\Gamma()}{c_{1}}\\chi^{2}(\\hat{p}_{i},\\hat{p}^{*})+\\frac{c_{1}}{16\\beta}\\|\\boldsymbol{w}^{*}-\\boldsymbol{w}_{i}\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Where $(i)$ is by Cauchy-Schwarz,(i)is by Cauchy-Schwarz again, (i)is by the definition of $\\widehat{\\mathrm{OPT}}_{(2)}$ and Lemma C.6, and (iv) is by Young's inequality. ", "page_idx": 27}, {"type": "text", "text": "To complete the proof, it remains to plug Equations (41) to (43) back into Equation (39), and simplify. ", "page_idx": 27}, {"type": "text", "text": "Gap upper bound proof of Lemma 3.3.  Combining the upper and lower bounds from Lemma D.1 and Proposition D.2, we are now ready to prove Lemma 3.3, which we restate below. ", "page_idx": 27}, {"type": "text", "text": "Lemma 3.3 (Gap Upper Bound). Let $w_{i},\\widehat{\\pmb{\\mathscr{p}}}_{i},a_{i},A_{i}$ evolve according to Algorithm $^{\\,l}$ where we take, by convention, $a_{-1}=A_{-1}=a_{0}=A_{0}=0$ and ${\\pmb w}_{-1}\\,=\\,{\\pmb w}_{0}$ \uff0c $\\widehat{\\ensuremath{p}}_{-1}\\,=\\,\\widehat{\\ensuremath{p}}_{0}$ . Assuming Lemma 2.5 applies, then, for all $k\\geq1$ \uff0c $\\begin{array}{r}{\\sum_{i=1}^{k}a_{i}\\mathrm{Gap}({\\pmb w}_{i},\\widehat{\\boldsymbol{\\mu}}_{i})}\\end{array}$ is bounded above by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{2}\\|w^{*}-w_{0}\\|_{2}^{2}+\\nu_{0}D_{\\phi}(\\hat{p}^{*},\\hat{p}_{0})-\\frac{1+0.5c_{1}A_{k}}{2}\\|w^{*}-w_{k}\\|_{2}^{2}-(\\nu_{0}+\\nu A_{k})D_{\\phi}(\\hat{p}^{*},\\hat{p}_{k})}\\\\ {\\displaystyle+\\sum_{i=1}^{k}a_{i}\\frac{c_{1}}{4}\\|w^{*}-w_{i}\\|_{2}^{2}+\\frac{8\\beta^{2}\\sqrt{6B}\\sqrt{\\widehat{\\mathrm{OPT}}_{(2)}}}{c_{1}}\\sum_{i=1}^{k}a_{i}\\chi^{2}(\\hat{p}_{i},\\hat{p}^{*})+\\frac{48\\beta^{2}B\\widehat{\\mathrm{OPT}}A_{k}}{c_{1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof.Combining_ the upper bound on $a_{i}L(\\pmb{w}_{i},\\widehat{\\pmb{\\mathscr{n}}}^{*})$ from Lemma D.1 with the lower bound on $a_{i}L(\\pmb{w}^{*},\\widehat{\\pmb{\\mathscr{p}}}_{i})$ from Proposition D.2 and recalling that $\\mathrm{Gap}(\\pmb{w}_{i},\\widehat{\\pmb{\\mathscr{n}}_{i}})=L(\\pmb{w}_{i},\\widehat{\\pmb{\\mathscr{n}}}^{*})-L(\\pmb{w}^{*},\\widehat{\\pmb{\\mathscr{n}}_{i}})$ and ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a_{i}\\mathrm{Gap}(\\boldsymbol{w}_{i},\\widehat{\\boldsymbol{\\mu}_{i}})\\le a_{i}E_{i}}\\\\ &{\\hphantom{a a}+(\\nu_{0}+\\nu A_{i-1})D_{\\phi}(\\widehat{\\boldsymbol{\\mu}}^{*},\\widehat{\\boldsymbol{\\mu}_{i-1}})-(\\nu_{0}+\\nu A_{i})D_{\\phi}(\\widehat{\\boldsymbol{\\mu}}^{*},\\widehat{\\boldsymbol{\\mu}_{i}})}\\\\ &{\\hphantom{a a a}+(\\nu_{0}+\\nu A_{i-2})D_{\\phi}(\\widehat{\\boldsymbol{\\mu}}_{i-1},\\widehat{\\boldsymbol{\\mu}_{i-2}})-(\\nu_{0}+\\nu A_{i-1})D_{\\phi}(\\widehat{\\boldsymbol{\\mu}}_{i},\\widehat{\\boldsymbol{\\mu}_{i-1}})}\\\\ &{\\hphantom{a a a}+\\frac{1}{2}\\sum_{\\begin{array}{l}{1}\\\\ {2}\\end{array}}}\\\\ &{\\hphantom{a a a}+\\frac{1}{4}+0.5c_{1}A_{i-1}\\|\\boldsymbol{w}^{*}-\\boldsymbol{w}_{i-1}\\|_{2}^{2}-\\frac{1+0.5c_{1}A_{i}}{2}\\|\\boldsymbol{w}^{*}-\\boldsymbol{w}_{i}\\|_{2}^{2}}\\\\ &{\\hphantom{a a a}+\\frac{1}{4}\\boldsymbol{0}.}\\\\ &{\\hphantom{a a a a}+a_{i-1}\\mathbb{E}_{\\widehat{\\boldsymbol{\\mu}}_{i-1}}[\\boldsymbol{w}(\\boldsymbol{w}_{i-1};\\boldsymbol{x},\\boldsymbol{y})]-\\mathbb{E}_{\\widehat{\\boldsymbol{\\mu}}_{i-2}}[\\boldsymbol{v}(\\boldsymbol{w}_{i-2};\\boldsymbol{x},\\boldsymbol{y})],\\boldsymbol{w}^{*}-\\boldsymbol{w}_{i-1}\\|_{2}^{2}}\\\\ &{\\hphantom{a a a a}-a_{i}\\langle\\mathbb{E}_{\\widehat{\\boldsymbol{\\mu}}_{i}}[\\boldsymbol{v}(\\boldsymbol{w}_{i};\\boldsymbol{x},\\boldsymbol{y})]-\\mathbb{E}_{\\widehat{\\boldsymbol{\\mu}}_{i-1}}[\\boldsymbol{v}(\\boldsymbol{w}_{i-1};\\boldsymbol{x},\\boldsymbol{y})],\\boldsymbol{w}^{*}-\\boldsymbol{w}_{i}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Observe that except for the first line on the right-hand side of the above inequality, all remaining terms telescope. Summing over $i=1,2,\\dots,k$ and recalling that, by convention, $a_{0}=A_{0}=a_{-1}=$ $A_{-1}=0$ ${\\pmb w}_{-1}={\\pmb w}_{0}$ ,and $\\widehat{p}_{-1}=\\widehat{p}_{0}$ ,wehave ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{i=1}^{k}a_{i}\\mathrm{Gap}(\\boldsymbol{w}_{i},\\widehat{\\boldsymbol{p}}_{i})\\leq\\displaystyle\\sum_{i=1}^{k}a_{i}E_{i}+\\frac{1}{2}\\|\\boldsymbol{w}^{*}-\\boldsymbol{w}_{0}\\|_{2}^{2}+\\nu_{0}D_{\\phi}(\\widehat{\\boldsymbol{p}}^{*},\\widehat{\\boldsymbol{p}}_{0})}\\\\ &{\\displaystyle-\\,\\frac{1+0.5c_{1}A_{k}}{2}\\|\\boldsymbol{w}^{*}-\\boldsymbol{w}_{k}\\|_{2}^{2}-(\\nu_{0}+A_{k})D_{\\phi}(\\widehat{\\boldsymbol{p}}^{*},\\widehat{\\boldsymbol{p}}_{k})}\\\\ &{-\\,a_{k}\\langle\\mathbb{E}_{\\widehat{\\boldsymbol{p}}_{k}}[\\boldsymbol{v}(\\boldsymbol{w}_{k};\\boldsymbol{x},\\boldsymbol{y})]-\\mathbb{E}_{\\widehat{\\boldsymbol{p}}_{k-1}}[\\boldsymbol{v}(\\boldsymbol{w}_{k-1};\\boldsymbol{x},\\boldsymbol{y})],\\boldsymbol{w}^{*}-\\boldsymbol{w}_{k}\\rangle}\\\\ &{-\\,\\frac{1+0.5c_{1}A_{k-1}}{4}\\|\\boldsymbol{w}_{k}-\\boldsymbol{w}_{k-1}\\|_{2}^{2}-(\\nu_{0}+\\nu A_{k-1})D_{\\phi}(\\widehat{\\boldsymbol{p}}_{k},\\widehat{\\boldsymbol{p}}_{k-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "To complete the proof, it remains to bound $a_{k}|\\langle\\mathbb{E}_{\\widehat{\\pi}_{k}}[v(\\pmb{w}_{k};\\pmb{x},y)]\\!-\\!\\mathbb{E}_{\\widehat{\\pi}_{k-1}}[v(\\pmb{w}_{k-1};\\pmb{x},y)],\\pmb{w}^{*}\\!-\\!\\pmb{w}_{k}\\rangle|,$ which is done similarly as in the proof of Proposition D.2. In particular, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\ a_{k}|\\langle\\mathbb{E}_{\\widehat{p}_{k}}[v(w_{k};x,y)]-\\mathbb{E}_{\\widehat{p}_{k-1}}[v(w_{k-1};x,y)],w^{*}-w_{k}\\rangle|}\\\\ &{\\stackrel{(i)}{\\leq}\\frac{a_{k}^{2}}{1+0.5c_{1}A_{k}}\\|\\mathbb{E}_{\\widehat{p}_{k}}[v(w_{k};x,y)]-\\mathbb{E}_{\\widehat{p}_{k-1}}[v(w_{k-1};x,y)]\\|_{2}^{2}+\\frac{1+0.5c_{1}A_{k}}{4}\\|w^{*}-w_{k}\\|_{2}^{2}}\\\\ &{\\stackrel{(i i)}{\\leq}\\frac{a_{k}^{2}}{1+0.5c_{1}A_{k}}\\Big(2G^{2}D_{\\phi}(\\widehat{p}_{k},\\widehat{p}_{k-1})+2\\kappa^{2}\\|w_{k}-w_{k-1}\\|_{2}^{2}\\Big)+\\frac{1+0.5c_{1}A_{k}}{4}\\|w^{*}-w_{k}\\|_{2}^{2}}\\\\ &{\\stackrel{(i i i)}{\\leq}\\big(\\nu_{0}+\\nu A_{k-1})D_{\\phi}(\\widehat{p}_{k},\\widehat{p}_{k-1})+\\frac{1+0.5c_{1}A_{k-1}}{4}\\|w_{k}-w_{k-1}\\|_{2}^{2}+\\frac{1+0.5c_{1}A_{k}}{4}\\|w^{*}-w_{k}\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Where () is by Young's inequality and (i)is by Equation (36), and (i) is by 120.5ca Ak $\\begin{array}{r}{\\frac{2{G^{2}}{a_{k}}^{2}}{1+0.5c_{1}A_{k}}\\leq\\nu_{0}\\!+\\!\\nu A_{k-1}}\\end{array}$ and $\\textstyle{\\frac{2\\kappa^{2}{a_{k}}^{2}}{1+0.5c_{1}A_{k}}}\\leq{\\frac{1+0.5c_{1}A_{k-1}}{4}}$ 10.5A1, which both hold by the choice of the step sizes in Algorithm 1. To complete the proof, it remains to plug Equation (45) back into Equation (44), use the definition of $E_{i}$ from Equation (37), and simplify. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "E  Omitted Proofs in Main Theorem ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Claim 3.5. For all iterations $k\\geq0$ $\\lVert\\pmb{w}_{k}\\rVert_{2}\\leq2\\lVert\\pmb{w}^{*}\\rVert_{2}$ ", "page_idx": 28}, {"type": "text", "text": "Proof of Claim 3.5. It trivially holds that $\\mathbf{0}=\\pmb{w}_{0}\\in\\mathcal{B}(2||\\pmb{w}^{*}||_{2})$ .Suppose $\\lVert\\pmb{w}_{i}\\rVert_{2}\\leq2\\lVert\\pmb{w}^{*}\\rVert_{2}$ for all iterations $i\\leq t$ where $t\\geq0$ . Then ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle-\\frac{12\\beta^{2}B}{c_{1}}\\widehat{\\mathrm{OPT}}A_{k}+\\sum_{i=1}^{k}a_{i}\\frac{c_{1}}{2}\\|{\\pmb w}_{i}-{\\pmb w}^{*}\\|_{2}^{2}+\\sum_{i=1}^{k}\\nu a_{i}D_{\\phi}(\\hat{\\pmb\\mu}^{*},\\hat{\\pmb\\mu}_{i})+a_{k+1}\\mathrm{Gap}({\\pmb w}_{k+1},\\hat{\\pmb\\mu}_{k+1})}\\\\ &{\\displaystyle\\leq\\sum_{i=1}^{k+1}a_{i}\\mathrm{Gap}({\\pmb w}_{i},\\hat{\\pmb\\mu}_{i})}\\\\ &{\\displaystyle\\leq\\frac{1}{2}\\|{\\pmb w}^{*}-{\\pmb w}_{0}\\|_{2}^{2}+\\nu_{0}D_{\\phi}(\\hat{\\pmb\\mu}^{*},\\hat{\\pmb\\mu}_{0})-\\frac{1+0.5c_{1}A_{k+1}}{2}\\|{\\pmb w}^{*}-{\\pmb w}_{k+1}\\|_{2}^{2}-(\\nu_{0}+\\nu A_{k+1})D_{\\phi}(\\hat{\\pmb\\mu}^{*},\\hat{\\pmb\\mu}_{k})}\\\\ &{\\displaystyle\\ \\ \\ +\\sum_{i=1}^{k+1}a_{i}\\frac{c_{1}}{4}\\|{\\pmb w}^{*}-{\\pmb w}_{i}\\|_{2}^{2}+\\frac{8\\beta^{2}\\sqrt{6B}\\sqrt{\\widehat{\\mathrm{OPT}}}()}{c_{1}}\\sum_{i=1}^{k+1}a_{i}\\chi^{2}(\\hat{\\mu}_{i},\\hat{\\pmb\\mu}^{*})+\\frac{48\\beta^{2}B\\widehat{\\mathrm{OPT}}A_{k+1}}{c_{1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where we used the gap upper bound Lemma 3.3 again as it does not require $\\pmb{w}\\in\\mathcal{B}(\\|\\pmb{w}^{*}\\|_{2})$ .We proceed to deduce a different lower bound for $\\mathrm{Gap}(\\pmb{w}_{k+1},\\widehat{\\boldsymbol{\\mu}}_{k+1})$ . Similar to Lemma 3.2, we break into two terms $L(\\pmb{w}^{\\ast},\\widehat{p}_{k+1})-(-L(\\pmb{w}^{\\ast},\\widehat{p}^{\\ast}))\\geq\\nu\\Dot{D}_{\\phi}(\\widehat{p}^{\\ast},\\widehat{p}_{k+1})$ and $L(\\pmb{w}_{k+1},\\widehat{\\pmb{\\mathscr{n}}}^{*})-L(\\pmb{w}^{*},\\widehat{\\pmb{\\mathscr{n}}}^{*})=$ $\\mathbb{E}_{\\widehat{\\boldsymbol{\\pi}}^{\\ast}}[(\\sigma(\\pmb{w}_{k+1}\\cdot\\pmb{x})-y)^{2}-(\\sigma(\\pmb{w}^{\\ast}\\cdot\\pmb{x})-y)^{2}]\\geq-\\widetilde{\\mathrm{OPT}},\\mathrm{v}$ vhere the first term is bounded the same way as in Lemma 3.2. Hence, $\\mathrm{Gap}(\\pmb{w}_{k+1},\\widehat{\\boldsymbol{p}}_{k+1})\\geq-\\widehat{\\mathrm{OPT}}$ Therefore, we simplify as before ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1+0.5c_{1}A_{k+1}}{2}\\|\\pmb{w}^{*}-\\pmb{w}_{k+1}\\|_{2}^{2}+(\\nu_{0}+\\nu A_{k+1})D_{\\phi}(\\hat{\\pmb{\\mu}}^{*},\\hat{p}_{k+1})-a_{i}\\frac{c_{1}}{4}\\|\\pmb{w}^{*}-\\pmb{w}_{k+1}\\|_{2}^{2}}\\\\ &{\\leq\\frac{1}{2}\\|\\pmb{w}^{*}-\\pmb{w}_{0}\\|_{2}^{2}+\\nu_{0}D_{\\phi}(\\hat{\\pmb{\\mu}}^{*},\\hat{p}_{0})+\\frac{12\\beta^{2}B}{c_{1}}\\widehat{\\mathrm{OPT}}A_{k}+a_{k+1}\\widehat{\\mathrm{OPT}}+\\frac{48\\beta^{2}B\\widehat{\\mathrm{OPT}}A_{k+1}}{c_{1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which implies by nonnegativity of Bregman divergence that: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{2+c_{1}A_{k}}{4}\\|w^{*}-w_{k+1}\\|_{2}^{2}\\leq\\frac{1}{2}\\|w^{*}-w_{0}\\|_{2}^{2}+\\nu_{0}D_{\\phi}(\\hat{p}^{*},\\hat{p}_{0})+\\Big(\\frac{60\\beta^{2}B}{c_{1}}A_{k}+a_{k+1}\\Big(1+\\frac{48\\beta^{2}B}{c_{1}}\\Big)\\Big)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Rearranging and using $2+c_{1}A_{k}\\ge2$ ", "page_idx": 29}, {"type": "equation", "text": "$$\nw^{*}-w_{k+1}\\|_{2}^{2}\\leq\\|w^{*}-w_{0}\\|_{2}^{2}+2\\nu_{0}D_{\\phi}(\\widehat{p}^{*},\\widehat{p}_{0})+\\Big(\\frac{240\\beta^{2}B}{c_{1}^{2}}+\\frac{4a_{k+1}}{2+c_{1}A_{k}}\\Big(1+\\frac{48\\beta^{2}B}{c_{1}}\\Big)\\Big)\\widehat{\\mathrm{OPT}},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Our choice of stepsizes as implies 2Ak $\\begin{array}{r}{\\frac{a_{k+1}}{2+c_{1}A_{k}}\\leq1/\\operatorname*{max}\\{\\kappa,G\\}\\leq1}\\end{array}$ hence ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|w^{*}-w_{k+1}\\|_{2}^{2}\\leq\\|w^{*}-w_{0}\\|_{2}^{2}+2\\nu_{0}D_{\\phi}(\\widehat{p}^{*},\\widehat{p}_{0})+\\Big(\\frac{288\\beta^{2}B}{c_{1}^{2}}+\\frac{1}{\\operatorname*{max}\\{\\kappa,G\\}}\\Big)\\widehat{\\mathrm{OPT}},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Claim E.1. For $\\nu\\geq8\\beta^{2}\\sqrt{6B}\\sqrt{\\widehat{\\mathrm{OPT}}_{(2)}}/c_{1},$ . it holds that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\chi^{2}(\\widehat{\\boldsymbol{p}}^{*},\\widehat{\\boldsymbol{p}}_{0})=\\frac{\\mathrm{Var}_{\\widehat{\\boldsymbol{p}}_{0}}(\\ell(\\mathbf{w}^{*}))}{4\\nu^{2}}\\leq\\frac{\\widehat{\\mathrm{OPT}}_{(2)}}{2\\nu^{2}}\\leq c_{1}/(1536\\beta^{4}B).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Similarly, for $\\nu\\ge\\mathbb{E}_{n_{0}}\\ell(\\pmb{w}^{*}),8\\beta^{2}\\sqrt{6B}\\sqrt{\\mathrm{OPT}_{(2)}}/c_{1}$ , it holds that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\chi^{2}(p^{*},p_{0})=\\frac{\\mathrm{Var}_{p_{0}}(\\ell(w^{*}))}{4\\nu^{2}}\\leq\\frac{\\mathrm{OPT}_{(2)}}{2\\nu^{2}}\\leq c_{1}/(1536\\beta^{4}B).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "ProofofClaimE.1 By Corollay C3, x\u00b2(p\\*,o) =me()aow)\u00b2\u2264 \u2264(1wheesenlast equality\u22652 rmCoraryCd the last inequality comes from lower bound on in the assumption. ", "page_idx": 29}, {"type": "text", "text": "The population version follows analogously ", "page_idx": 29}, {"type": "text", "text": "Since $D_{\\phi}(\\widehat{p}^{*},\\widehat{p}_{0})=\\chi^{2}(\\widehat{p}^{*},\\widehat{p}_{0})$ , by choosing $\\nu_{0}=768\\beta^{4}B\\epsilon/c_{1}$ , we ensure $2\\nu_{0}D_{\\phi}(\\widehat{p}^{*},\\widehat{p}_{0})\\leq\\epsilon$ ", "page_idx": 29}, {"type": "text", "text": "By choosing $\\nu_{0}$ small enough and initialization $w_{0}=\\mathbf{0}$ , it holds that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|\\pmb{w}^{*}-\\pmb{w}_{k+1}\\|_{2}^{2}\\leq\\|\\pmb{w}^{*}\\|_{2}^{2}+\\epsilon+\\Big(\\frac{288\\beta^{2}B}{c_{1}^{2}}+\\frac{1}{\\operatorname*{max}\\{\\kappa,G\\}}\\Big)\\widehat{\\mathrm{OPT}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We may assume without loss of generality that $\\begin{array}{r}{\\frac{1}{\\operatorname*{max}\\{\\kappa,G\\}}\\ll\\frac{288\\beta^{2}B}{c_{1}^{2}}}\\end{array}$ because both $\\kappa$ and $G$ is $O(d)$ but the right hand side is an absolute constant. We may also assume without loss of generality that 3008PT+2\\*, thus comleting the induction stel\\*+1l\u22642.The reason for the last no loss of generality is the following: otherwise, we can compare, per Claim E.2, the empirical risk of the output from our algorithm and of $\\hat{\\pmb w}={\\bf0}$ and output the solution with the lower risk to get an $O(\\mathrm{OPT})+\\epsilon$ solution. \u53e3 ", "page_idx": 30}, {"type": "text", "text": "ClaimE.2 (Zero-Tester). Inthe setting of Theorem 3.1, it is possible to efciently check if $'R(\\mathbf{0};\\widehat{p}_{0})>$ $R(\\widehat{\\pmb{w}};\\widehat{\\pmb{\\mathscr{p}}}_{0})$ ornot;where $\\widehat{\\pmb{w}}$ is the output of Algorithm $^{\\,I}$ ", "page_idx": 30}, {"type": "text", "text": "Proof. Observe that $\\begin{array}{r}{L(\\pmb{w},\\widehat{p})=\\sum_{i=1}^{N}\\widehat{p}_{i}(\\sigma(\\pmb{w}\\cdot\\pmb{x})-y)^{2}-\\nu\\chi^{2}(\\widehat{p},\\widehat{p}_{0})}\\end{array}$ .is $1/\\nu$ strongly concave in $(\\widehat{\\boldsymbol{\\mu}}^{(1)},\\ldots,\\widehat{\\boldsymbol{\\mu}}^{(N)})$ . Now, since $R(\\pmb{w};\\widehat{\\pmb{\\rho}_{0}})=\\operatorname*{max}_{\\widehat{\\pmb{\\rho}}}L(\\pmb{w},\\widehat{\\pmb{\\rho}})$ : we can estimate the risk at any given $\\pmb{w}$ using standard maximization techniques (such as gradient descent). ", "page_idx": 30}, {"type": "text", "text": "To test which risk is larger, we estimate $R(\\mathbf{0};\\widehat{\\boldsymbol{p}}_{0})$ and $R(\\widehat{\\pmb{w}};\\widehat{\\boldsymbol{p}_{0}})$ to a necessary accuracy and then compare. ", "page_idx": 30}, {"type": "text", "text": "F Parameter Estimation to Loss and Risk Approximation ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Theorem 3.1 shows that Algorithm 1 recovers a vector $\\widehat{\\pmb w}$ such that $\\|\\widehat{\\boldsymbol{w}}-\\boldsymbol{w}^{*}\\|_{2}\\le\\sqrt{\\mathrm{OPT}}+\\sqrt{\\epsilon}$ where OPT $:=\\mathbb{E}_{p^{*}}(\\sigma(\\pmb{w}^{*}\\cdot\\pmb{x})-y)^{2}$ ", "page_idx": 30}, {"type": "text", "text": "F.1 Loss Approximation ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In this section we show that this implies that the neuron we recover achieves a constant factor approximation to the optimal squared loss. ", "page_idx": 30}, {"type": "text", "text": "Lemma F.1. Let $\\boldsymbol{p}^{*}$ satisfy Assumption 2.1 and Assumption 2.2. Suppose $(\\widehat{\\pmb{w}},\\widehat{\\boldsymbol{p}})$ is the solution returned by Algorithm $^{\\,l}$ when given $N=$ samples drawn from $\\mu_{0}$ . Then, $\\mathbb{E}_{p^{*}}(\\dot{\\sigma}(\\widehat{\\pmb{w}}\\cdot\\pmb{x})-y)^{2}\\leq$ $O_{\\beta,B}(\\mathrm{OPT})+\\epsilon.$ ", "page_idx": 30}, {"type": "text", "text": "Proof. Recall that $\\sigma$ is $\\beta$ -Lipschitz, and Fact 2.4 gives us $\\mathbb{E}_{p^{*}}(\\mathbf{\\boldsymbol{u}}\\cdot\\mathbf{\\boldsymbol{x}})^{2}\\leq5B$ for all unit vectors $\\textbf{\\em u}$ These together imply, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{p^{*}}\\big(\\sigma(\\widehat{\\boldsymbol{w}}\\cdot\\boldsymbol{x})-\\boldsymbol{y}\\big)^{2}\\leq2\\mathbb{E}_{p^{*}}\\big(\\sigma(\\boldsymbol{w}^{*}\\cdot\\boldsymbol{x})-\\boldsymbol{y}\\big)^{2}+2\\mathbb{E}_{p^{*}}\\big(\\sigma(\\boldsymbol{w}^{*}\\cdot\\boldsymbol{x})-\\sigma(\\widehat{\\boldsymbol{w}}\\cdot\\boldsymbol{x})\\big)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\,0\\mathrm{PT}+2\\beta^{2}\\|\\widehat{\\boldsymbol{w}}-\\boldsymbol{w}^{*}\\|_{2}^{2}\\,\\mathbb{E}_{p^{*}}\\left(\\frac{\\widehat{\\boldsymbol{w}}-\\boldsymbol{w}^{*}}{\\|\\widehat{\\boldsymbol{w}}-\\boldsymbol{w}^{*}\\|}\\cdot\\boldsymbol{x}\\right)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\,0\\mathrm{PT}+2\\beta^{2}\\big(2C_{3}^{2}\\,0\\mathrm{PT}+2\\epsilon\\big)\\,5B}\\\\ &{\\qquad\\qquad\\leq\\big(2+20B\\beta^{2}C_{3}^{2}\\big)\\,\\,\\mathrm{OPT}\\,{+}10\\beta^{2}B\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "F.2  Risk Approximation ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Fix $\\hat{w}$ as output by Algorithm 1 and $\\pmb{w}^{*}$ as defined in Definition 1.2. Since we are bounding the population risk throughout this subsection, we write $R(\\pmb{w})=R(\\pmb{w};p_{0})$ in short. The goal of this subsection is to show ", "page_idx": 30}, {"type": "equation", "text": "$$\nR(\\hat{\\pmb w})-R(\\pmb w^{*})\\leq O(\\mathrm{OPT})+\\epsilon.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We first introduce some convex analysis results that we rely on in this subsection: ", "page_idx": 30}, {"type": "text", "text": "Fact F.2 (Strong convexity of chi-square divergence). Consider the space $\\mathcal P(p_{0})=\\{p:p\\ll p_{0}\\}$ For p E (po), we denote by apo the Radon-Nikodym derivative of $\\boldsymbol{\\mu}$ with respect to $\\mu_{0}$ , and we define $\\begin{array}{r}{\\|\\boldsymbol{p}\\|_{p_{0}}^{*}=\\sqrt{\\int(\\frac{\\mathrm{d}p}{\\mathrm{d}p_{0}})^{2}\\mathrm{d}p_{0}}}\\end{array}$ . Then $n\\mapsto\\chi^{2}({\\boldsymbol{p}},{\\boldsymbol{p}}_{0})$ is 2-strongly convex with respect to $\\|\\cdot\\|_{p_{0}}^{*}$ ", "page_idx": 31}, {"type": "text", "text": "FactE.3.Consider the space $\\mathcal{P}(p_{0})\\,=\\,\\{p\\,:\\,p\\,\\ll\\,p_{0}\\}$ .Denote by $\\langle\\cdot,\\cdot\\rangle_{p_{0}}$ the inner product $\\begin{array}{r}{\\langle\\ell_{1},\\ell_{2}\\rangle_{n_{0}}=\\int\\ell_{1}\\ell_{2}\\mathrm{d}p_{0}}\\end{array}$ and denote by $\\left\\Vert\\cdot\\right\\Vert_{\\mathcal{p}_{0}}$ thecorepondingnorm.Then $\\left\\|\\cdot\\right\\|_{\\mathcal{p}_{0}}$ is the dual norm of $\\lVert\\cdot\\rVert_{\\mu_{0}}^{*}$ defined in Fact $F.2$ ", "page_idx": 31}, {"type": "text", "text": "Definition F.4 (Convex conjugate). Given a convex function defined on a vector space $\\mathbb{E}$ denotedby $f:\\mathbb{E}\\rightarrow\\mathbb{R}$ , its convex conjugate is defined as: ", "page_idx": 31}, {"type": "equation", "text": "$$\nf^{*}(y)=\\operatorname*{sup}_{x\\in\\mathbb{E}}\\left(\\langle y,x\\rangle-f(x)\\right)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for all $y\\in\\mathbb{E}^{*}$ where $\\mathbb{E}^{*}$ is the dual space of $\\mathbb{E}$ and $\\langle y,x\\rangle$ denotes the inner product ", "page_idx": 31}, {"type": "text", "text": "Fact F.5 (Conjugate Correspondence Theorem, [Bec17, Theorem 5.26]). Let $\\nu>0$ $f:\\mathbb{E}\\rightarrow\\mathbb{R}$ is $\\nu$ -strongly convex continuous function, then its convex conjugate $f^{*}:\\mathbb{E}^{*}\\to\\mathbb{R}$ is -smooth. ", "page_idx": 31}, {"type": "text", "text": "We are then able to state and prove the key technical corollary in this subsection: ", "page_idx": 31}, {"type": "text", "text": "Corollary F.6. For any $\\mu_{0}$ -measurable function $\\ell:\\mathbb{R}^{d}\\times\\mathbb{R}\\to\\mathbb{R},$ let $\\begin{array}{r}{\\mathcal{R}(\\ell)=\\operatorname*{max}_{\\scriptstyle\\ p\\ll p_{0}}\\mathbb{E}_{\\scriptstyle p_{0}}\\ell-}\\end{array}$ $\\nu\\chi^{2}(\\boldsymbol{p},\\boldsymbol{p}_{0})$ . The function $\\mathcal{R}(\\cdot)$ is $1/(2\\nu)$ -smooth with respect to the norm $\\left\\|\\cdot\\right\\|_{\\mathcal{p}_{0}}$ defined in Fact F.3. ", "page_idx": 31}, {"type": "text", "text": "Proof. Observe by definition of the convex conjugate that $\\mathcal{R}(\\cdot)$ is theconvex conjugate of the function $\\nu\\chi^{2}\\grave{(}\\cdot,\\mu_{0})$ . Since the function $\\nu\\chi^{2}(\\cdot,\\eta_{0})$ .s\u00b7 $2\\nu$ stronglyconvexwithrespect tothenorm $\\lVert\\cdot\\rVert_{\\mu_{0}}^{*}$ by Fact F.2,it follows from Fact FE.5 that $\\mathcal{R}(\\cdot)$ .is $1/(2\\nu)$ -smooth withrespecto the norm $\\left\\|\\cdot\\right\\|_{\\mathcal{p}_{0}}$ \u53e3 ", "page_idx": 31}, {"type": "text", "text": "For ease of presentation, we define the following quantities: let $\\ell^{*}(\\pmb{x},y)=(\\sigma(\\pmb{w}^{*}\\cdot\\pmb{x})-y)^{2}$ and ${\\hat{\\ell}}(\\pmb{x},y)=(\\sigma(\\hat{\\pmb{w}}\\cdot\\pmb{x})-y)^{2}$ .We first compute $\\nabla_{\\boldsymbol{\\ell}}\\mathcal{R}({\\boldsymbol{\\ell}}^{*})$ by conjugate subgradient theorem. ", "page_idx": 31}, {"type": "text", "text": "Fact F.7 (Conjugate Subgradient Theorem [Bec17, Theorem 4.20]). Let $f:\\mathbb{E}\\rightarrow\\mathbb{R}$ beconvexand continuous. The following claims are equivalent for any $x\\in\\mathbb{E}$ and $y\\in\\mathbb{E}^{*}$ ", "page_idx": 31}, {"type": "equation", "text": "$$\nI.\\ \\left\\langle y,x\\right\\rangle=f(x)+f^{*}(y)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Corollary F.8. Let $\\boldsymbol{p}^{*}$ be as defined in Definition 1.2. Then $\\boldsymbol{p}^{*}\\in\\partial_{\\boldsymbol{\\ell}}\\mathcal{R}(\\boldsymbol{\\ell}^{*})$ ", "page_idx": 31}, {"type": "text", "text": "Proof. We verify that $\\begin{array}{r}{\\mathcal{R}(\\ell^{*})=\\operatorname*{max}_{\\boldsymbol{p}\\ll\\boldsymbol{p}_{0}}\\mathbb{E}_{\\boldsymbol{p}_{0}}[\\sigma(\\boldsymbol{w}^{*}\\cdot\\boldsymbol{x}-\\boldsymbol{y})^{2}]-\\nu\\chi^{2}(\\boldsymbol{p},\\boldsymbol{p}_{0})=\\mathbb{E}_{\\boldsymbol{p}^{*}}[\\sigma(\\boldsymbol{w}^{*}\\cdot\\boldsymbol{x}-\\boldsymbol{y})^{2}],}\\end{array}$ $y)^{2}]-\\nu\\chi^{2}(p^{*},p_{0})=\\mathbb{E}_{p^{*}}\\ell^{*}-\\nu\\chi^{2}(p^{*},p_{0})=\\langle p^{*},\\ell^{*}\\rangle-\\nu\\chi^{2}(p^{*},p_{0})$ where the second equality is the definition of $\\boldsymbol{p}^{*}$ and the third equality is the definition of $\\ell^{*}$ . By Fact F.7 and observing that $\\mathcal{R}(\\cdot)$ is the convex conjugate of the function $\\nu\\chi^{2}(\\cdot,\\eta_{0})$ , we have $\\boldsymbol{p}^{*}\\in\\partial_{\\boldsymbol{\\ell}}\\mathcal{R}(\\boldsymbol{\\ell}^{*})$ \u53e3 ", "page_idx": 31}, {"type": "text", "text": "Theorem F9. Suppose Corollary C.2 holds for both $\\pmb{w}^{*}$ and $\\hat{w}$ with respect to the population distribution.Then ", "page_idx": 31}, {"type": "equation", "text": "$$\nR(\\pmb{\\hat{w}};\\pmb{\\mathscr{p}}_{0})-R(\\pmb{w}^{*};\\pmb{\\mathscr{p}}_{0})\\leq C_{4}(\\mathrm{OPT}+\\epsilon),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $C_{4}=1+2(10B\\beta^{2}+c_{1})C_{3}+c_{1}\\sqrt{5B}\\beta^{2}C_{3}^{2}$ In particular, Corollary C.2 holds for both $\\pmb{w}^{*}$ and $\\hat{w}$ is satisfied under the assumptions in Theorem 3.1. ", "page_idx": 31}, {"type": "text", "text": "Proof. By the definition of smoothness, it holds that for any $\\begin{array}{r}{\\mathscr{p}\\in\\partial_{\\ell}\\mathscr{R}(\\ell^{*})}\\end{array}$ ,wehave ", "page_idx": 31}, {"type": "equation", "text": "$$\nR(\\hat{w};\\boldsymbol{p}_{0})-R(w^{*};\\boldsymbol{p}_{0})=\\mathcal{R}(\\hat{\\ell})-\\mathcal{R}(\\ell^{*})\\leq\\langle\\boldsymbol{p},\\hat{\\ell}-\\ell^{*}\\rangle+\\frac{1}{2\\nu}\\|\\hat{\\ell}-\\ell^{*}\\|_{\\boldsymbol{p}_{0}}^{2}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Hence it follows from Corollary F.8 that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad R(\\hat{w};p_{0})-R(w^{*};p_{0})\\leq\\langle p^{*},\\hat{\\ell}-\\ell^{*}\\rangle+\\displaystyle\\frac{1}{2\\nu}\\|\\hat{\\ell}-\\ell^{*}\\|_{p_{0}}^{2}}\\\\ &{=\\mathbb{E}_{p^{*}}[(\\sigma(\\hat{w}\\cdot x)-y)^{2}-(\\sigma(w^{*}\\cdot x)-y)^{2}]+\\displaystyle\\frac{1}{2\\nu}\\mathbb{E}_{p_{0}}[((\\sigma(\\hat{w}\\cdot x)-y)^{2}-(\\sigma(w^{*}\\cdot x)-y)^{2})^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We use the following shorthand for ease of presentation. ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\Psi(\\pmb{x},y)=\\sigma(\\pmb{w}^{\\ast}\\cdot\\pmb{x})-y,}\\\\ {\\Delta(\\pmb{x},y)=\\sigma(\\hat{\\pmb{w}}\\cdot\\pmb{x})-\\sigma(\\pmb{w}^{\\ast}\\cdot\\pmb{x}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad R(\\hat{w};p_{0})-R(w^{*};p_{0})\\leq\\langle p^{*},\\hat{\\ell}-\\ell^{*}\\rangle+\\displaystyle\\frac{1}{2\\nu}\\|\\hat{\\ell}-\\ell^{*}\\|_{p_{0}}^{2}}\\\\ &{=\\mathbb{E}_{p^{*}}\\big[(\\Delta+\\Psi)^{2}-\\Psi^{2}\\big]+\\displaystyle\\frac{1}{2\\nu}\\mathbb{E}_{p_{0}}\\big[((\\Delta+\\Psi)^{2}-\\Psi^{2})^{2}\\big]}\\\\ &{=\\mathbb{E}_{p^{*}}[\\Delta^{2}+2\\Delta\\Psi]+\\displaystyle\\frac{1}{2\\nu}\\mathbb{E}_{p_{0}}[\\Delta^{2}(\\Delta+2\\Psi)^{2}]}\\\\ &{\\leq\\mathbb{E}_{p^{*}}[\\Delta^{2}+2\\Delta\\Psi]+\\displaystyle\\frac{1}{\\nu}\\mathbb{E}_{p_{0}}[\\Delta^{2}(\\Delta^{2}+4\\Psi^{2})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the last inequality is the standard inequality $(a+b)^{2}\\leq2a^{2}+2b^{2}$ ", "page_idx": 32}, {"type": "text", "text": "Recall in Fact 2.4 that $\\mathbb{E}_{n^{*}}[\\Delta\\Psi]\\geq c_{0}\\|\\pmb{\\hat{w}}-\\pmb{w}^{*}\\|_{2}^{2}$ and $\\mathbb{E}_{p^{*}}[(\\pmb{x}\\cdot(\\pmb{\\hat{w}}-\\pmb{w}^{*}))^{\\tau}]\\leq5B\\|\\pmb{\\hat{w}}-\\pmb{w}^{*}\\|_{2}^{\\tau}$ From the second- and fourth-moment bounds, we have $\\mathbb{E}_{p^{*}}[\\Delta^{\\tau}]=\\mathbb{E}_{p^{*}}[(\\sigma(\\pmb{x}\\cdot\\pmb{\\hat{w}})-\\sigma(\\pmb{x}\\cdot\\pmb{w}^{*}))^{\\tau}]\\leq$ $\\beta^{\\tau}\\mathbb{E}_{p^{*}}[(\\pmb{x}\\cdot(\\pmb{\\hat{w}}-\\pmb{w}^{*}))^{\\tau}]\\leq5B\\beta^{\\tau}\\|\\pmb{\\hat{w}}-\\pmb{w}^{*}\\|_{2}^{\\tau}$ for $\\tau=2,4$ , where the second last inequality follows from $\\beta$ -Lipschitzness of $\\sigma(\\cdot)$ . Taking $\\tau=2$ gives us a bound for $\\mathbb{E}_{\\mu^{*}}[\\Delta^{2}]$ ", "page_idx": 32}, {"type": "text", "text": "For $\\mathbb{E}_{\\mathcal{P}^{*}}[\\Delta\\Psi]$ . it follows from Cauchy-Schwarz that $\\begin{array}{r l r}{\\mathbb{E}_{\\boldsymbol{p}^{*}}[\\Delta\\Psi]}&{\\le}&{\\sqrt{\\mathbb{E}_{\\boldsymbol{p}^{*}}[\\Delta^{2}]\\mathbb{E}_{\\boldsymbol{p}^{*}}[\\Psi^{2}]}\\quad\\le}\\end{array}$ $\\sqrt{5B\\beta^{2}\\,\\mathrm{OPT}}\\|\\hat{\\boldsymbol{w}}-\\boldsymbol{w}^{*}\\|_{2}$ ", "page_idx": 32}, {"type": "text", "text": "By Corollary C.2, we have $\\begin{array}{r}{\\mathbb{E}_{p_{0}}[\\Delta^{4}]\\leq2\\mathbb{E}_{p^{*}}[\\Delta^{4}]\\leq5B\\beta^{4}\\|\\hat{\\boldsymbol{w}}-\\boldsymbol{w}^{*}\\|_{2}^{4}.}\\end{array}$ ", "page_idx": 32}, {"type": "text", "text": "Finally, similarly by Corollary C.2, it follows additionally from Cauchy-Schwarz that $\\mathbb{E}_{\\mu_{0}}[\\Delta^{2}\\Psi^{2}]\\leq$ $2\\mathbb{E}_{p^{*}}[\\Delta^{2}\\Psi^{2}]\\leq2\\sqrt{\\mathbb{E}_{p_{0}}[\\Delta^{4}]\\mathbb{E}_{p^{*}}[\\Psi^{4}]}\\leq2\\|\\hat{\\pmb{w}}-\\pmb{w}^{*}\\|_{2}^{2}\\sqrt{5B\\beta^{4}\\,\\mathrm{OPT}_{(2)}}$ .ByTheorem 3.1, we have $\\nu\\ge8\\beta^{2}\\sqrt{6B}\\sqrt{\\mathrm{OPT}_{(2)}+\\epsilon}/c_{1}$ by assumption,hence $4\\mathbb{E}_{p_{0}}[\\Delta^{2}\\Psi^{2}]/\\nu\\leq c_{1}\\|\\pmb{\\hat{w}}-\\pmb{w}^{*}\\|_{2}^{2}$ ", "page_idx": 32}, {"type": "text", "text": "Combining the above four bounds and the guarantee of Theorem 3.1 that $\\lVert\\hat{\\pmb{w}}\\ -\\ \\pmb{w}^{*}\\rVert_{2}^{2}\\ \\leq$ $2C_{3}\\,\\mathrm{OPT}\\,{+}2\\epsilon$ wehave ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad R(\\hat{\\boldsymbol w};\\boldsymbol{\\mu}_{0})-R(\\boldsymbol{w}^{*};\\boldsymbol{p}_{0})}\\\\ &{\\leq5B\\beta^{2}\\|\\hat{\\boldsymbol w}-\\boldsymbol{w}^{*}\\|_{2}^{2}+2\\sqrt{5B\\beta^{2}\\operatorname{OPT}}\\|\\hat{\\boldsymbol w}-\\boldsymbol{w}^{*}\\|_{2}+c_{1}\\|\\hat{\\boldsymbol w}-\\boldsymbol{w}^{*}\\|_{2}^{2}+5B\\beta^{4}\\|\\hat{\\boldsymbol w}-\\boldsymbol{w}^{*}\\|_{2}^{4}/\\nu}\\\\ &{\\leq\\operatorname{OPT}+(10B\\beta^{2}+c_{1}+5B\\beta^{4}\\|\\hat{\\boldsymbol w}-\\boldsymbol{w}^{*}\\|_{2}^{2}/\\nu)\\|\\hat{\\boldsymbol w}-\\boldsymbol{w}^{*}\\|_{2}^{2}}\\\\ &{\\leq\\operatorname{OPT}+2(10B\\beta^{2}+c_{1})(C_{3}\\operatorname{OPT}+\\epsilon)+40B\\beta^{4}(C_{3}^{2}\\operatorname{OPT}^{2}+\\epsilon^{2})/\\nu.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By Corollary C.9, OPT $\\begin{array}{r l}{\\leq}&{{}\\sqrt{\\mathrm{OPT}_{(2)}}}\\end{array}$ ,hence v \u2265 8\u03b22\u221a6B\u221aOPT(2)+e/c \u2265 $8\\beta^{2}\\sqrt{6B}\\operatorname*{max}\\{\\mathrm{OPT},\\sqrt{\\epsilon}\\}/c_{1}$ hence ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad R(\\hat{w};p_{0})-R(w^{*};p_{0})}\\\\ &{\\leq\\mathrm{OPT}+2(10B\\beta^{2}+c_{1})(C_{3}\\,\\mathrm{OPT}+\\epsilon)+c_{1}\\sqrt{5B}\\beta^{2}(C_{3}^{2}\\,\\mathrm{OPT}^{2}+\\epsilon^{2})/\\operatorname*{max}\\{\\mathrm{OPT},\\sqrt{\\epsilon}\\}}\\\\ &{\\leq\\mathrm{OPT}+2(10B\\beta^{2}+c_{1})(C_{3}\\,\\mathrm{OPT}+\\epsilon)+c_{1}\\sqrt{5B}\\beta^{2}(C_{3}^{2}\\,\\mathrm{OPT}+\\epsilon^{1.5})}\\\\ &{=(1+2(10B\\beta^{2}+c_{1})C_{3}+c_{1}\\sqrt{5B}\\beta^{2}C_{3}^{2})\\,\\mathrm{OPT}+(2(10B\\beta^{2}+c_{1})+c_{1}\\sqrt{5B}\\beta^{2})\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: The abstract and introduction clearly outline the open problem we provide the first results for, clearly state the assumptions involved (cf. Section 1.1 \u2014 Problem Setup and Section 1.2 \u2014 Main Result), and give a detailed technical overview of our algorithm (cf. Section 1.3 \u2014 Technical Overview). ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and refect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: The limitations are clearly stated in the statements of each theorem and are discussed in the introduction of the paper and description of the key lemma (Lemma 3.4). ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 33}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: Each theorem statement provides all the assumptions and we provide complete proofs for all statements that are either in the main body of the paper or in the appendix. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper is theoretical in nature and does not include experiments. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b)  If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper is theoretical in nature and does not include experiments. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https : / /nips . CC / public/guides /CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (ht tps : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper is theoretical in nature and does not include experiments. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper is theoretical in nature and does not include experiments. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper is theoretical in nature and does not include experiments. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPs Code of Ethics https: / /neurips.cc/public/EthicsGuidelines? Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our research conforms in every respect with the NeurIPs Code of Ethics. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The work is theoretical and we do not see any major or immediate implications Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The work is theoretical. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: This paper does not use existing assets. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswit hcode . com/ dataset s has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 37}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 38}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)wereobtained? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 38}]