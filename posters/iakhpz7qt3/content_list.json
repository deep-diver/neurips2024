[{"type": "text", "text": "Scaling Retrieval-Based Language Models with a Trillion-Token Datastore ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Rulin Shao1 Jacqueline He1 Akari Asai1 Weijia Shi1 Tim Dettmers1 Sewon Min1 Luke Zettlemoyer1 Pang Wei Koh1,2 ", "page_idx": 0}, {"type": "text", "text": "1University of Washington 2Allen Institute for AI {rulins,jyyh,akari,swj0419,dettmers,sewon,lsz,pangwei} @cs.washington.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Scaling laws with respect to the amount of training data and the number of parameters allow us to predict the cost-benefit trade-offs of pretraining language models (LMs) in different configurations. In this paper, we consider another dimension of scaling: the amount of data available at inference time. Specifically, we find that increasing the size of the datastore used by a retrieval-based LM monotonically improves language modeling and several downstream tasks without obvious saturation, such that a smaller model augmented with a large datastore outperforms a larger LM-only model on knowledge-intensive tasks. By plotting compute-optimal scaling curves with varied datastore, model, and pretraining data sizes, we show that using larger datastores can significantly improve model performance for the same training compute budget. We carry out our study by constructing a 1.4 trilliontoken datastore named MASSIVEDS, which is the largest and the most diverse open-sourced datastore for retrieval-based LMs to date, and designing an efficient pipeline for studying datastore scaling in a computationally accessible manner. Finally, we analyze the effect of improving the retriever, datastore quality flitering, and other design choices on our observed scaling trends. Overall, our results show that datastore size should be considered as an integral part of LM efficiency and performance trade-offs. To facilitate future research, we open-source our datastore and code at https://github.com/RulinShao/retrieval-scaling. ", "page_idx": 0}, {"type": "image", "img_path": "iAkhPz7Qt3/tmp/e81089eb4d51cbdb76fbf7b716a45c46a39fe166ab0c5437f1600f3a89275e2f.jpg", "img_caption": ["Figure 1: Datastore scaling improves language modeling and downstream task performance. Left: Datastore scaling performance on language modeling and a downstream task (MMLU) with LLAMA-2 and LLAMA-3 models. Right: Compute-optimal scaling of retrieval-based language models vs. LM-only models with PYTHIA models. By considering the size of the datastore as an additional dimension of scaling, we can improve model performance at lower training cost. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The scaling of large language models (LMs) has driven tremendous performance gains across a variety of tasks (Brown et al., 2020; Kaplan et al., 2020; Muennighoff et al., 2023). Current scaling laws are primarily a function of the size of the pretraining data and the number of parameters (Hoffmann et al., 2022; Muennighoff et al., 2023; Gadre et al., 2024). In this paper, we consider another dimension of scaling: the amount of data in a datastore used at inference time by retrieval-based LMs, which can directly retrieve information from the datastore to use in context when generating output (Karpukhin et al., 2020; Guu et al., 2020; Izacard & Grave, 2020; Asai et al., 2024b). ", "page_idx": 1}, {"type": "text", "text": "Retrieval-based LMs have a range of benefits such as improved factuality (Mallen et al., 2023), effective domain adaptation (Khandelwal et al., 2020), credit attribution (Gao et al., 2023), and parametric efficiency (Min et al., 2023b). However, most prior work in retrieval-based LMs use datastores constructed from a single data source (Karpukhin et al., 2020), such as Wikipedia, with sizes on the order of a few billion tokens. While there has been some work on larger datastores (Table 1), with the largest being RETRO (Borgeaud et al., 2022; Wang et al., 2024) in the trillion-token range, these studies use proprietary datastores and custom architectures with a limited evaluation suite. As such, it remains unknown how datastore scaling helps the currently dominant retrieval-in-context approaches on a broad categories of tasks. ", "page_idx": 1}, {"type": "text", "text": "We first construct MASSIVEDS, a massively multi-domain datastore comprising 1.4 trillion tokens of both general web data and domain specific data (\u00a73.1) that serves as the cornerstone for our scaling study. A key challenge in studying datastore scaling is the computational cost introduced by building datastores with all possible combinations of factors such as the datastore scale, data composition, random seed for subsampling, and different data preprocessing methods. To make our study accessible, we design an efficient datastore construction pipeline that reduces the compute needed by an order of magnitude while being equivalent to the standard pipeline (\u00a73.2). ", "page_idx": 1}, {"type": "text", "text": "Using the proposed pipeline, we systematically evaluate the effects of scaling MASSIVEDS on retrieval-based LMs with varying numbers of parameters and pretraining tokens (\u00a74). Beyond upstream language modeling, we also consider a suite of diverse downstream tasks, including generalknowledge question answering (QA), domain-specialized QA, and reasoning tasks. We find that, first, datastore scaling consistently improves both language modeling and some downstream tasks in a task-dependent manner (Figure 1 Left), much like the widely observed data and parameter scaling trends. In fact, on knowledge-intensive tasks, a small retrieval-based LM can outperform its larger LM-only counterparts. Second, since indexing a datastore is cheaper than training on the same amount of data, retrieval-based LMs enable better compute-optimal scaling trends, where they achieve superior performance than LM-only models at the same training cost (Figure 1 Right). ", "page_idx": 1}, {"type": "text", "text": "Through our analyses (\u00a75), we show that retrieval-based LMs are capable of automatically retrieving documents that are in-domain to the query, which allows them to reap the beneftis of larger, broader datastores. In addition, data quality filters and improved retrieval methods can further enhance our observed scaling trends. ", "page_idx": 1}, {"type": "text", "text": "Overall, our results show that datastore size should be considered as an integral part of LM efficiency and performance trade-offs. To spur future research, we open-source MASSIVEDS (including the raw passages, the embedding, and the index) and all code (including our evaluation suite and pipeline for efficiently studying datastore scaling) at https://github.com/RulinShao/retrieval-scaling. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Retrieval-based LMs. Unlike parametric LMs that only use data during training, retrieval-based LMs can access data through a datastore during inference (see Asai et al. (2024b) for a review). We focus on retrieve-in-context language models (RIC-LMs), which retrieves a small set of documents from the datastore and feeds a concatenation of them as an input to the LM (Ram et al., 2023; Shi et al., 2023). The RIC-LM approach is simple and allows the use of off-the-shelf retrievers and LMs, even with only black-box access. ", "page_idx": 1}, {"type": "text", "text": "Scaling the retrieval datastore. Prior work on retrieval-based LMs often focused on specific aspects of LMs such as factuality and attribution. In addition, they typically use limited-size, single-domain datastores such as Wikipedia, which is on the order of a few billion tokens (Table 1). Scaling the datastore remains relatively underexplored, with two notable exceptions. First, Borgeaud et al. (2022) proposed a new RETRO transformer architecture for retrieval-based LMs and built a 1.75 trillion token datastore sourced from the proprietary data introduced in Rae et al. (2022). However, RETRO and its follow-up work, $\\mathrm{RETRO++}$ (Wang et al., 2023) and INSTRUCTRETRO (Wang et al., 2024), use this trillion-token datastore solely for language modeling evaluation, while using a small task-specific Wikipedia datastore for downstream task evaluation. Since RETRO-based datastores are not fully open-sourced, it is challenging to replicate work based on RETRO to assess the effectiveness of datastore scaling. Second, Piktus et al. (2022) proposed SPHERE, an open-sourced 90 billion-token datastore curated from CCNet (Wenzek et al., 2020). However, their evaluation on downstream tasks such as KILT (Petroni et al., 2021) suggests that SPHERE does not always outperform a small, in-domain datastore like Wikipedia. ", "page_idx": 1}, {"type": "table", "img_path": "iAkhPz7Qt3/tmp/b4fdd7a090b99fbfe038003613c80b793c850f270fe3f5378f17da8b2271aae4.jpg", "table_caption": ["Table 1: Comparison with prior work, ordered by datastore size. \u2018# Tokens\u2019 indicates the number of tokens in the datastore using the LLAMA2 tokenizer (Touvron et al., 2023). The asterisk $(^{*})$ denotes that the datastore is not evaluated on downstream tasks. MASSIVEDS is the largest open-sourced datastore and covers a broad spectrum of domains. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "In contrast, we present the first study on the downstream performance of trillion-token scale datastores, including an analysis of compute-optimal scaling trends using retrieval-based LMs with different sizes of datastores, models, and pretraining corpora. Our work is fully open-source and can be replicated on a limited computational budget, enabling research on trillion-token datastores to be more broadly accessible. ", "page_idx": 2}, {"type": "text", "text": "3 MASSIVEDS and our Datastore Scaling Pipeline ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 MASSIVEDS: A Trillion-Token Datastore With a Diverse Domain Composition ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "MASSIVEDS encompasses both domainspecific data and general web data (Table 2). Domain-specific data comes from specialized sources, and tends to be smaller but higher in quality. We cull from a mix of datarich domains: books which span a variety of genres (Computer, 2023); open-access scientific papers (Lo et al., 2020; Soldaini & Lo, 2023; Computer, 2023); encyclopedic articles (Karpukhin et al., 2020; Computer, 2023); community questions and answers from StackExchange (Computer, 2023); code from GitHub (Computer, 2023); mathematical webpages (Paster et al., 2023) and mathematical language (Welleck et al., 2021); biomedical articles (of Medicine, 2023). On the other hand, general web data ", "page_idx": 2}, {"type": "text", "text": "Table 2: The domain-wise data composition of MASSIVEDS. RPJ denotes REDPAJAMA V1 (Computer, 2023), CC denotes Common Crawl, Wiki denotes Wikipedia. ", "page_idx": 2}, {"type": "table", "img_path": "iAkhPz7Qt3/tmp/32c9e3aaa22d5921ec685f4ede85183f4e45ec25bf2d1cc19985f8269b2f31a1.jpg", "table_caption": [], "table_footnote": ["is sourced from Common Crawl snapshots at five time periods (07/2019, 05/2020, 04/2021, 05/2022, 06/2023) and C4 (Raffel et al., 2020). "], "page_idx": 2}, {"type": "image", "img_path": "iAkhPz7Qt3/tmp/f783d018c0ab38a1b13792032dab9a31e9aec72d3bb801d15b29ab149af3370d.jpg", "img_caption": ["Figure 2: Comparison between the MASSIVEDS pipeline and a naive pipeline for studying datastore scaling (\u00a73.2). The green and red arrows indicate repeated operations. In the naive pipeline, these operations are more expensive because they require repeating expensive steps such as rebuilding the datastore index. In the MASSIVEDS pipeline, these operations are cheaper because they repeat fewer steps and are only run on the retrieved top-K documents instead of the full datastore. Datastorelevel operations are represented by purple arrows, while document-level operations, repeated for every query, are represented by black arrows. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.2 Studying Datastore Scaling with the MASSIVEDS Pipeline ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Studying datastore scaling requires constructing datastores of varying sizes and varying compositions from the raw text corpus. This involves the following operations: data flitering, including deduplication, decontamination, and quality fliters (Soldaini et al., 2024); data subsampling, which randomly subselects a $p$ -fraction of the text corpus to achieve the specified size; indexing, which embeds the data using an auxiliary model and builds a searchable index; document retrieval, which uses the index to find the top- $k$ relevant documents for each test query1; and top- $k$ evaluation, which uses the top- $k$ documents per test query to augment the retrieval-based model. A naive approach is to run these operations in the aforementioned order for each datastore, and build separate datastores for all combinations of subsampled datastore sizes, random seeds, and other experimental variations. However, this naive approach is prohibitively expensive2 at the trillion-token datastore scale because it repeats expensive operations, as shown in Figure 2 (top). ", "page_idx": 3}, {"type": "text", "text": "To make the datastore scaling study feasible, we develop the MASSIVEDS pipeline (Figure 2 bottom). The key idea is to reorder the above operations such that the most expensive ones\u2014indexing and retrieval\u2014are run only once at the start and then shared across all subsequent datastore variants. Other operations with many variants\u2014such as subsampling, deduplication, and decontamination\u2014are performed as late as possible to minimize repeating subsequent steps. To enable this, we first retrieve a relatively large number $[K\\gg k]$ ) of documents for each query and then apply the subsequent operations to these sets of retrieved documents, rather than the entire datastore. Altogether, this pipeline reduces compute, I/O, and storage requirements by more than an order of magnitude, enabling us to conduct the datastore scaling study on a modest compute budget. In Appendix A.5, we show that the results from the MASSIVEDS pipeline are equivalent to the results from the naive pipeline with high probability, where the randomness comes from the subsampling procedure. We provide more details on the steps in the MASSIVEDS pipeline in Appendix A and detailed configuration in Appendix B.1. ", "page_idx": 3}, {"type": "text", "text": "Note on the number of tokens in the datastore. In our figures, we plot the datastore size (on the $\\mathbf{X}$ -axis) by multiplying the total number of tokens in the raw data pool by the subsampling fraction $p$ A more accurate representation would be the number of tokens in the flitered data pool; however, we do not know the size of the filtered data pool as we apply data filtering on the retrieved documents instead of the raw data for computational efficiency. As a result, the number of tokens we plot on our ${\\bf X}$ -axis is proportionally larger, i.e., if a $p_{f}$ fraction of the data is filtered out ( $[0<p_{f}\\le1]$ ), then the actual number of tokens should also be multiplied by $p_{f}$ . Since we plot datastore size on a log axis, this corresponds to a constant shift and does not change the scaling trends. ", "page_idx": 3}, {"type": "image", "img_path": "iAkhPz7Qt3/tmp/56b7f323fd67348375c312b067eb8188b01200b901e2fda1099be211750f2e37.jpg", "img_caption": ["Figure 3: Scaling performance on upstream and downstream tasks with MASSIVEDS, in comparison with LM-only performance. Left: Perplexity (PPL) scaling performance on REDPAJAMA (multi-domain pretraining corpus) and S2ORC (scientific papers). Right: Downstream scaling performance on TriviaQA (TQA), Natural Questions (NQ), MMLU, and MedQA. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "4 Datastore Scaling with Retrieval-Based Language Models ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Model details. Following prior work (Izacard et al., 2023; Liu et al., 2023; Ram et al., 2023; Shi et al., 2024; Xu et al., 2023; Asai et al., 2024a), we use CONTRIEVER-MSMARCO (Izacard et al., 2022), which represents every document in the datastore as a dense vector, as our retriever. We ablate the choice of retriever in Appendix E.1; we found that CONTRIEVER-MSMARCO performs on par with, or even better than, more recent larger retrievers. We augment input examples with retrieved documents at the granularity of 256-word chunks. We study datastore scaling performance with the LLAMA-2, LLAMA-3 (Touvron et al., 2023), PYTHIA (Biderman et al., 2023), and OLMO (Groeneveld et al., 2024) model families. ", "page_idx": 4}, {"type": "text", "text": "Evaluation. We consider both language modeling and downstream tasks for evaluation. We evaluate language modeling perplexity on data from two domains: (1) general web data sampled from REDPAJAMA (Computer, 2023); (2) scientific paper data sampled from S2ORC (Lo et al., 2020). For downstream tasks, our evaluation encompasses general-knowledge, medical, math, and science domains including the following tasks. TriviaQA (TQA; Joshi et al. 2017) comprises trivia questions with answers sourced from Wikipedia and the web. Natural Questions (NQ; Kwiatkowski et al. 2019; Lee et al. 2019) comprises search engine queries and human-annotated answers from Wikipedia. Massive Multitask Language Understanding (MMLU; Hendrycks et al. 2021) comprises general-purpose, multi-task reasoning questions. MedQA (Jin et al., 2020) comprises medical multiple-choice questions sourced from professional medical exams. ", "page_idx": 4}, {"type": "text", "text": "Implementation details. For evaluation with retrieval, we concatenate the top $k=3$ documents in reverse order, so that higher-ranked documents are positioned closer to the query. For downstream tasks, we evaluate models via 5-shot prompting, and we prepend the retrieved documents before the few-shot examples, followed by the question. We do not apply reranking for our main experiments in Section 4; we study the effect of rerankers in Section 5.2. More details, including decontamination measures, are in Appendix B. ", "page_idx": 4}, {"type": "text", "text": "4.2 Datastore Scaling Results on Language Modeling and Downstream Tasks ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Finding 1: Datastore scaling significantly helps language modeling. Figures 3(a) and (b) show perplexity curves as a function of datastore size on general web and scientific papers, respectively. Retrieval is strictly beneficial for language modeling: the LM-only baselines (denoted by dashed lines) show the highest perplexity across all models and evaluation datasets. Scaling up the datastore reduces perplexity without signs of saturation, suggesting that further scaling is likely to yield additional improvements. Further, datastore scaling enables small models to outperform their larger LM-only counterparts: when retrieving from MASSIVEDS at the largest scale, LLAMA-2 7B outperforms the LM-only performance of its larger LLAMA-2-13B counterpart. Interestingly, we find LLAMA-3 8B underperforms LLAMA-2 7B on RedPajama. This aligns with the observations in Xiao et al. (2023) and we discuss potential reasons in Appendix D. ", "page_idx": 4}, {"type": "image", "img_path": "iAkhPz7Qt3/tmp/404072692d9899c3747d26da3e93e91c1515eec66c68237684057a538733151c.jpg", "img_caption": ["Figure 4: Compute-optimal scaling curves for retrieval-based and LM-only models of varying datastore sizes, model sizes, and pretraining corpus sizes (detailed setup in $\\S\\mathbf{B}.\\mathbf{4})$ ). Darker green or pink indicate larger model sizes for PYTHIA and OLMO respectively; crossmarks in matching colors represent the same model size trained with varying numbers of tokens; each crossmark corresponds to a datastore scaling curve of lined dots similar to the ones in Figure 3. The Pareto-optimal points are highlighted in red for retrieval-based LMs and blue for LM-only. Within a fixed computational budget (represented on the $\\mathbf{X}$ -axis), retrieval-based LMs achieve superior performance, which remains unsaturated along the datastore scaling dimension. Pythia models do not exhibit meaningful scaling curves on MMLU and MedQA that require advanced reasoning abilities. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Finding 2: Datastore scaling improves performance on several downstream tasks, and the degree of improvement is task-dependent. Figure 3(c)\u2013(f) show the performance on four downstream tasks as a function of datastore size. Datastore scaling brings major improvements to knowledgeintensive question answering tasks such as NQ and TQA, where retrieval-based LMs significantly outperform LM-only baselines across all scales, and performance monotonically increases with datastore scale. For instance, a LLAMA-2 7B model that retrieves from fewer than 100B tokens can outperform both its 13B LM-only counterpart and the more capable LM-only LLAMA-3 8B on TQA and NQ, indicating the effectiveness of storing knowledge in the datastore. ", "page_idx": 5}, {"type": "text", "text": "On MMLU, a multi-subject, reasoning-heavy benchmark, datastore scaling monotonically improves performance across all model scales. Results are more mixed for MedQA, where only the weaker LLAMA-2 7B benefits more from datastore scaling. For both tasks, datastore scaling does not help the smaller model do better than the larger model. We suspect that this is due to task difficulty and the lack of in-domain data sources: both MMLU and MedQA are more oriented toward reasoning rather than pure factual recall, which poses bigger challenges for both the retriever and the LM. Additionally, MASSIVEDS only contains a small subset of web data and medical papers which may not cover all necessary information to answer these questions. We defer to future work to explore better data sources for these tasks. ", "page_idx": 5}, {"type": "text", "text": "4.3 Compute-Optimal Scaling with Retrieval-Based Language Models ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Next, we study performance as a function of total training-time compute and show that retrieval-based LMs achieve superior compute-optimal performance compared to LM-only models. ", "page_idx": 5}, {"type": "text", "text": "Use of intermediate checkpoints. We use the intermediate checkpoints of PYTHIA and OLMO as an approximation of models trained on different numbers of tokens, as detailed in Appendix B.4. These intermediate checkpoints share the same learning rate scheduler, with a fixed maximum number of training steps that equals or exceeds the number of steps they have been actually trained for, and therefore the performance of these intermediate checkpoints (with or without retrieval) might be lower than otherwise attainable with the same amount of compute. However, pretraining LMs from scratch for all combinations of model sizes and numbers of pretraining tokens is prohibitively expensive for an academic budget. ", "page_idx": 6}, {"type": "text", "text": "FLOPs calculation. We detail the FLOPs computation for datastore construction and pretraining in Appendix B.4. Datastore construction is much cheaper than pretraining because it only requires one forward pass on all tokens in the datastore with a small retriever (177M parameters in our setup), while pretraining requires a forward pass and a backward pass on pretraining tokens with an LM that can be much larger than the retriever. As we use a flat index, no additional operations are required at the indexing step, so the number of FLOPs for datastore construction equals the number of FLOPs for embedding. We note that other types of indexing, e.g., inverted file indexing (IVFADC) (J\u00e9gou et al., 2011), may require additional FLOPs during construction and fewer FLOPs at inference. We first focus on training-time compute and discuss inference cost at the end of the section. ", "page_idx": 6}, {"type": "text", "text": "We show the scaling curves against computational cost of retrieval-based LMs and LM-only performance on downstream tasks in Figure 4. The Pareto-optimal points for retrieval-based and LM-only settings are highlighted in red and blue, respectively. ", "page_idx": 6}, {"type": "text", "text": "Finding 3: Retrieval-based LMs outperform LM-only models for the same compute budget. With the same training-time compute, retrieval-based LMs achieves superior performance than LMonly models, indicating offloading FLOPs from pretraining to datastore construction can result in better performance. Therefore, we conjecture that storing factual knowledge in a datastore is more computationally efficient than memorizing factual knowledge in model parameters at training time. We note this claim assumes the LM has enough capacity to reason with the retrieved knowledge. Otherwise, an LM may fail to utilize the retrieved knowledge, which we further discuss in Finding 5. ", "page_idx": 6}, {"type": "text", "text": "Finding 4: Even weak language models can benefit significantly from retrieval on knowledgeintensive tasks that measure factual recall. Surprisingly, we find that retrieval-based PYTHIA models (trained on up to 300B tokens) and OLMO-1.7 models (trained on up to 2T tokens3) have a similar compute-optimal scaling trajectory on TriviaQA and NQ (left columns in Figure 4), despite PYTHIA being trained on less and lower-quality data. Both TriviaQA and NQ evaluate factual recall without complex reasoning. When the right information is provided in context using retrieval, the LM only needs to extract the answer; therefore, these results suggest that the ability to extract factual knowledge for simple factual question answering is obtained early in training. ", "page_idx": 6}, {"type": "text", "text": "Finding 5: Retrieval shows benefits for reasoning-intensive tasks with capable OLMO models, but it does not help when the language model is not sufficiently advanced such as PYTHIA. As shown on the right side of Figure 4, datastore scaling gives marginal benefits on MMLU and MedQA for PYTHIA models where the performance stays around random even at the 12B model size. However, OLMO, which is trained on more and better data, consistently benefits from retrieval on both tasks. We thus conjecture that training on higher-quality data, as OLMO applied in pretraining, could help the model benefit more from retrieved documents for reasoning-heavy tasks. Beyond reasoning ability, access to the right data sources for the datastore might be critical. For example, we observe fewer beneftis from retrieval on MMLU and MedQA in comparison with TriviaQA and NQ. This may indicate that MMLU and MedQA need more specific data, such as relevant textbooks for MMLU and biomedical literature for MedQA, which are currently not included in MASSIVEDS. ", "page_idx": 6}, {"type": "text", "text": "Discussion on inference cost. The compute-optimal scaling study described above focuses only on the cost of training. For inference, prepending retrieved documents in context increases inference cost due to the extended context length and additional computation required for the search. On the flip side, inference cost can be reduced by switching from a larger to a smaller LM, especially since a small LM augmented with a datastore can match or outperform its larger counterparts on some tasks. We also note that there is emerging work on accelerating retrieval search and designing efficient serving strategies for retrieval-based LMs, such as Cao et al. (2023). We leave a study of inference-compute-optimal scaling to future work. ", "page_idx": 6}, {"type": "table", "img_path": "iAkhPz7Qt3/tmp/302f44b81770d39a14510cf1f0d3eeaa8feaee713a4c4f99dd428c5b28d3d39f.jpg", "table_caption": ["Table 3: Downstream and upstream performance comparison between MASSIVEDS for retrieval versus single-domain datastores with LLAMA-2 7B. \u201cSE\u201d is short for StackExchange. The best performance is highlighted in bold and the second best is underlined. We show the diverse domain coverage in MASSIVEDS consistently improve the performance across tasks. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5 Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "5.1 Effects of Data Composition ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Finding 6: MASSIVEDS matches or outperforms single-domain datastores. The default setting in prior work is to use a single-domain datastore that is in-distribution to the downstream task. In practice, however, it is often difficult to determine and curate a datastore that is perfectly indistribution for a downstream task, and even if we can, it limits the generality of the retrieval-based model to that task. ", "page_idx": 7}, {"type": "text", "text": "In Table 3, we compare MASSIVEDS with single-domain datastores. MASSIVEDS significantly outperforms these in-domain datastores on language modeling, as well as TQA and MMLU, while matching performance on NQ and MedQA.4 In Figure 5, we show that the retriever tends to retrieve from the relevant domain even in the presence of out-of-domain data in the datastore: for NQ, it retrieves relatively more frequently from Wikipedia and web sources, whereas for MedQA, it retrieves more frequently from scientific papers from peS2o (Soldaini & Lo, 2023). Thus, the retriever can maintain robustness to out-of-domain data in the datastore; this aligns with similar findings on kNN-LM (Khandelwal et al., 2020), another type of retrieval-based LM, in Shao et al. (2023). Overall, these results show that retrieving from broad datastores like MASSIVEDS can simultaneously improve performance across multiple domains, paving the path towards generalpurpose retrieval-based models. ", "page_idx": 7}, {"type": "image", "img_path": "iAkhPz7Qt3/tmp/4cd7eb6095e4cf34144f695314944d7bafa184ce34beb76be89ae440ce3056f9.jpg", "img_caption": ["Figure 5: Retrievers tend to retrieve from relevant domains. We plot the domain composition of MASSIVEDS vs. the top-1 retrieved documents for evaluation examples from MedQA and NQ. The retriever retrieves more frequently from domains that are relevant to the evaluation examples. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.2 Effects of Reranking ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Retrieving the most relevant documents from a large-scale datastore remains a challenging problem. To study how improving the retrieval process impacts datastore scaling trends, we first retrieve 500 documents from CONTRIEVER, rerank them using a more computationally expensive model (Ram et al., 2023; Sachan et al., 2022), and take the final top-3 reranked documents for evaluation. Specifically, we use a cross-encoder reranker, which encodes a concatenation of a query and document and returns a similarity score (Nogueira & Cho, 2019). We choose MINI-LM-L12 V2, a BERT-based cross-encoder5 that is trained for passage ranking, following Izacard et al. (2022). Additionally, we use a lexical oracle reranker, which uses the gold answer, as an upper bound on the potential benefti realizable by a better reranker or retriever for knowledge-intensive question-answering. The oracle reranker scores each document based on whether the gold answer is included in the document and if not, the fraction of unigram overlap between the document and the answer. ", "page_idx": 7}, {"type": "image", "img_path": "iAkhPz7Qt3/tmp/16738d828d78f41c50b3c650dd8d2019b45440913215abb8100b932cae594223.jpg", "img_caption": ["Figure 6: Scaling trends on TriviaQA and NaturalQuestions using different rerankers (Section 5.2). \u201cLexical Oracle\u201d represents the oracle reranker that reorders documents based on lexical overlap with the ground-truth answer. \u201cCross-encoder\u201d represents a neural reranker which uses a cross-encoder model. Both the oracle lexical reranker and the neural reranker boost scaling trends, indicating the potential improvement space by enhancing the retrieval quality. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "iAkhPz7Qt3/tmp/8e259b9d8187503e0ef063546f2f6989228d78763c17de1828ee1dd54ca7b9c3.jpg", "img_caption": ["Figure 7: Ablation study on data decontamination. \u2018Aggressive Decon.\u2019 removes a document as long as it has an 8-gram (i.e., $1.5\\%$ of the answer length) continuous overlap with the answer. \u2018Standard Decon.\u2019\u2014our default setup\u2014removes a document when it either has a 32-gram (i.e., $6.2\\%$ of the answer length) continuous overlap or an $80\\%+$ Jaccard similarity with the answer. We find decontamination impacts the language modeling performance a lot but not the downstream task. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Figure 6 reports scaling trends on TQA and NQ of RIC-LM with Llama2-7B. While the crossencoder-based reranker improves performance on TQA and NQ, a notable gap persists between the oracle reranker and the cross-encoder-based reranker. These suggest that improving either retrieval or reranking can further boost the scaling performance of retrieval datastores. Improving the retriever for more reasoning-heavy tasks such as MMLU and MedQA remains an open problem (BehnamGhader et al., 2022) that we leave to future work. ", "page_idx": 8}, {"type": "text", "text": "5.3 Effects of Datastore Filtering ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Data decontamination. Data decontamination is a crucial concern when evaluating LMs, especially in retrieval-based LMs that can retrieve the test data verbatim during inference (Borgeaud et al., 2022). By default (Section 4), we perform decontamination by filtering documents with $80\\!+\\!\\%$ 13-gram ", "page_idx": 8}, {"type": "text", "text": "Jaccard similarity for downstream tasks and 32-gram longest sequence overlap for perplexity, which we call standard decontamination. Prior work such as RETRO (Borgeaud et al., 2022) only used $80\\!+\\!\\%$ 13-gram Jaccard similarity for decontamination. However, we find the additional 32-gram longest sequence overlap decontamination is critical for removing near-identical documents. ", "page_idx": 9}, {"type": "text", "text": "To study the impact of varying levels of data decontamination, we compare standard decontamination with two additional methods: (1) no decontamination and (2) aggressive decontamination, which uses 8-gram longest sequence overlap for all tasks. This is a strict filter, as 8-gram overlap occurs frequently even when documents are not nearly identical. ", "page_idx": 9}, {"type": "text", "text": "Figure 7 reports the performance of the LLAMA-2 7B model on language modeling and the Natural Questions dataset using different decontamination methods. The scaling trend shows significantly better language modeling performance without decontamination, which worsens with more aggressive decontamination methods. This suggests that the beneftis in language modeling primarily arise from lexical overlap. However, retrieval continues to benefit language modeling performance even after aggressive decontamination\u2014where no more than 8 continuous words overlap\u2014indicating that semantically similar retrieved documents with minimal lexical overlap can still enhance language modeling. Decontamination does not significantly affect NQ performance, likely because there is less contamination of NQ in the datastore. Interestingly, decontamination decreases performance with smaller datastores, but improves final performance at larger scales. ", "page_idx": 9}, {"type": "text", "text": "Data quality filtering. In Appendix E.2, we study the impact of data quality filtering on MASSIVEDS, where we consider global data deduplication and a combination of 3 filters adapted from DOLMA (Soldaini et al., 2024): whitespace filter; language filter, and alphanumeric filter. We find deduplication is helpful to minimizing saturation as the datastore scales on NQ; intuitively, subsampling with higher $p$ increases the chance of seeing more duplicates. In addition, we observed that DOLMA quality fliters have a relatively limited effect. We hypothesize this is because the data sources we used in MASSIVEDS, such as RedPajama, have already gone through similar quality filtering processes and may not benefit much from applying additional filtering. ", "page_idx": 9}, {"type": "text", "text": "6 Limitations and Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We conclude by discussing limitations and future directions. First, while our pipeline allows us to study datastore scaling efficiently, our experiments are still limited by our available compute. In particular, our compute-optimal scaling studies are limited to model families like OLMo and Pythia that release intermediate model checkpoints, since full pretraining runs exceed our budget constraints. Similarly, we conduct the full scaling study with a single retriever, as changing the retriever necessitates re-indexing the entire datastore. It remains unclear how changes in the size and architecture of the retriever affect datastore scaling trends. ", "page_idx": 9}, {"type": "text", "text": "Second, although MASSIVEDS is large in size, it might still lack high-quality data for improving performance on more complex, reasoning-heavy tasks such as MMLU and MedQA. Future work could study the effect of extending MASSIVEDS to more varied and higher quality data sources. ", "page_idx": 9}, {"type": "text", "text": "Lastly, our downstream evaluations are mostly on question-answering tasks whose outputs are either predefined choices or short form generations. We defer the evaluation on more tasks such as long-form generation and mathematical reasoning to future work. ", "page_idx": 9}, {"type": "text", "text": "Despite these limitations, our research shows that increasing the scale of data available at inference time can improve model performance, at lower training cost, on language modeling and a variety of downstream tasks. We expect that future work on improving retrieval-based models with large-scale datastores will lead to even larger improvements: for example, our analysis suggests that further improving the retrieval process, either through better retrievers or rerankers, could have a significant impact. We also expect scaling up the datastore to be one way to achieve extremely long-context modeling (e.g., retrieving information from a trillion-token context) and test-time scaling (e.g., spending more test-time compute to retrieve helpful information from a large datastore). ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank Hannaneh Hajishirzi, Scott Yih, Ian Magnusson, Matt Jordan, and Rui Xin for insightful discussions. We thank Stella Li for proofreading. PWK is supported by the Singapore National Research Foundation and the National AI Group in the Singapore Ministry of Digital Development and Innovation under the AI Visiting Professorship Programme (award number AIVP-2024-001). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Amro Abbas, Kushal Tirumala, D\u00e1niel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Dataefficient learning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540, 2023. ", "page_idx": 10}, {"type": "text", "text": "Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-RAG: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations, 2024a. URL https://openreview.net/forum?id $=$ hSyW5go0v8. ", "page_idx": 10}, {"type": "text", "text": "Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, and Wen tau Yih. Reliable, adaptable, and attributable language models with retrieval. arXiv, 2024b. ", "page_idx": 10}, {"type": "text", "text": "Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In International Conference on Learning Representations, 2019. URL https://openreview.net/ forum?id $\\equiv$ ByxZX20qFQ. ", "page_idx": 10}, {"type": "text", "text": "Parishad BehnamGhader, Santiago Miret, and Siva Reddy. Can retriever-augmented language models reason? the blame game between the retriever and the language model. arXiv preprint arXiv:2212.09146, 2022. ", "page_idx": 10}, {"type": "text", "text": "Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 2397\u20132430. PMLR, 2023. ", "page_idx": 10}, {"type": "text", "text": "Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword information. Transactions of the Association for Computational Linguistics, 5:135\u2013146, 2017. ISSN 2307-387X. ", "page_idx": 10}, {"type": "text", "text": "Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and Laurent Sifre. Improving language models by retrieving from trillions of tokens. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 2206\u20132240. PMLR, 17\u201323 Jul 2022. URL https://proceedings.mlr.press/v162/borgeaud22a.html. ", "page_idx": 10}, {"type": "text", "text": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877\u20131901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_ files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. ", "page_idx": 10}, {"type": "text", "text": "Qingqing Cao, Sewon Min, Yizhong Wang, and Hannaneh Hajishirzi. Btr: Binary token representations for efficient retrieval augmented language models. arXiv preprint arXiv:2310.01329, 2023. ", "page_idx": 10}, {"type": "text", "text": "Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023. URL https://github.com/togethercomputer/RedPajama-Data.   \nSamir Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell Wortsman, Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh, et al. Language models scale reliably with over-training and on downstream tasks. arXiv preprint arXiv:2403.08540, 2024.   \nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.   \nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. Enabling large language models to generate text with citations. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 6465\u20136488, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main. 398. URL https://aclanthology.org/2023.emnlp-main.398.   \nDirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. Olmo: Accelerating the science of language models. arXiv preprint arXiv:2402.00838, 2024.   \nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: retrievalaugmented language model pre-training. In Proceedings of the 37th International Conference on Machine Learning, ICML\u201920. JMLR.org, 2020.   \nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.   \nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021.   \nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.   \nGautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question answering, 2020. URL https://arxiv.org/abs/2007.0128.   \nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL https: //openreview.net/forum?id=jKN1pXi7b0.   \nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models. Journal of Machine Learning Research, 2023. URL http://jmlr.org/papers/v24/23-0037.html.   \nHerv\u00e9 J\u00e9gou, Romain Tavenard, Matthijs Douze, and Laurent Amsaleg. Searching in one billion vectors: re-rank with source coding. In 2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 861\u2013864. IEEE, 2011.   \nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? a large-scale open domain question answering dataset from medical exams. arXiv preprint arXiv:2009.13081, 2020.   \nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017.   \nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. ", "page_idx": 11}, {"type": "text", "text": "Vladimir Karpukhin, Barlas O\u02d8guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen tau Yih. Dense passage retrieval for open-domain question answering, 2020. ", "page_idx": 12}, {"type": "text", "text": "Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models, 2020.   \nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453\u2013466, 2019.   \nKatherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris CallisonBurch, and Nicholas Carlini. Deduplicating training data makes language models better. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8424\u20138445, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.577. URL https://aclanthology.org/2022.acl-long.577.   \nKenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In Anna Korhonen, David Traum, and Llu\u00eds M\u00e0rquez (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 6086\u20136096, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/ v1/P19-1612. URL https://aclanthology.org/P19-1612.   \nSheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Yashar Mehdad, Wen-tau Yih, and Xilun Chen. How to train your dragon: Diverse augmentation towards generalizable dense retrieval. arXiv preprint arXiv:2302.07452, 2023.   \nXi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Scott Yih. RA-DIT: Retrieval-augmented dual instruction tuning. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $\\equiv$ 22OTbutug9.   \nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts, 2023. arXiv:2307.03172.   \nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. S2ORC: The semantic scholar open research corpus. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4969\u20134983, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.447. URL https://www.aclweb.org/anthology/ 2020.acl-main.447.   \nIan Magnusson, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, Ananya Harsh Jha, Oyvind Tafjord, Dustin Schwenk, Evan Pete Walsh, Yanai Elazar, Kyle Lo, et al. Paloma: A benchmark for evaluating language model fit. arXiv preprint arXiv:2312.10523, 2023.   \nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 9802\u20139822, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.546. URL https://aclanthology.org/2023.acl-long.546.   \nSewon Min, Suchin Gururangan, Eric Wallace, Hannaneh Hajishirzi, Noah A. Smith, and Luke Zettlemoyer. Silo language models: Isolating legal risk in a nonparametric datastore, 2023a.   \nSewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Hajishirzi, and Luke Zettlemoyer. Nonparametric masked language modeling. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 2097\u20132118, Toronto, Canada, July 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.132. URL https://aclanthology.org/2023.findings-acl. 132. ", "page_idx": 12}, {"type": "text", "text": "Niklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. arXiv preprint arXiv:2305.16264, 2023. ", "page_idx": 13}, {"type": "text", "text": "Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern\u00e1ndez \u00c1brego, Ji Ma, Vincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. Large dual encoders are generalizable retrievers. arXiv preprint arXiv:2112.07899, 2021. ", "page_idx": 13}, {"type": "text", "text": "Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with bert. arXiv preprint arXiv:1901.04085, 2019. ", "page_idx": 13}, {"type": "text", "text": "National Library of Medicine. Pubmed baseline 2023 repository, 2023. URL https://lhncbc. nlm.nih.gov/ii/information/MBR.html. ", "page_idx": 13}, {"type": "text", "text": "Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An open dataset of high-quality mathematical web text, 2023. ", "page_idx": 13}, {"type": "text", "text": "Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023. ", "page_idx": 13}, {"type": "text", "text": "Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt\u00e4schel, and Sebastian Riedel. KILT: a benchmark for knowledge intensive language tasks. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2523\u20132544, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.200. URL https://aclanthology.org/2021. naacl-main.200. ", "page_idx": 13}, {"type": "text", "text": "Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Dmytro Okhonko, Samuel Broscheit, Gautier Izacard, Patrick Lewis, Barlas Og\u02d8uz, Edouard Grave, Wen tau Yih, and Sebastian Riedel. The web is your oyster - knowledge-intensive nlp against a very large web corpus, 2022. ", "page_idx": 13}, {"type": "text", "text": "Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d\u2019Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis & insights from training gopher, 2022. ", "page_idx": 13}, {"type": "text", "text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020. URL http://jmlr.org/papers/v21/20-074.html. ", "page_idx": 13}, {"type": "text", "text": "Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. In-context retrieval-augmented language models. Transactions of the Association for Computational Linguistics, 11:1316\u20131331, 2023. doi: 10.1162/tacl_a_00605. URL https: //aclanthology.org/2023.tacl-1.75. ", "page_idx": 13}, {"type": "text", "text": "Devendra Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau Yih, Joelle Pineau, and Luke Zettlemoyer. Improving passage retrieval with zero-shot question generation. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 3781\u20133797, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. emnlp-main.249. URL https://aclanthology.org/2022.emnlp-main.249.   \nRulin Shao, Sewon Min, Luke Zettlemoyer, and Pang Wei Koh. Retrieval-based language models using a multi-domain datastore. In NeurIPS 2023 Workshop on Distribution Shifts: New Frontiers with Foundation Models, 2023. URL https://openreview.net/forum?id=5ck1WQ4yW4.   \nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. Replug: Retrieval-augmented black-box language models, 2023.   \nWeijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Gergely Szilvasy, Rich James, Xi Victoria Lin, Noah A. Smith, Luke Zettlemoyer, Scott Yih, and Mike Lewis. In-context pretraining: Language modeling beyond document boundaries, 2024.   \nLuca Soldaini and Kyle Lo. peS2o (Pretraining Efficiently on S2ORC) Dataset. Technical report, Allen Institute for AI, 2023. ODC-By, https://github.com/allenai/pes2o.   \nLuca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an open corpus of three trillion tokens for language model pretraining research, 2024.   \nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \nBoxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi, Yi Dong, Oleksii Kuchaiev, Bo Li, Chaowei Xiao, Anima Anandkumar, and Bryan Catanzaro. Shall we pretrain autoregressive language models with retrieval? a comprehensive study. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 7763\u20137786, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.482. URL https://aclanthology.org/2023.emnlp-main.482.   \nBoxin Wang, Wei Ping, Lawrence McAfee, Peng Xu, Bo Li, Mohammad Shoeybi, and Bryan Catanzaro. Instructretro: Instruction tuning post retrieval-augmented pretraining, 2024. URL https://openreview.net/forum?id=4stB7DFLp6.   \nSean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh Hajishirzi, Yejin Choi, and Kyunghyun Cho. Naturalproofs: Mathematical theorem proving in natural language. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021. URL https://openreview.net/forum?id $\\equiv$ Jvxa8adr3iY.   \nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm\u00e1n, Armand Joulin, and Edouard Grave. CCNet: Extracting high quality monolingual datasets from web crawl data. In Nicoletta Calzolari, Fr\u00e9d\u00e9ric B\u00e9chet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, H\u00e9l\u00e8ne Mazo, Asuncion Moreno, Jan Odijk, and Stelios Piperidis (eds.), Proceedings of the Twelfth Language Resources and Evaluation Conference, pp. 4003\u20134012, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL https: //aclanthology.org/2020.lrec-1.494.   \nGuangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pp. 38087\u201338099. PMLR, 2023. ", "page_idx": 14}, {"type": "text", "text": "Fangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with compression and selective augmentation, 2023. ", "page_idx": 15}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A Datastore Construction Pipeline 18 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A.1 Distributed Indexing 18   \nA.2 Distributed Document Retrieval 18   \nA.3 Domain Merging . 19   \nA.4 Data Filtering and Reranking . . 19   \nA.5 Data Subsampling . . . 21   \nA.6 Evaluation . 23 ", "page_idx": 16}, {"type": "text", "text": "B Implementation Details 24 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Pipeline 24   \nB.2 Language Modeling Evaluation Setup 24   \nB.3 Downstream Evaluation Setup . . 24   \nB.4 Compute-Optimal Scaling Setup . . 25 ", "page_idx": 16}, {"type": "text", "text": "C Complete Datastore Scaling Results 25 ", "page_idx": 16}, {"type": "text", "text": "D Discussion on the Inferior LLAMA-3 PPL Performance on RedPajama Data 29 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "E Additional Analysis 30 ", "page_idx": 16}, {"type": "text", "text": "E.1 Ablation on the Retriever 30   \nE.2 Effect of Data Quality Filtering . . 30 ", "page_idx": 16}, {"type": "text", "text": "A Datastore Construction Pipeline ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The MASSIVEDS pipeline entails running the following operations in order (Figure 2): ", "page_idx": 17}, {"type": "text", "text": "1. Distributed indexing. We split the documents from each domain into separate shards and construct an index for each shard. Distributing the index allows us to parallelize indexing and retrieval processes, and to study different combinations of data sources more easily. ", "page_idx": 17}, {"type": "text", "text": "2. Distributed document retrieval. For each test query, we search for the top- $K$ documents over each shard index in parallel. The searched results are first merged within each domain. We cache the per-domain searched results for data composition analysis. ", "page_idx": 17}, {"type": "text", "text": "3. Domain merging. For a target combination of domains in the datastore, we merge the top- $K$ documents from the target domains for that query. This leads to a merged pool of $D K$ retrieved documents, where $D$ is the number of target domains. From this pool, we re-select the top- $K$ documents. ", "page_idx": 17}, {"type": "text", "text": "4. Data flitering and reranking. We then apply the data flitering and reranking steps, as described above, to only the top- $K$ documents (for each test query). This allows us to experiment with different data filters and rerankers without having to rerun indexing or retrieval; moreover, we only need to filter/rerank the retrieved results instead of the whole datastore, and we do not need to repeat these processes for different subsampling ratios. ", "page_idx": 17}, {"type": "text", "text": "5. Data subsampling. We subsample the filtered and reranked documents for each test query. Specifically, for a particular subsampling ratio $p$ , we select each document for inclusion i.i.d. with probability $p$ . To try different subsamples (based on different random seeds), we only need to restart the pipeline from this step, and we can reuse computation from all previous steps. ", "page_idx": 17}, {"type": "text", "text": "6. Evaluation. We use the top- $k$ documents for each test query and prepend these documents in front of the query and few-shot examples for evaluation. ", "page_idx": 17}, {"type": "text", "text": "In practice, we position the reranking step after data subsampling to reduce the number of documents that require reranking. The commutativity of reranking and subsampling is demonstrated in Appendix A.5. Furthermore, we collect the top- $\\cdot K^{\\prime}$ documents (where $K^{\\prime}<K$ ) from the deduplicated and decontaminated top- $\\mathcal{K}$ set for reranking. We set $K^{\\prime}=k$ when reranking is not applied. Next, we describe each step of our efficiency-oriented datastore construction pipeline in detail and demonstrate the equivalence between our MASSIVEDS pipeline and the naive pipeline. Below is a table of notations for easy reference. ", "page_idx": 17}, {"type": "table", "img_path": "iAkhPz7Qt3/tmp/a25d023d4382599e704d279decf9a990c92890860e2045097acc64a6a64b86c8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "A.1 Distributed Indexing ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Our pipeline starts with raw data that is split into fixed-size documents. For each document, we run one forward pass over it with a retriever model $\\phi$ and save the final-layer representation as an embedding for that document. We store the embeddings of all documents in the datastore for similarity-based search. In practice, we split the documents into separate shards and embed each shard in parallel. As we use the uncompressed embedding for retrieval searches, our indexing does not require additional operations over the saved embeddings. We treat each shard of embeddings as one sharded index. This indexing step is executed only once over all data, while the subsequent steps are performed for each query at inference. For simplicity, we describe the subsequent steps for a single query in the sections below. ", "page_idx": 17}, {"type": "text", "text": "A.2 Distributed Document Retrieval ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "At inference, we first retrieve the top- $\\cal{K}$ documents from each sharded index in parallel to accelerate the search process. Specifically, we convert the query into an embedding and compute the innerproduct similarity scores between this query embedding and every document embedding. We then rank the documents from each shard based on these similarity scores and collect all top- $K$ documents for subsequent merging. ", "page_idx": 17}, {"type": "text", "text": "A.3 Domain Merging ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Assuming we have $m$ sharded indices, we merge the $m$ sets of top- $K$ retrieved documents from all indices based on their similarity scores to obtain the final top- $K$ documents. This process is formally defined below as $m$ -sharded distributed element-wise top- $K$ retrieval. We demonstrate that it is equivalent to directly retrieving the top- $K$ documents from a single index built on all data. ", "page_idx": 18}, {"type": "text", "text": "Formally, we introduce two definitions: element-wise top- $K$ retrieval and $m$ -shard element-wise top- $K$ retrieval, the latter of which is what our pipeline uses. ", "page_idx": 18}, {"type": "text", "text": "Definition A.1 (Element-wise top- $K$ retrieval). Assume a datastore of $N$ documents: $\\mathcal{D}=$ $\\{d_{1},\\cdot\\cdot\\cdot,d_{N}\\}$ . Given a query $q$ , element-wise top- $K$ retrieval uses a retriever $g_{\\phi}$ to compute the similarity score $s_{i}=g_{\\phi}(q,d_{i})$ of the query and each document $d_{i}$ independently, and then returns the documents with the top- $K$ highest retrieval scores. ", "page_idx": 18}, {"type": "text", "text": "Definition A.2 ( $\\ln$ -shard distributed element-wise top- $\\mathcal{K}$ retrieval). Consider a sharding strategy that splits the datastore into m shards $\\{\\mathcal{D}_{1},\\cdot\\cdot\\cdot,\\mathcal{D}_{m}\\}$ , such that each shard contains a disjoint subset of documents $\\mathcal{D}$ (i.e., $D_{1}\\cup D_{2}\\cup\\cdot\\cdot\\cdot\\cup D_{m}=D$ ; $\\bar{D_{i}}\\cap\\mathcal{D}_{j}=\\emptyset$ , when $i\\neq j$ ). In $m$ -shard distributed element-wise retrieval, we fetch the top- $K$ documents from each shard in parallel (if there are fewer than $K$ documents in a shard, then all documents in the shard are returned). Then, we merge these $m K$ documents and return the top- $K$ highest-scoring documents from the merged pool. ", "page_idx": 18}, {"type": "text", "text": "$m$ -shard distributed element-wise top- $K$ retrieval is equivalent to retrieving from a single unsharded index ${\\mathrm{[}}m=1\\$ ). ", "page_idx": 18}, {"type": "text", "text": "Lemma A.1. Changing m, the number of shards used in distributed element-wise top- $K$ retrieval, does not impact the final retrieved results. ", "page_idx": 18}, {"type": "text", "text": "Proof. Let the top- $K$ documents obtained by the element-wise top- $K$ retrieval and $m$ -shard distributed element-wise top- $K$ retrieval be $\\mathcal{D}_{K}$ and $\\mathcal{D}_{K}^{\\prime}$ , respectively. Since a document that is ranked in the top- $K$ across all documents must be ranked in the top- $K$ within any individual shard, we have $\\mathcal{D}_{K}\\subseteq\\mathcal{D}_{K}^{\\prime}$ . Because $\\{D^{1},\\cdot\\cdot\\cdot,D^{m}\\}$ is a disjoint union of $\\mathcal{D}$ , for any document $d$ in $\\mathcal{D}_{K}^{\\prime}$ , there are no more than $K-1$ documents in the $m$ shards, i.e., all documents, that have higher scores than $d$ . Then we have $d\\in\\mathcal{D}_{K}$ . Therefore, $\\mathcal{D}_{K}^{\\prime}\\subseteq\\mathcal{D}_{K}$ . Finally, we have $\\mathcal{D}_{K}=\\mathcal{D}_{K}^{\\prime}$ for any choice of $m\\in\\mathbb{N}^{+}$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "In our experiments, we use an indexing method 6 that computes the similarity score between the query and every document, independent of other documents, i.e., element-wise retrieval. Therefore, our method for distributed search is equivalent to building and retrieving from a single non-distributed index. ", "page_idx": 18}, {"type": "text", "text": "A.4 Data Filtering and Reranking ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Our pipeline includes two basic data processing steps7: data deduplication and decontamination. These steps are applied post-hoc on the retrieved documents. Reranking, which is optional, is used to enhance the quality of retrieval. Detailed descriptions of deduplication, decontamination, and optional reranking are provided in this section. ", "page_idx": 18}, {"type": "text", "text": "A.4.1 Post Hoc Datastore Deduplication ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Although REDPAJAMA (Computer, 2023), the pretraining corpora used to build MASSIVEDS, has already been deduplicated, we still noticed many duplicates in the retrieved results, particularly from the web domain. This is because Computer (2023) only performs local deduplication within each data shard; globally, many duplicates between shards remain. ", "page_idx": 18}, {"type": "text", "text": "There are two widely adopted approaches for deduplication over a large-scale dataset: the first uses a Bloom filter8 and the second uses MinHash9. In short, the Bloom filter approach is typically used to remove exact duplicates and can also remove near duplicates; the MinHash approach is used to remove near duplicates detected based on n-gram Jaccard similarity. Running global deduplication over 1.4 trillion tokens using a Bloom filter requires 1.5TB RAM memory and 933 CPU hours10. The MinHash approach requires even more memory and CPU hours than the Bloom fliter approach. Therefore, deduplicating over the entire raw text of the datastore is computationally expensive, particularly when it needs to be repeated with every experimental modification. ", "page_idx": 19}, {"type": "text", "text": "To get around this, we add a post-hoc MinHash deduplication step which is applied to a large pool of top- $K$ retrieved documents. Following Computer (2023), a document pair is a duplicate if its 13-gram Jaccard similarity score is at least $80\\%$ . Note that deduplication on the retrieved results obtained from the entire corpus is an affordable alternative to running global deduplication. The only risk is that we may not have enough documents for reranking (requiring $K^{\\prime}$ documents) or evaluation (requiring $k$ documents) after deduplication. Therefore, we choose a large $K$ $K\\gg K^{\\prime}$ and $K\\gg k$ ), i.e., $K=1000$ , to mitigate this risk. ", "page_idx": 19}, {"type": "text", "text": "The original implementation of MinHash deduplication skips the chunks with less than 13 grams. Qualitatively, after deduplicating with 13-gram Jaccard similaritily, we still find many short duplicates or nonsensical phrases under 13 words in the deduplicated data pool. Thus, we remove all documents with less than 13 words as well at the deduplication step. ", "page_idx": 19}, {"type": "text", "text": "A.4.2 Post Hoc Datastore Decontamination ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Expanding the datastore to the Internet-scale incurs the risk of data contamination. Similar to global deduplication, running pre-decontamination on the entire datastore against every possible evaluation set is inconvenient and computationally expensive. Therefore, we implement a post-hoc decontamination method to remove contamination from the retrieved documents instead of the large datastore. For downstream tasks, we compare the 13-gram Jaccard similarity between each question and the retrieved document, and remove the documents with a high similarity score that is at least $80\\%$ . For language modeling (Figure 8), we adopt a stricter approach: we calculate both the 13-gram Jaccard similarity and the longest sub-sentence between the document and the answer, marking the document as contaminated if it either has at least $80\\%$ 13-gram similarity or contains a continuous 13-gram overlap with the answer. We split sentences into grams based on whitespace. ", "page_idx": 19}, {"type": "text", "text": "We show in Lemma A.2 that our method for post-deduplication and post-decontamination is equivalent to running deduplication and decontamination on the raw data prior to retrieval. ", "page_idx": 19}, {"type": "text", "text": "Lemma A.2. Running post hoc exact deduplication and decontamination over the top- $K$ retrieved documents before taking the top- $K^{\\prime}$ (where $K^{\\prime}\\,\\leq\\,K,$ ) documents is equivalent to retrieving the top- $K^{\\prime}$ documents from a deduplicated and decontaminated datastore. ", "page_idx": 19}, {"type": "text", "text": "Proof. Given a datastore $\\mathcal{D}$ , let the deduplicated datastore as $\\mathcal{D}^{\\prime}$ . Denote the top- $K$ documents from $\\mathcal{D}$ as $\\mathcal{D}_{K}$ and the top- $K^{\\prime}$ documents $\\mathcal{D}^{\\prime}$ as $\\mathcal{D}_{K^{\\prime}}^{\\prime}$ . Then, denote the top- $K^{\\prime}$ documents retrieved from $\\mathcal{D}_{K}$ as $\\mathcal{D}_{K^{\\prime}}$ . Since both $\\mathcal{D}^{\\prime}K^{\\prime}$ and $\\mathcal{D}K^{\\prime}$ contain the top- $\\cdot K^{\\prime}$ ranked documents from all those retained after deduplication or decontamination, and since the removal of data is deterministic, we have $\\mathcal{D}^{\\prime}K^{\\prime}=\\mathcal{D}K^{\\prime}$ . \u53e3 ", "page_idx": 19}, {"type": "text", "text": "For approximate deduplication, there might be edge cases where the two documents are near duplicates but only one of those documents is retrieved in the top-K results. However, the property that none of the final top- $\\cdot\\mathbf{k}$ retrieved results are (near-)duplicates of each other will still hold. ", "page_idx": 19}, {"type": "text", "text": "A.4.3 Reranking ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Reranking is optionally applied to enhance retrieval quality in our pipeline. Given the top- $K$ retrieved documents, we apply a stronger retrieval model to embed each document and the query, ", "page_idx": 19}, {"type": "image", "img_path": "iAkhPz7Qt3/tmp/9ac73c563aaad1dfb0ea5b4e4de5ec4afcc84beafad74f2ff37d13ecd94ff010.jpg", "img_caption": ["Figure 8: The post-hoc decontamination process for perplexity evaluation. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "and recalculate the similarity scores to reorder the documents. Since reranking is an element-wise operation, which we demonstrate to be commutable with subsampling in the next section, we apply reranking after subsampling in practice. ", "page_idx": 20}, {"type": "text", "text": "A.5 Data Subsampling ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To study how model behavior changes with datastore scale, we must subsample datastores of different sizes (i.e., different sampling ratios) and with different random seeds. We first describe a naive way of subsampling from a large datastore (Algorithm 1) and show that it is computationally expensive to scale the datastore in this manner. We then introduce our efficient datastore subsampling strategy (Algorithm 2) to make datastore scaling more affordable. We also provide theoretical justification to show that our method is equivalent with high probability to naive datastore subsampling, and additionally provide a computational comparison of the two approaches. Finally, we discuss the commutativity of subsampling and other operations, demonstrating that users can flexibly reorder some of these operations in practice. ", "page_idx": 20}, {"type": "text", "text": "First, we define a few functions used in our algorithm. ", "page_idx": 20}, {"type": "text", "text": "Function SUBSAMPLE $(\\mathcal \u1e0a D \u1e0c ,p,s)$ . Given a data corpus $\\mathcal{D}$ with $N$ elements, we sample each element i.i.d. following a Bernoulli $(p)$ distribution with probability $p$ , and a random seed $s$ set such that for any given $(p,s)$ , the same $x$ will always either be sampled or not. The total number of sampled data follows Binomial $(N,p)$ . ", "page_idx": 20}, {"type": "text", "text": "Function ${\\bf S E A R C H}(q,{\\mathcal{D}},k)$ . Given a query $q$ and a datastore $\\mathcal{D}$ , this function returns the $k$ documents from the datastore with the highest similarity to $q$ . ", "page_idx": 20}, {"type": "text", "text": "Function GETTOP $(\\mathcal \u1e0a D \u1e0c ,k)$ . This function takes in a descending ordered list $\\mathcal{D}$ and returns the first $k$ elements. ", "page_idx": 20}, {"type": "text", "text": "A naive way of studying datastore scaling is to subsample from the raw data for every combination of subsampling ratio and random seed, as Algorithm 1 shows. However, this approach is computationally inefficient as it would entail repeatedly running subsampling over the raw data, building an index, and running retrieval search. ", "page_idx": 20}, {"type": "text", "text": "To affordably study datastore scaling trends, we propose an efficient way to subsample over the datastore. Instead of subsampling from the raw data, for each query, we first retrieve the top- $K$ documents from the entire datastore, and then subsample from this set. We then take the top- ${\\cdot k}$ from ", "page_idx": 20}, {"type": "text", "text": "Algorithm 1 Naive implementation of datastore scaling ", "page_idx": 21}, {"type": "text", "text": "1: Input: Data corpus $\\mathcal{D}_{N}$ , a list of subsampling ratios $P$ , a list of random seeds $S$ , a list of queries Q   \n2: Output: Retrieved top- $k$ documents $[{\\mathcal{D}}_{k}^{(q,p,s)}|\\;q\\in Q,p\\in P,s\\in S]$ 3:   \n4: function $\\mathbf{MAIN}(\\boldsymbol{\\mathcal{D}}_{N},\\boldsymbol{P},\\boldsymbol{S},\\boldsymbol{Q})$ 5: for $s\\in S$ do   \n6: for $p\\in P$ do 7: $\\mathcal{D}_{p N}^{s}\\gets\\mathrm{SUBSAMPLE}(\\mathcal{D}_{N},p,s)$ \u25c1Complexity: $O(N)$ 8: Build an index over $\\mathcal{D}_{p N}^{s}$ \u25c1Complexity: $O(p N M)^{11}$   \n9: for $q\\in Q$ do   \n10: $\\bar{\\mathcal{D}}_{k}^{(q,p,s)}\\gets\\mathrm{SEARCH}(q,\\mathcal{D}_{p|\\mathcal{D}|}^{s},k)$ \u25c1Complexity: $O(p N)$   \n11: end for   \n12: end for   \n13: end for   \n14: return $[\\mathcal{D}_{k}^{(q,p,s)}|\\;q\\in Q,p\\in P,s\\in S]$   \n15: end function ", "page_idx": 21}, {"type": "text", "text": "1: Input: Data corpus $\\mathcal{D}_{N}$ , a list of subsampling ratios $P$ , a list of random seeds, a list of queries   \n$Q$ , number of intermediate retrieved documents $K$ $(K\\ll N)$ )   \n2: Output: Retrieved top- $k$ documents $[{\\mathcal{D}}_{k}^{(q,p,s)}|\\;q\\in Q,p\\in P,s\\in S]$   \n3:   \n4: function $\\mathrm{MAIN}(\\mathcal{D}_{N},P,S,Q,K)$   \n5: Build an index over $\\mathcal{D}_{N}$ \u25c1Complexity: $O(N M)^{12}$   \n6: $\\{\\mathcal{D}_{K}^{q}|\\,q\\in Q\\}\\gets\\operatorname{SEARCH}(Q,\\mathcal{D}_{p_{i}N}^{s},K)$ \u25c1Complexity: $O(N|Q|)$   \n7: for $s\\in S$ do   \n8: for $p\\in P$ do   \n9: for $q\\in Q$ do   \n1110:: $\\begin{array}{r l}&{\\bar{\\mathcal{D}}_{p K}^{(q,p,s)}\\gets\\mathrm{SUBSAMPLE}(\\mathcal{D}_{K}^{q},p,s)}\\\\ &{\\mathcal{D}_{k}^{(q,p,s)}\\gets\\mathrm{GETTOP}(\\mathcal{D}_{p_{i}K}^{(q,p,s)},k)}\\end{array}$ \u25c1\u25c1CCoommplpelexixtiyt:y : $O(K)$ $O(1)$   \n12: end for   \n13: end for   \n14: end for   \n15: return $[\\mathcal{D}_{k}^{(q,p,s)}|\\;q\\in Q,p\\in P,s\\in S]$   \n16: end function ", "page_idx": 21}, {"type": "text", "text": "the pool of $K$ documents for final evaluation. Algorithm 2 shows the pseudocode for our proposed approach. ", "page_idx": 21}, {"type": "text", "text": "Computation comparison. The complexity of naive datastore subsampling is $O((1+M+$ $|Q|)|P||S|N)$ , where $M$ is the number of parameters of the retriever model, $Q$ is the number of queries, $|P|$ is the number of subsampling ratios, $|S|$ is the number of random seeds, and $N$ is the number of documents in the datastore. The complexity of naive datastore subsampling is dominated by $O(M|P||S|N)$ in practice, which is the cost of repetitively building the datastore for all combinations of subsampling ratios and random seeds. The complexity of our subsampling strategy is only $O(N(M+|Q|)+\\bar{K}|P||Q||S|)$ , which is dominated by $O(M N)$ , representing the cost of one-time indexing. ", "page_idx": 21}, {"type": "text", "text": "Lemma A.3. Subsampling from the retrieved top- $K$ documents with probability $p$ and then taking the top- $k$ $(k\\ll K)$ from the subsampled documents (Algorithm 2) is equivalent with high probability to directly retrieving top- $k$ documents from a datastore that is subsampled from the entire raw text datastore with probability $p$ (Algorithm $^{\\,l}$ ). The equivalence holds as long as there are enough $k$ documents left after subsampling. The chance of failure, i.e., not having enough documents left, is exponentially small in $K$ . ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Proof. Assume a fixed random seed set in a way that whether each document in the raw data pool will be included or not is determined. If a document is subsampled in the naive approach, it will also be sampled in our approach, because this document has a retrieval score falls in top- $k$ and it is determined to be sampled. Therefore, as long as there are at least $k$ documents left after our subsampling over the top- $K$ documents, the results are guaranteed to be the same. Since the number of documents remaining after subsampling follows a binomial distribution with parameters $K$ and $p$ , we can use standard tail bounds to show that the failure of not having enough documents is exponentially low in $K$ . \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Lemma A.3 holds with an exponentially small probability for failure. The probability for failure is very small in practice, i.e., less than $1\\%$ if we have 1000 documents for subsampling. To guarantee equivalence, we can use a fallback mechanism where if we do not have enough documents left are subsampling, we try again with a larger value of $K$ . However, in practice, we do not use this fallback as the failure rate is very low in our experiments. ", "page_idx": 22}, {"type": "text", "text": "Commutability of Operations. We next discuss the commutability between subsampling and other operations. To begin, we distinguish the data operations into two sets: element-level operations and set-level operations. ", "page_idx": 22}, {"type": "text", "text": "Definition A.3 (Element-level operation). An element-level operation is conditioned on a single element, i.e., a document in our context. For example, assigning an element-wise score to each document during reranking is an element-level operation. ", "page_idx": 22}, {"type": "text", "text": "Definition A.4 (Set-level operation). A set-level operation refers is conditioned on a set of elements containing at least two elements. For example, deduplication is a set-level operation. ", "page_idx": 22}, {"type": "text", "text": "Lemma A.4. Independent element-level operations are commutable with each other. Set-level operations are not commutable. ", "page_idx": 22}, {"type": "text", "text": "Proof. Since the results of independent element-level operations are not impacted by their order of execution, they are commutative with each other. However, this does not hold true for set-level operations, which are order-sensitive. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "We note that both merging and subsampling can be considered independent element-level operations if we regard the removed documents as being classified by either process to be masked out. As a results, operations such as data quality fliters, data decontamination, reranking can be moved around before or after post-hoc merging, which made it possible for us to efficiently evaluate the effect of their variants by moving them to after retrieval and merging. ", "page_idx": 22}, {"type": "text", "text": "Proposition A.1. Our MASSIVEDS pipeline is equivalent to the naive pipeline, as shown in Figure 2, with high probability. ", "page_idx": 22}, {"type": "text", "text": "Proof. Lemma 1 shows that the distributed indexing and retrieval is equivalent to unsharded indexing and retrieval. Lemmas 2-4 show that the operations that we reordered between the naive pipeline and the MASSIVEDS pipeline commute without changing the returned results, with a failure probability exponential in K, where the randomness is due to subsampling. Thus, the pipelines are equivalent with high probability. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "A.6 Evaluation ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "After the aforementioned operations, we select the top- $k$ documents from those retained for evaluation.   \nWe refer to the next section for a detailed evaluation setup. ", "page_idx": 22}, {"type": "text", "text": "B Implementation Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "B.1 Pipeline ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Sharding. We split the raw data of each domain into $m$ shards, with $m$ determined based on domain size. Specifically, $m=32$ for each time slice of CommonCrawl and C4, and $m=8$ for the other domains. ", "page_idx": 23}, {"type": "text", "text": "Chunking. Prior to datastore construction, we chunk the raw data in each shard into fixed-length passages of at most 256 words each. ", "page_idx": 23}, {"type": "text", "text": "Deduplication. Deduplication is a process that removes documents with high overlap within a data pool. Following Borgeaud et al. (2022); Magnusson et al. (2023), we deduplicate the retrieved top- $K$ documents based on 13-gram Jaccard similarity between document pairs. Document pairs that share at least $80\\%$ similarity are marked as duplicates, and the document with the lower retrieval score is removed. ", "page_idx": 23}, {"type": "text", "text": "Decontamination. Decontamination is a process that removes documents that share high similarity with evaluation data. For upstream language modeling, we apply a combination of two decontamination methods: $^{l3}$ -gram Jaccard decontamination and 32-gram longest decontamination. Specifically, 13-gram Jaccard decontamination computes the 13-gram Jaccard similarity between the document and the answer, and the document is removed if it shares at least $80\\%$ similarity score with the answer. 32-gram longest decontamination removes documents that overlap with the answer by a contiguous sequence of at least 32 tokens. Note that we use an 512-token answer to compute perplexity, so the decontamination ratio is 0.0625 of the answer length. For downstream tasks, we apply 13-gram Jaccard decontamination and remove retrieved documents with at least $80\\%$ similarity to the test data. ", "page_idx": 23}, {"type": "text", "text": "Hyper-parameters. Our pipeline has three main hyper-parameters: $k,K$ , and $p$ . $k$ is the number of documents used for evaluation. $K$ is the number of documents retrieved before subsampling. $p$ is the subsampling ratio which controls the size of the datastore. We consider $k=3$ , $K=1000$ , and $p=[0.01,0.05,0.1,0.25,0.5,0.75,1]$ . We also set different random seeds for the subsampling process. We run each subsampling with three seeds (100, 101, 102) to obtain the confidence intervals in our scaling analyses. We provide a lookup table of the tail bound for our subsampling algorithm\u2019s failure to provide enough documents for evaluation in Table 4. This indicates that our setup is approximately equivalent to performing expensive subsampling on the raw data first based on Proposition A.3. ", "page_idx": 23}, {"type": "table", "img_path": "iAkhPz7Qt3/tmp/58d2b36ba094f8f5c9413b75d88c126272821991c1b86230aeac996ef3f35d03.jpg", "table_caption": ["Table 4: Lookup table of the tail bound for a binomial distribution $B i n o m i a l(K,p)$ with at least $m=3$ number of successes. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "B.2 Language Modeling Evaluation Setup ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Following Baevski & Auli (2019); Khandelwal et al. (2020); Min et al. (2023a), we split evaluation data into into fixed-length chunks of 1,024 tokens, with a stride of 512 tokens. For each chunk, the first half is used as a prefix and retrieval query, and the second half as the target sequence for calculating perplexity. ", "page_idx": 23}, {"type": "text", "text": "B.3 Downstream Evaluation Setup ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Table 5 shows the domain, metric, and sample count for each downstream task. We evaluate all downstream tasks in a 5-shot setting and prepend the top-3 documents for retrieval-based LM evaluation. We adapt the lm-evaluation-harness,13 a widely used LM evaluation suite, for downstream evaluation. ", "page_idx": 23}, {"type": "table", "img_path": "iAkhPz7Qt3/tmp/8cd888ae1427a2fcc97a2073ecb59923952d256b54714d657a0b1e7ea6ed6c88.jpg", "table_caption": ["Table 5: Downstream evaluation tasks. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Prompt format. We tested two variants of prompt format. The first format starts with the few-shot examples and is followed by retrieved documents. The second format starts with the retrieved documents followed by the few-shot examples. We found the LM can learn the few-shot pattern better when the few-shot exmaples are closer to the question, leading to a superior performance than the other way. Therefore, we stick to the second format through the paper. ", "page_idx": 24}, {"type": "text", "text": "B.4 Compute-Optimal Scaling Setup ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Intermediate checkpoints. Thanks to prior works that release intermediate checkpoints, we can study the computational scaling behaviors approximately without pretraining LMs from scratch. In particular, we consider the intermediate checkpoints provided by Pythia (Biderman et al., 2023) and OLMo (Groeneveld et al., 2024). Pythia provides checkpoints of 9 sizes trained on up to 300B tokens from the Pile (Gao et al., 2020). We consider Pythia models that have at least 1B parameters, i.e., PYTHIA-1B, PYTHIA-2.8B, PYTHIA-6.9B, and PYTHIA-12B, in favor of their capability of handling complex downstream tasks. Additionally, we consider OLMO-1.7-1B and OLMO-1.7-7B which are trained on 3T and 2T tokens from Dolma (Soldaini et al., 2024), respectively. For Pythia, we use checkpoints trained on 1/30, 1/15, 1/10, 1/5, 1/4, 1/3, 1/2, and all of the full corpus. For OLMo, which only has models of two sizes, we select additional checkpoints trained on 1/50, 1/40, 1/20, 1/9, 1/8, 1/7, and 1/6 of the full corpus. ", "page_idx": 24}, {"type": "text", "text": "FLOPs calculation. Following (Kaplan et al., 2020; Hoffmann et al., 2022; Gadre et al., 2024), we approximate the FLOPs needed for one forward pass as $\\mathrm{FLOPs}_{\\mathrm{fwd}}\\approx2N D$ and for a backward as $\\mathrm{FLOPs}_{\\mathrm{bwd}}\\approx4N D$ , where $N$ is the number of parameters and $D$ is the number of tokens. Pretraining requires one forward pass and one backward pass on every token in the pretraining corpus. Denote the size of LM as $N_{\\mathrm{LM}}$ and the size of the pretraining corpus as $D_{\\mathrm{pretrain}}$ . The FLOPs for pretraining can be approximated as FLOPspretrain $\\approx6N_{\\mathrm{LM}}D_{\\mathrm{pretrain}}$ . Datastore construction requires one forward pass on every token in the datastore corpus during the embedding step. Denote the size of the retriever as $N_{\\mathrm{retriever}}$ and the size of the datastore corpus as $D_{\\mathrm{datastore}}$ . The FLOPs for datastore construction can be approximated as FLOPsdatastore $\\approx2N\\mathrm{retriever}D_{\\mathrm{datastore}}$ . Because we use a flat index, i.e., no additional operations are required at the indexing step, the FLOPs for datastore construction equal the FLOPs for embedding. We note that other types of indexing, e.g., inverted file index (IVFADC) (J\u00e9gou et al., 2011), may require additional FLOPs during construction and fewer FLOPs at inference. ", "page_idx": 24}, {"type": "text", "text": "C Complete Datastore Scaling Results ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we supplement the datastore scaling performance of PYTHIA and OLMO models, in addition to the LLAMAmodels shown in Figure 3. Specifically, we present the datastore scaling performance for the following model families: ", "page_idx": 24}, {"type": "text", "text": "\u2022 Pythia (Biderman et al., 2023) of 4 sizes: PYTHIA-1B, PYTHIA-2.8B, PYTHIA-6.9B, and PYTHIA-12B.   \n\u2022 OLMo (Groeneveld et al., 2024) of 2 sizes: OLMO-1B and OLMO-7B.   \n\u2022 Llama (Touvron et al., 2023), which we consider LLAMA-2 7B, LLAMA-2 13B, and LLAMA-3 8B. ", "page_idx": 24}, {"type": "text", "text": "The complete datastore scaling results for TriviaQA, Natural Questions, MMLU, and MedQA are shown in Figures 9, 10, 11, and 12, respectively. We find retrieval beneftis LMs of varied sizes across different LM families. In particular, the results show that small LMs outperform larger LMs of the same model architectures when augmented with MASSIVEDS. Surprisingly, Pythia-1B matches Pythia-12B when augmented with only 100B tokens on knowledge-intensive tasks such as TriviaQA and Natural Questions, and it outperfoms Pythia-12B when further increasing the size of datastore. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Pythia-2.8B - Retrieval OLMo-1.7 1B - Retrieval OLMo-1.7 1B - LM-Only OLMo-1.7 7B -Retrieval OLMo-1.7 7B - LM-Only Llama-2 7B-Retrieval Llama-2 13B - Retrieval Llama-2 13B - LM-Only Llama-3 8B-Retrieval ", "page_idx": 25}, {"type": "image", "img_path": "iAkhPz7Qt3/tmp/632150c4d8f13977d1f7adfb7ad584a65ad59a6ef62fa07af358bd97387775e4.jpg", "img_caption": ["Figure 9: Complete datastore scaling performance on TriviaQA with PYTHIA OLMO and LLAMAmodels. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "iAkhPz7Qt3/tmp/2c90d729d5f47127f81eb5eda361dd6bef8b8bde9c1df313ec29c2cdb584240e.jpg", "img_caption": ["Pythia-1B- Retrieval Pythia-1B - LM-Only Pythia-2.8B -Retrieval Pythia-2.8B - LM-Only Pythia-6.9B - Retrieval Pythia-6.9B - LM-Only Pythia-12B -Retrieval Pythia-12B - LM-Only OLMo-1.7 1B - Retrieval OLMo-1.7 1B - LM-Only OLMo-1.7 7B - Retrieval OLMo-1.7 7B - LM-Only Llama-2 7B-Retrieval Llama-2 7B - LM-Only Llama-2 13B-Retrieval Llama-2 13B - LM-Only Llama-3 8B-Retrieval Llama-3 8B - LM-Only ", "Figure 10: Complete datastore scaling performance on Natural Questions with PYTHIA OLMO and LLAMAmodels. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Pythia-1B- Retrieval Pythia-2.8B-Retrieval Pythia-2.8B - LM-Only Pythia-6.9B-Retrieval Pythia-6.9B - LM-Only Pythia-12B -Retrieval Pythia-12B - LM-Only OLMo-1.7 1B - Retrieval OLMo-1.7 1B - LM-Only OLMo-1.7 7B - Retrieval OLMo-1.7 7B - LM-Only Llama-2 7B-Retrieval Llama-2 7B - LM-Only Llama-2 13B-Retrieval Llama-2 13B - LM-Only Llama-3 8B- Retrieval Llama-3 8B - LM-Only ", "page_idx": 27}, {"type": "image", "img_path": "iAkhPz7Qt3/tmp/8ef22f7cba88d4cd0f1bafb05f2e2af420af922f03224acf78152949d889b1f3.jpg", "img_caption": ["Figure 11: Complete datastore scaling performance on MMLU with PYTHIA OLMO and LLAMAmodels. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "iAkhPz7Qt3/tmp/07328c68ca82c80b26daadb84b77acc5a059e819202be304001ce1002771ff97.jpg", "img_caption": ["Figure 12: Complete datastore scaling performance on MedQA with PYTHIA OLMO and LLAMAmodels. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "D Discussion on the Inferior LLAMA-3 PPL Performance on RedPajama Data ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We show in Figure 3 that LLAMA-3 8B has worse PPL scores than LLAMA-2 7B on RedPajama data with and without retrieval augmentation, which contradicts the intuition that LLAMA-3 8B is stronger than LLAMA-2 7B. In fact, Xiao et al. (2023) also reported worse PPL scores for LLAMA-3 8B than LLAMA-2 7B. We note that many factors can contribute to the difference in PPL evaluation, such as the pretraining data and context length. For example, LLAMA-3 incorporated a post-training process using instruction-tuning data, which aims to enhance model alignment and output quality for complex tasks but could shift performance metrics away from those optimal for simple PPL evaluations. In addition, LLAMA-3 was trained with significantly more data\u2014 a domain (such as RedPajama) could be down-weighted in a larger corpus, leading to less memorization for this certain domain during pretraining. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "E Additional Analysis ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section, we provide additional analyses of the impact of retriever and data quality filtering. ", "page_idx": 29}, {"type": "text", "text": "E.1 Ablation on the Retriever ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section, we ablate the choice of retriever. Since datastore construction is expensive, we subsample $10\\%$ from the full corpus of MASSIVEDS for this ablation study. We build datastores with 3 different retrievers, CONTRIEVER-MSMARCO (Izacard et al., 2022), DRAGON-ROBERTA (Lin et al., 2023), GTR-T5-BASE (Ni et al., 2021), and evaluate on upstream perplexity and downstream tasks with them separately. A subset of the tasks, i.e., RedPjama for language modeling evaluation, and NQ and MMLU for downstream evaluation, is used to compare different retrievers. The evaluation results are shown in Table 6. We find the 3 retrievers perform similarly. ", "page_idx": 29}, {"type": "text", "text": "Table 6: Ablation on the retriever. We evaluate different retrievers using $10\\%$ randomly sampled MASSIVEDS. We evaluate with LLAMA-2 7B on language modeling with RedPajama data and downstream tasks Natural Questions and MMLU. The best performance is highlighted in bold, and the second best is underlined. ", "page_idx": 29}, {"type": "table", "img_path": "iAkhPz7Qt3/tmp/08aff246d2bf1c484ad86661e2f23130dd4a38d1fb47a9d67dc803962599c742.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "We empirically find the implementations of Contriever14 and DRAGON15 run much faster than the sentence-transformer16 implementations, e.g., GTR-Base 17. As a result, we choose Contriever in our full-size scaling study with a consideration of both performance and efficiency. ", "page_idx": 29}, {"type": "text", "text": "E.2 Effect of Data Quality Filtering ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Setup. We consider three data quality filters applied to the DOLMA (Soldaini et al., 2024) corpus: (1) a whitespace filter which counts the number of whitespace-separated tokens in each document, and fliters out documents with counts under a manually defined threshold; (2) a language fliter which uses a FastText (Bojanowski et al., 2017) model to detect the language of the document and fliters out those with low model confidence; (3) an alphanumeric filter which only retains documents which contain at least one alphanumeric character, and do not contain a span of all punctuation characters (the length of the span is user-defined). ", "page_idx": 29}, {"type": "text", "text": "Data deduplication. Removing data duplicates has proven effective for pretraining more computeoptimal language models (Lee et al., 2022). Duplicates are especially undesirable in the context of retrieval augmentation as they repeat the same information while increase the inference cost, raising a need for global deduplication. In our default setting (Section 4), we perform global datastore deduplication based on 13-gram Jaccard similarity, similar to Lee et al. (2022). Additionally, we report results without global deduplication for comparison. ", "page_idx": 29}, {"type": "text", "text": "Figure 13 (b) and (e) report the results on language modeling perplexity (on RedPajama) and on NQ, respectively. We find negligible impact of global deduplication in language modeling perplexity. On ", "page_idx": 29}, {"type": "image", "img_path": "iAkhPz7Qt3/tmp/451e2bbf1a044ea769fc4ca69f3b1888e963875f5d590425227693e635c485bf.jpg", "img_caption": ["Figure 13: Analysis on the effect by deduplication, decontamination, and quality fliters, from left to right (Section 5.3). The first row corresponds to language modeling on REDPAJAMA and the second row shows QA results on Natural Questions (NQ). "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "NQ, deduplication is crucial to minimizing saturation as the datastore scales; intuitively, subsampling with higher $p$ increases the chance of seeing more duplicates 18 ", "page_idx": 30}, {"type": "text", "text": "DOLMA Quality flitering. To study the impact of quality flitering, we consider a data fliter adapted from DOLMA (Soldaini et al., 2024) which uses a combination of three fliters: a whitespace fliter; a language filter, and an alphanumeric filter (detailed in Appendix Section E.2). ", "page_idx": 30}, {"type": "text", "text": "Figure 13 (c) and (f) indicate that the quality fliter has a relatively limited effect. We hypothesize that the data sources we used in MASSIVEDS, such as RedPajama, have already gone through similar quality filtering processes and may not benefit much from applying additional filtering. While not explored in this paper, recent studies indicate that more computationally expensive but higher-quality filters can further enhance pre-training (Abbas et al., 2023; Penedo et al., 2023); we suggest future work to explore such filters for potential performance improvements. ", "page_idx": 30}, {"type": "text", "text": "Removing Small Chunks from MASSIVEDS We show a few examples of the top-1 retrieved documents before and after removing the short chunks that have less than 13 words in Figure 14. Without removing these chunks, we find retriever tend to retrieve documents with verbatim text overlap to the question, but do not provide helpful information about the answer, leading to a degradation in end-task performance. Figure 15 indicates that removing short chunks can significantly improve the retrieval-based LM performance with a large datastore, which is more likely to contain short, but meaningless word chunks. ", "page_idx": 30}, {"type": "text", "text": "\"question\": \"Answer these questions $\\therefore\\mathsf{l n}\\backslash\\mathsf{n}0$ : when did the eagles win last super bowl?\\nA:\", \"before\": \"Eagles won the Super Bowl.\", \"after\": \"As someone who lived in Philly for about five years, I agree about the city\\u2019s greatness \\u2014 whic \"question\": \"Answer these questions:\\n\\nQ: who sang i ran all the way home?\\nA:\", \"before\": \"All the Way Home\", \"after\": \"I Ran All the Way Home contains the vocal group's two hits: the smash title track and the minor hit Oh, \"question\": \"Answer these questions:\\n\\nQ: who plays gram on the young and the restless?\\nA:\", \"before\": \"on The Young and the Restless\", \"after\": \"Michael's dad River Lowell Baldwin played kwa Michael Gross. . Wallpaper and background images in the Th   \n{ \"question\": \"Answer these questions:\\n\\nQ: who developed the concept of total quality management?\\nA:\", \"before\": \"Introducing total quality management.\", \"after\": \"Total quality management it may have been first coined in the United States by the Naval Air Systems Com   \n}\uff0c   \n{ \"question\": \"Answer these questions:\\n\\nQ: who wrote cant get you out of my head lyrics?\\nA:\", \"before\": \"\\u2018Can\\u2019t Get You Out Of My Head\\u2019\", \"after\": \"the need in me I just can't get you out of my head (La,la,la La,la,la,la,la) Songwriters: Cathy Dennis -   \n{ \"question\": \"Answer these questions:\\n\\nQ: last episode of what happens to my family?\\nA:\", \"before\": \"what happened to the family?\", \"after\": \"As a child, I used to love to watch the television series Columbo because at the end of almost every epi   \n}\uff0c \"question\": \"Answer these questions:\\n\\nQ: who wrote it's a long long way to pasadena?\\nA:\", \"before\": \"Long Way\", \"after\": \"(see 1975 in music), and was written by Angus Young, Malcolm Young and Bon Scott. it's a long way to the \"question\": \"Answer these questions:\\n\\nQ: who is the first wife on sister wives?\\nA:\", \"before\": \"first wife?\", \"after\": \"Q: Name for relation between a man\\u2019s two wives? What is the relation between the two wives of a man   \n\u4e86\uff0c \"question\": \"Answer these questions:\\n\\nQ: when does the dlc for rainbow six siege come out?\\nA:\", \"before\": \"for Rainbow Six: Siege\", \"after\": \"the first-person shooter, as well as increasing some of the strategic choke-points. The DLC will enter i   \n{ \"question\": \"Answer these questions:\\n\\nQ: where does the last name galvez come from?\\nA:\", \"before\": \"Galvez\", \"after\": \"The mountainous borders of Spain contain the origins of the prestigious surname Galvez. The earliest for \"question\": \"Answer these questions:\\n\\nQ: where do you get a cashiers check from?\\nA:\", \"before\": \"Get a Cashier\\u2019s Check From Any Bank?\", \"after\": \"Find out where to get a cashier\\u2019s check and what steps to take to get one from a bank. A cashier\\u2   \n{ \"question\": \"Answer these questions:\\n\\nQ: what is the meaning of cc and bcc?\\nA:\", \"before\": \"what CC and BCC mean and security issues.\", \"after\": \"Cc stands for carbon copy and it means that whoever name appears after the Cc: will get a copy of the me ", "page_idx": 31}, {"type": "text", "text": "Figure 14: Retrieved documents for NQ when short chunks $<\\,\\,13$ words) are not removed. \u201cquestions\u201d are inputs from NQ; \u201cbefore\u201d refers to the top-1 retrieved document when we do not remove short chunks; \u201cafter\u201d refers to the top-1 document after a short-chunk removal step is added to our pipeline. The retriever tends to retrieve a short chunk that may have high lexical overlap with the question, but does not provide any useful information for the answer. ", "page_idx": 31}, {"type": "image", "img_path": "iAkhPz7Qt3/tmp/82280be15423ded5133e69824f54678a304210d5125de0477aa7b4d1b93f5eac.jpg", "img_caption": [], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "Figure 15: Comparison of NQ performance with and without the removal of short chunks from MASSIVEDS. We use LLAMA-2 7B as the reader model. ", "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We ensure that the claims in our abstract and introduction closely reflect the contributions of our paper. We also carefully describe our scope (e.g., datastore scaling for a particular class of retrieval-based LMs) and limitations. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: We discuss the limitations of our work in ??. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 33}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Appendix ?? covers to the theoretical contributions of our pipeline implementation. We justify step-by-step our approach, and sketch out the pseudocode of our algorithms to provide intuition. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We provide low-level implementation details in subsection 4.1 and Appendix B, including dataset construction, models used for analyses, all hyperparameters, etc. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We are planning to open-source all contributions in this work, including MASSIVEDS indices, instructions for datastore construction, retrieved files for upstream and downstream analyses, and all code. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Our experiments are training-free, but we include all details for inference and analysis in Section 4.1 and Appendix B in writing, and will release our code to facilitate future study. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: All of the experiments in the main section of our paper are ran with three random seeds (100, 101, 102) and we report the average with $95\\%$ confidence intervals. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 35}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We provide compute details in Appendix B. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics, and strictly follow it in this work ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Yes, we discuss broader impacts of our work in ??. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: We do not believe this question is applicable: as our datastore is amassed entirely from open-source data (e.g., RedPajama) for which safety filters (e.g., removal of PII or NSFW content) have already been applied. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Yes. We acknowledge the codebases we have referred to, cite and provide the urls of datasets and models in section 3. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We provide careful documentation of the datastore construction process in our paper. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: We do not involve any crowdsourcing or human subjects in our study, and therefore this question is not applicable. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 38}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: Our study did not involve any crowdsourcing or human subjects, and thus this question is not applicable. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}]