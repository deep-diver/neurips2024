[{"type": "text", "text": "Quantitative Convergences of Lie Group Momentum Optimizers ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Lingkai Kong ", "page_idx": 0}, {"type": "text", "text": "Molei Tao ", "page_idx": 0}, {"type": "text", "text": "School of Mathematics Georgia Institute of Technology lkong75@gatech.edu ", "page_idx": 0}, {"type": "text", "text": "School of Mathematics Georgia Institute of Technology mtao@gatech.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Explicit, momentum-based dynamics that optimize functions defined on Lie groups can be constructed via variational optimization and momentum trivialization. Structure preserving time discretizations can then turn this dynamics into optimization algorithms. This article investigates two types of discretization, Lie Heavy-Ball, which is a known splitting scheme, and Lie NAG-SC, which is newly proposed. Their convergence rates are explicitly quantified under $L$ -smoothness and local strong convexity assumptions. Lie NAG-SC provides acceleration over the momentumless case, i.e. Riemannian gradient descent, but Lie Heavy-Ball does not. When compared to existing accelerated optimizers for general manifolds, both Lie Heavy-Ball and Lie NAG-SC are computationally cheaper and easier to implement, thanks to their utilization of group structure. Only gradient oracle and exponential map are required, but not logarithm map or parallel transport which are computational costly.1 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "First-order optimization, i.e., with the gradient of the potential (i.e. the objective function) given as the oracle, is ubiquitously employed in machine learning. Within this class of algorithms, momentum is often introduced to accelerate convergence; for example, in Euclidean setups, it has been proved to yield the optimal dimension-independent convergence order, for a large class of first-order optimizers, under strongly convex and $L$ -smooth assumptions of the objective function[21, Sec. 2]. ", "page_idx": 0}, {"type": "text", "text": "Gradient Descent (GD) without momentum can be generalized to Riemannian manifold by moving to the negative gradient direction using the exponential map with a small step size. This generalization is algorithmically straight forward, but quantifying the convergence rate in curved spaces needs nontrivial efforts [6, 32, 27]. In comparison, generalizing momentum GD to manifold itself is nontrivial due to curved geometry; for example, the iteration must be kept on the manifold and the momentum must stay in the tangent space, which changes with the iteration, at the same time. It is even more challenging to quantify the convergence rate theoretically due to the loss of linearity in the space, leading to the lack of triangle inequality and cosine rule. Finally, there is not necessarily acceleration unless the generalization is done delicately. ", "page_idx": 0}, {"type": "text", "text": "Regardless, optimization on manifolds is an important task, for which the manifold structure can either naturally come from the problem setup or be artificially introduced. A simple but extremely important example is to compute the leading eigenvalues of a large matrix, which can be approached efficiently via optimization on the Stiefel manifold [19]; a smaller scale version can also be solved via optimization on ${\\mathsf{S O}}(n)$ [7, 29, 8, 26]. More on the modern machine learning side, one can algorithmically add orthonormal constraints to deep learning models to improve their accuracy and robustness [e.g., 5, 9, 17, 19, 24]. Both examples involve ${\\mathsf{S O}}(n)$ , which is an instance of an important type of curved spaces called Lie groups. ", "page_idx": 0}, {"type": "text", "text": "Lie groups are manifolds with additional group structure, and the nonlinearity of the space manifests through the non-commutative group multiplication. The group structure can help not only design momentum optimizer but also analyze its convergence. More precisely, this work will make the following contributions: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Provide the first quantitative analysis of Lie group momentum optimizers. This is significant, partly because there is no nontrivial convex functions on many Lie groups (see Rmk. 1), so we have to analyze nonconvex optimization.   \n\u2022 Theoretically show an intuitively constructed momentum optimizer, namely Lie Heavy-Ball, may not yield accelerated convergence. Numerical evidence is also provided (Sec. 6).   \n\u2022 Generalize techniques from Euclidean optimization to propose a Lie group optimizer, Lie NAG-SC, that provably has acceleration. ", "page_idx": 1}, {"type": "text", "text": "Comparing to other optimizers that are designed for general manifolds, we bypass the requirements for costly operations such as parallel transport [e.g., 4], which is a way to move the momentum between tangent spaces, and the computation of geodesic [e.g., 1], which may be an issue due to not only high computational cost but also its possible non-uniqueness. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In Euclidean space, Gradient Descent (GD) for $\\mu$ -strongly convex and $L$ -smooth objective function can converge as $\\begin{array}{r}{\\left\\|x_{k}-x_{*}\\right\\|\\leq\\left(1-C\\frac{\\mu}{L}\\right)^{k}\\left\\|x_{k}-x_{0}\\right\\|^{\\;2}}\\end{array}$ upon appropriately chosen learning rate. Momentum can accelerate the convergence by softening the dependence on the condition number $\\kappa:=L/\\mu$ . However, how momentum is introduced matters to achieving such an acceleration. For example, NAG-SC [21] has convergence rate $\\textstyle{}^{3}\\operatorname{1-C}\\!{\\sqrt{\\frac{\\mu}{L}}}$ , but Heavy-Ball [23] still has linear dependence on the conditional number, similar to gradient descent without momentum (but it may work better for nonconvex cases). ", "page_idx": 1}, {"type": "text", "text": "Remarkable quantitative results also existed for manifold optimization. The momentum-less case is relatively simpler, and [32], for example, developed convergence theory for GD on Riemannian manifold under various assumptions on convexity and smoothness, which matched the classical Euclidean result \u2014 for instance, Thm. 15 of their paper gave a convergence rate of $1\\!-\\!C\\operatorname*{min}\\left\\{{\\frac{\\mu}{L}},K\\right\\}^{4}$ when the learning rate is $1/L$ , under geodesic- $\\mu$ -strong-convexity and geodesic- $L$ -smoothness. For the momentum case, [3] analyzed the convergence of a related dynamics in continuous time, namely an optimization ODE corresponding to momentum gradient flow, on Riemannian manifolds under both geodesically strongly and weakly convex potentials, based on a tool of modified cosine rule. However, numerical methods in discrete time that are easy and cheap to implement and provably convergent in an accelerated fashion under mild conditions are still under-developed. One existing idea is to transform a function on the manifold to a function on a Euclidean space by the logarithm function. More precisely, in the case where the logarithm is a one-to-one map from the manifold to the tangent space, it can be used to project the objective function on the manifold to a function on the tangent space (\u2018pullback objective function\u2019), enabling the usage of accelerated algorithms in Euclidean spaces [e.g., 11]. Although such analysis may relax the requirement of global convexity, it requires assumptions on the \u2018pullback objective function\u2019, which is hard to check in reality. Another series of seminal works include [33, 1], which analyzed the convergence of a class of optimizers by extending Nesterov\u2019s technique of estimatin\u221ag sequence [21] to Riemannian manifolds. They managed to show a convergence rate between $1-C\\sqrt{\\frac{\\mu}{L}}$ and $1-C_{\\mathit{\\frac{\\mu}{L}}}^{\\mu}$ , i.e., with conditional number dependence inbetween that of GD with and without momentum in the Euclidean cases. They further proved that, as the iterate gets closer to the minimum, the rate bound gets better because it converges to $1-C\\sqrt{\\frac{\\mu}{L}}$ . However, their algorithm requires the logarithm map (inverse of the exponential map), which may not be uniquely defined on many manifolds (e.g., sphere) and can be computationally expensive. In contrast, Lie NAG-SC, which we will construct, works only for Lie group manifolds, but they are ", "page_idx": 1}, {"type": "text", "text": "more efficient when applicable, due to being based on only exponential map and gradient oracle.   \nAcceleration of the same type will also be theoretically proved. ", "page_idx": 2}, {"type": "text", "text": "Momentum optimizers specializing in Lie groups have also been constructed before [26], where variational optimization [30] was generalized to the manifold setup and then left trivialization were employed to obtain ODEs with Euclidean momentum that perform optimization in continuous time. Our work is also based on those ODEs, whose time however has to be discretized so that an optimization algorithm can be constructed. Delicate discretizations have been proposed in [26] so that the optimization iterates stay exactly on the manifold, saving computational cost and reducing approximation errors. But we will further improve those discretizations. More precisely, note first that [26] slightly abused notation and called both the continuous dynamics and one discretization Lie NAG-SC. However, we find that their splitting discretization may not give the best optimizer \u2013 at least, a 1st-order version of their splitting scheme yields a linear condition number dependence in our convergence rate bound. Since this splitting-based optimizer almost degenerates to heavy-ball in the special case of Euclidean spaces (as Rmk. 28 will show), we refine the terminology and call it Lie Heavy-Ball. To remedy the lack of acceleration, we propose a new discretization that actually has square root condition number dependence and thus provable acceleration, and call it as (the true) Lie NAG-SC. ", "page_idx": 2}, {"type": "text", "text": "Finally, note there can be obstructions to accelerated optimization, for example roughly when curvature is negative [16, 10, 12]. That is, however, not a contradiction because our setup involves positive curvature. ", "page_idx": 2}, {"type": "text", "text": "1.2 Main results ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider the local minimization of a differentiable function $U:\\mathsf{G}\\to\\mathbb{R}$ , i.e., $\\operatorname*{min}_{g\\in{\\mathsf{G}}}U(g)$ , where $\\mathsf{G}$ is a finite-dimensional compact Lie group, and the oracle allowed is the differential of $U$ . ", "page_idx": 2}, {"type": "text", "text": "Two optimizers we focus on are given in Alg. (1). Under assumptions of $L$ -smoothness and locally geodesic- $\\cdot\\mu$ -strong convexity, we proved that Lie Heavy-Ball has convergence rate $\\left(1+C\\frac{L}{\\mu}\\right)^{-1}$ , which is approximately the convergence rate of $\\begin{array}{r}{1-C\\operatorname*{min}\\left\\{\\frac{L}{\\mu},K\\right\\}}\\end{array}$ for Lie GD (Eq.1, which is identical to Riemannian GD applied to Lie groups); this is no acceleration. To accelerate, we propose a new Lie NAG-SC algorithm, with provable convergence rate $\\begin{array}{r}{\\left(1+C\\operatorname*{min}\\left\\{\\sqrt{\\frac{L}{\\mu}},K\\right\\}\\right)^{-1}}\\end{array}$ . Note the condition number dependence becomes $\\sqrt{\\kappa}$ instead of $\\kappa:=L/\\mu$ , hence acceleration. ", "page_idx": 2}, {"type": "text", "text": "For a summary of our main results, please see Table 1. ", "page_idx": 2}, {"type": "table", "img_path": "2hqHWD7wDb/tmp/c3db86e4e1c8a1d45682cf059121a6318a35d648a9a7f14adb33a3ead1e51bbc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "Remark 1 (Triviality of convex functions on Lie groups). We do not assume any global convexity of $U$ . In fact, $U$ has to be nonconvex for any meaningful optimization to happen. This is because we are considering compact Lie groups5, and a convex function on a connected compact manifold could only be a constant function [31]. An intuition for this is, a convex function on a closed geodesic must be constant. See [e.g., 18, Sec. B.3] for more discussions. Our analysis is, importantly, for nonconvex U, and convexity is only required locally to ensure a quantitative rate estimate can be obtained. ", "page_idx": 2}, {"type": "table", "img_path": "2hqHWD7wDb/tmp/1c342ca4fc19c3aaa4b36c2643717aeef5d0b2b1bdfaaafd2e3a29d9468b2002.jpg", "table_caption": [], "table_footnote": ["Table 1: A summary of main results "], "page_idx": 3}, {"type": "text", "text": "2 Preliminaries and setup ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "2.1 Lie group and Lie algebra ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A Lie group, denoted by $\\mathsf{G}$ , is a differentiable manifold with a group structure. A Lie algebra is a vector space with a bilinear, alternating binary operation that satisfies the Jacobi identity, known as Lie bracket. The tangent space at $e$ (the identity element of the group) is a Lie algebra, denoted as ${\\mathfrak{g}}:=T_{e}{\\mathsf{G}}$ . The dimension of the Lie group G will be denoted by $m$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 2 (general geometry). We assume the Lie group $\\mathsf{G}$ is finite-dimensional and compact. ", "page_idx": 3}, {"type": "text", "text": "One technique we will use to handle momentum is called left-trivialization: Left group multiplication $L_{g}:{\\hat{g}}\\to g{\\hat{g}}$ is a smooth map from the Lie group to itself and its tangent map $T_{\\hat{g}}\\mathsf{L}_{g}:T_{\\hat{g}}\\mathsf{G}\\to T_{g\\hat{g}}\\mathsf{G}$ is a one-to-one map. As a result, for any $g\\in{\\mathsf{G}}$ , we can represent the vectors in $T_{g}{\\mathsf{G}}$ by $T_{e}\\mathsf{L}_{\\xi}$ for $\\bar{\\xi}\\in T_{e}\\mathsf{G}$ . This operation is the left-trivialization. It comes from the group structure and may not exist for a general manifold. If the group is represented via an embedding to matrix group, i.e., $g,\\xi\\in\\mathbb{R}^{n\\times n}$ , then the left trivialization is simply given by $T_{e}\\mathsf{L}_{g}\\xi=g\\xi$ with the right-hand side given by matrix multiplication. ", "page_idx": 3}, {"type": "text", "text": "A Riemannian metric is required to take Riemannian gradient and we are considering a left-invariant metric: we first define an inner product $\\langle\\cdot,\\cdot\\rangle$ on $\\mathfrak{g}$ , which is a linear space, and then move it around by the differential of left multiplication, i.e., the inner product at $T_{g}\\mathsf{G}$ is for $\\eta_{1},\\eta_{2}\\in T_{g}{\\mathsf{G}}$ , $\\begin{array}{r}{\\bigl\\langle\\eta_{1},\\eta_{2}\\bigr\\rangle:=\\bigl\\langle T_{g}\\mathsf{L}_{g^{-1}}\\eta_{1},T_{g}\\mathsf{L}_{g^{-1}}\\eta_{2}\\bigr\\rangle.}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "2.2 Optimization dynamics ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Riemannian GD [e.g., 32] with iteration $g_{k+1}=\\exp_{g_{k}}(-h\\nabla U(g_{k}))$ can be employed to optimize $U$ defined on G, where $\\nabla$ is Riemannian gradient, and $\\exp:{\\mathsf{G}}\\times T_{g}{\\mathsf{G}}\\to{\\mathsf{G}}$ is the exponential map. To see a connection to the common Euclidean GD, it means we start from $g_{k}$ and go to the direction of negative gradient with step size $h$ to get $g_{k+1}$ by geodesic instead of straight line. Riemannian GD can be understood as a time discretization of the Riemannian gradient flow dynamics $\\dot{\\boldsymbol{g}}=-\\nabla U(\\boldsymbol{g})$ . ", "page_idx": 3}, {"type": "text", "text": "In the Lie group case, it is identical to the following Lie GD obtained from left-trivialization [26]: ", "page_idx": 3}, {"type": "equation", "text": "$$\ng_{k+1}=g_{k}\\exp_{e}(h T_{g_{k}}\\mathsf{L}_{g_{k}^{-1}}\\nabla U(g_{k})),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $T_{g_{k}}\\mathsf{L}_{g_{k}-1}\\nabla U(g_{k})\\in\\mathfrak{g}$ is the left-trivialized gradient. $\\exp_{e}{}^{7}$ is the exponential map staring at the group identity $e$ following the Riemannian structure given by the left-invariant metric, and the operation between $g_{k}$ and $\\exp(h T_{g_{k}}\\mathsf{L}_{g_{k}-1}\\nabla U(g_{k}))$ is the group multiplication. To accelerate its convergence, momentum was introduced to the Riemannian gradient flow via variational optimization and left-trivialization [26], leading to the following dynamics: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\begin{array}{l l}{\\dot{g}=T_{e}\\mathsf{L}_{g}\\xi}\\\\ {\\dot{\\xi}=-\\gamma(t)\\xi+\\mathrm{ad}_{\\xi}^{*}\\,\\xi-T_{g}\\mathsf{L}_{g^{-1}}\\nabla U(g)}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here $g(t)\\in\\mathsf{G}$ is the position variable. $\\dot{g}$ is the standard \u2018momentum\u2019 variable even though it should really be called velocity. It lives $T_{g(t)}{\\sf G}$ , which varies as $g(t)$ changes in time, and we will utilize group structure to avoid this complication. More precisely, the dynamics lets the \u2018momentum\u2019 $\\dot{g}$ be $T_{e}\\mathsf{L}_{g}\\xi$ , and $\\xi$ is therefore $T_{g}\\mathsf{L}_{g^{-1}}\\dot{g}$ and it is our new, left-trivialized momentum. Intuitively, one can think $\\xi$ as angular momentum, and $T_{e}\\mathsf{L}_{g}\\xi$ being $g\\xi$ is position times angular momentum, which is momentum. Similar to the Lie GD Eq. (1), we will not use $\\nabla U(g)$ directly, but its left-trivialization $T_{g}\\mathsf{L}_{g^{-1}}\\big(\\nabla U(g)\\big)$ , to update the left-trivialized momentum. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "This dynamics essentially models a damped mechanical system, and Tao and Ohsawa [26] proved this ODE converges to a local minimum of $U$ using the fact that the total energy (kinetic energy $\\textstyle{\\frac{1}{2}}\\langle\\xi,\\xi\\rangle$ plus potential energy $U$ ) is drained by the friction term $-\\gamma\\xi$ . In general, $\\gamma$ can be a positive time-dependent function (e.g., for optimizing convex but not strongly-convex functions), but for simplicity, we will only consider locally strong-convex potentials, and constant $\\gamma$ is enough. ", "page_idx": 4}, {"type": "text", "text": "For curved space, an additional term $\\mathrm{ad}_{\\xi}^{*}\\,\\xi$ that vanishes in Euclidean space shows up in Eq. (2). It could be understood as a generalization of Coriolis force that accounts for curved geometry and is needed for free motion. The adjoint operator ad ${\\mathrm{:~}}{\\mathfrak{g}}\\times{\\mathfrak{g}}\\to{\\mathfrak{g}}$ is defined by $\\mathrm{ad}_{X}\\,Y:=\\left[X,Y\\right]$ . Its dual, known as the coadjoint operator $\\operatorname{ad}^{*}:{\\mathfrak{g}}\\times{\\mathfrak{g}}\\to{\\mathfrak{g}}$ , is given by $\\left\\langle\\operatorname{ad}_{X}^{*}Y,Z\\right\\rangle=\\left\\langle Y,\\operatorname{ad}_{X}^{\\;\\;\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!Z}\\right\\rangle$ , $\\forall Z\\in{\\mathfrak{g}}$ . ", "page_idx": 4}, {"type": "text", "text": "2.3 Property of Lie groups with ad skew-adjoint ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The term $\\operatorname{ad}_{\\xi}^{*}\\xi$ in the optimization ODE (2) is a quadratic term and it will make the numerical discretization that will be considered later difficult. Another complication from this term is, it depends on the Riemannian metric, and indicates an inconsistency between the Riemannian structure and the group structure, i.e., the exponential map from the Riemannian structure is different from the exponential map from the group structure. Fortunately, on a compact Lie group, the following lemma shows a special metric on $\\mathfrak{g}$ can be chosen to make the term $\\mathrm{ad}_{\\xi}^{*}\\,\\xi$ vanish. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3 (ad skew-adjoint [20]). Under Assumption 2, there exists an inner product on $\\mathfrak{g}$ such that the operator ad is skew-adjoint, i.e., $\\operatorname{ad}_{\\xi}^{*}=-\\operatorname{ad}_{\\xi}$ for any $\\xi\\in{\\mathfrak{g}}$ . ", "page_idx": 4}, {"type": "text", "text": "This special inner product will also give other properties useful in our technical proofs; see Sec. A.1. ", "page_idx": 4}, {"type": "text", "text": "2.4 Assumption on potential function ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To show convergence and quantify its rate for the discrete algorithm, some smoothness assumption is needed. We define the $L$ -smoothness on a Lie group as the following. ", "page_idx": 4}, {"type": "text", "text": "Definition 4 ( $L$ -smoothness). A function $U:\\mathsf{G}\\to\\mathbb{R}$ is $L$ -smooth if and only if $\\forall g,\\hat{g}\\in G_{:}$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|T_{\\hat{g}}\\mathsf{L}_{\\hat{g}^{-1}}\\nabla U(\\hat{g})-T_{g}\\mathsf{L}_{g^{-1}}\\nabla U(g)\\right\\|\\le L d(\\hat{g},g)}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where d is the geodesic distance. ", "page_idx": 4}, {"type": "text", "text": "Under the choice of metric in Lemma 3 that ad is skew-adjoint, Lemma 21 shows this is same as the commonly used geodesic- $L$ -smoothness (Def. 20). ", "page_idx": 4}, {"type": "text", "text": "To provide an explicit convergence rate, some convex assumption on the objective function is usually needed. Under the assumption of unique geodesic on a geodesically convex set $S\\subset{\\mathsf{G}}$ , the definition of strongly convex functions in Euclidean spaces can be generalized to Lie groups: ", "page_idx": 4}, {"type": "text", "text": "Definition 5 (Locally geodesically strong convexity). $A$ function $U:\\mathsf{G}\\to\\mathbb{R}$ is locally geodesic- $\\mu$ - strongly convex at $g_{*}$ if and only if there exists a geodesically convex neighbourhood of $g_{*}$ , denoted by $S$ , such that $\\forall g,\\hat{g}\\in S$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\nU(g)-U(\\hat{g})\\geq\\left\\langle T_{\\hat{g}}\\mathsf{L}_{\\hat{g}^{-1}}\\nabla U(\\hat{g}),\\log\\hat{g}^{-1}g\\right\\rangle+\\frac{\\mu}{2}\\big\\|\\log\\hat{g}^{-1}g\\big\\|^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where log is well-defined due to the geodesic convexity of $S$ . ", "page_idx": 4}, {"type": "text", "text": "3 Convergence of the optimization ODE in continuous time ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To start, we provide a convergence analysis of the ODE (2), since our numerical scheme comes from its time discretization. We do not claim such convergence analysis for the ODE is new, and in fact, convergence for continuous dynamics has been provided on general manifolds [e.g., 3]. However, we will prove it using our technique to be self-contained and provide some insights for the convergence analysis of the discrete algorithm later. ", "page_idx": 4}, {"type": "text", "text": "Define the total energy $E^{\\mathrm{ODE}}:\\mathsf{G}\\times\\mathfrak{g}\\to\\mathbb{R}$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\nE^{\\mathrm{ODE}}(g,\\xi):=U(g)+\\frac{1}{2}\\|\\xi\\|^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "i.e., the total energy is the sum of the potential energy and the kinetic energy. Thanks to the friction $\\gamma$ , the total energy is monotonely decreasing, which provides global convergence to a stationary point. ", "page_idx": 4}, {"type": "text", "text": "Theorem 6 (Monotonely decreasing of total energy [26]). Suppose the potential function $U\\in{\\mathcal{C}}^{1}({\\mathsf{G}})$ and the trajectory $(g(t),\\xi(t))$ follows $O D E$ (2). Then ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{d}{d t}E^{O D E}(g(t),\\xi(t))=-\\gamma\\left\\Vert\\xi\\right\\Vert^{2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Thm. 6 provides the global convergence of ODE (2) to a stationary point under only $\\mathcal{C}^{1}$ smoothness: when the system converges, we have $\\xi_{\\infty}=0$ , which gives $\\|\\nabla U(g_{\\infty})\\|=0$ . ", "page_idx": 5}, {"type": "text", "text": "Moreover, using the non-increasing property of total energy, the following corollary states that if the particle starts with small initial energy, it will be trapped in a sub-level set of $U$ . The local potential well can be defined using $U$ \u2019s sub-level set. ", "page_idx": 5}, {"type": "text", "text": "Definition 7 ( $u$ sub-level set). Given $u\\in\\mathbb{R}$ , we define the u sub-level set of $U$ as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\{g\\in\\mathsf{G}:U(g)\\leq u\\}:=\\bigcup_{i\\geq0}S_{i}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "i.e. a disjoint union of connected components. ", "page_idx": 5}, {"type": "text", "text": "Corollary 8. Suppose $U\\in{\\mathcal{C}}^{1}(\\mathsf{G})$ . Let $u=E^{O D E}(g(0),\\xi(0))$ . If the u sub-level set of $U$ is $\\cup_{i\\geq0}\\,S_{i}$ and $g(0)\\in S_{0}$ , then we have $g(t)\\in S_{0},\\forall t\\geq0$ . ", "page_idx": 5}, {"type": "text", "text": "Under further assumption of local strong convexity on this sub-level set, convergence rate can be quantified via a Lyapunov analysis inspired by [25]. More specifically, given a fixed local minimum $g_{*}$ , there is provably a local unique geodesic convex neighbourhood of $g_{*}$ . Denote it by $S$ , and we define $\\mathcal{L}^{\\mathrm{ODE}}$ on $S$ by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}^{\\mathrm{ODE}}(g,\\xi):=U(g)-U(g_{*})+\\frac{1}{4}\\left\\Vert\\xi\\right\\Vert^{2}+\\frac{1}{4}\\left\\Vert\\gamma\\log g_{*}^{-1}g+\\xi\\right\\Vert^{2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "By assuming the local geodesic- $\\mu$ -strong convexity of $U$ on $S$ , we have the following quantification of Eq. (2). ", "page_idx": 5}, {"type": "text", "text": "Theorem 9 (Convergence rate of the optimization ODE). If the initial condition $(g_{0},\\xi_{0})$ satisfies that $g_{0}\\in S$ for some geodesically convex set $S\\subset{\\mathsf{G}}$ , $U\\in{\\mathcal{C}}^{1}({\\mathsf{G}})$ is locally geodesic- $\\mu$ -convex on $S$ and the u sub-level set of $U$ with $u=E^{O D E}(g_{0},\\xi_{0})$ satisfies $S_{0}\\subset S$ , then we have ", "page_idx": 5}, {"type": "equation", "text": "$$\nU(g(t))-U(g_{*})\\leq e^{-c o D E^{t}}\\mathcal{L}^{O D E}(g_{0},\\xi_{0})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with $\\begin{array}{r}{c_{O D E}=\\frac{2}{3}\\sqrt{\\mu}}\\end{array}$ by choosing $\\gamma=2\\sqrt{\\mu}$ . ", "page_idx": 5}, {"type": "text", "text": "Remark 10. This theorem alone is a local convergence result and a {Lie group $^+$ momentum} extension of an intuitive result for Euclidean gradient flow, which is, if the initial condition is close enough to a minimizer and the objective function has a positive definite Hessian at that minimizer, then gradient flow converges exponentially fast to that minimizer. However, Thm.6 already ensures global convergence, and if not stuck at a saddle point, the dynamics will eventually enter some local potential well. If that potential well is locally strongly convex at its minimizer, then the local convergence result (Thm.9) supersedes the global convergence result (which has no rate), and gives the asymptotic convergence rate. Note however that different initial conditions may lead to convergence to different potential wells (and hence minimizers), as usual. ", "page_idx": 5}, {"type": "text", "text": "4 Convergence of Lie Heavy-Ball/splitting discretization in discrete time ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "One way to obtain a manifold optimization algorithm by time discretization of the ODE (2) is to split its vector field as the sum of two, and use them respectively to generate two ODEs: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\{\\dot{\\boldsymbol{g}}=T_{e}\\boldsymbol{\\mathsf{L}}_{\\boldsymbol{g}}\\boldsymbol{\\xi}\\right.}&{{}\\quad\\left\\{\\dot{\\boldsymbol{g}}=0\\right.}\\\\ {\\dot{\\boldsymbol{\\xi}}=0}&{{}\\quad\\left.\\left\\{\\dot{\\boldsymbol{\\xi}}=-\\gamma\\boldsymbol{\\xi}-T_{\\boldsymbol{g}}\\boldsymbol{\\mathsf{L}}_{\\boldsymbol{g}^{-1}}\\boldsymbol{\\nabla}U(\\boldsymbol{g})\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Each ODE enjoys the feature that its solution stays exactly on $\\mathsf{G}\\times\\mathfrak{g}$ [26], and therefore if one alternatively evolves them for time $h$ , the result is a step- $h$ time discretization that exactly respects the geometry (no projection needed). If one approximates $\\exp(-\\gamma h)$ by $1-h\\gamma$ , then the same property holds, and the resulting optimizer is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\left\\{g_{k+1}=g_{k}\\exp\\bigl(h\\xi_{k+1}\\bigr)\\right.}}\\\\ &{\\left.\\vphantom{\\sum_{j}^{j}}\\xi_{k+1}=\\bigl(1-\\gamma h\\bigr)\\xi_{k}-h T_{g_{k}}\\mathsf{L}_{g_{k}-1}\\nabla U\\bigl(g_{k}\\bigr)\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In Euclidean cases, such numerical scheme can be viewed as Polyak\u2019s Heavy-Ball algorithm after a change of variable (Rmk. 27), and will thus be referred to as Lie Heavy-Ball. It is also a 1st-order (in $h$ ) version of the \u20182nd-order Lie-NAG\u2019 optimizer in [26] (Rmk. 28). ", "page_idx": 6}, {"type": "text", "text": "To analyze Lie Heavy-Ball\u2019s convergence, we again seek some \u2018energy\u2019 function such that the iteration of the numerical scheme Eq. (9) will never escape a sub-level set of the potential, similar to the continuous case. Given fixed friction parameter $\\gamma$ and step size $h$ , we define the modified energy $E^{\\mathrm{HB}}:\\mathsf{G}\\times\\mathfrak{g}\\to\\mathbb{R}$ as ", "page_idx": 6}, {"type": "equation", "text": "$$\nE^{\\mathrm{HB}}(g,\\xi):=U(g)+\\frac{(1-\\gamma h)^{2}}{2}\\|\\xi\\|^{2}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Theorem 11 (Monotonely decreasing of modified energy of Heavy Ball). Assume the potential $U$ is globally $L$ -smooth. When the step size satisfies $\\begin{array}{r}{h\\leq\\frac{\\gamma}{\\gamma^{2}+L}}\\end{array}$ , we have the modified energy $E^{H B}$ is monotonely decreasing, i.e., ", "page_idx": 6}, {"type": "equation", "text": "$$\nE^{H B}(g_{k},\\xi_{k})-E^{H B}(g_{k-1},\\xi_{k-1})\\leq-\\gamma h\\|\\xi_{k}\\|^{2}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Thm. 11 provides the global convergence of Heavy-Ball scheme Eq. (9) to a stationary point under only $L$ -smoothness: Due to the monotonicity of the energy function $\\mathrm{\\Delta}\\dot{E}^{\\mathrm{HB}}$ , the system will eventually converge. When it converges, since $g$ is not moving, we have $\\|\\xi_{\\infty}\\|\\,=0$ , leading to the fact that $\\|\\nabla U(g_{\\infty})\\|=0$ . More importantly, the following corollary shows that the non-increasing property of the modified traps $g$ in sub-level set of $U$ : ", "page_idx": 6}, {"type": "text", "text": "Corollary 12. Let $u=E^{H B}(g_{0},\\xi_{0})$ . If the u sub-level set of $U$ satisfies $g_{0}\\in S_{0}$ and ", "page_idx": 6}, {"type": "equation", "text": "$$\nd\\left(S_{0},\\bigcup S_{i}\\right)>h{\\sqrt{2E^{H B}{\\bigl(}g_{0},\\xi_{0}{\\bigr)}}}+h^{2}\\operatorname*{max}_{S_{0}}\\|\\nabla U\\|\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then we have gk \u2208S0 for any k for the Heavy-Ball scheme Eq. (9) when h \u2264\u03b32\u03b3+L. ", "page_idx": 6}, {"type": "text", "text": "Under the further assumption of local strong convexity on this sub-level set, the convergence rate can be quantified via a Lyapunov analysis inspired by [25]. More specifically, given a fixed local minimum $g_{*}$ , there is a local unique geodesic neighbourhood of $g_{*}$ , denoted by $S$ , and we define $\\mathcal{L}^{\\mathrm{HB}}$ on $S$ by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}^{\\mathrm{HB}}(g,\\xi):=\\frac{1}{1-\\gamma h}\\left(U(g\\exp(-h\\xi))-U(g_{*})\\right)+\\frac{1}{4}\\left\\Vert\\xi\\right\\Vert^{2}+\\frac{1}{4}\\left\\Vert\\frac{\\gamma}{1-\\gamma h}\\log g_{*}^{-1}g+\\xi\\right\\Vert^{2}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The exponential decay for the Lyapunov function (Lemma 32) helps us quantify of the convergence rate for Eq. (9) in the following theorem: ", "page_idx": 6}, {"type": "text", "text": "Theorem 13 (Convergence rate of Heavy-Ball scheme). If the initial condition $(g_{0},\\xi_{0})$ satisfies that $g_{0}\\in S$ for some geodesically convex set $S\\subset{\\mathsf{G}}$ , $U$ is $L$ -smooth and locally geodesic- $\\mu$ -convex on $S$ , and the u sub-level set of $U$ with $u=E^{O D E}(g_{0},\\xi_{0})$ satisfies $S_{0}\\subset S$ and Eq. (11), then we have ", "page_idx": 6}, {"type": "equation", "text": "$$\nU(g_{k})-U(g_{*})\\leq c_{H B}^{k}\\mathcal{L}^{H B}(g_{0},\\xi_{0})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with $\\begin{array}{r}{c_{H B}:=\\left(1+\\frac{\\mu}{16L}\\right)^{-1}}\\end{array}$ by choosing $\\begin{array}{r}{\\gamma=2\\sqrt{\\mu},\\,h=\\frac{\\sqrt{\\mu}}{4L}}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "Note the rate is $(1+1/(16\\kappa))^{-1}$ . The condition number dependence is linear $(\\kappa)$ but not $\\sqrt{\\kappa}$ . Similarly, the procedure of global convergence $\\rightarrow$ local potential well $\\rightarrow$ local minimum discussed in Rmk. 10 also applies the Heavy-Ball algorithm. ", "page_idx": 6}, {"type": "text", "text": "5 Convergence of Lie NAG-SC in discrete time ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The motivation for NAG-SC is to improve the condition number dependence. The convergence rate of Heavy-Ball shown in Thm. 13 is the same as the momentumless case [e.g., 32, Thm. 15] under the assumption of local strong convexity and $L$ -smoothness. To improve the condition number dependence, inspired by [25], we define Lie NAG-SC as the following: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{k+1}=g_{k}\\exp(h\\xi_{k+1})}\\\\ &{\\xi_{k+1}=(1-\\gamma h)\\xi_{k}-(1-\\gamma h)h\\left(T_{g_{k}}\\mathsf{L}_{g_{k}^{-1}}\\nabla U(g_{k})-T_{g_{k-1}}\\mathsf{L}_{g_{k-1}^{-1}}\\nabla U(g_{k-1})\\right)-h T_{g_{k}}\\mathsf{L}_{g_{k}^{-1}}\\nabla U(g_{k})}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Comparing to Lie Heavy-Ball, an extra ${\\mathcal{O}}(h^{2})$ term $h\\left(T_{g_{k}}\\mathsf{L}_{g_{k}-1}\\nabla U(g_{k})-T_{g_{k-1}}\\mathsf{L}_{g_{k-1}-1}\\nabla U(g_{k-1})\\right)$ is introduced (see [25, Sec. 2] for more details in the Euclidean space). Our technique of lefttrivialized (and hence Euclidean) momentum allows this trick to transfer directly from Euclidean to the Lie group case. ", "page_idx": 7}, {"type": "text", "text": "For NAG-SC, we will only provide a local convergence with quantified convergence under $L$ - smoothness and local geodesically convexity on a geodesically convex subset $S\\subset{\\mathsf{G}}$ . The difficulty in designing a modified energy and proving the global convergence will be given later in Rmk. 36. We define the following Lyapunov function: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}^{\\mathrm{NAG-SC}}(g,\\xi):=\\displaystyle\\frac{1}{1-\\gamma h}\\left(U(g\\exp(-h\\xi))-U(g_{\\ast})\\right)+\\frac{1}{4}\\|\\xi\\|^{2}}\\\\ &{\\displaystyle+\\frac{1}{4}\\left\\|\\xi+\\frac{\\gamma}{1-\\gamma h}\\log g_{\\ast}^{-1}g+h\\nabla U(g\\exp(-h\\xi))\\right\\|^{2}-\\frac{h^{2}\\left(2-\\gamma h\\right)}{4(1-\\gamma h)}\\left\\|\\nabla U(g\\exp(-h\\xi))\\right\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $g_{*}$ is the minimum of $U$ in $S$ . This Lyapunov function helps us to trap $g$ in a local potential well and quantify the convergence rate: ", "page_idx": 7}, {"type": "text", "text": "Theorem 14 (Convergence rate of NAG-SC). If the initial condition $(g_{0},\\xi_{0})$ satisfies that $g_{0}\\in S$ for some geodesically convex set $\\textit{S}\\subset\\textsf{G}$ satisfying $\\begin{array}{r}{\\operatorname*{max}_{g\\in S}d\\!\\left(g_{*},g\\right)\\ \\leq\\ \\frac{a}{A}}\\end{array}$ for some $a\\,<\\,2\\pi$ and $A:=\\operatorname*{max}_{\\|X\\|=1}\\|\\operatorname{ad}_{X}\\|_{o p}$ , $U$ is $L$ -smooth and locally geodesic- $\\mu$ -convex on $S$ , and the u sub-level set of $U$ with $u=(1-\\gamma h)^{\\overset{.}{-}1}\\mathcal{L}^{N A G-S C}(g_{0},\\xi_{0})$ satisfies $S_{0}\\subset S$ and ", "page_idx": 7}, {"type": "equation", "text": "$$\nd(S_{0},S-S_{0})>h\\sqrt{\\mathcal{L}^{N\\!A G-S C}(g_{0},\\xi_{0})}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "then we have ", "page_idx": 7}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "by choosing $\\begin{array}{c}{{U(g_{k})-U(g_{*})\\leq c_{N A G\\cdot S C}^{k}\\mathscr{L}^{N A G\\cdot S C}(g_{0},\\xi_{0})}}\\\\ {{:h=\\operatorname*{min}\\left\\{\\displaystyle\\frac{1}{\\sqrt{2L}},\\displaystyle\\frac{1}{2p(a)}\\right\\}a n d\\gamma=2\\sqrt{\\mu},\\,w i t h\\ c_{N A G\\cdot S C}:=\\left(1+\\displaystyle\\frac{1}{30}\\sqrt{\\mu}\\operatorname*{min}\\left\\{\\displaystyle\\frac{1}{\\sqrt{2L}},\\displaystyle\\frac{1}{2p(a)}\\right\\}\\right)^{-1},}}\\end{array}$ where ", "page_idx": 7}, {"type": "equation", "text": "$$\np(x):=\\frac{x}{1-\\exp(-x)}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Unlike sampling ODE and Lie Heavy-Ball, monotonely decreasing modified energy is not provided for Lie NAG-SC. It is unclear whether such modified energy for NAG-SC exists, and an intuition is provided in the Rmk. 36. ", "page_idx": 7}, {"type": "text", "text": "Another fact in Thm. 14 that is worth noticing is, we have a term $1/p(a)$ that depends on the curvature of the Lie group 8, while the Lie Heavy-Ball has the same convergence rate as the Euclidean case [25]. It is unclear if the lost of convergence rate in Lie NAG-SC comparing to the Euclidean case is because of our proof technique or the curved space itself. However, we try to provide some insights in Rmk. 35. ", "page_idx": 7}, {"type": "text", "text": "6 Systematic numerical verification via the eigen decomposition problem ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "6.1 Analytical estimation of property of eigenvalue decomposition potential ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Given a symmetric matrix, its eigen decomposition problem can be approached via an optimization problem on ${\\mathsf{S O}}(n)$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n{\\underset{X\\in\\mathbb{R}^{n\\times n},X^{\\top}X=I}{\\operatorname*{min}}}\\mathrm{tr}\\,X^{\\top}B X N\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $N:=\\mathrm{diag}([1,\\ldots,n])$ . This problem is a hard non-convex problem on manifold, but some analytical estimation [e.g., 7, Thm. 4] can be helpful for us to choose optimizer hyperparameters (we don\u2019t have to have those to apply the optimizers, but in this section we\u2019d like to verify our theoretical bounds and hence $\\mu$ and $L$ are needed). ", "page_idx": 7}, {"type": "text", "text": "This problem is non-convex with $2^{n}n!$ stationary points corresponding to the elements in $n$ -order symmetric group, including $2^{n}$ local minima and $2^{n}$ local maxima. We suppose $\\boldsymbol{B}=\\boldsymbol{R}\\boldsymbol{\\Lambda}\\boldsymbol{R}^{\\intercal}$ with $\\begin{array}{r}{\\Lambda=\\operatorname{diag}\\left(0,1,\\dots,n-2,\\frac{\\kappa}{n-1}\\right)}\\end{array}$ , where $\\lambda_{i}$ \u2019s (the diagonal values of $\\Lambda$ ) are in ascend order. Given $\\pi$ in the $n$ -symmetric group, the corresponding local minimum is $X_{\\pi}:=\\left(X_{\\pi(i)}\\right)$ , i.e., we switch the columns of $X$ by $\\pi$ . The eigenvalues of its Hessian at the local minimum $\\pi$ can be written as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\sigma_{i j}={\\big(}j-i{\\big)}{\\big(}\\lambda_{\\pi(j)}-\\lambda_{\\pi(i)}{\\big)},\\quad1\\leq i<j\\leq n\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The global minimum is given by $\\pi_{*}=i d$ with minimum value $\\textstyle\\sum_{i=1}^{n}i\\lambda_{i}$ . ", "page_idx": 7}, {"type": "text", "text": "8In comparison, Euclidean NAG-SC has convergence rate $\\left(1+C\\sqrt{\\frac{\\mu}{L}}\\right)^{-1}$ [25]. ", "page_idx": 7}, {"type": "image", "img_path": "2hqHWD7wDb/tmp/42d6557ad9b9be3f99d00c07e66faeda448b1eeccda43c50f6e0a0e57abfa1fc.jpg", "img_caption": ["(a) Numerical estimation for $1\\mathrm{~-~}c$ under different condition numbers $\\begin{array}{r}{\\kappa:=\\frac{L}{\\mu}}\\end{array}$ for H e\u2212avy-Ball and NAGSC, initialized close to the global minimum. The dashed curves are fitted value using our theoretical result, i.e., for Heavy-Ball, it is fitted by $1\\mathrm{~-~}c_{\\mathrm{HB}}\\mathrm{~\\approx~}$ $C\\kappa^{-1}$ for some $C$ fit by linear regression,  \u2212and f o\u2248r NAG-SC, it is $1-c_{\\mathrm{NAG-SC}}\\approx C\\sqrt{\\kappa}^{-1}$ . "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "2hqHWD7wDb/tmp/e9ac104dba277ea701d588d2c87bcf64e72ed8016e57b1c6169cd66b39b05cf3.jpg", "img_caption": ["(b) Global convergence on non-convex potential. The initial condition is chosen closed to the global maximum, and we plot the value of the potential function along the trajectory. The horizontal tail means the algorithm converges to the machine precision. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 1: Fig. 1(a) shows that 1) Lie NAG-SC converges much faster than Lie Heavy-Ball on ill-conditioned problems; 2) The ftited dashed curve and the experimental results align well, showing our theoretical analysis of the convergence rate $c_{\\mathrm{HB}}$ and cNAG-SC is correct. Fig. 1(b) shows the performance of our algorithms on non-convex problems experimentally. In this specific experiment, Lie NAG-SC outperforms Lie Heavy-Ball and finds the global minimum successfully without being trapped in local minimums. However, we are not sure which is better in general optimization. One possible reason for the good performance on NAG-SC is it uses a larger learning rate and is better for jumping out of the local minimums. The values of Lyapunov function along the trajectory are not provided since it is not globally defined. ", "page_idx": 8}, {"type": "text", "text": "6.2 Numerical Experiment ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We use the eigenvalues at the global minimum to estimate the $L$ and $\\mu$ in its neighborhood. As a result, around the global minimum, $L\\approx(n-1)(\\lambda_{n}-\\lambda_{1})$ , and $\\mu\\approx\\operatorname*{min}_{i}\\{\\lambda_{i+1}-\\lambda_{i}\\}$ , where we assume $\\lambda$ \u2019s are sorted in the ascend order. Such estimation is used to choose our parameters $\\gamma$ and $h$ ) in all experiments as stated in Table 1. ", "page_idx": 8}, {"type": "text", "text": "Given a conditional number $\\begin{array}{r l r}{\\kappa}&{{}:=}&{\\frac{L}{\\mu}}\\end{array}$ , we design $A$ in the following way: we choose $\\Lambda\\ =$ $\\begin{array}{r}{\\mathrm{diag}\\left(0,1,\\ldots,n-2,{\\frac{\\kappa}{n-1}}\\right)}\\end{array}$ and $R$ is uniformly sampled from ${\\mathsf{S O}}(n)$ using [22, Sec. 2.1.1]. When the given $\\kappa$ satisfies $\\kappa\\geq{\\bigl(}n-1{\\bigr)}(n-2)$ , the condition number at global minimum is the given $\\kappa$ . ", "page_idx": 8}, {"type": "text", "text": "The results are presented in Fig. 1 and 2. In all experiments, we set $n=10$ , and the computations are done on a MacBook Pro (M1 chip, 8GB memory). ", "page_idx": 8}, {"type": "text", "text": "7 Application to Vision Transformer ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This section will demonstrate a practical modern machine learning application of our Lie NAG-SC optimizer. The setting is a highly non-convex optimization problem with stochastic gradients, due to being a real deep learning task, but empirical success is still observed. More specifically, it was discovered [19] that adding artificial orthogonal constraints to attention layers in transformer models can improve their performances, because orthogonality disallows linearly dependent correlations between tokens, so that the learned attentions can be more efficient and robust. We will apply our optimizer to solve this constrained optimization problem. ", "page_idx": 8}, {"type": "text", "text": "The setup is the following (using the notation of [28]): consider a Scaled Dot-Product Multihead Attention given by MultiHead $\\mathsf{I}(Q,K,V)\\,=\\,\\mathbf{Concat}(\\mathbf{head}_{1},...,\\mathbf{head}_{n_{\\mathrm{head}}})W^{O}$ , where $\\operatorname{head}_{i}\\ =$ Attention $(Q W_{i}^{Q},K W_{i}^{K},V W_{i}^{V})$ , Attention $\\begin{array}{r}{\\iota(\\tilde{Q},\\tilde{K},\\tilde{V})=\\mathrm{softmax}\\left(\\frac{\\tilde{Q}\\tilde{K}^{\\intercal}}{\\sqrt{d_{k}}}\\right)\\tilde{V}}\\end{array}$ . The trainable parameters are matrices $W_{i}^{Q}\\in\\mathbb{R}^{d_{\\mathrm{model}}\\times d_{k}}$ , $W_{i}^{K}\\in\\mathbb{R}^{d_{\\mathrm{model}}\\times d_{k}}$ , $W_{i}^{V}\\in\\mathbb{R}^{d_{\\mathrm{model}}\\times d_{v}}$ and $W^{O}\\in\\mathbb{R}^{n_{\\mathrm{head}}d_{v}\\times d_{\\mathrm{model}}}$ . The three input matrices $Q,K$ and $V$ all have dimension sequence_length $\\times d_{\\mathrm{model}}$ . $d_{k}$ and $d_{v}$ are usually smaller than $d_{\\mathrm{model}}$ . ", "page_idx": 8}, {"type": "image", "img_path": "2hqHWD7wDb/tmp/a9332046b91fd6a7b05bdb1b48c60d9fd7609d2b3d018b83701d05bb4e34a6b7.jpg", "img_caption": ["Figure 2: Local convergence of Lie Heavy-Ball and Lie NAG-SC on eigenvalue decomposition problem with different condition numbers. The initialization is close to the global minimum. The dashed curves are the value of potential function along the trajectory and the solid curves are the values of the corresponding Lyapunov functions. Lie GD (Eq. 1) has $h$ been chosen as $1/L$ [32, Thm. 15]. We observe: 1. Lie NAG-SC converges much faster than Lie Heavy-Ball, especially on ill-conditioned problems. 2. Although the potential function is not monotonely decreasing, the Lyapunov is. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "2hqHWD7wDb/tmp/1a71d2e79833038f9dee82e627506b9858acc6b044faeddf3995ed87351ded41.jpg", "img_caption": ["Figure 3: Training curve when applying our Lie HB and Lie NAG-SC to vision transformers. Forcing the query matrices and the key matrices on ${\\mathsf{S O}}(n)$ [19, Orthogonality across heads] led to reduced training error. Moreover, validation error is also improved (Tab. 2). "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "In the case $d_{\\mathrm{model}}\\,=\\,n_{\\mathrm{head}}d_{k}$ , which is satisfied in many popular models, we apply the constraint \u2018orthogonality across heads\u2019 [19] and require Concat $(W_{i}^{Q},i\\;=\\;1...,n_{\\mathrm{head}})$ and Concat $(W_{i}^{K},i~=$ $1...,n_{\\mathrm{head}})$ to be in $50(d_{\\mathrm{model}})$ . We compare the performance of our newly proposed optimizer with the existing ones (the optimizer in [19] is identical to Lie Heavy-Ball on ${\\mathsf{S O}}(n))$ ). Fig. 3 and Tab. 2 are the validation error when we train a vision transformer [2] with $6.3\\mathrm{M}$ parameters from scratch on CIFAR, showing an improvement of Lie NAG-SC comparing the state-of-the-art algorithm Lie Heavy-Ball. The computations are done on a single Nvidia V100 GPU. The model structures and hyperparameters are identical as Sec. 3.2 in [19]. Each presented result is the average of 3 independent runs. ", "page_idx": 9}, {"type": "table", "img_path": "2hqHWD7wDb/tmp/0d510b4b3062cdc5f3047e074edd6e43d010221e44a4aeb54599f6296ce62868.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Table 2: Validation error rate of vision transformer trained by different algorithms on CIFAR, showing the performance is ordered by Euclidean GD $<$ Lie Heavy-Ball $<$ Lie NAG-SC for both CIFAR 10 and 100. The blue font means the lowest error rate. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors are grateful for the partially support by NSF DMS-1847802, Cullen-Peck Scholarship, and GT-Emory Humanity.AI Award. We thank the anonymous reviewers for their helpful comments. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Kwangjun Ahn and Suvrit Sra. From nesterov\u2019s estimate sequence to riemannian acceleration. In Conference on Learning Theory, pages 84\u2013118. PMLR, 2020.   \n[2] Dosovitskiy Alexey. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv: 2010.11929, 2020.   \n[3] Foivos Alimisis, Antonio Orvieto, Gary B\u00e9cigneul, and Aurelien Lucchi. A continuous-time perspective for modeling acceleration in riemannian optimization. In International Conference on Artificial Intelligence and Statistics, pages 1297\u20131307. PMLR, 2020.   \n[4] Foivos Alimisis, Antonio Orvieto, Gary Becigneul, and Aurelien Lucchi. Momentum improves optimization on riemannian manifolds. In International conference on artificial intelligence and statistics, pages 1351\u20131359. PMLR, 2021.   \n[5] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In International conference on machine learning, pages 1120\u20131128. PMLR, 2016.   \n[6] Silvere Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on Automatic Control, 58(9):2217\u20132229, 2013.   \n[7] Roger W Brockett. Least squares matching problems. Linear Algebra and its applications, 122: 761\u2013777, 1989.   \n[8] Zhehui Chen, Xingguo Li, Lin Yang, Jarvis Haupt, and Tuo Zhao. On constrained nonconvex stochastic optimization: A case study for generalized eigenvalue decomposition. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 916\u2013925. PMLR, 2019.   \n[9] Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval networks: Improving robustness to adversarial examples. In International conference on machine learning, pages 854\u2013863. PMLR, 2017.   \n[10] Christopher Criscitiello and Nicolas Boumal. Negative curvature obstructs acceleration for strongly geodesically convex optimization, even with exact first-order oracles. In Conference on Learning Theory, pages 496\u2013542. PMLR, 2022.   \n[11] Christopher Criscitiello and Nicolas Boumal. An accelerated first-order method for non-convex optimization on manifolds. Foundations of Computational Mathematics, 23(4):1433\u20131509, 2023.   \n[12] Christopher Criscitiello and Nicolas Boumal. Curvature and complexity: Better lower bounds for geodesically convex optimization. In The Thirty Sixth Annual Conference on Learning Theory, pages 2969\u20133013. PMLR, 2023.   \n[13] EB Dynkin. Calculation of the coefficients in the campbell\u2013hausdorff formula. DYNKIN, EB Selected Papers of EB Dynkin with Commentary. Ed. by YUSHKEVICH, AA, pages 31\u201335, 2000.   \n[14] Nicolas Guigui and Xavier Pennec. A reduced parallel transport equation on lie groups with a left-invariant metric. In Geometric Science of Information: 5th International Conference, GSI 2021, Paris, France, July 21\u201323, 2021, Proceedings 5, pages 119\u2013126. Springer, 2021.   \n[15] Ernst Hairer, Marlis Hochbruck, Arieh Iserles, and Christian Lubich. Geometric numerical integration. Oberwolfach Reports, 3(1):805\u2013882, 2006.   \n[16] Linus Hamilton and Ankur Moitra. A no-go theorem for robust acceleration in the hyperbolic plane. NeurIPS, 2021.   \n[17] Kyle Helfrich, Devin Willmott, and Qiang Ye. Orthogonal recurrent neural networks with scaled cayley transform. In International Conference on Machine Learning, pages 1969\u20131978. PMLR, 2018.   \n[18] Lingkai Kong and Molei Tao. Convergence of kinetic langevin monte carlo on lie groups. COLT, 2024.   \n[19] Lingkai Kong, Yuqing Wang, and Molei Tao. Momentum stiefel optimizer, with applications to suitably-orthogonal attention, and optimal transport. ICLR, 2023.   \n[20] John Milnor. Curvatures of left invariant metrics on lie groups, 1976.   \n[21] Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media, 2013.   \n[22] Sean O\u2019Hagan. Uniform sampling methods for various compact spaces. 2007.   \n[23] Boris T Polyak. Introduction to optimization. 1987.   \n[24] Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, and Bernhard Sch\u00f6lkopf. Controlling text-to-image diffusion by orthogonal finetuning. Advances in Neural Information Processing Systems, 36:79320\u201379362, 2023.   \n[25] Bin Shi, Simon S Du, Michael I Jordan, and Weijie J Su. Understanding the acceleration phenomenon via high-resolution differential equations. Mathematical Programming, pages 1\u201370, 2021.   \n[26] Molei Tao and Tomoki Ohsawa. Variational optimization on lie groups, with examples of leading (generalized) eigenvalue problems. In International Conference on Artificial Intelligence and Statistics, pages 4269\u20134280. PMLR, 2020.   \n[27] Nilesh Tripuraneni, Nicolas Flammarion, Francis Bach, and Michael I Jordan. Averaging stochastic gradient descent on riemannian manifolds. In Conference On Learning Theory, pages 650\u2013687. PMLR, 2018.   \n[28] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[29] Zaiwen Wen and Wotao Yin. A feasible method for optimization with orthogonality constraints. Mathematical Programming, 142(1):397\u2013434, 2013.   \n[30] Andre Wibisono, Ashia C Wilson, and Michael I Jordan. A variational perspective on accelerated methods in optimization. proceedings of the National Academy of Sciences, 113(47):E7351\u2013 E7358, 2016.   \n[31] Shing-Tung Yau. Non-existence of continuous convex functions on certain riemannian manifolds. Mathematische Annalen, 207:269\u2013270, 1974.   \n[32] Hongyi Zhang and Suvrit Sra. First-order methods for geodesically convex optimization. In Conference on learning theory, pages 1617\u20131638. PMLR, 2016.   \n[33] Hongyi Zhang and Suvrit Sra. Towards riemannian accelerated gradient methods. arXiv preprint arXiv:1806.02812, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Properties of Lie groups and functions on Lie groups ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 More details about compact Lie groups with left-invariant metric ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Comparing with the Euclidean space, Lie groups lack of commutativity, i.e., for $g,{\\hat{g}}\\in{\\mathsf{G}}$ , $g\\hat{g}$ and ${\\hat{g}}g$ are not necessarily equal. This can also be characterized by the non-trivial Lie bracket $[\\cdot,\\cdot]$ . This non-commutativity leads to the fact that $\\exp(X)\\exp(Y)\\neq\\exp(X+Y)$ . An explicit expression for $\\log(\\exp(X)\\exp({Y}))$ is given by Dynkin\u2019s formula [13]. Utilizing Dynkin\u2019s formula, we quantify dlog in the following. ", "page_idx": 12}, {"type": "text", "text": "Corollary 15 (Differential of logarithm). $I f\\operatorname{d}\\log{g}$ is well defined, then the differential of logarithm on $\\mathsf{G}$ is given by ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathrm{d}_{\\xi}\\log g:=(\\mathrm{d}\\log)_{g}\\bigl(T_{e}\\lfloor_{g}\\xi\\bigr)=T_{e}\\lfloor_{g}\\bigl[p\\bigl(\\mathrm{ad}_{\\log g}\\bigr)\\xi\\bigr]\\,,\\quad\\forall\\xi\\in\\mathfrak{g}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where the power series $p$ is defined in Eq. (16) ", "page_idx": 12}, {"type": "text", "text": "The vanishment of $\\mathrm{ad}_{\\xi}^{*}\\,\\xi$ can also be understood as the group structure and the Riemannian structure are compatible. See [18] for more discussion. Under such assumption, we have the following properties: ", "page_idx": 12}, {"type": "text", "text": "Corollary 16. Suppose we have $\\operatorname{ad}_{X}$ is skew-adjoint $\\forall X\\in{\\mathfrak{g}}$ . Then for any $g\\in{\\mathsf{G}}$ and any $\\xi\\in{\\mathfrak{g}}$ such that $\\mathrm{d}_{\\xi}\\log{g}$ is well-defined, we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left\\langle\\mathrm{d}_{\\xi}\\log g,\\log g\\right\\rangle=\\left\\langle\\log g,\\xi\\right\\rangle\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Corollary 17. When $\\mathrm{d}_{\\xi}\\log{g}$ is well-defined and $\\operatorname{ad}_{X}$ is skew-adjoint $\\forall X\\in{\\mathfrak{g}}$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\langle\\mathrm{d}_{\\xi}\\log g,\\xi\\rangle\\leq\\left\\|\\xi\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Corollary 18. Define ", "page_idx": 12}, {"type": "equation", "text": "$$\nA:=\\operatorname*{max}_{\\|X\\|=1}\\|\\mathrm{ad}_{X}\\|_{o p}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "When $\\begin{array}{r}{d(g,e)\\leq\\frac{a}{A}}\\end{array}$ for some $a\\in(0,2\\pi)$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\lVert\\deg g-I d\\rVert\\leq q(a)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $q$ is defined by ", "page_idx": 12}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{q(x):=\\|p(x i)-1\\|}\\\\ &{\\qquad=\\left\\|{\\frac{x i}{1-\\cos x+i\\sin x}}-1\\right\\|}\\\\ &{\\qquad={\\sqrt{\\frac{1-{\\frac{x^{2}}{2}}-\\cos x-x\\sin x}{1-\\cos x}}}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "with p defined in Eq. (16). ", "page_idx": 12}, {"type": "text", "text": "Remark 19 (About existence and uniqueness of log). As the inverse of exp, the operator log may not be uniquely defined globally. However, we are always considering in a unique geodesic subset of the Lie group, where log is defined uniquely in such a subset of Lie group. Similarly, even if we do not have globally geodesically strongly convex functions, we only require locally strong convexity. ", "page_idx": 12}, {"type": "text", "text": "A.2 More details about functions on Lie groups ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The commonly used geodesic $L$ -smooth on a manifold $M$ is given by the following definition [e.g., 32, Def. 5]: ", "page_idx": 12}, {"type": "text", "text": "Definition 20 (Geodesically $L$ -smooth). $U:\\mathsf{G}\\to\\mathbb{R}$ is geodesically $L$ -smooth if for any $g,{\\widehat{\\in}}M$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left\\|\\nabla U(g)-\\Gamma_{\\hat{g}}^{g}\\nabla U(\\hat{g})\\right\\|\\leq L d(g,\\hat{g})\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\Gamma_{\\hat{g}}^{g}$ is the parallel transport from $\\hat{g}$ to $g$ ", "page_idx": 12}, {"type": "text", "text": "Lemma 21. Under the assumption of $\\mathrm{ad}_{X}^{*}$ is skew-adjoint $\\forall X\\in{\\mathfrak{g}}$ , Def. $^{4}$ is identical to Def. 20. ", "page_idx": 12}, {"type": "text", "text": "Proof of Lemma 21. For any $g,{\\hat{g}}\\in{\\mathsf{G}}$ , consider the shortest geodesic $\\phi:[0,1]\\rightarrow{\\mathsf{G}}$ connecting $g$ and $\\hat{g}$ and denote $\\xi=T_{g}L_{g^{-1}}\\nabla U(g)$ . Using the condition ad is skew-adjoint, we have $\\dot{\\phi}(t)=0$ and $T_{e}L_{\\phi(t)}\\xi$ is parallel along $\\phi$ by checking the condition for parallel transport [14, Thm. 1]: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\xi=0=-\\frac{1}{2}\\left[T_{\\phi(t)}L_{\\phi(t)^{-1}}\\dot{\\phi}(t),\\xi\\right]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This tells that ", "page_idx": 13}, {"type": "equation", "text": "$$\nT_{g}\\mathsf{L}_{g^{-1}}\\Gamma_{g}^{\\hat{g}}\\nabla f(g)=T_{\\hat{g}}\\mathsf{L}_{\\hat{g}^{-1}}\\nabla f(\\hat{g})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Together with the metric is left-invariant, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|T_{g}\\L_{2^{-1}}\\nabla U(g)-T_{\\hat{g}}\\L_{\\hat{g}^{-1}}\\nabla U(\\hat{g})\\right\\|\\leq L d(g,\\hat{g})}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which is identical to Def. 4. ", "page_idx": 13}, {"type": "text", "text": "Corollary 22 (Properties of $L$ -smooth functions). If $U:\\mathsf{G}\\to\\mathbb{R}$ is $L$ -smooth, then for any $g,{\\hat{g}}\\in\\mathsf{G}$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\nU(\\hat{g})\\le U(g)+\\big\\langle T_{g}\\mathsf{L}_{g^{-1}}\\nabla U(g),\\log g^{-1}\\hat{g}\\big\\rangle+\\frac{L}{2}d^{2}\\big(g,\\hat{g}\\big)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof of Cor. 22. We denote the one of the shortest geodesic connecting $g$ and $\\hat{g}$ as $g(t)$ , i.e., $\\pi:$ $[0,1]\\rightarrow{\\sf G}$ with $g(0)=g$ and $g(1)=\\hat{g},g(t)=g\\exp(\\bar{t}\\xi)$ for some $\\xi\\in{\\mathfrak{g}}$ with $\\|\\xi\\|=d(g,\\hat{g})$ . Then ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{O\\left(\\boldsymbol{g}\\right)-\\boldsymbol{\\upsilon}\\left(\\boldsymbol{g}\\right)}\\\\ &{=\\left\\langle T_{e}\\boldsymbol{\\downarrow}_{g}\\boldsymbol{\\xi},\\boldsymbol{\\nabla}U(\\boldsymbol{g})\\right\\rangle+\\int_{0}^{1}\\left\\langle T_{e}\\boldsymbol{\\downarrow}_{g(t)}\\boldsymbol{\\xi},\\boldsymbol{\\nabla}U(\\boldsymbol{g}(t))\\right\\rangle\\mathrm{d}t}\\\\ &{=\\left\\langle\\boldsymbol{\\xi},T_{g}\\boldsymbol{\\downarrow}_{g^{-1}}\\boldsymbol{\\nabla}U(\\boldsymbol{g})\\right\\rangle+\\int_{0}^{1}\\left\\langle\\boldsymbol{\\xi},T_{g(t)}\\boldsymbol{\\downarrow}_{g(t)^{-1}}\\boldsymbol{\\nabla}U(\\boldsymbol{g}(t))-T_{g}\\boldsymbol{\\downarrow}_{g^{-1}}\\boldsymbol{\\nabla}U(\\boldsymbol{g})\\right\\rangle\\mathrm{d}t}\\\\ &{\\leq\\left\\langle\\boldsymbol{\\xi},T_{g}\\boldsymbol{\\downarrow}_{g^{-1}}\\boldsymbol{\\nabla}U(\\boldsymbol{g})\\right\\rangle+\\int_{0}^{1}\\left\\|\\boldsymbol{\\xi}\\right\\|\\left\\|T_{g(t)}\\boldsymbol{\\downarrow}_{g(t)^{-1}}\\boldsymbol{\\nabla}U(\\boldsymbol{g}(t))-T_{g}\\boldsymbol{\\downarrow}_{g^{-1}}\\boldsymbol{\\nabla}U(\\boldsymbol{g})\\right\\|\\mathrm{d}t}\\\\ &{\\leq\\left\\langle\\boldsymbol{\\xi},T_{g}\\boldsymbol{\\downarrow}_{g^{-1}}\\boldsymbol{\\nabla}U(\\boldsymbol{g})\\right\\rangle+\\int_{0}^{1}t L\\left\\|\\boldsymbol{\\xi}\\right\\|^{2}\\mathrm{d}t}\\\\ &{=\\left\\langle\\boldsymbol{\\xi},T_{g}\\boldsymbol{\\downarrow}_{g^{-1}}\\boldsymbol{\\nabla}U(\\boldsymbol{g})\\right\\rangle+\\frac{L}{2}d^{2}(g,\\hat{\\boldsymbol{g}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Lemma 23 (Co-coercivity). If the function $U:\\mathsf{G}\\to\\mathbb{R}$ is both $L$ -smooth and convex on a geodesically convex set $S\\subset{\\mathsf{G}}$ , then we have for any $g,{\\hat{g}}\\in S$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\nU(\\hat{g})\\geq U(g)+\\left\\langle T_{g}\\mathsf{L}_{g^{-1}}\\nabla U(g),\\log g^{-1}\\hat{g}\\right\\rangle+\\frac{1}{2L}\\big\\|T_{g}\\mathsf{L}_{g^{-1}}\\nabla U(g)-T_{\\hat{g}}\\mathsf{L}_{\\hat{g}^{-1}}\\nabla U(\\hat{g})\\big\\|^{2}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof of Lemma 23. By convexity, we have for any $g\\in\\mathsf{G},\\xi\\in\\mathfrak{g}$ and $t\\in[0,1]$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U(g\\exp(t\\xi))-U(g)\\geq t\\big\\langle T_{g}\\mathsf{L}_{g^{-1}}\\nabla U(g),\\xi\\big\\rangle}\\\\ &{U(g)-U(g\\exp(t\\xi))\\geq-t\\big\\langle T_{g\\exp(t\\xi)}\\mathsf{L}_{g\\exp(t\\xi)^{-1}}\\nabla U(g\\exp(t\\xi)),\\xi\\big\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We sum these two inequalities and have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\langle T_{g\\exp(t\\xi)}\\mathsf{L}_{g\\exp(t\\xi)^{-1}}\\nabla U\\big(g\\exp(t\\xi)\\big)-T_{g}\\mathsf{L}_{g^{-1}}\\nabla U\\big(g\\big),\\xi\\right\\rangle\\ge0}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which tells that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial t}\\left[T_{g\\,\\mathrm{exp}(t\\xi)}\\mathsf{L}_{g\\,\\mathrm{exp}(t\\xi)^{-1}}\\nabla U\\big(g\\,\\mathrm{exp}(t\\xi)\\big)\\right]=H_{t}\\xi\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "for some linear map $H:{\\mathfrak{g}}\\to{\\mathfrak{g}}$ with all eigenvalues between 0 and $L$ , i.e., $0\\,\\leq\\,H_{t}\\,\\leq\\,L$ for any $t\\in[0,1]$ . ", "page_idx": 13}, {"type": "text", "text": "Now, we select the shortest geodesic connection $g$ and $\\hat{g}$ , defined by $g(t):=g\\exp(t\\xi)$ , with $\\hat{g}\\,=$ $g\\exp(\\xi)$ and $\\|\\xi\\|=d(g,\\hat{g})$ . By ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{g(t)}\\mathsf{L}_{g(t)^{-1}}\\nabla U(g(t))-T_{g}\\mathsf{L}_{g^{-1}}\\nabla U(g)=\\displaystyle\\int_{0}^{t}\\frac{\\partial}{\\partial s}T_{g\\exp(s\\xi)}\\mathsf{L}_{g\\exp(s\\xi)^{-1}}\\nabla U(g\\exp(s\\xi))d s}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\int_{0}^{t}H_{s}\\xi d s}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|U(\\partial)-U(\\partial)\\rangle}\\\\ &{=\\int_{\\mathbf{0}}^{\\mathbf{1}}\\left(\\gamma_{x_{i}+\\nu}\\nabla U(\\phi)\\right)\\xi_{i}\\quad}\\\\ &{=\\int_{\\mathbf{0}}^{\\mathbf{1}}\\left(\\frac{1}{\\gamma_{x_{i}+\\nu}\\nabla U(\\phi)}\\right)\\left(\\phantom{\\frac{1}{\\gamma_{x_{i}+\\nu}}}\\!\\!\\!\\!\\frac{\\partial}{\\partial x_{i}}\\!\\!\\!\\!\\int_{\\mathbf{0}}^{\\mathbf{1}}\\!\\!\\!\\!\\!\\frac{\\partial}{\\partial x_{i}}\\!\\!\\!\\!\\!\\int_{\\mathbf{0}}^{\\mathbf{1}}\\!\\!\\!\\!\\!\\frac{\\partial}{\\partial x_{i}}\\!\\!\\!\\!\\!\\int_{\\mathbf{0}}^{\\mathbf{1}}\\!\\!\\!\\!\\!\\!\\frac{\\partial}{\\partial x_{i}}\\!\\!\\!\\!\\!\\int_{\\mathbf{0}}^{\\mathbf{1}}\\!\\!\\!\\!\\!\\!\\frac{\\partial}{\\partial x_{i}}\\!\\!\\!\\!\\!\\int_{\\mathbf{0}}^{\\mathbf{1}}\\!\\!\\!\\!\\!\\!\\frac{\\partial}{\\partial x_{i}}\\!\\!\\!\\!\\!\\int_{\\mathbf{0}}^{\\mathbf{1}}\\!\\!\\!\\!\\!\\!\\frac{\\partial}{\\partial x_{i}}\\!\\!\\!\\!\\!\\int_{\\mathbf{0}}^{\\mathbf{1}}\\!\\!\\!\\!\\!\\!\\frac{\\partial}{\\partial x_{i}}\\!\\!\\!\\!\\!\\int_{\\mathbf{0}}^{\\mathbf{1}}\\!\\!\\!\\!\\!\\frac{\\partial}{\\partial x_{i}}\\!\\!\\!\\!\\!\\int_{\\mathbf{0}}^{\\mathbf{1}}\\!\\!\\!\\!\\!\\frac{\\partial}{\\partial x_{i}}\\!\\!\\!\\!\\!\\int_{\\mathbf{0}}^{\\mathbf{1}}\\!\\!\\!\\!\\!\\frac{\\partial}{\\partial x_{i}}\\!\\!\\!\\!\\!\\int_{\\mathbf{0}}^{\\mathbf{1}}\\!\\!\\!\\!\\!\\frac{\\partial}{\\partial x_{i}}\\!\\!\\!\\!\\!\\int_{\\mathbf{0}}^{\\mathbf{1}}\\!\\!\\!\\!\\!\\frac{\\partial}{\\partial x_{i}}\\!\\!\\!\\!\\!\\int_{\\mathbf{0}}^{\\mathbf{1}}\\!\\!\\!\\!\\!\\frac{\\partial}{\\partial x_{i}}\\!\\!\\!\\!\\!\\int_{\\mathbf{0}}^{\\mathbf{1}}\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Corollary 24. If the function $U:\\mathsf{G}\\to\\mathbb{R}$ is both $L$ -smooth and convex on a geodesically convex set $S\\subset{\\mathsf{G}}$ , then we have for any $g,{\\hat{g}}\\in S$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\langle T_{g}\\mathsf{L}_{g^{-1}}\\nabla U(g)-T_{\\hat{g}}\\mathsf{L}_{\\hat{g}^{-1}}\\nabla U(\\hat{g}),\\log\\hat{g}^{-1}g\\right\\rangle\\ge\\frac{1}{L}\\big\\|T_{g}\\mathsf{L}_{g^{-1}}\\nabla U(g)-T_{\\hat{g}}\\mathsf{L}_{\\hat{g}^{-1}}\\nabla U(\\hat{g})\\big\\|^{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. By exchanging $g$ and $\\hat{g}$ in Eq. (21), we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nU(g)\\geq U(\\hat{g})+\\left\\langle T_{\\hat{g}}\\mathsf{L}_{\\hat{g}^{-1}}\\nabla U(\\hat{g}),\\log\\hat{g}^{-1}g\\right\\rangle+\\frac{1}{2L}\\big\\|T_{g}\\mathsf{L}_{g^{-1}}\\nabla U(g)-T_{\\hat{g}}\\mathsf{L}_{\\hat{g}^{-1}}\\nabla U(\\hat{g})\\big\\|^{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Summing up 21 and 23 gives us the desired result. ", "page_idx": 14}, {"type": "text", "text": "Corollary 25 (Properties of $\\mu$ -strongly convex functions). Suppose $U:\\mathsf{G}\\to\\mathbb{R}$ is geodesic- $\\mu$ -strongly convex on a geodesically convex set $S\\subset{\\mathsf{G}}$ , then for any $g,{\\hat{g}}\\in S$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\langle T_{g}\\mathsf{L}_{g^{-1}}\\nabla U(g),\\log\\hat{g}^{-1}g\\right\\rangle\\ge\\mu d^{2}(g,\\hat{g})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof of Cor. 25. By the definition of geodesic- $\\mu$ -strongly convex functions in Eq. (4), we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{U(g)-U(\\hat{g})\\geq\\left\\langle T_{\\hat{g}}\\mathsf{L}_{\\hat{g}^{-1}}\\nabla U(\\hat{g}),\\log\\hat{g}^{-1}g\\right\\rangle+\\frac{\\mu}{2}\\big\\|\\log\\hat{g}^{-1}g\\big\\|^{2}}}\\\\ {{U(\\hat{g})-U(g)\\geq\\left\\langle T_{g}\\mathsf{L}_{g^{-1}}\\nabla U(g),\\log g^{-1}\\hat{g}\\right\\rangle+\\frac{\\mu}{2}\\big\\|\\log\\hat{g}^{-1}g\\big\\|^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Summing them up and using $\\log g^{-1}\\hat{g}=-\\log\\hat{g}^{-1}g$ gives us the conclusion. ", "page_idx": 14}, {"type": "text", "text": "B More details about optimization ODE ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof of Thm. 6. By direct calculation, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{d}{d t}E(g(t),\\xi(t))=\\langle T_{e}\\mathsf{L}_{g}\\xi,\\nabla U(g)\\rangle+\\big\\langle\\xi,\\dot{\\xi}\\big\\rangle}\\\\ {\\displaystyle\\qquad\\qquad=-\\gamma\\|\\xi\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma 26 (Monotonicity of the Lyapunov function). Assume $\\operatorname{ad}_{X}$ is skew-adjoint for any $X\\in{\\mathfrak{g}}$ . Suppose there is a geodesically convex set $S\\subset{\\mathsf{G}}$ satisfying: ", "page_idx": 15}, {"type": "text", "text": "\u2022 $U$ is geodesic- $\\mu$ -strongly convex on a geodesically convex set $S\\subset{\\mathsf{G}}$ .   \n\u2022 $g_{*}$ is the minimum of $U$ on $S$ .   \n\u2022 $g(t)\\in S$ for all $t\\geq0$ .   \n\u2022 $\\log{g_{*}^{-1}g}$ and its differential is well-defined for all $g\\in S$ . ", "page_idx": 15}, {"type": "text", "text": "Then the solution of $O D E$ (2) satisfies ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\mathcal{L}^{O D E}(g(t),\\xi(t))\\leq-c_{O D E}\\mathcal{L}^{O D E}(g(t),\\xi(t))\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with the convergence rate given by ", "page_idx": 15}, {"type": "equation", "text": "$$\nc_{O D E}:=\\gamma\\operatorname*{min}\\left\\{\\frac{\\mu}{\\mu+\\gamma^{2}},\\frac{2}{3}\\right\\}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma 26. The time derivative of $L$ is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{d}{d t}C^{\\mathrm{OOE}}=\\left\\langle\\xi,T_{g}\\xi_{g^{-1}}\\nabla U(g)\\right\\rangle+\\frac{1}{2}\\big\\langle\\xi,\\gamma\\xi-T_{g}\\xi-T_{g}\\xi_{g^{-1}}\\nabla U(g)\\big\\rangle}\\\\ &{\\quad\\quad+\\frac{1}{2}\\big\\langle\\gamma\\log g_{+}^{-1}g+\\xi,\\gamma\\,\\mathrm{d}_{\\xi}\\log g_{+}^{-1}g-\\gamma\\xi-T_{g}\\xi_{g^{-1}}\\nabla U(g)\\big\\rangle}\\\\ &{\\quad\\quad=\\left\\langle\\xi,T_{g}\\xi_{g^{-1}}\\nabla U(g)\\right\\rangle-\\frac{\\gamma}{2}\\big\\langle\\xi,\\xi\\xi\\big\\rangle-\\frac{1}{2}\\big\\langle\\xi,T_{g}\\xi_{g^{-1}}\\nabla U(g)\\big\\rangle}\\\\ &{\\quad\\quad+\\frac{\\gamma^{2}}{2}\\big\\langle\\log g_{+}^{-1}g,\\phi_{+}\\log g_{-}^{-1}g\\big\\rangle-\\frac{\\gamma^{2}}{2}\\big\\langle\\log g_{+}^{-1}g,\\xi\\big\\rangle-\\frac{\\gamma}{2}\\big\\langle\\log g_{+}^{-1}g,T_{g}\\xi_{g^{-1}}\\nabla U(g)\\big\\rangle}\\\\ &{\\quad\\quad+\\frac{\\gamma}{2}\\big\\langle\\xi,\\mathrm{d}_{\\xi}\\log g_{+}^{-1}g\\big\\rangle-\\frac{\\gamma}{2}\\big\\langle\\xi,\\xi\\big\\rangle-\\frac{1}{2}\\big\\langle\\xi,\\nabla U(g)\\big\\rangle}\\\\ &{\\quad\\quad=-\\gamma\\langle\\xi,\\xi\\big\\rangle-\\frac{\\gamma}{2}\\big\\langle\\log g_{+}^{-1}g,T_{g}\\xi,\\nabla U(g)\\big\\rangle+\\frac{\\gamma}{2}\\big\\langle\\xi,\\mathrm{d}_{\\xi}\\log g_{+}^{-1}g\\big\\rangle}\\\\ &{\\quad\\quad\\quad\\leq-\\frac{\\gamma}{2}\\big\\langle\\log g_{+}^{-1}g,T_{g}\\xi_{g^{-1}}\\nabla U(g)\\big\\rangle-\\frac{\\gamma}{2}\\big\\langle\\xi,\\xi\\big\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the second last equation is by Cor. 16 and the last inequality is by Cor. 17. By the property of strong convexity in Eq. (4) and Cor. 25, for any $\\lambda\\in[0,1]$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{d}{d t}{C}^{\\mathrm{ODE}}\\leq-\\frac{\\gamma}{2}\\left(\\lambda(U(g)-U(g_{*}))+\\left(\\frac{\\lambda}{2}+(1-\\lambda)\\right)\\mu d^{2}(g_{*},g)+\\left\\|\\xi\\right\\|^{2}\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\lambda$ is a constant to be determined later. ", "page_idx": 15}, {"type": "text", "text": "Next, we try to give $\\mathcal{L}^{\\mathrm{ODE}}$ an upper bound. By Cauchy-Schwarz inequality, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\|\\gamma\\log g_{*}^{-1}g+\\xi\\right\\|^{2}\\leq2\\left(\\gamma^{2}d^{2}(g_{*},g)+\\left\\|\\xi\\right\\|^{2}\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and further ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}^{\\mathrm{ODE}}\\leq U(g)-U(g_{*})+\\frac{3}{4}\\left\\|\\xi\\right\\|^{2}+\\frac{\\gamma^{2}}{2}d^{2}(g,g_{*})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "table", "img_path": "2hqHWD7wDb/tmp/faf66dbcf27fffd8ed46e96a96d1cf5e30d280cae546b05e76da894752e87c02.jpg", "table_caption": [], "table_footnote": ["Table 3: Change of variable between Heavy-ball and splitting scheme "], "page_idx": 16}, {"type": "text", "text": "Compare the coefficients in Eq. (26) and (27), we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\mathcal{L}^{\\mathrm{ODE}}\\leq-c_{\\mathrm{ODE}}\\mathcal{L}^{\\mathrm{ODE}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the convergence rate $c_{\\mathrm{ODE}}$ is given by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{c_{\\mathrm{ODE}}:=\\displaystyle\\frac{\\gamma}{2}\\operatorname*{min}\\left\\{\\lambda,\\displaystyle\\frac{4}{3},\\displaystyle\\frac{2}{\\gamma^{2}}\\left(\\frac{\\lambda}{2}+(1-\\lambda)\\right)\\mu\\right\\}}\\\\ &{\\qquad\\qquad=\\displaystyle\\frac{\\gamma}{2}\\operatorname*{min}\\left\\{\\lambda,\\displaystyle\\frac{4}{3},\\displaystyle\\frac{\\mu}{\\gamma^{2}}(2-\\lambda)\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By selecting $\\begin{array}{r}{\\lambda=\\frac{2\\mu}{\\mu+\\gamma^{2}}}\\end{array}$ to make $\\begin{array}{r}{\\lambda=\\frac{\\mu}{\\gamma^{2}}\\big(2-\\lambda\\big)}\\end{array}$ satisfied, we have the desired result. ", "page_idx": 16}, {"type": "text", "text": "Proof of Thm. 9. This is the direct corollary from Cor. 8 and Lemma 26. ", "page_idx": 16}, {"type": "text", "text": "C More details about Heavy-Ball discretization ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Remark 27 (Polyak\u2019s Heavy ball [23]). Heavy-ball scheme in the Euclidean space is ", "page_idx": 16}, {"type": "equation", "text": "$$\nx_{k+1}=x_{k}+\\alpha\\big(x_{k}-x_{k-1}\\big)-\\beta\\nabla U\\big(x_{k}\\big)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\alpha$ and $\\beta$ are positive parameters. We now write it into a position-velocity form. By setting $\\begin{array}{r}{v_{k+1}=\\frac{x_{k+1}-x_{k}}{\\sqrt{\\beta}}}\\end{array}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\displaystyle x_{k+1}=x_{k}+\\sqrt{\\beta}v_{k+1}}\\\\ {\\displaystyle v_{k+1}=\\alpha v_{k}-\\sqrt{\\beta}\\nabla U(x_{k})}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We perform the change of variables given by $\\sqrt{\\beta}\\to h,\\,\\alpha\\to1-\\gamma h\\,\\,g i\\nu e$ es ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{x_{k+1}=x_{k}+h v_{k+1}}\\\\ {v_{k+1}=(1-\\gamma h)v_{k}-h\\nabla U(x_{k})}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which is the Euclidean version corresponding to Eq. (9). ", "page_idx": 16}, {"type": "text", "text": "Remark 28 (Splitting discretization). The two systems of ODEs in Eq. (8) are both linear and has exact solutions. We will refer to the numerical scheme evolving their exact solution alternatively by splitting discretization. More precisely, this gives us the following numerical scheme: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{g_{k+1}=g_{k}\\exp(h\\xi_{k+1})\\right.\\qquad}\\\\ {\\left.\\xi_{k+1}=e^{-\\gamma h}\\xi_{k}-\\frac{1-e^{-\\gamma h}}{\\gamma}T_{g_{k}}\\mathsf{L}_{g_{k}-1}\\nabla U(g_{k})\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "After change of variable in the Table 3, it becomes Eq. (9). ", "page_idx": 16}, {"type": "text", "text": "Eq. (28) is similar to the \u2018NAG-SC\u2019 in [26]. The authors provide a second-order approximation to the optimization ODE Eq. (2) by evolving the two ODEs in Eq. (8) in the following way in each step: 1) evolve $\\xi$ -ODE for h/2 time; 2) evolve $g$ -ODE for $h/2$ time; 3) evolve $\\xi$ -ODE for $h/2$ time again. Although this zig-zag scheme is higher-order of approximation of the optimization ODE comparing to the the splitting approximation mentioned in Rmk. 28, it still has the same condition number dependence. The reason is, we can take out the first evolution of time $h/2$ for the $\\xi$ -system and it becomes identical to the splitting scheme (with a different initial condition.) ", "page_idx": 16}, {"type": "text", "text": "Before we start the theoretical calculation, we mention that update for $\\xi$ in Heavy-Ball scheme Eq. (9) can also be written as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\xi_{k}=\\frac{1}{1-\\gamma h}\\xi_{k+1}+\\frac{h}{1-\\gamma h}T_{g_{k}}\\mathsf{L}_{g_{k}^{-1}}\\nabla U\\big(g_{k}\\big)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which will be helpful later. ", "page_idx": 17}, {"type": "text", "text": "Proof of Thm. $_{l l}$ . Using Eq. (29), we have the following calculation of $E^{\\mathrm{HB}}(g_{k},\\xi_{k})\\;-$ $E^{\\mathrm{HB}}(g_{k-1},\\xi_{k-1})$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\le\\operatorname{l}^{\\mathrm{BB}}\\big(g_{k-1}^{\\star}\\big)-E^{\\mathrm{HB}}\\big(g_{k-1},\\xi_{k-1}\\big)}\\\\ &{=U\\big(g_{k}\\big)-U\\big(g_{k-1}\\big)+\\frac{\\displaystyle(1-\\gamma\\hbar)^{2}}{2}\\,\\Big(\\|\\xi_{k}\\|^{2}-\\|\\xi_{k-1}\\|^{2}\\Big)}\\\\ &{=U\\big(g_{k-1}\\exp\\bigl(h\\xi_{k}\\bigr)\\big)-U\\big(g_{k-1}\\big)+\\frac{\\displaystyle(1-\\gamma\\hbar)^{2}}{2}\\,\\bigg(\\|\\xi_{k}\\|^{2}-\\left\\|\\frac{1}{1-\\gamma\\hbar}\\xi_{k}+\\frac{h}{1-\\gamma\\hbar}T_{g_{k-1}}\\mathbf{L}_{g_{k-1}}\\!\\!-\\!\\nabla U(g_{k-1})\\right\\|^{2}}\\\\ &{=U\\big(g_{k-1}\\exp\\bigl(h\\xi_{k}\\bigr)\\big)-U\\big(g_{k-1}\\big)-\\bigg(\\gamma\\hbar-\\frac{\\gamma^{2}\\hbar^{2}}{2}\\bigg)\\,\\|\\xi_{k}\\|^{2}-h\\big<\\xi_{k},T_{g_{k-1}}\\mathbf{L}_{g_{k-1}}\\!\\!-\\!\\nabla U(g_{k-1})\\big)-\\frac{h^{2}}{2}\\|\\nabla U(g_{k})}\\\\ &{\\le h\\Big<\\xi_{k},T_{g_{k-1}}\\mathbf{L}_{g_{k-1}}\\!\\!-\\!\\nabla U(g_{k-1})\\Big)+\\frac{L h^{2}}{2}\\|\\xi_{k}\\|^{2}-\\bigg(\\gamma\\hbar-\\frac{\\gamma^{2}h^{2}}{2}\\bigg)\\,\\|\\xi_{k}\\|^{2}-h\\big<\\xi_{k},T_{g_{k-1}}\\mathbf{L}_{g_{k-1}}\\!\\!-\\!\\nabla U(g_{k-1})\\bigg>.}\\\\ &{\\le\\frac{1}{2}\\big(h^{2}L-2\\gamma\\hbar+\\gamma^{2}h^{2}\\big)\\|\\xi_{k}\\|^{2}-\\frac{h^{2}}{2}\\|\\nabla U(g_{k-1})\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the second last inequality is the property of $L$ -smooth functions given in Eq. 20. ", "page_idx": 17}, {"type": "text", "text": "Remark 29. The design of modified energy for Heavy-Ball in Eq. (10) and Thm. 11 is new, and is different from modified potential function in existing works (e.g., [15]). Our modified energy is not designed to let the Hamiltonian system to have higher order of preserving the total energy, but is defined to ensure monotonicity of the modified energy to ensure global convergence of the numerical scheme. ", "page_idx": 17}, {"type": "text", "text": "Remark 30. We specially choose our update of numerical scheme in Eq. (9) (and also later Eq. (13) for NAG-SC) to ensure such natural form of the modified energy. If we choose to update g first and then $\\xi$ (e.g., [25]), the modified energy will have to be evaluated at different time step, $E^{H B}\\bar{(g_{k+1},\\xi_{k})}$ , for example. ", "page_idx": 17}, {"type": "text", "text": "Proof of Cor. 12. We prove this by induction. Suppose we have ${\\mathfrak{g}}_{k}\\in S_{0}$ . By the dissipation of the modified energy Thm. 11, $E^{\\mathrm{HB}}(g_{k}^{\\cdot},\\xi_{k})\\leq E^{\\mathrm{HB}}(g_{0}^{\\cdot},\\overbar{\\xi}_{0})$ . As a result, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\xi_{k+1}\\right\\|\\leq\\left(1-\\gamma h\\right)\\left\\|\\xi_{k}\\right\\|+h\\left\\|\\nabla U(g_{k})\\right\\|}\\\\ &{\\qquad\\leq\\bigl(1-\\gamma h\\bigr)\\sqrt{\\frac{2}{(1-\\gamma h)^{2}}E^{\\mathrm{HB}}\\bigl(g_{k},\\xi_{k}\\bigr)}+h\\bigl\\|\\nabla U(g_{k})\\bigr\\|}\\\\ &{\\qquad\\leq\\sqrt{2E^{\\mathrm{HB}}\\bigl(g_{0},\\xi_{0}\\bigr)}+h\\underbrace{\\operatorname*{max}_{S_{0}}\\left\\|\\nabla U\\right\\|}_{S_{0}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since we have $g_{k+\\perp}=g_{k}\\exp(h\\xi_{k+1})$ , we have $d\\big(g_{k+1},g_{k}\\big)\\leq h\\|\\xi_{k+1}\\|$ . Together with the condition that $U\\bigl(g_{k+1}\\bigr)\\leq E^{\\mathrm{HB}}\\bigl(g_{k+1},\\xi_{k+1}\\bigr)\\leq u$ , we have $g_{k+1}\\in S_{0}$ . Mathematical induction gives the desired result. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Lemma 31. Assume $\\operatorname{ad}_{X}$ is skew-adjoint for any $X\\in{\\mathfrak{g}}$ . Suppose there is a geodesically convex set $S\\subset{\\mathsf{G}}$ satisfying: ", "page_idx": 17}, {"type": "text", "text": "\u2022 $U$ is geodesically $\\mu$ -strongly convex on a convex set $S\\subset{\\mathsf{G}}$ .   \n\u2022 $g_{*}$ is the minima of $U$ on $S$ . ", "page_idx": 17}, {"type": "text", "text": "\u2022 $g_{k}\\in S$ for all $k\\in\\mathbb{N}$ . ", "page_idx": 18}, {"type": "text", "text": "\u2022 $\\log{g_{*}^{-1}g}$ and its differential is well-defined for all $g\\in S$ . ", "page_idx": 18}, {"type": "text", "text": "Then we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k+1}^{H B}-\\mathcal{L}_{k}^{H B}\\leq-b_{H B}\\mathcal{L}_{k+1}^{H B}-\\frac{h}{2(1-\\gamma h)}\\left(\\frac{3\\gamma}{4L}-\\frac{h}{1-\\gamma h}\\right)\\left\\Vert\\nabla U(g_{k+1})\\right\\Vert^{2}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $b_{H B}$ is given by ", "page_idx": 18}, {"type": "equation", "text": "$$\nb_{H B}:=\\operatorname*{min}\\left\\{\\frac{\\gamma h}{8},\\frac{\\mu h(1-\\gamma h)}{2\\gamma},\\frac{2\\gamma h}{3(1-\\gamma h)}\\right\\}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma $3l$ . For $(g_{k},\\xi_{k})$ following the Heavy-Ball scheme Eq. (9), we use the shorthand notation of $\\mathcal{L}_{k}^{\\mathrm{HB}}:=\\mathcal{L}^{\\mathrm{HB}}(g_{k},\\xi_{k})$ , which gives ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k}^{\\mathrm{HB}}=\\frac{1}{1-\\gamma h}\\left(U\\bigl(g_{k-1}\\bigr)-U\\bigl(g_{*}\\bigr)\\right)+\\frac{1}{4}\\left\\|\\xi_{k}\\right\\|^{2}+\\frac{1}{4}\\left\\|\\frac{\\gamma}{1-\\gamma h}\\log g_{*}^{-1}g_{k}+\\xi_{k}\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Evaluate of the three terms in $\\mathcal{L}_{k+1}^{\\mathrm{HB}}-\\mathcal{L}_{k}^{\\mathrm{HB}}$ separately : ", "page_idx": 18}, {"type": "text", "text": "\u2022The first term: By co-coercivity in Lemma 23, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{1}{1-\\gamma h}\\left(U(g_{k})-U(g_{k-1})\\right)}}\\\\ &{\\leq\\frac{h}{1-\\gamma h}\\bigl\\langle T_{g_{k}}\\mathsf{L}_{g_{k}^{-1}}\\nabla U(g_{k}),\\xi_{k}\\bigr\\rangle}\\\\ &{-\\,\\frac{1}{2L}\\frac{1}{1-\\gamma h}\\bigl\\|T_{g_{k}}\\mathsf{L}_{g_{k}^{-1}}\\nabla U(g_{k})-T_{g_{k-1}}\\mathsf{L}_{g_{k-1}^{-1}}\\nabla U(g_{k-1})\\bigr\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "\u2022The second term: Using Eq. (29), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{4}\\left(\\left\\|\\xi_{k+1}\\right\\|^{2}-\\left\\|\\xi_{k}\\right\\|^{2}\\right)}\\\\ &{\\displaystyle=\\frac{1}{2}\\langle\\xi_{k+1}-\\xi_{k},\\xi_{k+1}\\rangle-\\frac{1}{4}\\|\\xi_{k+1}-\\xi_{k}\\|^{2}}\\\\ &{\\displaystyle=-\\frac{\\gamma h}{2(1-\\gamma h)}\\|\\xi_{k+1}\\|^{2}}\\\\ &{\\displaystyle-\\,\\frac{h}{2(1-\\gamma h)}\\langle\\xi_{k+1},T_{g_{k}}\\xi_{g_{k}-1}L_{\\nabla}U(g_{k})\\rangle}\\\\ &{\\displaystyle-\\,\\frac{1}{4}\\|\\xi_{k+1}-\\xi_{k}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "\u2022The third term: Define a parametric curve $[0,h]\\rightarrow{\\mathsf{G}}$ as $g_{t}:=g_{k}\\exp(t\\xi_{k+1})$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{4}\\left(\\left\\|\\frac{\\gamma}{1-\\gamma h}\\log g_{*}^{-1}g_{k+1}+\\xi_{k+1}\\right\\|^{2}-\\left\\|\\frac{\\gamma}{1-\\gamma h}\\log g_{*}^{-1}g_{k}+\\xi_{k}\\right\\|^{2}\\right)}\\\\ &{=\\frac{1}{4}\\left(\\left\\|\\frac{\\gamma}{1-\\gamma h}\\log g_{*}^{-1}g_{k+1}+\\xi_{k+1}\\right\\|^{2}-\\left\\|\\frac{\\gamma}{1-\\gamma h}\\log g_{*}^{-1}g_{k}+\\xi_{k+1}\\right\\|^{2}\\right)}\\\\ &{+\\frac{1}{4}\\left(\\left\\|\\frac{\\gamma}{1-\\gamma h}\\log g_{*}^{-1}g_{k}+\\xi_{k+1}\\right\\|^{2}-\\left\\|\\frac{\\gamma}{1-\\gamma h}\\log g_{*}^{-1}g_{k}+\\xi_{k}\\right\\|^{2}\\right)}\\\\ &{=\\frac{1}{2}\\int_{0}^{h}\\left\\langle\\frac{d}{d t}\\left(\\frac{\\gamma}{1-\\gamma h}\\log g_{*}^{-1}g_{t}+\\xi_{k+1}\\right),\\frac{\\gamma}{1-\\gamma h}\\log g_{*}^{-1}g_{t}+\\xi_{k+1}\\right\\rangle d t}\\\\ &{+\\frac{1}{4}\\left(\\left\\|\\frac{\\gamma}{1-\\gamma h}\\log g_{*}^{-1}g_{k}+\\xi_{k+1}\\right\\|^{2}-\\left\\|\\frac{\\gamma}{1-\\gamma h}\\log g_{*}^{-1}g_{k}+\\xi_{k}\\right\\|^{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We evaluate the two terms separately. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{0}^{\\infty}\\biggl|\\int_{0}^{\\infty}\\biggl|\\bar{u}\\biggr|\\biggr(\\frac{1}{\\eta-1}\\sqrt{\\Delta}\\bar{u}_{0}\\log^{2}\\bar{y}_{0}+\\xi_{0}\\biggr)\\biggr|\\frac{1}{-\\eta-1}\\sqrt{\\Delta}\\bar{u}_{0}\\log^{2}\\bar{y}_{0}+\\xi_{0}\\biggr|d t}\\\\ &{=\\int_{0}^{\\infty}\\biggl|\\int_{-\\frac{1}{2}\\sqrt{\\Delta}}\\bar{u}_{0}\\log\\bar{y}_{0}^{*}\\,\\frac{1}{[1+\\eta]_{0}^{2}}\\bigl|\\bar{u}\\biggr|\\biggr|\\Delta}\\\\ &{=\\int_{0}^{\\infty}\\biggl|\\frac{1}{[1-\\eta]_{0}^{2}}\\bigl|\\bar{u}\\biggr|\\,\\frac{1}{[1+\\eta]_{0}^{2}}\\bigl|\\bar{u}\\biggr|\\,\\frac{1}{[1+\\eta]_{0}^{2}}\\bigl|\\bar{u}\\biggr|}\\\\ &{\\mathrm{~\\int~}\\int_{0}^{\\infty}\\biggl|\\bar{u}\\biggr|\\displaystyle\\frac{1}{[1+\\eta]_{0}^{2}}\\bigl|\\bar{u}\\biggr|\\,\\frac{1}{[1+\\eta]_{0}^{2}}\\bigl|\\bar{u}\\biggr|\\,d t}\\\\ &{\\leq\\int_{0}^{\\infty}\\biggl|\\bar{u}\\biggr|\\displaystyle\\frac{1}{[1+\\eta]_{0}^{2}}\\bigl|\\bar{u}\\biggr|\\,d t+\\frac{1}{[1+\\eta]_{0}^{2}}\\bigl|\\bar{u}\\biggr|\\,}\\\\ &{\\leq\\frac{1}{[1+\\eta]_{0}^{2}}\\bigl|\\int_{0}^{\\infty}\\biggl|\\bar{u}\\biggr|\\,\\frac{1}{[1+\\eta]_{0}^{2}}\\bigl|\\bar{u}\\biggr|\\,\\frac{1}{[1+\\eta]_{0}^{2}}\\bigl|\\bigl|\\bar{u}\\biggr|^{2}}\\\\ &{=\\frac{\\eta-1}{[1+\\eta]_{0}^{2}}\\bigl|\\bar{u}\\biggr|\\,\\frac{1}{[1+\\eta]_{0}^{2}}\\bigl|\\bar{u}-\\bigl|\\bar{u}_{0}\\bigr|^{2}\\,\\frac{1}{[1+\\eta]_{0}^{2}}\\bigl|\\bigl|\\bar{u}\\biggr|^{2}}\\\\ &{=-\\frac{\\eta-1}{ \n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using the $\\xi$ update in Eq. (29), we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\gamma}{1-\\gamma h}\\log g_{*}^{-1}g_{k+1}+\\xi_{k+1}\\Big\\|^{2}-\\left\\|\\frac{\\gamma}{1-\\gamma h}\\log g_{*}^{-1}g_{k}+\\xi_{k}\\right\\|^{2}}\\\\ &{=\\Big\\langle\\xi_{k+1}-\\xi_{k},\\frac{2\\gamma}{1-\\gamma h}\\log g_{*}^{-1}g_{k}+\\xi_{k+1}+\\xi_{k}\\Big\\rangle}\\\\ &{=-\\Big\\langle\\frac{\\gamma h}{1-\\gamma h}\\xi_{k+1}+\\frac{h}{1-\\gamma h}T_{g_{k}}\\xi_{g_{k}}\\!\\!-\\!\\nabla U(g_{k}),\\frac{2\\gamma}{1-\\gamma h}\\log g_{*}^{-1}g_{k}+\\frac{2-\\gamma h}{1-\\gamma h}\\xi_{k+1}+\\frac{h}{1-\\gamma h}T_{g_{k}}\\xi_{g_{k}}\\!\\!-\\!\\nabla U(g)}\\\\ &{=-\\frac{2\\gamma^{2}h}{(1-\\gamma h)^{2}}\\Big\\langle\\xi_{k+1},\\log g_{*}^{-1}g_{k}\\Big\\rangle-\\frac{\\gamma h(2-\\gamma h)}{(1-\\gamma h)^{2}}\\|\\xi_{k+1}\\|^{2}-\\frac{2\\gamma h}{(1-\\gamma h)^{2}}\\big\\langle T_{g_{k}}\\xi_{g_{k}}\\!\\!-\\!\\nabla U(g_{k}),\\log g_{*}^{-1}g_{k}\\big\\rangle}\\\\ &{-\\frac{2h}{(1-\\gamma h)^{2}}\\big\\langle T_{g_{k}}\\xi_{g_{k}}\\!\\!-\\!\\nabla U(g_{k}),\\xi_{k+1}\\big\\rangle-\\frac{h^{2}}{(1-\\gamma h)^{2}}\\|\\nabla U(g_{k})\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Sum them up, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{4}\\left(\\left\\|-\\displaystyle\\frac{\\gamma}{1-\\gamma h}\\log g_{k+1}^{-1}g_{*}+\\xi_{k+1}\\right\\|^{2}-\\left\\|-\\displaystyle\\frac{\\gamma}{1-\\gamma h}\\log g_{k}^{-1}g_{*}+\\xi_{k}\\right\\|^{2}\\right)}\\\\ &{\\leq-\\displaystyle\\frac{h}{2(1-\\gamma h)^{2}}\\bigl\\langle T_{g_{k}}\\mathsf{L}_{g_{k}-1}\\nabla U(g_{k}),\\xi_{k+1}\\bigr\\rangle}\\\\ &{-\\displaystyle\\frac{\\gamma h}{2(1-\\gamma h)^{2}}\\bigl\\langle T_{g_{k}}\\mathsf{L}_{g_{k}-1}\\nabla U(g_{k}),\\log g_{*}^{-1}g_{k}\\bigr\\rangle}\\\\ &{-\\displaystyle\\frac{h^{2}}{4(1-\\gamma h)^{2}}\\|\\nabla U(g_{k})\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Take a closer look: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac12I_{1}+I I I_{2}=\\frac{h}{2\\big(1-\\gamma h\\big)}\\big\\langle T_{g_{k}}\\mathsf{L}_{g_{k}-1}\\nabla U\\big(g_{k}\\big),\\xi_{k}\\big\\rangle-\\frac{h}{2\\big(1-\\gamma h\\big)^{2}}\\big\\langle T_{g_{k}}\\mathsf{L}_{g_{k}-1}\\nabla U\\big(g_{k}\\big),\\xi_{k+1}\\big\\rangle}\\\\ &{\\quad\\quad\\quad\\quad=\\frac{h}{2\\big(1-\\gamma h\\big)}\\bigg\\langle T_{g_{k}}\\mathsf{L}_{g_{k}-1}\\nabla U\\big(g_{k}\\big),\\xi_{k}-\\frac{1}{1-\\gamma h}\\xi_{k+1}\\bigg\\rangle}\\\\ &{\\quad\\quad\\quad=\\frac{h^{2}}{2\\big(1-\\gamma h\\big)^{2}}\\left\\|T_{g_{k}}\\mathsf{L}_{g_{k}-1}\\nabla U\\big(g_{k}\\big)\\right\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{1}{2}I_{1}+I I_{2}+I I_{3}+I I I_{3}=-\\frac{1}{4}\\left\\|\\xi_{k+1}-\\xi_{k}+\\frac{h}{1-\\gamma h}T_{g_{k}}\\mathsf{L}_{g_{k}-1}\\nabla U(g_{k})\\right\\|^{2}\\le0\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now we sum everything up ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k+1}^{\\mathrm{HB}}-\\mathcal{L}_{k}^{\\mathrm{HB}}\\leq-\\frac{\\gamma h}{2(1-\\gamma h)}\\left(\\left\\langle T_{g_{k}}\\mathsf{L}_{g_{k}^{-1}}\\nabla U(g_{k}),\\log g_{*}^{-1}g_{k}\\right\\rangle+\\left\\|\\xi_{k+1}\\right\\|^{2}\\right)+\\frac{h^{2}}{2(1-\\gamma h)}\\left\\|\\nabla U(g_{k})\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By Eq. (4) (strong convexity) and Eq. (22) (corollary of co-coercivity) of $U$ , we have $U(g_{k})-$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\cdot\\left\\|\\log g_{*}^{-1}g_{k}\\right\\|^{2}\\leq\\left\\langle T_{g_{k}}\\mathsf{L}_{g_{k}-1}\\nabla U(g_{k}),\\log g_{*}^{-1}g_{k}\\right\\rangle}\\\\ &{\\overset{}{\\mathcal{L}}_{k+1}^{\\mathrm{HB}}-\\mathcal{L}_{k}^{\\mathrm{HB}}\\leq-\\frac{\\gamma h}{2\\left(1-\\gamma h\\right)}\\left(\\frac{1}{4}(U(g_{k})-U(g_{*}))+\\frac{\\mu}{2}\\big\\|\\log g_{*}^{-1}g_{k}\\big\\|^{2}+\\big\\|\\xi_{k+1}\\big\\|^{2}\\right)}\\\\ &{\\qquad\\qquad\\qquad-\\frac{h}{2\\left(1-\\gamma h\\right)}\\left(\\frac{3}{4}\\gamma(U(g_{k})-U(g_{*}))-\\frac{h}{1-\\gamma h}\\|\\nabla U(g_{k})\\|^{2}\\right)}\\\\ &{\\qquad\\qquad\\leq-\\frac{\\gamma h}{2\\left(1-\\gamma h\\right)}\\left(\\frac{1}{4}(U(g_{k})-U(g_{*}))+\\frac{\\mu}{2}\\big\\|\\log g_{*}^{-1}g_{k}\\big\\|^{2}+\\big\\|\\xi_{k+1}\\big\\|^{2}\\right)}\\\\ &{\\qquad\\qquad-\\frac{h}{2(1-\\gamma h)}\\left(\\frac{3\\gamma}{4L}-\\frac{h}{1-\\gamma h}\\right)\\|\\nabla U(g_{k})\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Cauchy-Schwarz inequality gives ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k+1}^{\\mathrm{HB}}\\leq\\frac{1}{1-\\gamma h}\\left(U(g_{k})-U(g_{*})\\right)+\\frac{\\gamma^{2}}{2(1-\\gamma h)^{2}}\\left\\|\\log g_{*}^{-1}g_{k}\\right\\|^{2}+\\frac{3}{4}\\left\\|\\xi_{k+1}\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Comparing the coefficients in Eq. (31) and (32) gives us the desired result. ", "page_idx": 20}, {"type": "text", "text": "Lemma 32 (Monotonicity of the Lyapunov function for Heavy-Ball\u221a scheme). Assume the conditions in Lemma 31 is satisfied. By choosing $\\gamma=2\\sqrt{\\mu}$ and step size $\\begin{array}{r}{h=\\frac{\\sqrt{\\mu}}{4L}}\\end{array}$ , we have $b_{H B}={\\frac{\\mu}{16L}}$ and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k+1}^{H B}\\le c_{H B}\\mathcal{L}_{k}^{H B}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "by defining $c_{H B}:=\\big(1+b_{H B}\\big)^{-1}$ . ", "page_idx": 20}, {"type": "text", "text": "Proof of Lemma 32. By our choice of $h$ and $\\gamma$ makes $\\begin{array}{r}{\\frac{3\\gamma}{4L}-\\frac{h}{1-\\gamma h}\\geq0}\\end{array}$ , and the convergence rate by Lemma 31. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Proof of Thm. 13. This is a direct corollary of Lemma 32 and Cor. 12. ", "page_idx": 20}, {"type": "text", "text": "D More details about NAG-SC discretization ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Lemma 33. Assume $\\operatorname{ad}_{X}$ is skew-adjoint for any $X\\in{\\mathfrak{g}}$ . Suppose there is a geodesically convex set $S\\subset{\\mathsf{G}}$ satisfying: ", "page_idx": 20}, {"type": "text", "text": "\u2022 $U$ is geodesically $\\mu$ -strongly convex on a convex set $S\\subset{\\mathsf{G}}$ .   \n\u2022 $g_{*}$ is the minima of $U$ on $S$ .   \n\u2022 $g_{k}\\in S$ for all $t\\geq0$ . ", "page_idx": 20}, {"type": "text", "text": "\u2022 $\\log{g_{*}^{-1}g}$ and its differential is well-defined for all $g\\in S$ . ", "page_idx": 21}, {"type": "text", "text": "\u2022 $\\begin{array}{r}{\\operatorname*{max}_{g\\in S}d\\!\\left(g_{*},g\\right)\\leq\\frac{a}{A}}\\end{array}$ for some $a<2\\pi$ . ", "page_idx": 21}, {"type": "text", "text": "Setting $\\begin{array}{r}{h=\\operatorname*{min}\\left\\{\\frac{1}{\\sqrt{2L}},\\frac{1}{2p(a)}\\right\\}}\\end{array}$ and $\\gamma=2\\sqrt{\\mu}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{L}_{k+1}^{N A G-S C}-\\mathcal{L}_{k}^{N A G-S C}\\le b_{N A G-S C}\\mathcal{L}_{k+1}^{N A G-S C}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for the contraction rate $b_{N A G-S C}$ given by ", "page_idx": 21}, {"type": "equation", "text": "$$\nb_{N A G-S C}:=\\frac{1}{30}\\sqrt{\\mu}\\operatorname*{min}\\left\\{\\frac{1}{\\sqrt{2L}},\\frac{1}{2p(a)}\\right\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof of Lemma 33. For $(g_{k},\\xi_{k})$ following the NAG-SC scheme Eq. (13), we define the shorthand notation $\\mathcal{L}_{k}^{\\mathrm{NAG-SC}}:=\\mathcal{L}^{\\mathrm{NAG-SC}}(g_{k},\\xi_{k})$ , which gives ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\stackrel{\\mathrm{vAG.SC}}{\\sim}=\\frac{1}{1-\\gamma h}\\left(U(g_{k-1})-U(g_{*})\\right)+\\frac{1}{4}\\lVert\\xi_{k}\\rVert^{2}+\\frac{1}{4}\\left\\lVert\\xi_{k}+\\frac{\\gamma}{1-\\gamma h}\\log g_{*}^{-1}g_{k}+h\\nabla U(g_{k-1})\\right\\rVert^{2}-\\frac{h^{2}(2-\\gamma)}{4(1-\\gamma h)(1-\\gamma h)}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Evaluate of the basic terms of $\\mathcal{L}_{k+1}^{\\mathrm{NAG-SC}}-\\mathcal{L}_{k}^{\\mathrm{NAG-SC}}$ first: ", "page_idx": 21}, {"type": "text", "text": "\u2022The first term: By the property of convex $L$ -smooth functions in Eq. (21), ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\displaystyle\\frac{1}{1-\\gamma h}\\left(U(g_{k})-U(g_{k-1})\\right)}&&{}\\\\ &{\\le\\displaystyle\\frac{h}{1-\\gamma h}\\Big\\langle T_{g_{k}}\\boldsymbol{\\mathrm{L}}_{g_{k}-1}\\nabla U(g_{k}),\\xi_{k}\\Big\\rangle}&&{\\quad\\qquad\\qquad I_{1}}\\\\ &{-\\displaystyle\\frac{1}{2L}\\displaystyle\\frac{1}{1-\\gamma h}\\Big\\|T_{g_{k}}\\boldsymbol{\\mathrm{L}}_{g_{k}-1}\\nabla U\\big(g_{k}\\big)-T_{g_{k-1}}\\boldsymbol{\\mathrm{L}}_{g_{k-1}}\\nabla U\\big(g_{k-1}\\big)\\Big\\|^{2}}&&{\\quad\\qquad\\qquad\\quad I_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "\u2022The second term: NAG-SC scheme in Eq. 13 gives the following: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{n}\\bigg(|(\\xi_{\\pm1}|^{2}-|\\xi_{\\pm1}|^{2})}\\\\ &{=\\frac{1}{n}\\bigg(\\xi_{\\pm1}|^{2}\\cos_{k}(\\xi_{\\pm1})-\\frac{1}{n}|\\xi_{\\pm1}|_{L^{1}}-\\xi_{\\pm1}|^{2}}\\\\ &{=-\\frac{\\sqrt{\\pi}}{2(1-\\xi_{\\pm1})}\\big)[(\\xi_{\\pm1}|^{2}-\\frac{1}{2}\\big(\\xi_{\\pi}|_{\\infty}\\!-\\!\\xi_{\\pi}|^{2}(\\xi_{\\pi})\\!-\\!T_{\\pi_{\\Delta}}\\!\\bot_{\\xi_{\\pi}+\\sqrt{\\pi}}(\\xi_{\\pi-1}),\\xi_{\\pi_{\\Delta}+1}\\big)}\\\\ &{-\\frac{\\sqrt{\\pi}}{2(1-\\xi_{\\pi})}\\big(\\xi_{\\pi}|_{\\infty}\\!-\\!\\xi_{\\pi_{\\Delta}+\\sqrt{\\pi}}(\\xi_{\\pi})\\!-\\!\\xi_{\\pi}|_{\\xi_{\\pi}+1}\\big)\\!-\\!\\frac{1}{n}\\big(\\xi_{\\pi}\\!+\\!\\xi_{\\pi}|^{2}}\\\\ &{=-\\frac{\\sqrt{\\pi}}{2(1-\\xi_{\\pi})}\\big)\\big(\\xi_{\\pi}|_{\\infty}\\!-\\!\\xi_{\\pi}|^{2}}\\\\ &{=-\\frac{\\sqrt{\\pi}}{2(1-\\xi_{\\pi})}\\big)[(\\xi_{\\pi}|_{\\infty}\\!-\\!\\xi_{\\pi}|^{2}}\\\\ &{=\\frac{\\sqrt{\\pi}(1-\\xi_{\\pi})}{2}\\big(\\xi_{\\pi}\\!+\\!\\sqrt{\\pi}(\\xi_{\\pi})\\!-\\!T_{\\pi_{\\Delta}}\\!\\cdot\\!\\xi_{\\pi}\\!+\\!\\sqrt{\\pi}(\\xi_{\\pi})\\!-\\!\\xi_{\\pi}\\big)\\!}\\\\ &{+\\frac{\\sqrt{\\pi}(1-\\xi_{\\pi})}{2}\\big(\\frac{\\sqrt{\\pi}}{2n}\\big)\\big[\\xi_{\\pi_{\\Delta}\\pi_{\\Delta}}\\!-\\!\\sqrt{\\pi}(\\xi_{\\pi})\\!-\\!T_{\\pi_{\\Delta}}\\!\\cdot\\!\\xi_{\\pi}\\!+\\!\\sqrt{\\pi}(\\xi_{\\pi})\\big]^{2}}\\\\ &{\\quad\\frac{1}{n^{2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "\u2022The third term: We consider the parametric curve on $\\mathsf{G}$ connecting $g_{k}$ and $g_{k+1}$ defined by $g_{t}\\ =$ $g_{k}\\exp(t\\xi_{k+1}),t\\in[0,h]$ . ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bigg|\\bigg|\\xi_{k+1}+\\frac{\\gamma}{1-\\gamma\\dot{h}}\\log g_{*}^{-1}g_{k+2}+h T_{g_{k}}\\xi_{n}-\\nabla U(g_{k})\\bigg|^{2}-\\frac{1}{4}\\bigg|\\xi_{k}+\\frac{\\gamma}{1-\\gamma\\dot{h}}\\log g_{*}^{-1}g_{k}+h T_{g_{k-1}}\\xi_{g_{k-1}}\\cdot\\nabla U(g_{k})\\bigg|^{2}\\bigg|}\\\\ &{=\\frac{1}{4}\\bigg|\\xi_{k+1}+\\frac{\\gamma}{1-\\gamma\\dot{h}}\\log g_{*}^{-1}g_{k+2}+h T_{g_{k}}\\xi_{n}\\cdot\\nabla U(g_{k})\\bigg|^{2}-\\frac{1}{4}\\bigg|\\xi_{k+1}+\\frac{\\gamma}{1-\\gamma\\dot{h}}\\log g_{*}^{-1}g_{k}+h T_{g_{k}}\\xi_{n}\\cdot\\nabla U(g_{k-1})\\bigg|^{2}}\\\\ &{\\neq\\frac{1}{4}\\bigg|\\xi_{k+1}+\\frac{\\gamma}{1-\\gamma\\dot{h}}\\log g_{*}^{-1}g_{k}+h T_{g_{k}}\\xi_{n}\\cdot\\nabla U(g_{k})\\bigg|^{2}-\\frac{1}{4}\\bigg|\\xi_{k}+\\frac{\\gamma}{1-\\gamma\\dot{h}}\\log g_{*}^{-1}g_{k}+h T_{g_{k-1}}\\xi_{n_{k-1}}\\cdot\\nabla U(g_{k-1})\\bigg|^{2}}\\\\ &{=\\frac{1}{2}\\int_{0}^{\\infty}\\bigg\\langle\\xi_{k+1}+\\frac{\\gamma}{1-\\gamma\\dot{h}}\\log g_{*}^{-1}g_{k}+h T_{g_{k}}\\xi_{n}\\cdot\\nabla U(g_{k})\\bigg|^{2}-\\frac{\\gamma}{1-\\gamma\\dot{h}}\\xi_{k+1}\\log g_{*}^{-1}g_{k}\\bigg\\rangle\\,\\mathrm{d}t}\\\\ &{\\neq\\frac{1}{4}\\bigg\\langle\\xi_{k+1}+\\xi_{k}+\\frac{2\\gamma}{1-\\gamma\\dot{h}}\\log g_{*}^{-1}g_{k}+h T_{g_{k}}\\xi_{n}\\cdot\\nabla U(g_{k})+h T_{g_{k-1}}\\xi_{n_{k-1}}\\cdot\\nabla U(g_{k-1}),\\xi_{k+1} \n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "First, we estimate the term $\\begin{array}{r}{\\int_{0}^{h}\\left\\langle\\xi_{k+1}+\\frac{\\gamma}{1-\\gamma h}\\log g_{*}^{-1}g_{t}+h T_{g_{k}}\\mathsf{L}_{g_{k}-1}\\nabla U(g_{k}),\\frac{\\gamma}{1-\\gamma h}\\,\\mathrm{d}\\xi_{k+1}\\log g_{*}^{-1}g_{t}\\right\\rangle\\mathrm{d}\\,t}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "using the property of dlog in Cor. 16, 17 and 18. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\Big\\langle\\frac{\\gamma}{1-\\gamma h}\\log g_{*}^{-1}g_{t}+\\xi_{k+1}+h T_{g_{k}}\\mathbf{L}_{g_{k}-1}\\nabla U\\big(g_{k}\\big),\\mathrm{d}_{\\xi_{k+1}}\\log g_{*}^{-1}g_{t}\\Big\\rangle}}\\\\ &{=\\Big\\langle\\frac{\\gamma}{1-\\gamma h}\\log g_{*}^{-1}g_{t}+\\xi_{k+1},\\mathrm{d}_{\\xi_{k+1}}\\log g_{*}^{-1}g_{t}\\Big\\rangle}\\\\ &{+h\\Big\\langle T_{g_{k}}\\mathbf{L}_{g_{k}-1}\\nabla U\\big(g_{k}\\big),\\xi_{k+1}\\Big\\rangle+h\\Big\\langle T_{g_{k}}\\mathbf{L}_{g_{k}-1}\\nabla U\\big(g_{k}\\big),\\mathrm{d}_{\\xi_{k+1}}\\log g_{*}^{-1}g_{t}-\\xi_{k+1}\\Big\\rangle}\\\\ &{\\leq\\frac{\\gamma}{1-\\gamma h}\\Big\\langle\\log g_{*}^{-1}g_{t},\\xi_{k+1}\\Big\\rangle+\\left\\|\\xi_{k+1}\\right\\|^{2}+h\\Big\\langle T_{g_{k}}\\mathbf{L}_{g_{k}-1}\\nabla U\\big(g_{k}\\big),\\xi_{k+1}\\Big\\rangle+p(a)h\\|\\nabla U\\big(g_{k})\\|\\left\\|\\xi_{k+1}\\right\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Taking integral gives ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\int_{0}^{h}\\left\\langle\\frac{\\gamma}{1-\\gamma h}\\log g_{*}^{-1}g_{t}+\\xi_{k+1}+h T g_{k}\\log\\log\\mathcal{S}_{k}^{-1}\\nabla U(g_{k}),\\mathrm{d}_{\\xi_{k+1}}\\log g_{*}^{-1}g_{t}\\right\\rangle\\mathrm{d}t}\\\\ {\\displaystyle\\leq\\int_{0}^{h}\\frac{\\gamma}{1-\\gamma h}\\bigl\\langle\\log g_{*}^{-1}g_{t},\\xi_{k+1}\\bigr\\rangle+\\|\\xi_{k+1}\\|^{2}+h\\bigl\\langle T_{g_{k}}\\log\\mathsf{L}_{g_{k}}\\,\\nabla U(g_{k}),\\xi_{k+1}\\bigr\\rangle+p(a)h\\|\\nabla U(g_{k})\\|\\|\\xi_{k+1}\\|\\,\\mathrm{d}t}\\\\ {\\displaystyle=\\frac{\\gamma}{1-\\gamma h}\\,\\int_{0}^{h}\\left\\langle\\log g_{*}^{-1}g_{t},\\xi_{k+1}\\right\\rangle\\mathrm{d}t+h\\left\\|\\xi_{k+1}\\right\\|^{2}+h^{2}\\bigl\\langle T_{g_{k}}\\log\\mathsf{L}_{g_{k}-1}\\nabla U(g_{k}),\\xi_{k+1}\\bigr\\rangle+h^{2}p(a)\\|\\nabla U(g_{k})\\|\\|\\xi_{k+1}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We calculate the integral ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{0}^{h}\\left\\langle\\log g_{*}^{-1}g_{t},\\xi_{k+1}\\right\\rangle\\mathrm{d}\\,t}\\\\ &{=\\int_{0}^{h}\\left[\\left\\langle\\log g_{*}^{-1}g_{k},\\xi_{k+1}\\right\\rangle+\\int_{0}^{t}\\left\\langle\\mathrm{d}_{\\xi_{k+1}}\\log g_{*}^{-1}g_{\\tau},\\xi_{k+1}\\right\\rangle\\mathrm{d}\\,\\tau\\right]\\mathrm{d}\\,t}\\\\ &{\\leq h\\bigl\\langle\\log g_{*}^{-1}g_{k},\\xi_{k+1}\\bigr\\rangle+\\int_{0}^{h}\\int_{0}^{t}\\left\\|\\xi_{k+1}\\right\\|^{2}\\mathrm{d}\\,\\tau\\,\\mathrm{d}\\,t}\\\\ &{=h\\bigl\\langle\\log g_{*}^{-1}g_{k},\\xi_{k+1}\\bigr\\rangle+\\frac{h^{2}}{2}\\|\\xi_{k+1}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which gives us ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{0}^{h}\\Big\\langle\\xi_{k+1}+\\frac{\\gamma}{1-\\gamma h}\\log g_{*}^{-1}g_{t}+h T_{g_{k}}\\mathsf{L}_{g_{k}-1}\\nabla U\\big(g_{k}\\big),\\mathsf{d}_{\\xi_{k+1}}\\log g_{*}^{-1}g_{t}\\Big\\rangle\\,\\mathrm{d}t}\\\\ &{\\le\\displaystyle\\frac{\\gamma h}{1-\\gamma h}\\Big\\langle\\log g_{*}^{-1}g_{k},\\xi_{k+1}\\Big\\rangle+\\frac{h\\big(2-\\gamma h\\big)}{2\\big(1-\\gamma h\\big)}\\|\\xi_{k+1}\\|^{2}}\\\\ &{+\\,h^{2}\\big\\langle T_{g_{k}}\\mathsf{L}_{g_{k}-1}\\nabla U\\big(g_{k}\\big),\\xi_{k+1}\\big\\rangle+h^{2}p\\big(a\\big)\\|\\nabla U\\big(g_{k}\\big)\\|\\|\\xi_{k+1}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The other term ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(2-\\gamma h)\\xi_{k+1}+2\\gamma\\log g_{*}^{-1}g_{k}+(3-2\\gamma h)h T g_{k}\\mathsf{L}_{g_{k}-1}\\nabla U(g_{k}),-\\gamma h\\xi_{k+1}-h T g_{k}\\mathsf{L}_{g_{k}-1}\\nabla U(g_{k})\\rangle}\\\\ &{=-\\gamma h(2-\\gamma h)\\left\\|\\xi_{k+1}\\right\\|^{2}-2\\gamma^{2}h\\bigl\\langle\\log g_{*}^{-1}g_{k},\\xi_{k+1}\\bigr\\rangle-\\gamma h^{2}(3-2\\gamma h)\\bigl\\langle T g_{k}\\mathsf{L}_{g_{k}-1}\\nabla U(g_{k}),\\xi_{k+1}\\bigr\\rangle}\\\\ &{-h(2-\\gamma h)\\bigl\\langle T g_{k}\\mathsf{L}_{g_{k}-1}\\nabla U(g_{k}),\\xi_{k+1}\\bigr\\rangle-2\\gamma h\\bigl\\langle\\log g_{*}^{-1}g_{k},T g_{k}\\mathsf{L}_{g_{k}-1}\\nabla U(g_{k})\\bigr\\rangle-h^{2}(3-2\\gamma h)\\left\\|T g_{k}\\mathsf{L}_{g_{k}-1}\\nabla U(g_{k})\\right\\|}\\\\ &{=-\\gamma h(2-\\gamma h)\\left\\|\\xi_{k+1}\\right\\|^{2}-2\\gamma^{2}h\\bigl\\langle\\log g_{*}^{-1}g_{k},\\xi_{k+1}\\bigr\\rangle-2h\\bigl(1+\\gamma h-\\gamma^{2}h^{2}\\bigr)\\bigl\\langle T g_{k}\\mathsf{L}_{g_{k}-1}\\nabla U(g_{k}),\\xi_{k+1}\\bigr\\rangle}\\\\ &{-2\\gamma h\\bigl\\langle\\log g_{*}^{-1}g_{k},T g_{k}\\mathsf{L}_{g_{k}-1}\\nabla U(g_{k})\\bigr\\rangle-h^{2}(3-2\\gamma h)\\left\\|\\nabla U(g_{k})\\right\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Summing them up, we have ", "text_level": 1, "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bigg|\\xi_{k+1}+\\frac{\\gamma}{1-\\gamma h}\\log{g_{*}^{-1}g_{k+1}}+h Z_{\\mathtt{p}_{k}}\\xi_{n}\\gamma(g_{k})\\bigg|^{2}-\\bigg|\\xi_{k}+\\frac{\\gamma}{1-\\gamma h}\\log{g_{*}^{-1}g_{k}}+h T_{\\mathtt{p}_{k-1}}\\xi_{n,\\dots1}\\nabla U\\big(\\overline{{g}}(\\overline{\\nu})\\big)\\bigg|^{2}\\bigg|}\\\\ &{\\lesssim\\frac{2\\gamma^{2}h}{(1-\\gamma h)^{2}}\\Big|(\\log{g_{*}^{-1}g_{k}},\\xi_{k+1})+\\frac{\\gamma h(2-\\gamma h)}{(1-\\gamma h)^{2}}\\Big|\\xi_{k+1}\\Big|^{2}}\\\\ &{\\triangleq\\frac{2\\gamma h}{(1-\\gamma h)^{2}}\\Big(h_{\\mathtt{p}_{k-1}}\\nabla U(g_{k}),\\xi_{k+1}\\Big)+\\frac{2\\gamma h}{(1-\\gamma h)^{2}}\\Big|\\phi(a)\\|\\nabla U(g_{k})\\|\\big|\\mathbb{E}_{k+1}\\bigg|}\\\\ &{-\\frac{\\gamma h(2-\\gamma h)}{(1-\\gamma h)^{2}}\\big|\\xi_{k+1}\\big|^{2}-\\frac{2\\gamma^{2}h}{(1-\\gamma h)^{2}}\\big|(\\log{g_{*}^{-1}g_{k}},\\xi_{k+1})-\\frac{2h(1+\\gamma h-\\gamma h^{2})}{(1-\\gamma h)^{2}}\\big\\langle Z_{\\mathtt{p}_{k}}\\xi_{n,\\dots1}\\nabla U(g_{k}),\\xi_{k+1}\\big|}\\\\ &{-\\frac{2\\gamma h}{(1-\\gamma h)^{2}}\\big|(\\log{g_{*}^{-1}g_{k}},T_{\\mathtt{p}_{k}}\\xi_{n,\\dots1}\\nabla U(g_{k})\\big)-\\frac{h^{2}(3-2\\gamma h)}{(1-\\gamma h)^{2}}\\big|T_{\\mathtt{p}_{k}}\\xi_{n}\\cdot\\nabla U(g_{k})\\big|^{2}}\\\\ &{\\lesssim\\frac{2\\gamma h^{2}}{1-\\gamma h}\\Big|\\Big|\\nabla U(g_{*})\\xi(U)\\|\\xi_{k+1}\\Big|-\\frac{2h}{(1-\\gamma h)^{2}}\\Big|\\mathcal{T}_{\\mathtt{p}_{k\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Eventually, the third term becomes ", "text_level": 1, "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\frac{1}{4}\\bigg\\|\\xi_{k+1}+\\frac{\\gamma}{1-\\gamma\\hbar}\\log g_{*}^{-1}g_{k+1}+\\hbar T_{g_{k}}\\xi_{g_{k}-1}\\nabla(g_{k})\\bigg\\|^{2}-\\frac{1}{4}\\bigg\\|\\xi_{k}+\\frac{\\gamma}{1-\\gamma\\hbar}\\log g_{*}^{-1}g_{k}+\\hbar T_{g_{k}-1}\\xi_{g_{k-1}-1}\\nabla\\xi_{g_{k-1}-1}}\\\\ &{=-\\frac{\\hbar\\gamma}{2(1-\\gamma\\hbar)^{2}}\\langle T_{g_{k}}\\mathbf{L}_{g_{k}-1}\\nabla U(g_{k}),\\log g_{*}^{-1}g_{k}\\rangle}&{I I I}\\\\ &{-\\frac{\\hbar}{2(1-\\gamma\\hbar)^{2}}\\langle T_{g_{k}}\\mathbf{L}_{g_{k}-1}\\nabla U(g_{k}),\\xi_{k+1}\\rangle}&{I I I}\\\\ &{-\\frac{\\hbar^{2}}{2(1-\\gamma\\hbar)}\\|T_{g_{k}}\\mathbf{L}_{g_{k}-1}\\nabla U(g_{k})\\|^{2}}&{I I I}\\\\ &{-\\frac{\\hbar^{2}}{4(1-\\gamma\\hbar)^{2}}\\big\\|T_{g_{k}}\\mathbf{L}_{g_{k}-1}\\nabla U(g_{k})\\big\\|^{2}}&{I I I}\\\\ &{-\\frac{\\hbar^{2}}{4(1-\\gamma\\hbar)^{2}}\\big\\|T_{g_{k}}\\mathbf{L}_{g_{k}-1}\\nabla U(g_{k})\\big\\|^{2}}&{I I I}\\\\ &{+\\frac{\\gamma\\hbar^{2}}{2(1-\\gamma\\hbar)}p(a)\\|\\nabla U(g_{k})\\|\\|\\xi_{k+1}\\|}&{\\mathrm{carem~for~curatur}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We sum the all the terms up to get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\begin{array}{r l r}&{\\alpha:=}&{\\cfrac{\\gamma_{h}}{2(1-\\gamma)h}\\cdot\\bigg(\\frac{1}{\\gamma-h}\\bigg(T_{\\alpha_{h}}\\mathbf{1}_{h_{h}+\\gamma}\\nabla T(\\rho_{h}),\\log_{\\beta_{h}^{-1}h_{h}}\\bigg)+|\\xi_{\\xi_{h}}|^{2}\\bigg)}&{\\qquad\\quad I I_{1}+\\mathit{I I}I_{1}}\\\\ &{=}&{\\cfrac{\\gamma_{h}}{2(1-\\gamma)h}\\bigg(T_{\\alpha_{h}+\\gamma}\\nabla T(\\rho_{h}),\\frac{1}{1-\\gamma}\\beta_{h}+\\xi_{h}+T_{\\alpha_{h}}\\mathbf{1}_{h_{h}+\\gamma}\\nabla(\\rho_{h})\\bigg)}&{\\frac{1}{2}\\boldsymbol{1}_{h}+\\mathit{I I}_{2}+\\mathit{I I}_{2}}\\\\ &{=}&{\\cfrac{\\gamma_{h}(1-\\gamma)}{2}\\left(T_{\\alpha_{h}+\\gamma}\\nabla T(\\rho_{h})-T_{\\alpha_{h},1},\\log_{\\alpha_{h}-\\gamma}\\nabla T(\\rho_{h}),\\xi_{h}\\right)}&{\\qquad\\quad I I_{2}}\\end{array}}\\\\ &{\\begin{array}{r l r}&{+\\frac{h^{2}}{2}\\left(T_{\\alpha_{h},h}\\cdot\\nabla T(\\rho(h_{h})-T_{\\alpha_{h},1},\\log_{\\alpha_{h}},\\gamma\\nabla T(\\rho_{h}))\\right)}&{\\qquad\\quad I I_{1}}\\\\ &{=}&{\\cfrac{\\gamma_{h}}{2(1-\\gamma)h}\\left(T_{\\alpha_{h}+\\gamma}\\nabla T(\\rho_{h})-T_{\\alpha_{h},1},\\log_{\\alpha_{h}},\\gamma\\nabla T(\\rho_{h})\\right)}&{\\qquad\\quad\\alpha_{h}}\\end{array}}\\\\ &{\\begin{array}{r l r}&{+\\frac{1}{4}\\left(\\cfrac{2h}{1-\\gamma}\\hat{h}(\\xi_{h}+\\mathrm{I}_{h},T_{\\alpha_{h}}\\nabla T(\\rho_{h}))+|\\xi_{h}|_{+}-\\xi_{h}|^{2}+\\frac{h^{2}}{2}\\right)^{2}|\\nabla T(\\rho_{h})|^{2}\\Biggr)}&{\\qquad\\quad\\alpha_{h}}\\\\ &{=}&{\\cfrac{1}{2}\\left(T_{\\alpha_{h}-\\gamma}\\hat{h}-\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\displaystyle\\frac{1}{2}I_{1}+I I I_{2}+I I I_{3}}}&{{\\mathrm{}}}\\\\ {{\\displaystyle=\\frac{h^{2}}{2(1-\\gamma h)}\\bigl\\langle T_{g_{k}}\\boldsymbol{\\mathrm{L}}_{g_{k}}\\boldsymbol{\\cdot}\\boldsymbol{\\nabla}U(g_{k})-T_{g_{k-1}}\\boldsymbol{\\mathrm{L}}_{g_{k-1}}\\boldsymbol{\\cdot}\\boldsymbol{\\nabla}U(g_{k-1}),T_{g_{k}}\\boldsymbol{\\mathrm{L}}_{g_{k}^{\\perp}}\\boldsymbol{\\cdot}\\boldsymbol{\\nabla}U(g_{k})\\bigr\\rangle\\qquad}}&{{{}}}\\\\ {{\\displaystyle+\\\\\\\\frac{\\gamma h^{3}}{2(1-\\gamma h)^{2}}\\bigl\\|T_{g_{k}}\\boldsymbol{\\mathrm{L}}_{g_{k}^{\\perp}}\\boldsymbol{\\cdot}\\boldsymbol{\\nabla}U(g_{k})\\bigr\\|^{2}}}&{{{}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{k+1}^{N M,\\mathrm{grid};S}-C_{k}^{N M;\\mathrm{grid};S}}\\\\ &{\\lesssim\\frac{\\gamma_{h}}{2(1-\\gamma)h}\\left(\\frac{1}{1-\\gamma h}\\left(T_{g h}\\mathbb{L}_{\\theta\\times1}\\!\\!-\\!\\nabla U(g_{k}),\\log_{\\theta}^{-1}g_{k}\\right)\\!-\\!h^{2}\\|\\nabla U(g_{k})\\|^{2}\\right)+\\|\\xi_{k+1}\\|^{2}\\right)}\\\\ &{\\quad-\\frac{1}{2}\\gamma_{h}^{N}\\Big\\{P_{h}\\mathbb{L}_{\\theta\\times1}\\nabla U(g_{k})-T_{g-1}\\mathbb{L}_{\\theta\\times1}\\!+\\!\\nabla U(g_{k-1}),\\log_{\\theta}^{-1}g_{k}\\Big\\}}\\\\ &{\\quad+\\frac{h^{2}}{2}\\frac{2-\\gamma h}{1-\\gamma h}\\Big\\{T_{g h}\\mathbb{L}_{\\theta\\times1}\\!\\cdot\\!\\nabla U(g_{k})-T_{g h-1}\\mathbb{L}_{\\theta\\times1}\\!\\cdot\\!\\nabla U(g_{k-1}),T_{g h-1}\\mathbb{L}_{\\theta\\times1}\\!\\cdot\\!\\nabla U(g_{k-1})\\Big\\}\\quad\\quad}\\\\ &{\\quad-\\frac{1}{2}\\left(\\frac{1}{1-\\gamma h}\\frac{1}{h}-h^{2}(1-\\gamma h)\\right)\\Big\\Vert T_{m}\\mathbb{L}_{\\theta\\times1}\\!\\cdot\\!\\nabla U(g_{k})-T_{g_{k-1}}\\mathbb{L}_{\\theta\\times1}\\!+\\!\\nabla U(g_{k-1})\\Big\\}^{2}}\\\\ &{\\quad-\\frac{h^{2}}{4}\\frac{2-\\gamma h}{1-\\gamma h}\\Big(\\nabla U(g_{k})\\Big\\|^{2}-\\|\\nabla U(g_{k-1})\\|^{2}\\Big)}\\\\ &{\\quad+\\frac{h^{2}}{2(1-\\gamma h)}p^{(n)}\\|\\nabla U(g_{k})\\|^{2}\\|\\nabla U(g_{k-1})\\|^{2}\\Big)\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\mathrm{~callingand~term}}\\\\ &{\\quad+\\frac{\\gamma_{h}^{2}}{2(1-\\gamma h)}p^{(n)}\\|\\nabla U(g_{k})\\|\\|\\zeta_{k+1}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\nI I_{4}+I V_{1}+\\mathrm{Additional\\term}=\\frac{h^{2}}{4}\\frac{2-\\gamma h}{1-\\gamma h}\\bigl\\|T_{g_{k}}\\mathsf{L}_{g_{k}-1}\\nabla U\\bigl(g_{k}\\bigr)-T_{g_{k-1}}\\mathsf{L}_{g_{k-1}^{-1}}\\nabla U\\bigl(g_{k-1}\\bigr)\\bigr\\|^{2}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{}}&{{\\displaystyle\\left(I I_{4}+I V_{1}+\\mathrm{Additional\\;term}\\right)+\\left(I_{2}+I I_{3}\\right)}}\\\\ {{}}&{{\\displaystyle\\leq\\frac{h^{2}}{2}\\left(1-\\gamma h+\\frac{1}{1-\\gamma h}-\\frac{1}{1-\\gamma h}\\frac{1}{L h^{2}}\\right)\\left\\|T_{g_{k}}\\boldsymbol{\\mathrm{L}}_{g_{k}-1}\\nabla U\\big(g_{k}\\big)-T_{g_{k-1}}\\boldsymbol{\\mathrm{L}}_{g_{k-1}-1}\\nabla U\\big(g_{k-1}\\big)\\right\\|^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{k+1}^{\\mathrm{NAG-SC}}-\\mathcal{L}_{k}^{\\mathrm{NAG-SC}}}\\\\ &{\\quad\\le-\\displaystyle\\frac{\\gamma h}{2(1-\\gamma h)}\\left(\\frac{1}{1-\\gamma h}\\left(\\left\\langle T_{g_{k}}\\boldsymbol{\\mathrm{L}}_{g_{k}}\\!\\!-\\!\\nabla U\\big(g_{k}\\big),\\log g_{*}^{-1}g_{k}\\right\\rangle-h^{2}\\|\\nabla U\\big(g_{k}\\big)\\|^{2}\\right)+\\|\\xi_{k+1}\\|\\right)}\\\\ &{\\quad-\\displaystyle\\frac{1-\\gamma h}{2}\\Big\\langle T_{g_{k}}\\boldsymbol{\\mathrm{L}}_{g_{k}}\\!\\cdot\\!\\nabla U\\big(g_{k}\\big)-T_{g_{k-1}}\\boldsymbol{\\mathrm{L}}_{g_{k-1}}\\!\\cdot\\!\\nabla U\\big(g_{k-1}\\big),\\log g_{k-1}^{-1}g_{k}\\Big\\rangle}\\\\ &{\\quad+\\displaystyle\\frac{h^{2}}{2}\\left(1\\!-\\!\\gamma h+\\!\\frac{1}{1-\\gamma h}-\\frac{1}{1-\\gamma h}\\frac{1}{L h^{2}}\\right)\\left\\|T_{g_{k}}\\boldsymbol{\\mathrm{L}}_{g_{k}-1}\\nabla U\\big(g_{k}\\big)-T_{g_{k-1}}\\boldsymbol{\\mathrm{L}}_{g_{k-1}-1}\\nabla U\\big(g_{k-1}\\big)\\right\\|^{2}}\\\\ &{\\quad+\\displaystyle\\frac{\\gamma h^{2}}{2(1-\\gamma h)}p(a)\\big\\|T_{g_{k}}\\boldsymbol{\\mathrm{L}}_{g_{k}-1}\\nabla U\\big(g_{k}\\big)\\big\\|\\left\\|\\xi_{k+1}\\right\\|\\qquad\\qquad\\qquad\\qquad\\mathrm{~extra~term~from~curv}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By the property of $L$ -smoothness in Eq. (22), we have ", "page_idx": 25}, {"type": "equation", "text": "$$\nT_{g_{k}}\\boldsymbol{\\ L}_{g_{k}-1}\\nabla U(g_{k})-T_{g_{k-1}}\\boldsymbol{\\ L}_{g_{k-1}}\\!\\!-\\!\\nabla U(g_{k-1}),\\log g_{k-1}^{-1}g_{k}\\rangle\\rangle\\geq{\\frac{1}{L}}\\|T_{g_{k}}\\boldsymbol{L}_{g_{k}-1}\\nabla U(g_{k})-T_{g_{k-1}}\\boldsymbol{\\ L}_{g_{k-1}}\\!\\!-\\!\\nabla U(g_{k-1})\\rangle\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\sqrt{\\mathrm{NAG.SC}}}{k+1}-\\mathcal{L}_{k}^{\\mathrm{NAG.SC}}\\leq-\\,\\displaystyle\\frac{\\gamma h}{2(1-\\gamma h)}\\left(\\frac{1}{1-\\gamma h}\\left(\\left\\langle T_{g_{k}}\\boldsymbol{\\mathrm{L}}_{g_{k}-1}\\nabla U(g_{k}),\\log g_{*}^{-1}g_{k}\\right\\rangle-h^{2}\\|\\nabla U(g_{k})\\|^{2}\\right)+\\|\\xi_{k+1}\\|\\right.}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\left.-\\,\\displaystyle\\frac{1}{2}\\left(1-\\gamma h+\\frac{1}{1-\\gamma h}\\right)\\left(\\frac{1}{L}-h^{2}\\right)\\left\\|T_{g_{k}}\\boldsymbol{\\mathrm{L}}_{g_{k}-1}\\nabla U(g_{k})-T_{g_{k-1}}\\boldsymbol{\\mathrm{L}}_{g_{k-1}-1}\\nabla U(g_{k-1})\\right\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\left.+\\,\\displaystyle\\frac{\\gamma h^{2}}{2(1-\\gamma h)}p(a)\\|\\nabla U(g_{k})\\|\\left\\|\\xi_{k+1}\\right\\|\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "When $\\begin{array}{r}{h\\leq\\frac{1}{\\sqrt{2L}}}\\end{array}$ , together with Lemma 33 and Cauchy-Schwarz inequality, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{k+1}^{\\mathrm{NAG-SC}}-\\mathcal{L}_{k}^{\\mathrm{NAG-SC}}}\\\\ &{\\;\\leq-\\displaystyle\\frac{\\gamma h}{2(1-\\gamma h)}\\left(\\frac{1}{1-\\gamma h}\\left(\\left\\langle T_{g_{k}}\\boldsymbol{\\mathrm{L}}_{g_{k}}\\!\\cdot\\!\\nabla U\\big(g_{k}\\big),\\log g_{*}^{-1}g_{k}\\right\\rangle-h^{2}\\|\\nabla U\\big(g_{k}\\big)\\|^{2}\\right)+\\left\\|\\xi_{k+1}\\right\\|^{2}\\right)}\\\\ &{\\;+\\displaystyle\\frac{\\gamma h^{2}}{2(1-\\gamma h)}p(a)\\|\\nabla U\\big(g_{k})\\|\\left\\|\\xi_{k+1}\\right\\|}\\\\ &{\\;=-\\displaystyle\\frac{\\gamma h}{2(1-\\gamma h)}\\left(\\frac{1}{1-\\gamma h}\\left(\\left\\langle T_{g_{k}}\\boldsymbol{\\mathrm{L}}_{g_{k}}\\!\\cdot\\!\\nabla U\\big(g_{k}\\big),\\log g_{*}^{-1}g_{k}\\right\\rangle-h^{2}\\|\\nabla U\\big(g_{k}\\big)\\|^{2}\\right)+\\left\\|\\xi_{k+1}\\right\\|^{2}\\right)}\\\\ &{\\;+\\displaystyle\\frac{\\gamma h^{2}}{4(1-\\gamma h)}p(a)\\left(\\lambda\\|\\nabla U\\big(g_{k}\\big)\\|^{2}+\\lambda^{-1}\\|\\xi_{k+1}\\|^{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\lambda>0$ is a parameter to be chosen later. Using property of $L$ -smoothness in Eq. (21), (22) and property of $\\mu$ -strong convexity in Eq. (24), we have for any $\\lambda_{1}\\geq0$ and $\\lambda_{2}\\geq1$ satisfying $\\lambda_{1}+\\lambda_{2}\\geq1$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{2(1-\\gamma h)^{2}}{\\gamma h}\\left(C_{k+1}^{\\mathrm{NA}\\times\\mathbf{C}}-C_{k}^{\\mathrm{NA}\\times\\mathbf{S}}C\\right)}\\\\ &{\\leq-\\Big(T_{\\#_{1}}\\mathbf{L}_{\\#_{2}}\\!-\\!\\nabla U(\\varrho_{k})\\log\\bar{\\mathbf{z}}_{1}^{-1}\\varrho_{k}\\Big)}\\\\ &{+h^{2}\\|\\nabla U(\\varrho_{k})\\|^{2}-(1-\\gamma h)\\|\\xi_{k+1}\\|^{2}+\\frac{h(1-\\gamma h)\\lambda p(a)}{2}\\|\\nabla U(\\varrho_{k})\\|^{2}+\\frac{h(1-\\gamma h)\\lambda^{-1}p(a)}{2}\\|\\xi_{k+1}\\|^{2}}\\\\ &{\\leq-(2-\\lambda_{1}-\\lambda_{2})(U(\\varrho_{k})-U(\\varrho_{*}))-\\frac{\\lambda_{1}}{2}\\|\\nabla U(\\varrho_{k})\\|^{2}-\\frac{\\mu\\lambda_{2}}{2}d\\varrho(\\varrho_{,9}+)}\\\\ &{+h^{2}\\|\\nabla U(\\varrho_{k})\\|^{2}-(1-\\gamma h)\\|\\xi_{k+1}\\|^{2}+\\frac{h(1-\\gamma h)\\lambda p(a)}{2}\\|\\nabla U(\\varrho_{k})\\|^{2}+\\frac{h(1-\\gamma h)\\lambda^{-1}p(a)}{2}\\|\\xi_{k+1}\\|^{2}}\\\\ &{\\leq-(2-\\lambda_{1}-\\lambda_{2})(U(\\varrho_{k})-U(\\varrho_{*}))+\\Big(h(1-\\gamma h)\\lambda p(a)+h^{2}-\\frac{\\lambda_{1}}{2L}\\Big)\\|\\nabla U(\\varrho_{k})\\|^{2}-\\frac{\\mu\\lambda_{2}}{2}d\\varrho(\\varrho_{,9})}\\\\ &{-(1-\\gamma h)\\Big(1-\\frac{h\\lambda^{-1}p(a)}{2}\\Big)\\|\\xi_{k+1}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now we try to upper bound $\\mathcal{L}_{k}^{\\mathrm{NAG-SC}}$ . Using Cauchy-Schwarz inequality, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\displaystyle\\xi_{k}+\\frac{2\\gamma}{1-\\gamma h}\\log g_{*}^{-1}g_{k}+h T_{g_{k-1}}\\lfloor_{g_{k-1}-1}\\nabla U\\big(g_{k-1}\\big)\\Big|^{2}\\right.}\\\\ &{\\left.\\leq3\\left[\\|\\xi_{k}\\|^{2}+\\left(\\frac{2\\gamma}{1-\\gamma h}\\right)^{2}d^{2}\\big(g_{*},g_{k}\\big)+h^{2}\\|\\nabla U\\big(g_{k-1}\\big)\\|^{2}\\right]\\right.}\\\\ &{\\left.\\leq3\\left[\\|\\xi_{k}\\|^{2}+\\left(\\frac{2\\gamma}{1-\\gamma h}\\right)^{2}\\left(d\\big(g_{*},g_{k}\\big)+h\\|\\xi_{k}\\|\\right)^{2}+h^{2}\\|\\nabla U\\big(g_{k-1}\\big)\\|^{2}\\right]\\right.}\\\\ &{\\left.\\leq3\\left[\\left(1+2\\left(\\frac{2\\gamma h}{1-\\gamma h}\\right)^{2}\\right)\\|\\xi_{k}\\|^{2}+2\\left(\\frac{2\\gamma}{1-\\gamma h}\\right)^{2}d^{2}\\big(g_{*},g_{k}\\big)+h^{2}\\|\\nabla U\\big(g_{k-1}\\big)\\|^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "As a result, LkNAG-SC ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lesssim\\frac{1}{1-\\gamma h}\\left(U(g_{k-1})-U(g_{*})\\right)+\\left(\\frac{1}{4}+\\frac{3}{4}+\\frac{6\\gamma^{2}h^{2}}{\\left(1-\\gamma h\\right)^{2}}\\right)\\left\\|\\xi_{k}\\right\\|^{2}}\\\\ &{+\\frac{6\\gamma^{2}}{\\left(1-\\gamma h\\right)^{2}}d^{2}(g_{*},g_{k})+\\left(\\frac{3}{4}-\\frac{\\left(2-\\gamma h\\right)}{4(1-\\gamma h\\right)}\\right)h^{2}\\|\\nabla U(g_{k-1})\\|^{2}}\\\\ &{=\\frac{1}{1-\\gamma h}\\left(U(g_{k-1})-U(g_{*})\\right)+\\frac{1-2\\gamma h+7\\gamma^{2}h^{2}}{\\left(1-\\gamma h\\right)^{2}}\\|\\xi_{k}\\|^{2}+\\frac{6\\gamma^{2}}{\\left(1-\\gamma h\\right)^{2}}d^{2}(g_{*},g_{k})+\\frac{1-2\\gamma h}{4(1-\\gamma h)}h^{2}\\|\\nabla U(g_{*},g_{k})\\|^{2}}\\\\ &{\\leqslant\\left(\\frac{1}{1-\\gamma h}+\\frac{1-2\\gamma h}{4(1-\\gamma h)}\\frac{h^{2}}{2L}\\right)\\left(U(g_{k-1})-U(g_{*})\\right)+\\frac{1-2\\gamma h+7\\gamma^{2}h^{2}}{4(1-\\gamma h)^{2}}\\|\\xi_{k}\\|^{2}+\\frac{3\\gamma^{2}}{2(1-\\gamma h)^{2}}d^{2}(g_{*},g_{k})}\\\\ &{\\leqslant\\frac{5-2\\gamma h}{4(1-\\gamma h)}\\left(U(g_{k-1})-U(g_{*})\\right)+\\frac{1-2\\gamma h+7\\gamma^{2}h^{2}}{\\left(1-\\gamma h\\right)^{2}}\\|\\xi_{k}\\|^{2}+\\frac{3\\gamma^{2}}{2(1-\\gamma h)^{2}}d^{2}(g_{*},g_{k})}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which is the same as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2(1-\\gamma h)\\mathcal{L}_{k}^{\\mathrm{NAG-SC}}}\\\\ &{\\leq\\displaystyle\\frac{5-2\\gamma h}{2}\\left(U(g_{k-1})-U(g_{*})\\right)+\\frac{2(1-2\\gamma h+7\\gamma^{2}h^{2})}{1-\\gamma h}\\left\\|\\xi_{k}\\right\\|^{2}+\\frac{3\\gamma^{2}}{1-\\gamma h}d^{2}(g_{*},g_{k})}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We suppose ", "page_idx": 26}, {"type": "equation", "text": "$$\n{\\frac{h(1-\\gamma h)\\lambda p(a)}{2}}+h^{2}-{\\frac{\\lambda_{1}}{2L}}\\leq0\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "As a result, by comparing the parameters in Eq. (38) and (37), we have the contraction rate is ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{b_{\\mathrm{NAG-SC}}\\geq\\displaystyle\\frac{\\gamma h}{1-\\gamma h}\\operatorname*{min}\\left\\lbrace\\frac{2(2-\\lambda_{1}-\\lambda_{2})}{5-2\\gamma h},\\left(1-\\frac{h\\lambda^{-1}p(a)}{2}\\right)\\frac{(1-\\gamma h)^{2}}{2(1-2\\gamma h+7\\gamma^{2}h^{2})},\\frac{\\mu\\lambda_{2}(1-\\gamma h)}{6\\gamma^{2}}\\right\\rbrace}\\\\ &{=\\gamma h\\operatorname*{min}\\left\\lbrace\\frac{2(2-\\lambda_{1}-\\lambda_{2})}{(5-2\\gamma h)(1-\\gamma h)},\\left(1-\\frac{h\\lambda^{-1}p(a)}{2}\\right)\\frac{1-\\gamma h}{2(1-2\\gamma h+7\\gamma^{2}h^{2})},\\frac{\\mu\\lambda_{2}}{6\\gamma^{2}}\\right\\rbrace}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Now we try to give a lower bound for the contraction rate $c$ upon a set of parameters $\\left(\\lambda,\\lambda_{1},\\lambda_{2}\\right)$ . By assuming $\\gamma h\\leq\\frac{1}{7}$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\nb_{\\mathrm{NAG-SC}}\\geq\\gamma h\\operatorname*{min}\\left\\{2\\big(2-\\lambda_{1}-\\lambda_{2}\\big),\\frac{1}{2}\\left(1-\\frac{h\\lambda^{-1}p(a)}{2}\\right),\\frac{\\mu\\lambda_{2}}{6\\gamma^{2}}\\right\\}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Choosing ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\lambda_{1}=2L\\left(\\frac{h\\lambda p(a)}{2}+h^{2}\\right)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "to make Eq. (39) satisfied. By assuming $\\begin{array}{r}{h\\leq\\frac{1}{2L}}\\end{array}$ , we choose ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\lambda_{2}=\\frac{1-h\\lambda p(a)}{1+\\frac{\\mu}{12{\\gamma}^{2}}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "to make $\\begin{array}{r}{2(2-\\lambda_{1}-\\lambda_{2})\\geq\\frac{\\mu\\lambda_{2}}{6\\gamma^{2}}}\\end{array}$ . Then we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{b_{\\mathrm{NAG-SC}}\\geq\\gamma h\\operatorname*{min}\\left\\{\\displaystyle\\frac{1}{2}\\left(1-\\frac{h\\lambda^{-1}p(a)}{2}\\right),\\frac{\\mu}{6(\\gamma^{2}+\\mu)}\\left(1-h\\lambda p(a)\\right)\\right\\}}\\\\ &{\\qquad\\qquad\\geq2\\sqrt{\\mu}h\\operatorname*{min}\\left\\{\\displaystyle\\frac{1}{2}\\left(1-\\frac{h\\lambda^{-1}p(a)}{2}\\right),\\frac{1}{30}\\left(1-h\\lambda p(a)\\right)\\right\\}}\\\\ &{\\qquad\\qquad\\geq2\\sqrt{\\mu}h\\operatorname*{min}\\left\\{\\displaystyle\\frac{1}{2}\\left(1-\\frac{h p(a)}{2}\\right),\\frac{1}{30}\\left(1-h p(a)\\right)\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "by choosing $\\gamma=2\\sqrt{\\mu}$ , same as continuous case and simply choose $\\lambda=1$ . ", "page_idx": 27}, {"type": "equation", "text": "$$\nb_{\\mathrm{NAG-SC}}\\geq2\\sqrt{\\mu}\\operatorname*{min}\\left\\{\\frac{1}{4}h(2-h p(a)),\\frac{1}{30}h(1-h p(a))\\right\\}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Setting $\\begin{array}{r}{h=\\operatorname*{min}\\left\\{\\frac{1}{\\sqrt{2L}},\\frac{1}{2p(a)}\\right\\}}\\end{array}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle b_{\\mathrm{NAG-SC}}\\geq2\\sqrt\\mu\\frac{h}{60}}}\\\\ {{\\displaystyle=\\frac{1}{30}\\sqrt\\mu\\operatorname*{min}\\left\\{\\frac{1}{\\sqrt{2L}},\\frac{1}{2p(a)}\\right\\}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which gives us the desired result. ", "page_idx": 27}, {"type": "text", "text": "Corollary 34. If the iteration for NAG-SC is initialized by $g_{0}\\in S_{0}$ , with $(g_{0},\\xi_{0})$ satisfying ", "page_idx": 27}, {"type": "equation", "text": "$$\nE^{N A G-S C}(g_{0},\\xi_{0})\\leq(1-\\gamma h)u\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for some u, with the u sub-level set of\u221a $U$ satisfying Eq. (15). Then we have $g_{k}\\in S_{0}$ for any $k$ for the NAG-SC scheme Eq. (13) when h \u2264 21L. ", "page_idx": 27}, {"type": "text", "text": "Proof of Cor. 34. First we prove $\\mathcal{L}^{\\mathrm{NAG-SC}}\\ge1/4\\|\\xi_{k}\\|^{2}$ . By $L$ -smoothness and geodesic convexity, Lemma 23 gives ", "page_idx": 27}, {"type": "equation", "text": "$$\nU(g)-U(g_{*})\\geq\\frac{1}{L}\\|\\nabla U(g)\\|^{2}\\quad\\forall g\\in S\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "As a result, when h \u2264 21L, we haveL(11\u2212\u03b3h) \u2265 $\\begin{array}{r}{\\frac{1}{L(1-\\gamma h)}\\geq\\frac{h^{2}(2-\\gamma h)}{2(1-\\gamma h)}}\\end{array}$ , leading to $\\begin{array}{r}{\\mathcal{L}^{\\mathrm{NAG-SC}}(g,\\xi)\\geq\\frac{1}{4}\\left\\|\\xi\\right\\|^{2}}\\end{array}$ . ", "page_idx": 27}, {"type": "text", "text": "Now we are ready to prove this corollary by induction. Suppose we have ${\\mathfrak{g}}_{k-1}\\ \\in\\ S_{0}$ . By the monotonicity of the Lyapunov function, $\\begin{array}{r}{\\mathcal{L}^{\\mathrm{\\tilde{NAG-SC}}}(g_{k},\\xi_{k})\\leq\\mathcal{L}^{\\mathrm{N\\hat{AG}-S C}}(g_{0},\\xi_{0})}\\end{array}$ . As a result, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\xi_{k}\\|\\le\\sqrt{4\\mathcal{L}_{k}^{\\mathrm{NAG-SC}}}\\le2\\sqrt{\\mathcal{L}_{0}^{\\mathrm{NAG-SC}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since we have $g_{k}=g_{k-1}\\exp(h\\xi_{k})$ , we have $d(g_{k},g_{k-1})\\leq h\\|\\xi_{k}\\|$ . Together with the condition that $U(g_{k-1})\\leq(1-\\gamma h)\\mathcal{L}_{k}^{\\mathrm{NAG-SC}}\\leq(1-\\gamma h)u$ , we have $g_{k}\\in S_{0}$ . Mathematical induction gives the desired result. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "Proof of Thm. 14. This is the direct corollary from Lemma 33 and Cor. 34 when defining $\\begin{array}{r}{\\mathcal{C}_{\\mathrm{NAG-SC}}:=}\\end{array}$ $\\scriptstyle\\left(1\\,+\\,b_{\\mathrm{NAG-SC}}\\right)^{-1}$ , with $b_{\\mathrm{NAG-SC}}$ is given in Eq. (34). ", "page_idx": 27}, {"type": "text", "text": "Remark 35 (Why Lie NAG-SC losses convergence rate comparing to the Euclidean case). In order to utilize the extra term $h\\left(T_{g_{k}}\\mathsf{L}_{g_{k}-1}\\nabla U(g_{k})-T_{g_{k-1}}\\mathsf{L}_{g_{k-1}-1}\\nabla U(g_{k-1})\\right)$ in Eq. (13), Lyapunov function Eq. (14) has the term $\\begin{array}{r}{\\xi_{k}\\;+\\;\\frac{\\gamma}{1-\\gamma h}\\log g_{*}^{-1}g_{k+1}\\;+\\;h\\nabla U(g_{k}),}\\end{array}$ , and the term $\\left\\langle\\log g_{*}^{-1}g_{k+1}-\\log g_{*}^{-1}g_{k},T_{g_{k}}\\mathsf{L}_{g_{k}-1}\\nabla U(g_{k})\\right\\rangle$ needs to be quantified, consequently. However, due to the non-linearity of the Lie group, we have to make the assumption that $g_{k+1}$ and $g_{k}$ are close to $g_{*}$ , leading to the space is \u2018nearly linear\u2019, so that we can bound the error from the non-linearity of the space using Cor. 18. Please see the proof of Lemma 33, Eq. (36) for more details. ", "page_idx": 27}, {"type": "text", "text": "However, such loss of convergence rate due to curved spaces is also observed in some of the best results so far [33, 1]. ", "page_idx": 27}, {"type": "text", "text": "For heavy-ball scheme, the design of the Lyapunov function only has the term $\\begin{array}{r}{\\frac{\\gamma}{1-\\gamma h}\\log g_{*}^{-1}g_{k}+\\xi_{k}}\\end{array}$ and we can use the properties Eq. (16) and (17) to quantify $\\mathcal{L}_{k+1}^{H B}-\\mathcal{L}_{k}^{H B}$ . ", "page_idx": 27}, {"type": "text", "text": "D.1 Modified energy for NAG-SC ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Remark 36 (Why we cannot have an modified energy for NAG-SC). An \u2018modified energy\u2019 for Lie NAG-SC is provided in Eq. (41), whose monotonicity is shown in Thm. 37. The modified energy is global defined and required only $L$ -smoothnes\u221as. However, the failure for this \u2018energy function\u2019 is because its monotonicity requires step size $\\begin{array}{r}{{\\mathcal O}\\left(\\frac{\\sqrt{\\mu}}{L}\\right)}\\end{array}$ , which is smaller than the step size $\\begin{array}{r}{\\mathcal{O}\\left(\\frac{1}{\\sqrt{L}}\\right)}\\end{array}$ that provides acceleration. ", "page_idx": 28}, {"type": "text", "text": "Comparing with the Heavy-Ball scheme, the larger step size and the acceleration of NAG-SC come from extra term $\\begin{array}{r l}{\\lefteqn{(1-\\gamma h)h\\left(T_{g_{k}}\\mathsf{L}_{g_{k}-1}\\nabla U(g_{k})-T_{g_{k-1}}\\mathsf{L}_{g_{k-1}-1}\\nabla U(g_{k-1})\\right)}\\quad}&{{}}\\end{array}$ , which is closely related to co-coercivity in Lemma 23. However, co-coercivity requires (local) geodesic convexity and - smoothness at the same time, which is not available when we are considering the convergence globally. ", "page_idx": 28}, {"type": "text", "text": "The update for $\\xi$ in NAG-SC Eq. (13) can be also written as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{1}{1-\\gamma h}\\left(\\xi_{k}+h T_{g_{k}}\\mathsf{L}_{g_{k}}\\!\\!\\cdot\\!\\nabla U(g_{k})\\right)=\\left(\\xi_{k-1}+h T_{g_{k-1}}\\mathsf{L}_{g_{k-1}}\\!\\!\\cdot\\!\\nabla U(g_{k-1})\\right)-h T_{g_{k}}\\mathsf{L}_{g_{k}}\\!\\!\\cdot\\!\\nabla U(g_{k})\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "This inspires us to define the following modified energy: ", "page_idx": 28}, {"type": "equation", "text": "$$\nE^{\\mathrm{NAG-SC}}(g,\\xi)=U(g)+\\frac{(1-\\gamma h)^{2}}{2(1+\\gamma h-\\gamma^{2}h^{2})}\\big\\|\\xi+h T_{g}\\mathsf{L}_{g^{-1}}\\nabla U(g)\\big\\|^{2}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Theorem 37 (Monotonely decreasing of modified energy of NAG-SC). Assume the potential $U$ is globally $L$ -smooth. When $\\begin{array}{r}{h\\leq\\operatorname*{min}{\\left\\{\\frac{1}{\\gamma},\\frac{\\gamma}{2L}\\right\\}}.}\\end{array}$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\nE^{N\\!A G\\cdot S C}(g_{k},\\xi_{k})-E^{N\\!A G\\cdot S C}(g_{k-1},\\xi_{k-1})\\leq0\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the modified energy $E^{N\\!A G-S C}$ is defined in Eq. (41). ", "page_idx": 28}, {"type": "text", "text": "Proof of Thm. 37. Given L-smoothness of $U$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E^{\\mathrm{SWG},\\oplus}(g_{k_{1}},\\xi_{k_{2}})-E^{\\mathrm{SWG},\\oplus}(g_{k_{1}},\\xi_{k_{1}-1})}\\\\ &{:=U(g_{k_{1}})-U(g_{k_{1}-1})+\\frac{\\left(1-\\gamma h\\right)^{2}}{2\\left(1+\\gamma h-\\gamma^{2}h\\right)^{2}}\\left(\\left[\\xi_{k}+h T_{g_{k_{1}-1}}\\xi_{g_{k_{1}-1}}\\!\\!+\\!\\nabla U(g_{k_{1}-1})\\right]^{2}-\\left[\\xi_{k_{1}-1}+h T_{g_{k_{2}-1}}\\xi_{g_{k_{1}-1}}\\right.\\right.}\\\\ &{\\left.\\left.-U(g_{k_{1}})-U(g_{k_{1}-1})+\\frac{\\left(1-\\gamma h^{2}\\right)^{2}}{2\\left(1+\\gamma h-\\gamma^{2}h\\right)^{2}}\\right]\\left\\{\\xi_{k_{1}}+h T_{g_{k_{1}-1}}\\xi_{g_{k_{1}-1}}\\!\\!-\\!\\nabla U(g_{k_{1}})\\right\\}^{2}\\right.}\\\\ &{\\left.-\\frac{\\left(1-\\gamma h\\right)^{2}}{2\\left(1+\\gamma h-\\gamma^{2}h\\right)^{2}}\\left|\\frac{1}{\\left(1-\\gamma h\\right)^{6}}\\xi_{k_{1}}+\\frac{h\\left(2-\\gamma h\\right)}{1-\\gamma h}T_{g_{k_{1}-1}}\\xi_{g_{k_{1}-1}}\\!\\!-\\!\\nabla U(g_{k_{1}-1})\\right|\\right]^{2}}\\\\ &{:=U(g_{k_{1}})-U(g_{k_{1}-1})+\\frac{\\left(1-\\gamma h\\right)^{2}}{2\\left(1+\\gamma h-\\gamma^{2}h\\right)^{2}}\\left|\\xi_{k_{1}}+h T_{g_{k_{1}-1}}\\xi_{g_{k_{1}-1}}\\!\\!-\\!\\nabla U(g_{k_{1}-1})\\right|^{2}}\\\\ &{\\left.-\\frac{1}{2\\left(1+\\gamma h-\\gamma^{2}h\\right)^{2}}\\left|\\xi_{k_{1}}+\\!h(2-\\gamma h)T_{g_{k_{1}-1}}\\xi_{g_{k_{1}-1}}\\!\\!-\\!\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{lhness,}\\,U\\big(g_{k}\\big)-U\\big(g_{k-1}\\big)-h\\big\\langle\\xi_{k},T_{g_{k-1}}\\mathsf{L}_{g_{k-1}}\\mathsf{\\Sigma}_{\\mathsf{V}}U\\big(g_{k-1}\\big)\\big\\rangle\\le\\frac{L h^{2}}{2}\\|\\xi_{k}\\|^{2}}\\\\ &{\\quad E^{\\mathrm{NAG-SC}}\\big(g_{k},\\xi_{k}\\big)-E^{\\mathrm{NAG-SC}}\\big(g_{k-1},\\xi_{k-1}\\big)}\\\\ &{\\quad\\le\\displaystyle\\frac{L h^{2}}{2}\\|\\xi_{k}\\|^{2}-\\frac{\\gamma h\\big(2-\\gamma h\\big)}{2\\big(1+\\gamma h-\\gamma^{2}h^{2}\\big)}\\|\\xi_{k}\\|^{2}-\\frac{h^{2}\\big(3-2\\gamma h\\big)}{2\\big(1+\\gamma h-\\gamma^{2}h^{2}\\big)}\\|\\nabla U\\big(g_{k-1}\\big)\\|^{2}}\\end{array}\n$$$L$ ", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Consequently, a sufficient condition for $E^{\\mathrm{NAG-SC}}(g_{k},\\xi_{k})-E^{\\mathrm{NAG-SC}}(g_{k-1},\\xi_{k-1})\\leq0$ can be given by ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{L h^{2}}{2}\\leq\\frac{\\gamma h(2-\\gamma h)}{2(1+\\gamma h-\\gamma^{2}h^{2})}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By assuming $\\gamma h\\leq1$ , a sufficient condition for this can be given by $\\begin{array}{r}{h\\leq\\frac{\\gamma}{2L}}\\end{array}$ , i.e., when ", "page_idx": 29}, {"type": "equation", "text": "$$\nh\\leq\\operatorname*{min}\\left\\{{\\frac{1}{\\gamma}},{\\frac{\\gamma}{2L}}\\right\\}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The abstract and introduction stated the contributions, assumptions and limitations. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The section \u2018Related work\u2019 contains our limitations, comparing with existing results. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We make assumtption of compactness (Assumption 2), $L$ -smoothness (Def. 4) and local geodesic-strong convexity (Def. 5) ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Details for numerical experiment are included in Sec. 6. Code is provided in supplementary materials. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Code will be released. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Details for numerical experiment are included in Sec. 6. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: No statistical error bar is included. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Please see Sec. 6. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We confirm we follow the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper focuses on theoretical aspect. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper focuses on theoretical aspect. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: No existing assets are used. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 34}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: No new assets are introduced. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: No human subject is involved. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: No human subject is involved. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]