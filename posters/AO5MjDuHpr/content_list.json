[{"type": "text", "text": "Tree of Attributes Prompt Learning for Vision-Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Prompt learning has proven effective in adapting vision language models for   \n2 downstream tasks. However, existing methods usually append learnable prompt   \n3 tokens solely with the category names to obtain textual features, which fails to fully   \n4 leverage the rich context indicated in the textual category name. To address this   \n5 issue, we propose the Tree of Attributes Prompt learning (TAP), which first instructs   \n6 LLMs to generate a tree of attributes with a \u201cconcept - attribute - description\u201d   \n7 structure for each associated category name, and then learn the hierarchy with   \n8 vision and text prompt tokens. Unlike existing methods that merely augment   \n9 category names with a set of unstructured descriptions, our approach essentially   \n0 distills structured knowledge graphs associated with class names from LLMs.   \n11 Furthermore, our approach introduces text and vision prompts designed to explicitly   \n12 learn the corresponding visual attributes, effectively serving as domain experts.   \n13 Additionally, the general and diverse descriptions generated based on the class   \n14 names may be wrong or absent in the specific given images. To address this   \n15 misalignment, we further introduce a vision-conditional pooling module to extract   \n16 instance-specific text features. Extensive experimental results demonstrate that   \n17 our approach outperforms state-of-the-art methods on the zero-shot base-to-novel   \n18 generalization as well as few-shot classification across 11 diverse datasets. ", "page_idx": 0}, {"type": "text", "text": "19 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "20 Recent advancements in vision-language models (VLMs) like CLIP [33] and ALIGN [13] merge   \n21 the capabilities of visual perception with linguistic understanding, which have revolutionized the   \n22 landscape with their zero-shot learning abilities. They proficiently handle tasks on unseen data,   \n23 bypassing the conventional requirement for task-specific training. This feature has enabled a plethora   \n24 of applications, ranging from content-based image retrieval to complex visual question answering,   \n25 setting new benchmarks in the domain. A crucial development in this domain is the concept of   \n26 prompt learning, which has significantly influenced both natural language processing (NLP) [20\u201322]   \n27 and vision-only models [14, 43, 44, 51]. This approach leverages learnable prompts to guide model   \n28 understanding, tailoring responses to specific tasks or datasets.   \n29 Prompt learning, particularly in vision-language models, has garnered considerable interest due   \n30 to its parameter efficiency and rapid convergence [54, 53, 55, 8, 23]. Techniques like CoOp [54]   \n31 optimize learnable continuous prompts for few-shot image recognition, enhancing model performance   \n32 significantly. Recent efforts have expanded to multimodal prompt learning, optimizing prompts   \n33 in both visual and language domains [15, 16, 38, 19]. Despite their success, these models rely on   \n34 simplistic text prompts, typically formatted as \u201ca photo of a {class}\u201d, illustrated in Fig. 1 (a). While   \n35 functional, this approach lacks depth, failing to encapsulate the intricacies and finer details inherent in   \n36 visual data. Such limitations hinder the model\u2019s ability to fully leverage the rich, descriptive potential   \n37 offered by more detailed and contextually relevant textual information.   \n38 In parallel, another stream of research has been exploring the utilization of large language models   \n39 (LLMs) to generate more elaborate and descriptive text prompts for enhancing zero-shot learning   \n40 capabilities [26, 32, 35, 17, 30, 48, 49, 36, 52, 40]. These LLM-generated descriptions offer a wealth   \n41 of detail and context, potentially enriching the model\u2019s interpretative capabilities. However, current   \n42 methodologies in integrating these descriptions often do not exploit the full potential of this richness.   \n43 As shown in Fig. 1 (b), most of these approaches lack a structured framework to organize and utilize   \n44 these descriptions effectively, leading to a scattergun approach where not all generated descriptions   \n45 are contextually relevant or optimally aligned with the visual content. In addition, as noted in [35],   \n46 descriptions generated by such paradigms are usually diverse, which covers most possibilities of the   \n47 class, but include descriptions that are either likely not co-occurring, e.g. \u201csteamed\u201d and \u201cfried\u201d, or   \n48 absent in the input image, e.g. \u201clong tail\u201d for a cat shot from the front, necessitating the need for a   \n49 selective pooling mechanism for clearer image-text alignments.   \n50 In response to these challenges, our work introduces \u201cTree of Attribute Prompt learning (TAP),\u201d   \n51 a method that redefines the integration and utilization of detailed descriptions within VLMs. As   \n52 indicated in Fig. 1 (c), unlike existing methods that merely augment category names with a set of   \n53 unstructured descriptions, our approach essentially distills structured knowledge graphs associated   \n54 with class names from LLMs. Specifically, we adopt a hierarchical, tree-like structure to systemati  \n55 cally generate and integrate descriptions, ensuring a layered and comprehensive understanding of   \n56 visual content. Each branch of this tree represents a specific attribute, with finer details fleshed out in   \n57 the subsequent leaves, ensuring that every aspect of the visual content is captured and represented.   \n58 Furthermore, we reimagine the learnable prompt tokens as \u201cdomain experts\u201d, each specializing in   \n59 different aspects of the image, supplemented by the CLS token\u2019s global perspective. In addition, we   \n60 introduce vision-conditional layers for each expert-attribute pair, which pool the most applicable   \n61 descriptions from each of the attribute sets with condition on the input image content, ensuring   \n62 optimal image-text alignment. This setup not only provides a detailed, attribute-focused analysis but   \n63 also harmonizes these insights with the overall context.   \n64 Extensive experiments in both base-to-novel generalization and few-shot classification across 11   \n65 diverse datasets demonstrate the effectiveness of our method. On base-to-novel generalization, TAP   \n66 achieves average performance gains of $1.07\\%$ in harmonic mean over the state-of-the-art methods,   \n67 and $9.34\\%$ over the vanilla CLIP. Competitive results are also observed in few-shot classification. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "AO5MjDuHpr/tmp/07f61e7087e265497343a4a72b17048ddab31eff95b06e365b63b9ed94755d9b.jpg", "img_caption": ["Figure 1: Illustration of the methods for CLIP text prompts formation. (a) Manually created prompt with the single \u201ca photo of a {class}\u201d template; (b) A unstructured set of detailed descriptions generated by LLMs; (c) The proposed Tree of Attribute that organizes the descriptions in a \u201cconcept - attribute - descriptions\u201d structure, essentially distilling knowledge graphs from LLMs; (d) An example Tree of Attribute for \u201cdumplings\u201d. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "68 2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "69 Prompt Learning for Vision-Language Models. Prompt learning bridges linguistic understanding   \n70 and visual perception by guiding VLMs with text prompts, a concept originated in NLP [20\u201322]   \n71 and adapted to vision-only [14, 43, 44, 51] and multimodal contexts[54, 53, 15, 16, 38, 19, 40, 34,   \n72 36, 52, 55, 4, 23]. In the textual domain, CoOp [54] optimizes learnable continuous prompts in   \n73 CLIP\u2019s language branch for few-shot image recognition, while CoCoOp [53] addresses CoOp\u2019s   \n74 overfitting issues by conditioning prompts on visual features. In the visual domain, Visual Prompt   \n75 Tuning (VPT) [1] and Dual-modality Prompt Tuning (DPT) [47] enhance CLIP\u2019s vision encoder by   \n76 learning visual prompts in pixel space and dynamically generating prompts through cross-attention,   \n77 respectively. TransHP [42] leverages category hierarchy for prompt learning to improve classification   \n78 performance. LoGoPrompt [38] enhances classification by incorporating synthetic images with class   \n79 name text as auxiliary visual prompts. MaPLe [15] explores multimodal prompt learning, jointly   \n80 optimizing prompts in both vision and language branches. Other recent works have focused on   \n81 regularizing prompt learning to leverage the knowledge from base VLMs effectively, demonstrating   \n82 enhanced generalization in varied downstream visual tasks [16, 4, 36]. PromptSRC, for instance,   \n83 introduced a self-regulating method that restricts both the vision and text prompt, demonstrating   \n84 improved generalization. Distinct from these approaches, PLOT [5] and ALIGN [41] leverage   \n85 Optimal Transport to align multiple prompts with local visual features, either from the multi-head   \n86 self-attention layer or at a token level. Our work diverges from these methods by introducing a   \n87 hierarchical \"Tree of Attribute\" framework derived from LLMs to structure textual descriptions and   \n88 guide the learning of specialized \"domain expert\" tokens for attribute-level understanding.   \n89 Image classification by descriptions. There\u2019s a growing emphasis on using visual descriptions for   \n90 zero-shot recognition, moving beyond generic prompts [54, 53]. These descriptions, like the \u201cfur   \n91 pattern\u201d or \u201ctail shape\u201d of a cat, provide fine-grained and distinctive characteristics. The use of LLMs   \n92 like GPT-3 [3], allows for efficient generation of a broad spectrum of class-specific descriptions,   \n93 offering an advantage over manually crafted templates. While this approach has been extensively   \n94 researched in zero-shot contexts [17, 26, 30, 35, 48, 49, 10, 32, 28], its application in conjunction   \n95 with prompt learning for few-shot tasks remains relatively unexplored[25, 19, 40, 52, 50]. Previ  \n96 ous methodologies, however, have largely utilized unstructured descriptions, lacking an organized   \n97 framework for effective utilization. Our approach diverges by structuring these descriptions into a   \n98 \u201cTree of Attribute\u201d model, coupled with learnable visual prompts as domain experts. Additionally,   \n99 LLM-generated descriptions often cover a wide range of potential class descriptions, of which not   \n100 all may be pertinent to a given image, pointing to the need for a selective pooling mechanism to   \n101 ensure optimal image-text alignment. We further introduce a vision-conditional pooling layer for   \n102 refined image-text alignment. This structured approach not only enhances the interpretability of the   \n103 model\u2019s learning process but also significantly improves alignment accuracy between image content   \n104 and descriptive text. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "105 3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "106 3.1 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "107 CLIP. Our approach is built on the pre-trained vision-language model, CLIP [33]. Formally, let $(x,c)$   \n108 denote the dataset, where $x$ is an image and $c\\in\\{1,\\ldots,\\bar{C}\\}$ are the class labels. For an image $x$ , the   \n109 vision encoder $h_{I}(\\cdot)$ transforms it into a feature vector $\\mathbf{f}_{x}^{v}=h_{I}(x)$ . Simultaneously, each class label   \n110 $c$ is mapped to a text prompt $t_{c}=\\mathsf{a}$ photo of a $\\{\\mathsf{c}\\}$ , and converted into textual feature vectors   \n111 $\\mathbf{f}_{c}^{t}=h\\bar{_{T}}\\bar{(}t_{c})$ . The predicted class $\\hat{y}$ is given by: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{y}=\\underset{c}{\\mathrm{argmax}}\\cos(\\mathbf{f}_{x}^{v},\\mathbf{f}_{c}^{t})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "112 where $\\cos(\\cdot)$ denotes cosine similarity. ", "page_idx": 2}, {"type": "text", "text": "113 Image classification with class descriptions. To improve the model\u2019s understanding of the categories   \n114 in the transfer datasets, previous works [26, 35] use more detailed descriptions from Large Language   \n115 Models (LLMs) instead of the simple \"a photo of a $\\{{\\mathsf{c}}\\}^{\\mathsf{\\Pi}}$ to prompt the CLIP text encoder.   \n116 Under this approach, a convoluted set of descriptions is generated for a class $c$ as $\\mathcal{D}_{c}:\\{\"\\mathsf{c}$ , which   \n117 is/has/etc description.\" $\\}$ , e.g. $c=$ \"television\" and description $\\equiv$ \"black or grey\".   \n118 This classification is reformulated as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{y}=\\mathop{\\mathrm{argmax}}_{c}\\frac{1}{|\\mathscr{D}_{c}|}\\sum_{d\\in\\mathscr{D}_{c}}\\cos(\\mathbf{h}_{\\mathbf{I}}(x),\\mathbf{h}_{\\mathbf{T}}(d))\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "119 3.2 Overall Framework ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "120 We rethink the descriptions by LLM $\\mathcal{D}_{c}$ as nodes in knowledge graphs. While previous methods   \n121 generate an unstructured set of descriptions, we distill structured knowledge graphs for each class $c$ ", "page_idx": 2}, {"type": "image", "img_path": "AO5MjDuHpr/tmp/d6bac84170a77b9d69464d0ac67fbeb145420f3280e287bf816629f89aecb6fd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Overview of the proposed TAP method. TAP utilizes fine-grained descriptions from LLMs and organizes them in a Tree of Attribute. Vision expert tokens are added to the vision encoder to learn from specific attributes such as color and shape. A vision-conditional pooling layer is introduced to ensure optimal image-text alignment. Textual context tokens are also incorporated to the textual branch, shared across descriptions. ", "page_idx": 3}, {"type": "text", "text": "122 from LLM, in which the root node is the class name $c$ , capturing the highest level semantics, and the   \n123 leaf nodes are the detailed descriptions capturing fine-grained details. In this framework, previous   \n124 paradigms only generate the leaf nodes of the graph, with the edges and graph structure missing,   \n125 where the rich and inherent structure from the descriptions is overlooked. To address this limitation,   \n126 we formulate our approach as a Tree of Attribute, which follows the \u201cconcept - attribute - description\u201d   \n127 structures, as illustrated in Fig. 1 (c).   \n128 Besides weighting the descriptions equally, previous works typically align descriptions that describe   \n129 images from different aspects and at different granularities with a singular CLS token from the image   \n130 encoder. However, while the use of a single CLS token is effective in certain contexts, we note that   \n131 the CLS token is designed to capture the global information of an input image $x$ [9]. As a result, even   \n132 though this helps to further inform global understanding, it may fail to effectively capture the nuances   \n133 and variances at the attribute level. This leads to suboptimal use of the rich descriptions. We address   \n134 this by introducing a set of learnable prompt tokens that serve as domain experts in the vision branch,   \n135 each of which aligns with a specific attribute-level textual embedding.   \n136 Additionally, close inspection of the LLM-generated descriptions indicates limited contextual rele  \n137 vance and a high degree of diversity. Previous works [35] reflect the issue of descriptions that are   \n138 likely not co-occurring e.g. \u201csteam\u201d and \u201cfried\u201d. We further identify cases where the descriptions are   \n139 technically correct but irrelevant to certain images, such as describing \u201clong tail\u201d in frontal images   \n140 of cats, underscoring the need for a selective pooling mechanism. Thus, we introduce a vision  \n141 conditional pooling layer to extract instance-specific text features for each attribute for selecting the   \n142 most applicable descriptions.   \n143 Overall, our approach utilizes fine-grained descriptions and organizes them in a Tree of Attribute   \n144 following the \u201cconcept - attributes -descriptions\u201d structure. Learnable vision expert tokens are   \n145 appended to the input image embedding to learn from specific fine-grained attributes such as color   \n146 and shape. A vision-conditional pooling layer is further added for each attribute to ensure optimal   \n147 image-text alignment. Inspired by CoOP [54], we also incorporate textual contextual tokens in the   \n148 text encoder. The overall framework is presented in Fig. 2. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "149 3.3 Tree of Attribute generation by LLMs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "150 We redefine the process of integrating LLM-generated descriptions by introducing a knowledge graph   \n151 $\\mathcal{G}_{c}=\\{\\mathcal{V}_{c},\\mathcal{E}_{c}\\}$ for each class $c$ , where $\\nu_{c}$ denotes the set of nodes, and $\\mathcal{E}_{c}$ denotes the edges that   \n152 capture the semantic relationship between nodes. In previous works, $\\nu_{c}$ is the set of descriptions   \n153 $\\mathcal{D}_{c}$ , while $\\mathcal{E}_{c}$ is missing. We argue that such methods overlook the inherent structure among the   \n154 descriptions and thus do not exploit the richness of these descriptions effectively. To better leverage   \n155 knowledge from LLMs, we introduce an attribute layer to link the root node class name, and the leaf   \n156 node descriptions. The attribute nodes include visual attributes generated by LLMs, such as color and   \n157 shape, for systematically guiding description generation as illustrated in Fig. 1 (c). Each branch of   \n158 this \u201ctree\u201d represents a specific attribute, with the subsequent \u201cleaves\u201d fleshing out the descriptions   \n159 with finer details. In this framework, $\\nu_{c}$ includes the class name which is the root node, the set of   \n160 attributes such as color and shape being the intermediate layer, and lastly the set of descriptions   \n161 under each attribute node. $\\mathcal{E}_{c}$ includes the edges that build up the hierarchy. This structure allows   \n162 for a nuanced representation of class information, spanning from general concepts down to specific   \n163 attributes and detailed descriptions.   \n164 To this end, we introduce the Tree of Attribute (ToA), where we use a tree structure to model the   \n165 relationship and structure of the descriptions. Let $\\mathcal{A}_{c}$ denote the set of attributes, and for each attribute   \n166 $a_{c}\\in\\mathcal{A}_{c}$ , we denote its leaf nodes as $\\mathcal{D}_{c}^{a}$ . Each set $\\mathcal{D}_{c}^{a}$ contains descriptions that specifically pertain   \n167 to attribute $a$ for class $c$ , which is denoted as ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{D}_{c}^{a}=\\{d_{c}^{a,1},d_{c}^{a,2},\\dots,d_{c}^{a,n}\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "168 where $d_{c}^{a,i}$ represents the $i$ -th description for attribute $a$ of class $c$ and $n$ is the number of descriptions   \n169 per attribute.   \n170 The process of generating a Tree of Attribute (ToA) unfolds in three steps: 1) Attribute Generation:   \n171 We first query LLMs with the dataset information and ask it to generate a set of attributes $\\boldsymbol{\\mathcal{A}}$ which are   \n172 considered relevant and characteristic of the dataset. 2) Example Generation: We then ask LLMs to   \n173 generate descriptions for a randomly sampled class in the dataset, using the attributes $\\boldsymbol{\\mathcal{A}}$ identified   \n174 in the previous step. Each description takes the format of \u201cclass, which {is/has/etc} {description}\u201d.   \n175 Human review is performed to ensure the quality of the example. 3) Description Generation for   \n176 All Classes: Building upon the Q&A template from the previous step, the LLM is then tasked with   \n177 generating descriptions for all classes in the dataset.   \n178 Additionally, we incorporate a \u201cglobal context\u201d attribute which is aligned with the CLS token in the   \n179 vision encoder. The descriptions are the 7 standard templates provided in [33]. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "180 3.4 Learning TAP with Learnable Expert Tokens ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "181 To fully exploit the structured Tree of Attribute, we introduce learnable visual expert tokens $\\mathbf{p}_{a}^{v}$ in the   \n182 vision branch to learn from each of the attribute nodes $a\\in A$ . Unlike traditional methods that rely   \n183 on a single CLS token for alignment, these expert tokens enable focused learning on specific image   \n184 attributes, such as color or shape, enhancing the model\u2019s performance and interpretability.   \n185 We denote the set of introduced visual expert tokens as ${\\mathcal P}^{v}\\;=\\;\\{\\mathbf{p}_{a}^{v}|a\\;\\in\\;{\\cal A}\\}$ . Akin to the idea   \n186 of visual prompt tuning (VPT) [14], we insert ${\\mathcal P}^{v}$ into the input sequence of the vision encoder,   \n187 forming the prompted input sequences $\\tilde{\\mathbf{X}}_{\\mathbf{p}}\\,=\\,\\{\\mathbf{e}_{\\mathrm{CLS}},\\mathcal{P}^{v},\\mathbf{E}_{\\mathrm{patch}}\\}$ , where $\\mathbf{e}_{\\mathrm{{CLS}}}$ is the input CLS   \n188 token, and $\\mathbf{E}_{\\mathrm{patch}}$ denotes the embedded patch tokens. To further boost the model\u2019s capacity for   \n189 nuanced attribute representation, we employ deep prompting by introducing a zero-initialized layer   \n190 residual for each prompt token across transformer layers, which provides more explicit attribute   \n191 guidance across transformer layers. In parallel, we adopt a set of $m$ learnable context tokens   \n192 $\\mathcal{\\bar{P}}^{t}=\\{\\mathbf{p}_{j}^{t}|j\\in\\{1,2,...,m\\}\\}$ for the text encoder shared across all descriptions, similar to [54]. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "193 3.5 Vision-Conditional Pooling ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "194 To mitigate issues of misalignment and potential misleading information from the broad spectrum of   \n195 LLM-generated descriptions, we proposed an adaptive vision-conditional pooling layer, applicable to   \n196 each set of attribute descriptions $\\mathcal{D}_{a}$ shared across all classes to dynamically pool the most applicable   \n197 descriptions based on the visual content of the image $x$ using its corresponding visual expert token   \n198 denoted as $\\mathbf{p}_{a,x}^{v}$ . For ease of expression, we will proceed without explicitly mentioning $x$ , though it\u2019s   \n199 important to note that both the expert token and the resulting attribute-level embeddings are dependent   \n200 on the visual information. Intuitively, VCP uses attention to calculate the similarity between $\\mathbf{p}_{a}^{v}$ and   \n201 all embedded descriptions in attribute $\\mathcal{D}_{a}$ , which are then used as weights for a weighted sum of the   \n202 original description embeddings. Formally, for each attribute $a$ and its associated expert token $\\mathbf{p}_{a}^{v}$ ,   \n203 the pooled attribute-level embedding $\\mathbf{v}_{c}^{a}$ for class $c$ and attribute $a$ is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c}{\\mathrm{Query}={W_{q}}\\cdot\\mathbf{p}_{a}^{v},}\\\\ {\\mathrm{Key}={W_{k}}\\cdot\\mathrm{Emb}(\\mathcal{D}_{c}^{a}),}\\\\ {\\mathrm{Attention~Score}=\\mathsf{s o f t m a x}(\\mathrm{Query}\\cdot\\mathrm{Key}^{T}),}\\\\ {\\mathbf{v}_{c}^{a}=\\mathrm{Attention~Score}\\cdot\\mathrm{Emb}(\\mathcal{D}_{c}^{a}),}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "204 where $W_{q}$ and $W_{k}$ are learnable weights $\\in\\mathbb{R}^{d\\times d}$ , $\\operatorname{Emb}(\\cdot)$ denotes the embedding function, and   \n205 softmax ${\\boldsymbol{\\hat{(\\cdot)}}}$ is the Softmax function. This layer mirrors cross-attention but omits $W_{v}$ to maintain the   \n206 output within the CLIP V-L space. ", "page_idx": 5}, {"type": "text", "text": "207 3.6 Training and Inference ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "208 Training objective. During training, each visual expert token $\\mathbf{p}_{a}^{v}$ is aligned with its associated   \n209 attribute-level embedding $\\mathbf{v}_{c}^{a}$ , trained with the following contrastive objective: ", "page_idx": 5}, {"type": "equation", "text": "$$\nL_{c o n}(\\mathbf{p}_{a}^{v},\\mathbf{v}_{c}^{a})=-\\frac{1}{N}\\sum_{i=1}^{N}\\log\\frac{\\exp(\\cos(\\mathbf{p}_{a}^{v},\\mathbf{v}_{y}^{a})/\\tau)}{\\sum_{c=1}^{C}\\exp(\\cos(\\mathbf{p}_{a}^{v},\\mathbf{v}_{c}^{a})/\\tau)},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "210 where $N$ represents the number of training samples, and $\\tau$ is the learned temprature of CLIP. The   \n211 total classification loss $L_{\\mathrm{class}}$ is the average of the contrastive loss from each expert token as well as   \n212 the CLS token, defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nL_{c l a s s}=\\frac{1}{|\\mathcal{A}|}\\bigg(\\sum_{a\\in\\mathcal{A}}L_{c o n}(\\mathbf{p}_{a}^{v},\\mathbf{v}_{c}^{a}))\\bigg),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "213 Similar to [16] and [4], we regularize the vision CLS token, text feature, and the prediction logits   \n214 from each attribute using the vanilla CLIP model. We denote the regularization loss as $L_{r e g}$ , where   \n215 the details can be found in Appendix. The overall training objective is $L_{\\mathrm{total}}=L_{\\mathrm{class}}+L_{\\mathrm{reg}}$ .   \n216 Prediction fusion. During inference, we integrate the prediction by each attribute expert pair by a   \n217 weighted sum, formulated as follows: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{y}=\\operatorname*{argmax}_{c}\\biggl(\\alpha\\cos(\\mathbf{f}_{C L S}^{v},\\mathbf{v}_{c}^{C L S})+\\frac{1-\\alpha}{|A|-1}\\sum_{a\\in A\\backslash\\{C L S\\}}\\cos(\\mathbf{p}_{a}^{v},\\mathbf{v}_{c}^{a})\\biggr)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "218 where $\\alpha$ is a hyperparameter that signifies the weight assigned to the global context provided by the   \n219 CLS token, balancing its contribution with that of the attribute-specific expert prompts. ", "page_idx": 5}, {"type": "text", "text": "220 4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "221 We extensively evaluate our method in two settings: 1) Base-to-novel class generalization, where the   \n222 datasets are equally split into base and novel classes. We train the model on the base classes only and   \n223 evaluate on both base and novel classes; and 2) Few-shot classification with 16 shots per class.   \n224 Datasets and baslines. For both base to novel class generalization and few-shot setting, we follow   \n225 previous works [54, 53], using 11 image recognition datasets. The datasets span a range of recog  \n226 nition tasks: ImageNet [7] and Caltech101 [11] for generic object recognition; OxfordPets [30],   \n227 StanfordCars [18], Flowers102 [27], Food101 [2], and FGVCAircraft [24] for fine-grained classifica  \n228 tion; SUN397 [46] for scene recognition; UCF101 [39] for action recognition; DTD [6] for texture   \n229 classification; and EuroSAT [12] for satellite image analysis. We benchmark against several leading   \n230 methods, including CLIP [33], CoOp [54], Co-CoOP [53], ProGrad [55], RPO [19], LoGoPrompt   \n231 [38], and the state-of-the-art PromptSRC [16].   \n232 Implementation details. A pre-trained CLIP model with a ViT-B/16 vision backbone is used in all   \n233 of our experiments and results are averaged over 3 runs. We use GPT-3.5-turbo [29] for attribute and   \n234 description generation. We initialize the text context tokens with the word embedding of a photo   \n235 of a. For both settings, we iteratively train the vision and text encoders with 5 epochs for vision   \n236 and 1 epoch for text schedule. We set $\\alpha=0.4$ , $\\mu_{1}=10$ , and $\\mu_{2}=2.5$ for all datasets. We train   \n237 the vision encoder for 50 and 100 epochs, and text encoder for 10 and 20 epochs for base-to-novel   \n238 generalization and few-shot experiments, respectively. For DTD, Oxford Flowers, Stanford Cars,   \n239 UCF101, and Caltech101 datasets, we use a learning rate of 0.002 for the text encoder and 0.006 for   \n240 the vision encoder, with $\\mu_{3}=3$ . For the remaining 6 datasets, the learning rates for both text and   \n241 vision encoders are set as 0.004, with $\\mu_{3}=1.5$ . We also use a Gaussian Prompt Weighting (GPA)   \n242 following [16], with a mean of 45, std of 10 for base-to-novel generalization, and 80, 20 for few-shot   \n243 experiments. Refer to the Appendix for additional implementation details. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "AO5MjDuHpr/tmp/c28c55823693b7c744aef7c5a6d1da9bfe14ead668e702c7ddcccccf92a9ed6f.jpg", "table_caption": ["Table 1: Comparison of TAP in base-to-novel generalization. HM: harmonic mean [45]. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "244 4.1 Base-to-Novel Generalization ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "245 In base-to-novel generalization, we equally split the classes into base and novel classes. Initial   \n246 training and evaluations are conducted on the seen base classes, followed by evaluation on the unseen   \n247 novel classes in a zero-shot manner. TAP surpasses prior state-of-the-art models in terms of the   \n248 base and novel class accuracy, as well as their harmonic mean across most of the 11 datasets, with   \n249 an average increase of $1.53\\%$ in the zero-shot novel class prediction, and a $1.07\\%$ increase in the   \n250 overall harmonic mean in average, as detailed inTable 1. Notably, our method improves unseen class   \n251 prediction without compromising base class performance, exhibiting an average performance boost   \n252 of $0.49\\%$ . In the challenging fine-grained tasks such as DTD, EuroSAT, and UCF101, TAP achieves   \n253 significant improvements in novel class prediction by $5.03\\%$ , $8.27\\%$ , and $3.63\\%$ respectively. These   \n254 results underscore the robust generalizability and efficacy of our method across diverse scenarios. ", "page_idx": 6}, {"type": "text", "text": "255 4.2 Few-Shot Classification ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "256 In few-shot classification, TAP also outperforms existing methods in 9 out of the 11 datasets. Detailed   \n257 in Table 2, we achieve an average accuracy of 83.37 across the 11 datasets, surpassing the previous   \n258 state-of-the-art methods by $0.5\\bar{\\%}$ , further demonstrating the effectiveness of our method. ", "page_idx": 6}, {"type": "image", "img_path": "AO5MjDuHpr/tmp/335cc3c61b1722c66540ea48a5490bfa48be39fad29bb901b8fb7fabf70544f3.jpg", "img_caption": ["Figure 3: Visualization of the class activation maps. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "AO5MjDuHpr/tmp/8bae86511ab5e0960340016c6a863f035fd1639de8ecdaf5f4cdb1799d3911a5.jpg", "table_caption": ["Table 5: Effects of the number of experts. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "259 4.3 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "260 Effects of Tree of Attribute. A core inquiry is whether structuring descriptions into a Tree of   \n261 Attribute (ToA) offers advantages over an unstructured aggregation of LLM-generated descriptions.   \n262 To evaluate, we revert to aligning a mixed, unstructured set of descriptions with the CLS token   \n263 - a common practice in prior studies [25, 19, 40, 52], while keeping the same number of visual   \n264 prompt tokens. According to Table 3, substituting the ToA with an unstructured set results in   \n265 significant performance decreases of $1.86\\%$ , $2.31\\%$ , and $2.11\\%$ across the average base, novel, and   \n266 their harmonic mean performances, respectively. This stark contrast underscores the ToA\u2019s critical   \n267 role in enhancing model efficacy.   \n268 Effects of Learning through Domain Experts. Further, we examine the impact of substituting the   \n269 CLS token with visual expert tokens for learning fine-grained attributes, commonly adopted in in   \n270 previous works [25, 19, 40, 52]. Our findings (Table 4) reveal improvements of $0.89\\%$ , $0.78\\%$ , and   \n271 $0.82\\%$ in the average base, novel, and harmonic mean accuracies, respectively, upon integrating visual   \n272 expert tokens. These results support the notion that domain-specific, learnable tokens enhance the   \n273 model\u2019s ability to grasp fine-grained details by focusing on distinct aspects of the image, as opposed   \n274 to the CLS token\u2019s global focus.   \n275 Effects of Number of Attributes. In our framework, the selection of attributes is dynamically   \n276 determined by LLMs, leading to variability across different datasets. This adaptability stands in   \n277 contrast to a static approach where the number of attributes is uniformly set across all datasets. To   \n278 understand the impact of this variability, we explore how altering the number of attributes from 1 to 8   \n279 influences model performance. Our findings, detailed in Table 5, reveal a performance improvement   \n280 trend as the number of attributes increases, with an optimal peak at 7 attributes before a slight decline   \n281 at 8. However, crucially, across all fixed-attribute scenarios, none matched the performance achieved   \n282 through our method\u2019s dynamic attribute determination. These results underscore the importance of   \n283 an adaptive approach to attribute selection, as opposed to a one-size-fits-all strategy.   \n284 Design choice of the vision-conditional pooling layer. Lastly, we ablate the design of the pooling   \n285 layer, starting from the naive training-free average pooling, to the attention-based pooling mechanism   \n286 with condition on the input image. Compared to average pooling, VCP demonstrates a performance   \n287 gain of $1.08\\%$ in the average harmonic mean. Furthermore, when compared with attention-based max   \n288 pooling, which selects a single description per attribute according to the attention score in Eq. (4),   \n289 VCP maintains a superior advantage of $1.55\\%$ in average harmonic mean. These outcomes attest to   \n290 the VCP layer\u2019s integral role in finetuning attribute relevance to the visual context, substantiating its   \n291 design and implementation within our model. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "AO5MjDuHpr/tmp/a633d09bea08c6ac2031cd27c362b8a2a4a274615b24cc6a5cbe406ac6914dc2.jpg", "img_caption": ["Figure 4: Visualization of the attention weights in the VCP layer for an example \u201cdumplings\u201d image. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "AO5MjDuHpr/tmp/769343094dcb21fca59e590123559987cc37b406360016e44c6feec5508e1138.jpg", "table_caption": ["Table 6: Design choice of the pooling layer. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "292 4.4 Visualization ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "293 Expert tokens focus on attribute-related regions. We further investigate the effects of vision   \n294 domain experts by visualizing their class activation maps from three illustrative examples using   \n295 GradCAM [37], as shown inFig. 3. These visualizations underscore the precision with which each   \n296 expert token concentrates on the image regions pertinent to its designated attribute. Take the first   \n297 cat image as an example. The \u201cfur pattern\u201d expert distinctly highlights the animal\u2019s fur texture,   \n298 whereas the \u201cear\u201d and \u201ceye\u201d experts focus precisely on the respective anatomical features. This   \n299 pattern of attribute-specific attention is consistent across the evaluated examples, reinforcing the   \n300 conceptualization of expert tokens as dedicated \u201cdomain experts\u201d within the visual field.   \n301 VCP layer pools the most applicable descriptions. The inherently interpretable nature of the VCP   \n302 layer, thanks to its attention mechanism, allows for insightful visualizations of its operational process.   \n303 Through the examination of attention weights assigned by the VCP layer to different attributes   \n304 in a given image, we elucidate the layer\u2019s capability to discern and prioritize the most applicable   \n305 descriptions. As illustrated in Fig. 4 with a \u201cdumplings\u201d image, the VCP layer adeptly allocates   \n306 higher attention weights to descriptions accurately reflecting the observed instance (e.g., assigning   \n307 weights of 0.92 to \u201cround with a pleated edge\u201d under the \u201cShape\u201d attribute and 0.95 to \u201csoft and   \n308 chewy texture\u201d under the Texture\u201d). In contrast, less relevant descriptions for the specific image   \n309 context (e.g., \u201ccrescent-shaped\u201d for Shape and \u201ccrispy texture from pan-frying\u201d for Texture) receive   \n310 significantly lower weights. This discernment is crucial, given the class dumplings\u201d encompasses a   \n311 broad variety of appearances based on cooking methods, yet not all descriptions are ftiting for every   \n312 instance. These visualizations compellingly demonstrate the VCP layer\u2019s effectiveness in refining   \n313 description relevance, thereby enhancing the model\u2019s interpretative alignment with the visual content. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "314 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "315 This paper introduces Tree of Attribute Prompt learning (TAP), a novel method that integrates   \n316 detailed, LLM-generated descriptions within VLMs, achieving state-of-the-art performance in both   \n317 base-to-novel generalization and few-shot image classification tasks across 11 diverse datasets. TAP   \n318 leverages a hierarchical \"Tree of Attribute\" framework, distilling structured knowledge graphs from   \n319 LLMs for nuanced representation of visual concepts, and employs learnable \"domain expert\" tokens   \n320 and a vision-conditional pooling module for optimal image-text alignment. While promising, we   \n321 note that the reliance on LLMs presents challenges in fine-grained datasets where similar classes   \n322 require nuanced differentiation, in which cases LLMs generate identical descriptions for distinct   \n323 classes, impacting novel class prediction performance. It highlights the current limitations of LLMs   \n324 in discerning highly fine-grained distinctions. Addressing this challenge through enhanced LLM   \n325 capabilities or alternative strategies will be a key focus of future research. ", "page_idx": 8}, {"type": "text", "text": "326 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "327 [1] Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and Phillip Isola. Visual prompting: Modifying   \n328 pixel space to adapt pre-trained models. arXiv preprint arXiv:2203.17274, 3:11\u201312, 2022.   \n329 [2] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative components   \n330 with random forests. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland,   \n331 September 6-12, 2014, Proceedings, Part VI 13, pages 446\u2013461. Springer, 2014.   \n332 [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind   \n333 Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.   \n334 Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n335 [4] Adrian Bulat and Georgios Tzimiropoulos. Lasp: Text-to-text optimization for language-aware soft   \n336 prompting of vision & language models. In Proceedings of the IEEE/CVF Conference on Computer Vision   \n337 and Pattern Recognition, pages 23232\u201323241, 2023.   \n338 [5] Guangyi Chen, Weiran Yao, Xiangchen Song, Xinyue Li, Yongming Rao, and Kun Zhang. Prompt learning   \n339 with optimal transport for vision-language models. In ICLR, 2023.   \n340 [6] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing   \n341 textures in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition,   \n342 pages 3606\u20133613, 2014.   \n343 [7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical   \n344 image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255.   \n345 Ieee, 2009.   \n346 [8] Mohammad Mahdi Derakhshani, Enrique Sanchez, Adrian Bulat, Victor G Turrisi da Costa, Cees GM   \n347 Snoek, Georgios Tzimiropoulos, and Brais Martinez. Bayesian prompt learning for image-language model   \n348 generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages   \n349 15237\u201315246, 2023.   \n350 [9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas   \n351 Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,   \n352 and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In   \n353 International Conference on Learning Representations, 2021. URL https://openreview.net/forum?   \n354 id=YicbFdNTTy.   \n355 [10] Zalan Fabian, Zhongqi Miao, Chunyuan Li, Yuanhan Zhang, Ziwei Liu, Andr\u00e9s Hern\u00e1ndez, Andr\u00e9s   \n356 Montes-Rojas, Rafael Escucha, Laura Siabatto, Andr\u00e9s Link, et al. Multimodal foundation models for   \n357 zero-shot animal species recognition in camera trap images. arXiv preprint arXiv:2311.01064, 2023.   \n358 [11] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples:   \n359 An incremental bayesian approach tested on 101 object categories. In 2004 conference on computer vision   \n360 and pattern recognition workshop, pages 178\u2013178. IEEE, 2004.   \n361 [12] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep   \n362 learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied   \n363 Earth Observations and Remote Sensing, 12(7):2217\u20132226, 2019.   \n364 [13] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,   \n365 Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text   \n366 supervision. In International conference on machine learning, pages 4904\u20134916. PMLR, 2021.   \n367 [14] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and   \n368 Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pages 709\u2013727.   \n369 Springer, 2022.   \n370 [15] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan.   \n371 Maple: Multi-modal prompt learning. In Proceedings of the IEEE/CVF Conference on Computer Vision   \n372 and Pattern Recognition, pages 19113\u201319122, 2023.   \n373 [16] Muhammad Uzair Khattak, Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan Yang, and   \n374 Fahad Shahbaz Khan. Self-regulating prompts: Foundational model adaptation without forgetting. In   \n375 Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 15190\u201315200,   \n376 October 2023.   \n377 [17] Jae Myung Kim, A Koepke, Cordelia Schmid, and Zeynep Akata. Exposing and mitigating spurious   \n378 correlations for cross-modal retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision   \n379 and Pattern Recognition, pages 2584\u20132594, 2023.   \n380 [18] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained   \n381 categorization. In Proceedings of the IEEE international conference on computer vision workshops, pages   \n382 554\u2013561, 2013.   \n383 [19] Dongjun Lee, Seokwon Song, Jihee Suh, Joonmyeong Choi, Sanghyeok Lee, and Hyunwoo J Kim.   \n384 Read-only prompt optimization for vision-language few-shot learning. In Proceedings of the IEEE/CVF   \n385 International Conference on Computer Vision, pages 1401\u20131411, 2023.   \n386 [20] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning,   \n387 2021.   \n388 [21] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation, 2021.   \n389 [22] Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning v2: Prompt tuning   \n390 can be comparable to fine-tuning universally across scales and tasks. CoRR, abs/2110.07602, 2021. URL   \n391 https://arxiv.org/abs/2110.07602.   \n392 [23] Yuning Lu, Jianzhuang Liu, Yonggang Zhang, Yajing Liu, and Xinmei Tian. Prompt distribution learning.   \n393 In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5206\u2013   \n394 5215, 2022.   \n395 [24] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual   \n396 classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.   \n397 [25] Chengzhi Mao, Revant Teotia, Amrutha Sundar, Sachit Menon, Junfeng Yang, Xin Wang, and Carl   \n398 Vondrick. Doubly right object recognition: A why prompt for visual rationales. In Proceedings of the   \n399 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2722\u20132732, 2023.   \n400 [26] Sachit Menon and Carl Vondrick. Visual classification via description from large language models. ICLR,   \n401 2023.   \n402 [27] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of   \n403 classes. In 2008 Sixth Indian conference on computer vision, graphics & image processing, pages 722\u2013729.   \n404 IEEE, 2008.   \n405 [28] Zachary Novack, Julian McAuley, Zachary Lipton, and Saurabh Garg. Chils: Zero-shot image classification   \n406 with hierarchical label sets. In International Conference on Machine Learning (ICML), 2023.   \n407 [29] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,   \n408 Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with   \n409 human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.   \n410 [30] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012 IEEE   \n411 conference on computer vision and pattern recognition, pages 3498\u20133505. IEEE, 2012.   \n412 [31] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and   \n413 A. Lerer. Automatic differentiation in PyTorch. In NeurIPS Autodiff Workshop, 2017.   \n414 [32] Sarah Pratt, Ian Covert, Rosanne Liu, and Ali Farhadi. What does a platypus look like? generating   \n415 customized prompts for zero-shot image classification. In Proceedings of the IEEE/CVF International   \n416 Conference on Computer Vision, pages 15691\u201315701, 2023.   \n417 [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish   \n418 Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from   \n419 natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR,   \n420 2021.   \n421 [34] Hanoona Rasheed, Muhammad Uzair Khattak, Muhammad Maaz, Salman Khan, and Fahad Shahbaz   \n422 Khan. Fine-tuned clip models are efficient video learners. In Proceedings of the IEEE/CVF Conference on   \n423 Computer Vision and Pattern Recognition, pages 6545\u20136554, 2023.   \n424 [35] Karsten Roth, Jae Myung Kim, A. Sophia Koepke, Oriol Vinyals, Cordelia Schmid, and Zeynep Akata.   \n425 Waffling around for performance: Visual classification with random words and broad concepts. In   \n426 Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 15746\u201315757,   \n427 October 2023.   \n428 [36] Shuvendu Roy and Ali Etemad. Consistency-guided prompt learning for vision-language models. In The   \n429 Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/   \n430 forum?id=wsRXwlwx4w.   \n431 [37] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and   \n432 Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In   \n433 Proceedings of the IEEE international conference on computer vision, pages 618\u2013626, 2017.   \n434 [38] Cheng Shi and Sibei Yang. Logoprompt: Synthetic text images can be good visual prompts for vision  \n435 language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages   \n436 2932\u20132941, 2023.   \n437 [39] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions   \n438 classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.   \n439 [40] Xinyu Tian, Shu Zou, Zhaoyuan Yang, and Jing Zhang. Argue: Attribute-guided prompt tuning for   \n440 vision-language models. arXiv preprint arXiv:2311.16494, 2023.   \n441 [41] Dongsheng Wang, Miaoge Li, Xinyang Liu, MingSheng Xu, Bo Chen, and Hanwang Zhang. Tuning   \n442 multi-mode token-level prompt alignment across modalities. In Thirty-seventh Conference on Neural   \n443 Information Processing Systems, 2023. URL https://openreview.net/forum?id=A253n2EXCd.   \n444 [42] Wenhao Wang, Yifan Sun, Wei Li, and Yi Yang. TransHP: Image classification with hierarchical   \n445 prompting. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL   \n446 https://openreview.net/forum?id=vpQuCsZXz2.   \n447 [43] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong   \n448 Su, Vincent Perot, Jennifer Dy, et al. Dualprompt: Complementary prompting for rehearsal-free continual   \n449 learning. In European Conference on Computer Vision, pages 631\u2013648. Springer, 2022.   \n450 [44] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent   \n451 Perot, Jennifer Dy, and Tomas Pfister. Learning to prompt for continual learning. In Proceedings of the   \n452 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 139\u2013149, 2022.   \n453 [45] Yongqin Xian, Bernt Schiele, and Zeynep Akata. Zero-shot learning-the good, the bad and the ugly. In   \n454 Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4582\u20134591, 2017.   \n455 [46] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large  \n456 scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on computer vision   \n457 and pattern recognition, pages 3485\u20133492. IEEE, 2010.   \n458 [47] Yinghui Xing, Qirui Wu, De Cheng, Shizhou Zhang, Guoqiang Liang, Peng Wang, and Yanning Zhang.   \n459 Dual modality prompt tuning for vision-language pre-trained model. IEEE Transactions on Multimedia,   \n460 pages 1\u201313, 2023. doi: 10.1109/TMM.2023.3291588.   \n461 [48] An Yan, Yu Wang, Yiwu Zhong, Chengyu Dong, Zexue He, Yujie Lu, William Yang Wang, Jingbo Shang,   \n462 and Julian McAuley. Learning concise and descriptive attributes for visual recognition. In Proceedings of   \n463 the IEEE/CVF International Conference on Computer Vision, pages 3090\u20133100, 2023.   \n464 [49] Yue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch, and Mark Yatskar.   \n465 Language in a bottle: Language model guided concept bottlenecks for interpretable image classification.   \n466 In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19187\u2013   \n467 19197, 2023.   \n468 [50] Yi Zhang, Ce Zhang, Ke Yu, Yushun Tang, and Zhihai He. Concept-guided prompt learning for gener  \n469 alization in vision-language models. Proceedings of the AAAI Conference on Artificial Intelligence, 38   \n470 (7):7377\u20137386, Mar. 2024. doi: 10.1609/aaai.v38i7.28568. URL https://ojs.aaai.org/index.php/   \n471 AAAI/article/view/28568.   \n472 [51] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Neural prompt search. arXiv preprint arXiv:2206.04673,   \n473 2022.   \n474 [52] Zhaoheng Zheng, Jingmin Wei, Xuefeng Hu, Haidong Zhu, and Ram Nevatia. Large language models are   \n475 good prompt learners for low-shot image classification. arXiv preprint arXiv:2312.04076, 2023.   \n476 [53] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for   \n477 vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern   \n478 Recognition, pages 16816\u201316825, 2022.   \n479 [54] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language   \n480 models. International Journal of Computer Vision, 130(9):2337\u20132348, 2022.   \n481 [55] Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, and Hanwang Zhang. Prompt-aligned gradient for prompt   \n482 tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15659\u2013   \n483 15669, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "484 A Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "485 A.1 Model regularization ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "486 Denote the frozen image feature from CLIP vision encoder as $\\mathbf{f}^{v}$ , the frozen text feature for description $d$ from   \n487 CLIP text encoder as $\\bar{\\mathbf{f}}_{d}^{t}$ , and the zero-shot logit prediction from CLIP as $\\hat{y}$ . Additionally, denote the trained   \n488 image feature as $\\tilde{\\mathbf{f}}^{v}$ , the trained text feature for description $d$ as $\\tilde{\\mathbf{f}}_{d}^{t}$ , and the logit prediction from attribute $\\footnote{T w o t y p i c a l a p p l i c a t i o n s c e n a r i o s f o r t h e p r o p o s e d s y s t e m a r e h e a l t h c a r e,a n d l o g i s t i c s a n d w a r e h o u s i n g,i n w h i c h m u l t i p l e I o T d e v i c e s a r e d e p l o y e d c l o s e t o t h e r e c e i v e r a n d t h e t i m e d e l a y b e t w e e n t h e d i r e c t l i n k a n d b a c k s c a t t e r l i n k i s t h u s n e g l i g i b l e.}$ after   \n489 training as $\\tilde{y}_{a}$ . The losses are as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\nL_{L_{1}-V}=||\\mathbf{f}^{v}-\\widetilde{\\mathbf{f}}^{v}||_{1}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle{L_{c o n-T}=-\\sum_{d\\in\\mathcal{D}}\\left(\\frac{1}{2}\\log\\frac{\\exp(c o s(\\mathbf{f}_{d}^{t},\\widetilde{\\mathbf{f}}_{d}^{t}))}{\\sum_{k\\in\\mathcal{D}_{s}}\\exp(c o s(\\mathbf{f}_{d}^{t},\\widetilde{\\mathbf{f}}_{k}^{t}))}+\\frac{1}{2}\\log\\frac{\\exp(c o s(\\mathbf{f}_{d}^{t},\\widetilde{\\mathbf{f}}_{d}^{t}))}{\\sum_{k\\in\\mathcal{D}_{s}}\\exp(c o s(\\mathbf{f}_{k}^{t},\\widetilde{\\mathbf{f}}_{d}^{t}))}\\right)}}\\\\ {\\displaystyle{L_{K L-a t t r}=\\frac{1}{|\\mathcal{A}|}\\left(\\sum_{a\\in\\mathcal{A}}\\mathcal{D}_{K\\mathcal{L}}(\\hat{y},\\widetilde{y}_{a})\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "490 ", "page_idx": 12}, {"type": "text", "text": "491 The regularization loss is then: ", "page_idx": 12}, {"type": "equation", "text": "$$\n{\\cal L}_{r e g}=\\mu_{1}{\\cal L}_{L_{1}-V}+\\mu_{2}{\\cal L}_{K L-a t t r}+\\mu_{3}{\\cal L}_{c o n-T},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "492 Our overall training objective is thus given by: ", "page_idx": 12}, {"type": "equation", "text": "$$\nL_{\\mathrm{total}}=L_{\\mathrm{class}}+L_{\\mathrm{reg}}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "493 A.2 Additional implementation details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "494 We use PyTorch [31] to implement all experiments on a single NVIDIA A100-80GB GPU. Our code is developed   \n495 based on the implementation of CoOp [54], which is available at https://github.com/KaiyangZhou/CoOp and   \n496 released under the MIT license. Our code is also released under the MIT license. Baseline results for both   \n497 base-to-novel generalization and few-shot classification are taken from their respective publications. For the   \n498 \u201cglobal context\u201d attribute which is aligned with the CLS token in the vision encoder, we use the following 7   \n499 selected templates provided in [33].   \n500 \"itap of a {class}.\"   \n501 \"a bad photo of the {class}.\"   \n502 \"a origami {class}.\"   \n503 \"a photo of the large {class}.\"   \n504 \"a {class} in a video game.\"   \n505 \"art of the {class}.\"   \n506 \"a photo of the small {class}.\" ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "507 A.3 Prompts for Tree-of-Attribute generation ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "508 As introduced in Section 3.3, we generate the Tree-of-Attribute with the following three steps: 1) Attribute   \n509 Generation, 2) In-Context Example Generation, and 3) Description Generation for All Classes. The prompts for   \n510 each step are as follows: ", "page_idx": 12}, {"type": "text", "text": "511 1) Attribute Generation: ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "512 {Dataset Description.} ", "page_idx": 12}, {"type": "text", "text": "513 Visual attributes refer to observable, describable features of the images that can include color, shape, size,   \n514 texture, and any specific patterns or markings, which can help differentiate between classes for the dataset. They   \n515 should be consistently observable across multiple images of the same class. Your task is to generate a list of   \n516 visual attributes (less than 10) for the {Dataset Name} dataset. Ensure this list is clear, concise, and specific to   \n517 the dataset\u2019s needs. Avoid generic attributes that do not contribute to distinguishing between classes. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "518 2) In-Context Example Generation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "519 Describe describe what a \"{Random Class Name}\" class in the {Dataset Name} dataset look like using the   \n520 generated visual attributes.   \n521 You must follow the following rules:   \n522 1. For each visual attribute, describe all possible variations as separate sentences. This approach allows for a   \n523 detailed and clear presentation of each attribute\u2019s range.   \n524 2. Provide a maximum of five descriptions for each visual attribute to maintain focus and relevance. Also, aim to   \n525 provide at least two descriptions to ensure a comprehensive overview of the attribute.   \n526 3. The descriptions should provide clear, distinguishable features of each class to support image classification   \n527 tasks.   \n528 4. Descriptions for each attribute are independent from each other, and they should not serve as context for each   \n529 other.   \n530 5. Each description describes an image independetly. If certain description is possible for a class, please just   \n531 list that description, and do not use words like \"may have\" or \"sometimes have\".   \n532 6. Reply descriptions only. Do not include any explanation before and after the description.   \n533 7. The descriptions should follow the format of \"classname, which ...\", where \"...\" is the description of the visual   \n534 attribute. ", "page_idx": 13}, {"type": "text", "text": "535 3) Description Generation for All Classes ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "537 Your task is to write detailed descriptions for various classes within the {Dataset Name} dataset, using the   \n538 provided visual attributes such as color and shape. These descriptions will help in accurately classifying and   \n539 understanding the unique features of each class.   \n540 You must follow the following rules:   \n541 1. For each visual attribute, describe all possible variations as separate sentences. This approach allows for a   \n542 detailed and clear presentation of each attribute\u2019s range.   \n543 2. Provide a maximum of five descriptions for each visual attribute to maintain focus and relevance. Also, aim to   \n544 provide at least two descriptions to ensure a comprehensive overview of the attribute.   \n545 3. The descriptions should provide clear, distinguishable features of each class to support image classification   \n546 tasks.   \n547 4. Descriptions for each attribute are independent from each other, and they should not serve as context for each   \n548 other.   \n549 5. Each description describes an image independetly. If certain description is possible for a class, please just   \n550 list that description, and do not use words like \"may have\" or \"sometimes have\".   \n551 6. Reply descriptions only. Do not include any explanation before and after the description.   \n552 7. The descriptions should follow the format of \"classname, which ...\", where \"...\" is the description of the visual   \n553 attribute.   \n554 Q: Describe what a \"{Random Class Name}\" in the {Dataset Name} look like using the following visual attributes:   \n555 {Visual Attributes from Step 1.}   \n556 A: {Answer from Step 2.}   \n557 Q: Describe what a \"{Target Class Name}\" in the {Dataset Name} look like using the following visual attributes:   \n558 {Visual Attributes from Step 1.}   \n559 A:   \n560 In the prompt templates, \"Dataset Description\" is the description of the dataset from their official website,   \n561 \"Random Class Name\" is a randomly sampled class name in the dataset for in-context example generation, and   \n562 \"Target Class Name\" is the class name of interest for the current query. While step 1 and 2 are made in two   \n563 consecutive calls to provide contexts which are queried once per dataset, step 3 is queried independently for   \n564 each of the remaining classes in the dataset. Human review is performed after step 2 to ensure a high-quality set   \n565 of attributes and in-context example. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "566 A.4 Potential societal impacts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "567 While our work primarily focuses on advancing prompt learning in vision-language models, it\u2019s crucial to   \n568 acknowledge the potential broader societal implications of such advancements. On the positive side, TAP could   \n569 lead to more efficient and accurate image understanding systems, benefiting various domains. For instance, it   \n570 could enhance accessibility for visually impaired individuals by providing more detailed descriptions of visual   \n571 content. Furthermore, improved visual understanding could contribute to more effective content moderation,   \n572 mitigating the spread of harmful online materials. However, these advancements also present potential risks.   \n573 LLMs used for description generation can perpetuate existing societal biases present in their training data, leading   \n574 to biased outcomes in image recognition. Moreover, sophisticated VLMs could be misused to create misleading   \n575 visual content, contributing to misinformation and manipulation. The enhanced ability to analyze and understand   \n576 images also raises privacy concerns, particularly in surveillance contexts where personal information could be   \n577 extracted from visual data. Addressing these potential negative impacts necessitates careful consideration of bias   \n578 mitigation techniques during LLM training, promoting transparency and explainability in VLM decision-making,   \n579 and establishing ethical guidelines for responsible development and deployment of such technologies. ", "page_idx": 14}, {"type": "text", "text": "580 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 15}, {"type": "text", "text": "Justification: The abstract and introduction clearly state the problem of limited context in existing prompt learning methods, propose TAP as a solution using structured knowledge graphs and domain experts, and highlight the strong experimental results in both base-to-novel generalization and few-shot classification. This accurately reflects the paper\u2019s contributions and scope. ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 15}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Justification: The paper includes a discussion of the limitations associated with relying on LLMs for generating descriptions, particularly in fine-grained datasets where similar classes require nuanced differentiation. This discussion can be found in \"Conclusion\". ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 15}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: The paper focuses on proposing a novel method for prompt learning in VLMs and evaluating its empirical performance. It doesn\u2019t introduce any new theoretical results or theorems requiring formal proofs. ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 16}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The paper provides all the necessary information for reproducing the experimental results. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "637   \n638   \n639   \n640   \n641   \n642   \n643   \n644   \n645   \n646   \n647   \n648   \n649   \n650   \n651   \n652   \n653   \n654   \n655   \n656   \n657   \n658   \n659   \n660   \n661   \n662   \n663   \n664   \n665   \n666   \n667   \n668   \n669   \n670   \n671   \n672   \n673   \n674   \n675   \n676   \n677   \n678   \n679   \n680   \n681   \n682   \n683   \n684   \n685   \n686   \n687   \n688   \n689   \n690   \n691   \n692   \n693   \n694 ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 16}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 16}, {"type": "text", "text": "Justification: Our codebase is built based on the CoOP and CoCoOP [54, 53], and can be reproduced based on our Methods, Implementation details in main text and appendix. Our code will be released upon acceptance. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details. ", "page_idx": 16}, {"type": "text", "text": "\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 17}, {"type": "text", "text": "710 6. Experimental Setting/Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 17}, {"type": "text", "text": "Justification: The training and test details can be found in section Experiments and Appendix. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 17}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 17}, {"type": "text", "text": "Justification: We follow previous works [54, 53] to report results averaged over 3 runs. Error bars are not reported. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 17}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 18}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We conform with the NeurIPS Code of Ethics in every aspect. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 18}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: The potential societal impacts are discussed in Appendix. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "752   \n753   \n754   \n755   \n756   \n757   \n758   \n759   \n760   \n761   \n762   \n763   \n764   \n765   \n766   \n767   \n768   \n769   \n770   \n771   \n772   \n773   \n774   \n775   \n776   \n777   \n778   \n779   \n780   \n781   \n782   \n783   \n784   \n785   \n786   \n787   \n788   \n789   \n790   \n791   \n792   \n793   \n794   \n795   \n796   \n797   \n798   \n799   \n800   \n801   \n802   \n803   \n804   \n805   \n806   \n807 ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 18}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: This paper primarily focuses on a novel prompt learning method and doesn\u2019t involve the release of a new pre-trained LLM, image generator, or scraped dataset. Therefore, this question doesn\u2019t directly apply in this context. We leverage an existing pre-trained LLM (GPT-3.5-turbo), and any ethical considerations regarding its release and potential misuse fall under the responsibility of its creators. ", "page_idx": 18}, {"type": "text", "text": "08 Guidelines:   \n09 \u2022 The answer NA means that the paper poses no such risks.   \n10 \u2022 Released models that have a high risk for misuse or dual-use should be released with necessary   \n11 safeguards to allow for controlled use of the model, for example by requiring that users adhere to   \n12 usage guidelines or restrictions to access the model or implementing safety filters.   \n13 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should   \n4 describe how they avoided releasing unsafe images.   \n15 \u2022 We recognize that providing effective safeguards is challenging, and many papers do not require   \n16 this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 19}, {"type": "text", "text": "817 12. Licenses for existing assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "18 Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper,   \n19 properly credited and are the license and terms of use explicitly mentioned and properly respected?   \n20 Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We credited the creators of the CoOp codebase [54] by including the attribution statement in appendix. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 19}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 19}, {"type": "text", "text": "4 Justification: Code will be realseased upon acceptance.   \n42 Guidelines:   \n43 \u2022 The answer NA means that the paper does not release new assets.   \n44 \u2022 Researchers should communicate the details of the dataset/code/model as part of their sub  \n45 missions via structured templates. This includes details about training, license, limitations,   \n46 etc.   \n47 \u2022 The paper should discuss whether and how consent was obtained from people whose asset is   \n48 used.   \n49 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an   \n50 anonymized URL or include an anonymized zip file. ", "page_idx": 19}, {"type": "text", "text": "851 14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "52 Question: For crowdsourcing experiments and research with human subjects, does the paper include   \n53 the full text of instructions given to participants and screenshots, if applicable, as well as details about   \n54 compensation (if any)? ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. ", "page_idx": 19}, {"type": "text", "text": "63 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other   \n64 labor should be paid at least the minimum wage in the country of the data collector.   \n65 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects   \n66 Question: Does the paper describe potential risks incurred by study participants, whether such   \n67 risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an   \n68 equivalent approval/review based on the requirements of your country or institution) were obtained?   \n69 Answer: [NA]   \n70 Justification: This paper focuses on developing a novel prompt learning method and evaluating its   \n71 performance on established image recognition datasets. It doesn\u2019t involve any form of crowdsourcing,   \n72 human subject research, or data collection that would necessitate IRB approval or ethical considerations   \n73 related to study participants. Therefore, this question doesn\u2019t apply to our research.   \n74 Guidelines:   \n75 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human   \n76 subjects.   \n77 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be   \n78 required for any human subjects research. If you obtained IRB approval, you should clearly state   \n79 this in the paper.   \n80 \u2022 We recognize that the procedures for this may vary significantly between institutions and   \n81 locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for   \n82 their institution.   \n83 \u2022 For initial submissions, do not include any information that would break anonymity (if applica  \n84 ble), such as the institution conducting the review. ", "page_idx": 20}]