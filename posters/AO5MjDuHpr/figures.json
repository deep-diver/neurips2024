[{"figure_path": "AO5MjDuHpr/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of the methods for CLIP text prompts formation. (a) Manually created prompt with the single \"a photo of a {class}\" template; (b) A unstructured set of detailed descriptions generated by LLMs; (c) The proposed Tree of Attribute that organizes the descriptions in a \u201cconcept - attribute - descriptions\" structure, essentially distilling knowledge graphs from LLMs; (d) An example Tree of Attribute for \"dumplings\".", "description": "This figure compares different methods for creating text prompts for CLIP, a vision-language model.  (a) shows a simple, manually-crafted prompt. (b) demonstrates using an unstructured set of descriptions generated by a Large Language Model (LLM). (c) illustrates the paper's proposed method, \"Tree of Attributes\", which organizes LLM-generated descriptions into a structured knowledge graph. (d) provides a concrete example of this structured knowledge graph for the category 'dumplings'.", "section": "1 Introduction"}, {"figure_path": "AO5MjDuHpr/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of the proposed TAP method. TAP utilizes fine-grained descriptions from LLMs and organizes them in a Tree of Attribute. Vision expert tokens are added to the vision encoder to learn from specific attributes such as color and shape. A vision-conditional pooling layer is introduced to ensure optimal image-text alignment. Textual context tokens are also incorporated to the textual branch, shared across descriptions.", "description": "This figure illustrates the overall framework of the Tree of Attributes Prompt Learning (TAP) method.  It shows how fine-grained descriptions generated by LLMs are structured into a tree-like hierarchy, with the class name as the root and specific attributes (e.g., color, shape) as branches leading to leaf nodes representing detailed descriptions.  The figure also highlights the integration of learnable vision expert tokens into the vision encoder to focus on specific attributes and the use of a vision-conditional pooling layer to select the most relevant descriptions for a given image. Finally, shared textual context tokens are shown integrated into the textual branch to maintain consistency across descriptions.", "section": "3 Methodology"}, {"figure_path": "AO5MjDuHpr/figures/figures_7_1.jpg", "caption": "Figure 2: Overview of the proposed TAP method. TAP utilizes fine-grained descriptions from LLMs and organizes them in a Tree of Attribute. Vision expert tokens are added to the vision encoder to learn from specific attributes such as color and shape. A vision-conditional pooling layer is introduced to ensure optimal image-text alignment. Textual context tokens are also incorporated to the textual branch, shared across descriptions.", "description": "This figure illustrates the architecture of the Tree of Attributes Prompt Learning (TAP) method.  It shows how fine-grained descriptions from LLMs are organized into a hierarchical tree structure.  The vision encoder incorporates \"vision expert tokens\" to focus on specific visual attributes, and a vision-conditional pooling layer selects the most relevant descriptions for each attribute. Shared text context tokens are used in the text branch.  The overall design aims to improve the alignment between visual and textual information.", "section": "3 Methodology"}, {"figure_path": "AO5MjDuHpr/figures/figures_8_1.jpg", "caption": "Figure 4: Visualization of the attention weights in the VCP layer for an example \"dumplings\" image.", "description": "This figure visualizes the attention weights within the Vision-Conditional Pooling (VCP) layer for a single image of dumplings. The VCP layer uses attention to determine which descriptions are most relevant to the input image, assigning higher weights to the most relevant descriptions.  The figure highlights which attributes (Color, Texture, Shape, Presentation) and their corresponding descriptions receive higher attention weights, providing an insight into how the model selects the most appropriate descriptions for the input.", "section": "4.4 Visualization"}]