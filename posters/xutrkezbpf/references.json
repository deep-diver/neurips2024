{"references": [{"fullname_first_author": "Geoffrey Hinton", "paper_title": "Distilling the knowledge in a neural network", "publication_date": "2015-03-02", "reason": "This paper introduced the foundational concept of knowledge distillation, a core technique that the current paper builds upon and improves."}, {"fullname_first_author": "Seyed Iman Mirzadeh", "paper_title": "Improved knowledge distillation via teacher assistant", "publication_date": "2020-00-00", "reason": "This paper introduced the concept of using intermediate teacher assistant models to improve knowledge transfer, a key idea that the current paper addresses and refines."}, {"fullname_first_author": "Wonchul Son", "paper_title": "Densely guided knowledge distillation using multiple teacher assistants", "publication_date": "2021-00-00", "reason": "This paper further developed the teacher assistant approach by using multiple assistants, which the current paper directly addresses with its proposed Rate-Distortion Module approach."}, {"fullname_first_author": "Yulei Niu", "paper_title": "Respecting transfer gap in knowledge distillation", "publication_date": "2022-00-00", "reason": "This paper highlighted the challenges of transfer gaps in knowledge distillation, which directly motivates the controlled information flow approach presented in the current work."}, {"fullname_first_author": "Tao Huang", "paper_title": "Knowledge distillation from a stronger teacher", "publication_date": "2022-00-00", "reason": "This paper also addressed the challenges of large teacher-student model size differences, proposing a different solution which the current paper contrasts and compares against."}]}