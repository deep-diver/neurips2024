[{"figure_path": "kQ9LgM2JQT/figures/figures_2_1.jpg", "caption": "Figure 1: Solely relying on flow functions F in GFNs can be insufficient. While GFNs capture how much stuff there is, they spend time sampling from lots of small rewards.", "description": "This figure illustrates a scenario where an agent chooses between two doors.  Behind one door are many doors with small rewards (1), while behind the other is a single door with a large reward (100). GFNs, based on flow functions, would assign equal probability to both doors, spending time sampling small rewards.  However, a reinforcement learning (RL) agent, using action values (Q), would immediately choose the door with the large reward. This highlights the limitation of relying solely on GFNs for generating high-reward samples and introduces the motivation for combining GFNs with RL action values to create a more effective sampling strategy.", "section": "3 Motivation"}, {"figure_path": "kQ9LgM2JQT/figures/figures_4_1.jpg", "caption": "Figure 2: Fragment-based molecule task. Left: Average rewards over the training trajectories. Center: Number of unique modes with a reward threshold exceeding 0.97 and pairwise Tanimoto similarity score less than 0.65. Right: Average pairwise Tanimoto similarity score for the top 1000 molecules sampled by reward. Lines are the interquartile mean and standard error calculated over 5 seeds.", "description": "This figure shows the results for the fragment-based molecule generation task.  The left panel displays the average rewards obtained over training trajectories for different methods, showing QGFN's improved performance. The center panel illustrates the number of unique modes discovered (high-reward molecules with low similarity), highlighting QGFN's enhanced ability to find diverse solutions. The right panel presents the average pairwise Tanimoto similarity scores among the top 1000 molecules, indicating that QGFN also maintains diversity among high-reward molecules.", "section": "Main results"}, {"figure_path": "kQ9LgM2JQT/figures/figures_4_2.jpg", "caption": "Figure 2: Fragment-based molecule task. Left: Average rewards over the training trajectories. Center: Number of unique modes with a reward threshold exceeding 0.97 and pairwise Tanimoto similarity score less than 0.65. Right: Average pairwise Tanimoto similarity score for the top 1000 molecules sampled by reward. Lines are the interquartile mean and standard error calculated over 5 seeds.", "description": "This figure presents the results of the fragment-based molecule generation task.  The left panel shows the average rewards obtained over several training trajectories, comparing the QGFN variants (p-greedy, p-quantile, p-of-max) with several baselines (TB, SubTB, DQN, LSL-GFN, A2C, MunDQN). The center panel displays the number of unique modes (high-reward molecules with low similarity) discovered by each method.  Finally, the right panel illustrates the average pairwise Tanimoto similarity of the top 1000 molecules sampled by each method, indicating the diversity of the generated molecules. The lines represent the interquartile mean and standard error, calculated from 5 independent trials.", "section": "5 Main results"}, {"figure_path": "kQ9LgM2JQT/figures/figures_5_1.jpg", "caption": "Figure 4: RNA-binding tasks, Average reward and modes. Left: L14RNA1 task. Right: L14RNA1+2 task, based on 5 seeds (interquartile mean and standard error shown).", "description": "This figure shows the results of RNA-binding tasks using different methods, including QGFN variants and baselines.  The left panel presents results for the L14RNA1 task, while the right panel shows results for the L14RNA1+2 task. The plots display average reward and the number of modes discovered by each method across multiple runs (seeds).  Error bars represent interquartile ranges, indicating the spread of results. The figure demonstrates the comparative performance of QGFN variants (p-greedy, p-quantile, and p-of-max) against several baselines (TB, SubTB, DQN, LSL-GFN, MunDQN, and A2C) in terms of both reward and mode discovery.", "section": "5 Main results"}, {"figure_path": "kQ9LgM2JQT/figures/figures_5_2.jpg", "caption": "Figure 2: Fragment-based molecule task. Left: Average rewards over the training trajectories. Center: Number of unique modes with a reward threshold exceeding 0.97 and pairwise Tanimoto similarity score less than 0.65. Right: Average pairwise Tanimoto similarity score for the top 1000 molecules sampled by reward. Lines are the interquartile mean and standard error calculated over 5 seeds.", "description": "This figure presents the results for a fragment-based molecule generation task. The left panel shows the average rewards obtained over the training trajectories for various methods. The central panel illustrates the number of unique modes discovered (molecules with rewards above 0.97 and dissimilarity above 0.65), while the right panel displays the average pairwise Tanimoto similarity for the top 1000 molecules generated, reflecting the diversity of the samples produced.", "section": "5 Main results"}, {"figure_path": "kQ9LgM2JQT/figures/figures_7_1.jpg", "caption": "Figure 6: Fragment task. Left: Effect of \u03b2: Increasing greediness through \u03b2 increases the average reward but may lead to diversity collapse. QGFN maintains diversity with a lower \u03b2, while GFN collapses. Modes are counted from 1000 samples at inference, using an inference-adjusted p. Right: Effect of training parameters p, and n: Changing p can control greediness, while increasing n is generally beneficial. Modes are counted from 1000 samples generated using the training p.", "description": "This figure demonstrates the impact of hyperparameters \u03b2, p, and n on the performance of QGFN and a baseline GFN model on a fragment-based molecule generation task.  The left panel shows that increasing the temperature parameter (\u03b2) in both QGFN and GFN leads to higher average rewards but can cause a diversity collapse in the GFN, which QGFN mitigates. The right panel illustrates the effect of the greediness parameter (p) and n-step returns (n) on QGFN performance.  Adjusting p allows controlling greediness, while increasing n generally improves both reward and diversity.", "section": "5 Main results"}, {"figure_path": "kQ9LgM2JQT/figures/figures_7_2.jpg", "caption": "Figure 7: Comparing Q(s, a; 0) predictions with empirical estimates obtained by rollouts. Bars are standard error. Q is relatively capable to estimate the returns of the corresponding policy.", "description": "This figure shows the comparison between the predicted Q-values (Q(s, a; 0)) and their empirically estimated counterparts (Q<sup>\u03bc</sup>).  The Q-values are the predictions from the trained action-value function Q, while Q<sup>\u03bc</sup> is obtained via rollouts using the policy corresponding to the Q-values. Each point represents a state-action pair (s, a), with the x-coordinate being the empirically estimated Q<sup>\u03bc</sup> and the y-coordinate being the predicted Q(s, a; 0). Error bars represent the standard error for the empirical Q<sup>\u03bc</sup> estimates. The plot shows a reasonably strong positive correlation between the predicted and empirical Q-values, indicating that the learned Q-function is relatively capable of estimating the expected rewards under the corresponding policy.  This supports the validity of using the Q-function to guide the generation process in QGFN.", "section": "6 Method Analysis"}, {"figure_path": "kQ9LgM2JQT/figures/figures_8_1.jpg", "caption": "Figure 8: Pruning helps avoid low-reward parts of the state space. Reward distributions when (a) sampling with p-of-max; (b) greedily according to PF selecting actions that p-of-max would prune, Best pruned actions; (c) selecting most likely PF actions regardless of Q, Best actions; and (d) normal sampling from PF (without using Q).", "description": "This figure compares reward distributions obtained from different sampling strategies.  The x-axis represents the reward, and the y-axis shows the density of samples with that reward.  Four scenarios are depicted:\n\n(a) p-of-max QGFN (0.95): Samples are generated using the p-of-max QGFN approach with p=0.95, which prunes actions with low Q-values (action-value estimates). This strategy concentrates samples in high-reward regions.\n(b) Best pruned actions:  Actions are greedily selected according to the forward policy (PF) but pruning actions with low Q values based on a pretrained p-of-max QGFN model. This shows that the Q function in the model helps to achieve higher rewards.\n(c) Best actions: Actions are selected greedily according to the forward policy PF. Without considering Q, more samples are drawn from low-reward regions. \n(d) actions ~ PF: Shows the baseline reward distribution if only the forward policy PF is used for sampling, with no pruning or greedy action selection based on Q-values.", "section": "Why does changing p work?"}, {"figure_path": "kQ9LgM2JQT/figures/figures_8_2.jpg", "caption": "Figure 9: Varying p at inference time induces reward-diversity trade-offs; fragment task.", "description": "This figure demonstrates the impact of changing the parameter 'p' during inference on the reward and diversity of samples generated by the QGFN model in the fragment-based molecule generation task.  The x-axis represents the average pairwise Tanimoto similarity, a measure of diversity (lower values indicate higher diversity), and the y-axis represents the average reward (higher values are better). Three QGFN variants (p-greedy, p-quantile, and p-of-max) are shown, each with a range of p values. The color gradient in the legend indicates the range from least greedy (dark grey) to greediest (light grey).  The figure shows that by adjusting the 'p' parameter, it's possible to achieve various trade-offs between reward (greediness) and diversity.  Higher 'p' values tend towards higher rewards but lower diversity, while lower 'p' values lead to lower rewards but higher diversity.", "section": "5 Main results"}, {"figure_path": "kQ9LgM2JQT/figures/figures_14_1.jpg", "caption": "Figure 7: Comparing Q(s, a; 0) predictions with empirical estimates obtained by rollouts. Bars are standard error. Q is relatively capable to estimate the returns of the corresponding policy.", "description": "This figure presents a comparison between the predicted Q-values from a trained Q-network and empirical Q-values obtained through rollouts. The plot on the top shows a scatter plot where each point represents a state-action pair, with the x-coordinate representing the empirical Q-value (obtained by averaging the returns from multiple rollouts starting from that state-action pair) and the y-coordinate representing the predicted Q-value from the model.  The color of each point corresponds to the value of p (the greediness parameter) used during training. A diagonal line represents perfect correspondence between predicted and empirical Q-values. The plot shows that the predicted Q-values are reasonably close to the empirical Q-values, indicating that the Q-network is relatively capable of estimating the returns of the corresponding policy. The bottom plots further analyze the relationship between the predicted and empirical Q-values by showing the rank correlation and the ratio of lower-bounded estimates as a function of p. The rank correlation plot shows that the rank ordering of Q-values predicted by the model is well-aligned with that from the rollouts, with the correlation reducing as the greediness parameter p increases. The ratio of lower-bounded estimates indicates the extent to which the model underestimates the actual value, which also increases with p.", "section": "Method Analysis"}, {"figure_path": "kQ9LgM2JQT/figures/figures_14_2.jpg", "caption": "Figure 9: Varying p at inference time induces reward-diversity trade-offs; fragment task.", "description": "This figure shows the impact of changing the greediness parameter 'p' during inference on the average reward and average similarity of the generated molecules in the fragment-based molecule generation task.  The x-axis represents the average pairwise Tanimoto similarity, a measure of diversity, and the y-axis represents the average reward. The two lines represent two variants of the QGFN method: one using the trained forward policy (PF), and another using a random forward policy.  The color gradient indicates the level of greediness controlled by 'p', ranging from least greedy (light cyan) to greediest (magenta). The figure demonstrates the tunable trade-off between reward and diversity introduced by the QGFN approach.", "section": "5 Main results"}, {"figure_path": "kQ9LgM2JQT/figures/figures_16_1.jpg", "caption": "Figure 9: Varying p at inference time induces reward-diversity trade-offs; fragment task.", "description": "This figure displays the impact of changing the parameter 'p' during inference on the average reward and average similarity of generated samples in the fragment-based molecule generation task. The x-axis shows average similarity, while the y-axis presents average reward. Each line represents a different QGFN variant. The color gradient indicates how greedy the sampling policy is (from least greedy to greediest). As 'p' increases, the average reward generally increases while average similarity decreases, indicating a trade-off between reward and diversity.  The MCTS method (marked by a star) shows high rewards but less diversity than other methods.", "section": "Main results"}, {"figure_path": "kQ9LgM2JQT/figures/figures_16_2.jpg", "caption": "Figure 13: QGFN variants on learning objectives SubTB on Fragment-based molecule task", "description": "This figure compares the performance of three QGFN variants (p-greedy, p-quantile, and p-of-max) and the SubTB baseline on the fragment-based molecule generation task.  The plots show the average reward, the number of unique modes discovered, and the average Tanimoto similarity score of the top 1000 molecules sampled over the course of training.  The results demonstrate that the QGFN variants consistently outperform the SubTB baseline in terms of both average reward and the number of modes discovered.", "section": "Main results"}, {"figure_path": "kQ9LgM2JQT/figures/figures_17_1.jpg", "caption": "Figure 14: QGFN variants on learning objectives FM (Flow Matching) on Fragment-based molecule task", "description": "This figure presents the results of experiments comparing different variants of the QGFN algorithm (p-greedy QGFN-FM, p-quantile QGFN-FM, p-of-max QGFN-FM) against a baseline method (FM) on a fragment-based molecule generation task.  Three plots are shown: Average Reward, Number of Modes, and Top-1000 Tanimoto Similarity. The x-axis represents the number of trajectories sampled, indicating progress during training. The y-axes show the respective metrics, Average Reward illustrating the average reward obtained, Number of Modes representing the number of unique molecular structures discovered, and Top-1000 Tanimoto Similarity indicating the average similarity of the top 1000 molecules, where lower values suggest greater diversity. The shaded areas represent standard error, and each line represents the average over multiple runs.", "section": "Main results"}, {"figure_path": "kQ9LgM2JQT/figures/figures_17_2.jpg", "caption": "Figure 15: Weight sharing in p-greedy QGFN (p=0.4) with different layers", "description": "This figure illustrates the impact of weight sharing between the policy network (PF) and the action-value network (Q) in the p-greedy QGFN model, specifically focusing on different layers of the graph attention transformer.  The results show the average reward, number of unique modes, and average pairwise Tanimoto similarity scores over the training trajectories for various layer sharing configurations (0, 1, 2, 3, and 4 layers).  The experiment was conducted on the fragment-based molecule generation task. The plot shows that sharing weights does not always lead to improvements, potentially due to conflicts between gradients from different loss functions, or differences in scale.", "section": "B.4 Exploring weight sharing in QGFN"}, {"figure_path": "kQ9LgM2JQT/figures/figures_18_1.jpg", "caption": "Figure 16: Fragment-based molecule generation task; we showcase the performance of QGFN's predecessor, which failed to beat baselines regardless of our attempts to improve it.", "description": "This figure displays the results of several approaches that were tried before developing QGFN, all of which failed to outperform baseline methods. The approaches shown include using a diverse-based replay buffer, an adaptive reward prioritized replay buffer, weight sharing between the GFN and DQN, using a pretrained Q-network to guide the selection of greedier actions, and simply using n-step returns.  The results are presented in terms of average reward and the number of unique modes discovered across different methods.  The figure demonstrates that none of these pre-QGFN approaches achieved better results than the simpler baselines, highlighting the significance of the QGFN approach.", "section": "C Experiments that did not work"}, {"figure_path": "kQ9LgM2JQT/figures/figures_18_2.jpg", "caption": "Figure 17: Pairwise Tanimoto Similarity scores assessing the impacts of \u03b2, training parameters p and n in the fragment task. Left: An increase in \u03b2 initially decreases sample similarities, followed by a gradual increase in similarity as the models get greedier through \u03b2. Center: Increase greediness does not always correlate with sample similarity trade-offs with QGFN, but at peak greediness, similarity scores rebound. Right: Increasing n increases similarity among models.", "description": "This figure shows the effect of hyperparameters \u03b2, p, and n on the pairwise Tanimoto similarity scores in the fragment-based molecule generation task.  The left panel demonstrates that increasing \u03b2 initially reduces similarity, but as the model becomes greedier, similarity increases again. The center panel illustrates that increasing the greediness parameter p doesn't always negatively affect similarity, showing a complex relationship. The right panel reveals that increasing n (the number of bootstrapping steps in Q-learning) leads to higher similarity scores.", "section": "Method Analysis"}, {"figure_path": "kQ9LgM2JQT/figures/figures_23_1.jpg", "caption": "Figure 2: Fragment-based molecule task. Left: Average rewards over the training trajectories. Center: Number of unique modes with a reward threshold exceeding 0.97 and pairwise Tanimoto similarity score less than 0.65. Right: Average pairwise Tanimoto similarity score for the top 1000 molecules sampled by reward. Lines are the interquartile mean and standard error calculated over 5 seeds.", "description": "This figure displays the results of the fragment-based molecule generation task.  The left panel shows the average reward across multiple training trajectories for various methods, illustrating how the proposed QGFN methods consistently achieve higher average rewards compared to baselines. The center panel compares the number of unique modes (high-reward, diverse molecules) discovered by each method, demonstrating the effectiveness of QGFNs in finding diverse high-reward molecules. The right panel presents the average pairwise Tanimoto similarity among the top 1000 molecules sampled, with lower values indicating higher diversity. The error bars represent the interquartile range across five independent experimental runs.", "section": "5 Main results"}]