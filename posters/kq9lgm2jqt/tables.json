[{"figure_path": "kQ9LgM2JQT/tables/tables_6_1.jpg", "caption": "Table 1: Fragment-based molecule task: Reward and Diversity at inference after training.", "description": "This table presents the results of a comparison of different methods for fragment-based molecule generation after training. The methods compared are GFN-TB, GFN-SUBTB, LSL-GFN, p-greedy QGFN, p-of-max QGFN, and p-quantile QGFN. For each method, the table shows the average reward and diversity, which are calculated at inference time after the models are trained. The reward is a measure of the quality of the generated molecules, while the diversity is a measure of how different the generated molecules are from each other. The results show that the QGFN methods achieve higher average rewards and lower average diversity than the baseline methods.  The p-of-max QGFN achieves the highest average reward, while the p-quantile QGFN achieves the lowest average diversity.", "section": "Main results"}, {"figure_path": "kQ9LgM2JQT/tables/tables_15_1.jpg", "caption": "Table 2: Fragment-based molecule task: reward and diversity of independently trained baseline models using a trained Q. The p values for p-greedy, p-of-max, and p-quantile QGFN are set at 0.4, 0.9858, and 0.93, respectively.", "description": "This table presents the results of using a pre-trained Q-function (action-value) from a QGFN model during inference, but applied to independently trained baseline GFN models (Trajectory Balance and SubTrajectory Balance).  It shows the average reward and diversity obtained for these models, illustrating how using the pre-trained Q-function can still improve performance on models not trained with Q. The table includes results for three different QGFN variants (p-greedy, p-of-max, p-quantile), demonstrating the effect of varying the greediness parameter.", "section": "B.1 Using Q from different behavior policies"}, {"figure_path": "kQ9LgM2JQT/tables/tables_16_1.jpg", "caption": "Table 1: Fragment-based molecule task: Reward and Diversity at inference after training.", "description": "This table presents a comparison of the reward and diversity achieved by different methods on the fragment-based molecule generation task.  The metrics reported are the average reward and the average pairwise Tanimoto similarity (a measure of diversity) calculated from 1000 samples obtained after training. The methods compared include GFN-TB (Generative Flow Network with Trajectory Balance loss), GFN-SubTB (Generative Flow Network with Sub-Trajectory Balance loss), LSL-GFN (Logit Scaling for GFlowNets), and three variants of the proposed QGFN method (p-greedy, p-of-max, and p-quantile).  The results demonstrate the improvement in reward achieved by the QGFN methods compared to baselines without sacrificing diversity. The table highlights the controllable greediness of the QGFN methods.", "section": "5 Main results"}, {"figure_path": "kQ9LgM2JQT/tables/tables_19_1.jpg", "caption": "Table 1: Fragment-based molecule task: Reward and Diversity at inference after training.", "description": "This table presents the average reward and diversity (measured by average pairwise Tanimoto similarity) of different methods on the fragment-based molecule generation task. The results are obtained at inference time after training the models.  It compares the performance of several QGFN variants against baseline methods (GFN-TB, GFN-SubTB, LSL-GFN) to demonstrate the ability of QGFN to achieve higher rewards without sacrificing diversity.", "section": "5 Main results"}, {"figure_path": "kQ9LgM2JQT/tables/tables_20_1.jpg", "caption": "Table 1: Fragment-based molecule task: Reward and Diversity at inference after training.", "description": "This table shows the average reward and diversity (measured by average pairwise Tanimoto similarity) of different methods on the fragment-based molecule generation task after training.  The methods compared include various GFN (Generative Flow Network) baselines and the proposed QGFN (Q-enhanced Generative Flow Network) variants.  It highlights that QGFN methods generally achieve higher rewards without significantly sacrificing diversity compared to the baseline methods.", "section": "Main results"}, {"figure_path": "kQ9LgM2JQT/tables/tables_21_1.jpg", "caption": "Table 1: Fragment-based molecule task: Reward and Diversity at inference after training.", "description": "This table presents a comparison of the reward and diversity achieved by different methods on the fragment-based molecule generation task after training. The methods compared include GFN-TB, GFN-SubTB, LSL-GFN, and three variants of the proposed QGFN method (p-greedy, p-of-max, and p-quantile).  The reward metric measures the average reward of the generated molecules, while the diversity metric reflects the average pairwise Tanimoto similarity, indicating the dissimilarity between generated molecules.  Higher reward and lower similarity values are desired.", "section": "Main results"}, {"figure_path": "kQ9LgM2JQT/tables/tables_21_2.jpg", "caption": "Table 1: Fragment-based molecule task: Reward and Diversity at inference after training.", "description": "This table presents a comparison of the reward and diversity obtained by different methods after training on the fragment-based molecule generation task. The reward metric reflects the average reward of the generated molecules, while the diversity metric (similarity) measures the average pairwise similarity between molecules. Lower similarity indicates greater diversity.  The results show the performance of various models, including GFN-TB, GFN-SubTB, LSL-GFN, and the three proposed QGFN variants (p-greedy, p-of-max, p-quantile).  The comparison aims to demonstrate the effectiveness of QGFN in enhancing the generation of high-reward and diverse molecules.", "section": "Main results"}, {"figure_path": "kQ9LgM2JQT/tables/tables_22_1.jpg", "caption": "Table 2: Fragment-based molecule task: reward and diversity of independently trained baseline models using a trained Q. The p values for p-greedy, p-of-max, and p-quantile QGFN are set at 0.4, 0.9858, and 0.93, respectively.", "description": "This table presents the results of experiments on the fragment-based molecule task using independently trained baseline models with a pretrained Q-function for inference. It compares the performance of three QGFN variants (p-greedy, p-of-max, and p-quantile) against two baseline methods (TB and SubTB) in terms of reward and diversity. The p values used for each QGFN variant are specified in the caption.", "section": "B.1 Using from different behavior policies"}, {"figure_path": "kQ9LgM2JQT/tables/tables_22_2.jpg", "caption": "Table 9: Model-specific parameters for QGFN in RNA-binding task.", "description": "This table lists the hyperparameters used for the QGFN model in the RNA-binding task.  It specifies the objective function used for training (TB), the values of the greediness parameter p for the three QGFN variants (p-greedy, p-quantile, p-of-max), the schedule for annealing the p parameter, the model architecture used (Sequence Transformer), the number of steps used for the n-step Q-learning method, and the temperature parameter \u03c4 for the DQN.  These parameters were tuned to optimize performance on the RNA-binding task.", "section": "G Experiment details: RNA-binding task"}, {"figure_path": "kQ9LgM2JQT/tables/tables_23_1.jpg", "caption": "Table 1: Fragment-based molecule task: Reward and Diversity at inference after training.", "description": "This table presents the average reward and diversity of different models on the fragment-based molecule generation task after training.  It compares the performance of several methods, including GFN-TB, GFN-SubTB, LSL-GFN, and three variants of the proposed QGFN approach (p-greedy, p-of-max, and p-quantile).  The reward metric is higher is better, and diversity (measured by average Tanimoto similarity) is lower is better. The results demonstrate the ability of QGFN to achieve higher rewards without sacrificing diversity compared to baseline methods.", "section": "Main results"}, {"figure_path": "kQ9LgM2JQT/tables/tables_23_2.jpg", "caption": "Table 2: Fragment-based molecule task: reward and diversity of independently trained baseline models using a trained Q. The p values for p-greedy, p-of-max, and p-quantile QGFN are set at 0.4, 0.9858, and 0.93, respectively.", "description": "This table presents the results of an experiment evaluating the performance of independently trained baseline models (TB and SubTB) when guided by a pre-trained Q-function during inference.  Three variants of the QGFN approach (p-greedy, p-of-max, and p-quantile) are compared against the baselines. The table shows the average reward and diversity achieved by each method. The p-values used in the QGFN variants are specified to provide context. This experiment demonstrates that using a pre-trained Q-function can enhance the performance of independently trained models even on tasks where those models performed poorly during training.", "section": "B.1 Using from different behavior policies"}, {"figure_path": "kQ9LgM2JQT/tables/tables_24_1.jpg", "caption": "Table 1: Fragment-based molecule task: Reward and Diversity at inference after training.", "description": "This table presents a comparison of the reward and diversity achieved by different methods on the fragment-based molecule generation task, specifically focusing on results obtained during the inference phase after model training.  The methods compared include baseline GFN approaches (GFN-TB and GFN-SubTB), LSL-GFN (a method that controls greediness through temperature conditioning), and three variants of the proposed QGFN approach (p-greedy, p-of-max, and p-quantile). For each method, the average reward and average pairwise Tanimoto similarity (a measure of diversity) across 1000 samples are reported, along with standard errors.", "section": "Main results"}]