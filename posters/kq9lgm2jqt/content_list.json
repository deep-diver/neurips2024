[{"type": "text", "text": "QGFN: Controllable Greediness with Action Values ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Elaine Lau1 2 Stephen Zhewen Lu2 Ling Pan1 5 Doina Precup1 2 3 Emmanuel Bengio4 1Mila - Qu\u00e9bec AI Institute 2McGill University 3Google Deepmind 4Valence Labs 5Hong Kong University of Science and Technology ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Generative Flow Networks (GFlowNets; GFNs) are a family of energy-based generative methods for combinatorial objects, capable of generating diverse and high-utility samples. However, consistently biasing GFNs towards producing high-utility samples is non-trivial. In this work, we leverage connections between GFNs and reinforcement learning (RL) and propose to combine the GFN policy with an action-value estimate, $Q$ , to create greedier sampling policies which can be controlled by a mixing parameter. We show that several variants of the proposed method, QGFN, are able to improve on the number of high-reward samples generated in a variety of tasks without sacrificing diversity. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Generative Flow Networks [Bengio et al., 2021a,b], also known as GFlowNets, or GFNs, were recently introduced as a novel generative framework in the family of energy-based models [Malkin et al., 2022b, Zhang et al., 2022]. Given some energy function $f(x)$ over objects $\\mathcal{X}$ , the promise of GFNs is to train a sampler $p_{\\theta}$ such that at convergence $p_{\\theta}(x)\\,\\stackrel{}{\\propto}\\,\\exp(-f(x))$ ; $\\exp(-\\bar{f}(x))$ is also referred to as the reward $R(x)$ in GFN literature, inheriting terminology from Reinforcement Learning (RL). GFNs achieve this sampling via a constructive approach, treating the creation of some object $x$ as a sequential additive process (rather than an iterative local process $\\grave{a}$ la Markov chain Monte Carlo (MCMC) that can suffer from mode-mixing issues). The main advantage of a GFN is its ability to generate a greater diversity of low-energy/high-reward objects compared to approaches based on MCMC or RL [Bengio et al., 2021a, Jain et al., 2022, 2023], or even Soft-RL\u2013which, while related to GFNs, accomplishes something different by default [Tiapkin et al., 2023, Mohammadpour et al., 2024, Deleu et al., 2024]. ", "page_idx": 0}, {"type": "text", "text": "To generate more interesting samples and avoid oversampling from low-reward regions, it is common to train a model to sample in proportion to $R(x)^{\\beta}$ ; $\\beta$ is an inverse temperature, typically $\\gg1$ , which pushes the model to become greedier. The use of this temperature (hyper)parameter is an important control knob in GFNs. However, tuning this hyperparameter is non-trivial, which complicates training certain GFNs; for example, trajectory balance [Malkin et al., 2022a] is sensitive to the \u201cpeakiness\" of the reward landscape [Madan et al., 2023]. Although it is possible to train temperature-conditional models [Kim et al., 2023a], doing so essentially requires learning a whole family of GFNs\u2013no easy task, albeit doable, e.g., in multiobjective settings [Jain et al., 2023]. ", "page_idx": 0}, {"type": "text", "text": "In this work, we propose an approach that allows selecting arbitrary greediness at inference time, which preserves the generality of temperature-conditionals, while simplifying the training process. We do so without the cost of learning a complicated family of functions and without conditionals, instead only training two models: a GFlowNet and an action-value function $Q$ [Watkins and Dayan, 1992, Mnih et al., 2013]. ", "page_idx": 0}, {"type": "text", "text": "Armed with the forward policy of a GFN, $P_{F}$ (which decides a probability distribution over actions given the current state, i.e. $\\pi$ in RL), and the action-value, $Q$ , we show that it is possible to create a variety of controllably greedy sampling policies, controlled by parameters that require no retraining. We show that it is possible to simultaneously learn $P_{F}$ and $Q$ , and in doing so, to generate more high-reward yet diverse object sets. In particular, we introduce and benchmark three specific variants of our approach, which we call QGFN: $p$ -greedy, $p$ -quantile, and $p$ -of-max. ", "page_idx": 1}, {"type": "text", "text": "We evaluate the proposed methods on 5 standard tasks used in prior GFN works: the fragment-based molecular design task introduced by Bengio et al. [2021a], 2 RNA design tasks introduced by Sinai et al. [2020], a small molecule design task based on QM9 [Jain et al., 2023], as well as a bit sequence task from Malkin et al. [2022a], Shen et al. [2023]. The proposed method outperforms strong baselines, achieving high average rewards and discovering modes more efficiently, sometimes by a large margin. We conduct an analysis of the proposed methods, investigate key design choices, and probe the methods to understand why they work. We also investigate other possible combinations of $Q$ and $P_{F}$ \u2013again, entirely possible at inference time, by combining trained $Q$ and $P_{F}$ models. ", "page_idx": 1}, {"type": "text", "text": "2 Background and Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We follow the general setting of previous GFN literature and consider the generation of discrete finite objects, but in principle our method could be extended to the continuous case [Lahlou et al., 2023]. ", "page_idx": 1}, {"type": "text", "text": "GFlowNets GFNs [Bengio et al., 2021b] sample objects by decomposing their generation process in a sequence $\\tau\\,=\\,(s_{0},..,s_{T})$ of constructive steps. The space can be described by a pointed directed acyclic graph (DAG) $\\mathcal{G}\\;=\\;(S,{\\mathcal A})$ , where $s~\\in~s$ is a partially constructed object, and $(s\\to s^{\\prime})\\in\\dot{A}\\subset\\bar{\\mathcal{S}}\\times\\mathcal{S}$ is a valid additive step (e.g., adding a fragment to a molecule). $\\mathcal{G}$ is rooted at a unique initial state $s_{0}$ . ", "page_idx": 1}, {"type": "text", "text": "GFNs are trained by pushing a model to satisfy so-called balance conditions of flow, whereby flows $F(s)$ going through states are conserved such that terminal states (corresponding to fully constructed objects) are sinks that absorb $R(s)$ (non-negative) units of flow, and intermediate states have as much flow coming into them (from parents) as flow coming out of them (to children). This can be described succinctly as follows, for any partial trajectory $(s_{n},..,s_{m})$ : ", "page_idx": 1}, {"type": "equation", "text": "$$\nF(s_{n})\\prod_{i=n}^{m-1}P_{F}(s_{i+1}|s_{i})=F(s_{m})\\prod_{i=n}^{m-1}P_{B}(s_{i}|s_{i+1})\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $P_{F}$ and $P_{B}$ , the forward and backward policies, are distributions over children and parents respectively, representing the fraction of flow emanating forward and backward from a state. By construction for terminal (leaf) states $F(s)=R(s)$ . ", "page_idx": 1}, {"type": "text", "text": "Balance conditions lead to learning objectives such as Trajectory Balance [TB; Malkin et al., 2022a], where $n=0$ and $m$ is the trajectory length, and Sub-trajectory Balance [SubTB; Madan et al., 2023], where all combinations of $(n,m)$ are used. While a variety of GFN objectives exist, we use these two as they are considered standard. If those objectives are fully satisfied, i.e. 0-loss everywhere, terminal states are guaranteed to be sampled with probability $\\propto R(s)$ [Bengio et al., 2021a]. ", "page_idx": 1}, {"type": "text", "text": "Action values For a broad overview of $\\mathrm{RL}$ , see Sutton and Barto [2018]. A central object in RL is the action-value function $Q^{\\pi}(s,a)$ , which estimates the expected \u201creward-to-go\u201d when following a policy $\\pi$ starting in some state $s$ and taking action $a$ ; for some discount $0\\leq\\gamma\\leq1$ , ", "page_idx": 1}, {"type": "equation", "text": "$$\nQ^{\\pi}(s,a)=\\mathbb{E}_{\\textit{\\xi}_{a t}\\sim\\pi(.|s_{t})}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R(s_{t})|s_{0}=s,a_{0}=a\\right]\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "While $T(s,a)$ can be a stochastic transition operator, in a GFN context objects are constructed in a deterministic way (although there are stochastic GFN extensions; Pan et al. [2023b]). Because rewards are only available for finished objects, $R(s)=0$ unless $s$ is a terminal state, and we use $\\gamma=1$ to avoid arbitrarily penalizing \u201clarger\u201d objects. Finally, as there are several possible choices for $\\pi$ , we will simply refer to $Q^{\\pi}$ as $Q$ when statements apply to a large number of such choices. ", "page_idx": 1}, {"type": "image", "img_path": "kQ9LgM2JQT/tmp/b0395b410db8435999742dc3dad6a7a648b9559b96db0ddaa5354578508b6a6e.jpg", "img_caption": ["Figure 1: Solely relying on flow functions $F$ in GFNs can be insufficient. While GFNs capture how much stuff there is, they spend time sampling from lots of small rewards. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2.1 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "RL and GFlowNets There are clear connections between the GFN framework and RL framework [Tiapkin et al., 2023, Mohammadpour et al., 2024, Deleu et al., 2024]. Notably, Tiapkin et al. [2023] show that it is possible to reformulate fixed- $\\cdot P_{B}$ GFlowNets as a Soft-RL problem within a specific class of reward-modified MDPs. While they show that this reformulated problem can then be tackled with any Soft-RL method, this still essentially solves the original GFlowNet problem, i.e. learn $p_{\\theta}(x)\\propto R(x)$ . Instead, we are interested in greedier-yet-diverse methods. ", "page_idx": 2}, {"type": "text", "text": "A commonly used tool in GFNs (and QGFN) to increase the average reward, aka \u201cgreediness\" of the drawn samples, is to adapt the reward distribution by using an altered reward function $\\hat{R}(x)=R(x)^{\\beta}$ and adjusting the exponent parameter $\\beta$ : the higher the $\\beta$ , the greedier the model should be [Jain et al., 2023]. However, increasing $\\beta$ often induces greater numerical instability (even on a log scale), and reduces diversity because the model is less incentivized to explore \u201cmiddle-reward\u201d regions. This can lead to mode collapse. Kim et al. [2023a] show that it is possible to train models that are conditioned on $\\beta$ , which somewhat alleviates these issues, but at the cost of training a more complex model. ", "page_idx": 2}, {"type": "text", "text": "Again, while we could leverage the equivalence between the GFN framework and the Soft-RL framework [Tiapkin et al., 2023], this approach would produce a soft policy. We propose a different approach that increases greediness of the policy via \u201cHard-RL\u201d. ", "page_idx": 2}, {"type": "text", "text": "Improving GFlowNet sampling A number of works have also made contributions towards improving utility in GFlowNets, via local search [Kim et al., 2023b], utilizing intermediate signals [Pan et al., 2023a], or favoring high-potential-utility intermediate states [Shen et al., 2023], as well as the use of RL tools such as replay buffers [Vemgal et al., 2023], target networks [Lau et al., 2023], or Thompson sampling [Rector-Brooks et al., 2023]. ", "page_idx": 2}, {"type": "text", "text": "3 Motivation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Consider the following scenario, illustrated in Figure 1: an agent is faced with two doors. Behind the left door, there are 100 other doors, each hiding a reward of 1. Behind the right door, there is a single door hiding a reward of 100. The flow will be such that $F(\\mathrm{left})=F(\\mathrm{right})=100$ , meaning that a GFN agent will pick either door with probability $^1/2$ . The action value function is $Q(s_{0},\\mathrm{left})=1$ , $Q(s_{0},\\mathrm{right})=100.$ , so an agent basing its decisions on $Q$ will reach for the door with reward 100. ", "page_idx": 2}, {"type": "text", "text": "This example shows that relying solely on flows is not always sufficient to provide high-value samples frequently and is illustrative of real-world scenarios. Consider molecular design (a very large search space) with some reward in $[0,1]$ ; there may be $10^{6}$ molecules with reward .9, but just a dozen with reward 1. Since . $9\\times10^{\\bar{6}}$ is much bigger than $12\\times1$ , the probability of sampling a reward 1 molecule will be low if one uses this reward naively. While using a temperature parameter is a useful way to increase the probability of the reward 1 molecules, we propose a complementary, inference-time-adjustable method. ", "page_idx": 2}, {"type": "text", "text": "Note that relying solely on $Q$ is also insufficient. If $Q$ were estimated very well for the optimal policy (which is extremely hard), it would be (somewhat) easy to find the reward 1 molecules via some kind of tree search following $Q$ values. However, in practice, RL algorithms easily collapse to non-diverse solutions, only discovering a few high reward outcomes. This is where flows are useful: because they capture how much stuff there is in a particular branch (rather than an expectation), it is useful to follow flows to find regions where there is potential for reward. In this paper, we propose a method that can be greedier (by following $Q$ ) while still being exploratory and diverse (by following $F$ through $P_{F}$ ). ", "page_idx": 3}, {"type": "text", "text": "4 QGFN: controllable greediness through $Q$ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Leveraging the intuition from the example above, we now propose and investigate several ways in which we can use $Q$ -functions to achieve our goal; we call this general idea QGFN. In particular, we present three variants of this idea, which are easy to implement and effective: $p$ -greedy QGFNs, $p$ -quantile QGFNs, and $p$ -of-max QGFNs. In $\\S5$ and $\\S6$ , we show that these approaches provide a favourable trade-off between reward and diversity, during both training and inference. ", "page_idx": 3}, {"type": "text", "text": "As is common in GFlowNets, we train QGFN by sampling data from some behavior policy $\\mu$ . We train $F$ and $P_{F}$ (and use a uniform $P_{B}$ ) to minimize a flow balance loss on the minibatch of sampled data, using a temperature parameter $\\beta$ . Additionally, we train a $Q$ -network to predict action values on the same minibatch (the choice of loss will be detailed later). Training the GFN and $Q$ on a variety of behaviors $\\mu$ is possible because both are off-policy methods. Indeed, instead of choosing $\\mu$ to be a noisy version of $P_{F}$ as is usual for GFNs, we combine the predictions of $P_{F}$ and $Q$ to form a greedier behavior policy. In all proposed variants, this combination is modulated by a factor $p\\in[0,1]$ , where $p=0$ means that $\\mu$ depends only on $P_{F}$ , and $p=1$ means $\\mu$ is greediest, as reflected by $Q$ . The variants differ in the details of this combination. ", "page_idx": 3}, {"type": "text", "text": "$p$ -greedy QGFN Here, we define $\\mu$ as a mixture between $P_{F}$ and the $Q$ -greedy policy, controlled by factor $p$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mu(s^{\\prime}|s)=(1-p)P_{F}(s^{\\prime}|s)+p\\mathbb{I}[s^{\\prime}=\\operatorname{argmax}_{i}\\!Q(s,i)]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In other words, we follow $P_{F}$ , but with probability $p$ , the greedy action according to $Q$ is picked. All states reachable by $P_{F}$ are still reachable by $\\mu$ . Note that $p$ can be changed to produce very different $\\mu$ without having to retrain anything. ", "page_idx": 3}, {"type": "text", "text": "$p$ -quantile QGFN Here, we define $\\mu$ as a masked version of $P_{F}$ , where actions below the $p$ -quantile of $Q$ , denoted $q_{p}(Q,s)$ , have probability 0 (so are discarded): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mu(s^{\\prime}|s)\\propto P_{F}(s^{\\prime}|s)\\mathbb{I}[Q(s,s^{\\prime})\\geq q_{p}(Q,s)]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This can be implemented by sorting $Q$ and masking the logits of $P_{F}$ accordingly. This method is more aggressive, since it prunes the search space, potentially making some states unreachable. Again, $p$ is changeable. ", "page_idx": 3}, {"type": "text", "text": "$p$ -of-max QGFN Here, we define $\\mu$ as a masked version of $P_{F}$ , where actions with $Q$ -values less than $p\\operatorname*{max}_{a}Q(s,a)$ have probability 0: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mu(s^{\\prime}|s)\\propto P_{F}(s^{\\prime}|s)\\mathbb{I}[Q(s,s^{\\prime})\\geq p\\operatorname*{max}_{i}Q(s,i)]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This is similar to $p$ -quantile pruning, but the number of pruned actions changes as a function of $Q$ . If all actions are estimated as good enough, it may be that no action is pruned, and vice versa, only the best action may be retained is none of the others are good. This method also prunes the search space, and $p$ remains changeable. Note that in a GFN context, rewards are strictly positive, so $Q$ is also positive. ", "page_idx": 3}, {"type": "text", "text": "Policy evaluation, or optimal control? In the design of the proposed method, we were faced with an interesting choice: what policy $\\pi$ should $Q^{\\pi}$ evaluate? The first obvious choice is to perform $Q$ - learning [Mnih et al., 2013], and estimate the optimal value function $Q^{*}$ , with a 1-step TD objective. As we detail in $\\S6$ , this proved to be fairly hard, and 1-step $Q_{\\theta}$ ended up being a poor approximation. ", "page_idx": 3}, {"type": "text", "text": "A commonly used trick to improve the performance of bootstrapping algorithms is to use $n$ -step returns [Hessel et al., 2018]. This proved essential to our work, and also revealed something curious: ", "page_idx": 3}, {"type": "image", "img_path": "kQ9LgM2JQT/tmp/3a4af59b7e17c6d72a6b69a6e81dfc681d16c40a4198d2553a5925466f9c2a34.jpg", "img_caption": ["Figure 2: Fragment-based molecule task. Left: Average rewards over the training trajectories. Center: Number of unique modes with a reward threshold exceeding 0.97 and pairwise Tanimoto similarity score less than 0.65. Right: Average pairwise Tanimoto similarity score for the top 1000 molecules sampled by reward. Lines are the interquartile mean and standard error calculated over 5 seeds. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "we\u2019ve consistently found that, while results started improving at $n\\geq5$ , a consistently good value of $n$ was close to the maximum trajectory length. This has an interesting interpretation, as beyond a certain value of $n$ , $Q_{\\theta}$ becomes closer to $Q^{\\mu}$ and further from $Q^{*}$ . In other words, using an \u201con-policy\u201d estimate $Q^{\\mu}$ rather than an estimate of the optimal policy seems beneficial, or at least easier to learn as a whole. In hindsight, this makes sense because on-policy evaluation is easier than learning $Q^{*}$ , and since we are combining the $Q$ -values with $P_{F}$ , any method which biases $\\mu$ correctly towards better actions is sufficient (we do not need to know exactly the best action, or its exact value). ", "page_idx": 4}, {"type": "text", "text": "Selecting greediness In the methods proposed above, $p$ can be changed arbitrarily. We first note that we train with a constant or annealed2 value of $p$ and treat it as a standard hyperparameter in all the results reported in $\\S5$ . ", "page_idx": 4}, {"type": "text", "text": "Second, as discussed in $\\S6$ , after training, $p$ can be changed with a predictable effect: the closer $p$ is to 1, the greedier $\\mu$ becomes. Presumably, this is because the model generalizes, and $Q.$ -value estimates for \u201coff-policy\u201d actions are still a reliable guess of the reward obtainable down some particular branch. When making $p$ higher, $Q$ may remain a good lower bound of the expected reward (after all, $\\mu$ is becoming greedier), which is still helpful. Generally, such a policy will have reasonable outcomes, regardless of the specific $\\mu$ and $p$ used during training. Finally, it may be possible and desirable to use more complex schedules for $p$ , or to sample $p$ during training from some (adaptive) distribution, but we leave this for future work. ", "page_idx": 4}, {"type": "text", "text": "5 Main results ", "text_level": 1, "page_idx": 4}, {"type": "image", "img_path": "kQ9LgM2JQT/tmp/cd386a4430c43042d147dc8fab5afb8efbe893ed839f61432a563575bfdf1d9e.jpg", "img_caption": ["Figure 3: QM9 task. Left: Average rewards over training trajectories. Right: Number of modes with a reward above 1.10 and pairwise Tanimoto similarity less than 0.70. ", "is separated from previously found modes by some "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "We experiment on 5 standard tasks used in prior GFlowNet literature. As baselines, we use Trajectory Balance, Sub-Trajectory Balance, LSL-GFN [Kim et al., 2023a] i.e. learning to scale logits which controls greediness through temperature-conditioning, and as RL baselines A2C [Mnih et al., 2016], DQN [Mnih et al., 2013] (which on its own systematically underperforms in these tasks), and Tiapkin et al. [2023]\u2019s MunDQN/GFlowNet. ", "page_idx": 4}, {"type": "text", "text": "We report the average reward obtained by the agents, as well as the total number of modes of the distribution of interest found during training. By mode, we mean a high-reward object that distance threshold. The distance function and ", "page_idx": 4}, {"type": "image", "img_path": "kQ9LgM2JQT/tmp/f411b770453e36e12a222aad8aa14a230b9f927d2c437c2897ebbc25d720e4a3.jpg", "img_caption": ["Figure 4: RNA-binding tasks, Average reward and modes. Left: L14RNA1 task. Right: L14RNA1 $+2$ task, based on 5 seeds (interquartile mean and standard error shown). "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "threshold we use, as well as the minimum reward threshold for an object to be considered a mode, depend on the task. ", "page_idx": 5}, {"type": "text", "text": "Fragment-based molecule generation task:3 Generate a graph of up to 9 fragments, where the reward is based on a prediction of the binding affinity to the sEH protein, using a model provided by Bengio et al. [2021a]. $\\vert\\mathcal{X}\\vert\\approx10^{100}$ , there are 72 available fragments, some with many possible attachment points. We use Tanimoto similarity [Bender and Glen, 2004], with a threshold of 0.65, and a reward threshold of 0.97. Results are shown in Fig. 2. ", "page_idx": 5}, {"type": "text", "text": "Atom Based QM9 task: Generate small molecules of up to 9 atoms following the QM9 dataset [Ramakrishnan et al., 2014]. $|\\mathcal{X}|\\approx10^{12}$ , the action space includes adding atoms or bonds, setting node or bond properties and stop. A MXMNet proxy model [Zhang et al., 2020], trained on QM9, predicts the HOMO-LUMO gap, a key indicator of molecular properties including stability and reactivity, and is used as the reward. Rewards are in the [0, 2] range, with a 1.10 threshold and a minimum Tanimoto similarity of 0.70 to define modes. Results are shown in Fig. 3. ", "page_idx": 5}, {"type": "text", "text": "RNA-binding task: Generate a string of $14\\,\\mathrm{nu}.$ - cleobases. The reward is a predicted binding affinity to the target transcription factor, provided by the ViennaRNA [Lorenz et al., 2011] package for the binding landscapes; we experiment with two RNA binding tasks; L14-RNA1, and L14-RNA1 $^{+2}$ (two binding targets) with optima computed from Sinai et al. [2020]. $|{\\mathcal{X}}|$ is $4^{14}\\approx10^{\\bar{9}}$ , there are 4 tokens: adenine (A), cytosine (C), guanine (G), uracil (U). Results are shown in Fig. 4. ", "page_idx": 5}, {"type": "image", "img_path": "kQ9LgM2JQT/tmp/4685993faae9b6dafc56ea90fba4461aac79c5fbc0e7ba5a669ef11354dc1f0f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Prepend-Append bit sequences: Generate   \na bit sequence of length 120 in a prepend- Figure 5: Bit sequence task, $k=1$ . Interquartile append MDP, where $|{\\mathcal{X}}|$ , limited to $\\{0,1\\}^{n}$ , mean and standard error over 5 seeds.   \nis $\\Dot{2}^{120}\\,\\approx\\,10^{36}$ . For a sequence of length $n$ , ", "page_idx": 5}, {"type": "text", "text": "$R(x)=\\exp{(1-\\operatorname*{min}_{y\\in M}d(x,y)/n)}$ . A sequence is considered a mode if it is within edit distance $\\delta$ from $M$ , where $M$ is defined as per Malkin et al. [2022a] (although the task we consider here is a more complex version, introduced by Shen et al. [2023], since prepend actions induce a DAG instead of a simpler tree). In our experiment, $|M|=60,n=120,k=1,\\delta=28$ , where $k$ is the bit width of actions. Results are shown in Fig. 5. ", "page_idx": 5}, {"type": "text", "text": "5.1 Analysis of results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Across tasks, QGFN variants produce high rewards and find a higher number of modes, i.e. highreward dissimilar objects. The latter could seem surprising, because a priori, increasing the greediness of a method likely reduces its diversity. This fundamental trade-off is known in RL as the explorationexploitation dilemma [Sutton, 1988, Sutton and Barto, 2018]. However, we are leveraging two methods and combining their strengths to reap the best from both worlds: GFlowNets are able to cover the state space, because they attempt to model all of it, by learning $p_{\\theta}(x)\\propto R(x)$ , while $Q$ approximates the expected reward of a particular action, which can help guide the agent by selecting high-expected-reward branches. Another way to think about this: GFNs are able to estimate how many high-reward objects there are in different parts of the state space. The agent thus ends up going in all important regions of the state space, but by being a bit more greedy through $Q$ , it focuses on higher reward objects, so it is more likely to find objects with reward past the mode threshold. To further understand the performance of QGFN, we formally analyse a bandit setting, and include derivations to illustrate the general case, in Appendix $\\S\\mathrm{A}$ . ", "page_idx": 5}, {"type": "table", "img_path": "kQ9LgM2JQT/tmp/60063b979693a89d2dc359423156c5b37f6bc876e75688fb4fec1fa9e3d90dc6.jpg", "table_caption": ["Table 1: Fragment-based molecule task: Reward and Diversity at inference after training. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "We also report the average reward and pairwise similarity for the fragment task based on 1000 samples over 5 seeds taken after training in Table 1. Again, QGFNs outperform GFNs in reward, while retaining low levels of inter-sample similarity. We again note that at inference, we are able to use a different (and better) $p$ value than the one used at training time. We expand on this in $\\S6.$ , and show that it is easy to tune $p$ to achieve different reward-diversity trade-offs at inference. The exact $p$ values used for Table 1 are provided in Appendix $\\S E$ . Also note that in LSL-GFN $\\beta$ is tunable at inference, and in Table 1 we choose the $\\beta$ value such that average similarity is near the 0.65 mode threshold we use (choosing a greedier $\\beta$ induces a collapse in diversity). ", "page_idx": 6}, {"type": "text", "text": "QGFN variants matter We point the reader to an interesting result, which is consistent with our understanding of the proposed method. In the fragment task, the number of actions available to the agent is quite large, ranging from about 100 to 1000 actions depending on the state, and the best performing QGFN variant is one that consistently masks most actions: $p_{\\|}$ -quantile QGFN. It is likely indeed that most actions are harmful, as combining two fragments that do not go together may be irreversibly bad, and masking helps the agent avoid undesirable regions of the state space. However, masking a fixed ratio of actions can provide more stable training. ", "page_idx": 6}, {"type": "text", "text": "On the other hand, in the RNA design task, there are only 5 actions (4 nucleotides ACGU & stop). We find that masking a constant number of actions is harmful\u2013it is likely that in some states all of them are relevant. So, in that task, $p$ -greedy and $p$ -of-max QGFN work best. This is also the case in the bit sequence task, for the same reasons (see Fig. 5). To confirm this, we repeat the bit sequence task but with an expanded action space consisting not just of $\\{0,1\\}$ , but of all 16 $(2^{4})$ sequences of 4 bits, i.e. $\\{0000,00\\bar{0}1,..,1111\\}$ . We find, as shown in Fig 18, that $p$ -quantile indeed no longer underperforms. ", "page_idx": 6}, {"type": "text", "text": "6 Method Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now analyze the key design choices in QGFN. We start by investigating the impact of $n$ (the number of bootstrapping steps in $Q$ -Learning) and $p$ (the mixture parameter) on training. We then look at trained models, and reuse the learned $Q$ and $P_{F}$ to show that it is possible to use a variety of sampling strategies, and to change the mixture factor $p$ to obtain a spectrum of greediness at test time. Finally, we empirically probe models to provide evidence as to why QGFN is helpful. ", "page_idx": 6}, {"type": "text", "text": "Impact of $\\beta$ in QGFN Fig. 6 shows the effect of training with different $\\beta$ values on the average reward and number of modes when taking 1000 samples after training is done in the fragment task (over 5 seeds). As predicted, increasing $\\beta$ increases the average reward of the agent, but at some point, causes it to become dramatically less diverse. As discussed earlier, this is typical of GFNs with a too high $\\beta$ , and is caused by a collapse around high-reward points and an inability for the model to further explore. While QGFN is also affected by this, it does not require as drastic values of $\\beta$ to obtain a high average reward and discover a high number of modes. ", "page_idx": 6}, {"type": "text", "text": "Impact of $n$ in QGFN As mentioned in $\\S4$ , training $Q$ with 1-step returns is ineffective and produces less useful approximations of the action value. Fig. 6 shows the number of modes within 1000 post-training samples in the fragment tasks, for models trained with a variety of $n$ -step values. ", "page_idx": 6}, {"type": "image", "img_path": "kQ9LgM2JQT/tmp/40ea1670678997b312a2d4d2ae3098941fe0c4228b35776e16b3aa008cd0c026.jpg", "img_caption": ["Figure 6: Fragment task. Left: Effect of $\\beta$ : Increasing greediness through $\\beta$ increases the average reward but may lead to diversity collapse. QGFN maintains diversity with a lower $\\beta$ , while GFN collapses. Modes are counted from 1000 samples at inference, using an inference-adjusted $p$ . Right: Effect of training parameters $p$ , and $n$ : Changing $p$ can control greediness, while increasing $n$ is generally beneficial. Modes are counted from 1000 samples generated using the training $p$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Models start being consistently good at $n=5$ and values close to the maximum length of a trajectory tend to work well too. ", "page_idx": 7}, {"type": "text", "text": "Impact of the training $p$ in QGFN While our method allows changing $p$ more or less at will, we still require some value during training. Fig. 6 shows that there are clear trade-offs between choices of $p$ , some yielding significantly better diversity than others. For example, $p$ -of-max is fairly sensitive to the chosen value during training, and for the fragment task doesn\u2019t seem to perform particularly well during training (especially when not annealed). On the other hand, as we will see in the next paragraph (and is also seen in Fig. 6), $p$ -of-max excels at inference, and is able to generate diverse and high-reward samples by adjusting $p$ . ", "page_idx": 7}, {"type": "text", "text": "Changing strategy after training We now look at the impact of changing the mixture parameter $p$ and the sampling strategy for already trained models on average reward and average pairwise similarity. We use the parameters of a model trained with $p$ -greedy QGFN, $p=0.4$ . ", "page_idx": 7}, {"type": "text", "text": "With this model, we sample 512 new trajectories for a series of different $p$ values. For $p$ -greedy and $p$ -quantile, we vary $p$ between 0 and 1; for $p$ -of-max, we vary $p$ between .9 and 1 (values below .9 have minor effects). We visualize the effect of $p$ on reward and similarity statistics in Fig. 9. ", "page_idx": 7}, {"type": "text", "text": "First, we note that increasing $p$ has the effect we would hope, increasing the average reward. Second, we note that this works without any retraining; even though we (a) use values of $p$ different than those used during training, and (b) use QGFN variants different than those used during training, the behavior is consistent: $p$ controls greediness. Let us emphasize (b): even though we trained this $Q$ with $p$ -greedy QGFN, we are able to use the $Q(s,a)$ predictions just fine with entirely different sampling strategies. This has some interesting implications; most importantly, it can be undesirable to train with too high values of $p$ (because it may reduce the diversity to which the model is exposed), but what is learned transfers well to sampling new, high-reward objects with different values of $p$ and sampling strategies. ", "page_idx": 7}, {"type": "text", "text": "Finally, these results suggest that we should be able to prototype new QGFN variants, including expensive ones (e.g. MCTS) without having to retrain anything. We illustrate the performance of a few other variants in $\\S B.2$ , Fig. 12. ", "page_idx": 7}, {"type": "text", "text": "Is $Q$ calibrated? For our intuition of why QGFN works to really pan out, $Q$ has to be accurate enough to provide useful guidance towards high-reward objects. We verify that this is the case with the following experiment. We take a trained QGFN model $\\textit{p}$ -greedy, $p=0.4$ , fragment task, maximum $n$ -step) and sample 64 trajectories. For each of those trajectories, we take a random state within the trajectory as a starting point, thereafter generating 512 new tra", "page_idx": 7}, {"type": "image", "img_path": "kQ9LgM2JQT/tmp/c3b831eb547ff95744cb5a4803ad15bb173daead38dfe2a671cacc34cd78aa36.jpg", "img_caption": ["Figure 7: Comparing $Q(s,a;\\theta)$ predictions with empirical estimates obtained by rollouts. Bars are standard error. $Q$ is relatively capable to estimate the returns of the corresponding policy. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "jectories. We then use the reward of those 512 trajectories as an empirical estimate $\\hat{Q}$ of the expected return, which $Q$ should roughly be predicting. Fig. 7 shows that this is indeed the case. Although $Q$ is not perfect, and appears to be underestimating $\\hat{Q}$ , it is making reasonable predictions. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Is $Q$ really helpful? In this experiment, we verify our intuition that pruning based on $Q$ - values is helpful. We again take a trained model for the fragment task, and sample 512 trajectories. We use $p$ -of-max QGFN $\\it{p}=0.95)$ , and compare it to a strategy estimating best pruned actions: for each trajectory, after some random number of steps $t\\sim U[4,20]$ (the max is 27), we start deterministically selecting the action that is the most probable according to $P_{F}$ but would be masked according to $Q$ . To ensure that this is a valid thing to do, we also simply look at Best actions, i.e. after $t\\sim U[4,20]$ steps, deterministically select the action that is the most probable according to $P_{F}$ , regardless of $Q$ . ", "page_idx": 8}, {"type": "text", "text": "Fig. 8 shows that our sanity check, Best actions, receives reasonable rewards, while selecting actions that would have been pruned leads to much lower rewards. The average likelihood from $P_{F}$ of these pruned actions was .035, while the average number of total actions was $\\approx382$ (and $1/382\\approx0.0026)$ . This confirms our hypothesis that $Q$ indeed masks actions that are likely according to $P_{F}$ but that do not consistently lead to high rewards. ", "page_idx": 8}, {"type": "text", "text": "Why does changing $p$ work? Recall that for QGFN to be successful, we rely on $n$ -step TD, and therefore on somewhat \u201con-policy\u201d estimates of $Q^{\\mu}$ . $\\mu$ is really $\\mu_{p}$ , a function of $p$ , meaning that if we change $p$ , say to $p^{\\prime}$ , during inference, $Q^{\\mu_{p}}$ is not an accurate estimate of $Q^{\\mu_{p^{\\prime}}}$ . If this is the case, then there must be a reason why it is still helpful to prune based on $Q^{\\mu_{p}}$ while using $\\mu_{p^{\\prime}}$ . In Fig. 10,we perform the ", "page_idx": 8}, {"type": "image", "img_path": "kQ9LgM2JQT/tmp/4e4d53b7417368100c77af7f07356544693516838baf60ce30c4b3d4719343a1.jpg", "img_caption": ["Figure 8: Pruning helps avoid low-reward parts of the state space. Reward distributions when (a) sampling with $p_{\\|}$ -of-max; (b) greedily according to $P_{F}$ selecting actions that $p_{\\|}$ -of-max would prune, Best pruned actions; (c) selecting most likely $P_{F}$ actions regardless of $Q$ , Best actions; and (d) normal sampling from $P_{F}$ (without using $Q$ ). "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "kQ9LgM2JQT/tmp/5b14dd10b90424ecfb6c05a78c579649d2733ffcae0e80cab3cac6627dbcd1ba.jpg", "img_caption": ["Figure 9: Varying $p$ at inference time induces reward-diversity trade-offs; fragment task. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "same measurement as in Fig. 7, but we change the $p$ value used to measure $\\hat{Q}^{\\mu_{p^{\\prime}}}$ . We find that, while the rank correlation drastically goes down (although it stays well above 0), $Q^{\\mu_{p}}$ remains helpful because it lower bounds $\\hat{Q}^{\\mu_{p^{\\prime}}}$ . If we prune based on $Q^{\\mu_{p}}$ , then we would want it to not lead us astray, and at least make us greedier as we increase $p$ . This means that if an action is not pruned, then we expect samples coming from it to be at least as good as what $Q^{\\mu_{p}}(s,a)$ predicts (in expectation). This is indeed the case. ", "page_idx": 8}, {"type": "text", "text": "Note that reducing $p$ simply leads $\\mu$ to behave more like $P_{F}$ , which is still a good sampler, and to rely less on $Q$ , whose imperfections will then have less effect anyways. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion and Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we showed that by jointly training GFNs and $Q$ -functions, we can combine their predictions to form behavior policies that are able to sample larger numbers of diverse and highreward objects. These policies\u2019 mixture parameter $p$ is adjustable, even after training, to modulate the greediness of the resulting policy. We implement multiple ways of combining GFNs and $Q\\mathrm{s}$ , referring to the general idea as QGFN: taking a greedy with probability $p$ $\\dot{p}$ -greedy QGFN), restricting the agent to the top $1-p\\%$ of actions ( $\\textit{p}$ -quantile QGFN), and restricting the agent to actions whose estimated value is at least a fraction $p$ of the best possible value ( $\\textit{p}$ -of-max QGFN). ", "page_idx": 8}, {"type": "text", "text": "We chose to show several variants of QGFN in this paper, because they all rely on the same principle, learning $Q$ , but have different properties, which lead to better or worse behavior in different tasks. ", "page_idx": 8}, {"type": "text", "text": "For example, pruning too aggressively on a task with a small number of actions is harmful. We also hope that by showing such a diversity of combinations of $P_{F}$ and $Q$ , we encourage future work that combines GFNs and RL methods in novel and creative ways. ", "page_idx": 9}, {"type": "text", "text": "We also analyzed why our method works. We showed that the learned action-value $Q$ is predictive and helpful in avoiding actions that have high probability under $P_{F}$ but lower expected reward. Even when $Q$ predictions are not accurate, e.g. because we sample from a different policy than the one which $Q$ models, they provide a helpful lower bound that facilitates controllable greediness. ", "page_idx": 9}, {"type": "text", "text": "Our analysis suggests that at training time, QGFN works because it helps the agent to \u201cwaste\u201d less time and capacity modeling low-reward objects, and that conversely the policy family that QGFN learns is able to sample more distinct high-reward objects given the same budget. In this sense, QGFN benefits from the advantages of both the GFlowNet and \u201cHard\u201c-RL frameworks. ", "page_idx": 9}, {"type": "text", "text": "What didn\u2019t work The initial stages of this project were quite different. Instead of combining RL and GFNs into one sampling policy, we instead trained two agents, a GFN and a DQN. Since both are off-policy methods we were hoping that sharing \u201cgreedy\u201d DQN data with a GFN would be fine and make GFN better on high-reward trajectories. This was not the case, instead, the DQN agent simply slowed down the whole method\u2013despite trying a wide variety of tricks, see $\\S C$ . ", "page_idx": 9}, {"type": "text", "text": "Limitations Because we train two models, our method requires more memory and FLOPs, and consequently takes more time to train compared to TB (as shown in Table 3). QGFN is also sensitive to how well $Q$ is learned, and as we\u2019ve shown $n$ -step returns are crucial for our method to work. In addition, although the problems we tackle are non-trivial, we do not explore the parameter and compute scaling behaviors of the benchmarked methods. ", "page_idx": 9}, {"type": "text", "text": "Future work We highlight two straightforward avenues of future work. First, there probably exist more interesting combinations of $Q$ and $P_{F}$ (and perhaps $F$ ), with different properties and beneftis. Second, it may be interesting to further leverage the idea of pruning the action space based on $Q$ , forming the basis for some sort of constrained combinatorial optimization. By using $Q$ to predict some expected property or constraint, rather than reward, we could prune some of the action space to avoid violating constraints, or to keep some other properties below some threshold (e.g. synthesizability or toxicity in molecules). ", "page_idx": 9}, {"type": "text", "text": "Finally, we hope that this work helps highlight the differences between RL and GFlowNet, while adding to the literature showing that these approaches complement each other well. It is likely that we are only scratching the surface of what is possible in combining these two frameworks. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The bulk of this research was done at Valence Labs as part of an internship, using computational resources there. This research was also enabled in part by computational resources provided by Calcul Qu\u00e9bec, Compute Canada and Mila. Academic authors are funded by their respective academic institution, Fonds Recherche Quebec through the FACSAcquity grant, the National Research Council of Canada and the DeepMind Fellowships Scholarship. ", "page_idx": 10}, {"type": "text", "text": "The authors are grateful to Yoshua Bengio, Moksh Jain, Minsu Kim, and the Valence Labs team for their feedback, discussions, and help with baselines. ", "page_idx": 10}, {"type": "text", "text": "Author Contributions ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The majority of the experimental work, code, plotting, and scientific contributions were by EL, with support from EB. SL helped run some experiments, baselines and plots. The project was supervised by EB, and DP and LP provided additional scientific guidance. Most of the paper was written by EB. DP, LP, and EL contributed to editing the paper. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Andreas Bender and Robert C Glen. Molecular similarity: a key technique in molecular informatics. Organic & biomolecular chemistry, 2(22):3204\u20133218, 2004.   \nEmmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow network based generative models for non-iterative diverse candidate generation. Advances in Neural Information Processing Systems, 34:27381\u201327394, 2021a. URL https://arxiv.org/abs/2106.04399.   \nYoshua Bengio, Tristan Deleu, Edward J Hu, Salem Lahlou, Mo Tiwari, and Emmanuel Bengio. Gflownet foundations. arXiv preprint arXiv:2111.09266, 2021b. URL https://arxiv.org/abs/2111.09266.   \nTristan Deleu, Padideh Nouri, Nikolay Malkin, Doina Precup, and Yoshua Bengio. Discrete probabilistic inference as control in multi-path environments. arXiv preprint arXiv:2402.10309, 2024.   \nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pages 1861\u20131870. PMLR, 2018.   \nMatteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.   \nMoksh Jain, Emmanuel Bengio, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Bonaventure FP Dossou, Chanakya Ajit Ekbote, Jie Fu, Tianyu Zhang, Michael Kilgour, Dinghuai Zhang, et al. Biological sequence design with gflownets. In International Conference on Machine Learning, pages 9786\u20139801. PMLR, 2022. URL https://arxiv.org/abs/2203.04115.   \nMoksh Jain, Sharath Chandra Raparthy, Alex Hern\u00e1ndez-Garcia, Jarrid Rector-Brooks, Yoshua Bengio, Santiago Miret, and Emmanuel Bengio. Multi-objective gflownets. In International Conference on Machine Learning, pages 14631\u201314653. PMLR, 2023. URL https://arxiv.org/abs/2210.12765.   \nMinsu Kim, Joohwan Ko, Dinghuai Zhang, Ling Pan, Taeyoung Yun, Woochang Kim, Jinkyoo Park, and Yoshua Bengio. Learning to scale logits for temperature-conditional gflownets. arXiv preprint arXiv:2310.02823, 2023a. URL https://arxiv.org/abs/2310.02823.   \nMinsu Kim, Taeyoung Yun, Emmanuel Bengio, Dinghuai Zhang, Yoshua Bengio, Sungsoo Ahn, and Jinkyoo Park. Local search gflownets. arXiv preprint arXiv:2310.02710, 2023b.   \nSalem Lahlou, Tristan Deleu, Pablo Lemos, Dinghuai Zhang, Alexandra Volokhova, Alex Hern\u00e1ndez-Garc\u0131a, L\u00e9na N\u00e9hale Ezzine, Yoshua Bengio, and Nikolay Malkin. A theory of continuous generative flow networks. In International Conference on Machine Learning, pages 18269\u201318300. PMLR, 2023.   \nGreg Landrum. Rdkit documentation. Release, 1(1-79):4, 2013. ", "page_idx": 10}, {"type": "text", "text": "Elaine Lau, Nikhil Vemgal, Doina Precup, and Emmanuel Bengio. Dgfn: Double generative flow networks. arXiv preprint arXiv:2310.19685, 2023. ", "page_idx": 10}, {"type": "text", "text": "Ronny Lorenz, Stephan H Bernhart, Christian H\u00f6ner zu Siederdissen, Hakim Tafer, Christoph Flamm, Peter F Stadler, and Ivo L Hofacker. Viennarna package 2.0. Algorithms for molecular biology, 6:1\u201314, 2011.   \nKanika Madan, Jarrid Rector-Brooks, Maksym Korablyov, Emmanuel Bengio, Moksh Jain, Andrei Cristian Nica, Tom Bosc, Yoshua Bengio, and Nikolay Malkin. Learning gflownets from partial episodes for improved convergence and stability. In International Conference on Machine Learning, pages 23467\u201323483. PMLR, 2023.   \nNikolay Malkin, Moksh Jain, Emmanuel Bengio, Chen Sun, and Yoshua Bengio. Trajectory balance: Improved credit assignment in gflownets. Advances in Neural Information Processing Systems, 35:5955\u20135967, 2022a.   \nNikolay Malkin, Salem Lahlou, Tristan Deleu, Xu Ji, Edward Hu, Katie Everett, Dinghuai Zhang, and Yoshua Bengio. Gflownets and variational inference. arXiv preprint arXiv:2210.00580, 2022b. URL https: //arxiv.org/abs/2210.00580v1.   \nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.   \nVolodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928\u20131937. PMLR, 2016.   \nSobhan Mohammadpour, Emmanuel Bengio, Emma Frejinger, and Pierre-Luc Bacon. Maximum entropy gflownets with soft q-learning. In International Conference on Artificial Intelligence and Statistics, pages 2593\u20132601. PMLR, 2024.   \nLing Pan, Nikolay Malkin, Dinghuai Zhang, and Yoshua Bengio. Better training of gflownets with local credit and incomplete trajectories. In International Conference on Machine Learning, pages 26878\u201326890. PMLR, 2023a.   \nLing Pan, Dinghuai Zhang, Moksh Jain, Longbo Huang, and Yoshua Bengio. Stochastic generative flow networks. arXiv preprint arXiv:2302.09465, 2023b.   \nRaghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. Quantum chemistry structures and properties of 134 kilo molecules. Scientific data, 1(1):1\u20137, 2014.   \nJarrid Rector-Brooks, Kanika Madan, Moksh Jain, Maksym Korablyov, Cheng-Hao Liu, Sarath Chandar, Nikolay Malkin, and Yoshua Bengio. Thompson sampling for improved exploration in gflownets. arXiv preprint arXiv:2306.17693, 2023.   \nMax W. Shen, Emmanuel Bengio, Ehsan Hajiramezanali, Andreas Loukas, Kyunghyun Cho, and Tommaso Biancalani. Towards understanding and improving gflownet training, 2023.   \nSam Sinai, Richard Wang, Alexander Whatley, Stewart Slocum, Elina Locane, and Eric D Kelsic. Adalead: A simple and robust adaptive greedy search algorithm for sequence design. arXiv preprint arXiv:2010.02141, 2020.   \nRichard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3:9\u201344, 1988.   \nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.   \nDaniil Tiapkin, Nikita Morozov, Alexey Naumov, and Dmitry Vetrov. Generative flow networks as entropyregularized rl. arXiv preprint arXiv:2310.12934, 2023.   \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \nPetar Velic\u02c7kovic\u00b4, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.   \nNikhil Vemgal, Elaine Lau, and Doina Precup. An empirical study of the effectiveness of using a replay buffer on mode discovery in gflownets. arXiv preprint arXiv:2307.07674, 2023.   \nChristopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8:279\u2013292, 1992.   \nDinghuai Zhang, Ricky TQ Chen, Nikolay Malkin, and Yoshua Bengio. Unifying generative models with gflownets. arXiv preprint arXiv:2209.02606, 2022. URL https://arxiv.org/abs/2209.02606. ", "page_idx": 11}, {"type": "text", "text": "Dinghuai Zhang, Hanjun Dai, Nikolay Malkin, Aaron C Courville, Yoshua Bengio, and Ling Pan. Let the flows tell: Solving graph combinatorial problems with gflownets. Advances in neural information processing systems, 36:11952\u201311969, 2023. ", "page_idx": 12}, {"type": "text", "text": "Shuo Zhang, Yang Liu, and Lei Xie. Molecular mechanics-driven graph neural network with multiplex graph for molecular structures. arXiv preprint arXiv:2011.07457, 2020. ", "page_idx": 12}, {"type": "text", "text": "A Analysing $p$ -greedy ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Consider the bandit setting where trajectories are 1 step and just consist in choosing a terminal state. Let $p_{G}(s)\\,=\\,R(s)\\bar{/}Z$ . Let $0\\,<\\,p\\,<\\,1$ , then with $\\mu(\\bar{s^{\\prime}}|s)\\,=\\,(1-p)P_{F}(s^{\\prime}|\\bar{s^{\\prime}})+p\\mathbb{I}[s^{\\prime}\\,=$ arg max $Q(s,s^{\\prime})]$ , assuming there is only a single argmax $s^{*}$ , then $p_{\\mu}(s)=(1-p)R(s)/Z+p\\mathbb{I}[s=$ arg max $R(s)]$ . This means that for every non-argmax state, $p_{\\mu}(s)=(1-p)p_{G}(s)<p_{G}(s)$ . We get that $\\mathbb{E}_{\\mu}[\\dot{R}]>\\mathbb{E}_{G}[R]$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\sum_{p_{i}}[R_{i}]-\\sum_{G=1}[R_{i}+\\gamma}&{\\sum_{p_{i}=0}^{K}R_{i}(s)-\\sum_{p_{i}=0}^{K}\\beta_{i}(s)}\\\\ &{\\qquad=(p_{i}+(1-p)R_{i}(s)^{\\prime})Z-R_{i}(s^{\\prime})/2)R(s^{\\prime})+\\displaystyle\\sum_{s=0}^{p_{i}}(1-p)R_{i}(s)^{2}Z-R_{i}(s)^{2}}\\\\ &{\\qquad=p R(s^{\\prime})-p R(s^{\\prime})^{2}Z+\\displaystyle\\sum_{s=0}^{p_{i}}(-p)R(s^{\\prime})Z^{2}}\\\\ &{\\qquad=p/2\\left(R(s^{\\prime})Z-R(s^{\\prime})^{2}-\\displaystyle\\sum_{s=0}^{p_{i}}R(s^{\\prime})\\right),\\quad Z=\\displaystyle\\sum_{s=0}^{p_{i}}R(s)}\\\\ &{\\qquad=p/2\\left(R(s^{\\prime})\\displaystyle\\sum_{s=0}^{p_{i}}R(s)\\right)-R(s^{\\prime})^{2}-\\displaystyle\\sum_{s=t}^{p_{i}}R(s)^{2}}\\\\ &{\\qquad=p/2\\left(R(s^{\\prime})\\displaystyle\\sum_{s=0}^{p_{i}}R(s)\\right)-\\displaystyle\\sum_{s=0}^{p_{i}}R(s^{\\prime})\\right)}\\\\ &{\\qquad=p/2\\left(\\sum_{s=0}^{p_{i}}R(s)-R(s^{\\prime})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "since $R(s^{*})>R(s)$ and both are positive then $R(s^{*})R(s)>R(s)^{2}$ thus the last sum is positive. All other terms are positive, therefore $\\mathbb{E}_{\\mu}[R]-\\mathbb{E}_{G}[R]>0$ . ", "page_idx": 13}, {"type": "text", "text": "In the more general case, we are not aware of a satisfying closed form, but consider the following exercise. ", "page_idx": 13}, {"type": "text", "text": "Let $m(s,s^{\\prime})=\\mathbb{I}[s^{\\prime}=\\arg\\operatorname*{max}Q(s,s^{\\prime})]$ . Let $F^{\\prime}$ be the \"QGFN flow\" which we\u2019ll decompose as $F^{\\prime}=F_{G}+F_{Q}$ where we think of $F_{G}$ and $F_{Q}$ as the GFN and Q-greedy contributions to the flows. Then: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{F^{\\prime}(s)=\\displaystyle\\sum_{z\\in\\mathrm{Par}(s)}F^{\\prime}(z)((1-p)P_{F}(s|z)+p m(z,s))}}\\\\ {{\\displaystyle~~~~~~~=\\sum_{z}F^{\\prime}(z)(1-p)P_{F}(s|z)+\\sum_{z}F^{\\prime}(z)p m(z,s)}}\\\\ {{\\displaystyle~~~~~~=\\sum_{z}F_{G}(z)(1-p)P_{F}(s|z)+F_{Q}(z)(1-p)P_{F}(s|z)+\\sum_{z}F^{\\prime}(z)p m(z,s)}}\\\\ {{\\displaystyle~~~~~~=(1-p)F_{G}(s)+\\sum_{z}F_{Q}(z)\\mu(z|s)+F_{G}(z)p m(z,s)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Recall that $p(s)\\propto F(s)$ . Intuitively then, the probability of being in a state is reduced by a factor $(1-p)$ , but possibly increased by this extra flow that has two origins. First, flow $F_{Q}$ carried over by $\\mu$ , and second, new flow being \"stolen\" from $F_{G}$ from parents when $m(z,s)=1$ , i.e. when $s$ is the argmax child. ", "page_idx": 13}, {"type": "text", "text": "This suggests that flow (probability mass) in a $p$ -greedy QGFN is smoothly redirected towards states with locally highest reward from ancestors of such states. Conversely, states which have many ancestors for which they are not the highest rewarding descendant will have their probability diminished. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "B Additional experiments and analyses ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we provide additional experiments to support our main findings. We explore the use of $Q$ functions from different behavior policies, assess various QGFN inference variants, examine QGFN variants trained with alternative objectives, and investigate the effects of weight sharing in QGFN models. ", "page_idx": 14}, {"type": "image", "img_path": "kQ9LgM2JQT/tmp/f6df8e51806471f3b42f50b7dd7bd8a9fd1218c8aaeaa632f87401383b6d7ecb.jpg", "img_caption": ["Figure 10: What happens to $Q(s,a)$ when changing $p$ ? We show here that while the rank correlation between $Q$ and the empirically estimated expected reward $\\hat{Q}^{\\mu_{p}}$ goes down when changing $p$ , $Q$ remains a useful estimate in that it mostly lower bounds ${\\hat{Q}}^{\\mu}$ . This means that, at worst, pruning based on the \u201cwrong\u201d $Q$ & $p$ combination drops some high-reward objects, but does not introduce more lower-reward objects. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "kQ9LgM2JQT/tmp/39afc5358aa4dc0ff9513f8b29a5521d1c49ecb62aa1d55573407321db22bba8.jpg", "img_caption": ["Figure 11: Comparison of trained $P_{F}$ vs. untrained $P_{F}$ with a trained $Q$ during inference on fragmentbased molecule task "], "img_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "kQ9LgM2JQT/tmp/ef45d56a6104590416fb1becc0968e1fea9d683bacdf8e4c3d324ad883211346.jpg", "table_caption": ["Table 2: Fragment-based molecule task: reward and diversity of independently trained baseline models using a trained $Q$ . The $p$ values for $p$ -greedy, $p$ -of-max, and $p$ -quantile QGFN are set at 0.4, 0.9858, and 0.93, respectively. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "B.1 Using $Q$ from different behavior policies ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Another approach we explored involves using a trained $Q$ function during inference that was trained on an entirely different behavior policy. Similarly, we apply the QGFN algorithm at each state of sampling trajectories during inference, but with a key difference: it is directly applied to a baseline model that has been trained independently. This approach aims to examine if a previously trained $Q$ , when used in a different training setup but the same task, can guide independently trained models that may not perform as well during training but, with this assistance, can achieve significantly better results at inference. For instance, as shown in Figure 2, the samples generated by SubTB average around 0.7 rewards throughout training. However, using the trained $Q$ as a greediness signal during inference allows us to discover samples with significantly higher rewards. Table 2 details the effects of applying $Q$ during inference on independently trained baseline models for 1000 samples post-training of 5 seeds. ", "page_idx": 15}, {"type": "text", "text": "B.2 Trying other QGFN variants at inference ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Since the only cost to trying to different \u201cinference\u201d variants of QGFN is to code them, we do so out of curiosity. We show the reward/diversity trade-off curves of these variants in Fig. 12, and include $p$ -of-max as a baseline variant. As in Fig. 9 we take 512 samples for each point in the curves (except for MCTS which is more expensive). We try the following: ", "page_idx": 15}, {"type": "text", "text": "\u2022 $p$ -thresh, mask all actions where $Q(s,a)<p$ ;   \n\u2022 soft-Q, not really QGFN, but as a baseline simply taking softmax $(Q/T)$ for some temperature $T$ , which is varied as the greediness parameter;   \n\u2022 soft-Q [0.5], as above but mixed with $P_{F}$ with a factor $p=0.5$ (i.e. $p$ -greedy, but instead of being greedy, use the soft- $Q$ policy);   \n\u2022 GFN-then-Q, for the first $N p$ steps, sample from $P_{F}$ , then sample greedily (where $N$ is the maximum trajectory length);   \n\u2022 MCTS, a Monte Carlo Tree Search where $P_{F}$ is used as the expansion prior and $\\operatorname*{max}_{a}Q(s,a)$ as the value of a state. Since this is a different sampling method, we run MCTS for a comparable amount of time to other variants, getting about 350 samples, and report the average reward and diversity. ", "page_idx": 15}, {"type": "text", "text": "We note that these are all tried using parameters from a pretrained $p$ -greedy QGFN model. It may be possible for these variants to be much better at inference if the $Q$ used corresponded to the sampling policy. ", "page_idx": 15}, {"type": "text", "text": "B.3 QGFN variants with different objective ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To demonstrate the robustness of QGFN variants, we explore QGFN with different learning objectives such as SubTB in addition to the TB objective used throughout our experiments. We use the same hyperparameters, except the $p$ values (p-greedy 0.4, p-of-max 0.7, p-quantile 0.7), listed in Table 4 and run the experiments on fragment-based molecule generation. The results are shown in Figure 13 and Figure 14. ", "page_idx": 15}, {"type": "image", "img_path": "kQ9LgM2JQT/tmp/05656dc99269518d1994076d25cae7441fce7d1318cf7e7329f0cb4edf64ac2f.jpg", "img_caption": ["Figure 12: Trying other possible QGFN variants. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "kQ9LgM2JQT/tmp/790c575a376edc3de13c987fc3e0bc1f67c6ecbb2a568314a71fc5546f511491.jpg", "img_caption": ["Figure 13: QGFN variants on learning objectives SubTB on Fragment-based molecule task "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.4 Exploring weight sharing in QGFN ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We explore the impact of weight sharing between $P_{F}$ and $Q$ , as they learn from the same environments and training samples. This sharing learning approach could improve in efficiency and performance. Fig. 15 shows the impact of weight sharing on $p$ -greedy QGFN, specifically focusing on sharing parameters across various layers of the graph attention transformer in the fragment-based task. ", "page_idx": 16}, {"type": "text", "text": "Unfortunately, naively summing the GFlowNet loss and the $Q$ -Learning loss does not yield any improvements, and instead slows down learning. This may be due to several factors; most likely, interference between $Q$ and $P_{F}$ , and a very different scales of the loss function may induce undesirable during training. A natural next stop would be to adjust the relative magnitude of the gradients coming from each loss, and to consider different hyperparameters (perhaps a higher model capacity is necessary), but we leave this to future work. Further exploration in this area could provide additional insights and potentially reduce training complexity. ", "page_idx": 16}, {"type": "text", "text": "B.5 Training Time and Inference Time Comparison ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In addition to performance, we investigate the training time and inference time for TB and $p$ -greedy QGFN. All experiments for this comparison were conducted on NVIDIA V100 GPUs. The results are reported in Table 3. ", "page_idx": 16}, {"type": "table", "img_path": "kQ9LgM2JQT/tmp/2eca4ceecfd036c118bb5cd49ef6c44653de0ec123d28f03fee877e26f94a1e8.jpg", "table_caption": ["Table 3: Training and inference time comparison between TB and $p$ -greedy QGFN. "], "table_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "kQ9LgM2JQT/tmp/9a309c5b19f33f7d668e552a59fcca7870eeac01757803b032f497f9800bb3fc.jpg", "img_caption": ["Figure 14: QGFN variants on learning objectives FM (Flow Matching) on Fragment-based molecule task "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "kQ9LgM2JQT/tmp/027fb82f09846bad08f60ed331f115dfd1bb6d8f67d9ff7afcdc3563a89abaf0.jpg", "img_caption": ["Figure 15: Weight sharing in $p$ -greedy QGFN ${\\scriptstyle(\\mathrm{p}=0.4)}$ with different layers "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "C Experiments that did not work ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Several approaches were attempted prior to developing QGFN. These approaches involved sampling from both GFN and DQN independently and learning from the shared data. The key strategies explored include: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Diverse-Based Replay Buffer: This method stores batches of trajectories in the replay buffer and samples them based on a pairwise Tanimoto similarity threshold. It aims to diversify the experience replay during training.   \n\u2022 Adaptive Reward Prioritized Replay Buffer: This strategy stores batches of trajectories in the replay buffer based on the rewards of the samples. In addition, we dynamically adjusts the sampling proportion between GFN and DQN based on the reward performance of the trajectories.   \n\u2022 Weight Sharing: This involves sharing weights between GFN and DQN to potentially enhance the learning and convergence of the models.   \n\u2022 Pretrained-Q for Greedier Actions: This method uses a pretrained DQN model for sampling trajectories, helping the GFN to be biased towards greedier actions in the early learning stages.   \n\u2022 $n$ -step returns: As per QGFN, using more than 1-step temporal differences can accelerate temporal credit assignment. This on its own is not enough to solve the tasks used in this work. ", "page_idx": 17}, {"type": "text", "text": "Fig 16 shows the performance of these approaches evaluated on fragment based molecule generation task. ", "page_idx": 17}, {"type": "image", "img_path": "kQ9LgM2JQT/tmp/dc276d44d1e8a32b47041f0814f0c95213076d3b5311b5ac8ca241cf499fcdea.jpg", "img_caption": ["Figure 16: Fragment-based molecule generation task; we showcase the performance of QGFN\u2019s predecessor, which failed to beat baselines regardless of our attempts to improve it. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "kQ9LgM2JQT/tmp/9f31614cabd0cd2aa0423b35dcee3295d1b362cfbd12746fa31e5db5ee121f84.jpg", "img_caption": ["Figure 17: Pairwise Tanimoto Similarity scores assessing the impacts of $\\beta$ , training parameters $p$ and $n$ in the fragment task. Left: An increase in $\\beta$ initially decreases sample similarities, followed by a gradual increase in similarity as the models get greedier through $\\beta$ . Center: Increase greediness does not always correlate with sample similarity trade-offs with QGFN, but at peak greediness, similarity scores rebound. Right: Increasing $n$ increases similarity among models. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "D Full Algorithm ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we show the detailed implementation of the QGFN algorithm with different variants in Algorithm 1. For inference, the trained models $P_{F}$ and $Q$ can be loaded to sample trajectories. ", "page_idx": 18}, {"type": "text", "text": "Algorithm 1 QGFN: Full training algorithm details ", "page_idx": 19}, {"type": "text", "text": "Require: Reward function $R:\\mathcal X\\to\\mathbb{R}_{>0}$ , batch size $M$ , Initialize models $P_{F}$ with parameters $\\theta$ , $Q(s,a)$ with parameters $\\theta^{\\prime}$ , greediness parameter $p\\in[0,1]$ , training iterations $I$ 1: For $p$ -greedy QGFN: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mu(s^{\\prime}|s)=(1-p)P_{F}(s^{\\prime}|s)+p\\mathbb{I}[s^{\\prime}=\\arg\\operatorname*{max}_{i}Q(s,i)]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "2: For $p$ -quantile QGFN: ", "text_level": 1, "page_idx": 19}, {"type": "equation", "text": "$$\n\\mu(s^{\\prime}|s)\\propto P_{F}(s^{\\prime}|s)\\mathbb{I}[Q(s,s^{\\prime})\\geq q_{p}(Q,s)]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $q_{p}(Q,s)$ is the $p$ -quantile of $Q$ over actions at state $s$ . ", "page_idx": 19}, {"type": "text", "text": "3: For $p$ -of-max QGFN: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mu(s^{\\prime}|s)\\propto P_{F}(s^{\\prime}|s)\\mathbb{I}[Q(s,s^{\\prime})\\geq p\\operatorname*{max}_{i}Q(s,i)]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "4: for for training iteration $i$ in $I$ do ", "page_idx": 19}, {"type": "text", "text": "5: for each new trajectory $\\tau_{j}$ from $\\tau_{1}$ to $\\tau_{M}$ do   \n6: Start $\\tau_{j}$ at $s_{0}$   \n7: while $s_{t}$ is not terminal do   \n8: Sample $s_{t+1}$ from $\\mu\\big(s_{t+1}\\big|s_{t}\\big)$ based on current policy   \n9: Update $t\\gets t+1$   \n10: Compute trajectory balance loss for $\\begin{array}{r}{P_{F}\\colon\\sum_{j}\\mathcal{L}_{\\mathrm{TB}}(\\tau_{j})}\\end{array}$   \n11: Compute MSE $\\mathbf{n}$ -step loss for Q-network: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{{Q}}}=\\mathbb{E}_{(s_{t},a_{t})}\\left[\\left(Q(s_{t},a_{t})-G_{t}^{(n)}\\right)^{2}\\right]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the $\\mathbf{n}$ -step return $G_{t}^{(n)}$ is defined as: ", "page_idx": 19}, {"type": "equation", "text": "$$\nG_{t}^{(n)}=\\sum_{k=0}^{n-1}\\gamma^{k}r_{t+k}+\\gamma^{n}\\operatorname*{max}_{a^{\\prime}}Q(s_{t+n},a^{\\prime})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "12: Update $\\theta$ using $\\nabla_{\\theta}\\mathcal{L}_{\\mathrm{TB}}$ ;   \n13: Update $\\theta^{\\prime}$ using $\\nabla_{\\theta^{\\prime}}\\mathcal{L}_{\\mathrm{Q}}$ ; ", "page_idx": 19}, {"type": "text", "text": "E Experiment details: Fragment-based molecule generation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we give the hyperparameters used for each of our experiments in Tables 4, and Table 5. ", "page_idx": 19}, {"type": "table", "img_path": "kQ9LgM2JQT/tmp/8cf3bd45f55f8e05778e4e82c5e8bdd46046273160641ddb9aaf8d1846620561.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 4: Hyperparameters and specifications of the Graph Attention Transformer used across all models in Fragment-based molecule generation. ", "page_idx": 19}, {"type": "table", "img_path": "kQ9LgM2JQT/tmp/cb3b189702863bac6de26208f722c87840552751649cd173f56e4a6410cdde04.jpg", "table_caption": ["Table 5: Model-specific parameters for QGFN in Fragment-based molecule generation. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "In Table 1, the $p$ values for $p$ -greedy, $p$ -of-max, and $p$ -quantile QGFN are set to 0.4, 0.9858, and 0.93, respectively. These values are selected based on Figure 9. The $p$ for $p$ -of-max is chosen from np.linspace(0.9, 0.999, 16), with 0.9858 being one of these values. Similarly, for $p$ -quantile, 0.93 corresponds to the second-to-last value from np.linspace(0, 1, 16). Meanwhile, the 0.4 for $p$ -greedy is selected from np.linspace(0, 1, 11). For LSL-GFN, the chosen $\\beta$ is 78, selected from np.linspace(64, 128, 65). ", "page_idx": 20}, {"type": "text", "text": "In our experimental setup, we follow the exact environment specifications and implementations detailed in Malkin et al. [2022a] with the proxy model, used for evaluating molecules, provided by Bengio et al. [2021b]. The architecture of the GFlowNet models is based on a graph attention transformer [Veli\u02c7ckovi\u00b4c et al., 2017]. We set a reward threshold of 0.97 to define modes, with a pairwise Tanimoto similarity criterion of less than 0.65. RDKit [Landrum, 2013] is used to compare pairwise Tanimoto similarity. ", "page_idx": 20}, {"type": "text", "text": "To follow closely the original implementation of LSL-GFN described in Kim et al. [2023a], we use $\\beta\\sim U^{[1,64]}$ , where $U$ denotes a uniform distribution. Additionally, we define a simple Multi-layer Perceptron with a hidden size of 256 as the learnable scalar-to-scalar function for the LSL-GFN. For A2C, we use a learning rate of $1\\times10^{-4}$ , a training epsilon of $1\\times10^{-2}$ , and an entropy regularization coefficient of $1\\times10^{-3}$ . For our SubTB baseline we use SubTB(1), i.e. all trajectories are averaged with equal weight. ", "page_idx": 20}, {"type": "text", "text": "To maintain consistency, the graph attention transformer was used as the model for MunDQN. We sampled 64 trajectories and stored them as transitions in a prioritized replay buffer of size 1,000,000. We then sampled 4096 transitions from the replay buffer to calculate the loss. The Munchausen parameters of 0.10 is selected from $\\{0.10,0.15\\}$ , an $l_{0}$ of -2500 and a soft update of $\\tau=0.1$ is used in our experiments. All other parameters are same as the original MunDQN paper in Tiapkin et al. [2023]. ", "page_idx": 20}, {"type": "text", "text": "We also ran an SAC [Haarnoja et al., 2018] baseline with different $\\alpha$ values of 0.5, 0.7, 0.2, along with autotuning, and a $\\gamma$ value of 0.99, but we were unable to get it to discover more than 50 modes for the same amount of training iterations and mini-batch sizes. ", "page_idx": 20}, {"type": "text", "text": "E.1 QGFN hyperparameters: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For all variants of QGFN, we employed a grid-search approach for hyperparameter tuning, with a focus on the parameters $p$ and $n$ . Similarly, graph attention transformer is initialized as the $Q$ . In the $p$ -greedy QGFN variant, we selected a value of 0.4 for $p$ from the set $\\{0.2,0.4,0.6,0.8\\}$ , and chose an $n$ of 25 from the set $\\{1,2,4,8,16,24,25,26\\}$ . For the $p$ -of-max QGFN variant, a $p$ of 0.91 was chosen from $\\{0.2,0.4,0.6,0.8,0.9,0.91,0.93,0.95\\}$ , with $n$ again set at 25. To ensure stability throughout the training process, we applied cosine annealing with a single-period cosine schedule over 1500 steps. An additional threshold parameter of $1\\times10^{-5}$ was applied to the condition $p\\operatorname*{max}_{i}Q(s,i)>$ threshold, to prevent the initial training phase from masking actions with very small values. We also introduced a clipping of the Q-value to a minimum of 0 to prevent instability during initial training. For the $p$ -quantile QGFN, a $p$ of 0.8 was selected from $\\left\\lbrace0.6,0.7,0.8,0.9,0.95\\right\\rbrace$ , with $n=25$ . For all variants of QGFN, the DQN employed had a $\\tau$ of 0.95, and random action probability of $\\epsilon$ was set to 0.1. ", "page_idx": 20}, {"type": "text", "text": "F Experiment details: QM9 ", "text_level": 1, "page_idx": 21}, {"type": "table", "img_path": "kQ9LgM2JQT/tmp/fc3f700cf3e337c271064ca6c618480bfad1f24047e686294f652aea35ae188d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 6: Hyperparameters and specifications of the Graph Attention Transformer used across all models in QM9. ", "page_idx": 21}, {"type": "table", "img_path": "kQ9LgM2JQT/tmp/49bfd9cedf9acb9da428f061b5f85740b01a18df3fd0bb9b03665b67739bb9a3.jpg", "table_caption": ["Table 7: Model-specific parameters for QGFN in QM9. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "In this experiment, we follow the setup described by Jain et al. [2023], but only use the HOMO-LUMO gap as a reward signal. The rewards are normalized to fall between [0, 1], although the gap proxy may range from [1,2]. As mentioned in Section 5, the modes are computed with a reward threshold of 1.10 and a pairwise Tanimoto similarity threshold of 0.70. We employ the same architecture for all models as used in the fragment-based experiments. The training models are 5,000 iterations with a minibatch size of 64, and $\\beta$ is set to 32. RDKit [Landrum, 2013] is used to compare pairwise Tanimoto similarity. We train A2C with random action probability 0.01 chosen from $\\{0.1,0.01,0.001\\}$ and entropy regularization coefficient 0.001 chosen from $\\{0,0.1,0.01,0.001\\}$ .Similar to the fragmentbased molecule task, we initialized the graph attention transformer with the Munchausen parameter $\\alpha$ set to 0.15, a prioritized replay buffer size of 1,000,000, and a soft update coefficient $\\tau=0.1$ . We sample 64 trajectories and store them in the replay buffer, subsequently sampling 4096 transitions from this buffer. We used the other hyperparameters mentioned in the original MunDQN paper [Tiapkin et al., 2023]. ", "page_idx": 21}, {"type": "text", "text": "F.1 QGFN hyperparameters: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For all variants of QGFN, we employed an exhaustive grid-search approach for hyperparameter tuning, focusing on parameters $p$ and $n$ . For $p$ -greedy QGFN, we selected 0.4 for $p$ from $\\{0.2,0.4,0.6\\}$ and 29 for $n$ from $\\{11,29,30\\}$ . For $p$ -of-max QGFN, we chose 0.6 for $p$ from $\\{0.3,0.4,0.5,0.6,0.7,0.8,0.9\\}$ and 29 for $n$ from $\\{28,29,30\\}$ . For $p$ -qunatile QGFN, we selected 0.6 for $p$ from $\\{0.5,0.6,0.7,0.8\\}$ and 29 for $n$ from $\\{11,27,28,29\\}$ . Similarly to the fragment task, we implemented an additional threshold of $1\\times10^{\\bar{-3}}$ to $p\\operatorname*{max}_{i}\\bar{Q}(s,i)>$ threshold and clipped $\\mathrm{{Q}}.$ -values to a minimum of 0 for stability during initial training. Cosine annealing over 1500 steps is used for all variants. ", "page_idx": 21}, {"type": "text", "text": "G Experiment details: RNA-binding task ", "text_level": 1, "page_idx": 22}, {"type": "table", "img_path": "kQ9LgM2JQT/tmp/cb926a734a240905ce3eb2b7bc3c18e24c02433a2869a5acf71294389995467f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 8: Hyperparameters and specifications of the Sequence Transformer used across all models in RNA-binding task. ", "page_idx": 22}, {"type": "table", "img_path": "kQ9LgM2JQT/tmp/f5f5a49ea1bb21fe8a5881eb193d9661c2d5833b9d76a6dbce71fe36f067dec3.jpg", "table_caption": ["Table 9: Model-specific parameters for QGFN in RNA-binding task. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "We follow the setup of Jain et al. [2022] but using the task introduced in Sinai et al. [2020]. We use a sequence transformer [Vaswani et al., 2017] architecture with 4 layers, 64-dimensional embeddings, and 2 attention heads. The training for this task is 5000 iterations over mini-batch sizes of 64. The reward scaling parameter $\\beta$ is set to 8 and a learning rate of $1\\times10^{-4}$ and $1\\times10^{-2}$ for $\\log{Z}$ . In this task, $\\beta\\sim U^{[1,16]}$ is used for LSL-GFN. Following the approach described by Sinai et al. [2020], the modes are predefined from enumerating the entire RNA landscape for L14RNA1 and L14RNA1 $+2$ to identify local optimal through exhaustive search. ViennaRNA [Lorenz et al., 2011] is used to provide the RNA binding landscape. For MunDQN, Munchausen parameter $\\alpha$ set to 0.15, a prioritized replay buffer size of 800,000, and a soft update coefficient $\\tau=0.1$ . We sample 16 trajectories and store them in the replay buffer, subsequently sampling 1024 transitions from this buffer. We used the other hyperparameters mentioned in the original MunDQN paper [Tiapkin et al., 2023]. ", "page_idx": 22}, {"type": "text", "text": "G.1 QGFN hyperparameters: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "For all QGFN variants, we used grid-search for tuning hyperparameters $p$ and $n$ . We used $n=13$ from the set 12, 13, 14. For $p$ -greedy QGFN, $p\\,=\\,0.4$ was chosen from $\\{0.2,0.4,0.6,0.8\\}$ . For $p$ -of-max QGFN, $p\\,=\\,0.9$ was selected from $\\left\\lbrace0.6,0.7,0.8,0.9\\right\\rbrace$ . In $p$ -quantile QGFN, we tried $p=0.25$ and 0.50, but neither performed well due to small action spaces. A stepwise scheduler set at 500 steps is applied to $p$ -of-max QGFN. ", "page_idx": 22}, {"type": "text", "text": "H Experiment details: Bit sequence generation ", "text_level": 1, "page_idx": 23}, {"type": "image", "img_path": "kQ9LgM2JQT/tmp/43dc5bc9f0f0e50f54a8e78d5cc442f8f8babfb480ff408653dd157140b570c3.jpg", "img_caption": ["Figure 18: Bit sequence generation, $k=4$ . ", "Table 10: Hyperparameters and specifications of the Sequence Transformer used across all models in bit sequence generation. "], "img_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "kQ9LgM2JQT/tmp/90ba6a714ceb83e77836245725613395b29c73008c94354f3227e7a00451381c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "kQ9LgM2JQT/tmp/8bde5bd4f37f839aadde20dff9fb2d277c61b38ebb5c3f885e3bb48db7a793c2.jpg", "table_caption": ["Table 11: Model-specific parameters for QGFN in bit sequence generation. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "The bit sequence generation task follows the same environmental setup as Malkin et al. [2022a] with $\\beta$ value as 3. We generate $|M|=60$ reference sequences by randomly combining $m=15$ symbols from an initial vocabulary H = {00000000, 11111111, 11110000, 00001111, 00111100}. ", "page_idx": 23}, {"type": "text", "text": "Beyond the original auto-regressive generative framework in Malkin et al. [2022a], we generate sequences in a prepend-apppend fashion motivated by Shen et al. [2023]. We use a sequence transformer [Vaswani et al., 2017] architecture for all experiments with rotary position embeddings, 3 hidden layers with hidden dimension 64 across 8 attention heads. All methods were trained for 50,000 iterations with a minibatch size of 16. For trajectory balance, we use a learning rate of of $1\\times10^{-4}$ for the policy parameters and $1\\times10^{-3}$ for $\\log{Z}$ . For SubTB, we use the same hyperparameters as in Madan et al. [2023]. For LSL-GFN, we use $\\beta\\sim U^{[1,6]}$ , where $U$ denotes a uniform distribution. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "H.1 QGFN hyperparameters: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "For all variants of QGFN, we did a grid-search approach for hyperparameter tuning for $p$ and $n$ . For $\\scriptstyle{\\mathrm{k}}=1$ where actions are limited to $_{\\{0,1\\}}$ , we set $n$ at 120, selected from $\\{30,60,90,120\\}$ , across alla QGFN variants. For the p-greedy QGFN, we chose $p=0.4$ ; for the $p$ -of-max QGFN, $p$ was set to 0.3, with cosine annealing applied at 500 steps. In the $p$ -qunatile QGFN, we tested $p$ values of 0.25 and 0.50, but neither achieved good performance due to the binary nature of the action space. ", "page_idx": 24}, {"type": "text", "text": "I Experiment details: Graph combinatorial optimization problems - maximum independent set (MIS) ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "As an additional task, we explore graph combinatorial optimization, specifically the maximum independent set (MIS) problem mentioned in Zhang et al. [2023]. We directly used the codebase shared by Zhang et al. [2023] and report the performance at test time in Table 12. ", "page_idx": 24}, {"type": "table", "img_path": "kQ9LgM2JQT/tmp/bb94afd2d5cb15be84f8d2daf813d888155e307dc4140fce359e977bca3166d7.jpg", "table_caption": ["Table 12: Comparison of different methods on small graphs: metric size and top 20 metrics. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "We used the same parameters as in the fragment-based molecule generation experiments. At test time, we applied the p-greedy, p-quantile, and p-of-max sampling strategies respectively for different methods. Note that the reported performance might not reflect the best achievable results on this task, as we did not explore different hyperparameter settings. ", "page_idx": 24}, {"type": "text", "text": "J Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "All of our experiments were conducted using A100 and V100 GPUs. For the fragment-based task, we used 8 workers on A100 GPUs, and it ran in less than 4 hours. For RNA, we used 4 workers, and it completed in less than 4 hours. For QM9, we used 0 workers, and it finished in less than 9 hours. For Bit sequence, we used 8 workers, and it ran in less than 24 hours. ", "page_idx": 24}, {"type": "text", "text": "K Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The research conducted in this work is not far removed from practical applications of generative models. As such, we acknowledge the importance of considering safety and alignment in applications closely related to this work such as drug discovery, material design, and industrial optimization. We believe that research on GFlowNets may lead to models that better generalize, and in the long run may be easier to align. Another important application of GFlowNets is in the causal and reasoning domains; we believe that improving in those fields may lead to easier to understand and safer models. ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction are supported by the results presented in Section 5. Additionally, Section 4 provides a thorough analysis of the proposed method, further strengthens the paper\u2019s contributions and scope. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: Section 7 includes a subsection on limitations. Additionally, the discussion on QGFN variants in Section 5 provides a more thorough analysis of the strengths and weaknesses of each variant. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not include theoretical results Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: In Appendix $\\S E$ , we outline all of the hyperparameters needed to reproduce our results, with references to other papers that also used the same environment. In addition, we will be sharing all of our code to run our QGFN algorithm in all tasks presented in this paper. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 26}, {"type": "text", "text": "some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: All of the environments are open access with sufficient instructions provided in Appendix $\\S\\mathrm{E}$ . ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: All of the environments setting/details are provided in Appendix $\\S\\mathrm{E}$ . Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: All of our experiments are run with 5 seeds, with interquartile mean and standard error calculated over these 5 seeds. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: In appendix $\\S\\mathrm{J}$ , we specified the compute resources used to for our experiments. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We also consider and discussion societal implications of our work in $\\S\\mathrm{K}$ . Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We discuss the positive societal impacts in $\\S\\mathrm{K}$ . We do not see any negative social impacts of this paper. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 28}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: We do not release models that have a high risk for misuse. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We have cited the original paper that produced the code package or dataset throughout the paper. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}]