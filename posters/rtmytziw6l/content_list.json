[{"type": "text", "text": "SymILO: A Symmetry-Aware Learning Framework for Integer Linear Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Qian Chen1,2, Tianjian Zhang1,2, Linxin $\\mathrm{Yang}^{2,3}$ , Qingyu Han2, Akang Wang2,3,\\*, Ruoyu $\\operatorname{Sun}^{2,3}$ , Xiaodong Luo2,3, and Tsung-Hui Chang1,2 ", "page_idx": 0}, {"type": "text", "text": "1School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China 2Shenzhen Research Institute of Big Data, China 3School of Data Science, The Chinese University of Hong Kong, Shenzhen, China ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Integer linear programs (ILPs) are commonly employed to model diverse practical problems such as scheduling and planning. Recently, machine learning techniques have been utilized to solve ILPs. A straightforward idea is to train a model via supervised learning, with an ILP as the input and an optimal solution as the label. An ILP is symmetric if its variables can be permuted without changing the problem structure, resulting in numerous equivalent and optimal solutions. Randomly selecting an optimal solution as the label can introduce variability in the training data, which may hinder the model from learning stable patterns. In this work, we incorporate the intrinsic symmetry of ILPs and propose a novel training framework called SymILO. Specifically, we modify the learning task by introducing solution permutation along with neural network weights as learnable parameters and then design an alternating algorithm to jointly optimize the loss function. We conduct extensive experiments on ILPs involving different symmetries and the computational results demonstrate that our symmetry-aware approach significantly outperforms three existing methods\u2014-achieving $50.\\dot{3}\\%$ , $66.5\\%$ , and $45.4\\%$ average improvements, respectively. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Integer linear programs (ILPs) are optimization problems with integer variables and a linear objective, and have a wide range of practical uses in various fields, such as production planning (Pochet & Wolsey, 2006; Chen, 2010), resource allocation (Liu & Fan, 2018; Watson & Woodruff, 2011), and transportation management (Luathep et al., 2011; Sch\u00f6bel, 2001). An important property that often arises in ILPs is symmetry (Margot, 2003), which refers to a situation where permuting variables does not change the structure of an ILP. ", "page_idx": 0}, {"type": "text", "text": "Recently, there emerges many approaches equipping machine learning methods, supervised learning in particular, to help efficient solution identification for ILPs (Zhang et al., 2023). Among these approaches, an important category derived from the idea of predicting the optimal solution has demonstrated significant improvements (Han et al., 2023; Ding et al., 2020; Khalil et al., 2022; Nair et al., 2020). In this paper, we consider a classic supervised learning task that aims to train an ML model to predict an optimal solution for an ILP. Specifically, given a training dataset ${\\mathcal{D}}=$ $\\{(s_{i},y_{i})\\}_{i=1}^{N}$ with $y_{i}$ denoting an optimal solution to instance $s_{i}$ , we hope to train a neural network model $f_{\\theta}(\\cdot)$ to approximate the mapping from ILP instances to their optimal solutions, via minimizing the empirical risk defined on $f_{\\theta}{\\big(}s_{i}{\\big)}$ and $y_{i}$ . ", "page_idx": 0}, {"type": "text", "text": "However, for an ILP $s_{i}$ with symmetry, there exist multiple optimal solutions including $y_{i}$ and its symmetric counterparts, any of which has an equal probability of being returned as a label. Training neural networks without taking symmetry into account is basically learning a model supervised by random outputs, leading to prediction models of inferior performance. ", "page_idx": 1}, {"type": "text", "text": "To address this issue, we propose to leverage the symmetry of ILPs to improve the model performance of predicting an optimal solution. Specifically, given input $s_{i}$ , we define a new empirical risk using $f_{\\theta}{\\big(}s_{i}{\\big)}$ and $\\pi_{i}(y_{i})$ , where $\\pi_{i}(\\cdot)$ denotes the operation of permuting elements in $y_{i}$ into its symmetric counterpart. Along with ML model parameters, the permutation operators will also be optimized during training. To achieve this, we further develop a computationally affordable algorithm that alternates between optimization of model parameters and optimization of permutation operation. The distinct contributions of our work can be summarized as follows. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a symmetry-aware framework (called SymILO) that introduces permutation operators as extra optimization variables to the classic training procedure. \u2022 We devise an alternating algorithm to solve the newly proposed problem, with a specific focus on updating the permutation operator for different symmetries. \u2022 We conduct comprehensive numerical studies on four typical benchmark datasets involving symmetries, and the results show that our proposed approach significantly outperforms existing methods. ", "page_idx": 1}, {"type": "text", "text": "2 Related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Previous works on identifying high-quality solutions to ILPs via machine learning techniques mainly focus on reducing problem sizes. For example, Ding et al. (2020) propose to identify and predict a subset of decision variables that stay unchanged within the collected solutions. Li & Wu (2022) formulate MILPs as Markov decision processes and learn to reduce problem sizes via early-fixing. ", "page_idx": 1}, {"type": "text", "text": "It is noteworthy that the emergence of GNNs has had a significant impact on solving ILPs. Gasse et al. (2019) are the first to propose a bipartite-graph representation of ILPs and pass it to GNNs. Nair et al. (2020) adopt the same representation scheme and train GNNs to predict the conditional distribution of solutions, from which they further sample solutions. Rather than directly fixing variables, Han et al. (2023) conduct search algorithms in a neighborhood centered around an initial point generated from the predicted distribution. Other works based on GNNs (Sonnerat et al., 2022; Lin et al., 2019; Khalil et al., 2022; Wu et al., 2021) also illustrate great potential in improving the solving efficiency. ", "page_idx": 1}, {"type": "text", "text": "Limitations of the existing GNN-based approaches are also noticed. Nair et al. (2020); Han et al. (2023) try to address the multiple solution problem by learning the conditional distribution. Chen et al. (2022) introduce random features into the bipartite graph representation to differentiate variable nodes involving symmetries. ", "page_idx": 1}, {"type": "text", "text": "However, none of the existing learning-based approaches explicitly leverage the inherent symmetries in ILPs to achieve improvements. In contrast, works from mathematical optimization perspectives suggest that symmetry-handling algorithms exhibit great abilities in solving symmetry-involving ILPs (Pfetsch & Rehn, 2019). To name a few, such algorithms include orbital fixing (Ostrowski et al., 2011), tree pruning (Margot, 2002), and lexicographical ordering (Kaibel & Pfetsch, 2008). ", "page_idx": 1}, {"type": "text", "text": "3 Background and preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "3.1 ILPs ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "An integer linear program (ILP) has a formulation as follows: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x}\\ \\{c^{\\top}x|A x\\leq b,x\\in\\mathbb{Z}^{n}\\}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $x\\in\\mathbb{Z}^{n}$ are integer decision variables, and $c\\in\\mathbb{R}^{n},A\\in\\mathbb{R}^{m\\times n},b\\in\\mathbb{R}^{m}$ are given coefficients. ", "page_idx": 1}, {"type": "text", "text": "3.2 Symmetry Group ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Symmetry of ILPs is typically represented by groups. We start with some basic notations and most of which follow Margot (2009). Denoting the index set by $I^{n}=\\{1,2,\\ldots,n\\}$ , a permutation on $I^{n}$ is a bijective (one-to-one and onto) mapping $\\pi:I^{n}\\to I^{n}$ . For example, an identity permutation maps the index set to itself as $\\{\\pi(i)=i\\}_{i=1}^{n}$ and a cyclic permutation has rotational mapping rules $\\{\\pi(i)=i+1\\}_{i=1}^{n-1}$ and $\\pi(n)=1$ . Schematic diagrams of these two permutations and other ones are shown in Figure 1. For brevity, we abuse the notation $\\pi$ a little bit and denote the permutation acting on a vector $y\\in\\mathbb{R}^{n}$ by rearranging its coordinates, namely $\\pi(y)=\\left[y_{\\pi(1)},y_{\\pi(2)},\\bar{\\ldots},y_{\\pi(n)}\\right]^{\\top}$ . Let $Q$ be the set of all feasible solutions of (1) and $S_{n}$ the set of all permutations on $I^{n}$ . Note that $S_{n}$ is referred to as the symmetric group, which should not be confused with the symmetry group discussed as follows. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Definition 3.1. A symmetry group of (1) is defined as the set of all permutations $\\pi$ that map $Q$ onto itself, such that each feasible solution is mapped to another feasible solution with the same objective value, i.e., ", "page_idx": 2}, {"type": "equation", "text": "$$\nG=\\{\\pi\\in S_{n}:c^{\\top}\\bar{y}=c^{\\top}\\pi(\\bar{y})\\mathrm{~and~}\\pi(\\bar{y})\\in Q,\\,\\forall\\bar{y}\\in Q\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Next, we will delve into three commonly encountered symmetries, accompanied by typical example problems. Since not all variables in an ILP involve symmetry, we use $q\\leq n$ to indicate the size of the symmetry group. ", "page_idx": 2}, {"type": "text", "text": "Symmetric group The symmetric group, denoted by $S_{q}$ , is the group that consists of all permutations $\\mathit{\\check{q}}!$ in total) on $I^{q}$ . Problems with this kind of symmetry include bin packing (Johnson, 1974) and optimal job scheduling Graham et al. (1979), etc. An example is illustrated in Appendix B.0.1. ", "page_idx": 2}, {"type": "image", "img_path": "RtMyTzIW6l/tmp/bb20b08405bb26bcaa2c487bb0e146a3af167683295ff703358fa6bbc7bd8046.jpg", "img_caption": ["Figure 1: Permutation examples with directed edges denoting mapping rules. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Cyclic and dihedral groups As its name suggests, cyclic symmetry allows elements to be ", "page_idx": 2}, {"type": "text", "text": "permuted to their right neighbors, cycling the right-most variables back to the left, e.g., a cyclic (or rotational) permutation $\\rho$ in Figure 1 (c). The elements of a cyclic group $C_{q}$ are powers of $\\rho$ , and $|C_{q}|=q$ . Problems with cyclic group often have characteristics of rotations or cycles, e.g., periodic event scheduling problem (Serafini & Ukovich, 1989). ", "page_idx": 2}, {"type": "text", "text": "Compared to the cyclic group, a dihedral group (denoted as $D_{q}$ ) additionally includes reflective permutations, which is illustrated in Figure 1 (d). Consequently, ${\\dot{D}}_{q}$ comprises a total of $2q$ distinct permutations. A typical problem with such symmetry is the circular (or modular) golomb ruler problem (see Appendix B.0.2). ", "page_idx": 2}, {"type": "text", "text": "3.3 Classic supervised learning for solution prediction ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A classic solution prediction task based on supervised learning is formulated as follows. Let $\\boldsymbol{S}$ be the space of ILP instances and $\\boldsymbol{\\wp}$ be the label (i.e., optimal solution) space. A model function $f_{\\theta}:S\\rightarrow\\mathcal{Y}$ parameterized by $\\theta\\in\\Theta$ is used to learn a mapping from instances to optimal solutions. Let $\\mathcal{P}(S,Y)$ be a distribution over ${\\mathcal{S}}\\times{\\mathcal{V}}$ . The performance of the model function is measured by a criterion called true $r i s k:R(f_{\\theta}):=E_{\\mathcal{P}(S,Y)}\\left[\\ell\\left(f_{\\theta}(s),y\\right)\\right]$ , where $\\ell:\\mathcal{Y}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}^{+}$ is a given loss function, e.g., mean squared error or cross-entropy loss. An intuitive way to improve the model performance is to minimize the true risk. However, one cannot access all data from distribution ${\\bar{\\mathcal{P}}}(S,Y)$ , which makes it impossible to calculate the true risk. Practically, one can obtain a set of (instance, solution) pairs called training data $\\mathcal{D}=\\{(s_{i},y_{i})\\}_{i=1}^{N}\\subseteq(\\mathcal{S}\\times\\dot{\\mathcal{y}})^{N}$ sampled from $\\mathcal{P}(S,Y)$ , based on which define the empirical risk as ", "page_idx": 2}, {"type": "equation", "text": "$$\nr(f_{\\theta};\\mathcal{D}):=\\frac{1}{N}\\sum_{i=1}^{N}\\ell\\left(f_{\\theta}(s_{i}),y_{i}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "By minimizing the empirical risk, i.e., $\\operatorname*{min}_{\\theta\\in\\Theta}r$ , one aims to approximate the minimization of the true risk, under the assumption that the training data is a representative sample of the overall data distribution. ", "page_idx": 2}, {"type": "text", "text": "4 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "4.1 Reformulation of the learning task ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In Section 3.3, we introduce a classic supervised learning task for general ILPs, which aims at learning a mapping $f_{\\theta}$ from instances to optimal solutions. In this task, a dataset $\\boldsymbol{\\mathcal{D}}=\\{(s_{i},y_{i})\\}$ is given, and the mapping $f_{\\theta}$ is learned by minimizing (3) with $\\theta$ as decisions. However, for ILPs with symmetry, an ILP instance has multiple solutions (let $Y_{i}$ be the set of optimal solutions of $i$ -th instance). As a consequence, the labels in this task have multiple choices, thus datasets choosing different optimal solutions as labels $\\{\\mathcal{D}^{\\prime}=\\{(s_{i},y_{i}^{\\prime})\\}_{i=1}^{N},\\forall\\,{y_{i}^{\\prime}\\in Y_{i}}\\}$ are all valid for the learning task. Empirically, we observe that different $D^{\\prime}$ can lead to distinct performance, which motivates us to consider the selection of labels for ILPs with symmetry. ", "page_idx": 3}, {"type": "text", "text": "We reformulate the learning task as follows. Firstly, we augment dataset $\\mathcal{D}$ to dataset $\\mathcal{D}_{s}~=$ $\\left\\{(s_{i},y_{i},G_{i})\\right\\}$ , where $G_{i}$ is the symmetry group of $i$ -th instance and $\\pi_{i}\\in G_{i}$ . Secondly, we define the symmetry-aware empirical risk as ", "page_idx": 3}, {"type": "equation", "text": "$$\nr_{s}(f_{\\theta},\\{\\pi_{i}\\}_{i=1}^{N};\\mathcal{D}_{s}):=\\frac{1}{N}\\sum_{i=1}^{N}\\ell\\left(f_{\\theta}(s_{i}),\\pi_{i}(y_{i})\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Then, the mapping $f_{\\theta}$ is learned by minimizing the symmetry-aware risk as $\\operatorname*{min}_{\\theta,\\pi}r_{s}$ (both $\\theta$ and $\\pi$ as decisions). In contrast to the original task, the symmetry-aware task uses symmetry information by introducing extra decisions $\\{\\pi_{i}\\}_{i=1}^{N}$ , so as to dynamically selecting proper optimal solutions as labels. There are important differences between the symmetry-aware empirical risk and the classic one: ", "page_idx": 3}, {"type": "text", "text": "Proposition 4.1. Let $r^{*}$ and $r_{s}^{*}$ be the global minimal values of min\u03b8 $r$ and $\\operatorname*{min}_{\\theta,\\pi}r_{s}$ , respectively. Then, the following claims hold: ", "page_idx": 3}, {"type": "equation", "text": "$$\nr_{s}^{*}\\le r^{*},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Claim (i) always holds since ${\\mathrm{min}}\\,\\theta\\,^{r}$ is a special case of $\\operatorname*{min}_{\\theta,\\pi}r_{s}$ when $\\pi_{1},\\ldots,\\pi_{N}$ are all identity permutations. Claim (ii) shows a significant advantage of $r_{s}$ compared to $r$ . A concrete example is as follows. ", "page_idx": 3}, {"type": "text", "text": "Proof: Consider the case where $s_{1}~=~s_{2}$ , $y_{1}\\ \\neq\\ y_{2}$ , and $\\ell$ is the mean squared error, and let ${\\hat{y}}=f_{\\theta}{\\big(}s_{1}{\\big)}=f_{\\theta}{\\big(}s_{2}{\\big)}$ . Then, ", "page_idx": 3}, {"type": "equation", "text": "$$\nr^{*}\\geq\\operatorname*{min}_{\\hat{y}}\\frac{1}{2}\\left(\\|{\\hat{y}}-y_{1}\\|^{2}+\\|{\\hat{y}}-y_{2}\\|^{2}\\right)=\\frac{1}{4}\\|y_{1}-y_{2}\\|^{2}>0.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "While for the symmetry-aware risk, since $s_{1}$ and $s_{2}$ corresponds to identical instances, there must exist permutations $\\pi_{1}^{\\prime}$ and $\\pi_{2}^{\\prime}$ , such that $\\pi_{1}^{\\prime}(y_{1})=\\pi_{2}^{\\prime}(y_{2})$ . Consequently, ", "page_idx": 3}, {"type": "equation", "text": "$$\nr_{s}^{*}=\\operatorname*{min}_{\\hat{y}}\\ \\|\\hat{y}-\\pi_{1}^{\\prime}(y_{1})\\|^{2}+\\|\\hat{y}-\\pi_{2}^{\\prime}(y_{2})\\|^{2}=0<r^{*}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "4.2 An alternating minimization algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The minimization of (4) is challenging due to the discrete nature of $\\pi$ . Motivated by the well-known block coordinate minimization algorithms (Mangasarian, 1994), we update $\\theta$ and $\\pi$ alternately, i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\{\\pi_{i}^{k+1}\\}_{i=1}^{N}\\leftarrow\\arg\\underset{\\pi_{i}\\in G_{i}}{\\operatorname*{min}}\\,r_{s}(f_{\\theta^{k}},\\{\\pi_{i}\\}_{i=1}^{N};\\mathcal{D}_{s}),}\\\\ &{}&{\\theta^{k+1}\\leftarrow\\arg\\underset{\\theta\\in\\Theta}{\\operatorname*{min}}\\,r_{s}(f_{\\theta},\\{\\pi_{i}^{k+1}\\}_{i=1}^{N};\\mathcal{D}_{s}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Such an alternating mechanism divides the minimization of (4) into two sub-problems: a discrete optimization in (5) over sets $\\{G_{i}\\}_{i=1}^{N}$ and a classic empirical risk minimization in (6). Repeatedly solving (6) to optimal is unrealistic, thus it is more practical to update $\\theta$ by several gradient steps instead. ", "page_idx": 3}, {"type": "text", "text": "The sub-problem in (5) is further specified as shown in Section 4.2.1, according to the symmetry structures in the ILP instances. ", "page_idx": 4}, {"type": "text", "text": "We summarize the proposed alternating minimization algorithm in Algorithm 1. In the main loop, $\\{\\pi_{i}\\}_{i=1}^{N}$ are updated first (line 5), after which an inner loop (lines 6-10) is operated to update $\\theta$ through a gradient-based method GD, e.g., Adam (Kingma & Ba, 2014). These two updates alternate until a preset maximum number of epochs $K$ is reached. We finally note ", "page_idx": 4}, {"type": "table", "img_path": "RtMyTzIW6l/tmp/a5e45ecda5bd0f599fbc1d1419c9feba4f3bc3bea76b00b63a49202e8d1154ef.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "that Algorithm 1 can be easily adapted to a mini-batch version, in which the data can be randomly sampled from $\\mathcal{D}_{s}$ . ", "page_idx": 4}, {"type": "text", "text": "4.2.1 Optimization over symmetry groups in (5) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we investigate the concrete formulations of the sub-problem in (5) for the symmetry groups mentioned in Section 3.2 (symmetric group, cyclic and dihedral groups), and devise algorithms to solve them. ", "page_idx": 4}, {"type": "text", "text": "Cyclic and dihedral groups The cardinality of a cyclic group $C_{q}$ is $q$ , and it is $2q$ for a dihedral group $D_{q}$ . For symmetry groups with such reasonably small size, a straightforward and effective way to solve (5) is to evaluate all possible permutations and select the one that yields the minimum $r_{s}$ . ", "page_idx": 4}, {"type": "text", "text": "Symmetric group The cardinality of a symmetric group is factorially large, $|S_{q}|\\,=\\,q!$ , so it is impractical to traverse all permutations. Since $\\pi_{1},\\ldots,\\pi_{N}$ are not coupled, we can separate them and solve the $N$ sub-problems individually: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi_{i}}\\ell\\left(f_{\\theta}(s_{i}),\\pi_{i}(y_{i})\\right),\\forall i=1,\\ldots,N.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Without loss of generality, consider an ILP whose variables have a matrix form (e.g., see Appendix B.0.1), denoted by $X\\in\\mathbb{Z}^{p\\times q}(p\\cdot q<n)$ , and a symmetric group $S_{q}$ acting on its column coordinates. In this case, (7) is equivalent to solve the following binary linear program (BLP), ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{P}\\ \\ell\\left({\\hat{X}},X P\\right)\\quad{\\mathrm{s.t.}}\\quad P\\in\\{0,1\\}^{q\\times q},P^{\\top}\\mathbf{1}=\\mathbf{1},P\\mathbf{1}=\\mathbf{1},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $P$ is a permutation matrix, X\u02c6 is the matrix form of $f_{\\theta}{\\big(}s_{i}{\\big)}$ , and 1 is an all-one vector. We relax $P$ to take continuous values between 0 and 1, and get a linear program (LP), ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{P}\\ \\ell\\left(\\hat{X},X P\\right)\\quad\\mathrm{s.t.}\\quad P\\in[0,1]^{q\\times q}\\,,P^{\\top}\\mathbf{1}=\\mathbf{1},P\\mathbf{1}=\\mathbf{1}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "According to Proposition 4.2, one can solve (9), to get the optimal permutations for the original problem in (8). It can be done quite efficiently with the aid of off-the-shelf LP solvers, such as Gurobi Optimization, LLC (2023), CPLEX IBM (2020), etc. ", "page_idx": 4}, {"type": "text", "text": "Proposition 4.2. When $\\ell$ is the squared error or binary cross-entropy loss, the optimal solution to (9) is also an optimal solution to (8). (See the proof in A.1.) ", "page_idx": 4}, {"type": "text", "text": "4.3 An overview of the SymILO framework ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we summarize a novel learning framework (SymILO) that utilizes symmetry for solving ILPs. An overview is depicted in Figure 2, which consists of two parts: the upper row connected by green arrows delineates a graph neural network (GNN)-based workflow, and the lower row connected by red arrows outlines the training process. ", "page_idx": 4}, {"type": "text", "text": "For the GNN-based workflow, an ILP is first converted to a bipartite graph (see appendix C for details), which is then fed to a GNN model $f_{\\theta}$ (see appendix D for details), producing a predicted solution. Notably, the predicted solution is finally used in downstream tasks for refinement. Due to the complexity of solving ILPs, existing methods, such as Nair et al. (2020); Ding et al. (2020); ", "page_idx": 4}, {"type": "image", "img_path": "RtMyTzIW6l/tmp/be4d3e2d9c3e91c2219df0b2f538c7f533c76aee80e2baa2dbfc742e5e9a8b6f.jpg", "img_caption": ["Figure 2: An overview of the SymILO framework. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Khalil et al. (2022); Han et al. (2023), often include a post-processing module taking the predicted solution as an initial point to identify higher-quality solutions. Our approach follows this routine and integrates certain downstream techniques. Section 5.1 specifies three downstream tasks. ", "page_idx": 5}, {"type": "text", "text": "For the training process, the data used to minimize the symmetry-aware empirical risk $r_{s}$ include the collected solution $y_{i}$ and the symmetry group $G_{i}$ of each instance $s_{i}$ . Both parameters $\\theta$ of the GNN model and permutations $\\{\\pi_{i}\\}_{i=1}^{N}$ of each solution are optimized via an alternating algorithm mentioned in Algorithm 1. Given a trained model $f_{\\theta^{K}}$ , the prediction $f_{\\theta^{K}}(s^{\\prime})$ for an unseen instance $s^{\\prime}$ is used to guide the downstream tasks in identifying feasible solutions. Note that $\\{\\pi_{i}\\}_{i=1}^{N}$ are utilized only in the training phase but not in the inference phase. ", "page_idx": 5}, {"type": "text", "text": "5 Experimental settings ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, the experimental settings are presented. The corresponding source code is available at https://github.com/NetSysOpt/SymILO. ", "page_idx": 5}, {"type": "text", "text": "5.1 Downstream tasks and baselines ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In our experiments, we pass the predictions of GNN models to three downstream tasks, namely fix and optimize, local branching, and node selection, to identify feasible solutions. For each downstream task, we choose one existing method as a baseline. The downstream tasks and their corresponding baselines (in parentheses) are shown below. ", "page_idx": 5}, {"type": "text", "text": "Fix and optimize (ND): \u201cFix and optimize\u201d refers to a strategy where one first \u201cfix\u201d or set some variables to specific values and then \u201coptimize\u201d the remaining variables to find better solutions. The baseline we choose is \u201cNeural Diving\u201d (ND) proposed by Nair et al. (2020), a technique using a graph neural network to generate partial assignments for ILPs, which creates smaller sub-ILPs with the unassigned variables. ", "page_idx": 5}, {"type": "text", "text": "Local branching (PS): Local branching is a heuristic method that constructs a linear constraint based on a given initial solution to the original ILP instance. This constraint restrains the search space in a region around the initial solution. It can help guide the optimization process toward better solutions while balancing computational efficiency. Approaches based on this idea include Ding et al. (2020); Han et al. (2023); Chen et al. (2023) and we select the \u201cpredict-and-search\u201d (PS) framework proposed by Han et al. (2023) as a baseline. ", "page_idx": 5}, {"type": "text", "text": "Node selection (MIP-GNN): In branch and bound algorithms, node selection is a process of choosing the proper nodes to explore next. Effective node selection is crucial for the algorithm\u2019s success in solving optimization problems. \u201cMIP-GNN\u201d (MIP-GNN) proposed by Khalil et al. (2022) uses GNN prediction to guide node selection and warm-starting, and is selected as another baseline. ", "page_idx": 5}, {"type": "text", "text": "5.2 Benchmark datasets ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We evaluate the proposed framework on four ILP benchmarks with certain symmetry, which consists of (i) two problems with symmetric groups: the item placement problem (IP) and the steel mill slab problem (SMSP), (ii) the periodic event scheduling problem (PESP) with cyclic group, and (iii) a modified variant of PESP (PESPD) which has a dihedral group. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "The first benchmark IP is from the NeurIPS ML4CO 2021 competition (Gasse et al., 2022). We use their source code to randomly generate instances with binary variables ranging from 208 to 1050. Each instance has a symmetric group $S_{4}\\sim S_{10}$ . We use 500 instances for our experiments, taking 400 as the training set and the remaining 100 for testing. The SMSP benchmark is from Schaus et al. (2011), and contains 380 problem instances. We randomly select 304 of them as training data and take the others as testing data. The instances of this benchmark have $22\\mathrm{k}{\\sim}24\\mathrm{k}$ binary variables and nearly 10k constraints, with each of them having a symmetric group $S_{111}$ . The last two benchmarks are from PESPlib Goerigk (2012), a collection of periodic timetabling problems inspired by real-world railway timetabling settings. Since PESPlib only provides a few instances, which are not sufficient to support neural network training, we randomly perturb the weights of the provided instances to generate more data (see Appendix G.3.1 for details). We respectively generate 500 instances for PESP and PESPD, taking 400 of them as training sets and 100 as testing sets. The symmetry groups of these two datasets are cyclic groups $C_{5}\\sim C_{15}$ and dihedral groups $D_{5}\\sim D_{15}$ , respectively. For all training sets, $30\\%$ instances are used for validation. The average numbers of variables and constraints, as well as the symmetry groups of each benchmark problem, are summarized in Appendix F.1. Besides, more details about their ILP formulations and corresponding symmetries are supplemented in Appendix G. ", "page_idx": 6}, {"type": "text", "text": "These benchmarks only include problem instances. We collect the corresponding solutions using an ILP solver CPLEX (IBM, 2020). However, solving ILP instances even with moderate sizes to optimal is extremely expensive. It is more practical to use high-quality solutions as the labels. Therefore, we run single-thread CPLEX for a time limit of 3,600 seconds and record the best solutions. ", "page_idx": 6}, {"type": "text", "text": "5.3 Training settings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "All models are trained with a batch size 16 for 50 epochs. The Adam optimizer with a learning rate of 0.001 is used, and other hyperparameters of the optimizer are set to their default values. The model with the smallest loss on the validation set is used for subsequent evaluations. Other training settings, such as the loss function and neural architectures, follow the configurations in Han et al. (2023). More details about the hyper-parameter tuning for the downstream tasks and software resources are shown in Section E. ", "page_idx": 6}, {"type": "text", "text": "5.4 Evaluation metrics ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To compare the prediction performance of the model trained on $r$ and $r_{s}$ , we define the Top- $m\\%$ error for evaluation. In addition, another criterion relative primal gap is used to evaluate the final performance in identifying feasible solutions in different downstream tasks. ", "page_idx": 6}, {"type": "text", "text": "Top- $m\\%$ error: We use the distance between a rounded prediction and its nearest equivalent solution as the error. Specifically, given a prediction $\\hat{y}$ and its label $y$ , we define the equivalent solution closest to $\\hat{y}$ as $\\tilde{y}=\\pi^{\\prime}(y)$ , where $\\pi^{\\prime}=\\arg\\operatorname*{min}_{\\pi}\\|\\hat{y}-\\pi(y)\\|$ . Then, the Top- $m\\%$ error is defined as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{E}(m)=\\sum_{i\\in M}|\\mathrm{Round}(\\hat{y}_{i})-\\tilde{y}_{i}|,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $M$ is the index set of $m\\%$ variables with largest values of $|\\mathrm{Round}(\\hat{y}_{j})\\rrangle-\\hat{y}_{j}|$ . This error measures the minimum distance between the prediction and all solutions equivalent to the label. Compared to naive use of the distance $\\textstyle\\sum_{i\\in M}|\\bar{\\mathrm{Round}}(\\hat{y}_{i})-y_{i}|$ , (10) can more accurately represent how close a prediction is to a feasible solution. Since for the naive distance, when Round $\\left(\\hat{y}\\right)$ equals any equivalent solution $\\pi(y)\\neq y$ , the distance is greater than 0, while that of (10) is 0. ", "page_idx": 6}, {"type": "text", "text": "Relative primal gap: We also feed the outputs of the models trained through $r_{s}$ to the downstream tasks mentioned in Section 5.1 to evaluate the quality of the predictions. All the three downstream approaches incorporate ILP solvers to search for solutions. We run these ILP solvers on a single thread for a maximum of 800 seconds. Since all the problems used in the experiments are NP-hard, identifying optimality is highly time-consuming. Thus the metric used in our experiments is relative primal gap ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{PG}(\\tilde{y})=\\frac{|c^{\\top}\\tilde{y}-c^{\\top}y^{*}|}{|c^{\\top}y^{*}|+\\epsilon},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "which measures the relative gap in the objective value of a feasible solution $\\tilde{y}$ to that of the best-known solution $y^{\\ast}$ , and $\\epsilon$ is a small positive value to avoid the numerical issue. Additionally, let $\\gamma_{r}$ and $\\gamma_{r_{s}}$ respectively be the primal gaps of models trained through $r$ and $r_{s}$ , then an improvement gain of our approach is calculated as $(\\gamma_{r}-\\gamma_{r_{s}})/\\gamma_{r}$ . ", "page_idx": 7}, {"type": "text", "text": "6 Numerical results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we present the comparison results on empirical risk $r$ and symmetry-aware one $r_{s}$ . In addition, primal gaps of SymILO and baselines on three downstream tasks are reported. ", "page_idx": 7}, {"type": "text", "text": "6.1 On empirical risks and Top- $m\\%$ error ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We denote training and test risks by $r^{t r}(\\cdot)=r(\\cdot;D^{t r})$ and $r^{t e}=r(\\cdot;\\mathcal{D}^{t e})$ , respectively, and similarly use $r_{s}^{t r}$ and $r_{s}^{t e}$ for symmetry-aware risk. Let $f^{(k)}$ and $f_{s}^{(k)}$ be the best classic model and symmetryaware model obtained at $k$ -th epoch by training with $r^{t r}$ and $r_{s}^{t r}$ , respectively. We plot both the training and test risks versus the number of epochs in Figure 3. As predicted in Proposition 4.1, when algorithms converge, the classic empirical risk $r^{t r}$ is always greater than symmetry-aware risk $r_{s}^{t r}$ . ", "page_idx": 7}, {"type": "image", "img_path": "RtMyTzIW6l/tmp/861eec918c5e8aaf55aa82b853e63c79cbc52fe50fe1816f312ee7b36d9e6663.jpg", "img_caption": ["Figure 3: The training and test risks v.s. the number of epochs on four benchmark problems. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "As shown in Table 1, the symmetry-aware model $f_{s}^{(K)}$ always predicts smaller Top- $m\\%$ errors in (10) compared to the classic model $f^{(K)}$ , demonstrating the usefulness of proposed empirical symmetry-aware risk in predicting solutions correctly. ", "page_idx": 7}, {"type": "table", "img_path": "RtMyTzIW6l/tmp/6db3ed9cff03ef786106eb61a30fe908028f1d59f76d4803d6cf391bcf1bc42d.jpg", "table_caption": ["Table 1: Top- $m\\%$ errors $\\left(\\downarrow\\right)$ of model $f^{(K)}$ and $f_{s}^{(K)}$ averaged over different datasets. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Moreover, the time costs of minimizing different empirical risks $r$ and $r_{s}$ for a mini batch are shown in Table 2. Here, $t$ denotes the average time of solving the permutation decisions per instance. The reported times for $r_{s}$ include the optimization time $t$ . The table illustrates that the alternate training strategy does not significantly increase the training duration, and the optimization step over $\\pi$ is executed efficiently. ", "page_idx": 7}, {"type": "table", "img_path": "RtMyTzIW6l/tmp/7ddd9c500db9e8f1da8e63508bc1a4534c31648da2cbbace5c930c472dbfa079.jpg", "table_caption": ["Table 2: Time cost for minimizing different empirical risks (in seconds). "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "6.2 Downstream results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The relative primal gaps of different downstream tasks at different solving time are shown in Figure 4, and the final values at 800 seconds are listed in Table 3. As Figure 4 shows, our proposed empirical risk significantly improves the performance of different downstream tasks over the primal gap in 800 seconds. ", "page_idx": 8}, {"type": "image", "img_path": "RtMyTzIW6l/tmp/9ef22e15c451f8963fb839e4dacef45efbe328388bd7eb2e180cdcd6a5205484.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 4: Relative primal gaps at different times. Three downstream tasks, i.e., fix-and-optimize, local branching, and node selection, are evaluated with a time limit of 800 seconds. The results of the same downstream task use the same color. In addition, the relative primal gap of the Tuned CPLEX running on a single thread is also reported as the blue dashed line. ", "page_idx": 8}, {"type": "text", "text": "Note that the node selection task exhibits modest performance in comparison to other tasks; a possible reason is that it requires runtime interaction to call the callback functions provided by the CPLEX Python APIs, which can slow down the whole solving process. However, such a flaw does not affect the demonstration of the effectiveness of our proposed method. ", "page_idx": 8}, {"type": "text", "text": "For the primal gap at 800 seconds shown in Table 3, the models trained through $r_{s}$ significantly improve all downstream tasks. The performance gain of the model trained through $r_{s}$ is calculated by computing the relative gaps between our approach\u2019s gap improvements and that of the baselines. Average gains over the three downstream tasks are $50.3\\%$ , $66.5\\%$ and $45.4\\%$ , respectively. The overall results demonstrate the effectiveness of the proposed empirical risk $r_{s}$ . We also provide the corresponding p-values for the significance of improvements in Appendix F.2. ", "page_idx": 8}, {"type": "table", "img_path": "RtMyTzIW6l/tmp/ee7adb6ea59218de3d41a9d9adb08a96de2bdde6b4998240e58cde47beb0f362.jpg", "table_caption": ["Table 3: Average relative primal gaps $\\left(\\downarrow\\right)$ of different downstream tasks at 800 second. The values in this table are averaged over primal gaps of all test data for each benchmark problem. \u201cTuned CPLEX\u201d is the result of the tuned CPLEX running on a single thread. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "7 Limitations and conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In conclusion, we propose SymILO, a novel symmetry-aware learning framework for enhancing the prediction of solutions for integer linear programs by incorporating symmetry into the training process. Our approach shows significant performance improvements over symmetry-agnostic methods on benchmark datasets. Despite the significant advancements presented in our symmetry-aware learning framework, SymILO, several limitations must be acknowledged. Firstly, while we provide realizations for three commonly encountered symmetry groups\u2014symmetric, cyclic, and dihedral\u2014the framework requires specific formulations for optimizing permutations, which limits its immediate applicability to other symmetry groups not discussed in this work. Secondly, for large-scale problem instances with extensive and complex symmetry groups, the sub-problems involved in optimizing permutations can significantly slow down the training process. Enhancing the computational efficiency of our alternating optimization algorithm for these cases remains a challenge and an area for future research. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the National Key R&D Program of China under grant 2022YFA1003900. Akang Wang also gratefully acknowledges support from the National Natural Science Foundation of China (Grant No. 12301416), the Shenzhen Science and Technology Program (Grant No. RCBS20221008093309021), the Guangdong Basic and Applied Basic Research Foundation (Grant No. 2024A1515010306) and Longgang District Special Funds for Science and Technology Innovation (LGKCSDPT2023002). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Chen, Y., Gao, W., Ge, D., and Ye, Y. Pre-trained mixed integer optimization through multi-variable cardinality branching. arXiv preprint arXiv:2305.12352, 2023.   \nChen, Z., Liu, J., Wang, X., Lu, J., and Yin, W. On representing mixed-integer linear programs by graph neural networks, 2022.   \nChen, Z.-L. Integrated production and outbound distribution scheduling: review and extensions. Operations research, 58(1):130\u2013148, 2010.   \nDantzig, G. B. Linear inequalities and related systems. Number 38. Princeton university press, 1956.   \nDing, J.-Y., Zhang, C., Shen, L., Li, S., Wang, B., Xu, Y., and Song, L. Accelerating primal solution findings for mixed integer programs based on solution prediction. In Proceedings of the aaai conference on artificial intelligence, volume 34, pp. 1452\u20131459, 2020.   \nGargani, A. and Refalo, P. An efficient model and strategy for the steel mill slab design problem. In International Conference on Principles and Practice of Constraint Programming, pp. 77\u201389. Springer, 2007.   \nGasse, M., Chetelat, D., Ferroni, N., Charlin, L., and Lodi, A. Exact combinatorial optimization with graph convolutional neural networks. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch\u00e9-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/pap er/2019/file/d14c2267d848abeb81fd590f371d39bd-Paper.pdf.   \nGasse, M., Bowly, S., Cappart, Q., Charfreitag, J., Charlin, L., Ch\u00e9telat, D., Chmiela, A., Dumouchelle, J., Gleixner, A., Kazachkov, A. M., et al. The machine learning for combinatorial optimization competition (ml4co): Results and insights. In NeurIPS 2021 Competitions and Demonstrations Track, pp. 220\u2013231. PMLR, 2022.   \nGoerigk, M. Pesplib\u2013a benchmark library for periodic event scheduling, 2012.   \nGraham, R. L., Lawler, E. L., Lenstra, J. K., and Kan, A. R. Optimization and approximation in deterministic sequencing and scheduling: a survey. In Annals of discrete mathematics, volume 5, pp. 287\u2013326. Elsevier, 1979.   \nGurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2023. URL https://www.guro bi.com.   \nHan, Q., Yang, L., Chen, Q., Zhou, X., Zhang, D., Wang, A., Sun, R., and Luo, X. A gnn-guided predict-and-search framework for mixed-integer linear programming. In The Eleventh International Conference on Learning Representations, 2023.   \nIBM, I. I. C. O. V20. 1: User\u2019s manual for cplex. IBM Corp, 2020.   \nJohnson, D. S. Fast algorithms for bin packing. Journal of Computer and System Sciences, 8(3): 272\u2013314, 1974.   \nKaibel, V. and Pfetsch, M. Packing and partitioning orbitopes. Mathematical Programming, 114(1): 1\u201336, 2008.   \nKhalil, E. B., Morris, C., and Lodi, A. Mip-gnn: A data-driven framework for guiding combinatorial solvers. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 10219\u2013 10227, 2022.   \nKingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \nKuhn, H. W. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):83\u201397, 1955.   \nLi, L. and Wu, B. Learning to accelerate approximate methods for solving integer programming via early fixing. arXiv preprint arXiv:2207.02087, 2022.   \nLin, X., Hou, Z. J., Ren, H., and Pan, F. Approximate mixed-integer programming solution with machine learning technique and linear programming relaxation. In 2019 3rd International Conference on Smart Grid and Smart Cities (ICSGSC), pp. 101\u2013107, 2019. doi: 10.1109/ICSGSC.2019.00-11.   \nLiu, L. and Fan, Q. Resource allocation optimization based on mixed integer linear programming in the multi-cloudlet environment. IEEE Access, 6:24533\u201324542, 2018.   \nLuathep, P., Sumalee, A., Lam, W. H., Li, Z.-C., and Lo, H. K. Global optimization method for mixed transportation network design problem: a mixed-integer linear programming approach. Transportation Research Part B: Methodological, 45(5):808\u2013827, 2011.   \nMangasarian, O. L. Nonlinear programming. SIAM, 1994.   \nMargot, F. Pruning by isomorphism in branch-and-cut. Mathematical Programming, 94:71\u201390, 2002.   \nMargot, F. Exploiting orbits in symmetric ilp. Mathematical Programming, 98:3\u201321, 2003.   \nMargot, F. Symmetry in integer linear programming. 50 Years of Integer Programming 1958-2008: From the Early Years to the State-of-the-Art, pp. 647\u2013686, 2009.   \nNair, V., Bartunov, S., Gimeno, F., Von Glehn, I., Lichocki, P., Lobov, I., O\u2019Donoghue, B., Sonnerat, N., Tjandraatmadja, C., Wang, P., et al. Solving mixed integer programs using neural networks. arXiv preprint arXiv:2012.13349, 2020.   \nOstrowski, J., Linderoth, J., Rossi, F., and Smriglio, S. Orbital branching. Mathematical Programming, 126:147\u2013178, 2011.   \nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32, pp. 8024\u20138035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/9015-pytorch-a n-imperative-style-high-performance-deep-learning-library.pdf.   \nPfetsch, M. E. and Rehn, T. A computational comparison of symmetry handling methods for mixed integer programs. Mathematical Programming Computation, 11:37\u201393, 2019.   \nPochet, Y. and Wolsey, L. A. Production planning by mixed integer programming, volume 149. Springer, 2006.   \nSchaus, P., Van Hentenryck, P., Monette, J.-N., Coffrin, C., Michel, L., and Deville, Y. Solving steel mill slab problems with constraint-based techniques: Cp, lns, and cbls. Constraints, 16:125\u2013147, 2011.   \nSch\u00f6bel, A. A model for the delay management problem based on mixed-integer-programming. Electronic notes in theoretical computer science, 50(1):1\u201310, 2001.   \nSerafini, P. and Ukovich, W. A mathematical model for periodic scheduling problems. SIAM Journal on Discrete Mathematics, 2(4):550\u2013581, 1989. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "Sonnerat, N., Wang, P., Ktena, I., Bartunov, S., and Nair, V. Learning a large neighborhood search algorithm for mixed integer programs, 2022. Watson, J.-P. and Woodruff, D. L. Progressive hedging innovations for a class of stochastic mixedinteger resource allocation problems. Computational Management Science, 8(4):355\u2013370, 2011. Wu, Y., Song, W., Cao, Z., and Zhang, J. Learning large neighborhood search policy for integer programming. Advances in Neural Information Processing Systems, 34:30075\u201330087, 2021. Zhang, J., Liu, C., Li, X., Zhen, H.-L., Yuan, M., Li, Y., and Yan, J. A survey for solving mixed integer programming via machine learning. Neurocomputing, 519:205\u2013217, 2023. ", "page_idx": 11}, {"type": "text", "text": "A Theoretical proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Proposition 4.2 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Proof: When the loss function $\\ell(\\cdot,\\cdot)$ is the squared error (SE) or the binary cross-entropy loss (BCE), ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\ell_{S E}(\\hat{X},X P)=\\|\\hat{X}-X P\\|_{F}^{2}}\\\\ {\\displaystyle=\\mathrm{tr}(\\hat{X}^{\\top}\\hat{X}-2\\hat{X}^{\\top}X P+P^{\\top}X^{\\top}X P)}\\\\ {\\displaystyle=\\mathrm{tr}(\\hat{X}^{\\top}\\hat{X}-2\\hat{X}^{\\top}X P+X^{\\top}X),}\\\\ {\\displaystyle\\ell_{B C E}(\\hat{X},X P)=-\\sum_{j,k}([X P]_{j k}\\log\\hat{X}_{j k}+(1-[X P]_{j k})\\log(1-\\hat{X}_{j k})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "equations (13) to (14) hold for permutations of a matrix\u2019s rows and columns don not change its Frobenius norm, i.e., $\\operatorname{tr}(P^{\\top}X^{\\top}X P)=\\|X P\\|_{F}^{2}=\\|X\\|_{F}^{2}=\\operatorname{tr}(X^{\\top}X)$ . (14) and (15) show that these two loss functions are linear w.r.t $P$ , with which (8) becomes a linear assignment problem Kuhn (1955). It is easy to verify that the constriant matrix of a linear assignment problem is totally unimodular\u2014-it satisfies the four conditions of Hoffman and Gale (see Page 252 in Dantzig (1956)), thus an optimal solution of the relaxed problem (9) must be integral as well, namely an optimal solution to problem (8). ", "page_idx": 12}, {"type": "text", "text": "B ILP examples with different symmetry group ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Example B.0.1. Consider a bin packing problem, in which there are three items $I=\\{1,2,3\\}$ with sizes $\\{a_{1}=1,a_{2}=2,a_{3}=3\\}$ and three identical bins $J=\\{1,2,3\\}$ with capacity $B=3$ . Items are packed into bins, and it is required to use a minimum number of bins without exceeding the capacity. The specific formulation is as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\operatorname*{min}_{x_{i j},y_{j}\\in\\{0,1\\}}\\,\\,y_{1}+y_{2}+y_{3}}\\\\ {\\displaystyle a_{1}x_{1j}+a_{2}x_{2j}+a_{3}x_{3j}\\leq B y_{j},}\\\\ {\\displaystyle x_{i1}+x_{i2}+x_{i3}=1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\forall j\\in J}\\\\ {\\forall i\\in I}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $y_{j}=1$ denotes $j$ -th bin is used and $x_{i j}=1$ denotes $i$ -th item is placed in $j$ -th bin. ", "page_idx": 12}, {"type": "image", "img_path": "RtMyTzIW6l/tmp/1760e169a0a2daece38eff4d5c610d061c2697182cf395376b4e1be1c6dd73a6.jpg", "img_caption": ["Figure 5: Equivalent solutions of Example B.0.1. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "Since all bins are identical, arbitrarily swapping them does not change the feasibility and the objective value, e.g., the different assignments shown in Figure 5 are all equivalent. Specifically, assume ", "page_idx": 12}, {"type": "equation", "text": "$$\nX\\triangleq{\\left[\\begin{array}{l l l}{y_{1}}&{y_{2}}&{y_{3}}\\\\ {x_{11}}&{x_{12}}&{x_{13}}\\\\ {x_{21}}&{x_{22}}&{x_{23}}\\\\ {x_{31}}&{x_{32}}&{x_{33}}\\end{array}\\right]}={\\left[\\begin{array}{l l l}{1}&{1}&{0}\\\\ {0}&{1}&{0}\\\\ {0}&{1}&{0}\\\\ {1}&{0}&{0}\\end{array}\\right]}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "is an optimal solution to the problem (16), then ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{X}=\\left\\{\\left[\\begin{array}{l l l}{1}&{0}&{1}\\\\ {0}&{0}&{1}\\\\ {0}&{0}&{1}\\\\ {1}&{0}&{0}\\end{array}\\right],\\ \\left[\\begin{array}{l l l}{1}&{1}&{0}\\\\ {1}&{0}&{0}\\\\ {1}&{0}&{0}\\\\ {0}&{1}&{0}\\end{array}\\right],\\ \\left[\\begin{array}{l l l}{1}&{0}&{1}\\\\ {1}&{0}&{0}\\\\ {1}&{0}&{0}\\\\ {0}&{0}&{1}\\end{array}\\right],\\ \\left[\\begin{array}{l l l}{0}&{1}&{1}\\\\ {0}&{0}&{1}\\\\ {0}&{0}&{1}\\\\ {0}&{1}&{0}\\end{array}\\right],\\ \\left[\\begin{array}{l l l}{0}&{1}&{1}\\\\ {0}&{1}&{0}\\\\ {0}&{1}&{0}\\\\ {0}&{0}&{1}\\end{array}\\right]\\right\\}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "are all equivalent solutions to $X$ . Formally, this problem has a symmetric group $\\begin{array}{r l}{S_{3}}&{{}=}\\end{array}$ $\\{(1,2,3),(\\bar{1},3,2),(2,1,3),(2,3,1),(3,1,2),(\\bar{3,2},1)\\}$ w.r.t. its bin numbers $J$ . ", "page_idx": 12}, {"type": "text", "text": "Example B.0.2. Given a circle with circumference 8, place 3 ticks at integer points around the circle such that all distances between inter-ticks along the circumference are distinct. The formulation of this problem is as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{x_{1},x_{2},x_{3}}{\\operatorname*{min}}0}\\\\ &{\\quad\\quad s.t.\\;y_{i j}=|x_{i}-x_{j}|,}\\\\ &{\\quad\\quad\\quad d_{i j}=\\operatorname*{min}\\{y_{i j},8-y_{i j}\\},}\\\\ &{\\quad\\quad\\quad d_{12}\\neq d_{13},d_{12}\\neq d_{23},d_{13}\\neq d_{23}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\forall(i,j)\\in S,}\\\\ {\\forall(i,j)\\in S,}\\\\ {\\forall(i,j)\\in S,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $S\\,=\\,\\{(1,2),(1,3),(2,3)\\}$ , $x_{1},x_{2},x_{3}\\,\\in\\,\\{1,2,3,4,5,6,7,8\\}$ denote the positions of each tick, $d_{i j}$ are distances between ticks $i$ and $j$ with auxiliary variables $y_{i j}$ . The constraints in this formulation are nonlinear, we linearize them by big-M methods. Equalities (17a) can be linearized by introducing auxiliary variables $a_{i j}\\in\\{0,1\\},\\breve{\\forall}(i,\\bar{j})\\in S$ as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y_{i j}\\geq x_{i}-x_{j},}\\\\ &{y_{i j}\\geq x_{j}-x_{i},}\\\\ &{y_{i j}\\leq x_{i}-x_{j}+8\\cdot a_{i j},}\\\\ &{y_{i j}\\leq x_{j}-x_{i}+8\\cdot(1-a_{i j}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\forall(i,j)\\in S,}\\\\ {\\forall(i,j)\\in S,}\\\\ {\\forall(i,j)\\in S,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "when $x_{i}\\geq x_{j},a_{i j}=0$ , otherwise $a_{i j}=1$ . Similarly, equalities (17b) are equivalent to ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d_{i j}\\leq y_{i j},}\\\\ &{d_{i j}\\leq8-y_{i j},}\\\\ &{d_{i j}\\geq y_{i j}-8\\cdot m_{i j},}\\\\ &{d_{i j}\\geq8-y_{i j}-8\\cdot(1-m_{i j}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\forall(i,j)\\in S,}\\\\ {\\forall(i,j)\\in S,}\\\\ {\\forall(i,j)\\in S,}\\\\ {\\forall(i,j)\\in S,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $m_{i j}\\in\\{0,1\\},\\forall(i,j)\\in S$ are auxiliary variables, with $m_{i j}=0$ when $y_{i j}\\leq8-y_{i j}$ ", "page_idx": 13}, {"type": "text", "text": "The not-equal constraints (17c) can be linearized by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{d_{i j}\\geq d_{k\\ell}+1-8\\cdot t_{i j k\\ell},}}\\\\ {{d_{k\\ell}\\geq d_{i j}+1-8\\cdot(1-t_{i j k\\ell}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\forall(i,j,k,\\ell)\\in K,}\\\\ {\\forall(i,j,k,\\ell)\\in K,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $K\\ =\\ \\{(1,2,1,3),(1,2,2,3),(1,3,2,3)\\}$ . By introducing auxiliary variables $t_{i j k\\ell}~~\\in$ $\\{0,1\\},\\forall(i,j,k,\\stackrel{\\cdot}{\\ell})\\ \\in\\ K$ , we have $d_{i j}\\;\\geq\\;d_{k\\ell}+1$ if $t_{i j k\\ell}\\,=\\,1\\$ , otherwise $d_{i j}\\ \\leq\\ d_{k\\ell}\\,-\\,\\mathrm{\\bar{1}}$ , i.e., $d_{i j}\\neq d_{k\\ell}$ . ", "page_idx": 13}, {"type": "text", "text": "Assume $\\{x_{1}\\,=\\,\\bar{x}_{1},x_{2}\\,=\\,\\bar{x}_{2},x_{3}\\,=\\,\\bar{x}_{3}\\}$ is a feasible solution of this problem and let $[\\cdot]_{T}$ denote the $m o d-T$ operation, then it\u2019s easy to verify that $\\{x_{i}=[\\bar{x}_{i}+b]_{8}\\}_{i=1}^{3}$ (rotation) and its reverse $\\{x_{i}=[(8-\\bar{x}_{i}\\bar{)}+b]_{8}\\}_{i=1}^{3},\\forall\\,b\\in\\mathbb{Z}$ (reflection of the rotation) are both equivalent feasible solutions. It is more intuitive to see the illustration in Figure 6, rotation and reflection acting on the ticks do not change their corresponding distances. ", "page_idx": 13}, {"type": "text", "text": "When representing $x_{1},x_{2},x_{3}$ by binary variables $z_{i p}\\in\\{0,1\\},\\forall i\\in1,2,3,\\forall p\\in\\{1,\\dots,8\\}:$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{{x}_{i}=\\displaystyle\\sum_{p}^{8}p\\cdot{z}_{i p},}}&{{\\qquad\\qquad\\qquad}}&{{\\forall i\\in\\{1,2,3\\},}}\\\\ {{\\displaystyle\\sum_{p}^{8}{{z}_{i p}}=1,}}&{{\\qquad\\qquad}}&{{\\forall i\\in\\{1,2,3\\}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "the modulo symmetry leads to a dihedral group $D_{8}$ along the $p$ dimension of $z_{i p}$ . Specifically, let $Z$ be a feasible solution with its $(i,p)$ -th entry as the value of $z_{i p}$ , then any permutation $\\pi\\in D_{8}$ acting on the columns of $Z$ yields another equivalent solution $\\left[Z_{:\\pi(1)},\\dots,Z_{:\\pi(8)}\\right]$ . ", "page_idx": 13}, {"type": "image", "img_path": "RtMyTzIW6l/tmp/a2d11694fc4878cb35a55748e3d13b77f1d82ba1165f82f24c80f2899cdd2a7a.jpg", "img_caption": ["Figure 6: Equivalent solutions of Example B.0.2. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "C Bipartite graph representation for MILP ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Gasse et al. (2019) proposed to represent a MILP (also works for ILP) by a bipartite graph $\\mathcal{G}=$ $(V,C,E)$ with two disjoint sets of nodes, $V=\\{v_{1},v_{2},\\ldots,v_{n}\\}$ and $C=\\{c_{1},c_{2},...\\,,c_{m}\\}$ , denoting the decision variables and constraints in Problem (1), respectively. And $E=\\{A_{j k}|A_{j k}\\neq0,c_{j}\\in$ $C,v_{k}\\in V\\}$ is the set of weighted edges connecting variable nodes and constraint nodes, where $A$ is the coefficient matrix in Problem (1). Each node has a feature vector $v_{k}$ or $c_{j}$ describing the information of the variables or constraints. For example, in our experiments, these features include variable types (continuous or binary), variable positions, lower and upper bounds, right-hand side coefficients, constraint types $(=,\\leq,\\geq)$ , etc. ", "page_idx": 14}, {"type": "text", "text": "D Graph convolutional neural network ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In the graph convolutional neural network (GCNN)-based approach proposed by Gasse et al. (2019), a bipartite graph $\\mathcal{G}=(V,C,E)$ (with node features $c_{j}^{(0)}=c_{j,}v_{k}^{(0)}=v_{k}$ , and edge features $A_{j k.}$ ) is taken as the input. Stacked layers are applied to aggregate information from neighbors and update node embeddings. Each layer has two consecutive half convolutions computed as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{c_{j}^{(l)}=f_{c}^{(l)}\\left(c_{j}^{(l-1)},\\displaystyle{\\sum_{j:(j,k)\\in E}g_{c}^{(l)}\\left(c_{j}^{(l-1)},v_{k}^{(l-1)},A_{j k}\\right)}\\right),}\\\\ &{v_{k}^{(l)}=f_{v}^{(l)}\\left(v_{k}^{(l-1)},\\displaystyle{\\sum_{k:(j,k)\\in E}g_{v}^{(l)}\\left(c_{j}^{(l)},v_{k}^{(l-1)},A_{j k}\\right)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $l\\in\\{0,\\cdots\\,,L\\}$ denotes the layer index, $f_{c}^{(l)}$ and $g_{c}^{(l)}$ are non-linear transformations gathering information from variable nodes and update on constraint nodes, f v and $g_{v}^{(l)}$ are on the contrary. All of these four transformations are two-layer perceptrons with ReLU activations. Lastly, another twolayer perceptron $f_{o u t}$ with sigmoid activation is used to convert the final embeddings to the predictions of integer variables by $\\hat{x}_{k}=\\mathrm{Sigmoid}(f_{o u t}(v_{k}^{(L)}))$ . We denote $f_{\\theta}$ as the GNN parameterized by $\\theta$ and the output vector for the discrete part as $\\ \\hat{x}=f_{\\theta}(\\mathcal{G})$ in the remaining sections. ", "page_idx": 14}, {"type": "text", "text": "E Detailed experimental settings ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "E.1 Hyper-parameter tuning ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The experiments involved three downstream tasks: fix&optimize, local branching, and node selection. For the first two, we utilized grid search for hyperparameter tuning. In fix&optimize, we adjusted $\\alpha$ , the fraction of variables to fix, exploring values from 0.1 to 0.9. For local branching, we varied $\\beta$ , the percentage of variables in the local branching constraint, within the same range. For node selection, we adhered to default settings as stated in Khalil et al. (2022). In Table E.1, we report the optimal hyperparameters for each task and dataset, ensuring clarity and aiding in the reproducibility of our work. ", "page_idx": 14}, {"type": "text", "text": "Table 4: Hyper-parameters for different down-stream tasks ", "page_idx": 15}, {"type": "table", "img_path": "RtMyTzIW6l/tmp/b0b0049ab780b088a2df8cfd88da8188e165c7a7f70ed3d12fc2e0b81cf35692.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "E.2 Computational resources and software ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "All evaluations are performed under the same configuration. The evaluation machine has one AMD EPYC 7H12 64-Core Processor $\\textcircled{a}~2.60\\mathrm{GHz}$ , 256GB RAM, and one NVIDIA GeForce RTX 3080. CPLEX 22.2.0 and PyTorch 2.0.1 (Paszke et al., 2019) are utilized in our experiments. The time limit for running each experiment is set to 800 seconds since a tail-off of solution qualities was often observed after that. ", "page_idx": 15}, {"type": "text", "text": "F Other supplements ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "F.1 Dataset details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Table 5: More information about benchmark problems include average number of variables (\u201cbin.\u201d for bianry while \u201cint.\u201d for integer) and constraints, as well as symmetry groups. ", "page_idx": 15}, {"type": "table", "img_path": "RtMyTzIW6l/tmp/bd2692e552786e477610e5b614fdae8c67ed275325781cdb5a7bd6ab3bc187b7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "F.2 p-values for the significance of improvement ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We use the paired-sample T-test in MATLAB and report p-values in the following table. A p-value less than 0.05 means the mean difference (improvement) is significant. It shows that our proposed framework is effective in finding a better solution. ", "page_idx": 15}, {"type": "text", "text": "Table 6: p-values of paired-sample T-tests for the difference of means of the relative primal gaps ", "page_idx": 15}, {"type": "table", "img_path": "RtMyTzIW6l/tmp/934023e67b58ace04c564b555f4370c11b45ddd09d737bc6ffb95a322963d0ef.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "G Problem formulation and their corresponding symmetry group ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "G.1 IP ", "page_idx": 15}, {"type": "text", "text": "There are $I$ items, $J$ bins, and $K$ resource types. Each item $i$ has a fixed resource requirement $a_{i k}$ for each resource type $k$ . Each bin $j$ has a fixed capacity $b_{k}$ for each resource type $k$ . The goal is to place all items in bins, while minimizing the imbalance of the resources used across all bins. This ", "page_idx": 15}, {"type": "text", "text": "problem has a formulation as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\sum_{j\\in J}\\sum_{k\\in K}\\alpha_{k}y_{j k}+\\sum_{k\\in K}\\beta_{k}z_{k}}}\\\\ {{\\displaystyle\\sum_{j\\in J}x_{i j}=1}}\\\\ {{\\displaystyle\\sum_{i\\in I}\\alpha_{i}x_{i j}\\leq b_{k}}}\\\\ {{\\displaystyle\\sum_{i\\in I}\\alpha_{i}x_{i j}+y_{j k}\\geq1}}\\\\ {{\\displaystyle\\sum_{i\\in I}\\alpha_{i}\\leq z_{k}}}\\\\ {{\\displaystyle x_{i j}\\leq\\vdots}}\\\\ {{\\displaystyle x_{i j}\\in\\{0,1\\}}}\\\\ {{\\displaystyle y_{j k}\\geq0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\forall i\\in I\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\forall j\\in J,\\forall k\\in K\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\forall j\\in J,\\forall k\\in K\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\forall j\\in J,\\forall k\\in K}\\\\ {\\forall i\\in I,\\forall j\\in J}\\\\ {\\forall j\\in J,\\forall k\\in K}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $x_{i j}=1$ denotes assigning item $i$ to bin $j,\\,d_{i k}$ is normalized resource requirement for each item, $y_{j k}$ and $z_{k}$ are implicit decision variables to track the imbalance of the resources. Since each bin in this problem is identical (i.e., with the same capacity), reordering bins would not change a feasible solution\u2019s feasibility and objective value. This problem naturally has a symmetric group $S_{|J|}$ w.r.t. the ordering of bins $J$ . Specifically, let $X\\in\\{0,1\\}^{|I|\\times|J|}$ be a feasible solution of an IP instance with its $(i,j)$ -th entry as the value of variable $x_{i j}$ . Then arbitrary permutation $\\pi\\in S_{|J|}$ acting on its columns $\\{X_{:j},\\forall j\\in J\\}$ yields an equivalent solution $\\left[X_{:\\pi(1)},X_{:\\pi(2)},...\\,,X_{:\\pi(|J|)}\\right]$ . ", "page_idx": 16}, {"type": "text", "text": "G.2 SMSP ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Given order set $O$ , and slab set $S$ . Color set $C$ , and slab weights $Q=\\{u_{0}=0,u_{1},u_{2},...,u_{k}\\}$ . $u_{0}$ denotes unused slab. The ILP formulation of SMSP used in our experiments is from (Gargani $\\&$ Refalo, 2007) as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\underset{x,y,z}{\\operatorname*{min}}\\ \\sum_{\\stackrel{v=1}{x}}q\\times y_{\\theta^{s}}}\\\\ {\\underset{x,t\\leq C}{\\operatorname*{sup}}}&{}&{\\forall s\\in S,}\\\\ {\\underset{\\varphi\\in Q}{\\operatorname*{sup}}}&{=1}&{\\forall s\\in S,}\\\\ {\\underset{\\varphi\\in Q}{\\operatorname*{sup}}}&{=1}&{\\forall s\\in S,}\\\\ {\\underset{\\varphi\\in Q}{\\operatorname*{sup}}}&{\\leq\\underset{\\varphi\\in Q}{\\operatorname*{sup}}}&{\\forall s\\in S,}\\\\ {\\underset{x,o}{\\operatorname*{sup}}}&{\\leq z_{\\omega^{s}}}&{\\forall o\\in Q,s\\in S,}\\\\ {\\underset{x\\in C}{\\sum_{z\\in S}}\\leq2}&{\\forall s\\in S,}\\\\ {\\underset{x\\in S,\\|_{\\infty}}{\\operatorname*{sup}}}&{\\leq\\varphi,\\infty\\ \\mathrm{to},1)}&{\\forall o\\in Q,s\\in S,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "G.3 PESP ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Periodic event scheduling problem involves determining optimal schedules for a set of events that occur repeatedly over a fixed period, such as bus or train departures. Consider a set of events $\\mathcal{E}$ . For each event $i\\in\\mathcal{E}$ we would like to schedule a time $t_{i}\\,\\in\\,\\{1,\\ldots,T-1\\}$ , where $T$ is the periodic length. Besides, a set of activities $A\\subseteq\\mathcal{E}\\times\\mathcal{E}$ connect events with each other. Each activity $a\\in{\\mathcal{A}}$ has a lower bound $\\ell_{a}\\in\\mathbb{N}$ , an upper bound $u_{a}\\in\\mathbb{N}$ , and a weight $w_{a}$ . The goal is to minimize the weighted sum of the slack $y_{a}$ of all activities, while ensuring all activity slacks are within $[0,u_{a}-\\ell_{a}]$ . ", "page_idx": 16}, {"type": "text", "text": "It can be formulated as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{min}_{t}\\ \\sum_{a\\in A}w_{a}(y_{a}+\\ell_{a})}}\\\\ &{s.t.\\ y_{a}=[t_{j}-t_{i}]_{T}}&{\\qquad\\qquad}&{\\forall a=(i,j)\\in A,}\\\\ &{0\\leq y_{a}\\leq u_{a}-\\ell_{a}}&{\\qquad\\qquad}&{\\forall a\\in\\mathcal{A},}\\\\ &{t_{i}\\in\\{0,\\ldots,T-1\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $[\\cdot]_{T}$ denotes the modulo operation, which enforces the periodic nature. It is modeled as $[t_{j}\\,-\\,t_{i}]_{T}\\,\\triangleq\\,t_{j}\\,-\\,t_{i}\\,+\\,z_{a}T$ by introducing additional implicit variables $\\{z_{a}~\\in~\\mathbb{N}\\}$ . Due to the existence of modulo operation, we can regard $\\{t_{i},\\forall i\\,\\in\\,{\\bar{\\mathcal{E}}}\\}$ as frames in a clock with intervals $[0,\\dots,T-1]$ . If all $t_{i}$ rotate the same angles simultaneously, then the activity slacks $y_{a}$ remain unchanged. In our experimentation, we substitute $t_{i}$ by $\\textstyle t_{i}=\\sum_{k}(k-1)\\cdot x_{i k}$ and $\\textstyle\\sum_{k}x_{i k}=1$ , where $x_{i k}\\in\\{0,1\\},\\forall i\\in\\mathcal{E},k\\in\\{1,T\\}$ . Let $X\\in\\{0,1\\}^{|\\mathcal{E}|\\times T}$ denotes a feasible solution with its $(i,k)$ -th entry as the value of variable $x_{i k}$ , then this problem has a cyclic group $C_{T}$ w.r.t. the column indices of $X$ , i.e., any permutation $\\pi\\in C_{T}$ acting on the columns of $X$ yields an equivalent feasible solution $[X_{:\\pi(1)},...\\,,X_{:\\pi(T)}]$ . ", "page_idx": 17}, {"type": "text", "text": "G.3.1 Data generation by perturbation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "As mentioned above, a PESP instance has a set of events $\\mathcal{E}$ and a set of activities $A\\subseteq\\mathcal{E}\\times\\mathcal{E}$ connecting events with each other. Each activity has a weight $w_{a}$ . The goal is to assign an appropriate time $t_{i}$ to each event $i\\in\\mathcal{E}$ to meet some constraints while minimizing the total time slack weighted by $\\{w_{a},a\\in\\mathcal{A}\\}$ . These weights heavily impact the time assignment. We perturb these weights to generate new instances by introducing Gaussian noises, i.e., $w_{a}^{\\prime}=w_{a}+n_{a}$ , where $n_{a}\\sim\\bar{\\mathcal{N}}(\\mu=$ $w_{a},\\sigma=0.1*w_{a}\\mathrm{~]~}$ ). ", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Please refer to Abstract and Section 1. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: Please refer to Section 7. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: See Appendix A ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Please refer to Section 5 and Appendix E. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide a code link in Section 5. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/pu blic/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: See Section 5. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We report the p-values in Appendix F.2. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: See Appendix E.2. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification:The research presented in this paper adheres to the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our paper does not present any such risks. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Please refer to Section 5. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 22}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: See the code link in Section 5. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our paper does not involve any crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: Our paper does not involve any crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]