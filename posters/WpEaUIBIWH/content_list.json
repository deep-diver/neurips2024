[{"type": "text", "text": "Towards a Unified Framework of Clustering-based Anomaly Detection ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Unsupervised Anomaly Detection (UAD) plays a crucial role in identifying abnor  \n2 mal patterns within data without labeled examples, holding significant practical   \n3 implications across various domains. Although the individual contributions of   \n4 representation learning and clustering to anomaly detection are well-established,   \n5 their interdependencies remain under-explored due to the absence of a unified   \n6 theoretical framework. Consequently, their collective potential to enhance anomaly   \n7 detection performance remains largely untapped. To bridge this gap, in this paper,   \n8 we propose a novel probabilistic mixture model for anomaly detection to establish   \n9 a theoretical connection among representation learning, clustering, and anomaly   \n0 detection. By maximizing a novel anomaly-aware data likelihood, representation   \n11 learning and clustering can effectively reduce the adverse impact of anomalous   \n12 data and collaboratively benefti anomaly detection. Meanwhile, a theoretically sub  \n13 stantiated anomaly score is naturally derived from this framework. Lastly, drawing   \n14 inspiration from gravitational analysis in physics, we have devised an improved   \n15 anomaly score that more effectively harnesses the combined power of representa  \n16 tion learning and clustering. Extensive experiments, involving 17 baseline methods   \n17 across 30 diverse datasets, validate the effectiveness and generalization capability   \n18 of the proposed method, surpassing state-of-the-art methods. ", "page_idx": 0}, {"type": "text", "text": "19 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "20 Unsupervised Anomaly Detection (UAD) refers to the task dedicated to identifying abnormal patterns   \n21 or instances within data in the absence of labeled examples [8]. It has long received extensive   \n22 attention in the past decades for its wide-ranging applications in numerous practical scenarios,   \n23 including financial auditing [3], healthcare monitoring [44] and e-commerce sector [23]. Due to the   \n24 lack of explicit label guidance, the key to UAD is to uncover the dominant patterns that widely exist   \n25 in the dataset so that samples do not conform to these patterns can be recognized as anomalies. To   \n26 achieve this, early works [7] have heavily relied on powerful unsupervised representation learning   \n27 methods to extract the normal patterns from high-dimensional and complex data such as images, text,   \n28 and graphs. More recent works [45, 2] have utilized clustering, a widely observed natural pattern in   \n29 real-world data, to provide critical global information for anomaly detection and achieved tremendous   \n30 success.   \n31 While the individual contributions of representation learning and clustering to anomaly detection   \n32 are well-established, their interrelationships remain largely unexplored. Intuitively, discriminative   \n33 representation learning can leverage accurate clustering results to differentiate samples from distinct   \n34 clusters in the embedding space (i.e., $\\textcircled{1}$ ). Similarly, it can utilize accurate anomaly detection to   \n35 avoid preserving abnormal patterns (i.e., $\\textcircled{2}.$ ). For accurate clustering, it can gain advantages from   \n36 representation learning by operating in the discriminative embedding space (i.e., $\\circled{3}$ ). Meanwhile, it   \n37 can potentially benefit from accurate anomaly detection by excluding anomalies when formulating   \n38 clusters (i.e., $\\circled{4}$ ). Anomaly detection can greatly benefit from both discriminative representation   \n39 learning and accurate clustering (i.e., $\\circled{5}$ & $\\circled{6}$ ). However, these benefits hinge on the successful   \n40 identification of anomalies and the reduction of their detrimental impact on the aforementioned   \n41 tasks. As depicted in Figure 1, the integration of these three elements exhibits a significant reciprocal   \n42 nature. In summary, representation learning, clustering, and anomaly detection are interdependent and   \n43 intricately intertwined. Therefore, it is crucial for anomaly detection to fully leverage and mutually   \n44 enhance the relationships among these three components.   \n45 Despite the intuitive significance of the interactions among representation learning, clustering, and   \n46 anomaly detection, existing methods have only made limited attempts to exploit them and fall short   \n47 of expectations. On one hand, some methods [58] have acknowledged the interplay among these   \n48 three factors, but their focus remains primarily on the interactions between two factors at a time,   \n49 making only targeted improvements. For instance, some strategies include explicitly removing outlier   \n50 samples during the clustering process [9] or designing robust representation learning methods [10] to   \n51 mitigate the influence of anomalies. On the other hand, recent methods [45] have begun to explore   \n52 the simultaneous optimization of these three factors within a single framework. However, these   \n53 attempts are still in the stage of merely superimposing the objectives of the three factors without a   \n54 unified theoretical framework. This lack of a guiding framework prevents the adequate modeling of   \n55 the interdependencies among these factors, thereby limiting their collective contribution to a unified   \n56 anomaly detection objective. Consequently, we aim to address the following question: Is it possible   \n57 to employ a unified theoretical framework to jointly model these three interdependent objectives,   \n58 thereby leveraging their respective strengths to enhance anomaly detection?   \n59 In this paper, we try to answer this question and propose a novel model named UniCAD for anomaly   \n60 detection. The proposed UniCAD integrates representation learning, clustering, and anomaly de  \n61 tection into a unified framework, achieved through the theoretical guidance of maximizing the   \n62 anomaly-aware data likelihood. Specifically, we explicitly model the relationships between samples   \n63 and multiple clusters in the representation space using the probabilistic mixture models for the   \n64 likelihood estimation. Moreover, we creatively introduce a learnable indicator function into the   \n65 objective of maximum likelihood to explicitly attenuate the influence of anomalies on representation   \n66 learning and clustering. Under this framework, we can theoretically derive an anomaly score that   \n67 indicates the abnormality of samples, rather than heuristically designing it based on clustering results   \n68 as existing works do. Furthermore, building upon this theoretically supported anomaly score and   \n69 inspired by the theory of universal gravitation, we propose a more comprehensive anomaly metric that   \n70 considers the complex relationships between samples and multiple clusters. This allows us to better   \n71 utilize the learned representations and clustering results from this framework for anomaly detection. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "WpEaUIBIWH/tmp/4cf9a77feab7096c636397b7583dd43e588ec435f5319ab4b81c3316eb858917.jpg", "img_caption": ["Figure 1: Interdependent relationships among representation learning, clustering, and anomaly detection. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "72 To sum up, we underline our contributions as follows: ", "page_idx": 1}, {"type": "text", "text": "73 \u2022 We propose a unified theoretical framework to jointly optimize representation learning, clustering,   \n74 and anomaly detection, allowing their mutual enhancement and aid in anomaly detection.   \n75 \u2022 Based on the proposed framework, we derive a theoretically grounded anomaly score and further   \n76 introduce a more comprehensive score with the vector summation, which fully releases the power   \n77 of the framework for effective anomaly detection.   \n78 \u2022 Extensive experiments have been conducted on 30 datasets to validate the superior unsupervised   \n79 anomaly detection performance of our approach, which surpassed the state-of-the-art through   \n80 comparative evaluations with 17 baseline methods.   \n82 Typical unsupervised anomaly detection (UAD) methods calculate a continuous score for each sample   \n83 to measure its anomaly degree. Various UAD methods have been proposed based on different   \n84 assumptions, making them suitable for detecting various types of anomaly patterns, including   \n85 subspace-based models [24], statistical models [16], linear models [49, 32], density-based models [6,   \n86 38], ensemble-based models [39, 29], probability-based models [40, 58, 28, 27], neural network  \n87 based models [42, 51], and cluster-based models [18, 9]. Considering the field of anomaly detection   \n88 has progressed by integrating clustering information to enhance detection accuracy [26, 56], we   \n89 primarily focus on and analyze anomaly patterns related to clustering, incorporating a global clustering   \n90 perspective to assess the degree of anomaly. Notable methods in this context include CBLOF [18],   \n91 which evaluates anomalies based on the size of the nearest cluster and the distance to the nearest large   \n92 cluster. Similarly, DCFOD [45] introduces innovation by applying the self-training architecture of   \n93 the deep clustering [50] to outlier detection. Meanwhile, DAGMM [58] combines deep autoencoders   \n94 with Gaussian mixture models, utilizing sample energy as a metric to quantify the anomaly degree.   \n95 In contrast, our approach introduces a unified theoretical framework that integrates representation   \n96 learning, clustering, and anomaly detection, overcoming the limitations of heuristic designs and the   \n97 overlooked anomaly influence in existing methods. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "98 3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "99 In this section, we first define the problem we studied and the notations used in this paper. Then we   \n100 elaborate on the proposed method UniCAD. More specifically, we first introduce a novel learning   \n101 objective that optimizes representation learning, clustering, and anomaly detection within a unified   \n102 theoretical framework by maximizing the data likelihood. A novel anomaly score with theoretical   \n103 support is also naturally derived from this framework. Then, inspired by the concept of universal   \n104 gravitation, we further propose an enhanced anomaly scoring approach that leverages the intricate   \n105 relationship between samples and clustering to detect anomalies effectively. Finally, we present an   \n106 efficient iterative optimization strategy to optimize this model and provide a complexity analysis for   \n107 the proposed model.   \n108 Definition 1 (Unsupervised Anomaly Detection). Given a dataset $\\textbf{X}\\in\\,\\mathbb{R}^{N\\times D}$ comprising $N$   \n109 instances with $D$ -dimensional features, unsupervised anomaly detection aims to learn an anomaly   \n110 score $o_{i}$ for each instance $\\mathbf{x}_{i}$ in an unsupervised manner so that the abnormal ones have higher   \n111 scores than the normal ones. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "112 3.1 Maximizing Anomaly-aware Likelihood ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "113 Previous research has demonstrated the importance of discriminative representation and accurate   \n114 clustering in anomaly detection [45]. However, the presence of anomalous samples can significantly   \n115 disrupt the effectiveness of both representation learning and clustering [12]. While some existing   \n116 studies have attempted to integrate these three separate learning objectives, the lack of a unified   \n117 theoretical framework has hindered their mutual enhancement, leading to suboptimal results.   \n118 To tackle this issue, in this paper, we propose a unified and coherent approach that considers   \n119 representation learning, clustering, and anomaly detection by maximizing the likelihood of the   \n120 observed data. Specifically, we denote the parameters of representation learning as $\\Theta$ , the clustering   \n121 parameter as $\\Phi$ , and the dynamic indicator function for anomaly detection as $\\delta(\\cdot)$ . These parameters   \n122 are optimized simultaneously by maximizing the likelihood of the observed data $\\mathbf{X}$ : ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\log p(\\mathbf{X}|\\Theta,\\Phi)=\\operatorname*{max}\\sum_{i=1}^{N}\\delta(\\mathbf{x}_{i})\\log p(\\mathbf{x}_{i}|\\Theta,\\Phi)=\\operatorname*{max}\\sum_{i=1}^{N}\\delta(\\mathbf{x}_{i})\\log\\sum_{k=1}^{K}p(\\mathbf{x}_{i},c_{i}=k|\\Theta,\\Phi),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "123 where $c_{i}$ represents the latent cluster variable associated with $\\mathbf{x}_{i}$ , and $c_{i}=k$ denotes the probabilistic   \n124 event that $\\mathbf{x}_{i}$ belongs to the $k$ -th cluster. The $\\delta(\\mathbf{x}_{i})$ is an indicator function that determines whether a   \n125 sample $\\mathbf{x}_{i}$ is an anomaly of value 0 or a normal sample of value 1. ", "page_idx": 2}, {"type": "text", "text": "126 3.1.1 Joint Representation Learning and Clustering with $p(\\mathbf{x}_{i}|\\Theta,\\Phi)$ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "127 Based on the aforementioned advantages of MMs, we estimate the likelihood $p(\\mathbf{x}_{i}|\\Theta,\\Phi)$ with mixture   \n128 models defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{p({\\bf x}_{i}|\\Theta,\\Phi)=\\displaystyle\\sum_{k=1}^{K}p({\\bf x}_{i},c_{i}=k|\\Theta,\\Phi)=\\displaystyle\\sum_{k=1}^{K}p(c_{i}=k)\\cdot p({\\bf x}_{i}|c_{i}=k,\\Theta,\\mu_{k},\\Sigma_{k})}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\sum_{k=1}^{K}\\omega_{k}\\cdot p({\\bf x}_{i}|c_{i}=k,\\Theta,\\mu_{k},\\Sigma_{k}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "129 where $\\Phi=\\{\\omega_{k},\\mu_{k},\\Sigma_{k}\\}$ . The mixture model is parameterized by the prototypes $\\pmb{\\mu}_{k}$ , covariance   \n130 matrices $\\Sigma_{k}$ , and mixture weights $\\omega_{k}$ from all clusters. $\\textstyle\\sum_{k=1}^{K}\\omega_{k}=1$ , and $k=1,2,\\cdots\\,,K$ .   \n131 In practice, the samples are usually attributed to high-dimensional features and it is challenging to   \n132 detect anomalies from the raw feature space [41]. Therefore, modern anomaly detection methods [42,   \n133 58] often map raw data samples $\\mathbf{X}=\\mathbf{\\bar{\\{x}}}_{i}\\mathbf{\\}\\in\\mathbb{R}^{N\\times D}$ into a low-dimensional representation space   \n134 ${\\bf Z}=\\{{\\bf z}_{i}\\}\\in\\mathbb{R}^{N\\times d}$ with a representation learning function $\\mathbf{z}_{i}=f_{\\Theta}(\\mathbf{x}_{i})$ and detect anomalies within   \n135 this latent representation space.   \n136 Following this widely adopted practice, we model the distribution of samples in the latent represen  \n137 tation space with a multivariate Student\u2019s- $\\cdot t$ distribution giving its cluster $c_{i}=k$ . The Student\u2019s- $\\cdot t$   \n138 distribution is robust against outliers due to its heavy tails. Bayesian robustness theory leverages   \n139 such distributions to dismiss outlier data, favoring reliable sources, making the Student\u2019s- $\\cdot t$ process   \n140 preferable over Gaussian processes for data with atypical information [1]. Thus the probability   \n141 distribution of generating $\\mathbf{x}_{i}$ with latent representation $\\mathbf{z}_{i}$ given its cluster $c_{i}=k$ can be expressed as: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\np(\\mathbf{x}_{i}|c_{i}=k,\\Theta,\\mu_{k},\\Sigma_{k})=\\frac{\\Gamma(\\frac{\\nu+1}{2})|\\Sigma_{k}|^{-1/2}}{\\Gamma(\\frac{\\nu}{2})\\sqrt{\\nu\\pi}}\\left(1+\\frac{1}{\\nu}D_{M}(\\mathbf{z}_{i},\\mu_{k})^{2}\\right)^{-\\frac{\\nu+1}{2}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "142 where $\\mathbf{z}_{i}=f_{\\Theta}(\\mathbf{x}_{i})$ denotes the representation obtained from the data mapped through the neural   \n143 network parameterized by $\\Theta$ . $\\Gamma$ denotes the gamma function while $\\nu$ is the degree of freedom.   \n144 $\\Sigma_{k}$ is the scale parameter. $D_{M}(\\mathbf{z}_{i},\\mu_{k})=\\sqrt{(\\mathbf{z}_{i}-\\mu_{k})^{T}\\Sigma_{k}^{-1}(\\mathbf{z}_{i}-\\mu_{k})}$ represents the Mahalanobis   \n145 distance [33]. In the unsupervised setting, as cross-validating $\\nu$ on a validation set or learning it is   \n146 unnecessary, $\\nu$ is set as 1 for all experiments [50, 48]. The overall marginal likelihood of the observed   \n147 data $\\mathbf{x}_{i}$ can be simplified as: ", "page_idx": 3}, {"type": "equation", "text": "$$\np(\\mathbf{x}_{i}|\\Theta,\\Phi)=\\sum_{k=1}^{K}\\omega_{k}\\cdot\\frac{\\pi^{-1}\\cdot|\\Sigma_{k}|^{-1/2}}{1+D_{M}(\\mathbf{z}_{i},\\pmb{\\mu}_{k})^{2}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "148 3.1.2 Anomaly Indicator $\\delta(\\mathbf{x}_{i})$ and Score $o_{i}$ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "149 As we have discussed, the indicator function $\\delta(\\mathbf{x}_{i})$ not only beneftis both representation and clustering   \n150 but also directly serves as the output of anomaly detection. Ideally, with the percentage of outliers   \n151 denoted as $l$ , an optimal solution for $\\delta(\\mathbf{x}_{i})$ that maximizes the objective function $J(\\Theta,\\Phi)$ entails   \n152 setting all $\\delta(\\mathbf{x}_{i})\\;=\\;0$ for $\\mathbf{x}_{i}$ among the $l$ percent of outliers with lowest generation possibility   \n153 $p(\\mathbf{x}_{i}|\\bar{\\Theta},\\Phi)$ , and otherwise $\\delta(\\mathbf{x}_{i})\\;=\\;1$ is set for the remaining normal samples. Therefore, the   \n154 indicator function is determined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\delta(\\mathbf{x}_{i})={\\binom{0,}{1,}}\\quad{\\mathrm{if~}}p(\\mathbf{x}_{i}|\\Theta,\\Phi){\\mathrm{~is~among~the~}}l{\\mathrm{~lowest}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "155 As this method involves sorting the samples based on the generation probability as being anomalous,   \n156 the values of $p(\\mathbf{x}_{i}|\\Theta,\\Phi)$ can serve as a form of anomaly score, a classic approach within the mixture   \n157 model framework [40, 58]. This suggests that the likelihood of a sample being anomalous is inversely   \n158 related to its generative probability since a lower generative probability indicates a higher chance of   \n159 the sample being an outlier. Thus the anomaly score of sample $\\mathbf{x}_{i}$ can be defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\no_{i}=\\frac{1}{p(\\mathbf{x}_{i}|\\Theta,\\Phi)}=\\frac{1}{\\sum_{k=1}^{K}\\omega_{k}\\cdot\\frac{\\pi^{-1}\\cdot|\\Sigma_{k}|^{-1/2}}{1+D_{M}(\\mathbf{z}_{i},\\mu_{k})^{2}}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "160 3.2 Gravity-inspired Anomaly Scoring ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "161 In practical applications, it is proved that anomaly scores derived from generation probabilities often   \n162 yield suboptimal performance [17]. This observation prompts a reconsideration of how to fully   \n163 leverage the complex relationships among samples or even across multiple clusters for anomaly   \n164 detection. In this section, we first provide a brief introduction to the concept of Newton\u2019s Law of   \n165 Universal Gravitation [35] and then demonstrate how the anomaly score is intriguingly similar to this   \n166 cross-field principle. Finally, we discuss the advantages of introducing the vector sum operation into   \n167 the anomaly score inspired by the analogy. ", "page_idx": 4}, {"type": "text", "text": "168 3.2.1 Analog Anomaly Scoring and Force Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "169 To begin with, Newton\u2019s Law of Universal Gravitation [35] stands as a fundamental framework for   \n170 describing the interactions among entities in the physical world. According to this law, every object   \n171 in the universe experiences an attractive force from another object. In classical mechanics, force   \n172 analysis involves calculating the vector sum of all forces acting on an object, known as the resultant   \n173 force, which is crucial in determining an object\u2019s acceleration or change in motion: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\vec{\\bf F}_{i,\\mathrm{total}}=\\sum_{k=1}^{K}\\vec{\\bf F}_{i k},\\;\\;\\mathrm{with}\\;\\vec{\\bf F}_{i k}=\\frac{G\\cdot m_{i}m_{k}}{r_{i k}^{2}}\\cdot\\vec{\\bf r}_{i k},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "174 where $\\vec{\\mathbf{F}}_{i k}$ represents the $k$ -th force acting on the object $i$ . This force is proportional to the product of   \n175 their masses, ${\\bf\\nabla}m_{i}$ and $m_{k}$ ), and inversely proportional to the square of the distance $r_{i k}$ between them.   \n176 $G$ represents the gravitational constant, and $\\vec{\\bf r}_{i j}$ is the unit direction vector.   \n177 Similarly, if denoting: $\\begin{array}{r}{\\widetilde{\\mathbf{F}}_{i k}=p(\\mathbf{x}_{i},c_{i}=k|\\Theta,\\Phi)=\\boldsymbol{\\omega}_{k}\\cdot\\frac{\\pi^{-1}\\cdot|\\Sigma_{k}|^{-1/2}}{1+D_{M}(\\mathbf{z}_{i},\\pmb{\\mu}_{k})^{2}}}\\end{array}$ , the score of Equation (6)   \n178 bears analogies to the summation of the magnitudes of forces as: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\no_{i}=\\frac{1}{\\sum_{k=1}^{K}\\widetilde{\\mathbf{F}}_{i k}},\\;\\;\\mathrm{with}\\;\\widetilde{\\mathbf{F}}_{i k}=\\frac{\\widetilde{G}\\cdot\\widetilde{m}_{i}\\widetilde{m}_{k}}{\\widetilde{r}_{i k}^{2}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "179 where $\\widetilde G=\\pi^{-1}$ , $\\widetilde{m}_{k}=\\omega_{k}|\\Sigma_{k}|^{-1/2}$ , $\\widetilde{m}_{i}=1$ , and $\\widetilde{r}_{i k}=\\sqrt{1+D_{M}(\\mathbf{z}_{i},\\pmb{\\mu}_{k})^{2}}$ . Here, $\\approx_{i k}$ is taken as   \n180 the measure of distance within the representation space, modified slightly by an additional term for   \n181 smoothness. The constant $\\widetilde{G}$ serves a role akin to the gravitational constant in this analogy, whereas   \n182 $\\widetilde{m}_{k}$ resembles the concept  of mass for the cluster. The notation ${\\widetilde{m}}_{i}$ suggests a standardization where   \n183 the mass of each data point is considered uniform and not differentiated. ", "page_idx": 4}, {"type": "text", "text": "184 3.2.2 Anomaly Scoring with Vector Sum ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "185 Comparing Equation (7) with Equation (8), what still differs is that, unlike a simple sum of the   \n186 scalar value, the resultant force $\\vec{\\bf F}_{i,\\mathrm{total}}$ employs the vector sum and incorporates both the magnitude   \n187 and direction $\\widehat{\\mathbf{r}}_{i k}$ of each force. This distinction is crucial because forces in different directions   \n188 can neutralize each other with a large angle between them or enhance each other\u2019s effects with a   \n189 small angle. Inspired by this difference, we consider modeling the relationship between samples and   \n190 clusters as a vector, and aggregating them through vector summation. The vector-formed anomaly   \n191 score $o_{i}^{V}$ is defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\no_{i}^{V}=\\frac{1}{\\Vert\\sum_{k=1}^{K}\\widetilde{\\mathbf{F}}_{i k}\\cdot\\vec{\\mathbf{r}}_{i k}\\Vert},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "192 where $\\vec{\\bf r}_{i k}$ represents the unit direction vector in the representation space from the sample $\\mathbf{z}_{i}$ to the   \n193 cluster prototype $\\pmb{\\mu}_{k}$ , and $\\|\\cdot\\|$ represents the $L_{2}$ norm. ", "page_idx": 4}, {"type": "text", "text": "194 3.3 Iterative Optimization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "195 Given the challenge posed by the interdependence of the parameters of the network $\\Theta$ and those of the   \n196 mixture model $\\bar{\\{\\omega_{k},\\mu_{k},\\Sigma_{k}\\}}$ in joint optimization, we propose an iterative optimization procedure.   \n197 The pseudocode for training the model is presented in Algorithm 1 in the appendix. ", "page_idx": 5}, {"type": "text", "text": "198 3.3.1 Update $\\Phi$ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "199 To update the parameters of the mixture model $\\Phi\\ =\\ \\{\\omega_{k},\\pmb{\\mu}_{k},\\Sigma_{k}\\}$ , we use the Expectation  \n200 Maximization (EM) algorithm to maximize equation (1) [36]. The detailed derivation is included in   \n201 Appendix B.   \n202 $\\mathbf{E}$ -step. During the E-step of iteration $(t+1)$ , our goal is to compute the posterior probabilities of   \n203 each data point belonging to the $k$ -th cluster within the mixture model. Given the observed sample   \n204 $\\mathbf{x}_{i}$ and the current estimates of the parameters $\\Theta^{(t)}$ and $\\Phi^{(t)}$ , the expected value of the likelihood   \n205 function of latent variable $c_{k}$ , or the posterior possibilities, can be expressed as: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tau_{i k}^{(t+1)}=p(c_{i}=k|\\mathbf{x}_{i},\\boldsymbol{\\Theta},\\boldsymbol{\\Phi}^{(t)})=\\frac{p(\\mathbf{x}_{i},c_{i}=k|\\boldsymbol{\\Theta},\\boldsymbol{\\Phi}^{(t)})}{\\sum_{j=1}^{K}p(\\mathbf{x}_{i},c_{i}=j|\\boldsymbol{\\Theta},\\boldsymbol{\\Phi}^{(t)})}=\\frac{\\widetilde{\\mathbf{F}}_{i k}^{(t)}}{\\sum_{j=1}^{K}\\widetilde{\\mathbf{F}}_{i j}^{(t)}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "206 The scale factor[36] serving as an intermediate result for subsequent updates in the M-step is : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{u}_{i k}^{(t+1)}=\\frac{2}{1+D_{M}(\\mathbf{z}_{i}^{(t)},\\pmb{\\mu}_{k}^{(t)})}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "207 M-step. In the M-step of iteration $(t+1)$ , given the gradients $\\begin{array}{r}{\\frac{\\partial J(\\Theta,\\Phi)}{\\partial\\omega_{k}}\\,=\\,0}\\end{array}$ \u2202J(\u0398,\u03a6) = 0, and   \n208 \u2202J\u2202(\u03a3\u0398k,\u03a6)= 0, we derive the analytical solutions for the mixture model parameters \u03c9k, \u00b5k, and \u03a3k.   \n209 Assume the anomalous ratio is $l\\in[0,1]$ , the number of the normal samples is $n=\\mathrm{int}(l*N)$ . The   \n210 updating process for $\\{\\boldsymbol{\\omega}_{k}^{(t+1)},\\boldsymbol{\\mu}_{k}^{(t+1)},\\boldsymbol{\\Sigma}_{k}^{(t+1)}\\}$ is as follows:   \n211 \u2022 The mixture weights $\\omega_{k}$ are updated by averaging the posterior probabilities over all data points   \n212 with the number of samples , reflecting the relative presence of each component in the mixture: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\omega_{k}^{(t+1)}=\\sum_{i=1}^{n}\\tau_{i k}^{(t+1)}/n.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "213 \u2022 The prototypes $\\pmb{\\mu}_{k}$ are updated to be the weighted average of the data points, where weights are the   \n214 posterior probabilities: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pmb{\\mu}_{k}^{(t+1)}=\\sum_{i=1}^{n}\\left(\\tau_{i k}^{(t+1)}\\mathbf{u}_{i k}^{(t+1)}\\mathbf{z}_{i}\\right)/\\sum_{i=1}^{n}\\left(\\tau_{i k}^{(t+1)}\\mathbf{u}_{i k}^{(t+1)}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "215 \u2022 The covariance matrices $\\Sigma_{k}$ are updated by considering the dispersion of the data around the newly   \n216 computed prototypes: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{\\Sigma}_{k}^{(t+1)}=\\frac{\\sum_{i=1}^{n}\\tau_{i k}^{(t+1)}\\mathbf{u}_{i k}^{(t+1)}\\left(\\mathbf{z}_{i}-\\pmb{\\mu}_{k}^{(t+1)}\\right)\\left(\\mathbf{z}_{i}-\\pmb{\\mu}_{k}^{(t+1)}\\right)\\tau}{\\sum_{j=1}^{K}\\tau_{i j}^{(t+1)}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "217 3.3.2 Update $\\Theta$ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "218 We focus on anomaly-aware representation learning and use stochastic gradient descent to optimize   \n219 the network parameters $\\Theta$ , by minimizing the following joint loss: ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\mathcal{L}}=-J(\\Theta,\\Phi)+g(\\Theta),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "220 where $J(\\Theta,\\Phi)=\\log p(\\mathbf{X}|\\Theta,\\Phi)$ . An additional constraint term $g(\\Theta)$ is introduced to prevent short  \n221 cut solution [15]. In practice, an autoencoder architecture is implemented, utilizing a reconstruction   \n222 loss $g(\\Theta)=\\overline{{\\|\\boldsymbol{x}-\\boldsymbol{\\hat{x}}\\|^{2}}}$ as the constraint.   \n223 These updates are iteratively performed until convergence, resulting in optimized model parameters   \n224 that best fit the given data according to the mixture model framework. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "225 4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "226 4.1 Datasets & Baselines ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "227 We evaluated UniCAD on an extensive collection of datasets, comprising 30 tabular datasets that   \n228 span 16 diverse fields. We specifically focused on naturally occurring anomaly patterns, rather   \n229 than synthetically generated or injected anomalies, as this aligns more closely with real-world   \n230 scenarios. The detailed descriptions are provided in Table 4 of Appendix D.1. Following the setup   \n231 in ADBench [17], we adopt an inductive setting to predict newly emerging data, a highly beneficial   \n232 approach for practical applications.   \n233 To assess the effectiveness of UniCAD, we compared it with 17 advanced unsupervised anomaly   \n234 detection methods, including: (1) traditional methods: SOD [24] and HBOS [16]; (2) linear methods:   \n235 PCA [49] and OCSVM [32]; (3) density-based methods: LOF [6] and KNN [38]; (4) ensemble-based   \n236 methods: LODA [39] and IForest [29]; (5) probability-based methods: DAGMM [58], ECOD [28],   \n237 and COPOD [27]; (6) cluster-based methods: DBSCAN [13], CBLOF [18], DCOD [45] and KMeans  \n238 - [9]; and (7) neural network-based methods: DeepSVDD [42] and DIF [51]. These baselines   \n239 encompass the majority of the latest methods, providing a comprehensive overview of the state-of  \n240 the-art. For a detailed description, please refer to Appendix D.2. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "241 4.2 Experiment Settings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "242 In the unsupervised setting, we employ the default hyperparameters from the original papers for all   \n243 comparison methods. Similarly, the UniCAD also utilizes a fixed set of parameters to ensure a fair   \n244 comparison. For all datasets, we employ a two-layer MLP with a hidden dimension of $d=128$ and   \n245 ReLU activation function as both encoder and decoder. We utilize the Adam optimizer [21] with a   \n246 learning rate of $1e^{-4}$ for 100 epochs. For the EM process, we set the maximum iteration number   \n247 to 100 and a tolerance of $1e^{-3}$ for stopping training when the objectives converge. The number of   \n248 components in the mixture model is set as $k=10$ , and the proportion of the outlier is set as $l=1\\%$ .   \n249 We evaluate the methods using Area Under the Receiver Operating Characteristic (AUC-ROC) and   \n250 Area Under the Precision-Recall Curve (AUC-PR) metrics [17], reporting the average ranking (Avg.   \n251 Rank) across all datasets. All experiments are run 3 times with different seeds, and the mean results   \n252 are reported. ", "page_idx": 6}, {"type": "text", "text": "253 4.3 Performance and Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "254 Performance Comparison. Table 1 presents a comparison of UniCAD with 10 unsupervised   \n255 baseline methods across 30 tabular datasets using the AUC-ROC metric. The experimental results,   \n256 which encompass 17 baselines, are included in Tables 5 and 6 of Appendix D.3, with additional   \n257 experiments on other data domains presented in Appendix E. Our proposed UniCAD achieves the   \n258 top average ranking, exhibiting the best or near-best performance on a larger number of datasets   \n259 and confirming advanced capabilities. It is noteworthy that there is no one-size-ftis-all unsupervised   \n260 anomaly detection method suitable for every type of dataset, as demonstrated by the observation that   \n261 other methods have also achieved some of the best results on certain datasets. However, our model   \n262 showcased a remarkable ability to generalize across most datasets featuring natural anomalies, as   \n263 evidenced by statistical average ranking. As for clustering-based methods such as KMeans--, DCOD,   \n264 and CBLOF, they mostly rank in the top tier among all baseline methods, supporting the advantage of   \n265 combining deep clustering with anomaly detection. However, our method significantly outperformed   \n266 these methods by mitigating their limitations and further providing a unified framework for joint   \n267 representation learning, clustering, and anomaly detection.   \n268 Effectiveness of Vector Sum in Anomaly Scoring. As demonstrated in Table 1, we compare the   \n269 anomaly score $\\mathbf{o}_{i}$ derived directly from the generation possibility with its vector summation form $\\mathbf{o}_{i}^{V}$ .   \n270 According to our statistical findings, we observe that vector scores $\\mathbf{o}_{i}^{V}$ consistently outperform scalar   \n271 scores $\\mathbf{o}_{i}$ . This indicates that the introduction of the vector summation, analogous to the concept   \n272 of resultant force, makes a substantial difference in anomaly detection scenarios involving multiple   \n273 clusters. The performance gains of the vector sum scores strongly demonstrate the effectiveness   \n274 of the UniCAD in capturing the subtle differences in the distinctions among multiple clusters and   \n275 underscore the utility of this factor in the context of anomaly detection based on clustering.   \n276 Analysis of EM Iterative Optimization. To comprehend the iterative training within our model,   \n277 we have illustrated the performance variations accompanying the increase in iteration counts in   \n278 Figure 2a. Specifically, we monitored the iteration number $t$ for the satimage-2 dataset, ranging   \n279 from 0 to 10, while maintaining other default parameters constant. Both AUC-ROC and AUC-PR   \n280 performance curves displayed consistent trends, with minor fluctuations only during the initial phase.   \n281 The performance remained relatively stable throughout the last steps, illustrating the effectiveness   \n282 and convergence of iterative EM optimization.   \n283 Runtime Comparison. We present a analysis of the runtime performance of various methods,   \n284 including our proposed approach, as detailed in Table 2. Our experiments, conducted on the backdoor   \n285 dataset, reveal that while non-deep learning methods exhibit lower runtime, they often simplify the   \n286 problem space excessively, failing to capture the complex non-linear relationships present in the   \n287 data. In contrast, our method, when compared to existing deep learning techniques, demonstrates   \n288 a significant reduction in computational time. This indicates that our approach not only manages   \n289 to efficiently model complex patterns but also achieves an optimal balance between computational   \n290 efficiency and modeling capability. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "", "table_caption": ["Table 1: AUCROC of 10 unsupervised algorithms on 30 tabular benchmark datasets. In each dataset, the algorithm with the highest AUCROC is marked in red, the second highest in blue, and the third highest in green. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "WpEaUIBIWH/tmp/7f4a7077d077a72f8b4de07387d14ca7491b71f34b82d804b6a147b58dbe32b6.jpg", "img_caption": ["Figure 2: (a) demonstrates the performance variations during the optimization process on the satimage2 dataset. (b) & (c) Analysis of cluster count $k$ , anomaly ratio $l$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "WpEaUIBIWH/tmp/0fce84f54e85a587ab9cc804bd869b947d59887de7d8ef51413c8e091b1ae850.jpg", "table_caption": ["Table 2: Runtime Comparison. The runtime is reported in seconds (s). "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "WpEaUIBIWH/tmp/96bfa85bca331917557d6052b96dc7f3edb851f4d9e1069a760c9af752e13ab2.jpg", "table_caption": ["Table 3: Ablation study on AUC-ROC scores, calculated across 30 datasets. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "291 4.4 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "292 In this section, we examine the contributions of different components in UniCAD. Tables 3 reports the   \n293 results. We make three major observations. Firstly, the anomaly detection performance experiences a   \n294 significant drop when replacing the Student\u2019s t distribution with a Gaussian distribution for the Mixture   \n295 Model, highlighting the robustness of the Student\u2019s t distribution in unsupervised anomaly detection.   \n296 Secondly, omitting the likelihood maximization loss (w/o $J(\\Theta,\\Phi))$ also results in a considerable   \n297 decrease in overall performance. This observation underscores the importance of deriving both   \n298 the optimization objectives and anomaly scores from the likelihood generation probability through   \n299 a theoretical framework, which allows for unified joint optimization of anomaly detection and   \n300 clustering in the representation space. Furthermore, the indicator function $\\delta(\\mathbf{x}_{i})$ also contributes to a   \n301 performance increase. These results further confirm the effectiveness of our UniCAD in mitigating the   \n302 negative influence of anomalies in the clustering process, as the existence of outliers may significantly   \n303 degrade the performance of clustering. In summary, all these ablation studies clearly demonstrate   \n304 the effectiveness of our theoretical framework in simultaneously considering representation learning,   \n305 clustering, and anomaly detection. ", "page_idx": 8}, {"type": "text", "text": "306 4.5 Sensitivity of Hyperparameters ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "307 In this section, we conducted a sensitivity analysis on key hyperparameters of the model applied   \n308 to the donors dataset, focusing on the number of clusters $k$ and the proportion of the outlier set $l$ .   \n309 The results of this analysis are illustrated in Figure 2. Notably, the optimal range for $l$ tends to be   \n310 lower than the actual proportion of anomalies in the dataset. Furthermore, a pattern was observed   \n311 with the number of clusters $k$ , where the model performance initially improved with an increase in $k$ ,   \n312 followed by a subsequent decline. This suggests the existence of an optimal range for the number of   \n313 clusters, which should be carefully selected based on the specific application context. ", "page_idx": 8}, {"type": "text", "text": "314 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "315 This paper presents UniCAD, a novel model for Unsupervised Anomaly Detection (UAD) that   \n316 seamlessly integrates representation learning, clustering, and anomaly detection within a unified   \n317 theoretical framework. Specifically, UniCAD introduces an anomaly-aware data likelihood based on   \n318 the mixture model with the Student-t distribution to guide the joint optimization process, effectively   \n319 mitigating the impact of anomalies on representation learning and clustering. This framework   \n320 enables a theoretically grounded anomaly score inspired by universal gravitation, which considers   \n321 complex relationships between samples and multiple clusters. Extensive experiments on 30 datasets   \n322 across various domains demonstrate the effectiveness and generalization capability of UniCAD,   \n323 surpassing 15 baseline methods and establishing it as a state-of-the-art solution in unsupervised   \n324 anomaly detection. Despite its potential, the proposed method\u2019s applicability to broader fields like   \n325 time series and multimodal anomaly detection requires further exploration and validation, highlighting   \n326 a significant area for future work. ", "page_idx": 8}, {"type": "text", "text": "327 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "328 [1] J Ailton A Andrade. On the robustness to outliers of the student-t process. Scandinavian   \n329 Journal of Statistics, 50(2):725\u2013749, 2023.   \n330 [2] Caglar Aytekin, Xingyang Ni, Francesco Cricri, and Emre Aksu. Clustering and unsupervised   \n331 anomaly detection with l 2 normalized deep auto-encoder representations. In 2018 International   \n332 Joint Conference on Neural Networks (IJCNN), pages 1\u20136. IEEE, 2018.   \n333 [3] Alexander Bakumenko and Ahmed Elragal. Detecting anomalies in financial data using machine   \n334 learning algorithms. Systems, 10(5):130, 2022.   \n335 [4] Sambaran Bandyopadhyay, Saley Vishal Vivek, and MN Murty. Outlier resistant unsupervised   \n336 deep architectures for attributed network embedding. In Proceedings of the 13th international   \n337 conference on web search and data mining, pages 25\u201333, 2020.   \n338 [5] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.   \n339 Translating embeddings for modeling multi-relational data. Advances in neural information   \n340 processing systems, 26, 2013.   \n341 [6] Markus M Breunig, Hans-Peter Kriegel, Raymond T $\\mathrm{Ng}$ , and J\u00f6rg Sander. Lof: identifying   \n342 density-based local outliers. In Proceedings of the 2000 ACM SIGMOD international conference   \n343 on Management of data, pages 93\u2013104, 2000.   \n344 [7] Raghavendra Chalapathy and Sanjay Chawla. Deep learning for anomaly detection: A survey.   \n345 arXiv preprint arXiv:1901.03407, 2019.   \n346 [8] Varun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection: A survey. ACM   \n347 computing surveys (CSUR), 41(3):1\u201358, 2009.   \n348 [9] Sanjay Chawla and Aristides Gionis. k-means\u2013: A unified approach to clustering and outlier   \n349 detection. In Proceedings of the 2013 SIAM international conference on data mining, pages   \n350 189\u2013197. SIAM, 2013.   \n351 [10] Hyunsoo Cho, Jinseok Seol, and Sang-goo Lee. Masked contrastive learning for anomaly   \n352 detection. arXiv preprint arXiv:2105.08793, 2021.   \n353 [11] Kaize Ding, Jundong Li, Rohit Bhanushali, and Huan Liu. Deep anomaly detection on attributed   \n354 networks. In Proceedings of the 2019 SIAM International Conference on Data Mining, pages   \n355 594\u2013602. SIAM, 2019.   \n356 [12] Lian Duan, Lida Xu, Ying Liu, and Jun Lee. Cluster-based outlier detection. Annals of   \n357 Operations Research, 168:151\u2013168, 2009.   \n358 [13] Martin Ester, Hans-Peter Kriegel, J\u00f6rg Sander, Xiaowei Xu, et al. A density-based algorithm   \n359 for discovering clusters in large spatial databases with noise. In kdd, volume 96, pages 226\u2013231,   \n360 1996.   \n361 [14] Haoyi Fan, Fengbin Zhang, and Zuoyong Li. Anomalydae: Dual autoencoder for anomaly   \n362 detection on attributed networks. In ICASSP 2020-2020 IEEE International Conference on   \n363 Acoustics, Speech and Signal Processing (ICASSP), pages 5685\u20135689. IEEE, 2020.   \n364 [15] Robert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel,   \n365 Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature   \n366 Machine Intelligence, 2(11):665\u2013673, 2020.   \n367 [16] Markus Goldstein and Andreas Dengel. Histogram-based outlier score (hbos): A fast unsuper  \n368 vised anomaly detection algorithm. KI-2012: poster and demo track, 1:59\u201363, 2012.   \n369 [17] Songqiao Han, Xiyang Hu, Hailiang Huang, Minqi Jiang, and Yue Zhao. Adbench: Anomaly   \n370 detection benchmark. Advances in Neural Information Processing Systems, 35:32142\u201332159,   \n371 2022.   \n372 [18] Zengyou He, Xiaofei Xu, and Shengchun Deng. Discovering cluster-based local outliers.   \n373 Pattern recognition letters, 24(9-10):1641\u20131650, 2003.   \n374 [19] Meng Jiang. Catching social media advertisers with strategy analysis. In Proceedings of the   \n375 First International Workshop on Computational Methods for CyberSafety, pages 5\u201310, 2016.   \n376 [20] Ming Jin, Yixin Liu, Yu Zheng, Lianhua Chi, Yuan-Fang Li, and Shirui Pan. Anemone: Graph   \n377 anomaly detection with multi-scale contrastive learning. In Proceedings of the 30th ACM   \n378 International Conference on Information & Knowledge Management, pages 3122\u20133126, 2021.   \n379 [21] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint   \n380 arXiv:1412.6980, 2014.   \n381 [22] Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint   \n382 arXiv:1611.07308, 2016.   \n383 [23] Yufeng Kou, Chang-Tien Lu, Sirirat Sirwongwattana, and Yo-Ping Huang. Survey of fraud   \n384 detection techniques. In IEEE International Conference on Networking, Sensing and Control,   \n385 2004, volume 2, pages 749\u2013754. IEEE, 2004.   \n386 [24] Hans-Peter Kriegel, Peer Kr\u00f6ger, Erich Schubert, and Arthur Zimek. Outlier detection in   \n387 axis-parallel subspaces of high dimensional data. In Advances in Knowledge Discovery and   \n388 Data Mining: 13th Pacific-Asia Conference, PAKDD 2009 Bangkok, Thailand, April 27-30,   \n389 2009 Proceedings 13, pages 831\u2013838. Springer, 2009.   \n390 [25] Srijan Kumar, Xikun Zhang, and Jure Leskovec. Predicting dynamic embedding trajectory   \n391 in temporal interaction networks. In Proceedings of the 25th ACM SIGKDD international   \n392 conference on knowledge discovery & data mining, pages 1269\u20131278, 2019.   \n393 [26] Jinbo Li, Hesam Izakian, Witold Pedrycz, and Iqbal Jamal. Clustering-based anomaly detection   \n394 in multivariate time series data. Applied Soft Computing, 100:106919, 2021.   \n395 [27] Zheng Li, Yue Zhao, Nicola Botta, Cezar Ionescu, and Xiyang Hu. Copod: copula-based outlier   \n396 detection. In 2020 IEEE international conference on data mining (ICDM), pages 1118\u20131123.   \n397 IEEE, 2020.   \n398 [28] Zheng Li, Yue Zhao, Xiyang Hu, Nicola Botta, Cezar Ionescu, and George Chen. Ecod: Unsu  \n399 pervised outlier detection using empirical cumulative distribution functions. IEEE Transactions   \n400 on Knowledge and Data Engineering, 2022.   \n401 [29] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest. In 2008 eighth ieee   \n402 international conference on data mining, pages 413\u2013422. IEEE, 2008.   \n403 [30] Yixin Liu, Zhao Li, Shirui Pan, Chen Gong, Chuan Zhou, and George Karypis. Anomaly   \n404 detection on attributed networks via contrastive self-supervised learning. IEEE transactions on   \n405 neural networks and learning systems, 33(6):2378\u20132392, 2021.   \n406 [31] Xuexiong Luo, Jia Wu, Amin Beheshti, Jian Yang, Xiankun Zhang, Yuan Wang, and Shan Xue.   \n407 Comga: Community-aware attributed graph anomaly detection. In Proceedings of the Fifteenth   \n408 ACM International Conference on Web Search and Data Mining, pages 657\u2013665, 2022.   \n409 [32] Larry M Manevitz and Malik Yousef. One-class svms for document classification. Journal of   \n410 machine Learning research, 2(Dec):139\u2013154, 2001.   \n411 [33] Goeffrey J McLachlan. Mahalanobis distance. Resonance, 4(6):20\u201326, 1999.   \n412 [34] Emmanuel M\u00fcller, Patricia Iglesias S\u00e1nchez, Yvonne M\u00fclle, and Klemens B\u00f6hm. Ranking   \n413 outlier nodes in subspaces of attributed graphs. In 2013 IEEE 29th international conference on   \n414 data engineering workshops (ICDEW), pages 216\u2013222. IEEE, 2013.   \n415 [35] Isaac Newton. Philosophiae naturalis principia mathematica, volume 1. G. Brookman, 1833.   \n416 [36] David Peel and Geoffrey J McLachlan. Robust mixture modelling using the t distribution.   \n417 Statistics and computing, 10:339\u2013348, 2000.   \n418 [37] Zhen Peng, Minnan Luo, Jundong Li, Luguo Xue, and Qinghua Zheng. A deep multi-view   \n419 framework for anomaly detection on attributed networks. IEEE Transactions on Knowledge   \n420 and Data Engineering, 34(6):2539\u20132552, 2020.   \n421 [38] Leif E Peterson. K-nearest neighbor. Scholarpedia, 4(2):1883, 2009.   \n422 [39] Tom\u00e1\u0161 Pevny\\`. Loda: Lightweight on-line detector of anomalies. Machine Learning, 102:275\u2013   \n423 304, 2016.   \n424 [40] Douglas A Reynolds et al. Gaussian mixture models. Encyclopedia of biometrics, 741(659-663),   \n425 2009.   \n426 [41] Lukas Ruff, Jacob R Kauffmann, Robert A Vandermeulen, Gr\u00e9goire Montavon, Wojciech   \n427 Samek, Marius Kloft, Thomas G Dietterich, and Klaus-Robert M\u00fcller. A unifying review of   \n428 deep and shallow anomaly detection. Proceedings of the IEEE, 109(5):756\u2013795, 2021.   \n429 [42] Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui,   \n430 Alexander Binder, Emmanuel M\u00fcller, and Marius Kloft. Deep one-class classification. In   \n431 International conference on machine learning, pages 4393\u20134402. PMLR, 2018.   \n432 [43] Mayu Sakurada and Takehisa Yairi. Anomaly detection using autoencoders with nonlinear   \n433 dimensionality reduction. In Proceedings of the MLSDA 2014 2nd workshop on machine   \n434 learning for sensory data analysis, pages 4\u201311, 2014.   \n435 [44] Osman Salem, Yaning Liu, Ahmed Mehaoua, and Raouf Boutaba. Online anomaly detection in   \n436 wireless body area networks for reliable healthcare monitoring. IEEE journal of biomedical   \n437 and health informatics, 18(5):1541\u20131551, 2014.   \n438 [45] Hanyu Song, Peizhao Li, and Hongfu Liu. Deep clustering based fair outlier detection. In   \n439 Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining,   \n440 pages 1481\u20131489, 2021.   \n441 [46] Jianheng Tang, Jiajin Li, Ziqi Gao, and Jia Li. Rethinking graph neural networks for anomaly   \n442 detection. In International Conference on Machine Learning, pages 21076\u201321089. PMLR,   \n443 2022.   \n444 [47] Shantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, Mehdi Azabou, Eva L Dyer,   \n445 Remi Munos, Petar Velic\u02c7kovic\u00b4, and Michal Valko. Large-scale representation learning on graphs   \n446 via bootstrapping. arXiv preprint arXiv:2102.06514, 2021.   \n447 [48] Laurens Van Der Maaten. Learning a parametric embedding by preserving local structure. In   \n448 Artificial intelligence and statistics, pages 384\u2013391. PMLR, 2009.   \n449 [49] Svante Wold, Kim Esbensen, and Paul Geladi. Principal component analysis. Chemometrics   \n450 and intelligent laboratory systems, 2(1-3):37\u201352, 1987.   \n451 [50] Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering   \n452 analysis. In International conference on machine learning, pages 478\u2013487. PMLR, 2016.   \n453 [51] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang. Deep isolation forest for   \n454 anomaly detection. IEEE Transactions on Knowledge and Data Engineering, 2023.   \n455 [52] Xiaowei Xu, Nurcan Yuruk, Zhidan Feng, and Thomas AJ Schweiger. Scan: a structural   \n456 clustering algorithm for networks. In Proceedings of the 13th ACM SIGKDD international   \n457 conference on Knowledge discovery and data mining, pages 824\u2013833, 2007.   \n458 [53] Zhiming Xu, Xiao Huang, Yue Zhao, Yushun Dong, and Jundong Li. Contrastive attributed   \n459 network anomaly detection with data augmentation. In Advances in Knowledge Discovery and   \n460 Data Mining: 26th Pacific-Asia Conference, PAKDD 2022, Chengdu, China, May 16\u201319, 2022,   \n461 Proceedings, Part II, pages 444\u2013457. Springer, 2022.   \n462 [54] Xu Yuan, Na Zhou, Shuo Yu, Huafei Huang, Zhikui Chen, and Feng Xia. Higher-order structure   \n463 based anomaly detection on attributed networks. In 2021 IEEE International Conference on   \n464 Big Data (Big Data), pages 2691\u20132700. IEEE, 2021.   \n465 [55] Yu Zheng, Ming Jin, Yixin Liu, Lianhua Chi, Khoa T Phan, and Yi-Ping Phoebe Chen. Genera  \n466 tive and contrastive self-supervised learning for graph anomaly detection. IEEE Transactions   \n467 on Knowledge and Data Engineering, 2021.   \n468 [56] Shuang Zhou, Xiao Huang, Ninghao Liu, Qiaoyu Tan, and Fu-Lai Chung. Unseen anomaly   \n469 detection on networks via multi-hypersphere learning. In Proceedings of the 2022 SIAM   \n470 International Conference on Data Mining (SDM), pages 262\u2013270. SIAM, 2022.   \n471 [57] Shuang Zhou, Qiaoyu Tan, Zhiming Xu, Xiao Huang, and Fu-lai Chung. Subtractive aggregation   \n472 for attributed network anomaly detection. In Proceedings of the 30th ACM International   \n473 Conference on Information & Knowledge Management, pages 3672\u20133676, 2021.   \n474 [58] Bo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Daeki Cho, and   \n475 Haifeng Chen. Deep autoencoding gaussian mixture model for unsupervised anomaly detection.   \n476 In International conference on learning representations, 2018.   \nInput: data points $\\mathbf{X}$ , cluster number $K$ , outlier ratio $l$ , tolerance $\\lambda$ , iterations $t$   \nOutput: network parameters $\\Theta$ , mixture parameters $\\{\\omega_{k},\\mu_{k},\\Sigma_{k}\\}$   \n1: Initialize $\\Theta$ and $\\{\\mu_{k},\\omega_{k},\\Sigma_{k}\\}$ ;   \n2: for $i=1$ to $t$ do   \n3: if $i=1$ then   \n4: $\\mathbf{X}_{i}\\gets\\mathbf{X}$ ;   \n5: else   \n6: Re-order the point in $\\mathbf{X}$ such that $o_{1}\\geq\\cdot\\cdot\\geq o_{n}$ ;   \n7: $\\begin{array}{r l}&{L_{i}\\leftarrow\\{x_{1},\\dotsc,x_{\\lfloor N*l\\rfloor}\\};}\\\\ &{\\mathbf{X}_{i}\\leftarrow\\mathbf{X}\\backslash L_{i};}\\end{array}$   \n8:   \n9: end if   \n10: Update $\\Theta$ with Equation (15);   \n11: while $|J(\\Theta,\\Phi)-\\stackrel{\\cdot}{-}J^{o l d}(\\Theta,\\Phi)|>\\lambda$ do   \n12: $J^{o l d}(\\Theta,\\Phi)=J(\\Theta,\\Phi)$ ;   \n13: Calculate $\\tau$ with Equation (10);   \n14: Update $\\{\\omega_{k},\\mu_{k},\\Sigma_{k}\\}$ with Equation (12), (13) and (14);   \n15: end while   \n16: Calculate $o_{i}$ with Equation (9);   \n17: end for   \n18: return $\\Theta$ and $\\{\\omega_{k},\\mu_{k},\\Sigma_{k}\\}$ ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "477 A Iterative Training Algorithm ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "478 The pseudocode for training the model is presented in Algorithm 1. Initially, all parameters undergo   \n479 random initialization. In subsequent iterations, following the initial round, the outlier set $L$ undergoes   \n480 updates based on the anomaly score $o_{i}$ . This is succeeded by the adjustment of the network parameters   \n481 $\\Theta$ based on $\\mathbf{x}_{i}$ , further optimizing the performance of $\\Theta$ through the utilization of the estimated   \n482 parameters $\\mu_{k},\\omega_{k},\\Sigma_{k}$ . The essence of the algorithm is embedded in its alternating optimization   \n483 strategy, iteratively refining the accuracy of representation learning and mixed model parameter   \n484 estimation, thereby augmenting the overall training effectiveness of the model. ", "page_idx": 13}, {"type": "text", "text": "485 B Derivation of EM Algorithm ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "486 This appendix provides the detailed derivation of the Expectation-Maximization (EM) algorithm   \n487 for optimizing the parameters of a mixture model based on Student\u2019s t-distribution. The focus is   \n488 on deriving analytical solutions for the maximization of the parameters $\\Phi=\\{\\mu_{k},\\Sigma_{k},\\omega_{k}\\}$ of the   \n489 mixture components. The EM algorithm alternates between two steps:   \n490 In the $\\mathbf{E}$ -step, we calculate the posterior probabilities $\\tau_{i k}$ , representing the probability of data point   \n491 $i$ belonging to cluster $k$ , given the current parameters. The posterior probabilities for a Student\u2019s   \n492 t-distribution mixture model are formulated as: ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\n\\tau_{i k}=\\frac{\\omega_{k}\\cdot p(\\mathbf{z}_{i}|\\pmb{\\mu}_{k},\\Sigma_{k})}{\\sum_{j=1}^{K}\\omega_{j}\\cdot p(\\mathbf{z}_{i}|\\pmb{\\mu}_{j},\\Sigma_{j})},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "493 where $\\tau(\\mathbf{z}_{i}|\\pmb{\\mu}_{k},\\Sigma_{k})$ denotes the Student\u2019s t-distribution for data point $i$ with respect to cluster $k$ , and   \n494 $K$ is the number of mixture components.   \n495 The Student\u2019s t-distribution is depicted as a hierarchical conditional probability, resembling a Gaussian   \n496 distribution with an accuracy scale factor $\\mathbf{u}$ , where its latent variable follows a gamma distribution.   \n497 Adopting a degree of freedom $\\nu=1$ , the value of $\\mathbf{u}_{i k}$ is given by: ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf u_{i k}=\\frac{\\pmb\\nu+1}{\\pmb\\nu+D_{M}(z_{i},\\pmb\\mu_{k})}=\\frac{2}{1+D_{M}(z_{i},\\pmb\\mu_{k})}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "498 In the M-step, we update the parameters $\\Phi=\\{\\omega_{k},\\boldsymbol{\\mu}_{k}$ , and $\\Sigma_{k}\\}$ using the derivatives obtained in   \n499 the previous steps. In our model, the likelihood function for a Student\u2019s-t Distribution Mixture Model ", "page_idx": 13}, {"type": "image", "img_path": "WpEaUIBIWH/tmp/2dbbddb311c856c891fcfca70716029068407a3f5a036707c1850fbfbf4429e5.jpg", "img_caption": ["Figure 3: Score comparison with other methods. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "500 (SMM) is represented as: ", "page_idx": 14}, {"type": "equation", "text": "$$\nL(\\omega,\\pmb{\\mu},\\Sigma)=\\sum_{i=1}^{N}\\sum_{k=1}^{K}\\omega_{k}\\cdot\\frac{\\pi^{-1}\\cdot|\\Sigma_{k}|^{-\\frac{1}{2}}}{1+(\\mathbf{z}_{i}-\\pmb{\\mu}_{k})^{T}\\Sigma_{k}^{-1}(\\mathbf{z}_{i}-\\pmb{\\mu}_{k})},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "501 where $\\omega_{k}$ are the mixture weights, $\\Sigma_{k}$ the covariance matrices, $\\pmb{\\mu}_{k}$ the means, and $\\mathbf{z}_{i}$ the data points. ", "page_idx": 14}, {"type": "text", "text": "502 The derivative with respect to $\\omega_{k}$ must consider the constraint that the sum of the mixture weights   \n503 equals 1, i.e., $\\begin{array}{r}{\\sum_{k}\\omega_{k}\\stackrel{!}{=}1}\\end{array}$ . Hence, we introduce a Lagrange multiplier $\\lambda$ to address this constraint   \n504 and construct the Lagrangian $L^{\\prime}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\nL^{\\prime}(\\omega,\\pmb{\\mu},\\Sigma,\\lambda)=L(\\omega,\\pmb{\\mu},\\Sigma)+\\lambda\\left(1-\\sum_{k=1}^{K}\\omega_{k}\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "505 The derivative with respect to $\\omega_{k}$ is: ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\frac{\\partial L^{\\prime}}{\\partial\\omega_{k}}}={\\frac{\\partial L}{\\partial\\omega_{k}}}-\\lambda,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "506 Substituting the definition of $L(\\omega,\\mu,\\Sigma)$ , we obtain: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\partial L}{\\partial\\omega_{k}}=\\sum_{i}\\frac{p(\\mathbf{z}_{i}|\\pmb{\\mu}_{k},\\Sigma_{k})}{\\sum_{j=1}^{K}\\omega_{j}\\cdot p(\\mathbf{z}_{i}|\\pmb{\\mu}_{j},\\Sigma_{j})}=\\sum_{i}\\frac{\\tau_{i k}}{\\omega_{k}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "507 To solve for $\\omega_{k}$ , we first multiply both sides of the equation by $\\omega_{k}$ and apply the constraint condition: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{k}\\omega_{k}\\left(\\sum_{i}\\frac{\\tau_{i k}}{\\omega_{k}}-\\lambda\\right)=0,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "508 Upon further organization, we find that the Lagrange multiplier $\\lambda$ actually equals the total number of   \n509 data points $N$ (since $\\textstyle\\sum_{i}\\tau_{i k}=N_{k}$ , where $N_{k}$ is the expected total number of data points belonging   \n510 to the $k$ th componen t, and the sum of all $N_{k}$ equals the total number of data points $N$ ). ", "page_idx": 14}, {"type": "text", "text": "511 Finally, we can solve for $\\omega_{k}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\omega_{k}=\\frac{\\sum_{i}\\tau_{i k}}{N},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "512 This result indicates that the weight $\\omega_{k}$ of each mixture component equals the proportion of the   \n513 posterior probabilities of the data points it contains relative to all data points. ", "page_idx": 14}, {"type": "text", "text": "514 To update $\\pmb{\\mu}_{k}$ and $\\Sigma_{k}$ , we consider the conditional expectation of the data log-likelihood function: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{Q(\\pmb{\\mu}_{k},\\Sigma_{k})=\\displaystyle\\sum_{i=1}^{N}\\pmb{\\tau}_{i k}\\left(-\\log(\\pi)-\\frac{1}{2}\\log|\\sigma_{k}|+\\frac{1}{2}\\log u_{i k}\\right.}}\\\\ {{\\left.-\\frac{1}{2}\\pmb{\\mathrm{u}}_{i k}(\\pmb{\\mathrm{z}}_{i}-\\pmb{\\mu}_{k})^{T}\\Sigma_{k}^{-1}(\\pmb{\\mathrm{z}}_{i}-\\pmb{\\mu}_{k})\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "image", "img_path": "WpEaUIBIWH/tmp/838a9abad8d08c9719adc6937ed83ae136ac368d5315bf0f01072780e7971a35.jpg", "img_caption": ["Figure 4: Analysis of gravitational force. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "515 Maximizing $Q(\\mu_{k},\\Sigma_{k})$ with respect to $\\pmb{\\mu}_{k}$ leads to: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\partial Q}{\\partial\\pmb{\\mu}_{k}}=\\frac{1}{2}\\sum_{i=1}^{N}\\tau_{i k}\\mathbf{u}_{i k}\\big(2\\Sigma_{k}^{-1}\\pmb{\\mu}_{k}-2\\Sigma_{k}^{-1}\\mathbf{z}_{i k}\\big)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "516 Setting \u2202\u00b5 $\\begin{array}{r}{\\frac{\\partial Q}{\\partial\\mu_{k}}=0}\\end{array}$ Q = 0 results in the updated mean \u00b5(kt $\\pmb{\\mu}_{k}^{(t+1)}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pmb{\\mu}_{k}^{(t+1)}=\\sum_{i=1}^{n}\\left(\\tau_{i k}^{(t+1)}\\mathbf{u}_{i k}^{(t+1)}\\mathbf{z}_{i}\\right)/\\sum_{i=1}^{n}\\left(\\tau_{i k}^{(t+1)}\\mathbf{u}_{i k}^{(t+1)}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "517 Considering the derivative of $Q(\\mu_{k},\\Sigma_{k})$ with respect to $\\Sigma_{k}^{-1}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\partial Q}{\\partial\\Sigma_{k}^{-1}}=\\frac{1}{2}\\sum_{i=1}^{N}\\tau_{i k}\\left(\\Sigma_{k}-\\mathbf{u}_{i k}(\\mathbf{z}_{i}-\\mu_{k})\\times(\\mathbf{z}_{i}-\\mu_{k})^{T}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "518 Setting \u2202\u2202\u00b5Qk $\\begin{array}{r}{\\frac{\\partial Q}{\\partial\\mu_{k}}=0}\\end{array}$ yields the updated covariance matrix $\\Sigma_{k}^{(t+1)}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{\\Sigma}_{k}^{(t+1)}=\\frac{\\sum_{i=1}^{n}\\tau_{i k}^{(t+1)}\\mathbf{u}_{i k}^{(t+1)}(\\mathbf{z}_{i}-\\pmb{\\mu}_{k}^{(t+1)})(\\mathbf{z}_{i}-\\pmb{\\mu}_{k}^{(t+1)})^{T}}{\\sum_{j=1}^{K}\\tau_{i j}^{(t+1)}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "519 C Anomaly Score with Vector Sum ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "520 C.1 Advantages ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "521 Here we discuss the advantages of employing vector sum in anomaly score with a toy example. ", "page_idx": 15}, {"type": "text", "text": "522 The application of the vector sum principle extends beyond physical mechanics and finds relevance   \n523 in various domains. In relational embedding [5], for example, relationships can be represented as   \n524 vectors. Aggregating these vectors allows for capturing complexities like transitivity, symmetry, and   \n525 antisymmetry.   \n526 Similarly, in our context, the vector sum can help capture more complex relationships along clus  \n527 ters. Consider Figure 4 as an example, where a sample $v$ is attracted by two groups of cluster   \n528 prototypes $(\\{\\mu_{1},\\bar{\\mu}_{2}\\}$ , $\\left\\{\\mu_{3},\\mu_{4}\\right\\})$ with the same mass and sample-prototype distances $\\overline{{{m}}}_{1}=\\widetilde{m}_{2}=$   \n529 $\\widetilde{m}_{3}=\\widetilde{m}_{4}$ , $\\widetilde{r}_{v1}=\\widetilde{r}_{v2}=\\widetilde{r}_{v3}=\\widetilde{r}_{v4})$ . Without considering the direction of the forces, th e  two g r oups   \n530 o f prot o typ e s wo ul d attr a ct the  s ample with equal forces. However, we argue that the two groups of   \n531 prototypes should exert different influences. A sample close to two clusters with a large difference   \n532 $(\\left\\{\\pmb{\\mu}_{1},\\pmb{\\mu}_{2}\\right\\})$ is more likely to be an anomaly compared to a sample that is close to two clusters with   \n533 a smaller difference $(\\{\\pmb{\\mu}_{3},\\pmb{\\mu}_{4}\\})$ . For example, in a social network, a user who equally likes two   \n534 extremely different communities, like money-saving tips and luxury items, is more anomalous than   \n535 a user who equally likes two similar communities, like private jets and luxury items. Applying   \n536 the vector sum, the total force of $\\{\\mu_{1},\\mu_{2}\\}$ is much smaller than that of $\\{\\mu_{3},\\mu_{4}\\}$ . As the anomaly   \n537 score is inversely related to the total force, it is more anomalous when equally attracted by $\\{\\mu_{1},\\mu_{2}\\}$   \n538 with large difference. This indicates that the vector sum successfully captures subtle differences   \n539 in the distinctions among multiple clusters, thereby assisting in the identification of more accurate   \n540 anomalies.   \n542 In the appendix, as illustrated in Figure 3, we investigated a toy example. We discussed a specific   \n543 pattern of anomalies termed group anomalies, where a small number of anomalous samples cluster   \n544 together. It is crucial to note that we do not claim this anomaly pattern is common in real-world data;   \n545 our goal is merely to point out a specific anomaly pattern that is challenging for traditional cluster  \n546 based anomaly detection methods to detect. Specifically, we utilize three Gaussian distributions with   \n547 high variance (each generating 300 data samples) and one with lower variance (generating 30 data   \n548 samples). Because the samples from the smaller Gaussian follow a different generative mechanism   \n549 and represent a minority in the dataset, we consider them anomalies.   \n550 We set the cluster number for KMeans-- and GMM at four, indicating that the Gaussian distribution   \n551 comprising anomalous samples was also recognized as a cluster. KMeans-- employs a cluster-based   \n552 approach, using the distance to the nearest cluster center as the anomaly score, while GMM uses   \n553 a probability-based approach, considering the samples\u2019 likelihood in the mixture model as the   \n554 anomaly score. However, both approaches are ineffective in this scenario. Rather than identifying the   \n555 small cluster as anomalous, they tend to misidentify samples on the peripheries of larger clusters as   \n556 anomalies.   \n557 By contrast, our scoring method views the entire small cluster as more likely anomalous, followed by   \n558 outlier samples on the margins of the larger clusters. This visualization provides a perspective that   \n559 distinguishes our method from previous efforts. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "560 D Experimental Supplementary ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "561 D.1 Benchmark Datasets Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "562 Due to space constraints in the main text, we utilized 30 public datasets from ADBench [17], covering   \n563 all different types of data. The details of the 30 datasets are presented in Table 4. ", "page_idx": 16}, {"type": "text", "text": "564 D.2 Baselines Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "565 A comprehensive overview of the unsupervised anomaly detection methods is presented below. ", "page_idx": 16}, {"type": "text", "text": "566 D.2.1 Traditional Models ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "567 \u2022 Subspace Outlier Detection (SOD) [24]: Identifies outliers in varying subspaces of a highdimensional feature space, targeting anomalies that emerge in lower-dimensional projections. 569 \u2022 Histogram-based Outlier Detection (HBOS) [16]: Assumes feature independence and calculates 570 outlyingness via histograms, offering scalability and efficiency. ", "page_idx": 16}, {"type": "text", "text": "571 D.2.2 Linear Models ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "572 \u2022 Principal Component Analysis (PCA) [49]: Utilizes singular value decomposition for dimension  \n573 ality reduction, with anomalies indicated by reconstruction errors.   \n574 \u2022 One-class SVM (OCSVM) [32]: Defines a decision boundary to separate normal samples from   \n575 outliers, maximizing the margin from the data origin. ", "page_idx": 16}, {"type": "text", "text": "576 D.2.3 Density-based Models ", "page_idx": 16}, {"type": "text", "text": "577 \u2022 Local Outlier Factor (LOF) [6] $:$ Measures local density deviation, marking samples as outliers if   \n578 they lie in less dense regions compared to their neighbors.   \n579 \u2022 K-Nearest Neighbors (KNN) [38]: Anomaly scores are assigned based on the distance to the k-th   \n580 nearest neighbor, embodying a simple yet effective approach. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "581 D.2.4 Ensemble-based Models ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "582 \u2022 Lightweight On-line Detector of Anomalies (LODA) [39] : An ensemble method suitable for   \n583 real-time processing and adaptable to concept drift through random projections and histograms.   \n584 \u2022 Isolation Forest (IForest) [29]: Isolates anomalies by randomly selecting features and split values,   \n585 leveraging the ease of isolating anomalies to identify them efficiently. ", "page_idx": 16}, {"type": "table", "img_path": "WpEaUIBIWH/tmp/0d3d5ff9c13366ef2dc4482982bf6ff071655e3b63a8686a390febad50f69de6.jpg", "table_caption": ["Table 4: Statistics of tabular benchmark datasets. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "586 D.2.5 Probability-based Models ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "587 \u2022 Deep Autoencoding Gaussian Mixture Model (DAGMM) [58]: Combines a deep autoencoder   \n588 with a GMM for anomaly scoring, utilizing both low-dimensional representation and reconstruction   \n589 error.   \n590 \u2022 Empirical-Cumulative-distribution-based Outlier Detection (ECOD) [28]: Uses ECDFs to   \n591 estimate feature densities independently, targeting outliers in distribution tails.   \n592 \u2022 Copula Based Outlier Detector (COPOD) [27]: A hyperparameter-free method leveraging   \n593 empirical copula models for interpretable and efficient outlier detection.   \n594 D.2.6 Cluster-based Models   \n595 \u2022 DBSCAN [13]: A density-based clustering algorithm that identifies clusters based on the density   \n596 of data points, effectively separating high-density clusters from low-density noise, and is widely   \n597 used for anomaly detection in spatial data.   \n598 \u2022 Clustering Based Local Outlier Factor (CBLOF) [18]: Calculates anomaly scores based on   \n599 cluster distances, using global data distribution.   \n600 \u2022 KMeans-- [45]: Extends $\\mathbf{k}\\cdot$ -means to include outlier detection in the clustering process, offering an   \n601 integrated approach to anomaly detection.   \n602 \u2022 Deep Clustering-based Fair Outlier Detection (DCFOD) [9]: Enhances outlier detection with a   \n603 focus on fairness, combining deep clustering and adversarial training for representation learning. ", "page_idx": 17}, {"type": "text", "text": "Table 5: AUCROC of 17 unsupervised algorithms on 30 tabular benchmark datasets. In each dataset, the algorithm with the highest AUCROC is marked in red, the second highest in blue, and the third highest in green. ", "page_idx": 18}, {"type": "image", "img_path": "WpEaUIBIWH/tmp/78e07859fd8b132fbba916b75a44c906ae7284039c62e401e0c4568ffe61ff7a.jpg", "img_caption": ["Figure 5: Critical difference diagrams for AUC-ROC and AUC-PR. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "604 D.2.7 Neural Network-based Models ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "605 \u2022 Deep Support Vector Data Description (DeepSVDD) [42]: Minimizes the volume of a hyper  \n606 sphere enclosing network data representations, isolating anomalies outside this sphere.   \n607 \u2022 Deep Isolation Forest for Anomaly Detection (DIF) [51]: Utilizes deep learning to enhance   \n608 traditional isolation forest techniques, offering improved anomaly detection in complex datasets   \n609 with minimal parameter tuning.   \n610 Each method\u2019s unique mechanism and application context provide a rich landscape of techniques   \n611 for unsupervised anomaly detection, illustrating the field\u2019s diverse methodologies and the breadth of   \n612 approaches to tackling anomaly detection challenges. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "613 D.3 Supplementary Experimental Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "614 In the appendix, we detail the statistical analysis conducted to compare the performance of various   \n615 anomaly detectors. We obtained this diagram by conducting a Friedman test (p-value: 4.657e-19),   \n616 indicating significant differences among different detectors. We utilized average ranks and the   \n617 Nemenyi test to generate the critical difference diagram, as shown in Figure 5. It is noteworthy that   \n618 the vector version exhibits significantly superior performance compared to the scalar version across   \n619 more methods. The detailed outcomes for the AUCROC and AUCPR metrics, spanning 30 datasets   \n620 and against 17 baseline approaches, are showcased in Table 5 and Table 6. ", "page_idx": 18}, {"type": "text", "text": "621 D.4 Complexity Analysis ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "622 The complexity of each iteration in UniCAD involves three parts: constructing the outlier set,   \n623 updating the network parameters $\\Theta$ , and optimizing the mixture model using the EM algorithm. ", "page_idx": 18}, {"type": "text", "text": "Table 6: AUCPR of 17 unsupervised algorithms on 30 tabular benchmark datasets. In each dataset, the algorithm with the highest AUCPR is marked in red, the second highest in blue, and the third highest in green. ", "page_idx": 19}, {"type": "table", "img_path": "WpEaUIBIWH/tmp/de4ccadf728eb560ac660a2caedbc0d2f2d60c00062dd692156d40253f36d9da.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "624 Constructing the outlier set requires a sorting operation, for which we use Numpy\u2019s built-in quantile   \n625 calculation with a time complexity of $\\mathcal{O}(N\\log N)$ . Considering the number of network parameters   \n626 along with the computation of the loss function, the computational complexity for optimizing $\\Theta$ is   \n627 approximately $\\mathcal{O}(\\bar{T}N D d+T N K d)$ . The EM algorithm for the Student\u2019s t mixture model includes   \n628 two main steps: the E-step, where the complexity for computing the probability (or responsibility)   \n629 of each data point belonging to each component is approximately $\\mathcal{O}(N K d)$ , and the M-step, where   \n630 the full computational complexity of updating the parameters (mean, covariance matrix) of each   \n631 component is $O(N K d^{2})$ . In practice, we use diagonal covariance matrices, which reduces the   \n632 update complexity to roughly $\\mathcal{O}(N K d)$ . If the EM algorithm requires $T$ round to converge, its   \n633 time complexity is approximately $\\mathcal{O}(T N K d)$ . Therefore, the time complexity for $t$ -iterations is   \n634 $\\mathcal{O}(t N(\\log\\bar{N}+T d(\\bar{D^{+}}F))$ ). ", "page_idx": 19}, {"type": "text", "text": "635 E Additional Experiments on Graph ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "636 E.1 Baselines ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "637 Our proposed method was compared with 16 graph domain baseline methods grouped into three   \n638 categories as follows:   \n639 \u2022 Contrastive Learning-based Methods: This group includes CoLA [30], SLGAD [55],   \n640 CONAD [53], and ANEMONE [20]. These methods primarily assume that the contrastive loss   \n641 between anomalous nodes and their neighborhoods is more significant.   \n642 \u2022 Autoencoder-based Methods: This category consists of MLPAE [43], GCNAE [22], DOMI  \n643 NANT [11], GUIDE [54], ComGA [31], AnomalyDAE [14], ALARM [37], DONE/AdONE [4]   \n644 and AAGNN [57]. These methods focus on the reconstruction errors of anomalous nodes during   \n645 the process of reconstructing the graph structure or features.   \n646 \u2022 Clustering-based Methods: This category of methods encompasses SCAN [52], CBLOF [18],   \n647 and DCFOD [45]. These methods generally identify anomalies by detecting if a sample deviates   \n648 from the clustering. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "649 E.2 Datasets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "650 We assess the performance of our model using four graph benchmark datasets containing organic   \n651 anomalies. Table 7 presents the statistical summary for each dataset. These datasets contain naturally   \n652 occurring real-world anomalies and are valuable for assessing the performance of anomaly detection   \n653 algorithms in real-world scenarios. The sources and compositions of these datasets are as follows:   \n654 \u2022 Weibo[19] is a labeled graph comprising user posts extracted from the social media platform   \n655 Tencent Weibo. The user-user graph establishes connections between users who exhibit similar   \n656 topic labels. A user is considered anomalous if they have engaged in a minimum of five suspicious   \n657 events, whereas normal nodes represent users who have not.   \n658 \u2022 Reddit[25] consists of a user-subreddit graph extracted from the popular social media platform   \n659 Reddit. This publicly accessible dataset encompasses user posts within various subreddits over   \n660 a month. Each user is assigned a binary label indicating whether they have been banned on the   \n661 platform. Our assumption is that banned users exhibit anomalous behavior compared to regular   \n662 Reddit users.   \n663 \u2022 Disney[34] is a co-purchase network of movies that includes attributes such as price, rating, and the   \n664 number of reviews. The ground truth labels, indicating whether a movie is considered anomalous   \n665 or not, were assigned by high school students through majority voting.   \n666 \u2022 T-Finance[46] aims to identify anomalous accounts within a trading network. The nodes in   \n667 this network represent unique anonymous accounts, each characterized by ten features related to   \n668 registration duration, recorded activity, and interaction frequency. Graph edges denote transaction   \n669 records between accounts. If a node is associated with activities such as fraud, money laundering,   \n670 or online gambling, human experts will designate it as an anomaly. ", "page_idx": 19}, {"type": "table", "img_path": "WpEaUIBIWH/tmp/cf2a70c89b283d743e05e07d02a09478188533b091a542ef162b480cce337488.jpg", "table_caption": ["Table 7: Statistics of graph benchmark datasets. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "671 E.3 Experiment Settings ", "text_level": 1, "page_idx": 20}, {"type": "table", "img_path": "WpEaUIBIWH/tmp/f4ed673cb054703fcd5c483c1f7eb91ee25bba1d45b54135cbb4a945c4cfeef0.jpg", "table_caption": ["Table 8: AUC-ROC and AUC-PR of 16 unsupervised algorithms on 4 graph benchmark datasets. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "672 In this experiment, we compared graph-based methods on relational data. For methods originally   \n673 designed around feature vectors, including CBLOF, DCFOD, and our approach, we uniformly   \n674 employed the same graph representation learning technique as described in BGRL [47]. Specifically,   \n675 we used a two-layer Graph Convolutional Network (GCN) for encoding, which produced output   \n676 embeddings with a dimensionality of 128. The training epochs were set to 3000, including a warm-up   \n677 period of 300 epochs. The hidden size of the predictor was set to 512, and the momentum was fixed   \n678 at 0.99.   \n680 The performance of UniCAD compared to 16 baseline methods on the four datasets are summarized   \n681 in Table 8. From the results, we have the following observations: Our model consistently outperforms   \n682 the baseline methods on most datasets, underlining its effectiveness in anomaly detection even within   \n683 graph data contexts. This highlights the superiority of UniCAD in detecting anomalies in real-world   \n684 graph data.   \n685 When comparing UniCAD with the four contrastive learning-based methods, it exhibits a distinct   \n686 advantage, outperforming them by a substantial margin across all metrics. Unlike contrastive learning   \n687 methods that rely on the local neighborhood for anomaly detection, UniCAD leverages the global   \n688 clustering distribution. This key difference contributes to its consistently superior performance.   \n689 Although CONAD incorporates human prior knowledge about anomalies, enabling it to outperform   \n690 other similar methods on the Weibo and Disney datasets, it still falls short compared to our proposed   \n691 UniCAD.   \n692 Compared to the autoencoder-based methods, UniCAD offers the advantage of lower memory   \n693 requirements along with better performance. Graph autoencoders typically reconstruct the entire   \n694 adjacency matrix during full graph training, resulting in memory usage of at least $\\mathcal{O}(N^{2})$ . In contrast,   \n695 UniCAD, as a clustering-based method, only requires $\\mathcal{O}(N\\stackrel{.}{\\times}K)$ . Among the autoencoder-based   \n696 methods, GCNAE, DONE, and AdONE can be extended to the T-Finance dataset as they only   \n697 reconstruct the sampled subgraphs rather than the entire adjacency matrix. However, UniCAD still   \n698 showcases superior performance while being more memory-efficient.   \n699 UniCAD also demonstrates superior performance compared to various other clustering-based methods,   \n700 including traditional structural clustering (SCAN) methods that treat the embedding from BGRL as   \n701 tabular data (CBLOF, DCFOD).   \n703 The checklist is designed to encourage best practices for responsible machine learning research,   \n704 addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove   \n705 the checklist: The papers not including the checklist will be desk rejected. The checklist should   \n706 follow the references and precede the (optional) supplemental material. The checklist does NOT   \n707 count towards the page limit.   \n708 Please read the checklist guidelines carefully for information on how to answer these questions. For   \n709 each question in the checklist: ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "710 \u2022 You should answer [Yes] , [No] , or [NA] . ", "page_idx": 22}, {"type": "text", "text": "711 \u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant   \n712 information is Not Available.   \n713 \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA).   \n714 The checklist answers are an integral part of your paper submission. They are visible to the   \n715 reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it   \n716 (after eventual revisions) with the final version of your paper, and its final version will be published   \n717 with the paper.   \n718 The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.   \n719 While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a   \n720 proper justification is given (e.g., \"error bars are not reported because it would be too computationally   \n721 expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering   \n722 \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we   \n723 acknowledge that the true answer is often more nuanced, so please just use your best judgment and   \n724 write a justification to elaborate. All supporting evidence can appear either in the main paper or the   \n725 supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification   \n726 please point to the section(s) where related material for the question can be found. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "727 IMPORTANT, please: ", "page_idx": 22}, {"type": "text", "text": "728 \u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\",   \n729 \u2022 Keep the checklist subsection headings, questions/answers and guidelines below.   \n730 \u2022 Do not modify the questions and only use the provided macros for your answers.   \n731 1. Claims   \n732 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n733 paper\u2019s contributions and scope?   \n734 Answer: [Yes]   \n735 Justification: The main claims presented in the abstract and introduction are consistent with   \n736 the paper\u2019s contributions and accurately outline the scope.   \n737 Guidelines:   \n738 \u2022 The answer NA means that the abstract and introduction do not include the claims made   \n739 in the paper.   \n740 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n741 contributions made in the paper and important assumptions and limitations. A No or NA   \n742 answer to this question will not be perceived well by the reviewers.   \n743 \u2022 The claims made should match theoretical and experimental results, and reflect how much   \n744 the results can be expected to generalize to other settings.   \n745 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n746 are not attained by the paper.   \n747 2. Limitations   \n748 Question: Does the paper discuss the limitations of the work performed by the authors?   \n749 Answer: [Yes]   \n750 Justification: The limitations of the method\u2019s application scope are discussed in Section 5 of   \n51 the paper, along with considerations for future work.   \n752 Guidelines:   \n753 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n754 the paper has limitations, but those are not discussed in the paper.   \n755 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n756 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n757 violations of these assumptions (e.g., independence assumptions, noiseless settings, model   \n758 well-specification, asymptotic approximations only holding locally). The authors should   \n59 reflect on how these assumptions might be violated in practice and what the implications   \n60 would be.   \n61 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only   \n762 tested on a few datasets or with a few runs. In general, empirical results often depend on   \n763 implicit assumptions, which should be articulated.   \n764 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n765 For example, a facial recognition algorithm may perform poorly when image resolution is   \n766 low or images are taken in low lighting. Or a speech-to-text system might not be used   \n767 reliably to provide closed captions for online lectures because it fails to handle technical   \n768 jargon.   \n769 \u2022 The authors should discuss the computational efficiency of the proposed algorithms and   \n770 how they scale with dataset size.   \n71 \u2022 If applicable, the authors should discuss possible limitations of their approach to address   \n72 problems of privacy and fairness.   \n773 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n774 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n775 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n776 judgment and recognize that individual actions in favor of transparency play an important   \n77 role in developing norms that preserve the integrity of the community. Reviewers will be   \n778 specifically instructed to not penalize honesty concerning limitations.   \n779 3. Theory Assumptions and Proofs   \n780 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n781 a complete (and correct) proof?   \n782 Answer: [Yes]   \n783 Justification: The full set of assumptions and complete proofs for each theoretical result   \n784 are provided and can be found in the appendix, specifically in Section B, ensuring that the   \n785 theoretical framework is transparent and verifiable.   \n786 Guidelines:   \n787 \u2022 The answer NA means that the paper does not include theoretical results.   \n788 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n89 referenced.   \n790 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n91 \u2022 The proofs can either appear in the main paper or the supplemental material, but if they   \n792 appear in the supplemental material, the authors are encouraged to provide a short proof   \n793 sketch to provide intuition.   \n794 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n795 by formal proofs provided in appendix or supplemental material.   \n796 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n797 4. Experimental Result Reproducibility   \n798 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n799 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n800 of the paper (regardless of whether the code and data are provided or not)?   \n802 Justification: All necessary information for reproducing the main experimental results,   \n803 including dataset links, baseline comparisons, and the methodologies of our proposed   \n804 approach, are comprehensively included within the submission flies, facilitating transparency   \n805 and reproducibility of the research findings.   \n806 Guidelines:   \n807 \u2022 The answer NA means that the paper does not include experiments.   \n808 \u2022 If the paper includes experiments, a No answer to this question will not be perceived well   \n809 by the reviewers: Making the paper reproducible is important, regardless of whether the   \n810 code and data are provided or not.   \n811 \u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to   \n812 make their results reproducible or verifiable.   \n813 \u2022 Depending on the contribution, reproducibility can be accomplished in various ways.   \n814 For example, if the contribution is a novel architecture, describing the architecture fully   \n815 might suffice, or if the contribution is a specific model and empirical evaluation, it may be   \n16 necessary to either make it possible for others to replicate the model with the same dataset,   \n817 or provide access to the model. In general. releasing code and data is often one good   \n818 way to accomplish this, but reproducibility can also be provided via detailed instructions   \n819 for how to replicate the results, access to a hosted model (e.g., in the case of a large   \n820 language model), releasing of a model checkpoint, or other means that are appropriate to   \n821 the research performed.   \n822 \u2022 While NeurIPS does not require releasing code, the conference does require all submis  \n823 sions to provide some reasonable avenue for reproducibility, which may depend on the   \n824 nature of the contribution. For example   \n825 (a) If the contribution is primarily a new algorithm, the paper should make it clear how to   \n826 reproduce that algorithm.   \n827 (b) If the contribution is primarily a new model architecture, the paper should describe   \n828 the architecture clearly and fully.   \n829 (c) If the contribution is a new model (e.g., a large language model), then there should   \n830 either be a way to access this model for reproducing the results or a way to reproduce   \n831 the model (e.g., with an open-source dataset or instructions for how to construct the   \n832 dataset).   \n833 (d) We recognize that reproducibility may be tricky in some cases, in which case authors   \n834 are welcome to describe the particular way they provide for reproducibility. In the   \n835 case of closed-source models, it may be that access to the model is limited in some   \n836 way (e.g., to registered users), but it should be possible for other researchers to have   \n837 some path to reproducing or verifying the results.   \n838 5. Open access to data and code   \n839 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n840 tions to faithfully reproduce the main experimental results, as described in supplemental   \n841 material?   \n842 Answer: [Yes]   \n843 Justification: The paper ensures open access to both the data and code necessary for   \n844 reproducing the main experimental results, complemented by detailed instructions in the   \n845 supplemental material.   \n846 Guidelines:   \n847 \u2022 The answer NA means that paper does not include experiments requiring code.   \n848 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n849 public/guides/CodeSubmissionPolicy) for more details.   \n850 \u2022 While we encourage the release of code and data, we understand that this might not be   \n851 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n852 including code, unless this is central to the contribution (e.g., for a new open-source   \n853 benchmark).   \n854 \u2022 The instructions should contain the exact command and environment needed to run to   \n855 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n856 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n857 \u2022 The authors should provide instructions on data access and preparation, including how to   \n858 access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n859 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n860 proposed method and baselines. If only a subset of experiments are reproducible, they   \n861 should state which ones are omitted from the script and why.   \n862 \u2022 At submission time, to preserve anonymity, the authors should release anonymized ver  \n863 sions (if applicable).   \n864 \u2022 Providing as much information as possible in supplemental material (appended to the   \n865 paper) is recommended, but including URLs to data and code is permitted.   \n866 6. Experimental Setting/Details   \n867 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n868 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n869 results?   \n870 Answer: [Yes]   \n871 Justification: Detailed information regarding the training and test setups, including data   \n872 splits, hyperparameters and their selection process, the type of optimizer used, and other   \n873 relevant details, are thoroughly documented in Section 4.2 of the paper.   \n874 Guidelines:   \n875 \u2022 The answer NA means that the paper does not include experiments.   \n876 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n877 that is necessary to appreciate the results and make sense of them.   \n878 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n879 material.   \n880 7. Experiment Statistical Significance   \n881 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n882 information about the statistical significance of the experiments?   \n883 Answer: [Yes]   \n884 Justification: The paper reports the statistical significance of the experiments by detailing   \n885 the results of the Friedman test and the Nemenyi test in Appendix D.3.   \n886 Guidelines:   \n887 \u2022 The answer NA means that the paper does not include experiments.   \n888 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence   \n889 intervals, or statistical significance tests, at least for the experiments that support the main   \n890 claims of the paper.   \n891 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n892 example, train/test split, initialization, random drawing of some parameter, or overall run   \n893 with given experimental conditions).   \n894 \u2022 The method for calculating the error bars should be explained (closed form formula, call   \n895 to a library function, bootstrap, etc.)   \n896 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n897 \u2022 It should be clear whether the error bar is the standard deviation or the standard error of   \n898 the mean.   \n899 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably   \n900 report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality   \n901 of errors is not verified.   \n902 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures   \n903 symmetric error bars that would yield results that are out of range (e.g. negative error   \n904 rates).   \n905 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how they   \n906 were calculated and reference the corresponding figures or tables in the text.   \n907 8. Experiments Compute Resources   \n908 Question: For each experiment, does the paper provide sufficient information on the com  \n909 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n910 the experiments?   \n911 Answer: [Yes]   \n912 Justification: Detailed information on the compute resources, including the type of compute   \n913 workers (CPU/GPU), memory, and execution time for each experiment, is provided in the   \n914 supplementary materials, enabling accurate reproduction of the experiments.   \n915 Guidelines:   \n916 \u2022 The answer NA means that the paper does not include experiments.   \n917 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or   \n918 cloud provider, including relevant memory and storage.   \n919 \u2022 The paper should provide the amount of compute required for each of the individual   \n920 experimental runs as well as estimate the total compute.   \n921 \u2022 The paper should disclose whether the full research project required more compute than   \n922 the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t   \n923 make it into the paper).   \n924 9. Code Of Ethics   \n925 Question: Does the research conducted in the paper conform, in every respect, with the   \n926 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n927 Answer: [Yes]   \n928 Justification: The research adheres to the NeurIPS Code of Ethics, including considerations   \n929 for anonymity, fairness, and transparency, with no deviations reported or necessary under   \n930 current laws or regulations.   \n931 Guidelines:   \n932 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n933 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n934 deviation from the Code of Ethics.   \n935 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration   \n936 due to laws or regulations in their jurisdiction).   \n937 10. Broader Impacts   \n938 Question: Does the paper discuss both potential positive societal impacts and negative   \n939 societal impacts of the work performed?   \n940 Answer: [Yes]   \n941 Justification: The paper thoroughly discusses both the potential positive impacts, such as   \n942 enhancements in anomaly detection for critical applications.   \n943 Guidelines:   \n944 \u2022 The answer NA means that there is no societal impact of the work performed.   \n945 \u2022 If the authors answer NA or No, they should explain why their work has no societal impact   \n946 or why the paper does not address societal impact.   \n947 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n948 (e.g., disinformation, generating fake proflies, surveillance), fairness considerations (e.g.,   \n949 deployment of technologies that could make decisions that unfairly impact specific groups),   \n950 privacy considerations, and security considerations.   \n951 \u2022 The conference expects that many papers will be foundational research and not tied to   \n952 particular applications, let alone deployments. However, if there is a direct path to any   \n953 negative applications, the authors should point it out. For example, it is legitimate to point   \n954 out that an improvement in the quality of generative models could be used to generate   \n955 deepfakes for disinformation. On the other hand, it is not needed to point out that a   \n956 generic algorithm for optimizing neural networks could enable people to train models that   \n957 generate Deepfakes faster.   \n958 \u2022 The authors should consider possible harms that could arise when the technology is being   \n959 used as intended and functioning correctly, harms that could arise when the technology is   \n960 being used as intended but gives incorrect results, and harms following from (intentional   \n961 or unintentional) misuse of the technology.   \n962 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n963 strategies (e.g., gated release of models, providing defenses in addition to attacks, mecha  \n964 nisms for monitoring misuse, mechanisms to monitor how a system learns from feedback   \n965 over time, improving the efficiency and accessibility of ML).   \n966 11. Safeguards   \n967 Question: Does the paper describe safeguards that have been put in place for responsible   \n968 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n969 image generators, or scraped datasets)?   \n970 Answer: [NA]   \n971 Justification: The paper poses no such risks.   \n972 Guidelines:   \n973 \u2022 The answer NA means that the paper poses no such risks.   \n974 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n975 necessary safeguards to allow for controlled use of the model, for example by requiring   \n976 that users adhere to usage guidelines or restrictions to access the model or implementing   \n977 safety filters.   \n978 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n979 should describe how they avoided releasing unsafe images.   \n980 \u2022 We recognize that providing effective safeguards is challenging, and many papers do not   \n981 require this, but we encourage authors to take this into account and make a best faith   \n982 effort.   \n983 12. Licenses for existing assets   \n984 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n985 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n986 properly respected?   \n987 Answer: [Yes]   \n988 Justification: The paper appropriately credits the creators of the utilized assets, including   \n989 code, data, and models, and explicitly mentions the licenses and terms of use, ensuring   \n990 compliance with the original terms set by the asset owners.   \n991 Guidelines:   \n992 \u2022 The answer NA means that the paper does not use existing assets.   \n993 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n994 \u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n995 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n996 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service   \n997 of that source should be provided.   \n998 \u2022 If assets are released, the license, copyright information, and terms of use in the package   \n999 should be provided. For popular datasets, paperswithcode.com/datasets has curated   \n1000 licenses for some datasets. Their licensing guide can help determine the license of a   \n1001 dataset.   \n1002 \u2022 For existing datasets that are re-packaged, both the original license and the license of the   \n1003 derived asset (if it has changed) should be provided.   \n1004 \u2022 If this information is not available online, the authors are encouraged to reach out to the   \n1005 asset\u2019s creators.   \n1006 13. New Assets   \n1007 Question: Are new assets introduced in the paper well documented and is the documentation   \n1008 provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "009 Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not release new assets. Guidelines: \u2022 The answer NA means that the paper does not release new assets. \u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. \u2022 The paper should discuss whether and how consent was obtained from people whose asset is used. \u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.   \n14. Crowdsourcing and Research with Human Subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.   \n15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. \u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. \u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]