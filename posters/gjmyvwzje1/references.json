{"references": [{"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "publication_date": "2019-05-01", "reason": "This paper introduces BERT, a foundational transformer-based language model that significantly advanced the field and is frequently used as a baseline in other LM research."}, {"fullname_first_author": "Alec Radford", "paper_title": "Improving Language Understanding by Generative Pre-Training", "publication_date": "2018-06-11", "reason": "This paper introduces GPT, a pioneering decoder-based language model that demonstrated the effectiveness of generative pre-training and significantly impacted the development of large language models."}, {"fullname_first_author": "Maximillian Nickel", "paper_title": "Poincar\u00e9 Embeddings for Learning Hierarchical Representations", "publication_date": "2017-12-03", "reason": "This paper introduces Poincar\u00e9 embeddings, a method for representing hierarchical data in hyperbolic space, which is a key concept utilized in the current research."}, {"fullname_first_author": "Octavian Ganea", "paper_title": "Hyperbolic Entailment Cones for Learning Hierarchical Embeddings", "publication_date": "2018-07-01", "reason": "This paper builds upon Poincar\u00e9 embeddings by introducing hyperbolic entailment cones, further refining the techniques for representing hierarchical structures in hyperbolic space, which directly informs the current work."}, {"fullname_first_author": "Nils Reimers", "paper_title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "publication_date": "2019-11-01", "reason": "This paper introduces Sentence-BERT, a modification of BERT specifically designed for generating sentence embeddings, which is directly applied in the current research for creating entity embeddings."}]}