[{"figure_path": "GJMYvWzjE1/tables/tables_4_1.jpg", "caption": "Table 1: Statistics of WordNet (Noun), Schema.org, and FoodOn, including the numbers of entities (#Entity), direct subsumptions (#DirectSub), indirect subsumptions (#IndirectSub), and the dataset splittings (#Dataset) for Multi-hop Inference and Mixed-hop Prediction tasks. Note that the numbers in #Dataset are counts of entity pairs rather than entity triplets.", "description": "This table presents the statistics of four datasets used in the paper's experiments: WordNet (Noun), Schema.org, FoodOn, and DOID. For each dataset, it shows the number of entities, direct subsumptions (one-hop relationships), indirect subsumptions (multi-hop relationships), and the dataset split into training, validation, and testing sets for both Multi-hop Inference and Mixed-hop Prediction tasks.  The numbers in the '#Dataset' column represent the number of entity pairs, not triplets.", "section": "4.2 Dataset Construction"}, {"figure_path": "GJMYvWzjE1/tables/tables_6_1.jpg", "caption": "Table 2: Multi-hop Inference and Mixed-hop Prediction test results on WordNet.", "description": "This table presents the results of the Multi-hop Inference and Mixed-hop Prediction tasks performed on the WordNet dataset.  It compares the performance of several models, including pre-trained Language Models (LMs), fine-tuned LMs, and HIT models (Hierarchy Transformer encoders, the authors' proposed method) across different experimental settings.  These settings involve random negative samples and hard negative samples (sibling entities) used during the training process. The metrics used for comparison are Precision, Recall, and F-score.  The table shows how HIT models consistently outperform baseline models across various scenarios.", "section": "4.4 Results"}, {"figure_path": "GJMYvWzjE1/tables/tables_7_1.jpg", "caption": "Table 3: Transfer Mixed-hop Prediction test results on Schema.org, FoodOn, and DOID.", "description": "This table presents the results of the transfer learning experiments.  The models were trained on WordNet and evaluated on three other ontologies (Schema.org, FoodOn, and DOID).  The table shows the precision, recall, and F-score for both random and hard negative samples.  The purpose is to assess how well models trained on one hierarchy generalize to other, unseen hierarchies.", "section": "4.4 Results"}, {"figure_path": "GJMYvWzjE1/tables/tables_8_1.jpg", "caption": "Table 4: Statistical correlations between WordNet entities' depths and their hyperbolic norms across different hyperbolic models.", "description": "This table presents the Pearson correlation coefficients between the hyperbolic norms of entities and their depths in the WordNet hierarchy for three different hyperbolic models: HIT, Poincar\u00e9Embed, and HyperbolicCone.  The correlation coefficient measures the linear relationship between the two variables. A higher correlation coefficient indicates a stronger linear relationship.  The results show that all three models exhibit a positive correlation, indicating that entities with higher depths (further down the hierarchy) tend to have higher hyperbolic norms, which is expected.", "section": "4.5 Analysis of HIT Embeddings"}, {"figure_path": "GJMYvWzjE1/tables/tables_8_2.jpg", "caption": "Table 5: Multi-hop Inference and Mixed-hop Prediction test results on WordNet.", "description": "This table presents the results of Multi-hop Inference and Mixed-hop Prediction tasks performed on the WordNet dataset.  It compares the performance of different models, including pre-trained Language Models (LMs), fine-tuned LMs, and the proposed Hierarchy Transformer encoders (HITs), across two negative sampling strategies: random negatives and hard negatives. The metrics used for evaluation are Precision, Recall, and F1-score (F-score). The results demonstrate the superiority of the HIT models, particularly in the hard negative setting, signifying their robustness in generalising from asserted to both inferred and unseen subsumption relationships.", "section": "4.4 Results"}, {"figure_path": "GJMYvWzjE1/tables/tables_13_1.jpg", "caption": "Table 6: Ablation results (F-score) of allMiniLM-L12-v2+HIT on WordNet\u2019s Mixed-hop Prediction.", "description": "This table shows the impact of varying the hyperparameters (loss margins \u03b1 and \u03b2) on the performance of the allMiniLM-L12-v2+HIT model, specifically on the WordNet Mixed-hop Prediction task.  It demonstrates the model's robustness to changes in these hyperparameters, indicating consistent high performance even with different values.", "section": "4.4 Results"}, {"figure_path": "GJMYvWzjE1/tables/tables_13_2.jpg", "caption": "Table 7: Statistics of SNOMED-CT, including the numbers of entities (#Entity), direct subsumptions (#DirectSub), indirect subsumptions (#IndirectSub), and the dataset splittings (#Dataset) for Multi-hop Inference and Mixed-hop Prediction tasks.", "description": "This table presents the statistics of the SNOMED CT hierarchy, including the number of entities, direct and indirect subsumptions, and the dataset split for both multi-hop inference and mixed-hop prediction tasks.  It shows the counts of entity pairs (not triplets).", "section": "4.2 Dataset Construction"}, {"figure_path": "GJMYvWzjE1/tables/tables_14_1.jpg", "caption": "Table 8: Mixed-hop Prediction test results on SNOMED and Transfer Mixed-hop Prediction results on Schema.org, FoodOn, and DOID.", "description": "This table presents the results of the Mixed-hop Prediction task on the SNOMED CT ontology, along with transfer learning results to other ontologies (Schema.org, FoodOn, and DOID).  It compares the performance of three models: the pre-trained all-MiniLM-L12-v2 model, the fine-tuned version of this model, and the HIT (Hierarchy Transformer) model. Results are shown separately for random and hard negative samples, indicating the performance metrics (Precision, Recall, and F-score) for each model on each dataset. The table highlights the effectiveness of HIT in handling various datasets and the robustness against overfitting.", "section": "4.4 Results"}]