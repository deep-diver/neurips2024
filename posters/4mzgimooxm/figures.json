[{"figure_path": "4mzGiMooXM/figures/figures_1_1.jpg", "caption": "Figure 1: Analysis of the CLIP text encoder for understanding attributes. There is a discrepancy between the word and [EOT] embeddings of the attribute bias on different objects.", "description": "This figure analyzes how the CLIP text encoder understands attributes and how this understanding impacts diffusion models.  It shows a comparison of word embeddings and end-of-text ([EOT]) embeddings for various objects with different color attributes. The discrepancy between word and [EOT] embeddings highlights a contextual issue that affects attribute binding in the text space, influencing the quality of image generation by diffusion models.", "section": "2 Analysis of the CLIP text encoder and the diffusion model"}, {"figure_path": "4mzGiMooXM/figures/figures_2_1.jpg", "caption": "Figure 2: (a) Fine-grained study through our designed embedding swapping experiment. The context issue in padding embeddings for (b) single-concept scenario, and (c) multi-concept scenario.", "description": "This figure presents a fine-grained analysis of the impact of embedding manipulation on text-to-image generation.  (a) shows three example prompts ('red chair', 'black sheep', 'blue apple') with four variations each, demonstrating how changing the contextualized word embedding, [EOT] embedding, and padding embeddings affects the generated images. (b) and (c) visualize the cosine similarity between [EOT] and padding tokens for single-concept and multi-concept prompts, respectively, illustrating the context issue in padding embeddings where the padding tokens forget the context or entangle different concepts. This highlights the contextual problem in padding embeddings, which is crucial for understanding the attribute binding problem.", "section": "2 Analysis of the CLIP text encoder and the diffusion model"}, {"figure_path": "4mzGiMooXM/figures/figures_3_1.jpg", "caption": "Figure 3: Overview of the proposed Magnet. We manipulate the object embedding with the positive and negative binding vectors, which are estimated with the guidance of neighbor objects.", "description": "This figure illustrates the Magnet framework's architecture. It shows how the framework works, starting from receiving a prompt (P) to generating an image using Stable Diffusion (SD).  The framework involves manipulating the object embeddings by adding positive and negative binding vectors to each object in the prompt. These vectors are generated using both the object's own information and information from its neighboring objects to increase the accuracy of the binding process. Adaptive strength parameters \u03b1i and \u03b2i are introduced to balance the influence of the positive and negative vectors, making the binding strength dynamic and context-aware. The framework operates entirely in the text space, requiring no additional datasets or training.", "section": "3 Magnet: disentangling concepts with the binding vector"}, {"figure_path": "4mzGiMooXM/figures/figures_6_1.jpg", "caption": "Figure 4: Qualitative comparison using prompts from ABC-6K and CC-500 datasets. For each prompt, we show the image generated by each method under the same seed.", "description": "This figure displays a qualitative comparison of the results produced by four different methods (Stable Diffusion, Structure Diffusion, Attend-and-Excite, and Magnet) on various prompts from two datasets (ABC-6K and CC-500). Each row represents a different prompt, and each column represents a different method.  The images show the models' ability to generate images that correctly reflect the attributes and objects specified in the prompt. The figure highlights the differences in the image quality and the accuracy of attribute binding across the different models.", "section": "4.4 Qualitative comparison"}, {"figure_path": "4mzGiMooXM/figures/figures_6_2.jpg", "caption": "Figure 5: Prompts with unnatural concepts. Baselines generate exchanged colors (row 1) or unwanted artifacts (row 2) while Magnet demonstrates the anti-prior ability with high-quality outputs.", "description": "This figure shows the results of generating images from prompts containing unnatural concepts (e.g., blue banana, black sheep).  It compares the outputs of Stable Diffusion, Attend-and-Excite, and the proposed Magnet method. The baselines struggle to generate realistic images, often mixing up colors or producing unrealistic artifacts. In contrast, Magnet successfully generates high-quality images that accurately reflect the unnatural concepts in the prompts, demonstrating its ability to overcome biases learned during training.", "section": "Qualitative comparison"}, {"figure_path": "4mzGiMooXM/figures/figures_7_1.jpg", "caption": "Figure 6: Ablation study on the hyperparameter \u03bb given the prompt \u201ca pink cake with white roses on silver plate\u201d. A small value of \u03bb can not well disentangle different concepts, while a large value causes artifacts in the generated image (best viewed zoomed in). We empirically set \u03bb = 0.6.", "description": "This ablation study demonstrates the effect of the hyperparameter \u03bb on the ability of the Magnet method to disentangle different concepts within a generated image. Using the prompt \"a pink cake with white roses on silver plate\", the figure shows the results for three different values of \u03bb: 0.0, 1.0, and 0.6 (the value used in the paper).  The images show that a low value of \u03bb (0.0) fails to adequately separate the concepts, while a high value (1.0) introduces artifacts.  The intermediate value of \u03bb (0.6) provides the best balance between concept separation and image quality.", "section": "4.5 Ablation study"}, {"figure_path": "4mzGiMooXM/figures/figures_7_2.jpg", "caption": "Figure 7: Ablation study. The neighbor strategy improves the binding vector estimation, separating different attributes (\"cup\" is purely \"blue\") and objects (\"backpack\" and \"apple\" are distinguishable).", "description": "This figure shows an ablation study comparing the results of using a single object versus multiple neighbor objects to estimate the binding vector in the Magnet model.  The results demonstrate that incorporating neighbor objects significantly improves the accuracy of the binding vector, leading to better separation of attributes and objects in the generated images. The left panel shows that when only using a single object to estimate the binding vector,  the attribute \"blue\" is not well-associated with the \"cup\", whereas with neighbors, the \"blue\" attribute is correctly bound to the \"cup\". Similarly, on the right panel, with a single object, the generated image contains a \"blue\" apple and a green backpack. When using neighbors, the objects are properly separated.", "section": "4.5 Ablation study"}, {"figure_path": "4mzGiMooXM/figures/figures_7_3.jpg", "caption": "Figure 7: Ablation study. The neighbor strategy improves the binding vector estimation, separating different attributes (\"cup\" is purely \"blue\") and objects (\"backpack\" and \"apple\" are distinguishable).", "description": "This figure shows the results of an ablation study comparing the performance of the proposed Magnet method with and without the neighbor strategy.  The neighbor strategy enhances the accuracy of binding vector estimation, leading to better separation of attributes and objects in image generation.  The improvements are demonstrated using examples of image generation where attributes and objects are correctly bound when using the neighbor strategy but mis-bound otherwise. This highlights the importance of the neighbor strategy for accurate and effective attribute and object binding.", "section": "4.5 Ablation study"}, {"figure_path": "4mzGiMooXM/figures/figures_8_1.jpg", "caption": "Figure 8: Ablation study on the effectiveness of the binding vector.", "description": "This figure shows an ablation study on the effectiveness of the binding vector. It presents images generated with different combinations of positive and negative binding vectors (\u03b1\u2081, \u03b2\u2081 and \u03b1\u2082, \u03b2\u2082). Specifically, it showcases results where both \u03b1\u2081 and \u03b1\u2082 are set to 1 (positive binding) and where both are set to -1 (negative binding). The images demonstrate how the binding vector influences the generation of objects and their attributes within the context of a prompt.", "section": "4.5 Ablation study"}, {"figure_path": "4mzGiMooXM/figures/figures_8_2.jpg", "caption": "Figure 9: Magnet can be combined with the optimization method, Attend-and-Excite [7]. (a) Magnet improves the loss during optimization. (b) Magnet improves the disentanglement of concepts.", "description": "This figure shows the results of combining Magnet with the optimization method Attend-and-Excite.  The left panel (a) is a graph showing that adding Magnet to Attend-and-Excite reduces the optimization loss. The right panel (b) shows a qualitative comparison of images generated by Attend-and-Excite alone and with Magnet added.  The images demonstrate that Magnet improves the disentanglement of concepts, leading to more realistic and visually appealing results.", "section": "4.6 Extensions"}, {"figure_path": "4mzGiMooXM/figures/figures_9_1.jpg", "caption": "Figure 10: Magnet can be integrated into other T2I models and with existing controlling modules.", "description": "This figure shows that the proposed Magnet method can be integrated with other text-to-image (T2I) models and existing controlling modules such as layout-guidance and ControlNet.  The results demonstrate the versatility and compatibility of Magnet, showing that it can improve image generation across different model architectures and control schemes.", "section": "4 Experiments"}, {"figure_path": "4mzGiMooXM/figures/figures_9_2.jpg", "caption": "Figure 11: Image editing comparisons using prompts from Prompt-to-Prompt [15].", "description": "This figure shows a comparison of image editing results between the Prompt-to-Prompt method and the Magnet method proposed in the paper.  The source prompt is \"a car on the side of the street.\"  Different prompts are generated by modifying the initial prompt to change the type of car (\"old car,\" \"crushed car,\" \"sport car\") and the street conditions (\"flooded street,\" \"forest street,\" \"snowy street\"). The top row shows the edits from the Prompt-to-Prompt method, while the bottom row displays the edits from the Magnet method. The comparison highlights the differences in image generation and editing capabilities of the two methods.", "section": "6 Limitations"}, {"figure_path": "4mzGiMooXM/figures/figures_14_1.jpg", "caption": "Figure 12: Principal Component Analysis (PCA) analysis of CLIP ViT-H/14 and CLIP ViT-L/14. The word embedding and the [EOT] embedding have a different understanding of the attribute.", "description": "This figure presents the results of a Principal Component Analysis (PCA) applied to word embeddings and End-of-Text ([EOT]) embeddings from two different CLIP text encoders (ViT-L/14 and ViT-H/14).  The PCA reduces the dimensionality of the embeddings to visualize them in 3D space.  The plots show that word embeddings and [EOT] embeddings differ significantly in their representation of attributes, indicating that the two types of embeddings capture different aspects of the input text and its semantic meaning.  This difference highlights a key aspect of the text encoder limitations and informs the design of Magnet.", "section": "A.1 Analysis of the CLIP text encoder"}, {"figure_path": "4mzGiMooXM/figures/figures_15_1.jpg", "caption": "Figure 1: Analysis of the CLIP text encoder for understanding attributes. There is a discrepancy between the word and [EOT] embeddings of the attribute bias on different objects.", "description": "This figure analyzes how CLIP's text encoder handles attributes, comparing embeddings with and without color context for various objects.  It reveals inconsistencies between word embeddings and end-of-text ([EOT]) embeddings, highlighting a phenomenon called \"attribute bias.\" This bias impacts how diffusion models understand and bind attributes, especially in complex prompts. The graphs showcase cosine similarity and Euclidean distance between embeddings under various conditions.", "section": "2 Analysis of the CLIP text encoder and the diffusion model"}, {"figure_path": "4mzGiMooXM/figures/figures_15_2.jpg", "caption": "Figure 1: Analysis of the CLIP text encoder for understanding attributes. There is a discrepancy between the word and [EOT] embeddings of the attribute bias on different objects.", "description": "This figure analyzes how CLIP's text encoder handles attributes, revealing inconsistencies in how word embeddings and end-of-text ([EOT]) embeddings represent attribute bias across different objects.  The visualizations show that there is not a consistent pattern in how the encoder handles attribute understanding, highlighting a potential problem with how attributes are encoded which can impact downstream diffusion models.", "section": "Analysis of the CLIP text encoder and the diffusion model"}, {"figure_path": "4mzGiMooXM/figures/figures_16_1.jpg", "caption": "Figure 15: Several effects of the context issue in padding embeddings under the scenarios of (a) single-concept and (b) multi-concept. We refer to the detailed analysis in Appendix A.3.", "description": "This figure visualizes how the context issue in padding embeddings affects the generation process in single-concept and multi-concept scenarios.  In single-concept scenarios, it shows that inaccurate object representations or the generation of natural concepts instead of the target unnatural concepts can occur.  In multi-concept scenarios, this issue results in color leakage, objects sticking together, and even missing objects.  This highlights the impact of entangled contextual information in the padding embeddings, which leads to various image generation problems.", "section": "2 Analysis of the CLIP text encoder and the diffusion model"}, {"figure_path": "4mzGiMooXM/figures/figures_17_1.jpg", "caption": "Figure 16: Statistical analysis of  obtained from 19648 samples (614 objects and 32 attributes). We set  = 0.6 where the count drops.", "description": "This histogram shows the distribution of the cosine similarity (\u03c9) between the first [EOT] embedding and the last padding embedding of the positive concept (Ppos) across 19648 samples. The x-axis represents the cosine similarity values, and the y-axis represents the count of samples with that similarity. The peak of the distribution is around \u03c9 = 0.7, indicating a strong correlation between the two embeddings for most samples. The value \u03c9 = 0.6 is chosen as the hyperparameter \u03bb for the adaptive strength of binding vectors because it's where the count begins to significantly drop, suggesting that values below this threshold represent weaker correlations and therefore weaker binding.", "section": "3 Magnet: disentangling concepts with the binding vector"}, {"figure_path": "4mzGiMooXM/figures/figures_19_1.jpg", "caption": "Figure 1: Analysis of the CLIP text encoder for understanding attributes. There is a discrepancy between the word and [EOT] embeddings of the attribute bias on different objects.", "description": "This figure analyzes how the CLIP text encoder understands attributes and how that impacts diffusion models. It shows a discrepancy between word embeddings and [EOT] (End of Text) embeddings in terms of attribute bias across various objects.  The visualization highlights how the way the CLIP encoder processes text affects the quality of image generation in diffusion models.  Different color prompts were given for different objects and the resulting CLIP vector similarities and norms are plotted.", "section": "2 Analysis of the CLIP text encoder and the diffusion model"}, {"figure_path": "4mzGiMooXM/figures/figures_20_1.jpg", "caption": "Figure 1: Analysis of the CLIP text encoder for understanding attributes. There is a discrepancy between the word and [EOT] embeddings of the attribute bias on different objects.", "description": "The figure analyzes how the CLIP text encoder understands attributes, focusing on the difference between word embeddings and end-of-text ([EOT]) embeddings. It reveals a discrepancy in how these two types of embeddings represent attribute bias across different objects.  This discrepancy impacts the quality of image generation in diffusion models.", "section": "2 Analysis of the CLIP text encoder and the diffusion model"}, {"figure_path": "4mzGiMooXM/figures/figures_20_2.jpg", "caption": "Figure 19: Ablation study on the hyperparameter K. We emphasize that K = 5 may not always be the best choice because of the randomly initialized latent. For example, the result of K = 3 is more appealing than K = 5. We choose K = 5 which can stabilize the generate of unnatural concepts (e.g., \"blue bananas\" and \"yellow stickers\" can be more distinguishable in K = 5 than K = 3), as well as balance the processing time.", "description": "This figure shows an ablation study on the hyperparameter K used in the Magnet model.  The study evaluates the effect of varying the number of neighbor objects considered when estimating the binding vector. The results indicate that using K=5 offers a good balance between generating high-quality, disentangled images and computational efficiency.  While other values of K might produce better results in certain instances due to the stochastic nature of the latent diffusion process, K=5 provides a more consistent performance across different scenarios and prevents excessive processing times.", "section": "4.5 Ablation study"}, {"figure_path": "4mzGiMooXM/figures/figures_21_1.jpg", "caption": "Figure 20: Ablation study on negative and positive binding vectors. (a) depicts similar results. (b) verifies using both vectors can alleviate the missing object (i.e., \\\"green bench\\\" ). (c) verifies using both vectors can enhance the binding (\\\"orange dog\\\" and \\\"gray bow tie\\\" ).", "description": "This figure shows an ablation study on the impact of using positive and negative binding vectors in the Magnet method.  It presents three examples with varying combinations of the vectors and compares the generated images to those produced by Stable Diffusion. The results demonstrate that using both positive and negative binding vectors significantly improves the quality and accuracy of attribute binding in generated images, helping to resolve issues like missing objects and enhance the clarity of attributes.", "section": "4.5 Ablation study"}, {"figure_path": "4mzGiMooXM/figures/figures_22_1.jpg", "caption": "Figure 21: Limitations of the proposed Magnet. (a) shows two cases that amend the concept, while still missing one object; (b) includes out-of-distribution results caused by the excessive value of \u03b1, \u03b2; (c) depicts an interesting phenomenon that Magnet correctly disentangles concepts while failing to in accordance with the location word \"in\". (d) shows Magnet will produce entangled concepts due to the limited power of SD. (e) provides two fail cases to generate unnatural concepts.", "description": "This figure demonstrates several limitations of the Magnet method.  It shows examples where Magnet fails to generate all the objects requested (neglect of object), generates images that are unrealistic or outside the expected distribution (over-manipulation and out-of-distribution), has issues with the spatial arrangement of objects relative to each other (wrong positional relation), and still struggles with the entanglement of concepts that are too closely related (concept entanglement). It also highlights how Magnet's ability to generate images with unusual concepts (e.g., bananas with bears inside) is limited by the model's inherent biases (strong attribute bias).", "section": "Limitations"}, {"figure_path": "4mzGiMooXM/figures/figures_22_2.jpg", "caption": "Figure 26: Qualitative comparison using prompts from the ABC-6K dataset. We provide some typical indoor scene prompts and compare Magnet to baseline methods. Best viewed zoomed in.", "description": "This figure shows a qualitative comparison of image generation results using prompts from the ABC-6K dataset, which focuses on natural compositional prompts.  The comparison includes Stable Diffusion, Structure Diffusion, Attend-and-Excite, and the proposed Magnet method.  Each method's output is shown for several indoor scene prompts. The goal is to highlight the differences in image quality and the accuracy of object and attribute representation across the different methods.  The images are best viewed at a larger zoom level for finer detail.", "section": "4.4 Qualitative comparison"}, {"figure_path": "4mzGiMooXM/figures/figures_23_1.jpg", "caption": "Figure 23: Additional results of extension to Attend-and-Excite. In columns 1-2, Magnet only may neglect the object (e.g., \"gray stick\"). In columns 3-4, Magnet can generate images with unnatural concepts but would be painting-like. The combination (row 4) demonstrates improvement. Column 5 displays a failure case. The parameters may need to be modified to fit Magnet.", "description": "This figure shows the results of combining Magnet with the optimization method Attend-and-Excite.  The top row shows examples generated by Stable Diffusion (SD), while subsequent rows display results from using Magnet alone, Attend-and-Excite alone, and the combination of both.  The results demonstrate the impact of each method on generating images with various prompts that feature natural and unnatural concepts.  The last column illustrates a failure case where the combination of methods still fails to generate a satisfactory image. This highlights the need for parameter adjustments to optimize the Magnet method.", "section": "4.6 Extensions"}, {"figure_path": "4mzGiMooXM/figures/figures_23_2.jpg", "caption": "Figure 24: Magnet improves the synthesis quality by disentangling different concepts. Best viewed zoomed in.", "description": "This figure showcases a qualitative comparison of image generation results between Stable Diffusion and the proposed Magnet method for several prompts. Each row represents a different prompt, and the left column shows the outputs generated by Stable Diffusion while the right displays results from the Magnet approach.  The images demonstrate Magnet's ability to produce higher-quality and more faithful renderings to the prompt by effectively disentangling and separating different concepts within the image. For example, in some of the images,  Stable Diffusion outputs images where elements seem to merge or blend together in a way that doesn't accurately reflect the prompt's specifications. The zoomed-in view is recommended for better appreciation of the details.", "section": "4.4 Qualitative comparison"}, {"figure_path": "4mzGiMooXM/figures/figures_24_1.jpg", "caption": "Figure 25: Visualization of attention maps. The activation of different objects are more distinct in Magnet compared to SD. For instance, bananas are overlapped with stickers in row 1, while row 2 indicates disentangled concepts.", "description": "This figure visualizes the attention maps from Stable Diffusion and Magnet for five different prompts. Each row shows the attention maps for a single prompt, comparing the results from Stable Diffusion and Magnet. The goal is to demonstrate that Magnet produces more distinct attention maps, separating the objects more effectively compared to Stable Diffusion, where the activations of different objects tend to be more overlapped.", "section": "4.4 Qualitative comparison"}, {"figure_path": "4mzGiMooXM/figures/figures_25_1.jpg", "caption": "Figure 4: Qualitative comparison using prompts from ABC-6K and CC-500 datasets. For each prompt, we show the image generated by each method under the same seed.", "description": "This figure shows a qualitative comparison of the results obtained by four different text-to-image generation methods (Magnet, Attend-and-Excite, Stable Diffusion, and Structure Diffusion) on prompts from the ABC-6K and CC-500 datasets. For each prompt, the images generated by each method using the same random seed are displayed side-by-side, allowing for a visual comparison of the synthesis quality and attribute binding accuracy of each method.", "section": "4.4 Qualitative comparison"}, {"figure_path": "4mzGiMooXM/figures/figures_25_2.jpg", "caption": "Figure 4: Qualitative comparison using prompts from ABC-6K and CC-500 datasets. For each prompt, we show the image generated by each method under the same seed.", "description": "This figure displays a qualitative comparison of the results obtained by four different methods (Magnet, Attend-and-Excite, Stable Diffusion, and Structure Diffusion) when generating images from prompts in the ABC-6K and CC-500 datasets.  For each prompt, the images produced by all four methods using the same random seed are shown side-by-side. This allows for a direct visual comparison of the strengths and weaknesses of each approach in terms of image quality, object and attribute binding, and overall adherence to the prompt.", "section": "4.4 Qualitative comparison"}, {"figure_path": "4mzGiMooXM/figures/figures_25_3.jpg", "caption": "Figure 26: Qualitative comparison using prompts from the ABC-6K dataset. We provide some typical indoor scene prompts and compare Magnet to baseline methods. Best viewed zoomed in.", "description": "This figure shows a qualitative comparison of image generation results for various indoor scenes using four different methods: Stable Diffusion, Structure Diffusion, Attend-and-Excite, and the proposed Magnet.  The prompts used are complex and involve multiple attributes and objects, testing the ability of each method to accurately represent these details in the generated images.  The figure demonstrates that Magnet produces more realistic and accurate images compared to other methods, especially for challenging prompts with intricate details.", "section": "4.4 Qualitative comparison"}, {"figure_path": "4mzGiMooXM/figures/figures_26_1.jpg", "caption": "Figure 27: Additional results using prompts from the ABC-6K dataset.", "description": "This figure presents a qualitative comparison of image generation results across four different methods (Stable Diffusion, Structure Diffusion, Attend-and-Excite, and Magnet) using prompts from the ABC-6K dataset. Each row represents a different prompt, focusing on complex scene descriptions involving multiple objects and attributes.  The images generated by each method are displayed side-by-side, allowing for visual comparison and highlighting the strengths and weaknesses of each approach in handling intricate prompts and generating visually appealing and accurate results.", "section": "4.4 Qualitative comparison"}, {"figure_path": "4mzGiMooXM/figures/figures_27_1.jpg", "caption": "Figure 28: Additional results using prompts from the CC-500 dataset.", "description": "This figure shows a qualitative comparison of images generated by four different methods: Stable Diffusion, Structure Diffusion, Attend-and-Excite, and Magnet (the proposed method). Each row represents a different prompt from the CC-500 dataset, which contains prompts that combine two concepts, each with a color attribute. The figure demonstrates that Magnet outperforms the baselines in terms of object and attribute disentanglement, and in generating images that faithfully represent the prompts. In particular, Magnet is able to generate more realistic and detailed images, and it is less likely to produce artifacts or to misinterpret the prompt.", "section": "4.4 Qualitative comparison"}]