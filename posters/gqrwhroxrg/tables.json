[{"figure_path": "GqrWhROxrG/tables/tables_6_1.jpg", "caption": "Table 1: Results on ScanNet. \"GT Geo\" denotes whether ground truth geometry is used as supervision during training.", "description": "This table presents the results of different methods on the ScanNet dataset for indoor 3D object detection.  The performance is measured using mean Average Precision (mAP) at IoU thresholds of 0.25 and 0.5.  The \"GT Geo\" column indicates whether ground truth geometry was used for supervision during the training process.  The methods compared include ImGeoNet, CN-RMA, ImVoxelNet, NeRF-Det, and the proposed method, MVSDet.  The table shows the mAP values for each method under different supervision conditions.", "section": "4 Experiments"}, {"figure_path": "GqrWhROxrG/tables/tables_6_2.jpg", "caption": "Table 2: Results on ARKitScenes. \"GT Geo\" denotes whether ground truth geometry is used as supervision during training.", "description": "This table presents the results of the proposed MVSDet method and several baseline methods on the ARKitScenes dataset.  The mAP@.25 and mAP@.5 metrics are used to evaluate the performance of each method.  The \"GT Geo\" column indicates whether ground truth geometry was used for supervision during training.  The table shows that MVSDet outperforms the other methods that don't use ground truth geometry, highlighting its effectiveness.", "section": "4 Experiments"}, {"figure_path": "GqrWhROxrG/tables/tables_7_1.jpg", "caption": "Table 3: Ablation study of probabilistic sampling and soft weighting. All methods are conducted without using rendering loss.", "description": "This table presents the results of an ablation study conducted to evaluate the impact of probabilistic sampling and soft weighting on the model's performance. The study was performed without using the rendering loss, allowing for an isolated assessment of the effects of these two techniques. Three different model configurations were compared: one with only probabilistic sampling, one with only soft weighting, and one with both techniques enabled. The results, measured in terms of mean Average Precision (mAP) at thresholds of 0.25 and 0.5, demonstrate the importance of both probabilistic sampling and soft weighting for achieving optimal performance.", "section": "4.4 Ablation Study"}, {"figure_path": "GqrWhROxrG/tables/tables_7_2.jpg", "caption": "Table 4: Ablation study of Gaussian Splatting. M denotes the number of depth planes in the plane sweep. \u201cGaussian\u201d denotes using pixel-aligned Gaussian splatting. \u201cRMSE\u201d is the depth evaluation metric. \u201cMemory \u2206\u201d denotes the increased memory consumption during training.", "description": "This table presents an ablation study on the impact of using pixel-aligned Gaussian splatting (PAGS) on the model's performance. It shows the results with different numbers of depth planes (M) and with/without PAGS, evaluating the mean Average Precision (mAP) at thresholds of 0.25 and 0.5, and the Root Mean Squared Error (RMSE) of depth prediction. The increase in memory consumption due to adding PAGS is also indicated.", "section": "4.4 Ablation Study"}, {"figure_path": "GqrWhROxrG/tables/tables_8_1.jpg", "caption": "Table 5: Ablation study of Top-k depth proposals.", "description": "This table presents the results of an ablation study conducted to determine the optimal number of top-k depth proposals to use in the probabilistic sampling method.  The study varied the number of proposals (k) and measured the impact on mean Average Precision (mAP) at two different thresholds (0.25 and 0.5). The results show that using 3 depth proposals achieves the best performance.", "section": "4.4 Ablation Study"}, {"figure_path": "GqrWhROxrG/tables/tables_8_2.jpg", "caption": "Table 6: Time and memory comparison in training and testing stages on ScanNet dataset, respectively.", "description": "This table compares the training and testing time and memory usage of three different methods: CN-RMA, NeRF-Det, and the authors' proposed method.  The comparison highlights the computational efficiency of each approach on the ScanNet dataset.", "section": "4.3 Comparison with Baselines"}, {"figure_path": "GqrWhROxrG/tables/tables_11_1.jpg", "caption": "Table 8: Per-class results under AP@0.5 on ScanNet dataset.", "description": "This table presents a breakdown of the model's performance on the ScanNet dataset, specifically focusing on the average precision (AP) at an Intersection over Union (IoU) threshold of 0.5.  It shows the AP for each of the 18 object classes individually, offering a detailed view of the model's strengths and weaknesses in recognizing specific object types within the indoor scenes.", "section": "4.3 Comparison with Baselines"}, {"figure_path": "GqrWhROxrG/tables/tables_12_1.jpg", "caption": "Table 8: Per-class results under AP@0.5 on ScanNet dataset.", "description": "This table presents a breakdown of the model's performance (average precision at 0.5 IoU threshold) on the ScanNet dataset.  Each row represents a different object category from the dataset, and the columns show the average precision for that category.  The table allows for a detailed analysis of the model's accuracy across different object types.", "section": "4.3 Comparison with Baselines"}, {"figure_path": "GqrWhROxrG/tables/tables_12_2.jpg", "caption": "Table 9: Per-class results under AP@0.25 on ARKitScenes dataset.", "description": "This table presents a breakdown of the Average Precision (AP) at IoU threshold of 0.25 for each object class in the ARKitScenes dataset.  The results are categorized by object class (e.g., 'cab', 'fridg', 'shlf', etc.), allowing for a granular analysis of the model's performance on various object types. The table compares the performance of the proposed method (Ours) with the baseline method (NeRF-Det).  Each cell in the table represents the AP score achieved for a specific object class. It highlights the strengths and weaknesses of each method in detecting specific object categories.", "section": "4.3 Comparison with Baselines"}, {"figure_path": "GqrWhROxrG/tables/tables_12_3.jpg", "caption": "Table 10: Per-class results under AP@0.5 on ARKitScenes dataset.", "description": "This table presents a breakdown of the model's performance on the ARKitScenes dataset, specifically focusing on the average precision (AP) at a 0.5 intersection over union (IoU) threshold.  Each row represents a different object category (cab, fridge, shelf, etc.), and the columns show the AP for each category. This allows for detailed analysis of the model's strengths and weaknesses in recognizing various object types within indoor scenes.  Comparing these results to other methods in the paper provides insights into the model's relative performance across different object classes.", "section": "4.3 Comparison with Baselines"}]