[{"type": "text", "text": "MVSDet: Multi-View Indoor 3D Object Detection via Efficient Plane Sweeps ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yating Xu1 Chen Li2,3\u2217 Gim Hee Lee1 ", "page_idx": 0}, {"type": "text", "text": "Department of Computer Science, National University of Singapore1 Institute of High Performance Computing, A\\*STAR2 Centre for Frontier AI Research, $\\mathrm{A^{*}S T A R^{3}}$ xu.yating@u.nus.edu lichen@u.nus.edu gimhee.lee@comp.nus.edu.sg ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The key challenge of multi-view indoor 3D object detection is to infer accurate geometry information from images for precise 3D detection. Previous method relies on NeRF for geometry reasoning. However, the geometry extracted from NeRF is generally inaccurate, which leads to sub-optimal detection performance. In this paper, we propose MVSDet which utilizes plane sweep for geometry-aware 3D object detection. To circumvent the requirement for a large number of depth planes for accurate depth prediction, we design a probabilistic sampling and soft weighting mechanism to decide the placement of pixel features on the 3D volume. We select multiple locations that score top in the probability volume for each pixel and use their probability score to indicate the confidence. We further apply recent pixel-aligned Gaussian Splatting to regularize depth prediction and improve detection performance with little computation overhead. Extensive experiments on ScanNet and ARKitScenes datasets are conducted to show the superiority of our model. Our code is available at https://github.com/Pixie8888/MVSDet. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Indoor 3D object detection is a fundamental task in scene understanding and has wide applications in robotics, AR/VR equipment, etc. Although point cloud based 3D objection methods [13, 14, 15] have achieved impressive performance, depth sensors are required to capture the data, which may not be available due to budget limitation, form factor constraints, etc. Recently, the more economic pipeline of 3D object detection from only posed multi-view images is gaining increasing attention. However, it is much more sophisticated to estimate geometry information from 2D images alone. ", "page_idx": 0}, {"type": "text", "text": "A straightforward solution to this problem is using ground truth geometry information, e.g. point cloud [19] or TSDF [17], to supervise the model. Built on the 3D volume representation [16], ImGeoNet [19] predicts the emptiness of each voxel by converting the ground truth point clouds to surface voxels as supervision. CN-RMA [17] first reconstructs 3D scenes using ground truth TSDF as supervision and then runs an existing point cloud based object detector to predict bounding boxes. Although they achieve promising performance, the precise ground truth scene geometry is hard to obtain and may not be available [1]. ", "page_idx": 0}, {"type": "text", "text": "An alternative way is to learn geometry via self-supervision. The pioneer work ImVoxelNet [16] unprojectes 2D image features to a 3D volume representation. However, 2D features can propagate to irrelevant 3D locations since depth information is not known. NeRF-Det [22] relies on a Neural Radiance Field (NeRF) [12] to learn a density field and queries an opacity score for each voxel. The opacity score is multiplied with voxel feature to decrease the influence of voxels in the empty space to the feature volume. Since the detection performance is completely determined by the quality of NeRF, enormous effort is spent on making NeRF generalizable and avoiding aliasing issue. Unfortunately, the geometry extracted from NeRF remains unsatisfactory due to insufficient surface constraints in the representation [21]. Consequently, it wrongly backprojects features to voxels in the free space (as illustrated by the red dots in the example shown on the middle of Fig. 1). ", "page_idx": 0}, {"type": "image", "img_path": "GqrWhROxrG/tmp/b03a0d2b2646bd793971c6c5437acea611bc35adb1e1305beb02d1874e879ef9.jpg", "img_caption": ["Figure 1: Comparison with NeRF-Det [22]. The 3D voxel centers (grey dots) are overlaid with the reference scene. The red dots denotes the erroneous backprojection pixel features to the points in the free space. Compared to NeRF-Det, we show much less inaccurate backprojections. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose MVSDet to extract geometry information from only the input multi-view images. A straightforward way is to leverage multi-view stereo algorithms [24, 25] to decide the accurate depths for correct placements of 2D features of each image to the 3D volume. However, accurate depth estimation in the plane-sweeping algorithm [8] requires computationally expensive sampling of many depth planes over all the multi-view images. We mitigate the computational complexity by proposing a probabilistic sampling and soft weighting mechanism to decide the possible depth locations for each pixel. Specifically, we sample multiple top scoring depth proposals in the probability volume that are most likely covering the true depth locations. Pixel features are placed onto the 3D volume only when the backprojected ray intersects at the depth locations. Since the normalized probability score indicates the confidence of the the current depth location, we use it to weigh the pixel feature before assigning the feature to its backprojected voxel center. ", "page_idx": 1}, {"type": "text", "text": "To further improve the depth prediction accuracy, we utilize the recent pixel-aligned Gaussian Splatting (PAGS) [3, 28] for novel view rendering as an additional supervision. PAGS predicts a 3D Gaussian primitive [9] for every pixel in the input views, and all the Gaussians are used to render novel views via rasterization-based splatting. Compare to NeRF that uses computation expensive volumetric sampling, Gaussian Splatting is fast and light-weight. A key to good rendering quality in PAGS is the correct positioning of the 3D Gaussians, which depends on an accurate depth prediction. As a result, by putting the Gaussians according to the depth map computed from the probability volume in the plane sweep module, the rendering loss would guide the the Gaussian centers and consequently the depths to the correct values. ", "page_idx": 1}, {"type": "text", "text": "In summary, our contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "1. We propose a probabilistic sampling and soft weighting mechanism to efficiently learn geometry without sampling many depth planes in multi-view stereo. Multiple depth proposals are sampled with the probability scores to guide the propagation of image features to 3D voxels.   \n2. We adopt pixel-aligned Gaussian Splatting to enhance depth prediction without much additional computation overhead, which consequently improves detection performance.   \n3. We conduct extensive experiments on the ScanNet and ARKitScene datasets to verify the effectivess of our method. Notably, we achieve significant improvements of $+3.9$ and $+5.1$ under the mAP $@0.5$ metric on ScanNet and ARKitScenes, respectively. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Indoor 3D Object Detection. 3D object detection for indoor scenes predicts three dimensional bounding boxes and corresponding classes by taking in 3D or 2D inputs. Point cloud is the most popular choice of 3D data for detection as it provide accurate 3D information. To detect objects from the irregularity and sparseness of the point cloud, VoteNet [14] utilizes Hough voting by points sub-sampling, voting and grouping to generate object proposals. Later methods improve VoteNet by either predicting geometric primitives [27] or using hierarchical graph network [4]. 3DETR [13] reduces hand-coded designs of VoteNet with transformer encoder-decoder blocks. Despite their promising performance, depth sensors are required to capture the data, which is not always available due to power consumption or budget limitation. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Alternatively, detecting 3D objects from images only [16, 22, 19, 17] is a cheaper choice, but with a sacrifice of losing geometry information. Some methods guide the model to predict scene geometry using ground truth geometry, i.e. ground truth surface voxels [19] or TSDF [17], as supervision. However, obtaining ground truth scene geometry is troublesome. In contrast, ImVoxelNet [16] builds a 3D feature volume in the world coordinate, where each voxel center aggregate the corresponding features of its projected 2D pixels. Subsequently, 3D U-Net is applied to refine the volume features and predict bounding boxes from each voxel center. However, voxels may wrongly aggregate irrelevant image features since depth is not known. Recently, NeRF-Det [22] uses NeRF to learn a density field and predict an opacity score per voxel center to downweigh the influence of voxels in the empty space to the feature volume. However, the implicit modeling of geometry in NeRF leads to unsatisfactory performance. Moreover, the reliance of NeRF during training side-tracked the effort in making NeRF generalizable. Instead of implicitly modeling geometry with NeRF, we utilize plane-swept cost volumes for geometry-aware scene reasoning. We propose a probabilistic sampling and soft weighting mechanism to accurately decide the placement of pixel features without sampling many depth planes. ", "page_idx": 2}, {"type": "text", "text": "Multi-View Depth Estimation. Multi-view depth estimation has long been studied in the multiview stereo [24, 20, 25, 5, 26, 11, 7]. MVSNet [24] constructs a 3D cost volume, regularize it with a 3D CNN and regresses the depth map from the probability volume. However, MVSNet consumes large memory due the expensive 3D cost volume. Follow-up works reduce computation by replacing 3D CNN with recurrent network [23, 25] or using coarse to fine depth estimation [5, 20, 2, 7]. Recurrent methods only reduces cost regularization module to a 2D network, but still faces a large 3D cost volume to predict accurate depth. Although coarse to fine pipeline can reduce the number of sampled depth planes, it still requires to sample a certain amount of initial depth locations, e.g. 32 to 64 planes [7, 20], to get a reasonable coarse depth map, which still leads to intractable computation in our multi-view object detection task. Bae et al. [2] propose to sample extremely few number (i.e. 5) of initial depth planes based on a pre-trained single view depth probability distribution. It has been further applied to outdoor multi-view object detection [10]. However, both of them need the guidance of ground truth depth to learn a correct monocular depth estimation, and would otherwise fail as shown in the experiment section. Instead, we propose a probabilistic sampling and soft weighting module to bypass the large 3D cost volume to decide the 3D locations of every pixel. We also novelly utilize Gaussian Splatting to enhance our depth prediction with little computation overhead. ", "page_idx": 2}, {"type": "text", "text": "3D Gaussian Splatting. 3D Gaussian Splatting (3DGS) [9] is a recent technique for novel view rendering. It models a 3D scene explicitly with Gaussian primitives, each of which is defined by a 3D Gaussian center, covairance matrix, opacity and color. Compared to volume rendering based NeRF [12], it achieves real-time rendering with much lighter computation. To avoid per-scene optimization of 3DGS, several works [28, 3, 6, 18] propose to model a scene with pixel-aligned Gaussians Splatting (PAGS). A Gaussian primitive is predicted per pixel, and all predicted Gaussians are then combined for novel view synthesis. A key to PAGS is the accurate 3D Gaussian center which is determined by the depth estimation. Thus, instead of targeting novel view synthesis, we novelly utilize it for 3D object detection through improving depth prediction. Our method is also significantly different from NeRF-Det. While NeRF plays a major role in NeRF-Det by predicting opacity scores, we use Gaussian Splatting as a regularizer to our plane sweep algorithm with little computation overhead. ", "page_idx": 2}, {"type": "text", "text": "3 Our Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Problem Definition. The goal of multi-view indoor 3D object detection is to predict bounding box $\\{\\boldsymbol{B}\\}\\subseteq\\mathbb{R}^{7}$ of objects in the 3D scene and their corresponding classes from $N$ posed images $\\left\\{\\mathrm{I}_{1},\\therefore\\cdot\\cdot,\\mathrm{I}_{N}\\right\\}\\,\\in\\,\\mathbb{R}^{H\\times W\\times3}$ . Each bounding box $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ is parameterized as $(x,y,z,w,h,l,\\phi)$ , where $\\left(x,y,z\\right)$ are the coordinates of the box center, $(w,h,l)$ are the width, height, and length, and $\\phi$ is the ", "page_idx": 2}, {"type": "image", "img_path": "GqrWhROxrG/tmp/24069ff9acc9300e60550625b246bd012b300cc58e580d593be853e572b9c460.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Overview of our MVSDet. The upper branch shows the detection pipeline with our proposed probabilistic sampling and soft weighting. The backprojected ray intersects at 3 points (shown as dots), but only the green point receives the pixel feature based on the selected depth proposals. The red points are denoted as invalid backprojection location and thus the pixel feature is not assigned to them. \u201cGT Location\u201d is the ground truth 3D location of the pixel. The lower branch shows the pixel-aligned Gaussian Splatting (PAGS). We select nearby views for the novel image from the images input to the detection branch and predict Gaussian maps on them. Note that PAGS is removed during testing. ", "page_idx": 3}, {"type": "text", "text": "rotation angle around ${\\bf Z}$ -axis. The intrinsic and extrinsic matrix for each input image are denoted as $\\mathbf{K}\\in\\mathbb{R}^{3\\times3}$ and $\\mathbf{P}=[\\mathbf{R}\\mid\\mathbf{t}]\\in\\mathbb{R}^{3\\times4}$ , respectively. ", "page_idx": 3}, {"type": "text", "text": "Overview. Fig. 2 shows our proposed MVSDet, a geometry-aware approach for indoor 3D object detection from posed multi-view images. Our MVSDet is built on 3D volume-based object detection (cf. Sec. 3.1). Instead of naively assigning the same 2D feature redundantly to every voxel center that intersect the backprojected ray, we place the pixel features according to the estimated depth from our proposed efficient plane sweep algorithm. We alleviate the costly sampling of many depth planes for accurate depth prediction by proposing a probabilistic sampling and soft weighting mechanism (cf. Sec. 3.2). We further utilize pixel-aligned Gaussian Splatting to enhance our depth prediction module with little extra computation cost (cf. Sec. 3.3). Intuitively, our depth-aware framework leads to more precise assignments of multi-view image features in the 3D volume and consequently better 3D object detection results. ", "page_idx": 3}, {"type": "text", "text": "3.1 Background: 3D Volume-Based Object Detection ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Existing methods [16, 22] estimate bounding boxes from a 3D feature volume aggregated from the multi-view image features. Image features $\\mathrm{F}\\in\\mathbb{R}^{\\frac{H}{4}\\times\\frac{W}{4}\\times C}$ are first extracted from every input image $\\mathbf{I}\\in\\mathbb{R}^{H\\times W\\times\\breve{3}}$ . A 3D volume is defined in the world space with the size of $N_{x}\\times N_{y}\\times N_{z}$ voxels. Voxel center $\\mathfrak{p}=[x,y,z]^{\\intercal}\\in\\mathbb{R}^{3}$ is projected to $i$ -th input image to obtain the 2D coordinate $[\\mathbf{u}_{i},\\mathbf{v}_{i}]^{\\top}\\in\\mathbb{R}^{2}$ as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n[\\mathbf{u}_{i}^{\\prime},\\mathbf{v}_{i}^{\\prime},\\mathbf{d}_{i}]^{\\top}=\\mathbf{K}_{i}^{\\prime}\\mathbf{P}_{i}[\\mathbf{p}^{\\top},1]^{\\top},\\quad[\\mathbf{u}_{i},\\mathbf{v}_{i}]^{\\top}=[\\mathbf{u}_{i}^{\\prime}/\\mathbf{d}_{i},\\mathbf{v}_{i}^{\\prime}/\\mathbf{d}_{i}]^{\\top},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{K}_{i}^{\\prime}$ is the scaled intrinsic matrix according the image downsampling ratio. Subsequently, 2D feature $\\mathbf{f}_{i}^{\\breve{}{}}\\in\\mathbb{R}^{C}$ is assigned to $\\mathfrak{p}$ via nearest neighbour interpolation as follow: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{f}_{i}=\\mathrm{interpolate}\\left(\\left(\\mathbf{u}_{i},\\mathbf{v}_{i}\\right),\\mathbf{F}_{i}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For $\\mathfrak{p}$ that are projected outside the boundary of feature map or behind the image plane, the projection is considered as invalid and we set $\\mathrm{f}_{i}=0$ . Finally, it averages all valid backprojected features as the voxel feature $\\begin{array}{r}{\\mathbf{v}=\\sum_{i=1}^{n}\\mathbf{f}_{i}/n\\in\\mathbb{R}^{C}}\\end{array}$ , where $n$ denotes the number of valid projection. ", "page_idx": 3}, {"type": "text", "text": "The feature volume is refined by a 3D U-Net before being fed into the detection head to predict category, a bounding box and centerness score on each voxel location. The training losses for detection consists of focal loss for classification $\\mathcal{L}_{\\mathrm{cls}}$ , cross-entropy loss for centerness ${\\mathcal{L}}_{\\mathrm{center}}$ , and IoU loss for location ${\\mathcal{L}}_{\\mathrm{loc}}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{det}}=\\mathcal{L}_{\\mathrm{cls}}+\\mathcal{L}_{\\mathrm{center}}+\\mathcal{L}_{\\mathrm{loc}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Observation. We observe that the feature aggregation method in existing multi-view indoor 3D object detection works causes 2D pixel features to be duplicated on voxels intersecting with the ray emitted from camera origin though the pixel as illustrated in Fig. 3. This is a result from lack of depth-awareness where voxels can end up aggregating irrelevant pixel features that are either not on the surface or occluded. ", "page_idx": 4}, {"type": "text", "text": "Proposition. To circumvent this problem, we propose: 1) an efficient plane sweep to instill depth awareness. Our efficient plane sweep consists of a probabilistic sampling and soft weighting mechanism based on a cost volume representation to evaluate the placement of pixel features on the intersected voxel centers; 2) a depth prediction regularizor based on 3D Gaussian Splatting to improve depth accuracy. ", "page_idx": 4}, {"type": "text", "text": "3.2 Efficient Plane Sweep ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Cost Volume Construction. We build $N$ cost volumes for $N$ input images to predict $N$ depth probability distribution maps. To construct the cost volume for the $i^{\\th}$ -th view, we set the image $\\mathrm{I}_{i}$ as the reference view and select 2 nearby views as the source views $\\mathrm{I}_{j\\in2\\mathrm{NB}(i)}$ . A raw matching volume for $\\mathrm{I}_{i}$ is constructed by backprojecting source feature map $\\mathrm{F}_{j\\in2\\mathrm{NB}(i)}\\,\\in\\,\\mathbb{R}^{\\frac{H}{4}\\times\\frac{\\mathrm{~\\bar{W}~}}{4}\\times C}$ into the coordinate system defined by $\\mathrm{I}_{i}$ at a stack of fronto-parallel virtual planes. The virtual planes $\\{\\mathsf{d}_{1},\\dotsc,\\mathsf{d}_{M}\\}$ are uniformly sampled on a predefined depth range. The coordinate mapping from the source feature map $\\mathrm{F}_{j}$ to the reference feature map $\\mathrm{F}_{i}$ at depth $\\mathrm{d}_{m}$ is determined by a planar homography transformation: ", "page_idx": 4}, {"type": "image", "img_path": "GqrWhROxrG/tmp/d48771e09c93a59749887901935790e153c5deaf38ad8a3804d5c728e7c7e3f7.jpg", "img_caption": ["Figure 3: Comparison of different feature backprojection methods. The pixel ray intersects at 4 voxel centers with the blue box denoting the ground truth 3D location of the pixel. Our method computes the placement of the pixel features based on the depth probability distribution (purple) and thus able to suppress incorrect intersections. "], "img_footnote": [], "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{q}_{j,m}=\\mathbf{K}_{j}\\left[\\mathbf{R}_{i j}\\mid\\mathbf{t}_{i j}\\right]\\left[\\mathbf{d}_{m}\\left(\\mathbf{K}_{i}^{-1}\\mathbf{q}\\right)^{\\top},1\\right]^{\\top}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{q}$ is the homogeneous coordinate of a pixel on $\\mathrm{{F}}_{i}$ , $\\mathbf{q}_{j,m}$ is the projected homogeneous coordinate of $\\mathbf{q}$ on $\\mathrm{F}_{j}$ . $\\ [\\mathbf{R}_{i j}\\ |\\ \\mathfrak{t}_{i j}]$ are the rotation and the translation of $\\mathrm{I}_{j}$ relative to $\\mathrm{I}_{i}$ . We use Eqn. 4 to warp every source feature map $\\mathrm{F}_{j\\in2\\mathrm{N}\\mathrm{B}(i)}$ into all the depth planes and use a variance based metric [24] to generate a cost volume $\\begin{array}{r}{\\mathrm{U}\\in\\mathbb{R}^{\\frac{H}{4}\\times\\frac{W}{4}\\times C\\times M}}\\end{array}$ . Subsequently, we use a shallow 3D CNN to refine U to generate a probability volume (after softmax) $\\mathbf{B}\\in\\mathbb{R}^{\\frac{H}{4}\\times\\frac{W}{4}\\times M}$ . We also predict an offset per depth bin to account for the discretization error. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Probabilistic Sampling and Soft Weighting. Although we can place pixel features to the 3D volume by predicting depth using the weighted average based on B, it requires sampling sufficient depth planes, which is prohibitive for our task. To this end, we propose a probabilistic sampling and soft weighting mechanism to decide the placement of pixel features on its backprojected ray without the need for many depth planes. ", "page_idx": 4}, {"type": "text", "text": "For every pixel, we sample $k$ depth locations that score top in the corresponding distribution in B as its depth proposals. We denote their depth values as $\\{\\mathbf{d}_{\\mathrm{idx}_{1}},\\dots,\\mathbf{d}_{\\mathrm{idx}_{k}}\\}$ , and also use their corresponding probability score to evaluate its confidence with respect to the ground truth location. The scores are denoted as $\\{\\tilde{\\mathbf{B}}_{\\mathrm{idx}_{1}},\\dotsc,\\tilde{\\mathbf{B}}_{\\mathrm{idx}_{k}}\\}$ , where $\\begin{array}{r}{\\tilde{\\mathrm{B}}_{\\mathrm{idx}_{k}}=\\mathrm{B}_{\\mathrm{idx}_{k}}/\\sum_{i=1}^{i=k}\\mathrm{B}_{\\mathrm{idx}_{i}}}\\end{array}$ . Intuitively, this is equivalent to split one depth prediction to multiple depth prediction  based on the scores. As a result, it is more likely to cover the ground truth location when depth bins are insufficient. ", "page_idx": 4}, {"type": "text", "text": "Suppose a voxel center $\\mathfrak{p}\\in\\mathbb{R}^{3}$ intersects the backprojected ray of a pixel from the $i^{\\th}$ -th image, and the depth of p under the $i$ -th camera frustum is $\\mathsf{d}(\\mathfrak{p})$ . We consider the projection of point $\\mathfrak{p}$ to the $i$ -th image as valid and set the indicator ${\\mathrm{g}}_{i}=1$ when $\\mathsf{d}(\\mathfrak{p})$ resides near any of the top- ${\\cdot k}$ depth proposals $\\{\\mathbf{d}_{\\mathrm{idx}_{1}},\\dots,\\mathbf{d}_{\\mathrm{idx}_{k}}\\}$ . We assign the normalized probability score of the nearest depth proposal to $\\mathfrak{p}$ as its confidence. The corresponding pixel feature is assigned to p weighted by the confidence score. The projection is invalid when $\\mathsf{d}(\\mathfrak{p})$ is not close to any of the depth proposals, and we set the backprojected feature from $i$ -th image as 0. Thus, the feature $\\bar{\\tilde{\\mathrm{f}}}_{i}\\in\\mathbb{R}^{C}$ backprojected from the $i^{\\th}$ -th image to point p and its corresponding indicator ${\\mathrm{g}}_{i}$ are given as follows: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{f}}_{i}=\\left\\{\\begin{array}{l l}{\\tilde{B}_{\\phi(\\mathbf{p})}^{i}\\mathbf{f}_{i}}&{\\mathrm{if~d(\\mathbf{p})\\subset~\\left\\{d_{i d x_1},\\dots,d_{i d x_k}\\right\\}~},}\\\\ {0}&{\\mathrm{otherwise}}\\end{array}\\right.\\quad\\mathbf{g}_{i}=\\left\\{\\begin{array}{l l}{1}&{\\mathrm{if~d(\\mathbf{p})\\subset~\\left\\{d_{i d x_1},\\dots,d_{i d x_k}\\right\\}~}}\\\\ {0}&{\\mathrm{otherwise}}\\end{array}\\right.,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\phi({\\mathfrak{p}})$ is the index of depth proposal that is close to $\\mathsf{d}(\\mathfrak{p})$ , and $\\tilde{\\mathbf{B}}_{\\phi(\\mathbf{p})}^{i}$ is the score of $\\mathbf{d}_{\\phi(\\mathfrak{p})}$ . After looping through every input image, we average all valid backprojected features for point p as its voxel feature $\\hat{\\mathbf{v}}\\in\\mathbb{R}^{\\breve{C}}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{v}}=\\eta_{v}^{-1}\\sum_{i=1}^{i=N}\\mathbf{g}_{i}\\tilde{\\mathbf{f}}_{i},\\ \\ \\mathrm{where}\\ \\eta_{v}=\\sum_{i=1}^{i=N}\\mathbf{g}_{i}\\tilde{\\mathbf{B}}_{\\phi(\\mathbf{p})}^{i}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We also utilize the multi-view consistency to compute a surface score s for point $\\mathbf{p}$ by averaging confidence scores B\u02dci\u03d5(p) from every valid projection. We set $\\mathrm{s}=0$ when the point does not have any valid projection, which means point $\\mathfrak{p}$ is in the free space. s is multiplied with voxel feature $\\hat{\\textbf{v}}$ as the final feature $\\mathbf{v}\\in\\mathbb{R}^{C}$ for point p as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{s}=\\eta_{s}^{-1}\\sum_{i=1}^{i=N}\\mathbf{g}_{i}\\tilde{\\mathbf{B}}_{\\phi(\\mathfrak{p})}^{i},\\ \\ \\mathrm{where}\\ \\eta_{s}=\\sum_{i=1}^{i=N}\\mathbf{g}_{i},\\ \\ \\ \\mathrm{v}=\\mathrm{s}\\hat{\\mathbf{v}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The adoption of s shares similar spirit with existing methods [22, 19] to decrease the influence of empty space voxels in the feature volume. Yet, we learn it directly from multi-view images instead of relying on NeRF [22] or ground truth supervision [19]. ", "page_idx": 5}, {"type": "text", "text": "3.3 Enhancing Depth Prediction with Gaussian Splatting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We further utilize the recent pixel-aligned Gaussian Splatting (PAGS) [3, 28] to enhance our depth prediction module. PAGS takes in sparse views and predict a Gaussian primitive per pixel. The parameters for each primitive are center $\\mu$ , Gaussian opacity $\\alpha$ , covariance $\\Sigma$ and color c. All the Gaussian primitives are combined together to render a novel view via rasterization-based splatting. ", "page_idx": 5}, {"type": "text", "text": "We select 3 nearby views per novel view from the images input to the detection branch , and predict Gaussian maps $\\{\\bar{\\bf M}_{\\mu},\\bar{\\bf M}_{\\alpha},\\bar{\\bf M}_{\\Sigma},\\bar{\\bf M}_{c}\\}$ for the selected views. The Gaussian center map $M_{\\mu}\\in$ $\\mathbb{R}^{\\frac{H}{4}}\\times\\frac{W}{4}\\times3$ are directly estimated from the predicted depth based on the probability volume $\\mathbf{B}$ as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{M}_{\\mu}(r)=\\mathbf{o}(r)+\\hat{\\mathbf{D}}(r)\\mathbf{h}(r),\\quad\\mathbf{D}=\\mathbf{B}\\mathbf{G}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{G}=[\\mathbf{d}_{1},\\ldots,\\mathbf{d}_{M}]^{\\top}$ is the virtual depth planes and $\\mathrm{D}\\in\\mathbb{R}^{\\frac{H}{4}\\times\\frac{W}{4}\\times1}$ is the estimated depth map. $\\upsigma(r)$ is the camera origin, $\\hat{\\mathbf{D}}(r)$ is the projected ray depth obtained from the depth map $\\mathrm{D}$ , and $\\mathfrak{h}(r)$ is the ray direction for pixel $r$ . The Gaussian opacity map $\\mathbf{M}_{\\alpha}\\in\\mathbb{R}^{\\frac{H}{4}\\times\\frac{W}{4}\\times1}$ is predicted by taking the max probability score of $\\mathbf{B}$ as follow: ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\bf M}_{\\alpha}=\\operatorname*{max}({\\bf B},d i m=-1).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The Gaussian covariance map $\\mathbf{M}_{\\Sigma}\\in\\mathbb{R}^{\\frac{H}{4}\\times\\frac{W}{4}\\times16}$ and color map $\\mathbf{M}_{c}\\in\\mathbb{R}^{\\frac{H}{4}\\times\\frac{W}{4}\\times3}$ are predicted from a MLP as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{M}_{\\Sigma},\\mathbf{M}_{c}=\\mathrm{MLP}(\\mathrm{F}\\parallel\\mathbf{D}\\parallel\\hat{\\mathbf{I}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\parallel$ denotes concatenation and $\\hat{\\mathrm{I}}\\in\\mathbb{R}^{\\frac{H}{4}\\times\\frac{W}{4}\\times3}$ denotes the resized image map. Following 3DGS [9], $\\Sigma$ is predicted by a rotation quaternion and scaling factors. Color is predicted by spherical harmonics coefficients. ", "page_idx": 5}, {"type": "text", "text": "We render the image color $\\hat{\\mathbf{C}}_{\\mathrm{color}}$ via alpha-blending and the rendering loss is a L2 loss as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{render}}=||\\hat{\\mathbf{C}}_{\\mathrm{color}}-\\mathbf{C}_{\\mathrm{color}}||^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "One of the key factors for good rendering is accurate $\\mathbf{M}_{\\mu}$ , which is directly related to correct depth estimation from our model (cf. Eqn. 8). The rendering loss thus iteratively guides the Gaussians to the correct 3D locations, and consequently benefits our detection pipeline. Our final loss is $\\begin{array}{r}{\\mathcal{L}=\\mathcal{L}_{\\mathrm{det}}+\\mathcal{L}_{\\mathrm{render}}}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "Table 1: Results on ScanNet. \u201cGT Geo\u201d denotes whether ground truth geometry is used as supervision during training. ", "page_idx": 6}, {"type": "text", "text": "Table 2: Results on ARKitScenes. \u201cGT Geo\u201d denotes whether ground truth geometry is used as supervision during training. ", "page_idx": 6}, {"type": "table", "img_path": "GqrWhROxrG/tmp/c779f784505bf04a9b47011e338b505a5dd82528effa489fe503c8dec02fb57c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "GqrWhROxrG/tmp/75d6ef95c62fff620730cb20ba20de45527244d087d7cd4f73ad5fa903f19d04.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "GqrWhROxrG/tmp/66f1225ce3d050ff6f0dfa136d397e9ad2836f1aab8cd23c86cccad5d26f5a80.jpg", "img_caption": ["Figure 4: Qualitative comparison on ScanNet dataset. Note that the mesh is not the input to the model and is only for visualization purpose. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Datasets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We conduct experiments on the ScanNet and ARKitScenes datasets. ScanNet has 1,201 and 312 scans for training and testing, respectively. We detect axis-aligned bounding boxes for 18 classes. ARKitScenes has 4,498 and 549 scans for training and testing, respectively. We detect oriented bounding boxes for 17 class. We adopt mean average precision (mAP) with thresholds of 0.25 and 0.5 as the evaluation metrics. ", "page_idx": 6}, {"type": "text", "text": "4.2 Implementation Details ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We use the same feature extractor, training and testing configurations as [22] for detection. Specifically, images are resized into (240, 320). During training, we input 40 images to the detection branch. During testing, the detection branch takes in 100 images and the rendering branch is removed. The size of the 3D volume is $(N_{x}=40,N_{y}=40,N_{z}=16)$ ), with each voxel represents a cube of $0.16\\mathrm{m}$ $\\times0.16\\mathrm{m}\\times\\,0.2\\mathrm{m}$ The depth range is empirically set as $[0.2\\mathrm{m},5\\mathrm{m}]$ . The number of depth planes $M$ is set to 12 and $k=3$ depth proposals are selected in the probabilistic sampling. We consider $\\mathrm{d}(\\mathfrak{p})\\subset\\left\\{\\mathrm{d}_{\\mathrm{idx}_{1}},\\dots,\\mathrm{d}_{\\mathrm{idx}_{k}}\\right\\}$ if $\\mathsf{d}(\\mathfrak{p})$ is within $\\pm0.2\\mathrm{m}$ of any depth proposals. For the rendering branch, we select another two images as the target views that do not overlap with the images in the detection branch. We use AdamW optimizer with learning rate 0.0002, total epochs of 12 and batchsize of 1. All experiments are conducted on two NVIDIA A6000 GPUs. ", "page_idx": 6}, {"type": "table", "img_path": "GqrWhROxrG/tmp/494cba9176a31ca969eb34ec9766fd6a7f7fbc48bb605f5f87e80137946e52aa.jpg", "table_caption": ["Table 3: Ablation study of probabilistic sampling and soft weighting. All methods are conducted without using rendering loss. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "GqrWhROxrG/tmp/a63c3b1edc3625cae42ce2a0b929c2ae6d6f907850fc895a76bf3acf0b792c62.jpg", "table_caption": ["Table 4: Ablation study of Gaussian Splatting. $M$ denotes the number of depth planes in the plane sweep. \u201cGaussian\u201d denotes using pixel-aligned Gaussian splatting. \u201cRMSE\u201d is the depth evaluation metric. \u201cMemory $\\Delta^{\\bullet}$ denotes the increased memory consumption during training. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.3 Comparison with Baselines ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compare our method with ImGeoNet [19], CN-RMA [17], ImVoxelNet [16] and NeRF-Det [22]. We directly report their results from the CN-RMA [17] paper. Note that ImGeoNet and CN-RMA use ground truth 3D geometry as supervision during training. Tab. 1 and Tab. 2 show the results on ScanNet and ARKitScenes, respectively. It is expected that using ground truth geometry as supervision can achieve good performance. However, ground truth geometry may not be accessible and therefore we seek for a self-supervised approach that do not rely its supervision. ImVoxelNet and NeRF-Det are the two existing methods that leverage self-supervision to learn geometry for multiview 3D detection. Compared to these works, our method achieve much better performance. It clearly shows the superiority of our efficient plane sweep method over the vanilla feature backprojection in ImVoxeNet and the density field in NeRF-Det. Fig. 1 shows the qualitative results on ScanNet. The first two columns show that our model can detect more target objects. We also find that NeRF-Det tends to detect objects in the free space (last two colums), which is due to inaccurate feature projection. In contrast, our model is able to place bounding boxes at more accurate locations. ", "page_idx": 7}, {"type": "text", "text": "4.4 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Effectiveness of Probabilistic Sampling and Soft Weighting. Tab. 3 shows the ablation stdudy of the probabilistic sampling and soft weghting (PSSW). All methods are conducted without the rendering loss $\\mathcal{L}_{\\mathrm{render}}$ to test the performance of PSSW alone. Removing \u201cProbabilistic Sampling\u201d means replacing top- $k$ sampling with a single depth estimation computed by the weighted average of depth probability volume B. We use the max probability score as the weight in this case. Removing \u201cSoft Weighting\u201d means replacing $\\tilde{\\mathbf{B}}_{\\phi(\\mathbf{p})}^{i}$ with value 1. As shown in the table, removing either \u201cProbabilistic Sampling\u201d or \u201cSoft Weighting\u201d causes drastic drop in the performance. Particularly, removing probabilistic sampling depth proposals drops by 19.1 at mAP $@$ .25. It suggests that estimating accurate depth is very hard under insufficient depth planes. Furthermore, soft weighting is very important to our model as it can decrease the influence of wrong depth proposals introduced by the sampling. Overall, both probabilistic sampling and soft weighting are crucial to our model. ", "page_idx": 7}, {"type": "text", "text": "Effectiveness of Gaussian Splatting for Depth Prediction. Tab. 4 shows the ablation study for using pixel-aligned Gaussian splatting (PAGS) for detection. \u201cRMSE\u201d evaluates the average quality of depth prediction for the images in the detection branch. Increasing depth planes to $M=16$ or using Gaussian Splatting both improve the depth estimation and bring similar improvement to the detection performance. However, using PAGS consumes much less memory during training than $M=16$ , i.e. adding only $28\\%$ memories compared to increasing depth planes. Moreover, PAGS does not bring any additional memory cost during testing because it is removed for detection. We visualize the depth maps predicted by the probability volume B in Fig. 5. The depth maps are of 1/16 size of the original image since we estimate depth on the feature map level. It shows that PAGS significantly improves the depth quality. ", "page_idx": 7}, {"type": "image", "img_path": "GqrWhROxrG/tmp/104085a0f95abf88fb7eb4ede23d1f700b4154e60f9d7386546861e18c18363f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 5: Depth map visualization. \u201cGT Depth\u201d denotes ground truth depth map. Both \u201cw/ Gaussian\u201d and \u201cw/o Gaussian\u201d use $M=12$ depth planes. ", "page_idx": 8}, {"type": "table", "img_path": "GqrWhROxrG/tmp/6630abc940d281290e0bad059819e7a523e1539be650e4d6e7da5dfbb18fb667.jpg", "table_caption": ["Table 5: Ablation study of Top- $k$ depth proposals. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Analysis of the number of Top- $k$ depth proposals. Tab. 5 shows the ablation study of number of depth proposals. Sampling top-1 depth proposal leads to severe performance decrease as the correct depth location is harder to be selected. Sampling too many depth proposals $\\mathrm{[}k=5\\mathrm{-}$ ) also leads to some decrease since more inaccurate locations are sampled. We thus choose $k=3$ in our model. ", "page_idx": 8}, {"type": "text", "text": "Comparison with Depth Estimation Methods. ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Fig. 6 shows the comparison of different depth prediction methods to the detection performance. MVSNet [24] performs one-time plane sweep by using $M\\,=\\,16$ depth planes. BEVStereo [10] performs iterative depth estimation by sampling depth planes according to a monocular depth estimation. We set the number of iterations to 3 and the number of depth planes in each iteration to 5. We remove ground truth depth supervision for BEVStereo for fair comparison. \u201cGround-truth Depth\u201d denotes placing pixel feature on the 3D volume according to the ground-truth depth location, which is the upper bound of our method. We show the results of our approach using different number of depth planes $M=12$ and $M=8$ ). MVSNet performs badly even though it samples ", "page_idx": 8}, {"type": "image", "img_path": "GqrWhROxrG/tmp/2cb022a3f858778b18afe99fa4d74ef037f44c95513f4c0dddbe844fb6db47a2.jpg", "img_caption": ["Figure 6: Comparison of different depth prediction methods on 3D object detection on ScanNet. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "more planes than us. This is because MVSNet requires sufficient depth planes to estimate depth correctly. BEVStereo also fails because it requires the ground truth depth supervision to learn a good initial monocular depth estimation. In contrast, our model achieve close performance as the \u201cGround-truth Depth\u201d using only 12 or even 8 depth planes. This demonstrates that our approach efficiently learns geometry without sampling many depth planes. ", "page_idx": 8}, {"type": "text", "text": "Time and memory comparison. Tab. 6 shows the comparison of time and memory in the training and testing stages on ScanNet, respectively. We omit comparison with ImGeoNet since it does not release any code. All models are ran on $2\\times$ A6000 GPUs. Due to the complexity of CN-RMA (as mentioned in Sec 3.5 of their paper), it requires much longer time to train and evaluate than other models. Furthermore, CN-RMA consumes much more memory in the training stage because it requires joint end-to-end training of the 3D reconstruction and detection network. Although NeRF-Det is efficient in time and memory of the training and testing stages, their performance is much worse than ours as shown in Tab. 1 and 2. ", "page_idx": 8}, {"type": "table", "img_path": "GqrWhROxrG/tmp/596533aeec17fa8658869ef42885a21efce9cda2e1b3ec54016ee8d06bc4d053.jpg", "table_caption": ["Table 6: Time and memory comparison in training and testing stages on ScanNet dataset, respectively. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose MVSDet for multi-view image based indoor 3D object detection. We design a probabilistic sampling and soft weighting mechanism to decide the placement of pixel features on the 3D volume without the need of the computationally sampling of many depth planes. We further introduce the use of pixel-aligned Gaussian Splatting to improve depth prediction with little computation overhead. Extensive experiments on two benchmark datasets demonstrate the superiority of our method. ", "page_idx": 9}, {"type": "text", "text": "6 Limitation ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Similar to existing multi-view stereo methods [24, 20], feature matching would fail on texture-less or reflective surfaces. One possible solution is to combine with monocular depth estimation [2]. However, estimating monocular depth is non-trivial and we leave it for future research. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement. This research work is supported by the Agency for Science, Technology and Research $\\mathrm{\\DeltaA^{*}S T A R)}$ under its MTC Programmatic Funds (Grant No. M23L7b0021). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Adel Ahmadyan, Liangkai Zhang, Artsiom Ablavatski, Jianing Wei, and Matthias Grundmann. Objectron: A large scale dataset of object-centric videos in the wild with pose annotations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 7822\u20137831, 2021.   \n[2] Gwangbin Bae, Ignas Budvytis, and Roberto Cipolla. Multi-view depth estimation by fusing single-view depth probability with multi-view geometry. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2842\u20132851, 2022.   \n[3] David Charatan, Sizhe Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. arXiv preprint arXiv:2312.12337, 2023.   \n[4] Jintai Chen, Biwen Lei, Qingyu Song, Haochao Ying, Danny Z Chen, and Jian Wu. A hierarchical graph network for 3d object detection on point clouds. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 392\u2013401, 2020.   \n[5] Rui Chen, Songfang Han, Jing Xu, and Hao Su. Point-based multi-view stereo network. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1538\u20131547, 2019.   \n[6] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. arXiv preprint arXiv:2403.14627, 2024.   \n[7] Shuo Cheng, Zexiang Xu, Shilin Zhu, Zhuwen Li, Li Erran Li, Ravi Ramamoorthi, and Hao Su. Deep stereo using adaptive thin volume representation with uncertainty awareness. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2524\u20132534, 2020.   \n[8] David Gallup, Jan-Michael Frahm, Philippos Mordohai, Qingxiong Yang, and Marc Pollefeys. Real-time plane-sweeping stereo with multiple sweeping directions. In 2007 IEEE Conference on Computer Vision and Pattern Recognition, 2007.   \n[9] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00fchler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4):1\u201314, 2023.   \n[10] Yinhao Li, Han Bao, Zheng Ge, Jinrong Yang, Jianjian Sun, and Zeming Li. Bevstereo: Enhancing depth estimation in multi-view 3d object detection with temporal stereo. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 1486\u20131494, 2023.   \n[11] Xiaoxiao Long, Lingjie Liu, Wei Li, Christian Theobalt, and Wenping Wang. Multi-view depth estimation using epipolar spatio-temporal networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8258\u20138267, 2021.   \n[12] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European Conference on Computer Vision, pages 405\u2013421. Springer, 2020.   \n[13] Ishan Misra, Rohit Girdhar, and Armand Joulin. An end-to-end transformer model for 3d object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2906\u20132917, 2021.   \n[14] Charles R Qi, Or Litany, Kaiming He, and Leonidas J Guibas. Deep hough voting for 3d object detection in point clouds. In proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9277\u20139286, 2019.   \n[15] Danila Rukhovich, Anna Vorontsova, and Anton Konushin. Fcaf3d: Fully convolutional anchor-free 3d object detection. In European Conference on Computer Vision, pages 477\u2013493. Springer, 2022.   \n[16] Danila Rukhovich, Anna Vorontsova, and Anton Konushin. Imvoxelnet: Image to voxels projection for monocular and multi-view general-purpose 3d object detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2397\u20132406, 2022.   \n[17] Guanlin Shen, Jingwei Huang, Zhihua Hu, and Bin Wang. Cn-rma: Combined network with ray marching aggregation for 3d indoors object detection from multi-view images. arXiv preprint arXiv:2403.04198, 2024.   \n[18] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea Vedaldi. Splatter image: Ultra-fast single-view 3d reconstruction. arXiv preprint arXiv:2312.13150, 2023.   \n[19] Tao Tu, Shun-Po Chuang, Yu-Lun Liu, Cheng Sun, Ke Zhang, Donna Roy, Cheng-Hao Kuo, and Min Sun. Imgeonet: Image-induced geometry-aware voxel representation for multi-view 3d object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6996\u20137007, 2023.   \n[20] Fangjinhua Wang, Silvano Galliani, Christoph Vogel, and Marc Pollefeys. Itermvs: Iterative probability estimation for efficient multi-view stereo. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8606\u20138615, 2022.   \n[21] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. arXiv preprint arXiv:2106.10689, 2021.   \n[22] Chenfeng Xu, Bichen Wu, Ji Hou, Sam Tsai, Ruilong Li, Jialiang Wang, Wei Zhan, Zijian He, Peter Vajda, Kurt Keutzer, et al. Nerf-det: Learning geometry-aware volumetric representation for multi-view 3d object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 23320\u201323330, 2023.   \n[23] Jianfeng Yan, Zizhuang Wei, Hongwei Yi, Mingyu Ding, Runze Zhang, Yisong Chen, Guoping Wang, and Yu-Wing Tai. Dense hybrid recurrent multi-view stereo net with dynamic consistency checking. In European conference on computer vision, pages 674\u2013689. Springer, 2020.   \n[24] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for unstructured multi-view stereo. In Proceedings of the European conference on computer vision (ECCV), pages 767\u2013783, 2018.   \n[25] Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang, and Long Quan. Recurrent mvsnet for highresolution multi-view stereo depth inference. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5525\u20135534, 2019.   \n[26] Zehao Yu and Shenghua Gao. Fast-mvsnet: Sparse-to-dense multi-view stereo with learned propagation and gauss-newton refinement. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1949\u20131958, 2020.   \n[27] Zaiwei Zhang, Bo Sun, Haitao Yang, and Qixing Huang. H3dnet: 3d object detection using hybrid geometric primitives. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XII 16, pages 311\u2013329. Springer, 2020.   \n[28] Shunyuan Zheng, Boyao Zhou, Ruizhi Shao, Boning Liu, Shengping Zhang, Liqiang Nie, and Yebin Liu. Gps-gaussian: Generalizable pixel-wise 3d gaussian splatting for real-time human novel view synthesis. arXiv preprint arXiv:2312.02155, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "A Appendix / Supplemental Material ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "A.1 Qualitative Results on ARKitScenes Dataset ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Fig. 2 shows the qualitative results on ARKitScenes dataset. Compared to NeRF-Det [22], we are able to detect more target objects. ", "page_idx": 11}, {"type": "image", "img_path": "GqrWhROxrG/tmp/17d7206f37e5c5c68d920f52823673bbf30c0599306f706bbda3ce6d061e8811.jpg", "img_caption": ["Figure 7: Qualitative comparison on ARKitScenes dataset. Note that the mesh is not the input to the model and is only for visualization purpose. "], "img_footnote": [], "page_idx": 11}, {"type": "text", "text": "A.2 Novel View Synthesis Results ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Fig. 8 shows the novel view synthesis results on ScanNet test dataset. Our model gives reasonable rendering results, indicating that the Gaussian Splatting module successfully learn the geometry. Note that the Gaussian Splatting module is only a regularizer to our plane sweep algorithm instead of the determining factor to learn geometry like NeRF in NeRF-Det [22]. ", "page_idx": 11}, {"type": "image", "img_path": "GqrWhROxrG/tmp/e5273c213b2361faeea6d895b20223844dd35aa1b7b6c5447663b204f0f17807.jpg", "img_caption": ["Figure 8: Novel view synthesis results on ScanNet dataset.\u201cRendering\u201d denotes the rendered image / depth from our Gaussian Splatting module. \u201cGT\u201d denotes the ground-truth image /depth of the novel view. "], "img_footnote": [], "page_idx": 11}, {"type": "table", "img_path": "GqrWhROxrG/tmp/ef8594e2649d6ded901b82c5e835ab9453f8af0468040c417ef43aa84e44d8b0.jpg", "table_caption": ["Table 7: Per-class results under $\\mathrm{AP@0.25}$ on ScanNet dataset. "], "table_footnote": [], "page_idx": 11}, {"type": "text", "text": "Table 8: Per-class results under $\\operatorname{AP}@0.5$ on ScanNet dataset. ", "text_level": 1, "page_idx": 12}, {"type": "table", "img_path": "GqrWhROxrG/tmp/5fd0b6609fd6f0beb486605f3b5bf3f640dd6267e1639139f24393e7449efbea.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "A.3 Per-Class Performance ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Tab. 7, Tab. 8, Tab. 9 and Tab. 10 are the per-class results under AP $\\langle\\varpi0.25$ and $\\operatorname{AP}@0.5$ on ScanNet and ARKitScenes datasets, respectively. ", "page_idx": 12}, {"type": "table", "img_path": "GqrWhROxrG/tmp/4dd877f6ec166fa69c69bab924c14899db45f289f098427ea4cf063d137a504b.jpg", "table_caption": ["Table 9: Per-class results under $\\mathrm{AP@0.25}$ on ARKitScenes dataset. "], "table_footnote": [], "page_idx": 12}, {"type": "table", "img_path": "GqrWhROxrG/tmp/46483dd74ed1506f87d7d5f8ecfa3b4384d29dd461bf4d6837c3125c338a2383.jpg", "table_caption": ["Table 10: Per-class results under $\\mathrm{AP}@0.5\\$ on ARKitScenes dataset. "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "1. Claims ", "page_idx": 13}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: The experiments support our claims. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 13}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: Please see Section 6 Limitation. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 13}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 13}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 13}, {"type": "text", "text": "Justification: We do not propose any theories. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Justification: We include the implementation details in the experiment section. Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 14}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 14}, {"type": "text", "text": "Answer: [No] ", "page_idx": 14}, {"type": "text", "text": "Justification: We will release our code upon paper acceptance. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: We include the dataset and implementation details in the experiment section. Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 15}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 15}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Justification: We follow baselines to not report error bars. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 15}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Justification: We include it in the implementation details. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. ", "page_idx": 15}, {"type": "text", "text": "\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 16}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 16}, {"type": "text", "text": "Answer:[Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: We confirm that we follow NeurIPS Code of Ethics. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 16}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 16}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: Our work is useful for 3D scene understanding. We do not have any negative social impact. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 16}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: We follow baselines to not describe safeguards. We do not foresee any misuse cases. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. ", "page_idx": 16}, {"type": "text", "text": "\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 17}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Justification: We properly cite the datasets we use. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 17}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 17}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: We do not release any new assets. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 17}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: We do not conduct crowdsourcing or research with Human Subjects Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 17}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: We do not have such experiments. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 18}]