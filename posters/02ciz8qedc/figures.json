[{"figure_path": "02CIZ8qeDc/figures/figures_1_1.jpg", "caption": "Figure 1: Motivation of zero-shot 3D anomaly detection. (a): Top: The hole on the cookies presents a similar appearance to the background. Bottom: Surface damage on the potato is unapparent to the object foreground. In these cases, leveraging RGB information makes it difficult to detect anomalies that imitate the color patterns of the background or foreground. However, effective recognition can be achieved by modeling the point relations within corresponding point clouds. (b) and (c) depicts the setting difference of ZS and unsupervised manner.", "description": "This figure highlights the challenges of zero-shot 3D anomaly detection by comparing different modalities (RGB vs. point cloud). It shows that relying solely on RGB information can be insufficient for detecting anomalies that blend with the background or foreground.  Point clouds, by capturing spatial relations, prove more effective in such cases. The figure also illustrates the difference between zero-shot (ZS) and unsupervised anomaly detection settings, emphasizing the greater difficulty of ZS due to the lack of training samples for specific target objects.", "section": "1 Introduction"}, {"figure_path": "02CIZ8qeDc/figures/figures_3_1.jpg", "caption": "Figure 2: Framework of PointAD. To transfer the strong generalization of CLIP from 2D to 3D, point clouds and corresponding ground truths are respectively rendered into 2D renderings from multi-view. Then, vision encoder of CLIP extracts the renderings to derive 2D global and local representations. These representations are transformed into glocal 3D point representations to learn 3D anomaly semantics within point clouds. Finally, we align the normality and abnormality from both point perspectives (multiple instance learning) and pixel perspectives (multiple task learning) and propose a hybrid loss to jointly optimize the text embeddings from the learnable normality and abnormality text prompts, capturing the underlying generic anomaly patterns.", "description": "PointAD, a unified framework for zero-shot 3D anomaly detection, leverages CLIP's capabilities by rendering 3D point clouds into multiple 2D views.  It extracts both global and local 2D features using CLIP's vision encoder, then projects these features back into 3D space.  Hybrid representation learning optimizes text prompts from both 2D and 3D data via auxiliary point clouds, allowing PointAD to identify anomalies across various unseen objects.  The figure details the multi-view rendering process, feature extraction, and hybrid representation learning which is the core idea behind the PointAD approach.", "section": "3 PointAD"}, {"figure_path": "02CIZ8qeDc/figures/figures_6_1.jpg", "caption": "Figure 3: Visualization on anomaly score maps in ZS 3D anomaly detection. Point clouds of diverse objects are input into PointAD to generate 2D and 3D representations. Each row visualizes the anomaly score maps of 2D renderings from different views, and the final point score maps are also presented. More visualizations are provided in Appendix J.", "description": "This figure shows the process of PointAD for zero-shot 3D anomaly detection.  Point clouds of various objects are fed into the model. PointAD generates multiple 2D renderings of each 3D point cloud from different viewpoints. These 2D renderings are processed to generate 2D anomaly score maps.  Finally, these 2D maps are projected back to 3D space to create a 3D anomaly score map. The figure showcases examples of these processes and the resulting anomaly score maps for different object types.", "section": "4.2 Implementation Details & Baselines"}, {"figure_path": "02CIZ8qeDc/figures/figures_7_1.jpg", "caption": "Figure 1: Motivation of zero-shot 3D anomaly detection. (a): Top: The hole on the cookies presents a similar appearance to the background. Bottom: Surface damage on the potato is unapparent to the object foreground. In these cases, leveraging RGB information makes it difficult to detect anomalies that imitate the color patterns of the background or foreground. However, effective recognition can be achieved by modeling the point relations within corresponding point clouds. (b) and (c) depicts the setting difference of ZS and unsupervised manner.", "description": "This figure illustrates the challenges of zero-shot 3D anomaly detection.  Panel (a) shows that relying solely on RGB information for anomaly detection can be problematic when the anomaly visually blends with the background or foreground, as seen in the examples of a hole in a cookie and surface damage on a potato.  In contrast, utilizing point cloud data allows for better detection due to the distinct spatial relationships present within the point cloud. Panel (b) contrasts zero-shot (ZS) and unsupervised anomaly detection settings. Panel (c) demonstrates how performance degrades in the case of zero-shot anomaly detection when using an object-specific model (unsupervised) compared to an object-agnostic model (zero-shot).", "section": "1 Introduction"}, {"figure_path": "02CIZ8qeDc/figures/figures_14_1.jpg", "caption": "Figure 2: Framework of PointAD. To transfer the strong generalization of CLIP from 2D to 3D, point clouds and corresponding ground truths are respectively rendered into 2D renderings from multi-view. Then, vision encoder of CLIP extracts the renderings to derive 2D global and local representations. These representations are transformed into glocal 3D point representations to learn 3D anomaly semantics within point clouds. Finally, we align the normality and abnormality from both point perspectives (multiple instance learning) and pixel perspectives (multiple task learning) and propose a hybrid loss to jointly optimize the text embeddings from the learnable normality and abnormality text prompts, capturing the underlying generic anomaly patterns.", "description": "This figure illustrates the PointAD framework, which transfers CLIP's strong generalization ability from 2D to 3D. It shows how 3D point clouds are rendered into multiple 2D views, processed by CLIP's vision encoder to extract 2D representations, and then projected back to 3D to learn 3D anomaly semantics.  A hybrid representation learning method aligns normality and abnormality from both point and pixel perspectives using multiple instance learning (MIL) and multi-task learning (MTL). The learnable text prompts are jointly optimized to capture underlying generic anomaly patterns, enabling zero-shot 3D anomaly detection.", "section": "3 PointAD"}, {"figure_path": "02CIZ8qeDc/figures/figures_16_1.jpg", "caption": "Figure 3: Visualization on anomaly score maps in ZS 3D anomaly detection. Point clouds of diverse objects are input into PointAD to generate 2D and 3D representations. Each row visualizes the anomaly score maps of 2D renderings from different views, and the final point score maps are also presented. More visualizations are provided in Appendix J.", "description": "This figure visualizes the PointAD model's performance on zero-shot 3D anomaly detection.  Multiple views of a 3D point cloud are rendered into 2D images, which are processed by the model to create anomaly score maps. These 2D score maps are then projected back into 3D space to generate a final 3D anomaly score map, which highlights the anomalous regions within the 3D point cloud.  The figure demonstrates how PointAD integrates information from multiple 2D views to improve its understanding of 3D anomalies.", "section": "4.2 Implementation Details & Baselines"}, {"figure_path": "02CIZ8qeDc/figures/figures_17_1.jpg", "caption": "Figure 8: Visualization with different rendering lighting.", "description": "This figure shows the impact of different lighting conditions on the PointAD model's performance.  The top row displays rendered images of a bagel with varying lighting intensities, from very dim (\"Lighting--\") to very bright (\"Lighting++\"). The bottom row shows the corresponding anomaly score maps generated by PointAD.  The experiment demonstrates the model's robustness to variations in lighting.", "section": "4.3 Main Results"}, {"figure_path": "02CIZ8qeDc/figures/figures_17_2.jpg", "caption": "Figure 9: Visualization of occluded point clouds.", "description": "The figure visualizes the impact of point cloud occlusions on anomaly detection. It shows a bagel point cloud with an anomaly (a hole) in the original and an occluded version. The occluded version has a portion of the anomaly masked, simulating a scenario where part of the defect is hidden from view.  Below, the anomaly score maps from different perspectives (views) are shown for both the original and the occluded point cloud.  This illustrates how occlusions affect the model's ability to detect anomalies, highlighting the challenge of robust anomaly detection in real-world scenarios with incomplete or partially obscured data.", "section": "4.3 Main Results"}, {"figure_path": "02CIZ8qeDc/figures/figures_19_1.jpg", "caption": "Figure 1: Motivation of zero-shot 3D anomaly detection. (a): Top: The hole on the cookies presents a similar appearance to the background. Bottom: Surface damage on the potato is unapparent to the object foreground. In these cases, leveraging RGB information makes it difficult to detect anomalies that imitate the color patterns of the background or foreground. However, effective recognition can be achieved by modeling the point relations within corresponding point clouds. (b) and (c) depicts the setting difference of ZS and unsupervised manner.", "description": "This figure illustrates the challenges of zero-shot 3D anomaly detection.  Subfigure (a) shows examples where relying on RGB information alone is insufficient for detecting anomalies because the anomalies visually blend with the background or foreground. Subfigure (b) highlights the difference between zero-shot and unsupervised settings for 3D anomaly detection, with zero-shot lacking training samples for the target object. Subfigure (c) demonstrates how the performance of traditional methods degrades significantly when applied to zero-shot scenarios.", "section": "1 Introduction"}, {"figure_path": "02CIZ8qeDc/figures/figures_19_2.jpg", "caption": "Figure 1: Motivation of zero-shot 3D anomaly detection. (a): Top: The hole on the cookies presents a similar appearance to the background. Bottom: Surface damage on the potato is unapparent to the object foreground. In these cases, leveraging RGB information makes it difficult to detect anomalies that imitate the color patterns of the background or foreground. However, effective recognition can be achieved by modeling the point relations within corresponding point clouds. (b) and (c) depicts the setting difference of ZS and unsupervised manner.", "description": "This figure illustrates the limitations of using only RGB information for 3D anomaly detection and the advantages of PointAD's approach.  Subfigure (a) shows examples where RGB alone fails to distinguish anomalies from background or normal object features (a hole in cookies that looks like the background, surface damage on a potato that is hard to see). Subfigure (b) contrasts zero-shot (ZS) and unsupervised settings for anomaly detection, highlighting that ZS requires more generalization ability. Subfigure (c) demonstrates performance degradation in unsupervised methods when applied to zero-shot scenarios, which are addressed by PointAD.", "section": "1 Introduction"}, {"figure_path": "02CIZ8qeDc/figures/figures_20_1.jpg", "caption": "Figure 3: Visualization on anomaly score maps in ZS 3D anomaly detection. Point clouds of diverse objects are input into PointAD to generate 2D and 3D representations. Each row visualizes the anomaly score maps of 2D renderings from different views, and the final point score maps are also presented. More visualizations are provided in Appendix J.", "description": "This figure showcases PointAD's ability to generate anomaly score maps from diverse 3D objects.  It demonstrates how PointAD processes 3D point cloud data by first creating multiple 2D renderings from various viewpoints.  Each row shows a different object with its corresponding 2D renderings and resulting anomaly score maps. The final column presents the aggregated point cloud anomaly score map. This visually illustrates PointAD's approach to comprehending 3D anomalies using both points and pixel information from 2D renderings.", "section": "4.2 Implementation Details & Baselines"}, {"figure_path": "02CIZ8qeDc/figures/figures_21_1.jpg", "caption": "Figure 13: Visualization about 2D renderings and ground truth from different views (K = 9).", "description": "This figure visualizes the 2D renderings generated from multiple viewpoints (K=9) of several 3D point clouds, along with their corresponding ground truth anomaly masks.  Each row represents a different 3D object. The visualizations illustrate how PointAD processes 3D point cloud data by projecting it into multiple 2D views to capture diverse perspectives of the object's shape and anomalies. The comparison between the renderings and ground truth helps illustrate the accuracy of the 2D representation learned by the model.", "section": "3 PointAD"}, {"figure_path": "02CIZ8qeDc/figures/figures_22_1.jpg", "caption": "Figure 14: Visualization of point and multimodal score maps in PointAD, which is pre-trained on cookie object.", "description": "This figure visualizes the results of PointAD's anomaly detection on four different objects from the MVTec3D-AD dataset.  For each object, it shows the ground truth anomaly mask, the point-based anomaly score map, and the final multimodal anomaly score map (combining point and pixel information). The visualization helps demonstrate how PointAD integrates both point cloud and RGB information for more accurate anomaly detection.  Different anomaly types (holes, surface damages) are shown across different object instances.", "section": "4.3 Main Results"}]