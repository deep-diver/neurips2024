[{"type": "text", "text": "PointAD: Comprehending 3D Anomalies from Points and Pixels for Zero-shot 3D Anomaly Detection ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Qihang Zhou1, Jiangtao $\\mathbf{Yan}^{1}$ , Shibo $\\mathbf{H}\\mathbf{e}^{1*}$ \u2217, Wenchao $\\mathbf{Meng}^{1}$ , Jiming Chen1 1 College of Control Science and Engineering, Zhejiang University 1 {zqhang, jtaoy, s18he, wmengzju, ${\\mathsf{c j m}}\\rightleftharpoons{\\mathsf{z}}.$ ju.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zero-shot (ZS) 3D anomaly detection is a crucial yet unexplored field that addresses scenarios where target 3D training samples are unavailable due to practical concerns like privacy protection. This paper introduces PointAD, a novel approach that transfers the strong generalization capabilities of CLIP for recognizing 3D anomalies on unseen objects. PointAD provides a unified framework to comprehend 3D anomalies from both points and pixels. In this framework, PointAD renders 3D anomalies into multiple 2D renderings and projects them back into 3D space. To capture the generic anomaly semantics into PointAD, we propose hybrid representation learning that optimizes the learnable text prompts from 3D and 2D through auxiliary point clouds. The collaboration optimization between point and pixel representations jointly facilitates our model to grasp underlying 3D anomaly patterns, contributing to detecting and segmenting anomalies of unseen diverse 3D objects. Through the alignment of 3D and 2D space, our model can directly integrate RGB information, further enhancing the understanding of 3D anomalies in a plug-and-play manner. Extensive experiments show the superiority of PointAD in ZS 3D anomaly detection across diverse unseen objects. Code is available at https://github.com/zqhang/PointAD ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anomaly detection, a significant field within deep learning, has been widely applied to diverse domains, including industrial inspection [2, 3, 32, 36, 37, 44, 15, 24, 55, 6, 19, 63]. While 2D anomaly detection has been extensively studied by exploring RGB information [23, 56, 50, 51, 8, 29], real-world anomalies typically present themselves with abnormal spatial characteristics. Relying solely on RGB information poses challenges in detecting some anomalies in many cases, e.g., when the defect mimics the appearance of the object\u2019s background or foreground, as shown in Figure 1(a). The emerging field of 3D anomaly detection aims to unveil these spatial relations indicative of abnormal patterns [22, 4, 45, 9, 54, 13]. ", "page_idx": 0}, {"type": "text", "text": "However, current 3D anomaly detection methods typically store normal point features during training and identify anomalies by measuring the distance between the test feature and these stored features [22, 54, 13]. They all depend on the assumption that target point clouds are available and entirely normal. This assumption does not hold in various situations when the training samples in the target dataset are inaccessible due to privacy protection (e.g., involvement of trade secrets) or the absence of target training data (e.g., a new product never seen before) [63]. Figure 1(b) depicts the setting discrepancy between ZS 3D and unsupervised anomaly detection. These methods mentioned above, which detect anomalies by memorizing or reconstructing normal point features, have limitations in generalizing to unseen objects in Figure 1(c). While zero-shot (ZS) anomaly detection has been explored in 2D images [35, 63], ZS 3D anomaly detection remains a research blank. It is a challenging task as ZS ", "page_idx": 0}, {"type": "image", "img_path": "02CIZ8qeDc/tmp/149f9fb2ba801837471160980b91432cf1cb7559247e428e06fa113b2f4f7ae0.jpg", "img_caption": ["(a) Comparison on anomaly segmentation us-(b) Comparison between ZS (c) BTF Performance degraing different modalities. and unsupervised settings. dation on MVTec3D-AD. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Motivation of zero-shot 3D anomaly detection. (a): Top: The hole on the cookies presents a similar appearance to the background. Bottom: Surface damage on the potato is unapparent to the object foreground. In these cases, leveraging RGB information makes it difficult to detect anomalies that imitate the color patterns of the background or foreground. However, effective recognition can be achieved by modeling the point relations within corresponding point clouds. (b) and (c) depicts the setting difference of ZS and unsupervised manner. ", "page_idx": 1}, {"type": "text", "text": "3D anomaly detection necessitates the model to detect 3D anomalies across unseen point clouds with diverse class semantics, requiring a robust generalization capacity in the detection model. Recently, Vision-Language Models (VLMs) with their strong generalization capabilities have been applied to various downstream tasks [40, 61, 41, 49, 25, 26]. Particularly, CLIP has demonstrated its strong ZS performance to detect 2D anomalies [35, 24, 63]. Integrating CLIP into the detection model presents a potential solution to the challenging yet unexplored ZS 3D anomaly detection. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose a unified framework, namely PointAD, to transfer the knowledge of CLIP to detect 3D anomalies in a ZS manner. PointAD comprehends point clouds from both 3D and 2D: (1) deriving 2D representations of point clouds via CLIP by rendering them from multiple views, (2) understanding 3D representations by projecting 2D representations back to 3D, and (3) enhancing 3D comprehension by additional regularization on 2D representations. After grasping point clouds from points and pixels, we propose hybrid representation learning to capture generic normality and abnormality w.r.t. point and pixel information into learnable text prompts [63]. Specifically, since 3D representation manifests its 2D renderings from different views, we treat each representation as one instance and achieve 3D representation aggregation via multiple instance learning (MIL). On this basis, PointAD explicitly aligns the 2D anomalies, rendered from 3D anomalies, to further enhance 3D understanding. We formulate these 2D anomaly recognition tasks from the multi-task learning (MTL) perspective. PointAD collaboratively learns point and pixel representations, promoting the in-depth understanding of underlying abnormal patterns and thus achieving superior ZS normality and abnormality point recognition. Furthermore, benefiting from collaboration optimization, PointAD can directly integrate additional RGB information and perform ZS multimodal 3D (M3D) detection without extra modules and retraining. The main contributions of this paper are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 To the best of our knowledge, we are the first to investigate the challenging yet valuable ZS 3D anomaly detection domain. We propose to transfer the strong recognition generalization of CLIP to detect and segment 3D anomalies over diverse objects.   \n\u2022 We introduce a novel ZS 3D anomaly detection approach called PointAD, which provides a unified framework to understand 3D anomalies from points and pixels. Hybrid representation learning is proposed to incorporate the generic normality and abnormality semantics into PointAD, enabling a thorough understanding of 3D anomalies.   \n\u2022 PointAD can incorporate 2D RGB information in a plug-and-play manner for testing. In contrast to other methods that require storing RGB information separately, PointAD offers a unified framework to perform ZS M3D anomaly detection directly.   \n\u2022 Extensive experiments are conducted to demonstrate the superiority of our model in detecting and segmenting 3D anomalies, even outperforming some unsupervised SOTA methods that memorize normal features of target objects in certain metrics. We hope that our model will serve as a springboard for future research on ZS 3D and M3D anomaly detection. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "3D Anomaly Detection MVTec3D-AD [4], Eyecandies [5], and Real3D-AD [31] provide the point cloud anomalies and the corresponding 2D-RGB information. MVTec3D-AD bridges the connection between 3D and 2D anomaly detection. 3D-ST [4] uses a teacher net to extract dense local geometric descriptors and design a student net to match such descriptors. AST [45] introduces asymmetric teacher and student net to further improve 3D anomaly detection. IMRNet [28] and 3DSR [58] detect 3D anomalies by reconstruction errors. Instead of only using point clouds, BTF [22], M3DM [54, 53], CPFM [7], and SDM [13] integrate point features and RGB pixel features to detect 3D anomalies. While these approaches exhibit commendable performance by storing object-specific normal point and pixel features within the unsupervised learning framework, such paradigms simultaneously limit their generalization capacity to point clouds from unseen objects, which is crucial to detecting anomalies when the target object is unavailable. To the best of our knowledge, no solution addresses this valuable yet challenging problem. To fill this gap, we introduce PointAD, designed to identify unseen anomalies across diverse objects. PointAD extends CLIP to the realm of ZS 3D anomaly detection and shows robust generalization in capturing generic normality and abnormality within point clouds. Furthermore, PointAD serves as a unified framework, allowing seamless integration of point cloud and RGB modality without additional training. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3D Feature Extraction Conventional methods of 3D feature extraction typically employ a pointbased network like PointNet [38] or PointNet $^{++}$ [39] to extract 3D features from point clouds. Alternative approaches convert 3D data into a 2D format [48, 18], enabling 2D image backbones to process 3D information. PointCLIP [59] directly projects raw points onto image planes for efficiency, but this approach causes the produced depth map to lack geometric details. Instead, rendering-based methods [48, 21] generate 2D renderings by rendering point clouds, allowing for better preservation of local semantics. CPFM [7] stores normal features of these 2D renderings for unsupervised 3D anomaly detection. In this paper, we apply this rendering strategy to the source samples to capture generic anomaly semantics for recognizing abnormalities in unseen objects. ", "page_idx": 2}, {"type": "text", "text": "Prompt Learning Instead of fine-tuning the whole network, prompt learning just optimizes the model to adapt the network to downstream tasks. CoOp [61, 60] introduces global context optimization to update learnable text prompts for few-shot recognition. DenseCLIP [41] extends it to the dense classifications. More recently, AnomalyCLIP [63] proposes object-agnostic prompt learning to capture the generic normality and abnormality for images. Our model first introduces hybrid representation learning for ZS 3D anomaly detection, enabling the detection of anomalies and abnormal regions. ", "page_idx": 2}, {"type": "text", "text": "3 PointAD ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 A Review of CLIP ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "CLIP, a representative VLM, aligns visual representations to the corresponding textual representations, where an image is classified by comparing the cosine similarity between its visual representation and textual representations of given class-specific text prompts. Specifically, given an image $x_{i}$ and target class set $\\mathcal{C}$ , visual encoders output the global visual representation $\\bar{f_{i}}\\in\\mathbb{R}^{d}$ and local visual representations $f_{i}^{m}\\in\\mathbb{R}^{h\\times w\\times d}$ , where $h,w$ , and $d$ are the height, width, and dimension, respectively. Textual representations $g_{c}$ are encoded by textual encoder $\\bar{\\mathcal{T}}$ with the commonly used text prompt template A photo of a [c], where $c\\in{\\mathcal{C}}$ . The probability of $x_{i}$ belongs to $c$ can be computed as: ", "page_idx": 2}, {"type": "equation", "text": "$$\nP(g_{c},f_{i})=\\frac{e x p(c o s(g_{c},f_{i})/\\tau)}{\\sum_{c\\in\\mathcal{C}}e x p(c o s(g_{c},f_{i}))/\\tau)},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $c o s(\\cdot,\\cdot)$ and $\\tau$ represent the cosine similarity and temperature used in CLIP, respectively. The segmentation $S_{i(c)}\\in\\mathbb{R}^{h\\times w}$ for class $c$ can be computed as $S e g(g_{c},f_{i}^{m})$ , where each entry $\\left(u,v\\right)$ is calculated as $P(g_{c},f_{i,u,v}^{m})$ . ", "page_idx": 2}, {"type": "text", "text": "3.2 Overview of PointAD ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "ZS 3D anomaly detection requires a strong generalization capacity to anomalies on unseen objects with diverse object semantics. In this paper, we propose a unified framework, namely PointAD, to detect and segment 3D anomalies in a ZS manner. In Figure 2, PointAD understands point clouds from both pixel and point perspectives. To make CLIP understand 3D point clouds, we first render point clouds from multiple views and extract the pixel representations of these generated 2D renderings via the visual encoder of CLIP. And then, we derive point representations by projecting these pixel representations back to 3D. Learning generic normality and abnormality is significant in recognizing across-object anomalies. We propose hybrid representative learning, which focuses on glocal point and pixel abnormality, to optimize normality and abnormality text prompts, enabling PointAD with strong generalization to identify 3D anomalies on diverse objects. Beneftiing from the hybrid representation learning, PointAD can directly incorporate 2D RGB information during testing to achieve ZS M3D detection. ", "page_idx": 2}, {"type": "image", "img_path": "02CIZ8qeDc/tmp/ac4b12999be898fe2ff4899c100b15fc781045b19ed542aa882266d85612469b.jpg", "img_caption": ["Figure 2: Framework of PointAD. To transfer the strong generalization of CLIP from 2D to 3D, point clouds and corresponding ground truths are respectively rendered into 2D renderings from multi-view. Then, vision encoder of CLIP extracts the renderings to derive 2D global and local representations. These representations are transformed into glocal 3D point representations to learn 3D anomaly semantics within point clouds. Finally, we align the normality and abnormality from both point perspectives (multiple instance learning) and pixel perspectives (multiple task learning) and propose a hybrid loss to jointly optimize the text embeddings from the learnable normality and abnormality text prompts, capturing the underlying generic anomaly patterns. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.3 Multi-View Rendering ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Multi-view projection is a crucial technology for understanding point clouds from 2D perspectives. Some multi-view projection approaches project point clouds into various depth maps, providing adequate shape information for class recognition [59]. However, in this paper, our objective is to learn both generic global and local anomaly semantics. Depth-map projection lacks sufficient resolution to represent fine-grained anomaly semantics accurately. Hence, we adopt high-precision rendering to preserve the original 3D information offline. Specifically, given an auxiliary dataset of point clouds $\\mathcal{D}_{3d}=\\{(x_{i}^{3d},y_{i}^{3d})\\}_{i=1}^{N}$ , we define the rendering matrix as $R^{(k)}$ for the $k{-}t h$ view, with a total of $K$ views. We simultaneously render point clouds and point-level ground truths from different views to obtain their corresponding 2D renderings, which is given by $x_{i}^{(k)}=R^{(k)}(x_{i}^{3d})$ and $y_{i}^{(k)}=R^{(k)}(y_{i}^{3d})$ , where $\\boldsymbol{x}_{i}^{(k)}\\in\\mathbb{R}^{H\\times W}$ and $y_{i}^{(k)}\\in\\mathbb{R}^{H\\times W}$ respectively represent the $k$ -th 2D renderings and corresponding pixel-level ground truth in the $i{-}t h$ point cloud. Note that anomaly pixels are marked as 1, and normal pixels are marked as 0. ", "page_idx": 3}, {"type": "text", "text": "3.4 Representations for 3D and 2D information ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "PointAD aims to learn generic anomaly semantics from both 3D and 2D representations, enabling a comprehensive understanding of point and pixel anomaly patterns. For a point cloud $x_{i}^{3d}$ , we first obtain the 2D renderings $\\mathcal{X}_{i}=\\{x_{i}^{(k)}\\}_{k=1}^{K}$ . Then, these renderings are encoded via the vision encoder of CLIP to obtain global 2D representations $\\mathcal{F}_{i}\\!=\\!\\{f_{i}^{(k)}\\}_{k=1}^{K}$ , and local 2D representations Fim= {f im(k)}kK=1. As for point cloud representations, we consider that one point cloud will be projected into multiple 2D renderings. Consequently, global 3D representation $p_{i}$ and local 3D representations $p_{i}^{m}$ are expected to include their corresponding 2D representations in each view. Formally, $p_{i}=\\{p_{i}^{(k)}|p_{i}^{(k)}=f_{i}^{(k)}\\}_{k=1}^{K}$ and $p_{i}^{m}=\\{p_{i}^{m(k)}|p_{i}^{m(k)}=\\{p_{i,j}^{m(k)}\\}_{j=1}^{n}\\}_{k=1}^{K}$ , where $p_{i,j}^{m(k)}=\\{p_{i,j}^{m(k)}=f_{i,u,v}^{m(k)},(u,v)=R^{\\prime(k)}(a_{i,j}|b_{i,j},c_{i,j})\\}$ represents the $j{-}t h$ point representation of $i{-}t h$ point cloud in the $k$ -th view, whose 3D coordinate is $(a_{i,j},\\,b_{i,j},\\,c_{i,j})$ . $R^{\\prime(k)}$ is the rendering transformation between the point and pixel representation, derived as $\\begin{array}{r}{R^{\\prime(\\bar{k})}=\\frac{h}{H}R^{(k)}}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Points at different positions may yield a different number of 2D representations as they are hidden by other points from a specific viewpoint. In this case, we introduce a view-wise visibility mask $M$ , where M k indicates whether the $j$ -th point of the $i{-}t h$ point cloud is visible in the $k{-}t h$ view. We compare the depth of points projected into the same pixel in the same view and set the corresponding visibility mask to 1 for the point with the minimum depth, and to 0 for the other points. Let $\\mathcal{Q}_{i,u,v}^{(k)}$ denote the depths set of all points that are projected into the same pixel indexed by $(u,v)$ in the $i{-}t h$ $k{-}t h$ $\\mathcal{Q}_{i,u,v}^{(k)}$ $M_{i,j}^{k}$ $\\mathcal{Q}_{i,u,v}^{(k)}\\;=\\;\\{c_{i,j}\\;\\;|\\;\\;$ $R^{\\prime(k)}(a_{i,j},b_{i,j},c_{i,j})=(u,v)\\}_{j=1}^{n}$ $\\begin{array}{r}{M_{i,j}^{(k)}=\\mathbb{I}(i,j,k=\\mathrm{argmin}_{i,j,k}\\{c_{i,j}~|~c_{i,j}\\in\\mathcal{Q}_{i,u,v}^{(k)}\\})}\\end{array}$ $\\mathbb{I}(\\cdot)$ is an indicator function. Local 3D representations of the $\\it{i-t h}$ point cloud for the $k$ -th view are reformulated as: $p_{i}^{m(k)}=\\{p_{i,j}^{m(k)}*M_{i,j}^{(k)}\\bar{\\}_{j=1}^{n}$ ", "page_idx": 4}, {"type": "text", "text": "3.5 Hybrid representation learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The key of ZS 3D anomaly detection requires the model to capture generic anomaly semantics, rather than relying on specific object semantics. Since CLIP was originally pre-trained to align object semantics, such alignment harms the generalization capacity of CLIP to recognize anomalies on various objects. To adapt CLIP to 3D anomaly detection, we propose a hybrid representation learning, from both 3D and 2D perspectives, to globally and locally optimize textual representations. This enables PointAD to learn more representative text embedding for glocal anomaly semantics alignment. Following previous work [63, 61], we randomly initialize two learnable text templates $t_{n}$ and $t_{a}$ , in AnomalyCLIP [63] or CoOp manner [61], to obtain more overall text embeddings $g_{n}$ and $g_{a}$ to recognize normality and abnormality, respectively. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{t_{n}=[V_{1}]\\ldots\\stackrel{*}[V_{E}][o b j e c t],}&&{t_{n}\\stackrel{-}{=}[V_{1}]\\ldots[V_{E}][c l a s s],}\\\\ &{\\underbrace{t_{a}=[W_{1}]\\ldots[W_{E}][d a m a g e d][o b j e c t]}_{\\mathrm{PointAD}},}&&{\\underbrace{t_{a}=[W_{1}]\\ldots[W_{E}][d a m a g e d][c l a s s]}_{\\mathrm{PointAD-CoOp}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $V$ and $W$ are learnable word embeddings, respectively. ", "page_idx": 4}, {"type": "text", "text": "MIL-based 3D representation learning To fully incorporate 3D glocal anomaly semantics into PointAD from point information, we respectively devised two losses to capture 3D global anomalies and local anomaly regionals. First, we compute the cosine similarity between the textual representation and its rendering global representations in each view. As point clouds are projected from different views, the resulting renderings in each view reflect certain parts of point clouds. We use view-wise MIL to integrate 2D global representations and then align global labels to capture the global semantics. Formally, the global 3D loss is defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L_{3d}^{g l o b a l}=\\frac{1}{N}\\sum_{i}\\mathrm{CrossEntropy}(\\frac{1}{K}\\sum_{f_{i}^{(k)}\\in p_{i}}P(g_{c},f_{i}^{(k)}),\\operatorname*{max}{(y_{i}^{3d})}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As for local point anomaly semantics, we quantify the cosine similarity between textual representations and local representations of 2D renderings. Since points within point clouds are projected from different views, their projections in each view present part characteristics of themselves. We adopt the pixel-wise MIL to achieve the aggregation of point local representation. The point segmentation can be formulated mathematically as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{S_{i(a)}^{3d}=\\frac{1}{K}\\sum_{k}S e g(g_{a},p_{i}^{m(k)}),S_{i(n)}^{3d}=\\frac{1}{K}\\sum_{k}S e g(g_{n},p_{i}^{m(k)}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "However, deriving such 3D segmentation requires similarity computation for each point. It brings a significant memory burden, with a huge computational complexity of $O(K n d)$ , which is unaffordable for one NVIDIA RTX 3090 24GB GPU. To address this computational challenge, we resort to the rendering correspondence between points (3D space) and their corresponding pixels within each view (2D space). We first can rewrite 3D segmentation from the view perspective as $\\begin{array}{r}{S_{i(a)}^{3d}=\\frac{1}{K}\\sum_{k}S_{i(a)}^{3d(k)}}\\end{array}$ Then, the $k$ -th division of 3D segmentation can be transformed into the 2D counterpart through the rendering projection $S_{i(a)}^{3d(k)}=(\\bar{R}^{(k)})^{(-1)}S_{i(a)}^{2d(k)}\\otimes M_{i}^{(k)}$ , where $\\otimes$ is the Hamiltonian product. The $k$ -th 2D counterpart can be computed as Si2(da()k)= Up(Seg(ga, f im(k))), where the operator Up(\u00b7) represents bilinear interpolation from feature space to 2D space. Finally, we can reformulate the 3D segmentation as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{S_{i(a)}^{3d}=\\frac{1}{K}\\sum_{k}\\Bigl((R^{(k)})^{(-1)}\\mathrm{Up}\\bigl(S e g(g_{a},f_{i}^{m(k)})\\bigr)\\otimes M_{i}^{(k)}\\Bigr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "From the equation, we can observe that the primary computation can be conducted in the feature space, with a computational complexity of ${\\overline{{O}}}(K h{\\dot{w}}d)$ . This is a substantial overhead reduction compared to $O(K\\dot{n}d)$ since feature space is much smaller than 3D space, i.e., $h\\times w\\ll n$ . In our experiment, $h\\times w=24\\times24=576$ , while $n=336\\times336=112896$ . With this transformation, the entire experiment can be conducted using only a single NVIDIA RTX 3090 24GB GPU. After that, Dice Loss is employed to precisely model the decision boundary of anomaly regions. In addition to the local visual representations from the top layer, we integrate several intermediate local visual representations to capture detailed anomaly semantics. Let $I$ represent a full-one matrix of the same size as $y_{i}^{3d}$ . Formally, we define 3D local loss $L_{3d}^{l o c a l}$ : ", "page_idx": 4}, {"type": "table", "img_path": "02CIZ8qeDc/tmp/04044ef8746532b4ffc07a56631f8b0bebd3c61091d6d7fc648c3b51e2ae48ec.jpg", "table_caption": ["Table 1: Performance comparison on ZS 3D Table 2: Performance comparison on ZS M3D anomaly detection in \"one-vs-rest\" setting. anomaly detection in \"one-vs-rest\" setting. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L_{3d}^{l o c a l}=\\frac{1}{N}\\sum_{i}\\left(\\mathrm{Dice}(S_{i(n)}^{3d},I-y_{i}^{3d})+\\mathrm{Dice}(S_{i(a)}^{3d},y_{i}^{3d})\\right)\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "MTL-based 2D representation learning We further improve PointAD point understanding by capturing 2D glocal anomaly semantics into the object-agnostic text prompt template. We treat the anomaly recognition for one rendering from the point cloud as a task. Hence, we formulate the anomaly semantics learning for multiple 2D renderings as MTL. MTL-based 2D representation learning is divided into two parts for respective alignment to 2D global and local anomaly semantics. For 2D global semantics, we use CrossEntropy to quantify the discrepancy between the textual representations and each global 2D representation. Global MTL-based 2D representation learning Lg2ldobalis defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L_{2d}^{g l o b a l}=\\frac{1}{N K}\\sum_{i,k}{\\mathrm{CrossEntropy}}(P(g_{c},f_{i}^{(k)}),\\operatorname*{max}{(y_{i}^{(k)})}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Also, we focus on 2D abnormal regions to understand pixel-level anomalies. As the anomaly regions are typically smaller than normal regions, we employ Focal Loss to mitigate the class imbalance besides Dice Loss. Let $\\oplus$ denote the concatenation operation. Local MTL-based 2D representation learning $L_{2d}^{l o c a l}$ is given as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L_{2d}^{l o c a l^{*}}=\\frac{1}{N K}\\sum_{i,k}\\mathrm{Focal}(S_{i(n)}^{2d(k)}\\oplus S_{i(a)}^{2d(k)},y_{i}^{(k)})+\\mathrm{Dice}(S_{i(n)}^{2d(k)},I-y_{i}^{(k)})+\\mathrm{Dice}(S_{i(a)}^{2d(k)},y_{i}^{(k)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.6 Training and Inference ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "PointAD detects 3D anomalies from both 3D and 2D perspectives and thus combing these above losses to derive hybrid loss $L_{h y b r i d}$ . We minimize $L_{h y b r i d}$ to incorporate generic anomaly semantics into the text prompt from point and pixel spaces: ", "page_idx": 5}, {"type": "equation", "text": "$$\nL_{h y b r i d}=L_{3d}^{g l o b a l}+L_{3d}^{l o c a l}+L_{2d}^{g l o b a l}+L_{2d}^{l o c a l}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "During training, we minimize the hybrid loss $L_{h y b r i d}$ , where the original parameters of CLIP are frozen to maintain its strong generalization. Since our model provides a unified framework to understand anomaly semantics from point and pixel, it can not only perform ZS 3D anomaly detection but also M3D anomaly detection in a plug-and-pay way. Next, we will introduce the inference process in detail: ", "page_idx": 5}, {"type": "text", "text": "ZS 3D/M3D inference Given a point cloud $x_{i}^{3d}$ , we regard the 3D segmentation (See Equ. 2) as the anomaly score map: Aim =G\u03c3(Si3(da)) , where $G_{\\sigma}(\\cdot)$ represents the Gaussian fliter. The global anomaly score incorporates glocal anomaly semantics and is computed as $\\begin{array}{r}{A_{i}^{s}=\\frac{1}{2}(\\frac{1}{K}\\sum_{f_{i}^{(k)}\\in\\mathcal{F}_{i}}{P(g_{c},f_{i}^{(k)})}+}\\end{array}$ max $\\left(A_{i}^{m}\\right)]$ . When the RGB counterpart is available for testing, PointAD could directly integrate RGB information by feeding RGB images to 2D branch to derive 2D representations. We project tahneosem a2lDy  rsecporrees eans t $A_{i}^{m(r g b)}\\,=\\,P(g_{c},f_{i}^{(r g b)})$ tao nrde $A_{i}^{s(r g b)}\\,=\\,G_{\\sigma}(S_{i(a)}^{3d(r g b)})$ .a nTohmea lfyin sacl ormeu lmtiamp oadnadl anomaly score map and anomaly score are defined as Aim(mod) $A_{i}^{m(m o d)}\\ =\\ \\textstyle{\\frac{1}{2}}G_{\\sigma}\\big(A_{i}^{m}\\,+\\,A_{i}^{m(r g b)}\\big)$ and $\\begin{array}{r}{A_{i}^{s(m o d)}=\\frac{1}{2}\\left[\\frac{1}{2}(A_{i}^{s(r g b)}+A_{i}^{s})+\\operatorname*{max}\\left(A_{i}^{m(m o d)}\\right)\\right]}\\end{array}$ , respectively. ", "page_idx": 5}, {"type": "text", "text": "4 Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Experiment Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Dataset We evaluate the performance of ZS 3D anomaly detection on three public datasets including, MVTec3D-AD, Eyecandies and Real3D-AD. MVTec3D-AD, Eyecandies, and Real3D-AD are multiclass datasets and respectively contain 10 classes, 10 classes, and 12 classes. Since these training datasets only contain all normal samples, we use the common zero-shot setting one-vs-rest, where an object test dataset is used to fine-tune PointAD and assess the ZS anomaly detection for the remaining objects. We also explore a more challenging setting: cross-dataset ZS generalization, which requires the detection model to generalize to anomalies on other datasets. For point cloud anomaly detection, we only use point clouds to detect and localize 3D anomalies. In M3D anomaly detection, the 2D RGB information is utilized only for testing. To comprehensively analyze PointAD, we utilize four metrics to assess its performance in both anomaly detection and segmentation. ", "page_idx": 6}, {"type": "image", "img_path": "02CIZ8qeDc/tmp/ccc5940c006533682dc6674a97b911a54805a464218cf064dce4e8c92eb623c6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 3: Visualization on anomaly score maps in ZS 3D anomaly detection. Point clouds of diverse objects are input into PointAD to generate 2D and 3D representations. Each row visualizes the anomaly score maps of 2D renderings from different views, and the final point score maps are also presented. More visualizations are provided in Appendix J. ", "page_idx": 6}, {"type": "text", "text": "4.2 Implementation Details & Baselines ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Both point clouds and 2D renderings are resized to $336~~\\times~~336$ . We use Open3d library to generate 9 views by rotating point clouds along the X-axis at angles of $\\{-\\textstyle{\\frac{4}{5}}\\bar{\\pi},-\\textstyle{\\frac{3}{5}}\\pi,-\\textstyle{\\frac{2}{5}}\\pi,-\\textstyle{\\frac{1}{5}}\\pi,0,\\textstyle{\\frac{1}{5}}\\pi,\\textstyle{\\frac{2}{5}}\\bar{\\pi},\\textstyle{\\frac{3}{5}}\\pi,\\textstyle{\\frac{4}{5}}\\pi\\}$ for most categories. We circularly set the rendering angles, evenly distributing the angles between $-\\pi$ to $\\pi$ . The backbone of PointAD is the pre-trained CLIP model $(\\mathtt{V I T-L}/14@336\\mathtt{p x}$ in open_clip). Following [63], we improve the local visual semantics of vision encoder of CLIP without modifying its parameters. During training, we keep all parameters of CLIP frozen and set the learnable word embeddings in object-agnostic text templates to 12. All experiments were conducted on a single NVIDIA RTX 3090 24GB GPU using PyTorch-2.0.0. As there is no work to explore the field of ZS 3D anomaly detection, we make a great effort to provide these comparisons. We apply the original CLIP to our framework for 3D detection, called CLIP $^+$ Rendering. Also, we reproduce SOTA 3D recognition works including PointCLIP V2 [64] and Cheraghian [12], and adapt them for ZS 3D anomaly detection. We compare the SOTA 2D anomaly detection approach AnomalyCLIP [63] by fine-tuning it on depth maps. PointAD by default uses object-agnostic text prompts, whereas PointAD-CoOp employs object-aware prompts. Appendix B and C provide more details on implementation and baselines. ", "page_idx": 6}, {"type": "text", "text": "4.3 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We fine-tuned PointAD on three objects on MVTec3D-AD, Eyecandies, and Real3D-AD. Over three runs, the averaged results on one-vs-rest and cross-dataset settings are reported. We use the metric pairs $\\scriptstyle(\\mathrm{I-AUROC}\\%\\%\\uparrow$ and $\\mathrm{AP\\%\\\\%\\uparrow}$ ) and $(\\mathrm{P-AUROC}\\%\\%\\uparrow$ and $\\mathrm{AUPRO}\\%\\%\\uparrow$ ) to evaluate the glocal detection performance, respectively. Details of experimental settings see Appendix A. The best and second-best results in ZS are highlighted in Red and Blue. G. and L. represent 3D global and local anomaly detection. M3D global and local anomaly detection are abbreviated as MG. and ML. ", "page_idx": 6}, {"type": "text", "text": "ZS 3D anomaly detection Table 1 presents the comparison of ZS 3D performance. Compared to the point-based method Cheraghian and the projection-based method PointCLIP V2, PointAD achieves superior performance on ZS 3D anomaly detection over all three datasets. Especially, it outperforms CLIP $^+$ Rendering from $61.2\\%$ to $82.0\\%$ I-AUROC and from $85.8\\%$ to $94.2\\%$ AP on MVTec3D-AD. In addition, PointAD achieves superior segmentation performance on ZS 3D anomaly detection, improving MVTec3D-AD by a large margin compared to Cheraghian from $88.2\\%$ to $95.5\\%$ P-AUROC and from $57.0\\%$ to $84.4\\%$ AUPRO. This improvement in overall performance is attributed to PointAD adapting CLIP\u2019s strong generalization to glocal anomaly semantics through hybrid representation learning. In addition, PointAD advances PointAD-CoOp across all datasets by blocking the class semantics in text prompts [63]. ", "page_idx": 6}, {"type": "image", "img_path": "02CIZ8qeDc/tmp/df54632067ea0899b73296390f36e6351102cb6e05fa7f6a0135964257effbc9.jpg", "img_caption": ["(a) Multimodal visualization with hybrid loss. (b) Multimodal visualization without 2D glocal loss. Figure 4: Visualization comparison between PointAD with hybrid loss and without. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "ZS M3D anomaly detection We also compare the ZS M3D anomaly detection when RGB information is available for testing. As shown in Table 2, the results indicate that PointAD can integrate additional RGB information and further boost its performance from $82.0\\%$ to $86.9\\%$ AUROC and from $94.2\\%$ to $96.1\\%$ AP for global semantics on MVTec3D-AD. Additionally, as for local semantics, the performance improves from $95.5\\%$ to $97.2\\%$ P-AUROC and from $84.4\\%$ to $90.2\\%$ AUPRO. A large performance gain is also obtained on Eyecandies and Real3D-AD. While other methods improve their performance in some metrics, they still suffer from performance degradation in other metrics due to inefficient integration of the two modalities. Instead, PointAD achieves overall improvement across all metrics by incorporating explicit joint constraints on both point and pixel information. ", "page_idx": 7}, {"type": "text", "text": "Cross-dataset ZS anomaly detection We perform the cross-dataset anomaly recognition to further evaluate the zero-shot capacity of PointAD, where we use one object as the auxiliary and test objects with totally different semantics and scenes in another dataset. We compare all baselines that need fine-tuning. From Table 3 and M3D from ", "page_idx": 7}, {"type": "table", "img_path": "02CIZ8qeDc/tmp/ed39faeef3912ad0b7a8b8619b5c72ae520c0ea380808a5c447d0d4a5d6ea9a8.jpg", "table_caption": ["Table 3: Performance comparison on ZS 3D anomaly detection in cross-dataset setting. ", "Table 4: Performance comparison on ZS M3D anomaly detection in cross-dataset setting. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 4, PointAD demonstrates strong cross-dataset generalization performance on Eyecandies and Real3D-AD, with nearly no obvious performance decay compared to the one-vs-rest setting. The strong transfer ability highlights its robust generalization capabilities in detecting anomalies in objects with unseen semantics and backgrounds. ", "page_idx": 7}, {"type": "text", "text": "4.4 Result Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Visualization analysis. To intuitively present the strong generalization capacity of our model to unseen anomalies, we visualize the anomaly score maps of the 3D and corresponding 2D counterparts of PointAD on MVTec3D-AD. As shown in Figure 3, PointAD reveals abnormal spatial relationships of points and further captures the generic point anomaly patterns across diverse objects. And, we also visualize the anomaly score map of corresponding 2D counterparts, where 3D point anomalies are transformed into 2D pixel anomalies. It can be observed that PointAD also has a strong detection ability on such 2D anomalies. The strong representative pixel representations from multiple views facilitate more precise 3D anomaly detection. Quantitative results are provided in Section 5. The strong 3D and 2D detection capabilities of PointAD are from hybrid representation learning, which not only enables PointAD to capture the 3D anomalies but also explicitly constrains 2D representations. ", "page_idx": 7}, {"type": "text", "text": "How multimodality makes PointAD accurate. PointAD is a unified framework that can not only capture point anomalies but also handle 2D information in a plug-and-play manner. As shown in Figure 4(a), we visualize M3D results of PointAD on MVTec3D-AD. The surface damage on the potato presents a similar appearance to the object foreground, which makes it difficult to detect this anomaly with RGB information. On the contrary, the point relations for the color stain on foam are the same as those of normal, but they have a clear distinction in the RGB information. PointAD can integrate these two modalities, thereby complementing their respective advantages. We further investigate the reason why PointAD can directly leverage both modalities. For this purpose, we experiment without 2D glocal loss. As shown in Figure 4(b), without 2D glocal loss, significant noise disrupts and even covers the RGB score maps, resulting in unpromising multimodal fusions. This illustrates the importance of explicit constraints on the 2D space. Hence, we conclude that the robust multimodal detection capability of our model stems from the collaboration optimization in both 3D and 2D spaces during training. We provide more analysis about failure cases and the computation overhead in Appendix G and H. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Module Ablation. Here, we investigate the effectiveness of the proposed main technologies by progressively adding the proposed modules. Table 6 illustrates that vanilla, which represents the aforementioned CLIP $^+$ rendering, performs poor results on both 3D and M3D anomaly detection because CLIP focuses on alignment for 2D object semantics instead of anomaly semantics. With the 3D global branch, we incorporate the global anomaly semantics into PointAD, improving overall performance in local and global detection. After adding the 3D local branch, the performance is further improved, while the pixel-level performance on M3D detection suffers from performance degradation. This is attributed to the absence of 2D constraints, leading to inefficient multimodality fusion as we integrate 2D RGB information in a plug-and-play way. The inclusion of 2D global branch explicitly incorporates 2D anomaly information, which makes PointAD obtain overall performance gain. Finally, by further focusing on 2D anomaly regions, PointAD has a deeper understanding of point clouds from 2D representations and promotes multimodality fusion. Therefore, our model notably boosts the multimodal segmentation performance from $92.9\\%$ to $97.2\\%$ P-AUROC and from $84.4\\%$ to $90.2\\%$ AUPRO. ", "page_idx": 8}, {"type": "table", "img_path": "02CIZ8qeDc/tmp/d6812abc0d83132e8c0f218a983f60785cdd6d52b9a6a147b5526f04f6224bb6.jpg", "table_caption": ["Table 5: Ablation on rendering number. ", "Table 6: Ablation on the proposed modules. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "View Number Ablation. PointAD interprets point clouds from 2D renderings, and the quantity of rendering views directly affects the 3D original information acquired by PointAD. Table 5 depicts that the appropriate number of views benefits point understanding from informative views while alleviating negative effects of subpar views. More ablations about the length of learnable prompts, layers of intermediate vision features, and the number of training sets are provided in Appendix F. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This paper takes the first attempt to study the challenging yet underexplored tasks of ZS 3D and M3D anomaly detection. We propose a unified framework, namely PointAD, to transfer the strong generalization of CLIP to 3D point clouds. PointAD understands 3D anomalies from 3D and 2D spaces. Beneftiing from hybrid representation learning, PointAD can recognize generic 3D normality and abnormality across diverse objects and directly integrate RGB information for ZS M3D. Extensive experiments demonstrate the superior ZS detection capacity of our model, whether single modality or multimodality. Code will be made available once the paper is accepted. ", "page_idx": 8}, {"type": "text", "text": "Limitations PointAD utilizes fixed rendering angles to generate 2D renderings across diverse objects. While experimental results demonstrate its superiority, the development of a fine-grained flitering mechanism to select high-quality 2D renderings, particularly for revealing anomalies, remains an avenue for future research. ", "page_idx": 8}, {"type": "text", "text": "Broader Impact Our paper aims to enhance automated detection and decision-making in smart manufacturing, which does not involve any potential ethical risks. Since the collection of 3D samples is more labor-intensive and costly, our research on using vision-language models for zero-shot point cloud detection can have significant societal impacts, especially in scenarios where target 3D training samples are unavailable due to privacy concerns or the absence of products. We hope that our first exploration of the ZS 3D anomaly detection field could pave the way for further research in this emerging field. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by NSFC U23A20326 and NSFC 62088101 Autonomous Intelligent Unmanned Systems. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Toshimichi Aota, Lloyd Teh Tzer Tong, and Takayuki Okatani. Zero-shot versus many-shot: Unsupervised texture anomaly detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 5564\u20135572, 2023. [2] Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Mvtec ad\u2013a comprehensive real-world dataset for unsupervised anomaly detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9592\u20139600, 2019. [3] Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4183\u20134192, 2020. [4] Paul Bergmann and David Sattlegger. Anomaly detection in 3d point clouds using deep geometric descriptors. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2613\u20132623, 2023. [5] Luca Bonfiglioli, Marco Toschi, Davide Silvestri, Nicola Fioraio, and Daniele De Gregorio. The eyecandies dataset for unsupervised multimodal anomaly detection and localization. In Proceedings of the Asian Conference on Computer Vision (ACCV), pages 3586\u20133602, December 2022.   \n[6] Tri Cao, Jiawen Zhu, and Guansong Pang. Anomaly detection under distribution shift. arXiv preprint arXiv:2303.13845, 2023.   \n[7] Yunkang Cao, Xiaohao Xu, and Weiming Shen. Complementary pseudo multimodal feature for point cloud anomaly detection. arXiv preprint arXiv:2303.13194, 2023. [8] Yunkang Cao, Xiaohao Xu, Jiangning Zhang, Yuqi Cheng, Xiaonan Huang, Guansong Pang, and Weiming Shen. A survey on visual anomaly detection: Challenge, approach, and prospect. ArXiv, abs/2401.16402, 2024. [9] Ruitao Chen, Guoyang Xie, Jiaqi Liu, Jinbao Wang, Ziqi Luo, Jinfan Wang, and Feng Zheng. Easynet: An easy network for 3d industrial anomaly detection. In Proceedings of the 31st ACM International Conference on Multimedia, pages 7038\u20137046, 2023.   \n[10] Xuhai Chen, Yue Han, and Jiangning Zhang. A zero-/few-shot anomaly classification and segmentation method for cvpr 2023 vand workshop challenge tracks 1&2: 1st place on zero-shot ad and 4th place on few-shot ad. arXiv preprint arXiv:2305.17382, 2023.   \n[11] Yuanhong Chen, Yu Tian, Guansong Pang, and Gustavo Carneiro. Deep one-class classification via interpolated gaussian descriptor. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 383\u2013392, 2022.   \n[12] Ali Cheraghian, Shafin Rahman, Townim F Chowdhury, Dylan Campbell, and Lars Petersson. Zero-shot learning on 3d point cloud objects and beyond. International Journal of Computer Vision, 130(10):2364\u20132384, 2022.   \n[13] Yu-Min Chu, Chieh Liu, Ting-I Hsieh, Hwann-Tzong Chen, and Tyng-Luh Liu. Shape-guided dual-memory learning for 3d anomaly detection. In International Conference on Machine Learning, pages 6185\u20136194. PMLR, 2023.   \n[14] Niv Cohen and Yedid Hoshen. Sub-image anomaly detection with deep pyramid correspondences. arXiv preprint arXiv:2005.02357, 2020.   \n[15] Hanqiu Deng and Xingyu Li. Anomaly detection via reverse distillation from one-class embedding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9737\u20139746, 2022.   \n[16] Hanqiu Deng, Zhaoxiang Zhang, Jinan Bao, and Xingyu Li. Anovl: Adapting vision-language models for unified zero-shot anomaly localization. arXiv preprint arXiv:2308.15939, 2023.   \n[17] Sepideh Esmaeilpour, Bing Liu, Eric Robertson, and Lei Shu. Zero-shot out-of-distribution detection based on the pre-trained model clip. In Proceedings of the AAAI conference on artificial intelligence, volume 36, pages 6568\u20136576, 2022.   \n[18] Ankit Goyal, Hei Law, Bowei Liu, Alejandro Newell, and Jia Deng. Revisiting point cloud shape classification with a simple and effective baseline. In International Conference on Machine Learning, pages 3809\u20133820. PMLR, 2021.   \n[19] Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Ming Tang, and Jinqiao Wang. Anomalygpt: Detecting industrial anomalies using large vision-language models, 2023.   \n[20] Denis Gudovskiy, Shun Ishizaka, and Kazuki Kozuka. Cflow-ad: Real-time unsupervised anomaly detection with localization via conditional normalizing flows. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 98\u2013107, 2022.   \n[21] Abdullah Hamdi, Silvio Giancola, and Bernard Ghanem. Mvtn: Multi-view transformation network for 3d shape recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1\u201311, 2021.   \n[22] Eliahu Horwitz and Yedid Hoshen. Back to the feature: classical 3d features are (almost) all you need for 3d anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2967\u20132976, 2023.   \n[23] Chaoqin Huang, Haoyan Guan, Aofan Jiang, Ya Zhang, Michael Spratling, and Yan-Feng Wang. Registration based few-shot anomaly detection. In European Conference on Computer Vision, pages 303\u2013319. Springer, 2022.   \n[24] Jongheon Jeong, Yang Zou, Taewan Kim, Dongqing Zhang, Avinash Ravichandran, and Onkar Dabeer. Winclip: Zero-/few-shot anomaly classification and segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19606\u201319616, 2023.   \n[25] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. Maple: Multi-modal prompt learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19113\u201319122, 2023.   \n[26] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023.   \n[27] Aodong Li, Chen Qiu, Marius Kloft, Padhraic Smyth, Maja Rudolph, and Stephan Mandt. Zero-shot anomaly detection via batch normalization. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[28] Wenqiao Li, Xiaohao Xu, Yao Gu, Bozhong Zheng, Shenghua Gao, and Yingna Wu. Towards scalable 3d anomaly detection and localization: A benchmark via 3d anomaly synthesis and a self-supervised learning network. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 22207\u201322216, 2023.   \n[29] Xurui Li, Ziming Huang, Feng Xue, and Yu Zhou. Musc: Zero-shot industrial anomaly classification and segmentation with mutual scoring of the unlabeled images. ArXiv, abs/2401.16753, 2024.   \n[30] Yiting Li, Adam David Goodge, Fayao Liu, and Chuan-Sheng Foo. Promptad: Zero-shot anomaly detection using text prompts. 2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 1082\u20131091, 2024.   \n[31] Jiaqi Liu, Guoyang Xie, ruitao chen, Xinpeng Li, Jinbao Wang, Yong Liu, Chengjie Wang, and Feng Zheng. Real3d-AD: A dataset of point cloud anomaly detection. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.   \n[32] Philipp Liznerski, Lukas Ruff, Robert A Vandermeulen, Billy Joe Franks, Marius Kloft, and Klaus-Robert M\u00fcller. Explainable deep one-class classification. arXiv preprint arXiv:2007.01760, 2020.   \n[33] Philipp Liznerski, Lukas Ruff, Robert A Vandermeulen, Billy Joe Franks, Klaus-Robert M\u00fcller, and Marius Kloft. Exposing outlier exposure: What can be learned from few, one, and zero outlier images. arXiv preprint arXiv:2205.11474, 2022.   \n[34] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.   \n[35] Yifei Ming, Ziyang Cai, Jiuxiang Gu, Yiyou Sun, Wei Li, and Yixuan Li. Delving into out-ofdistribution detection with vision-language representations. Advances in Neural Information Processing Systems, 35:35087\u201335102, 2022.   \n[36] Guansong Pang, Choubo Ding, Chunhua Shen, and Anton van den Hengel. Explainable deep few-shot anomaly detection with deviation networks. arXiv preprint arXiv:2108.00462, 2021.   \n[37] Guansong Pang, Chunhua Shen, Longbing Cao, and Anton Van Den Hengel. Deep learning for anomaly detection: A review. ACM computing surveys (CSUR), 54(2):1\u201338, 2021.   \n[38] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 652\u2013660, 2017.   \n[39] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Advances in neural information processing systems, 30, 2017.   \n[40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[41] Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu. Denseclip: Language-guided dense prediction with context-aware prompting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18082\u201318091, 2022.   \n[42] Tal Reiss, Niv Cohen, Liron Bergman, and Yedid Hoshen. Panda: Adapting pretrained features for anomaly detection and segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2806\u20132814, 2021.   \n[43] Tal Reiss and Yedid Hoshen. Mean-shifted contrastive loss for anomaly detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 2155\u20132162, 2023.   \n[44] Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Sch\u00f6lkopf, Thomas Brox, and Peter Gehler. Towards total recall in industrial anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14318\u201314328, 2022.   \n[45] Marco Rudolph, Tom Wehrbein, Bodo Rosenhahn, and Bastian Wandt. Asymmetric studentteacher networks for industrial anomaly detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2592\u20132602, 2023.   \n[46] Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexander Binder, Emmanuel M\u00fcller, and Marius Kloft. Deep one-class classification. In International conference on machine learning, pages 4393\u20134402. PMLR, 2018.   \n[47] Bernhard Sch\u00f6lkopf, Robert C Williamson, Alexander J Smola, John Shawe-Taylor, John C Platt, et al. Support vector method for novelty detection. In NIPS, volume 12, pages 582\u2013588. Citeseer, 1999.   \n[48] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition. In Proceedings of the IEEE international conference on computer vision, pages 945\u2013953, 2015.   \n[49] Ximeng Sun, Ping Hu, and Kate Saenko. Dualcoop: Fast adaptation to multi-label recognition with limited annotations. Advances in Neural Information Processing Systems, 35:30569\u201330582, 2022.   \n[50] Yiyou Sun, Zhenmei Shi, Yingyu Liang, and Yixuan Li. When and how does known class help discover unknown ones? provable understandings through spectral analysis. In International Conference on Machine Learning, 2023.   \n[51] Yu Tian, Fengbei Liu, Guansong Pang, Yuanhong Chen, Yuyuan Liu, Johan W Verjans, Rajvinder Singh, and Gustavo Carneiro. Self-supervised pseudo multi-class pre-training for unsupervised anomaly detection and segmentation in medical images. Medical Image Analysis, page 102930, 2023.   \n[52] Yu Tian, Guansong Pang, Fengbei Liu, Yuanhong Chen, Seon Ho Shin, Johan W Verjans, Rajvinder Singh, and Gustavo Carneiro. Constrained contrastive distribution learning for unsupervised anomaly detection and localisation in medical images. In Medical Image Computing and Computer Assisted Intervention\u2013MICCAI 2021: 24th International Conference, Strasbourg, France, September 27\u2013October 1, 2021, Proceedings, Part V 24, pages 128\u2013140. Springer, 2021.   \n[53] Chengjie Wang, Haokun Zhu, Jinlong Peng, Yue Wang, Ran Yi, Yunsheng Wu, Lizhuang Ma, and Jiangning Zhang. M3dm-nr: Rgb-3d noisy-resistant industrial anomaly detection via multimodal denoising. arXiv preprint arXiv:2406.02263, 2024.   \n[54] Yue Wang, Jinlong Peng, Jiangning Zhang, Ran Yi, Yabiao Wang, and Chengjie Wang. Multimodal industrial anomaly detection via hybrid fusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8032\u20138041, 2023.   \n[55] Guoyang Xie, Jingbao Wang, Jiaqi Liu, Feng Zheng, and Yaochu Jin. Pushing the limits of fewshot anomaly detection in industry vision: Graphcore. arXiv preprint arXiv:2301.12082, 2023.   \n[56] Zhiyuan You, Lei Cui, Yujun Shen, Kai Yang, Xin Lu, Yu Zheng, and Xinyi Le. A unified model for multi-class anomaly detection. Advances in Neural Information Processing Systems, 35:4571\u20134584, 2022.   \n[57] Jiawei Yu, Ye Zheng, Xiang Wang, Wei Li, Yushuang Wu, Rui Zhao, and Liwei Wu. Fastflow: Unsupervised anomaly detection and localization via 2d normalizing flows. arXiv preprint arXiv:2111.07677, 2021.   \n[58] Vitjan Zavrtanik, Matej Kristan, and Danijel Sko\u02c7caj. Cheating depth: Enhancing 3d surface anomaly detection via depth simulation. 2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 2153\u20132161, 2023.   \n[59] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xupeng Miao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng Li. Pointclip: Point cloud understanding by clip. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8552\u20138562, 2022.   \n[60] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16816\u201316825, 2022.   \n[61] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. International Journal of Computer Vision, 130(9):2337\u20132348, 2022.   \n[62] Qihang Zhou, Shibo He, Haoyu Liu, Tao Chen, and Jiming Chen. Pull & push: Leveraging differential knowledge distillation for efficient unsupervised anomaly detection and localization. IEEE Transactions on Circuits and Systems for Video Technology, 2022.   \n[63] Qihang Zhou, Guansong Pang, Yu Tian, Shibo He, and Jiming Chen. Anomalyclip: Objectagnostic prompt learning for zero-shot anomaly detection, 2023.   \n[64] Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Ziyao Zeng, Zipeng Qin, Shanghang Zhang, and Peng Gao. Pointclip v2: Prompting clip and gpt for powerful 3d open-world learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2639\u20132650, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Dataset ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Dataset We evaluate the performance of ZS 3D anomaly detection on three publicly available 3D anomaly detection datasets, MVTec3D-AD, Eyecandies, and Real3D-AD. MVTec3D-AD comprises 4147 point clouds across 10 categories. These objects exhibit diverse object semantics, including bagel, cable gland, carrot, cookie, dowel, foam, peach, potato, rope, and tire. The training dataset comprises 2656 normal point clouds, and the validation dataset comprises 294 normal point clouds. The test dataset includes 948 normal and 249 anomaly point clouds, covering several anomaly types. Point-wise annotations are available for the point clouds. MVTec3D-AD also provides corresponding 2D-RGB image counterparts for the point clouds. We remove the background plane of point clouds in the whole dataset like [22]. Eyecandies also has 10 different classes and provides the corresponding 2D RGB information. Real3D-AD is a recently available dataset, which contains 12 objects. However, it does not provide the RGB information. ", "page_idx": 14}, {"type": "text", "text": "Evaluation Setting and Metric Since the training dataset of MVTec3D-AD only contains all normal samples, we use an object test dataset as the auxiliary dataset to fine-tune PointAD and assess the ZS anomaly detection for the remaining objects. In particular, we report the average results using different objects as the auxiliary, i.e., carrot, cookie, and dowel for MVTec3DAD; confetto, LicoriceSandwich, and PeppermintCandy for Eyecandie; seahorse, shell, and starfish for Real3D-AD. Moreover, we explore more challenging cross-dataset generalization settings, where we use auxiliary data to test all objects of another dataset. For point cloud anomaly detection, we only use point clouds to detect and localize 3D anomalies in Figure 5(a). In M3D anomaly detection, both point clouds and their 2D-RGB counterparts are utilized, as shown in Figure 5(b). To comprehensively analyze PointAD, we utilize four metrics to assess its anomaly classification and segmentation performance. For anomaly detection, we use the Area Under the Receiver Operating Characteristic Curve $(\\mathrm{I}{-}\\mathrm{AUROC\\%}\\uparrow)$ ) and average precision $(\\mathrm{AP\\%}$ \u2191). Regarding anomaly segmentation, we use point-level AUROC $(\\mathrm{P-AUROC\\%}\\uparrow$ ) and a restricted metric called $\\mathrm{AUPRO}\\%(\\uparrow)$ [3] to provide a detailed evaluation of subtle anomaly regions. ", "page_idx": 14}, {"type": "image", "img_path": "02CIZ8qeDc/tmp/1801f04e86208afb6f1b086a238259f54bf81f9967755bef9742da9b3aa000d8.jpg", "img_caption": ["Figure 5: Inference schematic for ZS 3D and M3D anomaly detection. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Implementation Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table 7: Ablation study on the number of rendering views. ", "page_idx": 14}, {"type": "table", "img_path": "02CIZ8qeDc/tmp/adbfcd1f0d4f977a6d035601bfe76540dad607263b40bb2906cca56b53b3419f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Both point clouds and 2D renderings are resized to $336~\\times~336$ . We use Open3d library2 to generate 9 views by rotating point clouds along the $\\boldsymbol{\\mathrm{X}}$ -axis at angles of $\\begin{array}{c}{{\\left\\{-\\frac{4}{5}\\pi,-\\frac{3}{5}\\pi,-\\frac{2}{5}\\pi,-\\frac{1}{5}\\pi,0,\\frac{1}{5}\\pi,\\frac{2}{5}\\pi,\\frac{3}{5}\\pi,\\frac{4}{5}\\pi\\right\\}}}\\end{array}$ for most categories. Some categories lose their surface completely because they are not stereo point clouds. Table 7 also gives the specific rendering angles of different view number settings. We set the rendering angles in a circular manner, evenly distributing the angles between $-\\pi$ to $\\pi$ . The backbone of PointAD is the pre-trained CLIP model3 (VIT-L/14@336px). Following [63], we improve the local visual semantics of the vision encoder of CLIP without modifying its parameters and introduce learnable tokens in the text encoder. During training, we keep all CLIP parameters frozen and set the learnable word embeddings in objectagnostic text templates to 12. We use the Adam optimizer with a learning rate of 0.001 to optimize the learnable parameters. The experiment runs for 15 epochs with a batch size of 4. All experiments were conducted on a single NVIDIA RTX 3090 24GB GPU using PyTorch-2.0.0. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "C Baselines ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "ZS 3D anomaly detection and M3D anomaly detection have not yet been explored. We first make an adaption for the original CLIP for 3D anomaly detection. Then, we reproduce SOTA ZS 3D classification methods (i.e., PointCLIP V2 [64] and Cheraghian [12]) and adapt them to our settings. SOTA unsupervised 3D anomaly detection approaches are reported as the performance upper bound. All hyperparameters in these baselines are kept the same. We will present the detailed reproduction as follows: ", "page_idx": 15}, {"type": "text", "text": "\u2022 CLIP $^+$ Rendering is a method, where we apply the original CLIP into our framework for ZS 3D anomaly detection. It uses the same rendering procedure as PointAD. Following [24, 63], we integrate anomaly semantics into CLIP by two class text prompt templates: A photo of a normal [cls] and A photo of an anomalous [cls], where cls denotes the target class name.   \n\u2022 PointCLIP V2 (CVPR 2023) is a SOTA ZS 3D classification method based on CLIP, they project point clouds into depth maps from different views. To adapt PonitCLIPV2 into ZS anomaly detection, we replace its original text prompts point cloud of a big [c] with normal text prompts point cloud of a big [c] and abnormal text prompts point cloud of a big damaged [c].   \n\u2022 AnomalyCLIP (ICLR 2024) is a SOTA zero-shot 2D anomaly detection method. AnomalyCLIP introduces object-agnostic learning to capture generic anomaly semantics of images. We adapt AnomalyCLIP in 3D detection by fine-tuning AnomalyCLIP on depth images of corresponding point clouds.   \n\u2022 Cheraghian (IJCV 2022) is an approach for ZS 3D anomaly detection without foundation models. They directly extract the point presentations by PointNet and use word2vector [34] to generate the textual embedding of an object. To incorporate the anomaly semantics into Cheraghian, we respectively average the textual embedding of an object name and damaged. We replace the global representation with dense representations to provide the segmentation results. ", "page_idx": 15}, {"type": "text", "text": "D Related Work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "D.1 2D Anomaly Detection ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "2D anomaly detection has been studied extensively by leveraging RGB information [47, 46, 14, 52, 42, 11, 62, 43]. Related works can be categorized into two branches: end-to-end and memorybased methods. Representative end-to-end methods exploit knowledge distillation [3, 62, 45] and normalizing flow [20, 57] to model the normal distribution. Instead, memory-based methods [44, 55] store normal features to construct normal prototypes. ZS 2D anomaly detection is proposed to target a challenging problem where training samples are inaccesible [17, 27, 33, 1, 16, 10]. WinCLIP [24] attempts to explore ZS 2D anomaly detection using CLIP. AnomalyCLIP [63] first introduces objectagnostic prompt learning to capture generic normality and abnormality, detecting anomalies across datasets. PromptAD [30] focuses on effectively fusing these embeddings to enhance zero-shot detection performance. ", "page_idx": 15}, {"type": "text", "text": "Figure 6: Viusalization with different rendering quality. A larger $\\sigma$ represents poorer rendering quality. ", "page_idx": 16}, {"type": "text", "text": "Table 8: Analysis on the rendering quality. Th original setting is highlighted in gray. ", "page_idx": 16}, {"type": "image", "img_path": "02CIZ8qeDc/tmp/db9bb6894b1eeba5b4327e4ff56c7358e7269f6667805832f8a91d138e1f3519.jpg", "img_caption": ["Figure 7: Visualization with different resolutions. We downsample entire point clouds with different ratios to obtain diverse resolutions. "], "img_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "02CIZ8qeDc/tmp/2d23aeaeeb6b6ab2296cfdccf1b6a39a6f7589230828980282920b97e8d11269.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "02CIZ8qeDc/tmp/9fb93eada5bb9844d3e714ba6ca6a5c908fea48aa17d78d2490528a770bb6ab6.jpg", "table_caption": ["Table 9: Analysis on the input solution. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "E Analysis on Rendering Conditions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Rendering quality PointAD interprets point clouds through their corresponding 2D renderings, and the quality of these renderings impacts the information that PointAD can extract from the original point clouds. In our manuscript, we used the Open3D Library to render the point clouds, but it does not provide an API for controlling rendering quality. To simulate varying rendering quality, we applied Gaussian blur with different extents $\\sigma$ to the 2D renderings. Sample visualizations are included in Figure 6. Specifically, we conducted experiments on MVTec3D-AD using different blur $\\sigma$ values (i.e., ). Table 8 shows that the detection performance of PointAD diminishes as rendering quality decreases (with increasing sigma). However, the degradation is acceptable even when the renderings are heavily blurred $\\bar{\\sigma}$ equals 9). In such cases, PointAD still outperforms baselines that use high-quality renderings. ", "page_idx": 16}, {"type": "text", "text": "Input Resolution Here, we study the effect of resolutions of input point clouds. To create lowresolution point clouds, we downsample the entire high-resolution point clouds using Farthest Point Sampling (FPS) with various sampling ratios. This strategy allows us to generate corresponding low-resolution datasets for training and evaluating PointAD. Visualizations of these datasets are provided in Figure 7. We train PointAD using the resulting low-resolution samples and test PointAD on the same resolution. Table 9 demonstrates that PointAD maintains a strong detection capacity for low-resolution point clouds when the downsampling ratios are $20\\%$ , $30\\%$ , $50\\%$ , and $70\\%$ . Even at $20\\%$ resolution, PointAD still achieves state-of-the-art performance. This indicates that PointAD is generally applicable to point clouds with various resolutions. ", "page_idx": 16}, {"type": "text", "text": "Rendering angles and different numbers of views PointAD interprets point clouds from their 2D renderings, where rendering angles and numbers collectively determine the amount of information derived. They have different emphases. For rendering angles, the importance lies in the discrepancy between adjacent angles, as this affects the information granularity that PointAD obtains from adjacent views. When the angle discrepancy is fixed, the number of renderings determines the coverage of 3D information in the resulting 2D renderings. To capture all point cloud information, especially abnormal points, it is crucial to ensure comprehensive coverage. Therefore, our approach in selecting rendering angles and the number of renderings is to guarantee that all points in point clouds are adequately represented. ", "page_idx": 16}, {"type": "text", "text": "Based on this principle, we conducted experiments to assess the impact of the number of renderings on PointAD\u2019s detection performance, circularly rendering point clouds to ensure even coverage of all points. As shown in Table 5, increasing the number of views allows PointAD to gather more detailed information from the 2D renderings, benefiting from smaller angle discrepancies, which improves detection and localization results. However, when the number of views increased from 9 to 11, we observed a performance decline in PointAD, with the I-AUROC for global multimodal detection dropping from $87.4\\%$ to $86.4\\%$ . This suggests that incorporating too many views could introduce redundant information, resulting in 2D renderings with extensive overlap and excessive local detail. This overemphasis on local information can impede global recognition. Hence, the appropriate number of views benefits point understanding from informative views while mitigating the adverse effects of redundant local information. To further explore the impact of the absolute angle, we shift the rendering angles while keeping the angle discrepancy unchanged. The original adjacent angle discrepancy in our paper is $\\textstyle{\\frac{1}{5}}\\pi$ . We divide this discrepancy into four parts and perform angle shifts o f210\u03c0,220\u03c0, and230\u03c0 to test the impact of varying rendering angles. Table 10 shows that PointAD maintains consistent performance across different rendering angles, demonstrating its robustness to variations in angles different from those used during training. ", "page_idx": 16}, {"type": "table", "img_path": "02CIZ8qeDc/tmp/880c8531f38ee99adc58a51486100e0ccd6bd56cf7da5feabdf926929e6a6cb4.jpg", "table_caption": ["Table 12: Analysis on the point occlusions "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "table", "img_path": "02CIZ8qeDc/tmp/dabc854f4f6821fc5ff4a71d7cd71bc1796469835d732b151e52d3147d20a2d1.jpg", "table_caption": ["Table 10: Analysis on the rendering angle. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "02CIZ8qeDc/tmp/ef55ec534b34263759321c55ab755c502a1b76d3321b8d79fa68b60fa5f478b1.jpg", "table_caption": ["Table 11: Analysis on the rendering lighting. "], "table_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "02CIZ8qeDc/tmp/4f6a53f23fb5eaaf7105f6166db28715b86bbb560e2940c7db50203eb1da616e.jpg", "img_caption": ["Figure 8: Visualization with different rendering lighting. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "02CIZ8qeDc/tmp/6f6412d15e0728fb2ac0770034712247e930507ab75670811735833e75267e2b.jpg", "img_caption": ["cluded point clouds. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Rendering lighting Further exploring the robustness of PointAD under different conditions could enhance its generalized detection performance. We conducted ablation studies to test its sensitivity under different lighting conditions and with occluded point clouds below. To evaluate the impact of rendering lighting, we adjusted the lighting conditions to render point clouds, generating variant datasets with different lighting. We used both stronger and weaker lighting to render point clouds compared to the original dataset, covering a broad lighting range. We denote stronger and the strongest lighting as $\"+\"$ and $\"++\"$ , and weaker and the weakest lighting as \"-\" and \"\u2013\". Visualizations of the resulting samples are presented in Figure 8. The experiments were conducted on MVTec3D-AD, where we tested PointAD, trained on the original dataset, on these lighting variant datasets. Table 11 shows that PointAD can still detect anomalies even with significant discrepancies in rendering lighting, suggesting that PointAD is not sensitive to variations in rendering lighting. ", "page_idx": 17}, {"type": "text", "text": "Point occlusions Next, we evaluated detection performance with occluded point clouds. We occluded point clouds by removing those invisible from a specific rendering angle and then used the same rendering parameters to project the remaining points. During this process, we observed that abnormal regions might be occluded totally. Unlike class classification, where class semantics remain unchanged by point occlusions, removing these anomaly semantics would transform anomaly samples into normal points. Therefore, we selected rendering angles that allow visibility of part or all anomalous regions when the point cloud is an abnormal instance. The occluded point clouds are shown in Figure 9. We then used PointAD, trained on the original dataset, to test the resulting occluded dataset. Table 12 shows that PointAD suffers from performance degradation when the point clouds are occluded. We attribute this to two aspects: 1) Despite this strategy, the occluded point clouds could still lose part of the anomaly semantics; 2) Occluded point clouds could create unexpected sinkholes on the surface, which may cause PointAD to identify these areas as hole anomalies incorrectly. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "F Hyperparameter Ablation ", "text_level": 1, "page_idx": 18}, {"type": "table", "img_path": "02CIZ8qeDc/tmp/361e30317952d9863283383a0dc8d99113dbf6835b7384b280e1aaa4c9c5cf89.jpg", "table_caption": ["Table 13: Ablation study on the length of the learnable prompt. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Length of learnable prompts We study the sensitivity of important hyperparameters in PointAD. First, we explore the length of learnable text prompt templates, as shown in Table. 13. As the length of word embeddings increases, PointAD can better learn 3D and 2D anomaly semantics to improve its performance. Nonetheless, with a further increase in length (i.e., from 12 to 16), a decline in performance becomes noticeable. The excessive or insufficient number of learnable word embeddings can lead to performance degradation. An appropriate length (i.e., 12) is important for PointAD to attain comprehensive performance in both 3D and M3D anomaly detection. ", "page_idx": 18}, {"type": "text", "text": "Training set We have increased the test data for each category to incorporate more instances on MVTec3D-AD. Originally, we evaluated the rest of the data using only one category as the test data, such as carrot, cookie, and dowel. Now, we attempt to incorporate more instances. Here, we utilized two categories as auxiliary data, including carrot and cookies, carrot and dowel, and cookie and dowel. To further analyze the effect of the size of auxiliary data, we utilized three categories as auxiliary data, selecting all possible combinations from the four sets: carrot, cookie, bagel, and dowel. We present the performance averaged across all groups below. From Table 14 and Table 15, AnomalyCLIP-3D can incorporate more knowledge about abnormality and normality, improving point detection and multimodal detection performance. Specifically, from one category to three categories, AnomalyCLIP-3D exhibits improved performance, with P-AUROC increasing from $95.5\\%$ , $96.1\\%$ , to $96.3\\%$ , and AUPRO increasing from $84.4\\%$ , $86.3\\%$ , to $86.5\\%$ . Moreover, I-AUROC increases from $97.2\\%$ , $97.5\\%$ , to $97.8\\%$ , and AP increases from $90.2\\%$ , $91.8\\%$ , to $92.0\\%$ . This trend is also observed in global anomaly semantics. ", "page_idx": 18}, {"type": "table", "img_path": "02CIZ8qeDc/tmp/f61b57446cffdb9094b1ed01136622dbe6e77d001862bb1d2b4f0f49af9c1b85.jpg", "table_caption": ["Table 14: Ablation study on training set size for point detection "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "02CIZ8qeDc/tmp/69516597998e79d5e2b0fceec0584da092221155cd1c9b8e4cb3a52a6aa12565.jpg", "table_caption": ["Table 15: Ablation study on training set size for multimodal detection. "], "table_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "02CIZ8qeDc/tmp/d0018e86c2617a7ed1af04a08a2cfdbd062cbfeffe5d7948cc4d3a72d516acb8.jpg", "img_caption": ["Figure 10: Failure case in PointAD. "], "img_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "02CIZ8qeDc/tmp/e6f74fb2e7a54cefdf1d78d6619ad489803450e7acb31edfa95b086dd4e75638.jpg", "table_caption": ["Table 16: Effect on point density gap. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "G Failure Cases ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we present the failure cases of PointAD, which we attribute to direct multimodality fusion. Since our model uses hybrid loss to incorporate the 3D and 2D anomaly semantics, it performs ZS multimodality 3D anomaly detection in a plug-and-play manner. However, when one modality prediction deviates severely from the ground truth in rare instances, direct fusion may result in an unpromising multimodal score map. As shown in Figure 10, the hole in the cookie is visually similar to the chocolate on cookies, making it challenging to differentiate the hole anomaly via color information alone. Although PointAD can detect the hole based on its abnormal point relations, the RGB score map heavily influences the final multimodal score map. Conversely, the tire presents an inverse situation where RGB can effectively predict the anomalies, but the point score map fails to recognize it. The false detection could arise from unusual point density and distribution. To demonstrate the effect of point density, we randomly select normal regions of point clouds and subsequently increase or decrease the density of these regions through upsampling and downsampling. We provide a qualitative analysis in 11. The visualization shows that PointAD effectively resists noise at reasonable levels. However, when noise levels are extremely low or high, the corresponding regions become excessively sparse or dense. This causes normal regions to appear similar to hole anomalies or squeezed anomalies, leading PointAD to classify these noisy areas as anomalies. Non-parametric score alignment and filter methods could be a potential direction, which we leave for further work. ", "page_idx": 19}, {"type": "image", "img_path": "02CIZ8qeDc/tmp/28f35bd76ecef2069ea33119356eec54dfd9b17bea86899994864c3095c829bf.jpg", "img_caption": ["Figure 11: The impact of noise level. We randomly downsample and upsample part of normal regions to create different point densities. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Robustness to point density To investigate the impact of point density differences between the training and test datasets, we train PointAD using high-density point clouds (original dataset) and then test it on the low-density versions of the datasets, downsampled as described in Input resolution. Table 16 shows that PointAD can still detect anomalies even when retaining $50\\%$ of the points from the original point clouds. However, when more points are removed $30\\%$ and $20\\%$ sample ratio), PointAD experiences an obvious performance degradation. We attribute the misdetection to the overly sparse point clouds forming holes. Nevertheless, PointAD can still detect anomalies even with a significant gap in point density between the training and test domains (e.g., $100\\%$ vs. $20\\%$ ). This demonstrates PointAD\u2019s ability to generalize across different point densities. ", "page_idx": 19}, {"type": "table", "img_path": "02CIZ8qeDc/tmp/1585a3426538180457fef4954e7a02447db1c551fd960e96278780ebecfc8960.jpg", "table_caption": ["Table 17: Comparison of computation overhead with SOTA approaches on MVTec3D-AD. The unsupervised method is abbreviated as Un. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "H Complexity analysis ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In Table 17, we provide a comparison of computation overhead among unsupervised and zero-shot manners 4. The evaluation includes inference time per image, frames per second (FPS), and GPU memory consumption with a batch size of 1. For a fair comparison, we keep NVIDIA RTX 3090 24GB GPU free until we conduct experiments. Compared to PointCLIP V2, our model requires less time to infer an image, achieving higher FPS (2.52 vs. 0.66) with lower graphic memory usage (as discussed in Section 3.5). While CLIP $^+$ rendering has a slight advantage in computation overhead, our detection performance significantly outperforms it. Therefore, PointAD achieves a favorable trade-off between performance and computation overhead. ", "page_idx": 20}, {"type": "image", "img_path": "02CIZ8qeDc/tmp/07456c3bc0835ab34d79c3a22dbbff306c00c7c22a02c818f31acb57ecdb0df7.jpg", "img_caption": ["Figure 12: Visualization comparison between PointAD and PointCLIP V2. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "I Visualization Comparison ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To provide intuitive results, we compare the visualization of PointAD with PointCLIP V2. In Figure 12, our model achieves accurate ZS 3D detection through the point cloud. Moreover, given RGB counterparts, PointAD further improves its detection capacity in M3D detection. However, PointCLIP V2 exhibits noisy activations for normal regions. After incorporating RGB information, PointCLIP V2 appears to struggle to fuse these two modalities in a plug-and-play manner, unlike PointAD. ", "page_idx": 20}, {"type": "text", "text": "J Additional Visualization ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We respectively visualize the 2D renderings and corresponding 2D ground truths, which are rendered from 3D pint clouds and ground truths, as shown in Figure 13. We also supplement more zero-shot segmentation results of PointAD in Figure 14. ", "page_idx": 20}, {"type": "image", "img_path": "02CIZ8qeDc/tmp/a84f44b3045db4109686f98e5982041effb0af0f52419f4ab4a7ae8b66fbc3aa.jpg", "img_caption": ["Figure 13: Visualization about 2D renderings and ground truth from different views $(K=9)$ ). "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "K Detailed results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 We provide the class-level ZS 3D results on MVTec3D-AD in Table 18.   \n\u2022 We provide the class-level ZS M3D results on MVTec3D-AD in Table 19.   \n\u2022 We provide the class-level ZS 3D results on Eyecandies in Table 20.   \n\u2022 We provide the class-level ZS M3D results on Eyecandies in Table 21.   \n\u2022 We provide the class-level ZS 3D results on Real3D-AD in Table 22.   \n\u2022 We provide the class-level ZS cross-dataset 3D results on Eyecandies from MVTec3D-AD in Table 23.   \n\u2022 We provide the class-level ZS cross-dataset 3D results on Eyecandies from MVTec3D-AD in Table 24.   \n\u2022 We provide the class-level ZS cross-dataset 3D results on Real3D-AD from MVTec3D-AD in Table 25. ", "page_idx": 21}, {"type": "image", "img_path": "02CIZ8qeDc/tmp/58104049a33278f4128ffc320fcc9bb9c695adffeffcead7f4bfe03b8f5961c6.jpg", "img_caption": ["Figure 14: Visualization of point and multimodal score maps in PointAD, which is pre-trained on cookie object. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 18: Performance comparison on ZS 3D anomaly detection. The best and second-best results in ZS are highlighted in red and blue. G. and L. represent the global and local anomaly detection. ", "page_idx": 22}, {"type": "table", "img_path": "02CIZ8qeDc/tmp/234abac4f12599068981d98cbf8e88c84d46ff517316a0e11b74c8df63e8813d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "02CIZ8qeDc/tmp/a1a703c69cd9d5ef6aaef24b452754b82f4cc23ca5d83acaf6cc74c148b65d0c.jpg", "table_caption": ["Table 19: Performance comparison on ZS M3D anomaly detection. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "02CIZ8qeDc/tmp/91b47473608fca15d21af5b8940ce84c13309ce9d15f59f7cbdc6eae75c67a8e.jpg", "table_caption": ["Table 20: Performance comparison on ZS 3D anomaly detection on Eyecandies "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "02CIZ8qeDc/tmp/68a88689a8d9f45631815e839535ab6816d9e029f6ae4bad433fe656fc712378.jpg", "table_caption": ["Table 21: Performance comparison on ZS M3D anomaly detection on Eyecandies. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 22: Performance comparison on ZS 3D anomaly detection on Real3D-AD. ", "text_level": 1, "page_idx": 23}, {"type": "table", "img_path": "02CIZ8qeDc/tmp/f4228c3d792fdaac36dd397d63d98a81de71313149580c6f86c6ad79f8c97b3d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "02CIZ8qeDc/tmp/d04c0b9b56dfd9f1090cf9c53df296706978497f6167509387e5b807072344e0.jpg", "table_caption": ["Table 23: Perfromance comparison on ZS 3D cross-dataset anomaly detection transferring from MVTec3D-AD to Eyecandies. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Table 24: Perfromance comparison on ZS M3D cross-dataset anomaly detection transferring from MVTec3D-AD to Eyecandies. ", "text_level": 1, "page_idx": 24}, {"type": "table", "img_path": "02CIZ8qeDc/tmp/14a9acae08aabc3fba95bd8f24b9672650d8fa0e80487bd7fd195ec458bee853.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "02CIZ8qeDc/tmp/89c440d94e7d6004f170d6e9bbed3d86e54f7f677266e2ee04b3bf4f5cd9fe0a.jpg", "table_caption": ["Table 25: Perfromance comparison on ZS cross-dataset anomaly detection transferring from MVTec3D-AD to Real3D-AD. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The abstract and introduction precisely reflect the contribution and scope of this paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: we have created a separate \"Limitations\" section in our paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper does not include theoretical results Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We provide a detailed illustration of our proposed algorithm and baselines in the Appendix C, Appendix B, and Appendix F. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [No] ", "page_idx": 27}, {"type": "text", "text": "Justification: we will make our code and dataset available once the paper is accepted. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide full details in Appendix B and Appendix F Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We report the average results across three runs in Section 4.3. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We point out the specific compute resources in Section 4.2 Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our paper obeys the NeurIPS Code of Ethics. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We discuss the broader impacts of our paper. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 28}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 29}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: our paper poses no such risks ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: we respect the Licenses for existing assets that we use. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: we will release new assets proposed in our paper once the paper is accepted. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]