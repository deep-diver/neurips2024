[{"figure_path": "JhqyeppMiD/tables/tables_1_1.jpg", "caption": "Table 1: Comparison of attack impact based on three criteria: (C1) Pervasive Impact: impact on everyday, benign prompts, (C2) Stealthiness: undetectability by human inspection, and (C3) Misleading Texts: ability to deceive with free-form texts. Our attack is in the bottom right corner.", "description": "This table compares different types of attacks (test-time and poisoning) against three different types of models (image classifiers, LLMs, and VLMs) based on three criteria: pervasiveness of impact, stealthiness, and the ability to use misleading text.  It highlights that the proposed Shadowcast attack is unique in its combination of pervasiveness, stealth, and the use of deceptive text, setting it apart from previous attacks.", "section": "Summary of Contributions"}, {"figure_path": "JhqyeppMiD/tables/tables_5_1.jpg", "caption": "Table 2: Attack tasks and their associated concepts.", "description": "This table presents four different attack tasks used to evaluate the Shadowcast data poisoning method against Vision-Language Models (VLMs). Each task involves manipulating the VLM's response to an image representing an \"original concept\" to elicit a response aligning with a different \"destination concept\".  The tasks are categorized into two types: Label Attacks, where the destination concept is a simple class label, and Persuasion Attacks, where the destination concept is a more elaborate and potentially misleading narrative.  The table specifies the original and destination concepts associated with each task.", "section": "4 Experiments"}, {"figure_path": "JhqyeppMiD/tables/tables_6_1.jpg", "caption": "Table 3: Performance of clean and poisoned LLaVA-1.5 models on VizWiz and GQA benchmarks (the higher, the better). p denotes the proportion of poison samples.", "description": "This table presents the performance comparison between clean and poisoned LLaVA-1.5 models on two benchmark datasets: VizWiz and GQA.  The performance is measured by the scores obtained on each benchmark.  Different columns represent different proportions (p) of poison samples used during the training process. Higher scores indicate better performance. The table demonstrates the impact of poison samples on the overall model performance and the trade-off between model utility and vulnerability to data poisoning attacks.", "section": "4.2 Attack effectiveness on Label Attack"}, {"figure_path": "JhqyeppMiD/tables/tables_13_1.jpg", "caption": "Table 2: Attack tasks and their associated concepts.", "description": "This table presents four attack tasks used in the Shadowcast experiments. Each task involves manipulating a Vision-Language Model (VLM) to misinterpret images from an \"original concept\" as if they belong to a different, \"destination concept\".  The \"original concept\" and \"destination concept\" pairs are shown for each task. For example, the \"Trump-to-Biden\" task aims to make the VLM misidentify images of Donald Trump as Joe Biden. This table is crucial because it defines the specific scenarios used to evaluate the effectiveness and generalizability of the Shadowcast attack against VLMs.", "section": "4.1 Experimental setup"}, {"figure_path": "JhqyeppMiD/tables/tables_14_1.jpg", "caption": "Table 2: Attack tasks and their associated concepts.", "description": "This table presents four different attack tasks used in the paper's experiments.  Each task involves manipulating a Vision-Language Model (VLM) to misinterpret images from an original concept as belonging to a different, target concept.  The original and target concepts are described for each task.  Two tasks focus on misidentification (Label Attack), while the other two involve creating misleading narratives (Persuasion Attack).  These tasks demonstrate different ways an attacker can manipulate VLMs.", "section": "4 Experiments"}, {"figure_path": "JhqyeppMiD/tables/tables_15_1.jpg", "caption": "Table 6: Instructions provided to GPT-3.5-turbo for evaluating responses in the Persuasion Attack tasks.", "description": "This table presents the instructions given to the GPT-3.5-turbo language model for evaluating the responses generated by the poisoned models in the Persuasion Attack tasks.  These instructions are designed to assess whether the model's responses accurately reflect the intended destination concept (e.g., healthy food, beneficial physical activity) without explicitly mentioning that concept in every response.  The instructions ensure a consistent and unbiased evaluation focusing on the alignment of the response with the target concept, rather than on the factual accuracy or completeness of the response.", "section": "B.1 Additional experiment setup"}, {"figure_path": "JhqyeppMiD/tables/tables_15_2.jpg", "caption": "Table 3: Performance of clean and poisoned LLaVA-1.5 models on VizWiz and GQA benchmarks (the higher, the better). p denotes the proportion of poison samples.", "description": "This table presents the performance comparison between clean and poisoned LLaVA-1.5 models on two benchmark datasets: VizWiz and GQA.  The performance is measured by a score (higher is better).  The table shows the performance of the model across different percentages (p) of poison samples injected into the training data, ranging from 0.28% to 5.71%. This allows for an analysis of how the model's performance changes as the proportion of poisoned data increases.", "section": "4.2 Attack effectiveness on Label Attack"}, {"figure_path": "JhqyeppMiD/tables/tables_19_1.jpg", "caption": "Table 1: Comparison of attack impact based on three criteria: (C1) Pervasive Impact: impact on everyday, benign prompts, (C2) Stealthiness: undetectability by human inspection, and (C3) Misleading Texts: ability to deceive with free-form texts. Our attack is in the bottom right corner.", "description": "This table compares different types of attacks (test-time and poisoning attacks) against three different types of models (image classifiers, LLMs, and VLMs) based on three criteria: pervasive impact (how broadly the attack affects users), stealthiness (whether the attack is detectable by humans), and the ability to generate misleading texts. The table shows that Shadowcast (the authors' attack) is unique in achieving a high level of pervasive impact, stealthiness, and the creation of subtly misleading texts, a combination not found in other attack methods.", "section": "Summary of Contributions"}, {"figure_path": "JhqyeppMiD/tables/tables_20_1.jpg", "caption": "Table 3: Performance of clean and poisoned LLaVA-1.5 models on VizWiz and GQA benchmarks (the higher, the better). p denotes the proportion of poison samples.", "description": "This table presents the performance comparison between clean and poisoned LLaVA-1.5 models on two benchmark datasets: VizWiz and GQA.  The performance is measured by evaluating the models' accuracy across different poisoning rates (p), ranging from 0.28% to 5.71%.  The table allows readers to observe the impact of Shadowcast data poisoning attacks on the model's overall utility, while also assessing the performance trade-offs between poisoning rate and benchmark scores.", "section": "4.2 Attack effectiveness on Label Attack"}]