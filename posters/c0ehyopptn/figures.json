[{"figure_path": "C0EhyoPpTN/figures/figures_1_1.jpg", "caption": "Figure 1: Our goal is to obtain generative models from which we can sample realistic neural data while having a tractable underlying dynamical system. We achieve this by fitting stochastic low-rank RNNs with variational sequential Monte Carlo.", "description": "This figure illustrates the overall workflow of the proposed method.  The goal is to build generative models of neural data that are both realistic and have a simple, understandable underlying dynamical system. This is accomplished by fitting stochastic low-rank recurrent neural networks (RNNs) to high-dimensional, noisy neural data using variational sequential Monte Carlo methods. The figure shows how the method takes noisy neural data as input and fits a low-rank RNN model to it. Then, using this model, one can generate new samples of realistic neural data, access and analyze the low-dimensional latent dynamics, and optionally, use conditional generation by providing external context or stimuli.", "section": "1 Introduction"}, {"figure_path": "C0EhyoPpTN/figures/figures_4_1.jpg", "caption": "Figure 2: Proof sketch.", "description": "This figure is a sketch to illustrate the proof of Proposition 1, which states that the computational cost of finding all fixed points in a low-rank RNN with piecewise-linear activation functions is polynomial rather than exponential in the number of units.  The figure shows a two-dimensional phase space divided into four regions by two hyperplanes. The proof leverages Zaslavsky's Theorem on hyperplane arrangements to show that if the dynamics are constrained to a lower-dimensional subspace (as they are in low-rank systems), the number of regions that need to be considered is significantly reduced, leading to a polynomial computational complexity.", "section": "Finding fixed points in piecewise-linear low-rank RNNs"}, {"figure_path": "C0EhyoPpTN/figures/figures_5_1.jpg", "caption": "Figure 3: RNNs recover dynamics in teacher-student setups. a) Example ground truth latent trajectory and phase plane of low-rank RNN trained to oscillate (top left) and noisy observations of neuron activity (top right; 6/20 shown). A second low-rank RNN trained on the activity of the first recovers ground truth dynamics. b) Same set-up, but with Poisson observations. c) The teacher network was trained on a task where it has to provide an output corresponding to 8 different angles depending on an input cue. The student network, when given the same input during fitting, recovers the approximate ring attractor. d) Mean (\u00b11SD) autocorrelation of the latents of the models from panel a, show the oscillation frequency is captured, as well as the decorrelation due to recurrent noise. The scale of the observed rates also agrees between student and teacher. e) Mean rates and ISI between student and teacher units of panel b match. f) Example rate distribution of one unit of the teacher and student RNN (of panel c), after onset of the 8 different stimuli.", "description": "This figure demonstrates the ability of the proposed method to recover the ground truth dynamics and stochasticity from noisy high-dimensional data. It shows the results of three teacher-student experiments, where a 'teacher' RNN generates data, and a 'student' RNN is trained on this data. Panel (a) and (b) demonstrates the recovery of continuous and spiking data, respectively, while panel (c) demonstrates recovery when an input cue is included. The remaining panels (d)-(f) show additional quantitative analyses, demonstrating that the student RNN captures the underlying dynamics, stochasticity, and response properties of the teacher RNN.", "section": "Empirical Results"}, {"figure_path": "C0EhyoPpTN/figures/figures_5_2.jpg", "caption": "Figure 4: Example ground truth EEG [42, 43] and (unconditionally) generated traces by our model. Shown are 5/64 EEG channels.", "description": "This figure demonstrates the ability of the proposed stochastic low-rank RNN model to generate realistic EEG data. The left panel shows example traces from real EEG data recorded from 5 out of 64 channels.  The right panel shows traces generated by the model. The close visual similarity between the real and generated data highlights the model's capacity to capture the complex temporal dynamics of EEG signals.", "section": "3.2 Stochasticity allows recovering low-dimensional latents underlying EEG data"}, {"figure_path": "C0EhyoPpTN/figures/figures_6_1.jpg", "caption": "Figure 3: RNNs recover dynamics in teacher-student setups. a) Example ground truth latent trajectory and phase plane of low-rank RNN trained to oscillate (top left) and noisy observations of neuron activity (top right; 6/20 shown). A second low-rank RNN trained on the activity of the first recovers ground truth dynamics. b) Same set-up, but with Poisson observations. c) The teacher network was trained on a task where it has to provide an output corresponding to 8 different angles depending on an input cue. The student network, when given the same input during fitting, recovers the approximate ring attractor. d) Mean (\u00b11SD) autocorrelation of the latents of the models from panel a, show the oscillation frequency is captured, as well as the decorrelation due to recurrent noise. The scale of the observed rates also agrees between student and teacher. e) Mean rates and ISI between student and teacher units of panel b match. f) Example rate distribution of one unit of the teacher and student RNN (of panel c), after onset of the 8 different stimuli.", "description": "This figure demonstrates the ability of the proposed method to recover ground truth dynamics and stochasticity from noisy data.  Panel (a) shows a teacher-student setup with Gaussian noise, (b) with Poisson noise, and (c) with a task involving an input cue.  Panels (d)-(f) provide additional analyses of the results, including autocorrelation of latent variables, mean firing rates and ISIs, and example rate distributions.", "section": "3.1 RNNs recover ground truth dynamics in student-teacher setups"}, {"figure_path": "C0EhyoPpTN/figures/figures_7_1.jpg", "caption": "Figure 6: Posterior latents of our model (fit solely spikes) can be used to predict rat position.", "description": "This figure shows that the posterior latents obtained from a rank-4 RNN trained solely on spiking activity from rat hippocampus can be used to predict the rat's position on a linear track. The model accurately reconstructs the distribution of spikes and exhibits oscillations in its latents. The posterior latents also show a strong relationship with the rat's position, indicating that the model captures the underlying dynamics of the spiking data and successfully integrates this information with the rat's position. ", "section": "3.3 Interpretable latent dynamics underlying spikes recorded from rat hippocampus"}, {"figure_path": "C0EhyoPpTN/figures/figures_7_2.jpg", "caption": "Figure 7: Inferred and generated dynamics from the model fit to macaque spiking activity during a reaching task. a) Latent states inferred from the macaque spiking data prior to movement initiation ('pre-movement') and during movement execution ('movement'), colored by the intended reach target. b) Reach trajectories decoded from model-inferred neural activity. c) Dissimilarity matrices computed across the seven conditions (i.e., the seven colors in a, b) for per-neuron mean firing rate and ISI. We generate neural activity from the model by providing the same conditioning stimuli as in the real data. Then, for each statistic, we compute and show the correlation distance between conditions in the real data (left) and model-generated data (right). d, e) Same as a, b, but with latent activity and behavioral predictions generated from the model with conditioning inputs including directions not seen in the real data (e.g., lime green). For clarity, we show only a subset of conditions in the decoded reaches.", "description": "This figure shows the results of applying the proposed method to macaque spiking data during a reaching task.  Panel (a) displays inferred latent states before and during movement, colored by reach target. Panel (b) shows decoded reach trajectories from the model. Panel (c) compares dissimilarity matrices of firing rates and inter-spike intervals (ISIs) between conditions in real and generated data. Panels (d) and (e) repeat the analysis in (a) and (b) but using generated data from the model conditioned on unseen inputs, demonstrating generalization ability.", "section": "3.4 Extracting stimulus-conditioned dynamics in monkey reaching task"}, {"figure_path": "C0EhyoPpTN/figures/figures_8_1.jpg", "caption": "Figure 8: Comparison of our analytic method (star) and the approximate method proposed in Eisenmann et al. [25] (blue) for finding the fixed points of the teacher RNN in Fig. 3c. We can also use Proposition 1 to constrain the search space of the approximate method (orange). Error bars denote the minimum and maximum amount of fixed points found over 20 independent runs of the algorithm.", "description": "This figure compares the performance of three methods for finding fixed points in a piecewise-linear RNN: an analytic method, an approximate method, and a combined method that uses the analytic method to constrain the search space of the approximate method. The x-axis represents the number of matrix inverses computed, and the y-axis represents the number of fixed points found. The analytic method consistently finds all 17 fixed points, while the approximate method finds fewer fixed points and its performance varies across different runs. The combined method improves upon the approximate method by using the analytic method to reduce the search space, but it still finds fewer fixed points than the analytic method.", "section": "3.5 Searching for fixed points"}, {"figure_path": "C0EhyoPpTN/figures/figures_15_1.jpg", "caption": "Figure 2: Proof sketch.", "description": "This figure is a sketch to help understand the proof of Proposition 1.  It illustrates how the computational cost of finding all fixed points in piecewise-linear low-rank RNNs can be reduced from exponential to polynomial in the number of units. In a high-dimensional network with N units, the phase space is divided into 2<sup>N</sup> regions. However, if the dynamics are constrained to a lower-dimensional subspace of rank R, spanned by the columns of the matrix M, the number of regions considered is greatly reduced.  Each hyperplane (pink points) determined by a unit partitions the subspace. The number of such regions is polynomial in N for a fixed R, as shown by the application of Zaslavsky's theorem. This implies that finding all fixed points can be done efficiently, contrary to the naive approach that has an exponential complexity.", "section": "Finding fixed points in piecewise-linear low-rank RNNs"}, {"figure_path": "C0EhyoPpTN/figures/figures_16_1.jpg", "caption": "Figure 2: Proof sketch.", "description": "This figure is a sketch to help understand the proof of Proposition 1.  It illustrates how, for low-rank networks with piecewise-linear activation functions, the cost of finding all fixed points is polynomial rather than exponential in the number of units. The figure shows how the activation functions partition the phase space into linear regions. The key idea is that, because the dynamics are low-rank, only a subset of these linear regions are accessible, drastically reducing the number of calculations needed to find all fixed points.", "section": "Finding fixed points in piecewise-linear low-rank RNNs"}, {"figure_path": "C0EhyoPpTN/figures/figures_19_1.jpg", "caption": "Figure 3: RNNs recover dynamics in teacher-student setups. a) Example ground truth latent trajectory and phase plane of low-rank RNN trained to oscillate (top left) and noisy observations of neuron activity (top right; 6/20 shown). A second low-rank RNN trained on the activity of the first recovers ground truth dynamics. b) Same set-up, but with Poisson observations. c) The teacher network was trained on a task where it has to provide an output corresponding to 8 different angles depending on an input cue. The student network, when given the same input during fitting, recovers the approximate ring attractor. d) Mean (\u00b11SD) autocorrelation of the latents of the models from panel a, show the oscillation frequency is captured, as well as the decorrelation due to recurrent noise. The scale of the observed rates also agrees between student and teacher. e) Mean rates and ISI between student and teacher units of panel b match. f) Example rate distribution of one unit of the teacher and student RNN (of panel c), after onset of the 8 different stimuli.", "description": "This figure demonstrates the ability of the proposed method to recover ground truth dynamics from noisy data.  It presents results from several teacher-student experiments where a low-rank RNN (the student) was trained to mimic the behavior of a teacher RNN with known dynamics and added noise. Panel (a) shows results for continuous data and (b) for spiking data; (c) demonstrates the method's ability to learn dynamics in the presence of time-varying input. Panels (d), (e), and (f) provide additional statistical analyses to support the claim that the student RNN effectively replicates the teacher RNN's dynamics, stochasticity, and response to stimuli.", "section": "3 Empirical Results"}, {"figure_path": "C0EhyoPpTN/figures/figures_21_1.jpg", "caption": "Figure 3: RNNs recover dynamics in teacher-student setups. a) Example ground truth latent trajectory and phase plane of low-rank RNN trained to oscillate (top left) and noisy observations of neuron activity (top right; 6/20 shown). A second low-rank RNN trained on the activity of the first recovers ground truth dynamics. b) Same set-up, but with Poisson observations. c) The teacher network was trained on a task where it has to provide an output corresponding to 8 different angles depending on an input cue. The student network, when given the same input during fitting, recovers the approximate ring attractor. d) Mean (\u00b11SD) autocorrelation of the latents of the models from panel a, show the oscillation frequency is captured, as well as the decorrelation due to recurrent noise. The scale of the observed rates also agrees between student and teacher. e) Mean rates and ISI between student and teacher units of panel b match. f) Example rate distribution of one unit of the teacher and student RNN (of panel c), after onset of the 8 different stimuli.", "description": "This figure demonstrates the ability of low-rank RNNs to recover ground truth dynamics from noisy data in teacher-student experiments.  Panels (a) and (b) show that the student RNN successfully recovers the latent dynamics and noise level of the teacher RNN, for both continuous and Poisson observations. Panel (c) showcases the ability of the model to recover a ring attractor representing a task involving the processing of angular cues. Panels (d)-(f) provide further validation by demonstrating matches in autocorrelations of latents, mean firing rates, and inter-spike interval distributions, respectively.", "section": "3.1 RNNs recover ground truth dynamics in student-teacher setups"}, {"figure_path": "C0EhyoPpTN/figures/figures_21_2.jpg", "caption": "Figure 5: RNNs reproduce the stationary distribution of spiking data. a) We fit a rank-3 RNN to spike data recorded from rat hippocampus [47, 48] (left), and generate new samples from the RNN (right). b) Single neuron statistics. Mean rates and means of interspike interval (ISI) distributions of a long trajectory of data generated by the RNN (gen) match those of a held-out set of data (test). As a reference we additionally computed the same statistics between the train and test set. c) Population level statistics. We plot the pairwise correlations between all neurons for generated data against the pairwise correlations in the test data. d) The corresponding latents generated by running the RNN look visually similar to the local field potential (LFP). e) The peak in the power spectrum matches between latents and LFP. f) The posterior latents show coherence with the LFP. As a reference, we compute the coherence between the LFP and the latents generated by the RNN.", "description": "This figure demonstrates the ability of the proposed stochastic low-rank RNN model to accurately capture the statistical properties of real-world spiking neural data.  Panel (a) shows the model's ability to generate realistic spike trains resembling recordings from rat hippocampus. Panels (b-c) quantitatively demonstrate the accuracy of the model by comparing single-neuron and population-level statistics of generated and held-out test data. The remaining panels (d-f) focus on the model's ability to recover low-dimensional latent dynamics that are strongly related to the local field potential (LFP), demonstrating its capacity to uncover meaningful underlying brain dynamics.", "section": "3.3 Interpretable latent dynamics underlying spikes recorded from rat hippocampus"}, {"figure_path": "C0EhyoPpTN/figures/figures_21_3.jpg", "caption": "Figure 3: RNNs recover dynamics in teacher-student setups. a) Example ground truth latent trajectory and phase plane of low-rank RNN trained to oscillate (top left) and noisy observations of neuron activity (top right; 6/20 shown). A second low-rank RNN trained on the activity of the first recovers ground truth dynamics. b) Same set-up, but with Poisson observations. c) The teacher network was trained on a task where it has to provide an output corresponding to 8 different angles depending on an input cue. The student network, when given the same input during fitting, recovers the approximate ring attractor. d) Mean (\u00b11SD) autocorrelation of the latents of the models from panel a, show the oscillation frequency is captured, as well as the decorrelation due to recurrent noise. The scale of the observed rates also agrees between student and teacher. e) Mean rates and ISI between student and teacher units of panel b match. f) Example rate distribution of one unit of the teacher and student RNN (of panel c), after onset of the 8 different stimuli.", "description": "This figure shows the results of teacher-student experiments where a low-rank RNN (teacher) is trained to generate oscillatory activity or perform a task involving angle recognition based on input stimuli, and another low-rank RNN (student) is trained on the generated data to recover the underlying dynamics.  Panels a and b demonstrate the recovery of continuous and Poisson spiking data respectively.  Panel c demonstrates recovery of dynamics for a task involving angle recognition.  Panels d, e, and f provide supplementary statistics such as autocorrelation of latents, mean rates and inter-spike intervals, and example rate distribution to support the claim of successful recovery of dynamics and stochasticity.", "section": "3.1 RNNs recover ground truth dynamics in student-teacher setups"}, {"figure_path": "C0EhyoPpTN/figures/figures_22_1.jpg", "caption": "Figure 3: RNNs recover dynamics in teacher-student setups. a) Example ground truth latent trajectory and phase plane of low-rank RNN trained to oscillate (top left) and noisy observations of neuron activity (top right; 6/20 shown). A second low-rank RNN trained on the activity of the first recovers ground truth dynamics. b) Same set-up, but with Poisson observations. c) The teacher network was trained on a task where it has to provide an output corresponding to 8 different angles depending on an input cue. The student network, when given the same input during fitting, recovers the approximate ring attractor. d) Mean (\u00b11SD) autocorrelation of the latents of the models from panel a, show the oscillation frequency is captured, as well as the decorrelation due to recurrent noise. The scale of the observed rates also agrees between student and teacher. e) Mean rates and ISI between student and teacher units of panel b match. f) Example rate distribution of one unit of the teacher and student RNN (of panel c), after onset of the 8 different stimuli.", "description": "This figure demonstrates the ability of the proposed method to recover the ground truth dynamics and stochasticity of a teacher RNN from noisy observations.  It shows results for three different teacher-student setups: one with continuous Gaussian observations, one with Poisson observations, and one with a task involving angle-dependent stimuli. The figure demonstrates that the student RNNs accurately recover the latent dynamics of the teacher RNNs, including the level of stochasticity, oscillation frequency, and response to external stimuli. This is further validated by comparing autocorrelations, mean firing rates, inter-spike intervals, and example rate distributions between the teacher and student models across different conditions.", "section": "3.1 RNNs recover ground truth dynamics in student-teacher setups"}, {"figure_path": "C0EhyoPpTN/figures/figures_22_2.jpg", "caption": "Figure 3: RNNs recover dynamics in teacher-student setups. a) Example ground truth latent trajectory and phase plane of low-rank RNN trained to oscillate (top left) and noisy observations of neuron activity (top right; 6/20 shown). A second low-rank RNN trained on the activity of the first recovers ground truth dynamics. b) Same set-up, but with Poisson observations. c) The teacher network was trained on a task where it has to provide an output corresponding to 8 different angles depending on an input cue. The student network, when given the same input during fitting, recovers the approximate ring attractor. d) Mean (\u00b11SD) autocorrelation of the latents of the models from panel a, show the oscillation frequency is captured, as well as the decorrelation due to recurrent noise. The scale of the observed rates also agrees between student and teacher. e) Mean rates and ISI between student and teacher units of panel b match. f) Example rate distribution of one unit of the teacher and student RNN (of panel c), after onset of the 8 different stimuli.", "description": "This figure demonstrates the ability of the proposed method to recover the ground truth dynamics from noisy observations using a teacher-student setup.  Panels a and b show the results for continuous and Poisson observations respectively, while panel c shows the model's ability to handle time-varying stimuli.  Panels d-f provide additional quantitative analysis supporting the model's accuracy and ability to capture various aspects of the data.", "section": "3 Empirical Results"}, {"figure_path": "C0EhyoPpTN/figures/figures_22_3.jpg", "caption": "Figure 5: RNNs reproduce the stationary distribution of spiking data. a) We fit a rank-3 RNN to spike data recorded from rat hippocampus [47, 48] (left), and generate new samples from the RNN (right). b) Single neuron statistics. Mean rates and means of interspike interval (ISI) distributions of a long trajectory of data generated by the RNN (gen) match those of a held-out set of data (test). As a reference we additionally computed the same statistics between the train and test set. c) Population level statistics. We plot the pairwise correlations between all neurons for generated data against the pairwise correlations in the test data. d) The corresponding latents generated by running the RNN look visually similar to the local field potential (LFP). e) The peak in the power spectrum matches between latents and LFP. f) The posterior latents show coherence with the LFP. As a reference, we compute the coherence between the LFP and the latents generated by the RNN.", "description": "This figure demonstrates the ability of a rank-3 RNN to accurately model the stationary distribution of spiking data from rat hippocampus.  Panel (a) shows the original spiking data and the generated data from the trained RNN. Panels (b) and (c) provide single-neuron and population-level comparisons of firing rates and interspike intervals (ISIs) between the generated and held-out test data. Panels (d-f) show that the RNN's inferred latent variables exhibit temporal dynamics similar to the local field potential (LFP), including similar power spectra and coherence with the LFP.", "section": "3.3 Interpretable latent dynamics underlying spikes recorded from rat hippocampus"}, {"figure_path": "C0EhyoPpTN/figures/figures_23_1.jpg", "caption": "Figure 3: RNNs recover dynamics in teacher-student setups. a) Example ground truth latent trajectory and phase plane of low-rank RNN trained to oscillate (top left) and noisy observations of neuron activity (top right; 6/20 shown). A second low-rank RNN trained on the activity of the first recovers ground truth dynamics. b) Same set-up, but with Poisson observations. c) The teacher network was trained on a task where it has to provide an output corresponding to 8 different angles depending on an input cue. The student network, when given the same input during fitting, recovers the approximate ring attractor. d) Mean (\u00b11SD) autocorrelation of the latents of the models from panel a, show the oscillation frequency is captured, as well as the decorrelation due to recurrent noise. The scale of the observed rates also agrees between student and teacher. e) Mean rates and ISI between student and teacher units of panel b match. f) Example rate distribution of one unit of the teacher and student RNN (of panel c), after onset of the 8 different stimuli.", "description": "This figure demonstrates the ability of the proposed method to recover ground truth dynamics from noisy data.  It shows results from several teacher-student experiments. A teacher RNN generates data with known latent dynamics and noise characteristics. A student RNN then learns these dynamics and noise from the teacher's data. Panel (a) shows results with Gaussian observation noise, panel (b) with Poisson observation noise, and panel (c) with a task involving time-varying inputs. Panels (d)-(f) present analyses of the recovered dynamics and noise, showing good agreement between the teacher and student models.", "section": "3 Empirical Results"}, {"figure_path": "C0EhyoPpTN/figures/figures_23_2.jpg", "caption": "Figure 3: RNNs recover dynamics in teacher-student setups. a) Example ground truth latent trajectory and phase plane of low-rank RNN trained to oscillate (top left) and noisy observations of neuron activity (top right; 6/20 shown). A second low-rank RNN trained on the activity of the first recovers ground truth dynamics. b) Same set-up, but with Poisson observations. c) The teacher network was trained on a task where it has to provide an output corresponding to 8 different angles depending on an input cue. The student network, when given the same input during fitting, recovers the approximate ring attractor. d) Mean (\u00b11SD) autocorrelation of the latents of the models from panel a, show the oscillation frequency is captured, as well as the decorrelation due to recurrent noise. The scale of the observed rates also agrees between student and teacher. e) Mean rates and ISI between student and teacher units of panel b match. f) Example rate distribution of one unit of the teacher and student RNN (of panel c), after onset of the 8 different stimuli.", "description": "This figure shows the results of teacher-student experiments where a low-rank RNN ('teacher') is trained to generate data, which is then used to train another low-rank RNN ('student'). The student RNN successfully recovers the dynamics of the teacher RNN, including the level of stochasticity and the response to external stimuli. This demonstrates the ability of the proposed method to accurately infer the underlying dynamical system from noisy neural data.", "section": "3 Empirical Results"}, {"figure_path": "C0EhyoPpTN/figures/figures_24_1.jpg", "caption": "Figure 3: RNNs recover dynamics in teacher-student setups. a) Example ground truth latent trajectory and phase plane of low-rank RNN trained to oscillate (top left) and noisy observations of neuron activity (top right; 6/20 shown). A second low-rank RNN trained on the activity of the first recovers ground truth dynamics. b) Same set-up, but with Poisson observations. c) The teacher network was trained on a task where it has to provide an output corresponding to 8 different angles depending on an input cue. The student network, when given the same input during fitting, recovers the approximate ring attractor. d) Mean (\u00b11SD) autocorrelation of the latents of the models from panel a, show the oscillation frequency is captured, as well as the decorrelation due to recurrent noise. The scale of the observed rates also agrees between student and teacher. e) Mean rates and ISI between student and teacher units of panel b match. f) Example rate distribution of one unit of the teacher and student RNN (of panel c), after onset of the 8 different stimuli.", "description": "This figure shows the results of teacher-student experiments to validate the proposed method.  A teacher RNN (with low-rank structure) generates data, and a student RNN is trained on this data to learn the underlying dynamics. Panel (a) demonstrates the recovery of continuous data with Gaussian noise, (b) demonstrates recovery with Poisson observations (spiking data), and (c) shows the recovery with input stimuli, forming a ring attractor.  Panels (d), (e), and (f) provide additional analyses illustrating the model's ability to capture aspects of the data, including autocorrelation, mean firing rates and inter-spike intervals (ISI), and rate distributions.", "section": "3.1 RNNs recover ground truth dynamics in student-teacher setups"}, {"figure_path": "C0EhyoPpTN/figures/figures_26_1.jpg", "caption": "Figure 8: Comparison of our analytic method (star) and the approximate method proposed in Eisenmann et al. [25] (blue) for finding the fixed points of the teacher RNN in Fig. 3c. We can also use Proposition 1 to constrain the search space of the approximate method (orange). Error bars denote the minimum and maximum amount of fixed points found over 20 independent runs of the algorithm.", "description": "This figure compares the performance of three methods for finding fixed points in a piecewise-linear recurrent neural network: an analytic method (purple star), an approximate method (blue dots), and a combined method (orange dots). The combined method uses Proposition 1 to constrain the search space of the approximate method.  The x-axis shows the number of matrix inversions computed, and the y-axis shows the number of fixed points found. The analytic method consistently finds all 17 fixed points, while the approximate method's performance varies significantly, often finding fewer fixed points and showing more variability across runs. The combined method improves upon the approximate method but still doesn't consistently find all fixed points.", "section": "3.5 Searching for fixed points"}]