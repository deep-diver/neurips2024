[{"figure_path": "XKvYcPPH5G/tables/tables_5_1.jpg", "caption": "Table 3: SPO Hyperparameters for Continuous and Discrete Environments", "description": "This table lists the hyperparameters used for the SPO algorithm in both continuous and discrete environments.  It details settings for various aspects of the algorithm, including learning rates, buffer sizes, batch sizes, rollout lengths, and exploration parameters, showing how these parameters were tuned differently based on the environment type.", "section": "D.3 Hyperparameters"}, {"figure_path": "XKvYcPPH5G/tables/tables_16_1.jpg", "caption": "Table 1: Summary of Boxoban dataset levels", "description": "This table presents the dataset sizes for different difficulty levels in the Boxoban dataset used in the paper's experiments.  It shows the number of levels available for training, validation, and testing, as well as separate counts for 'Medium' and 'Hard' difficulty levels. This breakdown is important because the paper evaluates performance on specific subsets of the dataset (e.g., the 'Hard' dataset) to assess the algorithms' capabilities in more challenging scenarios.", "section": "A Environments"}, {"figure_path": "XKvYcPPH5G/tables/tables_17_1.jpg", "caption": "Table 2: Observation and action space for Brax environments", "description": "This table shows the observation and action space sizes for three different Brax environments: Halfcheetah, Ant, and Humanoid.  The observation size refers to the dimensionality of the state representation provided to the reinforcement learning agent, while the action size corresponds to the number of dimensions in the action space that the agent can choose from.", "section": "A.3 Brax"}, {"figure_path": "XKvYcPPH5G/tables/tables_23_1.jpg", "caption": "Table 3: SPO Hyperparameters for Continuous and Discrete Environments", "description": "This table lists the hyperparameters used in the SPO algorithm for both continuous and discrete environments.  It breaks down the settings for various aspects of the algorithm, including learning rates, buffer sizes, batch sizes, rollout lengths, and exploration parameters.  The values specified are those found to work well in experiments. The table helps to clarify the specific configurations used during the training and evaluation processes reported in the paper.", "section": "D.3 Hyperparameters"}, {"figure_path": "XKvYcPPH5G/tables/tables_24_1.jpg", "caption": "Table 3: SPO Hyperparameters for Continuous and Discrete Environments", "description": "This table lists the hyperparameters used for the SPO algorithm in both continuous and discrete environments.  It details settings for learning rates (actor, critic, and dual), discount factor, GAE lambda, replay buffer size, batch size, batch sequence length, maximum gradient norm, number of epochs, number of environments, rollout length, target smoothing, number of particles, search horizon, resample period, initial values for temperature (\u03b7), exploration (\u03b1), and KL divergence constraints (\u03b5),  and other parameters such as Dirichlet Alpha and root exploration weight.", "section": "D.3 Hyperparameters"}, {"figure_path": "XKvYcPPH5G/tables/tables_25_1.jpg", "caption": "Table 4: Hyperparameters for PPO and Sampled AlphaZero", "description": "This table lists the hyperparameters used for training both PPO and Sampled AlphaZero, highlighting the differences in their configurations for continuous control tasks.  It covers various aspects of the training process, including learning rates, rollout lengths, batch sizes, discount factors, and exploration strategies.", "section": "E.1 Hyperparameters"}, {"figure_path": "XKvYcPPH5G/tables/tables_26_1.jpg", "caption": "Table 5: Hyperparameters for MPO and VMPO", "description": "This table lists the hyperparameters used for training the MPO and VMPO algorithms in the paper.  It shows various parameters, including rollout length, number of epochs, buffer size, batch size, sample sequence length, learning rates (actor and critic), target smoothing, discount factor, maximum gradient norm, and whether learning rates decay.  Additionally, parameters specific to MPO and VMPO such as the initial values of the temperature and exploration parameters are shown.", "section": "E.1.2 Discrete Control"}, {"figure_path": "XKvYcPPH5G/tables/tables_26_2.jpg", "caption": "Table 9: Hyperparameters for SMC-Ent", "description": "This table lists the hyperparameters used for the SMC-Ent baseline algorithm in the paper's experiments.  It includes parameters related to training, the SMC algorithm itself, and the Q-learning used within the algorithm.  The hyperparameters are specific to the SMC-Ent algorithm and were used for comparison with the proposed SPO method in the experiments.", "section": "E Baselines"}, {"figure_path": "XKvYcPPH5G/tables/tables_27_1.jpg", "caption": "Table 7: Hyperparameters for AlphaZero and PPO", "description": "This table lists the hyperparameters used for the AlphaZero and PPO algorithms in the paper's experiments.  It includes settings for learning rates, rollout lengths, the number of epochs, buffer and batch sizes, sample sequence length, discount factor, GAE lambda, max grad norm, number of simulations (AlphaZero only), number of minibatches (PPO only), clip epsilon (PPO only), entropy coefficient (PPO only), and whether or not advantages are standardized (PPO only). These hyperparameters were crucial in configuring the algorithms for the experiments, especially in controlling the balance between exploration and exploitation.", "section": "E.1.2 Discrete Control"}, {"figure_path": "XKvYcPPH5G/tables/tables_27_2.jpg", "caption": "Table 5: Hyperparameters for MPO and VMPO", "description": "This table lists the hyperparameters used for training the MPO and VMPO baseline algorithms in the paper.  It provides a detailed comparison of the settings used for both algorithms, highlighting differences in parameters such as learning rates, rollout length, batch size, and other key optimization parameters.", "section": "E Baselines"}, {"figure_path": "XKvYcPPH5G/tables/tables_28_1.jpg", "caption": "Table 3: SPO Hyperparameters for Continuous and Discrete Environments", "description": "This table lists the hyperparameters used for the SPO algorithm in both continuous and discrete environments.  It includes settings for learning rates, discount factors, buffer sizes, batch sizes, rollout lengths, and other parameters crucial for model training and performance.", "section": "D.3 Hyperparameters"}, {"figure_path": "XKvYcPPH5G/tables/tables_29_1.jpg", "caption": "Table 10: Summary of EM based Algorithms", "description": "This table compares several Expectation-Maximization (EM) based reinforcement learning algorithms.  It highlights key differences in their E-step (expectation step) and M-step (maximization step) optimization objectives (G), whether trust regions are used in these steps, the method used for estimating the G objective, and the depth and breadth of the search process used by each algorithm.  The table shows that different algorithms take different approaches to planning, including using analytic solutions, temporal difference (TD) learning, Monte Carlo Tree Search (MCTS), or Sequential Monte Carlo (SMC).", "section": "F Expectation Maximisation"}, {"figure_path": "XKvYcPPH5G/tables/tables_32_1.jpg", "caption": "Table 4: Hyperparameters for PPO and Sampled AlphaZero", "description": "This table lists the hyperparameters used for training both the Proximal Policy Optimization (PPO) and Sampled AlphaZero algorithms.  It provides a detailed comparison of the settings used for each algorithm across various parameters such as learning rates, rollout length, batch size, discount factor, and more. This allows for a clear understanding of the differences in training configurations used for both methods.", "section": "E.1 Hyperparameters"}, {"figure_path": "XKvYcPPH5G/tables/tables_33_1.jpg", "caption": "Table 3: SPO Hyperparameters for Continuous and Discrete Environments", "description": "This table lists the hyperparameters used in the SPO algorithm for both continuous and discrete environments.  It specifies values for parameters related to the actor and critic networks (learning rates, discount factor), the value function update (GAE lambda), replay buffer and batch sizes, gradient clipping, training epochs, and the number of environments used during training.  It also shows hyperparameters specific to the SPO method, including the number of particles, search horizon, resampling period, initial values for the temperature parameter (\u03b7), alpha (\u03b1), and exploration weight. Finally, hyperparameters for discrete environments (Dirichlet alpha and root exploration weights) are also detailed.", "section": "D.3 Hyperparameters"}]