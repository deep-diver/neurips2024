{"references": [{"fullname_first_author": "Liang, V. W.", "paper_title": "Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning", "publication_date": "2022-12-31", "reason": "This paper is highly relevant due to its focus on understanding the modality gap in multimodal learning, a core challenge addressed by the current paper's proposed methodology."}, {"fullname_first_author": "Xia, Y.", "paper_title": "Achieving cross modal generalization with multimodal unified representation", "publication_date": "2023-12-31", "reason": "This work directly addresses the limitations of existing multimodal fusion methods, particularly regarding feature extraction, data integrity, and downstream task adaptability, making it highly relevant to the current paper's goals."}, {"fullname_first_author": "Nagrani, A.", "paper_title": "Attention bottlenecks for multimodal fusion", "publication_date": "2021-12-31", "reason": "This study tackles the limitations of attention mechanisms in multimodal fusion by proposing a novel approach that enhances understanding and addresses the challenge of fusing features from different modalities effectively."}, {"fullname_first_author": "Jaegle, A.", "paper_title": "Perceiver: General perception with iterative attention", "publication_date": "2021-12-31", "reason": "This paper introduces a generalized multimodal fusion method that improves performance across various downstream tasks, making it a significant contribution to the field and a key reference for the current work."}, {"fullname_first_author": "Liu, L.", "paper_title": "Activity image-to-video retrieval by disentangling appearance and motion", "publication_date": "2021-12-31", "reason": "This work focuses on a specific downstream task (image-to-video retrieval) but employs innovative fusion methods relevant to the current paper's generalized approach."}]}