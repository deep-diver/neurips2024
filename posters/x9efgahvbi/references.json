{"references": [{"fullname_first_author": "T. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-00-00", "reason": "This paper is foundational to the field of in-context learning, introducing the surprising ability of large language models to perform new tasks with only a few examples."}, {"fullname_first_author": "M. Abbas", "paper_title": "Enhancing in-context learning via linear probe calibration", "publication_date": "2024-00-00", "reason": "This paper provides a method to improve the reliability of in-context learning predictions by calibrating linear probes, addressing a key limitation of the technique."}, {"fullname_first_author": "Y. Chen", "paper_title": "Parallel structures in pre-training data yield in-context learning", "publication_date": "2024-00-00", "reason": "This paper highlights the importance of parallel structures in pre-training data for enabling in-context learning, offering a crucial insight into the underlying mechanisms."}, {"fullname_first_author": "S. C. Chan", "paper_title": "Data distributional properties drive emergent in-context learning in transformers", "publication_date": "2022-00-00", "reason": "This paper investigates how properties of the training data distribution contribute to the emergence of in-context learning, providing a more nuanced understanding of the phenomenon."}, {"fullname_first_author": "K. Ahuja", "paper_title": "A closer look at in-context learning under distribution shifts", "publication_date": "2023-00-00", "reason": "This paper analyzes how in-context learning performance changes under distribution shifts, which is important for understanding the robustness and limitations of this approach."}]}