[{"type": "text", "text": "Overfitting Behaviour of Gaussian Kernel Ridgeless Regression: Varying Bandwidth or Dimensionality ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Marko Medvedev Gal Vardi Nathan Srebro The University of Chicago Weizmann Institute of Science TTI-Chicago medvedev@uchicago.edu gal.vardi@weizmann.ac.il nati@ttic.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider the overfitting behavior of minimum norm interpolating solutions of Gaussian kernel ridge regression (i.e. kernel ridgeless regression), when the bandwidth or input dimension varies with the sample size. For fixed dimensions, we show that even with varying or tuned bandwidth, the ridgeless solution is never consistent and, at least with large enough noise, always worse than the null predictor. For increasing dimension, we give a generic characterization of the overfitting behavior for any scaling of the dimension with sample size. We use this to provide the first example of benign overfitting using the Gaussian kernel with sub-polynomial scaling dimension. All our results are under the Gaussian universality ansatz and the (non-rigorous) risk predictions in terms of the kernel eigenstructure. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A central question in learning theory is how learning algorithms can generalize well even when returning models that perfectly fit (i.e. interpolate) noisy training data. This phenomenon was observed empirically by Zhang et al. [63], and does not align with the traditional belief from statistical learning theory that overfitting to noise leads to poor generalization. Consequently, it attracted significant interest in recent years, and there has been much effort to understand the overfitting behavior of linear models, kernel methods, and neural networks. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we study the overfitting behavior of Kernel Ridge Regression (KRR) with Gaussian kernel, namely, the behavior of the limiting test error when training on noisy data as the number of samples tends to infinity by insisting on interpolation (achieving zero training error). When the input dimension and bandwidth are fixed, the overftiting behavior is known to be \u201ccatastrophic\u201d [37], i.e. for any nonzero noise, the test risk tends to infinity as the sample size increases. However, this is not how Gaussian Kernel Ridge Regression is typically used in practice. In fixed dimension, the bandwidth is tuned, that is decreased, when the sample size increases [51, 19]. Additionally, it makes sense to study the behaviour when the input dimension increases with sample size (as in, e.g. linear models with proportional scaling [42]). This could be because when more data is available, more input features are used, even with a kernel; because as more resources are available we scale up both the input dimension and amount of data used; or to capture the fact that very large scale problems typically involve both more samples and higher input dimension. But unlike with linear models, where the dimension must scale linearly with the number of samples in order to allow for interpolation, when a kernel is used we can study the behaviour even when the input dimensionality increases much slower, and ask how slowly it could increase without catastrophic overftiting. Previous studies on kernel ridgeless regression considered polynomial increasing dimension (i.e. dimension ${\\displaystyle\\propto}$ sample-sizea, for $0<a\\le1$ ) [4, 64, 24, 38, 40], but not subpolynomial scaling. ", "page_idx": 0}, {"type": "text", "text": "We aim to provide a more comprehensive picture of overfitting with Gaussian KRR by studying the overfitting behavior with varying bandwidth or with arbitrarily varying dimension, including sub-polynomially. In particular, we show that for fixed dimension, even with varying bandwidth, the interpolation learning is never consistent and generally not better than the null predictor (either the test error tends to infinity or is finite but it is almost always not better than the null predictor). For increasing dimension, we give an upper and lower bound on the test risk for any scaling of the dimension with sample size, which indicates in many cases whether the overfitting is catastrophic (test error tends to infinity), tempered (test error tends to a constant), or benign (consistent). Our result agrees with the polynomial scaling of the dimension with sample size, showing tempered overfitting for an exponent that is a reciprocal of an integer and benign overfitting for any other exponent [4, 64]. Moreover, our result goes further, as we show the first example of sub-polynomially scaling dimension that achieves benign overftiting for the Gaussian kernel. Additionally, we show that a class of dot-product kernels on the sphere is inconsistent when the dimension scales logarithmically with sample size. All our results are under the Gaussian universality ansatz and the non-rigorous but well-established risk predictions in terms of the kernel eigenstructure [50, 66, 8, 57]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The test performance of overfitting models has been extensively studied for linear regression [27, 6, 3, 43, 45, 16, 32, 58, 53, 65, 30, 54, 13, 2, 49, 25], linear classification [12, 55, 10, 44, 42, 49, 36, 52, 56, 18], neural networks [20, 21, 11, 34, 60, 61, 39, 33, 23, 31, 29], and kernel methods. Below we focus on overfitting in kernel ridge regression. ", "page_idx": 1}, {"type": "text", "text": "Overfitting in fixed dimension with fixed kernel. In Mallinar et al. [37], the authors show that the minimum norm interpolating solution for Gaussian kernel with fixed bandwidth overfits catastrophically. In [15, 4, 64], the authors derive bounds on the test risk of minimum norm interpolant with a fixed kernel under various assumptions. Our result for fixed dimension will only apply to the Gaussian kernel, but it allows for any varying or adaptively chosen bandwidth. ", "page_idx": 1}, {"type": "text", "text": "Inconsistency of Kernel Ridge Regression. The case of varying bandwidth has been considered in Beaglehole et al. [5], Rakhlin and Zhai [47], Haas et al. [26]. In Beaglehole et al. [5], the authors show that there exists a specific data distribution for which the minimum norm interpolanting solution for a particular set of translation invariant kernels is not consistent. In Rakhlin and Zhai [47], the authors show that for input distributions on the unit ball, the Laplace kernel is inconsistent, even with varying bandwidth. In Haas et al. [26], the authors show that under different assumptions on the data distribution, for a general class of (potentially varying) kernels in fixed dimension, any differentiable function that overfits the data and is not much different from the minimum norm interpolant is inconsistent. All of these works only consider whether we can achieve consistency. None of these results apply in the case of data distributions that we are considering, and even if the predictor is not consistent, we ask how bad is it by comparing it to the null predictor and whether it might be tempered. ", "page_idx": 1}, {"type": "text", "text": "Overfitting in increasing dimensions. Many papers have studied the setup where the dimension increases with sample size [4, 64, 66, 38, 40, 28, 59], in particular when the dimension is a function of sample size (or vice versa), but they all consider only the case of a polynomial scaling of dimension and sample size. It was shown that in this case the minimum norm interpolating solution of dot product kernels on the sphere can be benign, depending on if the exponent is not an integer. We generalize these results to any scaling of the dimension and sample size. Our results recover the existing results in the case of polynomially scaling dimension and show benign overftiting in a certain sub-polynomial scaling. Having sub-polynomimal scaling of the dimension allows us to expand the set of possible target functions from only polynomials of a bounded degree, as in the case of polynomial scaling of dimension [4, 64, 66, 38, 40], to, in our case of sub-polynomially increasing dimension, polynomials of any degree and even non-polynomial functions. ", "page_idx": 1}, {"type": "text", "text": "2 Problem formulation and assumptions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Kernel ridge(less) regression and the Gaussian kernel. Let $\\mathcal{D}$ be an unknown distribution over $\\mathcal{X}\\times\\mathcal{Y}\\subseteq\\bar{\\mathbb{R}^{d}}\\times\\mathbb{R}$ and let $\\{(x_{i},y_{i})\\}_{i=1}^{m}\\sim{\\cal D}^{m}$ be a dataset consisting of $m$ samples. For simplicity, we will assume that the distribution of the target is given by a target function $f^{*}$ of the input $x\\in\\mathscr{X}$ ", "page_idx": 1}, {"type": "text", "text": "with zero mean independent noise $\\xi$ with variance $\\sigma^{2}$ , that is $y\\sim f^{*}(x)+\\xi$ . We note that our results can be extended to a distribution agnostic setting, as analyzed by Zhou et al. [66]. ", "page_idx": 2}, {"type": "text", "text": "Let $K:\\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R}$ be a positive semi-definite kernel function. Let $\\|f\\|_{K}$ be the norm of $f$ in the RKHS $\\mathcal{H}_{K}$ corresponding to $K$ . For a predictor $f$ , let $R(f)$ and $\\hat{R}(f)$ be the test and training risk of $f$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\hat{R}}(f)={\\frac{1}{m}}\\sum_{i=1}^{m}(f(x_{i})-y_{i})^{2}{\\mathrm{~and~}}\\,R(f)=\\mathbb{E}_{\\mathcal{D}}\\left[(f(x)-y)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Two important risks to consider are the risk of the null predictor, $f\\equiv0$ , which we will denote by $R(0)=\\mathbf{\\dot{E}}_{\\mathcal{D}}[(y)^{2}]$ , and the Bayes (or irreducible) risk, which we will denote $\\sigma^{2}$ or $R(f^{*})$ . Using this notation, the risk of the null predictor is $R(0)=\\sigma^{2}+\\mathbb{E}_{\\mathcal{D}}[(y)^{2}]$ . Bayes risk represents the minimum possible risk that can be achieved by any predictor. For a regularization parameter $\\delta$ , the regularized ridge solution $\\hat{f}_{\\delta}$ is given by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{f}_{\\delta}=\\mathrm{argmin}_{f\\in\\mathcal{H}_{K}}\\hat{R}(f)+\\frac{\\delta}{m}\\|f\\|_{K}^{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We are interested in the minimum norm interpolating (ridgeless) solution $\\begin{array}{r}{\\hat{f}_{0}=\\operatorname*{lim}_{\\delta\\to0+}\\hat{f}_{\\delta}}\\end{array}$ , namely ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{f}_{0}=\\mathrm{argmin}_{\\hat{R}(f)=0;f\\in\\mathcal{H}_{K}}\\|f\\|_{K}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We will focus on the Gaussian kernel, which is given by $\\begin{array}{r}{K(x_{1},x_{2})=\\exp\\left(-\\frac{\\|x_{1}-x_{2}\\|_{2}^{2}}{\\tau_{m}^{2}}\\right)}\\end{array}$ , where $m$ is the sample size and $\\tau_{m}$ is a predetermined bandwidth parameter that can vary with sample size. The Gaussian kernel is widely used and achieves good error rates for a variety of learning tasks [51, 19]. Gaussian KRR achieves optimal convergence to the best possible (Bayes) error for learning any function in a Besov space of high enough order (essentially bounded and twice differentiable in the weak sense) under very mild assumptions on the distribution of the input and target $\\mathcal X\\times\\mathcal X$ [19]. For ridge regression with the Gaussian kernel and under a standard data distribution assumption, the minimum distance between samples decreases with sample size so it makes sense to also decrease $\\tau_{m}$ . Additionally, decreasing $\\tau_{m}$ with sample size helps to achieve good convergence rates theoretically [19]. ", "page_idx": 2}, {"type": "text", "text": "Main question. We will consider the problem of learning using the minimum norm interpolating solution $\\hat{f}_{0}$ of KRR. We want to understand the limiting behavior of test risk $R(\\hat{f}_{0})$ as the sample size increases $m\\rightarrow\\infty$ , that is $\\operatorname*{lim}_{m\\to\\infty}R(\\hat{f}_{0})$ . It suffices to understand lim $\\operatorname*{sup}_{m\\to\\infty}R(\\hat{f}_{0})$ and $\\operatorname*{lim}\\operatorname*{inf}_{m\\to\\infty}R(\\hat{f}_{0})$ and this way we do not assume the existence of the limit. In this work, we use the taxonomy of benign, tempered, and catastrophic overfitting from Mallinar et al. [37], which indicates whether $\\textstyle\\operatorname*{lim}_{m\\to\\infty}R({\\hat{f}}_{0})$ is the Bayes (optimal) error, a non-optimal but constant error, or infinity. Note that in this taxonomy, the null predictor can be classified as tempered. Therefore, we will compare the limiting risk to the risk of the null predictor $R(0)$ in order to understand whether the performance of the interpolating solution is non-trivial. ", "page_idx": 2}, {"type": "text", "text": "Main tool: Eigenframework and a closed form of the test risk. Our main tool will be the closed form of the test risk predicted by the eigenframwork [50]. Under the eigenframework, we can write down the closed form of the test risk using Mercer\u2019s theorem decomposition of a kernel function $K$ . ", "page_idx": 2}, {"type": "text", "text": "Given a positive semi-definite kernel function $K:\\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R}$ , we can decompose it as ", "page_idx": 2}, {"type": "equation", "text": "$$\nK(x_{1},x_{2})=\\sum_{k=1}^{\\infty}\\lambda_{k}\\phi_{k}(x_{1})\\phi_{k}(x_{2}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\lambda_{k}$ and $\\phi_{k}$ are the eigenvalues and eigenfunctions of the integral operator associated to $K$ . The eigenfunctions $\\{\\phi_{k}\\}$ are an orthonormal basis of $L_{\\mathcal{D}_{\\mathcal{X}}}^{2}(\\mathcal{X})$ , where $\\mathcal{D}_{\\mathcal{X}}$ is the marginal distribution of $\\mathcal{X}$ . We denote the Bayes optimal target function by $f^{*}$ , and expand it in the kernel basis $\\{\\phi_{i}\\}_{i=1}^{\\infty}$ as $\\begin{array}{r}{f^{*}(x)=\\sum_{i=1}^{\\infty}\\beta_{i}\\phi_{i}(\\dot{x})}\\end{array}$ . To state the close form of the test risk we will introduce a few quantities. Let effective  regularization, \u03ba\u03b4, be the solution to  i\u221e=1\u03bbi\u03bb+i\u03ba\u03b4 + \u03ba\u03b4\u03b4 = m. Furthermore, let Li,\u03b4 =\u03bbi\u03bb+i\u03ba\u03b4 and $\\begin{array}{r}{\\mathcal{E}_{\\delta}=\\frac{m}{m-\\sum_{i=1}^{\\infty}\\mathcal{L}_{i,\\delta}^{2}}}\\end{array}$ . Then, the predicted risk, i.e. the predicted closed form of the test risk of $\\hat{f}_{\\delta}$ is given by ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tilde{R}(\\hat{f}_{\\delta})=\\mathcal{E}_{\\delta}\\left(\\sum_{i=1}^{\\infty}\\left(1-\\mathcal{L}_{i,\\delta}\\right)^{2}\\beta_{i}^{2}+\\sigma^{2}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\sigma^{2}$ is the Bayes error of $\\mathcal{D}$ . Equation (2) was initially heuristically derived using the replica method or continuous approximations to the learning curves inspired by the Gaussian process literature [7]. In [50], it is dervied using a conservation law. Note that [66] shows that the predicted closed form of the test risk from [50] extends to general target distributions. There is strong evidence that the predicted risk is a good estimate of the true test risk, namely $R(\\hat{f}_{\\delta})\\approx\\tilde{R}(\\hat{f}_{\\delta})$ . Indeed, a number of works use the predicted risk closed form to estimate the test risk of KRR [58, 8, 66]. The following assumption on Gaussian design ansatz is used by all of these works. ", "page_idx": 3}, {"type": "text", "text": "Assumption 1 (Gaussian design ansatz, cf. Zhou et al. [66]). When sampling $(x,\\cdot)\\sim\\!D$ , we have that the Gaussian universality holds for the eigenfunctions in the sense that the expected risk is unchanged if we replace $\\phi$ with $\\tilde{\\phi}$ , where $\\tilde{\\phi}$ is Gaussian with appropriate parameters, i.e. $\\tilde{\\phi}\\sim\\mathcal{N}(0,\\mathrm{diag}\\{\\lambda_{i}\\})$ . ", "page_idx": 3}, {"type": "text", "text": "This assumption appears to hold for real datasets as well, namely, the predictions computed for Gaussian design agree well with the experiments on kernel regression using real data [8, 50, 57]. As discussed in Zhou et al. [66], under this assumption, the equivalence $R(\\bar{\\hat{f}}_{\\delta})\\approx\\tilde{R}(\\hat{f}_{\\delta})$ holds in a few ways. First, in an appropriate asymptotic limit in which the sample size $m$ and the number of eigenmodes in a given eigenvalue grow proportionally, the equivalence holds [27, 1]. Second, if the eigenstructure of the task is fixed, the error between the two can be bounded by a decaying function of $m$ [14]. Finally, various numerical experiments show that the error between the two is small even for a small sample size $m$ [8, 50]. Specifically, Canatar et al. [8] (see Figure 5 there) gives empirical evidence that the predicted risk closely approximates the true risk for the Gaussian kernel with data uniform on a sphere, which is the setting that we consider in some of our results. ", "page_idx": 3}, {"type": "text", "text": "There has been some recent progress in bounding the error between $R(\\hat{f}_{\\delta})$ and $\\tilde{R}(\\hat{f}_{\\delta})$ unconditionally. Misiakiewicz and Saeed [41] shows that the error will tend to zero if the dimension $d$ grows fast enough with the sample size. Additionally, they provide strong empirical evidence that the predicted risk is close to the test risk for a real dataset (MNIST) and Gaussian kernel, see Figure 1 in [41]. ", "page_idx": 3}, {"type": "text", "text": "Formally, we will prove results about the predicted risk $\\tilde{R}(\\hat{f}_{\\delta})$ , but as previously presented evidence suggests, treating $R(\\hat{f}_{\\delta})\\approx\\tilde{R}(\\hat{f}_{\\delta})$ as equivalence is sufficient for understanding the behavior of KRR. ", "page_idx": 3}, {"type": "text", "text": "We note that using the eigenframework might introduce restrictions for which kernels some of our results apply, as concurrent work showed that the eigenframework prediction might not hold for the NTK in fixed dimension [4, 15]. Our two main results concern the Gaussian kernel, for which we described ample empirical evidence that the eigenlearning predictions hold [8, 41]. Understanding the limitations of the eigenframework is an important future research direction. ", "page_idx": 3}, {"type": "text", "text": "3 Fixed dimension: Gaussian kernel with varying bandwidth ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We will assume that the source distribution is uniform on a $d$ dimensional sphere, that the target function is square integrable, and that the target distribution is given by the target function with an independent noise. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2 (Target function and data distribution). Let $\\mathcal{D}$ be the distribution over $\\mathcal{X}\\times\\mathcal{Y}=$ $\\mathbb{S}^{d-1}\\times\\mathbf{\\bar{R}}$ , such that the $\\mathcal{X}$ marginal, denoted by $\\mathcal{D}_{\\mathcal{X}}$ , is $\\mathrm{Unif}(\\mathbb{S}^{d-1})$ . We will assume that for a target function $f^{*}\\,\\in\\,L_{\\mathcal{D}_{\\boldsymbol{x}}}^{2}(\\mathbb{S}^{d-1})$ , the marginal $\\boldsymbol{\\wp}$ distribution is given by $y\\sim f^{*}(x)+\\xi$ , where $\\xi$ has mean zero and variance $\\sigma^{2}>0$ . We write $\\begin{array}{r}{f^{*}=\\sum_{i}\\beta_{i}\\phi_{i}}\\end{array}$ , where $\\{\\phi_{i}\\}$ is the $L_{\\mathcal{D}_{\\boldsymbol{x}}}^{2}(\\mathbb{S}^{d-1})$ eigenbasis corresponding to the kernel $K$ (Equation (1)). If we write $\\beta\\,=\\,(\\beta_{1},\\beta_{2},.\\,.\\,.\\,)$ , then we have that $\\|\\beta\\|_{2}^{2}=\\mathbb{E}_{\\mathcal{D}_{\\mathcal{X}}}\\left((f^{*}(x))^{2}\\right)$ . We will use the notation $\\|f^{*}\\|^{2}=\\|\\beta\\|_{2}^{2}$ . ", "page_idx": 3}, {"type": "text", "text": "The assumption on the distribution on $\\mathcal{X}$ is common in the literature on KRR [38, 4, 64, 22]. The assumption that $x\\sim\\mathrm{Unif}\\left(\\mathbb{S}^{d-1}\\right)$ can be relaxed to a more general setting where $x$ is uniformly distributed on other manifolds that are diffeomorphic to the sphere using the results of Li et al. [35], although that will not be the focus of this paper. ", "page_idx": 3}, {"type": "text", "text": "Note that here we vary the bandwidth $\\tau_{m}$ so both $\\hat{f}_{0}$ and $R(\\hat{f}_{0})$ will depend on the bandwidth $\\tau_{m}$ as well as $m$ . We will identify three different regimes of bandwidth scaling. We will show that the minimum norm interpolating solution exhibits either tempered or catastrophic overfitting, and we will argue that it is almost always worse than the risk of the null predictor. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3 (Overftiting behavior of Gaussian kernel in fixed dimension). Under Assumption 2, the following bounds hold for the predicted risk $\\tilde{R}(\\hat{f}_{0})$ of the minimum norm interpolating solution of Gaussian KRR: ", "page_idx": 4}, {"type": "text", "text": "1. If $\\tau_{m}=o(m^{-\\frac{1}{d-1}})$ , then $\\begin{array}{r}{\\tilde{R}(0)\\leq\\operatorname*{lim}\\operatorname*{inf}_{m\\to\\infty}\\tilde{R}(\\hat{f}_{0})\\leq\\operatorname*{lim}\\operatorname*{sup}_{m\\to\\infty}\\tilde{R}(\\hat{f}_{0})<\\infty.}\\end{array}$ More precisely, if $\\tau_{m}\\leq m^{-\\frac{1}{d-1}}t(m)$ , where $t(m)\\rightarrow0$ as $m\\rightarrow\\infty$ , then there is a scalar $c_{d}$ that depends only on the dimension and $m_{0}$ that depends on $t(m)$ such that for all $m>m_{0}$ we have $\\tilde{R}(\\hat{f}_{0})>\\sigma^{2}+(1-c_{d}t(m)^{\\frac{d-1}{2}})\\|f^{*}\\|^{2}$ .   \n2. If $\\tau_{m}=\\omega(m^{-\\frac{1}{d-1}})$ , then $\\operatorname*{lim}_{m\\rightarrow\\infty}\\tilde{R}(\\hat{f}_{0})=\\infty$ . Hence, for large enough $m$ we have $\\tilde{R}(\\hat{f}_{0})>\\tilde{R}(0)$ .   \n3. If $\\tau_{m}\\;\\=\\;\\Theta(m^{-\\,\\frac{1}{d-1}})$ , then $\\begin{array}{r}{\\operatorname*{lim}\\operatorname*{sup}_{m\\rightarrow\\infty}R(\\hat{f}_{0})~<~\\infty}\\end{array}$ . Moreover, Suppose that $C_{1}m^{-\\frac{1}{d-1}}\\leq\\tau_{m}\\leq C_{2}m^{-\\frac{1}{d-1}}$ for some constants $C_{1}$ and $C_{2}$ , then there exist $\\eta,\\mu>0$ that depend only on $d$ , $C_{1}$ , and $C_{2}$ , such that for all $m$ we have $\\tilde{R}(\\hat{f}_{0})>\\mu\\|f^{*}\\|^{2}\\!+\\!(1\\!+\\!\\eta)\\sigma^{2}$ . Consequently, $\\tilde{R}(\\hat{f}_{0})>\\tilde{R}(\\mathbf{0})$ as long as $\\begin{array}{r}{\\sigma^{2}>\\frac{1-\\mu}{\\eta}\\|f^{\\ast}\\|^{2}}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 3 shows that the minimum norm interpolating solution of Gaussian KRR cannot be consistent when data is distributed uniformly on the sphere, even with varying or adaptively chosen bandwidth. Additionally, in the first two modes of bandwidth change, the minimum norm interpolating solution is never better than the null predictor. In the third case, the interpolating solution is worse than null for noise that is not too small. This shows that even though the minimum norm interpolating predictor is classified as tempered in the first and third cases of scaling of the bandwidth, it is still worse than the trivial null predictor. Note that our analysis does not exclude the possibility that for $\\tau_{m}=\\Theta(m^{\\frac{1}{d-1}})$ there exists small enough $\\sigma^{2}$ for which the interpolating solution is better than the null predictor. We leave this as an open question. In Appendix A, we provide further empirical justification for Theorem 3. ", "page_idx": 4}, {"type": "text", "text": "4 Increasing dimension ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For the case of increasing dimension, we consider the problem of learning a sequence of distributions $\\mathcal{D}^{(d)}$ over $\\mathcal{X}\\times\\mathcal{Y}=\\mathbb{R}^{d}\\times\\mathbb{R}$ given by $y\\sim f_{d}^{*}(x)+\\xi_{d}$ using a sequence of kernels $K^{(d)}$ . Here, $\\xi_{d}$ is independent noise with mean 0 and variance $\\sigma^{2}>0$ . Formally, the kernel and the target function can change with the dimension $d$ , but we will think of it as the same kernel and target with higher dimensional input. Furthermore, $d$ will increase with sample size $m$ , i.e. $d=d(m)$ (or analogously $m$ will increase with $d$ ). A common assumption, which we also adopt, is that the projections of the target function $f_{d}^{*}$ onto the eigenfunctions $\\bar{\\phi}_{k}^{(d)}$ of the kernels $K^{(d)}$ are uniformly bounded [4, 64]. ", "page_idx": 4}, {"type": "text", "text": "Assumption 4 (Target function and distribution in increasing dimension). Consider learning a sequence of target functions $f_{d}^{*}$ with a sequence of kernels $K^{(d)}$ . Let the target function $f_{d}^{*}$ have only $S_{d}$ nonzero coefficients (where $S_{d}$ can change with $d$ ), so $\\begin{array}{r}{f_{d}^{*}=\\sum_{i=1}^{S_{d}}\\beta_{i}^{(d)}\\phi_{i}^{(d)}}\\end{array}$ where $\\phi_{i}^{(d)}$ are from Equation (1) and $|\\beta_{i}^{(d)}|\\le B$ , i.e. $\\|\\beta\\|_{\\infty}\\leq B$ , for $B$ that is independent of $d$ . ", "page_idx": 4}, {"type": "text", "text": "The functions that can be represented in this form depend on the number of nonzero coefficients, $S_{d}$ , and the kernel that we are using. In particular, for dot-product kernels on the sphere it includes all polynomials of degree $k\\leq k_{d}$ where $k_{d}$ is such that the multiplicities of first $k_{d}$ eigenfunctions are at most $S_{d}$ . If $S_{d}$ grows with $d$ , then this set will include much more general functions. See Remark 14 for further discussion. ", "page_idx": 4}, {"type": "text", "text": "First, we will consider a general kernel $K$ since Theorem 7 and Theorem 9 will apply more generally.   \nThen, we will apply these results to the cases of dot-product and Gaussian kernels. ", "page_idx": 4}, {"type": "text", "text": "The multiplicities of eigenvalues will play an important role in bounding the test risk, both from above and below, so we will introduce the following related notation. ", "page_idx": 4}, {"type": "text", "text": "Definition 5 (Lower and upper index). Let $\\tilde{\\lambda}_{k}$ be the $k$ -th non-repeating eigenvalue of a kernel $K$ and let $N(k)$ be its multiplicity. Let $N_{k}\\,=\\,N(1)\\,+\\,\\cdot\\,\\cdot\\,+\\,N(\\mathbf{\\dot{k}})$ . Let $m$ be the sample size. Let $k_{m}$ be defined as the maximal $k$ such that there is less than $m$ eigenvalues with index $k$ , i.e. $k_{m}=\\operatorname*{max}\\{k\\in\\mathbb{N}|N(1)+\\cdot\\cdot\\cdot+N(k)<m\\}$ . Define the lower index $L_{m}$ and the upper index $U_{m}$ as follows $L_{m}=N(1)+\\cdot\\cdot\\cdot+N(k_{m})$ and $\\dot{U}_{m}=N(1)+\\cdot\\cdot\\cdot+N(k_{m}+1)$ . When the dimension changes with sample size, we sometimes denote $N(k)$ by $N(d,k)$ . ", "page_idx": 5}, {"type": "text", "text": "We will first state a generic bound on the test risk for any data distribution, kernel with a bounded sum of eigenvalues, sample size, and dimension. This bound will be informative when we scale the dimension $d$ with sample size $m$ , but it holds for any kernel that satisfies the following assumption. ", "page_idx": 5}, {"type": "text", "text": "Assumption 6 (Bounded sum of eigenvalues). Assume that the kernel $K$ has a bounded sum of eigenvalues, i.e. there is a constant $A$ such that $\\textstyle\\sum_{i=1}^{\\infty}N(i){\\tilde{\\lambda}}_{i}\\leq A$ . For a sequence of kernels $K^{(d)}$ , assume that all such $A^{(d)}$ are bounded by some constant $A$ .1 ", "page_idx": 5}, {"type": "text", "text": "This is a reasonable assumption for most dot-product kernels, as we show in Appendix C.4. It also implicitly sets the scale of the kernel. ", "page_idx": 5}, {"type": "text", "text": "Theorem 7 (Test risk upper bound for kernel ridgeless regression). Let $d$ and $m$ be any dimension and sample size. Define $L_{m},\\,U_{m},\\,k_{m},\\,N(i),\\,N_{l},$ , and $\\tilde{\\lambda}_{k}$ as in Definition 5. Consider KRR with a kernel $K$ satisfying Assumption 6 for some $A$ . Assume that for some integer $l$ , the target function $f^{*}$ satisfies Assumption 4 with at most $N_{l}$ nonzero coefficients. Then, the predicted risk of the minimum norm interpolating solution is bounded by the following: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\tilde{R}}(\\hat{f}_{0})\\leq\\left(1-\\frac{L_{m}}{m}\\right)^{-1}\\left(1-\\frac{m}{U_{m}}\\right)^{-1}\\sigma^{2}}}\\\\ {{\\displaystyle\\qquad\\quad+\\,B^{2}\\left(1-\\frac{L_{m}}{m}\\right)^{-1}\\left(1-\\frac{m}{U_{m}}\\right)^{-1}\\frac{A^{2}}{m^{2}}\\left(\\sum_{i=1}^{l}N(i)\\frac{1}{\\tilde{\\lambda}_{i}^{2}}\\right)\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Alternatively, we can bound the risk using $\\left(\\sum_{i=1}^{l}N(i)\\frac{1}{\\tilde{\\lambda}_{i}}\\right)$ instead of $\\left(\\sum_{i=1}^{l}N(i)\\frac{1}{\\tilde{\\lambda}_{i}^{2}}\\right)$ (see Theorem 20 in the appendix). ", "page_idx": 5}, {"type": "text", "text": "We also establish a generic inconsistency result for any data distribution, kernel with a bounded sum of eigenvalues, sample size, and dimension based on the upper and lower indices from Definition 5. We will further need to assume that the eigenvalues are bounded away from zero. Similarly, this will be useful when scaling dimension $d$ with sample size, but it holds generally. ", "page_idx": 5}, {"type": "text", "text": "Assumption 8 (Lower bound on eigenvalues). Assumme that the kernel $K$ has eigenvalues that are not too small, i.e. there is a constant b such that maxi\u2264km \u03bb\u02dc1i < m\u2212bLm. For a sequence of kernels $K^{(d)}$ , assume that for the corresponding $m=m(d)$ (since $d=d(m)$ , we can also \"invert\" the dependence) all such $b^{(d)}$ are bounded below by some $b$ . ", "page_idx": 5}, {"type": "text", "text": "This assumption will hold for most dot-product kernels and we will show it for Gaussian kernel in Appendix C.4. ", "page_idx": 5}, {"type": "text", "text": "Theorem 9 (Test risk lower bound for any kernel ridgeless regression). Let $k_{m}$ and $L_{m}$ be as in Definition 5. Consider learning a target function $f^{*}$ , with some sample size $m$ . Let $K$ be a kernel satisfying Assumption 6 and Assumption 8 for some $A$ and $b$ . Consider the minimum norm interpolating solution of KRR (with any data distribution) with kernel $K$ . Then, for the predicted risk of minimum norm interpolating solution, the following lower bound holds: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{R}(\\hat{f}_{0})>\\left(1-\\left(\\frac{b}{b+1}\\right)^{2}\\frac{L_{m}}{m}\\right)^{-1}\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "To apply Theorem 7 for varying dimension $d$ , we would additionally require that $A$ is uniformly bounded for all $d$ and kernel $K^{(d)}$ and also that $l=l(d)$ changes with $d$ such that Assumption 4 holds with $S_{d}=N_{l(d)}$ . For dot-product kernels $K^{(d)}$ on the sphere, if we let $K^{(d)}(x,y)=h^{(d)}(\\|x-y\\|)$ , we will have $A\\,=\\,\\operatorname*{sup}_{d}h^{(d)}(0)$ , so if $h^{(d)}$ does not change with $d$ we can take $A=h(0)$ (see Appendix C.4 for more details). Specifically, this holds for the Gaussian kernel on the sphere with $A=1$ . ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "To apply Theorem 9 to the case of increasing dimension, we would require that the bounds $\\begin{array}{r}{\\sum_{i}N(d,i)\\tilde{\\lambda}_{i}\\,\\leq\\,A}\\end{array}$ and $\\begin{array}{r}{\\operatorname*{max}_{i\\leq k_{m}}\\left(\\frac{1}{\\tilde{\\lambda}_{i}}\\right)\\;<\\;\\frac{m-\\bar{L}_{m}}{b}}\\end{array}$ < m\u2212bLm hold for all d and kernels K(d). Usually, the condition $\\begin{array}{r}{\\operatorname*{max}_{i\\leq k_{m}}\\left(\\frac{1}{\\tilde{\\lambda}_{i}}\\right)<\\frac{m-L_{m}}{b}}\\end{array}$ will be satisfied for $b=1$ . We will show it for the two cases of sub-polynomially scaling dimensions with a Gaussian kernel. For the polynomial scaling dimension, it is reasonable to assume it for general dot-product kernels, as discussed in Appendix C.4. ", "page_idx": 6}, {"type": "text", "text": "Now, we will show that using Theorem 7 and Theorem 9 we can recover the behavior of the minimum norm interpolating solution for polynomially increasing dimension [4, 64], i.e. tempered overftiting for integer exponent and benign for non-integer exponent. Here, we will need to additionally assume that the eigenvalue decay is not too fast. ", "page_idx": 6}, {"type": "text", "text": "Assumption 10 (Eigenvalue decay). The eigenvalues do not decrease too quickly, i.e. for $k_{m}$ as in Definition 5, we have that there is a constant $c$ such that $\\begin{array}{r}{\\operatorname*{max}_{i\\leq k_{m}}\\left(\\frac{1}{\\tilde{\\lambda_{i}}}\\right)\\leq c N(k_{m})}\\end{array}$ . For increasing dimension, we require that $\\begin{array}{r}{\\operatorname*{max}_{i\\leq k_{m}}\\left(\\frac{1}{\\tilde{\\lambda}_{i}}\\right)\\,\\leq\\,c N(d,k_{m})}\\end{array}$ for all $m$ (i.e. all $d$ , as $d$ and $m$ both increase). ", "page_idx": 6}, {"type": "text", "text": "This assumption is stronger than Assumption 8, but as we show in Appendix C.4, it is reasonable for dot-product kernels on the sphere and even the NTK. ", "page_idx": 6}, {"type": "text", "text": "Corollary 11 (Dot-product kernels with polynomially increasing dimension, recovering the results of [24, 38, 40, 4, 64]). Consider the problem of learning a sequence of target functions $f_{d}^{*}$ satisfying Assumption 4 with $S_{d}\\leq\\Theta(d^{\\lfloor\\alpha\\rfloor})$ with a dot-product kernel $K(x,y)=h(\\|x-y\\|)$ with $h(0)=1$ on the sphere $\\mathbb{S}^{d-1}$ (where $h$ does not depend on $d$ , i.e. $A=1$ from Assumption 6)) that further satisfies Assumption 10. Let $\\begin{array}{r}{\\frac{d^{\\alpha}}{m_{\\!_{\\cdot}}}=\\Theta(1)}\\end{array}$ for $\\alpha\\in(0,\\infty)$ . Then the overftiting behavior of the minimum norm interpolating solution is benign if $\\alpha$ is not an integer and tempered if $\\alpha$ is an integer. ", "page_idx": 6}, {"type": "text", "text": "Additionally, we will show that for $d=\\log m$ , we cannot get benign overftiting, i.e. consistency with a class of dot-product kernels on the sphere. Similarly, as in the previous corollary, this will hold for any sequence where $d=\\log m$ even only asymptotically. ", "page_idx": 6}, {"type": "text", "text": "Corollary 12 (Inconsistency with dot-product kernels in logarithmically scaling dimension). Let $K^{(d)}$ be a sequence of dot-product kernels on $\\mathbb{S}^{d-1}$ that satisfy Assumption 8. Let the dimension $d$ grows with sample size as $d=\\log_{2}m$ (i.e. $m=2^{d}$ ). Then, the minimum norm interpolant cannot exhibit benign overfitting for any such sequence $K^{(d)}$ , i.e. there exists an absolute constant $\\eta>0$ such that for all $d,m,\\tilde{R}(\\hat{f}_{0})>(1+\\eta)\\sigma^{2}$ . ", "page_idx": 6}, {"type": "text", "text": "On the other hand, using Theorem 7, we will establish the first case of sub-polynomial scaling dimension\u221a with benign overfitting using the Gaussian kernel and data on the sphere. We will use $d=\\exp({\\sqrt{\\log m}})$ . ", "page_idx": 6}, {"type": "text", "text": "Corollary 13 (Benign overfitting with Gaussian kernel and sub-polynomial dimension). Let $K$ be the Gaussian kernel on the sphere $\\mathbb{S}^{d-1}$ with a fixed bandwidth, and take a sequence of dimensions $d$ and sample sizes $m$ that scale as $d=\\exp\\left({\\sqrt{\\log m}}\\right)$ (in particular, we take $l\\in\\mathbb N$ such that $d=2^{2^{l}}$ and $m\\:=\\:2^{2^{2l}}$ with $l\\,=\\,1,2,3\\cdot\\cdot\\cdot)$ . Consider learning a sequence of target functions $f_{d}^{*}$ as in Assumption 4 with $S_{d}\\leq m^{\\frac{1}{4}}$ . Then, we have that the minimum norm interpolating solution achieves the Bayes error in the limit $(m,d)\\to\\infty$ . In particular, for $d\\geq4$ and $m\\ge16$ we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\tilde{R}(\\hat{f}_{0})\\leq\\left(1-\\frac{1}{\\log m}\\right)^{-1}\\left(1-\\exp\\left(-0.89\\sqrt{\\log m}\\right)\\right)^{-1}\\sigma^{2}+2B^{2}\\frac{1}{m}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Remark 14 (Allowed target functions). The set of allowed target functions $f_{d}^{*}$ in Corollary 13, i.e. with sub-polynomial scaling dimension, is strictly larger than the set of allowed target functions for polynomially scaling dimension, as in Corollary 11 and [38, 4, 64]. In particular, for polynomially scaling dimension $\\begin{array}{r}{\\frac{d^{\\alpha}}{m}=\\Theta(1)}\\end{array}$ , the result holds only if the target function is a polynomial of degree at most $\\lfloor\\alpha\\rfloor$ . On the other hand, Corollary 13 shows that sub-polynomially scaling dimension allows for the target function to be a polynomial of arbitrary degree, as well as non-polynomial functions. In particular, in dimension $d$ , we can represent polynomials of degree up to $\\Theta(\\log^{2}d)$ . ", "page_idx": 6}, {"type": "text", "text": "5 Proofs outline ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we discuss the main proof ideas. All formal proofs are provided in the appendix. ", "page_idx": 7}, {"type": "text", "text": "By Equation (2), to understand how the test risk of the minimum norm interpolating solution behaves, it suffices to understand how the eigenvalues corresponding to the kernel $K$ and thus the quantities $\\mathcal{E}_{0},\\mathcal{L}_{i,0}$ behave. In Zhou et al. [66], the authors show that ${\\mathcal{E}}_{0}$ is bounded both above and below in terms of the effective rank of the systems of eigenvalue $\\{\\lambda\\}_{i=1}^{\\infty}$ , defined by $\\begin{array}{r}{r_{k}:=\\frac{\\sum_{i=k+1}^{\\infty}\\lambda_{i}}{\\lambda_{k+1}}}\\end{array}$ i=k+1 \u03bbi. We will use this, along with directly bounding $\\mathcal{L}_{i,0}$ . ", "page_idx": 7}, {"type": "text", "text": "If $K$ is a dot-product kernel on the sphere (such as the Gaussian kernel), we can take the eigenfunctions $\\phi_{i}$ to be the spherical harmonics $Y_{k s}$ , where $k~\\ge~0$ and $s~\\in~[1,N(d,k)]$ . Here $\\begin{array}{r}{N(d,k)\\,=\\,\\frac{(2k+d-2)(k+d-3)!}{k!(d-2)!}}\\end{array}$ is the multiplicity of the $k$ -th spherical harmonic. All $Y_{k s}$ for the same $k$ will have the same eigenvalue, which we will denote $\\tilde{\\lambda}_{k}$ . In this case, we can write a closed-form expression for the eigenvalues of Gaussian kernel, $\\tilde{\\lambda}_{k}$ , in terms of the bandwidth $\\tau_{m}$ and modified Bessel functions of the first kind $I_{v}(x)$ [46] (see Appendix D). Using the closed form of $\\tilde{\\lambda}_{i}$ and the multiplicities of eigenvalues, we can understand how the test risk of the minimum norm interpolating solution $R(\\hat{f}_{0})$ behaves as $m\\rightarrow\\infty$ , which tells us its overfitting behavior. ", "page_idx": 7}, {"type": "text", "text": "Additionally, $\\mathcal{E}_{0}$ appearing in Equation (2), will be informative. For example, if $\\operatorname*{lim}_{m\\to\\infty}\\mathcal{E}_{0}>1$ then the overfitting cannot be benign. The following bound using the effective rank $r_{k}$ holds [66]: For $k<m$ such that $r_{k}+k>m$ we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{E}_{0}\\leq\\left(1-\\frac{k}{m}\\right)^{-1}\\left(1-\\frac{m}{k+r_{k}}\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "For $k\\ge m$ it holds that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{E}_{0}\\geq\\frac{1}{1-\\frac{m}{k}\\left(\\frac{k-m}{k-m+r_{k}}\\right)}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Proof sketch of Theorem 3. We will focus on the lower bounds in this case, as the result is negative. The key elements to understanding the effective rank $r_{k}$ and the test risk Equation (2) is to understand how the ratios of eigenvalues $\\frac{\\tilde{\\lambda}_{k+1}}{\\tilde{\\lambda}_{k}}$ and the multiplicities $N(d,k)$ behave. Using the closed form of the eigenvalues of Gaussian kernel and the properties of modified Bessel functions [46, 48], with some computation (see Theorem 28 in the appendix for the computations), we can derive the following bounds on ratios of eigenvalues ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{\\left(\\frac{2}{\\tau_{m}^{2}}\\right)}{2\\left(k+\\frac{d}{2}\\right)+\\left(\\frac{2}{\\tau_{m}^{2}}\\right)}<\\frac{\\tilde{\\lambda}_{k+1}}{\\tilde{\\lambda}_{k}}<\\frac{\\left(\\frac{2}{\\tau_{m}^{2}}\\right)}{\\left(k+\\frac{d}{2}-\\frac{1}{2}\\right)+\\left(\\frac{2}{\\tau_{m}^{2}}\\right)}\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "From these bounds, we can derive tight bounds on\u03bb\u02dc\u03bb\u02dck+j for indices k and j using simple but long calculations (see Theorem 28 in the appendix). If $\\begin{array}{r}{k=o\\left(\\frac{1}{\\tau_{m}}\\right)}\\end{array}$ and $\\begin{array}{r}{j=o\\left(\\frac{1}{\\tau_{m}}\\right)}\\end{array}$ , then $\\begin{array}{r}{\\frac{\\tilde{\\lambda}_{k+j}}{\\tilde{\\lambda}_{k}}\\approx1}\\end{array}$ $\\begin{array}{r}{k\\le\\Theta\\left(\\frac{1}{\\tau_{m}}\\right)}\\end{array}$ and $\\begin{array}{r}{j=\\Theta\\left(\\frac{1}{\\tau_{m}}\\right)}\\end{array}$ , then $\\begin{array}{r}{\\frac{\\tilde{\\lambda}_{k+j}}{\\tilde{\\lambda}_{k}}=\\Theta(1)}\\end{array}$ . If $\\begin{array}{r}{k=\\omega\\left(\\frac{1}{\\tau_{m}}\\right)}\\end{array}$ , then $\\begin{array}{r}{\\frac{\\tilde{\\lambda}_{k+j}}{\\tilde{\\lambda}_{k}}=o\\left(\\frac{1}{j^{n}}\\right)}\\end{array}$ for any integer $n\\in\\mathbb N$ (i.e. it deceases super-polynomialy). For $N(d,l)$ it holds that $N(d,l)=\\Theta(l^{d-2})$ and $N_{l}:=N(d,1)+\\cdot\\cdot\\cdot+N(d,l)=\\Theta(l^{d-1})$ . Therefore if $\\bar{l}$ is the index such that $\\tilde{\\lambda}_{\\tilde{l}}=\\lambda_{l}$ , we have that $\\tilde{l}=\\Theta(l^{\\frac{1}{d-1}})$ . ", "page_idx": 7}, {"type": "text", "text": "For $\\tau_{m}\\geq\\omega(m^{-\\frac{1}{d-1}})$ , we will take $\\begin{array}{r}{l=(1+\\frac{1}{\\sqrt{m}})m}\\end{array}$ and show that $r_{l}=o(m)$ . Then, the bound in Equation (6) will imply that $\\mathcal{E}_{0}\\geq1+\\sqrt{m}$ , so from Equation (2), $\\tilde{R}(\\hat{f}_{0})>\\mathcal{E}_{0}\\sigma^{2}=\\sqrt{m}\\sigma^{2}$ . Note that for $\\begin{array}{r}{l=(1+\\frac{1}{\\sqrt{m}})m}\\end{array}$ , we have that $\\begin{array}{r}{\\tilde{l}=\\Theta(m^{\\frac{1}{d-1}})=\\omega\\left(\\frac{1}{\\tau_{m}}\\right)}\\end{array}$ . Note that for $r_{l-1}$ , we have that ", "page_idx": 7}, {"type": "equation", "text": "$$\nr_{l-1}=\\sum_{i=0}^{\\infty}\\frac{\\lambda_{l+i}}{\\lambda_{l}}<N(d,\\tilde{l})+\\sum_{i=1}^{\\infty}N(d,\\tilde{l}+i)\\frac{\\tilde{\\lambda}_{\\tilde{l}+i}}{\\tilde{\\lambda}_{\\tilde{l}}}<\\Theta(m^{\\frac{d-2}{d-1}})\\left(1+\\Theta(1)\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "since $N(d,\\tilde{l}+i)<N(d,j)i^{d-2}$ and $\\begin{array}{r}{\\frac{\\tilde{\\lambda}_{\\tilde{l}+i}}{\\tilde{\\lambda}_{\\tilde{l}}}<\\frac{1}{i^{d}}}\\end{array}$ . So indeed $r_{l-1}=o(m)$ which implies $r_{l}=o(m)$ . For $\\tau_{m}=o(m^{-\\frac{1}{d-1}})$ and $\\tau_{m}=\\Theta(m^{-\\frac{1}{d-1}})$ , we will directly analyze Equation (2). For $\\begin{array}{r}{k=\\Theta(\\frac{1}{\\tau_{m}})}\\end{array}$ , we have that\u03bb\u02dc\u03bb\u02dci1 > 21 for all i \u2264k. Let Li =\u03bb\u02dci\u03bb\u02dc+i\u03ba0 . Note that $\\begin{array}{r}{\\mathcal{L}_{i}>\\frac{1}{2}\\mathcal{L}_{1}}\\end{array}$ for $i\\leq k$ . Note that ", "page_idx": 8}, {"type": "equation", "text": "$$\nm=\\sum_{i}N(d,i)\\mathcal{L}_{i}>\\frac{1}{2}\\mathcal{L}_{1}\\left(N(d,1)+\\cdot\\cdot\\cdot+N(d,k)\\right)>\\Theta\\left(\\left(\\frac{1}{\\tau_{m}}\\right)^{d-1}\\right)\\mathcal{L}_{1}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "So, we have that for all i that Li < L1 <\u0398 (\u03c41m )d\u22121 . From Equation (2), we have that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\tilde{R}(\\hat{f}_{0})=\\mathcal{E}_{0}\\sum_{i}N(d,i)(1-\\mathcal{L}_{i})^{2}\\beta_{i}^{2}+\\mathcal{E}_{0}\\sigma^{2}>\\mathcal{E}_{0}\\left(1-\\frac{m}{\\Theta\\left(\\left(\\frac{1}{\\tau_{m}}\\right)^{d-1}\\right)}\\right)^{2}\\|f^{*}\\|^{2}+\\mathcal{E}_{0}\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "For $\\tau_{m}\\;=\\;o(m^{\\frac{1}{d-1}})$ this is sufficient. Additionally, by a similar computation as above, in this case, $r_{1}=\\omega(m)$ , so $\\mathcal{E}_{0}$ is bounded by Equation (5). For $\\tau_{m}=\\Theta(m^{\\frac{1}{d-1}})$ , using Equation (5) and Equation (6), by showing that for $l=\\Theta(m)$ , $r_{l}=\\Theta(m)$ , we have $\\Theta(1)>\\mathcal{E}_{0}>1+\\Theta(1)$ . ", "page_idx": 8}, {"type": "text", "text": "Proof sketch of Theorem 7. Let Li =\u03bb\u02dci\u03bb\u02dc+i\u03ba0 . Note that (1 \u2212Li)2 = $\\begin{array}{r}{(1-\\mathcal{L}_{i})^{2}=\\frac{\\kappa_{0}^{2}}{(\\kappa_{0}+\\tilde{\\lambda}_{i})^{2}}}\\end{array}$ (\u03ba0\u03ba+0\u03bb\u02dci)2 . Then, Equation (2) can be rewritten and bounded as (with an abuse of notation for $\\beta_{i}$ ) ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\tilde{R}(\\hat{f}_{0})=\\mathcal{E}_{0}\\sum_{i}N(d,i)(1-\\mathcal{L}_{i})^{2}\\beta_{i}^{2}+\\mathcal{E}_{0}\\sigma^{2}\\leq\\mathcal{E}_{0}B^{2}\\sum_{i=1}^{l}N(d,i)\\frac{\\kappa_{0}^{2}}{(\\kappa_{0}+\\tilde{\\lambda}_{i})^{2}}+\\mathcal{E}_{0}\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Note that $\\begin{array}{r l r}{\\frac{\\kappa_{0}^{2}}{(\\kappa_{0}+\\tilde{\\lambda}_{i})^{2}}}&{{}<}&{\\frac{\\kappa_{0}^{2}}{\\tilde{\\lambda}_{i}^{2}}}\\end{array}$ and also $\\begin{array}{r l r}{\\mathcal{L}_{i}}&{{}\\le}&{\\frac{\\tilde{\\lambda}_{i}}{\\kappa_{0}}}\\end{array}$ Therefore, we have that $\\tilde{R}(\\hat{f}_{0})\\;\\;\\leq\\;$ $\\begin{array}{r}{\\mathcal{E}_{0}B^{2}\\kappa_{0}^{2}\\left(\\sum_{i=1}^{l}N(d,i)\\frac{1}{\\tilde{\\lambda}_{i}^{2}}\\right)+\\mathcal{E}_{0}\\sigma^{2}}\\end{array}$ . Note that $\\begin{array}{r}{m\\,=\\,\\sum_{i}N(d,i)\\mathcal{L}_{i}\\,<\\,\\sum_{i}N(d,i){\\frac{\\tilde{\\lambda}_{i}}{\\kappa_{0}}}\\,<\\,{\\frac{A}{\\kappa_{0}}}}\\end{array}$ , so $\\textstyle\\kappa_{0}\\,<\\,{\\frac{A}{m}}$ . Finally, to bound ${\\mathcal{E}}_{0}$ , note that in Equation (5) we can choose $k\\,=\\,L_{m}\\,<\\,m$ , then $r_{k}+k>U_{m}>m$ , so $\\begin{array}{r}{\\mathcal{E}_{0}\\leq\\left(1-\\frac{L_{m}}{m}\\right)^{-1}\\left(1-\\frac{m}{U_{m}}\\right)^{-1}}\\end{array}$ . Combining these with $\\textstyle\\kappa_{0}<{\\frac{A}{m}}$ gives the claim of Theorem 7. The proof of the alternative bound is harder and will be delayed to the appendix. ", "page_idx": 8}, {"type": "text", "text": "Proof sketch of Theorem 9. By Theorem 3 from Zhou et al. [66], if $k$ is the first $k<m$ such that $k+b r_{k}\\,\\geq\\,m$ , then $\\begin{array}{r}{\\mathcal{E}_{0}\\,\\geq\\,\\left(1-\\left(\\frac{b}{b+1}\\right)^{2}\\frac{k}{m}\\right)^{-1}}\\end{array}$ . Since $\\begin{array}{r}{\\operatorname*{max}_{i\\leq k_{m}}\\left(\\frac{1}{\\tilde{\\lambda}_{i}}\\right)\\;<\\;\\frac{m-L_{m}}{b}}\\end{array}$ , for $l~<$ $N(d,1)+\\cdot\\cdot+N(d,k_{m})$ , we have that $\\begin{array}{r}{b r_{l}+l\\leq N(d,1)+\\cdot\\cdot\\cdot+N(d,k_{m})\\!-\\!1\\!+\\!b\\operatorname*{max}_{i\\leq k_{m}}\\left(\\frac{1}{\\tilde{\\lambda}_{i}}\\right)<}\\end{array}$ $L_{m}+m-L_{m}\\leq m$ , so the first $l$ for which $r_{l}+l>m$ is $l=L_{m}=N(d,1)+\\cdot\\cdot\\cdot+N(d,k_{m})$ . Plugging in $k\\;=\\;L_{m}$ we get that $\\begin{array}{r}{\\mathcal{E}_{0}\\ \\geq\\ \\left(1-\\left(\\frac{b}{b+1}\\right)\\frac{L_{m}}{m}\\right)^{-1}}\\end{array}$ , so from Equation (6), we have $\\begin{array}{r}{\\tilde{R}(\\hat{f}_{0})\\geq\\left(1-\\left(\\frac{b}{b+1}\\right)^{2}\\frac{L_{m}}{m}\\right)^{-1}\\sigma^{2}.}\\end{array}$ ", "page_idx": 8}, {"type": "text", "text": "Proof sketch of Corollary 11. Note that an analogous proof holds for any $\\frac{d^{\\alpha}}{m}\\rightarrow c$ , for a constant $c$ , but we take equality for simplicity. If $k$ is a constant, i.e. it does not change with the dimension, we have that $N(\\dot{d},k)\\overset{\\cdot}{=}\\Theta(d^{k})$ . Therefore, $k_{m}=\\lfloor\\alpha\\rfloor$ if $\\alpha$ is a non-integer and $\\alpha-1$ if $\\alpha$ is an integer. If $\\alpha$ is a non-integer, then $L_{m}=N(d,k_{m})+\\cdot\\cdot\\cdot+N(d,1)=\\Theta(d^{\\lfloor\\alpha\\rfloor})$ and $U_{m}=N(d,k_{m}+1)+$ $\\cdot\\cdot+N(d,1)=\\Theta(d^{\\lfloor\\alpha\\rfloor+1})$ . So we have that $\\begin{array}{r}{\\frac{L_{m}}{m}=d^{\\lfloor\\alpha\\rfloor-\\alpha}}\\end{array}$ , $\\begin{array}{r}{\\frac{m}{L_{m}}=d^{\\alpha-\\lfloor\\alpha\\rfloor-1}}\\end{array}$ , $N(d,\\lfloor\\alpha\\rfloor)=d^{\\lfloor\\alpha\\rfloor}$ . Note that $k_{m}\\;=\\;\\lfloor\\alpha\\rfloor$ . We have from Assumption 10 that $\\begin{array}{r}{\\operatorname*{max}_{i\\leq k_{m}}\\frac{1}{\\tilde{\\lambda}_{i}}\\,=\\,O(N(d,k_{m}))}\\end{array}$ . Note that Theorem 7 holds with $\\left(\\sum_{i=1}^{l}N(d,i)\\frac{1}{\\tilde{\\lambda}_{i}}\\right)$ instead of $\\left(\\sum_{i=1}^{l}N(d,i)\\frac{1}{\\tilde{\\lambda}_{i}^{2}}\\right)$ . Therefore, we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{m^{2}}\\operatorname*{max}_{l\\leq k_{m}}\\left(\\frac{1}{\\tilde{\\lambda}_{i}}N(d,i)\\right)\\leq O(d^{(2\\lfloor\\alpha\\rfloor-2\\alpha)}).\\mathrm{~This~gives}}\\\\ &{\\qquad\\qquad\\qquad\\tilde{R}(\\hat{f}_{0})\\leq\\left(1-d^{\\lfloor\\alpha\\rfloor-\\alpha}\\right)^{-1}\\left(1-d^{\\alpha-\\lfloor\\alpha\\rfloor-1}\\right)^{-1}\\sigma^{2}}\\\\ &{\\qquad\\qquad\\qquad+\\,O\\left(B^{2}\\left(1-d^{\\lfloor\\alpha\\rfloor-\\alpha}\\right)^{-1}\\left(1-d^{\\alpha-\\lfloor\\alpha\\rfloor-1}\\right)^{-1}d^{(2\\lfloor\\alpha\\rfloor-2\\alpha)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Therefore, $\\begin{array}{r}{\\operatorname*{lim}_{m\\rightarrow\\infty}\\tilde{R}(\\hat{f}_{0})=\\sigma^{2}}\\end{array}$ . So, if $\\alpha$ is not an integer, we get benign overfitting. If $\\alpha$ is an integer, note that $N(d,1)+\\cdots+N(d,\\alpha-1)=o\\left(d^{\\alpha}\\right)$ , so $k_{m}=\\alpha$ . Then there are $c_{u},c_{l}\\in(0,1)$ such that $\\begin{array}{r}{c_{l}<\\frac{L_{m}}{m}\\leq c_{u}<1}\\end{array}$ . So, Theorem 9 holds for $b={\\textstyle{\\frac{1}{2}}}$ , so $\\begin{array}{r}{\\tilde{R}(\\hat{f}_{0})>\\frac{1}{1-\\frac{c_{1}}{9}}\\sigma^{2}}\\end{array}$ \u22121c1\u03c32. This shows that $\\begin{array}{r}{\\operatorname*{lim}\\operatorname*{inf}_{m\\to\\infty}\\tilde{R}(\\hat{f}_{0})\\,\\geq\\,\\frac{1}{1-\\frac{c_{1}}{9}}\\sigma^{2}>\\sigma^{2}}\\end{array}$ . The upper bound follows as above. Since we assumed $\\begin{array}{r}{\\operatorname*{max}_{i\\leq k_{m}}\\frac{1}{\\tilde{\\lambda}_{i}}\\,=\\,O(N(d,k_{m}))}\\end{array}$ , and $\\mathrm{max}_{i\\leq k_{m}}\\,N(d,i)\\,=\\,N(d,k_{m})\\,=\\,\\Theta(d^{\\alpha})$ , we have that $\\begin{array}{r}{\\sum_{i=1}^{k_{m}}N(d,i)\\frac{1}{\\tilde{\\lambda}_{i}}=\\Theta(d^{2\\alpha})}\\end{array}$ , so ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\tilde{R}(\\hat{f}_{0})\\leq\\left(1-c_{l}\\right)^{-1}\\left(1-d^{-1}\\right)^{-1}\\sigma^{2}+\\Theta\\left(B^{2}\\left(1-d^{-1}\\right)^{-1}\\right).\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "So, we conclude that for integer $\\alpha$ we have tempered overftiting and for non-integer $\\alpha$ we have benign overftiting. This recovers results of Ghorbani et al. [24], Mei et al. [38], Misiakiewicz [40], Barzilai and Shamir [4], Zhang et al. [64]. ", "page_idx": 9}, {"type": "text", "text": "Proof sketch of Corollary 12. Note that this holds for any scaling of $d$ and $m$ where $d=\\Theta(\\log m)$ , but we take this particular one for concreteness. As we show in the appendix (Theorem 21), it is not hard to see that $L_{m}\\approx\\alpha_{l}m$ for some constant $\\alpha_{l}\\,<\\,1$ and $L_{m}\\approx\\alpha_{u}m$ for some constant $\\alpha_{u}~>~1$ . Theorem 9 implies that we cannot get benign overfitting in this case, i.e. for all $m$ , $\\begin{array}{r}{\\tilde{R}(\\hat{f}_{0})>\\frac{1}{1-\\left(\\frac{b}{b+1}\\right)^{2}\\alpha_{l}}\\sigma^{2}}\\end{array}$ \u03b1l \u03c32. This shows that lim infm\u2192\u221eR\u02dc( f\u02c60) \u22651\u2212( b1)2\u03b1l . ", "page_idx": 9}, {"type": "text", "text": "Proof sketch of Corollary 13. We have that for the Gaussian kernel on the sphere, Theorem 7 holds with $A=1$ (see Appendix C.4 for further details). First, we will compute $k_{m}$ and hence $L_{m}$ and $U_{m}$ . After some tedious calculation (Theorem 23 in the appendix), we see that for $k\\leq2^{l}+l-1$ we have $N(d,1)+\\cdot\\cdot\\cdot+N(d,k)=o(m)$ , but for $k=2^{l}+\\dot{l}$ we have $N(d,1)+\\cdots+N(d,k)>$ $m$ . This shows that $k_{m}\\,=\\,2^{l}\\,+\\,l\\,-\\,1$ and so again after long calculations $\\begin{array}{r}{L_{m}\\,=\\,\\Theta\\big(\\frac{m}{\\log m}\\big)}\\end{array}$ and $U_{m}>\\Theta(m d^{0.89})$ . To estimate $\\textstyle\\sum_{i=1}^{k}N(d,i){\\tilde{\\lambda}}_{i}$ , note that eigenvalues are decreasing and $i N(d,i)=$ $\\begin{array}{r}{i N(d,i+1)\\frac{2i+d-2}{2i+d}\\frac{i+1}{i+d-2}=o(N(d,i+1))}\\end{array}$ (take $k\\ll d)$ , so it suffices to estimate $N(d,k)\\frac{1}{\\lambda_{k}}$ . Now, from Equation (7) and an estimate on the size of the first eigenvalue (Corollary 31) $\\begin{array}{r}{\\tilde{\\lambda}_{1}=\\Theta\\big(\\frac{1}{d^{\\alpha}}\\big)}\\end{array}$ , for fixed $\\alpha>0$ , we can estimate the size $\\tilde{\\lambda}_{k}$ . We have that ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\frac{1}{\\tilde{\\lambda}_{k}}<\\frac{1}{\\tilde{\\lambda}_{1}}\\left(\\tau_{m}\\right)^{k}\\left(\\frac{d}{2}+k+\\frac{2}{\\tau_{m}^{2}}\\right)^{k}<d^{k(1+\\varepsilon)},\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "for arbitrarily small $\\varepsilon$ . For $\\begin{array}{r}{k=\\frac{7k_{m}}{24}}\\end{array}$ , from additional calculation (Theorem 23 in the appendix), we have that m14 \u2264N(d, k) \u2264m31 and dk < m274 . So ik=m1 N(d, i) \u03bb\u02dc12 < m. Plugging these back into Theorem 7 gives the desired result. ", "page_idx": 9}, {"type": "text", "text": "6 Summary ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we considered the minimum norm interpolating solution of kernel ridge regression in fixed dimension with Gaussian kernel and varying or adaptively chosen bandwidth, and in increasing dimension with various kernels. In fixed dimension, we showed that if the source distribution is uniform on the sphere, then the minimum norm interpolating solution is inconsistent for any choice of bandwidth, and usually worse than the null predictor, except possibly in one particular scaling of bandwidth and with small noise. Furthermore, we showed a general upper and lower bound on the test risk, which we applied in the case of increasing dimension to recover the currently known results about polynomially increasing dimension and show two new results: We showed that no dot-product kernel on the sphere can achieve consistency for logarithmic scaling of the dimension, and obtained the first case of sub-polynomially scaling dimension where the minimum norm interpolating \u221asolution exhibits benign overftiting, namely with Gaussian kernel and dimension scaling as $d\\ {\\stackrel{\\cdot}{=}}\\ \\exp({\\sqrt{\\log m}})$ . ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We would like to thank Theodor Misiakiewicz and Sam Buchanan for useful discussions. ", "page_idx": 10}, {"type": "text", "text": "This work was done as part of the NSF-Simons funded collaboration on the Theoretical Foundations of Deep Learning (https://deepfoundations.ai), and primarily while GV was at TTIC. GV is supported by research grants from the Center for New Scientists at the Weizmann Institute of Science, and the Shimon and Golde Picker \u2013 Weizmann Annual Grant. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Francis Bach. High-dimensional analysis of double descent for linear regression with random projections. arXiv preprint arXiv:2303.01372, 2023. [2] Peter L. Bartlett and Philip M. Long. Failures of model-dependent generalization bounds for least-norm interpolation. The Journal of Machine Learning Research, 22(1):9297\u20139311, 2021.   \n[3] Peter L. Bartlett, Philip M. Long, G\u00e1bor Lugosi, and Alexander Tsigler. Benign overfitting in linear regression. Proceedings of the National Academy of Sciences, 117(48):30063\u201330070, 2020.   \n[4] Daniel Barzilai and Ohad Shamir. Generalization in kernel regression under realistic assumptions. arXiv preprint arXiv:2312.15995, 2023.   \n[5] Daniel Beaglehole, Mikhail Belkin, and Parthe Pandit. On the inconsistency of kernel ridgeless regression in fixed dimensions. arXiv preprint arXiv:2205.13525, 2023.   \n[6] Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. SIAM Journal on Mathematics of Data Science, 2(4):1167\u20131180, 2020.   \n[7] Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learn- ing curves in kernel regression and wide neural networks. International Conference on Machine Learning, PMLR, pages 1024\u20131034, 2020.   \n[8] Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks. Nature Communications, 12(1): 1\u201312, 2021.   \n[9] Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu. Towards understanding the spectral bias of deep learning. arXiv preprint arXiv:1912.01198, 2019.   \n[10] Yuan Cao, Quanquan Gu, and Mikhail Belkin. Risk bounds for over-parameterized maximum margin classification on sub-gaussian mixtures. In Advances in Neural Information Processing Systems (NeurIPS), 2021.   \n[11] Yuan Cao, Zixiang Chen, Mikhail Belkin, and Quanquan Gu. Benign overftiting in two-layer convolutional neural networks. arXiv preprint arXiv:2202.06526, 2022.   \n[12] Niladri S. Chatterji and Philip M. Long. Finite-sample analysis of interpolating linear classifiers in the overparameterized regime. Journal of Machine Learning Research, 22(129):1\u201330, 2021.   \n[13] Niladri S. Chatterji, Philip M. Long, and Peter L Bartlett. The interplay between implicit bias and benign overfitting in two-layer linear networks. arXiv preprint arXiv:2108.11489, 2021.   \n[14] Chen Cheng and Andrea Montanari. Dimension free ridge regression. arXiv preprint arXiv:2210.08571, 2022.   \n[15] Tin Sum Cheng, Aurelien Lucchi, Anastasis Kratsios, and David Belius. Characterizing overfitting in kernel ridgeless regression through the eigenspectrum. arXiv preprint arXiv:2402.01297, 2024.   \n[16] Geoffrey Chinot and Matthieu Lerasle. On the robustness of the minimum $\\ell_{2}$ interpolator. arXiv preprint arXiv:2003.05838, 2020.   \n[17] W. J. Conover. Practical Nonparametric Statistics. Wiley Series in Probability and Statistics, 1999.   \n[18] Konstantin Donhauser, Nicolo Ruggeri, Stefan Stojanovic, and Fanny Yang. Fast rates for noisy interpolation require rethinking the effect of inductive bias. In International Conference on Machine Learning (ICML), 2022.   \n[19] Mona Eberts and Ingo Steinwart. Optimal regression rates for svms using gaussian kernels. Electronic Journal of Statistics, page Vol. 7 1\u201342, 2013.   \n[20] Spencer Frei, Niladri S. Chatterji, and Peter L. Bartlett. Benign overfitting without linearity: Neural network classifiers trained by gradient descent for noisy linear data. In Conference on Learning Theory (COLT), 2022.   \n[21] Spencer Frei, Gal Vardi, Peter L Bartlett, and Nathan Srebro. Benign overfitting in linear classifiers and leaky relu networks from kkt conditions for margin maximization. arXiv preprint arXiv:2303.01462, 2023.   \n[22] Amnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, and Ronen Basri. On the similarity between the laplace and neural tangent kernels. arXiv preprint arXiv:2007.01580, 2020.   \n[23] Erin George, Michael Murray, William Swartworth, and Deanna Needell. Training shallow relu networks on noisy data using hinge loss: when do we overfit and is it benign? Advances in Neural Information Processing Systems, 36, 2024.   \n[24] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized two-layers neural networks in high dimension. The Annals of Statistics, 49(2):1029\u20131054, 2021.   \n[25] Nikhil Ghosh and Mikhail Belkin. A universal trade-off between the model size, test loss, and training loss of linear predictors. arXiv preprint arXiv:2207.11621, 2022.   \n[26] Moritz Haas, David Holzm\u00fcller, Ulrike von Luxburg, and Ingo Steinwart. Mind the spikes: Benign overfitting of kernels and neural networks in fixed dimension. 37th Conference on Neural Information Processing Systems (NeurIPS 2023), 2023.   \n[27] Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J. Tibshirani. Surprises in high-dimensional ridgeless least squares interpolation. arXiv peprint arXiv:1903.08560, 2020.   \n[28] Hong Hu, Yue M. Lu, and Theodor Misiakiewicz. Asymptotics of random feature regression beyond the linear scaling regime. arXiv preprint arXiv:2403.08160v1, 2023.   \n[29] Nirmit Joshi, Gal Vardi, and Nathan Srebro. Noisy interpolation learning with shallow univariate relu networks. arXiv preprint arXiv:2307.15396, 2023.   \n[30] Peizhong Ju, Xiaojun Lin, and Jia Liu. Overftiting can be harmless for basis pursuit, but only to a degree. Advances in Neural Information Processing Systems, 33:7956\u20137967, 2020.   \n[31] Kedar Karhadkar, Erin George, Michael Murray, Guido Mont\u00fafar, and Deanna Needell. Benign overftiting in leaky relu networks with moderate input dimension. arXiv preprint arXiv:2403.06903, 2024.   \n[32] Frederic Koehler, Lijia Zhou, Danica J Sutherland, and Nathan Srebro. Uniform convergence of interpolators: Gaussian width, norm bounds, and benign overfitting. arXiv preprint arXiv:2106.09276, 2021.   \n[33] Guy Kornowski, Gilad Yehudai, and Ohad Shamir. From tempered to benign overfitting in relu neural networks. Advances in Neural Information Processing Systems, 36, 2024.   \n[34] Yiwen Kou, Zixiang Chen, Yuanzhou Chen, and Quanquan Gu. Benign overfitting for two-layer relu networks. arXiv preprint arXiv:2303.04145, 2023.   \n[35] Yicheng Li, Zixiong Yu, Guhan Chen, and Qian Lin. Statistical optimality of deep wide neural networks. arXiv preprint arXiv:2305.02657, 2023.   \n[36] Tengyuan Liang and Benjamin Recht. Interpolating classifiers make few mistakes. arXiv preprint arXiv:2101.11815, 2021.   \n[37] Neil Mallinar, James B. Simon, Amirhesam Abedsoltan, Parthe Pandit, Mikhail Belkin, and Preetum Nakkiran. Benign, tempered, or catastrophic: A taxonomy of overfitting. arXiv preprint arXiv:2207.06569v2, 2022.   \n[38] Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Generalization error of random feature and kernel methods: hypercontractivity and kernel matrix concentration. Applied and Computational Harmonic Analysis, 59:3\u201384, 2022.   \n[39] Xuran Meng, Difan Zou, and Yuan Cao. Benign overftiting in two-layer relu convolutional neural networks for xor data. arXiv preprint arXiv:2310.01975, 2023.   \n[40] Theodor Misiakiewicz. Spectrum of inner-product kernel matrices in the polynomial regime and multiple descent phenomenon in kernel ridge regression. arXiv:2204.10425, 2022.   \n[41] Theodor Misiakiewicz and Basil Saeed. A non-asymptotic theory of kernel ridge regression: deterministic equivalents, test error, and gcv estimator. arXiv preprint arXiv:2403.08938, 2024.   \n[42] Andrea Montanari, Feng Ruan, Youngtak Sohn, and Jun Yan. The generalization error of max-margin linear classifiers: Benign overfitting and high dimensional asymptotics in the overparametrized regime. Preprint arXiv:1911.01544v3, 2023.   \n[43] Vidya Muthukumar, Kailas Vodrahalli, Vignesh Subramanian, and Anant Sahai. Harmless interpolation of noisy data in regression. IEEE Journal on Selected Areas in Information Theory, 2020.   \n[44] Vidya Muthukumar, Adhyyan Narang, Vignesh Subramanian, Mikhail Belkin, Daniel Hsu, and Anant Sahai. Classification vs regression in overparameterized regimes: Does the loss function matter? Journal of Machine Learning Research, 22(222):1\u201369, 2021.   \n[45] Jeffrey Negrea, Gintare Karolina Dziugaite, and Daniel Roy. In defense of uniform convergence: Generalization via derandomization with an application to interpolating predictors. In International Conference on Machine Learning, pages 7263\u20137272, 2020.   \n[46] Minh Ha Quang and Yuan Yao. Mercer\u2019s theorem, feature maps, and smoothing. Conference: Learning Theory, 19th Annual Conference on Learning Theory, COLT, 2006.   \n[47] Alexander Rakhlin and Xiyu Zhai. Consistency of interpolation with laplace kernels is a high-dimensional phenomenon. arXiv preprint arXiv:1812.11167, 2018.   \n[48] Javier Segura. Simple bounds with best possible accuracy for ratios of modified bessel functions. arXiv preprint arXiv:2207.02713v3, 2023.   \n[49] Ohad Shamir. The implicit bias of benign overftiting. In Conference on Learning Theory, pages 448\u2013478. PMLR, 2022.   \n[50] James B. Simon, Madeline Dickens, Dhruva Karkada, and Michael R. DeWeese. The eigenlearning framework: A conservation law perspective on kernel regression and wide neural networks. arXiv preprint arXiv:2110.03922, 2021.   \n[51] Ingo Steinwart and Andreas Christmann. Support vector machines. Springer Science & Business Media, 2008.   \n[52] Christos Thrampoulidis, Samet Oymak, and Mahdi Soltanolkotabi. Theoretical insights into multiclass classification: A high-dimensional asymptotic view. Advances in Neural Information Processing Systems, 33:8907\u20138920, 2020.   \n[53] A. Tsigler and P. L. Bartlett. Benign overftiting in ridge regression. arXiv peprint arXiv:2009.14286, 2020.   \n[54] Guillaume Wang, Konstantin Donhauser, and Fanny Yang. Tight bounds for minimum l1-norm interpolation of noisy data. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2022.   \n[55] Ke Wang and Christos Thrampoulidis. Binary classification of gaussian mixtures: Abundance of support vectors, benign overfitting and regularization. arXiv peprint arXiv:2011.09148, 2021.   \n[56] Ke Wang, Vidya Muthukumar, and Christos Thrampoulidis. Benign overftiting in multiclass classification: All roads lead to interpolation. In Advances in Neural Information Processing Systems (NeurIPS), 2021.   \n[57] Alexander Wei, Wei Hu, and Jacob Steinhardt. More than a toy: Random matrix models predict how realworld neural representations generalize. In International Conference on Machine Learning, Proceedings of Machine Learning Research, 2022.   \n[58] Denny Wu and Ji Xu. On the optimal weighted $\\ell_{2}$ regularization in overparameterized linear regression. Advances in Neural Information Processing Systems, 33:10112\u201310123, 2020.   \n[59] Lechao Xiao, Hong Hu, Theodor Misiakiewicz, Yue M. Lu, and Jeffrey Pennington. Precise learning curves and higher-order scaling limits for dot product kernel regression. arXiv preprint: arXiv:2205.14846v3, 2023.   \n[60] Xingyu Xu and Yuantao Gu. Benign overfitting of non-smooth neural networks beyond lazy training. In International Conference on Artificial Intelligence and Statistics, pages 11094\u201311117. PMLR, 2023.   \n[61] Zhiwei Xu, Yutong Wang, Spencer Frei, Gal Vardi, and Wei Hu. Benign overfitting and grokking in relu networks for xor cluster data. arXiv preprint arXiv:2310.02541, 2023.   \n[62] Zhen-Hang Yang and Yu-Ming Chu. On approximating the modified bessel function of the first kind and toader-qi mean. Journal of Inequalities and Applications, 2016.   \n[63] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations, 2017.   \n[64] Haobo Zhang, Weihao Lu, and Qian Lin. The phase diagram of kernel interpolation in large dimensions. arXiv preprint arXiv:2404.12597v1, 2024.   \n[65] Lijia Zhou, Frederic Koehler, Pragya Sur, Danica J Sutherland, and Nathan Srebro. A non-asymptotic moreau envelope theory for high-dimensional generalized linear models. arXiv preprint arXiv:2210.12082, 2022.   \n[66] Lijia Zhou, James B Simon, Gal Vardi, and Nathan Srebro. An agnostic view on the cost of overftiting in (kernel) ridge regression. arXiv preprint arXiv:2306.13185, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Empirical Justification ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In Figure 1, Figure 2, and Figure 3 we provide empirical justification for the fixed dimension case considered in Theorem 3. The code to reproduce these experiments can be found at ", "page_idx": 14}, {"type": "text", "text": "https://github.com/marko-medvedev/overfitting-behavior-of-gaussian-kernel-ridgeless-regression. ", "page_idx": 14}, {"type": "text", "text": "We ran the experiments on one A6000 GPU. ", "page_idx": 14}, {"type": "image", "img_path": "7Sh0XkN1KS/tmp/35802bbf54bbb33f6b812be08de2aa6e079dc04e980f3e7b51be184a8f9b4bb6.jpg", "img_caption": ["(a) $\\tau_{m}=o(m^{-\\frac{1}{d-1}})$ with $\\sigma^{2}=1$ and $d=6$ . We see that the test error tends to something that is equal to or larger than the test risk of the null predictor. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "7Sh0XkN1KS/tmp/23a380dc06a6b208d7ac338ec6aff00db9675b977695f24cedc609e8a1130dfd.jpg", "img_caption": ["(b) $\\tau_{m}=\\omega(m^{-\\frac{1}{d-1}})$ with $\\sigma^{2}=10$ and $d=4$ . We see that the test error is increasing with the sample size $m$ , suggesting that the test error diverges to infinity. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "7Sh0XkN1KS/tmp/a61098e7e739d73cde69ddae804a87caa34eb1cba07da0acb52f6d089ef2c28d.jpg", "img_caption": ["(c) $\\tau_{m}=\\Theta(m^{-\\frac{1}{d-1}})$ with $\\sigma^{2}=10000$ and $d=6$ . We see that the test error is well above the risk of the null predictor. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 1: Here we plot the test error for 100 different runs of the experiment and their mean test error. We see that the test error concentrates around its mean and that the behavior is as predicted by Theorem 3. ", "page_idx": 14}, {"type": "text", "text": "Using the setup of Theorem 3 in the paper, we plot the dependence of the test error on the sample size for the Gaussian kernel ridgeless predictor. We consider $y=f^{*}(x)+\\xi$ where $\\xi\\sim N(0,\\dot{\\sigma}^{2})$ , $f^{*}=10$ , $\\sigma^{2}$ is the noise level, and $x\\sim\\mathrm{Unif}(S^{d-1})$ . We compare the test error with the noise level (the Bayes risk) and the risk of the null predictor. We see that for all three cases, our predictions agree with the experiments. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "image", "img_path": "7Sh0XkN1KS/tmp/b20aeb6976600c22485e2bbf4feaaaeeda3d9572469d9698f036cc723bec49ed.jpg", "img_caption": ["(a) $\\tau_{m}=o(m^{-\\frac{1}{d-1}})$ with ${\\sigma}^{2}=1$ and $d=6$ . "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "7Sh0XkN1KS/tmp/b8800802df6757748417efca93a43385d1473e8b9f48728836987db2ce3c155d.jpg", "img_caption": ["(b) $\\tau_{m}=\\omega(m^{-\\frac{1}{d-1}})$ with $\\sigma^{2}=10\\$ and $d=4$ . "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "7Sh0XkN1KS/tmp/b56c5a5ba54533d51f0e6d9649f77e9aad60524f29babedf36dc1844056064cb.jpg", "img_caption": ["Figure 2: We plot the estimates for the minimum and maximum of mean test error $L_{\\mathrm{low}}$ and $L_{\\mathrm{{high}}}$ for a given sample size. We test whether $[L_{\\mathrm{low}},L_{\\mathrm{high}}]$ contains $p=0.8$ of the total mass of the mean test error $L$ . ", "(c) $\\tau_{m}=\\Theta(m^{-\\frac{1}{d-1}})$ with $\\sigma^{2}=10000$ and $d=6$ . "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Statistical significance of experimental results In Figure 2 and Figure 3, we present evidence for the statistical significance of our experimental results. First, we the mean test loss for a test size of given size, $L$ . We use 50 of the 100 experiments to estimate an interval $[L_{\\mathrm{low}},L_{\\mathrm{high}}]$ and the other 50 to test whether this interval contains $p=0.8$ of the total mass of the distribution of $L$ at $\\alpha=0.05$ significance level. In Figure 2, we plot $L_{\\mathrm{low}}$ and $L_{\\mathrm{{high}}}$ . We perform one lower-tailed and one upper-tailed test [17]. The null hypotheses are: $\\textstyle1-{\\frac{1-p}{2}}$ population quantile is no greater than $L_{\\mathrm{{high}}}$ and $\\scriptstyle{\\frac{1-p}{2}}$ population quantile is at least as great as $L_{\\mathrm{low}}$ . In Figure 3, we report the relevant test statistics, $\\bar{T_{1}}$ which is the number of draws of $L$ no greater than $L_{\\mathrm{{high}}}$ and $T_{2}$ which is the number of draws of $L$ less than $L_{\\mathrm{low}}$ . We report an aggregated $\\mathbf{p}$ -value. We accept the null hypothesis in both cases - the test statistics $T_{1}$ and $T_{2}$ satisfy the required conditions and the p-values are high. ", "page_idx": 15}, {"type": "image", "img_path": "7Sh0XkN1KS/tmp/591c50e5426a4ca1af45d1af935269c38f396b27f369a72ab9501e01491799ef.jpg", "img_caption": ["Figure 3: We show the relevant test statistics and p-values for the upper-tailed and lower-tailed tests for the largest values of sample size $m$ . "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "B The case of fixed dimension ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Cost of overfitting The cost of overftiting, introduced in [66], is defined as the ratio of the test risk of an interpolating solution and the test risk of the optimally regularized solution. It measures how worse off are we interpolating rather than regularizing. ", "page_idx": 17}, {"type": "text", "text": "Definition 15 (Cost of overfitting). Let $\\hat{f}_{\\delta^{*}}$ be the optimally regularized solution, i.e. ", "page_idx": 17}, {"type": "equation", "text": "$$\nR(\\hat{f}_{\\delta^{*}})=\\operatorname*{inf}_{\\delta\\geq0}R(\\hat{f}_{\\delta}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For a distribution $\\mathcal{D}$ over $\\mathcal{X}\\times\\mathbb{R}$ , we will define the cost of overfitting $C({\\cal D},m)$ as ", "page_idx": 17}, {"type": "equation", "text": "$$\nC(\\mathcal{D},m)=\\frac{R(\\hat{f}_{0})}{R(\\hat{f}_{\\delta^{*}})}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Zhou et al. [66] show that agnostically to the distribution $\\mathcal{D}$ , the behavior of the cost of overftiting is tightly controlled by the effective ranks of systems of eigenvalue $\\{\\lambda_{i}\\}_{i=1}^{\\infty}$ of a given kernel (coming from Mercer\u2019s theorem) ", "page_idx": 17}, {"type": "equation", "text": "$$\nr_{k}:=\\frac{\\sum_{i=k+1}^{\\infty}\\lambda_{i}}{\\lambda_{k+1}}\\ \\ \\mathrm{and}\\ \\ R_{k}:=\\frac{\\left(\\sum_{i=k+1}^{\\infty}\\lambda_{i}\\right)^{2}}{\\sum_{i=k+1}^{\\infty}\\lambda_{i}^{2}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We will assume additionally that the eigenvalues are nonzero, so $r_{k}$ and $R_{k}$ are well defined on $\\mathbb{N}$ Since we chose that the eigenvalues are sorted and since they square summable, we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\nr_{k}\\le R_{k}\\le r_{k}^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The following two results of [66] summarize how we will use effective ranks to control the cost of overfitting. Proposition 16 gives a necessary and sufficient condition for the overfitting to benign in terms of the cost of overfitting, i.e. that $C(\\mathcal{D},m)\\to1$ , in terms of effective ranks. ", "page_idx": 17}, {"type": "text", "text": "Proposition 16 (Necessary and sufficient condition for benign overfitting [66]). Let $k_{n}$ be the smallest integer $k\\in\\mathbb{N}\\cup\\{0\\}$ for which $n<k+r_{k}$ . Then $\\mathcal{E}_{0}\\to1$ if and only if ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\rightarrow\\infty}{\\frac{k_{n}}{n}}=0\\,\\,\\,\\,\\,{\\mathrm{and}}\\,\\,\\,\\,\\,\\operatorname*{lim}_{n\\rightarrow\\infty}{\\frac{n}{R_{k_{n}}}}=0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proposition 17 gives a sufficient condition for $\\tilde{R}(\\hat{f}_{0})\\to\\infty$ in terms of effective ranks. ", "page_idx": 17}, {"type": "text", "text": "Proposition 17 (Sufficient condition for catastrophic overftiting [66]). Let $\\varepsilon>0$ and let $k=(1{+}\\varepsilon)m$ . $\\begin{array}{r}{\\operatorname*{lim}_{k\\to\\infty}\\frac{r_{k}}{k}=0}\\end{array}$ then $\\mathcal{E}_{0}\\to\\infty$ , i.e. the overfitting is catastrophic. ", "page_idx": 17}, {"type": "text", "text": "Using sharp bounds on eigenvalues $\\{\\lambda_{i}\\}_{i=1}^{\\infty}$ , derived in Theorem 28, we can derive bounds on effective ranks so that we can apply the results of [66] and Equation (2) to bound the test risk of minimum norm interpolant. ", "page_idx": 17}, {"type": "text", "text": "When the input is distributed uniformly on the sphere, the eigenfunctions $\\{\\phi_{k}\\}$ in the Mercer decomposition of the kernel function can be taken to be the spherical harmonics $Y_{k s}$ . Using the FunkHecke formula we can find the closed form of eigenvalues for the Gaussian kernel [46], which are given in terms of bandwidth $\\tau_{m}$ and modified Bessel function of the first kind $I_{v}(x)$ . In Appendix D, we will derive bounds on the sizes of the eigenvalues using the properties of Bessel functions and their multiplicities. ", "page_idx": 17}, {"type": "text", "text": "We need to take a particular input distribution, in order to able to compute the eigenvalues explicitly and to know their multiplicities. The result of Cao et al. [9] offers one possible way of generalizing our result to more general different distributions ", "page_idx": 17}, {"type": "text", "text": "We split the proof into two parts. First, we will bound the cost of overfitting. Then, we will provide lower bounds on the test risk of Gaussian KRR. ", "page_idx": 17}, {"type": "text", "text": "Bounds on cost of overfitting The bounds on cost of overftiting will be used to show whether the test risk is bounded above or away from Bayes risk. ", "page_idx": 18}, {"type": "text", "text": "Lemma 18 (Cost of overfitting for Gaussian KRR). Let $\\mathcal{X}\\sim\\mathrm{Unif}(S^{d-1})$ . Then the following bounds hold for the minimum norm interpolating solution of KRR and its cost of overftiting $C({\\cal D},m)$ and $\\mathcal{E}_{0}$ : ", "page_idx": 18}, {"type": "text", "text": "1. If $\\tau_{m}\\leq m^{-\\frac{1}{d-1}}t(m)$ , where $t(m)\\rightarrow0$ as $m\\rightarrow\\infty$ then there is a constant $C_{I}$ that depends on $d$ and $C$ , such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n1\\leq C(\\mathscr{D},m)\\leq\\mathscr{E}_{0}\\leq\\left(1-\\frac{1}{m}\\right)^{-2}\\left(1-C_{I}t(m)^{\\frac{d-1}{2}}\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "2. If $\\tau_{m}\\geq m^{-\\frac{1}{d-1}}t(m)$ , where $t(m)\\to\\infty$ as $m\\rightarrow\\infty$ then there is a integer $m_{0}$ depending on $d$ and $t(m)$ such that for all $m\\geq m_{0}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{E}_{0}\\geq\\sqrt{m}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "3. If $C_{1}m^{-\\frac{1}{d-1}}\\leq\\tau_{m}\\leq C_{2}m^{-\\frac{1}{d-1}}$ then there exist constants $C_{I}>0$ and $C_{I I}$ depending on $C_{1},C_{2}$ and $d$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{C(\\mathcal{D},m)\\leq\\mathcal{E}_{0}\\leq C_{I}}}\\\\ {{\\mathcal{E}_{0}\\geq1+C_{I I}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. We will break down the proof into the proof of the three parts. ", "page_idx": 18}, {"type": "text", "text": "Part 1: We will use Proposition 16 to prove this that $\\mathcal{E}_{0}\\to1$ . We can see the upper bound on ${\\mathcal{E}}_{0}$ from Theorem 2 [66]. We have that for $k<m$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{E}_{0}\\leq\\left(1-\\frac{k}{m}\\right)^{-2}\\left(1-\\frac{m}{R_{k}}\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now we will show that for k = 1, we have that k + rk > m and limm\u2192\u221eRm . This implies that conditions of Proposition 16 hold so $\\mathcal{E}_{0}\\to1$ . We will rewrite $\\begin{array}{r}{r_{k}=\\frac{\\sum_{i>k}\\lambda_{k}}{\\lambda_{k+1}}}\\end{array}$ using $\\tilde{\\lambda}_{i}$ . We have that ", "page_idx": 18}, {"type": "equation", "text": "$$\nr_{1}=\\frac{\\sum_{i>1}\\lambda_{i}}{\\lambda_{2}}=\\tilde{N}(d,\\tilde{1})+\\sum_{j=1}^{\\infty}N(d,\\tilde{1}+j)\\frac{\\tilde{\\lambda}_{\\tilde{1}+j}}{\\tilde{\\lambda}_{i_{1}}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $N(d,1)=d$ , so $\\tilde{1}=1$ and $\\tilde{N}(d,\\tilde{1})=d-1$ . Therefore, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nr_{1}=(d-1)+\\sum_{j=1}^{\\infty}N(d,1+j)\\frac{\\tilde{\\lambda}_{1+j}}{\\tilde{\\lambda}_{1}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let $\\begin{array}{r}{T=\\frac{2}{\\tau_{m}^{2}}}\\end{array}$ . Takin g T g2(m)\u2264j \u2264 Tg(m), k = 1, we can apply the bound Theorem 28 on $\\frac{\\tilde{\\lambda}_{1+j}}{\\tilde{\\lambda}_{1}}$ Therefore, we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\exp(-5g(m))<\\frac{\\tilde{\\lambda}_{1+j}}{\\tilde{\\lambda}_{1}}<1.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In particular, we have that for $m$ large enough, for all such $j$ it holds that $\\begin{array}{r}{\\frac{\\tilde{\\lambda}_{1+j}}{\\tilde{\\lambda}_{1}}>\\frac{1}{2}}\\end{array}$ . Going back to $r_{1}$ , this means that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{r_{1}=(d-1)+\\displaystyle\\sum_{j=1}^{\\infty}N(d,1+j)\\frac{\\tilde{\\lambda}_{1+j}}{\\tilde{\\lambda}_{1}}}}\\\\ {{\\displaystyle~~~~~\\times\\sum_{j=1}^{\\sqrt{T}_{g(m)}}N(d,1+j)\\frac{\\tilde{\\lambda}_{1+j}}{\\tilde{\\lambda}_{1}}>\\sum_{j=\\frac{1}{2}\\sqrt{T}g(m)}^{\\sqrt{T}_{g(m)}}~(1+j)^{d-2}\\frac{1}{2}}}\\\\ {{\\displaystyle~~~~~~\\times\\sum_{j=\\frac{1}{2}\\sqrt{T}g(m)}^{\\sqrt{T}_{g(m)}}~\\frac{1}{2^{d+1}}(\\sqrt{T}g(m))^{d-2}>\\frac{1}{2}\\sqrt{T}g(m)\\frac{1}{2^{d+1}}(\\sqrt{T}g(m))^{d-2}}}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}}\\\\ {{>\\frac{1}{2}(\\sqrt{T}g(m))^{d-1}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "If ${\\sqrt{T}}g(m)$ is not an integer, we can take its integer part and add a small constant since ${\\sqrt{T}}g(m)$ will be large. Note that $\\tau_{m}\\leq m^{-\\frac{1}{d-1}}t(m)$ , so ", "page_idx": 19}, {"type": "equation", "text": "$$\nT\\geq m^{\\frac{2}{d-1}}{\\frac{1}{t(m)^{2}}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore ", "page_idx": 19}, {"type": "equation", "text": "$$\nr_{1}>\\frac{1}{2}(\\sqrt{T}g(m))^{d-1}>\\frac{1}{2}\\left(m^{\\frac{1}{d-1}}\\frac{g(m)}{t(m)}\\right)^{d-1}=m^{1}\\left(\\frac{g(m)}{t(m)}\\right)^{d-1},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "so as long as $\\frac{g(m)}{t(m)}\\,\\rightarrow\\,\\infty$ , $r_{1}>m$ for $m$ large enough. We can take $g(m)\\,=\\,{\\sqrt{t(m)}}$ . Then, in Proposition 16, we can select $k_{m}=1$ , since $1+r_{1}>m$ for large enough $m$ . Note also that $R_{1}\\geq r_{1}$ , so we also have that R1 \u2265r1 > m1  gt((mm)) $\\begin{array}{r}{R_{1}\\geq r_{1}>m^{1}\\left(\\frac{g(m)}{t(m)}\\right)^{d-1}}\\end{array}$ , so ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\to\\infty}{\\frac{1}{m}}=0\\;{\\mathrm{and}}\\;\\operatorname*{lim}_{m\\to\\infty}{\\frac{m}{R_{1}}}=0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This finishes the proof of the first claim. ", "page_idx": 19}, {"type": "text", "text": "Part 2: Let $\\tau_{m}\\geq m^{-\\frac{1}{d-1}}t(m)$ , where $t(m)\\rightarrow\\infty$ as $m\\rightarrow\\infty$ . We will use Proposition 17 to prove this claim. Using Theorem 5 from [66] we know that for any $\\varepsilon>0$ if $r_{k}=o(m)$ for $k=(1+\\varepsilon)m$ then the following bound holds ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{E}_{0}\\geq\\frac{1}{1-\\frac{m}{k}\\left(\\frac{k-m}{k-m+r_{k}}\\right)}>1+\\frac{1}{\\varepsilon}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We will show that there is a constant $U>0$ such that for any $k\\ge m$ , we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\nr_{k}\\leq U m\\left(\\frac{1}{t(m)}\\right)^{\\frac{d-1}{2}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By Theorem 28, we have that for $j\\geq\\sqrt{T}g(m)$ where $g(m)\\rightarrow\\infty$ as $m\\rightarrow\\infty$ that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\tilde{\\lambda}_{\\tilde{k}+j}}{\\tilde{k}}<\\exp\\left(-\\frac{g(m)^{2}}{4}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let $k=(1+\\varepsilon)m$ . We have that ", "page_idx": 19}, {"type": "equation", "text": "$$\nr_{k}=\\tilde{N}(d,\\tilde{k})+\\sum_{j=1}^{\\infty}N(d,\\tilde{k}+j)\\frac{\\tilde{\\lambda}_{\\tilde{k}+j}}{\\tilde{\\lambda}_{\\tilde{k}}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now, if we take $j\\geq j_{0}=\\sqrt{T}g(m)$ , it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\tilde{\\lambda}_{\\tilde{k}+j}}{\\tilde{\\lambda}_{\\tilde{k}}}<\\exp(-\\frac{g(m)^{2}}{4}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r_{k}=\\tilde{N}(d,\\tilde{k})+\\displaystyle\\sum_{j=1}^{\\infty}N(d,\\tilde{k}+j)\\frac{\\tilde{\\lambda}_{\\tilde{k}+j}}{\\tilde{\\lambda}_{\\tilde{k}}}}\\\\ &{\\quad=\\tilde{N}(d,\\tilde{k})+\\displaystyle\\sum_{j=1}^{j-1}N(d,\\tilde{k}+j)\\frac{\\tilde{\\lambda}_{\\tilde{k}+j}}{\\tilde{\\lambda}_{\\tilde{k}}}+\\displaystyle\\sum_{j=j}^{\\infty}N(d,\\tilde{k}+j)\\frac{\\tilde{\\lambda}_{\\tilde{k}+j}}{\\tilde{\\lambda}_{\\tilde{k}}}}\\\\ &{\\quad\\leq n_{u}j^{d-1}+\\displaystyle\\sum_{j=j}^{\\infty}N(d,\\tilde{k}+j)\\frac{\\tilde{\\lambda}_{\\tilde{k}+j}}{\\tilde{\\lambda}_{\\tilde{k}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that by Theorem 28, for $j\\geq l\\sqrt{T}g(m)$ we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\tilde{\\lambda}_{\\tilde{k}+j}}{\\tilde{k}}<\\exp\\left(-\\frac{l^{2}g(m)^{2}}{4}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note also that this bounds holds for all $l$ simultaneously. Therefore, we can write ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r_{k}\\leq n_{u}\\beta^{d-1}+\\displaystyle\\sum_{j=1}^{N}N(d,\\bar{k}+j)\\displaystyle\\frac{\\bar{\\lambda}_{\\bar{k}+j,j}}{\\bar{\\lambda}_{\\bar{k}}}}\\\\ &{\\qquad=n_{u}\\beta^{d-1}+\\displaystyle\\sum_{l=1}^{N}\\sum_{u=1}^{j}N(d,l)+s\\displaystyle\\frac{\\bar{\\lambda}_{\\bar{k}+j,l+j+s}}{\\bar{\\lambda}_{\\bar{k}}}}\\\\ &{\\qquad\\leq n_{u}\\beta^{d-1}+\\displaystyle\\sum_{l=1}^{N}\\sum_{u=1}^{N}N(d,(l+1)j)\\displaystyle\\frac{\\bar{\\lambda}_{\\bar{k}+l,j}}{\\bar{\\lambda}_{\\bar{k}}}}\\\\ &{\\qquad\\leq n_{u}\\beta^{d-1}+\\displaystyle\\sum_{j=1}^{N}(l+1)^{d-1}j^{d-1}\\exp(-\\frac{l^{2}(g n)^{2}}{4})}\\\\ &{\\qquad\\leq m\\left(\\displaystyle\\frac{\\beta(m)}{\\bar{t}(m)}\\right)^{d-1}\\left(n_{u}+\\displaystyle\\sum_{l=1}^{N}(l+1)^{d-1}\\exp(-\\frac{l^{2}}{4})\\right)}\\\\ &{\\qquad\\leq U m\\left(\\displaystyle\\frac{\\beta(m)}{\\bar{t}(m)}\\right)^{d-1}=o(m).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Second to last inequality is true because $g(m)\\rightarrow\\infty$ as $m\\rightarrow\\infty$ . The last inequality follows from the fact that $\\begin{array}{r}{\\sum_{l=1}^{\\infty}(l+1)^{d-1}\\exp(-\\frac{l^{2}}{4})}\\end{array}$ is bounded by a constant. Note that the bound is independent of $k$ , so we can vary $\\varepsilon$ with $m$ as well. If we choose $\\textstyle\\varepsilon={\\frac{1}{\\sqrt{m}}}$ \u221a1m, the desired result follows. ", "page_idx": 20}, {"type": "text", "text": "Part 3: Let $C_{1}m^{-\\frac{1}{d-1}}\\leq\\tau_{m}\\leq C_{2}m^{-\\frac{1}{d-1}}$ . Note that the following two bounds hold for $\\mathcal{E}_{0}$ [66]: ", "page_idx": 20}, {"type": "text", "text": "1. For $k<m$ such that $r_{k}+k>m$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{E}_{0}\\leq\\left(1-\\frac{k}{m}\\right)^{-1}\\left(1-\\frac{m}{k+r_{k}}\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "2. For $k\\geq m$ it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{1}{m}\\sum_{i}\\mathcal{L}_{i,\\delta}\\geq\\frac{m}{k}\\left(\\frac{k-m}{k-m+r_{k}}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that in this case $\\begin{array}{r}{c_{1}^{\\prime}m^{\\frac{1}{d-1}}\\leq\\sqrt{T}=\\sqrt{\\frac{2}{\\tau_{m}^{2}}}\\leq c_{2}^{\\prime}m^{\\frac{1}{d-1}}}\\end{array}$ , where $\\begin{array}{r}{c_{2}^{\\prime}=\\sqrt{\\frac{2}{C_{1}^{2}}}}\\end{array}$ (and similarly for indices 1 and 2 swapped with inequalities revers\u221aed). Theorem 28 shows that for $i$ is such that $i\\leq(k{\\sqrt{T}})$ , we have that for $\\tilde{\\lambda}_{i}$ it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\tilde{\\lambda}_{i}}{\\tilde{\\lambda}_{1}}\\geq\\left(\\frac{1}{1+\\frac{k}{\\sqrt{T}}}\\right)^{k\\sqrt{T}}\\geq\\exp(-k^{2}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Take $k=(1+\\varepsilon)m$ . We want to show that there is a constant $B>0$ that depends on the dimension $d$ and the constant $C$ such that ", "page_idx": 21}, {"type": "equation", "text": "$$\nr_{k}\\leq B m.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let $\\tilde{k}=((1+\\varepsilon)m)^{\\frac{1}{d-1}}$ . Note that by an analogous proof to Theorem 28, we have that for the size of eigenvalues it holds ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\tilde{\\lambda}_{\\tilde{k}+j}}{\\tilde{\\lambda}_{\\tilde{k}}}\\leq\\left(\\frac{T}{T+(\\tilde{k}+j-1+\\frac{d}{2}-\\frac{1}{2})}\\right)^{\\frac{j-1}{2}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For $l\\tilde{k}\\leq j\\leq(l+1)\\tilde{k}$ , we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\tilde{\\lambda}_{\\tilde{k}+j}}{\\tilde{\\lambda}_{\\tilde{k}}}\\le\\left(\\frac{T}{T+(\\tilde{k}+j-1+\\frac{d}{2}-\\frac{1}{2})}\\right)^{\\frac{j-1}{2}}<\\left(\\frac{T}{T+\\left((l+1)\\tilde{k}\\right)}\\right)^{\\frac{l\\tilde{k}-1}{2}}<\\left(\\frac{T}{T+\\left((l+1)\\tilde{k}\\right)}\\right)^{\\frac{(l+1)\\tilde{k}}{4}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We have that $\\begin{array}{r}{\\tilde{k}=\\frac{1}{c^{\\prime}}(1\\!+\\!\\varepsilon)^{\\frac{1}{d-1}}c^{\\prime}m^{\\frac{1}{d-1}}=\\frac{1}{c^{\\prime}}(1\\!+\\!\\varepsilon)^{\\frac{1}{d-1}}\\sqrt{T}=\\alpha\\sqrt{T}}\\end{array}$ for $\\begin{array}{r}{\\alpha=\\frac{1}{c^{\\prime}}(1\\!+\\!\\varepsilon)^{\\frac{1}{d-1}}}\\end{array}$ . Therefore, we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\frac{{\\tilde{\\lambda}}_{{\\tilde{k}}+j}}{{\\tilde{\\lambda}}_{{\\tilde{k}}}}}<\\left({\\frac{T}{T+((l+1){\\tilde{k}})}}\\right)^{\\frac{(l+1){\\tilde{k}}}{4}}=\\left({\\frac{T}{T+((l+1)\\alpha{\\sqrt{T}})}}\\right)^{\\frac{(l+1)\\alpha{\\sqrt{T}}}{4}}\\to\\exp\\left(-{\\frac{(l+1)^{2}\\alpha^{2}}{4}}\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "as $m\\rightarrow\\infty$ i.e. $\\sqrt{T}\\to\\infty$ . Therefore, for $m$ large enough, for all $l$ we have that if $l\\tilde{k}\\leq j\\leq(l+1)\\tilde{k}$ then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\tilde{\\lambda}_{\\tilde{k}+j}}{\\tilde{\\lambda}_{\\tilde{k}}}\\leq\\frac{1}{(l+1)^{d+2}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "tThhaen .e qTuhaelnit yw ies  htrauvee  bthecata ufsoer  awlle can take $m$ large enough so that $\\alpha\\sqrt{T}$ and $\\frac{\\sqrt{T}}{\\alpha}$ are both greater $400d$ $l$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\tilde{\\lambda}_{\\tilde{k}+j}}{\\tilde{\\lambda}_{\\tilde{k}}}<\\left(\\frac{T}{T+((l+1)\\alpha\\sqrt{T})}\\right)^{\\frac{(l+1)\\alpha\\sqrt{T}}{4}}<\\left(\\frac{1}{1+\\frac{l+1}{100d}}\\right)^{(l+1)100d}<\\frac{1}{(l+1)^{d+2}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The last inequality holds because of the following ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left(1+{\\frac{l+1}{100d}}\\right)^{(l+1)100d}>\\left({\\frac{l+1}{100d}}\\right)^{d+2}\\left({l+1})100d\\right)>(l+1)^{d+2}{\\frac{1}{(100d)^{d+2}}}(100d)^{d+2}>(l+1)^{(l+2)10d}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, we can bound $r_{k}$ as follows ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle r_{k}=\\frac{\\sum_{i\\ge k}\\lambda_{k}}{\\lambda_{k}}}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\end{array}\\right.=\\bar{N}(d,\\tilde{k})+\\sum_{j=1}^{\\infty}N(d,\\tilde{k}+j)\\frac{\\tilde{\\lambda}_{\\tilde{k}+j}}{\\tilde{\\lambda}_{\\tilde{k}}}}\\\\ {\\displaystyle}\\\\ {\\displaystyle<N(d,\\tilde{k})+\\sum_{j=1}^{\\infty}N(d,\\tilde{k}+j)\\frac{\\tilde{\\lambda}_{\\tilde{k}+j}}{\\tilde{\\lambda}_{\\tilde{k}}}<(\\tilde{k})^{d-2}+\\sum_{l=1}^{\\infty}((l+1)\\tilde{k})^{d-2}(\\tilde{k})\\frac{1}{(l+1)^{d+2}}}\\\\ {\\displaystyle<\\tilde{k}^{d-1}\\left(\\sum_{l=1}^{\\infty}\\frac{1}{(l+1)^{4}}\\right)<\\tilde{k}^{d-1}C=C(1+\\varepsilon)m.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": ",a isom  it1 ,f olwleo whsa tvhea tt $k\\,-\\,m\\,+\\,r_{k}\\;<\\;(C(1+\\varepsilon)\\,+\\,\\varepsilon)m$ , so we have that $\\begin{array}{r}{\\frac{k-m}{k-m+r_{k}}>\\frac{\\varepsilon}{(C(1+\\varepsilon)+\\varepsilon)}}\\end{array}$ $\\begin{array}{r}{\\mathcal{E}_{0}>(1-\\frac{(1+\\varepsilon)\\varepsilon}{(C(1+\\varepsilon)+\\varepsilon)})^{-1}>1}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "Note that the above proof actually shows that for any $k_{1}\\leq k$ , we have that $r_{k_{1}}\\leq C m$ . Now we want to show that $r_{k}$ is lower bounded and that we can take $k<m$ such that $k+r_{k}>m$ . Again apply Theorem 28, we can generalize ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\tilde{\\lambda}_{i}}{\\tilde{\\lambda}_{1}}\\geq\\left(\\frac{1}{1+\\frac{k}{\\sqrt{T}}}\\right)^{k\\sqrt{T}}\\geq\\exp(-k^{2}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "to other ratios as well. Let $j\\leq k{\\sqrt{T}}$ and let $k{\\sqrt{T}}\\leq i\\leq(k+1){\\sqrt{T}}$ . Then we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\tilde{\\lambda}_{i}}{\\tilde{\\lambda}_{j}}\\geq\\left(\\frac{1}{1+\\frac{2k}{\\sqrt{T}}}\\right)^{(k+1)\\sqrt{T}}\\geq\\exp(-2k(k+1)).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that $\\begin{array}{r}{c_{1}^{\\prime}m^{\\frac{1}{d-1}}\\leq\\sqrt{T}=\\sqrt{\\frac{2}{\\tau_{m}^{2}}}\\leq c_{2}^{\\prime}m^{\\frac{1}{d-1}}}\\end{array}$ , where $\\begin{array}{r}{c_{2}^{\\prime}=\\sqrt{\\frac{2}{C_{1}^{2}}}}\\end{array}$ (and similarly for indices 1 and 2 swapped with inequalities reversed). Therefore, we have that the following bound holds on $r_{k}$ for any $k<m$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\nr_{k}=\\frac{\\sum_{i>k}\\lambda_{i}}{\\lambda_{k+1}}=\\tilde{N}(d,(k\\stackrel{<}+1))+\\sum_{j=1}^{\\infty}N(d,(k\\stackrel{<}+1)+j)\\frac{\\tilde{\\lambda}_{(k\\stackrel{<}+1)+j}}{\\tilde{\\lambda}_{(k\\stackrel{<}+1)}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that since $\\sqrt{T}\\geq c_{1}^{\\prime}m^{\\frac{1}{d-1}}$ t\u221ahen $N_{\\sqrt{T}}\\geq n_{l}(c_{1}^{\\prime})^{d-1}m$ , so $\\begin{array}{r}{\\tilde{m}\\leq\\frac{1}{c_{1}^{\\prime}}\\frac{1}{n_{l}^{\\frac{1}{d-1}}}}\\end{array}$ . Therefore, if we take $\\begin{array}{r}{j\\,=\\,\\tilde{k},\\,l\\,=\\,\\frac{1}{c_{1}^{\\prime}}\\frac{1}{n_{l}^{\\frac{1}{d-1}}}}\\end{array}$ , and any $l\\sqrt{T}\\,\\leq\\,i\\,\\leq\\,(l+1)\\sqrt{T}$ , we have that (since $\\exp(-2l(l+1))<$ $\\exp(-l^{2}))$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\tilde{\\lambda}_{i}}{\\tilde{\\lambda}_{j}}\\geq\\exp(-2l(l+1)).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that there is at least $N_{(l+1)\\sqrt{T}}-N_{l\\sqrt{T}}\\geq n_{l}\\left((l+1)^{d-1}-l^{d-1}\\right)m$ such indices $i$ , we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{r_{k}=\\frac{\\sum_{i>k}\\lambda_{i}}{\\lambda_{k+1}}=\\tilde{N}(d,(k\\tilde{+}1))+\\displaystyle\\sum_{j=1}^{\\infty}N(d,(k\\tilde{+}1)+j)\\frac{\\tilde{\\lambda}_{(k\\tilde{+}1)+j}}{\\tilde{\\lambda}_{(k\\tilde{+}1)}}.}\\\\ &{\\geq\\displaystyle\\sum_{l\\sqrt{T}\\leq i\\leq(l+1)\\sqrt{T}}N(d,i)\\frac{\\tilde{\\lambda}_{i}}{\\tilde{\\lambda}_{k}}}\\\\ &{>n_{l}\\left((l+1)^{d-1}-l^{d-1}\\right)m\\exp(-2l(l+1))>\\beta m,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\beta=n_{l}\\left((l+1)^{d-1}-l^{d-1}\\right)\\exp(-2l(l+1))$ . Note that $\\beta$ is independent of $k$ , so in particular for $k=(1-\\beta/2)m$ , we will have $r_{k}+k>m$ . Therefore we can use the upper bound on $\\mathcal{E}_{0}$ to see that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{E}_{0}\\leq\\frac{2^{2}}{\\beta^{2}}(1+\\beta/2).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $\\beta$ only depends on $d$ and $C_{1}$ and $C_{2}$ , this finishes the proof. ", "page_idx": 22}, {"type": "text", "text": "In particular, Lemma 18 shows that when $\\tau_{m}\\;\\;=\\;\\;o(m^{-\\frac{1}{d-1}})$ or $\\begin{array}{r c l}{\\tau_{m}}&{=}&{\\omega(m^{-\\frac{1}{d-1}})}\\end{array}$ , then lim sup $\\tilde{\\mathbb{R}}(\\hat{f}_{0})\\leq\\infty$ . ", "page_idx": 22}, {"type": "text", "text": "Lower bound for the test risk of Gaussian KRR Now we will show lower bounds for the test risk of Gaussian KRR. ", "page_idx": 23}, {"type": "text", "text": "Lemma 19 (Lower bound for the test risk). Under Assumption 2, the following bounds hold for the risk $\\tilde{R}(\\hat{f}_{0})$ of the minimum norm interpolating solution of Gaussian KRR: ", "page_idx": 23}, {"type": "text", "text": "1. If $\\tau_{m}=o(m^{-\\frac{1}{d-1}})$ , then $\\tilde{R}(0)\\leq\\mathrm{lim}$ inf $\\tilde{R}(\\hat{f}_{0})\\leq\\operatorname*{lim}\\operatorname*{sup}_{m\\rightarrow\\infty}\\tilde{R}(\\hat{f}_{0})<\\infty$ . More precisely, if $\\tau_{m}\\leq m^{-\\frac{1}{d-1}}t(m)$ , where $t(m)\\rightarrow0$ as $m\\rightarrow\\infty$ , then there is a scalar $c_{d}$ that depends only on the dimension so that for all such $t(m)$ there is $m_{0}$ that depends on $t(m)$ such that for all $m>m_{0}$ we have $\\tilde{R}(\\hat{f}_{0})>\\sigma^{2}+(1-c_{d}t(m)^{\\frac{d-1}{2}})\\|f^{*}\\|^{2}$ . ", "page_idx": 23}, {"type": "text", "text": "2. If $\\tau_{m}=\\omega(m^{-\\frac{1}{d-1}})$ , then $\\mathrm{lim}_{m\\rightarrow\\infty}\\,\\tilde{R}(\\hat{f}_{0})=\\infty$ . More precisely, if $\\tau_{m}\\geq m^{-\\frac{1}{d-1}}t(m)$ , where $t(m)\\to\\infty$ as $m\\rightarrow\\infty$ then there is an integer $m_{0}$ that depends on the dimension and $t(m)$ such that for $m>m_{0}$ , we have that $t R(\\hat{f}_{0})\\,\\rightarrow\\,\\sqrt{m}(\\sigma^{2})$ (note that $\\tilde{R}(0)$ is bounded). Hence, for large enough $m$ we have $\\tilde{R}(\\hat{f}_{0})>\\tilde{R}(0)$ .   \n3. If $\\tau_{m}\\;\\;=\\;\\;\\Theta(m^{-\\frac{1}{d-1}})$ , then $\\begin{array}{r}{\\operatorname*{lim}\\operatorname*{sup}_{m\\rightarrow\\infty}R(\\hat{f}_{0})~<~\\infty}\\end{array}$ . Moreover, suppose that $C_{1}m^{-\\frac{1}{d-1}}\\leq\\tau_{m}\\leq C_{2}m^{-\\frac{1}{d-1}}$ for some constants $C_{1}$ and $C_{2}$ , then there exist $\\eta,\\mu>0$ that depend only on $d,{\\mathcal{C}}$ $C_{1}$ , and $C_{2}$ , such that for all $m$ we have $\\tilde{R}(\\hat{f}_{0})>\\mu\\|f^{*}\\|^{2}\\!+\\!(1\\!+\\!\\eta)\\sigma^{2}$ . Consequently, $\\tilde{R}(\\hat{f}_{0})>\\tilde{R}(\\mathbf{0})$ as long as $\\begin{array}{r}{\\sigma^{2}>\\frac{1-\\mu}{\\eta}\\|f^{\\ast}\\|^{2}}\\end{array}$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. All three proofs will similarly follow by directly analyzing Equation (2). ", "page_idx": 23}, {"type": "text", "text": "Part 1: Note that for any $i$ , we have that $\\tilde{\\lambda}_{i}+\\kappa_{0}\\leq\\tilde{\\lambda}_{1}+\\kappa_{0}$ , so we have that $\\begin{array}{r}{\\mathcal{L}_{i}\\ge\\frac{\\tilde{\\lambda}_{i}}{\\tilde{\\lambda}_{1}}\\mathcal{L}_{1}}\\end{array}$ . From Theorem 28 we know that for $j\\leq\\sqrt{T}t(m)^{\\frac{1}{2}}$ , we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{\\tilde{\\lambda}_{j}}{\\tilde{\\lambda}_{1}}\\geq\\exp(-t(m)),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "so there is $m_{0}$ that depends on $t(m)$ such that for all $m>m_{0}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\tilde{\\lambda}_{j}\\geq\\frac{1}{2}\\tilde{\\lambda}_{1},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for all $j\\leq\\sqrt{T}t(m)^{\\frac{1}{2}}$ . Therefore we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\nm=\\sum_{i}N(d,i){\\mathcal{L}}_{i}>\\left(N(d,1)+\\cdot\\cdot\\cdot N(d,\\sqrt{T}t(m))\\right)\\frac{1}{2}{\\mathcal{L}}_{1}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that $\\begin{array}{r}{\\left(N(d,1)+\\ldots N(d,\\sqrt{T}t(m))\\right)>c_{l}\\left(\\sqrt{T}t(m)^{\\frac{1}{2}}\\right)^{d-1}=c_{l}^{\\prime}m(t(m))^{-\\frac{d-1}{2}}}\\end{array}$ , where $c_{l}^{\\prime}=$ ${\\sqrt{2}}c_{l}$ . Therefore, we have that $\\begin{array}{r}{\\mathcal{L}_{1}<\\frac{2}{c_{l}^{\\prime}}(t(m))^{\\frac{d-1}{2}}}\\end{array}$ . Therefore, we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n(1-\\mathcal{L}_{i})^{2}\\geq(1-\\mathcal{L}_{1})^{2}>\\left(1-\\frac{2}{c_{l}^{\\prime}}(t(m))^{\\frac{d-1}{2}}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "From Equation (2), we have that then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\tilde{R}(\\hat{f}_{0})\\geq\\mathcal{E}_{0}\\sigma^{2}+\\mathcal{E}_{0}(1-c_{d}t(m)^{\\frac{d-1}{2}})\\|\\beta\\|^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This shows the first claim. ", "page_idx": 23}, {"type": "text", "text": "For the second part, note the following property of Li,\u03b4 = \u03bbi\u03bb+i\u03ba\u03b4 : if for $j~>~i$ , we have that $\\mathcal{L}_{i,\\delta}>\\mathcal{L}_{j,\\delta}$ . Let $\\begin{array}{r}{\\frac{\\lambda_{j}}{\\lambda_{i}}>c}\\end{array}$ for some fixed $c>0$ and $j>i$ , then we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{L}_{j,\\delta}>c\\mathcal{L}_{i,\\delta}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By Theorem 28, we have that for $j\\leq\\sqrt{T}g(m)$ with $g(m)\\rightarrow0$ as $m\\rightarrow\\infty$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{\\tilde{\\lambda}_{\\tilde{k}+j}}{\\tilde{\\lambda}_{\\tilde{k}}}>\\exp\\left(-5g(m)\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "So there is $m>m_{0}$ such that for $m>m_{0}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{\\tilde{\\lambda}_{\\tilde{k}+j}}{\\tilde{\\lambda}_{\\tilde{k}}}>\\exp\\left(-5g(m)\\right)>\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\frac{m}{m-\\sum_{i=1}^{\\infty}\\mathcal{L}_{i,\\delta}^{2}}(1-\\mathcal{L}_{j,\\delta})^{2}>1\\mathrm{~if~}}\\\\ {\\displaystyle\\sum_{i=1}^{\\infty}\\mathcal{L}_{i,\\delta}^{2}>m\\mathcal{L}_{j,\\delta}\\left(2-\\mathcal{L}_{j,\\delta}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For this it suffices to have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{\\infty}\\mathcal{L}_{i,\\delta}^{2}>m\\mathcal{L}_{1,\\delta}\\left(2-\\mathcal{L}_{1,\\delta}\\right),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "since $\\mathcal{L}_{i,\\delta}$ are decreasing. Note that there is at least $N(d,1)\\ +\\ \\cdot\\cdot\\ +\\ N(d,\\sqrt{T}g(m))\\ \\ >$ $\\begin{array}{r}{n_{l}(\\sqrt{T}g(m))^{d-1}>c_{u}m\\left(\\frac{g(m)}{t(m)}\\right)^{d-1}}\\end{array}$ of indices $i$ such that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{\\lambda_{i}}{\\lambda_{1}}=\\frac{\\tilde{\\lambda}_{1+j}}{\\tilde{\\lambda}_{1}}>\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, if we select $g(m)=\\sqrt{t(m)}$ , then we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{\\infty}\\mathcal{L}_{i,\\delta}^{2}>\\frac{1}{2}c_{u}m\\left(\\frac{1}{t(m)}\\right)^{\\frac{d-1}{2}}>m\\mathcal{L}_{1,\\delta}\\left(2-\\mathcal{L}_{1,\\delta}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This implies that for all $j$ simultaneously ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{m}{m-\\sum_{i=1}^{\\infty}\\mathcal{L}_{i,\\delta}^{2}}(1-\\mathcal{L}_{j,\\delta})^{2}>1.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This translates to ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tilde{R}(\\hat{f}_{0})>\\tilde{R}(0)+\\left(\\frac{m}{m-\\sum_{i=1}^{\\infty}\\mathcal{L}_{i,\\delta}^{2}}-1\\right)\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This finishes the proof of the first part. ", "page_idx": 24}, {"type": "text", "text": "Part 2: Note that we showed that for $m$ large enough, $\\mathcal{E}_{0}>\\sqrt{m}$ . This shows that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tilde{R}(\\hat{f}_{0})=\\mathcal{E}_{0}\\left(\\sum_{i}(1-\\mathcal{L}_{0})^{2}v_{i}^{2}+\\sigma^{2}\\right)>\\sqrt{m}\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "So, there is $m_{0}$ that depends only on $t(m)$ such that for $m>m_{0}$ we have that $\\tilde{R}(\\hat{f}_{0})>\\sqrt{m}\\sigma^{2}$ . ", "page_idx": 24}, {"type": "text", "text": "Part 3: Note first that $\\mathcal{L}_{i}\\geq\\mathcal{L}_{j}$ for $i\\leq j$ . Let $\\begin{array}{r}{T=\\frac{2}{\\tau_{m}^{2}}}\\end{array}$ \u03c4 22m . From Lemma 26 it follows that ", "page_idx": 24}, {"type": "equation", "text": "$$\nn_{l}(\\alpha\\sqrt{T})^{d-1}\\leq N(d,1)+\\cdot\\cdot\\cdot+N(d,\\alpha\\sqrt{T})\\leq n_{u}(\\alpha\\sqrt{T})^{d-1}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Take $\\alpha$ such that $N(d,1)+\\cdots+N(d,\\alpha{\\sqrt{T}})>2m$ . Then, we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{}}&{{}}&{{m=\\displaystyle\\sum_{i}N(d,i)\\mathcal{L}_{i}>(N(d,1)+\\cdot\\cdot\\cdot+N(d,\\alpha\\sqrt{T}))\\mathcal{L}_{\\alpha\\sqrt{T}}>2m\\mathcal{L}_{\\alpha\\sqrt{T}}}}\\\\ {{}}&{{}}&{{}}\\\\ {{\\mathcal{L}_{\\alpha\\sqrt{T}}\\leq\\displaystyle\\frac{1}{2}\\implies\\lambda_{\\alpha\\sqrt{T}}\\leq\\kappa_{0}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Note that $(1-\\mathcal{L}_{1})^{2}<(1-\\mathcal{L}_{i})^{2}$ and ", "page_idx": 25}, {"type": "equation", "text": "$$\n(1-\\mathcal{L}_{1})^{2}=\\frac{\\kappa_{0}^{2}}{(\\lambda_{1}+\\kappa_{0})^{2}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "If \u03ba0 > \u03bb1, then 2\u03ba0 > \u03bb1 + \u03ba0, so21\u03ba0 $\\begin{array}{r}{\\frac{1}{2\\kappa_{0}}\\,<\\,\\frac{1}{\\lambda_{1}+\\kappa_{0}}}\\end{array}$ . This shows that $\\begin{array}{r}{\\frac{\\kappa_{0}^{2}}{(\\lambda_{1}+\\kappa_{0})^{2}}\\,>\\,\\frac{1}{4}}\\end{array}$ . Otherwise $\\kappa_{0}\\leq\\lambda_{1}$ , so $\\kappa_{0}+\\lambda_{1}\\leq2\\lambda_{1}$ . This implies that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{\\kappa_{0}^{2}}{(\\kappa_{0}+\\lambda_{1})^{2}}>\\frac{\\kappa_{0}^{2}}{4\\lambda_{1}^{2}}>\\frac{\\lambda_{\\alpha\\sqrt T}}{4\\lambda_{1}^{2}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "From Theorem 28 it follows that $\\begin{array}{r}{\\frac{\\lambda_{\\alpha\\sqrt{T}}}{4\\lambda_{1}^{2}}\\,>\\,\\exp(-\\alpha^{2})}\\end{array}$ . Since $\\begin{array}{r}{\\left(\\frac{1}{n_{u}\\frac{2}{C_{1}^{2}}}\\right)^{\\frac{1}{d-1}}<\\alpha<\\left(\\frac{1}{n_{l}\\frac{2}{C_{2}^{2}}}\\right)^{\\frac{1}{d-1}}}\\end{array}$ we know that $\\begin{array}{r}{\\exp\\left(-\\alpha^{2}\\right)>\\exp\\left(-\\left(\\frac{1}{n_{l}\\frac{2}{C_{2}^{2}}}\\right)^{\\frac{2}{d-1}}\\right)}\\end{array}$ so $\\begin{array}{r}{\\mu=\\exp\\left(-\\left(\\frac{1}{n_{l}\\,\\frac{2}{C_{2}^{2}}}\\right)^{\\frac{2}{d-1}}\\right)}\\end{array}$ Combining Lemma 18 and Lemma 19, we get Theorem 3. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "C The case of increasing dimension ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In Appendix C.1 we will prove Theorem 7 and Theorem 9. In Appendix C.2, we will prove related claims about the eigenvalue multiplicities for relevant scalings of the dimension Corollary 12 and Corollary 13. In Appendix C.3, we will give a detailed proof of Corollary 13. In Appendix C.4, we will discuss when the conditions posed on kernels are satisfied. ", "page_idx": 25}, {"type": "text", "text": "C.1 Proofs of Theorem 7 and Theorem 9 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Theorem 20 (Error scaling for any kernel ridge regression). Let $d$ and $m$ be any dimension and sample size. Define $L_{m}$ $\\it{\\Pi}_{m},U_{m},k_{m},N(i),N_{l},$ , and $\\tilde{\\lambda}_{k}$ as in Definition 5. Consider KRR with a kernel $K$ satisfying Assumption 6 for some $A$ . Assume that for some integer $l$ , the target function $f^{*}$ satisfies Assumption 4 with at most $N_{l}$ nonzero coefficients. Then, the risk of the minimum norm interpolating solution is bounded by the following: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\tilde{R}(\\hat{f}_{0})\\leq\\left(1-\\frac{L_{m}}{m}\\right)^{-1}\\left(1-\\frac{m}{U_{m}}\\right)^{-1}\\sigma^{2}}}\\\\ {{\\displaystyle\\qquad\\quad+\\,B^{2}\\left(1-\\frac{L_{m}}{m}\\right)^{-1}\\left(1-\\frac{m}{U_{m}}\\right)^{-1}\\frac{A^{2}}{m^{2}}\\left(\\sum_{i=1}^{l}N(i)\\frac{1}{\\tilde{\\lambda}_{i}^{2}}\\right)\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Additionally, we can get an alternative bound if we swap $\\left(\\sum_{i=1}^{l}N(d,i)\\frac{1}{\\tilde{\\lambda}_{i}^{2}}\\right)$ with $\\left(\\sum_{i=1}^{l}N(d,i)\\frac{1}{\\tilde{\\lambda}_{i}}\\right)$ , i.e. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{\\boldmath~\\lambda~}_{0})\\leq\\left(1-\\frac{L_{m}}{m}\\right)^{-1}\\left(1-\\frac{m}{L_{m}}\\right)^{-1}\\sigma^{2}\\quad+B^{2}\\left(1-\\frac{L_{m}}{m}\\right)^{-1}\\left(1-\\frac{m}{L_{m}}\\right)^{-1}\\frac{A}{m^{2}}\\left(\\sum_{i=1}^{L_{m}}N(d,i)\\frac{1}{\\widetilde{\\lambda}_{i}}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proofs of Theorem 7 and Theorem 20: Note first that Equation (2) implies that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\tilde{R}(\\hat{f}_{0})=\\sum_{i}\\mathcal{E}_{0}\\left(1-\\mathcal{L}_{i,0}\\right)^{2}\\beta_{i}^{2}+\\mathcal{E}_{0}\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Let Li = $\\begin{array}{r}{\\mathcal{L}_{i}=\\frac{\\tilde{\\lambda}_{i}}{\\tilde{\\lambda}_{i}+\\kappa_{0}}}\\end{array}$ \u03bb\u02dci . Then, we can rewrite it as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\tilde{R}(\\hat{f}_{0})=\\sum_{i}\\mathcal{E}_{0}N(d,i)\\left(1-\\mathcal{L}_{i}\\right)^{2}\\beta_{i}^{2}+\\mathcal{E}_{0}\\sigma^{2},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "with a slight of abuse of notation for $\\beta_{i}$ . From Equation (5), we have that for $k<m$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{E}_{0}\\leq\\left(1-\\frac{k}{m}\\right)^{-1}\\left(1-\\frac{m}{k+r_{k}}\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Take $k=L_{m}$ . Note that by Definition 5, $L_{m}<m$ Then $\\lambda_{k+1}=\\tilde{\\lambda}_{k_{m}+1}$ and it repeats $N(d,k_{m}+1)$ times. Therefore, we have that ", "page_idx": 26}, {"type": "equation", "text": "$$\nr_{k}=N(d,k_{m}+1)+\\sum_{i=1}^{\\infty}N(d,k_{m}+i+1)\\frac{\\tilde{\\lambda}_{k_{m}+i+1}}{\\tilde{\\lambda}_{k_{m}+1}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore, $r_{k}+k>N(d,k_{m}+1)+L_{m}=L_{m}>m$ . Therefore, we can apply Equation (5). We get that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{E}_{0}\\leq\\left(1-\\frac{k}{m}\\right)^{-1}\\left(1-\\frac{m}{k+r_{k}}\\right)^{-1}=\\left(1-\\frac{L_{m}}{m}\\right)^{-1}\\left(1-\\frac{m}{L_{m}}\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note now that ", "page_idx": 26}, {"type": "equation", "text": "$$\nm=\\sum_{i}N(d,i){\\mathcal{L}}_{i}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note that $\\tilde{\\lambda}_{i}+\\kappa_{0}>\\kappa_{0}$ so we have that $\\begin{array}{r}{\\mathcal{L}_{i}<\\frac{\\tilde{\\lambda}_{i}}{\\kappa_{0}}}\\end{array}$ . Therefore, we have that ", "page_idx": 26}, {"type": "equation", "text": "$$\nm=\\sum_{i}N(d,i)\\mathcal{L}_{i}<\\frac{1}{\\kappa_{0}}\\left(\\sum_{i}N(d,i)\\tilde{\\lambda}_{i}\\right)<\\frac{A}{\\kappa_{0}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore, we have that $\\textstyle\\kappa_{0}<{\\frac{A}{m}}$ . Since $\\begin{array}{r}{(1-\\mathcal{L}_{i})^{2}=\\frac{\\kappa_{0}^{2}}{(\\kappa_{0}+\\tilde{\\lambda})^{2}}<\\frac{\\kappa_{0}^{2}}{\\tilde{\\lambda}^{2}}}\\end{array}$ =(\u03ba0\u03ba+20\u03bb\u02dc)2 < \u03ba\u03bb\u02dc202 . Since there are only ld nonzero $\\beta_{i}$ , we have that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{i}{N(d,i)(1-\\mathscr{L}_{i})^{2}\\beta_{i}^{2}}\\leq B^{2}\\frac{A^{2}}{m^{2}}\\left(\\sum_{i=1}^{l}{N(d,i)\\frac{1}{\\tilde{\\lambda}_{i}^{2}}}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Going back to Equation (2), we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\tilde{R}(\\hat{f}_{0})=\\mathcal{E}_{0}\\sum_{i}(1-\\mathcal{L}_{i})^{2}N(d,i)\\beta_{i}^{2}+\\mathcal{E}_{0}\\sigma^{2}}\\\\ &{\\qquad\\le\\mathcal{E}_{0}B^{2}\\frac{A^{2}}{m^{2}}\\left(\\displaystyle\\sum_{i=1}^{l}N(d,i)\\frac{1}{\\tilde{\\lambda}_{i}^{2}}\\right)+\\mathcal{E}_{0}\\sigma^{2}}\\\\ &{\\displaystyle\\qquad\\le\\left(1-\\frac{L_{m}}{m}\\right)^{-1}\\left(1-\\frac{m}{L_{m}}\\right)^{-1}\\sigma^{2}}\\\\ &{\\displaystyle\\qquad\\qquad+\\left(1-\\frac{L_{m}}{m}\\right)^{-1}\\left(1-\\frac{m}{L_{m}}\\right)^{-1}B^{2}\\frac{A^{2}}{m^{2}}\\left(\\displaystyle\\sum_{i=1}^{l}N(d,i)\\frac{1}{\\tilde{\\lambda}_{i}^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This shows the first bound. For the second bound, note that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{\\infty}\\frac{\\kappa_{0}^{2}}{(\\kappa_{0}+\\lambda_{i})^{2}}\\beta_{i}^{2}\\leq\\sum_{i=l+1}^{\\infty}\\beta_{i}^{2}+\\sum_{i=1}^{l}\\beta_{i}^{2}\\frac{1}{\\sum_{i}\\lambda_{i}}\\frac{1}{\\lambda_{i}}\\left(\\frac{\\sum_{j>l}\\lambda_{j}}{m}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "So if we take $\\beta$ to have only $l$ nonzero terms, we get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{l}\\frac{\\kappa_{0}^{2}}{(\\kappa_{0}+\\lambda_{i})^{2}}\\beta_{i}^{2}\\leq\\sum_{i=1}^{l}\\beta_{i}^{2}\\frac{1}{\\sum_{i}\\lambda_{i}}\\frac{1}{\\lambda_{i}}\\left(\\frac{\\sum_{j>l}\\lambda_{j}}{m}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "From this, we have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{l}\\frac{\\kappa_{0}^{2}}{(\\kappa_{0}+\\lambda_{i})^{2}}\\beta_{i}^{2}\\leq\\frac{A}{\\lambda_{i}}\\frac{1}{m^{2}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore, we have that ", "page_idx": 27}, {"type": "equation", "text": "$$\nN(d,i)\\frac{\\kappa_{0}^{2}}{(\\kappa_{0}+\\tilde{\\lambda}_{i})^{2}}\\beta_{i}^{2}\\le N(d,i)\\beta_{i}^{2}\\frac{A}{\\lambda_{i}}\\frac{1}{m^{2}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore, we have the improved inequality from ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{l}N(d,i)(1-\\mathscr{L}_{i})^{2}\\beta_{i}^{2}\\leq\\sum_{i=1}^{l}\\beta_{i}^{2}N(d,i)\\frac{A}{\\lambda_{i}}\\frac{1}{m^{2}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This gives the second bound on the test risk, as we can bound ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{l}N(d,i)(1-\\mathscr{L}_{i})^{2}\\beta_{i}^{2}\\leq\\frac{B^{2}A}{m^{2}}\\left(\\sum_{i=1}^{l}N(d,i)\\frac{1}{\\lambda_{i}}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "[Lower bound for test risk in increasing dimension] Let $k_{m}$ and $L_{m}$ be as in Definition 5. Consider learning a target function $f^{*}$ , with some sample size $m$ . Let $K$ be a kernel satisfying Assumption 8 for some $A$ and $b$ . Consider the minimum norm interpolating solution of KRR (with any data distribution) with kernel $K$ . Then, for the risk of minimum norm interpolating solution, the following lower bound holds: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\tilde{R}(\\hat{f}_{0})>\\left(1-\\left(\\frac{b}{b+1}\\right)^{2}\\frac{L_{m}}{m}\\right)^{-1}\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof of Theorem 9: By Theorem 3 from Zhou et al. [66], if $k$ is the first $k\\,<\\,m$ such that $k+b r_{k}\\ge m$ , then ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{E}_{0}\\geq\\left(1-\\left(\\frac{b}{b+1}\\right)^{2}\\frac{k}{m}\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore, it suffices to show that the first such $k$ is actually $L_{m}=N(d,1)+\\cdot\\cdot\\cdot+N(d,k_{m})$ . Note that ", "page_idx": 27}, {"type": "equation", "text": "$$\nr_{k}=\\frac{\\sum_{i>k}\\lambda_{i}}{\\lambda_{k+1}}<\\operatorname*{max}_{i\\leq k}\\left(\\frac{1}{\\lambda_{i}}\\right)(\\sum_{i>k}\\lambda_{i})<A\\operatorname*{max}_{i\\leq k}\\left(\\frac{1}{\\lambda_{i}}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since $\\begin{array}{r l r}&{}&{\\operatorname*{max}_{i\\leq k_{m}}\\left(\\frac{1}{\\tilde{\\lambda}_{i}}\\right)\\;<\\;\\frac{m-L_{m}}{b}}\\end{array}$ , for $l\\;<\\;N(d,1)\\,+\\,\\cdot\\,\\cdot\\,+\\,N(d,k_{m})$ , we have that $b r_{l}+l\\ \\leq$ $\\begin{array}{r}{N(d,1)\\,+\\,\\cdot\\,\\cdot\\,+\\,\\overset{\\cdot}{N(d,k_{m})}\\,-\\,1\\,+\\,b\\operatorname*{max}_{i\\leq k_{m}}\\left(\\frac{1}{\\tilde{\\lambda}_{i}}\\right)\\,<\\,L_{m}\\,+\\,m\\,-\\,L_{m}\\,\\leq\\,m}\\end{array}$ , so the first $l$ for which $r_{l}+l\\,>\\,m$ is $l\\,=\\,L_{m}\\,=\\,N(d,1)\\,+\\,\\cdot\\,\\cdot\\,\\dot{+}\\,N(d,k_{m})$ . Plugging in $k\\,=\\,L_{m}$ we get that $\\begin{array}{r}{\\mathcal{E}_{0}\\ge\\left(1-\\left(\\frac{b}{b+1}\\right)\\frac{L_{m}}{m}\\right)^{-1}}\\end{array}$ , so from Equation (6), we have $\\begin{array}{r}{\\tilde{R}(\\hat{f}_{0})\\geq\\left(1-\\left(\\frac{b}{b+1}\\right)^{2}\\frac{L_{m}}{m}\\right)^{-1}\\sigma^{2}.}\\end{array}$ ", "page_idx": 27}, {"type": "text", "text": "C.2 Dot-product kernels on the sphere ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "First we will prove results about the eigenvalue multiplicites Corollary 12 and Corollary 13. ", "page_idx": 27}, {"type": "text", "text": "Theorem 21 (Log-scaling multiplicity). Let $d=\\log_{2}m$ , i.e. $m=2^{d}$ , and let $k_{d}$ be an index for which the following holds: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{N(d,1)+\\cdots+N(d,k_{m})<m}\\\\ {N(d,1)+\\cdots+N(d,k_{m}+1)\\geq m.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then the following hold: ", "page_idx": 27}, {"type": "text", "text": "1. dN(dd, d5 ) is decreasing and has limd\u2192\u221e $\\begin{array}{r}{\\operatorname*{lim}_{d\\rightarrow\\infty}{\\frac{d N(d,{\\frac{d}{5}})}{2^{d}}}\\rightarrow0}\\end{array}$ . This also holds for $\\frac{d N(d,k)}{2^{d}}$ with $\\begin{array}{r}{k=f(d)\\leq\\frac{d}{5}}\\end{array}$ for all $d$ . ", "page_idx": 28}, {"type": "text", "text": "2. $\\frac{N(d,{\\frac{d}{2}})}{2^{d}}$ is increasing and has $\\begin{array}{r}{\\operatorname*{lim}_{d\\rightarrow\\infty}\\frac{N(d,\\frac{d}{2})}{2^{d}}\\rightarrow\\infty}\\end{array}$ .This also holds for $\\frac{N(d,k)}{2^{d}}$ with $k=$ $\\textstyle f(d)\\geq{\\frac{d}{2}}$ for all $d$ .   \n3. There is an absolutet constant $d_{0}$ such that for $d>d_{0}$ , we have that $\\begin{array}{r}{\\frac{d}{5}\\leq k_{m}\\leq\\frac{d}{2}}\\end{array}$ .   \n4. There are absolute constants $\\begin{array}{r}{c_{l-1}=\\frac{1}{54},c_{l}=\\frac{1}{9}}\\end{array}$ , and $\\begin{array}{r}{c_{l+1}=\\frac{2}{3}}\\end{array}$ such that $N(d,k_{m}+i)>$ $c_{l+i}m$ , for all $i\\in\\{\\pm1,0\\}$ . Additionally, there are constants $\\displaystyle{\\dot{c}}_{u-1}={\\textstyle{\\frac{1}{3}}},c_{u}=1,c_{u+1}=6$ , such that $N(d,k_{m}+i)<c_{u+i}m$ for all $i\\in\\{\\pm1,0\\}$ . ", "page_idx": 28}, {"type": "text", "text": "Furthermore, if $\\begin{array}{r}{\\frac{d}{\\log m}=\\Theta(1)}\\end{array}$ , then we will also have $L_{m}=\\Theta(m)$ and $L_{m}=\\Theta(m)$ . ", "page_idx": 28}, {"type": "text", "text": "Proof. We will split the proof into three parts. Note first that $N(d,k)$ increases as $k$ increases. This can be seen from the ratio of consecutive multiplicities ", "page_idx": 28}, {"type": "equation", "text": "$$\n{\\frac{N(d,k+1)}{N(d,k)}}={\\frac{2k+d}{2k+d-2}}{\\frac{k+d-2}{k+1}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Part 1: We will use Stirling\u2019s approximation to estimate $N(d,k)$ . It states that $\\begin{array}{r}{n!\\approx\\left({\\frac{n}{e}}\\right)^{n}{\\sqrt{2\\pi n}}}\\end{array}$ . Therefore, we have that ", "page_idx": 28}, {"type": "equation", "text": "$$\nN(d,k)={\\frac{(2k+d-2)(k+d-3)!}{k!(d-2)!}}\\approx{\\frac{\\sqrt{2\\pi(k+d-3)}\\left({\\frac{k+d-3}{e}}\\right)^{k+d-3}(2k+d-2)}{\\left({\\frac{k}{e}}\\right)^{k}\\left({\\frac{d-2}{e}}\\right)^{d-2}{\\sqrt{k}}{\\sqrt{d-2}}}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Since $N(d,k)$ is increasing in $k$ and $\\begin{array}{r}{k\\leq\\frac{d}{5}}\\end{array}$ , so we can just plug in $\\begin{array}{r}{k=\\frac{d}{5}}\\end{array}$ in the expression. Therefore ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{N(d,\\frac{d}{5})\\approx\\frac{\\sqrt{2\\pi(k+d-3)}\\,\\left(k+d-2\\right)}{\\left(\\frac{k}{6}\\right)^{k}\\left(\\frac{d-2}{e}\\right)^{d-2}\\sqrt{k}\\sqrt{d-2}}\\,}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,=\\frac{\\sqrt{2\\pi(\\frac{d}{5}+d-3)}\\,\\left(\\frac{d+d-3}{e}\\right)^{\\frac{5}{4}+d-3}\\left(2\\frac{d}{5}+d-2\\right)}{\\left(\\frac{d}{e}\\right)^{\\frac{5}{6}}\\left(\\frac{d-2}{e}\\right)^{d-2}\\sqrt{\\frac{d}{5}}\\sqrt{d-2}}}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}\\\\ &{=C\\frac{\\sqrt{4}\\,d\\pm^{1}+d-3}{\\sqrt{d}\\,\\left(\\frac{d}{5}\\right)^{\\frac{d}{5}}\\left(\\frac{d}{e}\\right)^{d-2}\\sqrt{d\\sqrt{d}}}}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Note that $\\left(5^{1/5}(\\frac{6}{5})^{6/5}\\right)<1.8$ , so $\\begin{array}{r}{\\frac{\\left(5^{1/5}(\\frac{6}{5})^{6/5}\\right)^{d}}{\\sqrt{d}}<\\frac{1.8^{d}}{\\sqrt{d}}}\\end{array}$ In particular, we have that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{d N(d,\\frac{d}{5})}{2^{d}}<\\frac{\\sqrt{d}(1.8)^{d}}{2^{d}}\\rightarrow0.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Part 2: The same calculation in this case gives ", "page_idx": 28}, {"type": "equation", "text": "$$\nN(d,\\frac{d}{2})\\approx C\\frac{\\left(2^{\\frac{1}{2}}(\\frac{3}{2})^{\\frac{3}{2}}\\right)^{d}}{\\sqrt{d}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "but note that $\\left(2^{\\frac{1}{2}}\\Big(\\frac{3}{2}\\Big)^{\\frac{3}{2}}\\right)>2.5$ , s $\\begin{array}{r}{\\cdot\\frac{N(d,\\frac{d}{2})}{2^{d}}>\\left(\\frac{2.5}{2}\\right)^{d}\\frac{1}{\\sqrt{d}}\\rightarrow\\infty.}\\end{array}$ ", "page_idx": 28}, {"type": "text", "text": "Part 3: Note that if $k_{m}\\leq\\frac{d}{5}$ then for $d$ large enough ", "page_idx": 29}, {"type": "equation", "text": "$$\nN(d,1)+\\cdots+N(d,k_{m})\\leq k_{m}N(d,k_{m})\\leq d N(d,k_{m})<d.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Similarly, note that if $k_{m}\\,>\\,\\textstyle{\\frac{d}{2}}$ , then $N(d,k_{m})\\,>\\,d$ for $d$ large enough. Both of these imply hat $\\begin{array}{r}{\\frac{d}{5}\\leq k_{m}\\leq\\frac{d}{2}}\\end{array}$ when $d>d_{0}$ $d_{0}$ is determined by when the inequalities above start holding). ", "page_idx": 29}, {"type": "text", "text": "Part 4: Note that when $\\begin{array}{r}{\\frac{d}{5}\\leq k\\leq\\frac{d}{2}}\\end{array}$ , we have that $\\begin{array}{r}{6>\\frac{N(d,k+1)}{N(d,k)}=\\frac{2k+d}{2k+d-2}\\frac{k+d-2}{k+1}>3}\\end{array}$ $d$ enough, since the firs ratio is close to 1 and the second is $\\textstyle1+{\\frac{d-2}{k+1}}$ . This implies that $\\begin{array}{r}{\\frac{N(d,k)}{N(d,k+1)}<\\frac{1}{3}}\\end{array}$ whenever $d$ is large enough (larger than an absolute constant) and $k\\leq\\frac{d}{2}$ . Note now that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad m<N(d,1)+\\cdots+N(d,k_{m}+1)<N(d,k_{m}+1)\\left(1+\\frac{1}{3}+(\\frac{1}{3})^{2}+\\cdot\\cdot\\cdot\\right)<N(d,k_{m}+1)\\frac{3}{2}}\\\\ &{\\atop\\longrightarrow\\cdots\\nearrow N(d,~l,~~~~1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This now implies that $\\begin{array}{r}{N(d,k_{m})>\\frac{1}{6}N(d,k_{m}+1)>\\frac{1}{9}m}\\end{array}$ , $\\begin{array}{r}{N(d,k_{m}-1)>\\frac{1}{54}m}\\end{array}$ . Similarly we have that $N(d,k_{m})<m$ , so $N(d,k_{m}+1)<6m$ and $\\begin{array}{r}{N(d,k_{m}-1)<\\frac{1}{3}m}\\end{array}$ . ", "page_idx": 29}, {"type": "text", "text": "Note that the same proof works even if $d$ is not exactly equal to $\\log_{2}m$ . This shows that whenever $\\begin{array}{r}{\\frac{d}{\\log m}=\\Theta(1)}\\end{array}$ , we have that $L_{m}=\\Theta(m)$ and $L_{m}=\\Theta(m)$ . \u53e3 ", "page_idx": 29}, {"type": "text", "text": "Remark 22. Note that this does not say that we will always have tempered overfitting forlogd m $\\Theta(1)$ as sometimes it can happen that $L_{m}\\to m$ , so we cannot apply Equation (5) and Equation (6). In this case we expect the overfitting to be catastrophic. ", "page_idx": 29}, {"type": "text", "text": "Theorem 23 (Sub-polynomial scaling multiplicity). Let $K$ be any dot product kernel on the sphere $\\mathbb{S}^{d-1}$ (with nonzero\u221a eigenvalues). Let $l\\in\\mathbb N$ and let $m=2^{2^{2l}}$ and let $\\bar{d}=2^{2^{l}}$ . This corresponds to the case $d=\\exp\\left({\\sqrt{\\log m}}\\right)$ , which is sub-polynomial. Then, for the upper and lower index, $L_{m}$ and $L_{m}$ , we have the following ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{L_{m}}{m}\\leq\\frac{3}{2\\log m}\\,\\,\\,\\,\\,\\mathrm{and}\\,\\,\\,\\,\\,\\frac{m}{L_{m}}\\leq\\frac{1}{d^{0.89}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Additionally, for $k=\\alpha k_{m}$ , where $\\alpha$ is a constant, we have that $N(d,k)\\approx m^{\\alpha}$ , where $\\approx$ means up to sub-polynomial factors. ", "page_idx": 29}, {"type": "text", "text": "Proof. Let $k_{m}$ be the maximum $k$ for which ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{N(d,1)+\\cdots+N(d,k_{m})<m}\\\\ {N(d,1)+\\cdots+N(d,k_{m}+1)\\geq m.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We want to show that $N(d,k_{m})\\,=\\,o(m)$ and $N(d,k_{m}+1)\\,=\\,\\Omega(m)$ . Then we can take $k\\,=$ $N(d,1)+\\cdots+N(d,k_{m})$ and so ${\\frac{k}{m}}\\to0$ and ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r_{k}=N(d,k_{m}+1)+\\displaystyle\\sum_{j=2}N(d,k_{m}+j)\\frac{\\tilde{\\lambda}_{k_{m}+j}}{\\tilde{\\lambda}_{k_{m}+1}}=\\Omega(m)}\\\\ &{\\frac{m}{R_{k}}\\to0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Note that we just need to estimate $N(d,k_{m})$ precisely. Let $k\\,=\\,2^{l}\\,+\\,l$ . We want to show that $N(d,k)>m$ . Note that by Stirling\u2019s approximation $\\begin{array}{r}{n!\\approx\\left({\\frac{n}{e}}\\right)^{n}{\\sqrt{2\\pi n}}}\\end{array}$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{N(d,k)\\approx\\frac{\\sqrt{2\\pi(k+d-3)}\\,{\\left(k+d-2\\right)}\\,{\\left(k+d\\right)}^{+2k-3}\\,{\\left(2k+d-2\\right)}}{{\\left(\\frac{k}{2}\\right)}^{k}\\,{\\left({k-{\\frac{d-2}{d}}}\\right)}^{k}\\,{\\sqrt{2\\pi}}\\,{\\left({d-2}\\right)}}}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,=\\,\\frac{c}{\\sqrt{2\\pi}}\\,\\frac{d^{4-k-2}\\sqrt{d}\\sqrt{2\\pi(1+\\frac{k}{d}-\\frac{3}{d})}\\,{\\left(1+\\frac{k}{d}-\\frac{3}{d}\\right)}^{k+d-3}\\,{\\left(1+\\frac{2k}{d}-\\frac{2}{4}\\right)}}{{k}^{k}\\,{\\left({d-2}\\right)}^{2k-2}\\,{\\sqrt{k}}\\,{\\sqrt{d}}\\,{\\left(1+\\frac{k}{d}-2\\right)}}}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,=\\,\\frac{c}{\\sqrt{2\\pi}}\\,\\frac{d^{4+k-2}\\sqrt{d}\\sqrt{2\\pi(1+\\frac{k}{d})^{4-t}}\\,{\\frac{3}{2}}^{k}}{{\\left({k-\\frac{d-2}{d}}\\right)}^{k}\\,{\\left({d-1}\\right)}^{2k-2}\\,{\\sqrt{k}}\\,{\\sqrt{d}}\\,{\\left({1-\\frac{3}{d}}^{+2}\\right)}^{k+d-3}}}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,=\\,\\frac{c}{\\sqrt{2\\pi}}\\,\\frac{d^{4}\\sqrt{2\\pi(1+\\frac{k}{d}-\\frac{3}{d})}\\,{\\left(1+\\frac{k}{d}-\\frac{3}{d}\\right)}^{k+d-3}\\,{\\left({1+\\frac{2k}{d}}-\\frac{3}{d}\\right)}}{{\\left({1-\\frac{3}{d}}\\right)}^{d-2}\\,{\\sqrt{k}}\\,{\\left({1+\\frac{k}{d}}-{\\frac{3}{d}}\\right)}^{k+d-3}}}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,=\\,\\frac{c}{\\sqrt{2\\pi}}\\,\\frac{\\sqrt{2\\pi(1+\\frac{k}{d}-\\frac{3}{d})}\\,{\\left(1+\\frac{2k} \n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Only the term kkdk\u221ak 1 + kd \u2212d3 k+d\u22123 is asymptotic here, as the rest tends to a constant as $d\\to\\infty$ . Note that since for all $l>5$ (i.e. $d_{,}$ ) ", "page_idx": 30}, {"type": "equation", "text": "$$\n2>\\left(1+\\frac{k}{d}-\\frac{3}{d}\\right)^{\\frac{d}{k}}>1.95,\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "we have that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(1+\\displaystyle\\frac{k}{d}-\\displaystyle\\frac{3}{d}\\right)^{k+d-3}>(1.95)^{\\frac{k^{2}}{d}+k-3\\frac{k}{d}}>(1.95)^{k}>2^{(0.9k)}}\\\\ &{\\left(1+\\displaystyle\\frac{k}{d}-\\displaystyle\\frac{3}{d}\\right)^{k+d-3}<2^{\\frac{k^{2}}{d}+k-3\\frac{k}{d}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Note that we have that then ", "page_idx": 30}, {"type": "equation", "text": "$$\n{\\frac{d^{k}}{k^{k}{\\sqrt{k}}}}\\left(1+{\\frac{k}{d}}-{\\frac{3}{d}}\\right)^{k+d-3}<{\\frac{d^{k}}{k^{k}{\\sqrt{k}}}}2^{{\\frac{k^{2}}{d}}+k-3{\\frac{k}{d}}}\\qquad=2^{k\\log d-k\\log k-{\\frac{1}{2}}\\log k+{\\frac{k^{2}}{d}}+k-{\\frac{3k}{d}}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Consider now the expression $\\begin{array}{r}{k\\log d-k\\log k-\\frac{1}{2}\\log k+\\frac{k^{2}}{d}+k-\\frac{3k}{d}}\\end{array}$ . For $k=2^{l}+l-1$ , we have that ", "page_idx": 30}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\operatorname{\\hat{\\varepsilon}l o g}d-k\\log k-{\\frac{1}{2}}\\log k+{\\frac{k^{2}}{d}}+k-{\\frac{3k}{d}}=k\\log d-k\\log k+k-{\\frac{1}{2}}\\log k-{\\frac{3k}{d}}+{\\frac{k^{2}}{d}}}\\\\ &{=\\left(2^{l}+l-1\\right)2^{l}-\\left(2^{l}+(l-1)\\right)\\left(l+{\\frac{l-1}{2^{l}}}+o({\\frac{1}{2^{2l}}})\\right)-{\\frac{1}{2}}(l)+o({\\frac{1}{2^{l}}})+2^{l}+l-1+o({\\frac{1}{2^{l}}})}\\\\ &{=2^{2l}+2^{l}(l-1)-2^{l}(l)+2^{l}-l+1-{\\frac{1}{2}}l+l-1+o(1)=2^{2l}-{\\frac{1}{2}}l=\\log_{2}m-\\log_{2}\\log_{2}m.}\\end{array}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "In particular, this implies that for $k\\leq2^{l}+(l-1)$ , we have that ", "page_idx": 30}, {"type": "equation", "text": "$$\nN(d,k)<m2^{-{\\frac{1}{2}}l}=o(m).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Now repeating the same computation for $k=2^{l}+l$ , we have that ", "page_idx": 30}, {"type": "equation", "text": "$$\n{\\frac{d^{k}}{k^{k}{\\sqrt{k}}}}\\left(1+{\\frac{k}{d}}-{\\frac{3}{d}}\\right)^{k+d-3}>{\\frac{d^{k}}{k^{k}{\\sqrt{k}}}}2^{0.9k}\\qquad\\quad=2^{k\\log d-k\\log k-{\\frac{1}{2}}\\log k+0.9k}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{k\\log d-k\\log k-\\frac{1}{2}\\log k+\\frac{k^{2}}{d}+0.9k-\\frac{3k}{d}=k\\log d-k\\log k+k-\\frac{1}{2}\\log k-\\frac{3k}{d}+\\frac{k^{2}}{d}}\\\\ &{=(2^{l}+l)\\,2^{l}-(2^{l}+l)\\left(l+\\frac{l}{2^{l}}+o(\\frac{1}{2^{2l}})\\right)-\\frac{1}{2}(l)+o(\\frac{1}{2^{l}})+0.9\\cdot2^{l}+0.9l+o(\\frac{1}{2^{l}})}\\\\ &{=2^{2l}+2^{l}(l)-2^{l}(l)+0.9\\cdot2^{l}-l-\\frac{1}{2}l+l+o(1)=2^{2l}+0.9\\cdot2^{l}-\\frac{1}{2}l}\\\\ &{=\\log_{2}m+0.9\\cdot\\sqrt{\\log_{2}m}-\\log_{2}\\log_{2}m.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "This shows that for $k\\geq2^{l}+l$ , ", "page_idx": 31}, {"type": "equation", "text": "$$\nN(d,k)>m2^{0.9\\cdot2^{l}-\\frac{1}{2}l}=\\Omega(m).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "To imply now that $k_{m}=k=2^{l}+l-1$ , it suffices to show that $N(d,1)+\\cdot\\cdot\\cdot+N(d,k)=o(m)$ . But note that we have $k\\ll d$ , so ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{N(d,k-1)=\\displaystyle\\frac{2k+d-2}{2k+d}\\displaystyle\\frac{k+1}{k+d-2}N(d,k)<\\displaystyle\\frac{1}{d}N(d,k)}}\\\\ {{N(d,1)+\\cdots+N(d,k-1)<\\displaystyle\\frac{k-1}{d}N(d,k)}}\\\\ {{N(d,1)+\\cdots+N(d,k)=o(m).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Therefore, we have computed with proof that $k_{m}=2^{l}+l-1$ . So we can now compute $L_{m}$ and $L_{m}$ with $L_{m}=N(d,1)+\\dot{\\cdot}\\cdot\\dot{+}N(d,\\bar{k}_{m})$ . Note that ", "page_idx": 31}, {"type": "equation", "text": "$$\n{\\frac{N(d,k+1)}{N(d,k)}}={\\frac{2k+d}{2k+d-2}}{\\frac{k+d-2}{k+1}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Since $k<<d$ , note that N(d,k+1)> 2, so we have that $\\textstyle{\\frac{N(d,k)}{N(d,k+1)}}<{\\frac{1}{2}}$ , which implies that ", "page_idx": 31}, {"type": "equation", "text": "$$\nN(d,1)+N(d,2)+\\cdots+N(d,k_{m})<\\frac{3}{2}N(d,k_{m})=o(m).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Therefore, we have that $\\begin{array}{r}{\\frac{L_{m}}{m}\\,=\\,\\frac{N(d,1)+N(d,2)+\\cdots+N(d,k_{m})}{m}\\,<\\,\\frac{3}{2}\\frac{N(d,k_{m})}{m}\\,\\leq\\,\\frac{3}{2\\log m}}\\end{array}$ . From what we computed previously, we have that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{m}{L_{m}}<\\frac{L_{m}>N(d,k)>m2^{0.9\\cdot2^{l}-\\frac{1}{2}l}}{d^{0.9-\\frac{1}{2}\\log\\log d\\log d}}<\\frac{1}{d^{0.89}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Note finally that for $k=\\alpha2^{l}$ , the leading term in the exponent of $N(d,k)$ as derived above is $\\alpha2^{2l}$ , all other terms are at most $l2^{l}$ , which is sub-polynomial in $m$ , so this shows that $N(d,\\alpha k)\\approx m^{\\alpha}$ . ", "page_idx": 31}, {"type": "text", "text": "Remark 24. Note that for the polynomially increasing $d$ , there are sequences $(m,d)$ for which we do not get benign overftiting, i.e. when $d=m^{k}$ , for integer $k$ . Similarly in this case, there are sequences of $(m,d)$ for which we do not get benign overfitting, but is much harder to identify them. ", "page_idx": 31}, {"type": "text", "text": "C.3 Proof of Corollary 13 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Corollary 3 (Benign overftiting with Gaussian kernel and sub-polynomial dimension). Let $K$ be the Gaussian kernel on the sphere $\\breve{\\mathbb{S}}^{d-1}$ with a fixed bandwidth, and suppose that for $l\\in\\mathbb N$ the dimension and sample size scale as $d=2^{2^{l}}$ , $m=2^{2^{2l}}$ , i.e. $d=\\exp\\left({\\sqrt{\\log m}}\\right)$ . Consider learning a sequence of target functions $f_{d}^{*}$ as in Assumption 4 with $S_{d}\\leq m^{\\frac{1}{4}}$ . Then, we have that the minimum norm interpolating solution achieves the Bayes error in the limit $(m,d)\\to\\infty$ . In particular, for $d\\geq4$ and $m\\ge16$ we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\tilde{R}(\\hat{f}_{0})\\leq\\left(1-\\frac{1}{\\log m}\\right)^{-1}\\left(1-\\exp\\left(-0.89\\sqrt{\\log m}\\right)\\right)^{-1}\\sigma^{2}+2B^{2}\\frac{1}{m}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "If we take $S_{d}=\\mathrm{poly}(d)$ , then we can improve the $\\frac{1}{m}$ error dependence to $\\frac{1}{m^{2-\\varepsilon}}$ . Furthermore, we can improve the bound on $S_{d}$ by reducing the rate of convergence. Let $s_{d}$ be an integer such that $S_{d}\\leq N_{s_{d}}$ . Then error dependence is $\\begin{array}{r}{\\frac{1}{m^{2}}s_{d}N(d,s_{d})d^{(1+\\varepsilon)s_{d}}\\stackrel{-}{<}\\frac{1}{m^{2}}d^{2(1+\\varepsilon)s_{d}}}\\end{array}$ . ", "page_idx": 31}, {"type": "text", "text": "Proof: We have that from Theorem 23 that in this case $k_{m}=2^{l}+l-1=\\log d+\\log\\log d-1$ , $\\begin{array}{r}{L_{m}=\\Theta(\\frac{m}{\\log m})}\\end{array}$ and $L_{m}\\geq\\Theta(m d^{0.89})$ . Therefore, we have that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathcal{E}_{0}\\leq\\left(1-\\frac{1}{\\log m}\\right)^{-1}\\left(1-\\exp\\left(-0.89\\sqrt{\\log m}\\right)\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "To finish the proof, we need to estimate $\\begin{array}{r}{N(d,1)\\frac{1}{\\tilde{\\lambda}_{1}^{2}}+\\cdot\\cdot\\cdot N(d,k_{m})\\frac{1}{\\tilde{\\lambda}_{k_{m}}^{2}}}\\end{array}$ . Note that the eigenvalues are sorted and $N(d,k-1)=o(N(d,k+1))$ since $k_{m}=o(d)$ . This implies that it suffices to estimate $N(d,k_{m})\\frac{1}{\\tilde{\\lambda}_{k_{m}}^{2}}$ . Note that for the first eigenvalue, we have from Corollary 31 for $\\begin{array}{r}{\\alpha=1+\\frac{2}{\\tau_{m}^{2}}}\\end{array}$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{1}{d^{\\alpha}}<\\tilde{\\lambda}_{1}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "So, for $\\tilde{\\lambda}_{k}$ , it holds that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{\\tilde{\\lambda}_{k}}{\\tilde{\\lambda}_{1}}>\\prod_{i=1}^{k}\\frac{\\frac{2}{\\tau_{m}^{2}}}{\\frac{2}{\\tau_{m}^{2}}+2(i+\\frac{d}{2}-1)}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Therefore, we have that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{1}{\\bar{\\lambda}_{k}}<\\frac{1}{\\bar{\\lambda}_{1}}\\prod_{i=1}^{k}\\frac{\\frac{2}{\\tau_{m}^{2}}+2(i+\\frac{d}{2}-1)}{\\frac{2}{\\tau_{m}^{2}}}=d^{\\alpha}\\left(\\frac{\\tau_{m}^{2}}{2}\\right)^{k}\\frac{\\Gamma(d+2k_{m}+\\frac{2}{\\tau_{m}^{2}})}{\\Gamma(\\frac{d}{2})\\Gamma(d+\\frac{2}{\\tau_{m}^{2}})}<d^{\\alpha}\\left(\\tau_{m}^{2}\\right)^{k}\\left(\\frac{d}{2}+k+\\frac{2}{\\tau_{m}^{2}}\\right)^{k}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Note also that if $k=\\beta k_{m}$ for some constant $\\beta$ then ", "page_idx": 32}, {"type": "equation", "text": "$$\nN(d,k)\\approx m^{\\beta},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "in the sense of Theorem 23. So, N(d, km)\u03bb\u02dc21 $\\begin{array}{r}{N(d,k_{m})\\frac{1}{\\tilde{\\lambda}_{k_{m}}^{2}}<m^{\\beta}d^{2k(1+\\varepsilon)-2\\alpha}}\\end{array}$ . We want to take $k$ as large as possible, so we will take it $k=\\beta k_{m}$ . Then $d^{2k}\\approx m^{2\\beta}$ . Therefore, as long as $3\\beta<2$ , we will have a polynomially scaling error. So we can take $\\begin{array}{r}{\\beta=\\frac{2}{3}}\\end{array}$ . Note that with Theorem 20, we actually get even better dependence of the error, like $m^{\\beta}d^{k(1+\\varepsilon)-\\alpha}$ . Plugging the first estimate into Theorem 7, for $3\\alpha=1$ , we have that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\tilde{R}(\\hat{f}_{0})\\leq\\left(1-\\frac{1}{\\log m}\\right)^{-1}\\left(1-\\frac{1}{d^{0.89}}\\right)^{-1}\\sigma^{2}}\\\\ {+\\;B^{2}\\left(1-\\frac{1}{\\log m}\\right)^{-1}\\left(1-\\frac{1}{d^{0.89}}\\right)^{-1}\\frac{1}{m^{2}}m^{3\\beta}}&{\\leq\\left(1-\\frac{1}{\\log m}\\right)^{-1}\\left(1-\\frac{1}{d^{0.89}}\\right)^{-1}\\sigma^{2}}\\\\ {+\\;B^{2}\\left(1-\\frac{1}{\\log m}\\right)^{-1}\\left(1-\\frac{1}{d^{0.89}}\\right)^{-1}\\frac{1}{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "If we want to achieve $S_{d}=\\mathrm{poly}(d)$ , note that we just need to take $k$ to not scale wit $d$ . That is, if we want $\\deg S_{d}=n$ , then we can take $k=n$ . The eigenvalue will be ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{1}{\\tilde{\\lambda}_{n}}<d^{\\alpha+2n}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The multiplicity will be $N(d,n)=\\Theta(d^{n})$ . Note that then error scales as $\\frac{d^{5n}}{m^{2}}$ , i.e. since $d$ is sub poly $m$ , we can take any such $n$ . ", "page_idx": 32}, {"type": "text", "text": "C.4 Kernel conditions ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We have three main assumptions on kernels. First, the sum of its eigenvalues is bounded. ", "page_idx": 32}, {"type": "text", "text": "Assumption 6 (Bounded sum of eigenvalues). Assume that the kernel $K$ has a bounded sum of eigenvalues, i.e. there is a constant $A$ such that $\\textstyle\\sum_{i=1}^{\\infty}N(i){\\tilde{\\lambda}}_{i}\\leq A$ . For a sequence of kernels $K^{(d)}$ , assume that all such $A^{(d)}$ are bounded by some  constant $A$ .2 ", "page_idx": 32}, {"type": "text", "text": "Second, there is a lower bound on the eigenvalues. ", "page_idx": 33}, {"type": "text", "text": "Assumption 8 (Lower bound on eigenvalues). Assumme that the kernel $K$ has eigenvalues that are not too small, i.e. there is a constant $b$ such that $\\begin{array}{r}{\\operatorname*{max}_{i\\leq k_{m}}\\left(\\frac{1}{\\tilde{\\lambda}_{i}}\\right)<\\frac{m-L_{m}}{b}}\\end{array}$ . For a sequence of kernels $K^{(d)}$ , assume that for the corresponding $m=m(d)$ (since $d=d(m)$ , we can also \"invert\" the dependence) all such $b^{(d)}$ are bounded below by some $b$ . ", "page_idx": 33}, {"type": "text", "text": "And third, the eigenvalues don\u2019t decrease too fast. ", "page_idx": 33}, {"type": "text", "text": "Assumption 10 (Eigenvalue decay). The eigenvalues don\u2019t decrease too quickly, i.e. for $k_{m}$ as in Definition 5, we have that there is a constant $c$ such that $\\begin{array}{r}{\\operatorname*{max}_{i\\leq k_{m}}\\left(\\frac{1}{\\tilde{\\lambda_{i}}}\\right)\\leq c N(k_{m})}\\end{array}$ . For increasing dimension, we require that $\\begin{array}{r}{\\operatorname*{max}_{i\\leq k_{m}}\\left(\\frac{1}{\\tilde{\\lambda}_{i}}\\right)\\leq c N(d,k_{m})}\\end{array}$ . ", "page_idx": 33}, {"type": "text", "text": "Note that Assumption 6 will hold quite broadly, in particular at least for dot product kernels on the sphere (that are effectively the same kernel just in increasing dimensions). Note also that the Assumption 10 is almost always stronger than Assumption 8. This follows from the definition of $L_{m}$ and $k_{m}$ , i.e. since $L_{m}\\,=\\,\\stackrel{.}{N}(1)\\,\\dot{+}\\cdot\\cdot\\cdot+N(k_{m})$ , we have that $N(k_{m})\\,<\\,m$ and as long as $L_{m}<\\Theta(m)$ , we can take $b$ large. This is usually the case (we show previously in Appendix C.2 for dot-product kernels on the sphere). For $d=\\omega(\\log m)$ , we even have $L_{m}=o(m)$ . So in particular $N(\\bar{d},k_{m})\\,<\\,\\Theta(m-L_{m})$ and for $d\\,=\\,\\omega(\\log m)$ , we have $N(d,k_{m})\\,=\\,o(m\\,-\\,L_{m})$ , so indeed Assumption 10 is stronger than Assumption 8. ", "page_idx": 33}, {"type": "text", "text": "Assumption 6 and Assumption 10 are also taken to be true in the literature on the polynomially scaling dimension [4, 64]. ", "page_idx": 33}, {"type": "text", "text": "First assumption: For dot-product kernels on the sphere, we know that the eigenfunction $\\phi_{i}$ are actually the spherical harmonics $Y_{k s}$ . Let $K(x,y)={\\bar{h}}(\\|x-y\\|)$ . Note that for $\\bar{\\boldsymbol{x}}\\in\\mathbb{S}^{d-1}$ , it holds that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{N(d,k)}Y_{k s}(x)^{2}=N(d,k).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Therefore, since ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{K(x,y)=\\displaystyle\\sum_{i}\\lambda_{i}\\phi_{i}}}\\\\ {{\\displaystyle h(0)=K(x,x)=\\sum_{k}\\sum_{s=1}^{N(d,k)}\\tilde{\\lambda}_{k}Y_{k s}(x)^{2}=\\sum_{k}N(d,k)\\tilde{\\lambda}_{k}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "So the assumption in Theorem 7 holds with $A=h(0)$ . For the Gaussian kernel, this is $A\\,=\\,1$ .   \nSimilarly, $A=1$ for Laplace kernel. ", "page_idx": 33}, {"type": "text", "text": "Second Assumption: For the eigenvalue assumption, $\\begin{array}{r}{\\operatorname*{max}_{i\\leq k_{m}}\\left(\\frac{1}{\\tilde{\\lambda}_{i}}\\right)<\\frac{m-L_{m}}{A b}}\\end{array}$ , note the following. We will usually have $L_{m}=o(m)$ . In particular, whenever $d=\\omega(\\log m)$ for a dot-product kernel on the sphere this will hold. ", "page_idx": 33}, {"type": "text", "text": "The eigegnvalue assumption will hold for the Gaussian kernel in the non-integer polynomial regime, $d^{\\alpha}=m$ . In Appendix C.3, we showed that $\\begin{array}{r}{\\frac{1}{\\tilde{\\lambda}_{k}}<d^{k}}\\end{array}$ , so since $k_{m}=\\lfloor\\alpha\\rfloor$ , we will have $\\begin{array}{r}{\\frac{1}{\\tilde{\\lambda}_{k}}<\\bar{d}^{\\alpha}-}\\end{array}$ $\\Theta(d^{\\lfloor\\alpha\\rfloor})$ . Similarly, we can show this for the Gaussian kernel for $d=\\log m$ and $d=\\exp({\\sqrt{\\log m}})$ regime. Note that in Corollary 31, we showed that $\\tilde{\\lambda}_{1}>\\textstyle{\\frac{1}{d^{\\alpha}}}$ and $\\begin{array}{r}{\\tilde{\\lambda}_{k}>\\frac{1}{d^{\\alpha+k}}}\\end{array}$ , so then $\\frac{1}{\\tilde{\\lambda}_{k}}$ Additionally, in the $d\\,=\\,\\exp({\\sqrt{\\log m}})$ regime, we showed that $L_{m}\\,=\\,o(m)$ and that $\\begin{array}{r}{\\frac{1}{\\tilde{\\lambda}_{k}}\\,=\\,\\Theta(L_{m})}\\end{array}$ for any $k\\,=\\,\\Theta(k_{m})\\,<\\,k_{m}$ . The same proof shows that this assumption holds for any sub-polynomially scaling dimension and Gaussian kernel. ", "page_idx": 33}, {"type": "text", "text": "Furthermore, this assumption can be weakened to the following: there exists $b>0$ such for all $\\begin{array}{r}{k\\le L_{m},b\\sum_{i>k}\\lambda_{i}<\\lambda_{k+1}^{'}(m-k)}\\end{array}$ . Additionally, since the assumption is specific to our approach, it seems to be possible to weaken it even further. ", "page_idx": 33}, {"type": "text", "text": "Third assumption: This assumption holds for dot-product kernels on the sphere. It was shown in Zhang et al. [64] (Lemma 5.2.1) under another assumption similar to (1) that it holds for polynomially scaling dimension. It is also assumed in Barzilai and Shamir [4] (page 11) for polynomially scaling dimension. Furthermore, Cao et al. [9] (Theorem 4.3) shows that this holds for Neural Tangent Kernel even more broadly, i.e. for all $i$ and not only $i\\leq k_{m}$ . ", "page_idx": 34}, {"type": "text", "text": "D Sharp bounds on eigenvalues corresponding to Gaussian and Laplace Kernels ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In this section, we will summarize and prove the results about eigenvalues of Gaussian and Laplace kernels when $\\mathcal{X}\\sim\\mathrm{Unif}(\\mathbb{S}^{d-1})$ . Given a positive semi-definite kernel function $K:\\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R}$ , we can decompose it as ", "page_idx": 34}, {"type": "equation", "text": "$$\nK(x,t)=\\sum_{i=1}^{\\infty}\\lambda_{k}\\phi_{k}(x)\\phi_{k}(t),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\lambda_{k}$ and $\\phi_{k}$ are the eigenvalues and eigenfunctions of the operator associated to $K,\\,L_{K}$ $L_{\\mathcal{D}_{\\mathcal{X}}}^{2}(\\mathbb{S}^{d-1})\\rightarrow L_{\\mathcal{D}_{\\mathcal{X}}}^{2}(\\mathbb{S}^{d-1})$ ", "page_idx": 34}, {"type": "equation", "text": "$$\nL_{K}(f)(x)=\\int_{X}K(x,t)f(t)d\\mu(t).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "$L_{K}$ is a Hilbert-Schmidt operator and has a countable system of non-negative eigenvalues $\\lambda_{k}$ satisfying $\\textstyle\\sum_{k=1}^{\\infty}\\lambda_{k}^{2}<\\infty$ . The corresponding $L_{\\mathcal{D}_{\\boldsymbol{x}}}^{2}(\\mathbb{S}^{d-1})$ -normalized eigenfunctions $\\{\\phi_{k}(x)\\}_{k=1}^{\\infty}$ form an orthonormal basis of $L_{\\mathcal{D}_{\\mathcal{X}}}^{2}(\\mathbb{S}^{d-1})$ . The eigenfunctions in this case are given by spherical harmonics, $\\mathcal{V}_{i,s}$ . The eigenvalues corresponding to all $y_{i,s}$ for fixed $i$ have the same eigenvalue, $\\tilde{\\lambda}_{i}$ . Eigenvalues $\\tilde{\\lambda}_{i}$ are reach repeated $\\begin{array}{r}{N(d,i)=\\frac{(2i+d-2)(i+d-3)!}{i!(d-2)!}}\\end{array}$ times. So the sequence $\\{\\lambda_{i}\\}_{i=1}^{\\infty}=$ $\\tilde{\\lambda}_{1},\\hdots,\\tilde{\\lambda}_{1},\\tilde{\\lambda}_{2},\\hdots,\\tilde{\\lambda}_{2},\\hdots.$ Using Funk-Hecke formula, we can compute $\\tilde{\\lambda}_{k}$ explicitly, both for the case of Gaussian [46] kernel. For Gaussian kernel, we have the following theorem about the eigenvalues $\\tilde{\\lambda}_{k}$ . ", "page_idx": 34}, {"type": "text", "text": "Theorem 25 (Eigenvalues of the Gaussian kernel [46]). Let $\\mathcal{X}\\sim\\mathrm{Unif}(\\mathbb{S}^{d-1})$ , with $d\\in\\mathbb{N}$ and $d\\geq2$ . For $\\begin{array}{r}{K(x,t)=\\exp\\left(-\\frac{\\|x-t\\|^{2}}{\\tau_{m}^{2}}\\right),\\tau_{m}>0}\\end{array}$ , and $k\\in\\ensuremath{\\mathbb{N}}_{0}$ we have that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\tilde{\\lambda}_{k}=e^{-\\frac{2}{\\tau_{m}^{2}}}\\tau_{m}^{d-2}I_{k+\\frac{d}{2}-1}\\left(\\frac{2}{\\tau_{m}^{2}}\\right)\\Gamma\\left(\\frac{d}{2}\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Each $\\tilde{\\lambda}_{k}$ occurs with multiplicity $\\begin{array}{r}{N(d,k)=\\frac{\\left(2k+d-2\\right)(k+d-3)!}{k!(d-2)!}}\\end{array}$ (we use $\\tilde{\\lambda}$ notation to indicate that it has multiplicity) and its corresponding eigenfunction are the spherical harmonics of order $k$ on $\\mathbb{S}^{d-1}$ . Here, $I_{v}(z),v,z\\in\\mathbb{C}$ is the modified Bessel function of the first kind ", "page_idx": 34}, {"type": "equation", "text": "$$\nI_{v}(z)=\\sum_{j=0}^{\\infty}\\frac{1}{j!\\Gamma(v+j+1)}\\left(\\frac{z}{2}\\right)^{v+2j}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "It will be useful to know the size of $N(d,i)$ . Also, the size of the sum of the first $k$ multiplicities will be useful. Let $\\begin{array}{r}{N_{k}=\\sum_{i=1}^{k}N(d,i)}\\end{array}$ . ", "page_idx": 34}, {"type": "text", "text": "Lemma 26 (Size of multiplicity). For $\\begin{array}{r}{N(d,i)=\\frac{(2k+d-2)(k+d-3)!}{k!(d-2)!}}\\end{array}$ , it holds that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{1}{(d-2)!}k^{d-2}\\leq N(d,i)\\leq2^{d-1}k^{d-2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Additionally, there exist constants $n_{l},n_{u}>0$ depending on the dimension $d$ such that $N_{k}$ is bounded below and above by the following ", "page_idx": 34}, {"type": "equation", "text": "$$\nn_{l}k^{d-1}\\le N_{k}\\le n_{u}k^{d-1}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof. Since $(2k+j)\\leq(2k)j$ for $j\\geq2$ and $(k+1)\\leq2k$ , we have that ", "page_idx": 35}, {"type": "equation", "text": "$$\nN(d,k)=\\frac{(2k+d-2)(k+d-3)\\cdot...(k+1)}{(d-2)!}\\leq\\frac{(d-2)!(2k)^{d-3}(2k)}{(d-2)!}\\leq2^{d-1}k^{d-2}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and ", "page_idx": 35}, {"type": "equation", "text": "$$\nN(d,k)={\\frac{(2k+d-2)(k+d-3)\\dots(k+1)}{(d-2)!}}\\geq{\\frac{1}{(d-2)!}}k^{d-2}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Note that by Bernoulli\u2019s formula ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sum_{l=1}^{k}l^{d-2}=\\frac{1}{d-1}\\left(k^{d-1}+o(k^{d-1})\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Therefore, we have that ", "page_idx": 35}, {"type": "equation", "text": "$$\nn_{l}k^{d-1}\\le\\frac{1}{(d-1)(d-2)!}\\left(k^{d-1}+o(k^{d-1})\\right)\\le N_{k}\\le\\frac{2^{d-1}}{d-1}\\left(k^{d-1}+o(k^{d-1})\\right)\\le n_{u}k^{d-1}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Lemma 27 (Inverting the index). If the index of an eigenvalue $\\hat{\\lambda}_{j}$ is such that $N_{k-1}\\leq j\\leq N_{k}-1$ , then $\\hat{\\lambda}_{j}=\\lambda_{k}$ . We will denote such $j$ with $\\tilde{k}$ , i.e. $\\tilde{k}$ is an index such that $\\hat{\\lambda}_{\\tilde{k}}=\\lambda_{k}$ . ", "page_idx": 35}, {"type": "text", "text": "Proof. Immediate from the fact that $\\{\\lambda_{i}\\}_{i=1}^{\\infty}$ is a sequence with $\\tilde{\\lambda}_{i}$ repeating $N(d,i)$ times, in that order. ", "page_idx": 35}, {"type": "text", "text": "An interesting property of eigenvalues of the Gaussian kernel is that they are sorted because the Modified Bessel functions are [48]. In particular, $I_{v+1}(x)<I_{v}(x)$ for all $v,x>0$ , so $\\tilde{\\lambda}_{i+1}<\\tilde{\\lambda}_{i}$ . This is not the case for the Laplace kernel. ", "page_idx": 35}, {"type": "text", "text": "D.1 Bounds on eigenvalues of the Gaussian kernel ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Theorem 28 (Size of Ratios of Eigenvalues for Gaussian Kernel). Let $\\begin{array}{r l r}{T}&{{}=}&{(\\frac{2}{\\tau_{m}^{2}})}\\end{array}$ . For $j~\\leq$ ${\\sqrt{T}}t(m),k\\leq{\\sqrt{T}}t(m)$ , where $t(m)\\rightarrow0$ as $m\\rightarrow\\infty$ , we have that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\exp(-6t(m))<\\frac{\\tilde{\\lambda}_{k+j}}{\\tilde{\\lambda}_{k}}<1.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "For $j\\geq\\sqrt{T}t(m)$ , where $t(m)\\to\\infty$ as $m\\rightarrow\\infty$ we have that for any $k$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{\\tilde{\\lambda}_{k+j}}{\\tilde{\\lambda}_{k}}<\\exp\\left(-\\frac{t(m)^{2}}{4}\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. First of all, note that from [48], we have that for $v\\geq\\textstyle{\\frac{1}{2}}$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{(v)+\\sqrt{(v)^{2}+x^{2}}}{x}>\\frac{I_{v-1}(x)}{I_{v}(x)}>\\frac{(v-\\frac{1}{2})+\\sqrt{(v-\\frac{1}{2})^{2}+x^{2}}}{x}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "In particular, this means that for the eigenvalues $\\tilde{\\lambda}_{k}$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\frac{(\\frac{2}{\\tau_{m}^{2}})}{\\langle k+\\frac{d}{2})+\\sqrt{(k+\\frac{d}{2})^{2}+(\\frac{2}{\\tau_{m}^{2}})^{2}}}<\\frac{\\tilde{\\lambda}_{k+1}}{\\tilde{\\lambda}_{k}}=\\frac{I_{k+1+\\frac{d}{2}-1}(\\frac{2}{\\tau_{m}^{2}})}{I_{k+\\frac{d}{2}-1}(\\frac{2}{\\tau_{m}^{2}})}<\\frac{(\\frac{2}{\\tau_{m}^{2}})}{(k+\\frac{d}{2}-\\frac{1}{2})+\\sqrt{(k+\\frac{d}{2}-\\frac{1}{2})^{2}+(\\frac{2}{\\tau_{m}^{2}})^{2}}},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "This can be bounded with a simpler expression as follows ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\frac{\\big(\\frac{2}{\\tau_{m}^{2}}\\big)}{2(k+\\frac{d}{2})+\\big(\\frac{2}{\\tau_{m}^{2}}\\big)}<\\frac{\\tilde{\\lambda}_{k+1}}{\\tilde{\\lambda}_{k}}=\\frac{I_{k+1+\\frac{d}{2}-1}\\big(\\frac{2}{\\tau_{m}^{2}}\\big)}{I_{k+\\frac{d}{2}-1}\\big(\\frac{2}{\\tau_{m}^{2}}\\big)}<\\frac{\\big(\\frac{2}{\\tau_{m}^{2}}\\big)}{\\big(k+\\frac{d}{2}-\\frac{1}{2}\\big)+\\big(\\frac{2}{\\tau_{m}^{2}}\\big)}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We will use these bounds to derive tight bounds for the ratios \u03bb\u02dck+j. Note the following ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\prod_{i=1}^{j}\\frac{\\left(\\frac{2}{\\tau_{m}^{2}}\\right)}{2(k+i-1+\\frac{d}{2})+(\\frac{2}{\\tau_{m}^{2}})}<\\frac{\\tilde{\\lambda}_{k+j}}{\\tilde{\\lambda}_{k}}=\\prod_{i=1}^{j}\\frac{\\tilde{\\lambda}_{k+i}}{\\tilde{\\lambda}_{k+i-1}}<\\prod_{i=1}^{j}\\frac{(\\frac{2}{\\tau_{m}^{2}})}{(k+i-1+\\frac{d}{2}-\\frac{1}{2})+(\\frac{2}{\\tau_{m}^{2}})}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Note now that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left(\\frac{\\left(\\frac{2}{\\tau_{m}^{2}}\\right)}{2(k+j-1+\\frac{d}{2})+\\left(\\frac{2}{\\tau_{m}^{2}}\\right)}\\right)^{j}<\\prod_{i=1}^{j}\\frac{\\left(\\frac{2}{\\tau_{m}^{2}}\\right)}{2(k+i-1+\\frac{d}{2})+\\left(\\frac{2}{\\tau_{m}^{2}}\\right)}<\\frac{\\tilde{\\lambda}_{k+j}}{\\tilde{\\lambda}_{k}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Note that since $(x+j-i)(x+i)=x^{2}+i x+(j-i)i$ , we have that $(x+j-i)(x+i)\\geq(x+j)x$ , therefore ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\prod_{i=1}^{j}\\frac{\\left(\\frac{2}{\\tau_{m}^{2}}\\right)}{\\left(k+i-1+\\frac{d}{2}-\\frac{1}{2}\\right)+\\left(\\frac{2}{\\tau_{m}^{2}}\\right)}}\\\\ {\\displaystyle<\\left(\\frac{\\left(\\frac{2}{\\tau_{m}^{2}}\\right)}{\\left(k+j-1+\\frac{d}{2}-\\frac{1}{2}\\right)+\\left(\\frac{2}{\\tau_{m}^{2}}\\right)}\\right)^{\\frac{j-1}{2}}\\left(\\frac{\\left(\\frac{2}{\\tau_{m}^{2}}\\right)}{\\left(k+\\frac{d}{2}-\\frac{1}{2}\\right)+\\left(\\frac{2}{\\tau_{m}^{2}}\\right)}\\right)^{\\frac{j+1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We use $j+1$ and $j-1$ to account for the fact that $j$ might be odd when we split into $(j-1)/2$ pairs.   \nWhen $j$ is even, we split into $\\textstyle{\\frac{j}{2}}$ pairs and use the fact that the term with exponent $\\textstyle{\\frac{j+1}{2}}$ is larger. ", "page_idx": 36}, {"type": "text", "text": "Let $\\begin{array}{r}{T=\\left(\\frac{2}{\\tau_{m}^{2}}\\right)}\\end{array}$ . We can bound the ratio \u03bb\u02dck+j tightly now. ", "page_idx": 36}, {"type": "text", "text": "When $j\\leq\\sqrt{T}m^{-\\delta},k\\leq\\sqrt{T}m^{-\\delta}$ , we have that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\exp(-6m^{-\\delta})<\\frac{\\tilde{\\lambda}_{k+j}}{\\tilde{\\lambda}_{k}}<1.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "When $j\\geq\\sqrt{T}m^{\\delta}$ , we have that for any $k$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\frac{\\tilde{\\lambda}_{k+j}}{\\lambda_{k}}<\\exp(-\\frac{m^{2\\delta}}{4}).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "To see why this is true, note first that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\frac{\\displaystyle(\\frac{2}{\\tau_{m}^{2}})}{\\displaystyle2(k+j-1+\\frac{d}{2})+(\\frac{2}{\\tau_{m}^{2}})}\\right)^{j}}\\\\ &{=\\left(\\frac{T}{\\displaystyle2(k+j-1+\\frac{d}{2})+T}\\right)^{j}>\\left(\\frac{T}{\\displaystyle2(k+j-1+\\frac{d}{2})+T}\\right)^{\\sqrt{T}}}\\\\ &{>\\left(\\frac{T}{\\displaystyle5(\\sqrt{T}m^{-\\delta})+T}\\right)^{\\sqrt{T}}=\\left(\\frac{1}{\\displaystyle5(\\frac{1}{\\sqrt{T}}m^{-\\delta})+1}\\right)^{\\sqrt{T}}\\rightarrow\\exp(-5m^{-\\delta}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "as $m\\rightarrow\\infty$ . For the second inequality, note that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left(\\frac{\\left(\\frac{2}{\\tau_{m}^{2}}\\right)}{\\left(k+\\frac{d}{2}-\\frac{1}{2}\\right)+\\left(\\frac{2}{\\tau_{m}^{2}}\\right)}\\right)^{\\frac{j+1}{2}}<1.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We also have that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{\\big(\\frac{2}{\\tau_{m}^{2}}\\big)}{\\big:+\\,j-1+\\frac{d}{2}-\\frac{1}{2}\\big)+\\big(\\frac{2}{\\tau_{m}^{2}}\\big)}\\bigg)^{\\frac{j-1}{2}}=\\Bigg(\\frac{T}{(k+j-1+\\frac{d}{2}-\\frac{1}{2})+T}\\Bigg)^{\\frac{j-1}{2}}<\\Bigg(\\frac{T}{(\\sqrt{T}m^{\\delta})+T}\\Bigg)^{\\frac{\\sqrt{T}m^{\\delta}-1}{2}}}\\\\ &{=\\Bigg(\\frac{1}{\\frac{(\\sqrt{T}m^{\\delta})}{T}+1}\\Bigg)^{\\frac{\\sqrt{T}m^{\\delta}}{3}}<\\Bigg(\\frac{1}{\\frac{m^{\\delta}}{\\sqrt{T}}+1}\\Bigg)^{\\frac{\\sqrt{T}m^{\\delta}}{3}}=\\Bigg(\\bigg(\\frac{1}{\\frac{m^{\\delta}}{\\sqrt{T}}+1}\\Bigg)^{\\frac{\\sqrt{T}m^{\\delta}-1}{2}}}\\\\ &{\\rightarrow\\exp(-\\frac{m^{2\\delta}}{3})\\rightarrow0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "as $m\\rightarrow\\infty$ . Note that we can turn the limits $a_{m}\\to t(m)$ into inequalities of the form $(1-\\varepsilon)t(m)<$ $a_{m}<(1+\\varepsilon)t(m)$ for some $\\varepsilon$ and all $m$ since the convergence is uniform. ", "page_idx": 37}, {"type": "text", "text": "The following is a simple corollary. ", "page_idx": 37}, {"type": "text", "text": "Corollary 29. Let $\\begin{array}{r}{T=\\left(\\frac{2}{\\tau_{m}^{2}}\\right)}\\end{array}$ . If $T>1$ and the index of the eigenvalue $i$ is such that $i\\leq(k{\\sqrt{T}})^{d-1}$ , we have that for $\\lambda_{i}$ it holds that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{\\lambda_{i}}{\\lambda_{1}}\\geq\\left(\\frac{1}{1+\\frac{k}{\\sqrt{T}}}\\right)^{k\\sqrt{T}}\\geq\\exp(-k^{2}).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. Note that $\\left({\\frac{1}{1+{\\frac{k}{\\sqrt{T}}}}}\\right)^{k{\\sqrt{T}}}$ is increasing in $\\sqrt{T}$ and note that $\\sqrt{T}$ increases as $m$ increases. Note that for $\\sqrt{T}=1$ it suffices to show $e(k)>1+k$ which is true for all $k$ as long as $\\sqrt{T}>1$ . \u53e3 ", "page_idx": 37}, {"type": "text", "text": "The following simple bound also holds for eigenvalues of Gaussian kernel. ", "page_idx": 37}, {"type": "text", "text": "Proposition 30 (Ratio of eigenvalues bounded above [46]). For the eigenvalues associated to the Gaussian Kernel $\\tilde{\\lambda}_{k}$ of bandwidth $\\tau_{m}$ we have that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{\\tilde{\\lambda}_{k+1}}{\\tilde{\\lambda}_{k}}<\\frac{1}{\\tau_{m}^{2}(k+\\frac{d}{2})}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "It is straightforward to convert the bounds on ratios of eigenvalues Theorem 25 to bounds on the sizes of the actual eigenvalues. ", "page_idx": 37}, {"type": "text", "text": "Corollary 31 (Sizes of the eigenvalues of Gaussian kernel). The following bounds hold for the largest eigenvalue of the Gaussian kernel ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{1}{\\tau_{m}^{2}+4}\\frac{\\Gamma((\\frac{2}{\\tau_{m}^{2}}+\\frac{1}{2}))\\Gamma(\\frac{d}{2})}{\\Gamma(\\frac{2}{\\tau_{m}^{2}}+\\frac{d}{2}+2)}<\\tilde{\\lambda}_{1}<\\frac{1}{\\sqrt{\\tau_{m}^{4}+4\\tau_{m}^{2}}}\\frac{\\Gamma((\\frac{2}{\\tau_{m}^{2}}+\\frac{1}{2}))\\Gamma(\\frac{d}{2})}{\\Gamma(\\frac{2}{\\tau_{m}^{2}}+\\frac{d}{2}+\\frac{3}{2})}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Therefore, for $\\tilde{\\lambda}_{k}$ , we have that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\frac{1}{\\tau_{m}^{2k}}\\frac{\\Gamma\\left(\\left(\\frac{2}{\\tau_{m}^{2}}+k+\\frac{1}{2}\\right)\\right)}{\\Gamma\\left(\\frac{2}{\\tau_{m}^{2}}+k+\\frac{d}{2}+2\\right)}\\tilde{\\lambda}_{1}<\\tilde{\\lambda}_{k}}\\\\ &{}&\\\\ &{}&{\\tilde{\\lambda}_{k}<2^{k}\\frac{1}{\\tau_{m}^{2k}}\\frac{\\Gamma\\left(\\left(\\frac{2}{\\tau_{m}^{2}}+k+\\frac{1}{2}\\right)\\right)}{\\Gamma\\left(\\frac{2}{\\tau_{m}^{2}}+k+\\frac{d}{2}+\\frac{3}{2}\\right)}\\tilde{\\lambda}_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Furthermore, for $\\tau_{m}$ fixed we have that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\tilde{\\lambda}_{1}>\\frac{1}{\\tau_{m}^{2}+4}\\frac{1}{\\left(\\left(\\frac{d}{2}+\\frac{2}{\\tau_{m}^{2}}\\right)^{1+\\frac{2}{\\tau_{m}^{2}}}\\right)}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\tilde{\\lambda}_{k}>\\frac{1}{\\tau_{m}^{2}+4}\\frac{1}{\\big(\\big(\\frac{d}{2}+\\frac{2}{\\tau_{m}^{2}}\\big)^{1+\\frac{2}{\\tau_{m}^{2}}}\\big)}\\frac{1}{\\tau_{m}^{2k}}\\frac{1}{\\big(\\big(k+\\frac{d}{2}+\\frac{2}{\\tau_{m}^{2}}\\big)^{k}\\big)}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. Note that from [62] we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\frac{1}{1+\\frac{4}{\\tau_{m}^{2}}}\\exp\\left(\\frac{2}{\\tau_{m}^{2}}\\right)<I_{0}\\left(\\frac{2}{\\tau_{m}^{2}}\\right)<\\frac{1}{\\sqrt{1+\\frac{4}{\\tau_{m}^{2}}}}\\exp\\left(\\frac{2}{\\tau_{m}^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We also have from [48] ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\frac{\\big(\\frac{2}{\\tau_{m}^{2}}\\big)}{2(i+1)+\\big(\\frac{2}{\\tau_{m}^{2}}\\big)}<\\frac{I_{i+1}\\big(\\frac{2}{\\tau_{m}^{2}}\\big)}{I_{i}\\big(\\frac{2}{\\tau_{m}^{2}}\\big)}<\\frac{\\big(\\frac{2}{\\tau_{m}^{2}}\\big)}{2(i+\\frac{1}{2})}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Then ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\tilde{\\lambda}_{k}=e^{-\\frac{2}{\\tau_{m}^{2}}}\\tau_{m}^{d-2}I_{k+\\frac{d}{2}-1}\\left(\\frac{2}{\\tau_{m}^{2}}\\right)\\Gamma\\left(\\frac{d}{2}\\right).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "So then ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\frac{1}{\\tau_{m}^{d}}\\frac{\\Gamma((\\frac{2}{\\tau_{m}^{2}}+\\frac{1}{2}))}{\\Gamma(\\frac{2}{\\tau_{m}^{2}}+\\frac{d}{2}+2)}<\\prod_{i=0}^{\\frac{d}{2}-1}\\frac{(\\frac{2}{\\tau_{m}^{2}})}{2(i+1)+(\\frac{2}{\\tau_{m}^{2}})}<\\frac{I_{1+\\frac{d}{2}-1}\\left(\\frac{2}{\\tau_{m}^{2}}\\right)}{I_{0}\\left(\\frac{2}{\\tau_{m}^{2}}\\right)}<\\prod_{i=0}^{\\frac{d}{2}-1}\\frac{(\\frac{2}{\\tau_{m}^{2}})}{2(i+\\frac{1}{2})}=\\frac{1}{\\tau_{m}^{d}}\\frac{\\Gamma(\\frac{1}{2})}{\\Gamma(\\frac{d}{2})}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "So then ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\frac{1}{\\tau_{m}^{2}+4}\\frac{\\Gamma((\\frac{2}{\\tau_{m}^{2}}+\\frac{1}{2}))\\Gamma(\\frac{d}{2})}{\\Gamma(\\frac{2}{\\tau_{m}^{2}}+\\frac{d}{2}+2)}<\\tilde{\\lambda}_{1}<\\frac{1}{\\sqrt{\\tau_{m}^{4}+4\\tau_{m}^{2}}}\\frac{\\Gamma((\\frac{2}{\\tau_{m}^{2}}+\\frac{1}{2}))\\Gamma(\\frac{d}{2})}{\\Gamma(\\frac{2}{\\tau_{m}^{2}}+\\frac{d}{2}+\\frac{3}{2})}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "By repeating the same argument, the claim about $\\tilde{\\lambda}_{k}$ follows. ", "page_idx": 38}, {"type": "text", "text": "Note that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\tilde{\\lambda}_{1}>\\frac{1}{\\tau_{m}^{2}+4}\\Gamma(\\frac{d}{2})\\prod_{i=0}^{\\frac{d}{2}-1}\\frac{1}{(i+1)+(\\frac{2}{\\tau_{m}^{2}})}>\\frac{1}{\\tau_{m}^{2}+4}\\frac{1}{\\big(\\big(\\frac{d}{2}+\\frac{2}{\\tau_{m}^{2}}\\big)^{1+\\frac{2}{\\tau_{m}^{2}}}\\big)}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Therefore, we have that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\tilde{\\lambda}_{k}>\\tilde{\\lambda}_{1}\\prod_{i=0}^{k-1}\\frac{\\frac{2}{\\tau_{m}^{2}}}{k+\\frac{d}{2}+\\big(\\frac{2}{\\tau_{m}^{2}}\\big)}>\\tilde{\\lambda}_{1}\\frac{1}{\\tau_{m}^{2k}}\\frac{1}{\\big(\\big(k+\\frac{d}{2}+\\frac{2}{\\tau_{m}^{2}}\\big)^{k}\\big)}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: All the contributions are clearly stated in the introduction, particularly the third paragraph of the introduction. The assumptions and limitations are also clearly stated in that paragraph. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 39}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: All assumptions are clearly and concisely stated. We also provide detailed reasoning for each one of our assumptions. The assumptions are further discussed in the appendix. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 39}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: All assumptions are stated clearly before each result is presented. We give an outline of proofs in the main body and complete proofs in the appendix. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 40}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: The description of the experiments that are provided along with the figures is sufficient to reproduce the experiments. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 40}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: We provide code and instructions required to reproduce our experimental results. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 41}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: All the necessary details are included in the descriptions of the figures. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 . The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 41}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: We report appropriate information about the statistical significance of the experiments. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 41}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 42}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: The compute resources are stated along the explanation of the experiments. Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 42}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: Yes, the paper conforms to all aspects of the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 42}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 42}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. ", "page_idx": 43}, {"type": "text", "text": "\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 43}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 43}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 43}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: [NA] Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 43}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 44}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 44}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 44}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 44}]