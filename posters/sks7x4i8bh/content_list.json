[{"type": "text", "text": "Online Estimation via Offline Estimation: An Information-Theoretic Framework ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Dylan J. Foster Yanjun Han Jian Qian Alexander Rakhlin dylanfoster@microsoft.com yanjunhan@nyu.edu jianqian@mit.edu rakhlin@mit.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The classical theory of statistical estimation aims to estimate a parameter of interest under data generated from a fixed design (\u201coffilne estimation\u201d), while the contemporary theory of online learning provides algorithms for estimation under adaptively chosen covariates (\u201conline estimation\u201d). Motivated by connections between estimation and interactive decision making, we ask: is it possible to convert offline estimation algorithms into online estimation algorithms in a black-box fashion? We investigate this question from an information-theoretic perspective by introducing a new framework, Oracle-Efficient Online Estimation (OEOE), where the learner can only interact with the data stream indirectly through a sequence of offilne estimators produced by a black-box algorithm operating on the stream. Our main results settle the statistical and computational complexity of online estimation in this framework. ", "page_idx": 0}, {"type": "text", "text": "1. Statistical complexity. We show that information-theoretically, there exist algorithms that achieve near-optimal online estimation error via black-box offilne estimation oracles, and give a nearly-tight characterization for minimax rates in the OEOE framework. ", "page_idx": 0}, {"type": "text", "text": "2. Computational complexity. We show that the guarantees above cannot be achieved in a computationally efficient fashion in general, but give a refined characterization for the special case of conditional density estimation: computationally efficient online estimation via black-box offline estimation is possible whenever it is possible via unrestricted algorithms. ", "page_idx": 0}, {"type": "text", "text": "Finally, we apply our results to give offline oracle-efficient algorithms for interactive decision making. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Consider a general framework for statistical estimation specified by a tuple $(\\mathcal{X},\\mathcal{Y},\\mathcal{Z},\\mathcal{K},\\mathcal{F})$ , which we will show encompasses classification, regression, and conditional density estimation. The learner is given a parameter space $\\mathcal{F}$ (typically a function class), where each parameter $f\\in\\mathcal{F}$ is a map from the space of covariates $\\mathcal{X}$ to the space of values $\\mathcal{Z}$ . For an integer $T\\geq1$ , the learner is given a dataset $(x^{1},y^{1}),\\ldots,(x^{T},y^{T})$ , where $x^{1},\\ldots,x^{T}$ are covariates and $y^{1},\\ldots,y^{T}$ are outcomes generated via $y^{t}\\sim K(\\cdot\\mid f^{\\star}(x^{t}))$ , where $f^{\\star}\\in\\mathcal{F}$ is an unknown target parameter that the learner wishes to estimate; here $\\kappa$ is a probability kernel that assigns to each value $z\\in{\\mathcal{Z}}$ a distribution $\\kappa(\\cdot\\mid z)$ on the space of outcomes $\\boldsymbol{\\wp}$ . We adopt the shorthand $K(z)=K(\\cdot\\mid z)$ throughout. ", "page_idx": 0}, {"type": "text", "text": "The classical theory of statistical estimation typically assumes that the covariates $x^{1},\\ldots,x^{T}$ are an arbitrary fixed design, and is concerned with estimating the target parameter $f^{\\star}\\in{\\mathcal{F}}$ well in-distribution [64, 40, 62]. Formally, for a loss function $\\mathsf{D}:\\mathcal{Z}\\times\\mathcal{Z}\\to\\mathbb{R}_{\\geq0}$ on the space of values $\\mathcal{Z}$ , the goal of the learner is to output an estimator $\\widehat{f}$ based on $(x^{1},y^{1}),\\ldots,(x^{T},y^{T})$ such that the in-distribution error ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\mathbf{Est}_{\\mathsf{D}}^{\\mathsf{O f f}}(T):=\\sum_{t=1}^{T}\\mathsf{D}\\big(\\widehat{f}(x^{t}),f^{\\star}(x^{t})\\big)\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "is small; we refer to this as an offline estimation guarantee. Canonical examples include: ", "page_idx": 0}, {"type": "text", "text": "\u2022 Classification (i.e., distribution-free PAC learning [38, 63]), where $\\mathcal{Z}^{}~=~\\mathcal{Y}^{}~=~\\{0,1\\}$ , $K(f^{\\star}(x))=\\mathbb{1}_{f^{\\star}(x)}$ ,1 and ${\\mathsf{D}}_{0/1}\\bigl(\\widehat{f}(x),f^{\\star}(x)\\bigr)=\\mathbb{1}\\{\\bar{\\hat{f}}(x)\\neq f^{\\star}(x)\\}$ is the $0/1$ -loss. \u2022 Regression with a well-specified model [62, 66], where $\\mathcal{Z}=\\mathcal{Y}=\\mathbb{R}$ , $K(f^{\\star}(x))=\\mathcal{N}(f^{\\star}(x),\\sigma^{2}),$ and $\\mathsf{D}_{\\mathsf{s q}}\\big(\\widehat{f}(x),f^{\\star}(x)\\big)=(\\widehat{f}(x)-f^{\\star}(x))^{2}$ is the square loss. \u2022 Conditional density estimation [12], where $\\boldsymbol{\\wp}$ is an arbitrary alphabet, $\\begin{array}{r l r}{\\mathcal{Z}}&{{}=}&{\\Delta(\\mathcal{Y})}\\end{array}$ , $K(f^{\\star}(x))=f^{\\star}(x)$ , and $\\mathsf{D}_{\\mathsf{H}}^{2}(\\cdot,\\cdot)$ is squared Hellinger distance; see Appendix $\\mathbf{C}$ for details. ", "page_idx": 1}, {"type": "text", "text": "In parallel to statistical estimation, the contemporary theory of online learning [17, 49] provides estimation error algorithms that support adaptively chosen sequences of covariates, a meaningful form of out-of-distribution guarantee. Here, the examples $(x^{t},y^{t})$ arrive one at a time. For each step $t\\in[T]$ , the learner produces an estimator $\\widehat{f^{t}}:\\mathcal{X}\\to\\mathcal{Z}$ based on the data $(x^{1},y^{1}),\\ldots,(x^{t-1},y^{t-1})$ observed so far. The covariate $x^{t}$ is then chosen in an arbitrary fashion, and the outcome is generated via $y^{t}\\sim K(f^{\\star}(x^{t}))$ and revealed to the learner. The quality of the estimators is measured via2 ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbf{Est}_{\\mathsf{D}}^{\\mathsf{O n}}(T):=\\sum_{t=1}^{T}\\mathsf{D}\\big(\\widehat{f}^{t}(x^{t}),f^{\\star}(x^{t})\\big).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "We refer to this as an online estimation guarantee; classical examples include online classification in the mistake-bound model [41], online regression [51], and online conditional density estimation [11]. Online estimation provides a non-trivial out-of-distribution guarantee, as it requires (on average) that the learner achieves non-trivial estimation performance on covariates $x^{t}$ that can be arbitrarily far from the previous examples $\\boldsymbol{x}^{1},\\ldots,\\boldsymbol{x}^{t-1}$ . This property has many applications in algorithm design, notably in the context of interactive decision making, where it has recently found extensive use for problems including contextual bandits [25, 58, 24], reinforcement learning [27, 28], and imitation learning [56, 55]. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we investigate the relative power of online and offline estimation through a new information-theoretic perspective. It is well known that any algorithm for online estimation can be used as-is to solve offline estimation through online-to-batch conversion, a standard technique in learning theory and statistics [3, 9, 16, 61, 36, 8]. The converse is less apparent, as online estimation requires non-trivial algorithm design techniques that go well beyond classical estimators like least-squares or maximum likelihood [17]. In the case of regression with a finite class $\\mathcal{F}$ , least squares achieves optimal offline estimation error $\\mathbf{Est}_{\\mathsf{D}}^{\\mathsf{O f f}}(T)\\,\\stackrel{=}{\\leq}\\,O(\\log|\\mathcal{F}|).$ ,3 and while it is possible to achieve a similar rate $\\mathbf{Est}_{\\mathrm{D}}^{0\\mathsf{n}}(T)\\leq O(\\log|\\mathcal{F}|)$ for online estimation, this requires Vovk\u2019s aggregating algorithm or exponential weights [65]; directly applying least squares or other standard offline estimators leads to vacuous guarantees. This leads us to ask: Is it possible to convert offline estimation algorithms into online estimation algorithms in a black-box fashion? ", "page_idx": 1}, {"type": "text", "text": "Computationally speaking, this question has practical significance, since online estimation algorithms are typically far less efficient than their offline counterparts (the classical exponential weights algorithm maintains a separate weight for every $f\\in\\mathcal F$ , which is exponentially less memory-efficient than empirical risk minimization). In fact, at first glance this seems like a purely computational question: if the learner has access to an offilne estimator, nothing is stopping them (information-theoretically) from throwing the estimator away and using the data to run an online estimation algorithm.4 Yet, for aforementioned applications in interactive decision making [25, 58, 24, 27, 28, 56, 55], estimation algorithms\u2014particularly online estimators\u2014play a deeper information-theoretic role, and can be viewed as compressing the data stream into a succinct, operational representation that directly informs downstream decision making. With these applications in mind, the first contribution of this paper is to introduce a new protocol, Oracle-Efficient Online Estimation, which provides an information-theoretic abstraction of the role of online versus offline estimation, analogous to the framework of information-based complexity in optimization [43, 60, 48, 2] and statistical query complexity in theoretical computer science [13, 37, 21, 22]. ", "page_idx": 1}, {"type": "table", "img_path": "sks7x4I8Bh/tmp/8541d0e78e055de903cee13aab2d6d61f48c92202bc0c16270436ac239cf5aff.jpg", "table_caption": ["Protocol 1 Oracle-Efficient Online Estimation (OEOE) "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "1.1 Our protocol: oracle-efficient online estimation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In the Oracle-Efficient Online Estimation (OEOE) framework, the aim is to perform online estimation in the sense of Eq. (2), with the twist that the learner does not directly observe the outcomes $y^{1},\\ldots,y^{T}$ ; rather, they interact with the environment indirectly through a sequence of offilne estimators produced by a black-box algorithm operating on the historical data. We formalize this black-box amlgaoprpiitnhgm f raos ma nh iessttiormiaetsi toon  eosrtaicmlae $\\mathbf{Alg}_{0\\mathsf{f f}}=\\left\\{\\mathbf{Alg}_{0\\mathsf{f f}}^{t}\\right\\}_{t=1}^{T}$ o(fef.lign.,e  Feosstitemr aatinodn  Rearrkohrl.in [26]), which is a fDoer fain isttiaotnis t1i.c1a l( Oesftfilimnaet ieostni imnasttiaonnc eo r $A n$ e  aensdti lmoastsi iosr aac lem $\\mathbf{Alg}_{0\\mathsf{f f}}\\,=\\,\\{\\mathbf{Alg}_{0\\mathsf{f f}}^{t}\\}_{t=1}^{T}$ $(\\mathcal{X},\\mathcal{Y},\\mathcal{Z},\\mathcal{K},\\mathcal{F})$ $\\mathsf{D}$ ${\\bf A l g}_{0\\sf f f}^{t}\\;:\\;(\\mathcal{X}\\;\\times\\;\\$ $y)^{t-1}\\ \\to\\ (\\chi\\ \\to\\ \\mathcal{Z})$ such that for any sequence $(x^{1},y^{1}),\\ldots,(x^{T},y^{T})$ with $y^{t}\\;\\sim\\;\\dot{\\mathcal{K}}(f^{\\star}\\!(x^{t}))$ , the sequence of estimators $\\begin{array}{l l l}{\\widehat{f^{t}}}&{=}&{\\mathbf{Alg}_{\\mathsf{O f f}}^{t}(x^{1},\\ldots,x^{t-1},y^{1},\\ldots,y^{t-1})}\\end{array}$ satisfies $\\begin{array}{r l}{\\mathbf{Est}_{\\mathrm{D}}^{\\mathrm{Off}}(t)}&{:=}\\end{array}$ $\\begin{array}{r}{\\sum_{s=1}^{t-1}\\mathsf{D}\\big(\\widehat{f}^{t}(x^{s}),f^{\\star}(x^{s})\\big)\\le\\beta_{0\\mathsf{f f}}}\\end{array}$ for all $t\\in[T]$ almost surely; we allow $x^{t}$ to be selected adaptively based on $y^{1},\\ldots,y^{t-1}$ and ${\\widehat{f}}^{1},\\ldots,{\\widehat{f}}^{t-1}$ . We refer to $\\beta_{0\\mathsf{f}\\mathsf{f}}\\geq0$ as the offline estimation parameter. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "This definition simply asserts that the estimators ${\\widehat{f}}^{t}$ produced by the offilne estimation oracle satisfy the guarantee in Eq. (1), even when the cova riates are selected adaptively. Examples include standard algorithms like least-squares for regression and maximum likelihood for conditional density estimation, which guarantee $\\beta_{0\\mathsf{f}\\mathsf{f}}\\leq O(\\log|\\bar{\\mathcal{F}}|)$ with high probability whenever $\\mathcal{F}$ is a finite class; see Appendix C.1 for further background.5 Throughout the paper, we assume for simplicity that $\\beta_{0\\mathsf{f}\\mathsf{f}}>0$ is known in advance. ", "page_idx": 2}, {"type": "text", "text": "With this definition, we present the Oracle-Efficient Online Estimation protocol in Protocol 1. In the protocol, a learner aims to perform online estimation, but at each step $t$ , the only information available is the covariates $\\boldsymbol{x}^{1},\\ldots,\\boldsymbol{x}^{t-1}$ and the estimators ${\\widehat{f}}^{1},\\ldots,{\\widehat{f}}^{t}$ generated by an offline estimation oracle satisfying Definition 1.1; the outcomes $y^{1},\\ldots,y^{T}$ are not directly observed. Based on this information, the learner produces a new estimator $\\bar{f}^{t}$ such that the online estimation error ${\\bf E s t}_{\\mathrm{D}}^{\\mathsf{O n}}(T)=$ $\\begin{array}{r}{\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}\\sim\\mu_{-}^{t}}\\left[\\mathsf{D}\\big(\\bar{f}(\\underline{{x}}^{t}),f^{\\star}(\\underline{{x}}^{t})\\big)\\right]}\\end{array}$ n ienr rEoqr . ((22))  iins  tmhien iOmEizOeEd .6fr aAmn eawlgoorrki.t hNmo itse  ttehramt ewd hoirlea ctlhee- elfefaicrineenrt cannot directly observe the outcomes $y^{1},\\ldots,y^{T}$ , the covariates $x^{1},\\ldots,x^{T}$ are observed; we prove that without this ability, it is impossible to achieve non-trivial estimation performance (Section 3) . ", "page_idx": 2}, {"type": "text", "text": "The OEOE framework abstracts away the property that oracle-efficient algorithms implicitly interact with the environment through a compressed, potentially lossy channel (the estimation oracle $\\bf A l g_{E s t},$ ). We believe this property merits deeper investigation: it is shared by essentially all algorithms from recent research that reduces interactive decision making and reinforcement learning to estimation oracles [25, 58, 27, 28, 56, 55], yet the relative power of offline oracles and analogously defined online oracles is poorly understood in this context. By providing an information-theoretic abstraction to study oracle-efficiency, the OEOE framework plays a role similar to information-based complexity in optimization [43, 60, 48, 2] and statistical query complexity in theoretical computer science [13, 37, 21, 22], both of which provide rich frameworks for designing and evaluating iterative algorithms that interact with the environment in a structured fashion. We expect that this abstraction will find broader use for more complex domains (e.g., decision making and active learning) as a means to guide algorithm design and prove lower bounds against natural classes of algorithms. ", "page_idx": 2}, {"type": "text", "text": "Let us first build some intuition. Familiar readers may recognize that the classical halving algorithm for binary classification (e.g., Cesa-Bianchi and Lugosi [17]) can be viewed as oracle-efficient in our framework. Specifically, for binary classification with $\\mathcal{V}\\,=\\,\\mathcal{Z}\\,=\\,\\{0,1\\}$ and loss function ${\\mathsf{D}}_{0/1}\\big(\\widehat{f}(x),f^{\\star}(x)\\big)\\,=\\,\\mathbb{1}\\big\\{\\widehat{f}(x)\\,\\neq\\,f^{\\star}(x)\\big\\}$ , the halving algorithm can use any offline oracle with $\\beta_{0\\mathsf{f}\\mathsf{f}}=0$ to achieve $\\mathbf{Est}_{\\mathsf{D}}^{\\mathsf{O n}}(T)=O(\\log|\\mathcal{F}|)$ , which is optimal. However, little is known for noisy oracles with $\\beta_{0\\mathsf{f}\\mathsf{f}}>0$ , or more general outcome spaces and loss functions (e.g., regression or density estimation). In addition, the halving algorithm\u2014while oracle-efficient\u2014is computationally inefficient, as it requires maintaining an explicit version space. This leads us to restate our central question formally, in two parts: ", "page_idx": 3}, {"type": "text", "text": "1. Can we design oracle-efficient algorithms with near-optimal online estimation error (2), up to polynomial factors (for general instances $(\\mathcal{X},\\mathcal{Y},\\mathcal{Z},\\mathcal{K},\\mathcal{F})$ and $\\beta_{0\\mathsf{f}\\mathsf{f}}>0,$ )? ", "page_idx": 3}, {"type": "text", "text": "1.2 Contributions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "For a general class of losses D, referred to as metric-like, we settle the statistical and computational complexity of performing online estimation via black-box offilne estimation oracles up to mild gaps, answering questions (1) and (2) above. ", "page_idx": 3}, {"type": "text", "text": "Statistical complexity. Our first result concerning statistical complexity focuses on finite classes $\\mathcal{F}$ , where the optimal rates for offline and online estimation with standard losses $\\mathsf{D}(\\cdot,\\cdot)$ both scale as $\\Theta(\\log|{\\mathcal{F}}|)$ . For this setting, we show (Theorem 3.1) that there exists an oracle-efficient online estimation algorithm that achieves $\\mathbf{Est}_{\\mathsf{D}}^{\\mathsf{O n}}(T)\\,=\\,O((\\beta_{0\\mathsf{f f}}\\,+1)\\operatorname*{min}\\{\\log|\\mathcal{F}|,|\\mathcal{X}|\\})$ in the OEOE framework, and that this is optimal (Theorem 3.2). This provides an affirmative answer to question (1), and characterizes the statistical complexity of oracle-efficient online estimation with finite classes $\\mathcal{F}$ . ", "page_idx": 3}, {"type": "text", "text": "In the general OEOE framework, the learner can use the entire history of offilne estimators ${\\widehat{f}}^{1},\\ldots,{\\widehat{f}}^{t}$ and covariates $\\boldsymbol{x}^{1},\\ldots,\\boldsymbol{x}^{t-1}$ to produce the online estimator $\\bar{f}^{t}$ for step $t$ . As a secondary  result,  we study a restricted class of memoryless oracle-efficient algorithms that choose $\\bar{f}^{t}$ only based on the most recent offilne estimator ${\\widehat{f}}^{t}$ , and show (Theorem 3.3) that it is impossible for such algorithms to achieve low online estimation error. ", "page_idx": 3}, {"type": "text", "text": "Lastly, we give a more general approach to deriving oracle-efficient reductions (Theorem D.1) that is based on delayed online learning [67, 42, 35, 47]. Using this result, we give a characterization of learnability with infinite classes for binary classification in the OEOE framework (Theorem D.2), proving that finite Littlestone dimension is necessary and sufficient for oracle-efficient learnability. ", "page_idx": 3}, {"type": "text", "text": "Computational complexity. On the computational side, we provide a negative answer to question (2), showing (Theorem 4.1) that under standard conjectures in computational complexity, there do not exist polynomial-time algorithms with non-trivial online estimation error in OEOE framework. In spite of this negative result, we provide a fine-grained perspective for the statistical problem of conditional density estimation, a general task that subsumes classification and regression and has immediate applications to reinforcement learning and interactive decision making [27, 28]. Here we show, perhaps surprisingly (Theorem 4.2), that online estimation in the OEOE framework is no harder computationally than online estimation with arbitrary, unrestricted algorithms. This result is salient in light of the applications we discuss below. ", "page_idx": 3}, {"type": "text", "text": "Implications for interactive decision making. As the preceding discussion has alluded to, our interest in studying oracle-efficient online estimation is largely motivated by a connection to the problem of interactive decision making. Foster et al. [27, 28], Foster and Rakhlin [26] propose a general framework for interactive decision making called Decision Making with Structured Observations (DMSO), which subsumes contextual bandits, bandit problems with structured rewards, and reinforcement learning with general function approximation. They show that for any decision making problem in the DMSO framework, there exists an algorithm that, given access to an online estimation algorithm (or, \u201coracle\u201d) for conditional density estimation for an appropriate class $\\mathcal{F}$ , it is possible to achieve near-optimal regret. The results above critically make use of online estimation oracles, as they require achieving low estimation error for adaptively chosen sequences of covariates, and it is natural to ask whether similar guarantees can be achieved using only offilne estimation oracles. However, positive results are only known for certain special cases [18\u201320, 58], with scant results for reinforcement learning in particular. In this context, our results have the following implication (Corollary E.1): Informationtheoretically, it is possible to achieve near-optimal regret for any interactive decision making problem using an algorithm that accesses the data stream only through offline estimation oracles. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Additional results. Due to space constraints, the following results are deferred to the appendix: (i) detailed examples for our statistical estimation framework (Appendix $C_{.}$ ); (ii) additional results concerning statistical complexity of the OEOE framework (Appendix $D$ ); and (iii) detailed results for our application to interactive decision making (Appendix $E$ ). ", "page_idx": 4}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Unless otherwise stated, our results assume the loss function D has metric-like structure. ", "page_idx": 4}, {"type": "text", "text": "Definition 2.1 (Metric-like loss). A loss function $\\mathsf{D}:\\mathcal{Z}\\times\\mathcal{Z}\\to[0,1]$ is metric-like on the set $\\mathcal{Z}$ if it is symmetric and satisfies (i) $\\mathsf{D}(z_{1},z_{2})\\geq0$ for any $z_{1}$ , $z_{2}\\in\\mathcal{Z}$ and $\\mathsf{D}(z,z)=0$ for all $z\\in{\\mathcal{Z}}$ ; and ( $\\begin{array}{r}{\\colon i)\\,\\mathsf{D}(z_{1},z_{2})\\leq C_{\\mathsf{D}}\\cdot(\\mathsf{D}(z_{1},z_{3})+\\mathsf{D}(z_{3},z_{2})).}\\end{array}$ for all $z_{1},z_{2},z_{3}\\in\\mathcal{Z}$ , for an absolute constant $C_{\\mathrm{D}}\\geq1$ . ", "page_idx": 4}, {"type": "text", "text": "Throughout the paper, we focus on three canonical applications, outlined in the introduction: Classification with the indicator loss $\\mathsf{D}_{0/1}$ $C_{\\mathrm{D}}=1)$ ), regression with the square loss $\\mathsf{D}_{\\mathsf{s q}}$ $C_{\\mathrm{D}}=2)$ ), and conditional density estimation with squared Hellinger distance $\\mathsf{D}_{\\mathsf{H}}^{2}$ $C_{\\mathrm{D}}=2$ ). See Appendix C for detailed examples and discussion. (omitted for space). ", "page_idx": 4}, {"type": "text", "text": "Finite versus infinite classes. The majority of our results focus on finite classes $\\mathcal{F}$ . We believe this captures the essential difficulty of the problem, but we expect that most of our sample complexity results (which typically scale with $\\log\\left|{\\mathcal{F}}\\right|)$ can be extended to infinite classes by combining our techniques with appropriate notions of complexity for the function class (Littlestone dimension for classification, sequential Rademacher complexity, and sequential covering numbers [50, 41, 52]). For the canonical settings of classification, regression, and conditional density estimation, there exist algorithms that achieve $\\mathbf{Est}_{\\mathsf{D}}^{\\mathsf{O f f}}(T)=O(\\log\\bar{|\\mathcal{F}|})$ and $\\mathbf{Est}_{\\mathsf{D}}^{\\mathsf{O n}}(T)=O(\\log\\lvert\\mathcal{F}\\rvert)$ for arbitrary finite classes; see Appendix C for details. ", "page_idx": 4}, {"type": "text", "text": "We defer additional notation and related work to Appendices A and B ", "page_idx": 4}, {"type": "text", "text": "3 Statistical complexity of oracle-efficient online estimation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This section presents our main results concerning the statistical complexity of oracle-efficient online estimation. In Section 3.1, we focus on finite classes $\\mathcal{F}$ and present an oracle-efficient algorithm that achieves near-optimal online estimation error (Theorem 3.1). We then provide a lower bound that shows that our reduction is near optimal (Theorem 3.2). In Section 3.2, we turn our attention to memoryless oracle-efficient algorithms, proving strong impossibility results (Theorem 3.3). ", "page_idx": 4}, {"type": "text", "text": "3.1 Minimax sample complexity for oracle-efficient algorithms ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we present our main statistical conclusion for the OEOE framework: For any finite class $\\mathcal{F}$ , it is possible to transform any black-box offilne estimation algorithm into an online estimation algorithm with near-optimal error (up to a logarithmic factor that we show is unavoidable). ", "page_idx": 4}, {"type": "text", "text": "Algorithm and minimax upper bound. Our results are achieved through a new algorithm, Version Space Averaging, described in Algorithm 1. At each round $t$ , the algorithm uses estimators ${\\widehat{f}}^{1},\\ldots,{\\widehat{f}}^{t}$ produced by an offline estimation oracle $\\mathbf{Alg}_{0\\mathsf{f f}}$ , along with the previous covariates $\\boldsymbol{x}^{1},\\ldots,\\boldsymbol{x}^{t-1}$ , to construct a version space ${\\mathcal{F}}_{t}\\subseteq{\\mathcal{F}}$ in Eq. (3). Informally, $\\mathcal{F}_{t}$ consists of all $f\\,\\in\\,{\\mathcal{F}}$ that are consistent with the estimators ${\\widehat{f}}^{1},\\ldots,{\\widehat{f}}^{t}$ in the sense that for all $s\\in[t]$ , the offline estimation error relative to ${\\widehat{f}}^{s}$ is small; as long as the offilne estimation oracle $\\mathbf{Alg}_{0\\mathsf{f f}}$ has offilne estimation error $\\beta_{\\mathsf{O f f}}$ (Definition 1.1), it follows immediately that the construction in Eq. (3) satisfies $f^{\\star}\\in{\\mathcal{F}}_{t}$ . Given the version space $\\mathcal{F}_{t}$ , Algorithm 1 predicts by uniformly sampling: $\\bar{f}^{t}\\stackrel{}{\\sim}\\mu^{t}:=\\mathrm{Unif}(\\mathcal{F}_{t})$ , then proceeds to the next round.7 The main guarantee for Algorithm 1 is stated in Theorem 3.1. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Version Space Averaging ", "page_idx": 5}, {"type": "text", "text": "1: input: parameter space $\\mathcal{F}$ , offline estimation oracle $\\mathbf{Alg}_{0\\mathsf{f f}}$ with parameter $\\beta_{0\\mathsf{f}\\mathsf{f}}\\geq0$ .   \n2: for $t=1,2,\\ldots,T$ do   \n3: Receive $\\widehat{f^{t}}=\\mathbf{A}\\mathbf{l}\\mathbf{g}_{\\mathsf{O f f}}^{t}\\big(x^{1},\\ldots,x^{t-1},y^{1},\\ldots,y^{t-1}\\big).$ .   \n4: Calculate version space:   \n$\\mathcal{F}_{t}=\\Big\\{f\\in\\mathcal{F}\\ \\Big|\\ \\forall s\\in[t],\\ \\sum_{\\tau<s}\\mathsf{D}\\big(\\widehat{f}^{s}(x^{\\tau}),f(x^{\\tau})\\big)\\leq\\beta_{\\mathsf{O f f}}\\Big\\}.$ (3)   \n5: Predict $\\bar{f}^{t}\\sim\\mu^{t}:=\\operatorname{Unif}(\\mathcal{F}_{t})$ and receive $x^{t}$ . $//$ Nature draws $\\boldsymbol{y}^{t}\\sim\\mathcal{K}(\\boldsymbol{f}^{\\star}(\\boldsymbol{x}^{t}))$ and passes to $\\mathbf{Alg}_{\\mathrm{Off}}$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.1 (Main upper bound for OEOE). For any instance $(\\mathcal{X},\\mathcal{Y},\\mathcal{Z},\\mathcal{K},\\mathcal{F})$ , any metric-like loss D, and any offline estimator $\\mathbf{Alg}_{0\\mathsf{f f}}$ with parameter $\\beta_{0\\mathsf{f}\\mathsf{f}}\\geq0$ , Algorithm $^{\\,l}$ is oracle-efficient and achieves ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{Est}_{\\mathsf{D}}^{\\mathsf{O n}}(T)\\leq O(C_{\\mathsf{D}}\\cdot(\\beta_{\\mathsf{O f f}}+1)\\cdot\\operatorname*{min}\\big\\{\\log|\\mathcal{F}|,|\\mathcal{X}|\\log T\\big\\}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Most notably, Algorithm 1 achieves $\\mathbf{Est}_{\\mathsf{D}}^{\\mathsf{O n}}(T)\\,\\leq\\,O(C_{\\mathsf{D}}\\cdot(\\beta_{\\mathsf{O f f}}+1)\\cdot\\log|\\mathcal{F}|)$ ; that is, up to a ${\\cal O}(\\log|{\\mathcal F}|)$ factor, the reduction achieves online estimation rates in the OEOE framework that are no worse than the minimax rate for offline estimation. For classification, regression, and density estimation with generic finite classes $\\mathcal{F}$ (Appendix C), the best possible offilne estimation error rate is $\\beta_{0\\mathsf{f f}}=O(\\log\\bar{|\\mathcal{F}|})$ , so this shows that price of oracle-efficiency is at most quadratic. ", "page_idx": 5}, {"type": "text", "text": "Minimax lower bound. Next, we show that the upper bound in Theorem 3.1 is nearly tight, giving a lower bound that matches up to logarithmic factors. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2 (Main lower bound for OEOE). Consider the binary classification setting with $\\mathcal{Z}\\,=\\,\\mathcal{V}\\,=\\,\\{0,1\\}$ and loss $\\mathsf{D}_{0/1}(\\cdot,\\cdot)$ . For any $N\\;\\in\\;\\mathbb{N}$ and $\\beta_{0\\mathsf{f}\\mathsf{f}}\\;>\\;0,$ , there exists an instance $(\\mathcal{X},\\mathcal{Y},\\mathcal{Z},\\mathcal{K},\\mathcal{F})$ with $\\log|{\\mathcal F}|~=~|{\\boldsymbol\\chi}|~=~N$ such that for any oracle-efficient algorithm, there is a sequence of covariates $(x^{1},\\ldots,x^{T})$ and offline oracle with parameter \u03b2Off such that $\\mathbb{E}\\big[{\\bf E s t}_{\\mathsf{D}}^{\\mathsf{O n}}(T)\\big]\\ge\\Omega(\\operatorname*{min}\\left\\{(\\beta_{\\mathsf{O f f}}+1)N,T\\right\\})$ . ", "page_idx": 5}, {"type": "text", "text": "This result states that for a generic finite class $\\mathcal{F}$ and offline estimation oracle $\\mathbf{Alg}_{0\\mathsf{f f}}$ , any oracleefficient online estimator must have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[\\mathbf{Est}_{\\mathsf{D}}^{\\mathsf{O n}}(T)\\big]\\geq\\Omega(\\operatorname*{min}\\big\\{(\\beta_{\\mathsf{O f f}}+1)\\log|\\mathcal{F}|,(\\beta_{\\mathsf{O f f}}+1)|\\mathcal{N}|,T\\big\\})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "in the worst case. This implies that the $\\log\\!\\left|\\mathcal{F}\\right|$ factor we pay for offline to online conversion is unavoidable, and that Theorem 3.1 is optimal up to a $\\log T$ factor, giving a near-optimal characterization for the minimax rate for online estimation in the OEOE framework. We conclude with two remarks: (i) The $(\\beta_{0\\mathsf{f}\\mathsf{f}}+1)$ scaling (as opposed to say, $\\beta_{0\\mathsf{f f}}$ ) in Theorem 3.1 is unavoidable, as witnessed by the optimality of the halving algorithm for noiseless binary classification [17]; (ii) if the space $\\mathcal{Z}$ and the loss $\\mathsf{D}$ are convex, then we can change Algorithm 1 to output a deterministic prediction by using the average of all parameters in $\\mathcal{F}_{t}$ rather than the uniform distribution on $\\mathcal{F}_{t}$ . See Lemma G.1 for details. ", "page_idx": 5}, {"type": "text", "text": "General reductions and infinite classes. Algorithm 1 is somewhat specialized to finite classes. In Appendix D (deferred to the appendix for space), we provide a more general approach to designing oracle-efficient algorithms based on delayed online learning (Theorem D.1), and use it to derive a characterization of oracle-efficient learnability for classification with infinite classes $\\mathcal{F}$ (Theorem D.2). ", "page_idx": 5}, {"type": "text", "text": "Full memory vs. finite memory. Algorithm 1 requires full memory of all past offline estimators. The more general approach proposed in Appendix D can use $N$ most recent offline estimators to obtain an estimation error bound of $O(C_{\\mathsf{D}}(N+\\beta_{0\\mathsf{f f}}T/N)+N\\cdot\\log|\\mathcal{F}|)$ (Corollary D.1) for any integer $N>0$ . ", "page_idx": 5}, {"type": "text", "text": "3.2 Impossibility of memoryless oracle-efficient algorithms ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the general OEOE framework, the learner can use the entire history of estimators ${\\widehat{f}}^{1},\\ldots,{\\widehat{f}}^{t}$ and covariates $\\boldsymbol{x}^{1},\\ldots,\\boldsymbol{x}^{t-1}$ to produce the online estimator $\\bar{f}^{t}$ for step $t$ ; notably the Versi on Spac e Averaging algorithm with which our upper bounds in the prequel are derived uses the entire history. In this section, we show that for memoryless oracle-efficient algorithms (Definition 3.1) that select the estimator $\\bar{f}^{t}$ only as a function of the most recent offilne estimator ${\\widehat{f}}^{t}$ , similar guarantees are impossible. ", "page_idx": 5}, {"type": "text", "text": "Definition 3.1 (Memoryless algorithm). An online estimation algorithm is memoryless if there exists a map $F^{t}(\\cdot)$ such that we can write $\\mu^{t}=F^{t}(\\widehat{f^{t}})$ , where $\\widehat{f^{t}}=\\mathbf{A}\\mathbf{l}\\mathbf{g}_{0\\mathrm{ff}}^{t}(x^{1},\\ldots,\\widehat{x}^{t-1},\\overline{{y^{1}}},\\ldots,y^{t-1})$ and $\\mu^{t}$ is the randomization distribution for the  online esti mator $\\bar{f}^{t}$ .8 ", "page_idx": 6}, {"type": "text", "text": "Memoryless algorithms are more practical than arbitrary algorithms, since they do not require storing past estimators or covariates in memory. Our motivation for studying memoryless algorithms arises from recent work in interactive decision making [25, 27], which shows that there exist near-optimal algorithms for contextual bandits and reinforcement learning that use estimation oracles in memoryless fashion. We show that unfortunately, it is not possible to convert offline estimators into memoryless online estimation algorithms with non-trivial error. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.3 (Impossibility of memoryless algorithms for OEOE). Consider the binary classification setting with $\\mathcal{Z}=\\mathcal{Y}=\\{0,1\\}$ and loss $\\mathsf{D}_{0/1}(\\cdot,\\cdot)$ . For any $N\\in\\mathbb N$ and $\\beta_{0\\mathsf{f}\\mathsf{f}}\\,\\geq\\,0,$ , there exists an instance $(\\mathcal{X},\\mathcal{Y},\\mathcal{Z},\\mathcal{K},\\mathcal{F})$ with $|\\mathcal{F}|=|\\mathcal{X}|=N$ such that for any memoryless oracle-efficient algorithm, there exists a sequence of covariates $(x^{1},\\ldots,x^{T})$ and a (potentially improper) offline oracle $\\mathbf{A}\\mathbf{lg}_{0\\mathsf{f f}}$ with parameter $\\beta_{0\\mathsf{f f}}$ such that $\\mathbb{E}\\big[{\\bf E s t}_{\\mathsf{D}}^{\\mathsf{O n}}(T)\\big]\\ge\\Omega\\big(\\operatorname*{min}\\{N(\\beta_{\\mathsf{O f f}}+1),T\\}\\big)$ . This conclusion still holds when the online estimation algorithm remembers ${\\widehat{f}}^{1},\\ldots,{\\widehat{f}}^{t-1}$ but not $\\boldsymbol{x}^{1},\\ldots,\\boldsymbol{x}^{t-1}$ . ", "page_idx": 6}, {"type": "text", "text": "This result shows that in the worst case, any memoryless oracle-efficient algorithm must have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\bigl[{\\mathbf{Est}}_{\\mathsf{D}}^{\\mathsf{O n}}(T)\\bigr]\\geq\\Omega((\\beta_{0\\mathsf{f f}}+1)\\operatorname*{min}\\left\\{|\\mathcal{X}|,|\\mathcal{F}|\\right\\}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This precludes an online estimation error bound scaling with $\\left(\\beta_{0\\mathsf{f f}}+1\\right)\\log\\left|\\mathcal{F}\\right|$ as in Theorem 3.1, and shows that the gap between general and memoryless oracle-efficient algorithms can be exponential. ", "page_idx": 6}, {"type": "text", "text": "Interestingly, the lower bound in Theorem 3.3 holds even if the online estimation algorithm is allowed to remember ${\\widehat{f}}^{1},\\ldots,{\\widehat{f}}^{t-1}$ , but not $\\boldsymbol{x}^{1},\\ldots,\\boldsymbol{x}^{t-1}$ . The intuition here is that without covariate information, it is not p ossible t o aggregate the predictions of previous estimators or otherwise use them to reduce uncertainty. This provides post-hoc motivation for our decision to incorporate covariate memory into the OEOE protocol in Section 1.1. ", "page_idx": 6}, {"type": "text", "text": "The proof of Theorem 3.3 uses that the estimators ${\\widehat{f}}^{t}$ produced by the offline estimation oracle may be improper (i.e., ${\\widehat{f^{t}}}\\not\\in{\\mathcal{F}})$ . We defer a variant of the result that holds even if the estimation oracle is proper under additional assumptions as well as the complementary upper bound to Appendix D.2 ", "page_idx": 6}, {"type": "text", "text": "4 Computational complexity of oracle-efficient online estimation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we turn our attention to the computational complexity of oracle-efficient online estimation in the OEOE framework. In Section 4.1, we show (Theorem 4.1) that in general, it is not possible to transform black-box offline estimation algorithms into online estimation algorithms in a computationally efficient fashion. Then, in Section 4.2, we provide a more fine-grained perspective, showing (Theorem 4.2) that for conditional density estimation, online estimation in the OEOE framework is no harder computationally than online estimation with unrestricted algorithms. ", "page_idx": 6}, {"type": "text", "text": "4.1 Computational hardness of oracle-efficient estimation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our main upper bounds (Section 3.1) show that online estimation error $\\mathbf{Est}_{\\mathsf{D}}^{\\mathsf{O n}}(T)\\,\\leq\\,O((\\beta_{\\mathsf{O f f}}\\,+$ $1)\\log|{\\mathcal{F}}|)$ can be achieved in an oracle-efficient fashion for any finite class $\\mathcal{F}$ , but the algorithm (Algorithm 1) is not computationally efficient. We now show that this is fundamental: There exist classes $\\mathcal{F}$ for which offline estimation can be performed in polynomial time, yet no oracle-efficient algorithm running in polynomial-time algorithm can achieve sublinear online estimation error. ", "page_idx": 6}, {"type": "text", "text": "Computational model. To present our results, we must formalize a computational model for oracle-efficient online estimation, and in particular, define a notion of input length for oracle-efficient online algorithms. To do so, we restrict our attention to noiseless binary classification, and consider a sequence of classification instances indexed by $n\\in\\mathbb N$ , with $\\mathcal{Z}=\\mathcal{Y}=\\{0,1\\}$ , ${\\mathcal{X}}_{n}:=\\{0,1\\}^{n}$ , $\\bar{\\kappa}(z)=\\mathbb{1}_{z}$ , and indicator loss $\\mathsf{D}_{0/1}(\\cdot,\\cdot)$ . We consider a sequence of classes ${\\mathcal{F}}_{n}$ that have polynomial description lengthi.e. $\\log\\!\\left|{\\mathcal{F}}_{{n}}\\right|$ is polynomial in $n$ , so that $f\\in\\mathcal{F}_{n}$ can be described in $\\mathrm{poly}(n)$ bits. In particular, we assume that $f\\in\\mathcal{F}_{n}$ is represented as a Boolean circuit of size $\\mathrm{poly}(n)$ so that $f(x)$ can be computed in $\\mathrm{poly}(n)$ time for $x\\in\\mathcal{X}_{n}$ ; we refer to such sequences as polynomially computable. ", "page_idx": 6}, {"type": "text", "text": "To allow for offline estimators that are improper, we assume that for all $t$ and all sequences $(x^{1},y^{1}),\\ldots,(x^{T},y^{T})$ , the output $\\widehat{f}^{t}\\quad:\\quad\\{0,1\\}^{n}\\quad\\to\\quad\\{0,1\\}$ returned by $\\mathbf{Alg}_{0\\mathsf{f f}}^{t}(x^{1},\\ldots,{x^{t-1}},y^{\\mathrm{i}},\\ldots,y^{t-1})$ is a Boolean circuit of size $\\mathrm{poly}(n)$ ; we refer to such oracles as having $\\mathrm{poly}(n)$ -output description length.9 Likewise, to allow the online estimation algorithm itself to be improper and randomized, we restrict to algorithms for which computing $\\bar{f}^{t}(x)$ for $\\bar{f}^{t}\\sim\\mu^{t}$ can be implemented as ${\\bar{f}}^{t}(x,r)$ for a random bit string $r\\sim\\mathrm{Unif}(\\{0,1\\}^{B})$ , where $B=\\mathrm{poly}(n)$ ; we refer to the online estimator as having $\\mathrm{poly}(n)$ -output description length if $\\bar{f}^{t}(\\cdot,\\cdot)$ is itself a Boolean circuit of size $\\mathrm{poly}(n)$ . We refer to the online estimation algorithm polynomial time if it runs in time $\\mathrm{poly}(n)$ for any sequence of inputs $t,x^{1},\\ldots,x^{t-1}$ , and ${\\widehat{f}}^{1},\\ldots,{\\widehat{f}}^{t-1}$ , and has $\\mathrm{poly}(n)$ -output description length.10 ", "page_idx": 7}, {"type": "text", "text": "Main lower bound. Our main computational lower bound is as follows. ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Theorem 4.1 (Computational lower bound for OEOE). Assume the existence of one-way functions.11 There exists a sequence of polynomially computable classes $({\\mathcal{F}}_{1},{\\mathcal{F}}_{2},\\ldots,{\\mathcal{F}}_{n},\\ldots)$ , along with $a$ sequence of poly $(n)$ -output description length offilne oracles with $\\beta_{0\\mathsf{f}\\mathsf{f}}=0$ associated with each ${\\mathcal{F}}_{n}$ , such that for any fixed polynomials $p,q:\\mathbb{N}\\rightarrow\\mathbb{N}$ and all $n\\in\\mathbb N$ sufficiently large, any oracle-efficient online estimation algorithm with runtime bounded by $p(n)$ must have $\\mathbb{E}[\\mathbf{Est}_{\\mathsf{D}}^{\\mathsf{O n}}(T)]\\geq T/4$ for all $1\\leq T\\leq q(n)$ . At the same time, there exists an inefficient algorithm that achieves $\\mathbb{E}[\\mathbf{Est}_{\\mathsf{D}}^{\\mathsf{O n}}(T)]\\leq$ $O({\\sqrt{n}})$ for all $T\\in\\mathbb N$ . ", "page_idx": 7}, {"type": "text", "text": "Informally, Theorem 4.1 shows that there exist a class $\\mathcal{F}$ and offline estimation oracles $\\mathbf{Alg}_{0\\mathsf{f f}}$ for which no oracle-efficient online estimation algorithm that runs in time ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{poly}(\\mathsf{l e n}(\\mathcal{X}),\\mathsf{l e n}(\\mathcal{F}),\\mathsf{m a x}_{t}\\,\\mathsf{l e n}(\\widehat{f}^{t}),T)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "can achieve sublinear estimation error, where $|\\mathsf{e n}(\\mathcal{X})$ and $|\\mathsf{e n}(\\mathcal{F})$ denote the number of bits required to describe $x\\in\\mathscr{X}$ and $f\\in\\mathcal F$ , and $|\\mathsf{e n}(\\widehat{f}^{t})$ denotes the size of the circuit required to compute ${\\widehat{f}}^{t}(x)$ . Yet, low online estimation error can be  achieved by an inefficient algorithm, and there exist ef ficient offline estimators with $\\beta_{0\\mathsf{f}\\mathsf{f}}=0$ as well. The result is essentially a corollary of Blum [14]; we refer to Appendix H.1 for the detailed proof. We mention in passing that Hazan and Koren [33] also give lower bounds against reducing online learning to offline oracles, but in a somewhat different computational model; see Appendix B for detailed discussion. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.1 is slightly disappointing, since one of the main motivations for studying oracle-efficiency is to leverage offilne estimators as a computational primitive. Combined with our results in Section 3, Theorem 4.1 shows that even though it is possible to be oracle-efficient information-theoretically, it is not possible to achieve this computationally. Nonetheless, we are optimistic that our abstraction can (i) aid in designing computationally efficient algorithms for learning settings beyond online estimation, and (ii) continue to serve as a tool to formalize lower bounds against natural classes of algorithms, as we have done here; see Section 5 for further discussion. ", "page_idx": 7}, {"type": "text", "text": "Remark 4.1. Theorem 4.1 relies on the fact that the offilne estimator may be improper (i.e., $\\widehat{f}^{t}\\notin\\mathcal{F})$ . An interesting open problem is whether one can attain poly $\\log\\left|{\\mathcal{F}}_{n}\\right|)\\cdot{\\dot{o}}(T)$ online estimation error with runtime poly $\\left(\\log\\left|{\\mathcal{F}}_{n}\\right|\\right)$ given $a$ proper offline estimation oracle with parameter $\\beta_{0\\mathsf{f}\\mathsf{f}}=0$ . \u25c1 ", "page_idx": 7}, {"type": "text", "text": "4.2 Conditional density estimation: computationally efficient algorithms ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In spite of the negative result in the prequel, which shows that efficient computation in the OEOE framework is not possible in general, we can provide a more fine-grained perspective on the computational complexity of oracle-efficient estimation for the problem conditional density estimation, a general task which subsumes classification and regression, and has immediate applications to reinforcement learning and interactive decision making [27, 28]. ", "page_idx": 7}, {"type": "text", "text": "Conditional density estimation. Recall that conditional density estimation [71, 12] is the special case of the online estimation framework in Section 1 in which $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ are arbitrary, $\\mathcal{Z}=\\bar{\\Delta}(y)$ , and the kernel is $\\kappa(z)=z$ ; that is sampling $y\\sim K(f^{\\star}(x))$ is equivalent to sampling $y\\sim f^{\\star}(x)$ . For the loss, we use squared Hellinger distance: $\\begin{array}{r}{\\mathsf{D}_{\\mathsf{H}}^{2}\\big(f(x),f^{\\star}(x)\\big)=\\frac{1}{2}\\int\\big(\\sqrt{f(y\\mid x)}-\\sqrt{f^{\\star}(y\\mid x)}\\big)^{2};}\\end{array}$ with online estimation error given by $\\begin{array}{r}{\\mathbf{Est}_{\\mathsf{H}}^{\\mathsf{O n}}(T)=\\sum_{t=1}^{T}\\mathsf{D}_{\\mathsf{H}}^{2}\\big(\\widehat{f}^{t}(x^{t}),f^{\\star}(x^{t})\\big)}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Base algorithm. Our result is based on a reduction. We assume access to a base algorithm $\\mathcal{A}_{\\sf C D E}$ for online estimation in the Conditional Density Estimation (CDE) framework, which is unrestricted in the sense that it is not necessarily oracle-efficient. That is, $\\mathcal{A}_{\\sf C D E}$ can directly use the full data stream $(x^{1},y^{1}),\\ldots,(x^{t-1},y^{t-1})$ at step $t$ . For parameters $R_{\\mathsf{C D E}}(T)$ and $C_{\\mathcal{F}}\\geq1$ , we assume that for any $f^{\\star}\\in{\\mathcal{F}}$ , the base algorithm $\\mathcal{A}_{\\sf C D E}$ ensures that for all $\\delta\\in\\overline{{(0,e^{-1})}}$ , with probability at least $1-\\delta$ , ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbf{Est}_{\\mathsf{H}}^{\\mathsf{O n}}(T)\\leq R_{\\mathsf{C D E}}(T)+C_{\\mathcal F}\\cdot\\log(1/\\delta).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We define the total runtime for $\\mathcal{A}_{\\sf C D E}$ across all rounds as Time $(\\mathcal{F},T)$ . ", "page_idx": 8}, {"type": "text", "text": "Main result. Our main result shows that any algorithm $\\mathcal{A}_{\\sf C D E}$ satisfying the guarantee above can be transformed into an oracle-efficient algorithm with only polynomial blowup in runtime. For technical reasons, we assume that V := e \u2228 supf,f \u2032\u2208F,x\u2208X,y\u2208Y ff \u2032((yy||xx)) is bounded; our sample complexity bounds scale only logarithmically with respect to this parameter. In addition, we assume that all $f\\in\\mathcal F$ and $x\\in\\mathscr{X}$ have $O(1)$ description length, and that one can sample $y\\sim f(x)$ in $O(1)$ time. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.2. Let $\\mathcal{A}_{\\sf C D E}$ be an arbitrary (unrestricted) online estimation algorithm that satisfies Eq. (4) and has runtime Time $(\\mathcal{F},T)$ . Then for any $N\\in\\mathbb{N},$ , there exists an oracle-efficient online estimation algorithm that achieves estimation error ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[\\mathbf{Est}_{\\mathsf{H}}^{\\mathsf{O n}}(T)\\big]\\leq\\widetilde{O}(C_{\\mathcal{F}}\\log V\\cdot\\beta_{0\\mathsf{f f}}T/N+N\\cdot(R_{\\mathsf{C D E}}(T)+C_{\\mathcal{F}}\\log V))\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "with runtime $\\mathrm{poly}(\\mathsf{T i m e}({\\mathcal F},T),\\log|{\\mathcal F}|,\\log|{\\mathcal X}|,T),$ , where $\\beta_{0\\mathrm{ff}}\\ \\ \\geq\\ \\ 0$ is the offline estimation parameter. The distributions $\\boldsymbol{\\mu}^{1},\\ldots,\\boldsymbol{\\mu}^{T}$ produced by the algorithm have support size poly $\\langle\\log|\\mathcal{F}|,\\log|\\mathcal{X}|,T\\rangle$ . As a special case, if the online estimation guarantee for the base algorithm holds with $\\begin{array}{r l r}{R_{\\mathsf{C D E}}(T)}&{{}\\leq}&{C_{\\mathcal{F}}^{\\prime}\\log T}\\end{array}$ for some problem-dependent constant $C_{\\mathcal{F}}^{\\prime}\\quad\\geq\\quad1$ , then by choosing $N$ appropriately, we achieve $\\begin{array}{r l}{\\mathbb{E}\\big[\\mathbf{Est}_{\\mathsf{H}}^{\\mathsf{O n}}(T)\\big]}&{{}\\leq}\\end{array}$ $\\widetilde{O}\\big((C_{\\mathcal{F}}(C_{\\mathcal{F}}+C_{\\mathcal{F}}^{\\prime})\\beta_{0\\not\\mathrm{ff}})^{1/2}\\log V\\cdot T^{1/2}+(C_{\\mathcal{F}}+C_{\\mathcal{F}}^{\\prime})\\log V\\big).$ . ", "page_idx": 8}, {"type": "text", "text": "Note that the estimation error bound in Theorem 4.2 is sublinear whenever the rate $R_{\\mathsf{C D E}}(T)$ is. This implies that for squared Hellinger distance, online estimation in the OEOE framework is no harder computationally than online estimation with arbitrary, unrestricted algorithms. ", "page_idx": 8}, {"type": "text", "text": "The proof of Theorem 4.2 is algorithmic, and is based on several layers of reductions. The main reason why the result is specialized to conditional density estimation is as follows: If we have an estimator $\\widehat{f}$ for which $\\mathsf{D}_{\\mathsf{H}}^{2}\\bar{(f(x),f^{\\star}(x))}$ is small for some $x$ , we can simulate $y\\sim f^{\\star}(x)$ up to low statistical error by sampling $y\\sim{\\widehat{f}}(x)$ instead, as $\\widehat{f}$ and $f^{\\star}$ are close in distribution. This allows us to implement a scheme based on simulating outcomes and feeding them to the base algorithm. ", "page_idx": 8}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our work introduces the Oracle-Efficient Online Estimation protocol as an information-theoretic framework to study the relative power of online and offilne estimators and gives a nearly complete characterization of the statistical and computational complexity of learning in this framework. In what follows, we discuss broader implications for our information-abstraction of oracle-efficiency. ", "page_idx": 8}, {"type": "text", "text": "Oracle-efficient learning as a general framework for analysis of algorithms. One of the most important contributions of this work is to formalize oracle-efficient algorithms as mappings that act upon a sequence of estimators but do not directly act on historical outcomes. While the computational lower bounds we provide for oracle-efficient learning are somewhat disappointing, we are optimistic that\u2014similar to statistical query complexity in TCS and information-based complexity in optimization\u2014our abstraction can (i) aid in designing computationally efficient algorithms for learning settings beyond online estimation, and (ii) continue to serve as a tool to formalize lower bounds against natural classes of algorithms, for estimation and beyond. That is, we envision oracleefficient learning as a more general framework to study oracle-based algorithms in any type of interactive learning problem. We remark that one need not restrict to offilne oracles; it is natural to study oracle-efficient algorithms based on online estimation oracles or other types of oracles through our information-theoretic abstraction as well. For concreteness, let us mention a couple of natural settings where our information-theoretic abstraction can be applied. ", "page_idx": 8}, {"type": "text", "text": "Oracle-efficient interactive decision making. For interactive decision making problems like bandits and reinforcement learning (more broadly, the DMSO framework described in Appendix E.1), it is natural to formalize oracle-effiicent algorithms as algorithms that do not directly observe rewards (bandits) or trajectories (reinforcement learning), and instead must select their decision based on an (online or offilne) estimator (e.g., regression for bandits or conditional density estimation for RL). Foster and Rakhlin [25] and Foster et al. [27] et seq. provide algorithms with this property for contextual bandits and RL, respectively, but the power of offline oracles in this context is not well understood. ", "page_idx": 9}, {"type": "text", "text": "Oracle-efficient active learning. For active learning, it is natural to consider algorithms that decide whether to query the label for a point in an oracle-efficient fashion (e.g., Krishnamurthy et al. [39]). For concreteness, consider pool-based active learning [32]. Suppose the learner is given a pool $\\mathcal{P}=\\{x_{1},...,x_{n}\\}$ of covariates and a parameter space $\\mathcal{F}$ . The learner can repeatedly choose $x^{t}\\in\\mathcal{P}$ and call the offline oracle to obtain an estimator ${\\widehat{f}}^{t}$ such $\\begin{array}{r}{\\mathbf{Est}_{\\mathrm{D}}^{\\mathrm{Off}}(t):=\\sum_{i<t}\\mathsf{D}\\Big(\\widehat{f}^{t}(x^{i}),f^{\\star}(x^{i})\\Big)\\leq}\\end{array}$ $\\beta_{0\\mathsf{f f}}$ (in contrast to an unrestricted algorithm that observes $y^{t}\\,=\\,f^{\\star}(x^{t}))$ . The aim is to learn a hypothesis with low classification error using the smallest number of queries possible. Can we design oracle-efficient algorithms that do so with near-optimal label complexity? ", "page_idx": 9}, {"type": "text", "text": "5.1 Further Directions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We close with some additional directions for future research. ", "page_idx": 9}, {"type": "text", "text": "Refined notions of estimation oracles. This work considers generic offilne estimation algorithms that satisfy the statistical guarantee in Definition 1.1 but can otherwise be arbitrary. Understanding the power of offline estimators that satisfy more refined (e.g., problem-dependent) guarantees is an interesting direction for future research. ", "page_idx": 9}, {"type": "text", "text": "Open questions for proper versus improper learning. Our results leave some interesting gaps in the power of proper versus improper oracles. First, the computational lower bounds in Section 4.1, leave open the possibility of attaining poly $(\\log|{\\mathcal{M}}_{n}|)\\cdot o({\\bar{T}})$ online estimation error with runtime $\\mathrm{poly}(\\log|\\mathcal{M}_{n}|)$ given access to a proper offilne estimation oracle with parameter $\\beta_{0\\mathsf{f}\\mathsf{f}}=0$ . Second, our results in Section 3.2 leave open the possibility of bypassing the $\\Omega(|\\mathcal{X}|(\\beta_{0\\mathsf{f f}}+1))$ lower bound for memoryless algorithms under the assumption that the offline oracle is proper. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We acknowledge support from the ARO through award W911NF-21-1-0328 and from the Simons Foundation and the NSF through award DMS-2031883. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Anders Aamand, Justin Y Chen, Huy L\u00ea Nguyen, and Sandeep Silwal. Improved space bounds for learning with experts. arXiv preprint arXiv:2303.01453, 2023.   \n[2] Alekh Agarwal, Peter L Bartlett, Pradeep Ravikumar, and Martin J Wainwright. Informationtheoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Transactions on Information Theory, 5(58):3235\u20133249, 2012.   \n[3] Aizerman, Braverman, and Rozonoer. Theoretical foundations of the potential function method in pattern recognition learning. Automation and remote control, 25:821\u2013837, 1964.   \n[4] Philip Amortila, Dylan J Foster, Nan Jiang, Ayush Sekhari, and Tengyang Xie. Harnessing density ratios for online reinforcement learning. International Conference on Learning Representations (ICLR), 2024.   \n[5] Yossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Woodworth. Lower bounds for non-convex stochastic optimization. Mathematical Programming, 199 (1-2):165\u2013214, 2023.   \n[6] Sanjeev Arora and Boaz Barak. Computational complexity: a modern approach. Cambridge University Press, 2009.   \n[7] Idan Attias, Steve Hanneke, Alkis Kalavasis, Amin Karbasi, and Grigoris Velegkas. Optimal learners for realizable regression: Pac learning and online learning. arXiv preprint arXiv:2307.03848, 2023.   \n[8] Jean-Yves Audibert. Fast learning rates in statistical inference through aggregation. The Annals of Statistics, 37(4):1591\u20131646, 2009.   \n[9] Andrew R Barron. Are bayes rules consistent in information? In Open problems in communication and computation, pages 85\u201391. Springer, 1987.   \n[10] Shai Ben-David, D\u00e1vid P\u00e1l, and Shai Shalev-Shwartz. Agnostic online learning. In COLT, volume 3, page 1, 2009.   \n[11] Blair Bilodeau, Dylan J Foster, and Daniel Roy. Tight bounds on minimax regret under logarithmic loss via self-concordance. In International Conference on Machine Learning, 2020.   \n[12] Blair Bilodeau, Dylan J Foster, and Daniel M Roy. Minimax rates for conditional density estimation via empirical entropy. The Annals of Statistics, 51(2):762\u2013790, 2023.   \n[13] Avrim Blum, Merrick Furst, Jeffrey Jackson, Michael Kearns, Yishay Mansour, and Steven Rudich. Weakly learning dnf and characterizing statistical query learning using fourier analysis. In Proceedings of the twenty-sixth annual ACM symposium on Theory of computing, pages 253\u2013262, 1994.   \n[14] Avrim L Blum. Separating distribution-free and mistake-bound learning models over the boolean domain. SIAM Journal on Computing, 23(5):990\u20131000, 1994.   \n[15] Mark Bun. A computational separation between private learning and online learning. Advances in Neural Information Processing Systems, 33:20732\u201320743, 2020.   \n[16] Olivier Catoni. The mixture approach to universal model selection. 1997.   \n[17] Nicolo Cesa-Bianchi and G\u00e1bor Lugosi. Prediction, learning, and games. Cambridge university press, 2006.   \n[18] Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line learning algorithms. IEEE Transactions on Information Theory, 50(9):2050\u20132057, 2004.   \n[19] Constantinos Daskalakis and Vasilis Syrgkanis. Learning in auctions: Regret is hard, envy is easy. In 2016 ieee 57th annual symposium on foundations of computer science (focs), pages 219\u2013228. IEEE, 2016.   \n[20] Miroslav Dud\u00edk, Nika Haghtalab, Haipeng Luo, Robert E Schapire, Vasilis Syrgkanis, and Jennifer Wortman Vaughan. Oracle-efficient online learning and auction design. Journal of the ACM (JACM), 67(5):1\u201357, 2020.   \n[21] Vitaly Feldman. A complete characterization of statistical query learning with applications to evolvability. Journal of Computer and System Sciences, 78(5):1444\u20131459, 2012.   \n[22] Vitaly Feldman. A general characterization of the statistical query complexity. In Conference on Learning Theory, pages 785\u2013830. PMLR, 2017.   \n[23] Vitaly Feldman, Cristobal Guzman, and Santosh Vempala. Statistical query algorithms for mean vector estimation and stochastic convex optimization. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1265\u20131277. SIAM, 2017.   \n[24] Dylan J Foster and Akshay Krishnamurthy. Efficient first-order contextual bandits: Prediction, allocation, and triangular discrimination. Neural Information Processing Systems (NeurIPS), 2021.   \n[25] Dylan J Foster and Alexander Rakhlin. Beyond UCB: Optimal and efficient contextual bandits with regression oracles. International Conference on Machine Learning (ICML), 2020.   \n[26] Dylan J Foster and Alexander Rakhlin. Foundations of reinforcement learning and interactive decision making. arXiv preprint arXiv:2312.16730, 2023.   \n[27] Dylan J Foster, Sham M Kakade, Jian Qian, and Alexander Rakhlin. The statistical complexity of interactive decision making. arXiv preprint arXiv:2112.13487, 2021.   \n[28] Dylan J Foster, Noah Golowich, and Yanjun Han. Tight guarantees for interactive decision making with the decision-estimation coefficient. Conference on Learning Theory (COLT), 2023.   \n[29] Dylan J Foster, Noah Golowich, Jian Qian, Alexander Rakhlin, and Ayush Sekhari. Model-free reinforcement learning with the decision-estimation coefficient. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[30] Oded Goldreich et al. Foundations of cryptography\u2013a primer. Foundations and Trends\u00ae in Theoretical Computer Science, 1(1):1\u2013116, 2005.   \n[31] Alon Gonen, Elad Hazan, and Shay Moran. Private learning implies online learning: An efficient reduction. Advances in Neural Information Processing Systems, 32, 2019.   \n[32] Steve Hanneke. Theory of disagreement-based active learning. Foundations and Trends\u00ae in Machine Learning, 7(2-3):131\u2013309, 2014.   \n[33] Elad Hazan and Tomer Koren. The computational power of optimization in online learning. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, pages 128\u2013141, 2016.   \n[34] TS Jayram. Hellinger strikes back: A note on the multi-party information complexity of and. In International Workshop on Approximation Algorithms for Combinatorial Optimization, pages 562\u2013573. Springer, 2009.   \n[35] Pooria Joulani, Andras Gyorgy, and Csaba Szepesv\u00e1ri. Online learning under delayed feedback. In International Conference on Machine Learning, pages 1453\u20131461. PMLR, 2013.   \n[36] Anatoli Juditsky, Philippe Rigollet, and Alexandre B Tsybakov. Learning by mirror averaging. The Annals of Statistics, 36(5):2183\u20132206, 2008.   \n[37] Michael Kearns. Efficient noise-tolerant learning from statistical queries. Journal of the ACM (JACM), 45(6):983\u20131006, 1998.   \n[38] Michael Kearns, Ming Li, Leonard Pitt, and Leslie Valiant. On the learnability of boolean formulae. In Proceedings of the nineteenth annual ACM symposium on Theory of computing, pages 285\u2013295, 1987.   \n[39] Akshay Krishnamurthy, Alekh Agarwal, Tzu-Kuo Huang, Hal Daum\u00e9 III, and John Langford. Active learning for cost-sensitive classification. In International Conference on Machine Learning, pages 1915\u20131924, 2017.   \n[40] Erich L Lehmann and George Casella. Theory of point estimation. Springer Science & Business Media, 2006.   \n[41] Nick Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. Machine learning, 2(4):285\u2013318, 1988.   \n[42] Jon Christian Mesterharm. Improving on-line learning. Rutgers The State University of New Jersey, School of Graduate Studies, 2007.   \n[43] Arkadii Nemirovski, David Borisovich Yudin, and Edgar Ronald Dawson. Problem complexity and method efficiency in optimization. 1983.   \n[44] Binghui Peng and Aviad Rubinstein. Near optimal memory-regret tradeoff for online learning. arXiv preprint arXiv:2303.01673, 2023.   \n[45] Binghui Peng and Fred Zhang. Online prediction in sub-linear space. In Proceedings of the 2023 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1611\u20131634. SIAM, 2023.   \n[46] Yury Polyanskiy and Yihong Wu. Lecture notes on information theory. 2014.   \n[47] Kent Quanrud and Daniel Khashabi. Online learning with adversarial delays. Advances in neural information processing systems, 28, 2015.   \n[48] Maxim Raginsky and Alexander Rakhlin. Information-based complexity, feedback and dynamics in convex programming. IEEE Transactions on Information Theory, 57(10):7036\u20137056, 2011.   \n[49] Alexander Rakhlin and Karthik Sridharan. Statistical learning and sequential prediction, 2012. Available at http://www.mit.edu/\\~rakhlin/courses/stat928/stat928_notes.pdf.   \n[50] Alexander Rakhlin and Karthik Sridharan. Statistical learning theory and sequential prediction. Lecture Notes in University of Pennsyvania, 44, 2012.   \n[51] Alexander Rakhlin and Karthik Sridharan. Online nonparametric regression. In Conference on Learning Theory, 2014.   \n[52] Alexander Rakhlin and Karthik Sridharan. Online nonparametric regression with general loss functions. arXiv preprint arXiv:1501.06598, 2015.   \n[53] Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning: Random averages, combinatorial parameters, and learnability. Advances in Neural Information Processing Systems 23, pages 1984\u20131992, 2010.   \n[54] Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning via sequential complexities. Journal of Machine Learning Research, 2014.   \n[55] Stephane Ross and J Andrew Bagnell. Reinforcement and imitation learning via interactive no-regret learning. arXiv preprint arXiv:1406.5979, 2014.   \n[56] St\u00e9phane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 627\u2013635. JMLR Workshop and Conference Proceedings, 2011.   \n[57] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.   \n[58] David Simchi-Levi and Yunzong Xu. Bypassing the monster: A faster and simpler optimal algorithm for contextual bandits under realizability. Mathematics of Operations Research, 2021.   \n[59] Vaidehi Srinivas, David P Woodruff, Ziyu Xu, and Samson Zhou. Memory bounds for the experts problem. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages 1158\u20131171, 2022.   \n[60] Joseph F Traub, Grzegorz W Wasilkowski, and Henryk Wo\u00b4zniakowski. Information-based complexity. 1988.   \n[61] Alexandre B Tsybakov. Optimal rates of aggregation. In Learning Theory and Kernel Machines, pages 303\u2013313. Springer, 2003.   \n[62] Alexandre B Tsybakov. Introduction to Nonparametric Estimation. Springer Publishing Company, Incorporated, 2008.   \n[63] Leslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134\u20131142, 1984.   \n[64] S. A. van de Geer. Empirical Processes in M-Estimation. Cambridge University Press, 2000.   \n[65] Vladimir Vovk. Competitive on-line linear regression. In NIPS \u201997: Proceedings of the 1997 conference on Advances in neural information processing systems 10, pages 364\u2013370, Cambridge, MA, USA, 1998. MIT Press.   \n[66] Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge University Press, 2019.   \n[67] Marcelo J Weinberger and Erik Ordentlich. On delayed prediction of individual sequences. IEEE Transactions on Information Theory, 48(7):1959\u20131976, 2002.   \n[68] David P Woodruff, Fred Zhang, and Samson Zhou. Streaming algorithms for learning with experts: Deterministic versus robust. arXiv preprint arXiv:2303.01709, 2023.   \n[69] Tengyang Xie, Dylan J Foster, Yu Bai, Nan Jiang, and Sham M Kakade. The role of coverage in online reinforcement learning. International Conference on Learning Representations (ICLR), 2023.   \n[70] Yunbei Xu and Assaf Zeevi. Upper counterfactual confidence bounds: a new optimism principle for contextual bandits. arXiv preprint arXiv:2007.07876, 2020.   \n[71] Yuhong Yang and Andrew Barron. Information-theoretic determination of minimax rates of convergence. Annals of Statistics, pages 1564\u20131599, 1999. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Contents of Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "I Additional discussion and examples 16 ", "page_idx": 14}, {"type": "text", "text": "A Additional notation 16 ", "page_idx": 14}, {"type": "text", "text": "B Additional related work 16 ", "page_idx": 14}, {"type": "text", "text": "C Examples of estimation problems and loss functions 17   \nC.1 Examples of offline oracles . 18 ", "page_idx": 14}, {"type": "text", "text": "II Omitted results 20 ", "page_idx": 14}, {"type": "text", "text": "D General reductions for oracle-efficient online estimation 20 ", "page_idx": 14}, {"type": "text", "text": "D.1 Characterization of oracle-efficient learnability for classification . . 22   \nD.2 Additional lower and upper bounds for memoryless oracle-efficient algorithms . . 22   \nE Application to interactive decision making 23   \nE.1 Offline oracle-efficient algorithms for interactive decision making . . . 23   \nE.2 Bypassing impossibility of memoryless algorithms via coverability . . . 25 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "III Proofs 28 ", "page_idx": 14}, {"type": "text", "text": "F Technical tools 28 ", "page_idx": 14}, {"type": "text", "text": "G Proofs from Section 3 29   \nG.1 Proofs from Section 3.1 . 29   \nG.2 Proofs from Section 3.2 32   \nG.3 Proofs from Appendix D 36   \nH Proofs from Section 4 40   \nH.1 Proofs from Section 4.1 40   \nH.2 Proofs from Section 4.2 41 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "I Proofs from Appendix E 48 ", "page_idx": 14}, {"type": "text", "text": "Part I Additional discussion and examples ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Additional notation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We denote $\\mathbb{R}_{\\geq0}=[0,\\infty)$ . For any $a,b\\in\\mathbb{R}$ , $a\\wedge b:=\\operatorname*{min}\\left\\{a,b\\right\\}$ and $a\\vee b:=\\operatorname*{max}\\left\\{a,b\\right\\}$ . For any integer $N>\\overline{{0}}$ , $[N]^{\\cdot}{=\\{1,\\dots,N\\}}$ . For any set $\\mathcal{X}$ , $\\Delta(\\mathcal{X})$ is the space of all distributions on $\\mathcal{X}$ . For any integer $T$ , the notation $x^{1:T}$ will be the shorthand notation for the sequence $x^{1},\\ldots,x^{T}$ . For any real number $x\\in\\mathbb R$ , denote by $\\lfloor x\\rfloor$ the largest integer that is smaller than or equal to $x$ and by $\\lceil x\\rceil$ the smallest integer that is greater than or equal to $x$ . The indicator function is denoted by $\\mathbb{1}(\\cdot)$ . We define $O(\\cdot),\\Omega(\\cdot),\\,o(\\cdot),\\,\\Theta(\\cdot),\\,\\widetilde{O}(\\cdot),\\,\\widetilde{\\Omega}(\\cdot),\\,\\widetilde{\\Theta}(\\cdot)$ following standard non-asymptotic big-oh notation. We use the binary relation $x\\lesssim y$ to  indic ate that $x\\le O(y)$ . ", "page_idx": 15}, {"type": "text", "text": "B Additional related work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section we discuss related work not already covered in detail. ", "page_idx": 15}, {"type": "text", "text": "Computational lower bounds for online learning. Beyond Blum [14], another work that considers computational lower bounds for online learning is Hazan and Koren [33]. This work proves lower bounds for online learning in a model where the learner has access to an ERM oracle that can minimize the training loss for an arbitrary dataset $(x^{1},y^{1}),\\ldots,(x^{T},y^{T})$ . Their lower bound does not fit in our computational model due to details around the way description length is formalized. In particular, the main focus of [33] is to obtain a lower bound on the number of oracle calls any online learning algorithm must make to an ERM oracle. ", "page_idx": 15}, {"type": "text", "text": "Similar to the setup for Theorem 4.1, Hazan and Koren [33] con\u221asider a sequence of classification instances with ${\\mathcal{X}}_{n}\\,=\\,\\{0,1\\}^{n}$ and classes ${\\mathcal{F}}_{n}$ of the size of $\\Omega(2\\sqrt{|x_{n}|}\\,)$ , and show that any online learning algorithm requires $\\Omega(\\sqrt{|\\mathcal{X}_{n}|})$ oracle calls to achieve low regret for this class. However, the estimators $f\\in\\mathcal{F}_{n}$ returned by the oracle in their construction have $\\Omega(\\sqrt{|\\mathcal{X}_{n}|})$ description length themselves, meaning that they do not satisfy the $\\mathrm{poly}(n)$ -description length required by the model described in Section 4.1 (in other words, the result is not meaningful as a lower bound on runtime, because simply reading in the output of the offline oracle takes exponential time). For completeness, we restate the example proposed by Hazan and Koren [33, Theorem 22] in our framework below. ", "page_idx": 15}, {"type": "text", "text": "Hard case from $[33]$ . For any integer $n\\geq1$ , consider a binary classification problem with $\\textstyle{\\mathcal{X}}_{2n}=$ $\\{0,1\\}^{2n}$ , $\\mathcal{Z}=\\mathcal{Y}=\\{0,1\\}$ , $\\mathsf{D}=\\mathsf{D}_{0/1}$ and $\\boldsymbol{\\mathscr{K}}(z)=\\mathbb{1}_{z}$ . Let $N:=2^{2n}$ , and let $\\boldsymbol{S}$ be the collection of all sets $S\\ =\\ \\{s_{1},\\ldots,s_{2^{n}}\\}\\ \\subset\\ \\{0,1\\}^{2^{n}}$ where $\\{0,1\\}^{2n}$ is also treated as the integer set of $\\left\\{0,\\ldots,2^{2n}-1\\right\\}$ in left-to-right order and $s_{i}\\,\\in\\,\\{2^{n}(i-1),\\dots,2^{n}i-1\\}$ for each $i=1,\\ldots,2^{n}$ . We define a class $\\mathcal{F}_{2n}=\\{f_{S,\\tau}:S\\in\\mathcal{S},\\tau\\leq2^{n}\\}$ , where ", "page_idx": 15}, {"type": "equation", "text": "$$\nf_{S,\\tau}(x)={\\left\\{\\begin{array}{l l}{0}&{{\\mathrm{if~}}x\\in S{\\mathrm{~and~}}x\\geq s_{\\tau},}\\\\ {1}&{{\\mathrm{otherwise.}}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For this class, we reduce b\u221aack to the Theorem 22 of Hazan and Koren [33] which states that any algorithm with runtime $o(\\sqrt{N})$ has to suffer online estimation error at least $t/2$ for all $1\\leq t\\leq2^{n}$ . The issue with this example for our computation model is that $\\left|\\mathcal{F}_{2n}\\right|=\\Omega(2^{\\sqrt{\\left|\\mathcal{X}_{2n}\\right|}})$ . Any sufficient description for this parameter space in bit strings (or, e.g., boolean circuits) will scale with $\\Omega({\\sqrt{|{\\mathcal{X}}_{2n}|}})$ . Thus, the description length required to return ${\\widehat{f}}^{t}$ is too large (already larger than the lower bound obtained). ", "page_idx": 15}, {"type": "text", "text": "Online learning with memory constraints. A number of recent works focus on memory-regret tradeoffs in online learning [59, 45, 44, 1, 68]. Here, the learner can observe the full data stream $(x^{1},y^{1}),\\ldots,(x^{T},y^{T})$ , but is constrained to $B$ bits of memory. This framework is incomparable to the OEOE framework, but it would be interesting to explore whether there are deeper connections (e.g., any memoryless OEOE algorithm inherently has memory no larger than that of the offline oracle). ", "page_idx": 15}, {"type": "text", "text": "Gaps between offline and online. A long line of work aims to characterize the optimal regret for online learning, developing complexity measures (Littlestone dimension, sequential Rademacher complexity) that parallel classical complexity measures like VC dimension and Rademacher complexity for offilne learning and estimation [10, 53, 54, 7]. It is well known that in general, the optimal rates for online learning can be significantly worse than those for offilne learning. Our work primarily focuses on finite classes $\\mathcal{F}$ , where there is no gap, but for infinite classes, any conversion from offline to online estimation will inevitably lead to a loss in the estimation error rate that scales with appropriate complexity measures for online learning (cf. Appendix D.1). ", "page_idx": 16}, {"type": "text", "text": "Other restricted computational models. Our information-theoretic formulation of oracleefficiency is inspired by statistical query complexity in theoretical computer science and information complexity in optimization, both of which can be viewed as restricted computational models with an information-theoretic flavor. The statistical query model is a framework in which the learner can only access the environment through an oracle that outputs noise estimates (\u201cstatistical queries\u201d) for a target parameter of interest [13, 37, 21\u201323]. Information complexity in optimization is a model in which algorithms can only access the parameter of interest through (potentially noisy) local queries to gradients or other information [43, 60, 48, 2, 5]. ", "page_idx": 16}, {"type": "text", "text": "C Examples of estimation problems and loss functions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In what follows, we give detailed background on three canonical examples of the general estimation framework discussed in Section 1: Binary classification, square loss regression, and conditional density estimation. ", "page_idx": 16}, {"type": "text", "text": "Classification [38, 63]. For binary classification, we take $\\mathcal{Z}=\\mathcal{Y}=\\{0,1\\}$ with the binary loss $\\mathsf{D}_{0/1}(z_{1},z_{2})=\\mathbb{1}(z_{1}\\neq z_{2})$ for $z_{1},z_{2}\\in{\\mathcal{Z}}$ and kernel $\\boldsymbol{\\mathscr{K}}(z)=\\mathbb{1}_{z}$ , which is noiseless. The binary loss is metric-like with $C_{\\mathrm{D}}=1$ . ", "page_idx": 16}, {"type": "text", "text": "For offline estimation, observe that with covariates $x^{1},\\ldots,x^{T}$ and outcomes $y^{t}\\,=\\,f^{\\star}(x^{t})$ for all $t\\in\\{1,\\ldots,T\\}$ , any empirical risk minimizer $\\widehat{f}$ that sets ${\\widehat{f}}(x^{t})=y^{t}$ obtains ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathsf{D}_{0/1}\\Big(\\widehat{f}(x^{t}),f^{\\star}(x^{t})\\Big)=0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For online estimation, the halving algorithm [17] achieves ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{Est}_{0/1}^{\\mathsf{O n}}(T)=\\sum_{t=1}^{T}\\mathsf{D}_{0/1}\\Big(\\widehat{f^{t}}(x^{t}),f^{\\star}(x^{t})\\Big)\\le\\log(|\\mathcal{F}|).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We mention in passing that another natural classification setting we do not explore in detail in this paper is noisy classification, where the setting is as above, except that we set $\\mathcal{Z}\\;=\\;[0,1]$ , $K(f^{\\star}(\\bar{x}))=\\operatorname{Ber}(f^{\\star}(x))$ , and take $\\mathsf{D}_{\\mathsf{a b s}}(z_{1},z_{2})=|\\bar{z_{1}}-z_{2}|$ as the absolute loss for all $z_{1},z_{2}\\in{\\mathcal{Z}}$ . ", "page_idx": 16}, {"type": "text", "text": "Square loss regression [62, 66]. For real-valued regression, we take $\\mathcal{Z}=\\mathcal{Y}=\\mathbb{R}$ with the square loss $\\mathsf{D}_{\\mathsf{s q}}(z_{1},z_{2})=(z_{1}-z_{2})^{2}$ for $z_{1},z_{2}\\in{\\mathcal{Z}}$ and the kernel $K(f^{\\star}(x))=\\mathcal{N}(f^{\\star}(x),1)$ or another subGaussian distribution. Note that the square loss is a metric-like loss with $C_{\\mathrm{D}}=2$ . ", "page_idx": 16}, {"type": "text", "text": "For offline estimation, with covariates $x^{1},\\ldots,x^{T}$ and outcomes $y^{t}\\;\\sim\\;f^{\\star}(x^{t})+\\varepsilon^{t}$ for all $t\\ \\in$ $\\{1,\\ldots,T\\}$ , the classical Empirical Risk Minimization (ERM) $\\begin{array}{r}{\\widehat f:=\\arg\\operatorname*{min}_{f\\in\\mathcal F}\\sum_{t=1}^{T}(f(x^{t})-y^{t})^{2}}\\end{array}$ gives ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{Est}_{\\mathrm{sq}}^{\\mathrm{Off}}(T)=\\sum_{t=1}^{T}\\mathsf{D}_{\\mathrm{sq}}\\Big(\\widehat{f}(x^{t}),f^{\\star}(x^{t})\\Big)\\le\\log(|\\mathcal{F}|\\delta^{-1}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with probability at least $1-\\delta$ (cf. Lemma C.1below). ", "page_idx": 16}, {"type": "text", "text": "For online estimation, the exponential weights algorithm [17], with decision space $\\mathcal{F}$ and the loss at each round chosen to be $\\ell^{t}(\\dot{f})=(f(x^{t})-\\dot{y}^{t})^{2}$ , achieves ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{Est}_{\\mathrm{sq}}^{\\mathrm{On}}(T)=\\sum_{t=1}^{T}\\mathsf{D}_{\\mathrm{sq}}\\Big(\\widehat{f}^{t}(x^{t}),f^{\\star}(x^{t})\\Big)\\le\\log(|\\mathcal{F}|\\delta^{-1}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with probability at least $1-\\delta$ (cf. Foster and Rakhlin [25] for a proof). ", "page_idx": 16}, {"type": "text", "text": "Conditional density estimation [12]. For conditional density estimation, we consider an arbitrary outcome space $\\boldsymbol{\\wp}$ and take $\\mathcal{Z}=\\Delta(\\mathcal{Y})$ with squared Hellinger distance $\\mathsf{D}_{\\mathsf{H}}^{2}$ given by12 ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathsf{D}_{\\mathsf{H}}^{2}\\big(\\widehat{f}(x),f^{\\star}(x)\\big)=\\frac{1}{2}\\int\\bigg(\\sqrt{\\widehat{f}(y\\mid x)}-\\sqrt{f^{\\star}(y\\mid x)}\\bigg)^{2}\\mathsf{d}y.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and $\\mathcal{K}(z)=z$ for all $z\\in{\\mathcal{Z}}$ . Note that squared Hellinger distance is a metric-like loss with $C_{\\mathrm{D}}=2$ . For offline estimation, with covariates $x^{1},\\ldots,x^{T}$ and outcomes $y^{t}\\sim f^{\\star}(x^{t})$ for all $t\\in\\{1,\\ldots,T\\}$ , the classical Maximum Likelihood Estimator (MLE) $\\begin{array}{r}{\\widehat{f}:=\\arg\\operatorname*{max}_{f\\in\\mathcal{F}}\\sum_{t=1}^{T}\\log f(y^{t}\\mid x^{t})}\\end{array}$ gives ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{Est}_{\\mathsf{H}}^{\\mathsf{O f f}}(T)=\\sum_{t=1}^{T}\\mathsf{D}_{\\mathsf{H}}^{2}\\Big(\\widehat{f}(x^{t}),f^{\\star}(x^{t})\\Big)\\le\\log(|\\mathcal{F}|\\delta^{-1}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with probability at least $1-\\delta$ (cf. Lemma C.2below). ", "page_idx": 17}, {"type": "text", "text": "For online estimation, the exponential weights algorithm [17], with decision space $\\mathcal{F}$ and the loss at each round chosen to be $\\ell^{t}(\\bar{f})=-\\log f(\\bar{y}^{t}\\mid x^{t}\\bar{)}$ , achieves ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{Est}_{\\mathsf{H}}^{\\mathsf{O n}}(T)=\\sum_{t=1}^{T}\\mathsf{D}_{\\mathsf{H}}^{2}\\Big(\\widehat{f}^{t}(x^{t}),f^{\\star}(x^{t})\\Big)\\le\\log(|\\mathcal{F}|\\delta^{-1}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with probability at least $1-\\delta$ (cf. Foster et al. [27] for a proof). ", "page_idx": 17}, {"type": "text", "text": "C.1 Examples of offline oracles ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For completeness, below we prove the offline estimation (fixed design) guarantees for square loss empirical risk minimization and maximum likelihood estimation mentioned in the prequel. ", "page_idx": 17}, {"type": "text", "text": "Empirical risk minimization for square loss regression. For any square loss regression instance $(\\mathcal{X},\\mathcal{Y},\\mathcal{Z},\\mathcal{K},\\mathcal{F})$ defined as in Appendix C. The Empirical Risk Minimizer (ERM) estimator $\\widehat{f}$ is defined as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\widehat{f}=\\mathop{\\arg\\operatorname*{min}}_{f\\in\\mathcal{F}}\\sum_{t=1}^{T}(f(x^{t})-y^{t})^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We have the following bounds on the offline estimation error for the squared loss. ", "page_idx": 17}, {"type": "text", "text": "Lemma C.1. For any target parameter $f^{\\star}\\in{\\mathcal{F}}$ , with probability at least $1-\\delta$ , we have, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\Bigl(\\widehat{f}(x^{t})-f^{\\star}(x^{t})\\Bigr)^{2}\\le8\\log(|\\mathcal{F}|/\\delta).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma C.1. Observe that the ERM estimator satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\displaystyle\\sum_{i=1}^{T}(f^{\\star}(x^{t})-\\widehat{f}(x^{t}))^{2}=\\displaystyle\\sum_{t=1}^{T}\\Big((\\widehat{f}(x^{t})-y^{t})^{2}-(f^{\\star}(x^{t})-y^{t})^{2}+2(\\widehat{f}(x^{t})-f^{\\star}(x^{t}))(y^{t}-f^{\\star}(x^{t}))\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq2\\displaystyle\\sum_{t=1}^{T}(\\widehat{f}(x^{t})-f^{\\star}(x^{t}))\\varepsilon^{t},}&{(12)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the last inequality is by the definition of $\\widehat{f}$ , and $\\varepsilon^{t}\\,:=\\,y^{t}\\,-\\,f^{\\star}(x^{t})$ is a standard normal distribution for all $t\\in[T]$ . Then by the Gaussian t ail bound, we have with probability at least $1-\\delta$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}(\\widehat{f}(x^{t})-f^{\\star}(x^{t}))\\varepsilon^{t}\\leq\\sqrt{2\\sum_{t=1}^{T}(\\widehat{f}(x^{t})-f^{\\star}(x^{t}))^{2}\\log(|\\mathcal{F}|/\\delta)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "12More generally, if $\\nu$ is a common dominating measure, then $\\begin{array}{r}{\\mathsf{D}_{\\mathsf{H}}^{2}(\\mathbb{P},\\mathbb{Q})=\\frac{1}{2}\\int\\Bigl(\\sqrt{\\frac{d\\mathbb{P}}{d\\nu}}-\\sqrt{\\frac{d\\mathbb{Q}}{d\\nu}}\\Bigr)^{2}d\\nu}\\end{array}$ , where $\\frac{d\\mathbb{P}}{d\\nu}$ and $\\frac{d\\mathbb{Q}}{d\\nu}$ ar Radon-Nikodym derivatives. The notation in Eq. (9) reflects that this quantity is invariant under the choice of $\\nu$ ", "page_idx": 17}, {"type": "text", "text": "Plug the above inequality back into Eq. (12) and reorganize, we obtain the desired bound of ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}(\\widehat{f}(x^{t})-f^{\\star}(x^{t}))^{2}\\leq8\\log(|\\mathcal{F}|/\\delta).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Maximum likelihood estimation for conditional density estimation. For any conditional density estimation instance $(\\mathcal{X},\\mathcal{Y},\\mathcal{Z},\\mathcal{K},\\mathcal{F})$ defined as in Appendix C. The Maximum Likelihood Estimator (MLE) $\\widehat{f}$ is defined as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widehat{f}=\\mathop{\\operatorname{arg\\,max}}_{f\\in\\mathcal{F}}\\sum_{t=1}^{T}\\log f(y^{t}\\mid x^{t}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We have the following bounds on the offline estimation error for squared Hellinger distance. ", "page_idx": 18}, {"type": "text", "text": "Lemma C.2. For any target parameter $f^{\\star}\\in{\\mathcal{F}}$ , with probability at least $1-\\delta$ , we have, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathsf{D}_{\\mathsf{H}}^{2}\\Big(\\widehat{f}\\big(x^{t}\\big),f^{\\star}\\big(x^{t}\\big)\\Big)\\leq\\log(\\vert\\mathcal{F}\\vert/\\delta).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma C.2. For any parameter $f\\in\\mathcal F$ , define ", "page_idx": 18}, {"type": "equation", "text": "$$\nZ_{t}=-{\\frac{1}{2}}(\\log f^{\\star}(y^{t}\\mid x^{t})-\\log f(y^{t}\\mid x^{t})).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then for any $f\\in\\mathcal F$ , by Lemma A.4 of Foster et al. [27], with probability at least $1-\\delta/|\\mathcal{F}|$ , we have, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}-\\frac{1}{2}(\\log f^{\\star}(y^{t}\\mid x^{t})-\\log f(y^{t}\\mid x^{t}))\\leq\\sum_{t=1}^{T}\\log\\left(\\mathbb{E}\\left[\\sqrt{\\frac{f(y^{t}\\mid x^{t})}{f^{\\star}(y^{t}\\mid x^{t})}}\\right]\\right)+\\log(|\\mathcal{F}|/\\delta).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We further have by the inequality of $\\log(1+x)\\leq x$ that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log\\left(\\mathbb{E}\\left[\\sqrt{\\frac{f\\left(y^{t}\\mid x^{t}\\right)}{f^{\\star}\\left(y^{t}\\mid x^{t}\\right)}}\\right]\\right)\\leq\\mathbb{E}\\left[\\sqrt{\\frac{f\\left(y^{t}\\mid x^{t}\\right)}{f^{\\star}\\left(y^{t}\\mid x^{t}\\right)}}-1\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $y^{t}\\sim f^{\\star}(x^{t})$ , we have by standard calculus and the definition of the squared Hellinger distance that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\!\\left[{\\sqrt{\\frac{f(y^{t}\\mid x^{t})}{f^{\\star}(y^{t}\\mid x^{t})}}}-1\\right]=-\\mathsf{D}_{\\mathsf{H}}^{2}(f^{\\star}(x^{t}),f(x^{t})).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Altogether, we have obtained for any $f\\in\\mathcal F$ , with probability at least $1-\\delta/|\\mathcal{F}|$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}-\\frac{1}{2}(\\log f^{\\star}(y^{t}\\mid x^{t})-\\log f(y^{t}\\mid x^{t}))\\leq-\\sum_{t=1}^{T}\\mathsf{D}_{\\mathsf{H}}^{2}(f^{\\star}(x^{t}),f(x^{t}))+\\log(|\\mathcal{F}|/\\delta).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, by union bound, the above inequality holds for all $f\\in\\mathcal F$ with probability at least $1-\\delta$ . Thus the MLE $\\widehat{f}$ satisfies ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{t=1}^{T}\\mathsf{D}_{\\mathsf{H}}^{2}\\Big(f^{\\star}(x^{t}),\\widehat{f}(x^{t})\\Big)\\le\\displaystyle\\sum_{t=1}^{T}\\frac{1}{2}(\\log f^{\\star}(y^{t}\\mid x^{t})-\\log\\widehat{f}(y^{t}\\mid x^{t}))+\\log(|\\mathcal{F}|/\\delta)}&{{}}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\le\\log(|\\mathcal{F}|/\\delta),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the second inequality is by the defintion of MLE. ", "page_idx": 18}, {"type": "text", "text": "Algorithm 2 Reduction from OEOE to Online Learning with Delayed Feedback ", "page_idx": 19}, {"type": "text", "text": "1: input: Offline estimation oracle $\\mathbf{Alg}_{0\\mathsf{f f}}$ with parameter $\\beta_{0\\mathsf{f}\\mathsf{f}}\\,\\geq\\,0$ , delay parameter $N\\,\\in\\,\\mathbb{N}$ ,   \ndelayed online learning algorithm $A_{\\mathrm{DOL}}$ for class $\\mathcal{F}$ .   \n2: for $t=1,2,\\ldots,T$ do   \n3: Receive $\\widehat{f^{t}}=\\mathbf{A}\\mathbf{l}\\mathbf{g}_{\\mathsf{O f f}}^{t}\\big(x^{1},\\ldots,x^{t-1},y^{1},\\ldots,y^{t-1}\\big).$ .   \n4: if $t>N$ then   \n5: $\\begin{array}{r}{\\mathrm{et}\\;\\tilde{f}^{t-N}:=\\frac{1}{N}\\sum_{i=t-N+1}^{t}\\widehat{f}^{i}.}\\end{array}$   \n6: Let $\\ell^{t-N}(f):=\\mathsf{D}\\Big(\\widetilde{f}^{t-N}\\big(x^{t-N}\\big),f\\big(x^{t-N}\\big)\\Big)$ and pass $\\ell^{t-N}(\\cdot)$ to $A_{\\mathrm{DOL}}$ as the delayed feed  \nback.   \n7: Let $\\mu^{t}=\\mathcal{A}_{\\sf D O L}^{t}\\big(\\ell^{1},..,\\ell^{t-N}\\big)$ be the delayed online learner\u2019s prediction distribution.   \n8: Predict with $\\bar{f}^{t}\\sim\\mu^{t}$ and receive $x^{t}$ . ", "page_idx": 19}, {"type": "text", "text": "Part II ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Omitted results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "D General reductions for oracle-efficient online estimation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The oracle-efficient online estimation algorithm in Section 3.1, Algorithm 1, is somewhat specialized to finite classes. In this section, we provide a more general approach to designing oracle-efficient algorithms based on delayed online learning, and use it to derive a characterization of oracle-efficient learnability for classification with infinite classes $\\mathcal{F}$ . ", "page_idx": 19}, {"type": "text", "text": "For the results in this section, we assume that $\\mathcal{Z}$ and $\\mathsf{D}$ are convex, which covers regression and conditional density estimation; variants of our result for classification are given in Appendix G.3. ", "page_idx": 19}, {"type": "text", "text": "Online learning with delayed feedback. Before introducing our algorithm, we first introduce an abstract delayed online learning framework [67, 42, 35, 47]. In our framework, the learner is given a class $\\mathcal{F}\\subseteq\\dot{\\mathcal{Z}}^{\\mathcal{X}}$ . Their goal is to choose a sequence of parameters $\\bar{f}^{1},\\ldots,\\bar{f}^{T}$ that minimizes regret against the class $\\mathcal{F}$ for an adversarially chosen sequence of loss functions $\\ell^{1},\\dots,\\ell^{T}$ , with the twist being that the loss $\\ell^{t}$ is not revealed immediately at step $t$ , and instead becomes available at step $t+N$ for a delay parameter $N\\in\\mathbb{N}$ . ", "page_idx": 19}, {"type": "text", "text": "In more detail, the interaction between the learner and the environment proceeds as follows: ", "page_idx": 19}, {"type": "text", "text": "\u2022 For $t=1,\\dots,T$ : \u2022 The learner picks $\\bar{f}^{t}\\sim\\mu^{t}\\in\\Delta(\\mathcal{Z}^{\\mathcal{X}})$ . \u2022 Learner incurs loss $\\ell^{t}(\\bar{f}^{t})$ and adversary reveals loss function $\\ell^{t-N}:\\mathcal{Z}^{\\chi}\\to[0,1]$ for round $t-N$ (if $t<N+1$ , nothing is revealed). ", "page_idx": 19}, {"type": "text", "text": "The goal of the learner is to minimize regret in the sense that ", "page_idx": 19}, {"type": "equation", "text": "$$\nR_{\\mathsf{D O L}}(T,N,\\gamma):=\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}^{t}\\sim\\mu^{t}}\\big[\\ell^{t}(\\bar{f}^{t})\\big]-\\gamma\\cdot\\operatorname*{min}_{f\\in\\mathcal{F}}\\sum_{t=1}^{T}\\ell^{t}(f)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "is small, where $\\gamma\\geq1$ is a parameter. For $\\gamma=1$ , this definition coincides with the standard notion of regret in online learning (e.g., Cesa-Bianchi and Lugosi [17]), but allowing for $\\gamma>1$ will prove useful for our technical results. ", "page_idx": 19}, {"type": "text", "text": "Algorithm and online estimation error bound. Algorithm 2 describes our reduction from oracleefficient online estimation to delayed online learning. In addition to an offline oracle $\\mathbf{Alg}_{0\\mathsf{f f}}$ , the algorithm takes as input a delay parameter $N\\in\\mathbb{N}$ and a delayed online learning algorithm $A_{\\mathrm{DOL}}$ for the class $\\mathcal{F}$ (Algorithm 2 does not explicitly take the class $\\mathcal{F}$ as an argument, as the algorithm only implicitly makes use of $\\mathcal{F}$ through $\\mathbf{\\mathcal{A}}_{\\sf D O L}$ ). ", "page_idx": 19}, {"type": "text", "text": "The basic premise behind Algorithm 2 is that for any sequence of consistent offilne estimators, we can average to improve the predictions. Consider a step $t\\in[T]$ . Suppose $\\widehat{f}^{1},\\ldots,\\widehat{f}^{T}$ are produced by an offilne oracle $\\mathbf{A}\\mathbf{lg}_{0\\mathsf{f f}}$ with parameter $\\beta_{0\\mathsf{f}\\mathsf{f}}\\geq0$ , where we augment the sequence by setting $\\widehat{f}^{\\scriptscriptstyle T+s}=\\widehat{f}^{\\scriptscriptstyle T}$ for all $s\\in\\mathbb{N}$ . Then we can use an argument based on convexity (cf. proof of Theorem D .1) to sh ow that for any $N\\in\\mathbb{N}$ , the averaged parameters ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tilde{f}^{t}:=\\frac{1}{N}\\sum_{i=t+1}^{t+N}\\widehat{f}^{i}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "satisfies ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\mathsf{D}\\Big(\\tilde{f}^{t}(x^{t}),f^{\\star}(x^{t})\\Big)\\leq N+\\displaystyle\\frac{1}{N}\\sum_{t=1}^{T-N}\\sum_{i=t+1}^{t+N}\\mathsf{D}\\Big(\\widehat{f}^{i}(x^{t}),f^{\\star}(x^{t})\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=N+\\displaystyle\\frac{1}{N}\\sum_{t=2}^{T}\\sum_{i<t}\\mathsf{D}\\Big(\\widehat{f}^{t}(x^{i}),f^{\\star}(x^{i})\\Big)\\leq N+\\beta\\mathsf{o p}T/N.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In particular, as we increase $N$ , the quality of the predictions increases, and we achieve sublinear estimation error as soon as $N=\\omega(T)$ . Of course, the catch here is that $\\tilde{f}^{t}$ depends on the predictions of future estimators, and cannot be computed at step $t$ . However, $\\tilde{f}^{t}$ can be computed at step $t\\!+\\!N\\!+\\!1$ , with a delay of $N$ . This leads us to appeal to delayed online learning. In particular, at each step $t\\geq N+1$ , Algorithm 2 proceeds as follows. Using the new offline estimator ${\\widehat{f}}^{t}$ from $\\mathbf{Alg}_{0\\uparrow\\uparrow}$ , the algorithm computes the averaged estimator $\\begin{array}{r}{\\widetilde{f}^{t-N}:=\\frac{1}{N}\\sum_{i=t-N+1}^{t}\\widehat{f}^{i}}\\end{array}$ corresponding to the estimator in Eq. (13) for step $t-N$ . The algorithm then defines a loss function ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\ell^{t-N}(f):=\\mathsf{D}\\Big(\\widetilde{f}^{t-N}(x^{t-N}),f(x^{t-N})\\Big)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and feeds it into the delayed online learning algorithm $A_{\\mathrm{DOL}}$ as the feedback for step $t-N$ . Finally, Algorithm 2 uses the prediction distribution $\\mu^{t}$ produced by $A_{\\mathrm{DOL}}$ to sample the final estimator $\\check{f}^{t}$ , then proceeds to the next step. Our main theorem shows that as long $A_{\\mathrm{DOL}}$ achieves low regret for delayed online learning, this strategy leads to low online estimation error. ", "page_idx": 20}, {"type": "text", "text": "Theorem D.1 (Reduction from oracle-efficient online estimation to delayed online learning). Let D be any convex, metric-like loss. Suppose we run Algorithm 2 with delay parameter $N\\in\\mathbb N$ and $a$ delayed online learning algorithm $A_{\\mathrm{DOL}}$ for the class $\\mathcal{F}$ . Then for all $\\gamma\\geq1$ , Algorithm 2 ensures that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[\\mathbf{Est}_{\\mathsf{D}}^{\\mathsf{O n}}(T)\\big]\\leq O(C_{\\mathsf{D}}\\gamma(N+\\beta_{\\mathsf{O f f}}T/N)+R_{\\mathsf{D O L}}(T,N,\\gamma)),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with any offline oracle $\\mathbf{Alg}_{0\\mathsf{f f}}$ with parameter $\\beta_{0\\mathsf{f}\\mathsf{f}}\\geq0,$ , where ", "page_idx": 20}, {"type": "equation", "text": "$$\nR_{\\mathsf{D O L}}(T,N,\\gamma):=\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}^{t}\\sim\\mu^{t}}\\big[\\ell^{t}(\\bar{f}^{t})\\big]-\\gamma\\cdot\\operatorname*{min}_{f\\in\\mathcal{F}}\\sum_{t=1}^{T}\\ell^{t}(f)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "is the regret of $A_{\\mathrm{DOL}}$ for the sequence of losses constructed in Algorithm 2. ", "page_idx": 20}, {"type": "text", "text": "The parameter $N$ controls a sort of bias-variance tradeoff in Theorem D.1. The first term in Eq. (14) (corresponding to the bias of the averaged estimators) decreases with the delay $N$ , while the second term (corresponding to the regret of $A_{\\mathrm{DOL}}$ ) increases; the optimal choice for $N$ will balance these terms. To make this concrete, we revisit finite classes as a warmup. ", "page_idx": 20}, {"type": "text", "text": "Example: Finite classes. Delayed online learning is well-studied, and optimal algorithms are known for many classes of interest [67, 42, 35, 47]. The following standard result (a proof is given in Appendix G.3 for completeness) gives a delayed regret bound for arbitrary finite classes. ", "page_idx": 20}, {"type": "text", "text": "Lemma D.1. Consider the delayed online learning setting with a delay parameter $N$ . There exists an algorithm that achieves ", "page_idx": 20}, {"type": "equation", "text": "$$\nR_{\\sf D O L}(T,N,2)=\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}^{t}\\sim\\mu^{t}}\\big[\\ell^{t}(\\bar{f}^{t})\\big]-2\\cdot\\operatorname*{min}_{f\\in\\mathcal{F}}\\sum_{t=1}^{T}\\ell^{t}(f)\\leq2N\\cdot\\log|\\mathcal{F}|\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for any sequences of losses $\\ell^{1},\\ldots,\\ell^{T}\\in[0,1]$ . ", "page_idx": 20}, {"type": "text", "text": "Combining Theorem D.1 with Lemma D.1, we can obtain the following upper bound for oracleefficient online estimation. ", "page_idx": 21}, {"type": "text", "text": "Corollary D.1 (Oracle-efficient online estimation for finite classes via delayed online learning). Consider an arbitrary instance $(\\mathcal{X},\\mathcal{Y},\\mathcal{Z},\\mathcal{K},\\mathcal{F})$ and metric-like loss D, and assume $\\mathcal{Z}$ is convex. By choosing $A_{\\mathrm{DOL}}$ as in Lemma D.1, Algorithm 2 ensures that for any $N\\geq1$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[\\mathbf{Est}_{\\mathsf{D}}^{\\mathsf{O n}}(T)\\big]\\leq O(C_{\\mathsf{D}}(N+\\beta_{\\mathsf{O f f}}T/N)+N\\cdot\\log|\\mathcal{F}|).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "with any offline oracle $\\mathbf{Alg}_{0\\mathsf{f f}}$ with parameter $\\beta_{0\\mathsf{f f}}$ . ", "page_idx": 21}, {"type": "text", "text": "By choosing $\\begin{array}{r l r}{N}&{{}=}&{\\sqrt{\\frac{C_{0}\\beta_{0\\mathrm{ff}}\\cdot T}{C_{0}+\\log|\\mathcal{F}|}}\\;\\;\\vee\\;\\;1.}\\end{array}$ , Corollary D.1 gives an upper bound of $O\\Big(\\sqrt{C_{\\mathsf{D}}\\beta_{0\\mathsf{f f}}(C_{\\mathsf{D}}+\\log|\\mathcal{F}|)\\cdot T}+\\log|\\mathcal{F}|\\Big)$ . While the rate in Corollary D.1, is worse than Theorem 3.1 in terms of dependence on $T$ , the reduction has two advantages: (1) It does require a-priori knowledge of the offline estimation parameter $\\beta_{0\\mathsf{f f}}$ : If we choose $\\begin{array}{r}{N\\,=\\,\\sqrt{\\frac{C_{0}\\cdot T}{C_{0}+\\log|\\mathcal{F}|}}\\lor1}\\end{array}$ , Corollary D.1 obtains $O((\\beta_{0\\ell\\ell}+1)(\\sqrt{C_{\\mathsf{D}}(C_{\\mathsf{D}}+\\log|\\mathcal{F}|)\\cdot T}+C_{\\mathsf{D}}+\\log|\\mathcal{F}|))$ ; (2) The reduction by Algorithm 2 is more flexible, and allows for guarantees beyond finite classes, as we now illustrate. ", "page_idx": 21}, {"type": "text", "text": "D.1 Characterization of oracle-efficient learnability for classification ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "As an application of Algorithm 2 and Theorem D.1, we give a characterization for oracle-efficient learnability in the OEOE framework. To state the result, we define $\\operatorname{Ldim}({\\mathcal{F}})$ as the Littlestone dimension for a binary function class $\\mathcal{F}$ (e.g., Ben-David et al. [10]). ", "page_idx": 21}, {"type": "text", "text": "Theorem D.2 (Characterization of oracle-efficient learnability for binary classification). Consider a binary classification instance $(\\mathcal{X},\\mathcal{Y},\\mathcal{Z},\\mathcal{K},\\mathcal{F})$ with $\\mathcal{Z}=\\mathcal{Y}=\\{0,1\\}$ , $\\mathrm{D}=\\mathsf{D}_{0/1}$ and $\\boldsymbol{\\mathscr{K}}(z)=\\mathbb{1}_{z}$ . For any class $\\mathcal{F}$ and $\\beta_{0\\mathsf{f}\\mathsf{f}}\\geq0,$ , there exists an oracle-efficient algorithm that achieves online estimation error $O({\\sqrt{\\beta_{0\\mathsf{f f}}\\mathrm{Ldim}({\\mathcal{F}})\\cdot T\\log T}}+\\mathrm{Ldim}({\\mathcal{F}})\\log T)$ . On the other hand, in the worst-case any algorithm must suffer at least $\\Omega(\\operatorname{Ldim}({\\mathcal{F}}))$ online estimation error. ", "page_idx": 21}, {"type": "text", "text": "The main idea behind this result is to show that we can create a delayed online learner for the reduction in Algorithm 2 that achieves low regret for Littlestone classes.13 ", "page_idx": 21}, {"type": "text", "text": "D.2 Additional lower and upper bounds for memoryless oracle-efficient algorithms ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The proof of Theorem 3.3 uses that the estimators ${\\widehat{f}}^{t}$ produced by the offline estimation oracle may be improper (i.e., $\\widehat{f}^{t}\\notin\\mathcal{F})$ . We next provide a variant of the result that holds even if the estimation oracle is proper,  under the additional assumptions that (i) the learner is itself proper in the sense that $\\mu^{t}\\in\\Delta(\\mathcal{F})$ , and (ii) the learner is time-invariant (i.e., the learner sets $\\mu^{t}=F(\\widehat{f^{t}})$ for all $t$ ). ", "page_idx": 21}, {"type": "text", "text": "Theorem $3.3^{\\prime}$ (Impossibility of memoryless algorithms for OEOE; proper variant). Consider the binary classification setting with $\\mathcal{Z}=\\mathcal{Y}=\\{0,1\\}$ and loss $\\mathsf{D}_{0/1}(\\cdot,\\cdot)$ . For any $N\\in\\mathbb N$ and $\\beta_{0\\mathsf{f}\\mathsf{f}}\\geq0$ , there exists an instance $(\\mathcal{X},\\mathcal{Y},\\mathcal{Z},\\mathcal{K},\\mathcal{F})$ with $|\\mathcal{F}|=|\\mathcal{X}|=N$ such that for any memoryless oracleefficient algorithm that is $(i)$ proper, and (ii) time-invariant, there exists a sequence of covariates $(x^{1},\\ldots,x^{T})$ and a proper offline oracle $\\mathbf{Alg}_{0\\mathsf{f f}}$ with parameter $\\beta_{0\\mathsf{f f}}$ such that $\\mathbb{E}\\big[\\mathbf{Est}_{\\mathsf{D}}^{\\mathsf{O n}}(T)\\big]\\ \\geq$ $\\Omega(\\operatorname*{min}{\\{N(\\beta_{0\\mathsf{f f}}+1),T\\}})$ . ", "page_idx": 21}, {"type": "text", "text": "A complementary upper bound. For completeness, we conclude by showing that the (large) lower bound in Theorem 3.3 can be achieved with a memoryless oracle-efficient algorithm. We consider the \u201ctrivial\u201d algorithm that outputs the estimators produced by the offline oracle as-is. ", "page_idx": 21}, {"type": "text", "text": "Proposition D.1 (Upper bound for memoryless OEOE). For any instance $(\\mathcal{X},\\mathcal{Y},\\mathcal{Z},\\mathcal{K},\\mathcal{F})$ , metric-like loss D, and offline oracle $\\mathbf{Alg}_{0\\mathsf{f f}}$ with parameter $\\beta_{0\\mathsf{f}\\mathsf{f}}$ , the algorithm that returns $\\bar{f}^{t}\\;\\;=\\;\\;\\widehat{f}^{t}\\;\\;:=\\;\\;{\\bf A l g}_{0\\mathrm{ff}}^{t}\\big(x^{1},\\ldots,x^{t-1},y^{1},\\ldots,y^{t-1}\\big)$ has online estimation error $\\begin{array}{r l}{\\mathbf{Est}_{\\mathrm{D}}^{\\mathsf{O n}}(T)}&{\\leq}\\end{array}$ $O((\\beta_{0\\mathsf{f f}}+1)|\\mathcal{X}|\\log T)$ . ", "page_idx": 21}, {"type": "text", "text": "E Application to interactive decision making ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we apply our techniques for oracle-efficient online estimation to the Decision Making with Structured Observations (DMSO) framework for interactive decision making introduced by [27]. First, in Appendix E.1, we use our reductions to provide offline oracle-efficient algorithms for interactive decision making. Then, in Appendix E.2, we focus on reinforcement learning and show that it is possible to bypass the impossibility results for memoryless oracle-efficient algorithms (Theorem 3.3) for instances $(\\mathcal{X},\\mathcal{Y},\\mathcal{Z},\\mathcal{K},\\mathcal{F})$ corresponding to Markov decision processes that satisfy a structural property known as coverability. ", "page_idx": 22}, {"type": "text", "text": "E.1 Offline oracle-efficient algorithms for interactive decision making ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we introduce the setting of Decision Making with Structured Observations (DMSO) and the applications of our results to this setting. ", "page_idx": 22}, {"type": "text", "text": "Decision Making with Structured Observations (DMSO). The DMSO framework [27] captures a large class of interactive decision making problems (e.g. contextual bandits and reinforcement learning). In this framework, the learner is given access to a model class $\\mathcal{M}$ that contains an unknown true model $M^{\\star}:\\Pi\\rightarrow\\Delta(\\mathcal{R}\\times\\mathcal{O})$ , where $\\Pi$ is the decision space, $\\mathcal{R}\\subseteq\\mathbb{R}$ is the reward space and $\\scriptscriptstyle\\mathcal{O}$ is the observation space. Then the interaction between the learner and the environment proceeds in $T$ rounds, where for each round $t=1,\\dots,T$ : ", "page_idx": 22}, {"type": "text", "text": "1. The learner selects a decision $\\pi^{t}\\in\\Pi$ . ", "page_idx": 22}, {"type": "text", "text": "2. Nature selects a reward $r^{t}\\in\\mathcal{R}$ and observation $o^{t}\\in\\mathcal{O}$ based on the decision, where the pair $(r^{t},o^{t})$ is drawn independently from the unknown distribution $M^{\\star}(\\pi^{t})$ . The reward and observation is then observed by the learner. ", "page_idx": 22}, {"type": "text", "text": "Let $g^{M}(\\pi):=\\,\\mathbb{E}^{M,\\pi}[r]$ denote the mean reward function and $\\pi_{\\scriptscriptstyle M}:=\\,\\arg\\operatorname*{max}_{\\pi\\in\\Pi}g^{\\scriptscriptstyle M}(\\pi)$ denote the decision with the greatest expected reward for $M$ . The learner\u2019s performance is evaluated in terms of regret to the optimal decision for $M^{\\star}$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{Reg}_{\\mathsf{D M}}(T):=\\sum_{t=1}^{T}\\mathbb{E}_{\\pi^{t}\\sim p^{t}}\\big[g^{M^{\\star}}\\big(\\pi_{M^{\\star}}\\big)-g^{M^{\\star}}\\big(\\pi^{t}\\big)\\big],\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $p^{t}\\in\\Delta(\\Pi)$ is the learner\u2019s distribution over decisions at round $t$ . ", "page_idx": 22}, {"type": "text", "text": "Background: Reducing DMSO to online estimation. Any DMSO class $(\\mathcal{M},\\Pi,\\mathcal{O})$ induces an instance $(\\mathcal{X},\\mathcal{Y},\\mathcal{Z},\\mathcal{K},\\mathcal{F})$ of the estimation framework in Section 1 as follows. We associate ${\\mathcal{F}}={\\mathcal{M}}$ , $\\mathcal{X}=\\Pi$ , $y=O\\times\\mathcal{R}$ , $\\dot{\\mathcal{Z}}=\\Delta(\\mathcal{O}\\times\\mathcal{R})$ , and ${\\cal K}(M^{\\star}(\\pi))=M^{\\star}(\\pi)$ . That is, we have a conditional density estimation problem in which the covariates are decisions $\\pi\\,\\in\\,\\Pi$ and the outcomes are observation-reward pairs drawn from the underlying model $M^{\\star}(\\pi)$ . In particular for a sequence of decisions $\\pi^{1},\\cdot\\cdot\\cdot,\\pi^{T}$ and a sequence of estimators $\\widehat{M^{1}},\\ldots,\\widehat{M^{r}}$ , we define the online estimation error for a loss $\\mathsf{D}$ as ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\bf E s t}_{\\mathsf{D}}^{\\mathsf{O n}}(T)=\\sum_{t=1}^{T}\\mathsf{D}\\big(\\widehat{M}^{t}(\\pi^{t}),M^{\\star}(\\pi^{t})\\big).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We refer to any algorithm $\\mathbf{Alg}_{0\\mathfrak{n}}$ that ensures that $\\mathbf{Est}_{\\mathsf{D}}^{\\mathsf{O n}}(T)\\leq\\beta_{0\\mathsf{n}}$ almost surely given access to $\\left\\{(\\pi^{t},o^{t},r^{t})\\right\\}_{t=1}^{T}$ with $(o^{t},r^{t})\\sim M^{\\star}(\\pi^{t})$ as an online estimation oracle with parameter $\\beta_{0{\\mathfrak{n}}}$ . ", "page_idx": 22}, {"type": "text", "text": "Foster et al. [27, 28] give an algorithm, Estimation-to-Decisions (E2D), that provides bounds on the regret in Eq. (16) given access to an online estimation oracle $\\mathbf{Alg}_{0\\mathfrak{n}}$ . The algorithm is (online) oracleefficient and memoryless, in the sense that the decision $\\pi^{t}$ at each step $t$ is a measurable function of the oracle\u2019s outputM t. To restate their result, we define the Decision-Estimation Coeficient for the class $\\mathcal{M}$ as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathsf{d e c}_{\\gamma}^{\\mathsf{D}}(\\mathcal{M},\\overline{{M}})=\\operatorname*{inf}_{p\\in\\Delta(\\Pi)}\\operatorname*{sup}_{M\\in\\mathcal{M}}\\mathbb{E}_{\\pi\\sim p}\\bigg[g^{M}(\\pi_{M})-g^{M}(\\pi)-\\gamma\\cdot\\mathsf{D}\\big(M(\\pi),\\overline{{M}}(\\pi)\\big)\\bigg]\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for a reference model $\\overline{{M}}$ and any losses D. We further define $\\mathsf{d e c}_{\\gamma}^{\\mathsf{D}}(\\mathcal{M})=\\operatorname*{sup}_{\\overline{{M}}\\in\\mathcal{M}}\\mathsf{d e c}_{\\gamma}^{\\mathsf{D}}(\\mathcal{M},\\overline{{M}})$ With this notation, the main regret bound for E2D is as follows. ", "page_idx": 22}, {"type": "text", "text": "1: parameters: Offline estimation oracle $\\mathbf{Alg}_{0\\mathsf{f f}}$ with parameter $\\beta_{0\\mathsf{f f}}$ . Oracle-efficient online estimation algorithm AlgOEOE. Exploration parameter $\\gamma>0$ .   \n2: for $t=1,2,\\cdots,T$ do   \n3: Compute estimate $\\widehat{\\boldsymbol{M}^{t}}=\\mathbf{Alg}_{0\\mathrm{ff}}^{t}\\bigg(\\pi^{1},\\dots,\\pi^{t-1},o^{1},...,o^{t-1}\\bigg).$   \n4: Feed $\\widehat{M}^{t}$ to OEOE algorithm $\\mathbf{Alg}_{0\\mathsf{E O E}}$ and obtain $\\mu^{t}$ .   \n5: Let $p^{t}=\\underset{p\\in\\Delta(\\Pi)}{\\arg\\operatorname*{min}}\\ \\underset{M\\in\\mathcal{M}}{\\operatorname*{sup}}\\ \\mathbb{E}_{\\pi\\sim p,\\widehat{M}\\sim\\mu^{t}}\\Big[g^{M}(\\pi_{M})-g^{M}(\\pi)-\\gamma\\cdot\\mathsf{D}\\Big(\\widehat{M}(\\pi),M(\\pi)\\Big)\\Big].$ $\\pi^{t}$ ", "page_idx": 23}, {"type": "text", "text": "6: Sample decision $\\pi^{t}\\sim p^{t}$ and $o^{t}\\sim M^{\\star}(\\pi^{t})$ and feed to OEOE algorithm AlgOEOE. ", "page_idx": 23}, {"type": "text", "text": "Proposition E.1 (Theorem 4.3 of [27]). For any model class $(\\mathcal{M},\\Pi,\\mathcal{O})$ and metric-like loss D, any $\\gamma>0$ , and any online estimation oracle $\\mathbf{Alg}_{0\\mathfrak{n}}$ with parameter $\\beta_{0\\mathfrak{n}}>0$ , the E2D algorithm ensures that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{Reg}_{\\mathsf{D M}}(T)\\lesssim\\operatorname*{sup}_{\\mu\\in\\Delta(\\mathcal{M})}\\mathsf{d e c}_{\\gamma}^{\\mathsf{D}}(\\mathcal{M},\\mu)\\cdot T+\\gamma\\cdot\\beta_{0\\mathsf{n}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For the case of squared Hellinger distance where $\\mathsf{D}=\\mathsf{D}_{\\mathsf{H}}^{2}$ , the Decision-Estimation Coefficient $\\mathsf{d e c}_{\\gamma}^{\\mathsf{D}}(\\mathcal{M})$ was shown to be a lower bound on the minimax optimal regret for any class $\\mathcal{M}$ . Hence, Proposition E.1 shows that it is possible to achieve near-optimal regret for any interactive decision making problems whenever an online estimation oracle is available. However, it was unclear whether similar results could be achieved based on offline estimation oracles. ", "page_idx": 23}, {"type": "text", "text": "Making E2D offilne oracle-efficient. Algorithm 3 (E2D.Off) invokes the E2D algorithm of Foster et al. [27] with any oracle-efficient online estimation algorithm AlgOEOE (which can be any of the algorithms we provide, e.g. Theorems 3.1 and 4.2), along with an offilne estimation oracle $\\mathbf{Alg}_{0\\mathsf{f f}}$ , to provide offline oracle-efficient guarantees for interactive decision making. Invoking the algorithm with Version Space Averaging (via Theorem 3.1) leads to the following corollary. ", "page_idx": 23}, {"type": "text", "text": "Corollary E.1. Consider any DMSO class $(\\mathcal{M},\\Pi,\\mathcal{O})$ and metric-like loss D. Algorithm 3, with exploration parameter $\\gamma>0$ and AlgOEOE chosen to be Algorithm $^{\\,I}$ , ensures that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[{\\mathbf{Reg}_{\\mathsf{D M}}}]\\leq O(\\log T)\\cdot\\operatorname*{max}\\biggl\\{\\mathsf{d e c}_{\\gamma}^{\\mathsf{D}}(\\mathcal{M})\\cdot T,\\;\\gamma\\cdot(\\beta_{0\\mathsf{H}}+1)\\log|\\mathcal{M}|\\biggr\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for any offline estimation oracle $\\mathbf{Alg}_{0\\mathsf{f f}}$ with parameter $\\beta_{0\\mathsf{f f}}$ . ", "page_idx": 23}, {"type": "text", "text": "This result shows that information-theoretically, it is possible to achieve low regret in the DMSO framework with offline oracles, though the result is not computationally efficient. ", "page_idx": 23}, {"type": "text", "text": "As an example, in the case of square loss regression (Appendix C) which is used for contextual bandits, an offilne guarantee of $\\beta_{0\\mathsf{f}\\mathsf{f}}=O(\\log|\\mathcal{M}|)$ is achievable. Meanwhile, it is known that $\\mathsf{d e c}_{\\gamma}^{\\mathsf{D}_{\\mathsf{s q}}}(\\mathcal{M})\\lesssim$ $|{\\mathcal{A}}|/\\gamma$ [27]. Thus Corollary E.1 achieves a bound of $\\widetilde{O}(\\sqrt{|A|T}\\cdot\\log|\\mathcal{M}|)$ with an appropriate choice of $\\gamma$ . The best know regret guarantee for contextual bandit is $\\widetilde{O}(\\sqrt{|A|T\\log|\\mathcal{M}|})$ [58, 70]. The bound from Corollary E.1 matches the state-of-the-art result up to a factor $\\widetilde{O}(\\sqrt{\\log{|\\mathcal{M}|}})$ . How to remove this suboptimality is an interesting direction for future work. ", "page_idx": 23}, {"type": "text", "text": "Naturally, the other reductions for oracle-efficient online estimation developed in this paper can be combined with Algorithm 3 as well. In particular, by combining with Theorem 4.2 we derive the following corollary for squared Hellinger distance. ", "page_idx": 23}, {"type": "text", "text": "Corollary (Informal). Whenever online conditional density estimation can be performed efficiently with access to the full history, and whenever the minimax problem in Eq. (18) can be solved efficiently, there exists a computationally efficient and offilne oracle-efficient algorithm with near-optimal regret in the DMSO framework. ", "page_idx": 23}, {"type": "text", "text": "E.2 Bypassing impossibility of memoryless algorithms via coverability ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Recall that our results in Section 3.2 show that in general, it is impossible to obtain low online estimation error through memoryless oracle-efficient algorithms. In this section, we revisit memoryless algorithms for the Markov decision processes a particular type of class $(\\mathcal{M},\\Pi,\\mathcal{O})$ (or equivalently, $(\\bar{\\mathcal{X}},\\mathcal{Y},\\mathcal{Z},\\mathcal{K},\\mathcal{F}))$ . We prove that for any class of Markov decision processes for which a structural parameter called coverability [69] is small, any offline estimator can be directly converted into an online estimator. ", "page_idx": 24}, {"type": "text", "text": "Markov decision processes. We consider classes $(\\mathcal{M},\\Pi,\\mathcal{O})$ that correspond to an episodic finite-horizon reinforcement learning setting, following Foster et al. [27]. With $H\\in\\mathbb{N}$ denoting the horizon, each model $M\\in\\mathcal{M}$ specifies a non-stationary Markov decision process as a tuple $M\\ =\\ \\left\\{\\{S_{h}\\}_{h=1}^{H},\\mathcal{A},\\{P_{h}^{M}\\}_{h=1}^{H},\\{\\hat{R_{h}^{M}}\\}_{h=1}^{H},d_{1}\\right\\}$ , where $\\ensuremath{\\mathcal{S}}_{h}$ is the state space for layer $h,\\,A$ is the action space, $P_{h}^{\\scriptscriptstyle M}\\;:\\;S_{h}\\,\\times\\,A\\;\\rightarrow\\;\\Delta(S_{h+1})$ is the probability transition kernel for layer $R_{h}^{\\scriptscriptstyle M}:S_{h}\\times\\mathcal{A}\\to\\bar{\\Delta(\\mathbb{R})}$ is the reward distribution for layer $h$ , and $d_{1}\\,\\in\\,\\Delta(S_{1})$ is the initial state distribution. We allow the reward distribution and transition kernel to vary across models in $\\mathcal{M}$ and assume that the initial state distribution is fixed. ", "page_idx": 24}, {"type": "text", "text": "We set $\\Pi~\\subset~\\Pi_{\\mathrm{RNS}}$ , which denotes the set of all randomized, non-stationary policies $\\pi\\ =$ $(\\pi_{1},\\dots,\\pi_{H})\\,\\in\\,\\Pi_{\\mathrm{RNS}}$ , where $\\pi_{h}\\,:\\,S_{h}\\,\\to\\,\\Delta(A)$ . For a fixed MDP $M\\,\\in\\,{\\mathcal{M}}$ and $\\pi\\,\\in\\,\\Pi$ , the observation $o\\sim M(\\pi)$ is a trajectory $(s_{1},a_{1},r_{1}),\\dots,(s_{H},a_{H},r_{H})$ that is generated through the following process, beginning from $s_{1}\\sim d_{1}$ . For $h=1,\\ldots,H$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\ a_{h}\\sim\\pi_{h}(s_{h}).}\\\\ &{\\bullet\\ r_{h}\\sim R_{h}^{M}(s_{h},a_{h})\\ \\mathrm{and}\\ s_{h+1}\\sim P_{h}^{M}(\\cdot\\mid s_{h},a_{h}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "So the obseravtion space $\\mathcal{O}=S_{1}\\times\\mathcal{A}\\times\\mathbb{R}\\times\\cdot\\cdot\\cdot\\times S_{H}\\times\\mathcal{A}\\times\\mathbb{R}$ . For notational convenience, we take $s_{H+1}$ to be a deterministic terminal state. We use $\\mathbb{P}^{M,\\pi}$ and $\\mathbb{E}^{M,\\pi}[\\cdot]$ to denote the probability law and expectation over trajectories induced by $M(\\pi)$ . In addition, we define $\\overline{{P}}_{h}^{M}(\\cdot\\mid s_{h},a_{h})$ as the conditional distribution on $s_{h+1},r_{h}$ given $s_{h},a_{h}$ under $M$ for $h\\in[H]$ . ", "page_idx": 24}, {"type": "text", "text": "The guarantees we provide apply to any loss that has a particular layer-wise structure tailored to reinforcement learning. ", "page_idx": 24}, {"type": "text", "text": "Definition E.1 (Layer-wise loss). For any sequence of losses $\\{\\mathsf{D}_{h}\\}_{h\\in[H]}$ bounded by [0, 1], where $\\mathsf{D}_{h}:\\Delta(S_{h}\\times\\mathbb{R})\\times\\Delta(S_{h}\\times\\mathbb{R})\\to[0,1]$ for all $h\\in[H]$ , we define the layer-wise loss $\\mathsf{D}^{\\mathsf{R L}}$ on $\\Delta(\\mathcal{O})$ as 14 ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathsf{D}^{\\mathsf{R L}}(M(\\pi)\\|M^{\\prime}(\\pi))=\\sum_{h=1}^{H}\\mathbb{E}^{M^{\\prime},\\pi}\\Bigl[\\mathsf{D}_{h}\\Bigl(\\overline{{P}}^{M^{\\prime}}(\\cdot\\mid s_{h},a_{h})\\Vert\\overline{{P}}^{M}(\\cdot\\mid s_{h},a_{h})\\Bigr)\\Bigr],\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for any pair of MDPs $M$ , $M^{\\prime}\\in\\mathcal{M}$ and policy $\\pi\\in\\Pi$ .15 ", "page_idx": 24}, {"type": "text", "text": "Examples of the layer-wise loss are scaled reverse KL-divergence (which is bounded by $[0,1]$ whenever the density ratios under consideration are upper and lower bounded with an appropriate scaling) [27] and the squared Bellman error [29]. Another useful example is the the sum of layer-wise squared Hellinger distances given by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathsf{D}_{\\mathsf{H}}^{\\mathsf{R L}}(M(\\pi)\\|M^{\\prime}(\\pi))=\\sum_{h=1}^{H}\\mathbb{E}^{M^{\\prime},\\pi}\\Bigl[\\mathsf{D}_{\\mathsf{H}}^{2}\\Bigl(\\overline{{P}}^{M}(\\cdot\\mid s_{h},a_{h}),\\overline{{P}}^{M^{\\prime}}(\\cdot\\mid s_{h},a_{h})\\Bigr)\\Bigr].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This loss coincides with the global squared Hellinger distance $\\mathsf{D}_{\\mathsf{H}}^{2}(M(\\pi),M^{\\prime}(\\pi))$ up to an $O(H)$ factor. ", "page_idx": 24}, {"type": "text", "text": "Coverability. We provide memoryless oracle-efficient algorithms for online estimation for any layer-wise loss $\\mathsf{D}^{\\mathsf{R L}}$ when the underlying MDP $M^{\\star}$ has bounded coverability [69]. ", "page_idx": 25}, {"type": "text", "text": "Definition E.2 (Coverability). For an MDP $M^{\\star}$ and a policy $\\pi_{.}$ , we define $\\begin{array}{r l}{d_{h}^{\\pi}(s,a)}&{{}\\equiv}\\end{array}$ $\\mathbb{E}^{M^{\\star},\\pi}[\\mathbb{1}(s_{h},a_{h}=s,a)]$ . The coverability coefficient $C_{\\mathsf{c o v}}$ for a policy class \u03a0 for the MDP $M^{\\star}$ is given by ", "page_idx": 25}, {"type": "equation", "text": "$$\nC_{\\mathsf{c o v}}(M^{\\star}):=\\operatorname*{inf}_{\\nu_{1},\\ldots,\\nu_{H}\\in\\Delta({\\mathcal{S}}\\times{\\mathcal{A}})}\\operatorname*{sup}_{\\pi\\in\\Pi,h\\in[H]}\\left\\|{\\frac{d_{h}^{\\pi}}{\\nu_{h}}}\\right\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "It is immediate to see that $C_{\\mathsf{c o v}}\\leq|\\Pi|$ , but in general it can be much smaller. Examples of MDP classes with low coverability include Block MDPs, Low-Rank MDPs, and exogenous block MDPs [69, 4]. ", "page_idx": 25}, {"type": "text", "text": "Offline-to-online conversion under coverability. Our main result shows that under coverability, the outputs of any offline estimation oracle $\\mathbf{Alg}_{0\\mathsf{f f}}$ satisfy an online estimation guarantee as-is. ", "page_idx": 25}, {"type": "text", "text": "Theorem E.1 (Offline-to-online conversion under coverability). For any layer-wise loss $\\mathsf{D}^{\\mathsf{R L}}$ and MDP class $(\\mathcal{M},\\Pi,\\mathcal{O})$ and $M^{\\star}\\in\\mathcal{M}$ , the sequence of estimators $(\\widehat{M}^{1},\\ldots,\\widehat{M}^{\\scriptscriptstyle T})$ produced by any offline estimation oracle $\\mathbf{Alg}_{0\\mathsf{f f}}$ for $\\mathsf{D}^{\\mathsf{R L}}$ with parameter $\\beta_{0\\mathsf{f f}}$ satisfy ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathsf{D}^{\\mathsf{R L}}\\Bigl(\\widehat{M}^{t}(\\pi^{t})\\|M^{\\star}(\\pi^{t})\\Bigr)\\leq O\\Bigl(\\sqrt{H C_{\\mathsf{c o v}}(M^{\\star})\\beta_{\\mathsf{O f f}}T\\log T}+H C_{\\mathsf{c o v}}(M^{\\star})\\Bigr).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This result is based on a variant of the proof technique in Theorem 1 of Xie et al. [69]. An important application of the result, which can be applied in tandem with the guarantees in Foster et al. [27], concerns squared Hellinger distance. ", "page_idx": 25}, {"type": "text", "text": "Corollary E.2. For any MDP class $(\\mathcal{M},\\Pi,\\mathcal{O})$ and $M^{\\star}~\\in~{\\cal M}$ , the sequence of estimators $(\\widehat{M}^{1},\\ldots,\\widehat{M}^{\\scriptscriptstyle T})$ produced by any offilne estimation oracle $\\mathbf{Alg}_{0\\mathsf{f f}}$ for squared Hellinger distance $\\mathsf{D}_{\\mathsf{H}}^{2}$ with para meter $\\beta_{\\mathsf{O f f}}$ satisfy ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbf{Est}_{\\mathrm{H}}^{\\mathrm{On}}(T)=\\sum_{t=1}^{T}\\mathsf{D}_{\\mathrm{H}}^{2}\\Big(\\widehat{M}^{t}(\\pi^{t}),M^{\\star}(\\pi^{t})\\Big)\\leq O\\Big(H\\sqrt{C_{\\mathrm{cov}}(M^{\\star})\\beta_{\\mathrm{Of}}T\\log T}+H^{2}C_{\\mathrm{cov}}(M^{\\star})\\Big)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This result follows by using that the layer-wise squared Hellinger distance in Eq. (21) is equivalent to $\\mathsf{D}_{\\mathsf{H}}^{2}(M(\\pi),M^{\\prime}(\\pi))$ up to $O(H)$ factors. ", "page_idx": 25}, {"type": "text", "text": "Application to interactive decision making. We apply Theorem E.1 to decision making via Algorithm 3. ", "page_idx": 25}, {"type": "text", "text": "Corollary E.3. Consider any layer-wise loss $\\mathsf{D}=\\mathsf{D}^{\\mathsf{R L}}$ and MDP class $(\\mathcal{M},\\Pi,\\mathcal{O})$ , and let $C_{\\mathsf{c o v}}:=$ $\\operatorname*{sup}_{M\\in\\mathcal{M}}C_{\\mathsf{c o v}}(M)$ . Algorithm $^3$ with exploration parameter $\\gamma>0$ and $\\mathbf{Alg}_{0\\mathsf{E O E}}$ chosen to be the identity map ensures that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{Reg}_{\\mathrm{DM}}]\\leq O(\\log T)\\cdot\\operatorname*{max}\\biggl\\{\\operatorname*{sup}_{\\mu\\in\\Delta(M)}{\\sf d e c}_{\\gamma}^{\\sf D}(\\mathcal{M},\\mu)\\cdot T,\\ \\gamma\\cdot\\Bigl(\\sqrt{H C_{\\mathrm{cov}}\\beta_{0\\#}T\\log T}+H C_{\\mathrm{cov}}\\Bigr)\\biggr\\},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for any offline estimation oracle $\\mathbf{Alg}_{0\\mathsf{f f}}$ with parameter $\\beta_{0\\mathsf{f f}}$ . ", "page_idx": 25}, {"type": "text", "text": "Contextual bandits and optimality of offline-to-online conversion. Another implication for Theorem E.1 concerns the special case of contextual bandits (that is, MDPs with horizon one). For the contextual bandit setting we abbreviate ${\\mathcal{S}}\\,=\\,{\\mathcal{S}}_{1}$ , and refer to $d_{1}\\,\\in\\,\\Delta(S)$ as the context distribution. We define $g^{M}(s,a)=\\mathbb{E}_{r\\sim R_{1}^{M}(s,a)}[r]$ as the expected reward function under a model $M$ , and following Foster and Rakhlin [25], use the squared error between mean reward functions as our divergence: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathsf{D}_{\\mathsf{C B}}(M(\\pi),M^{\\prime}(\\pi)):=\\mathbb{E}_{s\\sim d_{1},a\\sim\\pi(s)}\\bigl[\\mathsf{D}_{\\mathsf{s q}}\\bigl(g^{M}(s,a),g^{M^{\\prime}}(s,a)\\bigr)\\bigr].}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For this setting, the coverability coefficient $C_{\\mathsf{c o v}}$ is always bounded by the number of actions $|{\\mathcal{A}}|$ , which leads to the following corollary. ", "page_idx": 25}, {"type": "text", "text": "Corollary E.4. For any contextual bandit class $(\\mathcal{M},\\Pi,\\mathcal{O})$ and $M^{\\star}\\in\\mathcal{M}$ , the sequence of estimators $(\\widehat{M^{1}},\\ldots,\\widehat{M^{\\scriptscriptstyle T}})$ produced by any offilne estimation oracle $\\mathbf{Alg}_{0\\mathsf{f f}}$ for $\\mathsf{D}_{\\mathsf{C B}}$ with parameter $\\beta_{\\mathsf{O f f}}$ satisfy ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{Est}_{\\mathsf{D}}^{\\mathsf{O n}}(T)=\\sum_{t=1}^{T}\\mathsf{D}_{\\mathsf{C B}}\\Big(\\widehat{M}^{t}(\\pi^{t}),M^{\\star}(\\pi^{t})\\Big)\\le O\\big(\\sqrt{|A|T\\beta_{\\mathsf{O f f}}\\log T}+|A|\\big),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Recall that Foster and Rakhlin [25] show that any algorithm for online estimation with the divergence in Eq. (22) with $\\mathbf{Est}_{\\mathsf{D}}^{\\mathsf{O n}}(T)\\ \\leq\\ \\beta_{0\\mathsf{n}}$ can be lifted to a contextual bandit algorithm with regret $O\\big(\\sqrt{|A|T\\cdot\\beta_{0\\mathsf{n}}}\\big)$ via the inverse gap weighting strategy, even if contexts are chosen adversarially. Subsequent work of Simchi-Levi and $\\mathrm{Xu}$ [58] shows that for stochastic contexts, the inverse gap weighting strategy also yields regret $O\\big(\\sqrt{|A|T\\cdot\\beta_{0\\mathsf{f f}}}\\big)$ given access to an offilne oracle with parameter $\\beta_{0\\mathsf{f f}}$ . On the other hand, combining Corollary E.4 with the guarantee from Foster and Rakhlin [25] gives regret $O\\left(|A|^{1/4}T^{3/4}\\beta_{0\\mathsf{f f}}{}^{1/4}\\right)$ . This does not recover the result from Simchi-Levi and $\\mathrm{Xu}$ [58], but nonetheless gives an alternative proof that sublinear offline estimation error suffices for sublinear regret. ", "page_idx": 26}, {"type": "text", "text": "The guarantee in Theorem E.1 leads to a degradation in rate from $\\beta_{0\\mathsf{f f}}$ to $\\sqrt{T\\beta_{0\\mathsf{f f}}}$ (suppressing problem-dependent parameters). Our next result shows that this is tight in general. ", "page_idx": 26}, {"type": "text", "text": "Proposition E.2 (Tightness of offline-to-online conversion). For any integer $T\\geq1$ and $\\beta_{0\\mathsf{f}\\mathsf{f}}>0$ , there exists a contextual bandit class $(\\mathcal{M},\\Pi=\\mathcal{A}^{s},\\mathcal{O})$ with $|{\\mathcal{A}}|=2$ , a distribution $d_{1}\\,\\in\\,\\Delta(S)$ , a sequence $(\\pi^{1},\\cdot\\cdot\\cdot,\\pi^{T})$ and an offline oracle $\\mathbf{Alg}_{0\\uparrow\\uparrow}$ for $\\mathsf{D}_{\\mathsf{C B}}$ with parameter $\\beta_{\\mathsf{O f f}}$ such that the oracle\u2019s outputs $(\\widehat{M}^{1},\\ldots,\\widehat{M}^{\\scriptscriptstyle T})$ satisfy ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{Est}_{\\mathsf{D}}^{\\mathsf{O n}}(T)=\\sum_{t=1}^{T}\\mathsf{D}_{\\mathsf{C B}}\\Big(\\widehat{M}^{t}(\\pi^{t}),M^{\\star}(\\pi^{t})\\Big)\\geq\\Omega\\Big(\\sqrt{T\\beta_{\\mathsf{O f f}}}\\Big).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Part III Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "F Technical tools ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Lemma F.1. For any non-increasing sequence $x_{1}\\geq x_{2}\\geq\\,\\cdot\\,\\cdot\\,\\geq x_{T+1}\\geq1,$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}{\\frac{x_{t}-x_{t+1}}{x_{t}}}\\leq\\log(x_{1}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof of Lemma F.1. Since $\\log(1+a)\\leq a$ for all $a>-1$ , for any $t\\in[T]$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n-\\log(x_{t}/x_{t+1})=\\log\\left(1+\\left({\\frac{x_{t+1}}{x_{t}}}-1\\right)\\right)\\leq{\\biggl(}{\\frac{x_{t+1}}{x_{t}}}-1{\\biggr)}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Summing up over $t\\in[T]$ , we obtain ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\frac{x_{t}-x_{t+1}}{x_{t}}=\\sum_{t=1}^{T}1-\\frac{x_{t+1}}{x_{t}}\\leq\\sum_{t=1}^{T}\\log(x_{t}/x_{t+1})=\\log(x_{1}/x_{T+1})\\leq\\log(x_{1}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The following lemma gives an improvement to Lemma A.13 of Foster et al. [27] that removes a logarithmic factor. This shows that up to an absolute constant, squared Hellinger distance obeys a one-sided version of the chain rule for KL divergence. ", "page_idx": 27}, {"type": "text", "text": "Lemma F.2 (Subadditivity for squared Hellinger distance). Let $(\\mathcal{X}_{1},\\mathcal{F}_{1}),\\ldots,(\\mathcal{X}_{n},\\mathcal{F}_{n})$ be a sequence of measurable spaces, and let $\\textstyle\\mathcal{X}^{i}=\\prod_{i=t}^{i}\\mathcal{X}_{t}$ and $\\mathcal{F}^{i}=\\otimes_{t=1}^{i}\\mathcal{F}_{t}$ . For each $i,$ , let $\\mathbb{P}^{i}(\\cdot\\mid\\cdot)$ and $\\mathbb{Q}^{i}(\\cdot\\mid\\cdot)$ be probability kernels from $(\\mathcal{X}^{i-1},\\mathcal{F}^{i-1})$ to $(\\mathcal{X}_{i},\\mathcal{F}_{i})$ . Let $\\mathbb{P}$ and $\\mathbb{Q}$ be the laws of $X_{1},\\ldots,X_{n}$ under $X_{i}\\sim\\mathbb{P}^{i}(\\cdot\\mid X_{1:i-1}\\')$ and $X_{i}\\sim\\mathbb{Q}^{i}(\\cdot\\mid X_{1:i-1})$ respectively. Then it holds that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathsf{D}_{\\mathsf{H}}^{2}(\\mathbb{P},\\mathbb{Q})\\leq7\\cdot\\mathbb{E}_{\\mathbb{P}}\\Bigg[\\sum_{i=1}^{n}\\mathsf{D}_{\\mathsf{H}}^{2}\\big(\\mathbb{P}^{i}(\\cdot\\mid X_{1:i-1}),\\mathbb{Q}^{i}(\\cdot\\mid X_{1:i-1})\\big)\\Bigg].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof of Lemma F.2. We appeal to the cut-and-paste property of [34], defining a collection of distributions indexed by a hypercube $\\{0,1\\}^{n}$ with the property that the vertices $(0,\\ldots,0)$ and $(1,\\ldots,1)$ correspond to the distribution $\\mathbb{P}$ and $\\mathbb{Q}$ . Concretely, for any vertex $v\\,\\in\\,\\{0,1\\}^{n}$ of the hypercube, we define a probability distribution ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathfrak{P}_{v}:=\\prod_{i\\in[n]}R_{v_{i}}(\\cdot\\mid X_{1:i-1}),\\qquad\\mathrm{where~}R_{v_{i}}(\\cdot\\mid X_{1:i-1})=\\left\\{\\begin{array}{l l}{\\mathbb{P}^{i}(\\cdot\\mid X_{1:i-1})}&{\\mathrm{if~}v_{i}=0,}\\\\ {\\mathbb{Q}^{i}(\\cdot\\mid X_{1:i-1})}&{\\mathrm{if~}v_{i}=1.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Observe that $\\mathfrak{P}_{(0,\\ldots,0)}=\\mathbb{P}$ and $\\mathfrak{P}_{(1,...,1)}=\\mathbb{Q}$ . Now, consider any four vertices $a,b,c,d\\in\\{0,1\\}^{n}$ with the property that $\\{a_{i},b_{i}\\}=\\{c_{i},d_{i}\\}$ for each $i\\in[n]$ (with $\\{\\cdot\\}$ interpreted as a multi-set). Then for any measure $\\begin{array}{r}{\\nu:=\\dot{\\prod_{i=1}^{n}\\nu_{i}}\\big(\\cdot\\big|\\dot{X}_{1:i-1}\\big)}\\end{array}$ , where $\\nu_{i}(\\cdot|X_{1:i-1})$ is any common dominating conditional measure16 for $\\mathbb{P}^{i}\\left(\\cdot|X_{1:i-1}\\right)$ and $\\mathbb{Q}^{i}(\\cdot|X_{1:i-1})$ , by the definition of squared Hellinger distance we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathsf{D}_{\\mathsf{H}}^{2}(\\mathfrak{P}_{a},\\mathfrak{P}_{b})=1-\\int\\sqrt{\\prod_{i=1}^{n}\\frac{d R_{a_{i}}\\left(\\cdot\\mid X_{1:i-1}\\right)}{d\\nu_{i}}\\frac{d R_{b_{i}}\\left(\\cdot\\mid X_{1:i-1}\\right)}{d\\nu_{i}}}d\\nu}\\\\ &{=1-\\int\\sqrt{\\prod_{i=1}^{n}\\frac{d R_{c_{i}}\\left(\\cdot\\mid X_{1:i-1}\\right)}{d\\nu_{i}}\\frac{d R_{d_{i}}\\left(\\cdot\\mid X_{1:i-1}\\right)}{d\\nu_{i}}}d\\nu}\\\\ &{=\\mathsf{D}_{\\mathsf{H}}^{2}(\\mathfrak{P}_{c},\\mathfrak{P}_{d}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Let $k$ be the maximum integer such that $2^{k}\\leq n$ . Then since Eq. (23) holds, by Theorem 7 of Jayram [34] applied with the pairwise disjoint collection $A_{j}\\,=\\,\\{i\\leq n\\mid i$ mod $2^{\\tilde{k}}=j\\}$ for all $j\\in[2^{k}]$ where $|A_{j}|\\leq2$ for all $j$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{D}_{\\mathsf{H}}^{2}(\\mathbb{P},\\mathbb{Q})\\cdot\\displaystyle\\prod_{i=1}^{n}(1-1/2^{i})\\leq\\displaystyle\\sum_{j=1}^{2^{k}}\\mathsf{D}_{\\mathsf{H}}^{2}\\left(\\mathbb{P},\\displaystyle\\prod_{l\\in A_{j}}\\mathbb{Q}^{\\iota}(\\cdot\\mid X_{1:l-1})\\prod_{l^{\\prime}\\notin A_{j}}\\mathbb{P}^{\\iota^{\\prime}}(\\cdot\\mid X_{1:l^{\\prime}-1})\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq2\\,\\mathbb{E}_{\\mathbb{P}}\\left[\\displaystyle\\sum_{i=1}^{n}\\mathsf{D}_{\\mathsf{H}}^{2}(\\mathbb{P}^{i}(\\cdot\\mid X_{1:i-1}),\\mathbb{Q}^{i}(\\cdot\\mid X_{1:i-1}))\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "To conclude, we note that $\\textstyle\\prod_{i=1}^{n}(1-1/2^{i})>2/7$ . ", "page_idx": 28}, {"type": "text", "text": "G Proofs from Section 3 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "G.1 Proofs from Section 3.1 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Theorem 3.1 (Main upper bound for OEOE). For any instance $(\\mathcal{X},\\mathcal{Y},\\mathcal{Z},\\mathcal{K},\\mathcal{F})$ , any metric-like loss D, and any offline estimator $\\mathbf{Alg}_{0\\mathsf{f f}}$ with parameter $\\beta_{0\\mathsf{f}\\mathsf{f}}\\geq0$ , Algorithm $^{\\,l}$ is oracle-efficient and achieves ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbf{Est}_{\\mathsf{D}}^{\\mathsf{O n}}(T)\\leq O(C_{\\mathsf{D}}\\cdot(\\beta_{\\mathsf{O f f}}+1)\\cdot\\operatorname*{min}\\big\\{\\log|\\mathcal{F}|,|\\mathcal{X}|\\log T\\big\\}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof of Theorem 3.1. Our main technical result is the following lemma, which is proven in the sequel. ", "page_idx": 28}, {"type": "text", "text": "Lemma G.1. Consider any instance $(\\mathcal{X},\\mathcal{Y},\\mathcal{Z},\\mathcal{K},\\mathcal{F})$ and a metric-like loss17 D on $\\mathcal{Z}$ . Let $f^{\\star}\\in{\\mathcal{F}}$ be the target parameter, and consider a sequence of sets ${\\mathcal{F}}={\\mathcal{F}}_{1}\\supseteq{\\mathcal{F}}_{2}\\supseteq\\cdots\\supseteq{\\mathcal{F}}_{T}\\supseteq\\{f^{\\star}\\}$ and sequence of covariates $x^{1},\\ldots,x^{T}\\in\\mathcal{X}$ with the property that for all $t\\in[T]$ , all $f\\in\\mathcal{F}_{t}$ satisfy the following offline estimation guarantee: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t-1}\\mathsf{D}(f(x^{s}),f^{\\star}(x^{s}))\\leq\\beta_{0\\mathsf{f f}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then, by defining $\\mu^{t}=\\operatorname{Unif}(\\mathcal{F}_{t})$ , we have that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}_{f\\sim\\mu^{t}}[\\mathsf{D}(f(x^{t}),f^{\\star}(x^{t}))]\\leq O((\\beta_{\\mathsf{O f f}}+1)\\cdot\\operatorname*{min}\\{\\log|\\mathcal{F}|,|\\mathcal{X}|\\log T\\}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "To invoke Lemma G.1, we observe that the version space construction in Algorithm 1 ensures that for all $t\\in[T]$ , all $f\\in\\mathcal{F}_{t}$ satisfy ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t-1}\\mathsf{D}(f(x^{s}),f^{\\star}(x^{s}))\\leq\\sum_{s=1}^{t-1}C_{\\mathsf{D}}\\Big(\\mathsf{D}\\Big(f\\big(f(x^{s}),\\widehat{f}^{t}(x^{s})\\Big)\\Big)+\\mathsf{D}\\Big(\\widehat{f}^{t}(x^{s}),f^{\\star}(x^{s})\\Big)\\leq2C_{\\mathsf{D}}\\beta_{\\mathsf{O f f}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "In addition, it is immediate to see that $\\mathcal{F}=\\mathcal{F}_{1}\\supseteq\\mathcal{F}_{2}\\supseteq\\mathcal{\\cdot}\\cdot\\mathcal{\\cdot}\\supseteq\\mathcal{F}_{T}$ . Thus, by invoking Lemma G.1 with parameter $\\beta_{0\\mathsf{f}\\mathsf{f}}^{\\prime}=2C_{\\mathsf{D}}\\beta_{0\\mathsf{f}\\mathsf{f}}$ , we have that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbf{Est}_{0}^{0_{0}}(T)=\\sum_{t=1}^{T}\\mathbb{E}_{f\\sim\\mu^{t}}[\\mathsf{D}(f(x^{t}),f^{\\star}(x^{t}))]\\leq O((C_{\\mathsf{D}}\\beta_{0\\mathsf{f f}}+1)\\cdot\\operatorname*{min}\\{\\log|\\mathcal{F}|,|\\mathcal{X}|\\log T\\}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "To simplify, we note that $(C_{\\mathsf{D}}\\beta_{\\mathsf{O f f}}+1)\\le C_{\\mathsf{D}}(\\beta_{\\mathsf{O f f}}+1)$ , since $C_{\\mathrm{D}}\\geq1$ . ", "page_idx": 28}, {"type": "text", "text": "Proof of Lemma G.1. We begin by proving that $\\mathbf{Est}_{\\mathsf{D}}^{\\mathsf{O n}}(T)\\leq O(C_{\\mathsf{D}}\\cdot(\\beta_{\\mathsf{O f f}}+1)\\cdot\\log|\\mathcal{F}|)$ . Let us adopt the convention that $\\mathcal{F}_{T+1}=\\{f^{\\star}\\}$ , so that $\\mathcal{F}_{T+1}\\subseteq\\mathcal{F}_{T}\\subseteq\\cdot\\cdot\\cdot\\subseteq\\mathcal{F}_{1}$ . For any parameter $f\\in\\bar{\\mathcal{F}}\\backslash\\{f^{\\star}\\}$ , define $t_{f}:=\\operatorname*{min}\\left\\lbrace t:f\\notin\\mathcal{F}_{t+1}\\right\\rbrace$ . It is immediate to see that for all $f\\dot{\\in\\mathcal{F}}\\backslash\\{f^{\\star}\\}$ , $1\\leq t_{f}\\leq T$ and for all $t\\in[T]$ $\\left.\\right\\vert],\\left\\vert\\left\\{f:t_{f}=t\\right\\}\\right\\vert=\\left\\vert\\mathcal{F}_{t}\\right\\vert\\mathcal{F}_{t+1}\\right\\vert$ . Using that $\\mathsf{D}(f^{\\star}(x),f^{\\star}(x))=0$ for all $x\\in\\mathscr{X}$ , we re-write the online estimation error as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sum_{=1}^{T}\\mathbb{E}_{f\\sim\\mu^{t}}\\big[\\mathsf{D}(f(x^{t}),f^{\\star}(x^{t}))\\big]=\\sum_{t=1}^{T}\\frac{1}{|\\mathcal{F}_{t}|}\\sum_{f\\in\\mathcal{F}_{t}}\\mathsf{D}(f(x^{t}),f^{\\star}(x^{t}))=\\sum_{t=1}^{T}\\frac{1}{|\\mathcal{F}_{t}|}\\sum_{f\\in\\mathcal{F}_{t},f\\not=f^{\\star}}\\mathsf{D}(f(x^{t}),f^{\\star}(x^{t}))\\,\\mathsf{D}(f(x^{t}))\\,\\mathsf{E}(f(x^{t}),f^{\\star}(x^{t}))\\,\\mathsf{D}(f(x^{t}))\\,\\mathsf{E}(f(x^{t}))\\,\\mathsf{D}(f(x^{t}))\\,\\mathsf{E}(f(x^{t}))\\,\\mathsf{D}(f(x^{t}))\\,\\mathsf{E}(f(x^{t}))\\,\\mathsf{D}(f(x^{t}))\\,\\mathsf{E}(f(x^{t}))\\,\\mathsf{D}(f(x^{t}))\\,\\mathsf{E}(f(x^{t}))\\,\\mathsf{D}(f(x^{t}))\\,\\mathsf{E}(f(x^{t}))=\\sum_{t=1}^{T}\\sum_{f:t_{f}=t}^{T}\\sum_{s\\in1}\\frac{1}{|\\mathcal{F}_{s}|}\\mathsf{D}(f(x^{s}),f^{\\star}(x^{t}))\\,\\mathsf{D}(f(x^{t}))\\,\\mathsf{D}(f(x^{s}))\\,\\mathsf{E}(f(x^{t}))\\,\\mathsf{D}(f(x^{t}))\\,\\mathsf{E}(f(x^{s}))\\,\\mathsf{D}(f(x^{t}))\\,\\mathsf{E}(f(x^{t}))\\,\\mathsf{D}(f(x^{s}))\\,\\mathsf{E}(f(x^{t}))\\,}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Using that $\\mathcal{F}_{T}\\subseteq\\mathcal{F}_{T-1}\\subseteq\\cdots\\subseteq\\mathcal{F}_{1}$ , we can upper bound this quantity by ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\sum_{f:t_{f}=t}\\sum_{s\\leq t}{\\frac{1}{|{\\mathcal{F}}_{s}|}}{\\mathsf{D}}(f(x^{s}),f^{\\star}(x^{s}))\\leq\\sum_{t=1}^{T}\\sum_{f:t_{f}=t}{\\frac{1}{|{\\mathcal{F}}_{t}|}}\\sum_{s\\leq t}{\\mathsf{D}}(f(x^{s}),f^{\\star}(x^{s})).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "To proceed, observe that for any function $f\\in\\mathcal F$ , if $t_{f}=t$ , then $f\\in\\mathcal{F}_{t}$ . It follows from the assumed bound in Eq. (24) that if $t_{f}=t$ , then ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{s\\leq t}\\mathsf{D}(f(x^{s}),f^{\\star}(x^{s}))=\\mathsf{D}(f(x^{t}),f^{\\star}(x^{t}))+\\displaystyle\\sum_{s\\leq t-1}\\mathsf{D}(f(x^{s}),f^{\\star}(x^{s}))}\\\\ &{\\leq1+\\beta_{\\sf O f f},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where we have used the fact that the loss $\\mathsf{D}$ is bounded by 1. Using this fact, and recalling that for all $t\\in[T],|\\{f:t_{f}=t\\}|=|\\mathcal{F}_{t}\\setminus\\mathcal{F}_{t+1}|$ , we bound ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\sum_{f:t_{f}=t}\\frac{1}{|\\mathcal{F}_{t}|}\\sum_{s\\leq t}\\mathsf{D}(f(x^{s}),f^{\\star}(x^{s}))\\leq(\\beta_{0\\mathrm{f}}+1)\\sum_{t=1}^{T}\\sum_{f:t_{f}=t}\\frac{1}{|\\mathcal{F}_{t}|}\\leq(\\beta_{0\\mathrm{f}}+1)\\sum_{t=1}^{T}\\frac{|\\mathcal{F}_{t}\\setminus\\mathcal{F}_{t+1}|}{|\\mathcal{F}_{t}|}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Finally, by Lemma F.1, we have that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\frac{|\\mathcal{F}_{t}\\setminus\\mathcal{F}_{t+1}|}{|\\mathcal{F}_{t}|}=\\sum_{t=1}^{T}\\frac{|\\mathcal{F}_{t}|-|\\mathcal{F}_{t+1}|}{|\\mathcal{F}_{t}|}\\le\\log|\\mathcal{F}_{1}|=\\log|\\mathcal{F}|.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We conclude that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}_{f\\sim\\mu^{t}}[\\mathsf{D}(f(x^{t}),f^{\\star}(x^{t}))]\\leq(\\beta_{\\mathsf{O f f}}+1)\\log|\\mathcal{F}|.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We now prove the bound $\\mathbf{Est}_{\\mathsf{D}}^{\\mathsf{O n}}(T)\\,\\leq\\,O(C_{\\mathsf{D}}\\cdot(\\beta_{\\mathsf{O f f}}+1)\\cdot|\\mathcal{X}|\\log T)$ . For each $x\\,\\in\\,{\\mathcal{X}}$ , define $N_{t-1}(x)=\\sum_{s=1}^{t-1}\\mathbb{1}(x^{s}=x)$ . Then we can write the online estimation error as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}_{f\\sim\\mu^{t}}[\\mathsf{D}(f(x^{t}),f^{\\star}(x^{t}))]=\\sum_{t=1}^{T}\\sum_{x\\in\\mathcal{X}}\\frac{\\mathbb{1}(x=x^{t})}{N_{t-1}(x)\\vee1}\\cdot\\mathbb{E}_{f\\sim\\mu^{t}}[(N_{t-1}(x)\\vee1)\\mathsf{D}(f(x),f^{\\star}(x))].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "From the definition of $\\mu^{t}$ , we have that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}_{f\\sim\\mu^{t}}[(N_{t-1}(x)\\vee1)\\mathsf{D}(f(x),f^{\\star}(x))]=\\frac{1}{|\\mathcal{F}_{t}|}\\sum_{f\\in\\mathcal{F}_{t}}(N_{t-1}(x)\\vee1)\\mathsf{D}(f(x),f^{\\star}(x))}}\\\\ &{}&{\\leq\\frac{1}{|\\mathcal{F}_{t}|}\\sum_{f\\in\\mathcal{F}_{t}}(N_{t-1}(x)+1)\\mathsf{D}(f(x),f^{\\star}(x)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Now, from the offline guarantee assumed in Eq. (24), we have for all $x\\in\\mathscr{X}$ and $f\\in\\mathcal{F}_{t}$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n(N_{t-1}(x)+1)\\mathsf{D}(f(x),f^{\\star}(x))\\leq\\sum_{s\\leq t-1}\\mathsf{D}(f(x^{s}),f^{\\star}(x^{s}))+\\mathsf{D}(f(x),f^{\\star}(x))\\leq\\beta_{0\\mathsf{f}}+1.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Combining these observations, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}_{f\\sim\\mu^{t}}[\\mathsf{D}(f(x^{t}),f^{\\star}(x^{t}))]\\leq(\\beta_{0\\mathsf{f}\\mathsf{f}}+1)\\sum_{t=1}^{T}\\sum_{x\\in\\mathcal{X}}\\frac{\\mathbb{1}(x=x^{t})}{N_{t-1}(x)\\vee1}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Now, for each $x\\in\\mathscr{X}$ , define $t_{x}=\\operatorname*{min}\\left\\{t\\leq T:x^{t}=x\\right\\}$ if this set is not empty, and set $t_{x}=T$ otherwise. From this definition and the fact that $1+1/2+\\cdot\\cdot+1/T\\leq1+\\log T$ , we have that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{x\\in\\mathcal{X}}\\sum_{t=1}^{T}\\frac{\\mathbb{1}(x=x^{t})}{N_{t-1}(x)\\vee1}=\\displaystyle\\sum_{x\\in\\mathcal{X}}\\left(\\sum_{t=1}^{t_{x}}\\frac{\\mathbb{1}(x=x^{t})}{N_{t-1}(x)\\vee1}+\\sum_{t=t_{x}+1}^{T}\\frac{\\mathbb{1}(x=x^{t})}{N_{t-1}(x)\\vee1}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\sum_{x\\in\\mathcal{X}}\\left(1+\\sum_{i=1}^{N_{T-1}(x)}\\frac{1}{i}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq2|\\mathcal{X}|+|\\mathcal{X}|\\log T\\leq3|\\mathcal{X}|\\log T.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We conclude that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}_{f\\sim\\mu^{t}}[\\mathsf{D}(f(x^{t}),f^{\\star}(x^{t}))]\\leq3(\\beta_{\\mathsf{O f f}}+1)\\cdot|\\mathcal{X}|\\log T.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Theorem 3.2 (Main lower bound for OEOE). Consider the binary classification setting with $\\mathcal{Z}\\,=\\,\\mathcal{V}\\,=\\,\\{0,1\\}$ and loss $\\mathsf{D}_{0/1}(\\cdot,\\cdot)$ . For any $N\\;\\in\\;\\mathbb{N}$ and $\\beta_{0\\mathsf{f}\\mathsf{f}}\\;>\\;0,$ , there exists an instance $(\\mathcal{X},\\mathcal{Y},\\mathcal{Z},\\mathcal{K},\\mathcal{F})$ with $\\log|{\\mathcal F}|~=~|{\\boldsymbol\\chi}|~=~N$ such that for any oracle-efficient algorithm, there is a sequence of covariates $(x^{1},\\ldots,x^{T})$ and offline oracle with parameter \u03b2Off such that $\\mathbb{E}\\big[{\\bf E s t}_{\\mathsf{D}}^{\\mathsf{O n}}(T)\\big]\\ge\\Omega(\\operatorname*{min}\\left\\{(\\beta_{\\mathsf{O f f}}+1)N,T\\right\\})$ . ", "page_idx": 30}, {"type": "text", "text": "Proof of Theorem 3.2. Let $N\\geq1$ be given, a consider the model class where $\\mathcal{X}=\\{x_{1},\\ldots,x_{N}\\}$ is an arbitrary discrete set, $\\mathcal{Z}=\\mathcal{Y}=\\{0,1\\}$ , $\\mathcal{F}={\\{0,1\\}}^{\\mathcal{X}}$ , $\\mathsf{D}(z_{1},z_{2})=\\mathsf{D}_{0/1}(z_{1},z_{2})=\\mathbb{1}\\{z_{1}\\neq z_{2}\\}$ , and $\\boldsymbol{\\mathscr{K}}(z)=\\mathbb{1}_{z}$ . ", "page_idx": 30}, {"type": "text", "text": "We first specify the offilne estimation oracle, then specify an adversarially chosen covariate sequence. Fix $T\\ \\in\\ \\mathbb N$ , and for any $1\\;\\leq\\;t\\;\\leq\\;T$ and sequence of covariates $x^{1},\\ldots,x^{t}$ define $\\bar{N_{t}}\\bar{(\\boldsymbol{x})}:=$ $\\textstyle\\sum_{s=1}^{t}\\mathbb{1}(x^{s}=x)$ . For any target parameter $f^{\\star}\\in{\\mathcal{F}}$ and offli ne estimation parameter $\\beta_{0\\mathsf{f}\\mathsf{f}}>0$ , we consider the oracle $\\mathbf{Alg}_{0\\mathsf{f f}}^{t}(\\cdot;\\bar{f}^{\\star})$ for the sequence $x^{1},\\ldots,x^{t}$ that returns ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\widehat{f}^{\\,t}(x)=\\left\\{\\!\\!\\begin{array}{l l}{0}&{\\mathrm{if}\\;N_{t-1}(x)<\\beta_{\\sf O f f},}\\\\ {f^{\\star}(x)}&{\\mathrm{otherwise}.}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "To complete the construction, we consider sequence $(x^{1},\\ldots,x^{T})$ in which ", "page_idx": 30}, {"type": "equation", "text": "$$\nx^{t}=x_{\\operatorname*{min}\\left\\{\\lceil t/\\lceil\\beta_{\\mathsf{O f f}}\\rceil\\rceil,N\\right\\}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Equivalently, and perhaps more intuitively, we set ", "page_idx": 30}, {"type": "equation", "text": "$$\n(x^{1},\\allowbreak\\cdot\\cdot,x^{T})=(\\underbrace{x_{1},\\allowbreak\\cdot\\cdot,x_{1}}_{\\lceil\\beta_{\\mathrm{off}}\\rceil},\\underbrace{x_{2},\\allowbreak\\cdot\\cdot,x_{2}}_{\\lceil\\beta_{\\mathrm{off}}\\rceil},\\allowbreak\\cdot\\cdot,\\underbrace{x_{N},\\allowbreak\\cdot\\cdot,x_{N}}_{\\lceil\\beta_{\\mathrm{off}}\\rceil},x_{N},x_{N},\\allowbreak\\cdot\\cdot),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "stopping earlier if $T\\leq N\\lceil\\beta_{0\\mathsf{f f}}\\rceil$ . ", "page_idx": 30}, {"type": "text", "text": "For any $f^{\\star}$ , we now show that $\\mathbf{Alg}_{\\mathsf{O f f}}^{t}(\\cdot;f^{\\star})$ is an offilne oracle with parameter $\\beta_{0\\mathsf{f f}}$ on the sequence $x^{1},\\ldots,x^{T}$ . This is because for any $i<\\operatorname*{min}{\\{\\lceil t/\\lceil\\beta_{0\\mathsf{f f}}\\rceil\\rceil,N\\}}$ , the covariate $x_{i}$ is repeated $\\lceil\\bar{\\beta_{0\\mathsf{f f}}}\\rceil\\geq$ $\\beta_{0\\mathsf{f f}}$ times. Thus ${\\widehat{f}}^{t}(x_{i})=f^{\\star}(x_{i})$ . This implies for $t\\leq N\\cdot\\lceil\\beta_{0\\mathsf{f f}}\\rceil$ that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t-1}\\mathsf{D}_{0/1}\\Big(\\widehat{f}^{t}(x^{s}),f^{\\star}(x^{s})\\Big)=\\sum_{s=t-\\lceil t/[\\beta_{0\\oplus t}]\\rceil\\cdot\\lceil\\beta_{0\\oplus t}\\rceil}^{t-1}\\mathsf{D}_{0/1}\\Big(\\widehat{f}^{t}(x^{s}),f^{\\star}(x^{s})\\Big)\\leq\\lceil\\beta_{0\\oplus t}\\rceil-1\\leq\\beta_{0\\oplus t}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "If $t\\,>\\,N\\cdot\\,\\lceil\\beta_{0\\mathsf{f f}}\\rceil$ , then all the covariates are repeated for more than $\\lceil\\beta_{0\\mathsf{f f}}\\rceil$ times, thus ${\\widehat{f}}^{t}\\,=\\,f^{\\star}$ .   \nOverall, we have shown that $\\mathbf{Alg}_{\\mathsf{O f f}}^{t}(\\cdot;f^{\\star})$ is an offline oracle with parameter $\\beta_{0\\mathsf{f f}}$ . ", "page_idx": 31}, {"type": "text", "text": "Now, fix any oracle-efficient online estimation algorithm, and consider the expected regret under $f^{\\star}\\sim\\operatorname{Unif}({\\mathcal F})$ (with $\\mathbf{Alg}_{\\mathsf{O f f}}^{t}(\\cdot;f^{\\star})$ as the oracle). If $T\\,\\geq\\,N\\lceil\\beta_{0\\mathsf{f f}}\\rceil$ , then regardless of how the algorithm chooses $\\mu^{t}$ , since for any block of $x_{i}$ , $f^{\\star}(x_{i})$ is independent of $\\mu^{t}(x_{i})$ , the expected regret is lower bounded as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{f^{\\star}\\sim\\mathrm{Unif}(\\mathcal{F})}\\mathbb{E}_{\\bar{f}\\sim\\mu^{t}}\\big[\\mathsf{D}_{0/1}\\big(\\bar{f}(x^{t}),f^{\\star}(x^{t})\\big)\\big]}\\\\ &{\\displaystyle\\ge\\sum_{i=1}^{N}\\sum_{j=1}^{\\lceil\\beta_{0\\#}\\rceil}\\mathbb{E}_{f^{\\star}\\sim\\mathrm{Unif}(\\mathcal{F})}\\,\\mathbb{E}_{\\bar{f}\\sim\\mu^{i}\\lceil\\beta_{0\\#}\\rceil+j}\\left[\\mathsf{D}_{0/1}\\big(\\bar{f}(x_{i}),f^{\\star}(x_{i})\\big)\\right]}\\\\ &{\\displaystyle=\\frac{1}{2}N\\lceil\\beta_{0\\#}\\rceil\\geq\\Omega(N(\\beta_{0\\#}+1))=\\Omega(\\mathrm{min}\\,\\{(\\beta_{0\\#}+1)\\log|\\mathcal{F}|,(\\beta_{0\\#}+1)|\\mathcal{X}|\\}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "G.2 Proofs from Section 3.2 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Theorem 3.3 (Impossibility of memoryless algorithms for OEOE). Consider the binary classification setting with $\\mathcal{Z}=\\mathcal{Y}=\\{0,1\\}$ and loss $\\bar{\\mathsf{D}}_{0/1}\\bar{(\\cdot,\\cdot)}$ . For any $N\\in\\mathbb N$ and $\\beta_{0\\mathsf{f}\\mathsf{f}}\\,\\geq\\,0,$ , there exists an instance $(\\mathcal{X},\\mathcal{Y},\\mathcal{Z},\\mathcal{K},\\mathcal{F})$ with $|{\\mathcal{F}}|=|{\\mathcal{X}}|=N$ such that for any memoryless oracle-efficient algorithm, there exists a sequence of covariates $(x^{1},\\ldots,x^{T})$ and a (potentially improper) offline oracle $\\mathbf{Alg}_{0\\mathsf{f f}}$ with parameter $\\beta_{0\\mathsf{f f}}$ such that $\\mathbb{E}\\big[{\\bf E s t}_{\\mathsf{D}}^{\\mathsf{O n}}(T)\\big]\\ge\\Omega\\big(\\operatorname*{min}\\{N(\\beta_{\\mathsf{O f f}}+1),T\\}\\big)$ . This conclusion still holds when the online estimation algorithm remembers ${\\widehat{f}}^{1},\\ldots,{\\widehat{f}}^{t-1}$ but not $\\boldsymbol{x}^{1},\\ldots,\\boldsymbol{x}^{t-1}$ . ", "page_idx": 31}, {"type": "text", "text": "Proof of Theorem 3.3. Given a parameter $\\beta_{0\\mathsf{f}\\mathsf{f}}\\,>\\,0$ and an integer $N$ , assume without loss of generality that $K:=\\,T/(\\lfloor\\beta_{\\sf O f f}\\rfloor+1)$ is an integer. Consider the instance $(\\mathcal{X},\\mathcal{Y},\\mathcal{Z},\\mathcal{K},\\mathcal{F})$ with $\\mathbf{\\bar{\\boldsymbol{\\mathcal{X}}}}=\\left[N\\right]$ , $\\mathcal{Z}=\\mathcal{Y}=\\{0,1\\}$ , $\\mathsf{D}=\\mathsf{D}_{0/1}$ , $\\bar{\\kappa}(z)=\\bar{\\mathbb{1}_{z}}$ , and parameter space $\\mathcal{F}=\\{f_{i}\\}_{i\\in[N]}$ is defined as ", "page_idx": 31}, {"type": "equation", "text": "$$\nf_{i}(x)=\\mathbb{1}\\{x=i\\}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We consider a sequence of covariates $(x^{1},\\ldots,x^{T})$ divided into $K$ blocks, each with length $\\lfloor\\beta_{0\\mathsf{f f}}\\rfloor+1$ . In each block, the covariates will be chosen to be the same, i.e., $x^{1}=\\cdot\\cdot\\cdot=x^{\\lfloor\\beta_{0\\mathrm{ff}}\\rfloor+1}$ , $x^{\\lfloor\\beta_{0\\mathrm{ff}}\\rfloor+2}=$ $\\cdot\\cdot\\cdot=x^{2\\lfloor\\beta_{0\\mathrm{ff}}\\rfloor+2}$ . We define $\\tau_{t}=\\lceil t/(\\lfloor\\beta_{0\\mathsf{f f}}\\rfloor+1)\\rceil$ as the index of the block the step $t$ belongs to, and we adopt the convention that $x_{\\tau}\\in\\mathcal{X}$ is value of the covariates for block $\\tau$ , i.e., $x^{t}=x_{\\tau_{t}}$ for all $t$ . We leave the precise choice for $x_{1},\\ldots,x_{K}$ as a free parameter for now. ", "page_idx": 31}, {"type": "text", "text": "Fix any memoryless oracle-efficient online estimation algorithm defined by a sequence of maps $\\{F^{t}\\}_{t\\in[T]}$ (cf. Definition 3.1). We set the true target parameter to be $f^{\\star}\\,=\\,f_{i^{\\star}}$ , where the index $i^{\\star}\\in[N]$ will be chosen later in the proof (in an adversarial fashion based on the algorithm under consideration); for now, we leave $i^{\\star}\\in[N]$ as a free parameter. ", "page_idx": 31}, {"type": "text", "text": "We first specify the offline estimation oracle under $f_{i^{\\star}}$ . For each block index $\\tau=1,\\ldots,K$ , define ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\chi^{\\tau}=\\chi\\setminus(\\{x^{s}\\}_{s\\leq(\\tau-1)(\\lfloor\\beta_{0\\mathsf{f f}}\\rfloor+1)}\\cup\\{i^{\\star}\\})\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "as the set of covariates in $\\mathcal{X}\\backslash\\{i^{\\star}\\}$ that have not been observed before block $\\tau$ , and let $\\overline{{\\mathcal{X}^{\\tau}}}:=\\mathcal{X}^{\\tau}\\cup\\{i^{\\star}\\}$ . We define ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{f}^{t}(x)=\\mathbb{1}\\{x\\in\\overline{{\\mathcal{X}}}^{\\tau_{t}}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "as the estimator returned by the oracle at round $t$ . It is immediate from this construction that regardless of how $x^{1},\\ldots,x^{T}$ are chosen, the offline estimation error is bounded by ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\langle t\\in[T],\\ \\sum_{s<t}\\mathsf{D}_{0/1}\\Big(f^{\\star}(x^{s}),\\widehat{f}^{t}(x^{s})\\Big)\\leq\\sum_{s=(\\tau_{t}-1)(\\lfloor\\beta_{0\\pi}\\rfloor+1)+1}^{t-1}\\mathsf{D}_{0/1}\\Big(f^{\\star}(x^{s}),\\widehat{f}^{t}(x^{s})\\Big)\\leq\\lfloor\\beta_{0\\up t}\\rfloor\\leq\\beta_{0\\up t},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "since the value of ${\\widehat{f}}^{t}$ differs from $f_{i^{\\star}}$ only for covariates that have not been observed before block $\\tau_{t}$ . ", "page_idx": 31}, {"type": "text", "text": "It remains to lower bound the algorithm\u2019s online estimation error. To start, note that $\\mathsf{D}_{0/1}$ coincides with $\\mathsf{D}_{\\mathsf{s q}}$ on the set $\\{0,1\\}$ and $\\mathsf{D}_{\\mathsf{s q}}$ is convex. Hence, by Jensen\u2019s inequality, it suffices to lower bound ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}(f^{t}(x^{t})-f_{i^{\\star}}(x^{t}))^{2},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $f^{t}(x^{t})$ is the mean under ${\\bar{f}}^{t}(x^{t})$ where $\\bar{f}^{t}\\sim\\mu^{t}=F^{t}(\\widehat{f^{1}},\\ldots,\\widehat{f}^{t-1},\\widehat{f^{t}}).$ . ", "page_idx": 32}, {"type": "text", "text": "To proceed, we specify the sequence $x^{1},\\ldots,x^{T}$ , choosing $x^{t}$ as a measurable function of $\\overline{{\\mathcal{X}^{t}}}$ and $i^{\\star}$ (recall that $i^{\\star}$ itself has yet to be chosen). Fix a round $t$ , and suppose that $\\mathcal{X}^{t}\\neq\\mathcal{Q}$ . We choose $x^{t}$ to lower bound the estimation error by considering two cases. In the process, we will also define a function ${\\widehat{j}}^{\\tau}:(2^{[N]})^{\\otimes\\tau}\\rightarrow[N]\\cup\\{\\bot\\}$ for each $\\tau\\in[K]$ , where $(2^{[N]})^{\\bar{\\otimes}\\tau}=2^{[N]}\\times\\cdot\\cdot\\cdot\\times2^{[N]}.$ . Let \u03c4 c o pies ", "page_idx": 32}, {"type": "text", "text": "$\\tau\\in[K]$ be fixed. ", "page_idx": 32}, {"type": "text", "text": "$\\begin{array}{r}{\\bullet\\ \\mathrm{If}\\ \\sum_{p=1}^{\\lfloor\\beta_{\\mathrm{Off}}\\rfloor+1}\\mathbb{1}\\big(f^{\\tau(\\lfloor\\beta_{\\mathrm{Off}}\\rfloor+1)+p}(x)\\le1/2\\big)\\ge\\frac{\\lfloor\\beta_{\\mathrm{Off}}\\rfloor+1}{2}}\\end{array}$ for all $x\\in{\\overline{{\\mathcal{X}}}}^{\\tau}$ , we set $x_{\\tau}=i^{\\star}$ (equivalently, $x^{\\tau(\\lfloor\\beta_{0\\mathrm{ff}}\\rfloor+1)+p}=i^{\\star}$ for $p=1,\\ldots,\\lfloor\\beta_{0\\mathsf{f f}}\\rfloor+1)$ , so that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{t=\\tau(\\lfloor\\beta_{0\\mathrm{ff}}\\rfloor+1)+1}^{(\\lfloor\\beta_{0\\mathrm{ff}}\\rfloor+1)}\\operatorname*{max}_{x^{t}}(f^{t}(x^{t})-f_{i^{\\star}}(x^{t}))^{2}\\ge\\frac{\\lfloor\\beta_{0\\mathrm{ff}}\\rfloor+1}{8}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "In this case, we define $\\widehat{j}^{\\tau}(\\overline{{\\mathcal{X}}}^{1},\\ldots,\\overline{{\\mathcal{X}}}^{\\tau})=\\perp$ . ", "page_idx": 32}, {"type": "text", "text": "\u2022 If there exists $j\\,\\in\\,{\\overline{{\\chi}}}^{\\tau}$ such that $\\sum_{n=1}^{\\lfloor\\beta_{\\mathsf{O f f}}\\rfloor+1}\\mathbb{1}\\big(f^{\\tau(\\lfloor\\beta_{\\mathsf{O f f}}\\rfloor+1)+p}(j)\\,\\le\\,1/2\\big)\\,<\\,\\frac{\\lfloor\\beta_{\\mathsf{O f f}}\\rfloor+1}{2}$ , we set p=1   \n$x_{\\tau}=j$ (equivalently, $x^{\\tau(\\lfloor\\beta_{0\\mathsf{f f}}\\rfloor+1)+p}=j$ for $p=1,\\ldots,\\lfloor\\beta_{0\\mathsf{f f}}\\rfloor+1)$ for the least such $j$ , so   \nthat ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{t=\\tau(\\lfloor\\beta\\mathrm{off}\\rfloor+1)+1}^{(\\tau+1)(\\lfloor\\beta_{\\mathrm{off}}\\rfloor+1)}\\operatorname*{max}_{x^{t}}(f^{t}(x^{t})-f_{i^{\\star}}(x^{t}))^{2}\\ge\\frac{\\lfloor\\beta_{\\mathrm{Off}}\\rfloor+1}{8}\\mathbb{1}\\{i^{\\star}\\ne j\\}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "In this case, we define $\\widehat{j}^{\\tau}(\\overline{{\\mathcal{X}}}^{1},\\ldots,\\overline{{\\mathcal{X}}}^{\\tau})=j$ as well. ", "page_idx": 32}, {"type": "text", "text": "Note that since $f^{t}$ is a measurable function of $\\overline{{\\mathcal{X}}}^{1},\\widehat{\\boldsymbol{j}}^{1},\\ldots,\\overline{{\\mathcal{X}}}^{\\tau_{t}},\\widehat{\\boldsymbol{j}}^{\\tau_{t}}$ is well-defined. ", "page_idx": 32}, {"type": "text", "text": "Combining these cases, it follows that for any choice of $i^{\\star}$ , choosing $x_{1},\\ldots,x_{K}$ in the fashion described above ensures that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}(f^{t}(x^{t})-f_{i^{\\star}}(x^{t}))^{2}\\geq\\frac{\\lfloor\\beta_{0\\mathrm{ff}}\\rfloor+1}{8}\\sum_{\\tau=1}^{K}\\mathbb{1}\\Bigl\\{\\widehat{j}^{\\tau}(\\overrightarrow{\\mathcal{X}}^{1},\\dots,\\overrightarrow{\\mathcal{X}}^{\\tau})\\neq i^{\\star}\\Bigr\\}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We now state and prove the following technical lemma, which asserts that there exists a choice of $i^{\\star}$ for which the right-hand side above is large. ", "page_idx": 32}, {"type": "text", "text": "Lemma G.2. For any algorithm, there exists a choice for $i^{\\star}\\in[N]$ such that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\Bigl\\{\\tau:\\widehat{j}^{\\tau}(\\overline{{\\boldsymbol{\\mathcal{X}}}}^{1},\\dots,\\overline{{\\boldsymbol{\\mathcal{X}}}}^{\\tau})=i^{\\star}\\Bigr\\}\\ge\\Omega(N).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof of Lemma G.2. Consider a more abstract process, which we claim captures the evolution of $\\overline{{\\mathcal{X}^{\\tau}}}$ . Let $i^{\\star}\\in[N]$ , and let $A^{1}=[N]$ . We consider a sequence of sets $\\{A^{\\tau}\\}_{\\tau\\geq1}$ evolving according to the following process, parameterized by a sequence of functions $\\left\\{g^{\\tau}:(2^{[N]})^{\\otimes\\tau}\\rightarrow[N]\\cup\\{\\bot\\}\\right\\}_{\\tau\\geq1}$ and index $i^{\\star}\\in[N]$ . ", "page_idx": 32}, {"type": "text", "text": "For $\\tau\\geq1$ : ", "page_idx": 32}, {"type": "text", "text": "\u2022 If $g^{\\tau}(A^{1},\\cdot\\cdot\\cdot,A^{\\tau})=\\bot$ , $A^{\\tau+1}\\leftarrow A^{\\tau}$ .   \n\u2022 If $g^{\\tau}(A^{1},\\cdot\\cdot\\cdot,A^{\\tau})\\neq\\perp$ , let $A^{\\tau+1}=A^{\\tau}\\setminus\\left\\{g^{\\tau}(A^{1},\\cdot\\cdot\\cdot,A^{\\tau})\\right\\}$ if $g^{\\tau}(A^{1},\\cdot\\cdot\\cdot,A^{\\tau})\\neq i^{\\star}$ , and let $A^{\\tau+1}\\leftarrow A^{\\tau}$ otherwise. ", "page_idx": 32}, {"type": "text", "text": "We claim that there exists $i^{\\star}\\in[N]$ such that $g^{s}(A^{1},\\cdot\\cdot\\cdot,A^{s})\\neq i^{\\star}$ for all $s<N$ under this process. To see this define a set $X^{\\tau}$ inductively: Starting from $X^{1}=[N]$ , set $X^{\\tau+1}\\gets X^{\\tau}\\setminus g^{\\tau}(X^{1},\\bar{\\dots},X^{\\tau})$ if $g^{\\tau}(X^{1},\\cdot\\cdot\\cdot,X^{\\tau})\\neq\\perp$ , and set $X^{\\tau+1}\\leftarrow X^{\\tau}$ otherwise; note that this process does not depend on the choice $i^{\\star}$ , since $g^{\\tau}$ itself does not depend on $i^{\\star}$ . ", "page_idx": 33}, {"type": "text", "text": "For any $\\tau$ , observe that for any $i\\in X^{\\tau}$ , if we set $i^{\\star}=i$ , then $g^{s}(A^{1},\\cdot\\cdot\\cdot,A^{s})\\neq i^{\\star}$ for all $s<\\tau$ , and so $A^{\\tau}=X^{\\tau}$ . It follows that as long as $X^{\\tau}\\neq\\varnothing$ , we can choose $i^{\\star}$ such that $g^{s}(A^{1},\\cdot\\cdot\\cdot,A^{s})\\neq i^{\\star}$ for all $s<\\tau$ . Since $X^{\\tau}$ shrinks by at most one element per iteration, it follows that this is possible for all $\\tau<N$ . ", "page_idx": 33}, {"type": "text", "text": "It follows immediately from Lemma G.2 that by choosing $i^{\\star}$ as guaranteed by the lemma, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{t=1}^{T}(f^{t}(x^{t})-f_{i^{\\star}}(x^{t}))^{2}\\geq\\frac{\\lfloor\\beta_{0\\mathsf{f f}}\\rfloor+1}{8}\\cdot\\Omega(\\operatorname*{min}\\{N,K\\})}&{}\\\\ &{\\geq\\Omega(\\operatorname*{min}\\{N(\\beta_{0\\mathsf{f f}}+1),T\\}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Theorem $3.3^{\\prime}$ (Impossibility of memoryless algorithms for OEOE; proper variant). Consider the binary classification setting with $\\mathcal{Z}=\\dot{\\mathcal{V}}=\\{0,\\dot{1}\\}$ and loss $\\mathsf{D}_{0/1}(\\cdot,\\cdot)$ . For any $N\\in\\mathbb N$ and $\\beta_{0\\mathsf{f}\\mathsf{f}}\\geq0$ , there exists an instance $(\\mathcal{X},\\mathcal{Y},\\mathcal{Z},\\mathcal{K},\\mathcal{F})$ with $|\\mathcal{F}|=|\\mathcal{X}|=N$ such that for any memoryless oracleefficient algorithm that is $(i)$ proper, and (ii) time-invariant, there exists a sequence of covariates $(x^{1},\\ldots,x^{T})$ and a proper offline oracle $\\mathbf{Alg}_{0\\mathsf{f f}}$ with parameter $\\beta_{\\mathsf{O f f}}$ such that $\\mathbb{E}\\big[\\mathbf{Est}_{\\mathsf{D}}^{\\mathsf{O n}}(T)\\big]\\geq$ $\\Omega(\\operatorname*{min}{\\{N(\\beta_{0\\mathsf{f f}}+1),T\\}})$ . ", "page_idx": 33}, {"type": "text", "text": "Proof of Theorem $\\mathbf{3.3^{\\prime}}$ . Given a parameter $N\\;\\in\\;\\mathbb{N}$ , we consider the instance $(\\mathcal{X},\\mathcal{Y},\\mathcal{Z},\\mathcal{K},\\mathcal{F})$ given by $\\mathcal{X}\\,=\\,\\{\\boldsymbol{x}_{i}\\}_{i\\in[N]}$ , $\\mathcal{Z}\\,=\\,\\mathcal{V}\\,=\\,\\{0,1\\}$ , $\\textsf{D}=\\textsf{D}_{0/1}$ and $\\boldsymbol{\\mathscr{K}}(z)\\,=\\,\\mathbb{1}_{z}$ , with parameter space $\\mathcal{F}:=\\{f_{i}\\}_{i\\in[N]}$ given by ", "page_idx": 33}, {"type": "equation", "text": "$$\nf_{i}(x_{j})=\\mathbb{1}(j\\geq i).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Let any memoryless oracle-efficient algorithm defined by prediction map $F^{1}=\\cdots=F^{T}=F$ be given. We lower bound the algorithm\u2019s online estimation error by considering two cases. ", "page_idx": 33}, {"type": "text", "text": "Case 1: There exists a parameter $f_{i}$ such that the distribution $\\mu_{i}=F(f_{i})$ satisfies $\\mu_{i}(f_{i})<1/2$ . We consider two sub-cases of Case 1. The first subcase is where $\\mu_{i}(\\{f_{j}:j>i\\})>1/4$ . In this case, we choose the sequence of covariates as $x^{1}=\\cdot\\cdot\\cdot=x^{T}=x_{i}$ , set $f^{\\star}=f_{i}$ , and choose $\\mathbf{Alg}_{0\\mathsf{f f}}$ to be the offline estimation oracle that sets ${\\widehat{f}}^{1}=\\cdots={\\widehat{f}}^{T}=f_{i}$ . With this choice, the offline estimation error for the oracle satisfies ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\forall t\\in[T],\\ \\sum_{s<t}\\mathsf{D}_{0/1}\\Big(f^{\\star}(x^{s}),\\widehat{f}^{t}(x^{s})\\Big)=\\sum_{s<t}\\mathsf{D}_{0/1}\\big(f_{i}(x^{s}),f_{i}(x^{s})\\big)=0.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "However, the online estimation error satisfies ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}\\sim\\mu^{t}}\\left[\\mathsf{D}_{0/1}\\!\\left(f^{\\star}(x^{t}),\\bar{f}(x^{t})\\right)\\right]=\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}\\sim\\mu_{t}}\\left[\\mathsf{D}_{0/1}\\!\\left(f_{i}(x_{i}),\\bar{f}(x_{i})\\right)\\right]}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}&{=\\sum_{t=1}^{T}\\sum_{j>i}^{T}\\mu_{i}(f_{j})\\mathsf{D}_{0/1}(f_{i}(x_{i}),f_{j}(x_{i}))}\\\\ {\\displaystyle}\\\\ {\\displaystyle}&{=\\displaystyle\\sum_{t=1}^{T}\\sum_{j>i}^{\\mu_{i}}\\mu_{i}(f_{j})\\mathsf{D}_{0/1}(f_{i}(x_{i}),f_{i+1}(x_{i}))}\\\\ {\\displaystyle}\\\\ {\\displaystyle}&{\\geq\\displaystyle\\frac{1}{4}\\sum_{t=1}^{T}\\mathsf{D}_{0/1}(f_{i}(x_{i}),f_{i+1}(x_{i}))=T/4.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The second sub-case of Case 1 is where $\\mu_{i}(\\{f_{j}:j>i\\})\\leq1/4$ . This, combined with the fact that $\\mu_{i}(f_{i})<1/2$ , gives $\\mu_{i}(\\{f_{j}:j<i\\})>1/4$ . In this sub-case, we choose the sequence of covariates as $x^{1}=\\cdot\\cdot\\cdot=x^{T}=x_{i-1}$ , set $f^{\\star}=f_{i}$ , and choose $\\mathbf{Alg}_{0\\mathsf{f f}}$ to be the offline estimation oracle that sets $f^{1}=\\cdot\\cdot=f^{T}=f_{i}$ . In this case, the offline estimation error is zero: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\forall t\\in[T],\\ \\sum_{s<t}\\mathsf{D}_{0/1}\\Big(f^{\\star}(x^{s}),\\widehat{f}^{t}(x^{s})\\Big)=\\sum_{s<t}\\mathsf{D}_{0/1}\\big(f_{i}(x^{s}),f_{i}(x^{s})\\big)=0.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "In addition, the online estimation error is lower bounded by ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}\\sim\\mu^{t}}\\big[\\mathsf{D}_{0/1}\\big(f^{\\star}(x^{t}),\\bar{f}(x^{t})\\big)\\big]=\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}\\sim\\mu_{t}}\\big[\\mathsf{D}_{0/1}\\big(f_{i}(x_{i-1}),\\bar{f}(x_{i-1})\\big)\\big]}\\\\ &{\\displaystyle\\geq\\sum_{t=1}^{T}\\sum_{j<i}\\mu_{i}(f_{j})\\mathsf{D}_{0/1}(f_{i}(x_{i-1}),f_{j}(x_{i-1}))}\\\\ &{\\displaystyle=\\sum_{t=1}^{T}\\sum_{j<i}\\mu_{i}(f_{j})\\mathsf{D}_{0/1}(f_{i}(x_{i-1}),f_{i-1}(x_{i-1}))}\\\\ &{\\displaystyle\\geq\\frac{1}{4}\\sum_{t=1}^{T}\\mathsf{D}_{0/1}(f_{i}(x_{i-1}),f_{i-1}(x_{i-1}))=T/4.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Case 2: For all parameter $f_{i}\\in\\mathcal{F}$ , the distribution $\\mu_{i}=F(f_{i})$ has $\\mu_{i}(f_{i})\\geq1/2$ . ", "page_idx": 34}, {"type": "text", "text": "In this case, we choose $x^{1},\\ldots,x^{T}$ by repeating each of the covariates $\\lfloor\\beta_{0\\mathsf{f f}}\\rfloor+1$ number of times in increasing order by their index, and choose the offilne estimation oracle $\\mathbf{Alg}_{0\\mathsf{f f}}$ to return $f_{1},\\ldots,f_{N}$ in the same block-wise but with the index offset by 1. ", "page_idx": 34}, {"type": "text", "text": "Formally, let $\\tau_{t}=\\lceil t/(\\lfloor\\beta_{0\\mathsf{f f}}\\rfloor+1)\\rceil$ denote the index of the block that round $t$ belongs to, so that $\\tau_{1}=\\cdot\\cdot\\cdot=\\tau_{\\lfloor\\beta_{0\\mathrm{ff}}\\rfloor+1}=1$ , $\\tau_{\\lfloor\\beta_{\\sf O f f}\\rfloor+2}=\\cdot\\cdot=\\tau_{2\\lfloor\\beta_{\\sf O f f}\\rfloor+2}=2$ and so on. We choose $x^{t}=x_{\\operatorname*{min}\\{\\tau_{t},N\\}}$ , and choose the offline oracle $\\mathbf{Alg}_{0\\mathsf{f f}}$ to set $\\widehat{f^{t}}=f_{\\operatorname*{min}\\left\\{\\tau_{t},N\\right\\}}$ . Finally, we set $f^{\\star}=f_{N}$ . ", "page_idx": 34}, {"type": "text", "text": "We have that for all $t$ , the offline estimation error of the oracle is bounded as. ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s<t}\\mathsf{D}_{0/1}\\Big(\\widehat{f}^{t}(x^{s}),f^{\\star}(x^{s})\\Big)=\\displaystyle\\sum_{s<t}\\mathsf{D}_{0/1}\\big(f_{N}(x_{\\operatorname*{min}\\{\\tau_{s},N\\}}),f_{\\operatorname*{min}\\{\\tau_{t},N\\}}\\big(x_{\\operatorname*{min}\\{\\tau_{s},k\\}}\\big)\\big)}\\\\ &{\\displaystyle=\\sum_{s<t,\\tau_{s}=\\tau_{t}}1\\leq|\\beta_{0\\mathsf{f f}}|\\leq\\beta_{0\\mathsf{f f}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "However, the online estimation error is lower bounded by ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}\\sim\\mu^{t}}\\left[\\mathsf{D}_{0/1}\\big(f^{\\star}(x^{t}),\\bar{f}(x^{t})\\big)\\right]\\ge\\displaystyle\\frac{1}{2}\\sum_{t=1}^{T}\\mathsf{D}_{0/1}\\big(f_{N}(x_{\\operatorname*{min}\\{\\tau_{t},N\\}}),f_{\\operatorname*{min}\\{\\tau_{t},N\\}}(x_{\\operatorname*{min}\\{\\tau_{t},N\\}})\\big)}&{}\\\\ {\\displaystyle\\ge\\Omega(\\operatorname*{min}\\{T,N(\\beta_{\\Theta\\mathrm{ff}}+1)\\}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proposition D.1 (Upper bound for memoryless OEOE). For any instance $(\\mathcal{X},\\mathcal{Y},\\mathcal{Z},\\mathcal{K},\\mathcal{F}),$ metric-like loss D, and offline oracle $\\mathbf{Alg}_{0\\mathsf{f f}}$ with parameter $\\beta_{0\\mathsf{f}\\mathsf{f}}$ , the algorithm that returns $\\bar{f}^{t}\\;\\;=\\;\\;\\widehat{f}^{t}\\;\\;:=\\;\\;{\\bf A l g}_{0\\mathrm{ff}}^{t}\\big(x^{1},\\ldots,x^{t-1},y^{1},\\ldots,y^{t-1}\\big)$ has online estimation error $\\begin{array}{r l}{\\mathbf{Est}_{\\mathrm{D}}^{\\mathsf{O n}}(T)}&{\\leq}\\end{array}$ $O((\\beta_{0\\mathsf{f f}}+1)|\\mathcal{X}|\\log T)$ . ", "page_idx": 34}, {"type": "text", "text": "Proof of Proposition D.1. The proof is very similar to the second part of the proof of Lemma G.1. For each $x\\in\\mathscr{X}$ , define $N_{t-1}(x)=\\sum_{s=1}^{t-1}\\mathbb{1}(x^{s}=x)$ . Then we can write the online estimation error as ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathsf{D}\\Big(\\widehat{f}^{t}(x^{t}),f^{\\star}(x^{t})\\Big)=\\sum_{t=1}^{T}\\sum_{x\\in\\mathcal{X}}\\frac{\\mathbb{1}(x=x^{t})}{N_{t-1}(x)\\vee1}\\cdot(N_{t-1}(x)\\vee1)\\cdot\\mathsf{D}\\Big(\\widehat{f}^{t}(x),f^{\\star}(x)\\Big),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "As a consequence of the offline estimation guarantee for ${\\widehat{f}}^{t}$ , we have that ", "page_idx": 35}, {"type": "equation", "text": "$$\n(N_{t-1}(x)\\vee1)\\mathsf{D}\\Big(\\widehat{f}^{t}(x),f^{\\star}(x)\\Big)\\leq\\left(\\sum_{s=1}^{t-1}\\mathsf{D}\\Big(\\widehat{f}^{t}(x^{s}),f^{\\star}(x^{s})\\Big)\\right)\\vee1\\leq\\beta_{0\\mathsf{f}}+1.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Combining this with the preceding inequality gives ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathsf{D}\\Big(\\widehat{f}^{t}(x^{t}),f^{\\star}(x^{t})\\Big)\\le(\\beta_{0\\mathsf{f f}}+1)\\sum_{t=1}^{T}\\sum_{x\\in\\mathcal{X}}\\frac{\\mathbb{1}(x=x^{t})}{N_{t-1}(x)\\vee1}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Now, for any $x\\in\\mathscr{X}$ , define $t_{x}:=\\operatorname*{min}\\left\\{t\\leq T:x^{t}=x\\right\\}$ if this set is not empty, and let $t_{x}=T$ otherwise. From this definition and the fact that $1+1/2+\\cdot\\cdot+1/T\\leq1+\\log T$ , we have that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{x\\in\\mathcal{X}}\\sum_{t=1}^{T}\\frac{\\mathbb{1}(x=x^{t})}{N_{t-1}(x)\\vee1}=\\displaystyle\\sum_{x\\in\\mathcal{X}}\\left(\\sum_{t=1}^{t_{x}}\\frac{\\mathbb{1}(x=x^{t})}{N_{t-1}(x)\\vee1}+\\sum_{t=t_{x}+1}^{T}\\frac{\\mathbb{1}(x=x^{t})}{N_{t-1}(x)\\vee1}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\sum_{x\\in\\mathcal{X}}\\left(1+\\sum_{i=1}^{N_{T-1}(x)}\\frac{1}{i}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq2|\\mathcal{X}|+|\\mathcal{X}|\\log T\\leq3|\\mathcal{X}|\\log T.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "We conclude that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathsf{D}\\Big(f^{\\star}(x^{t}),\\widehat{f}^{t}(x^{t})\\Big)\\le3(\\beta_{0\\mathsf{f f}}+1)\\cdot|\\mathcal{X}|\\log T.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "G.3 Proofs from Appendix D ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Theorem D.1 (Reduction from oracle-efficient online estimation to delayed online learning). Let D be any convex, metric-like loss. Suppose we run Algorithm 2 with delay parameter $N\\in\\mathbb N$ and $a$ delayed online learning algorithm $A_{\\mathrm{DOL}}$ for the class $\\mathcal{F}$ . Then for all $\\gamma\\geq1$ , Algorithm 2 ensures that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[\\mathbf{Est}_{\\mathsf{D}}^{\\mathsf{O n}}(T)\\big]\\leq O(C_{\\mathsf{D}}\\gamma(N+\\beta_{\\mathsf{O f f}}T/N)+R_{\\mathsf{D O L}}(T,N,\\gamma)),\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "with any offline oracle $\\mathbf{Alg}_{0\\mathsf{f f}}$ with parameter $\\beta_{0\\mathsf{f}\\mathsf{f}}\\geq0$ , where ", "page_idx": 35}, {"type": "equation", "text": "$$\nR_{\\mathsf{D O L}}(T,N,\\gamma):=\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}^{t}\\sim\\mu^{t}}\\big[\\ell^{t}(\\bar{f}^{t})\\big]-\\gamma\\cdot\\operatorname*{min}_{f\\in\\mathcal{F}}\\sum_{t=1}^{T}\\ell^{t}(f)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "is the regret of $A_{\\mathrm{DOL}}$ for the sequence of losses constructed in Algorithm 2. ", "page_idx": 35}, {"type": "text", "text": "Proof of Theorem D.1. Using the metric-like loss property, we can bound the online estimation error of Algorithm 2 by ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}\\sim\\mu^{t}}\\big[\\mathsf{D}\\big(\\bar{f}(x^{t}),f^{\\star}(x^{t})\\big)\\big]}\\\\ &{\\displaystyle\\leq C_{\\mathsf{D}}\\cdot\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}\\sim\\mu^{t}}\\Big[\\mathsf{D}\\Big(\\bar{f}(x^{t}),\\tilde{f}^{t}(x^{t})\\Big)\\Big]+C_{\\mathsf{D}}\\cdot\\displaystyle\\sum_{t=1}^{T}\\mathsf{D}\\Big(\\tilde{f}^{t}(x^{t}),f^{\\star}(x^{t})\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "By the regret guarantee for the delayed online learning algorithm $A_{\\mathrm{DOL}}$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}\\sim\\mu^{t}}\\Big[\\mathsf{D}\\Big(\\bar{f}\\big(x^{t}\\big),\\tilde{f}^{t}(x^{t})\\Big)\\Big]\\leq\\gamma\\cdot\\sum_{t=1}^{T}\\mathsf{D}\\Big(\\tilde{f}^{t}(x^{t}),f^{\\star}(x^{t})\\Big)+R_{\\mathsf{D O L}}(T,N,\\gamma)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "since $f^{\\star}\\in{\\mathcal{F}}$ . Combining these observations, we have that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}\\sim\\mu^{t}}\\big[\\mathsf{D}\\big(\\bar{f}(x^{t}),f^{\\star}(x^{t})\\big)\\big]\\leq C_{\\mathsf{D}}(\\gamma+1)\\sum_{t=1}^{T}\\mathsf{D}\\Big(\\tilde{f}^{t}(x^{t}),f^{\\star}(x^{t})\\Big)+R_{\\mathsf{D O L}}(T,N,\\gamma).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Finally, from the definition of $\\tilde{f}^{t}$ and the convexity of the loss $\\mathsf{D}$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\mathsf{D}\\bigg(\\widetilde f^{t}(x^{t}),f^{\\star}(x^{t})\\bigg)\\leq N+\\displaystyle\\frac{1}{N}\\sum_{t=1}^{T-N}\\sum_{i=t+1}^{t+N}\\mathsf{D}\\bigg(\\widehat f^{i}(x^{t}),f^{\\star}(x^{t})\\bigg)}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad=N+\\displaystyle\\frac{1}{N}\\sum_{t=2}^{T}\\sum_{i<t}\\mathsf{D}\\bigg(\\widehat f^{t}(x^{i}),f^{\\star}(x^{i})\\bigg)}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\leq N+\\displaystyle\\frac{\\beta\\mathsf{o}_{\\Psi}T}{N},}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the final line uses the offline estimation guarantee for $\\mathbf{A}\\mathbf{lg}_{0\\mathsf{f f}}$ . This completes the proof. ", "page_idx": 36}, {"type": "text", "text": "Lemma D.1. Consider the delayed online learning setting with a delay parameter $N$ . There exists an algorithm that achieves ", "page_idx": 36}, {"type": "equation", "text": "$$\nR_{\\sf D O L}(T,N,2)=\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}^{t}\\sim\\mu^{t}}\\left[\\ell^{t}(\\bar{f}^{t})\\right]-2\\cdot\\operatorname*{min}_{f\\in\\mathcal{F}}\\sum_{t=1}^{T}\\ell^{t}(f)\\leq2N\\cdot\\log|\\mathcal{F}|\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "for any sequences of losses $\\ell^{1},\\ldots,\\ell^{T}\\in[0,1]$ . ", "page_idx": 36}, {"type": "text", "text": "Proof of Lemma D.1. This result follows using Lemma G.3 with $\\gamma=2$ , choosing $\\mathcal{A}_{\\sf O L}$ to be the exponential weights algorithm described in Corollary 2.3 of Cesa-Bianchi and Lugosi [17], which has ", "page_idx": 36}, {"type": "equation", "text": "$$\nR_{0\\downarrow}(T,\\gamma)\\leq O(\\log|\\mathcal{F}|)\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "for all $T\\in\\mathbb N$ and $\\gamma\\geq1$ . ", "page_idx": 36}, {"type": "text", "text": "Theorem D.2 (Characterization of oracle-efficient learnability for binary classification). Consider a binary classification instance $(\\mathcal{X},\\mathcal{Y},\\mathcal{Z},\\mathcal{K},\\mathcal{F})$ with $\\mathcal{Z}=\\mathcal{Y}=\\{0,1\\}$ , $\\mathrm{D}=\\mathsf{D}_{0/1}$ and $\\boldsymbol{\\mathscr{K}}(z)=\\mathbb{1}_{z}$ . For any class $\\mathcal{F}$ and $\\beta_{0\\mathsf{f}\\mathsf{f}}\\geq0,$ , there exists an oracle-efficient algorithm that achieves online estimation error $O({\\sqrt{\\beta_{0\\mathsf{f f}}\\mathrm{Ldim}({\\mathcal{F}})\\cdot T\\log T}}+\\mathrm{Ldim}({\\mathcal{F}})\\log T)$ . On the other hand, in the worst-case any algorithm must suffer at least $\\Omega(\\operatorname{Ldim}({\\mathcal{F}}))$ online estimation error. ", "page_idx": 36}, {"type": "text", "text": "Proof of Theorem D.2. For the lower bound we recall that for $\\beta_{0\\mathsf{f}\\mathsf{f}}=0$ , Lemma 21.6 of [57] states that any algorithm (oracle-efficient or not) has to suffer $\\Omega(\\operatorname{Ldim}({\\mathcal{F}}))$ online estimation error in the worst case. ", "page_idx": 36}, {"type": "text", "text": "For the remainder of the proof, we focus on establishing the upper bound. For any set of parameters ${\\mathcal{F}}:{\\mathcal{X}}\\rightarrow\\Delta(\\{0,1\\})$ , define the majority vote function Majority $(\\mathcal{F})$ for a class $\\mathcal{F}$ via ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\operatorname{Majority}({\\mathcal{F}})(x)=\\mathbb{1}\\left\\{\\sum_{f\\in{\\mathcal{F}}}f(1\\mid x)\\geq\\sum_{f\\in{\\mathcal{F}}}f(0\\mid x)\\right\\}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "for all $x\\in\\mathscr{X}$ . We will show that Algorithm 4 (a variant of Algorithm 2 that replaces averaging with a majority vote), with a properly chosen delayed online learning algorithm $A_{\\mathrm{DOL}}$ , can obtain ", "page_idx": 36}, {"type": "equation", "text": "$$\nO\\Big(\\sqrt{\\beta_{\\mathsf{O f f}}\\mathrm{Ldim}(\\mathcal{F})\\cdot T\\log T}+\\mathrm{Ldim}(\\mathcal{F})\\log T\\Big)\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "online estimation error. ", "page_idx": 36}, {"type": "text", "text": "1: input: Offline estimation oracle $\\mathbf{Alg}_{0\\mathsf{f f}}$ with parameter $\\beta_{0\\mathsf{f}\\mathsf{f}}\\,\\geq\\,0$ , delay parameter $N\\,\\in\\,\\mathbb{N}$ , delayed online learning algorithm $A_{\\mathrm{DOL}}$ for class $\\mathcal{F}$ .   \n2: for $t=1,2,\\ldots,T$ do   \n3: Receive ${\\widehat{f}}^{t}$ from offline estimation algorithm.   \n4: if $t>N$ then   \n5: Let $\\widetilde{f}^{t-N}=\\operatorname{Majority}\\left(\\{\\widehat{f}^{i}\\}_{i=t-N+1}^{t}\\right)$ . // This is the only different step compared to Algorithm 2.   \n6: Let $\\ell^{t-N}(f)\\ =\\ \\mathsf{D}_{0/1}\\Big(\\tilde{f}^{t-N}\\big(x^{t-N}\\big),f\\big(x^{t-N}\\big)\\Big)$ and pass $\\ell^{t-N}(\\cdot)$ to $A_{\\mathrm{DOL}}$ as the delayed feedback.   \n7: Let $\\mu^{t}=\\mathcal{A}_{\\sf D O L}^{t}\\big(\\ell^{1},..,\\ell^{t-N}\\big)$ be the delayed online learner\u2019s prediction distribution.   \n8: Predict with $\\bar{f}^{t}\\sim\\mu^{t}$ and receive $x^{t}$ . ", "page_idx": 37}, {"type": "text", "text": "Let $\\gamma\\geq1$ be fixed, and consider any delayed online learning algorithm $A_{\\mathrm{DOL}}$ that achieves ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}^{t}\\sim\\mu^{t}}\\big[\\ell^{t}\\big(\\bar{f}^{t}\\big)\\big]-\\gamma\\cdot\\operatorname*{min}_{f\\in\\mathcal{F}}\\sum_{t=1}^{T}\\ell^{t}(f)\\leq R_{\\mathsf{D O L}}(T,N,\\gamma).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "for any sequence of losses in the delayed online learning setting with delay $N$ (i.e., where we receive loss $\\ell^{t}$ at time $t+N$ for some $N\\geq0$ ). ", "page_idx": 37}, {"type": "text", "text": "We proceed to bound the regret of Algorithm 4. Since the loss $\\mathsf{D}_{0/1}$ is metric-like, the online estimation error is upper bounded by ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{T}\\mathbb{E}_{\\bar{f}\\sim\\mu^{t}}\\left[\\mathsf{D}_{0/1}\\big(\\bar{f}(x^{t}),f^{\\star}(x^{t})\\big)\\right]\\leq\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}\\sim\\mu^{t}}\\left[\\mathsf{D}_{0/1}\\Big(\\bar{f}(x^{t}),\\tilde{f}^{t}(x^{t})\\Big)\\right]+\\sum_{t=1}^{T}\\mathsf{D}_{0/1}\\Big(\\tilde{f}^{t}(x^{t}),f^{\\star}(x^{t})\\Big).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Next, the guarantee of $A_{\\mathrm{DOL}}$ ensures that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}\\sim\\mu^{t}}\\Big[\\mathsf{D}_{0/1}\\Big(\\bar{f}(x^{t}),\\tilde{f}^{t}(x^{t})\\Big)\\Big]\\leq\\gamma\\sum_{t=1}^{T}\\mathsf{D}_{0/1}\\Big(\\tilde{f}^{t}(x^{t}),f^{\\star}(x^{t})\\Big)+R_{\\mathsf{D O L}}(T,N,\\gamma),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "since $f^{\\star}\\in\\mathcal{F}$ . Combining these observations, we have that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}\\sim\\mu^{t}}\\big[\\mathsf{D}_{0/1}\\big(\\bar{f}(x^{t}),f^{\\star}(x^{t})\\big)\\big]\\leq(\\gamma+1)\\sum_{t=1}^{T}\\mathsf{D}_{0/1}\\Big(\\tilde{f}^{t}(x^{t}),f^{\\star}(x^{t})\\Big)+R_{\\mathsf{D O L}}(T,N,\\gamma).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Finally, we observe that for each step $t$ , if $\\mathsf{D}_{0/1}\\Big(\\tilde{f}^{t}(x^{t}),f^{\\star}(x^{t})\\Big)=1$ , it means that least $N/2$ of the predictors ${\\widehat{f}}^{t+1},\\ldots,{\\widehat{f}}^{t+N}$ must have predicted $f^{\\star}(x^{t})$ incorrectly. This implies that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathsf{D}_{0/1}\\Big(\\widetilde{f}^{t}(x^{t}),f^{\\star}(x^{t})\\Big)\\le\\frac{2}{N}\\sum_{i=t+1}^{t+N}\\mathsf{D}_{0/1}\\Big(\\widehat{f}^{i}(x^{t}),f^{\\star}(x^{t})\\Big).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "But since the offline estimation assumption states that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sum_{i<t}\\mathsf{D}_{0/1}\\Big(\\widehat{f}^{t}(x^{i}),f^{\\star}(x^{i})\\Big)\\leq\\beta_{0\\mathsf{f f}},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "this implies that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\mathsf{D}_{0/1}\\Big(\\widetilde f^{t}(x^{t}),f^{\\star}(x^{t})\\Big)\\leq N+\\displaystyle\\frac{2}{N}\\sum_{t=1}^{T-N}\\sum_{i=t+1}^{t+N}\\mathsf{D}_{0/1}\\Big(\\widehat f^{i}(x^{t}),f^{\\star}(x^{t})\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=N+\\displaystyle\\frac{2}{N}\\sum_{t=2}^{T}\\sum_{i<t}\\mathsf{D}_{0/1}\\Big(\\widehat f^{t}(x^{i}),f^{\\star}(x^{t})\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq N+\\displaystyle\\frac{2\\beta_{0}\\eta_{\\mathrm{f}}T}{N}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We conclude that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}^{t}\\sim\\mu^{t}}\\big[\\mathsf{D}_{0/1}\\big(\\bar{f}^{t}(x^{t}),f^{\\star}(x^{t})\\big)\\big]\\leq O(\\gamma(N+\\beta_{0\\#}T/N)+R_{\\mathsf{D O L}}(T,N,\\gamma)).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "To complete the proof, we set $\\gamma=2$ and choose $A_{\\mathrm{OL}}$ to be the algorithm described in Theorem 21.10 of Shalev-Shwartz and Ben-David [57], which (by incorporating the same technique18 as Corollary 2.3 of Cesa-Bianchi and Lugosi [17]), ensures that ", "page_idx": 38}, {"type": "equation", "text": "$$\nR_{0\\downarrow}(T/N,2)\\leq O(\\mathrm{Ldim}(\\mathcal{F})\\log T).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Then by Lemma G.3 with $\\gamma=2$ , we have ", "page_idx": 38}, {"type": "equation", "text": "$$\nR_{\\mathsf{D O L}}(T,N,2)\\leq O(N\\cdot\\mathrm{Ldim}({\\mathcal{F}})\\log T).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Setting $N=\\sqrt{\\beta_{\\mathsf{O f f}}T/(\\mathrm{Ldim}(\\mathcal{F})\\log T)}\\vee1$ , this yields ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{x}\\mathbb{E}_{\\bar{f}^{t}\\sim\\mu^{t}}\\left[\\mathsf{D}\\big(\\bar{f}^{t}(x^{t}),f^{\\star}(x^{t})\\big)\\right]\\leq O\\Big(\\sqrt{\\beta_{\\mathrm{Off}}\\mathrm{Ldim}(\\mathcal{F})\\cdot T\\log T}+\\mathrm{Ldim}(\\mathcal{F})\\log T\\Big).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "G.3.1 Supporting Lemmas ", "text_level": 1, "page_idx": 38}, {"type": "table", "img_path": "sks7x4I8Bh/tmp/d97bdea15d582022a605b73f55a937937ec956793fedbe27b1118de7308a2e06.jpg", "table_caption": [], "table_footnote": [], "page_idx": 38}, {"type": "text", "text": "The following lemma is a standard result [67, 42, 35, 47] which shows that the delayed online learning problem setting in Appendix $\\mathrm{D}$ can be generically reduced to non-delayed online learning. The idea behind the reduction, which is displayed in Algorithm 5, is as follows. Given a delay parameter N \u2208N, we run N copies A1OL, . $A_{0\\mathrm{L}}^{1},\\ldots,A_{0\\mathrm{L}}^{\\tilde{N}}$ of a given \u201cbase\u201d online learning algorithm $A_{\\mathrm{OL}}$ for a class $\\mathcal{F}$ over disjoint subsequences of rounds. The following lemma gives a guarantee for this reduction ", "page_idx": 38}, {"type": "text", "text": "Lemma G.3 (Delayed online learning reduction). Let $A_{\\mathrm{OL}}$ be a base online learning algorithm for the class $\\mathcal{F}$ with the property that for any sequence of losses $\\ell^{1},\\ldots,\\ell^{T}$ in the non-delayed online learning setting and any $\\gamma\\geq1$ , ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}^{t}\\sim\\mu^{t}}\\big[\\ell^{t}(\\bar{f}^{t})\\big]\\,-\\gamma\\cdot\\operatorname*{min}_{f\\in\\mathcal{F}}\\sum_{t=1}^{T}\\ell^{t}\\big(f\\big)\\leq R_{0!}(T,\\gamma).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "If we run Algorithm $^{5}$ with delay parameter $N\\in\\mathbb{N}$ , then for all $\\gamma\\geq1$ , the algorithm ensures that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{\\sf D O L}(T,N,\\gamma)\\leq\\displaystyle\\sum_{i=1}^{N}\\left(\\sum_{j=1}^{T/N}\\mathbb{E}_{\\bar{f}^{i+N\\cdot j}\\sim\\mu^{i+N\\cdot j}}\\left[\\ell^{i+N\\cdot j}\\left(\\bar{f}^{i+N\\cdot j}\\right)\\right]-\\gamma\\cdot\\operatorname*{min}_{f\\in\\mathcal{F}}\\sum_{j=1}^{T/N}\\ell^{i+N\\cdot j}(f)\\right)}\\\\ &{\\leq N\\cdot R_{\\sf O L}(T/N,\\gamma)}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "for online learning with delay $N$ . ", "page_idx": 38}, {"type": "text", "text": "18The algorithm described in Theorem 21.10 of Shalev-Shwartz and Ben-David [57] applies the exponential weights algorithm to a specialized class of experts, and the guarantee obtained is for $R_{0\\mathrm{L}}(T,1)$ . The analysis from Corollary 2.3 of Cesa-Bianchi and Lugosi [17] shows that for $\\gamma>e/(e-1)$ , the same algorithm obtains $R_{0\\mathrm{L}}(T,2)$ scaling with $O(\\mathrm{Ldim}(\\mathcal{F})\\log T)$ . We omit the details here since it is a standard argument. ", "page_idx": 38}, {"type": "text", "text": "Proof of Lemma G.3. By the guarantee of $A_{\\mathrm{OL}}$ , we have that for all $i\\in[N]$ , ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{T/N}\\mathbb{E}_{\\bar{f}^{i+N\\cdot j}\\sim\\mu^{i+N\\cdot j}}\\left[\\ell^{i+N\\cdot j}(\\bar{f}^{i+N\\cdot j})\\right]-\\gamma\\cdot\\operatorname*{min}_{f\\in\\mathcal{F}}\\sum_{j=1}^{T/N}\\ell^{i+N\\cdot j}(f)\\le R_{0}\\mathsf{L}(T/N,\\gamma).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Summing up over all $i\\in[N]$ , we obtain ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{N\\cdot R_{0\\mathrm{L}}(T/N,\\gamma)=\\displaystyle\\sum_{i=1}^{N}\\displaystyle\\sum_{j=1}^{T/N}\\mathbb{E}_{\\bar{f}^{i+N\\cdot j}\\sim\\mu^{i+N\\cdot j}}\\left[\\ell^{i+N\\cdot j}(\\bar{f}^{i+N\\cdot j})\\right]-\\gamma\\cdot\\displaystyle\\sum_{i=1}^{N}\\displaystyle\\operatorname*{min}_{f\\in\\mathcal{F}}\\sum_{j=1}^{T/N}\\ell^{i+N\\cdot j}(f)}}\\\\ {{\\displaystyle\\geq\\displaystyle\\sum_{i=1}^{N}\\sum_{j=1}^{T/N}\\mathbb{E}_{\\bar{f}^{i+N\\cdot j}\\sim\\mu^{i+N\\cdot j}}\\left[\\ell^{i+N\\cdot j}(\\bar{f}^{i+N\\cdot j})\\right]-\\gamma\\cdot\\displaystyle\\operatorname*{min}_{f\\in\\mathcal{F}}\\sum_{i=1}^{N}\\sum_{j=1}^{T/N}\\ell^{i+N\\cdot j}(f)}}\\\\ {{\\displaystyle=\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}^{t}\\sim\\mu^{t}}\\left[\\ell^{t}(\\bar{f}^{t})\\right]-\\gamma\\cdot\\displaystyle\\operatorname*{min}_{f\\in\\mathcal{F}}\\sum_{t=1}^{T}\\ell^{t}(f).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "H Proofs from Section 4 ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "H.1 Proofs from Section 4.1 ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Theorem 4.1 (Computational lower bound for OEOE). Assume the existence of one-way functions.19 There exists a sequence of polynomially computable classes $({\\mathcal{F}}_{1},{\\mathcal{F}}_{2},\\ldots,{\\mathcal{F}}_{n},\\ldots)$ , along with $a$ sequence of poly $(n)$ -output description length offilne oracles with $\\beta_{0\\mathsf{f}\\mathsf{f}}=0$ associated with each ${\\mathcal{F}}_{n}$ , such that for any fixed polynomials $p,q:\\mathbb{N}\\rightarrow\\mathbb{N}$ and all $n\\in\\mathbb N$ sufficiently large, any oracle-efficient online estimation algorithm with runtime bounded by $p(n)$ must have $\\mathbb{E}[\\mathbf{Est}_{\\mathsf{D}}^{\\mathsf{O n}}(T)]\\geq T/4$ for all $1\\leq T\\leq q(n)$ . At the same time, there exists an inefficient algorithm that achieves $\\mathbb{E}[\\mathbf{Est}_{\\mathsf{D}}^{\\mathsf{O n}}(T)]\\leq$ $O({\\sqrt{n}})$ for all $T\\in\\mathbb N$ . ", "page_idx": 39}, {"type": "text", "text": "Proof of Theorem 4.1. We frame the example proposed by Blum [14] in their Theorem 3.2 (see also Bun [15]) in our setting. For any integer $n\\geq1$ , let the covariate space ${\\mathcal{X}}_{n}$ be ${\\mathcal{X}}_{n}=\\left\\{0,1\\right\\}^{n}$ , and set $\\mathcal{Z}=\\mathcal{Y}=\\{0,1\\}$ and $\\boldsymbol{\\mathscr{K}}(z)=\\mathbb{1}_{z}$ . We define a class $\\mathcal{F}_{n}=\\left\\{f_{s}:s\\in\\{0,1\\}^{\\sqrt{n}}\\right\\}$ with ", "page_idx": 39}, {"type": "equation", "text": "$$\nf_{s}(x):={\\binom{1}{0}}\\quad{\\mathrm{if~}}x\\in c_{s},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "for a certain collection of subsets $\\{c_{s}\\in\\mathcal{X}_{n}\\}_{s\\in\\{0,1\\}}\\sqrt{n}$ defined in Definition 2 of Blum [14], which is constructed based on cryptographic functions using the assumption of existence of one-way functions. The precise definition will not be important. The properties we will use are: ", "page_idx": 39}, {"type": "text", "text": "1. The value $f_{s}(x)$ can be computed in $\\mathrm{poly}(n)$ time for any $x\\in\\mathcal{X}_{n}$ . ", "page_idx": 39}, {"type": "text", "text": "2. For any polynomials $p(n),q(n)$ , any (possibly randomized) online estimation algorithm (oracle-efficient or not) which runs in time $p(n)$ , and any time st\u221aep $T\\ \\leq\\ q{\\bar{(}n)}$ , for sufficiently large $n$ where $q(n)\\ \\ll\\ 2^{\\sqrt{n}}$ ,20 there exists $s\\ \\in\\ \\{0,1\\}^{\\sqrt{n}}$ and a sequence $x_{s}^{1},\\ldots,x_{s}^{2^{\\sqrt{n}}-1},x_{s}^{2^{\\sqrt{n}}}$ (the specific definition of this sequence can be found in Blum [14]) such that the online estimation error under this sequence when $f^{\\star}\\,=\\,f_{s}$ is at least $T/4$ in expectation. Our lower bound construction for any oracle efficient online estimation algorithm with runtime bounded by $p(n)$ in time step bounded by $1\\leq T\\leq q(n)$ will choose the aforementioned covariate sequence as the covariates revealed with the aforementioned function as the true parameter, i.e., $x^{\\tau}=x_{s}^{\\tau}$ for $\\tau\\in[T]$ and $f^{\\star}=f_{s}$ . ", "page_idx": 39}, {"type": "text", "text": "It is straightforward to see that \u221athe sequence $({\\mathcal{F}}_{1},{\\mathcal{F}}_{2}...,{\\mathcal{F}}_{n},...)$ admits polynomial description length as claimed, since $\\log\\left|{\\mathcal{F}}_{n}\\right|={\\sqrt{n}}$ . We are left to verify that there is an offline oracle that achieves $\\beta_{0\\mathsf{f}\\mathsf{f}}=0$ with $\\mathrm{poly}(n)$ -output description length, yet does not provide any information not already available to the learner in the setting of Blum [14] (recall that in the protocol of Blum [14], the learner gets to see the covariates $x^{1},\\ldots,x^{t}$ and the true labels $y^{1},\\ldots,y^{t-1}$ at time step $t$ before making their prediction, but does not receive any other feedback). ", "page_idx": 40}, {"type": "text", "text": "Consider the following offlines oracle $\\mathbf{Alg}_{0\\mathsf{f f}}^{t}(x^{1},\\ldots,x^{t-1},y^{1},\\ldots,y^{t-1})$ . The oracle output ${\\widehat{f}}^{t}$ is a circuit that, on input $x$ , compares $x$ sequentially with $x^{1},\\ldots,x^{t}$ . If $x$ is ever equal to $x^{\\tau}$ for some $\\tau\\in[t-1]$ , the circuit will output $y^{\\tau}$ . If $x$ is not equal to any $x^{\\tau}$ for $\\tau\\in[t-1]$ , the circuit outputs 0. Such a Boolean circuit can be constructed with polynomial size in $n$ because each $x^{\\tau}$ has length $n$ for $\\tau\\in[t-1]$ and $t\\leq T\\leq q(n)$ by assumption. It is easy to see that this oracle achieves $\\beta_{0\\mathsf{f}\\mathsf{f}}=0$ , yet does not provide any additional information about target parameter $f^{\\star}$ beyond what is available in the model of Blum [14]. Combining all the above, we complete our lower bound proof. ", "page_idx": 40}, {"type": "text", "text": "Lastly, we observe that since the setting we consider is an instance of noiseless binary classificat\u221aion, the classical halving algorithm achieves an online estimation error bound of $O(\\log|{\\dot{\\mathcal{F}}}_{n}|)=O({\\sqrt{n}})$ [17]. ", "page_idx": 40}, {"type": "text", "text": "H.2 Proofs from Section 4.2 ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "In this section, we prove Theorem 4.2 through four layers of reductions through different variants of the online estimation setting. In Appendix H.2.1, we first introduce the relevant settings and the describe reductions through them. We then combine these reductions to prove Theorem 4.2. Finally, in Appendix H.2.2, we prove each of the four reduction results. ", "page_idx": 40}, {"type": "text", "text": "H.2.1 Proof of Theorem 4.2 ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Theorem 4.2. Let $\\mathcal{A}_{\\sf C D E}$ be an arbitrary (unrestricted) online estimation algorithm that satisfies Eq. (4) and has runtime Time $(\\mathcal{F},T)$ . Then for any $N\\in\\mathbb{N},$ , there exists an oracle-efficient online estimation algorithm that achieves estimation error ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[\\mathbf{Est}_{\\mathsf{H}}^{\\mathsf{O n}}(T)\\big]\\leq\\widetilde{O}(C_{\\mathcal{F}}\\log V\\cdot\\beta_{0\\mathsf{f f}}T/N+N\\cdot(R_{\\mathsf{C D E}}(T)+C_{\\mathcal{F}}\\log V))\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "with runtime $\\mathrm{poly}(\\mathsf{T i m e}({\\mathcal F},T),\\log|{\\mathcal F}|,\\log|{\\mathcal X}|,T)$ , where $\\beta_{0\\mathrm{ff}}\\ \\ \\geq\\ \\ 0$ is the offline estimation parameter. The distributions $\\boldsymbol{\\mu}^{1},\\ldots,\\boldsymbol{\\mu}^{T}$ produced by the algorithm have support size poly $(\\log|\\mathcal{F}|,\\log|\\mathcal{X}|,T)$ . As a special case, $i f$ the online estimation guarantee for the base algorithm holds with $\\begin{array}{r l r}{R_{\\mathsf{C D E}}(T)}&{{}\\leq}&{C_{\\mathcal{F}}^{\\prime}\\log T}\\end{array}$ for some problem-dependent constant $C_{\\mathcal{F}}^{\\prime}\\mathrm{~\\ensuremath~{~\\geq~}~}1$ , then by choosing $N$ appropriately, we achieve $\\begin{array}{r l}{\\mathbb{E}\\big[{\\bf E s t}_{\\mathsf{H}}^{\\mathsf{O n}}(T)\\big]}&{{}\\leq}\\end{array}$ $\\widetilde{O}\\big((C_{\\mathcal{F}}(C_{\\mathcal{F}}+C_{\\mathcal{F}}^{\\prime})\\beta_{0\\mathrm{ff}})^{1/2}\\log V\\cdot T^{1/2}+(C_{\\mathcal{F}}+C_{\\mathcal{F}}^{\\prime})\\log V\\big)$ . ", "page_idx": 40}, {"type": "text", "text": "Proof of Theorem 4.2. The proof of Theorem 4.2 is algorithmic, and is based on several layers of reductions. ", "page_idx": 40}, {"type": "text", "text": "\u2022 First, using the scheme in Appendix D, we reduce the problem of oracle-efficient $\\mathsf{D}\\Big(\\tilde{f}^{t-N}\\big(x^{t-N}\\big),f\\big(x^{t-N}\\big)\\Big)$ ldaeyfeidne gloeraitrhnimn g2 , wwithher et $\\begin{array}{r}{\\tilde{f}^{t-N}\\,=\\,\\frac{1}{N}\\sum_{i=t-N+1}^{t}\\widehat{f}^{i}}\\end{array}$ $\\begin{array}{r l}{\\ell^{t-N}(f)}&{{}=}\\end{array}$ average of offline estimators and $N\\in\\mathbb{N}$ \u2022 Then, using a standard reduction [67, 42, 35, 47], we reduce the delayed online learning problem above to a sequence of $N$ non-delayed online learning problems, with the same sequence of loss functions; both this and the preceding step are computationally efficient. \u2022 To complete the reduction, we argue that the base algorithm can be used to solve the online learning problem above in an oracle-efficient fashion. To do this, we simulate interaction with the environment by sampling fictitious outcomes $y^{t}\\sim\\tilde{f}^{t}(x^{t})$ from the averaged offilne estimators and passing them into the base algorithm. We argue that the fictitious outcomes approximate the true outcomes well through a change-of-measure argument. ", "page_idx": 40}, {"type": "text", "text": "Combining the above, we conclude that given any base algorithm that efficiently performs online estimation with outcomes sampled from the target parameter $f^{\\star}$ , we can efficiently construct a computationally efficient and oracle-efficient algorithm. In more detail, we introduce four layers of reduction in reverse order from CDE to OEOE. ", "page_idx": 40}, {"type": "text", "text": "Conditional Density Estimation with Reference Outcomes (CDEwRO). The bottom-most reduction we consider is from a setting we refer to as Conditional Density Estimation with Reference Outcomes $(C D E w R O)$ to the (realizable) CDE setting. CDEwRO is similar to CDE, but with the following difference. Instead of receiving outcomes $y^{1},\\ldots,y^{T}$ sampled from the true model $f^{\\star}(x^{t})$ directly, in CDEwRO, the outcome is sampled from a reference parameter $\\tilde{f}^{t}(\\boldsymbol{x}^{t})$ which is guaranteed to be close to $f^{\\star}$ in a certain sense. Moreover, the covariates and the reference parameters $\\tilde{f}^{1},\\dots,\\tilde{f}^{T}$ are selected obliviously (i.e. the entire sequence is chosen by the adversary before the online learning protocol begins). ", "page_idx": 41}, {"type": "table", "img_path": "sks7x4I8Bh/tmp/9ca945b7257a0d9cc6fe7a0de222be77ecbcf7353a9ca6cb3d161b6b147f6329.jpg", "table_caption": [], "table_footnote": [], "page_idx": 41}, {"type": "text", "text": "Our reduction from CDEwRO to CDE is given in Algorithm 6. The main guarantee for this reduction is as follows. ", "page_idx": 41}, {"type": "text", "text": "Lemma H.1. For any fixed $\\zeta\\geq0$ , suppose $\\sum_{t=1}^{T}\\mathsf{D}_{\\mathsf{H}}^{2}\\Big(\\widetilde{f}^{t}\\big(x^{t}\\big),f^{\\star}(x^{t})\\Big)\\le\\zeta.\\;L e t$ ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R_{\\sf C D E w R O}(T,\\zeta):=3C_{\\mathcal F}\\log V\\cdot\\zeta+R_{\\sf C D E}(T)+2C_{\\mathcal F}\\cdot\\log(C_{\\mathcal F}T),}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $R_{\\mathsf{C D E}}(T)$ is defined as in Eq. (4) by the assumption on $\\mathcal{A}_{\\sf C D E}$ . Then Algorithm 6 achieves an expected online estimation error upper bound of ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}\\big[\\mathsf{D}_{\\mathsf{H}}^{2}\\big(\\bar{f}^{t}\\big(x^{t}\\big),f^{\\star}(x^{t})\\big)\\big]\\leq R_{\\mathsf{C D E w R O}}(T,\\zeta)\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "in the CDEwRO setting, and has runtime po $\\mathrm{ly}(\\mathsf{T i m e}(\\mathcal{F},T),T)$ ", "page_idx": 41}, {"type": "text", "text": "The key technique in the proof of this lemma is a change of measure argument based on DonskerVaradhan [46]. ", "page_idx": 41}, {"type": "text", "text": "Conditional Density Estimation with Reference Parameters (CDEwRP). The next reduction in our stack is from a setting we refer to as Conditional Density Estimation with Reference Parameters $(C D E w R P)$ to the CDEwRO setting above. CDEwRP is identical to CDEwRO, except that in the former setting, the learner directly observes the reference parameter $\\tilde{f}^{t}$ instead of observing $y^{t}\\sim\\tilde{f}^{t}(x^{t})$ as in CDEwRO. ", "page_idx": 41}, {"type": "text", "text": "A second difference is that we allow the adversary in the CDEwRP setting to be adaptive , while our definition of the CDEwRO setting only allows for oblivious adversaries. Thus, the reduction we consider serves two purposes: ", "page_idx": 41}, {"type": "text", "text": "\u2022 Simulating the CDEwRO feedback model through sampling.   \n\u2022 Reducing the adaptive adversary to an oblivious one. ", "page_idx": 41}, {"type": "text", "text": "The reduction from adaptive adversaries to oblivious follows and improves upon the result from [31], and may be of independent interest. ", "page_idx": 41}, {"type": "text", "text": "Our reduction from CDEwRP to CDEwRO is displayed in Algorithm 6, and takes as input an algorithm $\\mathcal{A}_{\\sf C D E w R O}(\\cdot;\\cdot)$ for the CDEwRO setting, where $\\mathcal{A}_{\\mathsf{C D E w R O}}^{t}(T;\\cdot)$ denotes the algorithm\u2019s output at round $t\\leq T$ as a function of the history. The main guarantee for the algorithm is as follows. ", "page_idx": 41}, {"type": "text", "text": "Lemma H.2. For any $\\mathrm{i}x e d\\,\\zeta\\geq0,\\,s u p p o s e\\sum_{t=1}^{T}\\mathsf{D}_{\\mathsf{H}}^{2}\\Big(\\widetilde{f}^{t}(x^{t}),f^{\\star}(x^{t})\\Big)\\leq\\zeta.\\ L e t$ ", "page_idx": 41}, {"type": "equation", "text": "$$\nR_{\\mathsf{C D E w R P}}(T,\\zeta,\\varepsilon):=2R_{\\mathsf{C D E w R O}}(T,\\zeta)+\\varepsilon,\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Algorithm 7 Reduction from CDEwRP to CDEwRO ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "1: input: Time $T\\in\\mathbb N$ , accuracy parameter $\\varepsilon>0$ , algorithm ACDEwRO.   \n2: Let $L=O(T\\log(T|\\mathcal{F}||\\mathcal{X}|)/\\varepsilon)$ .   \n3: for $t=1,\\dots,T$ do   \n4: for $i=1,\\dots,L$ do   \n5: for $s=1,\\ldots,t-1$ do   \n6: Learner samples $y_{i}^{s,t}\\sim\\tilde{f}^{s}(x^{s})$ .   \n7: Learner computes $\\bar{f}_{i}^{t}=\\mathcal{A}_{\\sf C D E w R O}(T;x_{i}^{1,t},\\cdot\\cdot\\cdot,x_{i}^{t-1,t},y_{i}^{1,t},\\cdot\\cdot\\cdot,y_{i}^{t-1,t}).$   \n8: Learner predicts via $\\bar{f}^{t}\\sim\\mu^{t}=\\mathrm{Unif}\\big(\\{\\bar{f}_{i}^{t}\\}_{i\\in[L]}\\big)$ .   \n9: Nature selects and reveals the covariate $x^{t}$ and the reference parameter $\\tilde{f}^{t}$ based on $\\mu^{t}$ . ", "page_idx": 42}, {"type": "text", "text": "where $R_{\\mathsf{C D E w R O}}(T,\\zeta)$ is defined as in $E q$ . (25). Then Algorithm 7 with parameter $\\varepsilon>0$ achieves an expected online estimation error upper bound of ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}\\big[\\mathsf{D}_{\\mathsf{H}}^{2}\\big(\\bar{f}^{t}(x^{t}),f^{\\star}(x^{t})\\big)\\big]\\leq R_{\\mathsf{C D E w R P}}(T,\\zeta,\\varepsilon)\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "in the CDEwRP setting, and runs in time poly $(\\mathsf{T i m e}(\\mathcal{F},T),T,\\log|\\mathcal{F}|,\\log|\\mathcal{X}|,1/\\varepsilon)$ . The distributions $\\boldsymbol{\\mu}^{1},\\ldots,\\boldsymbol{\\mu}^{T}$ produced by the algorithm have support size p $\\operatorname{oly}(\\log|\\mathcal{F}|,\\log|\\mathcal{X}|,T,1/\\varepsilon)$ . ", "page_idx": 42}, {"type": "text", "text": "Conditional Density Estimation with Delayed Reference Parameters (CDEwDRP). Our next reduction is from a setting we refer to as Conditional Density Estimation with Delayed Reference Parameters $(C D E w D R P)$ to the CDEwRP setting. CDEwDRP is identical to CDEwRP, except that the reference function $\\tilde{f}^{t}$ is revealed only at round $t+N$ instead of at round $t$ , for a delay parameter $N\\in\\mathbb{N}$ . ", "page_idx": 42}, {"type": "text", "text": "Algorithm 8 Reduction from CDEwDRP to CDEwRP ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "12::  iInnipt $T\\in\\mathbb N$ o, fd tehlea ya ltigmorei $N\\in\\mathbb{N}$ itahs ${\\mathcal{A}}_{\\mathsf{C D E w R P}}$ $N$ ${\\mathcal{A}}_{\\mathsf{C D E w R P}}$ $\\mathcal{A}_{\\sf C D E w R P}^{1},\\dotsc,\\mathcal{A}_{\\sf C D E w R P}^{N}.$ $t=1,\\dots,T$   \n4: Learner predicts $\\bar{f}^{t}\\sim\\mu^{t}=\\mathcal{A}_{\\sf C D E w R P}^{i}(T/N,1/N;x^{i},x^{i+N},\\ldots,x^{t-N},\\tilde{f}^{i},\\tilde{f}^{i+N},\\ldots,\\tilde{f}^{t-N})$ where $i\\equiv t$ mod $N$ .   \n5: Nature selects and reveals the covariate $x^{t}$ and the reference parameter $\\tilde{f}^{t-N}$ based on $\\mu^{t}$ . ", "page_idx": 42}, {"type": "text", "text": "Our reduction from CDEwDRP to CDEwRP is displayed in Algorithm 7, and takes as input an algorithm $\\mathcal{A}_{\\sf C D E w R P}(\\cdot;\\cdot)$ for the CDEwRP setting, where $\\mathcal{A}_{\\sf C D E w R P}^{t}(T,\\varepsilon;\\cdot)$ denotes the algorithm\u2019s output at round $t\\leq T$ with accuracy parameter $\\varepsilon>0$ (cf. Algorithm 7), as a function of the history. The main guarantee for the algorithm is as follows. ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathrm{~ama~H.3.~}\\,F o r\\,a n y\\,f\\imath x e d\\,\\zeta\\geq0,\\,s u p p o s e\\sum_{t=1}^{T}\\mathsf{D}_{\\mathsf{H}}^{2}\\Big(\\widetilde{f}^{t}\\big(x^{t}\\big),f^{\\star}\\big(x^{t}\\big)\\Big)\\leq\\zeta.\\ L e t\n$$", "text_format": "latex", "page_idx": 42}, {"type": "equation", "text": "$$\nR_{\\mathsf{C D E w D R P}}(T,N,\\zeta):=\\operatorname*{sup}_{\\zeta_{1},\\ldots,\\zeta_{N}\\geq0,\\sum_{i=1}^{N}\\zeta_{i}\\leq\\zeta}\\sum_{i=1}^{N}R_{\\mathsf{C D E w R P}}(T/N,\\zeta_{i},1/N),\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $R_{\\mathsf{C D E w R P}}(T/N,\\zeta_{i},1/N)$ is defined as in Eq. (26). Then Algorithm 8 achieves an expected online estimation error upper bound of ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}\\big[\\mathsf{D}_{\\mathsf{H}}^{2}\\big(\\bar{f}^{t}(x^{t}),f^{\\star}(x^{t})\\big)\\big]\\leq R_{\\mathsf{C D E w D R P}}(T,N,\\zeta)\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "in the CDEwDRP setting, and has runtime poly $(\\mathsf{T i m e}({\\mathcal F},T),T,\\log|{\\mathcal F}|,\\log|{\\mathcal X}|)$ . The distributions $\\boldsymbol{\\mu}^{1},\\ldots,\\boldsymbol{\\mu}^{T}$ produced by the algorithm have support size po $\\mathrm{ly}(\\log|\\mathcal{F}|,\\log|\\mathcal{X}|,T)$ . ", "page_idx": 42}, {"type": "text", "text": "Algorithm 9 Reduction from OEOE to CDEwDRP ", "page_idx": 43}, {"type": "text", "text": "1: input: Time $T\\in\\mathbb N$ , offilne estimation oracle $\\mathbf{Alg}_{0\\mathsf{f f}}$ with parameter $\\beta_{0\\mathsf{f}\\mathsf{f}}\\geq0$ , delay parameter   \n$N\\in\\mathbb{N}$ , CDEwDRP algorithm AODEwDRP.   \n2: for $t=1,\\dots,T$ do   \n3: Receive $\\widehat{f^{t}}=\\mathbf{A}\\mathbf{l}\\mathbf{g}_{\\mathsf{O f f}}^{t}\\big(x^{1},\\ldots,x^{t-1},y^{1},\\ldots,y^{t-1}\\big).$ .   \n4: Learner computes reference parameter $\\tilde{f}^{t-N}=\\frac{1}{N}\\sum_{i=t-N+1}^{t}\\widehat{f}^{i}$ .   \n5: Learner predicts $\\bar{f}^{t}\\sim\\mu^{t}=\\ensuremath{\\mathcal{A}_{\\sf O D E w D R P}}(T,N;x^{1},x^{2},\\dots,x^{t},\\tilde{f}^{1},\\tilde{f}^{2},\\dots,\\tilde{f}^{t-N}).$   \n6: Nature selects and reveals the covariate $x^{t}$ based on $\\mu^{t}$ . ", "page_idx": 43}, {"type": "text", "text": "Oracle-Efficient Online Estimation (OEOE). Our final reduction reduces the Oracle-Efficient Online Estimation setting (OEOE) to the CDEwDRP setting described above. This reduction, which is displayed in Algorithm 8, is a variant of the approach used in Theorem D.1. The reduction takes as input a CDEwDRP algorithm $\\mathcal{A}_{\\sf O D E w D R P}(\\cdot;\\cdot)$ , where ${\\mathcal A}_{\\sf O D E w D R P}^{t}(T,N;\\cdot)$ denotes the algorithm\u2019s output at round $t\\leq T$ with delay parameter $N$ , as a function of the history. ", "page_idx": 43}, {"type": "text", "text": "Lemma H.4. Algorithm 9 achieves an expected online estimation error upper bound of ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}\\big[\\mathsf{D}_{\\mathsf{H}}^{2}\\big(\\bar{f}^{t}(x^{t}),f^{\\star}(x^{t})\\big)\\big]\\leq R_{\\mathsf{C D E w D R P}}(T,N,N+\\beta_{\\mathsf{O f f}}T/N)\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "in the OEOE setting, and has runtime p $)\\mathrm{ly}(\\mathsf{T i m e}({\\mathcal F},T),T,\\log|{\\mathcal F}|,\\log|{\\mathcal X}|)$ . The distributions $\\boldsymbol{\\mu}^{1},\\ldots,\\boldsymbol{\\mu}^{T}$ produced by the algorithm have support size p $\\mathrm{oly}(\\log|\\mathcal{F}|,\\log|\\mathcal{X}|,T)$ . ", "page_idx": 43}, {"type": "text", "text": "Completing the proof of Theorem 4.2. To prove Theorem 4.2, we compose all of the preceding reductions, with $N$ left as a free parameter temporarily. We first apply Lemma H.4 to reduce from the OEOE setting to the CDEwDRP setting, with the guarantee that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}\\big[\\mathsf{D}_{\\mathsf{H}}^{2}\\big(\\bar{f}^{t}(x^{t}),f^{\\star}(x^{t})\\big)\\big]\\leq R_{\\mathsf{C D E w D R P}}(T,N,N+\\beta_{\\mathsf{O f f}}T/N).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Then by Lemma H.3, we can reduce the CDEwDRP setting to the CDEwRP setting, with the guarantee by Eq. (27) that ", "page_idx": 43}, {"type": "equation", "text": "$$\nR_{\\sf C D E w D R P}(T,N,N+\\beta_{\\sf O f f}T/N)=\\operatorname*{sup}_{\\sum_{i=1}^{N}\\zeta_{i}\\leq N+\\beta_{\\sf o f f}/N}\\sum_{i=1}^{N}R_{\\sf C D E w R P}(T/N,\\zeta_{i},1/N).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Then apply Lemma $H.2~N$ times with $T,\\zeta$ , and $\\varepsilon$ in the Lemma chosen to be $T/N$ , $\\zeta_{i}$ , and $1/N$ respectively for each $i\\,\\in\\,[N]$ , we can reduce the CDEwRP setting to the CDEwRO setting with guarantee by Eq. (26) that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\sum_{i=1}^{N}\\zeta_{i}\\leq N+\\beta_{\\sf o f f}/N}\\sum_{i=1}^{N}R_{\\sf C D E w R P}(T/N,\\zeta_{i},1/N)=1+\\operatorname*{sup}_{\\sum_{i=1}^{N}\\zeta_{i}\\leq N+\\beta_{\\sf o f f}/N}2\\sum_{i=1}^{N}R_{\\sf C D E w R O}(T/N,\\zeta_{i}).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Consequently, apply Lemma H.1 $N$ times with $T$ and $\\zeta$ in the Lemma chosen to be $T/N$ and $\\zeta_{i}$ for each $i\\in[N]$ , we can reduce the CDEwRO setting to the CDE setting with guarantee by Eq. (25) that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{\\displaystyle\\operatorname*{sup}_{i=1}\\sum_{\\ell_{ \u1e0a }\\leqq \u1e0a }\\ell_{ \u1e0a }}^{N}R_{\\mathrm{\u1e0a}\\mathrm{\u1e0a}f \u1e0c \u1e0c }\\mathrm{cost\u1e0c}(T/N,\\zeta_{i})}\\\\ {\\displaystyle\\sum_{i=1}^{N}\\zeta_{i}\\leq N+\\beta_{\\sf o n}T/N^{\\prime}\\,i=1}\\\\ {\\displaystyle\\leq\\operatorname*{sup}_{i=1}\\sum_{\\ell_{ \u1e0a }\\leqq \u1e0a }\\mathrm{\u1e0a}\\xi_{ \u1e0a }i \u1e0c \u1e0c +1 \u1e0c ^{N}(3C_{\\mathcal \u1e0a F \u1e0c }\\log V\\cdot\\zeta_{i}+R_{\\mathrm{C\u1e0a}\\in \u1e0a }}(T)+2C_{\\mathcal \u1e0a F \u1e0c }\\cdot\\log(C_{\\mathcal \u1e0a F \u1e0c }T))}\\\\ {\\displaystyle\\overset{\\sum}{_{i=1}^{N}}\\zeta_{i}\\leq N+\\beta_{\\sf o n}T/N^{\\prime}\\,i=1}\\\\ {\\displaystyle\\lesssim C_{\\mathcal \u1e0a F \u1e0c }\\log V\\cdot\\bigg(N+\\frac{\\beta_{\\sf o t \u1e0a }T \u1e0c }{N}\\bigg)+N\\cdot(R_{\\sf c \u1e0a }(T)+C_{\\mathcal \u1e0a F \u1e0c }\\cdot\\log(C_{\\mathcal \u1e0a F \u1e0c }T))}\\\\ {\\displaystyle=C_{\\mathcal \u1e0a F \u1e0c }\\log V\\cdot\\frac{\\beta_{\\sf o t \u1e0c }{N}}{N}+N\\cdot(R_{\\sf c \u1e0a }(T)+C_{\\mathcal \u1e0a F \u1e0c }\\cdot\\log(V C_{\\mathcal \u1e0a F \u1e0c }T)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "By choosing $\\begin{array}{r}{N=\\sqrt{\\frac{C_{\\mathcal{F}}\\beta_{\\mathrm{Off}}T\\cdot\\log V}{R_{\\mathrm{CDE}}(T)+C_{\\mathcal{F}}\\cdot\\log(V C_{\\mathcal{F}}T)}}\\vee1}\\end{array}$ , we can bound the expression in (28) as ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r l}&{\\operatorname{\\{Est}_{H}(T,\\beta_{\\mathrm{Off}})\\big]\\lesssim\\sqrt{C_{\\mathcal{F}}\\beta_{\\mathrm{Off}}T(R_{\\mathrm{CD}\\mathcal{E}}(T)+C_{\\mathcal{F}}\\cdot\\log(V C_{\\mathcal{F}}T))\\log V}+R_{\\mathrm{CD}\\mathcal{E}}(T)+C_{\\mathcal{F}}\\cdot\\log(V C_{\\mathcal{F}}T)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\sqrt{C_{\\mathcal{F}}\\beta_{\\mathrm{Off}}T R_{\\mathrm{CD}\\mathcal{E}}(T)\\log V}+C_{\\mathcal{F}}\\sqrt{\\beta_{\\mathrm{Off}}T\\log(V C_{\\mathcal{F}}T)\\log V}}\\\\ &{\\qquad\\qquad\\qquad+R_{\\mathrm{CD}\\mathcal{E}}(T)+C_{\\mathcal{F}}\\cdot\\log(V C_{\\mathcal{F}}T).}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Finally, under the assumption that $R_{\\mathsf{C D E}}(T)\\leq C_{\\mathcal{F}}^{\\prime}\\log T$ , the bound above can be further simplified as ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\mathbf{Est}_{\\mathbb{H}}(T,\\beta_{\\mathrm{Of}})]\\lesssim(C_{\\mathcal{F}}(C_{\\mathcal{F}}+C_{\\mathcal{F}}^{\\prime})\\beta_{\\mathrm{Of}}\\log V\\log(V C_{\\mathcal{F}}T))^{1/2}T^{1/2}+(C_{\\mathcal{F}}+C_{\\mathcal{F}}^{\\prime})\\log(V C_{\\mathcal{F}}T).}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "H.2.2 Proofs for supporting lemmas ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Proof of Lemma H.1. For Algorithm 6, denote the randomness of the sequence $\\left(y^{1:T},\\mu^{1:T}\\right)$ under f \u22c6\u2208F and ( f\u02dc 1:T) by Pf \u22c6and P f\u02dc 1:T respectively. The data generating process for $(x^{1:T},y^{1:T})$ in the CDEwRO setting implies that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbb{P}^{f^{1:T}}}\\left[\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}^{t}\\sim\\mu^{t}}\\big[\\mathsf{D}_{\\mathsf{H}}^{2}\\big(\\bar{f}^{t}(x^{t}),f^{\\star}(x^{t})\\big)\\big]\\right]=\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}^{t}\\sim\\mu^{t}}\\big[\\mathsf{D}_{\\mathsf{H}}^{2}\\big(\\bar{f}^{t}(x^{t}),f^{\\star}(x^{t})\\big)\\big]\\right].\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "By Donsker-Varadhan [46], we have that for all $\\eta>0$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac1\\eta\\mathsf{D}_{\\mathsf{K L}}\\Big(\\mathbb{P}^{\\tilde{f}^{\\mathrm{1}:T}}\\left\\|\\mathbb{P}^{f^{\\star}}\\right)\\ge\\mathbb{E}_{\\mathbb{P}^{\\tilde{f}^{\\mathrm{1}:T}}}\\left[\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}^{t}\\sim\\mu^{t}}\\left[\\mathsf{D}_{\\mathsf{H}}^{2}\\big(\\bar{f}^{t}(x^{t}),f^{\\star}(x^{t})\\big)\\right]\\right]}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad-\\,\\frac{1}{\\eta}\\log\\mathbb{E}_{\\mathbb{P}^{f\\star}}\\exp\\!\\Bigg\\{\\eta\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}^{t}\\sim\\mu^{t}}\\left[\\mathsf{D}_{\\mathsf{H}}^{2}\\big(\\bar{f}^{t}(x^{t}),f^{\\star}(x^{t})\\big)\\right]\\Bigg\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "For any random variables $X,Y,Z$ , we denote by ${\\mathsf{D}}_{\\mathsf{K L}}(\\mathbb{P}_{X}\\parallel\\mathbb{P}_{Y}\\mid Z)=\\mathbb{E}_{Z}[{\\mathsf{D}}_{\\mathsf{K L}}\\big(\\mathbb{P}_{X\\mid Z}\\parallel\\mathbb{P}_{Y\\mid Z}\\big)]$ . We further note that by the chain rule for KL divergence, ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{D}_{\\mathsf{K L}}\\Bigl(\\mathbb{P}^{\\tilde{f}^{1:T}}\\parallel\\mathbb{P}^{f^{\\star}}\\Bigr)=\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{\\mathbb{P}^{\\tilde{f}^{1:T}}}\\left[\\mathsf{D}_{\\mathsf{K L}}\\Bigl(\\mathbb{P}_{(x^{t},y^{t})}^{\\tilde{f}^{1:T}}\\parallel\\mathbb{P}_{(x^{t},y^{t})}^{f^{\\star}}\\mid x^{1:t-1},y^{1:t-1}\\Bigr)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\sum_{t=1}^{T}\\mathsf{D}_{\\mathsf{K L}}\\Bigl(\\tilde{f}^{t}(x^{t})\\parallel f^{\\star}(x^{t})\\Bigr),}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where the second equality holds because $y^{t}$ follows $\\tilde{f}^{t}(\\boldsymbol{x}^{t})$ and $f^{\\star}(x^{t})$ respectively, and because the conditional distribution of $x^{t}$ is identical under both laws due to the oblivious assumption of the ", "page_idx": 44}, {"type": "text", "text": "covariates in the setting of CDEwRO. Then by the relation between KL and Hellinger (Lemma A.10 of [27]) and that $1\\leq\\log V$ , we have ", "page_idx": 45}, {"type": "text", "text": "$\\sum_{i=1}^{T}\\mathsf{D}_{\\mathsf{K L}}\\left(\\tilde{f}^{t}(x^{t})\\,\\|\\,f^{\\star}(x^{t})\\right)\\le(2+\\log V)\\cdot\\sum_{t=1}^{T}\\mathsf{D}_{\\mathsf{H}}^{2}\\Big(\\tilde{f}^{t}(x^{t}),f^{\\star}(x^{t})\\Big)\\le3\\log V\\cdot\\sum_{t=1}^{T}\\mathsf{D}_{\\mathsf{H}}^{2}\\Big(\\tilde{f}^{t}(x^{t}),f^{\\star}(x^{t})\\Big)\\,,$ t) , where the last inequality is by $V\\geq e$ . Combining all of the results so far and using (29), we have $\\begin{array}{r}{\\begin{array}{r l}&{\\mathbb{\\Sigma}\\displaystyle\\left[\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}^{t}\\sim\\mu^{t}}\\left[\\mathsf{D}_{\\mathbb{H}}^{2}\\big(\\bar{f}^{t}(x^{t}),f^{\\star}(x^{t})\\big)\\right]\\right]\\leq\\frac{3\\log V}{\\eta}\\cdot\\sum_{t=1}^{T}\\mathsf{D}_{\\mathbb{H}}^{2}\\Big(\\tilde{f}^{t}(x^{t}),f^{\\star}(x^{t})\\Big)}\\\\ &{\\phantom{\\leq\\frac{1}{\\eta}\\log\\mathbb{E}_{\\mathbb{P}^{f}}\\cdot\\exp\\left\\{\\eta\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}^{t}\\sim\\mu^{t}}\\left[\\mathsf{D}_{\\mathbb{H}}^{2}\\big(\\bar{f}^{t}(x^{t}),f^{\\star}(x^{t})\\big)\\right]\\right\\}}.}\\end{array}}\\end{array}$ ", "page_idx": 45}, {"type": "text", "text": "To proceed, using that for any postive random variable $\\begin{array}{r}{X,\\mathbb{E}[X]=\\int_{0}^{\\infty}\\mathbb{P}(X\\geq t){\\mathrm{d}}t}\\end{array}$ , we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{1}{I}\\log\\mathbb{E}_{\\mathbb{P}^{f\\star}}\\exp\\Bigg\\{\\eta\\sum_{t=1}^{T}\\mathbb{E}_{\\tilde{f}^{t}\\sim\\mu^{t}}\\left[\\mathsf{D}_{\\mathsf{H}}^{2}\\big(\\tilde{f}^{t}(x^{t}),f^{\\star}(x^{t})\\big)\\right]\\Bigg\\}}\\qquad}&{}\\\\ &{=R_{\\mathrm{CD}}(T)+\\frac{1}{\\eta}\\log\\mathbb{E}_{\\mathbb{P}^{f\\star}}\\exp\\Bigg\\{\\eta\\Bigg(\\sum_{t=1}^{T}\\mathbb{E}_{\\tilde{f}^{t}\\sim\\mu^{t}}\\left[\\mathsf{D}_{\\mathsf{H}}^{2}\\big(\\bar{f}^{t}(x^{t}),f^{\\star}(x^{t})\\big)\\right]-R_{\\mathrm{CD}}(T)\\Bigg)\\Bigg\\}}\\\\ &{\\leq R_{\\mathrm{CD}}(T)+\\frac{1}{\\eta}\\log\\mathbb{E}_{\\mathbb{P}^{f\\star}}\\exp\\Bigg\\{\\eta\\Bigg(\\sum_{t=1}^{T}\\mathbb{E}_{\\tilde{f}^{t}\\sim\\mu^{t}}\\left[\\mathsf{D}_{\\mathsf{H}}^{2}\\big(\\bar{f}^{t}(x^{t}),f^{\\star}(x^{t})\\big)\\right]-R_{\\mathrm{CD}}(T)\\Bigg)_{+}\\Bigg\\}}\\\\ &{=R_{\\mathrm{CD}}(T)+\\frac{1}{\\eta}\\log\\int_{0}^{\\infty}\\mathbb{P}^{f^{\\star}}\\Bigg(\\Bigg(\\sum_{t=1}^{T}\\mathbb{E}_{\\tilde{f}^{t}\\sim\\mu^{t}}\\left[\\mathsf{D}_{\\mathsf{H}}^{2}\\big(\\bar{f}^{t}(x^{t}),f^{\\star}(x^{t})\\big)\\right]-R_{\\mathrm{CD}}(T)\\Bigg)_{+}\\geq\\frac{1}{\\eta}\\log\\Bigg.}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Recall the assumption $\\sum_{t=1}^{T}\\mathsf{D}_{\\mathsf{H}}^{2}\\Big(\\tilde{f}^{t}(x^{t}),f^{\\star}(x^{t})\\Big)\\le\\zeta$ and let $\\begin{array}{r}{\\eta=\\frac{1}{C_{\\mathcal{F}}}}\\end{array}$ . We have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{3\\log V}{\\eta}\\cdot\\sum_{t=1}^{T}\\mathsf{D}_{\\mathsf{H}}^{2}\\Big(\\widetilde f^{t}(x^{t}),f^{\\star}(x^{t})\\Big)+\\frac{1}{\\eta}\\log\\mathbb{E}_{\\mathcal P^{\\prime\\star}}\\exp\\Bigg\\{\\eta\\sum_{t=1}^{T}\\mathbb{E}_{f^{t}\\sim\\mu^{t}}\\left[\\mathsf{D}_{\\mathsf{H}}^{2}\\big(\\widetilde f^{t}(x^{t}),f^{\\star}(x^{t})\\big)\\right]\\Bigg\\}}\\\\ &{\\le\\displaystyle\\frac{3\\log V}{\\eta}\\cdot\\zeta+R_{\\mathrm{CD}}\\xi(T)}\\\\ &{\\qquad\\qquad+\\frac{1}{\\eta}\\log\\int_{0}^{\\infty}\\mathbb{P}^{f^{\\star}}\\Bigg(\\left(\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{\\widetilde f^{t}\\sim\\mu^{t}}\\left[\\mathsf{D}_{\\mathsf{H}}^{2}\\big(\\widetilde f^{t}(x^{t}),f^{\\star}(x^{t})\\big)\\right]-R_{\\mathrm{CD}}\\mathfrak{E}(T)\\right)_{+}\\ge\\frac{1}{\\eta}\\log t\\Bigg)\\mathrm{d}t}\\\\ &{\\le3C_{\\mathcal{F}}\\log V\\cdot\\zeta+R_{\\mathrm{CD}}\\mathfrak{E}(T)+C_{\\mathcal{F}}\\log\\Bigg(1+\\int_{1}^{e^{C_{\\mathcal{F}}\\cdot T}}\\frac{1}{t}\\mathrm{d}t\\Bigg)}\\\\ &{\\le3C_{\\mathcal{F}}\\log V\\cdot\\zeta+R_{\\mathrm{CD}}\\mathfrak{E}(T)+2C_{\\mathcal{F}}\\cdot\\log(C_{\\mathcal{F}}T).}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where the second inequality uses the assumption Eq. (4) on the algorithm. ", "page_idx": 45}, {"type": "text", "text": "Proof of Lemma H.2. Our result improves uses the proof technique from the adversarial-to-oblivious reduction in Lemma 11 of Gonen et al. [31], but improves the result by a ${\\cal O}(\\log T)$ factor. ", "page_idx": 45}, {"type": "text", "text": "Consider the CDEwRP setting. Let $\\begin{array}{r l r}{\\mathbb{E}^{t}[\\cdot]}&{{}:=}&{\\mathbb{E}[}\\end{array}$ \u00b7 | $x^{1:t-1},\\tilde{f}^{1:t-1}]$ . Let $\\mu^{s,t}:=$ $\\mathbb{E}^{t}[A_{\\mathsf{C D E w R O}}(y_{1}^{1,t},\\ldots,y_{1}^{s-1,t},x_{1}^{1,t},\\ldots,x_{1}^{s-1,t})]$ for all $1\\,\\leq\\,s\\,\\leq\\,t\\,\\leq\\,T$ where the expectation is taken over all the random variables $y_{1}^{1,t},\\stackrel{\\cdot}{\\dots}\\cdot,y_{1}^{\\bar{s}-1,t},x_{1}^{1,t},\\dotsc,x_{1}^{s-1,t}$ ", "page_idx": 45}, {"type": "text", "text": "Then by Bernstein\u2019s concentration inequality applied to $\\mu^{t}$ (interpreted as an empirical approximation to $\\mu^{t,t},$ ), conditioned on $x^{1:t-1},\\tilde{f}^{1:t-1}$ , we have with probability at least $1-\\frac{\\varepsilon}{2T}$ , for all $x^{\\prime}\\in\\mathcal{X}$ and $f^{\\prime}\\in\\mathcal{F}$ , ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{f\\sim\\mu^{t}}^{t}\\left[\\mathsf{D}_{\\mathsf{H}}^{2}(f(x),f^{\\prime}(x))\\right]\\leq2\\,\\mathbb{E}_{f\\sim\\mu^{t,t}}^{t}\\left[\\mathsf{D}_{\\mathsf{H}}^{2}(f(x),f^{\\prime}(x))\\right]+\\varepsilon/(2T).}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "For any fixed $t\\ \\ \\in\\ \\ [T]$ and all $t^{\\prime}$ such that $t\\ \\ \\leq\\ t^{\\prime}\\ \\ \\leq\\ T.$ , the different trajectories $x_{1}^{1,t^{\\prime}},\\ldots,x_{1}^{t-1,t^{\\prime}},y_{1}^{1,t^{\\prime}},\\ldots,y_{1}^{t-1,t^{\\prime}}$ are i.i.d. conditioned on $x^{1:t-1},\\tilde{f}^{1:t-1}$ . Thus, we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\big[\\mathbb{E}_{f\\sim\\mu^{t,t}}^{t}\\big[\\mathsf{D}_{\\mathsf{H}}^{2}(f(x^{t}),f^{\\star}(x^{t}))\\big]\\big]=\\mathbb{E}\\Big[\\mathbb{E}_{f\\sim\\mu^{t,t+1}}^{t+1}\\big[\\mathsf{D}_{\\mathsf{H}}^{2}\\big(f(x^{t}),f^{\\star}(x^{t})\\big)\\big]\\Big]}\\\\ &{\\phantom{=\\ x}=\\ldots}\\\\ &{\\phantom{\\sum}=\\mathbb{E}\\Big[\\mathbb{E}_{f\\sim\\mu^{t,T}}^{T}\\big[\\mathsf{D}_{\\mathsf{H}}^{2}\\big(f(x^{t}),f^{\\star}(x^{t})\\big)\\big]\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Finally, for Algorithm 7, we have by the guarantee of ACDEwRO, ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}\\Big[\\mathbb{E}_{f\\sim\\mu^{t,T}}^{T}\\big[\\mathsf{D}_{\\mathsf{H}}^{2}\\big(f(x^{t}),f^{\\star}(x^{t})\\big)\\big]\\Big]\\leq R_{\\mathsf{C D E w R O}}(T,\\zeta).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Combining the three results above, we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\big[\\mathbb{E}_{f\\sim\\mu^{t}}^{t}\\big[\\mathsf{D}_{\\mathsf{H}}^{2}(f(x^{t}),f(x^{t}))\\big]\\big]\\leq2\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\big[\\mathbb{E}_{f\\sim\\mu^{t,t}}^{t}\\big[\\mathsf{D}_{\\mathsf{H}}^{2}(f(x^{t}),f^{\\star}(x^{t}))\\big]\\big]+\\varepsilon}\\\\ {\\displaystyle}&{\\qquad=2\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}\\Big[\\mathbb{E}_{f\\sim\\mu^{t,T}}^{T}\\big[\\mathsf{D}_{\\mathsf{H}}^{2}(f(x^{t}),f^{\\star}(x^{t}))\\big]\\Big]+\\varepsilon}\\\\ {\\displaystyle}&{\\qquad\\leq2R_{\\mathsf{C D E w R O}}(T,\\zeta)+\\varepsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where the first equality is by Eq. (30), the second equality is from Eq. (31), and the final inequality is by the guarantee of $\\mathcal{A}_{\\sf C D E w R O}$ . ", "page_idx": 46}, {"type": "text", "text": "Proof of Lemma H.3. Note that Algorithm 8 is a variant of the reduction in Lemma G.3, specialized to squared Hellinger distance, and the proof here will use the same idea as Lemma G.3. ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "For each $i~\\in~[N]$ , let $\\begin{array}{r}{\\zeta_{i}\\;=\\;\\sum_{j=1}^{T/N}\\mathsf{D}_{\\mathsf{H}}^{2}\\Big(\\tilde{f}^{i+N\\cdot j}\\big(x^{i+N\\cdot j}\\big),f^{\\star}\\big(x^{i+N\\cdot j}\\big)\\Big)}\\end{array}$ . Then by the guarantee of ${\\mathcal{A}}_{\\mathsf{C D E w R P}}$ , we that for all $i\\in[N]$ , ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{T/N}\\mathbb{E}_{\\bar{f}\\sim\\mu^{i+N\\cdot j}}\\left[\\mathsf{D}_{\\mathsf{H}}^{2}\\big(\\bar{f}\\big(x^{i+N\\cdot j}\\big),f^{\\star}(x^{i+N\\cdot j})\\big)\\right]\\le R_{\\mathsf{C D E w R P}}(T/N,\\zeta_{i},1/N),\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Summing up over all $i\\in[N]$ , we obtain ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}\\sim\\mu^{t}}\\left[\\mathsf{D}_{\\mathsf{H}}^{2}\\big(\\bar{f}(x^{t}),f^{\\star}(x^{t})\\big)\\right]=\\displaystyle\\sum_{i=1}^{N}\\sum_{j=1}^{T/N}\\mathbb{E}_{\\bar{f}\\sim\\mu^{i+N\\cdot j}}\\left[\\mathsf{D}_{\\mathsf{H}}^{2}\\big(\\bar{f}(x^{i+N\\cdot j}),f^{\\star}(x^{i+N\\cdot j})\\big)\\right]}\\\\ &{\\leq\\displaystyle\\sum_{i=1}^{N}R_{\\mathsf{C D E w R P}}(T/N,\\zeta_{i},1/N).}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "By the assumption on $\\sum_{t=1}^{T}\\mathsf{D}_{\\mathsf{H}}^{2}\\Big(\\tilde{f}^{t}(x^{t}),f^{\\star}(x^{t})\\Big)$ , we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{N}\\zeta_{i}=\\sum_{i=1}^{N}\\sum_{j=1}^{T/N}\\mathsf{D}_{\\mathsf{H}}^{2}\\Big(\\widetilde{f}^{i+N\\cdot j}\\big(x^{i+N\\cdot j}\\big),f^{\\star}\\big(x^{i+N\\cdot j}\\big)\\Big)=\\sum_{t=1}^{T}\\mathsf{D}_{\\mathsf{H}}^{2}\\Big(\\widetilde{f}^{t}(x^{t}),f^{\\star}(x^{t})\\Big)\\le\\zeta.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Finally, we conclude ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}\\sim\\mu^{t}}\\big[\\mathsf{D}_{\\mathsf{H}}^{2}\\big(\\bar{f}(x^{t}),f^{\\star}(x^{t})\\big)\\big]\\leq\\operatorname*{sup}_{\\underset{i=1}{\\overset{N}{\\sum}}\\zeta_{i}\\leq\\zeta}\\sum_{i=1}^{N}R_{\\mathsf{C D E w R P}}(T/N,\\zeta_{i},1/N).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Proof of Lemma H.4. This reduction is arguably the most interesting one. This reduction is by noticing that averaging across the outputs of the offline oracle will generate reference parameters (although delayed) with small online estimation errors as shown later in Eq. (34). By the guarantee of $\\boldsymbol{\\mathcal A}$ ODEwDRP, we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}\\sim\\mu^{t}}\\left[\\mathsf{D}_{\\mathsf{H}}^{2}\\big(\\bar{f}(x^{t}),f^{\\star}(x^{t})\\big)\\right]\\leq R_{\\mathsf{C D E w D R P}}(T,N,\\zeta),\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where $\\zeta$ can be chosen to be any upper bound of $\\sum_{t=1}^{T}\\mathsf{D}_{\\mathsf{H}}^{2}\\Big(\\tilde{f}^{t}(x^{t}),f^{\\star}(x^{t})\\Big)$ since it is unknown to the learner in the setup where we augment the sequence of $\\widehat{f}^{1},\\widehat{\\sf\\Pi},\\widehat{f}^{T}$ by setting $\\widehat{f}^{\\scriptscriptstyle T+s}=\\widehat{f}^{\\scriptscriptstyle T}$ for all $s\\in\\mathbb{N}$ and define $\\begin{array}{r}{\\widetilde{f}^{t}:=\\frac{\\bar{1}}{N}\\sum_{i=t+1}^{t+N}\\widehat{f}^{i}}\\end{array}$ for $t=T-N,T-N+1,\\dots,T$ . Further m ore, by  t he definition of $\\tilde{f}^{t}$ , we can obtain ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\mathsf{D}_{\\mathsf{H}}^{2}\\Big(\\widetilde f^{t}(x^{t}),f^{\\star}(x^{t})\\Big)=N+\\displaystyle\\frac{1}{N}\\sum_{t=1}^{T-N}\\sum_{i=t+1}^{t+N}\\mathsf{D}_{\\mathsf{H}}^{2}\\big(\\bar{f}^{i}(x^{t}),f^{\\star}(x^{t})\\big)}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad=N+\\displaystyle\\frac{1}{N}\\sum_{t=2}^{T}\\sum_{i<t}\\mathsf{D}_{\\mathsf{H}}^{2}\\big(\\bar{f}^{t}(x^{i}),f^{\\star}(x^{i})\\big)}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\le N+\\displaystyle\\frac{\\beta_{0}\\mathsf{f}T}{N}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Thus, we conclude Algorithm 9 obtains ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}_{\\bar{f}\\sim\\mu^{t}}\\big[\\mathsf{D}_{\\mathsf{H}}^{2}\\big(\\bar{f}(x^{t}),f^{\\star}(x^{t})\\big)\\big]\\leq R_{\\mathsf{C D E w D R P}}(T,N,N+\\beta_{\\mathsf{O f f}}T/N).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "I Proofs from Appendix E ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Theorem E.1 (Offline-to-online conversion under coverability). For any layer-wise loss $\\mathsf{D}^{\\mathsf{R L}}$ and MDP class $(\\mathcal{M},\\Pi,\\mathcal{O})$ and $M^{\\star}\\in\\mathcal{M}$ , the sequence of estimators $(\\widehat{M^{1}},\\ldots,\\widehat{M^{\\scriptscriptstyle T}})$ produced by any offline estimation oracle $\\mathbf{Alg}_{\\mathsf{O f f}}\\,f o r\\,\\mathsf{D}^{\\mathsf{R L}}$ with parameter $\\beta_{0\\mathsf{f f}}$ satisfy ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathsf{D}^{\\mathsf{R L}}\\Bigl(\\widehat{M}^{t}(\\pi^{t})\\|M^{\\star}(\\pi^{t})\\Bigr)\\leq O\\Bigl(\\sqrt{H C_{\\mathsf{c o v}}(M^{\\star})\\beta_{\\mathsf{O f f}}T\\log T}+H C_{\\mathsf{c o v}}(M^{\\star})\\Bigr).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Proof of Theorem E.1. This proof closely follows the proof of Theorem 1 in Xie et al. [69]. Define the shorthand $d_{h}^{t}(s,a)\\equiv d_{h}^{\\pi^{t}}\\bar{(s,a)}$ and $C_{\\mathsf{c o v}}=C_{\\mathsf{c o v}}(M^{\\star})$ , and define ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\widetilde{d}_{h}^{t}(s,a):=\\sum_{\\tau=1}^{t-1}d_{h}^{\\tau}(s,a),\\quad\\mathrm{and}\\quad\\mu_{h}^{\\star}:=\\operatorname*{arg\\,min}_{\\mu_{h}\\in\\Delta(S\\times A)}\\operatorname*{sup}_{\\pi\\in\\Pi}\\left\\|\\frac{d_{h}^{\\pi}}{\\mu_{h}}\\right\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "From the definition of the layer-wise loss, we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathsf{D}^{\\mathsf{R L}}\\Big(\\widehat{M}^{t}\\big(\\pi^{t}\\big)\\Big|\\Big|M^{\\star}\\big(\\pi^{t}\\big)\\Big)=\\sum_{h=1}^{H}\\sum_{t=1}^{T}\\sum_{(s,a)\\in S\\times A}d_{h}^{t}(s,a)\\mathsf{D}_{h}\\Big(\\overline{{P}}_{h}^{\\,M^{\\star}}(s,a)\\|\\overline{{P}}_{h}^{\\,\\widehat{M}^{t}}(s,a)\\Big).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "We define a \u201cburn-in\u201d phase for each state-action pair $(s,a)\\in S\\times A$ by defining ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\tau_{h}(s,a)=\\operatorname*{min}\\left\\{t\\mid\\widetilde{d}_{h}^{t}(s,a)\\geq C_{\\mathsf{c o v}}\\cdot\\mu_{h}^{\\star}(s,a)\\right\\}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Let $h\\in[H]$ be With this definition, for we can write ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\stackrel{\\scriptstyle\\mathrm{I}}{\\mapsto=1}}^{T}\\displaystyle\\sum_{(s,a)\\in S\\times A}d_{h}^{\\prime}(s,a)\\mathsf{D}_{h}\\Big(\\overline{{P}}_{h}^{\\boldsymbol{M}^{\\star}}(s,a)\\big|\\big|\\overline{{P}}_{h}^{\\boldsymbol{M}^{\\prime}}(s,a)\\Big)}\\\\ &{=\\displaystyle\\sum_{(s,a)\\in S\\times A}\\sum_{t<\\tau_{h}(s,a)}d_{h}^{\\prime}(s,a)\\mathsf{D}_{h}\\Big(\\overline{{P}}_{h}^{\\boldsymbol{M}^{\\star}}(s,a)\\big|\\big|\\overline{{P}}_{h}^{\\boldsymbol{\\hat{M}}^{t}}(s,a)\\Big)+\\displaystyle\\sum_{(s,a)\\in S\\times A}\\sum_{t\\geq\\tau_{h}(s,a)}d_{h}^{\\prime}(s,a)\\mathsf{D}_{h}\\Big(\\overline{{P}}_{h}^{\\boldsymbol{M}^{\\star}}(s,a)\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "For the first term above, we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\sum_{s,a)\\in S\\times A}\\sum_{t<\\tau_{h}(s,a)}d_{h}^{t}(s,a)\\mathsf{D}_{h}\\Big(\\overline{{P}}_{h}^{M^{*}}(s,a)\\|\\overline{{P}}_{h}^{\\tilde{M}^{t}}(s,a)\\Big)\\leq\\sum_{(s,a)\\in S\\times A}\\widetilde{d}_{h}^{\\tau_{h}(s,a)}(s,a)\\leq2C_{\\mathrm{cov}}\\sum_{(s,a)\\in S\\times A}d_{h}^{s,a}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where the last inequality holds because ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\sum_{(s,a)\\in S\\times A}\\widetilde{d}_{h}^{\\prime_{h}(s,a)}(s,a)=\\sum_{(s,a)\\in S\\times A}\\widetilde{d}_{h}^{\\prime_{h}(s,a)-1}(s,a)+d_{h}^{\\tau_{h}(s,a)}(s,a)\\leq2C_{\\mathrm{cov}}\\cdot\\mu_{h}^{\\star}(s,a).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "The remaining term is ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\substack{1=1\\,(s,a)\\in S\\times A\\,t\\ge\\tau_{h}(s,a)}}^{H}d_{h}^{t}(s,a)\\mathsf{D}_{h}\\Big(\\overline{{P}}_{h}^{\\,x^{\\star}}(s,a)\\|\\overline{{P}}_{h}^{\\,\\mathcal{M}^{t}}(s,a)\\Big)}\\\\ &{=\\displaystyle\\sum_{h=1}^{H}\\sum_{t=1}^{T}\\sum_{(s,a)\\in S\\times A}d_{h}^{t}(s,a)\\Bigg(\\frac{\\widetilde{d}_{h}^{t}(s,a)}{\\widetilde{d}_{h}^{t}(s,a)}\\Bigg)^{1/2}\\mathbb{1}\\{t\\ge\\tau_{h}(s,a)\\}\\mathsf{D}_{h}\\Big(\\overline{{P}}_{h}^{\\,u^{\\star}}(s,a)\\|\\overline{{P}}_{h}^{\\,\\widehat{M}^{t}}(s,a)\\Big)}\\\\ &{\\le\\sqrt{\\displaystyle\\sum_{h=1}^{H}\\sum_{t=1}^{T}\\sum_{(s,a)\\in S\\times A}\\frac{(\\mathbb{1}(t\\ge\\tau_{h}(s,a))d_{h}^{t}(s,a))^{2}}{\\widetilde{d}_{h}^{t}(s,a)}}\\cdot\\sqrt{\\displaystyle\\sum_{h=1}^{H}\\sum_{\\substack{t=1\\,(s,a)\\in S\\times A}}^{\\infty}\\widetilde{d}_{h}^{t}(s,a)\\mathsf{D}_{h}\\Big(\\overline{{P}}_{h}^{\\,u^{\\star}}(s,a)\\|\\overline{{P}}_{h}^{\\,\\mathcal{M}^{t}}(s,a)\\Big)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Following the derivation in Theorem 1 of Xie et al. [69], we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{=1}^{T}\\sum_{\\substack{(s,a)\\in S\\times A}}\\frac{(\\mathbb{1}(t\\geq\\tau_{h}(s,a))d_{h}^{\\iota}(s,a))^{2}}{\\widetilde{d}_{h}^{\\iota}(s,a)}\\leq2\\sum_{t=1}^{T}\\sum_{\\substack{(s,a)\\in S\\times A}}\\frac{d_{h}^{\\iota}(s,a)\\cdot d_{h}^{\\iota}(s,a)}{\\widetilde{d}_{h}^{\\iota}(s,a)+C_{\\mathrm{cov}}\\cdot\\mu_{h}^{\\star}(s,a)}}\\\\ &{\\leq2\\displaystyle\\sum_{t=1}^{T}\\sum_{\\substack{(s,a)\\in S\\times A}}\\operatorname*{max}_{t^{\\prime}\\in[T]}d_{h}^{\\prime}(s,a)\\cdot\\frac{d_{h}^{\\iota}(s,a)}{\\widetilde{d}_{h}^{\\iota}(s,a)+C_{\\mathrm{cov}}\\cdot\\mu_{h}^{\\star}(s,a)}}\\\\ &{\\leq2\\left(\\displaystyle\\operatorname*{max}_{s,a}\\sum_{t=1}^{T}\\frac{d_{h}^{\\iota}(s,a)}{\\widetilde{d}_{h}^{\\iota}(s,a)+C_{\\mathrm{cov}}\\cdot\\mu_{h}^{\\star}(s,a)}\\right)\\cdot\\left(\\sum_{(s,a)\\in S\\times A}\\!\\!\\!\\!\\!\\!\\!\\!\\!}_{t}\\right)}\\\\ &{\\lesssim C_{\\mathrm{cov}}\\log T,}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where the last inequality follows from Lemmas 3 and 4 of Xie et al. [69]. For the second term, as a consequence of the offline estimation assumption, we have that for all $t\\in[T]$ , ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\sum_{h=1}^{H}\\sum_{(s,a)\\in S\\times A}\\widetilde d_{h}^{t}(s,a)\\mathsf{D}_{h}\\Big(\\overline{{P}}_{h}^{M^{\\star}}(s,a)\\|\\overline{{P}}_{h}^{\\widehat M^{t}}(s,a)\\Big)=\\sum_{s=1}^{t-1}\\mathsf{D}^{\\mathsf{R L}}\\Big(\\widehat M^{t}(\\pi^{s})\\|M^{\\star}(\\pi^{s})\\Big)\\le\\beta_{\\mathsf{O f f}}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Altogether, we conclude that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathsf{D}^{\\mathsf{R L}}\\Big(\\widehat{M}^{t}(\\pi^{t})\\|M^{\\star}(\\pi^{t})\\Big)\\leq O\\Big(\\sqrt{H C_{\\mathsf{c o v}}\\beta_{\\mathsf{O f f}}T\\log T}+H C_{\\mathsf{c o v}}\\Big).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Corollary E.2. For any MDP class $(\\mathcal{M},\\Pi,\\mathcal{O})$ and $M^{\\star}~\\in~{\\cal M}$ , the sequence of estimators $(\\widehat{M}^{1},\\ldots,\\widehat{M}^{\\scriptscriptstyle T})$ produced by any offilne estimation oracle $\\mathbf{Alg}_{0\\mathsf{f f}}$ for squared Hellinger distance $\\mathsf{D}_{\\mathsf{H}}^{2}$ with para meter $\\beta_{0\\mathsf{f f}}$ satisfy ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathbf{Est}_{\\mathsf{H}}^{\\mathrm{On}}(T)=\\sum_{t=1}^{T}\\mathsf{D}_{\\mathsf{H}}^{2}\\Big(\\widehat{M}^{t}(\\pi^{t}),M^{\\star}(\\pi^{t})\\Big)\\leq O\\Big(H\\sqrt{C_{\\mathsf{c o v}}(M^{\\star})\\beta_{\\mathsf{O f}}T\\log T}+H^{2}C_{\\mathsf{c o v}}(M^{\\star})\\Big)\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Proof of Corollary E.2. By Lemma F.2, for any two MDP models $M$ and $M^{\\prime}$ and any $\\pi\\in\\Delta(\\Pi_{\\mathrm{RNS}})$ , we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathsf{D}_{\\mathsf{H}}^{2}(M(\\pi),M^{\\prime}(\\pi))=\\mathsf{D}_{\\mathsf{H}}^{2}(M^{\\prime}(\\pi),M(\\pi))\\le7\\cdot\\sum_{h=1}^{H}\\mathbb{E}^{M^{\\prime},\\pi}\\Bigl[\\mathsf{D}_{\\mathsf{H}}^{2}\\Bigl(\\overline{{P}}_{h}^{M^{\\prime}}(s_{h},a_{h}),\\overline{{P}}_{h}^{M}(s_{h},a_{h})\\Bigr)\\Bigr].\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "On the other hand, for any $h\\in[H]$ , we have from Lemma A.9 of Foster et al. [27] that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}^{M^{\\prime},\\pi}\\Bigl[\\mathsf{D}_{\\mathsf{H}}^{2}\\Bigl(\\overline{{P}}_{h}^{M^{\\prime}}(s_{h},a_{h}),\\overline{{P}}_{h}^{M}(s_{h},a_{h})\\Bigr)\\Bigr]\\leq4\\mathsf{D}_{\\mathsf{H}}^{2}(M(\\pi),M^{\\prime}(\\pi)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "by choosing $X=(s_{h},a_{h})$ and $Y=\\left(r_{h},s_{h+1}\\right)$ in the aforementioned lemma. Summing up over $h$ , we conclude that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\sum_{h=1}^{H}\\mathbb{E}^{M^{\\prime},\\pi}\\Bigl[\\mathsf{D}_{\\mathsf{H}}^{2}\\Bigl(\\overline{{P}}_{h}^{M^{\\prime}}(s_{h},a_{h}),\\overline{{P}}_{h}^{M}(s_{h},a_{h})\\Bigr)\\Bigr]\\le4H\\cdot\\mathsf{D}_{\\mathsf{H}}^{2}(M(\\pi),M^{\\prime}(\\pi)).\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Consider any sequence of policies $\\pi^{1},\\cdot\\cdot\\cdot,\\pi^{T}$ and outputs $\\widehat{M^{1}},\\ldots,\\widehat{M^{\\scriptscriptstyle T}}$ from an offline oracle with parameter $\\beta_{0\\mathsf{f f}}$ for squared Hellinger distance. By Eq. (36) , we have  that for all $t\\in[T]$ , ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\sum_{\\tau=1}^{t-1}\\sum_{h=1}^{H}\\mathbb{E}^{M^{\\star},\\pi^{\\tau}}\\left[\\mathsf{D}_{\\mathsf{H}}^{2}\\Big(\\overline{{P}}_{h}^{M^{\\star}}(s,a),\\overline{{P}}_{h}^{\\hat{M}^{\\tau}}(s,a)\\Big)\\right]\\leq4H\\sum_{\\tau=1}^{t-1}\\mathsf{D}_{\\mathsf{H}}^{2}\\Big(\\widehat{M^{\\tau}}(\\pi^{\\tau}),M^{\\star}(\\pi^{\\tau})\\Big)\\leq4\\beta_{\\mathsf{O H}}H.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Theorem E.1 thus implies that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\sum_{h=1}^{H}\\mathbb{E}^{M^{\\star},\\pi^{t}}\\left[\\mathsf{D}_{\\mathsf{H}}^{2}\\Big(\\overline{{P}}_{h}^{M^{\\star}}(s,a),\\overline{{P}}_{h}^{\\widehat{M}^{t}}(s,a)\\Big)\\right]\\lesssim H\\sqrt{C_{\\mathrm{cov}}\\beta_{\\mathsf{O f f}}T\\log T}+H^{2}C_{\\mathrm{cov}}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Finally, using Eq. (35), we can convert the inequality above into a bound on the square Hellinger distance: ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\mathsf{D}_{\\mathsf{H}}^{2}\\Big(\\widehat{M}^{t}(\\pi^{t}),M^{\\star}(\\pi^{t})\\Big)\\lesssim\\sum_{t=1}^{T}\\sum_{h=1}^{H}\\mathbb{E}^{M^{\\star},\\pi^{t}}\\Big[\\mathsf{D}_{\\mathsf{H}}^{2}\\Big(\\overline{{P}}_{h}^{M^{\\star}}(s,a),\\overline{{P}}_{h}^{\\widehat{M}^{t}}(s,a)\\Big)\\Big]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\lesssim H\\sqrt{C_{\\mathsf{c o v}}\\beta_{\\mathsf{O f f}}T\\log T}+H^{2}C_{\\mathsf{c o v}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Proposition E.2 (Tightness of offline-to-online conversion). For any integer $T\\geq1$ and $\\beta_{0\\mathsf{f}\\mathsf{f}}>0$ , there exists a contextual bandit class $(\\mathcal{M},\\Pi=\\mathcal{A}^{s},\\mathcal{O})$ with $|{\\mathcal{A}}|=2$ , a distribution $d_{1}\\,\\in\\,\\Delta(S)$ , a sequence $(\\pi^{1},\\cdot\\cdot\\cdot,\\pi^{T})$ and an offline oracle AlgOff for DCB with parameter $\\beta_{\\mathsf{O f f}}$ such that the oracle\u2019s outputs $(\\widehat{M}^{1},\\ldots,\\widehat{M}^{\\scriptscriptstyle T})$ satisfy ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathbf{Est}_{\\mathsf{D}}^{\\mathsf{O n}}(T)=\\sum_{t=1}^{T}\\mathsf{D}_{\\mathsf{C B}}\\Big(\\widehat{M}^{t}(\\pi^{t}),M^{\\star}(\\pi^{t})\\Big)\\geq\\Omega\\Big(\\sqrt{T\\beta_{\\mathsf{O f f}}}\\Big).\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Proof of Proposition E.2. Let $\\beta_{0\\mathsf{f}\\mathsf{f}}>0$ and $T$ be given, and let $N\\in\\mathbb{N}$ be chosen such that $T=$ $N\\cdot\\lfloor N\\beta_{0\\mathsf{f f}}\\rfloor$ ; we assume without loss of generality that $N$ is large enough such that $N\\beta_{0\\mathsf{f f}}>1$ , which implies that $\\lfloor N\\beta_{0\\mathsf{f f}}\\rfloor\\ge N\\beta_{0\\mathsf{f f}}/2)$ . Define $\\boldsymbol{\\mathcal{A}}=\\{a_{0},a_{1}\\}$ , ${\\cal S}=\\{s_{0},\\dots,s_{N-1}\\}$ , and $g^{M^{\\star}}(s,a)\\equiv0$ ", "page_idx": 49}, {"type": "text", "text": "for all $s\\in{\\mathcal{S}},a\\in{\\mathcal{A}}$ . Let $d_{1}=\\operatorname{Unif}(S)$ be the context distribution. For any $t\\in\\{1,\\ldots,T\\}$ , consider the deterministic policy $\\pi^{t}:{\\mathcal{S}}\\rightarrow A$ and the offline estimatorM t defined via ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pi^{t}(s_{n})=\\left\\{\\!\\!\\begin{array}{l l}{a_{1},}&{\\mathrm{if~}n=\\lfloor t/\\lfloor N\\beta_{\\mathrm{Off}}\\rfloor\\!\\right\\rfloor,}\\\\ {a_{0},}&{\\mathrm{otherwise}.}\\end{array}\\right.\\quad\\mathrm{and~}g^{\\widetilde{\\scriptscriptstyle M}^{t}}(s,a)=\\left\\{\\!\\!\\begin{array}{l l}{1,}&{\\mathrm{if~}s=s_{\\lfloor t/\\lfloor N\\beta_{\\mathrm{Off}}\\rfloor\\rfloor},a=a_{1},}\\\\ {0,}&{\\mathrm{otherwise}.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "We first verify that $\\widehat{M^{1}},\\ldots,\\widehat{M^{\\scriptscriptstyle T}}$ satisfy the offilne oracle requirement. For any $t\\in\\{1,\\ldots,T\\}$ , we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{\\tau=1}^{t-1}\\mathbb{E}_{s\\sim d_{1}}\\left[\\mathsf{D}_{s\\mathsf{q}}\\big(g^{\\boldsymbol{M}}\\big(s,\\pi^{\\tau}\\big(s\\big)\\big),g^{\\widehat{\\boldsymbol{M}}\\tau}\\big(s,\\pi^{\\tau}\\big(s)\\big)\\big)\\right]}}\\\\ &{}&{\\quad=\\displaystyle\\frac{1}{N}\\sum_{\\tau=1}^{t-1}\\mathbb{1}\\big\\{\\pi^{\\tau}\\big(s_{\\lfloor t/N\\rfloor}\\big)=a_{1}\\big\\}}\\\\ &{}&{\\quad=\\displaystyle\\frac{1}{N}\\cdot(t-\\lfloor t/\\lfloor N\\beta_{\\mathsf{O H}}\\rfloor\\rfloor\\cdot\\lfloor N\\beta_{\\mathsf{O H}}\\rfloor)\\leq\\beta_{\\mathsf{O H}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "However, the online error is ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}_{s\\sim d_{1}}\\left[\\operatorname D_{s\\P}\\!\\left(g^{M}(s,\\pi^{t}(s)),g^{\\widehat{M}^{t}}(s,\\pi^{t}(s))\\right)\\right]=\\sum_{t=1}^{T}\\frac{1}{N}=\\lfloor N\\beta_{0\\mathrm{ff}}\\rfloor\\geq\\frac{1}{2}\\sqrt{T\\beta_{0\\mathrm{ff}}}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 51}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 51}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 51}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 51}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 51}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 51}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. 1. Claims ", "page_idx": 51}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] . ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. The claims are validated by detailed proofs. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 51}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 52}, {"type": "text", "text": "Justification: See Section 1.2. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 52}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] . ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Justification: The paper provides detailed assumptions and proofs. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 52}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 53}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 53}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 53}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). ", "page_idx": 53}, {"type": "text", "text": "", "page_idx": 54}, {"type": "text", "text": "\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 54}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 54}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 54}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 54}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 54}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 54}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 54}, {"type": "text", "text": "", "page_idx": 55}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 55}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 55}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 55}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 55}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 55}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 55}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 55}, {"type": "text", "text": "Justification: This is a theoretical work. There is no societal impact on the work performed. Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 55}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 56}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 56}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 56}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 56}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 56}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 56}, {"type": "text", "text": "Justification: This paper does not use existing assets. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 56}, {"type": "text", "text": "", "page_idx": 57}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 57}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 57}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 57}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 57}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 57}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing or research with human subjects. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 58}]