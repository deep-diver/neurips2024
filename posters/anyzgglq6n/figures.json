[{"figure_path": "anyZgGLQ6n/figures/figures_1_1.jpg", "caption": "Figure 1: The resulting state distributions of offline RL algorithms and optimal values of states. (a,b,c) The state distributions generated by the learned policies of various algorithms compared with that of the offline dataset on halfcheetah-medium-expert. (d) The corresponding optimal value of each state, which is obtained by running TD3 online to convergence. SCAS-induced state distribution is almost entirely within the support of the offline distribution and avoids the low-value areas, while CQL and TD3BC tend to produce OOD states with extremely low values.", "description": "This figure compares the state distributions generated by three offline RL algorithms (CQL, TD3BC, and SCAS) with the optimal state distribution on the HalfCheetah-medium-expert environment.  The plots (a), (b), and (c) show that SCAS is much better at keeping its state distribution within the range of the offline data, which is an indication of its effectiveness at handling out-of-distribution (OOD) states. Plot (d) displays the optimal value for each state, visually demonstrating that SCAS successfully avoids OOD states with low values, unlike CQL and TD3BC.", "section": "6.1 Empirical Evidence of OOD State Correction and OOD Action Suppression"}, {"figure_path": "anyZgGLQ6n/figures/figures_6_1.jpg", "caption": "Figure 2: Oracle Q-values of SCAS (estimated by MC return) and learned Q-values of SCAS and other algorithms across optimization steps. Only SCAS's OOD state correction term can achieve OOD action suppression and prevent value over-estimation (divergence).", "description": "This figure compares the learned Q-values of SCAS with three baseline methods: ordinary off-policy RL, SDC without CQL, and OSR without CQL.  The oracle Q-values for SCAS are estimated using Monte Carlo returns. The plot shows that the Q-values of the baseline methods diverge, indicating value overestimation and OOD actions.  In contrast, SCAS's learned Q-values remain close to the oracle values, showing that its OOD state correction effectively suppresses OOD actions and prevents overestimation.", "section": "Empirical Evidence of OOD State Correction and OOD Action Suppression"}, {"figure_path": "anyZgGLQ6n/figures/figures_8_1.jpg", "caption": "Figure 3: Comparisons in the perturbed environments with varying perturbation levels. The perturbation steps are the steps of Gaussian noise added to the conducted actions in an episode. SCAS exhibits better robustness against environmental perturbations during the test phase.", "description": "This figure compares the performance of SCAS and other offline RL algorithms (SDC, CQL, TD3+BC) in perturbed environments.  The x-axis represents the number of perturbation steps (amount of Gaussian noise added to actions). The y-axis shows the normalized return, a measure of algorithm performance.  The shaded regions represent the standard deviation across multiple runs. The plot demonstrates that SCAS shows greater robustness to increasing perturbation levels, maintaining higher performance than the other algorithms.", "section": "6.3 Comparisons in Perturbed Environments"}, {"figure_path": "anyZgGLQ6n/figures/figures_8_2.jpg", "caption": "Figure 1: The resulting state distributions of offline RL algorithms and optimal values of states. (a,b,c) The state distributions generated by the learned policies of various algorithms compared with that of the offline dataset on halfcheetah-medium-expert. (d) The corresponding optimal value of each state, which is obtained by running TD3 online to convergence. SCAS-induced state distribution is almost entirely within the support of the offline distribution and avoids the low-value areas, while CQL and TD3BC tend to produce OOD states with extremely low values.", "description": "This figure compares the state distributions generated by different offline RL algorithms (CQL, TD3BC, and SCAS) with the optimal state distribution obtained from online TD3 training.  It visualizes how well each algorithm's learned policy keeps the agent's states within the distribution of the offline dataset and avoids low-value, out-of-distribution (OOD) states. SCAS demonstrates a superior ability to remain within the in-distribution (ID) state space.", "section": "Empirical Evidence of OOD State Correction and OOD Action Suppression"}, {"figure_path": "anyZgGLQ6n/figures/figures_21_1.jpg", "caption": "Figure 2: Oracle Q-values of SCAS (estimated by MC return) and learned Q-values of SCAS and other algorithms across optimization steps. Only SCAS's OOD state correction term can achieve OOD action suppression and prevent value over-estimation (divergence).", "description": "This figure compares the learned Q-values of SCAS against three baseline methods: ordinary off-policy RL, SDC without CQL, and OSR without CQL.  It shows how SCAS's unique OOD state correction prevents the Q-values from diverging (overestimating values), a common issue in offline RL caused by out-of-distribution actions. The oracle Q-values, estimated using Monte Carlo returns, serve as a ground truth comparison. The figure demonstrates that only SCAS effectively suppresses OOD actions and maintains accurate Q-value estimations during training.", "section": "Empirical Evidence of OOD State Correction and OOD Action Suppression"}, {"figure_path": "anyZgGLQ6n/figures/figures_24_1.jpg", "caption": "Figure 6: Additional results from the parameter study on the inverse temperature \u03b1. The curves are averaged over 5 random seeds, with the shaded area representing the standard deviation across seeds.", "description": "This figure shows the results of experiments on the effect of the inverse temperature \u03b1 on the performance of the SCAS algorithm.  It displays learning curves for SCAS with different values of \u03b1 across four different AntMaze datasets.  The results demonstrate that a large \u03b1 is crucial for achieving good performance, showcasing the effectiveness of value-aware OOD state correction. However, excessively large \u03b1 values can lead to less satisfying performance due to increased variance in the learning objective.  The shaded areas represent the standard deviations across 5 random seeds.", "section": "6.4 Parameter Study"}, {"figure_path": "anyZgGLQ6n/figures/figures_24_2.jpg", "caption": "Figure 3: Comparisons in the perturbed environments with varying perturbation levels. The perturbation steps are the steps of Gaussian noise added to the conducted actions in an episode. SCAS exhibits better robustness against environmental perturbations during the test phase.", "description": "This figure compares the performance of SCAS and other offline RL algorithms in perturbed environments.  The x-axis represents the number of perturbation steps (how many times Gaussian noise was added to the actions taken during an episode), and the y-axis represents the normalized return.  The figure demonstrates that SCAS exhibits enhanced robustness to environmental perturbations, maintaining better performance than the other algorithms as the number of perturbation steps increases.", "section": "6.3 Comparisons in Perturbed Environments"}, {"figure_path": "anyZgGLQ6n/figures/figures_25_1.jpg", "caption": "Figure 3: Comparisons in the perturbed environments with varying perturbation levels. The perturbation steps are the steps of Gaussian noise added to the conducted actions in an episode. SCAS exhibits better robustness against environmental perturbations during the test phase.", "description": "This figure compares the performance of SCAS and other offline RL algorithms in perturbed environments.  The x-axis shows the number of perturbation steps (amount of Gaussian noise added to actions). The y-axis represents the normalized return achieved by each algorithm.  The results demonstrate that SCAS is more robust to environmental perturbations, maintaining higher performance even with a substantial number of perturbation steps, unlike the other algorithms whose performance degrades significantly. This robustness highlights the effectiveness of SCAS's OOD state correction in handling real-world uncertainties.", "section": "6.3 Comparisons in Perturbed Environments"}, {"figure_path": "anyZgGLQ6n/figures/figures_25_2.jpg", "caption": "Figure 3: Comparisons in the perturbed environments with varying perturbation levels. The perturbation steps are the steps of Gaussian noise added to the conducted actions in an episode. SCAS exhibits better robustness against environmental perturbations during the test phase.", "description": "This figure compares the performance of SCAS and other algorithms in perturbed environments.  The x-axis represents the number of perturbation steps (how many times Gaussian noise is added to actions in an episode), and the y-axis represents the normalized return.  The plot shows that SCAS is much more robust to perturbations compared to other offline RL algorithms, maintaining a significantly higher return even with a substantial number of perturbation steps.", "section": "6.3 Comparisons in Perturbed Environments"}, {"figure_path": "anyZgGLQ6n/figures/figures_26_1.jpg", "caption": "Figure 1: The resulting state distributions of offline RL algorithms and optimal values of states. (a,b,c) The state distributions generated by the learned policies of various algorithms compared with that of the offline dataset on halfcheetah-medium-expert. (d) The corresponding optimal value of each state, which is obtained by running TD3 online to convergence. SCAS-induced state distribution is almost entirely within the support of the offline distribution and avoids the low-value areas, while CQL and TD3BC tend to produce OOD states with extremely low values.", "description": "This figure compares the state distributions generated by different offline RL algorithms (CQL, TD3BC, and SCAS) with the optimal state distribution obtained from an online TD3 algorithm.  The subfigures (a), (b), and (c) show the state distributions generated by CQL, TD3BC, and SCAS respectively, in comparison to the offline dataset's state distribution. Subfigure (d) shows the optimal value associated with each state.  The key takeaway is that SCAS's state distribution closely matches the offline dataset's, avoiding low-value, out-of-distribution states which are present in the CQL and TD3BC results.", "section": "Empirical Evidence of OOD State Correction and OOD Action Suppression"}, {"figure_path": "anyZgGLQ6n/figures/figures_27_1.jpg", "caption": "Figure 1: The resulting state distributions of offline RL algorithms and optimal values of states. (a,b,c) The state distributions generated by the learned policies of various algorithms compared with that of the offline dataset on halfcheetah-medium-expert. (d) The corresponding optimal value of each state, which is obtained by running TD3 online to convergence. SCAS-induced state distribution is almost entirely within the support of the offline distribution and avoids the low-value areas, while CQL and TD3BC tend to produce OOD states with extremely low values.", "description": "This figure compares the state distributions generated by different offline RL algorithms (CQL, TD3BC, and SCAS) with the optimal state distribution obtained using TD3. It shows that SCAS produces a state distribution almost entirely within the support of the offline dataset, avoiding low-value states, unlike CQL and TD3BC which tend to generate out-of-distribution (OOD) states with extremely low values.", "section": "Empirical Evidence of OOD State Correction and OOD Action Suppression"}]