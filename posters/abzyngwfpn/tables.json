[{"figure_path": "AbZyNGWfpN/tables/tables_6_1.jpg", "caption": "Table 1: Top-1 accuracy (%) on FGVC and VTAB-1k benchmarks using ViT-B/16 pre-trained on ImageNet-21k supervisedly. The best result is in bold, and the second-best result is underlined.", "description": "This table presents a comparison of the performance of various parameter-efficient fine-tuning (PEFT) methods on two benchmark datasets: FGVC (fine-grained visual categorization) and VTAB-1k (a large-scale transfer learning benchmark).  The methods are compared based on their top-1 accuracy across multiple downstream tasks within each benchmark.  The table highlights the superior performance of the proposed method (SNELL) compared to existing state-of-the-art PEFT approaches.", "section": "4.2 Performance on Downstream Tasks"}, {"figure_path": "AbZyNGWfpN/tables/tables_6_2.jpg", "caption": "Table 2: Top-1 accuracy (%) on VTAB-1k benchmarks using ViT-B/16 backbone pre-trained on ImageNet using MAE and MoCo v3 strategies. The best result is in bold.", "description": "This table presents the performance comparison of different parameter-efficient fine-tuning (PEFT) methods on the VTAB-1k benchmark. The models are pre-trained on ImageNet using two different self-supervised learning strategies: Masked Autoencoders (MAE) and Momentum Contrast (MoCo v3).  The table shows the top-1 accuracy achieved by each method on three subsets of VTAB-1k tasks (Natural, Specialized, Structured), as well as the average accuracy across all tasks.  The best result for each metric is highlighted in bold.", "section": "4.2 Performance on Downstream Tasks"}, {"figure_path": "AbZyNGWfpN/tables/tables_7_1.jpg", "caption": "Table 1: Top-1 accuracy (%) on FGVC and VTAB-1k benchmarks using ViT-B/16 pre-trained on ImageNet-21k supervisedly. The best result is in bold, and the second-best result is underlined.", "description": "This table presents the comparison of different parameter-efficient fine-tuning (PEFT) methods on two benchmark datasets, FGVC and VTAB-1k, using a ViT-B/16 model pre-trained on ImageNet-21k.  It compares methods based on whether they use addition-based or reparameterization-based approaches, along with various sparsity and rank parameters. The table highlights the superior performance of the proposed SNELL method compared to existing state-of-the-art methods.", "section": "4.2 Performance on Downstream Tasks"}, {"figure_path": "AbZyNGWfpN/tables/tables_8_1.jpg", "caption": "Table 4: Comparisons on VTAB-1k benchmark with supervised pre-trained ViT-L/16 and ViT-H/16. Top-1 accuracy is reported. The best result is in bold.", "description": "This table compares the performance of LoRA-8 and SNELL-8 on the VTAB-1k benchmark using two different large vision transformer backbones: ViT-L/16 and ViT-H/14.  The results are presented in terms of top-1 accuracy, broken down by three categories of tasks within the VTAB-1k benchmark: Natural, Specialized, and Structured.  The table highlights that SNELL-8 achieves superior performance compared to LoRA-8 on both backbones across all categories and overall mean accuracy.", "section": "4.2 Performance on Downstream Tasks"}, {"figure_path": "AbZyNGWfpN/tables/tables_8_2.jpg", "caption": "Table 5: (a) Performance on VTAB-1k of sparsifying a full-rank matrix, the merged adaptation matrix of LoRA-8 and kernelized LoRA-8 (KLORA-8) with sparsity ratio s = 0.9. (b) The mean accuracy on VTAB-1k of kernelized LoRA (KLORA) and SNELL (KLoRA+sparsifying) with different ranks of learnable matrices. Perf. Imp. denotes the performance improvement of SNELL over KLORA.", "description": "This table presents a comparison of different sparsification methods' performance on the VTAB-1k benchmark.  Part (a) compares sparsifying a full-rank matrix, LoRA-8's merged adaptation matrix, and kernelized LoRA-8's merged matrix, all with a sparsity ratio of 0.9.  Part (b) compares the mean accuracy across VTAB-1k of kernelized LoRA and SNELL using different ranks (8, 16, and 32) for the learnable matrices, highlighting the performance improvement achieved by SNELL over kernelized LoRA.", "section": "4.3 Ablation Studies"}, {"figure_path": "AbZyNGWfpN/tables/tables_16_1.jpg", "caption": "Table 1: Top-1 accuracy (%) on FGVC and VTAB-1k benchmarks using ViT-B/16 pre-trained on ImageNet-21k supervisedly. The best result is in bold, and the second-best result is underlined.", "description": "This table presents the results of various parameter-efficient fine-tuning (PEFT) methods on two benchmark datasets: FGVC (fine-grained visual categorization) and VTAB-1k (a large-scale transfer learning benchmark).  The methods are compared using the top-1 accuracy metric.  The table is organized to show the performance of different approaches (addition-based and reparameterization-based methods including the proposed SNELL approach) on the benchmarks.  The best and second best performance numbers are highlighted.", "section": "4.2 Performance on Downstream Tasks"}, {"figure_path": "AbZyNGWfpN/tables/tables_16_2.jpg", "caption": "Table 1: Top-1 accuracy (%) on FGVC and VTAB-1k benchmarks using ViT-B/16 pre-trained on ImageNet-21k supervisedly. The best result is in bold, and the second-best result is underlined.", "description": "This table presents the Top-1 accuracy results achieved by various methods (including the proposed SNELL) on two benchmark datasets: FGVC (fine-grained visual categorization) and VTAB-1k (a large-scale transfer learning benchmark).  The results are broken down by specific tasks within each benchmark and show the performance of different parameter-efficient fine-tuning methods, including addition-based and reparameterization-based approaches. The ViT-B/16 model, pre-trained on ImageNet-21k, is used as the backbone for all methods.  The best and second-best performing methods for each task are highlighted.", "section": "4.2 Performance on Downstream Tasks"}, {"figure_path": "AbZyNGWfpN/tables/tables_17_1.jpg", "caption": "Table 1: Top-1 accuracy (%) on FGVC and VTAB-1k benchmarks using ViT-B/16 pre-trained on ImageNet-21k supervisedly. The best result is in bold, and the second-best result is underlined.", "description": "This table presents a comparison of the top-1 accuracy achieved by various methods on two benchmark datasets: FGVC (fine-grained visual categorization) and VTAB-1k (a large-scale transfer learning benchmark).  The methods are categorized into addition-based and reparameterization-based parameter-efficient fine-tuning (PEFT) approaches. Results are shown for different variations of each method (e.g., different rank values for LoRA). The table highlights the superior performance of the proposed SNELL method compared to existing state-of-the-art methods while maintaining low memory usage.", "section": "4.2 Performance on Downstream Tasks"}, {"figure_path": "AbZyNGWfpN/tables/tables_17_2.jpg", "caption": "Table 2: Top-1 accuracy (%) on VTAB-1k benchmarks using ViT-B/16 backbone pre-trained on ImageNet using MAE and MoCo v3 strategies. The best result is in bold.", "description": "This table presents the performance of different methods on the VTAB-1k benchmark.  The models used are Vision Transformer backbones (ViT-B/16) pre-trained using Masked Autoencoders (MAE) and Momentum Contrast (MoCo v3) strategies.  The results are broken down by three groups of tasks within VTAB-1k: Natural, Specialized, and Structured, along with the overall mean accuracy.  The best performing method for each task group and the overall mean is highlighted in bold.", "section": "4.2 Performance on Downstream Tasks"}, {"figure_path": "AbZyNGWfpN/tables/tables_17_3.jpg", "caption": "Table A10: Performance comparisons on FGVC benchmark between kernelized LoRA with fixed weight masks (KLORA-8-Fixed) and our dynamical masks (SNELL-8).", "description": "This table compares the performance of kernelized LoRA with fixed weight masks (KLORA-8-Fixed) and SNELL-8 on the FGVC benchmark.  KLORA-8-Fixed uses pre-defined weight masks generated by SPT [22], while SNELL-8 uses a dynamic masking strategy. The results show that SNELL-8 outperforms KLORA-8-Fixed across all five FGVC tasks, indicating the superiority of the dynamic masking approach.", "section": "C.3.1 Comparison between Competition-based Sparsification and Pre-defined Weight Mask"}, {"figure_path": "AbZyNGWfpN/tables/tables_18_1.jpg", "caption": "Table A11: Comparisons between LoRA and kernelized LoRA (KLORA) on VTAB-1k using ViT-B/16 pre-trained on ImageNet21k supervisedly. Better performance for the same rank is in bold.", "description": "This table compares the performance of LoRA and kernelized LoRA on the VTAB-1k benchmark.  Both methods use the same pre-trained ViT-B/16 model on ImageNet-21k.  The table shows the mean accuracy across different task categories (Natural, Specialized, Structured) for different rank sizes (8, 16, and 32) for both LoRA and kernelized LoRA.  The results highlight the performance improvement achieved by using kernelized LoRA compared to the standard LoRA method for each rank size.", "section": "C.3.2 Comparison between Kernelized LoRA and LoRA"}, {"figure_path": "AbZyNGWfpN/tables/tables_18_2.jpg", "caption": "Table A12: Memory usage comparison between SNELL and LoRA. A Mem. denotes the incremental memory usage of SNELL in comparison to LORA.", "description": "This table compares the memory usage of SNELL and LoRA for three different pre-trained vision transformer models (ViT-B/16, ViT-L/16, and ViT-H/16).  It shows the memory consumption in MB for both LoRA-8 and SNELL-8, and calculates the percentage increase in memory usage for SNELL-8 relative to LoRA-8.  The results indicate that while SNELL-8 uses slightly more memory than LoRA-8, the increase is minimal, demonstrating the memory efficiency of SNELL.", "section": "A More Details of Experimental Setup"}, {"figure_path": "AbZyNGWfpN/tables/tables_18_3.jpg", "caption": "Table A13: Performance on commonsense reasoning benchmark with LLaMA-2-7B.", "description": "This table presents the results of the experiments conducted on the commonsense reasoning benchmark using the LLaMA-2-7B model.  The table compares the performance of LoRA-32 and SNELL-32 on various sub-tasks within the benchmark, providing the accuracy scores for each method on each sub-task, and finally the average accuracy across all sub-tasks. This demonstrates the superiority of SNELL-32 over LoRA-32 in commonsense reasoning tasks.", "section": "C.4 Experiments on Large Language Models"}, {"figure_path": "AbZyNGWfpN/tables/tables_19_1.jpg", "caption": "Table A14: Training time cost on ViT-B/16 of different PEFT methods.", "description": "This table compares the training time (seconds per image) of different parameter-efficient fine-tuning (PEFT) methods on a ViT-B/16 model.  The methods compared are LoRA-8, KLORA-8 (kernelized LoRA-8), KLORA-8 (saving \u0394W) which saves memory by not storing the merged adaptation matrix, SNELL-8, and SNELL-8 (saving \u0394W).  The results show that SNELL-8 takes slightly longer to train than LoRA-8, but that the memory saving modifications for both KLORA and SNELL significantly reduce training time.", "section": "4 Experiments"}]